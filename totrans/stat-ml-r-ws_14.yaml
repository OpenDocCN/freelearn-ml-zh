- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statistical Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce you to a range of statistical techniques
    that enable you to make inferences and estimations using both numerical and categorical
    data. We will explore key concepts and methods, such as hypothesis testing, confidence
    intervals, and estimation techniques, that empower us to make generalizations
    about populations from a given sample.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will grasp the core concepts of statistical
    inference and be able to perform hypothesis testing in different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Statistical inference for categorical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical inference for numerical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing the bootstrapped confidence interval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the central limit theorem used in t-distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing the confidence interval for the population mean using the t-distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing hypothesis testing for two means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing ANOVA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To run the code in this chapter, you will need to have the latest versions
    of the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dplyr`, 1.0.10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ggplot2`, 3.4.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`socviz`, 1.2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`infer`, 1.0.4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that the versions mentioned in the preceding list are the latest
    ones at the time I am writing this book. All the code and data for this chapter
    is available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_11/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_11/working.R).
  prefs: []
  type: TYPE_NORMAL
- en: Statistical inference for categorical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A categorical variable has distinct categories or levels, rather than numerical
    values. Categorical data is common in our daily lives, such as gender (male or
    female, although a modern view may differ), type of property sales (new property
    or resale), and industry. The ability to make sound inferences about these variables
    is thus essential for drawing meaningful conclusions and making well-informed
    decisions in diverse contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Being a categorical variable often means we cannot pass it to a `string` values
    such as `"finance"` or `"technology"`) to the model, a common approach is to one-hot
    encode the variable into multiple columns, with each column corresponding to a
    specific industry, indicating a binary value of `0` or `1`.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore various statistical techniques designed specifically
    to handle categorical data, enabling us to derive valuable insights and make inferences
    about populations based on available samples. We will also discuss important concepts,
    such as proportions, independence, and goodness of fit, which form the foundation
    for understanding and working with categorical variables, covering both cases
    with a single parameter and multiple parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Let us start by discussing the inference for a single parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical inference for a single parameter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A population parameter, the subject of interest and to be inferred, is a fixed
    quantity that describes a particular statistical attribute of a population, including
    the mean, proportion, or standard deviation. This quantity often stays hidden
    from us. For example, in order to get the most popular major in a university,
    we need to count the number of enrolled students in each major across the whole
    university and then return the major with the biggest count.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of statistical inference for a single parameter, we aim to estimate
    this unknown parameter or test hypotheses about its value based on the information
    gathered from a sample. In other words, we would use statistical inference tools
    to infer unknown population parameters based on the known sample at hand. In the
    previous example, we would infer the most popular major of the whole university
    by a limited sample of students enrolled in a specific academic year.
  prefs: []
  type: TYPE_NORMAL
- en: Let us first explore the **General Social Survey** (**GSS**) dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the General Social Survey dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GSS is a comprehensive dataset widely used by researchers and policymakers
    to understand social, cultural, and political trends in the United States. The
    GSS has been continued by the **National Opinion Research Center** (**NORC**)
    at the University of Chicago since 1972, with the objective of collecting data
    on a broad range of topics, including attitudes, behaviors, and opinions on various
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us load the GSS dataset from the `socviz` package (remember to install
    this package via `install.packages("socviz")`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The GSS dataset is now stored in the `gss_lon` variable, which contains a total
    of 62,466 rows and 25 columns, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The GSS dataset contains numerous variables that cover diverse topics, such
    as education, income, family structure, political beliefs, and religious affiliation.
    Let us examine the structure of the dataset using the `glimpse()` function from
    the `dplyr` package, designed to help you quickly explore and understand the structure
    of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 11**.1* shows a screenshot of the first few variables returned.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 â€“ Showing the first few rows of the result from running the glimpse()
    function](img/B18680_11_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 â€“ Showing the first few rows of the result from running the glimpse()
    function
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at calculating a specific statistic based on a categorical
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the sample proportion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `siblings` column in the dataset is a categorical variable that tracks the
    number of siblings in the family. In the following exercise, we would like to
    calculate the proportion of survey respondents whose family has two siblings in
    the latest year, 2016.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.1 â€“ calculating the sample proportion of siblings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we first obtain a summary of the `siblings` column and subset
    the dataset to focus on the year 2016, which will then be used to calculate the
    proportion of surveys with a specific number of siblings in the family:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain a summary of the `siblings` column using the `summary()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result suggests that most surveys are conducted for families with six siblings
    or more!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Subset the dataset for the year 2016:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running the code generates the chart in *Figure 11**.2*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.2 â€“ Visualizing the frequency count of the number of siblings in
    a bar chart](img/B18680_11_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 â€“ Visualizing the frequency count of the number of siblings in a
    bar chart
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the proportion of surveys with two siblings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `summarize()` function to calculate the mean of a series of
    binary values, which corresponds to the proportion of surveys with two siblings.
    We then use the `pull()` function to obtain the proportion from the resulting
    DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We use the sample proportion to estimate the population statistic. In other
    words, we calculate the proportion of families with two siblings based on the
    available samples to approximate the corresponding proportion if we were to calculate
    the same based on all the data in the population. Such an estimate comes with
    a confidence interval that quantifies the list of possible values for the population
    proportion.
  prefs: []
  type: TYPE_NORMAL
- en: The next section shows how to calculate the confidence interval for the sample
    proportion.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the confidence interval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The confidence interval is an important tool in making inferences about the
    population parameters based on sample data. A confidence interval provides an
    estimated range within which a population parameter, such as proportion, is likely
    to be found with a specified confidence level, such as 95%. When working with
    sample proportions, calculating confidence intervals allows us to understand the
    true proportion in the population better and gauge the uncertainty associated
    with the estimation of the population proportion.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the following steps to calculate the confidence interval:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the sample proportion, Â Ë†Â pÂ  (pronounced as p-hat). This is the value
    we calculated based on the sample data in 2016\. In other contexts, Â Ë†Â pÂ  is calculated
    by dividing the number of successes (for the attribute of interest) by the total
    sample size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the desired level of confidence, commonly denoted as (1 âˆ’ Î±) x 100%,
    where Î± represents the level of significance. In other words, it is the probability
    of rejecting the null hypothesis when it is true. The most frequently used confidence
    levels are 90%, 95%, and 99%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the standard error of the sample proportion, which is given by the
    following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SE = âˆšÂ _Â Â Â Ë†Â pÂ (1 âˆ’ Â Ë†Â pÂ )Â _Â n
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the standard error also corresponds to the standard deviation of the
    sample proportion, which is assumed to follow a Bernoulli distribution with a
    success probability of Â Ë†Â pÂ  (recall the introduction of Bernoulli distribution
    in the previous chapter). Such calculation relies on two assumptions: the observations
    in the samples are independent and there are sufficient observations in the sample.
    A common rule of thumb for checking the second assumption is to ensure both nÂ Ë†Â pÂ 
    > 10 and n(1 âˆ’ Â Ë†Â pÂ ) > 10.'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, instead of assuming a Bernoulli distribution, we can use the
    bootstrap procedure to estimate the standard error without any distributional
    assumption. Bootstrap is a non-parametric method that involves resampling the
    data with replacement to create new samples, calculating the statistic of interest
    (in this case, the proportion) for each resampled dataset, and estimating the
    standard error from the variability of the calculated statistics across the resampled
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 4. Find the critical value (z-score) corresponding to the preset confidence
    level. This can be done using the `qnorm()` function, which gives us the quantiles
    of the standard normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '5. Compute the **margin of error** (**ME**) as the product of the standard
    error and the critical value:'
  prefs: []
  type: TYPE_NORMAL
- en: ME = SE * z _ score
  prefs: []
  type: TYPE_NORMAL
- en: '6. Calculate the confidence interval by adding and subtracting the ME from
    the sample proportion, giving the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Lower limit = Â Ë†Â pÂ  âˆ’ ME
  prefs: []
  type: TYPE_NORMAL
- en: Upper limit = Â Ë†Â pÂ  + ME
  prefs: []
  type: TYPE_NORMAL
- en: The confidence interval provides a list of possible values for the population
    proportion according to the specific confidence level. See *Figure 11**.3* for
    a summary of the calculation process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 â€“ Summarizing the process of calculating the confidence interval
    based on sample proportion](img/B18680_11_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 â€“ Summarizing the process of calculating the confidence interval
    based on sample proportion
  prefs: []
  type: TYPE_NORMAL
- en: Let us stay with the bootstrap procedure a little longer. Without assuming any
    specific distribution, the bootstrap procedure is a flexible approach that can
    provide more accurate estimates of the standard error, especially for small sample
    sizes or when the data is not well behaved. However, It can be computationally
    intensive, especially for large datasets or when many bootstrap replications are
    generated.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11**.4* provides a schematic overview of the bootstrap procedure. Letâ€™s
    review:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we start with the whole dataset and specify the variable of interest,
    which is the `siblings` variable in this case. This is achieved via the `specify()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we draw samples from the variable with replacement, where the new sample
    will be the same size as the original dataset. Such resampling introduces randomness
    to the resulting dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We repeat the process many times, leading to a collection of bootstrapped artificial
    datasets using the `generate()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each replicated dataset, we will calculate the sample statistic of interest,
    which is the proportion of observations with two siblings in this case. This is
    done via the `calculate()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These sample statistics derived using repeated sampling of the original dataset
    will then form a distribution, called the bootstrapped distribution (plotted via
    `ggplot()`), whose standard deviation (extracted via the `summarize()` function)
    will be a good approximation of the standard error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.4 â€“ The schematic overview of obtaining the standard error using
    the bootstrap procedure](img/B18680_11_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 â€“ The schematic overview of obtaining the standard error using the
    bootstrap procedure
  prefs: []
  type: TYPE_NORMAL
- en: The bootstrapped samples convey different levels of uncertainty in the sample
    statistic and jointly form a density distribution of multiple artificial sample
    statistics. The standard deviation of the bootstrapped distribution then gives
    the standard error of the sample statistic. Note that functions such as `specify()`,
    `generate()`, and `calculate()` all come from the `infer` package in R. Remember
    to install this package before continuing with the following code.
  prefs: []
  type: TYPE_NORMAL
- en: Let us go through the following exercise to understand the bootstrap procedure
    for calculating the confidence interval.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.2 â€“ calculating the confidence interval via bootstrap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will explore calculating the confidence interval of the
    sample proportion. The confidence interval includes the list of estimates within
    which the true population proportion may assume, given the observed samples. It
    is a way to quantify the uncertainty in estimating the population proportion based
    on the actual observations. Besides a step-by-step walk-through of the calculation
    process using bootstrap, we will also compare the result with the alternative
    approach using the assumed Bernoulli distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a set of bootstrapped sample statistics using the specify-generate-calculate
    procedure from the `infer` package described earlier. Remember to build a binary
    variable to indicate the binary condition of having an observation with two siblings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we first create a binary indicator variable using the `if_else()` function
    to denote whether the family in the current survey has two siblings. We also remove
    rows with `NA` values in this column. Next, we use the `specify()` function to
    indicate the `siblings_two_ind` variable of interest and the level that corresponds
    to a success. We then use the `generate()` function to generate `500` bootstrapped
    samples, and use the `calculate()` function to obtain the corresponding sample
    statistic (proportion of success) in each bootstrapped sample by setting `stat
    = "``prop"`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let us observe the contents in the bootstrapped sample statistics:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that the `bs` object is a `tibble` DataFrame with 500 rows
    (corresponding to the total number of the bootstrapped sample) and 2 columns.
    The first column (`replicate`) denotes the number of bootstrapped samples, and
    the second column (`stat`) indicates the proportion of success (that is, the number
    of rows with `siblings_two_ind==2` divided by the total number of rows) in the
    bootstrapped sample.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the bootstrapped sample statistics in a density plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running the code generates the plot in *Figure 11**.5*. The spread of this distribution,
    which relates to the standard deviation, directly determines the magnitude of
    the standard error. Also, if we were to increase the number of bootstrapped samples,
    we would expect a smoother density curve.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 â€“ Visualizing the density plot of all bootstrapped sample proportions](img/B18680_11_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 â€“ Visualizing the density plot of all bootstrapped sample proportions
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the standard error as the standard deviation of the empirical distribution
    based on the bootstrapped sample proportions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `sd()` function to calculate the standard deviation of the
    `stat` column in `bs`, and then return the value via the `pull()` function. The
    standard error will then be scaled by the predetermined z-score and subtracted
    from and added to the original sample proportion to obtain the confidence interval.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the confidence interval of the original sample proportion with a
    95% confidence interval:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, since a 95% confidence level corresponds to a z-score of 2, we multiply
    it with the standard error before subtracting from and adding to the original
    sample proportion (`p_hat`) to obtain the confidence interval.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the confidence interval using the structure information by assuming
    a Bernoulli distribution for the probability of success:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the explicit form of the variance of the Bernoulli distribution
    to calculate the standard error. The result shows a fairly similar confidence
    interval compared with the one obtained using the bootstrap approach.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The confidence interval provides a measure of uncertainty for our estimate of
    the unknown population proportion using the observed sample proportion. Let us
    look at how to interpret the confidence interval in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the confidence interval of the sample proportion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Interpreting the confidence interval of the sample proportion involves understanding
    the meaning of the interval and the associated confidence level. In our previous
    example, the bootstrap approach reports a confidence interval of `[0.1938821,
    0.2226099]`. There are two levels of interpretation for this confidence interval.
  prefs: []
  type: TYPE_NORMAL
- en: First, the range of the confidence interval suggests that the true population
    proportion of families with two siblings is likely to fall between 19.39% and
    22.26%. This range is based on the sample data and estimates the uncertainty in
    the true proportion.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the 95% confidence interval means that if we were to conduct the survey
    many times (either in 2016 or other years), we would generate different random
    samples of the same size, based on which we can calculate the 95% confidence interval
    for each sample. Among these artificial samples, we will obtain a collection of
    intervals, and approximately 95% of them would include the true population proportion
    within the interval.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the confidence interval is still an estimate, and the true population
    proportion may fall outside the calculated interval. However, the confidence interval
    provides a useful way to quantify the uncertainty in the estimate and gives a
    list of plausible values for the true population proportion based on the observed
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: The next section introduces hypothesis testing for the sample proportion.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis testing for the sample proportion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hypothesis testing for the sample proportion is very much related to the confidence
    interval introduced in a previous section, which captures the level of uncertainty
    in the estimate for the unknown proportion based on the population data. Naturally,
    a sample with fewer observations leads to a wide confidence interval. Hypothesis
    testing for the sample proportion aims to determine whether there is enough evidence
    in a sample to support or reject a claim about the population proportion. The
    process starts with a null hypothesis (H0), which represents the baseline assumption
    about the population proportion. Correspondingly, there is an alternative hypothesis
    (H1) that represents the claim or statement we are testing against the null hypothesis.
    Hypothesis testing then compares the observed sample proportion to a specified
    null hypothesis in order to assess whether we have enough evidence to reject the
    null hypothesis in favor of the alternative hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us go through an overview of the procedure involved in carrying out hypothesis
    testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Formulate the hypothesis**. In this step, we set up the null hypothesis (H0)
    and alternative hypothesis (H1). The null hypothesis often says there is no effect,
    and the situation remains the status quo, as indicated by an equality sign in
    H0\. On the other hand, the alternative hypothesis states that there is an effect
    or difference, as indicated by an inequality sign in H1\. For example, we can
    set the following hypotheses for H0 and H1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H0: p = pÂ 0 (the population proportion is equal to a specified value, pÂ 0)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'H1: p â‰  pÂ 0 (the population proportion is not equal to pÂ 0)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choose a significance level (**ðœ¶**)**. The significance level is a probability
    threshold we use to reject the null hypothesis when it is true. Widely used significance
    levels include 0.05 (5%) and 0.01 (1%).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Calculate the test statistic**. Now that we observe a sample proportion based
    on the actual data, we can calculate the probability of observing such a sample
    proportion *if* the null hypothesis were true. This starts with calculating the
    test statistic (z-score) for the sample proportion using the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: z = Â Â Ë†Â pÂ  âˆ’ pÂ 0Â _Â âˆšÂ ___________Â pÂ 0(1 âˆ’ pÂ 0) / n
  prefs: []
  type: TYPE_NORMAL
- en: where Â Ë†Â pÂ  is the sample proportion, pÂ 0 is the population proportion assuming
    the null hypothesis, and n is the sample size. There are two things to note there.
    First, the denominator resembles the standard deviation based on the sample proportion
    covered earlier. Indeed, we are assuming a Bernoulli distribution with a success
    probability of pÂ 0\. With a total of n observations, the standard deviation for
    the sample proportion variable is âˆšÂ ___________Â pÂ 0(1 âˆ’ pÂ 0) / nÂ . Second, the
    whole term corresponds to the process of converting a number into a z-score of
    a specific distribution, a topic covered in the previous chapter. Here, we assume
    a normal distribution with mean pÂ 0 and standard deviation âˆšÂ ___________Â pÂ 0(1
    âˆ’ pÂ 0) / nÂ . We can then convert the observed sample proportion Â Ë†Â pÂ  to the corresponding
    z-score for ease of calculation later on.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we can also use the bootstrap approach to calculate the empirical
    p-value under the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 4. **Determine the p-value**. The z-score is a measure that falls on a standard
    Gaussian distribution. It is a test statistic, and we are often interested in
    the probability of observing the test statistic at this or an even more extreme
    value. This is called the p-value, denoted as Â Ë†Â pÂ , when we assume the null hypothesis
    is true. In other words, we try to assess how likely it is to observe some phenomenon,
    assuming the null hypothesis is true. If the probability of observing Â Ë†Â pÂ  or
    an even more extreme number is very small, we have confidence that the null hypothesis
    is false, and we can reject H0 in favor of H1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that for a two-tailed test, we can also calculate the p-value using the
    standard normal distribution and doubling the single-side probability:'
  prefs: []
  type: TYPE_NORMAL
- en: p-value = 2P(Z > |z|)
  prefs: []
  type: TYPE_NORMAL
- en: 5. **Make a decision**. Finally, we compare the p-value to the preset significance
    level (Î±) and use the following rule to make a decision.
  prefs: []
  type: TYPE_NORMAL
- en: If the p-value â‰¤ Î±, reject the null hypothesis in favor of the alternative hypothesis.
    Doing so suggests that there is enough evidence to suggest that the population
    proportion differs from the hypothesized proportion pÂ 0.
  prefs: []
  type: TYPE_NORMAL
- en: If the p-value > Î±, fail to reject the null hypothesis. This means that there
    is not enough evidence to suggest that the population proportion is different
    from pÂ 0.
  prefs: []
  type: TYPE_NORMAL
- en: Conducting the hypothesis testing follows a similar process. The only difference
    is the use of the `hypothesise()` function (placed after `specify()`), which serves
    as a null hypothesis. We then perform the same bootstrap procedure to obtain a
    density plot of the bootstrapped sample proportions, followed by calculating the
    total probability of obtaining a proportion at least as extreme as the one indicated
    in the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Let us go through an exercise to review the process of performing hypothesis
    testing for the sample proportion.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.3 â€“ performing hypothesis testing for the sample proportion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will set up a hypothetical population proportion in a
    null hypothesis and test the validity of this hypothesis based on the observed
    sample proportion:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the frequency count of families with and without two siblings in 2016
    in a bar plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running the code generates the plot in *Figure 11**.6*, which shows that families
    with two siblings account for around Â¼ of all families.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.6 â€“ Visualizing the frequency count of families with two siblings](img/B18680_11_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 â€“ Visualizing the frequency count of families with two siblings
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the sample proportion of families with two siblings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we first build a series of binary outcomes using `siblings_two_ind=="Y"`.
    Taking the average of this column gives the ratio of `TRUE` values, which gets
    executed in a `summarize()` context. We then extract the value of the sample proportion
    using `pull()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the specify-hypothesise-generate-calculate procedure to generate a collection
    of bootstrapped sample proportions under the null hypothesis, which specifies
    a population proportion of `0.19`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the density plot of the bootstrapped sample proportions along with
    the proportion suggested by the null hypothesis via a vertical line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running the code generates the plot in *Figure 11**.7*. The probability of observing
    a value at least as extreme as the one indicated by the red line (according to
    the null hypothesis) is thus the total area under the density curve toward the
    right of the red line. We then double the result to account for the opposite direction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.7 â€“ Visualizing the density plot of the bootstrapped sample proportions
    for hypothesis testing](img/B18680_11_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 â€“ Visualizing the density plot of the bootstrapped sample proportions
    for hypothesis testing
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the p-value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since this result is less than the preset significance level of 5%, we have
    sufficient evidence to favor the alternative hypothesis and reject the null hypothesis.
    In other words, the assumed 19% is statistically different from the true population
    proportion with a confidence level of up to 95%. We can therefore draw the conclusion
    that the true population proportion is not 19%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next section looks at the inference for the difference in sample proportions
    between two categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: Inference for the difference in sample proportions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The setting now is that we have two categorical variables. Take gender and
    degree, for example. The data will report a proportion of degree holders for both
    females and males. A natural question to ask is whether males are more likely
    to get a degree than females. A particular dataset will report a snapshot of these
    proportions, which may or may not suggest a higher percentage of degree holders
    are males. The tools from hypothesis testing could then come in to answer the
    following question: if males are a higher proportion of degree holders in the
    dataset, is such difference statistically significant? In other words, are males
    more likely to get a degree than females, or vice versa? This section attempts
    to answer this type of question.'
  prefs: []
  type: TYPE_NORMAL
- en: Inference for the difference in sample proportions between two categorical variables
    (for example, gender and degree) involves comparing the proportions of samples
    for each level in two different populations. This type of analysis is commonly
    used in experiments or observational studies to determine the existence of a significant
    difference in proportions between two groups. The main goal is to estimate the
    difference between the population proportions and determine whether this difference
    is statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: The procedure for hypothesis testing is similar to before. We first formulate
    the null hypothesis, which assumes no difference between the proportion of the
    two populations, that is, pÂ 1 = pÂ 2, or pÂ 1 âˆ’ pÂ 2 = 0\. The alternative hypothesis
    then states that their difference is not zero; that is, pÂ 1 âˆ’ pÂ 2 â‰  0\. Next,
    we choose a specific significance level and calculate the sample statistic (difference
    in sample proportion, including the pooled proportion between the two categorical
    variables) and the test statistic (via either a closed-form expression based on
    the assumed distribution or using the bootstrap method). Finally, we obtain the
    p-value and decide whether the observed result under the null hypothesis possesses
    statistical significance or not.
  prefs: []
  type: TYPE_NORMAL
- en: Let us go through a concrete exercise following our previous example.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.4 â€“ performing hypothesis testing for the difference in sample proportions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we focus on how to conduct hypothesis testing for the difference
    in the sample proportion between gender and status of higher degree. Here, we
    define a higher degree as a bachelorâ€™s and above. The proportion of higher-degree
    holders will likely differ between the male and female groups, and we will test
    whether such a difference is significant given the observed data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a binary column called `higher_degree` to the previous DataFrame, `gss2016`,
    to indicate the status of higher degree, including bachelorâ€™s and above:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the ratio between the two levels for `gender` and `higher_degree`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot these counts in a bar chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running the code generates the chart in *Figure 11**.8*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.8 â€“ Visualizing the frequency count of gender and higher-degree
    status](img/B18680_11_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 â€“ Visualizing the frequency count of gender and higher-degree status
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also plot them in percentages by specifying `position = "fill"` in the
    `geom_bar()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Running the code generates the chart in *Figure 11**.9*, which suggests no obvious
    difference in the proportion of higher-degree holders between the male and female
    groups.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 â€“ Visualizing the frequency count of gender and higher-degree
    status](img/B18680_11_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 â€“ Visualizing the frequency count of gender and higher-degree status
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the difference in sample proportions of higher-degree holders between
    males and females:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result also shows that the difference is quite small, with the female group
    being 0.7% higher than the male group (refer to the slightly higher blue bar of
    the female group in the previous figure). Let us see whether such a difference
    is statistically significant.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate one bootstrap sample set under the null hypothesis, which states that
    there is no difference in the ratio of higher-degree holders between the male
    and female groups:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use `higher_degree` as the response variable and `sex` as the explanatory
    variable in a logistic regression setting (to be introduced in [*Chapter 13*](B18680_13.xhtml#_idTextAnchor279)).
    Under the null hypothesis, we randomly sample from the original dataset and create
    a new artificial dataset of the same shape.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Repeat the same bootstrap sampling procedures 500 times and calculate the difference
    in sample proportions of higher-degree holders between female and male groups
    (note the sequence here) for each set of bootstrapped samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot these bootstrapped sample statistics in a density curve and plot the observed
    difference as a vertical red line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running the code generates the plot in *Figure 11**.10*, which shows that the
    red line is not located toward the extreme side of the empirical distribution.
    This suggests that the p-value, which will be calculated next, may be high.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.10 â€“ Showing the density plot for the bootstrapped sample statistics
    and observed differences](img/B18680_11_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 â€“ Showing the density plot for the bootstrapped sample statistics
    and observed differences
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the two-tailed p-value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows a pretty high p-value, which suggests that we lack sufficient
    evidence to reject the null hypothesis. In other words, there is not enough information
    to suggest that the proportion of higher-degree holders between males and females
    is different.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The hypothesis testing relies on a predefined significance level. That significance
    level, denoted as Î±, has something to do with the statistical error of the procedure.
    The next section introduces two common types of statistical error when performing
    hypothesis testing.
  prefs: []
  type: TYPE_NORMAL
- en: Type I and Type II errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two types of errors when conducting hypothesis testing and making
    a decision about the null hypothesis (H0) and the alternative hypothesis (H1).
    They are called Type I and Type II errors.
  prefs: []
  type: TYPE_NORMAL
- en: The Type I error refers to false positives. It happens when the null hypothesis
    is true but mistakenly rejected. In other words, we find evidence in our sample
    data that suggests a significant effect or difference exists and we favor the
    alternative hypothesis, even though it does not actually exist in the population.
    We denote the probability of experiencing a Type I error as Î±. It is also called
    the significance level, which was set to `0.05` in the previous example. A 5%
    significance level means that there is a 5% chance of rejecting the null hypothesis
    when it is true. The significance level thus represents the probability of committing
    a false positive error.
  prefs: []
  type: TYPE_NORMAL
- en: The Type II error focuses on the false negative case. It occurs when we fail
    to reject a false null hypothesis. In other words, we do not find evidence in
    our sample data to reject the null hypothesis, even though it does exist in the
    population. The probability of making a Type II error is denoted by Î², which is
    also referred to as the power of the test. The complement of the power, denoted
    as 1 âˆ’ Î², represents the probability of rejecting the null hypothesis when it
    is false.
  prefs: []
  type: TYPE_NORMAL
- en: Type I errors involve falsely rejecting the null hypothesis, while Type II errors
    involve failing to reject the null hypothesis when false. Both types of errors
    are important considerations in hypothesis testing because they can lead to incorrect
    conclusions. To minimize the risk of these errors, we can make a careful choice
    regarding the significance level (Î±) and also ensure that their study has sufficient
    power (1 âˆ’ Î²). The power of a test depends on the sample size, the effect size
    (which is a quantitative measure of the magnitude of an empirical relationship
    between variables), and the chosen significance level. Larger sample sizes and
    larger effect sizes both increase the power of a test, reducing the likelihood
    of Type II errors.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11**.11* provides an overview of the different types of outcomes in
    a hypothesis test. Note that the false positive and false negative are related
    to the quality of the decision. Depending on the type of a false decision, we
    would classify the errors as either Type I or Type II errors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11 â€“ Overview of different types of outcomes in a hypothesis test](img/B18680_11_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 â€“ Overview of different types of outcomes in a hypothesis test
  prefs: []
  type: TYPE_NORMAL
- en: The next section introduces the chi-square test, which tests the independence
    of two categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the independence of two categorical variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To check the independence of two categorical variables, the process involves
    checking the existence of a statistically significant relationship between them.
    One common procedure is the chi-square test for independence. It works by comparing
    the observed frequencies in a contingency table with the expected frequencies
    under the assumption of independence.
  prefs: []
  type: TYPE_NORMAL
- en: Let us first review the contingency table for two categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the contingency table
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A contingency table, also known as a cross-tabulation or crosstab, is a table
    used to display the frequency distribution of two or more categorical variables.
    It summarizes the relationships between the variables by showing how their categories
    intersect or co-occur in the data. It provides a good summary of the relationships
    between categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us stick with the example of the relationship between gender and degree.
    This time, we will look at all types of degrees, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To indicate its relationship with gender, we can plot the degree together with
    gender in a stacked bar plot as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Running the code generates the plot in *Figure 11**.12*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 â€“ Visualizing the relationship between gender and degree in
    a bar plot](img/B18680_11_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 â€“ Visualizing the relationship between gender and degree in a bar
    plot
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the figure provides no information on the exact count for each category.
    To obtain the exact frequency for each category of the two variables, we can use
    the contingency table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used the `table()` function to generate the contingency table after
    selecting both `sex` and `degree`.
  prefs: []
  type: TYPE_NORMAL
- en: The next section introduces the chi-square test to test for the independence
    between these two categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the chi-square test for independence between two categorical variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The chi-square test is a statistical test used to decide a possibly significant
    relationship (dependence) between two categorical variables in a collection of
    observed samples. It can be used to test for independence or goodness of fit.
    In this chapter, we focus mainly on the test for independence between two categorical
    variables. The test compares the observed frequencies with the expected ones in
    a contingency table, assuming that the variables are independent. If the observed
    and expected frequencies are significantly different, the test suggests that the
    variables are not independent; in other words, they are dependent on each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the same approach as before, we can generate an artificial bootstrapped
    dataset to obtain a sample statistic, called the chi-square statistic. This dataset
    is generated by permuting the original dataset under the assumption of independence
    in the null hypothesis. In the following code, we generate one permutated dataset
    of the same shape as the original dataset, assuming independence under the null
    hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create 500 permutated datasets and extract the corresponding chi-square
    statistic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the test, we obtain the expected frequency for each cell in the contingency
    table under the assumption of independence between the categorical variables.
    The expected frequency for a cell is computed as (*row sum * column sum*) */*
    *overall sum*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here, we first obtain the row-wise and column-wise sum, as well as the total
    sum. We then use the `outer()` function to obtain the outer product between these
    two vectors, which is then scaled by the total sum to obtain the expected frequency
    count in each cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we compute the observed chi-square statistic based on the available samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then plot the observed chi-square statistic within the density curve
    of previous bootstrapped sample statistics to get a sense of where the observed
    statistic is located, based on which we will be able to calculate the corresponding
    p-value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Running the code generates the plot in *Figure 11**.13*, which shows a high
    p-value.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13 â€“ Visualizing the density curve of bootstrapped chi-square statistics
    and the observed statistic](img/B18680_11_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 â€“ Visualizing the density curve of bootstrapped chi-square statistics
    and the observed statistic
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can calculate the p-value. As shown in the following code, the p-value
    of `0.72` is indeed quite high, and thus there is no sufficient evidence to reject
    the null hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will shift to look at statistical inference for numerical
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical inference for numerical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will switch to look at statistical inference using numerical
    data. We will cover two approaches. The first approach relies on the bootstrapping
    procedure and permutes the original dataset to create additional artificial datasets,
    which can then be used to derive the confidence intervals. The second approach
    uses a theoretical assumption on the distribution of the bootstrapped samples
    and relies on the t-distribution to achieve the same result. We will learn how
    to perform a t-test, derive a confidence interval, and conduct an **analysis of**
    **variance** (**ANOVA**).
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, bootstrapping is a non-parametric resampling method that
    allows us to estimate the sampling distribution of a particular statistic, such
    as the mean, median, or proportion, as in the previous section. This is achieved
    by repeatedly drawing random samples with replacement from the original data.
    By doing so, we can calculate confidence intervals and perform hypothesis tests
    without relying on specific distributional assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the t-distribution is a probability distribution used for hypothesis
    testing if the sample size is small and the standard deviation of the population
    data remains unknown. It is a more general approach that assumes the bootstrapped
    samples follow a specific distribution. We will then use this distribution to
    estimate confidence intervals and perform the hypothesis test.
  prefs: []
  type: TYPE_NORMAL
- en: The t-test is a widely used statistical test that allows us to compare the mean
    values of two groups or test whether the mean of a single group is equal to a
    specific value. This time, our interest is the mean of a group since the variable
    is numeric. The test relies on the t-distribution and takes into account the sample
    sizes, sample means, and sample variances.
  prefs: []
  type: TYPE_NORMAL
- en: Confidence intervals offer a list of possible values, where the true population
    statistic, such as the mean or proportion, is likely to lie, with a specified
    level of confidence (specified by the significance level Î±).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, ANOVA extends the t-test used when there are more than two groups
    to compare. ANOVA helps us determine possible significant differences among the
    group means by dividing the total variability of the observed data into two parts:
    between-group variability and within-group variability. It tests the null hypothesis
    that the mean values of all groups are equal. If the null hypothesis is rejected,
    we can continue to identify which specific group means differ from each other.'
  prefs: []
  type: TYPE_NORMAL
- en: Let us start with generating a bootstrap distribution for the median.
  prefs: []
  type: TYPE_NORMAL
- en: Generating a bootstrap distribution for the median
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed earlier, when building a bootstrap distribution for a single statistic,
    we first generate a collection of bootstrap samples via sampling with replacement,
    and then record the relevant statistic (in this case, the median) of each distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Let us go through an exercise to build the collection of bootstrap samples.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.5 â€“ generating a bootstrap distribution for the sample median
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will apply the same specify-generate-calculate workflow
    using the `infer` package to generate a bootstrap distribution for the sample
    median using the `mtcars` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `mtcars` dataset and view its structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The result shows that we have a dataset with 32 rows and 11 columns. In the
    following steps, we will use the `mpg` variable and generate a bootstrap distribution
    of its median:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate 10,000 bootstrap samples according to the `mpg` variable and obtain
    the median of all samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we specify `stat = "median"` in the `calculate()` function to extract
    the median in each bootstrap sample.
  prefs: []
  type: TYPE_NORMAL
- en: '2. Plot the bootstrap distribution as a density curve of the bootstrapped sample
    statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Running the code generates the plot in *Figure 11**.14*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.14 â€“ Visualizing the density curve of the bootstrapped sample median](img/B18680_11_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 â€“ Visualizing the density curve of the bootstrapped sample median
  prefs: []
  type: TYPE_NORMAL
- en: The next section looks at constructing the bootstrapped confidence interval.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the bootstrapped confidence interval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have looked at how to construct the bootstrapped confidence interval using
    the standard error method. This involves adding and subtracting the scaled standard
    error from the observed sample statistic. It turns out that there is another,
    simpler method, which just uses the percentile of the bootstrap distribution to
    obtain the confidence interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us continue with the previous example. Say we would like to calculate the
    95% confidence interval of the previous bootstrap distribution. We can achieve
    this by calculating the upper and lower quantiles (97.5% and 2.5%, respectively)
    of the bootstrap distribution. The following code achieves this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us also calculate the bootstrap confidence interval using the standard
    error method, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the result is close to the one obtained using the percentile method.
    However, the standard error method is a more accurate method than the percentile
    method.
  prefs: []
  type: TYPE_NORMAL
- en: The next section covers re-centering a bootstrap distribution upon testing a
    null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Re-centering a bootstrap distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The bootstrap distribution from the previous section is generated by randomly
    sampling the original dataset with replacement. Each set of bootstrap samples
    maintains the same size as the original sample sets. However, we cannot directly
    use this bootstrap distribution for hypothesis testing.
  prefs: []
  type: TYPE_NORMAL
- en: Upon introducing a null hypothesis, what we did in the previous hypothesis test
    section for two categorical variables is re-generated a new bootstrap distribution
    under the null hypothesis. We then place the observed sample statistic as a vertical
    red line along the bootstrap distribution to calculate the p-value, representing
    the probability of experiencing a phenomenon at least as extreme as the observed
    sample statistic. The only additional step is to generate the bootstrap distribution
    under the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: When generating bootstrap samples under the null hypothesis, the main idea is
    to remove the effect we are testing for and create samples, assuming the null
    hypothesis is true. In other words, we create samples that would be expected if
    there were no difference between the groups. For example, when comparing means
    between two groups, we would subtract the overall mean from each observation to
    center the data around 0 before performing the random sampling with replacement.
  prefs: []
  type: TYPE_NORMAL
- en: There is another way to achieve this. Recall that the original bootstrap distribution,
    by design, is centered around the observed sample statistic. Upon introducing
    the null hypothesis, we could simply move the original bootstrap distribution
    to be centered around the statistic in the null hypothesis, which is the null
    value. This shifted bootstrap distribution represents the same distribution if
    we were to remove the effect in the original dataset and then perform bootstrap
    sampling again. We can then place the observed sample statistic along the shifted
    bootstrap distribution to calculate the corresponding p-value, which represents
    the ratio of simulations that generate a sample statistic at least as favorable
    to the alternative hypothesis as the actual sample statistic. *Figure 11**.15*
    demonstrates this process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.15 â€“ Shifting the bootstrap distribution to be centered around
    the null value](img/B18680_11_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 â€“ Shifting the bootstrap distribution to be centered around the
    null value
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us generate the bootstrap distribution for hypothesis testing for the previous
    example. We want to test the null hypothesis with a population median of 16 for
    the `mpg` variable. The following code generates the bootstrapped sample statistics,
    where we specify the null value via `med = 16` and the point estimate with `null
    = "point"` in the `hypothesize()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we plot these bootstrapped sample statistics in a density plot, along
    with the observed sample statistic as a vertical red line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Running the code generates the plot in *Figure 11**.16*, which shows a small
    p-value.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.16 â€“ Density plot of bootstrapped sample medians and observed sample
    median (vertical red line)](img/B18680_11_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 â€“ Density plot of bootstrapped sample medians and observed sample
    median (vertical red line)
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover another distribution-based inference approach
    based on the **central limit** **theorem** (**CLT**).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the central limit theorem used in t-distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CLT says that the distribution from the sum (or average) of many independent
    and identically distributed random variables would jointly form a normal distribution,
    regardless of the underlying distribution of these individual variables. Due to
    the CLT, normal distribution is often used to approximate the sampling distribution
    of various statistics, such as the sample mean and the sample proportion.
  prefs: []
  type: TYPE_NORMAL
- en: The t-distribution is related to the CLT in the context of statistical inference.
    When weâ€™re estimating a population mean from a sample, we often have no access
    to the true standard deviation of the population. Instead, we resort to the sample
    standard deviation as an estimate. In this case, the sampling distribution of
    the sample mean doesnâ€™t follow a normal distribution, but rather a t-distribution.
    In other words, when we extract the sample mean from a set of observed samples,
    and we are unsure of the population standard deviation (as is often the case when
    working with actual data), the sample mean can be modeled as a realization from
    the t-distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The t-distribution is a family of continuous probability distributions that
    are symmetric and bell-shaped, which shows similarity to the normal distribution.
    However, the t-distribution shows heavier tails, which accounts for the greater
    uncertainty due to estimating the population standard deviation from the observed
    data. That is, observations of a t-distribution are more likely to fall into distant
    tails (such as beyond two standard deviations away from the mean) than the normal
    distribution. The shape of the t-distribution relies on the **degrees of freedom**
    (**df**), which depends on the sample size and determines the thickness of the
    tails. As more samples are collected, the df moves up, and the t-distribution
    gradually approximates the normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: We briefly covered the `qt()` function used to find the cutoffs under the t-distribution
    in the previous chapter. Now let us go through an exercise to get more familiar
    with calculations related to the t-distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.6 â€“ understanding the t-distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will use the `pt()` function to find probabilities under
    the t-distribution. For a given cutoff quantile value, `q`, and a given `df`,
    the `pt(q, df)` function gives us the probability under the t-distribution with
    `df` for values of `t` less than `q`. In other words, we have P(tÂ df < T) = `pt(q
    = T, df)`. We can also use the `qt()` function to find the quantiles for a specific
    probability under the t-distribution. That is, if P(tÂ df < T) = p, then T = `qt(p,
    df)`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the probability under the t-distribution with 10 df below `T=3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the probability under the t-distribution with 10 df above `T=3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we first calculate the probability of being below a specific cutoff
    value under the t-distribution, and then take the complement to find the probability
    above the threshold.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Find the probability under the t-distribution with `100` df above `T=3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since `df=100` has a better approximation to the normal distribution than `df=10`,
    the resulting probability, `z`, is thus smaller than `y`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Find the 95th percentile of the t-distribution with 10 df:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the cutoff value that bounds the upper end of the middle 95th percentile
    of the t-distribution with `10` df:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the upper end of the middle 95th percentile refers to the 97.5th percentile.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Find the cutoff value that bounds the upper end of the middle 95th percentile
    of the t-distribution with `100` df:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The next section discusses how to construct the confidence interval for the
    population mean using the t-distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the confidence interval for the population mean using the t-distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us review the process of statistical inference for the population mean.
    We start with a limited sample, from which we can derive the sample mean. Since
    we want to estimate the population mean, we would like to perform statistical
    inference based on the observed sample mean and quantify the range where the population
    statistic may exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the average miles per gallon, shown in the following code, is
    around 20 in the `mtcars` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Given this result, we wonâ€™t be surprised to encounter another similar dataset
    with an average `mpg` of 19 or 21\. However, we would be surprised if the value
    is 5, 50, or even 100\. When assessing a new collection of samples, we need a
    way to quantify the variability of the sample mean across multiple samples. We
    have learned two ways to do this: use the bootstrap approach to simulate artificial
    samples or use the CLT to approximate such variability. We will focus on the CLT
    approach in this section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the CLT, the sample mean of any sampling distribution would be
    approximately normally distributed, regardless of the original distribution. In
    other words, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: _Â xÂ  âˆ¼ N(mean = Î¼, SE = Â ÏƒÂ _Â âˆšÂ _Â nÂ Â )
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this is a theoretical distribution we are unable to obtain. For example,
    the population standard deviation, Ïƒ, stays unknown, and we only have access to
    the observed samples. Instead, we would estimate the standard error using the
    sample standard deviation, s, giving the following:'
  prefs: []
  type: TYPE_NORMAL
- en: _Â xÂ  âˆ¼ N(mean = Î¼, SE = Â sÂ _Â âˆšÂ _Â nÂ Â )
  prefs: []
  type: TYPE_NORMAL
- en: We would then employ the t-distribution of n âˆ’ 1 degree of freedom to make an
    inference for the population mean as it gives thicker tails due to the additional
    uncertainty introduced by s.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, note that the approximation using the CLT relies on a few assumptions.
    For example, the samples need to be independent of each other. This is often satisfied
    when the samples are randomly selected, or if the samples account for less than
    10% of the total population if they are selected without replacement. The sample
    size also needs to be larger to account for potential skewness in the samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can construct the 95% confidence interval using the `t.test()` function,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are performing a one-sample t-test, where the default null hypothesis
    states that the population mean is 0\. The result shows a very small p-value,
    suggesting that we could reject the null hypothesis in favor of the alternative
    hypothesis; that is, the population mean is not 0\. The 95% confidence interval
    (between `17.91768` and `22.26357`) is also constructed based on the t-distribution
    with a `df` of `31` and a t-statistic of `18.857`.
  prefs: []
  type: TYPE_NORMAL
- en: The next section reviews the hypothesis testing for two means using both bootstrap
    simulation and t-test approximation.
  prefs: []
  type: TYPE_NORMAL
- en: Performing hypothesis testing for two means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore the process of comparing two sample means using
    hypothesis testing. When comparing two sample means, we want to determine whether
    a significant difference exists between the means of two distinct populations
    or groups.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose now we have two groups of samples. These two groups could represent
    a specific value before and after treatment for each sample. Our objective is
    thus to compare the sample statistics of these two groups, such as the sample
    mean, and determine whether the treatment has an effect. To do this, we can perform
    a hypothesis test to compare mean values from the two independent distributions
    using either bootstrap simulation or t-test approximation.
  prefs: []
  type: TYPE_NORMAL
- en: When using the t-test in the hypothesis test to compare the mean values of two
    independent samples, the two-sample t-test assumes normal distribution for the
    data, and that the variances of the two populations are equal. However, in cases
    where these assumptions may not hold, alternative non-parametric tests or resampling
    methods, such as bootstrap, can be employed to make inferences about the population
    means.
  prefs: []
  type: TYPE_NORMAL
- en: Let us go through an exercise to see these two methods of hypothesis testing
    in play.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.7 â€“ comparing two means
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will explore two approaches (t-test and bootstrap) to
    compare two sample means and calculate the confidence interval of the difference
    in sample means:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a dummy dataset that consists of two groups of samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we created a `tibble` DataFrame with the `value` column indicating the
    sample observation and the `group` column indicating the group number. We would
    like to assess the difference in the sample mean between these two groups.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform bootstrap sampling `1000` times and calculate the bootstrap statistics
    under the null hypothesis that these two groups are independent of each other,
    and there is no difference in their means:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the pipeline for the hypothesis test starts by specifying the response
    (`value`) and explanatory (`group`) variables, setting up the null hypothesis,
    generating bootstrap samples under the null hypothesis, and then calculating the
    test statistic (in this case, the difference in means) for each bootstrap sample.
    The null hypothesis states that we assume the sample mean values for both groups
    come from the same population, and that any observed difference is merely due
    to chance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the confidence interval based on the bootstrap statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform a two-sample t-test using the `t.test()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result shows that the 95% confidence interval based on the t-distribution
    is close but still different from the one obtained via bootstrap sampling. We
    can also perform the t-test by passing in the model form:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The next section introduces ANOVA, or the analysis of variance.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing ANOVA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**ANOVA** is a statistical hypothesis testing method used to compare the means
    of more than two groups, which extends the two-sample t-test discussed in the
    previous section. The goal of ANOVA is to test potential significant differences
    among the group means (the between-group variability) while accounting for the
    variability within each group (the within-group variability).'
  prefs: []
  type: TYPE_NORMAL
- en: 'ANOVA relies on the F-statistic in hypothesis testing. The F-statistic is a
    ratio of two estimates of variance: the between-group variance and the within-group
    variance. The between-group variance measures the differences among the group
    means, while the within-group variance represents the variability within each
    group. The F-statistic can be calculated based on these two group variances.'
  prefs: []
  type: TYPE_NORMAL
- en: In hypothesis testing, the null hypothesis for ANOVA states that all group means
    are equal, and any observed differences are due to chance. The alternative hypothesis,
    on the other hand, suggests that at least one groupâ€™s mean differs from the others.
    If the F-statistic is sufficiently large, the between-group variance is significantly
    greater than the within-group variance, which provides evidence against the null
    hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at a concrete example. We first load the `PlantGrowth` dataset,
    which contains the weights of plants after they have been subjected to three different
    treatments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we perform the one-way ANOVA test using the same specify-hypothesize-generate-calculate
    procedure. Specifically, we first specify the response variable (`weight`) and
    the explanatory variable (`group`). We then set up the null hypothesis, stating
    no difference in the means of the groups, using `hypothesize(null = "independence")`.
    Next, we generate 1,000 permuted datasets using `generate(reps = 1000, type =
    "permute")`. Finally, we calculate the F-statistic for each permuted dataset using
    `calculate(stat = "``F")`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Last, we can calculate the p-value using the observed F-statistic and the distribution
    of the F-statistics obtained from the permuted datasets. When the p-value is smaller
    than the preset significance level (for example, `0.05`), we could reject the
    null hypothesis and say that there is a significant difference among the means
    of the groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The result suggests that we do not have enough confidence to reject the null
    hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered different types of statistical inferences for hypothesis
    testing, targeting both numerical and categorical data. We introduced inference
    methods for a single variable, two variables, and multiple variables, using either
    proportion (for categorical variable) or mean (for numerical variable) as the
    sample statistic. The hypothesis testing procedure, including both the parametric
    approach using model-based approximation and the non-parametric approach using
    bootstrap-based simulations, offers valuable tools such as the confidence interval
    and p-value. These tools allow us to make a decision about whether we can reject
    the null hypothesis in favor of the alternative hypothesis. Such a decision also
    relates to the Type I and Type II errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will cover one of the most widely used statistical
    and ML models: linear regression.'
  prefs: []
  type: TYPE_NORMAL
