<html><head></head><body>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch09" class="calibre1"/>Chapter 9. Real-world Applications for Regression Models</h1></div></div></div><p class="calibre8">We have arrived at the concluding chapter of the book. In respect of the previous chapters, the present one is very practical in its essence, since it mostly contains lots of code and no math or other theoretical explanation. It comprises four practical examples of real-world data science problems solved using linear models. The ultimate goal is to demonstrate how to approach such problems and how to develop the reasoning behind their resolution, so that they can be used as blueprints for similar challenges you'll encounter.</p><p class="calibre8">For each problem, we will describe the question to be answered, provide a short description of the dataset, and decide the metric we strive to maximize (or the error we want to minimize). Then, throughout the code, we will provide ideas and intuitions that are key to successfully completing each one. In addition, when run, the code will produce verbose output from the modeling, in order to provide the reader with all the information needed to decide the next step. Due to space restrictions, output will be truncated so it just contains the key lines (the truncated lines are represented by <code class="email">[…]</code> in the output) but, on your screen, you'll get the complete picture.</p><div><h3 class="title2"><a id="tip41" class="calibre1"/>Tip</h3><p class="calibre8">In this chapter, each section was provided with a separate IPython Notebook. They are different problems, and each of them is developed and presented independently.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch09lvl1sec50" class="calibre1"/>Downloading the datasets</h1></div></div></div><p class="calibre8">In this section <a id="id617" class="calibre1"/>of the book, we will download all the datasets that are going to be used in the examples in this chapter. We chose to store them in separate subdirectories of the same folder where the IPython Notebook is contained. Note that some of them are quite big (100+ MB).</p><div><h3 class="title2"><a id="tip42" class="calibre1"/>Tip</h3><p class="calibre8">We would like to thank the maintainers and the creators of the UCI dataset archive. Thanks to such repositories, modeling and achieving experiment repeatability are much easier than before. The UCI archive is from Lichman, M. (2013). UCI Machine Learning Repository [<a class="calibre1" href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p></div><p class="calibre8">For each dataset, we <a id="id618" class="calibre1"/>first download it, and then we present the first couple of lines. First, this will help demonstrate whether the file has been correctly downloaded, unpacked, and placed into the right location; second, it will show the structure of the file itself (header, fields, and so on):</p><div><pre class="programlisting">In:
try:
    import urllib.request as urllib2
except:
    import urllib2
import requests, io, os
import zipfile, gzip

def download_from_UCI(UCI_url, dest):
    r = requests.get(UCI_url)
    filename = UCI_url.split('/')[-1]
    print ('Extracting in %s' %  dest)
    try:
        os.mkdir(dest)
    except:
        pass
    with open (os.path.join(dest, filename), 'wb') as fh:
        print ('\tdecompression %s' % filename)
        fh.write(r.content)


def unzip_from_UCI(UCI_url, dest):
    r = requests.get(UCI_url)
    z = zipfile.ZipFile(io.BytesIO(r.content))
    print ('Extracting in %s' %  dest)
    for name in z.namelist():
        print ('\tunzipping %s' % name)
        z.extract(name, path=dest)

def gzip_from_UCI(UCI_url, dest):
    response = urllib2.urlopen(UCI_url)
    compressed_file = io.BytesIO(response.read())
    decompressed_file = gzip.GzipFile(fileobj=compressed_file)
    filename = UCI_url.split('/')[-1][:-4]
    print ('Extracting in %s' %  dest)
    try:
        os.mkdir(dest)
    except:
        pass
    with open( os.path.join(dest, filename), 'wb') as outfile:
        print ('\tgunzipping %s' % filename)
        cnt = decompressed_file.read()
        outfile.write(cnt)</pre></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec92" class="calibre1"/>Time series problem dataset</h2></div></div></div><p class="calibre8">Dataset from: Brown, M. S., Pelosi, M. &amp; Dirska, H. (2013). Dynamic-radius Species-conserving Genetic <a id="id619" class="calibre1"/>Algorithm for the Financial Forecasting of Dow Jones Index Stocks. Machine <a id="id620" class="calibre1"/>Learning and Data Mining in Pattern Recognition, 7988, 27-41.</p><div><pre class="programlisting">In:
UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00312/dow_jones_index.zip'
unzip_from_UCI(UCI_url, dest='./dji')
Out:
Extracting in ./dji
  unzipping dow_jones_index.data
  unzipping dow_jones_index.names
In:
! head -2 ./dji/dow_jones_index.data
Out:</pre></div><div><img src="img/00131.jpeg" alt="Time series problem dataset" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec93" class="calibre1"/>Regression problem dataset</h2></div></div></div><p class="calibre8">Dataset <a id="id621" class="calibre1"/>from: Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. The Million <a id="id622" class="calibre1"/>Song Dataset. In Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR), 2011.</p><div><pre class="programlisting">In:
UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip'
unzip_from_UCI(UCI_url, dest='./msd')
Out:
Extracting in ./msd
  unzipping YearPredictionMSD.txt
In:
! head -n 2 ./msd/YearPredictionMSD.txt
Out:</pre></div><div><img src="img/00132.jpeg" alt="Regression problem dataset" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec94" class="calibre1"/>Multiclass classification problem dataset</h2></div></div></div><p class="calibre8">Dataset <a id="id623" class="calibre1"/>from: Salvatore J. Stolfo, Wei Fan, Wenke Lee, Andreas Prodromidis, and Philip K. Chan. Cost-based Modeling and Evaluation for Data Mining With Application to Fraud and <a id="id624" class="calibre1"/>Intrusion Detection: Results from the JAM Project.</p><div><pre class="programlisting">In:
UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data.gz'
gzip_from_UCI(UCI_url, dest='./kdd')
Out:
Extracting in ./kdd
  gunzipping kddcup.dat
In:
!head -2 ./kdd/kddcup.dat
Out:</pre></div><div><img src="img/00133.jpeg" alt="Multiclass classification problem dataset" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_5"><a id="ch09lvl2sec95" class="calibre1"/>Ranking problem dataset</h2></div></div></div><p class="calibre8">Creator/Donor: Jeffrey<a id="id625" class="calibre1"/> C. Schlimmer</p><div><pre class="programlisting">In:
UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data'
download_from_UCI(UCI_url, dest='./autos')
Out:
Extracting in ./autos
  decompression imports-85.data
In:
!head -2 ./autos/imports-85.data
Out:</pre></div><div><img src="img/00134.jpeg" alt="Ranking problem dataset" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec51" class="calibre1"/>A regression problem</h1></div></div></div><p class="calibre8">Given some <a id="id626" class="calibre1"/>descriptors of a song, the goal of this problem is to predict the year when the song was produced. That's basically a regression problem, since the target variable to predict is a number in the range between 1922 and 2011.</p><p class="calibre8">For each song, in addition to the year of production, 90 attributes are provided. All of them are related to the timbre: 12 of them relate to the timbre average and 78 attributes describe the timbre's covariance; all the features are numerical (integer or floating point numbers).</p><p class="calibre8">The dataset is composed of more than half a million observations. As for the competition behind the dataset, the authors tried to achieve the best results using the first 463,715 observations as a training set and the remaining 51,630 for testing.</p><p class="calibre8">The metric used to evaluate the results is the <a id="id627" class="calibre1"/>
<strong class="calibre2">Mean Absolute Error</strong> (<strong class="calibre2">MAE</strong>) between the predicted year and the real year of production for the songs composing the testing set. The goal is to minimize the error measure.</p><div><h3 class="title2"><a id="note05" class="calibre1"/>Note</h3><p class="calibre8">The complete description of this problem and additional information (about the feature extraction phase) can be found at the website: <a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD">https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD</a></p></div><p class="calibre8">Now, let's start with some Python code. First of all, we load the dataset (remember, if that operation fails it means you should download the dataset by yourself before running the program in the previous section). Then, we split the training and testing parts according to the guidelines provided with the dataset. Finally, we print the size (in Megabytes) of the resulting DataFrame, in order to have an indication of the memory footprint of the dataset:</p><div><pre class="programlisting">In:
import matplotlib.pyplot as plt
%matplotlib inline  

import numpy as np
import pandas as pd

dataset = pd.read_csv('./msd/YearPredictionMSD.txt', 
                      header=None).as_matrix()
In:
X_train = dataset[:463715, 1:]
y_train = np.asarray(dataset[:463715, 0])

X_test = dataset[463715:, 1:]
y_test = np.asarray(dataset[463715:, 0])
In:
print("Dataset is MB:", dataset.nbytes/1E6)
del dataset
Out:
Dataset is MB: 375.17116</pre></div><p class="calibre8">Our dataset is <a id="id628" class="calibre1"/>not all that small, since it's almost 400 MB. We should, therefore, be very smart and use any appropriate trick to cope with it without running out of memory (and becoming heavily reliant on the swap file) or even crashing our operating system.</p><p class="calibre8">Let's now get a baseline for comparisons: we will use the plain vanilla linear regression (vanilla means that there's no additional flavor to it; as with plain ice cream, our model uses the standard hyper-parameters). We then print the training time (in seconds), the MAE in the train set, and the MAE in the test set:</p><div><pre class="programlisting">In:
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.cross_validation import KFold
from sklearn.metrics import mean_absolute_error
import time
In:
regr = LinearRegression()

tic = time.clock()
regr.fit(X_train, y_train)
print("Training time [s]:", time.clock()-tic)

print("MAE train set:", mean_absolute_error(y_train, 
                                  regr.predict(X_train)))

print("MAE test set:", mean_absolute_error(y_test, 
                                  regr.predict(X_test)))
Out:
Training time [s]: 9.989145000000002
MAE train set: 6.79557016727
MAE test set: 6.80049646319</pre></div><p class="calibre8">Using <a id="id629" class="calibre1"/>linear regression, we can achieve a MAE of 6.8 in around 10 seconds. In addition, the learner seems stable and robust since there is no difference between MAE in the train set and MAE in the test set (thanks to the generalization power of linear regression). Let's now try to do even better. We test a stochastic gradient descent variation, to see if we can achieve a better MAE more rapidly (eventually). We experiment with both a small and a high number of iterations:</p><div><pre class="programlisting">In:
regr = SGDRegressor()

tic = time.clock()
regr.fit(X_train, y_train)
print("Training time [s]:", time.clock()-tic)

print("MAE train set:", mean_absolute_error(y_train, 
                                  regr.predict(X_train)))

print("MAE test set:", mean_absolute_error(y_test, 
                                  regr.predict(X_test)))
Out:
Training time [s]: 1.5492949999999972
MAE train set: 3.27482912145e+15
MAE test set: 3.30350427822e+15
In:
regr = SGDRegressor(n_iter=100)

tic = time.clock()
regr.fit(X_train, y_train)
print("Training time [s]:", time.clock()-tic)

print("MAE train set:", mean_absolute_error(y_train, 
                                  regr.predict(X_train)))

print("MAE test set:", mean_absolute_error(y_test, 
                                  regr.predict(X_test)))
Out:
Training time [s]: 24.713879
MAE train set: 2.12094618827e+15
MAE test set: 2.14161266897e+15</pre></div><p class="calibre8">The results seem to suggest that SGD Regression is not appropriate for the shape of the dataset: getting better results may require much perseverance and take a very long time.</p><p class="calibre8">Now, we have two options: the first is to resort to an advanced classifier (such as an ensemble); otherwise, we can fine-tune the model with feature engineering. For this problem, let's go for the second choice, since the main goal of this book is to work on linear models. Readers are strongly advised to try the first approach as comparison.</p><p class="calibre8">Let's now try to <a id="id630" class="calibre1"/>use the polynomial expansion of the features, followed by a feature selection step. This ensures that we have all the features, built on top on the features of the problem, in order to select the best and run through a linear regressor.</p><p class="calibre8">Since we don't know a-priori which is the optimum number of features, let's treat that as a parameter and plot the MAE for the training and the testing set as a variable. Furthermore, since we're undertaking a regression task, feature selection should be targeting the best regression features; therefore the F-score for regression is used to rank and select the top K features.</p><p class="calibre8">We immediately encounter a problem: polynomial feature expansion creates so many additional features. Since we are operating on a quite big dataset, we might have to subsample the training set. Let's first count how many features are after the polynomial expansion:</p><div><pre class="programlisting">In:
from sklearn.preprocessing import PolynomialFeatures
PolynomialFeatures().fit_transform(X_train[:10,:]).shape[1]
Out:
4186</pre></div><p class="calibre8">With more than 4,000 features, we should select at least 10 times more observations, in order not to risk overfitting. We shuffle the dataset, and select one twelfth of it (so the number of observations is around 40,000). To manage that, we can use K-fold, and select just the indexes composing the test set of the first piece:</p><div><pre class="programlisting">In:
from sklearn.pipeline import Pipeline
from sklearn import feature_selection
from sklearn.feature_selection import SelectKBest
import gc

folds = 12
train_idx = list(KFold(X_train.shape[0], folds, random_state=101, shuffle=True))[0][1]

to_plot = []


for k_feat in range(50, 2001, 50):
    
    gc.collect()
    
    print('---------------------------')
    print("K = ", k_feat)
    
    poly = PolynomialFeatures()
    regr = LinearRegression()
    f_sel = SelectKBest(feature_selection.f_regression, k=k_feat)

    pipeline = Pipeline([('poly', poly), ('f_sel', f_sel), ('regr', regr)])

    tic = time.clock()
    pipeline.fit(X_train[train_idx], y_train[train_idx])
    print("Training time [s]:", time.clock()-tic)
    
    mae_train = mean_absolute_error(y_train[train_idx], pipeline.predict(X_train[train_idx]))
    mae_test = mean_absolute_error(y_test, pipeline.predict(X_test))

    print("MAE train set:", mae_train)

    print("MAE test set:", mae_test)
    
    to_plot.append((k_feat, mae_train, mae_test))
Out:
...[output]...
In:
plt.plot([x[0] for x in to_plot], [x[1] for x in to_plot], 'b', label='Train')
plt.plot([x[0] for x in to_plot], [x[2] for x in to_plot], 'r--', label='Test')
plt.xlabel('Num. features selected')
plt.ylabel('MAE train/test')
plt.legend(loc=0)

plt.show()
Out:</pre></div><div><img src="img/00135.jpeg" alt="A regression problem" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">It seems that we <a id="id631" class="calibre1"/>have found the optimal value for K, the number of selected features, in K=900. At this point:</p><div><ul class="itemizedlist"><li class="listitem">The training MAE and the testing MAE diverge: if we are to use more than 900 features, we may have start overfitting.</li><li class="listitem">The testing MAE is at its minimum. The reading for the MAE in the test set is 6.70.</li><li class="listitem">This is the best tradeoff between performance and training time (23 seconds).</li></ul></div><p class="calibre8">If you have a better machine (with 16 GB of RAM or more), you can re-run the last two cells, increasing the size of the training set (moving, for example, the number of folds from 12 to 8 or 4). The result should improve by some decimal places, although the training will be longer.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec96" class="calibre1"/>Testing a classifier instead of a regressor</h2></div></div></div><p class="calibre8">As an <a id="id632" class="calibre1"/>open problem (we're not providing a solution), we could have solved this problem as a classification task, since the number of target variables is <em class="calibre9">just</em> 89 (which is higher than the majority of the classification problems, though). Following this path, you will encounter different kinds of problems (the MAE is wrongly defined on a classification problem). We encourage readers to have a go at this, and try to match the result we obtained with the regression learner; here is the first step:</p><div><pre class="programlisting">In:
print(np.unique(np.ascontiguousarray(y_train)))
print(len(np.unique(np.ascontiguousarray(y_train))))
Out:
[ 1922.  1924.  1925.  1926.  1927.  1928.  1929.  1930.  1931.  1932.
  1933.  1934.  1935.  1936.  1937.  1938.  1939.  1940.  1941.  1942.
  1943.  1944.  1945.  1946.  1947.  1948.  1949.  1950.  1951.  1952.
  1953.  1954.  1955.  1956.  1957.  1958.  1959.  1960.  1961.  1962.
  1963.  1964.  1965.  1966.  1967.  1968.  1969.  1970.  1971.  1972.
  1973.  1974.  1975.  1976.  1977.  1978.  1979.  1980.  1981.  1982.
  1983.  1984.  1985.  1986.  1987.  1988.  1989.  1990.  1991.  1992.
  1993.  1994.  1995.  1996.  1997.  1998.  1999.  2000.  2001.  2002.
  2003.  2004.  2005.  2006.  2007.  2008.  2009.  2010.  2011.]
89
In:
from sklearn.linear_model import SGDClassifier
regr = SGDClassifier('log', random_state=101)

tic = time.clock()
regr.fit(X_train, y_train)
print("Training time [s]:", time.clock()-tic)

print("MAE train set:", mean_absolute_error(y_train, 
                                  regr.predict(X_train)))

print("MAE test set:", mean_absolute_error(y_test, 
                                  regr.predict(X_test)))
Out:
Training time [s]: 117.23069399999986 
MAE train set: 7.88104546974 
MAE test set: 7.7926593066</pre></div></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec52" class="calibre1"/>An imbalanced and multiclass classification problem</h1></div></div></div><p class="calibre8">Given some<a id="id633" class="calibre1"/> descriptors of a sequence of packets, flowing to/from a host connected to the Internet, the goal of this problem is to detect whether that sequence signals a malicious attack or not. If it does, we should also classify the type of attack. That's a multiclass classification problem, since the possible labels are multiple ones.</p><p class="calibre8">For<a id="id634" class="calibre1"/> each observation, 42 features are revealed: please note that some of them are categorical, whereas others are numerical. The dataset is composed of almost 5 million observations (but in this exercise we're using just the first million, to avoid memory constraints), and the number of possible labels is 23. One of them represents a non-malicious situation (<em class="calibre9">normal</em>); all the others represent 22 different network attacks. Some attention should be paid to the fact that the frequencies of response classes are imbalanced: for some attacks there are multiple observations, for others just a few.</p><p class="calibre8">No instruction is given about how to split the train/test, or how to evaluate results. In this problem, we will adopt an exploratory goal: trying to reveal accurate information for all the labels. We warmly advise readers to take some further steps and tune the learner to maximize the precision of the detection, just for the malicious activities in the dataset.</p><div><h3 class="title2"><a id="note06" class="calibre1"/>Note</h3><p class="calibre8">The full description of this problem can be found at the website: <a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1999+Data">https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1999+Data</a>.</p></div><p class="calibre8">First at all, let's load the data. The file doesn't contain the header; therefore we have to specify the column names while loading it with pandas:</p><div><pre class="programlisting">In:
import matplotlib.pyplot as plt
%matplotlib inline  
import matplotlib.pylab as pylab

import numpy as np
import pandas as pd

columns = ["duration", "protocol_type", "service", 
           "flag", "src_bytes", "dst_bytes", "land", 
           "wrong_fragment", "urgent", "hot", "num_failed_logins", 
           "logged_in", "num_compromised", "root_shell", 
           "su_attempted", "num_root", "num_file_creations", 
           "num_shells", "num_access_files", "num_outbound_cmds", 
           "is_host_login", "is_guest_login", "count", "srv_count", 
           "serror_rate", "srv_serror_rate", "rerror_rate", 
           "srv_rerror_rate", "same_srv_rate", "diff_srv_rate", 
           "srv_diff_host_rate", "dst_host_count", 
           "dst_host_srv_count", "dst_host_same_srv_rate", 
           "dst_host_diff_srv_rate", "dst_host_same_src_port_rate", 
           "dst_host_srv_diff_host_rate", "dst_host_serror_rate", 
           "dst_host_srv_serror_rate", "dst_host_rerror_rate", 
           "dst_host_srv_rerror_rate", "outcome"]

dataset = pd.read_csv('./kdd/kddcup.dat', names=columns, nrows=1000000)</pre></div><p class="calibre8">Let's now<a id="id635" class="calibre1"/> check the first few lines of the loaded dataset (in order to understand the overall shape of the dataset), its size (in terms of observations and features), and the types of features (to separate categorical from numerical ones):</p><div><pre class="programlisting">In:
print(dataset.head())
Out:
...[head of the dataset] ...
In:
dataset.shape
Out:
(1000000, 42)
In:
dataset.dtypes
Out:
duration                         int64 
protocol_type                   object 
service                         object
flag                            object
src_bytes                        int64 
dst_bytes                        int64 
land                             int64 
wrong_fragment                   int64
 urgent                          int64 
hot                              int64 
num_failed_logins                int64 
logged_in                        int64 
num_compromised                  int64 
root_shell                       int64 
su_attempted                     int64 
num_root                         int64 
num_file_creations               int64 
num_shells                       int64 
num_access_files                 int64 
num_outbound_cmds                int64 
is_host_login                    int64 
is_guest_login                   int64 
count                            int64 
srv_count                        int64 
serror_rate                    float64 
srv_serror_rate                float64 
rerror_rate                    float64 
srv_rerror_rate                float64 
same_srv_rate                  float64 
diff_srv_rate                  float64 
srv_diff_host_rate             float64 
dst_host_count                   int64 
dst_host_srv_count               int64 
dst_host_same_srv_rate         float64 
dst_host_diff_srv_rate         float64 
dst_host_same_src_port_rate    float64 
dst_host_srv_diff_host_rate    float64 
dst_host_serror_rate           float64 
dst_host_srv_serror_rate       float64 
dst_host_rerror_rate           float64 
dst_host_srv_rerror_rate       float64 
outcome                         object 
dtype: object</pre></div><p class="calibre8">It seems <a id="id636" class="calibre1"/>we're operating on a very big dataset, since it has 1M rows and 42 columns, with some of them being categorical. Now, let's separate the target variable from the features, encoding the strings (containing attack names) with ordinal numbers. To do so, we can use the <code class="email">LabelEncoder</code> object:</p><div><pre class="programlisting">In:
sorted(dataset['outcome'].unique())
Out:
['back.',  'buffer_overflow.',  'ftp_write.',  'guess_passwd.',  'imap.',  'ipsweep.',  'land.',  'loadmodule.',  'multihop.',  'neptune.',  'nmap.',  'normal.',  'perl.',  'phf.',  'pod.',  'portsweep.', 'satan.',  'smurf.', 'teardrop.', 'warezmaster.']
In:
from sklearn.preprocessing import LabelEncoder

labels_enc = LabelEncoder()

labels = labels_enc.fit_transform(dataset['outcome'])
labels_map = labels_enc.classes_
Out:
array(['back.', 'buffer_overflow.', 'ftp_write.', 'guess_passwd.', 'imap.', 'ipsweep.', 'land.', 'loadmodule.', 'multihop.', 'neptune.', 'nmap.', 'normal.', 'perl.', 'phf.', 'pod.', 'portsweep.', 'satan.', 'smurf.', 'teardrop.', 'warezmaster.'], dtype=object)</pre></div><p class="calibre8">Targets, <a id="id637" class="calibre1"/>variables are now in a separate array, encoded as integers. Let's now remove the target column from the dataset, and one-hot encode all the categorical features. To do so, we can simply use the Pandas <code class="email">get_dummies</code> function. The new shape of the dataset is therefore <em class="calibre9">larger</em>, because each level composing a categorical feature is now a binary feature:</p><div><pre class="programlisting">In:
dataset.drop('outcome', axis=1, inplace=True)
In:
observations = pd.get_dummies(dataset, sparse=True)
del dataset
In:
observations.shape
Out:
(1000000, 118)</pre></div><p class="calibre8">Since we have many available observations and many classes contain just a few samples, we can shuffle and split the dataset in two portions: one to be used in training, the other for testing:</p><div><pre class="programlisting">In:
from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = \
    train_test_split(observations.as_matrix(), labels,
                    train_size=0.5, random_state=101)

del observations</pre></div><p class="calibre8">Given the exploratory nature of our task, let's now define a function to print the confusion matrix, normalized by the number of occurrences per each class:</p><div><pre class="programlisting">In:
def plot_normalised_confusion_matrix(cm, labels_str, title='Normalised confusion matrix', cmap=plt.cm.Blues):
    pylab.rcParams['figure.figsize'] = (6.0, 6.0)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    plt.imshow(cm_normalized, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(labels_str))
    plt.xticks(tick_marks, labels_str, rotation=90)
    plt.yticks(tick_marks, labels_str)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()</pre></div><p class="calibre8">Let's now <a id="id638" class="calibre1"/>create a baseline for this task. We will use a simple <code class="email">SGDClassifier</code>, with logistic loss, in this first step. For both training and test sets we will print the overall accuracy of the solution, the normalized confusion matrix, and the classification report (containing the precision, recall, F1-score, and support for each class).</p><div><pre class="programlisting">In:
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

clf = SGDClassifier('log', random_state=101)
clf.fit(X_train, y_train)

y_train_pred = clf.predict(X_train)
y_test_pred = clf.predict(X_test)

print("TRAIN SET")
print("Accuracy:", accuracy_score(y_train, y_train_pred))

print("Confusion matrix:")
plot_normalised_confusion_matrix(confusion_matrix(y_train, y_train_pred), labels_map)

print("Classification report:")
print(classification_report(y_train, y_train_pred, target_names=labels_map))

print("TEST SET")
print("Accuracy:", accuracy_score(y_test, y_test_pred))

print("Confusion matrix:")
plot_normalised_confusion_matrix(confusion_matrix(y_test, y_test_pred), labels_map)

print("Classification report:")
print(classification_report(y_test, y_test_pred, target_names=labels_map))

Out:
TRAIN SET 
Accuracy: 0.781702 
Confusion matrix:</pre></div><div><img src="img/00136.jpeg" alt="An imbalanced and multiclass classification problem" class="calibre10"/></div><p class="calibre11"> </p><div><pre class="programlisting">Classification report:
                  precision    recall  f1-score   support

           back.       0.00      0.00      0.00      1005
buffer_overflow.       0.00      0.00      0.00         1
      ftp_write.       0.00      0.00      0.00         2
   guess_passwd.       0.00      0.00      0.00        30
           imap.       0.00      0.00      0.00         2
        ipsweep.       0.00      0.00      0.00      3730
           land.       0.00      0.00      0.00        10
     loadmodule.       0.00      0.00      0.00         3
       multihop.       1.00      1.00      1.00    102522
        neptune.       0.06      0.00      0.00      1149
           nmap.       0.96      0.64      0.77    281101
         normal.       0.00      0.00      0.00         1
           perl.       0.00      0.00      0.00         2
            phf.       0.00      0.00      0.00        22
            pod.       0.00      0.00      0.00      1437
      portsweep.       1.00      0.88      0.93      2698
          satan.       0.99      0.99      0.99    106165
          smurf.       0.00      0.00      0.00       110
       teardrop.       0.00      0.90      0.01        10

     avg / total       0.96      0.78      0.85    500000

TEST SET
Accuracy: 0.781338
Confusion matrix:</pre></div><div><img src="img/00137.jpeg" alt="An imbalanced and multiclass classification problem" class="calibre10"/></div><p class="calibre11"> </p><div><pre class="programlisting">Classification report:
                  precision    recall  f1-score   support

           back.       0.00      0.00      0.00       997
buffer_overflow.       0.00      0.00      0.00         4
      ftp_write.       0.00      0.00      0.00         6
   guess_passwd.       0.00      0.00      0.00        23
           imap.       0.00      0.00      0.00        10
        ipsweep.       0.00      0.00      0.00      3849
           land.       0.00      0.00      0.00         7
     loadmodule.       0.00      0.00      0.00         2
       multihop.       0.00      0.00      0.00         3
        neptune.       1.00      1.00      1.00    102293
           nmap.       0.05      0.00      0.00      1167
         normal.       0.96      0.64      0.77    281286
           perl.       0.00      0.00      0.00         1
            phf.       0.00      0.00      0.00         1
            pod.       0.00      0.00      0.00        18
      portsweep.       0.00      0.00      0.00      1345
          satan.       1.00      0.88      0.94      2691
          smurf.       0.99      1.00      0.99    106198
       teardrop.       0.00      0.00      0.00        89
    warezmaster.       0.00      0.90      0.01        10

     avg / total       0.96      0.78      0.85    500000</pre></div><p class="calibre8">Although <a id="id639" class="calibre1"/>the output is very long, some points are immediately visible in this baseline:</p><div><ul class="itemizedlist"><li class="listitem">Accuracy is low (0.80), but the classification is resilient to overfitting.</li><li class="listitem">Just two vertical lines dominate the confusion matrix. This indicates that the classifier fits only two classes during the training phase. Not surprisingly, they are the most populated ones.</li><li class="listitem">You can reach the same conclusion (that the class imbalance has influenced the results) by looking at the classification report: just a few classes have non-zero scores.</li></ul></div><p class="calibre8">Such a problem is a very frequent one, and it happens when you try to fit a linear learner on a strongly imbalanced dataset. Let's now try to oversample small classes, and sub-sample the most popular ones. In the following function, we implemented a bootstrap algorithm with replacement, where each class gets, in the output data, at least <code class="email">min_samples_out</code> observations and up to <code class="email">max_samples_out</code>. This should force the learning algorithm to <em class="calibre9">take account of</em> all classes with a similar weight.</p><div><pre class="programlisting">In:
import random
random.seed(101)

def sample_class_with_replacement(X, y, label, min_samples_out, max_samples_out):
    rows = np.where(y==label)[0]
    
    if len(rows) == 0:
        raise Exception
    
    n_estraction = min(max(len(rows), min_samples_out), max_samples_out)
    extracted = [random.choice(rows) for _ in range(n_estraction)]
    
    return extracted


train_idx = []

for label in np.unique(labels):
    try:
        idx = sample_class_with_replacement(X_train, y_train, label, 500, 20000)
        train_idx.extend(idx)
    except:
        pass
    
X_train_sampled_balanced = X_train[train_idx, :]
y_train_sampled_balanced = y_train[train_idx]</pre></div><p class="calibre8">Now, we can<a id="id640" class="calibre1"/> try to do better than the baseline, training out the learner on this modified (balanced) training set, and then applying it on the test set:</p><div><pre class="programlisting">In:
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import confusion_matrix

clf = SGDClassifier('log', random_state=101)
clf.fit(X_train_sampled_balanced, y_train_sampled_balanced)

y_train_pred = clf.predict(X_train_sampled_balanced)
y_test_pred = clf.predict(X_test)

print("TRAIN SET")
print("Accuracy:", accuracy_score(y_train_sampled_balanced, y_train_pred))

print("Confusion matrix:")
plot_normalised_confusion_matrix(confusion_matrix(
        y_train_sampled_balanced, y_train_pred), labels_map)

print("Classification report:")
print(classification_report(y_train_sampled_balanced, y_train_pred, target_names=labels_map))


print("TEST SET")
print("Accuracy:", accuracy_score(y_test, y_test_pred))

print("Confusion matrix:")
plot_normalised_confusion_matrix(confusion_matrix(y_test, y_test_pred), labels_map)

print("Classification report:")
print(classification_report(y_test, y_test_pred, target_names=labels_map))
Out:
TRAIN SET
Accuracy: 0.712668335121
[...]
TEST SET
Accuracy: 0.723616
[...]</pre></div><p class="calibre8">The results <a id="id641" class="calibre1"/>suggest we're heading toward the right direction. The confusion matrix looks more diagonal (meaning that matches between rows and columns occur more on the diagonal), and the accuracy increases to 0.72 in the test set. Let's now try some values to achieve hyperparameter optimization, and boost the score to its max. For that, we run a grid-search cross-validation, using three folds:</p><div><pre class="programlisting">In:
from sklearn.grid_search import GridSearchCV

parameters = {
    'loss': ('log', 'hinge'),
    'alpha': [0.1, 0.01, 0.001, 0.0001]
}

clfgs = GridSearchCV(SGDClassifier(random_state=101, n_jobs=1),
                   param_grid=parameters,
                   cv=3,
                   n_jobs=1,
                   scoring='accuracy'
                  )
clfgs.fit(X_train_sampled_balanced, y_train_sampled_balanced)
clf = clfgs.best_estimator_

print(clfgs.best_estimator_)

y_train_pred = clf.predict(X_train_sampled_balanced)
y_test_pred = clf.predict(X_test)

print("TRAIN SET")
print("Accuracy:", accuracy_score(y_train_sampled_balanced, 
                                  y_train_pred))

print("Confusion matrix:")
plot_normalised_confusion_matrix(
    confusion_matrix(y_train_sampled_balanced, y_train_pred), 
    labels_map)

print("Classification report:")
print(classification_report(
        y_train_sampled_balanced, y_train_pred,
        target_names=labels_map))

print("TEST SET")
print("Accuracy:", accuracy_score(y_test, y_test_pred))

print("Confusion matrix:")
plot_normalised_confusion_matrix(
    confusion_matrix(y_test, y_test_pred), labels_map)

print("Classification report:")
print(classification_report(
        y_test, y_test_pred, target_names=labels_map))

Out:
TRAIN SET
Accuracy: 0.695202531813
[...]
TEST SET
Accuracy: 0.706034
[...]</pre></div><p class="calibre8">Although we run <a id="id642" class="calibre1"/>a grid-search cross-validation, the results look the same as in the previous experiment. We now try a different approach: since there are many output classes, let's try to use a <code class="email">OneVsOne</code> strategy: instead of fitting one classifier per class, we fit a classifier for each pair of classes. That should result in a more accurate, although longer to train, model. Even here, each learner is cross-validated with a grid search and three folds:</p><div><pre class="programlisting">In:
from sklearn.multiclass import OneVsOneClassifier
from sklearn.grid_search import GridSearchCV

parameters = {
    'estimator__loss': ('log', 'hinge'),
    'estimator__alpha': [1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001]
}

clfgs = GridSearchCV(OneVsOneClassifier(SGDClassifier(random_state=101, n_jobs=1)),
                   param_grid=parameters,
                   cv=3,
                   n_jobs=1,
                   scoring='accuracy'
                  )
clfgs.fit(X_train_sampled_balanced, y_train_sampled_balanced)
clf = clfgs.best_estimator_

y_train_pred = clf.predict(X_train_sampled_balanced)
y_test_pred = clf.predict(X_test)

print("TRAIN SET")
print("Accuracy:", accuracy_score(y_train_sampled_balanced, y_train_pred))

print("Confusion matrix:")
plot_normalised_confusion_matrix(confusion_matrix(y_train_sampled_balanced, y_train_pred), labels_map)

print("Classification report:")
print(classification_report(y_train_sampled_balanced, y_train_pred, target_names=labels_map))


print("TEST SET")
print("Accuracy:", accuracy_score(y_test, y_test_pred))

print("Confusion matrix:")
plot_normalised_confusion_matrix(confusion_matrix(y_test, y_test_pred), labels_map)

print("Classification report:")
print(classification_report(y_test, y_test_pred, target_names=labels_map))
Out:
TRAIN SET
Accuracy: 0.846250612429
[...]
TEST SET
Accuracy: 0.905708
[...]</pre></div><p class="calibre8">The results are better, in both the training set and testing set. Let's now try a logistic regressor <a id="id643" class="calibre1"/>instead of <code class="email">SGDClassifier</code>:</p><div><pre class="programlisting">In:
from sklearn.linear_model import LogisticRegression

clf = OneVsOneClassifier(LogisticRegression(random_state=101, n_jobs=1))
clf.fit(X_train_sampled_balanced, y_train_sampled_balanced)

y_train_pred = clf.predict(X_train_sampled_balanced)
y_test_pred = clf.predict(X_test)

print("TRAIN SET")
print("Accuracy:", accuracy_score(y_train_sampled_balanced, 
                                  y_train_pred))

print("Confusion matrix:")
plot_normalised_confusion_matrix(
    confusion_matrix(y_train_sampled_balanced, y_train_pred), 
    labels_map)

print("Classification report:")
print(classification_report(
        y_train_sampled_balanced, y_train_pred,
        target_names=labels_map))

print("TEST SET")
print("Accuracy:", accuracy_score(y_test, y_test_pred))

print("Confusion matrix:")
plot_normalised_confusion_matrix(
    confusion_matrix(y_test, y_test_pred), labels_map)

print("Classification report:")
print(classification_report(
        y_test, y_test_pred, target_names=labels_map))

Out:
TRAIN SET
Accuracy: 0.985712204876
Confusion matrix:</pre></div><div><img src="img/00138.jpeg" alt="An imbalanced and multiclass classification problem" class="calibre10"/></div><p class="calibre11"> </p><div><pre class="programlisting">Classification report:
                  precision    recall  f1-score   support

           back.       1.00      0.98      0.99      1005
buffer_overflow.       1.00      1.00      1.00       500
      ftp_write.       1.00      1.00      1.00       500
   guess_passwd.       1.00      0.11      0.19       500
           imap.       1.00      1.00      1.00       500
        ipsweep.       1.00      0.99      1.00      3730
           land.       1.00      1.00      1.00       500
     loadmodule.       1.00      0.32      0.49       500
       multihop.       1.00      1.00      1.00     20000
        neptune.       0.91      1.00      0.95      1149
           nmap.       0.97      1.00      0.98     20000
         normal.       1.00      1.00      1.00       500
           perl.       1.00      1.00      1.00       500
            phf.       0.99      1.00      1.00       500
            pod.       1.00      1.00      1.00      1437
      portsweep.       0.98      1.00      0.99      2698
          satan.       1.00      1.00      1.00     20000
          smurf.       1.00      1.00      1.00       500
       teardrop.       0.55      0.83      0.66       500

     avg / total       0.99      0.99      0.98     75519

TEST SET
Accuracy: 0.996818
Confusion matrix:</pre></div><div><img src="img/00139.jpeg" alt="An imbalanced and multiclass classification problem" class="calibre10"/></div><p class="calibre11"> </p><div><pre class="programlisting">Classification report:
                  precision    recall  f1-score   support

           back.       1.00      0.98      0.99       997
buffer_overflow.       0.00      0.00      0.00         4
      ftp_write.       0.00      0.00      0.00         6
   guess_passwd.       1.00      0.13      0.23        23
           imap.       0.43      0.30      0.35        10
        ipsweep.       0.97      0.99      0.98      3849
           land.       0.38      0.86      0.52         7
     loadmodule.       0.00      0.00      0.00         2
       multihop.       0.00      0.00      0.00         3
        neptune.       1.00      1.00      1.00    102293
           nmap.       0.52      0.99      0.68      1167
         normal.       1.00      1.00      1.00    281286
           perl.       0.00      0.00      0.00         1
            phf.       0.17      1.00      0.29         1
            pod.       0.26      1.00      0.42        18
      portsweep.       0.96      0.99      0.98      1345
          satan.       0.96      1.00      0.98      2691
          smurf.       1.00      1.00      1.00    106198
       teardrop.       0.99      0.98      0.98        89
    warezmaster.       0.45      0.90      0.60        10

     avg / total       1.00      1.00      1.00    500000</pre></div><p class="calibre8">The results<a id="id644" class="calibre1"/> look much better than the baseline and the previous solution. In fact:</p><div><ol class="orderedlist"><li class="listitem" value="1">The accuracy measured on the balanced training set is close to the one recorded on the test set. This ensures the generalization capabilities of the model.</li><li class="listitem" value="2">The confusion matrix is <em class="calibre9">almost</em> diagonal. It means that all the classes have been included in the fitting (and the prediction) phase.</li><li class="listitem" value="3">The precision/recall and F1 score are non-zero for many classes, even for those with little support.</li></ol><div></div><p class="calibre8">At this point, we're satisfied with the solution. If you would like to dig further and test more model hypotheses on the full 5-million dataset, it's time now to move to non-linear classifiers. When doing so, please note in advance both the complexity and the running time required to get a prediction (after all, you're using a dataset containing more than 100 million values).</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec53" class="calibre1"/>A ranking problem</h1></div></div></div><p class="calibre8">Given<a id="id645" class="calibre1"/> some descriptors of a car and its price, the goal of this problem is to predict the degree to which the car is riskier than its price indicates. Actuaries in the insurance business call this process <em class="calibre9">symboling</em>, and the outcome is a rank: a value of +3 indicates the car is risky; -3 indicates that it's pretty safe (although the lowest value in the dataset is -2).</p><p class="calibre8">The description of the car <a id="id646" class="calibre1"/>includes its specifications in terms of various characteristics (brand, fuel type, body style, length, and so on). Moreover, you get its price and normalized loss in use as compared to other cars (this represents the average loss per car per year, normalized for all cars within a certain car segment).</p><p class="calibre8">There are 205<a id="id647" class="calibre1"/> cars in the dataset, and the number of features is 25; some of them are categorical, and others are numerical. In addition, the dataset expressively states that there are some missing values, encoded using the string <code class="email">"?"</code>.</p><p class="calibre8">Although it is not stated directly on the presentation page, the goal of our task is to minimize the <em class="calibre9">label ranking loss</em>, a measure that indicates how well we performed the ranking. This score works on probabilities, and a perfect rank gives a loss of zero. Using regression scores, such as MAE or MSE, has little relevance to this task, since the prediction must be an integer; also, a classification score such as <code class="email">accuracy</code> makes no sense either, since it doesn't tell us how far we are from the perfect solution. Another score we will see in the code is <a id="id648" class="calibre1"/>
<strong class="calibre2">label ranking average precision</strong> (<strong class="calibre2">LRAP</strong>). In this case, a perfect ranked output has a score of one (exactly like precision in the sense of classification). More information about the metrics is available on the <a id="id649" class="calibre1"/>Scikit-learn website: <a class="calibre1" href="http://scikit-learn.org/stable/modules/model_evaluation.html">http://scikit-learn.org/stable/modules/model_evaluation.html</a> or in the <em class="calibre9">Ranking Measures and Loss Functions in Learning to Rank</em> paper, presented in 2009 at the Advances in Neural Information Processing Systems Conference.</p><p class="calibre8">A full description of this problem can be found at: <a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/Automobile">https://archive.ics.uci.edu/ml/datasets/Automobile</a>.</p><p class="calibre8">First of all, let's load the data. The CSV file has no header; therefore we will manually set the column names. Also, since the author of the dataset has released the information, all the <code class="email">"?"</code> strings will be handled as missing data—that is, Pandas <code class="email">NaN</code> values:</p><div><pre class="programlisting">In:
import matplotlib.pyplot as plt
%matplotlib inline  
import matplotlib.pylab as pylab

import numpy as np
import pandas as pd

columns = ["symboling","normalized-losses","make","fuel-type",
           "aspiration","num-of-doors","body-style","drive-wheels",
           "engine-location","wheel-base","length","width","height",
           "curb-weight","engine-type","num-of-cylinders",
           "engine-size","fuel-system","bore","stroke",
           "compression-ratio","horsepower","peak-rpm","city-mpg",
           "highway-mpg","price"]

dataset = pd.read_csv('./autos/imports-85.data', 
                       na_values="?", names=columns)</pre></div><p class="calibre8">Although it <a id="id650" class="calibre1"/>doesn't guarantee everything is perfect, let's look at the first lines of the dataset. Here we're able to identify the missing data (containing <code class="email">NaN</code> values) and understand which features are categorical and which numerical:</p><div><pre class="programlisting">In:
print(dataset.head())
Out:
   symboling  normalized-losses         make fuel-type aspiration  \
0          3                NaN  alfa-romero       gas        std   
1          3                NaN  alfa-romero       gas        std   
2          1                NaN  alfa-romero       gas        std   
3          2                164         audi       gas        std   
4          2                164         audi       gas        std   

  num-of-doors   body-style drive-wheels engine-location  wheel-base  ...    \
0          two  convertible          rwd           front        88.6  ...     
1          two  convertible          rwd           front        88.6  ...     
2          two    hatchback          rwd           front        94.5  ...     
3         four        sedan          fwd           front        99.8  ...     
4         four        sedan          4wd           front        99.4  ...     

   engine-size  fuel-system  bore  stroke compression-ratio horsepower  \
0          130         mpfi  3.47    2.68                 9        111   
1          130         mpfi  3.47    2.68                 9        111   
2          152         mpfi  2.68    3.47                 9        154   
3          109         mpfi  3.19    3.40                10        102   
4          136         mpfi  3.19    3.40                 8        115   

   peak-rpm city-mpg  highway-mpg  price  
0      5000       21           27  13495  
1      5000       21           27  16500  
2      5000       19           26  16500  
3      5500       24           30  13950  
4      5500       18           22  17450  

[5 rows x 26 columns]
In:
dataset.dtypes
Out:
symboling              int64
normalized-losses    float64
make                  object
fuel-type             object
aspiration            object
num-of-doors          object
body-style            object
drive-wheels          object
engine-location       object
wheel-base           float64
length               float64
width                float64
height               float64
curb-weight            int64
engine-type           object
num-of-cylinders      object
engine-size            int64
fuel-system           object
bore                 float64
stroke               float64
compression-ratio    float64
horsepower           float64
peak-rpm             float64
city-mpg               int64
highway-mpg            int64
price                float64
dtype: object</pre></div><p class="calibre8">It seems we have <a id="id651" class="calibre1"/>many categorical features. Here, we have to think carefully about what to do. The dataset contains only 205 observations; therefore transforming all the categorical features to dummy features is not a great idea (we may end up with more features than observations). Let's try to be very conservative with the number of features created. Carefully checking the features, we can use the following approach:</p><div><ol class="orderedlist"><li class="listitem" value="1">Some categorical features are actually (wordy) numerical: they contain numbers indicating a number. For them, we just need to map words to numbers. In this case, no additional features are created.</li><li class="listitem" value="2">Some other categorical features are actually binary (two doors versus four doors, diesel versus gas, and so on). For them, we can map the two levels to different values (0 and 1). Even here, we don't need to create additional features.</li><li class="listitem" value="3">All the remaining should be dummy-encoded.</li></ol><div></div><p class="calibre8">This procedure is <a id="id652" class="calibre1"/>shown in the following cell. To create the first map, we simply use the <code class="email">map</code> method provided by Pandas. For the second mapping, we use the <code class="email">LabelEncoder</code> object provided by Scikit-learn; for the last one, we use the <code class="email">get_dummies</code> function, as seen in the previous example:</p><div><pre class="programlisting">In:
from sklearn.preprocessing import LabelEncoder


words_to_nums = {'two':2, 'three':3, 'four':4, 'five':5, 
                 'six':6, 'eight':8, 'twelve':12}

columns_to_map = ['num-of-cylinders', 'num-of-doors']
columns_to_dummy = ['make', 'body-style', 'drive-wheels', 
                    'engine-type', 'fuel-system']
columns_to_label_encode = ['fuel-type', 'aspiration', 
                           'engine-location']

for col in columns_to_map:
    dataset[col] = dataset[col].map(pd.Series(words_to_nums))

for col in columns_to_label_encode:
    dataset[col] = LabelEncoder().fit_transform(dataset[col])

dataset = pd.get_dummies(dataset, columns=columns_to_dummy)

dataset.shape
Out:
(205,66)</pre></div><p class="calibre8">Adopting this conservative approach, the final number of columns is 66 (previously it was 26). Now, let's extract the target value vector out of the DataFrame, and then map every <code class="email">NaN</code> value to the median of the feature, creating the observation matrix.</p><p class="calibre8">Why did we use the median (instead of the mean)? Because the dataset is so small that we don't want to introduce new values:</p><div><pre class="programlisting">In:
ranks = dataset['symboling'].as_matrix()
observations = dataset.drop('symboling', axis=1).as_matrix()
In:
from sklearn.preprocessing import Imputer
imp = Imputer(strategy="median", axis=0)
observations = imp.fit_transform(observations)</pre></div><p class="calibre8">Now, it's time to split<a id="id653" class="calibre1"/> the observations into training and testing. Since the dataset is very small, we decided to have the testing set made up of 25% of the observations (around 51 samples). Also, we tried to achieve a testing set containing the same percentage of samples for each class. For this purpose, we've used the <code class="email">StratifiedKFold</code> class.</p><div><pre class="programlisting">In:
from sklearn.cross_validation import StratifiedKFold

kf = StratifiedKFold(ranks, 4, shuffle=True, random_state=101)
idxs = list(kf)[0]

X_train = observations[idxs[0], :]
X_test = observations[idxs[1], :]
y_train = ranks[idxs[0]]
y_test = ranks[idxs[1]]</pre></div><p class="calibre8">The next step is creating two functions: the first should map classes to vectors of probability for each class (for example, class -2 becomes the vector <code class="email">[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</code>; class +3 becomes the vector <code class="email">[0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</code>, and so on). This step is required by the scoring function.</p><p class="calibre8">The second function we need is to check whether the classifier is trained on a dataset that includes all the classes (since we're operating on a training set composed of only 153 samples, and we will use cross validation, it's better to carefully check every step). To do so, we use a simple <code class="email">assert</code> equality:</p><div><pre class="programlisting">In:
def prediction_to_probas(class_pred):
    
    probas = []
    for el in class_pred:
        prob = [0.]*6
        prob[el+2] = 1.0
        probas.append(prob)
    return np.array(probas)

def check_estimator(estimator):
        assert sum(
            np.abs(clf.classes_ - np.array([-2, -1, 0, 1, 2, 3]))
        ) == 0</pre></div><p class="calibre8">Now, it's time to classify. We will initially use a simple <code class="email">LogisticRegression</code>. Since we have a multiclass<a id="id654" class="calibre1"/> problem, we can use more than one CPU in the training process. After training the classifier, we print the <code class="email">Ranking loss</code> and the <code class="email">Ranking avg precision score</code> to have a baseline for comparison purposes:</p><div><pre class="programlisting">In:
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(random_state=101)
clf.fit(X_train, y_train)
check_estimator(clf)

y_test_proba = prediction_to_probas(y_test)
y_pred_proba = clf.predict_proba(X_test)
In:
from sklearn.metrics import label_ranking_average_precision_score
from sklearn.metrics import label_ranking_loss

print("Ranking loss:", label_ranking_loss(y_test_proba, y_pred_proba))
print("Ranking avg precision:", label_ranking_average_precision_score(y_test_proba, y_pred_proba))
Out:
Ranking loss: 0.0905660377358
Ranking avg precision: 0.822327044025</pre></div><p class="calibre8">The baseline result is already not too bad. The <code class="email">Ranking loss</code> is close to zero (and, in this case, the average label precision is close to <code class="email">1</code>). Now we try to improve the solution, using a grid-search cross-validation. Since we have very few samples in the training set, we have to use a boosted validation, where each fold may contain samples appearing in other folds. <code class="email">StratifiedShuffleSplit</code> is the best option, ensuring as well that the validation set contains the same percentage of samples for each class. We will create five folds, each of them containing 70% of the training set as training, and the remaining 30 as testing.</p><p class="calibre8">The last thing we should create is a scoring function for the cross validation: Scikit-learn doesn't include any learn-to-rank scoring function out of the box in the <code class="email">GridSearchCV</code> object, therefore we have to build it. We decide to build it as the label ranking loss multiplied by <code class="email">-1</code>: since the goal of the grid search is to maximize the score, we have to invert it to find its minimum:</p><div><pre class="programlisting">In:
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.cross_validation import StratifiedShuffleSplit

def scorer(estimator, X, y):
    
    check_estimator(estimator)
    y_proba = prediction_to_probas(y)
    return -1*label_ranking_loss(y_proba, estimator.predict_proba(X))

params = {'C': np.logspace(-1, 1, 10)}

cv = StratifiedShuffleSplit(y_train, random_state=101,

                            n_iter=5, train_size=0.70)
gs_cv = GridSearchCV(LogisticRegression(random_state=101),
                     param_grid=params,
                     n_jobs=1,
                     cv=cv,
                     scoring=scorer)

gs_cv.fit(X_train, y_train)
clf = gs_cv.best_estimator_

y_pred_proba = clf.predict_proba(X_test)

print("Ranking loss:", 
      label_ranking_loss(y_test_proba, y_pred_proba))
print("Ranking avg precision:", 
      label_ranking_average_precision_score(y_test_proba, 
                                            y_pred_proba))
Out:
Ranking loss: 0.0716981132075
Ranking avg precision: 0.839622641509</pre></div><p class="calibre8">With the hyper parameter <a id="id655" class="calibre1"/>optimization procedure, combined with the cross-validation, we've been able to improve the performance. Now, let's check how the confusion matrix looks for this solution:</p><div><pre class="programlisting">In:
from sklearn.metrics import confusion_matrix

def plot_normalised_confusion_matrix(cm):
    labels_str = [str(n) for n in range(-2, 4)]
    pylab.rcParams['figure.figsize'] = (6.0, 6.0)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    plt.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)
    plt.colorbar()
    tick_marks = np.arange(len(labels_str))
    plt.xticks(tick_marks, labels_str, rotation=90)
    plt.yticks(tick_marks, labels_str)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

plot_normalised_confusion_matrix(confusion_matrix(y_test, clf.predict(X_test)), )
Out:</pre></div><div><img src="img/00140.jpeg" alt="A ranking problem" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">It looks pretty <a id="id656" class="calibre1"/>diagonal, except for the -2 class (where we have indeed very few samples). Overall, we consider a ranking loss lower than 0.1 to be an excellent result.</p></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec54" class="calibre1"/>A time series problem</h1></div></div></div><p class="calibre8">The last <a id="id657" class="calibre1"/>problem we're going to see in this chapter is about prediction in time. The standard name for these problems is time series analysis, since the prediction is made on descriptors extracted in the past; therefore, the outcome at the current time will become a feature for the prediction of the next point in time. In this exercise, we're using the closing values for several stocks composing the Dow Jones index in 2011.</p><p class="calibre8">Several features compose the dataset, but in this problem (to make a short and complete exercise) we're just using the closing values of each week for each of the 30 measured stocks, ordered in time. The dataset spans six months: we're using the first half of the dataset (corresponding to the first quarter of the year under observation, with 12 weeks) to train our algorithm, and the second half (containing the second quarter of the year, with 13 weeks) to test the predictions.</p><p class="calibre8">Moreover, since we don't expect readers to have a background in economics, we've tried to make things as simple as possible. In a real-life situation, such a prediction will be too simple to get money out of the market, but in this short example we've tried to keep the focus on the time series analysis, dropping all the other inputs and sources.</p><div><h3 class="title2"><a id="note07" class="calibre1"/>Note</h3><p class="calibre8">The full description of this problem can be found at: <a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/Dow+Jones+Index">https://archive.ics.uci.edu/ml/datasets/Dow+Jones+Index</a>.</p></div><p class="calibre8">According to the readme file distributed along with the dataset, there are no missing values; therefore, the loading operation is quite straightforward:</p><div><pre class="programlisting">In:
import matplotlib.pyplot as plt
%matplotlib inline  

import numpy as np
import pandas as pd

dataset = pd.read_csv('./dji/dow_jones_index.data')</pre></div><p class="calibre8">Let's now try to decode the rows we're interested in (stock and close): it seems that the closing values are all strings, starting with <code class="email">$</code> and followed by the floating point value relative to the closing price. We should then select the correct columns and cast the closing price to the right data type:</p><div><pre class="programlisting">In:
print(dataset.head())
Out:
   quarter stock       date    open    high     low   close     volume  \
0        1    AA   1/7/2011  $15.82  $16.72  $15.78  $16.42  239655616   
1        1    AA  1/14/2011  $16.71  $16.71  $15.64  $15.97  242963398   
2        1    AA  1/21/2011  $16.19  $16.38  $15.60  $15.79  138428495   
3        1    AA  1/28/2011  $15.87  $16.63  $15.82  $16.13  151379173   
4        1    AA   2/4/2011  $16.18  $17.39  $16.18  $17.14  154387761   

   percent_change_price  percent_change_volume_over_last_wk  \
0               3.79267                                 NaN   
1              -4.42849                            1.380223   
2              -2.47066                          -43.024959   
3               1.63831                            9.355500   
4               5.93325                            1.987452   

   previous_weeks_volume next_weeks_open next_weeks_close  \
0                    NaN          $16.71           $15.97   
1              239655616          $16.19           $15.79   
2              242963398          $15.87           $16.13   
3              138428495          $16.18           $17.14   
4              151379173          $17.33           $17.37   

   percent_change_next_weeks_price  days_to_next_dividend  \
0                        -4.428490                     26   
1                        -2.470660                     19   
2                         1.638310                     12   
3                         5.933250                      5   
4                         0.230814                     97   

   percent_return_next_dividend  
0                      0.182704  
1                      0.187852  
2                      0.189994  
3                      0.185989  
4                      0.175029  
In:
observations = {}

for el in dataset[['stock', 'close']].iterrows():
    
    stock = el[1].stock
    close = float(el[1].close.replace("$", ""))
    
    try:
        observations[stock].append(close)
    except KeyError:
        observations[stock] = [close]</pre></div><p class="calibre8">Let's now create a feature vector for each stock. In the simplest instance, it's just a row containing the sorted closing prices for the 25 weeks:</p><div><pre class="programlisting">In:
X = []
stock_names = sorted(observations.keys())

for stock in stock_names:
    X.append(observations[stock])

X = np.array(X)</pre></div><p class="calibre8">Let's now <a id="id658" class="calibre1"/>build a baseline: we can try the regressor on the first 12 weeks, and then test it by recursively offsetting the data—that is, to predict the 13th week, we use the first 12 weeks; to predict the value at the 14th week, we use 12 weeks ending with the 13th one. And so on.</p><p class="calibre8">Note that, in this very simple approach, we build just a classifier for all the stocks, independently by their price, and we use both R<sup class="calibre21">2</sup> and MAE to score our learner for each week of analysis (a score for the 13<sup class="calibre21">th</sup> week, one for the 14<sup class="calibre21">th</sup>, and so on). Finally, we compute mean and variance of these scores:</p><div><pre class="programlisting">In:
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

X_train = X[:, :12]
y_train = X[:, 12]

regr_1 = LinearRegression()
regr_1.fit(X_train, y_train)
In:
plot_vals = []

for offset in range(0, X.shape[1]-X_train.shape[1]):
    X_test = X[:, offset:12+offset]
    y_test = X[:, 12+offset]
    
    r2 = r2_score(y_test, regr_1.predict(X_test))
    mae = mean_absolute_error(y_test, regr_1.predict(X_test))
    
    print("offset=", offset, "r2_score=", r2)
    print("offset=", offset, "MAE     =", mae)
    
    plot_vals.append( (offset, r2, mae) )

print()
print("r2_score: mean=", np.mean([x[1] for x in plot_vals]), "variance=", np.var([x[1] for x in plot_vals]))
print("mae_score: mean=", np.mean([x[2] for x in plot_vals]), "variance=", np.var([x[2] for x in plot_vals]))
Out:
offset= 0 r2_score= 0.999813479679
offset= 0 MAE     = 0.384145971072
offset= 1 r2_score= 0.99504246854
offset= 1 MAE     = 1.602203752
offset= 2 r2_score= 0.995188278161
offset= 2 MAE     = 1.76248455475
offset= 3 r2_score= 0.998287091734
offset= 3 MAE     = 1.15856848271
offset= 4 r2_score= 0.997938802118
offset= 4 MAE     = 1.11955148717
offset= 5 r2_score= 0.985036566148
offset= 5 MAE     = 2.94239117688
offset= 6 r2_score= 0.991598279578
offset= 6 MAE     = 2.35632383083
offset= 7 r2_score= 0.995485519307
offset= 7 MAE     = 1.73191962456
offset= 8 r2_score= 0.992872581249
offset= 8 MAE     = 1.9828644662
offset= 9 r2_score= 0.990012202362
offset= 9 MAE     = 2.66825249081
offset= 10 r2_score= 0.996984329367
offset= 10 MAE     = 1.38682132207
offset= 11 r2_score= 0.999029861989
offset= 11 MAE     = 0.761720947323
offset= 12 r2_score= 0.996280599178
offset= 12 MAE     = 1.53124828142

r2_score: mean= 0.99489000457 variance= 1.5753065199e-05
mae_score: mean= 1.64526895291 variance= 0.487371842069</pre></div><p class="calibre8">On the 13 testing <a id="id659" class="calibre1"/>weeks, the R<sup class="calibre21">2</sup> is 0.99 on average (with a variance of 0.0000157) and the MAE is 1.64 on average (with a variance of 0.48). That's the baseline; let's plot it:</p><div><pre class="programlisting">In:
fig, ax1 = plt.subplots()
ax1.plot([x[0] for x in plot_vals], [x[1] for x in plot_vals], 'b-')
ax1.plot(plot_vals[0][0], plot_vals[0][1], 'bo')

ax1.set_xlabel('test week')
# Make the y-axis label and tick labels match the line color.
ax1.set_ylabel('r2_score', color='b')
for tl in ax1.get_yticklabels():
    tl.set_color('b')
ax1.set_ylim([0.9, 1.1])


ax2 = ax1.twinx()
ax2.plot([x[0] for x in plot_vals], [x[2] for x in plot_vals], 'r-')
ax2.plot(plot_vals[0][0], plot_vals[0][2], 'ro')
ax2.set_ylabel('mae score', color='r')
for tl in ax2.get_yticklabels():
    tl.set_color('r')
ax2.set_ylim([0, 3.3])

plt.xlim([-.1, 12.1])

plt.show()
Out:</pre></div><div><img src="img/00141.jpeg" alt="A time series problem" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Are we sure that the<a id="id660" class="calibre1"/> value of 12 weeks ago is still a good predictor for the current week? Let's now try to improve our scores by decreasing the training weeks. As an additional advantage, we will also have more training data. Let's try <a id="id661" class="calibre1"/>using <code class="email">5</code> (a little more than a month):</p><div><pre class="programlisting">In:
training_len = 5

X_train_short = X[:, :training_len]
y_train_short = X[:, training_len]


for offset in range(1, 12-training_len):
    X_train_short = np.vstack( (X_train_short, X[:, offset:training_len+offset]) )
    y_train_short = np.concatenate( (y_train_short, X[:, training_len+offset]) )    
In:
regr_2 = LinearRegression()
regr_2.fit(X_train_short, y_train_short)
In:
plot_vals = []

for offset in range(0, X.shape[1]-X_train.shape[1]):
    X_test = X[:, 12-training_len+offset:12+offset]
    y_test = X[:, 12+offset]
    
    r2 = r2_score(y_test, regr_2.predict(X_test))
    mae = mean_absolute_error(y_test, regr_2.predict(X_test))
    
    print("offset=", offset, "r2_score=", r2)
    print("offset=", offset, "MAE     =", mae)
    
    plot_vals.append( (offset, r2, mae) )

print()
print("r2_score: mean=", np.mean([x[1] for x in plot_vals]), "variance=", np.var([x[1] for x in plot_vals]))
print("mae_score: mean=", np.mean([x[2] for x in plot_vals]), "variance=", np.var([x[2] for x in plot_vals]))
Out:
offset= 0 r2_score= 0.998579501272
offset= 0 MAE     = 0.85687189133
offset= 1 r2_score= 0.999412004606
offset= 1 MAE     = 0.552138850961
offset= 2 r2_score= 0.998668959234
offset= 2 MAE     = 0.941052814674
offset= 3 r2_score= 0.998291291965
offset= 3 MAE     = 1.03476245234
offset= 4 r2_score= 0.997006831124
offset= 4 MAE     = 1.45857426198
offset= 5 r2_score= 0.996849578723
offset= 5 MAE     = 1.04394939395
offset= 6 r2_score= 0.998134003499
offset= 6 MAE     = 1.05938998285
offset= 7 r2_score= 0.998391605331
offset= 7 MAE     = 0.865007491822
offset= 8 r2_score= 0.999317752361
offset= 8 MAE     = 0.607975744054
offset= 9 r2_score= 0.996058731277
offset= 9 MAE     = 1.62548930127
offset= 10 r2_score= 0.997319345983
offset= 10 MAE     = 1.2305378204
offset= 11 r2_score= 0.999264102166
offset= 11 MAE     = 0.649407612032
offset= 12 r2_score= 0.998227164258
offset= 12 MAE     = 1.020568135

r2_score: mean= 0.998116990138 variance= 9.8330905525e-07
mae_score: mean= 0.995825057897 variance= 0.0908384278533</pre></div><p class="calibre8">With this <a id="id662" class="calibre1"/>approach, both R<sup class="calibre21">2</sup> and MAE have improved, on average, and their variance is perceptibly lower:</p><div><pre class="programlisting">In:
fig, ax1 = plt.subplots()
ax1.plot([x[0] for x in plot_vals], [x[1] for x in plot_vals], 'b-')
ax1.plot(plot_vals[0][0], plot_vals[0][1], 'bo')

ax1.set_xlabel('test week')
# Make the y-axis label and tick labels match the line color.
ax1.set_ylabel('r2_score', color='b')
for tl in ax1.get_yticklabels():
    tl.set_color('b')
ax1.set_ylim([0.95, 1.05])


ax2 = ax1.twinx()
ax2.plot([x[0] for x in plot_vals], [x[2] for x in plot_vals], 'r-')
ax2.plot(plot_vals[0][0], plot_vals[0][2], 'ro')
ax2.set_ylabel('mae score', color='r')
for tl in ax2.get_yticklabels():
    tl.set_color('r')
ax2.set_ylim([0, 2.2])

plt.xlim([-.1, 12.1])

plt.show()
Out:</pre></div><div><img src="img/00142.jpeg" alt="A time series problem" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Since the <a id="id663" class="calibre1"/>approach seems to be working better, let's now try to grid-search the best training length, spanning from 1 to 12:</p><div><pre class="programlisting">In:
training_lens = range(1,13)
models = {}


for training_len in training_lens:
    X_train_short = X[:, :training_len]
    y_train_short = X[:, training_len]

    for offset in range(1, 12-training_len):
        X_train_short = np.vstack( (X_train_short, X[:, offset:training_len+offset]) )
        y_train_short = np.concatenate( (y_train_short, X[:, training_len+offset]) )

    regr_x = LinearRegression()
    regr_x.fit(X_train_short, y_train_short)
    models[training_len] = regr_x

    plot_vals = []

    for offset in range(0, X.shape[1]-X_train.shape[1]):
        X_test = X[:, 12-training_len+offset:12+offset]
        y_test = X[:, 12+offset]

        r2 = r2_score(y_test, regr_x.predict(X_test))
        mae = mean_absolute_error(y_test, regr_x.predict(X_test))

        plot_vals.append( (offset, r2, mae) )

    fig, ax1 = plt.subplots()
    ax1.plot([x[0] for x in plot_vals], [x[1] for x in plot_vals], 'b-')
    ax1.plot(plot_vals[0][0], plot_vals[0][1], 'bo')

    ax1.set_xlabel('test week')
    # Make the y-axis label and tick labels match the line color.
    ax1.set_ylabel('r2_score', color='b')
    for tl in ax1.get_yticklabels():
        tl.set_color('b')
    ax1.set_ylim([0.95, 1.05])


    ax2 = ax1.twinx()
    ax2.plot([x[0] for x in plot_vals], [x[2] for x in plot_vals], 'r-')
    ax2.plot(plot_vals[0][0], plot_vals[0][2], 'ro')
    ax2.set_ylabel('mae score', color='r')
    for tl in ax2.get_yticklabels():
        tl.set_color('r')
    ax2.set_ylim([0, max([2.2, 1.1*np.max([x[2] for x in plot_vals])])])

    plt.xlim([-.1, 12.1])
    
    plt.title("results with training_len={}".format(training_len))

    plt.show()

    print("r2_score: mean=", np.mean([x[1] for x in plot_vals]), "variance=", np.var([x[1] for x in plot_vals]))
    print("mae_score: mean=", np.mean([x[2] for x in plot_vals]), "variance=", np.var([x[2] for x in plot_vals]))

Out:
... [images are omitted] ...
results with training_len=1
r2_score: mean= 0.998224065712 variance= 1.00685934679e-06
mae_score: mean= 0.95962574798 variance= 0.0663013566722

results with training_len=2
r2_score: mean= 0.998198628321 variance= 9.17757825917e-07
mae_score: mean= 0.969741651259 variance= 0.0661101843822

results with training_len=3
r2_score: mean= 0.998223327997 variance= 8.57207677825e-07
mae_score: mean= 0.969261583196 variance= 0.0715715354908

results with training_len=4
r2_score: mean= 0.998223602314 variance= 7.91949263056e-07
mae_score: mean= 0.972853132744 variance= 0.0737436496017

results with training_len=5
r2_score: mean= 0.998116990138 variance= 9.8330905525e-07
mae_score: mean= 0.995825057897 variance= 0.0908384278533

results with training_len=6
r2_score: mean= 0.997953763986 variance= 1.14333232014e-06
mae_score: mean= 1.04107069762 variance= 0.100961792252

results with training_len=7
r2_score: mean= 0.997481850128 variance= 1.85277659214e-06
mae_score: mean= 1.19114613181 variance= 0.121982635728

results with training_len=8
r2_score: mean= 0.99715522262 variance= 3.27488548806e-06
mae_score: mean= 1.23998671525 variance= 0.173529737205

results with training_len=9
r2_score: mean= 0.995975415477 variance= 5.76973840581e-06
mae_score: mean= 1.48200981286 variance= 0.22134177338

results with training_len=10
r2_score: mean= 0.995828230003 variance= 4.92217626753e-06
mae_score: mean= 1.51007677609 variance= 0.209938740518

results with training_len=11
r2_score: mean= 0.994520917305 variance= 7.24129427869e-06
mae_score: mean= 1.78424593989 variance= 0.213259808552

results with training_len=12
r2_score: mean= 0.99489000457 variance= 1.5753065199e-05
mae_score: mean= 1.64526895291 variance= 0.487371842069</pre></div><p class="calibre8">The best <a id="id664" class="calibre1"/>trade-off is with <code class="email">training_len=3</code>.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec97" class="calibre1"/>Open questions</h2></div></div></div><p class="calibre8">As you've seen, in this example we didn't normalize the data, using stocks with high and low prices together. This fact may confuse the learner, since the observations don't have the same center. With a bit of preprocessing, we may obtain better results, applying a per-stock normalization. Can you think what else could we do, and how could we test the algorithm?</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec55" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we've explored four practical data science examples involving classifiers and regressors. We strongly encourage readers to read, understand, and try to add further steps, in order to boost performance.</p></div></body></html>