- en: Cluster Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Quickly bring me a beaker of wine, so that I may wet my mind and say something
    clever."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Aristophanes, Athenian Playwright'
  prefs: []
  type: TYPE_NORMAL
- en: In the earlier chapters, we focused on trying to learn the best algorithm in
    order to solve an outcome or response, for example, a breast cancer diagnosis
    or level of Prostate Specific Antigen. In all these cases, we had *y,* and that
    *y* is a function of *x,* or *y = f(x)*. In our data, we had the actual *y* values
    and we could train the *x* accordingly. This is referred to as **supervised learning**.
    However, there are many situations where we try to learn something from our data
    and either we do not have the *y* or we actually choose to ignore it. If so, we
    enter the world of **unsupervised learning**. In this world, we build and select
    our algorithm based on how well it addresses our business needs versus how accurate
    it is.
  prefs: []
  type: TYPE_NORMAL
- en: Why would we try and learn without supervision? First of all, unsupervised learning
    can help you understand and identify patterns in your data, which may be valuable.
    Second, you can use it to transform your data in order to improve your supervised
    learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will focus on the former and the next chapter on the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s begin by tackling a popular and powerful technique known as **cluster
    analysis**. With cluster analysis, the goal is to group the observations into
    a number of groups (k-groups), where the members in a group are as similar as
    possible while the members between groups are as different as possible. There
    are many examples of how this can help an organization; here are just a few:'
  prefs: []
  type: TYPE_NORMAL
- en: The creation of customer types or segments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The detection of high-crime areas in a geography
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image and facial recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genetic sequencing and transcription
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petroleum and geological exploration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are many uses of cluster analysis but there are also many techniques.
    We will focus on the two most common: **hierarchical** and **k-means**. They are
    both effective clustering methods, but may not always be appropriate for the large
    and varied datasets that you may be called upon to analyze. Therefore, we will
    also examine **Partitioning Around Medoids** (**PAM**) using a **Gower-based**
    metric dissimilarity matrix as the input.  Finally, we will examine a new methodology
    I recently learned and applied using **Random Forest** to transform your data.
     The transformed data can then be used as an input to unsupervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A final comment before moving on. You may be asked if these techniques are
    more art than science as the learning is unsupervised. I think the clear answer
    is, *it depends*. In early 2016, I presented the methods here at a meeting of
    the Indianapolis, Indiana R-User Group. To a person, we all agreed that it is
    the judgment of the analysts and the business users that makes unsupervised learning
    meaningful and determines whether you have, say, three versus four clusters in
    your final algorithm.  This quote sums it up nicely:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The major obstacle is the difficulty in evaluating a clustering algorithm
    without taking into account the context: why does the user cluster his data in
    the first place, and what does he want to do with the clustering afterwards? We
    argue that clustering should not be treated as an application-independent mathematical
    problem, but should always be studied in the context of its end-use."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Luxburg et al. (2012)'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The hierarchical clustering algorithm is based on a dissimilarity measure between
    observations. A common measure, and what we will use, is **Euclidean distance**.
    Other distance measures are also available.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering is an agglomerative or bottom-up technique. By this,
    we mean that all observations are their own cluster. From there, the algorithm
    proceeds iteratively by searching all the pairwise points and finding the two
    clusters that are the most similar. So, after the first iteration, there are *n-1*
    clusters, and after the second iteration, there are *n-2* clusters, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: As the iterations continue, it is important to understand that in addition to
    the distance measure, we need to specify the linkage between the groups of observations.
    Different types of data will demand that you use different cluster linkages. As
    you experiment with the linkages, you may find that some may create highly unbalanced
    numbers of observations in one or more clusters. For example, if you have 30 observations,
    one technique may create a cluster of just one observation, regardless of how
    many total clusters that you specify. In this situation, your judgment will likely
    be needed to select the most appropriate linkage as it relates to the data and
    business case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists the types of common linkages, but note that there
    are others:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Linkage** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Ward | This minimizes the total within-cluster variance as measured by the
    sum of squared errors from the cluster points to its centroid |'
  prefs: []
  type: TYPE_TB
- en: '| Complete | The distance between two clusters is the maximum distance between
    an observation in one cluster and an observation in the other cluster |'
  prefs: []
  type: TYPE_TB
- en: '| Single | The distance between two clusters is the minimum distance between
    an observation in one cluster and an observation in the other cluster |'
  prefs: []
  type: TYPE_TB
- en: '| Average | The distance between two clusters is the mean distance between
    an observation in one cluster and an observation in the other cluster |'
  prefs: []
  type: TYPE_TB
- en: '| Centroid | The distance between two clusters is the distance between the
    cluster centroids |'
  prefs: []
  type: TYPE_TB
- en: The output of hierarchical clustering will be a **dendrogram**, which is a tree-like
    diagram that shows the arrangement of the various clusters.
  prefs: []
  type: TYPE_NORMAL
- en: As we will see, it can often be difficult to identify a clear-cut breakpoint
    in the selection of the number of clusters. Once again, your decision should be
    iterative in nature and focused on the context of the business decision.
  prefs: []
  type: TYPE_NORMAL
- en: Distance calculations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, Euclidean distance is commonly used to build the input
    for hierarchical clustering. Let's look at a simple example of how to calculate
    it with two observations and two variables/features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that observation *A* costs $5.00 and weighs 3 pounds. Further, observation
    *B* costs $3.00 and weighs 5 pounds. We can place these values in the distance
    formula: *distance between A and B is equal to the square root of the sum of the
    squared differences*, which in our example would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*d(A, B) = square root((5 - 3)² + (3 - 5)²) *, which is equal to *2.83*'
  prefs: []
  type: TYPE_NORMAL
- en: The value of 2.83 is not a meaningful value in and of itself, but is important
    in the context of the other pairwise distances. This calculation is the default
    in R for the `dist()` function. You can specify other distance calculations (maximum,
    manhattan, canberra, binary, and minkowski) in the function. We will avoid going
    in to detail on why or where you would choose these over Euclidean distance. This
    can get rather domain-specific, for example, a situation where Euclidean distance
    may be inadequate is where your data suffers from high-dimensionality, such as
    in a genomic study. It will take domain knowledge and/or trial and error on your
    part to determine the proper distance measure.
  prefs: []
  type: TYPE_NORMAL
- en: One final note is to scale your data with a mean of zero and standard deviation
    of one so that the distance calculations are comparable. If not, any variable
    with a larger scale will have a larger effect on distances.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With k-means, we will need to specify the exact number of clusters that we want.
    The algorithm will then iterate until each observation belongs to just one of
    the k-clusters. The algorithm's goal is to minimize the within-cluster variation
    as defined by the squared Euclidean distances. So, the kth-cluster variation is
    the sum of the squared Euclidean distances for all the pairwise observations divided
    by the number of observations in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the iteration process that is involved, one k-means result can differ
    greatly from another result even if you specify the same number of clusters. Let''s
    see how this algorithm plays out:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Specify** the exact number of clusters you desire (k).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Initialize** K observations are randomly selected as the initial *means.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterate**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K clusters are created by assigning each observation to its closest cluster
    center (minimizing within-cluster sum of squares)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The centroid of each cluster becomes the new *mean*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is repeated until convergence, that is the cluster centroids do not change
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the final result will vary because of the initial assignment
    in step 1\. Therefore, it is important to run multiple initial starts and let
    the software identify the best solution. In R, this can be a simple process as
    we will see.
  prefs: []
  type: TYPE_NORMAL
- en: Gower and partitioning around medoids
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you conduct clustering analysis in real life, one of the things that can
    quickly become apparent is the fact that neither hierarchical nor k-means is specifically
    designed to handle mixed datasets. By mixed data, I mean both quantitative and
    qualitative or, more specifically, nominal, ordinal, and interval/ratio data.
  prefs: []
  type: TYPE_NORMAL
- en: The reality of most datasets that you will use is that they will probably contain
    mixed data. There are a number of ways to handle this, such as doing **Principal
    Components Analysis** (**PCA**) first in order to create latent variables, then
    using them as input in clustering or using different dissimilarity calculations.
    We will discuss PCA in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: With the power and simplicity of R, you can use the **Gower** **dissimilarity
    coefficient** to turn mixed data to the proper feature space. In this method,
    you can even include factors as input variables. Additionally, instead of k-means,
    I recommend using the **PAM clustering algorithm**.
  prefs: []
  type: TYPE_NORMAL
- en: 'PAM is very similar to k-means but offers a couple of advantages. They are
    listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, PAM accepts a dissimilarity matrix, which allows the inclusion of mixed
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, it is more robust to outliers and skewed data because it minimizes a
    sum of dissimilarities instead of a sum of squared Euclidean distances (Reynolds,
    1992)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is not to say that you must use Gower and PAM together. If you choose,
    you can use the Gower coefficients with hierarchical and I've seen arguments for
    and against using it in the context of k-means. Additionally, PAM can accept other
    linkages. However, when paired they make an effective method to handle mixed data.
    Let's take a quick look at both of these concepts before moving on.
  prefs: []
  type: TYPE_NORMAL
- en: Gower
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Gower coefficient compares cases pairwise and calculates a dissimilarity
    between them, which is essentially the weighted mean of the contributions of each
    variable. It is defined for two cases called *i* and *j* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06473_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *S[ijk]* is the contribution provided by the *k*[th] variable, and *W[ijk]*
    is 1 if the *k*[th] variable is valid, or else *0*.
  prefs: []
  type: TYPE_NORMAL
- en: For ordinal and continuous variables, *S[ijk] = 1 - (absolute value of x[ij]
    - x[ik]) / r[k]*, where *r[k]* is the range of values for the *k*[th] variable.
  prefs: []
  type: TYPE_NORMAL
- en: For nominal variables, *S[ijk] = 1* if *x[ij] = x[jk]*, or else *0*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For binary variables, *S[ijk]* is calculated based on whether an attribute
    is present (+) or not present (-), as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Variables** | **Value of attribute *k*** |'
  prefs: []
  type: TYPE_TB
- en: '| Case *i* | + | + | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Case *j* | + | - | + | - |'
  prefs: []
  type: TYPE_TB
- en: '| Sijk | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Wijk | 1 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: PAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For **Partitioning Around Medoids**, let's first define a **medoid**.
  prefs: []
  type: TYPE_NORMAL
- en: A medoid is an observation of a cluster that minimizes the dissimilarity (in
    our case, calculated using the Gower metric) between the other observations in
    that cluster. So, similar to k-means, if you specify five clusters, you will have
    five partitions of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the objective of minimizing the dissimilarity of all the observations
    to the nearest medoid, the PAM algorithm iterates over the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly select k observations as the initial medoid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign each observation to the closest medoid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Swap each medoid and non-medoid observation, computing the dissimilarity cost.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the configuration that minimizes the total dissimilarity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2 through 4 until there is no change in the medoids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both Gower and PAM can be called using the `cluster` package in R. For Gower,
    we will use the `daisy()` function in order to calculate the dissimilarity matrix
    and the `pam()` function for the actual partitioning. With this, let's get started
    with putting these methods to the test.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like our motivation with the use of the Gower metric in handling mixed, in
    fact, *messy* data, we can apply random forest in an unsupervised fashion.  Selection
    of this method has some advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Robust against outliers and highly skewed variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No need to transform or scale the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handles mixed data (numeric and factors)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can accommodate missing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used on data with a large number of variables, in fact, it can be used
    to eliminate useless features by examining variable importance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dissimilarity matrix produced serves as an input to the other techniques
    discussed earlier (hierarchical, k-means, and PAM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A couple words of caution.  It may take some trial and error to properly tune
    the Random Forest with respect to the number of variables sampled at each tree
    split (`mtry = ?` in the function) and the number of trees grown.  Studies done
    show that the more trees grown, up to a point, provide better results, and a good
    starting point is to grow 2,000 trees (Shi, T. & Horvath, S., 2006).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how the algorithm works, given a data set with no labels:'
  prefs: []
  type: TYPE_NORMAL
- en: The current observed data is labeled as class 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A second (synthetic) set of observations are created of the same size as the
    observed data; this is created by randomly sampling from each of the features
    from the observed data, so if you have 20 observed features, you will have 20
    synthetic features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The synthetic portion of the data is labeled as class 2, which facilitates using
    Random Forest as an artificial classification problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a Random Forest model to distinguish between the two classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turn the model's proximity measures of just the observed data (the synthetic
    data is now discarded) into a dissimilarity matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize the dissimilarity matrix as the clustering input features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So what exactly are these proximity measures?
  prefs: []
  type: TYPE_NORMAL
- en: Proximity measure is a pairwise measure between all the observations. If two
    observations end up in the same terminal node of a tree, their proximity score
    is equal to one, otherwise zero.
  prefs: []
  type: TYPE_NORMAL
- en: At the termination of the Random Forest run, the proximity scores for the observed
    data are normalized by dividing by the total number of trees.  The resulting NxN
    matrix contains scores between zero and one, naturally with the diagonal values
    all being one.  That's all there is to it.  An effective technique that I believe
    is underutilized and one that I wish I had learned years ago.
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until a couple of weeks ago, I was unaware that there were less than 300 certified
    Master Sommeliers in the entire world. The exam, administered by the Court of
    Master Sommeliers, is notorious for its demands and high failure rate.
  prefs: []
  type: TYPE_NORMAL
- en: The trials, tribulations, and rewards of several individuals pursuing the certification
    are detailed in the critically-acclaimed documentary, **Somm**. So, for this exercise,
    we will try and help a hypothetical individual struggling to become a Master Sommelier
    find a latent structure in Italian wines.
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding and preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with loading the R packages that we will need for this chapter.
    As always, make sure that you have installed them first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset is in the `HDclassif` package, which we installed. So, we can load
    the data and examine the structure with the `str()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The data consists of `178` wines with 13 variables of the chemical composition
    and one variable `Class`, the label, for the cultivar or plant variety. We won''t
    use this in the clustering but as a test of model performance. The variables,
    `V1` through `V13`, are the measures of the chemical composition as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`V1`: alcohol'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V2`: malic acid'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V3`: ash'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V4`: alkalinity of ash'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V5`: magnesium'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V6`: total phenols'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V7`: flavonoids'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V8`: non-flavonoid phenols'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V9`: proanthocyanins'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V10`: color intensity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V11`: hue'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V12`: OD280/OD315'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V13`: proline'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The variables are all quantitative. We should rename them to something meaningful
    for our analysis. This is easily done with the `names()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As the variables are not scaled, we will need to do this using the `scale()`
    function. This will first center the data where the column mean is subtracted
    from each individual in the column. Then the centered values will be divided by
    the corresponding column''s standard deviation. We can also use this transformation
    to make sure that we only include columns 2 through 14, dropping class and putting
    it in a data frame. This can all be done with one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, check the structure to make sure that it all worked according to plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Before moving on, let''s do a quick table to see the distribution of the cultivars
    or `Class`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can now move on to the modeling step of the process.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having created our data frame, `df`, we can begin to develop the clustering
    algorithms. We will start with hierarchical and then try our hand at k-means.
    After this, we will need to manipulate our data a little bit to demonstrate how
    to incorporate mixed data with Gower and Random Forest.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build a hierarchical cluster model in R, you can utilize the `hclust()` function
    in the base `stats` package. The two primary inputs needed for the function are
    a distance matrix and the clustering method. The distance matrix is easily done
    with the `dist()` function. For the distance, we will use Euclidean distance.
    A number of clustering methods are available, and the default for `hclust()` is
    the complete linkage.
  prefs: []
  type: TYPE_NORMAL
- en: We will try this, but I also recommend Ward's linkage method. Ward's method
    tends to produce clusters with a similar number of observations.
  prefs: []
  type: TYPE_NORMAL
- en: The complete linkage method results in the distance between any two clusters
    that is the maximum distance between any one observation in a cluster and any
    one observation in the other cluster. Ward's linkage method seeks to cluster the
    observations in order to minimize the within-cluster sum of squares.
  prefs: []
  type: TYPE_NORMAL
- en: It is noteworthy that the R method `ward.D2` uses the squared Euclidean distance,
    which is indeed Ward's linkage method. In R, `ward.D` is available but requires
    your distance matrix to be squared values. As we will be building a distance matrix
    of non-squared values, we will require `ward.D2`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the big question is how many clusters should we create? As stated in the
    introduction, the short, and probably not very satisfying answer is that it depends.
    Even though there are cluster validity measures to help with this dilemma--which
    we will look at--it really requires an intimate knowledge of the business context,
    underlying data, and, quite frankly, trial and error. As our sommelier partner
    is fictional, we will have to rely on the validity measures. However, that is
    no panacea to selecting the numbers of clusters as there are several dozen validity
    measures.
  prefs: []
  type: TYPE_NORMAL
- en: As exploring the positives and negatives of the vast array of cluster validity
    measures is way outside the scope of this chapter, we can turn to a couple of
    papers and even R itself to simplify this problem for us. A paper by Miligan and
    Cooper, 1985, explored the performance of 30 different measures/indices on simulated
    data. The top five performers were CH index, Duda Index, Cindex, Gamma, and Beale
    Index. Another well-known method to determine the number of clusters is the **gap
    statistic** (Tibshirani, Walther, and Hastie, 2001). These are two good papers
    for you to explore if your cluster validity curiosity gets the better of you.
  prefs: []
  type: TYPE_NORMAL
- en: 'With R, one can use the `NbClust()` function in the `NbClust` package to pull
    results on 23 indices, including the top five from Miligan and Cooper and the
    gap statistic. You can see a list of all the available indices in the help file
    for the package. There are two ways to approach this process: one is to pick your
    favorite index or indices and call them with R, the other way is to include all
    of them in the analysis and go with the majority rules method, which the function
    summarizes for you nicely. The function will also produce a couple of plots as
    well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the stage set, let''s walk through the example of using the complete linkage
    method. When using the function, you will need to specify the minimum and maximum
    number of clusters, distance measures, and indices in addition to the linkage.
    As you can see in the following code, we will create an object called `numComplete`.
    The function specifications are for Euclidean distance, minimum number of clusters
    two, maximum number of clusters six, complete linkage, and all indices. When you
    run the command, the function will automatically produce an output similar to
    what you can see here--a discussion on both the graphical methods and majority
    rules conclusion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Going with the majority rules method, we would select three clusters as the
    optimal solution, at least for hierarchical clustering. The two plots that are
    produced contain two graphs each. As the preceding output states, you are looking
    for a significant knee in the plot (the graph on the left-hand side) and the peak
    of the graph on the right-hand side. This is the **Hubert Index** plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see that the bend or knee is at three clusters in the graph on the
    left-hand side. Additionally, the graph on the right-hand side has its peak at
    three clusters. The following **Dindex plot** provides the same information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are a number of values that you can call with the function and there
    is one that I would like to show. This output is the best number of clusters for
    each index and the index value for that corresponding number of clusters. This
    is done with `$Best.nc`. I''ve abbreviated the output to the first nine indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the first index, (`KL`), has the optimal number of clusters
    as five and the next index, (`CH`), has it as three.
  prefs: []
  type: TYPE_NORMAL
- en: 'With three clusters as the recommended selection, we will now compute the distance
    matrix and build our hierarchical cluster object. This code will build the distance
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will use this matrix as the input for the actual clustering with `hclust()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The common way to visualize hierarchical clustering is to plot a **dendrogram**.
    We will do this with the plot function. Note that `hang = -1` puts the observations
    across the bottom of the diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_08_05.png)'
  prefs: []
  type: TYPE_IMG
- en: The dendrogram is a tree diagram that shows you how the individual observations
    are clustered together. The arrangement of the connections (branches, if you will)
    tells us which observations are similar. The height of the branches indicates
    how much the observations are similar or dissimilar to each other from the distance
    matrix. Note that I specified `labels = FALSE`. This was done to aid in the interpretation
    because of the number of observations. In a smaller dataset of, say, no more than
    40 observations, the row names can be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To aid in visualizing the clusters, you can produce a colored dendrogram using
    the `sparcl` package. To color the appropriate number of clusters, you need to
    cut the dendrogram tree to the proper number of clusters using the `cutree()`
    function. This will also create the cluster label for each of the observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the `comp3` object is used in the function to build the colored dendrogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_08_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that I used `branchlength = 50`. This value will vary based on your own
    data. As we have the cluster labels, let''s build a table that shows the count
    per cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Out of curiosity, let''s go ahead and compare how this clustering algorithm
    compares to the **cultivar** labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this table, the rows are the clusters and columns are the cultivars. This
    method matched the cultivar labels at an 84 percent rate. Note that we are not
    trying to use the clusters to predict a cultivar, and in this example, we have
    no a priori reason to match clusters to the cultivars.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now try Ward''s linkage. This is the same code as before; it first
    starts with trying to identify the number of clusters, which means that we will
    need to change the method to `Ward.D2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This time around also, the majority rules were for a three cluster solution.
    Looking at the Hubert Index, the best solution is a three cluster as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Dindex adds further support to the three cluster solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s move on to the actual clustering and production of the dendrogram for
    Ward''s linkage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_08_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The plot shows three pretty distinct clusters that are roughly equal in size.
    Let''s get a count of the cluster size and show it in relation to the cultivar
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: So, cluster one has 64 observations, cluster two has `58`, and cluster three
    has 56\. This method matches the cultivar categories closer than using complete
    linkage.
  prefs: []
  type: TYPE_NORMAL
- en: 'With another table, we can compare how the two methods match observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'While cluster three for each method is pretty close, the other two are not.
    The question now is how do we identify what the differences are for the interpretation?
    In many examples, the datasets are very small and you can look at the labels for
    each cluster. In the real world, this is often impossible. A good way to compare
    is to use the `aggregate()` function, summarizing on a statistic such as the `mean`
    or median. Additionally, instead of doing it on the scaled data, let''s try it
    on the original data. In the function, you will need to specify the dataset, what
    you are aggregating it by, and the summary statistic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the `mean` by the cluster for each of the 13 variables in the
    data. With complete linkage done, let''s give Ward a try:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The numbers are very close. The cluster one for Ward's method does have slightly
    higher values for all the variables. For cluster two of Ward's method, the mean
    values are smaller except for Hue. This would be something to share with someone
    who has the domain expertise to assist in the interpretation. We can help this
    effort by plotting the values for the variables by the cluster for the two methods.
  prefs: []
  type: TYPE_NORMAL
- en: A nice plot to compare distributions is the **boxplot**. The boxplot will show
    us the minimum, first quartile, median, third quartile, maximum, and potential
    outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build a comparison plot with two boxplot graphs with the assumption
    that we are curious about the `Proline` values for each clustering method. The
    first thing to do is to prepare our plot area in order to display the graphs side
    by side. This is done with the `par()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we specified that we wanted one row and two columns with `mfrow = c(1,
    2))`. If you want it as two rows and one column, then it would have been `mfrow
    = c(2, 1))`. In the `boxplot()` function, we will need to specify that the *y*
    axis values are a function of the *x* axis values with the tilde `~` symbol:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_08_10.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the boxplot, the thick boxes represent the first quartile, median
    (the thick horizontal line in the box), and the third quartile, which is the **interquartile
    range**. The ends of the dotted lines, commonly referred to as **whiskers** represent
    the minimum and maximum values. You can see that cluster two in complete linkage
    has five small circles above the maximum. These are known as **suspected outliers**
    and are calculated as greater than plus or minus 1.5 times the interquartile range.
  prefs: []
  type: TYPE_NORMAL
- en: Any value that is greater than plus or minus three times the interquartile range
    are deemed outliers and are represented as solid black circles. For what it's
    worth, clusters one and two of Ward's linkage have tighter interquartile ranges
    with no suspected outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the boxplots for each of the variables could help you, and a domain
    expert can determine the best hierarchical clustering method to accept. With this
    in mind, let's move on to k-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we did with hierarchical clustering, we can also use `NbClust()` to determine
    the optimum number of clusters for k-means. All you need to do is specify `kmeans`
    as the method in the function. Let''s also loosen up the maximum number of clusters
    to `15`. I''ve abbreviated the following output to just the majority rules portion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, three clusters appear to be the optimum solution. Here is the Hubert
    plot, which confirms this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In R, we can use the `kmeans()` function to do this analysis. In addition to
    the input data, we have to specify the number of clusters we are solving for and
    a value for random assignments, the `nstart` argument. We will also need to specify
    a random seed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating a table of the clusters gives us a sense of the distribution of the
    observations between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of observations per cluster is well-balanced. I have seen on a number
    of occasions with larger datasets and many more variables that no number of k-means
    yields a promising and compelling result. Another way to analyze the clustering
    is to look at a matrix of the cluster centers for each variable in each cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that cluster one has, on average, a higher alcohol content. Let''s produce
    a boxplot to look at the distribution of alcohol content in the same manner as
    we did before and also compare it to Ward''s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_08_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The alcohol content for each cluster is almost exactly the same. On the surface,
    this tells me that three clusters is the proper latent structure for the wines
    and there is little difference between using k-means or hierarchical clustering.
    Finally, let''s do the comparison of the k-means clusters versus the cultivars:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This is very similar to the distribution produced by Ward's method, and either
    one would probably be acceptable to our hypothetical sommelier.
  prefs: []
  type: TYPE_NORMAL
- en: However, to demonstrate how you can cluster on data with both numeric and non-numeric
    values, let's work through some more examples.
  prefs: []
  type: TYPE_NORMAL
- en: Gower and PAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin this step, we will need to wrangle our data a little bit. As this
    method can take variables that are factors, we will convert alcohol to either
    high or low content. It also takes only one line of code utilizing the `ifelse()`
    function to change the variable to a factor. What this will accomplish is if alcohol
    is greater than zero, it will be `High`, otherwise, it will be `Low`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to create the dissimilarity matrix using the `daisy()` function
    from the `cluster` package and specifying the method as `gower`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The creation of the cluster object--let''s call it `pamFit`--is done with the
    `pam()` function, which is a part of the `cluster` package. We will create three
    clusters in this example and create a table of the cluster size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see how it does compared to the cultivar labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take this solution and build a descriptive statistics table using the
    power of the `compareGroups` package. In base R, creating presentation-worthy
    tables can be quite difficult and this package offers an excellent solution. The
    first step is to create an object of the descriptive statistics by the cluster
    with the `compareGroups()` function of the package. Then, using `createTable()`,
    we will turn the statistics to an easy-to-export table, which we will do as a
    .csv. If you want, you can also export the table as a PDF, HTML, or the LaTeX
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This table shows the proportion of the factor levels by the cluster, and for
    the numeric variables, the mean and standard deviation are displayed in parentheses.
    To export the table to a `.csv` file, just use the `export2csv()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If you open this file, you will get this table, which is conducive to further
    analysis and can be easily manipulated for presentation purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we'll create a dissimilarity matrix with Random Forest and create three
    clusters with PAM.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest and PAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform this method in R, you can use the `randomForest()` function.  After
    seeding the random seed, simply create the model object.  In the following code,
    I specify the number of trees as `2000` and set proximity measure to `TRUE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, placing a call to `rf` did not provide any meaningful output
    other than the variables sampled at each split (`mtry`).  Let''s examine the first
    five rows and first five columns of the *N x N* matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'One way to think of the values is that they are the percentage of times those
    two observations show up in the same terminal nodes! Looking at variable importance
    we see that the transformed Alcohol input could be dropped. We will keep it for
    simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'It is now just a matter of creating the dissimilarity matrix, which transforms
    the proximity values (*square root(1 - proximity)*) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have our input features, so let''s run a PAM clustering as we did earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: These results are comparable to the other techniques applied. Can you improve
    the results by tuning the Random Forest?
  prefs: []
  type: TYPE_NORMAL
- en: If you have messy data for a clustering problem, consider using Random Forest.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started exploring unsupervised learning techniques. We focused
    on cluster analysis to both provide data reduction and data understanding of the
    observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Four methods were introduced: the traditional hierarchical and k-means clustering
    algorithms, along with PAM, incorporating two different inputs (Gower and Random
    Forest). We applied these four methods to find a structure in Italian wines coming
    from three different cultivars and examined the results.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue exploring unsupervised learning, but instead
    of finding structure among the observations, we will focus on finding structure
    among the variables in order to create new features that can be used in a supervised
    learning problem.
  prefs: []
  type: TYPE_NORMAL
