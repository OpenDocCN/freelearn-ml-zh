<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Clustering</h1>
                </header>
            
            <article>
                
<p>Often, a set of data can be organized into a set of clusters. For example, you may be able to organize data into clusters that correspond to certain underlying properties (such as demographic properties including age, sex, geography, employment status, and so on) or certain underlying processes (such as browsing, shopping, bot interactions, and other such behaviors on a website). The machine learning techniques to detect and label these clusters are referred to as <strong>clustering</strong> techniques, naturally.</p>
<p class="mce-root">Up to this point, the machine learning algorithms that we have explored have been <strong>supervised</strong>. That is, we have a set of features or attributes paired with a corresponding label or number that we are trying to predict. We use this labeled data to fit our model to the behavior that we already knew about prior to training the model.</p>
<p>Most clustering techniques are <strong>unsupervised</strong>. As opposed to supervised techniques for regression and classification, we often do not know about the clusters in our dataset prior to finding them with a clustering model. Thus, we go into a clustering problem with an unlabeled dataset and an algorithm, and we generate the cluster labels for our data using the clustering algorithm.</p>
<p>Further, clustering techniques are distinguished from other machine learning techniques in that it is rather difficult to say what the <em>correct</em> or <em>accurate</em> clusters for the given dataset are. Depending on how many clusters you are looking for and the measures that you are using for similarity between data points, you might end up with a variety of sets of clusters, each having some underlying meaning. This does not mean that clustering techniques cannot be evaluated or validated, but it does mean that we need to understand our limitations and be careful when quantifying our results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding clustering model jargon</h1>
                </header>
            
            <article>
                
<p>Clustering is quite unique and comes with it's own set of terms, which are shown below. Keep in mind that the following list is only a partial list as there are many different types of clustering with corresponding jargon:</p>
<ul>
<li><strong>Clusters</strong> or <strong>groups</strong>: Each of these clusters or groups is a collection of data points into which our clustering technique organizes our data points.</li>
<li><strong>Intra</strong><strong>-group</strong> or <strong>intra-cluster</strong>: Clusters resulting from clustering can be evaluated using a measure of similarity between data points and other data points in the same resulting cluster. This is called intra-group or intra-cluster evaluation and similarity.</li>
<li><strong>Inter</strong><strong>-group</strong>or <strong>inter-cluster</strong>: Clusters resulting from clustering can be evaluated using a measure of dissimilarity between data points and other data points in other resulting clusters. This is called inter-group or inter-cluster evaluation and dissimilarity.</li>
<li><strong>Internal criteria</strong>: Often, we do not have a gold standard set of cluster labels that we can use to evaluate our resulting clusters. In these cases, we utilize inter and intra cluster similarities to measure the performance of our clustering technique.</li>
<li><strong>External criteria</strong>: In other cases, we might have a gold standard for cluster labels or grouping, such as a standard generated by human judges. These scenarios allow us to evaluate our clustering techniques using the standard, or external, criteria.</li>
<li><strong>Distance</strong> or <strong>similarity</strong>: This is a measure of how close two data points are. This could be a Euclidean distance in the space of your features or some other measure of closeness.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring Distance or Similarity</h1>
                </header>
            
            <article>
                
<p>In order to cluster data points together, we need to define and utilize some distance or similarity that quantitatively defines the closeness between data points. Choosing this measure is an essential part of every clustering project because it directly influences how the clusters are generated. Clusters resulting from the use of one similarity measure might be very different from those resulting from the use of another similarity measure.</p>
<p>The most common and simple of these distance measures is the <strong>Euclidean distance</strong> or the <strong>squared Euclidean distance</strong>. This is simply the straight line distance between two data points in your space of features (you might remember this distance as it was also used in our kNN example in <a href="f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml" target="_blank">Chapter 5</a>, <em>Classification</em>) or quantity squared. However, there are a whole host of other, sometimes more complicated, distance metrics. A few of these are shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img height="267" width="361" class="image-border" src="assets/a1ed3dd1-5ce1-4287-b5a4-64303d9af3cc.png"/></div>
<p>For example, the <strong>Manhattan</strong> distance is the absolute <em>x</em> distance plus <em>y</em> distance between the points, and the <strong>Minkowski</strong> distance generalizes between the Euclidean distance and the Manhattan distance. These distance metrics will be more robust against unusual values (or outliers) in your data, as compared to the Euclidean distance.</p>
<p>Other distance metrics, such as the <strong>Hamming</strong> distance, are applicable to certain kinds of data, such as strings. In the example shown, the Hamming distance between <strong>Golang</strong> and <strong>Gopher</strong> is four because there are four positions in the strings in which the strings are different. Thus, the Hamming distance might be a good choice of distance metric if you are working with text data, such as news articles or tweets.</p>
<p>For our purposes here, we will mostly stick to the Euclidean distance. This distance is implemented in <span><kbd>gonum.org/v1/gonum/floats</kbd> via the <kbd>Distance()</kbd> function. By way of illustration, let's say that we want to calculate the distance between a point at <kbd>(1, 2)</kbd> and a point at <kbd>(3, 4)</kbd>. We can do this as follows:</span></p>
<pre>// Calculate the Euclidean distance, specified here via
// the last argument in the Distance function.
distance := floats.Distance([]float64{1, 2}, []float64{3, 4}, 2)

fmt.Printf("\nDistance: %0.2f\n\n", distance)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating clustering techniques</h1>
                </header>
            
            <article>
                
<p>As we are not trying to predict a number or category, our previously discussed evaluation metrics for continuous and discrete variables do not really apply to clustering techniques. That does not mean that we will just avoid measuring the performance of clustering algorithms. We need to know how well our clustering is performing. We just need to introduce a few clustering-specific evaluation metrics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Internal clustering evaluation</h1>
                </header>
            
            <article>
                
<p>If we do not have a gold standard set of labels for our clusters for comparison, we are stuck with evaluating how well our clustering technique performs using internal criteria. In other words, we can still evaluate our clustering by making similarity and dissimilarity measurements within the clusters themselves.</p>
<p>The first of these internal metrics that we will present here is called the <strong>silhouette coefficient</strong>. The silhouette coefficient can be calculated for each clustered data point as follows:</p>
<div style="padding-left: 180px" class="mce-root CDPAlignLeft CDPAlign"><img height="36" width="84" src="assets/4f60ed8e-a9ef-4476-b62d-85d6e4c4bd18.jpg"/></div>
<p>Here, <em>a</em> is the mean distance between a data point and all other points in the same cluster (the Euclidean distance, for example), and <em>b</em> is the mean distance between a data point and all other points in the cluster nearest to the data point's cluster. The average of this silhouette coefficient for all data points represents how tightly packed the points are in each cluster. This average could be taken per cluster or for data points in all clusters.</p>
<p>Let's try calculating this for the iris dataset, which can be seen as a set of three clusters corresponding to each of the three iris species. First, in order to calculate the silhouette coefficient, we need to know the <strong>centroids</strong> of the three clusters. These centroids are simply the central points of the three clusters (in our four-dimensional feature space), and they will allow us to determine which cluster is the nearest to a certain data point's cluster.</p>
<p>To this end, we need to parse our iris data set file (originally introduced in <a href="4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml" target="_blank">Chapter 1</a>, <em>Gathering and Organizing Data</em>), separate our records by cluster label, average the features in each cluster<span>, and then calculate the corresponding centroids. First, we will define a <kbd>type</kbd> for our centroids:</span></p>
<pre>type centroid []float64</pre>
<p>Then, we can create a map that contains the centroid for each of our iris flowers species using <kbd>github.com/kniren/gota/dataframe</kbd>:</p>
<pre>// Pull in the CSV file.
irisFile, err := os.Open("iris.csv")
if err != nil {
    log.Fatal(err)
}
defer irisFile.Close()

// Create a dataframe from the CSV file.
irisDF := dataframe.ReadCSV(irisFile)

// Define the names of the three separate species contained in the CSV file.
speciesNames := []string{
    "Iris-setosa",
    "Iris-versicolor",
    "Iris-virginica",
}

// Create a map to hold our centroid information.
centroids := make(map[string]centroid)

// Filter the dataset into three separate dataframes,
// each corresponding to one of the Iris species.
for _, species := range speciesNames {

    // Filter the original dataset.
    filter := dataframe.F{
        Colname:    "species",
        Comparator: "==",
        Comparando: species,
    }
    filtered := irisDF.Filter(filter)

    // Calculate the mean of features.
    summaryDF := filtered.Describe()

    // Put each dimension's mean into the corresponding centroid.
    var c centroid
    for _, feature := range summaryDF.Names() {

        // Skip the irrelevant columns.
        if feature == "column" || feature == "species" {
            continue
        }
        c = append(c, summaryDF.Col(feature).Float()[0])
     }

     // Add this centroid to our map.
     centroids[species] = c
}

// As a sanity check, output our centroids.
for _, species := range speciesNames {
    fmt.Printf("%s centroid: %v\n", species, centroids[species])
}</pre>
<p>Compiling and running this gives us our centroids:</p>
<pre><strong>$ go build
$ ./myprogram 
Iris-setosa centroid: [5.005999999999999 3.4180000000000006 1.464 0.2439999999999999]
Iris-versicolor centroid: [5.936 2.7700000000000005 4.26 1.3259999999999998]
Iris-virginica centroid: [6.587999999999998 2.9739999999999998 5.552 2.026]</strong></pre>
<p>Next, we need to actually calculate the silhouette coefficients for each data point. To do this, let's modify the preceding code such that we have access to each filtered set of data points outside of the <kbd>for loop</kbd>:</p>
<pre>// Create a map to hold the filtered dataframe for each cluster.
clusters := make(map[string]dataframe.DataFrame)

// Filter the dataset into three separate dataframes,
// each corresponding to one of the Iris species.
for _, species := range speciesNames {

    ...

    // Add the filtered dataframe to the map of clusters.
    clusters[species] = filtered

    ...<br/>}</pre>
<p>Let's also create a convenience function to retrieve floats values from a row in a <kbd>dataframe.DataFrame</kbd>:</p>
<pre>// dfFloatRow retrieves a slice of float values from a DataFrame
// at the given index and for the given column names.
func dfFloatRow(df dataframe.DataFrame, names []string, idx int) []float64 {
        var row []float64
        for _, name := range names {
                row = append(row, df.Col(name).Float()[idx])
        }
        return row
}</pre>
<p>We can now loop over our records calculating the <em>a</em> and <em>b</em> that we need for the silhouette coefficients. We will also average the silhouette coefficients to get an overall evaluation metric for our clusters, as shown in the following code:</p>
<pre>// Convert our labels into a slice of strings and create a slice<br/>// of float column names for convenience.<br/>labels := irisDF.Col("species").Records()<br/>floatColumns := []string{<br/>    "sepal_length",<br/>    "sepal_width",<br/>    "petal_length",<br/>    "petal_width",<br/>}<br/><br/>// Loop over the records accumulating the average silhouette coefficient.<br/>var silhouette float64<br/><br/>for idx, label := range labels {<br/><br/>    // a will store our accumulated value for a.<br/>    var a float64<br/><br/>    // Loop over the data points in the same cluster.<br/>    for i := 0; i &lt; clusters[label].Nrow(); i++ {<br/><br/>        // Get the data point for comparison.<br/>        current := dfFloatRow(irisDF, floatColumns, idx)<br/>        other := dfFloatRow(clusters[label], floatColumns, i)<br/><br/>        // Add to a.<br/>        a += floats.Distance(current, other, 2) / float64(clusters[label].Nrow())<br/>    }<br/>    <br/>    // Determine the nearest other cluster.
    var otherCluster string
    var distanceToCluster float64
    for _, species := range speciesNames {

        // Skip the cluster containing the data point.
        if species == label {
            continue
        }

        // Calculate the distance to the cluster from the current cluster.
        distanceForThisCluster := floats.Distance(centroids[label], centroids[species], 2)

        // Replace the current cluster if relevant.
        if distanceToCluster == 0.0 || distanceForThisCluster &lt; distanceToCluster {
            otherCluster = species
            distanceToCluster = distanceForThisCluster
        }
    }

    // b will store our accumulated value for b.
    var b float64

    // Loop over the data points in the nearest other cluster.
    for i := 0; i &lt; clusters[otherCluster].Nrow(); i++ {

        // Get the data point for comparison.
        current := dfFloatRow(irisDF, floatColumns, idx)
        other := dfFloatRow(clusters[otherCluster], floatColumns, i)

        // Add to b.
        b += floats.Distance(current, other, 2) / float64(clusters[otherCluster].Nrow())
    }

    // Add to the average silhouette coefficient.
    if a &gt; b {
        silhouette += ((b - a) / a) / float64(len(labels))
    }
    silhouette += ((b - a) / b) / float64(len(labels))
}

// Output the final average silhouette coeffcient to stdout.
fmt.Printf("\nAverage Silhouette Coefficient: %0.2f\n\n", silhouette)</pre>
<p>Compiling and running this example evaluation yields the following:</p>
<pre><strong>$ go build
$ ./myprogram

Average Silhouette Coefficient: 0.51</strong></pre>
<p>How do we know if <kbd>0.51</kbd> is a good or bad average silhouette coefficient? Well, remember that the silhouette coefficient is proportional to the difference between mean intra-cluster distances and mean inter-cluster distances, and it is always going to be somewhere between <em>0.0</em> and <em>1.0</em>. Thus, higher values (those closer to <em>1.0</em>) imply closer packed clusters and are more distinct from other clusters.</p>
<p>Often, we might want to adjust our number of clusters and/or clustering technique to optimize the silhouette score (make it larger, that is). Here, we are working with data that has actually been hand-labeled, so <kbd>0.51</kbd> has to be a good score for this dataset. For other datasets, it may be higher or lower depending on the existence of clusters in the data and your choice of similarity metric.</p>
<p>The silhouette score is by no means the only way to evaluate our clusters internally. We could actually just use the quantities <em>a</em> or <em>b</em> from the silhouette score to evaluate the homogeneity of our clusters, or each clusters, dissimilarity with other clusters, respectively. Also, we could use the mean distance between points in a cluster and the centroid of the cluster to measure tightly packed clusters. Still further, we could use a variety of other evaluation metrics that will not be covered here in detail, such as the <strong>Calinski-Harabaz index</strong> (which is discussed further here: <a href="http://datamining.rutgers.edu/publication/internalmeasures.pdf">http://datamining.rutgers.edu/publication/internalmeasures.pdf</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">External clustering evaluation</h1>
                </header>
            
            <article>
                
<p>If we have a ground truth or gold standard for our clusters, then we can utilize a variety of external clustering evaluation techniques. This ground truth or gold standard means that we have access to, or can get (via manual human annotation), a set of data points where the true or desired cluster labels have been annotated.</p>
<p>Often, we do not have access to this sort of clustering gold standard, and, thus, we will not cover these sorts of evaluation techniques in detail here. However, if interested or relevant, you can look into the <strong>Adjusted Rand index</strong>, <strong>Mutual Information</strong>, <strong>Fowlkes-Mallows scores</strong>, <strong>completeness</strong>, and <strong>V-measures</strong>, which are all relevant external clustering evaluation metrics (read more details about these here: <a href="https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html">https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">k-means clustering</h1>
                </header>
            
            <article>
                
<p>The first clustering technique that we will cover here, and probably the most well-known clustering technique, is called <strong>k-means</strong> c<strong>lustering</strong>, or just <strong>k-means</strong>. k-means is an iterative method in which data points are clustered around cluster centroids that are adjusted during each iteration. The technique is relatively easy to grasp, but there are some related subtleties that are easy to miss. We will make sure to highlight these as we explore the technique.</p>
<p>As k-means clustering is so easy to implement, there are many proof-of-concept implementations of the algorithm in Go. You can find these by searching for k-means on this link (<a href="https://golanglibs.com/top?q=kmeans">https://golanglibs.com/top?q=kmeans</a>). However, we will utilize a implementation that is recent and fairly straightforward to use, <kbd>github.com/mash/gokmeans</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of k-means clustering</h1>
                </header>
            
            <article>
                
<p>Let's say that we have a bunch of data points defined by two variables, <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em>. These data points naturally exhibit some grouping into clusters, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="270" width="362" class="image-border" src="assets/eb2a3812-3684-4fe1-95d0-7b5cd79dab6b.png"/></div>
<p>To automatically cluster these points using k-means, we would first need to choose how many clusters will result from the clustering. This is the parameter <em>k</em>, which gives k-means its name. In this case, let's use <em>k = 3</em>.</p>
<p>We would then randomly choose the <em>x<sub>1</sub></em> <span>and</span> <em>x<sub>2</sub></em> locations of <em>k</em> centroids. These random centroids will serve as our starting point for the algorithm. Such random centroids are shown in the following figure via Xs:</p>
<div class="CDPAlignCenter CDPAlign"><img height="270" width="361" class="image-border" src="assets/97050bef-b68e-4829-8bd6-dc77a83420c2.png"/></div>
<p>To optimize these centroids and cluster our points, we then iteratively perform the following:</p>
<ol>
<li>Assign each data point to a cluster corresponding to the nearest centroid (as measured by our choice distance metric, such as Euclidean distance).</li>
<li>Calculate the mean <em>x<sub>1</sub></em> <span>and</span> <em>x<sub>2</sub></em> <span>locations within each cluster.</span></li>
<li>Update each centroid's location to the calculated <em>x<sub>1</sub></em> <span>and</span> <em>x<sub>2</sub></em> <span>locations.</span></li>
</ol>
<p>Repeat steps one to three until the assignment in step one no longer changes. This process is illustrated in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="540" width="650" class="image-border" src="assets/16178c0b-00bb-4f1e-86b0-39c78e5f880b.png"/></div>
<p>There is only so much that you can illustrate with a static figure, but hopefully this is helpful. If you would like to visually step through the k-means process to better understand the updates, you should check out <a href="http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html">http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html</a>, which includes an interactive animation of the k-means clustering process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">k-means assumptions and pitfalls</h1>
                </header>
            
            <article>
                
<p>k-means may seem like a very simple algorithm, which it is. However, it does make some underlying assumptions about your data, which are easy to overlook:</p>
<ul>
<li><strong>Spherical</strong> or <strong>spatially grouped clusters</strong>: k-means basically draws spherical or spatially close areas in our feature space to find clusters. This means that for non-spherical clusters (essentially, clusters that do not look like grouped blobs in our features space), k-means is likely to fail. To make this idea more concrete, non-spherical clusters, for which k-means will likely behave poorly, might look like the following:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="206" width="275" class="image-border" src="assets/1c96eb59-50de-4777-8844-4a552126a4cc.png"/></div>
<ul>
<li><strong>Similar size</strong>: k-means also assumes that your clusters are all of a similar size. Small outlying clusters can lead the simple k-means algorithm going off course to produce strange groupings.</li>
</ul>
<p>Moreover, there are a couple of pitfalls that we can fall into when using k-means to cluster our data:</p>
<ul>
<li>The choice of <em>k</em> is up to us. This means that we could choose an illogical <em>k</em>, but it also means that we could just continue increasing <em>k</em> until we have a cluster for each of our points (which would be pretty good clustering because each point is exactly the same as itself). To help guide your choice of <em>k</em>, you should utilize an <strong>elbow graph</strong> approach. In this approach, you increase <em>k</em> while calculating your evaluation metric. As you increase <em>k</em>, your evaluation metric should keep getting better, but eventually there will be an inflection point that indicates diminishing returns. The ideal <em>k</em> is at this elbow.</li>
<li>It is not guaranteed that k-means will always converge to the same clusters. As you are starting from random centroids, your k-means algorithm could converge to different local minimums on different runs. You should be aware of this and run your k-means algorithm from a variety of initializations to ensure stability.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">k-means clustering example</h1>
                </header>
            
            <article>
                
<p>The dataset that we will be using to illustrate clustering techniques is about delivery drivers. The dataset looks like this:</p>
<pre><strong>$ head fleet_data.csv 
Driver_ID,Distance_Feature,Speeding_Feature
3423311935,71.24,28.0
3423313212,52.53,25.0
3423313724,64.54,27.0
3423311373,55.69,22.0
3423310999,54.58,25.0
3423313857,41.91,10.0
3423312432,58.64,20.0
3423311434,52.02,8.0
3423311328,31.25,34.0</strong></pre>
<p>The first column, <kbd>Driver_ID</kbd>, includes various anonymous identifications of particular drivers. The second and third columns are attributes that we will utilize in our clusters. The <kbd>Distance_Feature</kbd> column is a mean distance driven per data, and <kbd>Speeding_Feature</kbd> is a mean percentage of time during which the driver is driving 5+ miles per hour faster than the speed limit.</p>
<p>The goal of the clustering will be to cluster the delivery drivers into groups based on <kbd>Distance_Feature</kbd> <span>and</span> <kbd>Speeding_Feature</kbd><span>. Remember, this is an unsupervised learning technique, and thus, we do not really know what clusters should or could be formed in the data. The hope is that we will learn something about the drivers that we did not know at the start of the exercise.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Profiling the data</h1>
                </header>
            
            <article>
                
<p>Yes, you guessed it! We have a new dataset and we need to profile this dataset to learn a little more about it. Let's first calculate summary statistics with <kbd>github.com/kniren/dataframe</kbd> and create histograms of each feature using <kbd>gonum.org/v1/plot</kbd>. We have already done this multiple times in <a href="c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml" target="_blank">Chapter 4</a>, <em>Regression</em> and <a href="f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml" target="_blank">Chapter 5</a>, <em>Classification</em>, so we will not rehash the code here. Let's just look at the results:</p>
<pre><strong>$ go build
$ ./myprogram
[7x4] DataFrame

    column   Driver_ID         Distance_Feature Speeding_Feature
 0: mean     3423312447.500000 76.041523        10.721000       
 1: stddev   1154.844867       53.469563        13.708543       
 2: min      3423310448.000000 15.520000        0.000000        
 3: 25%      3423311447.000000 45.240000        4.000000        
 4: 50%      3423312447.000000 53.330000        6.000000        
 5: 75%      3423313447.000000 65.610000        9.000000        
 6: max      3423314447.000000 244.790000       100.000000      
    &lt;string&gt; &lt;float&gt;           &lt;float&gt;          &lt;float&gt;</strong> </pre>
<p>Wow! Looks like most drivers speed about 10% of the time, which is kind of scary. One driver even appears to speed 100% of the time. I hope I'm not on his route.</p>
<p>The histograms features are shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="285" width="550" class="image-border" src="assets/2c6d6099-bc44-4b9b-9d77-4b0e0d8c9466.png"/></div>
<p>It looks like there is an interesting structure in the <kbd>Distance_Feature</kbd> data. This will actually factor into our clustering soon, but we can get another view of this structure by creating a scatter plot of our feature space:</p>
<pre>// Open the driver dataset file.
f, err := os.Open("fleet_data.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
driverDF := dataframe.ReadCSV(f)

// Extract the distance column.
yVals := driverDF.Col("Distance_Feature").Float()

// pts will hold the values for plotting
pts := make(plotter.XYs, driverDF.Nrow())

// Fill pts with data.
for i, floatVal := range driverDF.Col("Speeding_Feature").Float() {
    pts[i].X = floatVal
    pts[i].Y = yVals[i]
}

// Create the plot.
p, err := plot.New()
if err != nil {
    log.Fatal(err)
}
p.X.Label.Text = "Speeding"
p.Y.Label.Text = "Distance"
p.Add(plotter.NewGrid())
<br/>s, err := plotter.NewScatter(pts)
if err != nil {
    log.Fatal(err)
}
s.GlyphStyle.Color = color.RGBA{R: 255, B: 128, A: 255}
s.GlyphStyle.Radius = vg.Points(3)

// Save the plot to a PNG file.
p.Add(s)
if err := p.Save(4*vg.Inch, 4*vg.Inch, "fleet_data_scatter.png"); err != nil {
    log.Fatal(err)
} </pre>
<p>Compiling and running this creates the following scatter plot:</p>
<div class="CDPAlignCenter CDPAlign"><img height="270" width="270" class="image-border" src="assets/9d94a843-e1da-4bc9-975c-89ec82726b6e.png"/></div>
<p>Here, we can see a little bit more of the structure that we saw in the histograms. There appears to be at least two clear clusters of data here. This intuition about our data can serve as a mental check during the formal application of our clustering techniques, and it can give us a starting point to experiment with values of <em>k.</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating clusters with k-means</h1>
                </header>
            
            <article>
                
<p>Now let's get our hands dirty by actually applying k-means clustering to the delivery driver data. To utilize <kbd><span>github.com/mash/gokmeans</span></kbd><span>, we first need to create a slice of <kbd>gokmeans.Node</kbd> values, which will be input into the clustering:</span></p>
<pre>// Open the driver dataset file.<br/>f, err := os.Open("fleet_data.csv")<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>defer f.Close()        <br/><br/>// Create a new CSV reader.
r := csv.NewReader(f)
r.FieldsPerRecord = 3

// Initialize a slice of gokmeans.Node's to
// hold our input data.
var data []gokmeans.Node

// Loop over the records creating our slice of
// gokmeans.Node's.
for {

    // Read in our record and check for errors.
    record, err := r.Read()
    if err == io.EOF {
        break
    }
    if err != nil {
        log.Fatal(err)
    }

    // Skip the header.
    if record[0] == "Driver_ID" {
        continue
    }
<br/>    // Initialize a point.
    var point []float64

    // Fill in our point.
    for i := 1; i &lt; 3; i++ {

        // Parse the float value.
        val, err := strconv.ParseFloat(record[i], 64)
        if err != nil {
            log.Fatal(err)
        }

        // Append this value to our point.
        point = append(point, val)
    }

    // Append our point to the data.
    data = append(data, gokmeans.Node{point[0], point[1]})
}</pre>
<p>Then, generating our clusters is as easy as calling the <kbd>gomeans.Train(...)</kbd> function. Specifically, we will call this function with <em>k = 2</em> and a maximum of <kbd>50</kbd> iterations:</p>
<pre>// Generate our clusters with k-means.
success, centroids := gokmeans.Train(data, 2, 50)
if !success {
    log.Fatal("Could not generate clusters")
}

// Output the centroids to stdout.
fmt.Println("The centroids for our clusters are:")
for _, centroid := range centroids {
    fmt.Println(centroid)
}</pre>
<p>Running all of this together gives the following centroids for the generated clusters:</p>
<pre><strong>$ go build
$ ./myprogram 
The centroids for our clusters are:
[50.04763437499999 8.82875]
[180.01707499999992 18.29]</strong></pre>
<p>Nice! We have generated our first clusters. Now, we need to move on to evaluating the legitimacy of these clusters.</p>
<div class="packt_infobox">I have just output the centroids of the clusters here, because that is really all we need to know the group points. If we want to know if a data point is in the first or second cluster, we just need to calculate the distance to those centroids. The closer of the centroids corresponds to the group containing the data point.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the generated clusters</h1>
                </header>
            
            <article>
                
<p>The first way that we can evaluate the clusters that we just generated is visually. Let's create another scatter plot. However, this time let's <span>use different shapes for</span> each of the groups:</p>
<pre>// Open the driver dataset file.<br/>f, err := os.Open("fleet_data.csv")<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>defer f.Close()<br/><br/>// Create a dataframe from the CSV file.<br/>driverDF := dataframe.ReadCSV(f)<br/><br/>// Extract the distance column.<br/>yVals := driverDF.Col("Distance_Feature").Float()        <br/><br/>// clusterOne and clusterTwo will hold the values for plotting.
var clusterOne [][]float64
var clusterTwo [][]float64

// Fill the clusters with data.
for i, xVal := range driverDF.Col("Speeding_Feature").Float() {
    distanceOne := floats.Distance([]float64{yVals[i], xVal}, []float64{50.05, 8.83}, 2)
    distanceTwo := floats.Distance([]float64{yVals[i], xVal}, []float64{180.02, 18.29}, 2)
    if distanceOne &lt; distanceTwo {
        clusterOne = append(clusterOne, []float64{xVal, yVals[i]})
        continue
    }
    clusterTwo = append(clusterTwo, []float64{xVal, yVals[i]})
}

// pts* will hold the values for plotting
ptsOne := make(plotter.XYs, len(clusterOne))
ptsTwo := make(plotter.XYs, len(clusterTwo))

// Fill pts with data.
for i, point := range clusterOne {
    ptsOne[i].X = point[0]
    ptsOne[i].Y = point[1]
}

for i, point := range clusterTwo {
    ptsTwo[i].X = point[0]
    ptsTwo[i].Y = point[1]
}

// Create the plot.
p, err := plot.New()
if err != nil {
    log.Fatal(err)
}
p.X.Label.Text = "Speeding"
p.Y.Label.Text = "Distance"
p.Add(plotter.NewGrid())

sOne, err := plotter.NewScatter(ptsOne)
if err != nil {
    log.Fatal(err)
}
sOne.GlyphStyle.Radius = vg.Points(3)<br/>sOne.GlyphStyle.Shape = draw.PyramidGlyph{}

sTwo, err := plotter.NewScatter(ptsTwo)
if err != nil {
    log.Fatal(err)
}
sTwo.GlyphStyle.Radius = vg.Points(3)

// Save the plot to a PNG file.
p.Add(sOne, sTwo)
if err := p.Save(4*vg.Inch, 4*vg.Inch, "fleet_data_clusters.png"); err != nil {
    log.Fatal(err)
}</pre>
<p>This code generates the following scatterplot, which clearly shows our successful clustering:</p>
<div class="CDPAlignCenter CDPAlign"><img height="270" width="270" class="image-border" src="assets/e7416b8f-e070-48c4-ba9c-31af7ec9893c.png"/></div>
<p>Qualitatively, we can see that there is one cluster of driver that drive short distances primarily, and one cluster of drivers that drive long distances primarily. These actually correspond to rural and city delivery drivers respectively (or short haul and long haul drivers).</p>
<p>To more quantitatively evaluate our clusters, we can calculate the within-cluster mean distance between points in a cluster and the cluster centroid. To help us in this endeavor, let's create a function that will make things a little easier:</p>
<pre>// withinClusterMean calculates the mean distance between
// points in a cluster and the centroid of the cluster.
func withinClusterMean(cluster [][]float64, centroid []float64) float64 {

    // meanDistance will hold our result.
    var meanDistance float64

    // Loop over the points in the cluster.
    for _, point := range cluster {
        meanDistance += floats.Distance(point, centroid, 2) / float64(len(cluster))
    }

    return meanDistance
}</pre>
<p>Now, to evaluate our clusters, we simply need to call this function for each cluster:</p>
<pre>// Output our within cluster metrics.
fmt.Printf("\nCluster 1 Metric: %0.2f\n", withinClusterMean(clusterOne, []float64{50.05, 8.83}))
fmt.Printf("\nCluster 2 Metric: %0.2f\n", withinClusterMean(clusterTwo, []float64{180.02, 18.29}))</pre>
<p>Running this gives us the following metrics:</p>
<pre><strong>$ go build
$ ./myprogram 

Cluster 1 Metric: 11.68

Cluster 2 Metric: 23.52</strong><br/><br/></pre>
<p>As we can see, cluster one (the pink cluster in the scatter plot) is about twice as compact (tightly packed, that is) as cluster two. This is consistent without the plot and gives us a little more quantitative information about the clusters.</p>
<div class="packt_tip">Note, that here it was pretty clear that we were looking for two clusters. However, in other cases, the number of clusters may not be clear upfront, especially in cases where you have more features than you can visualize. In these scenarios, it is important that you utilize a method, such as the <kbd>elbow</kbd> method, to determine a proper <em>k</em>. More information about this method can be found at <a href="https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/">https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other clustering techniques</h1>
                </header>
            
            <article>
                
<p>There are a host of other clustering techniques that are not discussed here. These include DBSCAN and Hierarchical clustering. Unfortunately, the current implementations in Go are limited for these other clustering options. DBSCAN is implemented in <kbd>https://github.com/sjwhitworth/golearn</kbd>, but, to my knowledge, there are no current implementations of other clustering techniques.</p>
<p>This creates a great opportunity for contributions to the community! Clustering techniques are often not complicated and creating an implementation of another clustering technique might be a great way to give back to the Go data science community. Feel free to reach out to the author in Gophers Slack (<kbd>@dwhitena</kbd>) or other data science gophers in <kbd>#data-science</kbd> on Gophers Slack if you want to discuss an implementation, ask questions, or get help!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p>Distance metrics and evaluating clusters:</p>
<ul>
<li>Evaluation of clustering overview: <a href="https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html">https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html</a></li>
<li><span>A comparison of various distance/similarity metrics: <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144059">http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144059</a></span></li>
<li>Visualizing k-means clustering: <a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/">https://www.naftaliharris.com/blog/visualizing-k-means-clustering/</a></li>
<li><kbd>github.com/mash/gokmeans</kbd> docs: <a href="https://godoc.org/github.com/mash/gokmeans">https://godoc.org/github.com/mash/gokmeans</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have introduced the general principles of clustering, learned how to evaluate generated clusters, and learned how to use a Go implementation of k-means clustering. You should now be in good shape to detect grouping structure in your datasets.</p>
<p>Next, we will discuss the modeling of time series data, such as stock prices, sensor data, and so on.</p>


            </article>

            
        </section>
    </body></html>