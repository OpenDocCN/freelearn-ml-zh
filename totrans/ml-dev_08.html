<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Recent Models and Developments</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapters, we have explored a large number of training mechanisms for machine learning models, starting with simple pass-through mechanisms, such as the well-known feedforward neural networks. Then we looked at a more complex and reality-bound mechanism, accepting a determined sequence of inputs as the training input, with <strong>Recurrent Neural Networks</strong> (<strong>RNNs</strong>).</p>
<p>Now it's time to take a look at two recent players that incorporate other aspects of the real world. In the first case, we will have not only a single network optimizing its model, but also another participant, and they will both<span> improve each other's results. This is the case of</span> <strong>Generative Adversarial Networks</strong> <span>(</span><strong>GANs</strong><span>).</span></p>
<p>In the second case, we will talk about a different kind of model, which will try to determine the optimal set of steps to maximize a reward: <strong>reinforcement learning</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">GANs</h1>
                </header>
            
            <article>
                
<p>GANs are a new kind of unsupervised learning model, one of the very few disrupting models of the last decade. They have two models competing with and improving each other throughout the iterations.</p>
<p>This architecture was originally based on supervised learning and game theory, and its main objective is to basically learn to generate realistic samples from an original dataset of elements of the same class.</p>
<p>It's worth noting that the amount of research on GANs is increasing at an almost exponential rate, as depicted in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="371" width="540" src="assets/35e6af3d-756a-470f-a9d2-0f7fb907fa3f.jpg"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Source: The GAN Zoo (https://github.com/hindupuravinash/the-gan-zoo)</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of GAN applications</h1>
                </header>
            
            <article>
                
<p><span>GANs allow new applications to produce new samples from a previous set of samples, including completing missing information.</span></p>
<p>In the following screenshot, we depict a number of samples created with the <span><strong>LSGAN architecture</strong> on five scene datasets from LSUN, including a kitchen, church, dining room, and conference room:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="528" width="668" class="alignnone size-full wp-image-867 image-border" src="assets/6289b53f-de4e-4cd8-9116-52c6e6dd56e0.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">LSGAN created models</div>
<p>Another really interesting example is <span>class-conditional image sampling using the <strong>Plug and Play Generative Network (PPGN)</strong> to fill in 100 x 100 missing pixels in a real 227 x 227 image.</span></p>
<p><span>The following screenshot compares</span> <span>PPGN</span><span> </span><span>variations</span><span> </span><span>and the equivalent Photoshop image completion:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="264" width="334" src="assets/8b3e6c6c-b1df-4a67-9afa-a8869db14d78.jpg"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">PPGN infilling example</div>
<p class="mce-root CDPAlignLeft CDPAlign"><span>The PPGN can also generate images synthetically at a high resolution (227 x 227) for the volcano class. Not only are many images nearly photo-realistic, but the samples within this class are diverse:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="208" width="379" src="assets/7e1b41d5-17e8-4dac-9aa0-fc13daa0b726.jpg"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">PPGN generated volcano samples</div>
<p><span>The following screenshot illustrates the process vector arithmetic for visual concepts. It allows the use of operands representing objects in the image and can add or remove them, moving in a feature space:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="290" width="536" src="assets/360b4d4c-7ea5-4df5-b87d-f2d8f081c991.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Vector arithmetic operations for the feature space</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discriminative and generative models</h1>
                </header>
            
            <article>
                
<p>To understand the concept of adversarial networks, we will first define the two models that interact in a typical GAN setup:</p>
<ul>
<li class="qtext_para"><strong>Generator</strong>: This is tasked with taking in a sample from a standard random distribution (for example, a sample from an <span class="render_latex"><span class="MathJax"><span class="MJX_Assistive_MathML">n</span></span></span>-dimensional Gaussian) and producing a point that looks sort of like it could come from the same distribution as <em><span class="render_latex"><span class="MathJax"><span class="math"><span class="mrow"><span class="mi">X</span></span></span><span class="MJX_Assistive_MathML">X</span></span></span></em>. It could be said that the generator wants to fool the discriminator to output 1. Mathematically, it learns a function that maps the input data (<em>x</em><span>) to some desired output class label (</span><em>y</em><span>). In probabilistic terms, it learns the conditional distribution</span><span> </span><em>P(y|x)</em><span> </span><span>of the input data. It discriminates</span><span> </span><span><span>between two (or more) different classes of data—for example, a convolutional neural network that is trained to output 1 given an image of a human face and 0 otherwise. </span></span></li>
</ul>
<ul>
<li class="qtext_para"><strong>Discriminator</strong>: This is tasked with discriminating between samples from the true data and the artificial data generated by the generator. Each model will try to beat the other (the generator's objective is to fool the discriminator and the discriminator's objective is to not be fooled by the generator):</li>
</ul>
<div class="qtext_para CDPAlignCenter CDPAlign"><img height="320" width="367" src="assets/d5b720c1-e3b6-4fea-9c47-d20cd6587eee.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Training process for a GAN</div>
<p class="qtext_para">More formally, the generator tries to learn the joint probability of the input data and labels simultaneously, that is,<span> </span><em>P(x, y)</em>. So, it can be used for something else as well, such as creating likely new<span> </span><em>(x, y)</em><span> </span>samples. It <span>doesn't know anything about classes of data. Instead, its purpose is to</span><span> </span>generate<span> </span><span>new data that fits the distribution of the training data.</span></p>
<p class="qtext_para">In the general case, both<span> </span><span class="render_latex"><span class="MathJax"><span class="math"><span><span class="mrow"><span class="mi">the generator and the discriminator are neural</span></span></span></span></span></span> networks and they are trained in an alternating manner. Each of their objectives can be expressed as a loss function that we can optimize via gradient descent. </p>
<p class="qtext_para">The final result is that both models improve themselves in tandem; the generator produces better images, and the discriminator gets better at determining whether the generated sample is fake. In practice, the final result is a model that produces really good and realistic new samples (for example, random pictures of a natural environment).</p>
<p><strong>Summarizing, the main takeaways from GANs are:</strong></p>
<ul>
<li>GANs are generative models that use supervised learning to approximate an intractable cost function</li>
<li>GANs can simulate many cost functions, including the one used for maximum likelihood</li>
<li>Finding the Nash equilibrium in high-dimensional, continuous, non-convex games is an important open research problem</li>
<li>GANs are a key ingredient of PPGNs, which are able to generate compelling high-resolution samples from diverse image classes</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning is a field that has resurfaced recently, and it has become more popular in the fields of control, finding the solutions to games and situational problems, where a number of steps have to be implemented to solve a problem.</p>
<p>A formal definition of reinforcement learning is as follows:</p>
<div class="packt_quote">"Reinforcement learning is the problem faced by an agent that must learn behavior through trial-and-error interactions with a dynamic environment.” (Kaelbling et al. 1996).</div>
<p>In order to have a reference frame for the type of problem we want to solve, we will start by going back to a mathematical concept developed in the 1950s, called the <strong>Markov decision process</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Markov decision process</h1>
                </header>
            
            <article>
                
<p>Before explaining reinforcement learning techniques, we will explain the type of problem we will attack with them.</p>
<p>When talking about reinforcement learning, we want to optimize the problem of a Markov decision process. It consists of a mathematical model that aids decision making in situations where the outcomes are in part random, and in part under the control of an agent.</p>
<p>The main elements of this model are an <strong>Agent</strong>, an <strong>Environment</strong>, and a <strong>State</strong>, as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img height="167" width="278" src="assets/2335ec23-3edc-413c-98a7-c9c96d4bedda.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Simplified scheme of a reinforcement learning process</div>
<p>The agent can perform certain actions (such as moving the paddle left or right). These actions can sometimes result in a reward<span> </span><em>r<sub>t</sub></em>, which can be positive or negative (such as an increase or decrease in the score). Actions change the environment and can lead to a new state<span> </span><em>s<sub>t+1</sub></em>, where the agent can perform another action<span> </span><em>a<sub>t+1</sub></em>. The set of states, actions, and rewards, together with the rules for transitioning from one state to another, make up a Markov decision process. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision elements</h1>
                </header>
            
            <article>
                
<p>To understand the problem, let's situate ourselves in the problem solving environment and look at the main elements:</p>
<ul>
<li>The set of states</li>
<li>The action to take is to go from one place to another</li>
<li>The reward function is the value represented by the edge</li>
<li>The policy is the way to complete the task</li>
<li>A discount factor, which <span>determines the importance of future rewards</span></li>
</ul>
<p>The main difference with traditional forms of supervised and unsupervised learning is the time taken to calculate the reward, which in reinforcement learning is not instantaneous; it comes after a set of steps.</p>
<p>Thus, the next state depends on the current state and the decision maker's action, and the state is not dependent on all the previous states (it doesn't have memory), thus it complies with the Markov property. </p>
<p>Since this is a Markov decision process, the probability of state<span> </span><em>s<sub>t+1</sub></em><span> </span>depends only on the current state<span> </span><em>s<sub>t</sub></em><span> </span>and action<span> </span><em>a<sub>t</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="146" width="524" src="assets/3a39fd08-d2bc-4440-a22c-85a754637b0d.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Unrolled reinforcement mechanism</div>
<p><span>The goal of the whole process is to generate a policy <em>P,</em> that maximizes rewards. The training samples are tuples, <em>&lt;s, a, r&gt;</em>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing the Markov process</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning is an iterative interaction between an agent and the environment. The following occurs at each timestep:</p>
<ul>
<li>The process is in a state and the decision-maker may choose any action that is available in that state</li>
<li>The process responds at the next timestep by randomly moving into a new state and giving the decision-maker a corresponding reward</li>
<li>The probability that the process moves into its new state is influenced by the chosen action in the form of a state transition function </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic RL techniques: Q-learning</h1>
                </header>
            
            <article>
                
<p>One of the most well-known reinforcement learning techniques, and the one we will be implementing in our example, is<span> </span><strong>Q-learning</strong>.</p>
<p>Q-learning can be used to find an optimal action for any given state in a finite Markov decision process. Q-learning tries to maximize the value of the Q-function that represents the maximum discounted future reward when we perform action<span> </span><em>a</em><span> </span>in state<span> </span><em>s</em>.</p>
<p>Once we know the Q-function, the optimal action<span> </span><em>a</em> in state<span> </span><em>s</em><span> </span>is the one with the highest Q-value. We can then define a policy<span> π</span><em>(s),</em> that gives us the optimal action in any state, expressed as follows:</p>
<div style="padding-left: 180px"><img height="58" width="75" src="assets/af84694d-ce12-4ccd-9df8-c4ea163fe2c9.png"/></div>
<p>We can define the Q-function for a transition point (<em>s<sub>t</sub>, a<sub>t</sub>, r<sub>t</sub>, s<sub>t+1</sub></em>) in terms of the Q-function at the next point (<em>s<sub>t+1</sub></em>,<span> </span><em>a<sub>t+1</sub></em>,<span> </span><em>r<sub>t+1</sub></em>, <em>s<sub>t+2</sub></em>), similar to what we did with the total discounted future reward. This equation is known as the<span> </span><strong>Bellman equation for Q-learning:</strong></p>
<div style="padding-left: 120px" class="mce-root"><img height="67" width="358" src="assets/0153a9f1-4dc4-4b03-8b8a-108c8090f0c3.png"/></div>
<p>In practice, we  can think of the Q-function as a lookup table (called a<span> </span><strong>Q-table</strong>) where the states (denoted by<span> </span><em>s</em>) are rows and the actions (denoted by<span> </span><em>a</em>) are columns, and the elements (denoted by<span> </span><em>Q(s, a)</em>) are the rewards that you get if you are in the state given by the row and take the action given by the column. The best action to take at any state is the one with the highest reward:</p>
<pre><strong>initialize Q-table Q</strong><br/><strong>observe initial state s</strong><br/><strong>while (! game_finished):</strong><br/><strong>   select and perform action a</strong><br/><strong>   get reward r <br/>   advance to state s'</strong><br/><strong>   Q(s, a) = Q(s, a) + α(r + γ max_a' Q(s', a') - Q(s, a))</strong><br/><strong>   s = s'</strong></pre>
<p>You will realize that the algorithm is basically doing stochastic gradient descent on the Bellman equation, backpropagating the reward through the state space (or episode) and averaging over many trials (or epochs). Here, <kbd>α</kbd> is the learning rate that determines how much of the difference between the previous Q-value and the discounted new maximum Q-value should be incorporated.</p>
<p>We can represent this process with the following flowchart:</p>
<div class="CDPAlignCenter CDPAlign"><img height="296" width="211" src="assets/deb6d7a0-4377-43e1-bfea-b9b75915f19a.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Bellman, Richard, <em>A Markovian decision process.</em> </span>Journal of Mathematics and Mechanics<span> (1957): 679-684.</span></li>
<li><span>Kaelbling, Leslie Pack, Michael L. Littman, and Andrew W. Moore, <em>Reinforcement learning: A survey.</em> </span>Journal of artificial intelligence research<span> 4 (1996): 237-285.</span></li>
<li><span>Goodfellow, Ian, et al., <em>Generative adversarial nets, a</em></span><em>dvances in neural information processing systems,</em><span><span> 2014</span></span></li>
<li><span>Radford, Alec, Luke Metz, and Soumith Chintala, <em>Unsupervised representation learning with deep convolutional generative adversarial networks.</em> </span>arXiv preprint arXiv:1511.06434<span> (2015).</span></li>
<li><span>Isola, Phillip, et al., <em>Image-to-image translation with conditional adversarial networks</em>, </span>arXiv preprint arXiv:1611.07004<span> (2016).</span></li>
<li><span>Mao, Xudong, et al., <em>Least squares generative adversarial networks.</em> </span>arXiv preprint ArXiv:1611.04076<span> (2016).</span></li>
<li><span>Eghbal-Zadeh, Hamid, and Gerhard Widmer, <em>Likelihood Estimation for Generative Adversarial Networks.</em> </span>arXiv preprint arXiv:1707.07530<span> (2017).</span></li>
<li><span>Nguyen, Anh, et al., <em>Plug &amp; play generative networks: Conditional iterative generation of images in latent space</em>. </span>arXiv preprint arXiv:1612.00005<span> (2016).</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we have reviewed two of the most important and innovative architectures that have appeared in recent. Every day, new generative and reinforcement models are applied in innovative ways, whether to generate feasible new elements from a selection of previously known classes or even to win against professional players in strategy games.</span></p>
<p><span>In the next chapter, we will provide precise instructions so you can use and modify the code provided to better understand the different concepts you have acquired throughout the book.</span></p>
<p> </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>