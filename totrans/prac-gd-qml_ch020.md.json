["```py\n\nimport numpy as np \n\nimport tensorflow as tf \n\nseed = 1234 \n\nnp.random.seed(seed) \n\ntf.random.set_seed(seed)\n\n```", "```py\n\nfrom sklearn.metrics import accuracy_score \n\nfrom sklearn.model_selection import train_test_split\n\n```", "```py\n\nfrom sklearn.datasets import make_classification\n\n```", "```py\n\ntf.keras.backend.set_floatx(’float64’)\n\n```", "```py\n\nimport pennylane as qml \n\nstate_0 = [[1], [0]] \n\nM = state_0 * np.conj(state_0).T\n\n```", "```py\n\nimport matplotlib.pyplot as plt \n\ndef plot_losses(history): \n\n    tr_loss = history.history[\"loss\"] \n\n    val_loss = history.history[\"val_loss\"] \n\n    epochs = np.array(range(len(tr_loss))) + 1 \n\n    plt.plot(epochs, tr_loss, label = \"Training loss\") \n\n    plt.plot(epochs, val_loss, label = \"Validation loss\") \n\n    plt.xlabel(\"Epoch\") \n\n    plt.legend() \n\n    plt.show()\n\n```", "```py\n\nx, y = make_classification(n_samples = 1000, n_features = 20)\n\n```", "```py\n\nx_tr, x_test, y_tr, y_test = train_test_split( \n\n    x, y, train_size = 0.8) \n\nx_val, x_test, y_val, y_test = train_test_split( \n\n    x_test, y_test, train_size = 0.5)\n\n```", "```py\n\ndef TwoLocal(nqubits, theta, reps = 1): \n\n    for r in range(reps): \n\n        for i in range(nqubits): \n\n            qml.RY(theta[r * nqubits + i], wires = i) \n\n        for i in range(nqubits - 1): \n\n            qml.CNOT(wires = [i, i + 1]) \n\n    for i in range(nqubits): \n\n        qml.RY(theta[reps * nqubits + i], wires = i)\n\n```", "```py\n\nnqubits = 4 \n\ndev = qml.device(\"lightning.qubit\", wires = nqubits) \n\n@qml.qnode(dev, interface=\"tf\", diff_method = \"adjoint\") \n\ndef qnn(inputs, theta): \n\n    qml.AngleEmbedding(inputs, range(nqubits)) \n\n    TwoLocal(nqubits, theta, reps = 2) \n\n    return qml.expval(qml.Hermitian(M, wires = [0])) \n\nweights = {\"theta\": 12}\n\n```", "```py\n\nmodel = tf.keras.models.Sequential([ \n\n    tf.keras.layers.Input(20), \n\n    tf.keras.layers.Dense(4, activation = \"sigmoid\"), \n\n    qml.qnn.KerasLayer(qnn, weights, output_dim=1) \n\n])\n\n```", "```py\n\nqlayer = qml.qnn.KerasLayer(qnn, weights, output_dim=1) \n\nmodel = tf.keras.models.Sequential([ \n\n    tf.keras.layers.Input(20), \n\n    tf.keras.layers.Dense(4, activation = \"sigmoid\"), \n\n    qlayer \n\n])\n\n```", "```py\n\nearlystop = tf.keras.callbacks.EarlyStopping( \n\n    monitor=\"val_loss\", patience=2, verbose=1, \n\n    restore_best_weights=True)\n\n```", "```py\n\nopt = tf.keras.optimizers.Adam(learning_rate = 0.005) \n\nmodel.compile(opt, loss=tf.keras.losses.BinaryCrossentropy()) \n\nhistory = model.fit(x_tr, y_tr, epochs = 50, shuffle = True, \n\n    validation_data = (x_val, y_val), \n\n    batch_size = 10, \n\n    callbacks = [earlystop])\n\n```", "```py\n\nplot_losses(history)\n\n```", "```py\n\ntr_acc = accuracy_score(model.predict(x_tr) >= 0.5, y_tr) \n\nval_acc = accuracy_score(model.predict(x_val) >= 0.5, y_val) \n\ntest_acc = accuracy_score(model.predict(x_test) >= 0.5, y_test) \n\nprint(\"Train accuracy:\", tr_acc) \n\nprint(\"Validation accuracy:\", val_acc) \n\nprint(\"Test accuracy:\", test_acc)\n\n```", "```py\n\nimport optuna\n\n```", "```py\n\ndef objective(trial): \n\n    # Define the learning rate as an optimizable parameter. \n\n    lrate = trial.suggest_float(\"learning_rate\", 0.001, 0.1) \n\n    # Define the optimizer with the learning rate. \n\n    opt = tf.keras.optimizers.Adam(learning_rate = lrate) \n\n    # Prepare and compile the model. \n\n    model = tf.keras.models.Sequential([ \n\n        tf.keras.layers.Input(20), \n\n        tf.keras.layers.Dense(4, activation = \"sigmoid\"), \n\n        qml.qnn.KerasLayer(qnn, weights, output_dim=1) \n\n    ]) \n\n    model.compile(opt, loss=tf.keras.losses.BinaryCrossentropy()) \n\n    # Train it! \n\n    history = model.fit(x_tr, y_tr, epochs = 50, shuffle = True, \n\n        validation_data = (x_val, y_val), \n\n        batch_size = 10, \n\n        callbacks = [earlystop], \n\n        verbose = 0 # We want TensorFlow to be quiet. \n\n    ) \n\n    # Return the validation accuracy. \n\n    return accuracy_score(model.predict(x_val) >= 0.5, y_val)\n\n```", "```py\n\n    activation = trial.suggest_categorical( \n\n        \"activation_function\", [\"sigmoid\", \"elu\", \"relu\"]).\n\n    ```", "```py\n\nfrom optuna.samplers import TPESampler \n\nstudy = optuna.create_study(direction=’maximize’, \n\n    sampler=TPESampler(seed = seed))\n\n```", "```py\n\nvalues = {\"learning_rate\": [0.001, 0.003, 0.005, 0.008, 0.01]} \n\nsampler = optuna.samplers.GridSampler(values)\n\n```", "```py\n\nstudy.optimize(objective, n_trials=6)\n\n```", "```py\n\nTrial 0 finished with value: 0.9 and parameters: \n    {’learning_rate’: 0.01996042558751034}. \n    Best is trial 0 with value: 0.9\\. \n\nTrial 1 finished with value: 0.9 and parameters: \n    {’learning_rate’: 0.06258876833294336}. \n    Best is trial 0 with value: 0.9\\. \n\nTrial 2 finished with value: 0.9 and parameters: \n    {’learning_rate’: 0.04433504616170433}. \n    Best is trial 0 with value: 0.9\\. \n\nTrial 3 finished with value: 0.91 and parameters: \n    {’learning_rate’: 0.07875049978766316}. \n    Best is trial 3 with value: 0.91\\. \n\nTrial 4 finished with value: 0.92 and parameters: \n    {’learning_rate’: 0.07821760500376156}. \n    Best is trial 4 with value: 0.92\\. \n\nTrial 5 finished with value: 0.9 and parameters: \n    {’learning_rate’: 0.02798666792298152}. \n    Best is trial 4 with value: 0.92.\n\n```", "```py\n\nnp.random.seed(seed) \n\ntf.random.set_seed(seed)\n\n```", "```py\n\nx, y = make_classification(n_samples = 1000, n_features = 20, \n\n    n_classes = 3, n_clusters_per_class = 1)\n\n```", "```py\n\nfrom sklearn.preprocessing import OneHotEncoder \n\nhot = OneHotEncoder(sparse = False) \n\ny_hot = hot.fit_transform(y.reshape(-1,1))\n\n```", "```py\n\nx_tr, x_test, y_tr, y_test = train_test_split( \n\n    x, y_hot, train_size = 0.8) \n\nx_val, x_test, y_val, y_test = train_test_split( \n\n    x_test, y_test, train_size = 0.5)\n\n```", "```py\n\nnqubits = 4 \n\ndev = qml.device(\"lightning.qubit\", wires = nqubits) \n\n@qml.qnode(dev, interface=\"tf\", diff_method = \"adjoint\") \n\ndef qnn(inputs, theta): \n\n    qml.AngleEmbedding(inputs, range(nqubits)) \n\n    TwoLocal(nqubits, theta, reps = 2) \n\n    return [qml.expval(qml.Hermitian(M, wires = [0])), \n\n            qml.expval(qml.Hermitian(M, wires = [1])), \n\n            qml.expval(qml.Hermitian(M, wires = [2]))] \n\nweights = {\"theta\": 12}\n\n```", "```py\n\nmodel = tf.keras.models.Sequential([ \n\n    tf.keras.layers.Input(20), \n\n    tf.keras.layers.Dense(8, activation = \"elu\"), \n\n    tf.keras.layers.Dense(4, activation = \"sigmoid\"), \n\n    qml.qnn.KerasLayer(qnn, weights, output_dim = 3), \n\n    tf.keras.layers.Activation(activation = \"softmax\") \n\n])\n\n```", "```py\n\nopt = tf.keras.optimizers.Adam(learning_rate = 0.001) \n\nmodel.compile(opt, loss=tf.keras.losses.CategoricalCrossentropy()) \n\nhistory = model.fit(x_tr, y_tr, epochs = 50, shuffle = True, \n\n    validation_data = (x_val, y_val), \n\n    batch_size = 10, \n\n    callbacks = [earlystop])\n\n```", "```py\n\nplot_losses(history)\n\n```", "```py\n\ntr_acc = accuracy_score( \n\n    model.predict(x_tr).argmax(axis = 1), \n\n    y_tr.argmax(axis = 1)) \n\nval_acc = accuracy_score( \n\n    model.predict(x_val).argmax(axis = 1), \n\n    y_val.argmax(axis = 1)) \n\ntest_acc = accuracy_score( \n\n    model.predict(x_test).argmax(axis = 1), \n\n    y_test.argmax(axis = 1)) \n\nprint(\"Train accuracy:\", tr_acc) \n\nprint(\"Validation accuracy:\", val_acc) \n\nprint(\"Test accuracy:\", test_acc)\n\n```", "```py\n\nimport numpy as np \n\nfrom sklearn.metrics import accuracy_score \n\nfrom sklearn.model_selection import train_test_split \n\nfrom sklearn.datasets import make_classification \n\nseed = 1234 \n\nnp.random.seed(seed)\n\n```", "```py\n\nimport torch \n\ntorch.manual_seed(seed)\n\n```", "```py\n\nimport torch.nn as nn \n\nimport torch.nn.functional as F\n\n```", "```py\n\nclass TorchClassifier(nn.Module): \n\n    def __init__(self): \n\n        # Initialize super class. \n\n        super(TorchClassifier, self).__init__() \n\n        # Declare the layers that we will use. \n\n        self.layer1 = nn.Linear(16, 8) \n\n        self.layer2 = nn.Linear(8, 4) \n\n        self.layer3 = nn.Linear(4, 2) \n\n        self.layer4 = nn.Linear(2, 1) \n\n    # Define the transformation of an input. \n\n    def forward(self, x): \n\n        x = F.elu(self.layer1(x)) \n\n        x = F.elu(self.layer2(x)) \n\n        x = F.elu(self.layer3(x)) \n\n        x = torch.sigmoid(self.layer4(x)) \n\n        return x\n\n```", "```py\n\nmodel = TorchClassifier() \n\nprint(model)\n\n```", "```py\n\nTorchClassifier( \n  (layer1): Linear(in_features=16, out_features=8, bias=True) \n  (layer2): Linear(in_features=8, out_features=4, bias=True) \n  (layer3): Linear(in_features=4, out_features=2, bias=True) \n  (layer4): Linear(in_features=2, out_features=1, bias=True) \n)\n\n```", "```py\n\nmodel(torch.rand(16))\n\n```", "```py\n\ntensor([0.4240], grad_fn=<SigmoidBackward0>)\n\n```", "```py\n\nx, y = make_classification(n_samples = 1000, n_features = 16) \n\nx_tr, x_test, y_tr, y_test = train_test_split( \n\n    x, y, train_size = 0.8) \n\nx_val, x_test, y_val, y_test = train_test_split( \n\n    x_test, y_test, train_size = 0.5)\n\n```", "```py\n\nfrom torch.utils.data import Dataset \n\nclass NumpyDataset(Dataset): \n\n    def __init__(self, x, y): \n\n        if (x.shape[0] != y.shape[0]): \n\n            raise Exception(\"Incompatible arrays\") \n\n        y = y.reshape(-1,1) \n\n        self.x = torch.from_numpy(x).to(torch.float) \n\n        self.y = torch.from_numpy(y).to(torch.float) \n\n    def __getitem__(self, i): \n\n        return self.x[i], self.y[i] \n\n    def __len__(self): \n\n        return self.y.shape[0]\n\n```", "```py\n\ntr_data = NumpyDataset(x_tr, y_tr) \n\nval_data = NumpyDataset(x_val, y_val) \n\ntest_data = NumpyDataset(x_test, y_test)\n\n```", "```py\n\nprint(tr_data[0]) \n\nprint(\"Length:\", len(tr_data))\n\n```", "```py\n\n(tensor([ 1.4791,  1.4646,  0.0430,  0.0409, -0.3792, -0.5357, \n          0.9736, -1.3697, -1.2596,  1.5159, -0.9276,  0.6868, \n          0.5138,  0.4751,  1.0193, -1.7873]), \ntensor([0.])) \n\nLength: 800\n\n```", "```py\n\nfrom torch.utils.data import DataLoader \n\ntr_loader = iter(DataLoader( \n\n    tr_data, batch_size = 2, shuffle = True)) \n\nprint(next(tr_loader))\n\n```", "```py\n\n[tensor([[-1.2835, -0.4155,  0.4518,  0.6778, -1.3869, -0.4262, -0.1016, \n           1.4012, -0.9625,  1.0038,  0.3946,  0.1961, -0.7455,  0.4267, \n           -0.8352,  0.9295], \n          [-1.4578, -0.4947, -1.1755, -0.4800, -0.3247,  0.7821, -0.0078, \n           -0.5397, -1.0385, -1.3466,  0.4591,  0.5761,  0.2188, -0.1447, \n           0.3534,  0.5055]]), \n tensor([[0.], \n         [0.]])]\n\n```", "```py\n\nget_loss = F.binary_cross_entropy\n\n```", "```py\n\nprint(get_loss(torch.tensor([1.]), torch.tensor([1.])))\n\n```", "```py\n\ntr_loader = DataLoader(tr_data, batch_size = 100, shuffle = True)\n\n```", "```py\n\nopt = torch.optim.Adam(model.parameters(), lr = 0.005)\n\n```", "```py\n\ndef run_epoch(opt, tr_loader): \n\n    # Iterate through the batches. \n\n    for data in iter(tr_loader): \n\n        x, y = data # Get the data in the batch. \n\n        opt.zero_grad() # Reset the gradients. \n\n        # Compute gradients. \n\n        loss = get_loss(model(x), y) \n\n        loss.backward() \n\n        opt.step() # Update the weights. \n\n    return get_loss(model(tr_data.x), tr_data.y)\n\n```", "```py\n\na = torch.tensor([2.], requires_grad = True) \n\nb = torch.tensor([3.], requires_grad = True)\n\n```", "```py\n\nf = a**2 + b \n\nf.backward()\n\n```", "```py\n\ntr_losses = [] \n\nval_losses = [] \n\nwhile (len(val_losses) < 2 or val_losses[-1] < val_losses[-2]): \n\n    print(\"EPOCH\", len(tr_losses) + 1, end = \" \") \n\n    tr_losses.append(float(run_epoch(opt, tr_loader))) \n\n    # ^^ Remember that run_epoch returns the training loss. \n\n    val_losses.append(float( \n\n        get_loss(model(val_data.x), val_data.y))) \n\n    print(\"| Train loss:\", round(tr_losses[-1], 4), end = \" \") \n\n    print(\"| Valid loss:\", round(val_losses[-1], 4))\n\n```", "```py\n\nEPOCH 1 | Train loss: 0.6727 | Valid loss: 0.6527 \nEPOCH 2 | Train loss: 0.638 | Valid loss: 0.6315 \nEPOCH 3 | Train loss: 0.5861 | Valid loss: 0.5929 \nEPOCH 4 | Train loss: 0.5129 | Valid loss: 0.5277 \nEPOCH 5 | Train loss: 0.4244 | Valid loss: 0.4428 \nEPOCH 6 | Train loss: 0.3382 | Valid loss: 0.3633 \nEPOCH 7 | Train loss: 0.2673 | Valid loss: 0.3024 \nEPOCH 8 | Train loss: 0.2198 | Valid loss: 0.2734 \nEPOCH 9 | Train loss: 0.1938 | Valid loss: 0.2622 \nEPOCH 10 | Train loss: 0.1819 | Valid loss: 0.2616 \nEPOCH 11 | Train loss: 0.1769 | Valid loss: 0.2687\n\n```", "```py\n\nimport matplotlib.pyplot as plt \n\ndef plot_losses(tr_loss, val_loss): \n\n    epochs = np.array(range(len(tr_loss))) + 1 \n\n    plt.plot(epochs, tr_loss, label = \"Training loss\") \n\n    plt.plot(epochs, val_loss, label = \"Validation loss\") \n\n    plt.xlabel(\"Epoch\") \n\n    plt.legend() \n\n    plt.show() \n\nplot_losses(tr_losses, val_losses)\n\n```", "```py\n\ntrain_acc = accuracy_score( \n\n    (model(tr_data.x) >= 0.5).to(float), tr_data.y) \n\nval_acc = accuracy_score( \n\n    (model(val_data.x) >= 0.5).to(float), val_data.y) \n\ntest_acc = accuracy_score( \n\n    (model(test_data.x) >= 0.5).to(float), test_data.y) \n\nprint(\"Training accuracy:\", train_acc) \n\nprint(\"Validation accuracy:\", val_acc) \n\nprint(\"Test accuracy:\", test_acc)\n\n```", "```py\n\nfrom qiskit import * \n\nfrom qiskit.circuit.library import ZZFeatureMap, TwoLocal\n\n```", "```py\n\nx, y = make_classification(n_samples = 500, n_features = 16) \n\nx_tr, x_test, y_tr, y_test = train_test_split(x, y, train_size = 0.8) \n\nx_val, x_test, y_val, y_test = train_test_split(x_test, y_test, train_size = 0.5) \n\ntr_data = NumpyDataset(x_tr, y_tr) \n\nval_data = NumpyDataset(x_val, y_val) \n\ntest_data = NumpyDataset(x_test, y_test) \n\ntr_loader = DataLoader(tr_data, batch_size = 20, shuffle = True)\n\n```", "```py\n\nzzfm = ZZFeatureMap(2) \n\ntwolocal = TwoLocal(2, [’ry’,’rz’], ’cz’, ’linear’, reps = 1)\n\n```", "```py\n\nfrom qiskit_machine_learning.neural_networks import TwoLayerQNN\n\n```", "```py\n\nfrom qiskit_machine_learning.connectors import TorchConnector \n\nfrom qiskit.providers.aer import AerSimulator \n\nclass HybridQNN(nn.Module): \n\n    def __init__(self): \n\n        # Initialize super class. \n\n        super(HybridQNN, self).__init__() \n\n        # Declare the layers that we will use. \n\n        qnn = TwoLayerQNN(2, zzfm, twolocal, input_gradients = True, \n\n            quantum_instance = AerSimulator(method=\"statevector\")) \n\n        self.layer1 = nn.Linear(16, 2) \n\n        self.qnn = TorchConnector(qnn) \n\n        self.final_layer = nn.Linear(1,1) \n\n    def forward(self, x): \n\n        x = torch.sigmoid(self.layer1(x)) \n\n        x = self.qnn(x) \n\n        x = torch.sigmoid(self.final_layer(x)) \n\n        return x \n\nmodel = HybridQNN()\n\n```", "```py\n\nopt = torch.optim.Adam(model.parameters(), lr = 0.005)\n\n```", "```py\n\ntr_losses = [] \n\nval_losses = [] \n\nwhile (len(val_losses) < 2 or val_losses[-1] < val_losses[-2]): \n\n    print(\"EPOCH\", len(tr_losses) + 1, end = \" \") \n\n    tr_losses.append(float(run_epoch(opt, tr_loader))) \n\n    val_losses.append(float(get_loss(model(val_data.x), val_data.y))) \n\n    print(\"| Train loss:\", round(tr_losses[-1], 4), end = \" \") \n\n    print(\"| Valid loss:\", round(val_losses[-1], 4))\n\n```", "```py\n\nEPOCH 1 | Train loss: 0.6908 | Valid loss: 0.696 \nEPOCH 2 | Train loss: 0.6872 | Valid loss: 0.691 \nEPOCH 3 | Train loss: 0.6756 | Valid loss: 0.6811 \nEPOCH 4 | Train loss: 0.6388 | Valid loss: 0.6455 \nEPOCH 5 | Train loss: 0.5661 | Valid loss: 0.5837 \nEPOCH 6 | Train loss: 0.5099 | Valid loss: 0.5424 \nEPOCH 7 | Train loss: 0.4692 | Valid loss: 0.5201 \nEPOCH 8 | Train loss: 0.4425 | Valid loss: 0.5014 \nEPOCH 9 | Train loss: 0.4204 | Valid loss: 0.4947 \nEPOCH 10 | Train loss: 0.4019 | Valid loss: 0.4923 \nEPOCH 11 | Train loss: 0.3862 | Valid loss: 0.4774 \nEPOCH 12 | Train loss: 0.3716 | Valid loss: 0.4668 \nEPOCH 13 | Train loss: 0.3575 | Valid loss: 0.451 \nEPOCH 14 | Train loss: 0.3446 | Valid loss: 0.4349 \nEPOCH 15 | Train loss: 0.3332 | Valid loss: 0.4323 \nEPOCH 16 | Train loss: 0.3229 | Valid loss: 0.4259 \nEPOCH 17 | Train loss: 0.3141 | Valid loss: 0.4253 \nEPOCH 18 | Train loss: 0.3055 | Valid loss: 0.422 \nEPOCH 19 | Train loss: 0.2997 | Valid loss: 0.4152 \nEPOCH 20 | Train loss: 0.2954 | Valid loss: 0.4211\n\n```", "```py\n\nplot_losses(tr_losses, val_losses)\n\n```", "```py\n\ntr_acc = accuracy_score( \n\n    (model(tr_data.x) >= 0.5).to(float), tr_data.y) \n\nval_acc = accuracy_score( \n\n    (model(val_data.x) >= 0.5).to(float), val_data.y) \n\ntest_acc = accuracy_score( \n\n    (model(test_data.x) >= 0.5).to(float), test_data.y) \n\nprint(\"Training accuracy:\", tr_acc) \n\nprint(\"Validation accuracy:\", val_acc) \n\nprint(\"Test accuracy:\", test_acc)\n\n```", "```py\n\nfrom qiskit.providers.ibmq import * \n\nprovider = IBMQ.load_account() \n\ndev_list = provider.backends( \n\n    filters = lambda x: x.configuration().n_qubits >= 4, \n\n                        simulator = False) \n\ndev = least_busy(dev_list)\n\n```", "```py\n\nclass QiskitQNN(nn.Module): \n\n    def __init__(self): \n\n        super(QiskitQNN, self).__init__() \n\n        qnn = TwoLayerQNN(2, zzfm, twolocal, input_gradients = True) \n\n        self.qnn = TorchConnector(qnn) \n\n    def forward(self, x): \n\n        x = self.qnn(x) \n\n        return x \n\nmodel = QiskitQNN()\n\n```", "```py\n\nx, y = make_classification(n_samples = 100, n_features = 2, \n\n    n_clusters_per_class = 1, n_informative = 1, n_redundant = 1) \n\nx_tr, x_test, y_tr, y_test = train_test_split(x, y, train_size = 0.8) \n\nx_val, x_test, y_val, y_test = train_test_split(x_test, y_test, \n\n    train_size = 0.5) \n\ntr_data = NumpyDataset(x_tr, y_tr) \n\nval_data = NumpyDataset(x_val, y_val) \n\ntest_data = NumpyDataset(x_test, y_test)\n\n```", "```py\n\ntr_data.y = 2 * (tr_data.y - 1/2) \n\nval_data.y = 2 * (val_data.y - 1/2) \n\ntest_data.y = 2 * (test_data.y - 1/2)\n\n```", "```py\n\ntr_loader = DataLoader(tr_data, batch_size = 20, shuffle = True) \n\nval_loader = DataLoader(val_data) \n\ntest_loader = DataLoader(test_data)\n\n```", "```py\n\nget_loss = F.mse_loss \n\nopt = torch.optim.Adam(model.parameters(), lr = 0.005)\n\n```", "```py\n\nfrom qiskit_machine_learning.runtime import TorchRuntimeClient \n\nclient = TorchRuntimeClient(provider = provider, backend = dev, \n\n    model = model, optimizer = opt, loss_func = get_loss, \n\n    epochs = 5)\n\n```", "```py\n\nresult = client.fit(train_loader = tr_loader, val_loader = val_loader)\n\n```", "```py\n\npred = client.predict(test_loader).prediction\n\n```", "```py\n\nfrom qiskit_ibm_runtime import QiskitRuntimeService,Session,Sampler,Options \n\nfrom qiskit_machine_learning.algorithms.classifiers import VQC \n\n# channel = \"ibmq_quantum\" gives us access to IBM’s quantum computers. \n\nservice = QiskitRuntimeService(channel = \"ibm_quantum\", token = \"TOKEN\") \n\nwith Session(service = service, backend = \"ibmq_lima\"): \n\n    sampler = Sampler() \n\n    vqc = VQC(sampler = sampler, num_qubits = 2) \n\n    vqc.fit(x_tr, y_tr)\n\n```"]