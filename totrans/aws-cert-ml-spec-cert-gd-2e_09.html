<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer153">
			<p><a id="_idTextAnchor1221"/><a id="_idTextAnchor1222"/></p>
			<h1 class="chapter-number"><a id="_idTextAnchor1223"/>9</h1>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor1224"/>Amazon SageMaker Modeling</h1>
			<p>In the previous chapter, you learned several methods of model optimization and evaluation techniques. You also learned various ways of storing data, processing data, and applying different statistical approaches to data. So, how can you now build a pipeline for this? Well, you can read data, process data, and build <strong class="bold">machine learning (ML)</strong> models on the processed data. But what if my first ML model does not perform well? Can I fine-tune my model? The answer is <em class="italic">yes</em>; you can do nearly everything using Amazon SageMaker. In this chapter, you will walk you through the following topics using <span class="No-Break">Amazon SageMaker:</span></p>
			<ul>
				<li>Understanding different instances of <span class="No-Break">Amazon SageMaker</span></li>
				<li>Cleaning and preparing data in Jupyter Notebook in <span class="No-Break">Amazon SageMaker</span></li>
				<li>Model training in <span class="No-Break">Amazon SageMaker</span></li>
				<li>Using SageMaker’s built-in <span class="No-Break">ML algorithms</span></li>
				<li>Writing custom training and inference code <span class="No-Break">in SageMaker</span></li>
			</ul>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor1225"/><a id="_idTextAnchor1226"/>Technical requirements</h1>
			<p>You can download the data used in this chapter’s examples from GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter09"><span class="No-Break">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter09</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor1227"/><a id="_idTextAnchor1228"/>Creating notebooks in Amazon SageMaker</h1>
			<p>If you are working <a id="_idTextAnchor1229"/>with ML, then <a id="_idTextAnchor1230"/>you need to perform actions such as storing data, processing data, preparing data for model training, model training, and deploying the model for inference. They are complex, and each of these stages requires a machine to perform the task. With Amazon SageMaker, life becomes much easier when carrying out <span class="No-Break">these tasks.</span></p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor1231"/><a id="_idTextAnchor1232"/>What is Amazon SageMaker?</h2>
			<p>SageMaker provides training instances to train a model using the data and provides endpoint <a id="_idTextAnchor1233"/>instances to infer by using the model. It also provides notebook instances running on the Jupyter Notebook to clean and understand the data. If you are happy with your cleaning process, then you should store the cleaned data in S3 as part of the staging for training. You can launch training instances to consume this training data and produce an ML model. The ML model can be stored in S3, and endpoint instances can consume the model to produce results for <span class="No-Break">end users.</span></p>
			<p>If you draw this in a block diagram, then it will look similar to <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B21197_09_01.jpg" alt="Figure 9.1 – A pictorial representation of the different layers of the Amazon SageMaker instances" width="1093" height="356"/>
				</div>
			</div>
			<p class="IMG---Figure"><a id="_idTextAnchor1234"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – A pictorial representation of the different layers of the Amazon SageMaker instances</p>
			<p>Now, you will take a look at the Amazon SageMaker console and get a better feel for it. Once you log in to your AWS account and go to Amazon SageMaker, you will see something similar to <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B21197_09_02.jpg" alt="Figure 9.2 – A quick look at the SageMaker console" width="999" height="717"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor1235"/>Figure 9.2 – A quick look at the SageMaker console</p>
			<p>There are three different sections in the menu on the left, labeled <strong class="bold">Notebook</strong>, <strong class="bold">Training</strong>, and <strong class="bold">Inference</strong>, that have been expanded in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em> so that you can dive in and understand <span class="No-Break">them better.</span></p>
			<p><strong class="bold">Notebook</strong> has three di<a id="_idTextAnchor1236"/>fferent options that you <span class="No-Break">can use:</span></p>
			<ul>
				<li><strong class="bold">Notebook instances</strong>: This helps you create, open, start, and stop notebook instances. These in<a id="_idTextAnchor1237"/>stances are responsible for running Jupyter Notebooks. They allow you to choose the instance type based on the workload of the use case. The best practice is to use a notebook instance to orchestrate the data pipeline for processing a large dataset. For example, making a call from a notebook instance to AWS Glue for ETL services or Amazon EMR to run Spark applications. If you are asked to create a secure notebook instance outside AWS, then you need to take care of endpoint security, network security, launching the machine, managing storage on it, and managing Jupyter Notebook applications running on the instance. The user does not need to manage any of these <span class="No-Break">with SageMaker.</span></li>
				<li><strong class="bold">Lifecycle configurations</strong>: This is useful when there is a use case that requires a <a id="_idTextAnchor1238"/>different library, which is not available in the notebook instances. To install the library, the user will do either <strong class="source-inline">pip install</strong> or <strong class="source-inline">conda install</strong>. However, as soon as the notebook instance is terminated, the customization will be lost. To avoid su<a id="_idTextAnchor1239"/>ch a scenario, you can customize your notebook instance through a script provided through <strong class="bold">Lifecycle configurations</strong>. You can choose any of the environments present in <strong class="source-inline">/home/ec2-user/anaconda3/envs/</strong> and customize the specific environment <span class="No-Break">as required.</span></li>
				<li><strong class="bold">Git repositories</strong>: AWS CodeCommit, GitHub, or any other Git server can be associated wi<a id="_idTextAnchor1240"/>th the notebook instance for the persistence of your notebooks. If access is given, then the same notebook can be used by other developers to collaborate and save code in a source-control fashion. Git repositories can either be added separately using this option or they can be associated with a notebook instance during <span class="No-Break">the creation.</span></li>
			</ul>
			<p>As you ca<a id="_idTextAnchor1241"/>n see in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em>, <strong class="bold">Training</strong> offers <strong class="bold">Algorithms</strong>, <strong class="bold">Training</strong> <strong class="bold">jobs</strong>, and <strong class="bold">Hyperparameter</strong> <strong class="bold">tuning</strong> <strong class="bold">jobs</strong>. Let’s understand <span class="No-Break">their usage:</span></p>
			<ul>
				<li><strong class="bold">Algorithms</strong>: This is the first step toward deciding on an algorithm that you are going to<a id="_idTextAnchor1242"/> run on our cleaned data. You can either choose a custom algorithm or create a custom algorithm based on the use case. Otherwise, you can run SageMaker algorithms on the <span class="No-Break">cleaned data.</span></li>
				<li><strong class="bold">Training jobs</strong>: You can create training jobs from a notebook instance via API calls. You ca<a id="_idTextAnchor1243"/>n set the number of instances, input the data source details, perform checkpoint configuration, and output data configuration. Amazon SageMaker manages the training instances and stores the model artifacts as output in the specified location. Both in<a id="_idTextAnchor1244"/>cremental training (that is, to train the model from time to time for better results) and managed spot training (that is, to reduce costs) can also <span class="No-Break">be achieved.</span></li>
				<li><strong class="bold">Hyperparameter tuning jobs</strong>: Usually, hyperparameters are set for an algorithm prior to the training process. During the training process, you let the algorithm fi<a id="_idTextAnchor1245"/>gure out the best values for these parameters. With hyperparameter tuning, you obtain the best model that has the best value of hyperparameters. This can be done through a console or via API calls. The same can be orchestrated from a notebook <span class="No-Break">instance too.</span></li>
			</ul>
			<p><strong class="bold">Inference</strong> has ma<a id="_idTextAnchor1246"/>ny offerings and is evolving <span class="No-Break">every day:</span></p>
			<ul>
				<li><strong class="bold">Compilation jobs</strong>: If your model is trained using an ML framework su<a id="_idTextAnchor1247"/>ch as Keras, MXNet, ONNX, PyTorch, TFLite, TensorFlow, or XGBoost, and your model artifacts ar<a id="_idTextAnchor1248"/>e available on a S3 bucket, then you can choose either <strong class="bold">Target device</strong> or <strong class="bold">Target platform</strong>. The Target device option is used to specify where you will deploy yo<a id="_idTextAnchor1249"/>ur model, such as an AWS SageMaker ML instance or an AWS IoT Greengrass device. The Target platform option is used to decide the operating system, architecture, and accelerator on which you want your model to run. You can also store the compiled module in your S3 bucket for future use. This essentially helps you in cross-platform <span class="No-Break">model deployment.</span></li>
				<li><strong class="bold">Model packages</strong>: These ar<a id="_idTextAnchor1250"/>e used to create deployable SageMaker models. You can create your own algorithm, package it using the model package APIs, and publish it to <span class="No-Break">AWS Marketplace.</span></li>
				<li><strong class="bold">Models</strong>: Models are created using model artifacts. They are similar to mathematical eq<a id="_idTextAnchor1251"/>uations with variables; that is, you input the values for the variables and get an output. These models are stored in S3 and will be used for inference by <span class="No-Break">the endpoints.</span></li>
				<li><strong class="bold">Endpoint configurations</strong>: Amazon SageMaker allows you to deploy multiple weighted mo<a id="_idTextAnchor1252"/>dels to a single endpoint. This means you can route a specific number of requests to one endpoint. <em class="italic">What does this mean?</em> Well, let’s say you have one model in use. You want to replace it with a new model. However, you cannot simply remove the first model that is already in use. In this scenario, you can use the <strong class="source-inline">VariantWeight</strong> API to make the endpoints serve 80% of the requests with the old model and 20% of the requests with the new model. This is the most common production scenario where the data changes rapidly and the model needs to be trained and tuned periodically. Another possible use case is to test the model results with live data, then a certain percentage of the requests can be routed to the new model, and the results can be monitored to ascertain the accuracy of the model on real-time <span class="No-Break">unseen data.</span></li>
				<li><strong class="bold">Endpoints</strong>: These are used to create a URL to which the model is exposed and ca<a id="_idTextAnchor1253"/>n be requested to give the model results as <span class="No-Break">a response.</span></li>
				<li><strong class="bold">Batch transform jobs</strong>: If the use case demands that you infer results on several mi<a id="_idTextAnchor1254"/>llion records, then instead of individual inference jobs, you can run batch transform jobs on the unseen data. Note that there can be some confusion when you receive thousands of results from your model after parsing thousands of pieces of unseen data as a batch. To overcome this confusion, the <strong class="source-inline">InputFilter</strong>, <strong class="source-inline">JoinSource</strong>, and <strong class="source-inline">OutputFilter</strong> APIs can be used to associate input records with <span class="No-Break">output results.</span></li>
			</ul>
			<p>You have got an overview of Amazon SageMaker. Now, put your knowledge to work in the <span class="No-Break">next section.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The Amazon SageMaker console keeps changing. There’s a possibility that when you are reading this book, the console might <span class="No-Break">look different.</span></p>
			<h2 id="_idParaDest-224">Tr<a id="_idTextAnchor1255"/><a id="_idTextAnchor1256"/>aining Data Location and Formats</h2>
			<p>As you embark on the journey of setting up your AWS SageMaker training job, understanding the diverse data storage and reading options is crucial. To ensure a seamless training experience, delve into the supported options and <span class="No-Break">their benefits.</span></p>
			<p>First you will look at the <strong class="bold">supported data </strong><span class="No-Break"><strong class="bold">storage options</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="bold">Amazon Simple Storage Service (</strong><span class="No-Break"><strong class="bold">Amazon S3):</strong></span><ul><li><strong class="bold">Overview</strong>: Amazon SageMaker provides robust support for storing training datasets in Amazon S3, offering reliability <span class="No-Break">and scalability.</span></li><li><strong class="bold">Usage Example</strong>: You can configure your dataset using an Amazon S3 prefix, manifest file, or augmented <span class="No-Break">manifest file.</span></li></ul></li>
				<li><strong class="bold">Amazon Elastic File System (</strong><span class="No-Break"><strong class="bold">Amazon EFS):</strong></span><ul><li><strong class="bold">Overview</strong>: SageMaker extends its support to Amazon EFS, facilitating file system access to <span class="No-Break">the dataset.</span></li><li><strong class="bold">Usage Example</strong>: Data stored in Amazon EFS must be pre-existing before initiating the <span class="No-Break">training job.</span></li></ul></li>
				<li><strong class="bold">Amazon FSx </strong><span class="No-Break"><strong class="bold">for Lustre:</strong></span><ul><li><strong class="bold">Overview</strong>: Achieving high throughput and low-latency file retrieval, SageMaker mounts the FSx for Lustre file system to the <span class="No-Break">training instance.</span></li><li><strong class="bold">Usage Example</strong>: FSx for Lustre can scale seamlessly, providing a performant option for your <span class="No-Break">training data.</span></li></ul></li>
			</ul>
			<p>Here are the <strong class="bold">input modes for </strong><span class="No-Break"><strong class="bold">data access:</strong></span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">File Mode:</strong></span><ul><li><strong class="bold">Overview</strong>: Default input mode where SageMaker downloads the entire dataset to the Docker container before <span class="No-Break">training starts.</span></li><li><strong class="bold">Usage Example</strong>: Compatible with SageMaker local mode and supports sharding for <span class="No-Break">distributed training.</span></li></ul></li>
				<li><strong class="bold">Fast </strong><span class="No-Break"><strong class="bold">File Mode:</strong></span><ul><li><strong class="bold">Overview</strong>: Combining file system access with the efficiency of pipe mode, fast file mode identifies data files at the start but delays the download <span class="No-Break">until necessary.</span></li><li><strong class="bold">Usage Example</strong>: Streamlines training startup time, particularly beneficial when dealing with a <span class="No-Break">large dataset.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Pipe Mode:</strong></span><ul><li><strong class="bold">Overview</strong>: Streams data directly from an Amazon S3 data source, providing faster start times and <span class="No-Break">better throughput.</span></li><li><strong class="bold">Usage Example</strong>: Historically used, but largely replaced by the simpler-to-use fast <span class="No-Break">file mode.</span></li></ul></li>
			</ul>
			<p>And lastly, look at the <strong class="bold">specialized </strong><span class="No-Break"><strong class="bold">storage classes</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="bold">Amazon S3 Express </strong><span class="No-Break"><strong class="bold">One Zone:</strong></span><ul><li><strong class="bold">Overview</strong>: A high-performance, single Availability Zone storage class, optimizing compute performance <span class="No-Break">and costs.</span></li><li><strong class="bold">Usage Example</strong>: Supports file mode, fast file mode, and pipe mode for SageMaker <span class="No-Break">model training.</span></li></ul></li>
				<li><strong class="bold">Amazon EFS and Amazon FSx </strong><span class="No-Break"><strong class="bold">for Lustre:</strong></span><ul><li><strong class="bold">Overview</strong>: SageMaker supports both Amazon EFS and Amazon FSx for Lustre, offering flexibility in choosing the right storage solution for your <span class="No-Break">training data.</span></li><li><strong class="bold">Usage Example</strong>: Mounting the file systems to the training instance ensures seamless access <span class="No-Break">during training.</span></li></ul></li>
			</ul>
			<p>Understanding the nuances of data storage and reading options for AWS SageMaker training jobs empowers you to tailor your setup to specific requirements. In the upcoming sections, you’ll explore more facets of AWS SageMaker to deepen your understanding and proficiency in machine learning workflows. Let’s put our knowledge to work in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor1257"/>Getting hands-on with Amazon SageMaker notebook instances</h2>
			<p>The very fi<a id="_idTextAnchor1258"/>rst step, in this section, is to create a Jupyter Notebook, and this requires a notebook instance. You can start by creating a notebook instance, <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Sign in to your <span class="No-Break">AWS account.</span></li>
				<li>Navigate to <strong class="source-inline">Services &gt; </strong><span class="No-Break"><strong class="source-inline">Amazon SageMaker</strong></span><span class="No-Break">.</span></li>
				<li>In the left navigation pane, click on <strong class="bold">Notebook instances</strong> and then click on the <strong class="bold">Create notebook </strong><span class="No-Break"><strong class="bold">instance</strong></span><span class="No-Break"> button.</span></li>
				<li>Provide a <strong class="bold">Notebook instance name</strong> value such as <strong class="source-inline">notebookinstance</strong> and leave the <strong class="bold">Notebook instance type</strong> at its default <strong class="source-inline">ml.t2.medium</strong> setting. In the <strong class="bold">Permissions and encryption</strong> section, select <strong class="source-inline">Create a new role</strong> in <strong class="bold">IAM role</strong>. You will be asked to specify the bucket name. For the purpose of this example, it’s chosen as <span class="No-Break">any bucket.</span></li>
				<li>Following the successful creation of a role, you should see something similar to <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B21197_09_03.jpg" alt="Figure 9.3 – Amazon SageMaker role creation" width="1099" height="344"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">F<a id="_idTextAnchor1259"/>igure 9.3 – Amazon SageMaker role creation</p>
			<ol>
				<li value="6">Leave everything else on their default settings and click on the <strong class="bold">Create notebook </strong><span class="No-Break"><strong class="bold">instance</strong></span><span class="No-Break"> button.</span></li>
				<li>Once the instance is in the <strong class="source-inline">InService</strong> state, select the instance. Click on the <strong class="bold">Actions</strong> drop-down menu and choose <strong class="bold">Open Jupyter</strong>. This opens your <span class="No-Break">Jupyter Notebook.</span></li>
				<li>Now, you are all<a id="_idTextAnchor1260"/> set to run our Jupyter Notebook on the newly created instance. You will perform <strong class="bold">Exploratory Data Analysis (EDA) </strong>and plot different types of graphs to visualize the data. Once you are familiar with the Jupyter Notebook, you will build some models to predict house prices in Boston. You will apply the algorithms that you have learned in previous chapters and compare them to find the best model that offers the best prediction according to our data. Let’s <span class="No-Break">dive in.</span></li>
				<li>In the Jup<a id="_idTextAnchor1261"/>yter Notebook, click on <strong class="bold">New</strong> and select <strong class="bold">Terminal</strong>. Run the following commands in Command Prompt to download the code to <span class="No-Break">the instance:</span><pre class="source-code"><strong class="bold">sh-4.2$ cd ~/SageMaker/</strong></pre><pre class="source-code"><strong class="bold">sh-4.2$ git clone https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition.git</strong></pre></li>
			</ol>
			<ol>
				<li value="10">Once the Git repository is cloned to the SageMaker notebook instance, type <strong class="source-inline">exit</strong> into Command Prompt to quit. Now, your code is ready <span class="No-Break">to execute.</span></li>
				<li>Navigate to <strong class="source-inline">Chapter-9</strong> in the Jupyter Notebook’s Files section, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B21197_09_04.jpg" alt="Figure 9.4 – Jupyter Notebook" width="1289" height="791"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Fi<a id="_idTextAnchor1262"/>gure 9.4 – Jupyter Notebook</p>
			<ol>
				<li value="12">Click on the <a id="_idTextAnchor1263"/>first notebook in <strong class="source-inline">1.Boston-House-Price-SageMaker-Notebook-Instance-Example.ipynb</strong>. It will prompt you to choose the kernel for the notebook. Please select <strong class="source-inline">conda_python3</strong>, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B21197_09_05.jpg" alt="Figure 9.5 – Jupyter Notebook kernel selection" width="1113" height="741"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Fig<a id="_idTextAnchor1264"/>ure 9.5 – Jupyter Notebook kernel selection</p>
			<ol>
				<li value="13">From the notebook, navigate to <strong class="source-inline">Kernel &gt; Restart &amp; Clear Output</strong>. Click on the play icon to run the cells one after another. Please ensure you have run each individual cell and inspect the output from <span class="No-Break">each execution/run.</span></li>
				<li>You can exper<a id="_idTextAnchor1265"/>iment by adding cells and deleting cells to familiarize yourself with the Jupyter Notebook operations. In one of the paragraphs, there is a bash command that allows you to install the <strong class="source-inline">xgboost</strong> libraries from <span class="No-Break">the notebook.</span></li>
				<li>The final cell explains how you have compared the different scores of various modeling techniques to draw a conclusion mathematically. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.6</em> clearly shows that the best model for predicting house prices in Boston <span class="No-Break">is XGBoost:</span></li>
			</ol>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B21197_09_06.jpg" alt="Figure 9.6 – Comparing the models" width="1101" height="404"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figu<a id="_idTextAnchor1266"/>re 9.6 – Comparing the models</p>
			<ol>
				<li value="16">Once you’ve completed the execution of this notebook, please feel free to shut down the kernel and stop your notebook instance from the SageMaker console. This is a best practice to <span class="No-Break">reduce costs.</span></li>
			</ol>
			<p>In the next h<a id="_idTextAnchor1267"/>ands-on section, you will familiarize ourselves with Amazon SageMaker’s training and inference instances. You will also use the Amazon SageMaker API to make this process easier. You will use the same notebook instance as you did in the <span class="No-Break">previous example.</span></p>
			<h2 id="_idParaDest-226">Gettin<a id="_idTextAnchor1268"/><a id="_idTextAnchor1269"/>g hands-on with Amazon SageMaker’s training and inference instances</h2>
			<p>In this sectio<a id="_idTextAnchor1270"/>n, you will learn about traini<a id="_idTextAnchor1271"/>ng a model and hosting the model to generate its predicted results. Let’s dive in by using the notebook instance from the <span class="No-Break">previous example:</span></p>
			<ol>
				<li>Sign in to your AWS account <span class="No-Break">at </span><a href="https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances"><span class="No-Break">https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances</span></a><span class="No-Break">.</span></li>
				<li>Click on <strong class="bold">Start</strong> next to the instance that you created in the previous example, <strong class="source-inline">notebookinstance</strong>. Once the status moves to <strong class="source-inline">InService</strong>, open it in a new tab, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B21197_09_07.jpg" alt="Figure 9.7 – The InService instance" width="1031" height="364"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figur<a id="_idTextAnchor1272"/>e 9.7 – The InService instance</p>
			<ol>
				<li value="3">Navigate to the tab named <strong class="bold">SageMaker Examples</strong> from the Jupyter Notebook <span class="No-Break">home page.</span></li>
				<li>Select the <strong class="source-inline">k_n<a id="_idTextAnchor1273"/>earest_neighbors_covtype.ipynb</strong> notebook. Click on <strong class="bold">Use</strong> and create <span class="No-Break">a copy.</span></li>
				<li>When you run<a id="_idTextAnchor1274"/> the following code block, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.8</em>, you can also check a training job in <strong class="source-inline">Training &gt; Training jobs</strong> of the SageMaker <span class="No-Break">home page:</span></li>
			</ol>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B21197_09_08.jpg" alt="Figure 9.8 – The SageMaker fit API call" width="1099" height="476"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure<a id="_idTextAnchor1275"/> 9.8 – The SageMaker fit API call</p>
			<ol>
				<li value="6">The training job looks similar to <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.9</em>. It launches an ECS container in the backend and uses the IAM execution role created in the previous example to run the training job for <span class="No-Break">this request:</span></li>
			</ol>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B21197_09_09_New.jpg" alt="Figure 9.9 – Training jobs" width="1099" height="355"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure <a id="_idTextAnchor1276"/>9.9 – Training jobs</p>
			<ol>
				<li value="7">If you go inside and check the logs in CloudWatch, it gives you more details about the containers and the steps they performed. As an ML engineer, it’s worth going in and checking the CloudWatch metrics for <span class="No-Break">your algorithm.</span></li>
				<li>Now, if you run the f<a id="_idTextAnchor1277"/>ollowing paragraph<a id="_idTextAnchor1278"/>, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.10</em>, in the notebook, then it will create an endpoint configuration and an endpoint where the model from the earlier training job <span class="No-Break">is deployed.</span></li>
				<li>I have changed the instance type to reduce costs. It is the instance or the machine that will host your model. Please choose your instance wisely. You will learn about choosing instance types in the next section. I have also changed <strong class="source-inline">endpoint_name</strong> so that it can be <span class="No-Break">recognized easily:</span></li>
			</ol>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B21197_09_10.jpg" alt="Figure 9.10 – Creating the predictor object with endpoint details" width="1092" height="353"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9<a id="_idTextAnchor1279"/>.10 – Creating the predictor object with endpoint details</p>
			<ol>
				<li value="10">Navigate to <strong class="source-inline">Inference &gt; Endpoints</strong>. This will show you the endpoint that was created as a result of the previous paragraph’s execution. This endpoint has a configuration and can be navigated and traced through <strong class="source-inline">Inference &gt; </strong><span class="No-Break"><strong class="source-inline">Endpoint Configurations</strong></span><span class="No-Break">.</span></li>
				<li>If you view the <strong class="bold">Inference</strong> section in the notebook, you will notice that it uses the test data to pr<a id="_idTextAnchor1280"/>edict results. It uses the predic<a id="_idTextAnchor1281"/>tor object from the SageMaker API to make predictions. The predictor object contains the endpoint details, model name, and <span class="No-Break">instance type.</span></li>
				<li>The API call to the endpoint occurs in the <strong class="bold">Inference</strong> section and is authenticated via the IAM role with which the notebook instance is created. The same API calls can be traced through CloudWatch <span class="No-Break">invocation metrics.</span></li>
				<li>Finally, running the <strong class="source-inline">delete_endpoint</strong> method in the notebook will delete the endpoint. To delete the endpoint configurations, navigate to <strong class="source-inline">Inference &gt; Endpoint Configurations</strong> and select the configuration on the screen. Click on <strong class="source-inline">Actions &gt; Delete &gt; </strong><span class="No-Break"><strong class="source-inline">Delete</strong></span><span class="No-Break">.</span></li>
				<li>Now, please feel free to shut down the kernel and stop your notebook instance from the SageMaker console. This is a best practice to <span class="No-Break">reduce costs.</span></li>
			</ol>
			<p>In this section, you learned how to use the notebook instance, training instances, inference endpoints, and endpoint configurations to clean our data, train models, and generate predicted results from them. In the next section, you will learn about <span class="No-Break">model tuning.</span></p>
			<h1 id="_idParaDest-227">Model tuni<a id="_idTextAnchor1282"/><a id="_idTextAnchor1283"/>ng</h1>
			<p>In  <a href="B21197_07.xhtml#_idTextAnchor970"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><em class="italic">, Evaluating and Optimizing Models</em>, you learned many important concepts about mode<a id="_idTextAnchor1284"/>l tuning. Let’s now explore this topic from a <span class="No-Break">practical perspective.</span></p>
			<p>In order to tune a <a id="_idTextAnchor1285"/>model on SageMaker, you have to call <strong class="source-inline">create_hyper_parameter_tuning_job</strong> and pass the following <span class="No-Break">main parameters:</span></p>
			<ul>
				<li><strong class="source-inline">HyperParameterTuningJobName</strong>: This is the name of the tuning job. It is useful to track the training jobs that have been started on behalf of your <span class="No-Break">tuning job.</span></li>
				<li><strong class="source-inline">HyperParameterTuningJobConfig</strong>: Here, you can configure your tuning options. For example, which parameters you want to tune, the range of values for them, the type of optimization (such as random search or Bayesian search), the maximum number of training jobs you want to spin up, <span class="No-Break">and more.</span></li>
				<li><strong class="source-inline">TrainingJobDefinition</strong>: Here, you can configure your training job. For example, the data channels, the output location, the resource configurations, the evaluation metrics, and the <span class="No-Break">stop conditions.</span></li>
			</ul>
			<p>In SageMaker, the main metric that you want to use to evaluate the models to select the best one is known a<a id="_idTextAnchor1286"/>s an <span class="No-Break"><strong class="bold">objective metric</strong></span><span class="No-Break">.</span></p>
			<p>In the following example, you are configuring <strong class="source-inline">HyperParameterTuningJobConfig</strong> for a decision tree-based<a id="_idTextAnchor1287"/> algorithm. You want to check the best configuration for a <strong class="source-inline">max_depth</strong> hyperparameter, which is responsible for controlling the depth of <span class="No-Break">the tree.</span></p>
			<p>In <strong class="source-inline">IntegerParameterRanges</strong>, you have to specify <span class="No-Break">the following:</span></p>
			<ul>
				<li>The <span class="No-Break">hyperparameter name</span></li>
				<li>The minimum value that you want <span class="No-Break">to test</span></li>
				<li>The maximum value that you want <span class="No-Break">to test</span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">Each type of hyperparameter must fit in one of the parameter range sections, such as categorical, continuous, or <span class="No-Break">integer parameters.</span></p>
			<p>In <strong class="source-inline">ResourceLimits</strong>, you are specifying the number of training jobs along with the number of parallel jobs that you want to run. Remember that the goal of the tuning process is to execute ma<a id="_idTextAnchor1288"/>ny training jobs with different hyperparameter settings. This is so that the best one will be selected for the final model. That’s why you have to specify these training job <span class="No-Break">execution rules.</span></p>
			<p>You then set up our search strategy in <strong class="source-inline">Strategy</strong> and, finally, set up the objective function <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">HyperParameterTuningJobObjective</strong></span><span class="No-Break">:</span></p>
			<pre class="console"><strong class="source-inline">tuning_job_config = {</strong></pre>
			<pre class="console"><strong class="source-inline">    "</strong><strong class="source-inline">ParameterRanges": {</strong></pre>
			<pre class="console"><strong class="source-inline">      "CategoricalParameterRanges": [],</strong></pre>
			<pre class="console"><strong class="source-inline">      "ContinuousParameterRanges": [],</strong></pre>
			<pre class="console"><strong class="source-inline">      "IntegerParameterRanges": [</strong></pre>
			<pre class="console"><strong class="source-inline">        {</strong></pre>
			<pre class="console"><strong class="source-inline">          "MaxValue": "10",</strong></pre>
			<pre class="console"><strong class="source-inline">          "MinValue": "1",</strong></pre>
			<pre class="console"><strong class="source-inline">          "Name": "max_depth"</strong></pre>
			<pre class="console"><strong class="source-inline">        }</strong></pre>
			<pre class="console"><strong class="source-inline">      ]</strong></pre>
			<pre class="console"><strong class="source-inline">    },</strong></pre>
			<pre class="console"><strong class="source-inline">    "ResourceLimits": {</strong></pre>
			<pre class="console"><strong class="source-inline">      "MaxNumberOfTrainingJobs": 10,</strong></pre>
			<pre class="console"><strong class="source-inline">      "MaxParallelTrainingJobs": 2</strong></pre>
			<pre class="console"><strong class="source-inline">    },</strong></pre>
			<pre class="console"><strong class="source-inline">    "Strategy": "Bayesian",</strong></pre>
			<pre class="console"><strong class="source-inline">    "</strong><strong class="source-inline">HyperParameterTuningJobObjective": {</strong></pre>
			<pre class="console"><strong class="source-inline">      "MetricName": "validation:auc",</strong></pre>
			<pre class="console"><strong class="source-inline">      "Type": "Maximize"</strong></pre>
			<pre class="console"><strong class="source-inline">    }</strong></pre>
			<pre class="console"><strong class="source-inline">  }</strong></pre>
			<p>The second important configuration you need to set is <strong class="source-inline">TrainingJobDefinition</strong>. Here, you have to specify all the details regarding the training jobs that will be executed. One of the most important <a id="_idTextAnchor1289"/>settings is the <strong class="source-inline">TrainingImage</strong> setting, which refers to the container that will be started to execute the training processes. This container, as expected, must have your training <span class="No-Break">algorithm implemented.</span></p>
			<p>Here, you present an example<a id="_idTextAnchor1290"/> of a built-in algorithm, eXtreme Gradient Boosting, so that you can set the training image <span class="No-Break">as follows:</span></p>
			<pre class="console"><strong class="source-inline">training_image = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1')</strong></pre>
			<p>Then, you can go ahead and set your <span class="No-Break">training definitions:</span></p>
			<pre class="console"><strong class="source-inline">training_job_definition = {</strong></pre>
			<pre class="console"><strong class="source-inline">    "AlgorithmSpecification": {</strong></pre>
			<pre class="console"><strong class="source-inline">      "TrainingImage": training_image,</strong></pre>
			<pre class="console"><strong class="source-inline">      "TrainingInputMode": "File"</strong></pre>
			<pre class="console"><strong class="source-inline">    },</strong></pre>
			<p>Next, you have to specify the data input configuration, which is also known as the data channels. In the following section of code, you are setting up two data channels – train <span class="No-Break">and validation:</span></p>
			<pre class="console"><strong class="source-inline">    "InputDataConfig": [</strong></pre>
			<pre class="console"><strong class="source-inline">      {</strong></pre>
			<pre class="console"><strong class="source-inline">        "ChannelName": "train",</strong></pre>
			<pre class="console"><strong class="source-inline">        "CompressionType": "None",</strong></pre>
			<pre class="console"><strong class="source-inline">        "ContentType": "csv",</strong></pre>
			<pre class="console"><strong class="source-inline">        "DataSource": {</strong></pre>
			<pre class="console"><strong class="source-inline">          "S3DataSource": {</strong></pre>
			<pre class="console"><strong class="source-inline">            "S3DataDistributionType": "FullyReplicated",</strong></pre>
			<pre class="console"><strong class="source-inline">            "S3DataType": "S3Prefix",</strong></pre>
			<pre class="console"><strong class="source-inline">            "S3Uri": s3_input_train</strong></pre>
			<pre class="console"><strong class="source-inline">          }</strong></pre>
			<pre class="console"><strong class="source-inline">        }</strong></pre>
			<pre class="console"><strong class="source-inline">      },</strong></pre>
			<pre class="console"><strong class="source-inline">      {</strong></pre>
			<pre class="console"><strong class="source-inline">        "ChannelName": "validation",</strong></pre>
			<pre class="console"><strong class="source-inline">        "CompressionType": "None",</strong></pre>
			<pre class="console"><strong class="source-inline">        "ContentType": "csv",</strong></pre>
			<pre class="console"><strong class="source-inline">        "DataSource": {</strong></pre>
			<pre class="console"><strong class="source-inline">          "S3DataSource": {</strong></pre>
			<pre class="console"><strong class="source-inline">            "S3DataDistributionType": "FullyReplicated",</strong></pre>
			<pre class="console"><strong class="source-inline">            "S3DataType": "S3Prefix",</strong></pre>
			<pre class="console"><strong class="source-inline">            "S3Uri": s3_input_validation</strong></pre>
			<pre class="console"><strong class="source-inline">          }</strong></pre>
			<pre class="console"><strong class="source-inline">        }</strong></pre>
			<pre class="console"><strong class="source-inline">      }</strong></pre>
			<pre class="console"><strong class="source-inline">    ],</strong></pre>
			<p>You also need to specify where the results will <span class="No-Break">be stored:</span></p>
			<pre class="console"><strong class="source-inline">    "OutputDataConfig": {</strong></pre>
			<pre class="console"><strong class="source-inline">      "S3OutputPath": "s3://{}/{}/output".format(bucket,prefix)</strong></pre>
			<pre class="console"><strong class="source-inline">    },</strong></pre>
			<p>Finally, you set the resour<a id="_idTextAnchor1291"/>ce configurations, roles, static parameters, and stopping conditions. In the following section of code, you want to use two instances of type <strong class="source-inline">ml.c4.2xlarge</strong> with 10 GB <span class="No-Break">of storage:</span></p>
			<pre class="console">    <strong class="source-inline">"ResourceConfig": {</strong></pre>
			<pre class="console"><strong class="source-inline">      "InstanceCount": 2,</strong></pre>
			<pre class="console"><strong class="source-inline">      "InstanceType": "ml.c4.2xlarge",</strong></pre>
			<pre class="console"><strong class="source-inline">      "VolumeSizeInGB": 10</strong></pre>
			<pre class="console"><strong class="source-inline">    },</strong></pre>
			<pre class="console"><strong class="source-inline">    "RoleArn": &lt;&lt;your_role_name&gt;&gt;,</strong></pre>
			<pre class="console"><strong class="source-inline">    "StaticHyperParameters": {</strong></pre>
			<pre class="console"><strong class="source-inline">      "eval_metric": "auc",</strong></pre>
			<pre class="console"><strong class="source-inline">      "num_round": "100",</strong></pre>
			<pre class="console"><strong class="source-inline">      "objective": "binary:logistic",</strong></pre>
			<pre class="console"><strong class="source-inline">      "rate_drop": "0.3",</strong></pre>
			<pre class="console"><strong class="source-inline">      "tweedie_variance_power": "1.4"</strong></pre>
			<pre class="console"><strong class="source-inline">    },</strong></pre>
			<pre class="console"><strong class="source-inline">    "StoppingCondition": {</strong></pre>
			<pre class="console"><strong class="source-inline">      "MaxRuntimeInSeconds": 43200</strong></pre>
			<pre class="console"><strong class="source-inline">    }</strong></pre>
			<pre class="console"><strong class="source-inline">}</strong></pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">Please note that you are using other variables in this configuration file, <strong class="source-inline">bucket</strong> and <strong class="source-inline">prefix</strong>, which should be replaced by your bucket name and prefix key (if needed), respectively. You are also referring to <strong class="source-inline">s3_input_train</strong> and <strong class="source-inline">s3_input_validation</strong>, which are two variables that point to the train and validation datasets <span class="No-Break">in S3.</span></p>
			<p>Once you have set <a id="_idTextAnchor1292"/>your configurations, you can spin up the <span class="No-Break">tuning process:</span></p>
			<pre class="console"><strong class="source-inline">smclient.create_hyper_parameter_tuning_job(</strong></pre>
			<pre class="console"><strong class="source-inline">     HyperParameterTuningJobName = "my-tuning-example",</strong></pre>
			<pre class="console"><strong class="source-inline">     HyperParameterTuningJobConfig = tuning_job_config,</strong></pre>
			<pre class="console"><strong class="source-inline">     TrainingJobDefinition = training_job_definition</strong></pre>
			<pre class="console"><strong class="source-inline">)</strong></pre>
			<p>Next, let’s find out how to track the execution of <span class="No-Break">this process.</span></p>
			<h2 id="_idParaDest-228">Tracking y<a id="_idTextAnchor1293"/><a id="_idTextAnchor1294"/>our training jobs and selecting the best model</h2>
			<p>Once you have start<a id="_idTextAnchor1295"/>ed the tuning process, there are two additional steps that you might want to check: tracking the process of tuning and select<a id="_idTextAnchor1296"/>ing the winner model (that is, the one with the best set <span class="No-Break">of hyperparameters).</span></p>
			<p>In order to find your training jobs, you should go to the SageMaker console and navigate to Hyperparameter training jobs. You will then find a list of executed tuning jobs, <span class="No-Break">including yours:</span></p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B21197_09_11.jpg" alt="Figure 9.11 – Finding your tuning job" width="1650" height="265"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.<a id="_idTextAnchor1297"/>11 – Finding your tuning job</p>
			<p>If you access your tuning job, by clicking under its name, you will find a summary page, which includes the most relevant information regarding the tuning process. On the <strong class="bold">Training jobs</strong> tab, you will see all the training jobs that have <span class="No-Break">been executed:</span></p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B21197_09_12.jpg" alt="Figure 9.12 – Summary of the training jobs in the tuning process" width="1484" height="733"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1<a id="_idTextAnchor1298"/>2 – Summary of the training jobs in the tuning process</p>
			<p>Finally, if you click on the <strong class="bold">Best training job</strong> tab, you will find the best set of hyperparameters for your mod<a id="_idTextAnchor1299"/>el, including a handy button for c<a id="_idTextAnchor1300"/>reating a new model based on those best hyperparameters that have just <span class="No-Break">been found:</span></p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B21197_09_13_New.jpg" alt="Figure 9.13 – Finding the best set of hyperparameters" width="1176" height="216"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.13<a id="_idTextAnchor1301"/> – Finding the best set of hyperparameters</p>
			<p>As you can see, SageMaker is very intuitive, and once you know the main concepts behind model optimization, playing with SageMaker should be easier. Now, you understand how to use SageMaker for our specific needs. In the next section, you will explore how to select the instance type for various use cases and the security of <span class="No-Break">our notebooks.</span></p>
			<h1 id="_idParaDest-229">Choosing inst<a id="_idTextAnchor1302"/><a id="_idTextAnchor1303"/>ance types in Amazon SageMaker</h1>
			<p>SageMaker uses a pay-for-usage model. There is no minimum fee <span class="No-Break">for it.</span></p>
			<p>When you think about i<a id="_idTextAnchor1304"/>nstances on SageMaker, it all starts with a<a id="_idTextAnchor1305"/>n EC2 instance. This instance is responsible for all your processing. It’s a managed EC2 instance. These instances won’t show up in the EC2 console and cannot be SSHed either. The names of this instance type start <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">ml</strong></span><span class="No-Break">.</span></p>
			<p>SageMaker offers instances of the <span class="No-Break">following families:</span></p>
			<ul>
				<li>The <strong class="bold">t</strong> family: This is the burstable<a id="_idTextAnchor1306"/> CPU family. With this family, you get a balanced ratio of CPU and memory. This means that if you have a long-running training job, then you lose performance over time as you spend the CPU credits. If you have very small jobs, then they are cost-effective. For example, if you want a notebook instance to launch training jobs, then this family is the most appropriate <span class="No-Break">and cost-effective.</span></li>
				<li>The <strong class="bold">m</strong> family: In the previous fami<a id="_idTextAnchor1307"/>ly, you saw that CPU credits are consumed faster due to their burstable nature. If you have a long-running ML job that requires constant throughput, then this is the right family. It comes with a similar CPU and memory ratio as the <span class="No-Break"><strong class="bold">t</strong></span><span class="No-Break"> family.</span></li>
				<li>The <strong class="bold">r</strong> family: This is a memory-opti<a id="_idTextAnchor1308"/>mized family. <em class="italic">When do you need this?</em> Well, imagine a use case where you have to load the data in memory and do some data engineering on the data. In this scenario, you will require more memory and your job will <span class="No-Break">be memory-optimized.</span></li>
				<li>The <strong class="bold">c</strong> family: c-family instances are compute-optimized. This is a requirement for jobs that<a id="_idTextAnchor1309"/> need higher compute power and less memory to store the data. If you refer to the following table, c5.2x large has 8 vCPU and 16 GiB memory, which makes it compute-optimized with less memory. For example, if a use case needs to be tested on fewer records and it is compute savvy, then this instance family is the to-go option to get some sample records from a huge <strong class="bold">DataFrame</strong> and test <span class="No-Break">your algorithm.</span></li>
				<li>The <strong class="bold">p</strong> family: This is a GPU family that supports accelerated computing jobs such as training and inference<a id="_idTextAnchor1310"/>. Notably, <strong class="bold">p</strong>-family instances are ideal for handling large, distributed training jobs that result in less time required for training and are thus much more cost-effective. The p3/p3dn GPU compute instance can go up to 1 petaFLOP per second compute with up to 256 GB of GPU memory and 100 Gbps (gigabits) of networking with 8x NVIDIA v100 GPUs. They are highly optimized for training and are not fully utilized <span class="No-Break">for inference.</span></li>
				<li>The <strong class="bold">g</strong> family: For cost-effectiv<a id="_idTextAnchor1311"/>e, small-scale training jobs, <strong class="bold">g</strong>-family GPU instances are ideal. G4 has the lowest cost per inference for GPU instances. It uses T4 NVIDIA GPUs. The G4 GPU compute instance goes up to 520 TeraFLOPs of compute time with 8x NVIDIA T4 GPUs. This instance family is the best for <span class="No-Break">simple networks.</span></li>
			</ul>
			<p>In the following tab<a id="_idTextAnchor1312"/>le, you have a visual comparison between the CPU and memory ratio of 2x large instance types from <span class="No-Break">each family:</span></p>
			<table id="table001-8" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor1313"/></strong><span class="No-Break"><strong class="bold">t3.2x large</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">m5.2x large</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">r5.2x large</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">c5.2x large</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">p3.2x large</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">g4dn.2x large</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>8 vCPU, <span class="No-Break">32 GiB</span></p>
						</td>
						<td class="No-Table-Style">
							<p>8 vCPU, <span class="No-Break">32 GiB</span></p>
						</td>
						<td class="No-Table-Style">
							<p>8 vCPU, <span class="No-Break">64 GiB</span></p>
						</td>
						<td class="No-Table-Style">
							<p>8 vCPU, <span class="No-Break">16 GiB</span></p>
						</td>
						<td class="No-Table-Style">
							<p>8 vCPU, <span class="No-Break">61 GiB</span></p>
						</td>
						<td class="No-Table-Style">
							<p>8 vCPU, <span class="No-Break">32 GiB</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.1 – A table showing the CPU and memory ratio of different instance types</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">To remember this easily, you can think of t for Tiny, m for Medium, c for Compute, and p and g for GPU. The CPU-related family instance types are t, m, r, and c. The GPU-related family instance types are p <span class="No-Break">and g.</span></p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor1314"/><a id="_idTextAnchor1315"/>Choosing the right instance type for a training job</h2>
			<p>There is no rule of thumb to determine the instance type that you require. It changes based on the <a id="_idTextAnchor1316"/>size of the data, the complexity of the network, the ML algorithm in question, and several other factors such as time and cost. Asking the right questions will allow you to save money and make your <span class="No-Break">project cost-effective.</span></p>
			<p>If the deciding factor is <em class="italic">instance size</em>, then classifying the problem as one for CPUs or GPUs is the right step. Once that is done, then it is good to consider whether it could be multi-GPU or multi-CPU, answering the question about distributed training. This also solves your <em class="italic">instance count</em> factor. If it’s compute intensive, then it would be wise to check the memory <span class="No-Break">requirements too.</span></p>
			<p>The next deciding factor is the<em class="italic"> instance family</em>. The right question here is, <em class="italic">is the chosen instance optimized for time and cost?</em> In the previous step, you figured out whether the problem can be solved best by either a CPU or GPU, and this narrows down the selection process. Now, let’s learn about <span class="No-Break">inference jobs.</span></p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor1317"/><a id="_idTextAnchor1318"/>Choosing the right instance type for an inference job</h2>
			<p>The majority of the cost and complexity of ML in production is inference. Usually, inference <a id="_idTextAnchor1319"/>runs on a single input in real time. Inference jobs are usually less compute/memory-intensive. They have to be highly available as they run all the time and serve end-user requests or are integrated into a <span class="No-Break">wider application.</span></p>
			<p>You can choose any of the instance types that you learned about so far based on the given workload. Other <a id="_idTextAnchor1320"/>than that, AWS has <strong class="bold">Inf1</strong> and <strong class="bold">Elastic Inference</strong> type instances for inference. Elastic inference <a id="_idTextAnchor1321"/>allows you to attach a fraction of a GPU instance to any <span class="No-Break">CPU instance.</span></p>
			<p>Let’s look at an example where an application is integrated with inference jobs. In this case, the CPU and memory requirements for the application are different from the inference jobs’ CPU and memory requirements. For use cases such as this, you need to choose the right instance type and size. In such scenarios, it is good to have a separation between your application fleets and inference fleets. This might require some management. If such management is a problem for your requirement, then choose Elastic Inference, where the application and inference jobs can be colocated. This means that you can host multiple models on the same fleet, and you can load all of these different models on different accelerators in memory, and concurrent requests can <span class="No-Break">be served.</span></p>
			<p>It’s always <a id="_idTextAnchor1322"/>recommended that you run some examples in a lower environment before deciding on your instance types and family in the production environment. For Production environments, you need to manage your scalability configurations for your Amazon SageMaker hosted models. You will understand this in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor1323"/>Taking care of Scalability Configurations</h1>
			<p>To kickstart auto scaling for your model, you can take advantage of the SageMaker console, <strong class="bold">AWS Command Line Interface (AWS CLI)</strong>, or an <strong class="bold">AWS SDK</strong> through the <strong class="bold">Application Auto Scaling API</strong>. For those inclined towards the CLI or API, the process involves registering the model as a scalable target, defining the scaling policy, and then applying it. If you opt for the SageMaker console, simply navigate to <strong class="bold">Endpoints</strong> under <strong class="bold">Inference</strong> in the navigation pane, locate your model’s endpoint name, and choose it along with the variant name to activate <span class="No-Break">auto scaling.</span></p>
			<p>Let’s now dive into the intricacies of <span class="No-Break">scaling policies.</span></p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor1324"/>Scaling Policy Overview</h2>
			<p>Auto scaling is driven by scaling policies, which determine how instances are added or removed in response to varying workloads. Two options are at your disposal: target tracking and step <span class="No-Break">scaling policies.</span></p>
			<p>Target Tracking Scaling Policies: Our recommendation is to leverage target tracking scaling policies. Here, you select a CloudWatch metric and set a target value. Auto scaling takes care of creating and managing CloudWatch alarms, adjusting the number of instances to maintain the metric close to the specified target value. For instance, a scaling policy targeting the InvocationsPerInstance metric with a target value of 70 ensures the metric hovers around <span class="No-Break">that value.</span></p>
			<p>Step Scaling Policies: Step scaling is for advanced configurations, allowing you to specify instance deployment under specific conditions. However, for simplicity and full automation, target tracking scaling is preferred. Note that step scaling is managed exclusively through the AWS CLI or Application Auto <span class="No-Break">Scaling API.</span></p>
			<p>Creating a target tracking scaling policy involves specifying the metric, such as the average number of invocations per instance, and the target value, for example, 70 invocations per instance per minute. You have the flexibility to create target tracking scaling policies based on predefined or custom metrics. Cooldown periods, which prevent rapid capacity fluctuations, can also be <span class="No-Break">configured optionally.</span></p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor1325"/>Scale Based on a Schedule</h2>
			<p>Scheduled actions enable scaling activities at specific times, either as a one-time event or on a recurring schedule. These actions can work in tandem with your scaling policy, allowing dynamic decisions based on changing workloads. Scheduled scaling is managed exclusively through the AWS CLI or Application Auto <span class="No-Break">Scaling API.</span></p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor1326"/>Minimum and Maximum Scaling Limits</h2>
			<p>Before crafting a scaling policy, it’s essential to set minimum and maximum scaling limits. The minimum value, set to at least 1, represents the minimum number of instances, while the maximum value signifies the upper cap. SageMaker auto scaling adheres to these limits and automatically scales in to the minimum specified instances when traffic <span class="No-Break">becomes zero.</span></p>
			<p>You have three options to specify <span class="No-Break">these limits:</span></p>
			<ul>
				<li>Use the console to update the Minimum instance count and Maximum instance <span class="No-Break">count settings.</span></li>
				<li>Use the AWS CLI, including the --min-capacity and --max-capacity options with the <span class="No-Break">register-scalable-target command.</span></li>
				<li>Call the RegisterScalableTarget API, specifying the MinCapacity and <span class="No-Break">MaxCapacity parameters.</span></li>
			</ul>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor1327"/>Cooldown Period</h2>
			<p>The cooldown period is pivotal for preventing over-scaling during scale-in or scale-out activities. It slows down subsequent scaling actions until the period expires, safeguarding against rapid capacity fluctuations. You can configure the cooldown period within your <span class="No-Break">scaling policy.</span></p>
			<p>If not specified, the default cooldown period is 300 seconds for both scale-in and scale-out. Adjust this value based on your model’s traffic characteristics; consider increasing for frequent spikes or multiple scaling policies, and decrease if instances need to be <span class="No-Break">added swiftly.</span></p>
			<p>As you embark on optimizing your model’s scalability, keep these configurations in mind to ensure a seamless and cost-effective experience. In the next section, you will dive into and understand the different ways of securing our Amazon <span class="No-Break">SageMaker notebooks.</span></p>
			<h1 id="_idParaDest-237"><a id="_idTextAnchor1328"/><a id="_idTextAnchor1329"/>Securing SageMaker notebooks</h1>
			<p>If you are <a id="_idTextAnchor1330"/>reading this section of the chapter, then you have already learned how to use notebook instances, which type of training instances should be chosen, and how to configure and use endpoints. Now, let’s learn about securing those instances. The following aspects will help to secure <span class="No-Break">the instances:</span></p>
			<ul>
				<li><strong class="bold">Encryption</strong>: When you talk about securing something via encryption, you are talking about <a id="_idTextAnchor1331"/>safeguarding data. But what does this mean? It means protecting data at rest using encryption, protecting data in transit with encryption, and using KMS for better role separation and internet traffic privacy through TLS 1.2 encryption. SageMaker instances can be launched with encrypted volumes by using an AWS-managed KMS key. This helps you to secure the Jupyter Notebook server <span class="No-Break">by default.</span></li>
				<li><strong class="bold">Root access</strong>: When a user opens a shell terminal from the Jupyter Web UI, they <a id="_idTextAnchor1332"/>will be logged in as ec2-user, which is the default username in Amazon Linux. Now the user can run sudo to the root user. With root access, users can access and edit files. In many use cases, an administrator might not want data scientists to manage, control, or modify the system of the notebook server. This requires restrictions to be placed on the root access. This can be done by setting the <strong class="source-inline">RootAccess</strong> field to <strong class="source-inline">Disabled</strong> when you call <strong class="source-inline">CreateNotebookInstance</strong> or <strong class="source-inline">UpdateNotebookInstance</strong>. The data scientist will have access to their user space and can install Python packages. However, they cannot sudo into the root user and make changes to the <span class="No-Break">operating system.</span></li>
				<li><strong class="bold">IAM role</strong>: During the launch of a notebook instance, it is necessary to create an IAM role <a id="_idTextAnchor1333"/>for execution or to use an existing role for execution. This is used to launch the service-managed EC2 instance with an instance profile associated with the role. This role will restrict the API calls based on the policies attached to <span class="No-Break">this role.</span></li>
				<li><strong class="bold">VPC connection</strong>: When you launch a SageMaker notebook instance, by default, it gets created within the SageMaker service account, which has a service-managed VPC, and it will, by default, have access to the internet via an internet <a id="_idTextAnchor1334"/>gateway, and that gateway is managed by the service. If you are only dealing with AWS-related services, then it is recommended that you launch a SageMaker notebook instance in your VPC within a private subnet and with a well-customized security group. The AWS services can be invoked or used from this notebook instance via VPC endpoints attached to that VPC. The best practice is to control them via endpoint policies for better API controls. This enforces the restriction on data egress outside your VPC and secured environment. In order to capture all network traffic, you can turn on the VPC flow logs, which can be monitored and tracked <span class="No-Break">via CloudWatch.</span></li>
				<li><strong class="bold">Internet access</strong>: You can launch a Jupyter Notebook server without direct internet <a id="_idTextAnchor1335"/>access. It can be launched in a private subnet with a NAT or to access the internet through a virtual private gateway. To train and deploy inference containers, you can set the <strong class="source-inline">EnableNetworkIsolation</strong> parameter to <strong class="source-inline">True</strong> when you call <strong class="source-inline">CreateTrainingJob</strong>, <strong class="source-inline">CreateHyperParameterTuningJob</strong>, or <strong class="source-inline">CreateModel</strong>. Network isolation can be used along with the VPC, which ensures that containers cannot make any outbound <span class="No-Break">network calls.</span></li>
				<li><strong class="bold">Connecting a private network to your VPC</strong>: You can launch your SageMaker <a id="_idTextAnchor1336"/>notebook instance inside the private subnet of your VPC. This can access data from your private network by communicating with the private network, which can be done by connecting your private network to your VPC by using Amazon VPN or AWS <span class="No-Break">Direct Connect.</span></li>
			</ul>
			<p>In this <a id="_idTextAnchor1337"/>section, you learned several ways in which you can secure our SageMaker notebooks. In the next section, you will learn about <span class="No-Break">SageMaker Debugger.</span></p>
			<h1 id="_idParaDest-238"><a id="_idTextAnchor1338"/>SageMaker Debugger</h1>
			<p>In this section, you will learn about Amazon SageMaker Debugger, unraveling the intricacies of monitoring, profiling, and debugging ML <span class="No-Break">model training:</span></p>
			<ul>
				<li><strong class="bold">Monitoring and profiling</strong>: SageMaker Debugger captures model metrics and keeps a real-time eye on system resources during training, eliminating the need for additional code. It not only provides a window into the training process but empowers instant issue correction, expediting training and elevating <span class="No-Break">model quality.</span></li>
				<li><strong class="bold">Automatic detection and analysis</strong>: A true time-saver, Debugger automatically spots and notifies you of common training errors, such as oversized or undersized gradient values. Say goodbye to days of troubleshooting; Debugger reduces it to <span class="No-Break">mere hours.</span></li>
				<li><strong class="bold">Profiling capabilities</strong>: Venture into the realm of profiling with Debugger, which meticulously monitors system resource utilization metrics and allows you to profile training jobs. This involves collecting detailed metrics from your ML framework, identifying anomalies in resource usage, and swiftly <span class="No-Break">pinpointing bottlenecks.</span></li>
				<li><strong class="bold">Built-in analysis and actions</strong>: Debugger introduces built-in analysis rules that tirelessly examine the training data emitted, encompassing input, output, and transformations (tensors). But that’s not all—users have the freedom to craft custom rules, analyze specific conditions, and even dictate actions triggered by rule events, such as stopping training or <span class="No-Break">sending notifications.</span></li>
				<li><strong class="bold">Integration with SageMaker Studio</strong>: It is possible to visualize Debugger results seamlessly within SageMaker Studio, treating yourself to charts depicting CPU utilization, GPU activity, network usage, and more. There is also a heat map, offering a visual timeline of system <span class="No-Break">resource utilization.</span></li>
				<li><strong class="bold">Profiler output</strong>: Peek into profiling results, an exhaustive dossier on system resource usage covering GPU, CPU, network, memory, and I/O. It’s your one-stop shop for understanding the inner workings of your <span class="No-Break">training job.</span></li>
				<li><strong class="bold">Debugger insights and optimization</strong>: Beyond detection, Debugger evolves into an advisor, identifying issues in your training jobs, providing insights, and suggesting optimizations. Whether it’s tweaking the batch size or altering the distributed training strategy, Debugger guides you towards <span class="No-Break">optimal performance.</span></li>
				<li><strong class="bold">CloudWatch integration</strong>: Stay in the loop with Debugger’s integration with CloudWatch. Configure alerts for specific conditions and ensure you are always ahead of <span class="No-Break">potential hiccups.</span></li>
				<li><strong class="bold">Downloadable reports</strong>: Don’t miss a beat—download HTML reports summarizing Debugger’s insights and profiling results for thorough <span class="No-Break">offline analysis.</span></li>
			</ul>
			<p>In a nutshell, Amazon SageMaker Debugger emerges as a holistic toolkit, empowering you to monitor, profile, and debug your ML models with finesse. It’s not just a tool; it’s your ally in the journey to model optimization. In the next section, you will understand the usage of <span class="No-Break">SageMaker AutoPilot/AutoML.</span></p>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor1339"/>SageMaker Autopilot</h1>
			<p>ML model development has historically been a daunting task, demanding considerable expertise and time. Amazon SageMaker Autopilot emerges as a game-changer, simplifying this intricate process and transforming it into a <span class="No-Break">streamlined experience.</span></p>
			<p>Amazon SageMaker Autopilot presents a rich array of features to facilitate the development of <span class="No-Break">ML models:</span></p>
			<ul>
				<li><strong class="bold">Automatic model building</strong>: SageMaker Autopilot removes the complexities of constructing ML models by taking charge and automating the entire process with a simple mandate from the user: provide a tabular dataset and designate the target column <span class="No-Break">for prediction.</span></li>
				<li><strong class="bold">Data processing and enhancement</strong>: Autopilot seamlessly handles data preprocessing tasks, filling in missing data, offering statistical insights into dataset columns, and extracting valuable information from non-numeric columns. This guarantees that input data is finely tuned for <span class="No-Break">model training.</span></li>
				<li><strong class="bold">Problem type detection</strong>: Autopilot showcases intelligence by automatically detecting the problem type—whether it’s classification or regression—based on the characteristics of the <span class="No-Break">provided data.</span></li>
				<li><strong class="bold">Algorithm exploration and optimization</strong>: Users can explore a myriad of high-performing algorithms, with Autopilot efficiently training and optimizing hundreds of models to pinpoint the one that aligns best with the user’s requirements. The entire process is automated, lifting the burden off <span class="No-Break">the user.</span></li>
				<li><strong class="bold">Real-world examples</strong>: Picture a retail company aiming to predict customer purchasing behavior. With SageMaker Autopilot, the company inputs historical purchase data, designates the target variable (e.g., whether a customer makes a purchase or not), and Autopilot takes the reins, autonomously exploring and optimizing various ML models. This facilitates deploying a predictive model without the need for profound ML expertise. In another scenario, a financial institution assessing credit risk can leverage SageMaker Autopilot. By providing a dataset with customer information and credit history, and specifying the target variable (creditworthiness), the institution can harness Autopilot to automatically build, train, and optimize models for precise credit <span class="No-Break">risk prediction.</span></li>
				<li><strong class="bold">Model understanding and deployment</strong>: SageMaker Autopilot not only automates model creation but places a premium on interpretability. Users gain insights into how the generated models make predictions. The Amazon SageMaker Studio Notebook serves as a platform for accessing, refining, and recreating models, ensuring continuous <span class="No-Break">model enhancement.</span></li>
			</ul>
			<p>Amazon SageMaker Autopilot heralds a shift in the landscape of ML, making it accessible to a wider audience. By automating the heavy lifting of model development, Autopilot empowers users to focus on the strategic aspects of their business problems, liberating them from the intricacies of ML. As organizations embrace ML for decision-making, SageMaker Autopilot emerges as a revolutionary tool, unlocking the power of AI without the need for extensive data science expertise. In the next section, you will dive deeper into <span class="No-Break">model monitoring.</span></p>
			<h1 id="_idParaDest-240"><a id="_idTextAnchor1340"/>SageMaker Model Monitor</h1>
			<p>In the ever-evolving realm of ML, ensuring the reliability and robustness of models in real-world production settings is paramount. In this section, you will delve into the profound significance, practical applications, and potent features of Amazon SageMaker Model Monitor—an instrumental component tailored to tackle the challenge of model drift in live <span class="No-Break">production environments:</span></p>
			<ul>
				<li><strong class="bold">The essence of model monitoring</strong>: As ML models venture into real-world deployment, the ongoing degradation of their effectiveness—attributed to shifts in data distributions or alterations in user behavior—poses a substantial threat known as model drift. Continuous monitoring becomes the linchpin for proactively identifying and rectifying these deviations, safeguarding the accuracy and reliability of ML predictions and, consequently, <span class="No-Break">business outcomes.</span></li>
				<li><strong class="bold">An automated guardian</strong>: Amazon SageMaker Model Monitor emerges as a guiding light in the ML landscape, delivering an automated solution for the continual vigilance of ML models in production. From detecting data drift to ensuring model quality, it presents a comprehensive suite to meet the challenges posed by the ever-evolving nature of <span class="No-Break">real-world data.</span></li>
				<li><strong class="bold">Automated analysis</strong>: Model Monitor takes the reins of model analysis, automating the inspection of deployed models based on predefined or user-provided rules at regular intervals. This relieves users from the burden of constructing <span class="No-Break">custom tooling.</span></li>
				<li><strong class="bold">Statistical rules</strong>: With built-in statistical rules, Model Monitor spans a spectrum of potential issues, covering outliers, completeness, and drift in data distributions. These rules empower the system to pinpoint anomalies and deviations from the anticipated <span class="No-Break">model behavior.</span></li>
				<li><strong class="bold">CloudWatch integration</strong>: Seamlessly integrating with Amazon CloudWatch, Model Monitor emits metrics when rule violations occur. Users can set up alarms based on these metrics, ensuring prompt notification and allowing <span class="No-Break">timely intervention.</span></li>
				<li><strong class="bold">Data drift monitoring</strong>: Excelling in identifying changes in data distributions, Model Monitor provides insights into how input data evolves over time. Whether it’s a shift in units or a sudden influx of null values, Model Monitor <span class="No-Break">remains vigilant.</span></li>
				<li><strong class="bold">Model quality monitoring</strong>: Beyond data drift, the system monitors the performance of the model itself. Degradation in model accuracy triggers alerts, notifying users of potential issues that might impact the model’s <span class="No-Break">predictive capabilities.</span></li>
			</ul>
			<p>Amazon SageMaker Model Monitor orchestrates a seamless end-to-end flow for deploying and monitoring models. From model deployment and data capture to baselining and continuous monitoring, the process ensures a comprehensive approach to maintaining model stability <span class="No-Break">over time.</span></p>
			<p>In the expansive landscape of ML, Amazon SageMaker Model Monitor stands as a guiding force, addressing the critical need for the continuous monitoring of models in production. Its automated analysis, integration with CloudWatch, and focus on both data and model quality drift make it an indispensable tool for organizations relying on ML for pivotal decision-making. As businesses increasingly depend on the stability and accuracy of ML models, SageMaker Model Monitor stands tall, offering a robust solution to the ever-evolving challenges of the ML landscape. In the next section, you will learn about making our SageMaker training process faster with <span class="No-Break">Training Compiler.</span></p>
			<h1 id="_idParaDest-241"><a id="_idTextAnchor1341"/>SageMaker Training Compiler</h1>
			<p>If you’ve reached this section, you are about to delve into the world of <strong class="bold">SageMaker Training Compiler (SMTC)</strong>, a game-changing tool designed to supercharge the training of your ML models on SageMaker by optimizing intricate training scripts. Picture this: faster training, swifter model development, and an open door to experimentation. That’s the primary goal of SMTC—improving training speed to bring agility to your model development journey. The following are the major advantages of <span class="No-Break">using SMTC:</span></p>
			<ul>
				<li><strong class="bold">Scaling challenges</strong>: Embarking on the journey of training large-scale models, especially those with billions of parameters, often feels like navigating uncharted engineering territory. SMTC, however, rises to the occasion by optimizing the entire training process, conquering the challenges that come <span class="No-Break">with scaling.</span></li>
				<li><strong class="bold">Efficiency at its core</strong>: SMTC takes the reins of GPU memory usage, ushering in a realm where larger batch sizes become not just a possibility but a reality. This optimization translates into accelerated training times, a boon for any data scientist seeking <span class="No-Break">efficiency gains.</span></li>
				<li><strong class="bold">Cost savings</strong>: Time is money, and in the realm of ML, it’s no different. By accelerating training jobs, SMTC isn’t just speeding up your models; it’s potentially reducing your costs. How? Well, you pay based on training time, and faster training means less time on <span class="No-Break">the clock.</span></li>
				<li><strong class="bold">Throughput improvement</strong>: The tool has demonstrated throughput improvements, leading to faster training without sacrificing <span class="No-Break">model accuracy.</span></li>
			</ul>
			<p>A couple of examples of efficiency, cost savings, autoscaling through SMTC in the scenarios/use cases of LLMs, and batch size optimization for NLP problems are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Large Language Models (LLMs)</strong>: SMTC is particularly beneficial for training LLMs, including BERT, DistilBERT, RoBERTa, and GPT-2. These models involve massive parameter sizes, making the scaling of training a <span class="No-Break">non-trivial task.</span></li>
				<li><strong class="bold">Batch size optimization</strong>: SMTC allows users to experiment with larger batch sizes, which is especially useful for tasks where efficiency gains can be achieved, such as in <strong class="bold">Natural Language Processing (NLP)</strong> or <span class="No-Break">computer vision.</span></li>
			</ul>
			<p>SageMaker Training Compiler is not just a black box; it’s a meticulous craftsman at work. It takes your deep learning models from their high-level language representation and transforms them into hardware-optimized instructions. This involves graph-level optimizations, dataflow-level optimizations, and backend optimizations, culminating in an optimized model that dances gracefully with hardware resources. The result? Faster training, thanks to the magic of compilation. In the next section, you will learn about Amazon SageMaker Data Wrangler—an integral component within SageMaker <span class="No-Break">Studio Classic.</span></p>
			<h1 id="_idParaDest-242"><a id="_idTextAnchor1342"/>SageMaker Data Wrangler</h1>
			<p>In this section, you’ll unravel the significance and benefits of Data Wrangler, dissecting its role as an end-to-end solution for importing, preparing, transforming, featurizing, and <span class="No-Break">analyzing data:</span></p>
			<ul>
				<li><strong class="bold">Importing data with ease</strong>: Data Wrangler simplifies the process of importing data from various sources, such as Amazon <strong class="bold">Simple Storage Service (S3)</strong>, Amazon Athena, Amazon Redshift, Snowflake, and Databricks. Whether your data resides in the cloud or within specific databases, Data Wrangler seamlessly connects to the source and imports it, setting the stage for comprehensive <span class="No-Break">data handling.</span></li>
				<li><strong class="bold">Constructing data flows</strong>: Picture a scenario where you can effortlessly design a data flow, mapping out a sequence of ML data preparation steps. This is where Data Wrangler shines. By combining datasets from diverse sources and specifying the transformations needed, you sculpt a data prep workflow ready to integrate into your <span class="No-Break">ML pipeline.</span></li>
				<li><strong class="bold">Transforming data with precision</strong>: Cleanse and transform your dataset with finesse using Data Wrangler. Standard transforms, such as those for string, vector, and numeric data formatting, are at your disposal. Dive deeper into feature engineering with specialized transforms like text and date/time embedding, along with <span class="No-Break">categorical encoding.</span></li>
				<li><strong class="bold">Gaining insights and ensuring data quality</strong>: Data integrity is paramount, and Data Wrangler acknowledges this with its <strong class="bold">Data Insights and Quality Report</strong> feature. This allows you to automatically verify data quality, identify abnormalities, and ensure your dataset meets the highest standards before it becomes the backbone of your <span class="No-Break">ML endeavors.</span></li>
				<li><strong class="bold">In-depth analysis made simple</strong>: Delve into the intricacies of your dataset at any juncture with Data Wrangler’s built-in visualization tools. From scatter plots to histograms, you can analyze features with ease. Data analysis tools like target leakage analysis and quick modeling can also be leveraged to comprehend feature correlation and make <span class="No-Break">informed decisions.</span></li>
				<li><strong class="bold">Seamless export for further experiments</strong>: Data preparation doesn’t end with Data Wrangler—it extends to the next phases of your workflow. Export your meticulously crafted data prep workflow to various destinations. Whether it’s an Amazon S3 bucket, SageMaker Model Building Pipelines for automated deployment, the SageMaker Feature Store for centralized storage, or a custom Python script for tailored workflows—Data Wrangler ensures your data is where you <span class="No-Break">need it.</span></li>
			</ul>
			<p>Amazon SageMaker Data Wrangler isn’t just a tool; it’s a powerhouse for simplifying and enhancing your data handling processes. The ability to seamlessly integrate with your ML workflows, the precision in transforming data, and the flexibility in exporting for further utilization make Data Wrangler a cornerstone of the SageMaker ecosystem. In the next section, you will learn about SageMaker Feature Store – an organized repository for storing, retrieving, and seamlessly sharing <span class="No-Break">ML features.</span></p>
			<h1 id="_idParaDest-243"><a id="_idTextAnchor1343"/>SageMaker Feature Store</h1>
			<p>Imagine you are building a recommendation system. In the absence of Feature Store, you’d navigate a landscape of manual feature engineering, scattered feature storage, and constant vigilance <span class="No-Break">for consistency.</span></p>
			<p>Feature management in an ML pipeline is challenging due to the dispersed nature of feature engineering, involving various teams and tools. Collaboration issues arise when different teams handle different aspects of feature storage, leading to inconsistencies and versioning problems. The dynamic nature of features evolving over time complicates change tracking and ensuring reproducibility. SageMaker Feature Store addresses these challenges by providing a centralized repository for features, enabling seamless sharing, versioning, and consistent access across the ML pipeline, thus simplifying collaboration, enhancing reproducibility, and promoting <span class="No-Break">data consistency.</span></p>
			<p>Now, user data, including age, location, browsing history, and item data such as category and price, have a unified home with Feature Store. Training and inference become a joyride, with easy access and sharing of these features, promoting efficiency and <span class="No-Break">unwavering consistency.</span></p>
			<p>To navigate the terrain of SageMaker Feature Store, let’s familiarize ourselves with some <span class="No-Break">key terms:</span></p>
			<ul>
				<li><strong class="bold">Feature store</strong>: At its core, a feature store is the storage and data management layer for ML features. It stands as the single source of truth, handling storage, retrieval, removal, tracking, sharing, discovery, and access control <span class="No-Break">for features.</span></li>
				<li><strong class="bold">Online</strong> <strong class="bold">store</strong>: This is the realm of low latency and high availability, allowing real-time lookup of records. The online store ensures quick access to the latest record via the <span class="No-Break">GetRecord API.</span></li>
				<li><strong class="bold">Offline</strong> <strong class="bold">store</strong>: When sub-second latency reads are not a priority, the offline store stores historical data in your Amazon S3 bucket. It’s your go-to for storing and serving features for exploration, model training, and <span class="No-Break">batch inference.</span></li>
				<li><strong class="bold">Feature</strong> <strong class="bold">group</strong>: The cornerstone of Feature Store, a feature group contains the data and metadata crucial for ML model training or prediction. It logically groups features used to <span class="No-Break">describe records.</span></li>
				<li><strong class="bold">Feature</strong>: A property serving as an input for ML model training or prediction. In the Feature Store API, a feature is an attribute of <span class="No-Break">a record.</span></li>
				<li><strong class="bold">Feature</strong> <strong class="bold">definition</strong>: Comprising a name and data type (integral, string, or fractional), a feature definition is an integral part of a <span class="No-Break">feature group.</span></li>
				<li><strong class="bold">Record</strong>: A collection of values for features tied to a single record identifier. The record identifier and event time values uniquely identify a record within a <span class="No-Break">feature group.</span></li>
				<li><strong class="bold">Record</strong> <strong class="bold">identifier</strong> <strong class="bold">name</strong>: Each record within a feature group is defined and identified with a record identifier name. It must refer to one of the names of a feature defined in the feature group’s <span class="No-Break">feature definitions.</span></li>
				<li><strong class="bold">Event</strong> <strong class="bold">time</strong>: The time when record events occur is marked with timestamps, which are vital for differentiating records. The online store contains the record corresponding to the latest event time, while the offline store contains all <span class="No-Break">historic records.</span></li>
				<li><strong class="bold">Ingestion</strong>: The process of adding new records to a feature group, usually achieved through the <span class="No-Break">PutRecord API.</span></li>
			</ul>
			<p>Let’s combine the tools covered in this chapter so far to navigate through an example of fraud detection in financial transactions. <em class="italic">Table 9.2</em> shows a synthetic dataset for <span class="No-Break">financial transactions:</span></p>
			<table id="table002-5" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">TransactionID</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Amount</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Merchant</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">CardType</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">IsFraud</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">500.25</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Amazon</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Visa</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">120.50</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Walmart</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Mastercard</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">89.99</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Apple</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Amex</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">300.75</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Amazon</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Visa</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>5</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">45.00</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Netflix</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Mastercard</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.2 – Example dataset for financial transactions</p>
			<p>You will now see the applications of SageMaker Feature Store, SageMaker Training Compiler, SageMaker Debugger, and SageMaker Model Monitor on the <span class="No-Break">above dataset.</span></p>
			<ol>
				<li><strong class="bold">Feature engineering with SageMaker Feature Store</strong>: You can store transaction (financial transactions, in this example) features intelligently and it ensures consistency across the training and inference stages. Versioning comes into play, offering a timeline of your <span class="No-Break">features’ evolution.</span><ul><li>Define the features: <strong class="source-inline">Amount</strong>, <span class="No-Break"><strong class="source-inline">Merchant</strong></span><span class="No-Break">, </span><span class="No-Break"><strong class="source-inline">CardType</strong></span></li><li>Ingest data into Feature Store: Use the SageMaker Feature Store API to ingest the dataset into <span class="No-Break">Feature Store:</span><pre class="source-code"><strong class="source-inline"># Example code for ingesting data into Feature Store</strong></pre><pre class="source-code"><strong class="source-inline">from sagemaker.feature_store.feature_group import FeatureGroup</strong></pre><pre class="source-code"><strong class="source-inline">feature_group_name = "financial-transaction-feature-group"</strong></pre><pre class="source-code"><strong class="source-inline">feature_group = FeatureGroup(name=feature_group_name, sagemaker_session=sagemaker_session)</strong></pre><pre class="source-code"><strong class="source-inline">feature_group.load_feature_definitions(data_frame=df)</strong></pre><pre class="source-code"><strong class="source-inline">feature_group.create()</strong></pre><pre class="source-code"><strong class="source-inline">feature_group.ingest(data_frame=df, max_workers=3, wait=True)</strong></pre></li></ul></li>
				<li><strong class="bold">Optimize training with SageMaker Training Compiler</strong>:  You can utilize SageMaker Training Compiler to optimize and compile <span class="No-Break">training scripts:</span><pre class="source-code"><strong class="source-inline"># Example code for defining a training job with SageMaker Training Compiler</strong></pre><pre class="source-code"><strong class="source-inline">from sagemaker.compiler import compile_model</strong></pre><pre class="source-code"><strong class="source-inline">compiled_model = compile_model(</strong></pre><pre class="source-code"><strong class="source-inline">    target_instance_family='ml.m5.large',</strong></pre><pre class="source-code"><strong class="source-inline">    target_platform_os='LINUX',</strong></pre><pre class="source-code"><strong class="source-inline">    sources=['train.py'],</strong></pre><pre class="source-code"><strong class="source-inline">    dependencies=['requirements.txt'],</strong></pre><pre class="source-code"><strong class="source-inline">    framework='pytorch',</strong></pre><pre class="source-code"><strong class="source-inline">    framework_version='1.8.0',</strong></pre><pre class="source-code"><strong class="source-inline">    role='arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole-20201231T000001',</strong></pre><pre class="source-code"><strong class="source-inline">    entry_point='train.py',</strong></pre><pre class="source-code"><strong class="source-inline">    instance_type='ml.m5.large',</strong></pre><pre class="source-code"><strong class="source-inline">)</strong></pre></li>
				<li><strong class="bold">Precision debugging with SageMaker Debugger</strong>: You can integrate SageMaker Debugger hooks into your training script for real-time monitoring to identify training issues in real time, such as vanishing gradients or <span class="No-Break">model overfitting:</span><pre class="source-code"><strong class="source-inline"># Example code for integrating SageMaker Debugger in the training script</strong></pre><pre class="source-code"><strong class="source-inline">from smdebug import SaveConfig</strong></pre><pre class="source-code"><strong class="source-inline">from smdebug.pytorch import Hook</strong></pre><pre class="source-code"><strong class="source-inline"># Create an instance of your model</strong></pre><pre class="source-code"><strong class="source-inline">model = FraudDetectionModel(input_size, hidden_size, output_size)</strong></pre><pre class="source-code"><strong class="source-inline">hook = Hook.create_from_json_file()</strong></pre><pre class="source-code"><strong class="source-inline">hook.register_hook(model)</strong></pre><pre class="source-code"><strong class="source-inline"># Your training script here...</strong></pre><pre class="source-code"><strong class="source-inline"># Train the model train_model(model, train_loader, criterion, optimizer, num_epochs=5)</strong></pre></li>
				<li><strong class="bold">Model deployment and inference</strong>: You can deploy your trained model with SageMaker, tapping into the rich repository of features stored in the Feature Store. Real-time monitoring with SageMaker Model Monitor ensures the model’s health in the dynamic world <span class="No-Break">of inference.</span></li>
				<li><strong class="bold">Continuous monitoring with SageMaker Model Monitor</strong>: The story doesn’t end with deployment. SageMaker Model Monitor becomes your sentinel, continuously guarding the deployed model. Detecting concept drift and data quality nuances, it ensures your model remains a reliable guide in the real-world production environment. Use SageMaker Model Monitor to capture baseline statistics for your <span class="No-Break">deployed model:</span><pre class="source-code"><strong class="source-inline"># Example code for capturing baseline statistics with SageMaker Model Monitor</strong></pre><pre class="source-code"><strong class="source-inline">from sagemaker.model_monitor import DefaultModelMonitor</strong></pre><pre class="source-code"><strong class="source-inline">from sagemaker.model_monitor.dataset_format import DatasetFormat</strong></pre><pre class="source-code"><strong class="source-inline">monitor = DefaultModelMonitor(</strong></pre><pre class="source-code"><strong class="source-inline">    role=role,</strong></pre><pre class="source-code"><strong class="source-inline">    instance_count=1,</strong></pre><pre class="source-code"><strong class="source-inline">    instance_type='ml.m5.large',</strong></pre><pre class="source-code"><strong class="source-inline">    volume_size_in_gb=20,</strong></pre><pre class="source-code"><strong class="source-inline">    max_runtime_in_seconds=3600,</strong></pre><pre class="source-code"><strong class="source-inline">)</strong></pre><pre class="source-code"><strong class="source-inline">baseline_data_uri = 's3://path/to/baseline_data'</strong></pre><pre class="source-code"><strong class="source-inline">monitor.suggest_baseline(</strong></pre><pre class="source-code"><strong class="source-inline">    baseline_dataset=baseline_data_uri,</strong></pre><pre class="source-code"><strong class="source-inline">    dataset_format=DatasetFormat.csv(header=True),</strong></pre><pre class="source-code"><strong class="source-inline">    output_s3_uri='s3://path/to/baseline_output',</strong></pre><pre class="source-code"><strong class="source-inline">)</strong></pre></li>
			</ol>
			<p>In the next section, you will learn about Amazon SageMaker Edge Manager, a service provided by AWS to facilitate the deployment and management of ML models on <span class="No-Break">edge devices.</span></p>
			<h1 id="_idParaDest-244"><a id="_idTextAnchor1344"/>SageMaker Edge Manager</h1>
			<p>SageMaker Edge Manager is designed to address the challenges faced by ML developers when operating models on fleets of edge devices. Some of the key functions that SageMaker Edge Manager can perform are highlighted <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Model compilation</strong>: Utilizes Amazon SageMaker Neo to compile models for various target devices and operating environments, including Linux, Windows, Android, iOS, <span class="No-Break">and macOS.</span></li>
				<li><strong class="bold">Model deployment</strong>: Signs each model with an AWS key, packages it with its runtime, and includes all necessary credentials for deployment on <span class="No-Break">specific devices.</span></li>
				<li><strong class="bold">Model server concept</strong>: Introduces a model server concept to efficiently run multiple models on edge devices, optimizing hardware <span class="No-Break">resource utilization.</span></li>
				<li><strong class="bold">Continuous monitoring</strong>: Provides tools for continuous monitoring of model health, allowing developers to collect metrics, sample input/output data, and send this data securely to <span class="No-Break">the cloud.</span></li>
				<li><strong class="bold">Model drift detection</strong>: Allows the detection of model quality decay over time due to real-world data drift, enabling developers to take <span class="No-Break">corrective action.</span></li>
				<li><strong class="bold">Integration with SageMaker Ground Truth</strong>: Integrates with SageMaker Ground Truth for data labeling and retraining, ensuring that models stay accurate <span class="No-Break">and effective.</span></li>
			</ul>
			<p>Now let’s understand a few real-world challenges and their solutions using SageMaker <span class="No-Break">Edge Manager:</span></p>
			<ul>
				<li><strong class="bold">High </strong><span class="No-Break"><strong class="bold">resource requirements:</strong></span><ul><li><strong class="bold">Challenge</strong>: ML models, especially deep learning models, can have high <span class="No-Break">resource requirements.</span></li><li><strong class="bold">Solution</strong>: SageMaker Edge Manager uses SageMaker Neo to compile models, making them more efficient and allowing them to run up to 25 times faster on certain <span class="No-Break">target hardware.</span></li></ul></li>
				<li><strong class="bold">Running </strong><span class="No-Break"><strong class="bold">multiple models:</strong></span><ul><li><strong class="bold">Challenge</strong>: Many ML applications require running multiple <span class="No-Break">models simultaneously.</span></li><li><strong class="bold">Solution</strong>: Introduces a model server concept, enabling the efficient execution of multiple models in series or in parallel on <span class="No-Break">edge devices.</span></li></ul></li>
				<li><strong class="bold">Model quality decay </strong><span class="No-Break"><strong class="bold">in production:</strong></span><ul><li><strong class="bold">Challenge</strong>: Real-world data drifts over time, causing model <span class="No-Break">quality decay.</span></li><li><strong class="bold">Solution</strong>: SageMaker Edge Manager supports continuous monitoring, allowing developers to detect and address model quality decay using metrics and drift <span class="No-Break">detection tools.</span></li></ul></li>
			</ul>
			<p>The following are some examples showcasing different applications of SageMaker <span class="No-Break">Edge Manager:</span></p>
			<ul>
				<li><strong class="bold">Real-time predictions in autonomous vehicles</strong>: Edge devices in autonomous vehicles need to provide real-time predictions for navigation and obstacle avoidance. SageMaker Edge Manager optimizes models for these devices, ensuring <span class="No-Break">low-latency predictions.</span></li>
				<li><strong class="bold">Privacy-preserving personal devices</strong>: Personal devices such as smartphones and smart cameras can keep data on the device using SageMaker Edge Manager, preserving user privacy and reducing the need for extensive data transfer to <span class="No-Break">the cloud.</span></li>
				<li><strong class="bold">Continuous monitoring in Industrial IoT</strong>: Industrial IoT deployments with sensors on machines can benefit from the continuous monitoring provided by SageMaker Edge Manager. This helps identify and address model quality decay in a <span class="No-Break">dynamic environment.</span></li>
			</ul>
			<p>In summary, Amazon SageMaker Edge Manager streamlines the deployment and management of ML models on edge devices, addressing resource constraints, enabling efficient model execution, and ensuring continuous monitoring for sustained model accuracy. In the next section, you will learn about a no-code solution offered by <span class="No-Break">Amazon SageMaker.</span></p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor1345"/>SageMaker Canvas</h1>
			<p>In this section, you will learn the core of SageMaker Canvas, elucidating its features and the significance it holds for organizations keen on infusing ML into their <span class="No-Break">decision-making processes.</span></p>
			<p>Amazon SageMaker Canvas is a cloud-based service offered by AWS that streamlines the ML process through a visual interface for constructing, training, and deploying ML models—all without the need for coding. Nestled within the Amazon SageMaker suite, it caters to a diverse audience by <span class="No-Break">democratizing ML:</span></p>
			<ul>
				<li><strong class="bold">Code-free model building</strong>: SageMaker Canvas obliterates the traditional barriers encountered when adopting ML, enabling users to forge models without the need for code. This feature proves pivotal for business professionals seeking to harness the potency of ML for predictive analytics, despite lacking <span class="No-Break">coding expertise.</span><p class="list-inset">Case study: A marketing professional without any ML knowledge can utilize SageMaker Canvas to predict customer churn. The intuitive interface guides them through the process, making predictive analytics accessible to a <span class="No-Break">wider audience.</span></p></li>
				<li><strong class="bold">Versatile user interface</strong>: The user-friendly interface of SageMaker Canvas accommodates users with varying levels of expertise. It empowers users to create predictions across diverse use cases, from inventory planning to sentiment analysis, rendering it a versatile tool for businesses spanning <span class="No-Break">different industries.</span><p class="list-inset">Case study: A supply chain manager can leverage SageMaker Canvas to predict optimal inventory levels based on historical data, seasonality, and market trends, streamlining the planning process and <span class="No-Break">minimizing stockouts.</span></p></li>
				<li><strong class="bold">Built-in data preparation functions</strong>: SageMaker Canvas comes equipped with built-in data preparation functions and operators, facilitating the import and analysis of disparate cloud and on-premises data sources. This feature streamlines the exploration and visualization of relationships between features, enabling the seamless creation of <span class="No-Break">new features.</span><p class="list-inset">Case study: A data analyst can import and analyze customer data from various sources using SageMaker Canvas. This allows them to identify key factors influencing purchasing decisions and create predictive models to enhance targeted <span class="No-Break">marketing strategies.</span></p></li>
				<li><strong class="bold">Collaboration and model sharing</strong>: SageMaker Canvas fosters collaboration by enabling users to share, review, and update ML models across different tools and teams. This collaborative aspect ensures that knowledge and insights derived from ML are disseminated effectively within <span class="No-Break">the organization.</span><p class="list-inset">Case study: A data science team collaborates with business analysts using SageMaker Canvas to develop a fraud detection model. The model can be shared seamlessly, allowing real-time updates and improvements based on evolving <span class="No-Break">data patterns.</span></p></li>
			</ul>
			<p>Amazon SageMaker Canvas acts as a catalyst for transforming ML from a specialized skill to a tool accessible to a broader audience. Its features, including code-free model building, a versatile user interface, and collaborative capabilities, underscore its importance in simplifying the ML lifecycle. As organizations strive to harness the power of data-driven insights, SageMaker Canvas stands at the forefront, enabling them to innovate, make informed decisions, and thrive in an increasingly <span class="No-Break">competitive landscape.</span></p>
			<p>You have <a id="_idTextAnchor1346"/>now reached the end of this section and the end of this chapter. Next, let’s summarize what you <span class="No-Break">have learned.</span></p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor1347"/><a id="_idTextAnchor1348"/>Summary</h1>
			<p>In this chapter, you learned about the usage of SageMaker for creating notebook instances and training instances. As you went through, you learned how to use SageMaker for hyperparameter tuning jobs. As the security of your assets in AWS is an essential part of your work, you also learned the various ways to secure <span class="No-Break">SageMaker instances.</span></p>
			<p>AWS products are evolving every day to help you solve IT problems. It’s not easy to remember all the product names. The only way to learn is through practice. When you are solving a problem or building a product, focus on different technological areas of your product. Those areas can be scheduling jobs, logging, tracing, monitoring metrics, autoscaling, <span class="No-Break">and more.</span></p>
			<p>Compute time, storage, and networking are the baselines. It is recommended that you practice some examples for each of these services. Referring to the AWS documentation to resolve any doubts is also a useful option. It is always important to design your solutions in a cost-effective way and exploring cost optimizations when using these services is as important as building the solution itself. I wish you all <span class="No-Break">the best!</span></p>
			<h1 id="_idParaDest-247"><a id="_idTextAnchor1349"/>Exam Readiness Drill – Chapter Review Questions</h1>
			<p>Apart from a solid understanding of key concepts, being able to think quickly under time pressure is a skill that will help you ace your certification exam. That is why working on these skills early on in your learning journey <span class="No-Break">is key.</span></p>
			<p>Chapter review questions are designed to improve your test-taking skills progressively with each chapter you learn and review your understanding of key concepts in the chapter at the same time. You’ll find these at the end of <span class="No-Break">each chapter.</span></p>
			<p class="callout-heading">How To Access These Resources</p>
			<p class="callout">To learn how to access these resources, head over to the chapter titled <a href="B21197_11.xhtml#_idTextAnchor1477"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Accessing the Online </em><span class="No-Break"><em class="italic">Practice Resources</em></span><span class="No-Break">.</span></p>
			<p>To open the Chapter Review Questions for this chapter, perform the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Click the link – <a href="https://packt.link/MLSC01E2_CH09"><span class="No-Break">https://packt.link/MLSC01E2_CH09</span></a><span class="No-Break">.</span><p class="list-inset">Alternatively, you can scan the following <strong class="bold">QR code</strong> (<span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">):</span></p></li>
			</ol>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B21197_09_14.jpg" alt="Figure 9.14 – QR code that opens Chapter Review Questions for logged-in users" width="550" height="150"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.14 – QR code that opens Chapter Review Questions for logged-in users</p>
			<ol>
				<li value="2">Once you log in, you’ll see a page similar to the one shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.15</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B21197_09_15.jpg" alt="Figure 9.15 – Chapter Review Questions for Chapter 9" width="1408" height="752"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.15 – Chapter Review Questions for Chapter 9</p>
			<ol>
				<li value="3">Once ready, start the following practice drills, re-attempting the quiz <span class="No-Break">multiple times.</span></li>
			</ol>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor1350"/>Exam Readiness Drill</h2>
			<p>For the first three attempts, don’t worry about the <span class="No-Break">time limit.</span></p>
			<h3 id="_idParaDest-249"><a id="_idTextAnchor1351"/>ATTEMPT 1</h3>
			<p>The first time, aim for at least <strong class="bold">40%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix your <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-250"><a id="_idTextAnchor1352"/>ATTEMPT 2</h3>
			<p>The second time, aim for at least <strong class="bold">60%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix any remaining <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-251"><a id="_idTextAnchor1353"/>ATTEMPT 3</h3>
			<p>The third time, aim for at least <strong class="bold">75%</strong>. Once you score 75% or more, you start working on <span class="No-Break">your timing.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">You may take more than <strong class="bold">three</strong> attempts to reach 75%. That’s okay. Just review the relevant sections in the chapter till you <span class="No-Break">get there.</span></p>
			<h1 id="_idParaDest-252"><a id="_idTextAnchor1354"/>Working On Timing</h1>
			<p>Target: Your aim is to keep the score the same while trying to answer these questions as quickly as possible. Here’s an example of how your next attempts should <span class="No-Break">look like:</span></p>
			<table id="table003-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Attempt</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Score</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Time Taken</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 5</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">77%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>21 mins <span class="No-Break">30 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 6</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">78%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>18 mins <span class="No-Break">34 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 7</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">76%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>14 mins <span class="No-Break">44 seconds</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.3 – Sample timing practice drills on the online platform</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The time limits shown in the above table are just examples. Set your own time limits with each attempt based on the time limit of the quiz on <span class="No-Break">the website.</span></p>
			<p>With each new attempt, your score should stay above <strong class="bold">75%</strong> while your “time taken” to complete should “decrease”. Repeat as many attempts as you want till you feel confident dealing with the <span class="No-Break">time pressure.</span></p>
		</div>
	</div>
</div>
</body></html>