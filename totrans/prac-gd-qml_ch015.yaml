- en: Chapter 7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'VQE: Variational Quantum Eigensolver'
  prefs: []
  type: TYPE_NORMAL
- en: '*From so simple a beginning endless forms most beautiful and most* *wonderful
    have been, and are being, evolved.*'
  prefs: []
  type: TYPE_NORMAL
- en: — Charles Darwin
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters of this part of the book, we have studied how quantum
    algorithms can help us solve combinatorial optimization problems, but there are
    many other important types of optimization problems out there! This chapter will
    broaden the scope of our optimization methods to cover more general settings,
    including applications in fields such as chemistry and physics.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will achieve this by studying the famous **Variational Quantum** **Eigensolver**
    (**VQE**) algorithm, which can be seen as a generalization of the Quantum Approximate
    Optimization Algorithm that we studied back in *Chapter* *[*5*](ch013.xhtml#x1-940005),
    *QAOA: Quantum Approximate Optimization Algorithm*. Actually, it would be more
    precise to say that we can see QAOA as a particular case of VQE; in fact, VQE
    was introduced earlier than QAOA in a now famous paper by Peruzzo et al. [[76](ch030.xhtml#Xperuzzo2014variational)].*'
  prefs: []
  type: TYPE_NORMAL
- en: '*We shall begin by expanding our knowledge of Hamiltonians and by better understanding
    how to estimate their expectation values with quantum computers. That will allow
    us to define VQE in all its glory and to appreciate both the simplicity of its
    formulation and its wide applicability for finding the ground state of different
    types of Hamiltonians.'
  prefs: []
  type: TYPE_NORMAL
- en: We will then show how to use VQE with both Qiskit and PennyLane using examples
    from the field of chemistry. We will also show how to study the influence of errors
    on the algorithm by running simulations of noisy quantum computers, and we will
    even discuss some techniques to mitigate the adverse effect of readout errors.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you will know both the theoretical foundations of
    VQE and how to use it in a wide variety of practical situations, on simulators
    and on actual quantum computers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that we will cover in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Hamiltonians, observables, and their expectation values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the Variational Quantum Eigensolver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using VQE with Qiskit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using VQE with PennyLane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have quite a lot to learn and, in fact, endless forms most beautiful to discover.
    So, let’s not waste time and get started right away!
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Hamiltonians, observables, and their expectation values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve found in Hamiltonians a way to encode combinatorial optimization
    problems. As you surely remember, in these optimization problems, we start with
    a function ![f](img/file778.png "f") that assigns real numbers to binary strings
    of a certain length ![n](img/file244.png "n"), and we seek to find a binary string
    ![x](img/file269.png "x") with minimum cost ![f(x)](img/file800.png "f(x)"). In
    order to do that with quantum algorithms, we define a Hamiltonian ![H_{f}](img/file933.png
    "H_{f}") such that
  prefs: []
  type: TYPE_NORMAL
- en: '![\left\langle x \right|H_{f}\left| x \right\rangle = f(x)](img/file934.png
    "\left\langle x \right|H_{f}\left| x \right\rangle = f(x)")'
  prefs: []
  type: TYPE_IMG
- en: holds for every binary string ![x](img/file269.png "x") of length ![n](img/file244.png
    "n"). Then, we can solve our original problem by finding a ground state of ![H_{f}](img/file933.png
    "H_{f}") (that is, a state ![\left| \psi \right\rangle](img/file43.png "\left|
    \psi \right\rangle") such that the expectation value ![\left\langle \psi \right|H_{f}\left|
    \psi \right\rangle](img/file935.png "\left\langle \psi \right|H_{f}\left| \psi
    \right\rangle") is minimum).
  prefs: []
  type: TYPE_NORMAL
- en: This was just a very quick summary of *Chapter* *[*3*](ch011.xhtml#x1-590003),
    *Working with* *Quadratic Unconstrained Binary Optimization Problems*. When you
    read that chapter, you may have noticed that the Hamiltonian associated to ![f](img/file778.png
    "f") has an additional, very remarkable property. We have mentioned this a couple
    of times already, but it is worth remembering that, for every computational basis
    state ![\left| x \right\rangle](img/file267.png "\left| x \right\rangle"), it
    holds that*
  prefs: []
  type: TYPE_NORMAL
- en: '*![H_{f}\left| x \right\rangle = f(x)\left| x \right\rangle.](img/file936.png
    "H_{f}\left| x \right\rangle = f(x)\left| x \right\rangle.")'
  prefs: []
  type: TYPE_NORMAL
- en: This means that each ![\left| x \right\rangle](img/file267.png "\left| x \right\rangle")
    is an eigenvector of ![H_{f}](img/file933.png "H_{f}") with associated eigenvalue
    ![f(x)](img/file800.png "f(x)") (if you do not remember what eigenvectors and
    eigenvalues are, check *Appendix* * [*B*](ch025.xhtml#x1-226000B), *Installing
    the Tools*, for all the relevant definitions and concepts). In fact, this is easy
    to see because we have always used Hamiltonians that are sums of tensor products
    of ![Z](img/file8.png "Z") matrices, which are clearly diagonal. But tensor products
    of diagonal matrices are diagonal matrices themselves, and sums of diagonal matrices
    are still diagonal. Thus, since these Hamiltonians are diagonal, the computational
    basis states are their eigenvectors.*
  prefs: []
  type: TYPE_NORMAL
- en: '*What is more, if we have a state ![\left| \psi \right\rangle](img/file43.png
    "\left| \psi \right\rangle"), we can always write it as a linear combination of
    the computational basis states. In fact, it holds that'
  prefs: []
  type: TYPE_NORMAL
- en: '![\left| \psi \right\rangle = \sum\limits_{x}\alpha_{x}\left| x \right\rangle,](img/file937.png
    "\left| \psi \right\rangle = \sum\limits_{x}\alpha_{x}\left| x \right\rangle,")'
  prefs: []
  type: TYPE_IMG
- en: where the sum is over all the computational basis states ![\left| x \right\rangle](img/file267.png
    "\left| x \right\rangle") and ![\alpha_{x} = \left\langle x \middle| \psi \right\rangle](img/file938.png
    "\alpha_{x} = \left\langle x \middle| \psi \right\rangle"). This is easy to check,
    because
  prefs: []
  type: TYPE_NORMAL
- en: '![\left\langle x \middle| \psi \right\rangle = \left\langle x \right|\sum\limits_{y}\alpha_{y}\left|
    y \right\rangle = \sum\limits_{y}\alpha_{y}\left\langle x \middle| y \right\rangle
    = \alpha_{x}.](img/file939.png "\left\langle x \middle| \psi \right\rangle = \left\langle
    x \right|\sum\limits_{y}\alpha_{y}\left| y \right\rangle = \sum\limits_{y}\alpha_{y}\left\langle
    x \middle| y \right\rangle = \alpha_{x}.")'
  prefs: []
  type: TYPE_IMG
- en: The last identity follows from the fact that ![\left\langle x \middle| y \right\rangle](img/file940.png
    "\left\langle x \middle| y \right\rangle") is ![1](img/file13.png "1") if ![x
    = y](img/file941.png "x = y") and ![0](img/file12.png "0") otherwise (remember
    that the computational basis is an orthonormal basis).
  prefs: []
  type: TYPE_NORMAL
- en: Then, the expectation value of ![H_{f}](img/file933.png "H_{f}") in the state
    ![\left| \psi \right\rangle](img/file43.png "\left| \psi \right\rangle") can be
    computed as
  prefs: []
  type: TYPE_NORMAL
- en: '![\begin{array}{rlr} {\left\langle \psi \right|H_{f}\left| \psi \right\rangle
    = \sum\limits_{y}\alpha_{y}^{\ast}\left\langle y \right|H_{f}\sum\limits_{x}\alpha_{x}\left|
    x \right\rangle = \sum\limits_{x,y}\alpha_{y}^{\ast}\alpha_{x}\left\langle y \right|H_{f}\left|
    x \right\rangle = \sum\limits_{x,y}\alpha_{y}^{\ast}\alpha_{x}f(x)\left\langle
    y \middle| x \right\rangle} & \qquad & \\ {= \sum\limits_{x}\alpha_{x}^{\ast}\alpha_{x}f(x)
    = \sum\limits_{x}\left| \alpha_{x} \right|^{2}f(x).} & \qquad & \\ \end{array}](img/file942.png
    "\begin{array}{rlr} {\left\langle \psi \right|H_{f}\left| \psi \right\rangle =
    \sum\limits_{y}\alpha_{y}^{\ast}\left\langle y \right|H_{f}\sum\limits_{x}\alpha_{x}\left|
    x \right\rangle = \sum\limits_{x,y}\alpha_{y}^{\ast}\alpha_{x}\left\langle y \right|H_{f}\left|
    x \right\rangle = \sum\limits_{x,y}\alpha_{y}^{\ast}\alpha_{x}f(x)\left\langle
    y \middle| x \right\rangle} & \qquad & \\ {= \sum\limits_{x}\alpha_{x}^{\ast}\alpha_{x}f(x)
    = \sum\limits_{x}\left| \alpha_{x} \right|^{2}f(x).} & \qquad & \\ \end{array}")'
  prefs: []
  type: TYPE_IMG
- en: 'Moreover, we know that ![\left| \alpha_{x} \right|^{2} = \left| \left\langle
    x \middle| \psi \right\rangle \right|^{2}](img/file943.png "\left| \alpha_{x}
    \right|^{2} = \left| \left\langle x \middle| \psi \right\rangle \right|^{2}")
    is the probability of obtaining ![\left| x \right\rangle](img/file267.png "\left|
    x \right\rangle") when measuring ![\left| \psi \right\rangle](img/file43.png "\left|
    \psi \right\rangle") in the computational basis; in this way, the expectation
    value matches the statistical expected value of the measurement. As you surely
    remember, this is exactly the fact that we used back in *Chapter* * [*5*](ch013.xhtml#x1-940005),
    *QAOA: Quantum Approximate* *Optimization Algorithm*, to estimate the value of
    the cost function when running QAOA circuits in a quantum computer.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*These properties may seem dependent on the particular form of the Hamiltonians
    that we have been using. But, in fact, they are very general results, and we will
    use them extensively in our study of the VQE algorithm. But before we get to that,
    we will need to introduce the general notion of ”observable”, which is precisely
    the topic of the next subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1 Observables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up until this point, we have only considered measurements in the computational
    basis. This has worked well enough for our purposes, but, in doing so, we’ve ignored
    some details about how measurements are truly understood and described in quantum
    mechanics. We are now going to fill that gap.
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to go slowly through this section. Take your time and maybe
    prepare yourself a good cup of your favourite hot beverage. The ideas presented
    here may seem a little bit strange at first, but you will soon realize that they
    fit nicely with what we have been doing so far.
  prefs: []
  type: TYPE_NORMAL
- en: In quantum mechanics, any physical magnitude that you can measure — also known
    as a **(physical) observable** — is represented by a Hermitian operator. In case
    you don’t remember, these are linear operators ![A](img/file183.png "A") that
    are equal to their adjoints (their conjugate transposes), that is, they satisfy
    ![A^{\dagger} = A](img/file944.png "A^{\dagger} = A").
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: You may remember how in *Chapter* *[*3*](ch011.xhtml#x1-590003), *Working with
    Quadratic* *Unconstrained Binary Optimization Problems*, we worked extensively
    with Hamiltonians. These, in general, are Hermitian operators that are, indeed,
    associated with an observable magnitude. That magnitude is none other than the
    energy of the system!*
  prefs: []
  type: TYPE_NORMAL
- en: '*The nice thing about Hermitian operators is that, for them, one can always
    find an orthonormal basis of eigenvectors with real eigenvalues (please, check
    *Appendix* *[*B*](ch025.xhtml#x1-226000B), *Basic Linear Algebra*, if you need
    to review these notions). This means that there exist real numbers ![\lambda_{j}](img/file945.png
    "\lambda_{j}"), ![j = 1,\ldots,l](img/file946.png "j = 1,\ldots,l"), all of them
    different, and states ![\left| \lambda_{j}^{k} \right\rangle](img/file947.png
    "\left| \lambda_{j}^{k} \right\rangle"), where ![j = 1,\ldots,l](img/file946.png
    "j = 1,\ldots,l") and ![k = 1,\ldots,r_{j}](img/file948.png "k = 1,\ldots,r_{j}"),
    such that the states ![{\{\left| \lambda_{j}^{k} \right\rangle\}}_{j,k}](img/file949.png
    "{\{\left| \lambda_{j}^{k} \right\rangle\}}_{j,k}") form an orthonormal basis
    and*'
  prefs: []
  type: TYPE_NORMAL
- en: '*![A\left| \lambda_{j}^{k} \right\rangle = \lambda_{j}\left| \lambda_{j}^{k}
    \right\rangle,](img/file950.png "A\left| \lambda_{j}^{k} \right\rangle = \lambda_{j}\left|
    \lambda_{j}^{k} \right\rangle,")'
  prefs: []
  type: TYPE_NORMAL
- en: for every ![j = 1,\ldots,l](img/file946.png "j = 1,\ldots,l") and for every
    ![k = 1,\ldots,r_{j}](img/file948.png "k = 1,\ldots,r_{j}").
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are considering the possibility of having several eigenvectors ![\left|
    \lambda_{j}^{k} \right\rangle](img/file947.png "\left| \lambda_{j}^{k} \right\rangle")
    associated with the same eigenvalue ![\lambda_{j}](img/file945.png "\lambda_{j}"),
    hence the use of the superindices ![k = 1,\ldots,r_{j}](img/file948.png "k = 1,\ldots,r_{j}"),
    where ![r_{j}](img/file951.png "r_{j}") is the number of eigenvectors associated
    with the ![\lambda_{j}^{k}](img/file952.png "\lambda_{j}^{k}") eigenvalue. If
    all the eigenvalues are different (a quite common case), then we will have ![r_{j}
    = 1](img/file953.png "r_{j} = 1") for every ![j](img/file258.png "j") and we can
    simply drop the ![k](img/file317.png "k") superindices.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the connection of these Hermitian operators with physical measurements?
    Let’s consider an observable represented by a Hermitian operator ![A](img/file183.png
    "A"), and also an orthonormal basis of eigenvectors ![{\{\left| \lambda_{j}^{k}
    \right\rangle\}}_{j,k}](img/file949.png "{\{\left| \lambda_{j}^{k} \right\rangle\}}_{j,k}")
    such that ![A\left| \lambda_{j}^{k} \right\rangle = \lambda_{j}\left| \lambda_{j}^{k}
    \right\rangle](img/file954.png "A\left| \lambda_{j}^{k} \right\rangle = \lambda_{j}\left|
    \lambda_{j}^{k} \right\rangle"). This representation must be chosen to take the
    following into account:'
  prefs: []
  type: TYPE_NORMAL
- en: The possible outcomes of the measurement of the observable must be represented
    by the different eigenvalues ![\lambda_{j}](img/file945.png "\lambda_{j}")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability that a state ![\left| \psi \right\rangle](img/file43.png "\left|
    \psi \right\rangle") will, upon measurement, yield ![\lambda_{j}](img/file945.png
    "\lambda_{j}") must be ![{\sum}_{k}\left| \left\langle \lambda_{j}^{k} \middle|
    \psi \right\rangle \right|^{2}](img/file955.png "{\sum}_{k}\left| \left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle \right|^{2}")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of this is axiomatic. It is a fact of life that any physical observable
    can be represented by a Hermitian operator in such a way that those are requirements
    are satisfied. Moreover, it is a postulate of quantum mechanics that if the measurement
    returns the result associated to an eigenvalue ![\lambda_{j}](img/file945.png
    "\lambda_{j}"), the state of the system will then become the normalized projection
    of ![\left| \psi \right\rangle](img/file43.png "\left| \psi \right\rangle") onto
    the space of eigenvectors with eigenvalue ![\lambda_{j}](img/file945.png "\lambda_{j}").
    This means that if we measure a state in a superposition such as
  prefs: []
  type: TYPE_NORMAL
- en: '![\sum\limits_{j,k}\alpha_{j}^{k}\left| \lambda_{j}^{k} \right\rangle](img/file956.png
    "\sum\limits_{j,k}\alpha_{j}^{k}\left| \lambda_{j}^{k} \right\rangle")'
  prefs: []
  type: TYPE_IMG
- en: and we obtain ![\lambda_{j}](img/file945.png "\lambda_{j}") as the result, then
    the new state will be
  prefs: []
  type: TYPE_NORMAL
- en: '![\frac{\sum\limits_{k}\alpha_{j}^{k}\left| \lambda_{j}^{k} \right\rangle}{\sqrt{\sum\limits_{k}\left|
    \alpha_{j}^{k} \right|^{2}}}.](img/file957.png "\frac{\sum\limits_{k}\alpha_{j}^{k}\left|
    \lambda_{j}^{k} \right\rangle}{\sqrt{\sum\limits_{k}\left| \alpha_{j}^{k} \right|^{2}}}.")'
  prefs: []
  type: TYPE_IMG
- en: This is what we call the *collapse* of the original state and it is exactly
    the same phenomenon that we considered when studied measurements in the computational
    basis back in *Chapter* *[*1*](ch008.xhtml#x1-180001), *Foundations of Quantum*
    *Computing*.*
  prefs: []
  type: TYPE_NORMAL
- en: '*The word ”observable” is often used for both physical observables and for
    any Hermitian operators that represent them. Thus, we may refer to a Hermitian
    operator itself as an observable. To avoid confusions, we will usually not omit
    the ”physical” adjective when referring to physical observables.'
  prefs: []
  type: TYPE_NORMAL
- en: As a simple example, whenever we measure in the computational basis, we are
    indeed measuring some physical observable, and this physical observable can, of
    course, be represented by a Hermitian operator. This is, in a certain sense, the
    simplest observable in quantum computing and it is natural that it arises as a
    particular case of this, more general theory of quantum measurements.
  prefs: []
  type: TYPE_NORMAL
- en: The coordinated matrix of this measurement operator with respect to the computational
    basis could be the diagonal matrix
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\begin{pmatrix} 0 & & & \\ & 1 & & \\ & & \ddots & \\ & & & {2^{n} - 1}
    \\ \end{pmatrix}.](img/file958.png "\begin{pmatrix} 0 & & & \\  & 1 & & \\  &
    & \ddots & \\  & & & {2^{n} - 1} \\ \end{pmatrix}.") |'
  prefs: []
  type: TYPE_TB
- en: Exercise 7.1
  prefs: []
  type: TYPE_NORMAL
- en: Prove that, indeed, the previous matrix is the coordinate matrix on the computational
    basis of a Hermitian operator that represents a measurement in the computational
    basis.
  prefs: []
  type: TYPE_NORMAL
- en: When we measure a single qubit in the computational basis, the coordinate matrix
    with respect to the computational basis of the associated Hermitian operator could
    well be either of
  prefs: []
  type: TYPE_NORMAL
- en: '| ![N = \begin{pmatrix} 0 & 0 \\ 0 & 1 \\ \end{pmatrix},\qquad Z = \begin{pmatrix}
    1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.](img/file959.png "N = \begin{pmatrix} 0 &
    0 \\ 0 & 1 \\ \end{pmatrix},\qquad Z = \begin{pmatrix} 1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.")
    |'
  prefs: []
  type: TYPE_TB
- en: Yes, that last matrix was the unmistakable Pauli ![Z](img/file8.png "Z") matrix.
    Both of these operators represent the same observable; they only differ in the
    eigenvalues that they associate to the distinct possible outcomes. The first operator
    associates the eigenvalues ![0](img/file12.png "0") and ![1](img/file13.png "1")
    to the qubit’s value being ![0](img/file12.png "0") and ![1](img/file13.png "1")
    respectively, while the second observable associates the eigenvalues ![1](img/file13.png
    "1") and ![- 1](img/file312.png "- 1") to these outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Measurements in quantum mechanics are represented by Hermitian operators, which
    we refer to as observables. One possible operator corresponding to measuring a
    qubit in the computational basis can be the Pauli ![Z](img/file8.png "Z") matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what an observable is, we can study what its **expectation**
    **value** is and how it can be computed. The expectation value of any observable
    under a state ![\left| \psi \right\rangle](img/file43.png "\left| \psi \right\rangle")
    can be defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| ![{\langle A\rangle}_{\psi} = \sum\limits_{j,k}\left&#124; \left\langle \lambda_{j}^{k}
    \middle&#124; \psi \right\rangle \right&#124;^{2}\lambda_{j},](img/file960.png
    "{\langle A\rangle}_{\psi} = \sum\limits_{j,k}\left&#124; \left\langle \lambda_{j}^{k}
    \middle&#124; \psi \right\rangle \right&#124;^{2}\lambda_{j},") |'
  prefs: []
  type: TYPE_TB
- en: 'which is a natural definition that agrees with the statistical expected value
    of the results obtained when we measure ![\left| \psi \right\rangle](img/file43.png
    "\left| \psi \right\rangle") according to ![A](img/file183.png "A"). As intuitive
    as this expression may be, we can further simplify it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![\begin{array}{rlrl} {\langle A\rangle}_{\psi} & {= \sum\limits_{j,k}\left|
    \left\langle \lambda_{j}^{k} \middle| \psi \right\rangle \right|^{2}\lambda_{j}
    = \sum\limits_{j,k}\left\langle \psi \middle| \lambda_{j}^{k} \right\rangle\left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle\lambda_{j} = \sum\limits_{j,k}\left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle\left\langle \psi \middle| \lambda_{j}^{k}
    \right\rangle\lambda_{j}\qquad} & & \qquad \\ & {= \sum\limits_{j,k}\left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle\left\langle \psi \right|A\left| \lambda_{j}^{k}
    \right\rangle = \left\langle \psi \right|A\sum\limits_{j,k}\left\langle \lambda_{j}^{k}
    \middle| \psi \right\rangle\left| \lambda_{j}^{k} \right\rangle = \left\langle
    \psi \right|A\left| \psi \right\rangle.\qquad} & & \qquad \\ & \qquad & & \\ \end{array}](img/file961.png
    "\begin{array}{rlrl} {\langle A\rangle}_{\psi} & {= \sum\limits_{j,k}\left| \left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle \right|^{2}\lambda_{j} = \sum\limits_{j,k}\left\langle
    \psi \middle| \lambda_{j}^{k} \right\rangle\left\langle \lambda_{j}^{k} \middle|
    \psi \right\rangle\lambda_{j} = \sum\limits_{j,k}\left\langle \lambda_{j}^{k}
    \middle| \psi \right\rangle\left\langle \psi \middle| \lambda_{j}^{k} \right\rangle\lambda_{j}\qquad}
    & & \qquad \\  & {= \sum\limits_{j,k}\left\langle \lambda_{j}^{k} \middle| \psi
    \right\rangle\left\langle \psi \right|A\left| \lambda_{j}^{k} \right\rangle =
    \left\langle \psi \right|A\sum\limits_{j,k}\left\langle \lambda_{j}^{k} \middle|
    \psi \right\rangle\left| \lambda_{j}^{k} \right\rangle = \left\langle \psi \right|A\left|
    \psi \right\rangle.\qquad} & & \qquad \\  & \qquad & & \\ \end{array}")'
  prefs: []
  type: TYPE_IMG
- en: Notice that we have used the fact that ![A\left| \lambda_{j}^{k} \right\rangle
    = \lambda_{j}\left| \lambda_{j}^{k} \right\rangle](img/file954.png "A\left| \lambda_{j}^{k}
    \right\rangle = \lambda_{j}\left| \lambda_{j}^{k} \right\rangle") and that ![\left|
    \psi \right\rangle = {\sum}_{j,k}\left\langle \lambda_{j}^{k} \middle| \psi \right\rangle\left|
    \lambda_{j}^{k} \right\rangle](img/file962.png "\left| \psi \right\rangle = {\sum}_{j,k}\left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle\left| \lambda_{j}^{k} \right\rangle").
    This latter identity follows from the fact that ![{\{\left| \lambda_{j}^{k} \right\rangle\}}_{j,k}](img/file949.png
    "{\{\left| \lambda_{j}^{k} \right\rangle\}}_{j,k}") is an orthonormal basis and,
    in fact, it can be proved in exactly the same way we did for the computational
    basis at the beginning of this section.
  prefs: []
  type: TYPE_NORMAL
- en: This expression for the expectation value agrees with our previous work in *Chapter*
    *[*3*](ch011.xhtml#x1-590003), *Working with Quadratic Unconstrained Binary Optimization*
    *Problems*.*
  prefs: []
  type: TYPE_NORMAL
- en: '*Important note'
  prefs: []
  type: TYPE_NORMAL
- en: The expectation value of any Hermitian operator (observable) ![A](img/file183.png
    "A") is given by
  prefs: []
  type: TYPE_NORMAL
- en: '| ![{\langle A\rangle}_{\psi} = \sum\limits_{j,k}\left&#124; \left\langle \lambda_{j}^{k}
    \middle&#124; \psi \right\rangle \right&#124;^{2}\lambda_{j} = \left\langle \psi
    \right&#124;A\left&#124; \psi \right\rangle.](img/file963.png "{\langle A\rangle}_{\psi}
    = \sum\limits_{j,k}\left&#124; \left\langle \lambda_{j}^{k} \middle&#124; \psi
    \right\rangle \right&#124;^{2}\lambda_{j} = \left\langle \psi \right&#124;A\left&#124;
    \psi \right\rangle.") |'
  prefs: []
  type: TYPE_TB
- en: Notice that, from the very definition of the expectation value of an observable,
    we can easily derive the variational principle. This principle states, as you
    may recall from *Chapter* *[*3*](ch011.xhtml#x1-590003), *Working with Quadratic
    Unconstrained Binary* *Optimization Problems*, that the smallest expectation value
    of an observable ![A](img/file183.png "A") is always achieved at an eigenvector
    of that observable. To prove it, suppose that ![\lambda_{0}](img/file964.png "\lambda_{0}")
    is minimal among all the eigenvalues of ![A](img/file183.png "A"). Then, for any
    state ![\psi](img/file965.png "\psi") it holds that*
  prefs: []
  type: TYPE_NORMAL
- en: '*| ![{\langle A\rangle}_{\psi} = \sum\limits_{j,k}\left&#124; \left\langle
    \lambda_{j}^{k} \middle&#124; \psi \right\rangle \right&#124;^{2}\lambda_{j} \geq
    \sum\limits_{j,k}\left&#124; \left\langle \lambda_{j}^{k} \middle&#124; \psi \right\rangle
    \right&#124;^{2}\lambda_{0} = \lambda_{0},](img/file966.png "{\langle A\rangle}_{\psi}
    = \sum\limits_{j,k}\left&#124; \left\langle \lambda_{j}^{k} \middle&#124; \psi
    \right\rangle \right&#124;^{2}\lambda_{j} \geq \sum\limits_{j,k}\left&#124; \left\langle
    \lambda_{j}^{k} \middle&#124; \psi \right\rangle \right&#124;^{2}\lambda_{0} =
    \lambda_{0},") |'
  prefs: []
  type: TYPE_NORMAL
- en: where the last equality follows from the fact that ![{\sum}_{j,k}\left| \left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle \right|^{2} = 1](img/file967.png "{\sum}_{j,k}\left|
    \left\langle \lambda_{j}^{k} \middle| \psi \right\rangle \right|^{2} = 1"), since
    the sum of the probabilities of all the outcomes must add up to ![1](img/file13.png
    "1").
  prefs: []
  type: TYPE_NORMAL
- en: If we now take any eigenvector ![\left| \lambda_{0}^{k} \right\rangle](img/file968.png
    "\left| \lambda_{0}^{k} \right\rangle") associated to ![\lambda_{0}](img/file964.png
    "\lambda_{0}"), its expectation value will be
  prefs: []
  type: TYPE_NORMAL
- en: '![\left\langle \lambda_{0}^{k} \right|A\left| \lambda_{0}^{k} \right\rangle
    = \lambda_{0}\left\langle \lambda_{0}^{k} \middle| \lambda_{0}^{k} \right\rangle
    = \lambda_{0},](img/file969.png "\left\langle \lambda_{0}^{k} \right|A\left| \lambda_{0}^{k}
    \right\rangle = \lambda_{0}\left\langle \lambda_{0}^{k} \middle| \lambda_{0}^{k}
    \right\rangle = \lambda_{0},")'
  prefs: []
  type: TYPE_IMG
- en: proving that the minimum expectation value is indeed achieved at an eigenvector
    of ![A](img/file183.png "A"). Obviously, if there were several orthogonal eigenvectors
    associated to ![\lambda_{0}](img/file964.png "\lambda_{0}"), any normalized linear
    combination of them would also be a ground state of ![A](img/file183.png "A").
  prefs: []
  type: TYPE_NORMAL
- en: In this subsection, we have studied the mathematical expression for the expectation
    of any observable. But we don’t yet know how to estimate these expectation values
    with quantum computers. How could we do that? Just keep reading, because we will
    be exploring it in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.2 Estimating the expectation values of observables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of the VQE algorithm, we will need to estimate the expectation
    value of a general observable ![A](img/file183.png "A"). That is, we will no longer
    assume that ![A](img/file183.png "A") is diagonal, as we have done in all the
    previous chapters. For this reason, we will need to develop a new method for estimating
    the expectation value ![\left\langle \psi \right|A\left| \psi \right\rangle](img/file970.png
    "\left\langle \psi \right|A\left| \psi \right\rangle").
  prefs: []
  type: TYPE_NORMAL
- en: We know that, for a given state ![\left| \psi \right\rangle](img/file43.png
    "\left| \psi \right\rangle"), the expectation value of ![A](img/file183.png "A")
    can be computed by
  prefs: []
  type: TYPE_NORMAL
- en: '![\sum\limits_{j,k}\left| \left\langle \lambda_{j}^{k} \middle| \psi \right\rangle
    \right|^{2}\lambda_{j}.](img/file971.png "\sum\limits_{j,k}\left| \left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle \right|^{2}\lambda_{j}.")'
  prefs: []
  type: TYPE_IMG
- en: Thus, if we knew the eigenvalues ![\lambda_{j}](img/file945.png "\lambda_{j}")
    and the eigenvectors ![{\{\left| \lambda_{j}^{k} \right\rangle\}}_{j,k}](img/file949.png
    "{\{\left| \lambda_{j}^{k} \right\rangle\}}_{j,k}") of ![A](img/file183.png "A"),
    we could try to compute ![\left| \left\langle \lambda_{j}^{k} \middle| \psi \right\rangle
    \right|^{2}](img/file972.png "\left| \left\langle \lambda_{j}^{k} \middle| \psi
    \right\rangle \right|^{2}") and, hence, the expectation value of ![A](img/file183.png
    "A"). However, this is information that we usually don’t know. In fact, the purpose
    of VQE is, precisely, finding certain eigenvalues and eigenvectors of a Hamiltonian!
    Moreover, the number of eigenvectors grows exponentially with the number of qubits
    of our system, so, even if we knew them, computing expectation values in this
    way might be very computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we need to take an indirect route. For this, we will use the fact that
    we can always express an observable ![A](img/file183.png "A") on ![n](img/file244.png
    "n") qubits as a linear combination of tensor products of Pauli matrices (see,
    for example, *Chapter 7* on the famous lecture notes by John Preskill [[77](ch030.xhtml#Xpreskill1998lecture)]).
    Actually, ![A](img/file183.png "A") will be, in most cases, given to us in such
    a form, in the same way that the Hamiltonians of our combinatorial optimization
    problems were always expressed as sums of tensor products of ![Z](img/file8.png
    "Z") matrices.
  prefs: []
  type: TYPE_NORMAL
- en: So, consider, for example, that we are given an observable
  prefs: []
  type: TYPE_NORMAL
- en: '![A = \frac{1}{2}Z \otimes I \otimes X - 3I \otimes Y \otimes Y + 2Z \otimes
    X \otimes Z.](img/file973.png "A = \frac{1}{2}Z \otimes I \otimes X - 3I \otimes
    Y \otimes Y + 2Z \otimes X \otimes Z.")'
  prefs: []
  type: TYPE_IMG
- en: Notice that, thanks to linearity,
  prefs: []
  type: TYPE_NORMAL
- en: '![\begin{array}{rlrl} {\left\langle \psi \right|A\left| \psi \right\rangle}
    & {= \left\langle \psi \right|\left( {\frac{1}{2}Z \otimes I \otimes X - 3I \otimes
    Y \otimes Y + 2Z \otimes X \otimes Z} \right)\left| \psi \right\rangle\qquad}
    & & \qquad \\ & {= \left\langle \psi \right|\left( {\frac{1}{2}\left( {Z \otimes
    I \otimes X} \right)\left| \psi \right\rangle - 3\left( {I \otimes Y \otimes Y}
    \right)\left| \psi \right\rangle + 2\left( {Z \otimes X \otimes Z} \right)\left|
    \psi \right\rangle} \right)\qquad} & & \qquad \\ & {= \frac{1}{2}\left\langle
    \psi \right|\left( {Z \otimes I \otimes X} \right)\left| \psi \right\rangle -
    3\left\langle \psi \right|\left( {I \otimes Y \otimes Y} \right)\left| \psi \right\rangle
    + 2\left\langle \psi \right|\left( {Z \otimes X \otimes Z} \right)\left| \psi
    \right\rangle.\qquad} & & \qquad \\ \end{array}](img/file974.png "\begin{array}{rlrl}
    {\left\langle \psi \right|A\left| \psi \right\rangle} & {= \left\langle \psi \right|\left(
    {\frac{1}{2}Z \otimes I \otimes X - 3I \otimes Y \otimes Y + 2Z \otimes X \otimes
    Z} \right)\left| \psi \right\rangle\qquad} & & \qquad \\  & {= \left\langle \psi
    \right|\left( {\frac{1}{2}\left( {Z \otimes I \otimes X} \right)\left| \psi \right\rangle
    - 3\left( {I \otimes Y \otimes Y} \right)\left| \psi \right\rangle + 2\left( {Z
    \otimes X \otimes Z} \right)\left| \psi \right\rangle} \right)\qquad} & & \qquad
    \\  & {= \frac{1}{2}\left\langle \psi \right|\left( {Z \otimes I \otimes X} \right)\left|
    \psi \right\rangle - 3\left\langle \psi \right|\left( {I \otimes Y \otimes Y}
    \right)\left| \psi \right\rangle + 2\left\langle \psi \right|\left( {Z \otimes
    X \otimes Z} \right)\left| \psi \right\rangle.\qquad} & & \qquad \\ \end{array}")'
  prefs: []
  type: TYPE_IMG
- en: Then, in order to compute the expectation value of ![A](img/file183.png "A"),
    we can compute the expectation values of ![Z \otimes I \otimes X](img/file975.png
    "Z \otimes I \otimes X"), ![I \otimes Y \otimes Y](img/file976.png "I \otimes
    Y \otimes Y"), and ![Z \otimes X \otimes Z](img/file977.png "Z \otimes X \otimes
    Z") and combine their results. But wait a minute! Isn’t that even more complicated?
    After all, we would need to compute three expectation values instead of just one,
    right?
  prefs: []
  type: TYPE_NORMAL
- en: The key observation here lies in the fact that, while we may not know the eigenvalues
    and eigenvectors of ![A](img/file183.png "A") in advance, we can very easily obtain
    those of ![Z \otimes I \otimes X](img/file975.png "Z \otimes I \otimes X") or
    any other tensor product of Pauli matrices. It is so easy, in fact, that you will
    now learn how to do it yourself in the following two exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.2
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that ![\left| \lambda_{j} \right\rangle](img/file978.png "\left| \lambda_{j}
    \right\rangle") is an eigenvector of ![A_{j}](img/file979.png "A_{j}") with associated
    eigenvalue ![\lambda_{j}](img/file945.png "\lambda_{j}") for ![j = 1,\ldots,n](img/file980.png
    "j = 1,\ldots,n"). Prove that ![\left| \lambda_{1} \right\rangle \otimes \cdots
    \otimes \left| \lambda_{n} \right\rangle](img/file981.png "\left| \lambda_{1}
    \right\rangle \otimes \cdots \otimes \left| \lambda_{n} \right\rangle") is an
    eigenvector of ![A_{1} \otimes \cdots \otimes A_{n}](img/file982.png "A_{1} \otimes
    \cdots \otimes A_{n}") with associated eigenvalue ![\lambda_{1} \cdot \ldots \cdot
    \lambda_{n}](img/file983.png "\lambda_{1} \cdot \ldots \cdot \lambda_{n}").
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.3
  prefs: []
  type: TYPE_NORMAL
- en: 'Prove that:'
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvectors of ![Z](img/file8.png "Z") are ![\left| 0 \right\rangle](img/file6.png
    "\left| 0 \right\rangle") (with associated eigenvalue ![1](img/file13.png "1"))
    and ![\left| 1 \right\rangle](img/file14.png "\left| 1 \right\rangle") (with associated
    eigenvalue ![- 1](img/file312.png "- 1")).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The eigenvectors of ![X](img/file9.png "X") are ![\left| + \right\rangle](img/file61.png
    "\left| + \right\rangle") (with associated eigenvalue ![1](img/file13.png "1"))
    and ![\left| - \right\rangle](img/file63.png "\left| - \right\rangle") (with associated
    eigenvalue ![- 1](img/file312.png "- 1")).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The eigenvectors of ![Y](img/file11.png "Y") are ![\left( 1\slash\sqrt{2} \right)\left(
    {\left| 0 \right\rangle + i\left| 1 \right\rangle} \right)](img/file984.png "\left(
    1\slash\sqrt{2} \right)\left( {\left| 0 \right\rangle + i\left| 1 \right\rangle}
    \right)") (with associated eigenvalue ![1](img/file13.png "1")) and ![\left( 1\slash\sqrt{2}
    \right)\left( {\left| 0 \right\rangle - i\left| 1 \right\rangle} \right)](img/file985.png
    "\left( 1\slash\sqrt{2} \right)\left( {\left| 0 \right\rangle - i\left| 1 \right\rangle}
    \right)") (with associated eigenvalue ![- 1](img/file312.png "- 1")).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any non-null state is an eigenvector of ![I](img/file53.png "I") with associated
    eigenvalue ![1](img/file13.png "1").
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the results in these exercises, we can readily deduce that ![\left| 0
    \right\rangle\left| + \right\rangle\left| 0 \right\rangle](img/file986.png "\left|
    0 \right\rangle\left| + \right\rangle\left| 0 \right\rangle"), ![\left| 0 \right\rangle\left|
    - \right\rangle\left| 1 \right\rangle](img/file987.png "\left| 0 \right\rangle\left|
    - \right\rangle\left| 1 \right\rangle"), ![\left| 1 \right\rangle\left| + \right\rangle\left|
    1 \right\rangle](img/file988.png "\left| 1 \right\rangle\left| + \right\rangle\left|
    1 \right\rangle"), and ![\left| 1 \right\rangle\left| - \right\rangle\left| 0
    \right\rangle](img/file989.png "\left| 1 \right\rangle\left| - \right\rangle\left|
    0 \right\rangle") are eigenvectors of ![Z \otimes X \otimes Z](img/file977.png
    "Z \otimes X \otimes Z") with eigenvalue ![1](img/file13.png "1") and that ![\left|
    0 \right\rangle\left| + \right\rangle\left| 1 \right\rangle](img/file990.png "\left|
    0 \right\rangle\left| + \right\rangle\left| 1 \right\rangle"), ![\left| 0 \right\rangle\left|
    - \right\rangle\left| 0 \right\rangle](img/file991.png "\left| 0 \right\rangle\left|
    - \right\rangle\left| 0 \right\rangle"), ![\left| 1 \right\rangle\left| + \right\rangle\left|
    0 \right\rangle](img/file992.png "\left| 1 \right\rangle\left| + \right\rangle\left|
    0 \right\rangle"), and ![\left| 1 \right\rangle\left| - \right\rangle\left| 1
    \right\rangle](img/file993.png "\left| 1 \right\rangle\left| - \right\rangle\left|
    1 \right\rangle") are eigenvectors of ![Z \otimes X \otimes Z](img/file977.png
    "Z \otimes X \otimes Z") with eigenvalue ![- 1](img/file312.png "- 1"). All these
    states together form an orthonormal basis of eigenvectors of ![Z \otimes X \otimes
    Z](img/file977.png "Z \otimes X \otimes Z"), as you can easily check if you compute
    their inner products.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.4
  prefs: []
  type: TYPE_NORMAL
- en: Find orthonormal bases of eigenvectors for ![Z \otimes I \otimes X](img/file975.png
    "Z \otimes I \otimes X") and ![I \otimes Y \otimes Y](img/file976.png "I \otimes
    Y \otimes Y"). Compute their associated eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: So, now we know how to obtain the eigenvalues and eigenvectors of any tensor
    product of Pauli matrices. How can we use this to estimate their expectation values?
    Remember that, given a Hermitian matrix ![A](img/file183.png "A"), we can compute
    ![\left\langle \psi \right|A\left| \psi \right\rangle](img/file970.png "\left\langle
    \psi \right|A\left| \psi \right\rangle") by
  prefs: []
  type: TYPE_NORMAL
- en: '![\sum\limits_{j,k}\left| \left\langle \lambda_{j}^{k} \middle| \psi \right\rangle
    \right|^{2}\lambda_{j},](img/file994.png "\sum\limits_{j,k}\left| \left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle \right|^{2}\lambda_{j},")'
  prefs: []
  type: TYPE_IMG
- en: 'where the eigenvalues of ![A](img/file183.png "A") are ![\lambda_{j}](img/file945.png
    "\lambda_{j}") and the associated eigenvectors are ![{\{\left| \lambda_{j}^{k}
    \right\rangle\}}_{j,k}](img/file949.png "{\{\left| \lambda_{j}^{k} \right\rangle\}}_{j,k}").
    In our case, we only have two eigenvalues: ![1](img/file13.png "1") and ![- 1](img/file312.png
    "- 1"). So, if we are able to estimate the values ![\left| \left\langle \lambda_{j}^{k}
    \middle| \psi \right\rangle \right|^{2}](img/file972.png "\left| \left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle \right|^{2}"), we will have all the
    ingredients needed to ”cook” our expectation values.'
  prefs: []
  type: TYPE_NORMAL
- en: A priori, trying to get the values ![\left| \left\langle \lambda_{j}^{k} \middle|
    \psi \right\rangle \right|^{2}](img/file972.png "\left| \left\langle \lambda_{j}^{k}
    \middle| \psi \right\rangle \right|^{2}") out of a quantum computer can seem like
    a difficult task. For example, you may wonder whether it will be necessary to
    perform some weird fancy measurements on our quantum device in order to get these
    probabilities! Well, it turns out that we can easily estimate them on any quantum
    computer using ordinary measurements in the computational basis and a bunch of
    quantum gates. So, don’t worry. If you’ve just bought yourself a flashy quantum
    computer, there’s no need for a hardware upgrade just yet.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, how can we actually estimate these ![\left| \left\langle \lambda_{j}^{k}
    \middle| \psi \right\rangle \right|^{2}](img/file972.png "\left| \left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle \right|^{2}") values with the tools
    that we have? Let’s first work with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider the observable ![Z \otimes X \otimes Z](img/file977.png "Z \otimes
    X \otimes Z"). We have previously in this section obtained its eigenvectors, so
    let’s focus on one of them: ![\left| 0 \right\rangle\left| + \right\rangle\left|
    0 \right\rangle](img/file986.png "\left| 0 \right\rangle\left| + \right\rangle\left|
    0 \right\rangle"). If we wanted to compute ![\left| {\left( {\left\langle 0 \right|\left\langle
    + \right|\left\langle 0 \right|} \right)\left| \psi \right\rangle} \right|^{2}](img/file995.png
    "\left| {\left( {\left\langle 0 \right|\left\langle + \right|\left\langle 0 \right|}
    \right)\left| \psi \right\rangle} \right|^{2}"), where ![\left| \psi \right\rangle](img/file43.png
    "\left| \psi \right\rangle") is a certain 3-qubit state, we could just notice
    that'
  prefs: []
  type: TYPE_NORMAL
- en: '![\left| 0 \right\rangle\left| + \right\rangle\left| 0 \right\rangle = \left(
    {I \otimes H \otimes I} \right)\left| 0 \right\rangle\left| 0 \right\rangle\left|
    0 \right\rangle](img/file996.png "\left| 0 \right\rangle\left| + \right\rangle\left|
    0 \right\rangle = \left( {I \otimes H \otimes I} \right)\left| 0 \right\rangle\left|
    0 \right\rangle\left| 0 \right\rangle")'
  prefs: []
  type: TYPE_IMG
- en: and hence
  prefs: []
  type: TYPE_NORMAL
- en: '![\begin{array}{rlrl} {\left\langle 0 \right|\left\langle + \right|\left\langle
    0 \right|} & {= \left( {\left| 0 \right\rangle\left| + \right\rangle\left| 0 \right\rangle}
    \right)^{\dagger} = \left( {\left( {I \otimes H \otimes I} \right)\left| 0 \right\rangle\left|
    0 \right\rangle\left| 0 \right\rangle} \right)^{\dagger} = \left\langle 0 \right|\left\langle
    0 \right|\left\langle 0 \right|\left( {I \otimes H \otimes I} \right)^{\dagger}\qquad}
    & & \qquad \\ & {= \left\langle 0 \right|\left\langle 0 \right|\left\langle 0
    \right|\left( {I \otimes H \otimes I} \right),\qquad} & & \qquad \\ \end{array}](img/file997.png
    "\begin{array}{rlrl} {\left\langle 0 \right|\left\langle + \right|\left\langle
    0 \right|} & {= \left( {\left| 0 \right\rangle\left| + \right\rangle\left| 0 \right\rangle}
    \right)^{\dagger} = \left( {\left( {I \otimes H \otimes I} \right)\left| 0 \right\rangle\left|
    0 \right\rangle\left| 0 \right\rangle} \right)^{\dagger} = \left\langle 0 \right|\left\langle
    0 \right|\left\langle 0 \right|\left( {I \otimes H \otimes I} \right)^{\dagger}\qquad}
    & & \qquad \\  & {= \left\langle 0 \right|\left\langle 0 \right|\left\langle 0
    \right|\left( {I \otimes H \otimes I} \right),\qquad} & & \qquad \\ \end{array}")'
  prefs: []
  type: TYPE_IMG
- en: where we have used the fact that ![I](img/file53.png "I") and ![H](img/file10.png
    "H") are self-adjoint, and hence so is ![I \otimes H \otimes I](img/file998.png
    "I \otimes H \otimes I"). Keep in mind, however, that we will still write daggers
    throughout this example whenever we mean to consider the adjoint of ![I \otimes
    H \otimes I](img/file998.png "I \otimes H \otimes I") — even if it still represents
    the same operator.
  prefs: []
  type: TYPE_NORMAL
- en: From this, it follows directly that
  prefs: []
  type: TYPE_NORMAL
- en: '![\left| {\left( {\left\langle 0 \right|\left\langle + \right|\left\langle
    0 \right|} \right)\left| \psi \right\rangle} \right|^{2} = \left| {\left\langle
    0 \right|\left\langle 0 \right|\left\langle 0 \right|\left( {I \otimes H \otimes
    I} \right)^{\dagger}\left| \psi \right\rangle} \right|^{2}.](img/file999.png "\left|
    {\left( {\left\langle 0 \right|\left\langle + \right|\left\langle 0 \right|} \right)\left|
    \psi \right\rangle} \right|^{2} = \left| {\left\langle 0 \right|\left\langle 0
    \right|\left\langle 0 \right|\left( {I \otimes H \otimes I} \right)^{\dagger}\left|
    \psi \right\rangle} \right|^{2}.")'
  prefs: []
  type: TYPE_IMG
- en: But for any state ![\left| \varphi \right\rangle](img/file44.png "\left| \varphi
    \right\rangle"), we know that ![\left| {\left( {\left\langle 0 \right|\left\langle
    0 \right|\left\langle 0 \right|} \right)\left| \varphi \right\rangle} \right|^{2}](img/file1000.png
    "\left| {\left( {\left\langle 0 \right|\left\langle 0 \right|\left\langle 0 \right|}
    \right)\left| \varphi \right\rangle} \right|^{2}") is the probability of obtaining
    ![\left| 0 \right\rangle\left| 0 \right\rangle\left| 0 \right\rangle](img/file1001.png
    "\left| 0 \right\rangle\left| 0 \right\rangle\left| 0 \right\rangle") when measuring
    it in the computational basis. As a consequence, we can estimate the value ![\left|
    {\left( {\left\langle 0 \right|\left\langle + \right|\left\langle 0 \right|} \right)\left|
    \psi \right\rangle} \right|^{2}](img/file995.png "\left| {\left( {\left\langle
    0 \right|\left\langle + \right|\left\langle 0 \right|} \right)\left| \psi \right\rangle}
    \right|^{2}") by repeatedly preparing the state ![\left( {I \otimes H \otimes
    I} \right)\left| \psi \right\rangle = \left( {I \otimes H \otimes I} \right)^{\dagger}\left|
    \psi \right\rangle](img/file1002.png "\left( {I \otimes H \otimes I} \right)\left|
    \psi \right\rangle = \left( {I \otimes H \otimes I} \right)^{\dagger}\left| \psi
    \right\rangle"), measuring it in the computational basis, and then computing the
    relative frequency of ![\left| 0 \right\rangle\left| 0 \right\rangle\left| 0 \right\rangle](img/file1001.png
    "\left| 0 \right\rangle\left| 0 \right\rangle\left| 0 \right\rangle").
  prefs: []
  type: TYPE_NORMAL
- en: And this is not the only eigenvector for which this works. It turns out that
    for each and every eigenvector ![\left| \lambda_{A} \right\rangle](img/file1003.png
    "\left| \lambda_{A} \right\rangle") of ![Z \otimes X \otimes Z](img/file977.png
    "Z \otimes X \otimes Z"), there is a unique state in the computational basis ![\left|
    \lambda_{C} \right\rangle](img/file1004.png "\left| \lambda_{C} \right\rangle")
    such that
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\left&#124; \lambda_{A} \right\rangle = (I \otimes H \otimes I)\left&#124;
    \lambda_{C} \right\rangle.](img/file1005.png "\left&#124; \lambda_{A} \right\rangle
    = (I \otimes H \otimes I)\left&#124; \lambda_{C} \right\rangle.") |'
  prefs: []
  type: TYPE_TB
- en: 'Actually, the correspondence is bijective: for every state in the computational
    basis ![\left| \lambda_{C} \right\rangle](img/file1004.png "\left| \lambda_{C}
    \right\rangle"), there is also a unique eigenvector ![\left| \lambda_{A} \right\rangle](img/file1003.png
    "\left| \lambda_{A} \right\rangle") of ![Z \otimes X \otimes Z](img/file977.png
    "Z \otimes X \otimes Z") such that ![\left| \lambda_{C} \right\rangle = {(I \otimes
    H \otimes I)}^{\dagger}\left| \lambda_{A} \right\rangle](img/file1006.png "\left|
    \lambda_{C} \right\rangle = {(I \otimes H \otimes I)}^{\dagger}\left| \lambda_{A}
    \right\rangle"), where we have used the fact that, for unitary operators, ![U^{\dagger}
    = U^{- 1}](img/file1007.png "U^{\dagger} = U^{- 1}"). For example,'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\left&#124; 1 \right\rangle\left&#124; - \right\rangle\left&#124; 1 \right\rangle
    = \left( {I \otimes H \otimes I} \right)\left&#124; 1 \right\rangle\left&#124;
    1 \right\rangle\left&#124; 1 \right\rangle,\qquad\left&#124; 1 \right\rangle\left&#124;
    1 \right\rangle\left&#124; 1 \right\rangle = {(I \otimes H \otimes I)}^{\dagger}\left&#124;
    1 \right\rangle\left&#124; - \right\rangle\left&#124; 1 \right\rangle.](img/file1008.png
    "\left&#124; 1 \right\rangle\left&#124; - \right\rangle\left&#124; 1 \right\rangle
    = \left( {I \otimes H \otimes I} \right)\left&#124; 1 \right\rangle\left&#124;
    1 \right\rangle\left&#124; 1 \right\rangle,\qquad\left&#124; 1 \right\rangle\left&#124;
    1 \right\rangle\left&#124; 1 \right\rangle = {(I \otimes H \otimes I)}^{\dagger}\left&#124;
    1 \right\rangle\left&#124; - \right\rangle\left&#124; 1 \right\rangle.") |'
  prefs: []
  type: TYPE_TB
- en: This is the reason why we call ![I \otimes H \otimes I](img/file998.png "I \otimes
    H \otimes I") the **change of basis operator** between the computational basis
    and the basis of eigenvectors of ![Z \otimes X \otimes Z](img/file977.png "Z \otimes
    X \otimes Z").
  prefs: []
  type: TYPE_NORMAL
- en: In this way, if we want to estimate the probabilities ![\left| \left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle \right|^{2}](img/file972.png "\left|
    \left\langle \lambda_{j}^{k} \middle| \psi \right\rangle \right|^{2}") when the
    states ![\left| \lambda_{j}^{k} \right\rangle](img/file947.png "\left| \lambda_{j}^{k}
    \right\rangle") happen to be the eigenvectors of ![Z \otimes X \otimes Z](img/file977.png
    "Z \otimes X \otimes Z"), we just need to prepare ![\left( {I \otimes H \otimes
    I} \right)^{\dagger}\left| \psi \right\rangle](img/file1009.png "\left( {I \otimes
    H \otimes I} \right)^{\dagger}\left| \psi \right\rangle") and measure it in the
    computational basis. Then, given any eigenvector ![\left| \lambda_{A} \right\rangle](img/file1003.png
    "\left| \lambda_{A} \right\rangle") of ![Z \otimes X \otimes Z](img/file977.png
    "Z \otimes X \otimes Z"), the probability ![\left| \left\langle \lambda_{A} \middle|
    \psi \right\rangle \right|^{2}](img/file1010.png "\left| \left\langle \lambda_{A}
    \middle| \psi \right\rangle \right|^{2}") can be estimated by the relative frequency
    of the measurement outcome associated to the eigenstate ![\left| \lambda_{C} \right\rangle
    = {(I \otimes H \otimes I)}^{\dagger}\left| \lambda_{A} \right\rangle](img/file1006.png
    "\left| \lambda_{C} \right\rangle = {(I \otimes H \otimes I)}^{\dagger}\left|
    \lambda_{A} \right\rangle") in the computational basis. That’s because
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\left\langle \lambda_{C} \right&#124;\left( {{(I \otimes H \otimes I)}^{\dagger}\left&#124;
    \psi \right\rangle} \right) = \left\langle \lambda_{A} \right&#124;\left( {(I
    \otimes H \otimes I){(I \otimes H \otimes I)}^{\dagger}\left&#124; \psi \right\rangle}
    \right) = \left\langle \lambda_{A} \middle&#124; \psi \right\rangle,](img/file1011.png
    "\left\langle \lambda_{C} \right&#124;\left( {{(I \otimes H \otimes I)}^{\dagger}\left&#124;
    \psi \right\rangle} \right) = \left\langle \lambda_{A} \right&#124;\left( {(I
    \otimes H \otimes I){(I \otimes H \otimes I)}^{\dagger}\left&#124; \psi \right\rangle}
    \right) = \left\langle \lambda_{A} \middle&#124; \psi \right\rangle,") |'
  prefs: []
  type: TYPE_TB
- en: where we have used the fact that, for any operator ![L](img/file1012.png "L")
    and any states ![\left| \alpha \right\rangle](img/file1013.png "\left| \alpha
    \right\rangle") and ![\left| \beta \right\rangle](img/file1014.png "\left| \beta
    \right\rangle"), if ![\left| \beta \right\rangle = L\left| \alpha \right\rangle](img/file1015.png
    "\left| \beta \right\rangle = L\left| \alpha \right\rangle"), then ![\left\langle
    \beta \right| = \left\langle \alpha \right|L^{\dagger}](img/file1016.png "\left\langle
    \beta \right| = \left\langle \alpha \right|L^{\dagger}"), and ![L^{\dagger \dagger}
    = L](img/file1017.png "L^{\dagger \dagger} = L").
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final note, in this example, when we set out to compute the probabilities
    ![\left| \left\langle \lambda_{A} \middle| \psi \right\rangle \right|^{2}](img/file1010.png
    "\left| \left\langle \lambda_{A} \middle| \psi \right\rangle \right|^{2}"), we
    don’t have to run executions for each of the probabilities individually: we can
    compute them all simultaneously. All we have to do is measure ![{(I \otimes H
    \otimes I)}^{\dagger}\left| \psi \right\rangle](img/file1018.png "{(I \otimes
    H \otimes I)}^{\dagger}\left| \psi \right\rangle") in the computational basis
    a bunch of times and then retrieve the relative frequency of every outcome. This
    works because ![{(I \otimes H \otimes I)}^{\dagger}](img/file1019.png "{(I \otimes
    H \otimes I)}^{\dagger}") transforms all the eigenvectors of ![A](img/file183.png
    "A") into the states of the computational basis. Then, the probability ![\left|
    \left\langle \lambda_{A} \middle| \psi \right\rangle \right|^{2}](img/file1010.png
    "\left| \left\langle \lambda_{A} \middle| \psi \right\rangle \right|^{2}") will
    be the relative frequency of the outcome, in the computational basis, associated
    to ![{(I \otimes H \otimes I)}^{\dagger}\left| \lambda_{A} \right\rangle](img/file1020.png
    "{(I \otimes H \otimes I)}^{\dagger}\left| \lambda_{A} \right\rangle"). Of course,
    the higher the number of preparations and measurements, the more accurate our
    estimates will be.'
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: Notice the similarity of this kind of procedure with the standard measurement
    in the computational basis. When we measure ![\left| \psi \right\rangle](img/file43.png
    "\left| \psi \right\rangle") in the computational basis, we have probability ![\left|
    \left\langle x \middle| \psi \right\rangle \right|^{2}](img/file1021.png "\left|
    \left\langle x \middle| \psi \right\rangle \right|^{2}") of obtaining the outcome
    associated to ![\left| x \right\rangle](img/file267.png "\left| x \right\rangle").
    If we were measuring an observable that had all the ![\left| \lambda_{j}^{k} \right\rangle](img/file947.png
    "\left| \lambda_{j}^{k} \right\rangle") as eigenvectors with a distinct eigenvalue
    for each of them — this is an observable that’s able to distinguish all the eigenvectors
    in the basis — we would have probability ![\left| \left\langle \lambda_{j}^{k}
    \middle| \psi \right\rangle \right|^{2}](img/file972.png "\left| \left\langle
    \lambda_{j}^{k} \middle| \psi \right\rangle \right|^{2}") of getting the outcome
    associated to ![\left| \lambda_{j}^{k} \right\rangle](img/file947.png "\left|
    \lambda_{j}^{k} \right\rangle").
  prefs: []
  type: TYPE_NORMAL
- en: This is why we refer to the process of changing basis and, then, measuring in
    the computational basis, as performing a **measurement** **in the eigenvector
    basis** ![\{\left| \lambda_{j}^{k} \right\rangle\}](img/file1022.png "\{\left|
    \lambda_{j}^{k} \right\rangle\}") **of** ![A](img/file183.png "A"). It is exactly
    the same as if we had an observable that’s able to measure and distinguish all
    the eigenvectors of ![A](img/file183.png "A").
  prefs: []
  type: TYPE_NORMAL
- en: But wait, there’s more! Our being able to change bases in this case is by no
    means a happy coincidence. It turns out that for every tensor product of Pauli
    matrices ![A](img/file183.png "A"), there is a simple change of basis matrix that
    defines a perfect correspondence between the states in the computational basis
    and the eigenvectors of ![A](img/file183.png "A"). Again, this can be readily
    verified, and we invite you to do it in the following two exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.5
  prefs: []
  type: TYPE_NORMAL
- en: Since the computational basis is an eigenvector basis of ![Z](img/file8.png
    "Z"), a change of basis operator of ![Z](img/file8.png "Z") can be the identity
    ![I](img/file53.png "I"). Check that, in order to change from the computational
    basis to the basis of eigenvectors of the ![X](img/file9.png "X"), you can use
    the Hadamard matrix ![H](img/file10.png "H"), and that to change to the basis
    of eigenvectors of ![Y](img/file11.png "Y") you can use ![SH](img/file1023.png
    "SH").
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.6
  prefs: []
  type: TYPE_NORMAL
- en: Prove that if ![U_{1}](img/file175.png "U_{1}") and ![U_{2}](img/file176.png
    "U_{2}") are the respective change of basis operators from the computational basis
    to the eigenvector basis of two observables ![A_{1}](img/file1024.png "A_{1}")
    and ![A_{2}](img/file1025.png "A_{2}"), then ![U_{1} \otimes U_{2}](img/file179.png
    "U_{1} \otimes U_{2}") is the change of basis operator from the computational
    basis to the eigenvector basis of ![A_{1} \otimes A_{2}](img/file1026.png "A_{1}
    \otimes A_{2}").
  prefs: []
  type: TYPE_NORMAL
- en: Putting everything together, we can easily deduce that, for instance, ![I \otimes
    I \otimes H](img/file1027.png "I \otimes I \otimes H") takes the eigenvectors
    of ![Z \otimes I \otimes X](img/file975.png "Z \otimes I \otimes X") to the computational
    basis and that ![I \otimes {(SH)}^{\dagger} \otimes {(SH)}^{\dagger}](img/file1028.png
    "I \otimes {(SH)}^{\dagger} \otimes {(SH)}^{\dagger}") takes the eigenvectors
    of ![I \otimes Y \otimes Y](img/file976.png "I \otimes Y \otimes Y") to states
    in the computational basis as well.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in order to estimate the expectation value ![\left\langle \psi \right|\left(
    {Z \otimes I \otimes X} \right)\left| \psi \right\rangle](img/file1029.png "\left\langle
    \psi \right|\left( {Z \otimes I \otimes X} \right)\left| \psi \right\rangle"),
    we can use whatever circuit we need to prepare ![\left| \psi \right\rangle](img/file43.png
    "\left| \psi \right\rangle") followed by ![{(I \otimes I \otimes H)}^{\dagger}
    = I \otimes I \otimes H](img/file1030.png "{(I \otimes I \otimes H)}^{\dagger}
    = I \otimes I \otimes H"), then measure in the computational basis, and then get
    the probabilities as we have just discussed. In a similar way, to estimate ![\left\langle
    \psi \right|\left( {I \otimes Y \otimes Y} \right)\left| \psi \right\rangle](img/file1031.png
    "\left\langle \psi \right|\left( {I \otimes Y \otimes Y} \right)\left| \psi \right\rangle"),
    we will first prepare ![\left| \psi \right\rangle](img/file43.png "\left| \psi
    \right\rangle"), then apply ![I \otimes HS^{\dagger} \otimes HS^{\dagger}](img/file1032.png
    "I \otimes HS^{\dagger} \otimes HS^{\dagger}") and, finally, measure in the computational
    basis.
  prefs: []
  type: TYPE_NORMAL
- en: Notice, by the way, how ![I](img/file53.png "I") and ![H](img/file10.png "H")
    are self-adjoint, so, when we took their adjoints, there was no observable (no
    pun intended) effect. That’s not the case with ![SH](img/file1023.png "SH"), because
    ![{(SH)}^{\dagger} = H^{\dagger}S^{\dagger} = HS^{\dagger}](img/file1033.png "{(SH)}^{\dagger}
    = H^{\dagger}S^{\dagger} = HS^{\dagger}").
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: For any Hermitian operator ![A](img/file183.png "A"), there is always a unitary
    transformation that takes any basis of eigenvectors of ![A](img/file183.png "A")
    to the computational basis, and vice versa. However, this transformation could
    very well be difficult to implement. In the case where ![A](img/file183.png "A")
    is a tensor product of Pauli matrices we have just proved that we can always obtain
    the transformation as the tensor product of very simple one-qubit operations.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, after we have estimated the expectation value of every Pauli term in
    our observable (in our case, ![Z \otimes I \otimes X](img/file975.png "Z \otimes
    I \otimes X"), ![I \otimes Y \otimes Y](img/file976.png "I \otimes Y \otimes Y"),
    and ![Z \otimes X \otimes Z](img/file977.png "Z \otimes X \otimes Z")) we can
    multiply them by the corresponding coefficients in the linear combination and
    add everything together to get the final result. And we are done!
  prefs: []
  type: TYPE_NORMAL
- en: Now you know how to estimate the expectation value of an observable by measuring
    in different bases. You can proudly say, as the famous internet meme goes, ”All
    your base are belong to us.” And, in fact, this was the last technical element
    that we needed in order to introduce VQE, something we will immediately do in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Introducing VQE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of the **Variational Quantum Eigensolver** (**VQE**) is to find a ground
    state of a given Hamiltonian ![H_{1}](img/file544.png "H_{1}"). This Hamiltonian
    can describe, for instance, the energy of a certain physical or chemical process,
    and we will use some such examples in the following two sections, which will cover
    how to execute VQE with Qiskit and PennyLane. For the moment, however, we will
    keep everything abstract and focus on finding a state ![\left| \psi \right\rangle](img/file43.png
    "\left| \psi \right\rangle") such that ![\left\langle \psi \right|H_{1}\left|
    \psi \right\rangle](img/file703.png "\left\langle \psi \right|H_{1}\left| \psi
    \right\rangle") is minimum. Note that in this section, we will be using ![H_{1}](img/file544.png
    "H_{1}") to refer to the Hamiltonian so that it does not get confused with the
    Hadamard matrix that we will also be using in our computations.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: 'VQE is by no means the only quantum algorithm that has been proposed to find
    the ground states of Hamiltonians. Some very promising options use a quantum subroutine
    known as **Quantum** **Phase Estimation** (**QPE**) (see, for instance, the excellent
    surveys by McArdle et al. [[66](ch030.xhtml#Xmcardle2020quantum)] and by Cao et
    al. [[22](ch030.xhtml#Xcao2019quantum)]). The main disadvantage of these approaches
    is that QPE uses the Quantum Fourier Transform that we studied in *Chapter* *[*6*](ch014.xhtml#x1-1060006),
    *GAS: Grover Adaptative Search*, and, thus, requires quantum computers that are
    resilient to noise. An experimental demonstration of these limitations (and of
    the relative robustness of VQE) can be found, for instance, in the paper by O’Malley
    et al. [[70](ch030.xhtml#Xo2016scalable)]. For this reason, we will focus mainly
    on VQE and its applications, which seem to obtain better results with the NISQ
    computers that are available today.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The general structure of VQE is very similar to that of QAOA, which you surely
    remember from *Chapter* *[*5*](ch013.xhtml#x1-940005), *QAOA: Quantum Approximate
    Optimization* *Algorithm*: we prepare a parameterized quantum state, we measure
    it, we estimate its energy, and we change the parameters in order to minimize
    it; then, we repeat this process several times until some stopping criteria are
    met. The preparation and measurement of the state are done on the quantum computer,
    while the energy estimation and parameter minimization are handled by a classical
    computer.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The parametrized circuit, usually called **variational form** or **ansatz**,
    is usually chosen taking into account information from the problem domain. For
    instance, you could consider ansatzes that parametrize typical solutions to the
    kind of problem under study. We will show some examples of this in the last two
    sections of this same chapter. In any case, the ansatz is selected in advance
    and it is usually easy to implement on a quantum circuit.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'In many applications, we can distinguish two parts in the creation of the parameterized
    state: the preparation of an initial state ![\left| \psi_{0} \right\rangle](img/file631.png
    "\left| \psi_{0} \right\rangle"), that does not depend on any parameters, and
    then the variational form ![V(\theta)](img/file1034.png "V(\theta)") itself, that
    obviously depends on ![\theta](img/file89.png "\theta"). Thus, if we have ![\left|
    \psi_{0} \right\rangle = U\left| 0 \right\rangle](img/file1035.png "\left| \psi_{0}
    \right\rangle = U\left| 0 \right\rangle"), for some unitary transformation ![U](img/file51.png
    "U") implemented with some quantum gates, the ansatz gives us the state ![V(\theta)U\left|
    0 \right\rangle](img/file1036.png "V(\theta)U\left| 0 \right\rangle"). Notice,
    however, that we can always consider the whole operation ![V(\theta)U](img/file1037.png
    "V(\theta)U") as the ansatz and require the initial state to be ![\left| 0 \right\rangle](img/file6.png
    "\left| 0 \right\rangle"). To simplify our notation, this is what we will usually
    do, although we will explicitly distinguish between initial state and ansatz in
    some practical examples that we will consider later in the chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm [7.1](#x1-123008r1) gives the pseudocode for VQE. Notice the similarities
    with Algorithm [5.1](ch013.xhtml#x1-97007r1) from *Chapter* *[*5*](ch013.xhtml#x1-940005),
    *QAOA: Quantum Approximate Optimization* *Algorithm*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Algorithm 7.1** (VQE)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Require:** ![H_{1}](img/file544.png "H_{1}") given as a linear combination
    of tensor products of Pauli matrices'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a variational form (ansatz) ![V(\theta)](img/file1034.png "V(\theta)")
  prefs: []
  type: TYPE_NORMAL
- en: Choose a starting set of values for ![\theta](img/file89.png "\theta")
  prefs: []
  type: TYPE_NORMAL
- en: '**while** the stopping criteria are not met **do**'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the state ![\left| {\psi(\theta)} \right\rangle = V(\theta)\left| 0
    \right\rangle](img/file1038.png "\left| {\psi(\theta)} \right\rangle = V(\theta)\left|
    0 \right\rangle")    ![\vartriangleright](img/file655.png "\vartriangleright")
    *This* *is done on the quantum computer!*
  prefs: []
  type: TYPE_NORMAL
- en: From the measurements of ![\left| {\psi(\theta)} \right\rangle](img/file1039.png
    "\left| {\psi(\theta)} \right\rangle") in different bases, estimate ![\left\langle
    {\psi(\theta)} \right|H_{1}\left| {\psi(\theta)} \right\rangle](img/file1040.png
    "\left\langle {\psi(\theta)} \right|H_{1}\left| {\psi(\theta)} \right\rangle")
  prefs: []
  type: TYPE_NORMAL
- en: Update ![\theta](img/file89.png "\theta") according to the minimization algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '-'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the state ![\left| {\psi(\theta)} \right\rangle = V(\theta)\left| 0
    \right\rangle](img/file1038.png "\left| {\psi(\theta)} \right\rangle = V(\theta)\left|
    0 \right\rangle")    ![\vartriangleright](img/file655.png "\vartriangleright")
    *This* *is done on the quantum computer!*
  prefs: []
  type: TYPE_NORMAL
- en: From the measurements of ![\left| {\psi(\theta)} \right\rangle](img/file1039.png
    "\left| {\psi(\theta)} \right\rangle") in different bases, estimate ![\left\langle
    {\psi(\theta)} \right|H_{1}\left| {\psi(\theta)} \right\rangle](img/file1040.png
    "\left\langle {\psi(\theta)} \right|H_{1}\left| {\psi(\theta)} \right\rangle")
  prefs: []
  type: TYPE_NORMAL
- en: Let’s remark on a couple of things about this pseudocode. Notice that we require
    that ![H_{1}](img/file544.png "H_{1}") be given as a linear combination of tensor
    products of Pauli matrices; this is so that we can use the techniques that we
    introduced in the previous section to estimate ![\left\langle \psi \right|H_{1}\left|
    \psi \right\rangle](img/file703.png "\left\langle \psi \right|H_{1}\left| \psi
    \right\rangle"). Of course, the more terms we have in the linear combination,
    the bigger the number of bases in which we may need to perform measurements. Nevertheless,
    in some cases, we may group several measurements together. For example, if we
    have terms such as ![I \otimes X \otimes I \otimes X](img/file1041.png "I \otimes
    X \otimes I \otimes X"), ![I \otimes I \otimes X \otimes X](img/file1042.png "I
    \otimes I \otimes X \otimes X"), and ![I \otimes I \otimes X \otimes X](img/file1042.png
    "I \otimes I \otimes X \otimes X"), we can use ![I \otimes H \otimes H \otimes
    H](img/file1043.png "I \otimes H \otimes H \otimes H") as our change of basis
    matrix (be careful! This ![H](img/file10.png "H") is the Hadamard matrix, not
    the Hamiltonian!) because it works for the three terms at the same time — keep
    in mind that any orthonormal basis is an eigenvector basis for ![I](img/file53.png
    "I"), not just ![\{\left| 0 \right\rangle,\left| 1 \right\rangle\}](img/file1044.png
    "\{\left| 0 \right\rangle,\left| 1 \right\rangle\}"). Obviously, another hyperparameter
    that will impact the execution time of VQE is the number of times that we measure
    ![\left| \psi \right\rangle](img/file43.png "\left| \psi \right\rangle") in each
    basis. The higher this number, the more precise the estimation, but also the higher
    the time needed to estimate ![\left\langle \psi \right|H_{1}\left| \psi \right\rangle](img/file703.png
    "\left\langle \psi \right|H_{1}\left| \psi \right\rangle").
  prefs: []
  type: TYPE_NORMAL
- en: Notice also that the pseudocode of Algorithm [7.1](#x1-123008r1) concludes by
    estimating ![\left\langle {\psi(\theta)} \right|H_{1}\left| {\psi(\theta)} \right\rangle](img/file1040.png
    "\left\langle {\psi(\theta)} \right|H_{1}\left| {\psi(\theta)} \right\rangle")
    for the last state ![\left| {\psi(\theta)} \right\rangle](img/file1039.png "\left|
    {\psi(\theta)} \right\rangle") found by the minimization algorithm. This is a
    quite common use case, for instance, if we want to determine the ground state
    energy for a particular system. However, you are not restricted to just that.
    At the end of the VQE execution, you also know the ![\theta_{0}](img/file1045.png
    "\theta_{0}") parameters that were used to build the ground state, and you could
    use them to reconstruct ![\left| {\psi(\theta_{0})} \right\rangle = V(\theta_{0})\left|
    0 \right\rangle](img/file1046.png "\left| {\psi(\theta_{0})} \right\rangle = V(\theta_{0})\left|
    0 \right\rangle"). This state could then be used for other purposes, such as being
    sent into another quantum algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, in the next subsection, we are going to explore one of such uses:
    the computation of additional **eigenstates** (another name for our old friends,
    the eigenvectors) of Hamiltonians. You should be excited!'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Getting excited with VQE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have just explained, VQE is used to search for a ground state of a given
    Hamiltonian ![H](img/file10.png "H"). However, with a small modification, we can
    also use it to find **excited** **states**: eigenstates with higher energies.
    Let’s explain how to achieve this.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you have been given a Hamiltonian ![H](img/file10.png "H") and
    you have used VQE to find a ground state ![\left| \psi_{0} \right\rangle = V(\theta_{0})\left|
    0 \right\rangle](img/file1047.png "\left| \psi_{0} \right\rangle = V(\theta_{0})\left|
    0 \right\rangle") with energy ![\lambda_{0}](img/file964.png "\lambda_{0}"). Then,
    we may consider the modified Hamiltonian
  prefs: []
  type: TYPE_NORMAL
- en: '![H^{\prime} = H + C\left| \psi_{0} \right\rangle\left\langle \psi_{0} \right|,](img/file1048.png
    "H^{\prime} = H + C\left| \psi_{0} \right\rangle\left\langle \psi_{0} \right|,")'
  prefs: []
  type: TYPE_IMG
- en: where ![C](img/file234.png "C") is a positive real number.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to detailing why ![H^{\prime}](img/file1049.png "H^{\prime}")
    can enable us to find excited states, let’s explain what the term ![\left| \psi_{0}
    \right\rangle\left\langle \psi_{0} \right|](img/file1050.png "\left| \psi_{0}
    \right\rangle\left\langle \psi_{0} \right|") in that expression means. First of
    all, notice that this term represents a square matrix: it is the product of a
    column vector (![\left| \psi_{0} \right\rangle](img/file631.png "\left| \psi_{0}
    \right\rangle")) and a row vector (![\left\langle \psi_{0} \right|](img/file1051.png
    "\left\langle \psi_{0} \right|")) of the same length. Moreover, it is a Hermitian
    matrix, because'
  prefs: []
  type: TYPE_NORMAL
- en: '![\left( {\left| \psi_{0} \right\rangle\left\langle \psi_{0} \right|} \right)^{\dagger}
    = \left\langle \psi_{0} \right|^{\dagger}\left| \psi_{0} \right\rangle^{\dagger}
    = \left| \psi_{0} \right\rangle\left\langle \psi_{0} \right|.](img/file1052.png
    "\left( {\left| \psi_{0} \right\rangle\left\langle \psi_{0} \right|} \right)^{\dagger}
    = \left\langle \psi_{0} \right|^{\dagger}\left| \psi_{0} \right\rangle^{\dagger}
    = \left| \psi_{0} \right\rangle\left\langle \psi_{0} \right|.")'
  prefs: []
  type: TYPE_IMG
- en: Then, ![H^{\prime}](img/file1049.png "H^{\prime}") is the sum of two Hermitian
    matrices and is, therefore, also Hermitian. And what is its expectation value?
    If we have a generic quantum state ![\left| \psi \right\rangle](img/file43.png
    "\left| \psi \right\rangle"), then
  prefs: []
  type: TYPE_NORMAL
- en: '![\left\langle \psi \right|H^{\prime}\left| \psi \right\rangle = \left\langle
    \psi \right|H\left| \psi \right\rangle + C\left\langle \psi \middle| \psi_{0}
    \right\rangle\left\langle \psi_{0} \middle| \psi \right\rangle = \left\langle
    \psi \right|H\left| \psi \right\rangle + C\left| \left\langle \psi_{0} \middle|
    \psi \right\rangle \right|^{2}.](img/file1053.png "\left\langle \psi \right|H^{\prime}\left|
    \psi \right\rangle = \left\langle \psi \right|H\left| \psi \right\rangle + C\left\langle
    \psi \middle| \psi_{0} \right\rangle\left\langle \psi_{0} \middle| \psi \right\rangle
    = \left\langle \psi \right|H\left| \psi \right\rangle + C\left| \left\langle \psi_{0}
    \middle| \psi \right\rangle \right|^{2}.")'
  prefs: []
  type: TYPE_IMG
- en: That is, the expectation value of ![H^{\prime}](img/file1049.png "H^{\prime}")
    in a state ![\left| \psi \right\rangle](img/file43.png "\left| \psi \right\rangle")
    is the expectation value of ![H](img/file10.png "H") plus a non-negative value
    that quantifies the overlap of ![\left| \psi \right\rangle](img/file43.png "\left|
    \psi \right\rangle") and ![\left| \psi_{0} \right\rangle](img/file631.png "\left|
    \psi_{0} \right\rangle"). Hence we have two extreme cases for ![C\left| \left\langle
    \psi_{0} \middle| \psi \right\rangle \right|^{2}](img/file1054.png "C\left| \left\langle
    \psi_{0} \middle| \psi \right\rangle \right|^{2}"). If ![\left| \psi \right\rangle
    = \left| \psi_{0} \right\rangle](img/file1055.png "\left| \psi \right\rangle =
    \left| \psi_{0} \right\rangle"), this term will be ![C](img/file234.png "C").
    If ![\left| \psi \right\rangle](img/file43.png "\left| \psi \right\rangle") and
    ![\left| \psi_{0} \right\rangle](img/file631.png "\left| \psi_{0} \right\rangle")
    are orthogonal, the term will be ![0](img/file12.png "0").
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if we make ![C](img/file234.png "C") big enough, ![\left| \psi_{0} \right\rangle](img/file631.png
    "\left| \psi_{0} \right\rangle") will no longer be a ground state of ![H^{\prime}](img/file1049.png
    "H^{\prime}"). Let’s prove this is in a more formal way. To this end, let ![\lambda_{0}
    \leq \lambda_{1} \leq \ldots \leq \lambda_{n}](img/file1056.png "\lambda_{0} \leq
    \lambda_{1} \leq \ldots \leq \lambda_{n}") be the eigenvalues of ![H](img/file10.png
    "H") associated to each eigenvector in an orthonormal eigenvector basis ![\{\left|
    \lambda_{j} \right\rangle\}](img/file1057.png "\{\left| \lambda_{j} \right\rangle\}")
    (since different eigenvectors may have the same eigenvalue, some of the energies
    may be repeated). As ![\left| \psi_{0} \right\rangle](img/file631.png "\left|
    \psi_{0} \right\rangle") is, by hypothesis, a ground state, we shall assume that
    ![\left| \lambda_{0} \right\rangle = \left| \psi_{0} \right\rangle](img/file1058.png
    "\left| \lambda_{0} \right\rangle = \left| \psi_{0} \right\rangle"). The states
    ![\{\left| \lambda_{j} \right\rangle\}](img/file1057.png "\{\left| \lambda_{j}
    \right\rangle\}") are also eigenvectors of ![H^{\prime}](img/file1049.png "H^{\prime}")
    because, on the one hand, if ![j \neq 0](img/file1059.png "j \neq 0"),
  prefs: []
  type: TYPE_NORMAL
- en: '![H^{\prime}\left| \lambda_{j} \right\rangle = H\left| \lambda_{j} \right\rangle
    + C\left| \psi_{0} \right\rangle\left\langle \psi_{0} \middle| \lambda_{j} \right\rangle
    = H\left| \lambda_{j} \right\rangle + C\left| \lambda_{0} \right\rangle\left\langle
    \lambda_{0} \middle| \lambda_{j} \right\rangle = H\left| \lambda_{j} \right\rangle
    = \lambda_{j}\left| \lambda_{j} \right\rangle,](img/file1060.png "H^{\prime}\left|
    \lambda_{j} \right\rangle = H\left| \lambda_{j} \right\rangle + C\left| \psi_{0}
    \right\rangle\left\langle \psi_{0} \middle| \lambda_{j} \right\rangle = H\left|
    \lambda_{j} \right\rangle + C\left| \lambda_{0} \right\rangle\left\langle \lambda_{0}
    \middle| \lambda_{j} \right\rangle = H\left| \lambda_{j} \right\rangle = \lambda_{j}\left|
    \lambda_{j} \right\rangle,")'
  prefs: []
  type: TYPE_IMG
- en: since ![\left| \lambda_{0} \right\rangle](img/file1061.png "\left| \lambda_{0}
    \right\rangle") and ![\left| \lambda_{j} \right\rangle](img/file978.png "\left|
    \lambda_{j} \right\rangle") are orthogonal. On the other hand,
  prefs: []
  type: TYPE_NORMAL
- en: '![H^{\prime}\left| \lambda_{0} \right\rangle = H\left| \lambda_{0} \right\rangle
    + C\left| \psi_{0} \right\rangle\left\langle \psi_{0} \middle| \lambda_{0} \right\rangle
    = H\left| \lambda_{0} \right\rangle + C\left| \lambda_{0} \right\rangle\left\langle
    \lambda_{0} \middle| \lambda_{0} \right\rangle = H\left| \lambda_{0} \right\rangle
    + C\left| \lambda_{0} \right\rangle = (\lambda_{0} + C)\left| \lambda_{0} \right\rangle.](img/file1062.png
    "H^{\prime}\left| \lambda_{0} \right\rangle = H\left| \lambda_{0} \right\rangle
    + C\left| \psi_{0} \right\rangle\left\langle \psi_{0} \middle| \lambda_{0} \right\rangle
    = H\left| \lambda_{0} \right\rangle + C\left| \lambda_{0} \right\rangle\left\langle
    \lambda_{0} \middle| \lambda_{0} \right\rangle = H\left| \lambda_{0} \right\rangle
    + C\left| \lambda_{0} \right\rangle = (\lambda_{0} + C)\left| \lambda_{0} \right\rangle.")'
  prefs: []
  type: TYPE_IMG
- en: Thus, it follows that ![\left\langle \lambda_{j} \right|H^{\prime}\left| \lambda_{j}
    \right\rangle = \lambda_{j}](img/file1063.png "\left\langle \lambda_{j} \right|H^{\prime}\left|
    \lambda_{j} \right\rangle = \lambda_{j}") when ![j \neq 0](img/file1059.png "j
    \neq 0") and that ![\left\langle \lambda_{0} \right|H^{\prime}\left| \lambda_{0}
    \right\rangle = C + \lambda_{0}](img/file1064.png "\left\langle \lambda_{0} \right|H^{\prime}\left|
    \lambda_{0} \right\rangle = C + \lambda_{0}"). Hence, if ![C > \lambda_{1} - \lambda_{0}](img/file1065.png
    "C > \lambda_{1} - \lambda_{0}"), then ![\left| \psi_{0} \right\rangle = \left|
    \lambda_{0} \right\rangle](img/file1066.png "\left| \psi_{0} \right\rangle = \left|
    \lambda_{0} \right\rangle") will no longer be a ground state of ![H^{\prime}](img/file1049.png
    "H^{\prime}"), because the energy of ![\left| \lambda_{1} \right\rangle](img/file1067.png
    "\left| \lambda_{1} \right\rangle") will be lower than that of ![\left| \psi_{0}
    \right\rangle](img/file631.png "\left| \psi_{0} \right\rangle"). Thanks to the
    variational principle, we know that the minimum energy is attained at an eigenvector
    of ![H^{\prime}](img/file1049.png "H^{\prime}"), so ![\left| \lambda_{1} \right\rangle](img/file1067.png
    "\left| \lambda_{1} \right\rangle") must be a ground state of ![H^{\prime}](img/file1049.png
    "H^{\prime}").
  prefs: []
  type: TYPE_NORMAL
- en: We can then use VQE to search for a ground state of ![H^{\prime}](img/file1049.png
    "H^{\prime}") and obtain the state ![\left| \lambda_{1} \right\rangle](img/file1067.png
    "\left| \lambda_{1} \right\rangle"), as we intended.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: Notice that it could be the case that ![\lambda_{1} = \lambda_{0}](img/file1068.png
    "\lambda_{1} = \lambda_{0}"). In that situation, ![\left| \lambda_{1} \right\rangle](img/file1067.png
    "\left| \lambda_{1} \right\rangle") would be another ground state of ![H](img/file10.png
    "H"). Otherwise, it will be the first excited state of ![H](img/file10.png "H").
  prefs: []
  type: TYPE_NORMAL
- en: You may have also noticed that, even if the ground state is unique, the first
    excited eigenstate may not be so. This happens if and only if ![\left| \lambda_{2}
    \right\rangle](img/file1069.png "\left| \lambda_{2} \right\rangle") (and possibly
    other states in the basis) has the same energy as ![\left| \lambda_{1} \right\rangle](img/file1067.png
    "\left| \lambda_{1} \right\rangle"), (that is, ![\lambda_{2} = \lambda_{1}](img/file1070.png
    "\lambda_{2} = \lambda_{1}")). In that case, any normalized linear combination
    of those eigenvectors will be a ground state of ![H^{\prime}](img/file1049.png
    "H^{\prime}"). Any of them will serve our purposes equally well.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, once you obtain ![\left| \lambda_{1} \right\rangle](img/file1067.png
    "\left| \lambda_{1} \right\rangle"), you can consider ![H^{''} = H^{\prime} +
    C^{\prime}\left| \lambda_{1} \right\rangle\left\langle \lambda_{1} \right|](img/file1071.png
    "H^{''} = H^{\prime} + C^{\prime}\left| \lambda_{1} \right\rangle\left\langle
    \lambda_{1} \right|") and use VQE to search for ![\left| \lambda_{2} \right\rangle](img/file1069.png
    "\left| \lambda_{2} \right\rangle"), and so on and so forth. Keep in mind that,
    in this process, we would have to pick the constants ![C,C^{\prime},\ldots](img/file1072.png
    "C,C^{\prime},\ldots") properly — just to make sure that none of the eigenstates
    that we already know becomes a ground state again!
  prefs: []
  type: TYPE_NORMAL
- en: With this, our problem of finding eigenvectors of increasing energy is solved.
    Or is it?
  prefs: []
  type: TYPE_NORMAL
- en: There is just one little implementation detail that might be bothering you.
    In the previous section, we discussed how to estimate the expectation value of
    a Hamiltonian under the assumption that it was given as a sum of tensor products
    of Pauli matrices. However, the ![\left| \psi_{0} \right\rangle\left\langle \psi_{0}
    \right|](img/file1050.png "\left| \psi_{0} \right\rangle\left\langle \psi_{0}
    \right|") term is not of that form. In fact, we know ![\left| \psi_{0} \right\rangle](img/file631.png
    "\left| \psi_{0} \right\rangle") only as the result of applying VQE, so it is
    very likely that we will not know ![\left| \psi_{0} \right\rangle](img/file631.png
    "\left| \psi_{0} \right\rangle") explicitly; instead, we will have nothing more
    than some parameters ![\theta_{0}](img/file1045.png "\theta_{0}") such that ![V(\theta_{0})\left|
    0 \right\rangle = \left| \psi_{0} \right\rangle](img/file1073.png "V(\theta_{0})\left|
    0 \right\rangle = \left| \psi_{0} \right\rangle"). This, nonetheless, is enough
    to compute the expectation values that we need.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s step back a little bit and take a look at what we need to compute. At
    a given moment in the application of VQE, we have some parameters ![\theta](img/file89.png
    "\theta") and we want to estimate the expectation value of ![\left| \psi_{0} \right\rangle\left\langle
    \psi_{0} \right|](img/file1050.png "\left| \psi_{0} \right\rangle\left\langle
    \psi_{0} \right|") with respect to ![\left| {\psi(\theta)} \right\rangle = V(\theta)\left|
    0 \right\rangle](img/file1038.png "\left| {\psi(\theta)} \right\rangle = V(\theta)\left|
    0 \right\rangle"). This quantity is
  prefs: []
  type: TYPE_NORMAL
- en: '![\left\langle \psi(\theta) \middle| \psi_{0} \right\rangle\left\langle \psi_{0}
    \middle| \psi(\theta) \right\rangle = \left| \left\langle \psi_{0} \middle| \psi(\theta)
    \right\rangle \right|^{2} = \left| {\left\langle 0 \right|V{(\theta_{0})}^{\dagger}V(\theta)\left|
    0 \right\rangle} \right|^{2}.](img/file1074.png "\left\langle \psi(\theta) \middle|
    \psi_{0} \right\rangle\left\langle \psi_{0} \middle| \psi(\theta) \right\rangle
    = \left| \left\langle \psi_{0} \middle| \psi(\theta) \right\rangle \right|^{2}
    = \left| {\left\langle 0 \right|V{(\theta_{0})}^{\dagger}V(\theta)\left| 0 \right\rangle}
    \right|^{2}.")'
  prefs: []
  type: TYPE_IMG
- en: But this is just the probability of obtaining ![\left| 0 \right\rangle](img/file6.png
    "\left| 0 \right\rangle") as the outcome of measuring ![V{(\theta_{0})}^{\dagger}V(\theta)\left|
    0 \right\rangle](img/file1075.png "V{(\theta_{0})}^{\dagger}V(\theta)\left| 0
    \right\rangle") in the computational basis! This is something that we can easily
    estimate because we can prepare ![V{(\theta_{0})}^{\dagger}V(\theta)\left| 0 \right\rangle](img/file1075.png
    "V{(\theta_{0})}^{\dagger}V(\theta)\left| 0 \right\rangle") by first applying
    our ansatz ![V](img/file379.png "V"), using ![\theta](img/file89.png "\theta")
    as the parameters, to ![\left| 0 \right\rangle](img/file6.png "\left| 0 \right\rangle"),
    and then applying the inverse of our ansatz, with parameters ![\theta_{0}](img/file1045.png
    "\theta_{0}"), to the resulting state. We will repeat this process several times,
    always measuring the resulting state ![V{(\theta_{0})}^{\dagger}V(\theta)\left|
    0 \right\rangle](img/file1075.png "V{(\theta_{0})}^{\dagger}V(\theta)\left| 0
    \right\rangle") in the computational basis and computing the relative frequency
    of the outcome ![\left| 0 \right\rangle](img/file6.png "\left| 0 \right\rangle").
    This is illustrated in *Figure* * [*7.1*](#Figure7.1).*
  prefs: []
  type: TYPE_NORMAL
- en: '*![Figure 7.1: Circuit to compute \left| {\left\langle 0 \right|V{(\theta_{0})}^{\dagger}V(\theta)\left|
    0 \right\rangle} \right|^{2}.](img/file1077.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 7.1**: Circuit to compute ![\left| {\left\langle 0 \right|V{(\theta_{0})}^{\dagger}V(\theta)\left|
    0 \right\rangle} \right|^{2}](img/file1076.png "\left| {\left\langle 0 \right|V{(\theta_{0})}^{\dagger}V(\theta)\left|
    0 \right\rangle} \right|^{2}").'
  prefs: []
  type: TYPE_NORMAL
- en: The only thing that may, at first, seem difficult with this method is to obtain
    the circuit for ![V{(\theta_{0})}^{\dagger}](img/file1078.png "V{(\theta_{0})}^{\dagger}").
    However, this is fairly easy. You just need to remember that every unitary gate
    is reversible. Thus, you can take the circuit for ![V(\theta)](img/file1034.png
    "V(\theta)") and read the gates from right to left, reversing each one of them.
    As a simple example, if ![\theta_{0} = (a,b)](img/file1079.png "\theta_{0} = (a,b)")
    and ![V(\theta_{0}) = XR_{Z}(a)R_{X}(b)S](img/file1080.png "V(\theta_{0}) = XR_{Z}(a)R_{X}(b)S"),
    then ![V{(\theta_{0})}^{\dagger} = S^{\dagger}R_{X}( - b)R_{Z}( - a)X](img/file1081.png
    "V{(\theta_{0})}^{\dagger} = S^{\dagger}R_{X}( - b)R_{Z}( - a)X").
  prefs: []
  type: TYPE_NORMAL
- en: Do not forget about this technique for estimating ![\left| {\left\langle 0 \right|V{(\theta_{0})}^{\dagger}V(\theta)\left|
    0 \right\rangle} \right|^{2}](img/file1076.png "\left| {\left\langle 0 \right|V{(\theta_{0})}^{\dagger}V(\theta)\left|
    0 \right\rangle} \right|^{2}") because we will be using it again in *Chapter*
    * [*9*](ch018.xhtml#x1-1600009), *Quantum Support Vector* *Machines*, in a completely
    different context.*
  prefs: []
  type: TYPE_NORMAL
- en: '*This concludes our theoretical study of VQE. In the next section, we will
    learn how to use this algorithm with Qiskit.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Using VQE with Qiskit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will show how we can use Qiskit to run VQE on both simulators
    and actual quantum hardware. To do that, we will use a problem taken from quantum
    chemistry: determining the energy of the ![H_{2}](img/file1082.png "H_{2}") or
    dihydrogen molecule. Our first subsection is devoted to defining this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1 Defining a molecular problem in Qiskit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To illustrate how we can use VQE with Qiskit, we will consider a simple quantum
    chemistry problem. We will imagine that we have two atoms of hydrogen forming
    an ![H_{2}](img/file1082.png "H_{2}") molecule and that we want to compute its
    ground state and its energy. For that, we need to obtain the Hamiltonian of the
    system, which is a little bit different from the kind of Hamiltonian that we are
    used to. The Hamiltonians that we have considered so far are called **qubit**
    **Hamiltonians**, while the one that we need to describe the energy of the ![H_{2}](img/file1082.png
    "H_{2}") molecule is called a **fermionic Hamiltonian** — the name comes from
    the fact that it involves fermions, that is, particles such as electrons, protons,
    and neutrons.
  prefs: []
  type: TYPE_NORMAL
- en: We do not need to go into all the details of the computation of this type of
    Hamiltonian (if you are curious, you can refer to *Chapter 4* in the book by Sharkey
    and Chancé [[86](ch030.xhtml#Xsharkey2022quantum)]), because all the necessary
    methods are provided in the Qiskit Nature package. What is more, no quantum computer
    is involved in the process; it is all computed and estimated classically.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain the fermionic Hamiltonian for the dihydrogen molecule with Qiskit,
    we need to install the Qiskit Nature package as well as the pyscf library, which
    is used for the computational chemistry calculations (please, refer to *Appendix*
    *[*D*](ch027.xhtml#x1-240000D), *Installing the Tools*, for the installation procedure
    and note that we will be using version 0.4.5 of the package).*
  prefs: []
  type: TYPE_NORMAL
- en: '*Then, we can execute the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are defining a molecule consisting of two hydrogen atoms located at
    coordinates ![(0,0, - 0.37)](img/file1083.png "(0,0, - 0.37)") and ![(0,0,0.37)](img/file1084.png
    "(0,0,0.37)") (measured in angstroms), which is close to an equilibrium state
    for this molecule. We are using the default parameters, such as, for instance,
    establishing that the molecule is not charged. Then, we define an electronic structure
    problem; that is, we ask the pyscf library, through the Qiskit interface, to compute
    the fermionic Hamiltonian that takes into account the different possible configurations
    for the electrons of the two hydrogen atoms. This is done with something called
    **second** **quantization** (hence the name `second_q_ops` for the method that
    we use).
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run this piece of code, we obtain the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This is a truncated view of the fermionic Hamiltonian, involving something called
    **creation** and **annihilation** operators that describe how electrons move from
    one orbital to another (more details can be found in *Chapter 4* of the book by
    Sharkey and Chancé [[86](ch030.xhtml#Xsharkey2022quantum)]).
  prefs: []
  type: TYPE_NORMAL
- en: 'That is all very nice, but we can’t yet use it on our shiny quantum computers.
    For that, we need to transform the fermionic Hamiltonian into a qubit Hamiltonian,
    involving Pauli gates. There are several ways to do this. One of the most popular
    ones is the **Jordan-Wigner** transformation (again, see the book by Sharkey and
    Chancé [[86](ch030.xhtml#Xsharkey2022quantum)] for a thorough explanation), that
    we can use in Qiskit with the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon running this code, we will obtain the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now we are back on known territory once again! This is, indeed, one of the Hamiltonians
    that we have come to know and love. In fact, this is a Hamiltonian on ![4](img/file143.png
    "4") qubits, involving tensor products of ![I](img/file53.png "I"), ![X](img/file9.png
    "X"), ![Y](img/file11.png "Y") and ![Z](img/file8.png "Z") gates, as the ones
    appearing in terms such as `0.17141282644776915` `*` `IIIZ` or `0.04530261550379923`
    `*` `XXYY`.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is more important to us: this is the kind of Hamiltonian to which we can
    apply the VQE algorithm in order to obtain its ground state. And, without further
    ado, that is exactly what we will be doing in the next subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2 Using VQE with Hamiltonians
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a qubit Hamiltonian describing our electronic problem, let’s
    see how we can use VQE with Qiskit to find its ground state. Remember that, to
    use VQE, we first need to choose an ansatz. To start with, we will use something
    simple. We will select one of the variational forms provided by Qiskit: the `EfficientSU2`
    form. We can define it and draw its circuit for ![4](img/file143.png "4") qubits
    with the following instructions (remember that you need to install the pylatexenc
    library to draw with the `"``mpl``"` option; please, refer to *Appendix* * [*D*](ch027.xhtml#x1-240000D),
    *Installing the Tools*):*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE4]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have specified that we are using the variational form on ![4](img/file143.png
    "4") qubits, that we only use one repetition (that is, a single layer of CNOT
    gates) and that the entanglement that we want to use is linear: this means that
    each qubit is entangled with a CNOT gate to the following one. After running this
    piece of code, we will obtain the image depicted in *Figure* * [*7.2*](#Figure7.2).
    As you can see, we are using ![R_{Y}](img/file119.png "R_{Y}") and ![R_{Z}](img/file120.png
    "R_{Z}") gates, together with entangling gates (CNOT gates, in this case). In
    total, we have 16 different tunable parameters, represented by ![\theta\lbrack
    0\rbrack,\ldots,\theta\lbrack 15\rbrack](img/file1085.png "\theta\lbrack 0\rbrack,\ldots,\theta\lbrack
    15\rbrack") in the figure. We will discuss more variational forms, similar to
    this one, in *Chapters* * [*9*](ch018.xhtml#x1-1600009) and * [*10*](ch019.xhtml#x1-18100010).
    For now, it is enough to notice that this is a circuit that we can easily implement
    in current quantum hardware (because it only involves simple one and two-qubit
    gates), but that allows us to create somewhat complicated quantum states, with
    entanglement among all the qubits.***'
  prefs: []
  type: TYPE_NORMAL
- en: '**![Figure 7.2: The EfficientSU2 variational form on 4 qubits.](img/file1086.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 7.2**: The EfficientSU2 variational form on 4 qubits.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have selected our ansatz, we can define a VQE instance. In order to
    do that, we can use the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: After the necessary imports, we set a seed for reproducibility. Then, we selected
    COBYLA as our classical optimizer; that is, the algorithm in charge of varying
    the parameters in order to find those that achieve the minimum energy. We also
    set some random initial values for our parameters and we declared a `QuantumInstance`
    that encapsulates a state vector simulator. Finally, we declared our VQE instance
    with the ansatz, optimizer, initial values, and quantum instance options.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the VQE is now very easy. We only need to execute the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few seconds, we obtain the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '@empty'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This may seem like a lot of information but, in fact, some data is presented
    several times in different ways and, all in all, the format is quite similar to
    what we are used to from our experience using QAOA in Qiskit back in *Chapter*
    *[*5*](ch013.xhtml#x1-940005), *QAOA: Quantum Approximate Optimization Algorithm*.
    As you can see, we have obtained the optimal values for the circuit parameters,
    the state that is generated with those parameters (the `’``eigenstate``’` field)
    and what we were looking for: the energy of that state, which happens to be about
    ![- 1.8524](img/file1087.png "- 1.8524") hartrees (the unit of energy commonly
    used in molecular orbital calculations). This means that…we have solved our problem!
    Or have we? How can we be sure that the value that we have obtained is correct?*'
  prefs: []
  type: TYPE_NORMAL
- en: '*In this case, the Hamiltonian that we are using is quite small (only ![4](img/file143.png
    "4") qubits), so we can check our result using a classical solver that finds the
    exact ground state. We will use `NumPyMinimumEigensolver`, just as we did with
    the combinatorial optimization problems that we considered back in *Chapter* [*5*](ch013.xhtml#x1-940005),
    *QAOA: Quantum Approximate Optimization Algorithm*. For that, we can run the following
    piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of these instructions is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is certainly more concise than the VQE output, but the final energy is
    almost equal to the one we had obtained previously. Now we can really say it:
    we did it! We have successfully solved a molecular problem with VQE!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, we can use VQE with any type of Hamiltonian, not just with the ones
    that come from quantum chemistry problems. We can even use it with Hamiltonians
    for combinatorial optimization problems, as we did back in *Chapter* *[*5*](ch013.xhtml#x1-940005),
    *QAOA: Quantum Approximate Optimization Algorithm*. With what we already know,
    this is easy…so easy that we entrust it to you as an exercise.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Exercise 7.7'
  prefs: []
  type: TYPE_NORMAL
- en: Use Qiskit’s VQE implementation to solve the Max-Cut problem for a graph of
    ![5](img/file296.png "5") vertices in which the connections are ![(0,1),(1,2),(2,3),(3,4)](img/file1088.png
    "(0,1),(1,2),(2,3),(3,4)") and ![(4,0)](img/file1089.png "(4,0)").
  prefs: []
  type: TYPE_NORMAL
- en: Once we know how to find the ground state of a Hamiltonian with VQE, why not
    be a little more ambitious? In the next subsection, we will also be looking for
    excited states!
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.3 Finding excited states with Qiskit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Back in *Section* *[*7.2.1*](#x1-1240007.2.1), we learned how to use VQE iteratively
    to find not only the ground state of a Hamiltonian, but also states of higher
    energy that we call excited states. The algorithm that we studied is sometimes
    called **Variational** **Quantum Deflation** (this was the name used by Higgot,
    Wang, and Brierley when they introduced it [[53](ch030.xhtml#Xhiggott2019variational)])
    or **VQD**, and it is implemented by Qiskit in the `VQD` class.*
  prefs: []
  type: TYPE_NORMAL
- en: '*Using VQD in Qiskit is almost the same as using VQE. The only difference is
    that we need to specify how many eigenstates we want to obtain (of course, if
    we only request ![1](img/file13.png "1") this will be *exactly* like applying
    VQE). For instance, if we want to obtain two eigenstates (the ground state and
    the first excited state) in our molecular problem, we can use the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `k` parameter is the one that we use to specify the number of eigenstates.
    Upon running these instructions, we obtain the following output (we have omitted
    part of it for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '@empty'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this output is structured like that of the VQE execution. However,
    in this case, we get two entries in each field, one for each of the eigenstates
    that we requested.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned how to use both VQE and VQD with Hamiltonians that may
    have come from any source. However, the use case of finding ground states of molecular
    Hamiltonians is so important that Qiskit provides special methods for dealing
    with them in particular. We will learn how in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.4 Using VQE with molecular problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to using VQE to find ground states of any given Hamiltonian, we
    can use it directly with molecular problems that we define with the help of the
    Qiskit Nature utilities. For instance, we can use a VQE instance to solve the
    electronic problem that we defined in the previous subsection. To do that, we
    only need to run the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we have defined a `GroundStateEigensolver` object that we then
    use to solve our problem. This object, in turn, uses two objects that we had defined
    previously — `qconverter`, which is used to transform the fermionic Hamiltonian
    into a qubit Hamiltonian, and the instance of VQE that we used two subsections
    ago. When we run these instructions, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The information that we get in this case is at a higher level of abstraction
    than the one we obtained before. For instance, we get data about the number of
    particles, dipole moments, and so on (don’t worry if you do not understand these
    concepts; they are meant to make sense to chemists and physicists that work with
    this kind of problem). However, the numerical result of the electronic ground
    state energy is the same that we obtained with our previous application of VQE.
    The difference is that now, we are providing the solver not only with the Hamiltonian,
    but with the whole problem, and it can use that information to reconstruct the
    meaning of the calculations in physical terms. For instance, we now get some bonus
    information such as the **total ground state** **energy**, which is the sum of
    the energy due to the electronic structure (the one that we had computed previously)
    and the energy due to nuclear repulsion.
  prefs: []
  type: TYPE_NORMAL
- en: This type of output is much more legible. That’s why we will use this solver
    for the rest of this section.
  prefs: []
  type: TYPE_NORMAL
- en: As an additional example of how to use VQE to solve molecular problems, we are
    now going to consider a different, more elaborate ansatz. Earlier in this chapter,
    we mentioned how, when selecting the variational form and initial state to be
    used with VQE, it could prove useful to take into account information from the
    problem domain. This is the case of the **Unitary Coupled-Cluster** **Singles
    and Doubles** or **UCCSD** ansatz, which is widely used for molecular computations
    (see the survey by McArdle et al. [[66](ch030.xhtml#Xmcardle2020quantum)] for
    more details).
  prefs: []
  type: TYPE_NORMAL
- en: 'In Qiskit, we can use the UCSSD ansatz with the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `VQEUCCFactory` class creates a whole VQE instance, with the UCSSD ansatz
    as the default variational form. Here, we are using the `quantum_instance` object
    that we had defined previously. We can visualize the circuit for the ansatz created
    by `VQEUCCFactory` with the following instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we are calling the `get_solver` method, to which we pass the `problem`
    object defined previously to provide the information about the Hamiltonian involved
    in the computation. Then, we access the ansatz circuit through the `ansatz` attribute
    and we proceed to draw it. Upon running this instruction, we obtain the circuit
    depicted in *Figure* *[*7.3*](#Figure7.3). As you can see, the ansatz involves
    exponential functions of tensor products of Pauli matrices. There are also two
    ![X](img/file9.png "X") gates at the beginning of the circuit that set the initial
    state to which the variational form is later applied. In this case, the state
    is called the **Hartree-Fock** state, again a widely used option when solving
    molecular problems with quantum computers — and the default value with `VQEUCCFactory`.*
  prefs: []
  type: TYPE_NORMAL
- en: '*![Figure 7.3: UCCSD ansatz for our problem](img/file1090.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 7.3**: UCCSD ansatz for our problem'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can easily use VQE to solve our problem with the selected ansatz by
    running the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This result is very similar to the one that we obtained with the `EfficientSU2`
    ansatz.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.8
  prefs: []
  type: TYPE_NORMAL
- en: Write code to use VQE with the UCCSD ansatz to compute the total ground state
    energy for two atoms of hydrogen that are at distances ranging from ![0.2](img/file1091.png
    "0.2") to ![2.0](img/file1092.png "2.0") angstroms, in steps of ![0.01](img/file1093.png
    "0.01") angstroms. Plot the energy against the distance. This kind of plot is
    sometimes known as the **dissociation profile** of the molecule. *Hint:* when
    running VQE on a molecular problem, you can access the total ground state energy
    through the `total_energies` attribute of the result object.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to use VQE in different ways with simulators, we could
    try to run the algorithm on actual quantum computers. Nevertheless, before doing
    that, we will learn how to incorporate noise to our quantum simulator.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.5 Simulations with noise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Going from a perfect, classical simulation of an algorithm to an execution on
    an actual quantum device can, sometimes, be too big a step. As we have mentioned
    in many occasions, current quantum computers suffer from the effect of different
    types of noise, including readout errors, imperfections in gate implementation,
    and decoherence, the loss of quantum properties of our states if the circuits
    are too deep.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, it is usually a good idea to perform a simulation of our algorithm
    under the effects of noise before going to the actual quantum device. In this
    way, we can study the performance of our algorithms in a controlled environment,
    and calibrate and adjust some of their parameters before running them on a quantum
    computer. For instance, if we observe that the results differ much from ideal
    simulation, we could decide to reduce the depth of our circuits by using a simpler
    ansatz.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways of conducting noisy simulations with Qiskit. Here, we
    will show how to use one that is both easy and very useful. We will create a simulator
    that mimics the behaviour of a real device, including the noise it is affected
    by. We can do this with the help of the `AerSimulator` class in the following
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we need to load an IBM account in order to access the calibration
    of a real device (`ibmq_manila`, in our example). This calibration is updated
    periodically to stay real to the state of the quantum computer and includes, among
    other things, information about readout errors, gate errors, and coherence times.
    Of course, this data will change from time to time, but we have decided to include
    seeds for our `QuantumInstance` object to make the result reproducible given the
    same calibration data. Notice that we are now specifying the number of shots,
    because we are not using state vector simulation anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can run the VQE algorithm exactly as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'When we ran this code, we obtained the following output (your results may be
    different, depending on the device calibration):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Observe how the effect of noise has affected the performance of the algorithm
    and degraded it, giving a result for the total ground state energy that is not
    that close to the correct one.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: An alternative way of mimicking the behavior of real quantum computers is using
    objects of the `FakeProvider` class. The difference is that they use snapshots
    of past calibrations of the devices instead of the latest ones. You can find more
    details at [https://qiskit.org/documentation/apidoc/providers_fake_provider.html](https://qiskit.org/documentation/apidoc/providers_fake_provider.html).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you can create custom noise models that include the different
    noise types implemented in the `qiskit_aer``.` `noise` package. Check the documentation
    at [https://qiskit.org/documentation/apidoc/aer_noise.html](https://qiskit.org/documentation/apidoc/aer_noise.html)
    for further explanation.
  prefs: []
  type: TYPE_NORMAL
- en: A way to try of reducing the adverse effects of noise in our computations is
    using **readout error mitigation** methods. The idea behind the particular method
    that we are going to use is very simple. Imagine that we know that, when the state
    of our qubit is ![\left| 0 \right\rangle](img/file6.png "\left| 0 \right\rangle"),
    there is a certain percentage of times it in which we obtain the incorrect value
    ![1](img/file13.png "1") when we measure it. Then, we can take into account this
    information to correct the measurement results that we have obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Qiskit, using readout error mitigation is very easy. We only need to create
    our Quantum Instance object in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can run VQE as usual, using this new `QuantumInstance` variable. In
    our case, that led to the following result (again, yours will likely differ because
    of the device calibration):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, our current result — compared with our previous simulation with
    noise and no error mitigation — is now much closer to the real ground state energy
    (although you have surely noticed that there is still room for improvement).
  prefs: []
  type: TYPE_NORMAL
- en: In order to run this kind of error mitigation, we need to know the probability
    of measuring ![y](img/file270.png "y") when we actually have ![\left| x \right\rangle](img/file267.png
    "\left| x \right\rangle") for every pair of binary strings ![x](img/file269.png
    "x") and ![y](img/file270.png "y"). Of course, estimating these values is computationally
    very expensive, because the number of strings grows exponentially with the number
    of qubits. Alternatively, we could assume that the readout errors are local and
    estimate instead the probability of obtaining an incorrect result for each individual
    qubit only. In Qiskit, you choose to take this approach by replacing `CompleteMeasFitter`
    with `TensoredMeasFitter`. However, at the time of writing, not all backends support
    this possibility, so you’d better be careful if you decide to use it.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: There is much more to say about trying to mitigate the effects of noise in quantum
    computations. Unfortunately, studying error mitigation further would make this
    chapter much, much longer (and it is already fairly long!). Should you be interested
    in this topic, we can recommend that you check the paper by Bravyi et al. [[21](ch030.xhtml#Xbravyi2021mitigating)]
    to learn more about measurement error mitigation and the papers by Temme et al.
    [[94](ch030.xhtml#Xtemme2017error)] and by Endo et al. [[35](ch030.xhtml#Xendo2021hybrid)]
    to learn more about how to mitigate errors in general, including the ones causes
    by imperfect gate implementation. You may also want to take a look at Mitiq, a
    very easy-to-use software package for error mitigation that is compatible with
    Qiskit and other quantum computing libraries [[63](ch030.xhtml#Xlarose2022mitiq)].
  prefs: []
  type: TYPE_NORMAL
- en: The techniques that we have introduced to simulate noisy devices and to mitigate
    readout errors are not only applicable to the VQE algorithm. In fact, noisy simulation
    can be used when running any circuit, because we can just use a `backend` object
    created with the `AerSimulator``.``from_backend` function and a real quantum computer.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, readout error mitigation can be used with any algorithm that uses
    an object of the class `QuantumInstance` to run circuits. This includes QAOA and
    GAS, which we studied in *Chapters* *[*5*](ch013.xhtml#x1-940005) and *[*6*](ch014.xhtml#x1-1060006),
    respectively, as well as the QSVMs, the QNNs and the QGANs that we will study
    in *Chapters* *[*9*](ch018.xhtml#x1-1600009), *[*10*](ch019.xhtml#x1-18100010),
    *[*11*](ch020.xhtml#x1-19500011), and *[*12*](ch021.xhtml#x1-21200012).******
  prefs: []
  type: TYPE_NORMAL
- en: '**But the possibilities don’t end there. In fact, every `QuantumInstance` object
    provides an `execute` method that receives quantum circuits and executes them.
    So, you can create a `QuantumInstance` with a noisy backend and the `measurement_error_mitigation_cls`
    argument, and then invoke `execute` to obtain results with error mitigation.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.9
  prefs: []
  type: TYPE_NORMAL
- en: Create a noisy backend from a real quantum computer. Then, use it to run a simple
    two-qubit circuit consisting of a Hadamard gate on the first qubit, a CNOT gate
    with control on the first qubit and target in the second, and a final measurement
    of both qubits. Compare the results to those of ideal simulation. Then, create
    a `QuantumInstance` from your backend and using readout error mitigation. Run
    the circuit with it. Compare the results to what you obtained before.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 7.10
  prefs: []
  type: TYPE_NORMAL
- en: Run QAOA with a simple Hamiltonian on a noisy simulator with and without readout
    error mitigation. Compare the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know how to run simulations with noise, we are ready for the next
    big step: let’s run VQE on actual quantum devices.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.6 Running VQE on quantum computers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By now, you surely have guessed what we are going to say about running VQE on
    quantum devices. If you were thinking that we could just use a real backend when
    creating our `QuantumInstance` object, but that it would involve waiting multiple
    queues and that there must be a better way, you were completely spot on. In fact,
    we can use Runtime to send our VQE jobs to IBM’s quantum computers, waiting only
    in one execution queue. The way in which we can use VQE with Runtime is very similar
    to what we showed in *Section* *[*5.2.1*](ch013.xhtml#x1-1020005.2.1) for QAOA.
    We can use the `VQEClient` as follows:*
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE23]'
  prefs: []
  type: TYPE_NORMAL
- en: This is completely analogous to how we run VQE on local simulators, but now
    we are sending the task to the real quantum device called `ibmq_manila`. Notice
    that we have specified the number of shots and that we have opted to use the default
    optimizer since we haven’t provided a value for the optimizer argument. The default
    optimizer for this algorithm is SPSA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results that we obtained (after waiting some time in the queue) were the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can observe again the effect of noise in this execution. Of course, we can
    try to reduce it by setting `measurement_error_mitigation``=``True` and running
    the same code again. When we did that, we obtained the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: That is a little bit better, right?
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have covered everything we wanted to tell you about how to run
    VQE with Qiskit…or almost everything. In the next subsection, we will show you
    some new features that are being added to Qiskit and that can change the way in
    which algorithms such as VQE are used.
  prefs: []
  type: TYPE_NORMAL
- en: '7.3.7 The shape of things to come: the future of Qiskit'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quantum computing software libraries are in constant evolution and Qiskit is
    no exception to this rule. Although everything that we have studied in this section
    is the main way of running VQE with the latest version of Qiskit (which is 0.39.2
    at the time of writing this book), a new way of executing the algorithm is also
    being introduced and it will likely become the default one in the not-so-distant
    future.
  prefs: []
  type: TYPE_NORMAL
- en: This new way of doing things involves some modifications, the most important
    of which is replacing the use of `QuantumInstance` objects with `Estimator` variables.
    An `Estimator` is an object that is capable running a parametrized circuit to
    obtain a quantum state and then estimate (who would have guessed?) the expectation
    value of some observable on that state. Of course, this is exactly what we need
    in order to be able to run VQE, as you surely remember from *Section* *[*7.2*](#x1-1230007.2).*
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s see an example of how this would work. The following code is a possible
    way of running VQE to solve the same molecular problem that we have been considering
    throughout this section with the new implementations that are being introduced
    in Qiskit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we are importing `VQE` from `qiskit``.``algorithms``.``minimum_eigensolvers`,
    not from `qiskit``.``algorithms` as before. Also notice how the `Estimator` object
    has replaced the `QuantumInstance` one that we used to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running these instructions will give an output like the following one (shortened
    here for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '@empty'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This should sound familiar because it is the same kind of result that we obtained
    when using the current VQE implementation directly on a Hamiltonian.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, things are not going to change a lot in this new version. The
    main novelty is the use of `Estimator` objects. So, how do they work? Well, it
    depends. For instance, the one that we have imported from `qiskit``.``primitives`
    uses a state vector simulator to obtain a quantum state from a circuit. Then,
    it computes its expectation value by calling the `expectation_value` method, as
    we did back in *Section* *[*3.2.2*](ch011.xhtml#x1-660003.2.2). However, the `Estimator`
    class implemented in `qiskit_aer``.``primitives` uses the method that we explained
    in *Section* *[*7.1.2*](#x1-1220007.1.2) by appending additional gates to the
    parametrized circuit in order to perform measurements in different bases.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Unfortunately, at the time of writing this book, some of the features that
    we have covered in this section, such as noisy simulation and error mitigation,
    are still not completely supported by the new version of the algorithms. Moreover,
    some of the `Estimator` classes are not fully compatible with the new VQE implementation
    yet.'
  prefs: []
  type: TYPE_NORMAL
- en: However, Qiskit changes rapidly, so maybe, in the future, you can fully reproduce
    our code with `Estimator` objects instead of `QuantumInstance` ones by the time
    you will be reading these lines. Time will tell!
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The changes that we have described in this subsection are expected to also affect
    other algorithms implemented in Qiskit, such as VQD or QAOA. In the case of QAOA,
    instead of `Estimator` objects, you will need to use `Sampler` objects. As you
    can imagine, they will let you obtain samples from parametrized circuits, which
    can later be used by QAOA to estimate the value of the cost function.
  prefs: []
  type: TYPE_NORMAL
- en: And now, we promise, this is really all we wanted to tell you about running
    VQE with Qiskit. Our next stop is PennyLane.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Using VQE with PennyLane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will illustrate how to run VQE with PennyLane. The problem
    that we will work with will be, again, finding the ground state of the dihydrogen
    molecule. This is a task we are already familiar with and, moreover, this will
    allow us to compare our results with those that we obtained with Qiskit in the
    previous section. So, without further ado, let’s start by showing how to define
    the problem in PennyLane.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.1 Defining a molecular problem in PennyLane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As with Qiskit, PennyLane provides methods to work with quantum chemistry problems.
    To study the ![H_{2}](img/file1082.png "H_{2}") molecule, we can use the following
    instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You may be thinking that there is something fishy here. When we defined this
    same molecule in Qiskit, we used coordinates `[0.,` `0.,` `-0.37],[0.,` `0.,`
    `0.37]`, which seem different from the ones that we are using now. The explanation
    for this change is that, while Qiskit uses angstroms to measure atomic distances,
    PennyLane expects the values to be in atomic units. An angstrom is worth ![1.8897259886](img/file1094.png
    "1.8897259886") atomic units, hence the difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now obtain the qubit Hamiltonian that we need to use with VQE by running
    the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output that we obtain is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: If you compare this Hamiltonian to the one that we obtained with Qiskit for
    the same problem you will notice that they are very different. But don’t panic
    yet. While Qiskit gave us the Hamiltonian for the electronic structure of the
    molecule, PennyLane is accounting for the total energy, including nuclear repulsion.
    We will run the algorithm in a moment and, trust us, we will see how everything
    adds up.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.2 Implementing and running VQE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before using VQE, we need to decide what variational form we are going to use
    as the ansatz. To keep things simple, we will stick with the `EfficientSU2` that
    we used in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'This variational form is not included in PennyLane at the time of writing this
    book, but we can easily implement it with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we have fixed the number of repetitions to ![1](img/file13.png "1"),
    which was the case that we were using with Qiskit in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our variational form, we can use it to implement the VQE algorithm
    in PennyLane. To do that, we will define the energy function, which we compile
    as a quantum node because it needs to be evaluated on a device capable of running
    quantum circuits. We can do that with the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how we have used the `EfficientSU2` ansatz followed by an evaluation
    of the expectation value of our Hamiltonian (by using the `qml``.``expval` function
    that we introduced in *Chapter* *[*5*](ch013.xhtml#x1-940005), *QAOA: Quantum
    Approximate* *Optimization Algorithm*). Now, to execute VQE, we only need to select
    some initial values for the ansatz parameters and use a minimizer to find their
    optimal values. We can achieve that with the following piece of code:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE33]'
  prefs: []
  type: TYPE_NORMAL
- en: We have imported the `minimize` function from the `scipy``.``optimize` package
    (scipy is a powerful and very popular Python library for scientific computing).
    We have chosen at random some initial values for the variational form parameters.
    We have used `requires_grad``=``True` to allow the minimizer to compute gradients
    in order to optimize the parameters (we will have much more to say about this
    in *Part* *[*III*](ch016.xhtml#x1-138000III) of the book). Then, we have minimized
    the `energy` function using the default parameters of the `minimize` method. Notice
    how the `x0` argument is used to specify the initial values.*
  prefs: []
  type: TYPE_NORMAL
- en: '*The result we obtain upon running this code is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This includes the optimal values found by the optimizer (the `x` field) as well
    as the minimum energy. As you can check, this fits nicely with the results that
    we have obtained with Qiskit for the total molecular energy.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to run VQE on a simulator with PennyLane, we will turn
    to the task of executing the algorithm on actual quantum computers.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.3 Running VQE on real quantum devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may remember that, back in *Section* *[*5.3*](ch013.xhtml#x1-1040005.3),
    we mentioned that there is a PennyLane Runtime client that can be used to run
    VQE programs. This is exactly what we need now, so it is the perfect moment to
    learn how to use it.*
  prefs: []
  type: TYPE_NORMAL
- en: '*In fact, using this Runtime implementation is very easy, because it is quite
    similar to the one we used with Qiskit. First, we need be sure that we have `pennylane_qiskit`
    installed and our IBM Quantum account enabled (see *Appendix* *[*D*](ch027.xhtml#x1-240000D),
    *Installing the Tools*, for directions). Then, we can run the following instructions:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[PRE35]'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is pretty much self-explanatory: we are just selecting the options
    for our VQE execution, including the device to run the circuits which, in this
    case, happens to be `ibm_oslo`. After waiting for the job to finish running, we
    will obtain an output similar the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'You may be wondering if we can also use error mitigation to try to improve
    our results. The answer is yes, of course. In fact, it is straightforward to set
    up, because we only need to include the additional parameter `use_measurement_mitigation`
    `=` `True` when creating the `vqe_runner` object. Running with this option will
    give you a result similar to the following one, which is closer to the real optimal
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: With this, we conclude our study of VQE and, in fact, we conclude the part of
    the book devoted to optimization problems. Starting with the next chapter, we
    will dive into the fascinating world of quantum machine learning. Hang tight and
    prepare for the ride!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have studied Hamiltonians and observables in detail. In
    particular, we have learned how to derive mathematical expressions for their expectation
    values and how to estimate these quantities using quantum computers.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we studied the VQE algorithm and how it can be used to find ground states
    of general Hamiltonians. We also described a modification of VQE called VQD that
    can also be used to compute excited states and not just states of minimum energy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we moved to practical matters and learned how to use Qiskit to run VQE
    and VQD. We illustrated this with a very interesting problem: that of finding
    the ground state of a simple molecule. We then introduced methods to simulate
    the behavior of quantum algorithms when there is noise and how to reduce the adverse
    effect of readout errors with some mitigation techniques. We also studied how
    to run VQE problems on actual quantum computers with IBM runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: After that, we also learned how to implement and run VQE on PennyLane, again
    solving a molecular structure problem. We even studied how to use Runtime from
    PennyLane to send VQE instances to real quantum computers.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you are now able to understand all the mathematical
    details behind the VQE algorithm. You also know how to run it on different types
    of problems with both Qiskit and PennyLane. Moreover, you now can run noisy simulations
    of all the algorithms that we have studied (and of any other quantum algorithm
    that you may learn in the future) as well as perform readout error mitigation
    on simulated and actual quantum devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will start studying the second big topic of the book:
    quantum machine learning. Prepare to learn how (quantum) machines learn!********************************'
  prefs: []
  type: TYPE_NORMAL
