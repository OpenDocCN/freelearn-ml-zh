<html><head></head><body>
		<div id="_idContainer159">
			<h1 id="_idParaDest-230"><a id="_idTextAnchor296"/>Chapter 14: Managing SageMaker Features across Accounts</h1>
			<p>AWS publishes best practices around the management and governance of workloads. These practices touch on many areas, such as cost optimization, security, compliance, and ensuring the operational efficiency of workloads scaled on AWS. Multi-account patterns are one common architectural consideration when building, deploying, and operating workloads that utilize the features of Amazon SageMaker. </p>
			<p>In this section, we won't cover the well-established recommendations and considerations around the governance of AWS workloads across AWS accounts. Rather, we will specifically focus on some of the considerations around the usage of AWS features across AWS accounts. For more information about general recommendations for choosing the right account strategy, please refer to <strong class="bold">AWS Management and Governance services</strong> (<a href="https://aws.amazon.com/products/management-and-governance/">https://aws.amazon.com/products/management-and-governance/</a>) and the <strong class="bold">AWS Multi-Account Landing Zone strategy</strong> – <strong class="bold">AWS Control Tower</strong> (<a href="https://docs.aws.amazon.com/controltower/latest/userguide/aws-multi-account-landing-zone.html">https://docs.aws.amazon.com/controltower/latest/userguide/aws-multi-account-landing-zone.html</a>).</p>
			<p>The concept of a multi-account strategy is built on the <strong class="bold">AWS Well-Architected Framework</strong>, where having multiple AWS accounts allows you to better govern and manage machine learning activities on <strong class="bold">Amazon SageMaker</strong> across the <strong class="bold">Machine Learning Development Lifecycle (ML Lifecycle)</strong>. The benefits of using multiple AWS accounts are documented for general workloads. </p>
			<p>In this chapter, we'll discuss the following topics as they relate to managing SageMaker features across multiple AWS accounts:</p>
			<ul>
				<li>Examining an overview of the AWS multi-account environment</li>
				<li>Understanding the benefits of using multiple AWS accounts with Amazon SageMaker</li>
				<li>Examining multi-account considerations with Amazon SageMaker</li>
			</ul>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor297"/>Examining an overview of the AWS multi-account environment</h1>
			<p>There<a id="_idIndexMarker686"/> are many variations of multi-account strategies that are valid. Multi-account implementations can vary based on the organizational and technical needs of a customer. For the purposes of this chapter, we will focus on a basic multi-account strategy, focusing on only the accounts that are most relevant to a machine learning workload using Amazon SageMaker. We don't explicitly call out accounts (such as security or logging) because they are already well defined in the context of AWS governance practices. <em class="italic">Figure 14.1</em> illustrates the general, high-level accounts we will use to discuss the concepts in this chapter.</p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B17249_14_01.jpg" alt="Figure 14.1 – Example of AWS accounts and SageMaker features&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.1 – Example of AWS accounts and SageMaker features</p>
			<p>Using <em class="italic">Figure 14.1</em> as an example, the following <a id="_idIndexMarker687"/>AWS accounts may be used as part of an end-to-end<a id="_idTextAnchor298"/> ML Lifecycle. Please keep in mind that account naming and resource placement may vary considerably across implementations. Each account is described at a high level, in order to focus more on the account purpose versus the naming standard itself: </p>
			<ol>
				<li><strong class="bold">Shared Services account</strong>: This<a id="_idIndexMarker688"/> account can be named <a id="_idIndexMarker689"/>many things, and is also referred to as a <strong class="bold">DevOps</strong> or application management account. For the purposes of this <a id="_idIndexMarker690"/>chapter, we refer to this account as the one that can often include the services and tooling used for the management of end-to-end pipelines and the ongoing management of workloads.</li>
				<li><strong class="bold">Data platform/data lake</strong>: This <a id="_idIndexMarker691"/>account acts as the<a id="_idIndexMarker692"/> central repository for datasets, both raw and curated, used for model-building activities.</li>
				<li><strong class="bold">Data science account</strong>: This <a id="_idIndexMarker693"/>account (or accounts) represents <a id="_idIndexMarker694"/>the environments where model development activities are performed.</li>
				<li><strong class="bold">Test account</strong>: This<a id="_idIndexMarker695"/> account represents the environment where a <a id="_idIndexMarker696"/>model will be tested. This account typically includes integration and performance testing.</li>
				<li><strong class="bold">Production account</strong>: This<a id="_idIndexMarker697"/> account represents the <a id="_idIndexMarker698"/>environment hosting models supporting live applications and workloads. This account typically has the highest levels of controls and restrictions. </li>
				<li><strong class="bold">Service Catalog master account</strong>: The<a id="_idIndexMarker699"/> purpose of this<a id="_idIndexMarker700"/> account is to maintain a central hub of products that can be offered through the <strong class="bold">AWS Service Catalog</strong> and used to consistently provision resources in spoke accounts, such as<a id="_idIndexMarker701"/> the <strong class="bold">data science account</strong>. A spoke account<a id="_idIndexMarker702"/> is an AWS account that has been given access to portfolios managed from the master account.</li>
			</ol>
			<p>Again, these <a id="_idIndexMarker703"/>accounts are high-level representations of a potential account structure and are not inclusive of every variation that is valid given the requirements of your own environments. In the next section, we'll discuss the benefits of using multiple AWS accounts specifically as they relate to using Amazon SageMaker across the ML Lifecycle</p>
			<p> </p>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor299"/>Understanding the benefits of using multiple AWS accounts with Amazon SageMaker</h1>
			<p>In this section, we'll cover the general, high-level benefits of using multiple AWS accounts. We'll also discuss the considerations that are specific to using Amazon SageMaker across the ML Lifecycle:</p>
			<ul>
				<li><strong class="bold">Benefit #1</strong>: Implementing specific security controls <p>Using multiple AWS accounts allows customers to implement security controls that are specific to the workload, environment, or data. As an example, some workloads may have unique security requirements (such as PCI compliance) and require additional controls. Using multiple accounts allows you to maintain fine-grained controls that are isolated and auditable at the AWS account level.</p><p>For the model-building activities included in the ML Lifecycle, using multiple AWS accounts allows you to create and manage data science environments that include the controls that are specific to machine learning, as well as to your security requirements. With machine learning, data scientists need access to live production data. Typically, that data should be scrubbed of any sensitive data before a data scientist gains access. However, there are use cases where a data scientist may need access to that sensitive data. By separating data science environments that have access to sensitive data and those that do not have access to sensitive data, you're able to implement controls at the account level, as well as to audit at the account level. </p><p>For model deployment activities included in the ML Lifecycle, you will want to ensure your models serving live traffic or providing critical inference data are managed and controlled. This would be the case with any other production application. You wish to ensure availability. Just as you would not implement a live web application in the same account where developers have broad access, the same is true for machine learning workloads serving live production workloads. </p><p>As an example, a <strong class="bold">SageMaker endpoint</strong> serving <a id="_idIndexMarker704"/>a production application should be <a id="_idIndexMarker705"/>hosted in an AWS account that has all of the controls and restricted access in place (you would want this to be the case as with any other production workload). This ensures the endpoint isn't inadvertently deleted in a lower-level account that may have fewer controls and broader access permissions granted. </p></li>
				<li><strong class="bold">Benefit #2</strong>: Supporting the needs of multiple teams<p>Large organizations and enterprises are often looking for scalable mechanisms to support the resource needs and responsibilities of different teams. Across lines of business, it's common to have separate AWS accounts. The same is true for machine learning workloads. An example here includes <strong class="bold">data science environments</strong> (as discussed in <a href="B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a><em class="italic">, Data Science Environments</em>), where each <a id="_idIndexMarker706"/>team may have different requirements for an environment in which to build machine learning models. In this case, it's common to have multiple data science environments supporting multiple teams, as well as supporting the requirements acros<a id="_idTextAnchor300"/>s and within teams. </p></li>
			</ul>
			<h1 id="_idParaDest-233"><a id="_idTextAnchor301"/>Examining multi-account considerations with Amazon SageMaker</h1>
			<p>In this section, we'll <a id="_idIndexMarker707"/>cover multi-account considerations with Amazon SageMaker. We'll first look at a general reference architecture, then discuss some of the considerations for specific SageMaker features across the ML Lifecycle. </p>
			<p><em class="italic">Figure 14.2</em> shows an example of a <a id="_idIndexMarker708"/>multi-account structure mapping key SageMaker features and other common AWS services to the accounts they are typically used in. This is not a one-size-fits-all view, as there may be other AWS services or third-party tools that are performing one or more of the functions performed by the AWS services shown. As an <a id="_idIndexMarker709"/>example, your model registry <a id="_idIndexMarker710"/>may be the <strong class="bold">SageMaker model registry</strong>, or it could alternatively be <strong class="bold">Amazon DynamoDB</strong> or a tool such <a id="_idIndexMarker711"/>as <strong class="bold">MLflow</strong>:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B17249_14_02.jpg" alt="Figure 14.2 – Example of service use across AWS accounts&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.2 – Example of service use across AWS accounts</p>
			<p>The placement of the AWS, or equivalent, supporting the ML Lifecycle map to the phase, model build, or model deploy. This is in combination with the benefits addressed earlier in being able to implement security controls by accounts, as well as to support the requirements of the different roles and personas that operate within each account. The naming and structure of accounts may vary across multi-account implementations. Therefore, in the following list, we describe<a id="_idIndexMarker712"/> the purpose of each account, knowing these may vary across implementations:</p>
			<ul>
				<li>A Shared Services account, or DevOps account, is often used to centralize the tooling <a id="_idIndexMarker713"/>that is used to manage workloads across multiple accounts and environments. In this case, you see a few common services, such as the <strong class="bold">Amazon Elastic Container Registry</strong> for<a id="_idIndexMarker714"/> managing SageMaker compatible images for training and inference. You also often find developer tools that enable <strong class="bold">continuous integration</strong> (<strong class="bold">CI)</strong>/ <strong class="bold">continuous delivery or deployment</strong> (<strong class="bold">CD)</strong> practices.</li>
				<li>There are the tools that are needed to automate and orchestrate the steps of the machine learning workflow across account<a id="_idTextAnchor302"/>s. These can include native <strong class="bold">AWS Developer Tools</strong> or<a id="_idIndexMarker715"/> third-party tooling such as <strong class="bold">GitHub</strong> or <strong class="bold">Jenkins</strong>. The <a id="_idIndexMarker716"/>tools and services used in this account require<a id="_idIndexMarker717"/> cross-account <strong class="bold">identity and access management</strong> (<strong class="bold">IAM</strong>) permission policies. Finally, you need to create centralized dashboards for monitoring the health of your machine learning workloads. These shared<a id="_idIndexMarker718"/> dashboards are often placed in the Shared Services account, an <strong class="bold">infrastructure account</strong>, or one of the environment- or workload-specific accounts, such as production. </li>
				<li>The <strong class="bold">data platform</strong>, or <strong class="bold">data lake account</strong>, contains a data lake using a native<a id="_idIndexMarker719"/> service, such as <strong class="bold">AWS Lake Formation</strong> or a<a id="_idIndexMarker720"/> custom data lake. This account is also a common option for placing the centralized feature store that is used to store features for use across teams. </li>
				<li>The data science account<a id="_idIndexMarker721"/> is primarily used for model building activities so this includes all of the activities required to perform data understanding, feature engineering, model training across experiments, and<a id="_idIndexMarker722"/> model evaluation. This account requires access to SageMaker features<a id="_idIndexMarker723"/> needed for <a id="_idIndexMarker724"/>those model-building<a id="_idIndexMarker725"/> activities including features such as <strong class="bold">Amazon SageMaker Studio</strong>, <strong class="bold">SageMaker training jobs</strong>, <strong class="bold">SageMaker Pocessing jobs</strong>, and <strong class="bold">SageMaker Data Wrangler</strong>.</li>
				<li>In addition to the common features needed for model building, there are additional AWS services that get provisioned in this account when you are using <strong class="bold">SageMaker projects</strong>. By default, SageMaker projects<a id="_idIndexMarker726"/> automatically provision and configure AWS Developer Tools and the AWS Service Catalog products for built-in MLOps project templates in the account you are using for your model-building activities. </li>
				<li>Workload or<a id="_idIndexMarker727"/> environment-specific accounts, such as test and production, are used to host live models. These accounts also commonly host the broader solution where your model is used. From a SageMaker perspective, the features used in these accounts typically focus on model deploy and operate activities. </li>
				<li>Finally, you may also have an <strong class="bold">AWS Service Catalog master</strong> or infrastructure account that<a id="_idIndexMarker728"/> contains the portfolios of products that can be shared across multiple teams. This is known as the hub account. This can be used to create and manage a central catalog of products for data science environments or for custom MLOps project templates with SageMaker projects. </li>
			</ul>
			<p>Some AWS features are very specific to the persona and phase in the ML Lifecycle where they are needed. As an example, SageMaker training jobs are typically needed by data scientists for model-building activities or are needed as part of an automated model retraining workflow. However, there are several AWS services that span phases of the ML Lifecycle that require some unique considerations. These will be explored further in the next section. </p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor303"/>Considerations for SageMaker features</h2>
			<p>There are several <a id="_idIndexMarker729"/>SageMaker features that require additional considerations when attempting to implement them in a multi-account strategy, specifically because these features are used across the ML Lifecycle. Considerations for features, such as SageMaker Processing, SageMaker training jobs, and SageMaker hosting, are generally specific to a phase in the lifecycle. Therefore, their placement across accounts is covered in <em class="italic">Figure 14.3</em>. In this section, we'll cover a few of the SageMaker features that span the ML Lifecycle and require additional consideration as part of your multi-account strategy. </p>
			<h3>Amazon SageMaker Pipelines</h3>
			<p>SageMaker Pipelines<a id="_idIndexMarker730"/> allows you to code your machine learning pipelines using the Amazon SageMaker Python SDK. Pipelines includes SageMaker native steps focused on data preparation (via SageMaker Processing), model training (via SageMaker training jobs), and model deployment (via SageMaker batch transform). <strong class="bold">Pipelines</strong> also includes <strong class="source-inline">CallbackStep</strong> to integrate with other AWS services or third-party tasks. Finally, Pipelines has built-in steps for pipeline functionality, such as a conditional step. All of the current capabilities within SageMaker Pipelines focus on model building and model deployment for batch inference. As a result, we'll look at two common patterns that have cross-account considerations when using SageMaker Pipelines.</p>
			<p>In the first pattern, we'll discuss an end-to-end pipeline scenario where you are deploying a model for real-time inference using SageMaker hosting. In this case, you can use SageMaker Pipelines in your data science account to create a pipeline that can be used to automate the model-building activities. These activities include data preparation, model training, model evaluation, and a conditional step for model registration. Once a model passes evaluation and is registered, it can be used as a trigger for downstream deployment to your accounts (such as testing or production) that will host and integrate deployed endpoints. This same pipeline can be used for your retraining workflows. </p>
			<p>In this case, model deployment to higher environments can be done using a cross-account resource policy, as shown in <em class="italic">Figure 14.3</em>. The cross-account resource policy is created for the <strong class="bold">model group</strong> in the <strong class="bold">SageMaker model registry</strong>. That model group contains the model versions, the Amazon ECR repository for the inference image, and the S3 location of the model artifacts. A cross-account resource policy can be created with all three of these resources that then allows you to deploy a model that was created in your data science environment into your application or workload <a id="_idIndexMarker731"/>environments (such as testing or production).</p>
			<p>Refer to the following figure:</p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B17249_14_03.jpg" alt="Figure 14.3 – Cross-account resource policy to deploy a model trained in a data science account&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.3 – Cross-account resource policy to deploy a model trained in a data science account</p>
			<p>In the second pattern, we'll discuss an end-to-end pipeline scenario where you are deploying a model for batch inference using SageMaker hosting. In this case, you can use <a id="_idIndexMarker732"/>SageMaker Pipelines in your data science account to create a pipeline that can be used to automate the model-building activities. These include data preparation, model training, model evaluation, a conditional step for model registration, and a batch transform step. In this case, there are two options depending on your use case and requirements:</p>
			<ul>
				<li><strong class="bold">Run your end-to-end pipeline in your data science account</strong>: This option is valid if you are using batch transform to validate your models or you're running batch jobs that don't have production-level availability requirements.</li>
				<li><strong class="bold">Run your end-to-end pipeline in workload accounts</strong>: This option is valid if you are using batch transform to deploy models that have production-level availability requirements and/or require integration with systems in higher-level environments.</li>
			</ul>
			<h3>Amazon SageMaker projects</h3>
			<p>Amazon SageMaker projects<a id="_idIndexMarker733"/> build on SageMaker Pipelines by incorporating CI/CD practices (such as source and version control) combined with automated deployment pipelines into one or more target environments. When considering integrating SageMaker projects with multiple AWS accounts, the following are key points to understand: </p>
			<ul>
				<li>When you enable project templates for your Studio domain or domain users, the account where projects are enabled is the one that will be used for the built-in MLOps project templates offered through AWS Service Catalog. If you build custom MLOps project templates, you can still use the hub-and-spoke model to manage your portfolio and products in a Service Catalog master account.</li>
				<li>All built-in MLOps project templates will provision and configure the following <a id="_idIndexMarker734"/>resources in the same account where projects are enabled: <strong class="bold">AWS CodePipeline</strong>, <strong class="bold">AWS CodeBuild</strong>, <strong class="bold">AWS CodeCommit</strong>, and <strong class="bold">Amazon EventBridge</strong>. This is important as some organizations assume or <a id="_idIndexMarker735"/>require these services to be centrally configured <a id="_idIndexMarker736"/>and managed through a shared services <a id="_idIndexMarker737"/>account (or equivalent). </li>
				<li>The built-in <a id="_idIndexMarker738"/>MLOps project templates will deploy your SageMaker endpoints to the same account where projects are enabled. This behavior can be modified. However, the model registry still exists in the data science account. </li>
			</ul>
			<h3>Amazon SageMaker Feature Store</h3>
			<p><strong class="bold">Amazon SageMaker Feature Store</strong> allows creating and sharing features, both for model-building activities and model inference. Because a feature store can be used for both model-building activities as well as a dependency for model inference, it's important to ensure <a id="_idIndexMarker739"/>features remain consistent across teams and are consistently available when needed.</p>
			<p>When you create a feature store, it gets instantiated in the account that you created it in. However, that may not be the optimal choice when centralizing features for sharing across teams, or when using the feature store for real-time inference. If you create the feature store in your data science account, that account may have fewer controls and more access permissions in place for a broader set of roles. This creates risk when supporting production applications.</p>
			<p>There are two common cross-account patterns related to Feature Store that facilitate feature sharing and consistency across teams, as well as allowing the flexibility for team- or organization-specific feature stores when needed. </p>
			<p>In the first pattern, shown in <em class="italic">Figure 14.4</em>, a central feature store is created in a separate AWS account that is accessible via an IAM cross-account role for both the population and consumption of features. For the population of features, this is typically done through a feature pipeline that is automated and collecting data at regular frequencies. However, it can also be done from the data science environment for more static features. Features can then be consumed for both inference as well as for model-building activities. Model-building activities often consume features from the offline<a id="_idIndexMarker740"/> feature s<a id="_idTextAnchor304"/>tore using cross-account permissions:</p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B17249_14_04.jpg" alt="Figure 14.4 – Central Feature Store pattern&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.4 – Central Feature Store pattern</p>
			<p>In the second pattern, similar to <em class="italic">Figure 14.4</em>, there is a central feature store that is used for sharing features that <a id="_idIndexMarker741"/>may be common or useful across teams, but there is also the flexibility for individual teams to create their own feature stores in separate AWS accounts. This pattern is useful to facilitate the ability to share common features in a central store, while also allowing workload- or application-specific features to be secured in an account that only requires access by the specific teams or applications that need those features. </p>
			<h3>Amazon SageMaker Data Wrangler </h3>
			<p>Amazon SageMaker Data Wrangler<a id="_idIndexMarker742"/> allows data scientists to explore and prepare data for machine learning during the model build phases of the ML Lifecycle. Because Data Wrangler is purpose-built for feature engineering and data preparation, the most common persona that will work with Data Wrangler<a id="_idIndexMarker743"/> are <strong class="bold">ML builders</strong>. Most model-building activities are going to happen inside one or more data science accounts; however, you typically need a way to securely access data from a data platform or data lake account for those model-building activities.</p>
			<p><em class="italic">Figure 14.5</em> illustrates a common pattern for enabling cross-account access from a data science account, where Data Wrangler is being used, to a data platform/data lake account, where the data typically resides. In this case, we are using AWS Lake Formation for our secure data lake. The same concepts apply when utilizing other technologies for your data lake; however, the implementation may differ:</p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B17249_14_05.jpg" alt="Figure 14.5 – Cross-account access for SageMaker Data Wrangler&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.5 – Cross-account access for SageMaker Data Wrangler</p>
			<p>With Data Wrangler, you're<a id="_idIndexMarker744"/> able to enable cross-account permissions using AWS IAM. To do this, you need to set up cross-account permissions for Data Wrangler in the data science account that allows access to the data tables stored in your data platform/data lake account. This is accomplished through Lake Formation permissions. This setup allows you to still provide access to datasets for your data scientists, but also allows you to take advantage of the security controls that Lake Formation offers. </p>
			<p>For example, you can choose to share only specific tables or even to share only specific columns of tables stored in your data lake. Tables are shared using AWS Resource Access Manager. This provides a way to share Lake Formation tables across AWS accounts. This allows users to access shared tables in secondary accounts. These shared tables are accessible directly in Lake Formation, but they are also available as a data source, via Amazon Athena, in your Data Wrangler UI.</p>
			<h1 id="_idParaDest-235"><a id="_idTextAnchor305"/>Summary</h1>
			<p>In this chapter, we discussed the benefits of using multiple accounts to manage and operate machine learning workloads that use Amazon SageMaker across the ML Lifecycle. We also looked at common patterns for account isolation across the ML Lifecycle. Finally, we focused specifically on the SageMaker features that are most often used across accounts, and the considerations you should be aware of when architecting and building end-to-end machine learning solutions.</p>
			<p>This chapter wraps up the book where we covered best practices for SageMaker across features spanning the machine learning lifecycle of data preparation, model training, and operations. In this book, we discussed best practices, as well as considerations, that you can draw on when creating your own projects. We used an example use case, using open weather data to demonstrate the concepts throughout the chapters of the book. This was done so you can get hands-on with the concepts and practices discussed. We hope you're able to apply these practices to your own projects while benefiting from the overall capabilities and features offered by Amazon SageMaker. </p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor306"/>References</h1>
			<p>Please see the following references for general AWS best practices on governance and multi-account strategies, as well as information specific to SageMaker features: </p>
			<ul>
				<li>Establishing best practices in your AWS environment : <a href="https://aws.amazon.com/organizations/getting-started/best-practices/">https://aws.amazon.com/organizations/getting-started/best-practices/</a></li>
				<li>AWS Control Tower – AWS services to establish and manage multiple AWS accounts: <a href="https://aws.amazon.com/controltower/">https://aws.amazon.com/controltower/</a></li>
				<li>SageMaker – Deploying a model to a different AWS account: <a href="https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-cross-account-model/">https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-cross-account-model/</a></li>
				<li>SageMaker Data Wrangler – Enable cross-account access: <a href="https://aws.amazon.com/blogs/machine-learning/enable-cross-account-access-for-amazon-sagemaker-data-wrangler-using-aws-lake-formation/">https://aws.amazon.com/blogs/machine-learning/enable-cross-account-access-for-amazon-sagemaker-data-wrangler-using-aws-lake-formation/</a></li>
				<li>SageMaker Pipelines – Multi-account deployments: <a href="https://aws.amazon.com/blogs/machine-learning/multi-account-model-deployment-with-amazon-sagemaker-pipelines/">https://aws.amazon.com/blogs/machine-learning/multi-account-model-deployment-with-amazon-sagemaker-pipelines/</a></li>
				<li>SageMaker Feature Store: <a href="https://aws.amazon.com/blogs/machine-learning/enable-feature-reuse-across-accounts-and-teams-using-amazon-sagemaker-feature-store/">https://aws.amazon.com/blogs/machine-learning/enable-feature-reuse-across-accounts-and-teams-using-amazon-sagemaker-feature-store/</a></li>
			</ul>
		</div>
	</body></html>