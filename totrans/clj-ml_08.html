<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Anomaly Detection and Recommendation </h1></div></div></div><p>In this chapter, we will study a couple of modern forms of applied machine learning. We will first explore the problem of <em>anomaly detection</em> and we will discuss <em>recommendation systems</em> later in this chapter.</p><p>
<strong>Anomaly detection</strong><a id="id721" class="indexterm"/> is a machine learning technique in which we determine whether a given set of values for some selected features that represent the system are unexpectedly different from the normally observed values of the given features. There are several applications of anomaly detection, such as detection of structural and operational defects in manufacturing, network intrusion detection systems, system monitoring, and medical diagnosis.</p><p>
<strong>Recommendation systems</strong><a id="id722" class="indexterm"/> are essentially information systems that seek to predict a given user's liking or preference for a given item. Over recent years, there have been a vast number of recommendation systems, or <strong>recommender systems</strong>, that have been built for several business and social applications to provide a better experience for their users. Such systems can provide a user with useful recommendations depending on the items that the user has previously rated or liked. Most existing recommendation systems today provide recommendations to users about online products, music, and social media. There are also a significant number of financial and business applications on the Web that use recommendation systems.</p><p>Interestingly, both anomaly detection and recommendation systems are applied forms of machine learning problems, which we have previously encountered in this book. Anomaly detection is in fact an extension of binary classification, and recommendation is actually an extended form of linear regression. We will study more about these similarities in this chapter.</p><div><div><div><div><h1 class="title"><a id="ch08lvl1sec52"/>Detecting anomalies</h1></div></div></div><p>
<em>Anomaly detection</em> is<a id="id723" class="indexterm"/> essentially the identification of items or observed values that do not conform to an expected pattern (for more information, refer to "A Survey of Outlier Detection Methodologies"). The pattern could be determined by values that have been previously observed, or by some limits across which the input values can vary. In the context of machine learning, anomaly detection can be performed in both supervised and unsupervised environments.<a id="id724" class="indexterm"/> Either way, the problem of anomaly detection is to find input values that are significantly different from other input values. There are several applications of this technique, and in the broad sense, <a id="id725" class="indexterm"/>we can use anomaly detection for the following reasons:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">To detect problems</li><li class="listitem" style="list-style-type: disc">To detect a new phenomenon</li><li class="listitem" style="list-style-type: disc">To monitor unusual behavior</li></ul></div><p>The observed values that are found to be different from the other values are called outliers, anomalies, or exceptions. More formally, we define an <strong>outlier</strong><a id="id726" class="indexterm"/> as an observation that lies outside the overall pattern of a distribution. By <em>outside</em>, we mean an observation that has a high numerical or statistical distance from the rest of the data.</p><p>Some examples of outliers can be depicted by the following plots, where the red crosses mark normal observations and the green crosses mark the anomalous observations:</p><div><img src="img/4351OS_08_01.jpg" alt="Detecting anomalies"/></div><p>One possible approach to anomaly detection is to use a<a id="id727" class="indexterm"/> <em>probability distribution model</em>, which is built from<a id="id728" class="indexterm"/> the training data to detect anomalies. Techniques that use this approach are termed as <em>statistical methods</em> of anomaly detection.<a id="id729" class="indexterm"/> In this approach, <a id="id730" class="indexterm"/>an anomaly will have a low probability with respect to the overall probability distribution of the rest<a id="id731" class="indexterm"/> of the sample data. Hence, we try to fit a model onto the available sample data and use this formulated model to detect anomalies. The<a id="id732" class="indexterm"/> main problem with this approach is that it's hard to find a standard distribution model for stochastic data.</p><p>Another method that can be used to detect anomalies<a id="id733" class="indexterm"/> is a <em>proximity-based approach</em>. In this approach, we determine the proximity, <a id="id734" class="indexterm"/>or nearness, of a<a id="id735" class="indexterm"/> set of observed values with respect to the rest of the values in the sample data. For example, we could use the <strong>K-Nearest Neighbors</strong> (<strong>KNN</strong>) algorithm to determine the distances of a given observed value to its <em>k</em> nearest values. This technique is much simpler than estimating a statistical model over the sample data. This is because it's easier to determine a single measure, which is the proximity of an<a id="id736" class="indexterm"/> observed value, than it is to fit a standard model on the available training data. However, determining the proximity of a set of input values could be inefficient for larger datasets. For example, the KNN algorithm has a time complexity of <img src="img/4351OS_08_02.jpg" alt="Detecting anomalies"/>, and computing the proximity of a given set of values to its <em>k</em> nearest values could be inefficient for a large value of <em>k</em>. Also, the KNN algorithm could be sensitive to the value of the neighbors <em>k</em>. If the value of <em>k</em> is too large, clusters of values with less than <em>k</em> individual sets of input values could be falsely classified as anomalies. On the other hand, if <em>k</em> is too small, some anomalies that have a few neighbors with a low proximity may not be detected.</p><p>We can also determine whether a given set of observed values is an anomaly based on the density of data around it. This approach is termed as the <a id="id737" class="indexterm"/>
<strong>density-based approach</strong><a id="id738" class="indexterm"/> to anomaly detection. A given set of input values can be classified as an anomaly if the data<a id="id739" class="indexterm"/> around the given values is low. In anomaly detection, the density-based and proximity-based approaches are closely related. In fact, the density of data is generally defined in terms of the proximity or distance of a given set of values with respect to the rest of the data. For example, if we use the KNN algorithm to determine the proximity or distance of a given set of values to the rest of the data, we can define the density as the reciprocal of the average distance to the <em>k</em> nearest values, as follows:</p><div><img src="img/4351OS_08_03.jpg" alt="Detecting anomalies"/></div><p>
<em>Clustering-based approaches</em> can also be used to detect anomalies. Essentially, clustering can be used to determine groups or clusters of values in the sample data. The items in a cluster can be assumed to<a id="id740" class="indexterm"/> be closely related, and anomalies<a id="id741" class="indexterm"/> are values that cannot be related to previously encountered values in the clusters in the sample data. Thus,<a id="id742" class="indexterm"/> we could determine all the clusters in the sample data and then mark the smallest clusters as anomalies. Alternatively, we can form clusters from the sample data and determine the clusters, if any, of a given set of previously unseen values.</p><p>If a set of input values does not belong to any cluster, it's definitely an anomalous observation. The advantage of clustering techniques is that they can be used in combination with other machine learning techniques that we previously discussed. On the other hand, the problem with this approach is that most clustering techniques are sensitive to the number of clusters that have been chosen. Also, algorithmic parameters of clustering techniques, such as the average number of items in a cluster and number of clusters, cannot be determined easily. For example, if we are modeling some unlabeled data using the KNN algorithm, the number of clusters <em>K</em> would have to be determined either by trial-and-error or by scrutinizing the sample data for obvious clusters. However, both these techniques are not guaranteed to perform well on unseen data.</p><p>In models where the sample values are all supposed to conform to some mean value with some allowable tolerance, the <strong>Gaussian</strong> or <strong>normal distribution</strong><a id="id743" class="indexterm"/> is often used as<a id="id744" class="indexterm"/> the distribution model to train an anomaly detector. This model has two parameters—the mean <img src="img/4351OS_08_04.jpg" alt="Detecting anomalies"/> and the variance <img src="img/4351OS_08_05.jpg" alt="Detecting anomalies"/>. This distribution model is often used in statistical approaches to anomaly detection, where the input variables are normally found statistically close to some predetermined mean value.</p><p>The <strong>Probability Density Function</strong> (<strong>PDF</strong>) is<a id="id745" class="indexterm"/> often used<a id="id746" class="indexterm"/> by density-based methods of anomaly detection. This function essentially describes the likelihood that an input variable will take on a given value. For a random variable <em>x</em>, we can formally define the PDF as follows:</p><div><img src="img/4351OS_08_06.jpg" alt="Detecting anomalies"/></div><p>The PDF can also be used in combination with a normal distribution model for the purpose of anomaly detection. The PDF of a normal distribution is parameterized by the mean <img src="img/4351OS_08_04.jpg" alt="Detecting anomalies"/> and variance <img src="img/4351OS_08_05.jpg" alt="Detecting anomalies"/> of the distribution, and can be formally expressed as follows:</p><div><img src="img/4351OS_08_07.jpg" alt="Detecting anomalies"/></div><p>We will now demonstrate<a id="id747" class="indexterm"/> a simple implementation of an anomaly detector in Clojure, which is based on the PDF for a normal distribution as we previously discussed. For this example, we will use Clojure atoms to maintain all states in the model. Atoms are used to represent an atomic state in Clojure. By <em>atomic</em>, we mean that the underlying state changes completely or doesn't change at all—the changes in state are thus <em>atomic</em>.</p><p>We now define some functions to help us manipulate the features of the model. Essentially, we intend to represent these features and their values as a map. To manage the state of this map, we use an atom. Whenever the anomaly detector is fed a set of feature values, it must first check for any previous information on the features in the new set of values, and then it should start maintaining the state of any new features when it is necessary. As a function on its own cannot contain any external state in Clojure, we will use closures to bind state and functions together. In this implementation, almost all the functions return other functions, and the resulting anomaly detector will also be used just like a function. In summary, we will model the state of the anomaly detector using an atom, and then bind this atom to a function using a closure.</p><p>We start off by defining a function that initializes our model with some state. This state is essentially a map wrapped in an atom by using the <code class="literal">atom</code> function, as follows:</p><div><pre class="programlisting">(defn update-totals [n]
  (comp #(update-in % [:count] inc)
        #(update-in % [:total] + n)
        #(update-in % [:sq-total] + (Math/pow n 2))))

(defn accumulator []
  (let [totals (atom {:total 0, :count 0, :sq-total 0})]
    (fn [n]
      (let [result (swap! totals (update-totals n))
            cnt (result :count)
            avg (/ (result :total) cnt)]
        {:average avg
         :variance (- (/ (result :sq-total) cnt)
                      (Math/pow avg 2))}))))</pre></div><p>The <code class="literal">accumulator</code> function<a id="id748" class="indexterm"/> defined in the preceding code initializes an atom and returns a function that applies the <code class="literal">update-totals</code> function to a value <code class="literal">n</code>. The value <code class="literal">n</code> represents a value of an input variable in our model. The <code class="literal">update-totals</code> function also returns<a id="id749" class="indexterm"/> a function that takes a single argument, and then it updates the state in the atom by using the <code class="literal">update-in</code> function. The function returned by the <code class="literal">accumulator</code> function will use the <code class="literal">update-totals</code> function to update the state of the mean and variance of the model.</p><p>We now implement the following PDF function for normal distribution that can be used to monitor sudden changes in the feature values of the model:</p><div><pre class="programlisting">(defn density [x average variance]
  (let [sigma (Math/sqrt variance)
        divisor (* sigma (Math/sqrt (* 2 Math/PI)))
        exponent (/ (Math/pow (- x average) 2)
                    (if (zero? variance) 1
                        (* 2 variance)))]
    (/ (Math/exp (- exponent))
       (if (zero? divisor) 1
           divisor))))</pre></div><p>The <code class="literal">density</code> function<a id="id750" class="indexterm"/> defined in the preceding code is a direct translation of the PDF function for normal distribution. It uses functions and constants from the <code class="literal">Math</code> namespace such as, <code class="literal">sqrt</code>, <code class="literal">exp</code>, and <code class="literal">PI</code> to find the PDF of the model by using the accumulated mean and variance of the model. We will define the the <code class="literal">density-detector</code> function as shown in the following code:</p><div><pre class="programlisting"> (defn density-detector []
  (let [acc (accumulator)]
    (fn [x]
      (let [state (acc x)]
        (density x (state :average) (state :variance))))))</pre></div><p>The <code class="literal">density-detector</code> function<a id="id751" class="indexterm"/> defined in the preceding code initializes the state of our anomaly detector using the <code class="literal">accumulator</code> function, and it uses the <code class="literal">density</code> function on the state maintained by the accumulator to determine the PDF of the model.</p><p>Since we are dealing with maps wrapped in atoms, we can implement a couple of functions to perform this check by using the <code class="literal">contains?</code>, <code class="literal">assoc-in</code>, and <code class="literal">swap!</code> functions, as shown in the following code:</p><div><pre class="programlisting"> (defn get-or-add-key [a key create-fn]
  (if (contains? @a key)
    (@a key)
    ((swap! a #(assoc-in % [key] (create-fn))) key)))</pre></div><p>The <code class="literal">get-or-add-key</code> function<a id="id752" class="indexterm"/> defined in the preceding code looks up a given key in an atom containing a map by using the <code class="literal">contains?</code> function. Note the use of the <code class="literal">@</code> operator to dereference an atom into its wrapped value. If the key is found in the map, we simply call<a id="id753" class="indexterm"/> the map as a function as <code class="literal">(@a key)</code>. If the key is not found, we use the <code class="literal">swap!</code> and <code class="literal">assoc-in</code> functions to add a new key-value pair to the map in the atom. The value of this key-value pair is generated from the <code class="literal">create-fn</code> parameter that is passed to the g<code class="literal">et-or-add-key</code> function.</p><p>Using the <code class="literal">get-or-add-key</code> and <code class="literal">density-detector</code> functions we have defined, we can implement the following functions that return functions while detecting anomalies in the sample data so as to create the effect of maintaining the state of the PDF distribution of the model within these functions themselves:</p><div><pre class="programlisting">(defn atom-hash-map [create-fn]
  (let [a (atom {})]
    (fn [x]
      (get-or-add-key a x create-fn))))

(defn get-var-density [detector]
  (fn [kv]
    (let [[k v] kv]
      ((detector k) v))))

(defn detector []
  (let [detector (atom-hash-map density-detector)]
    (fn [x]
      (reduce * (map (get-var-density detector) x)))))</pre></div><p>The <code class="literal">atom-hash-map</code> function<a id="id754" class="indexterm"/> defined in the preceding code uses the <code class="literal">get-key</code> function with an arbitrary initialization function <code class="literal">create-fn</code> to maintain the state of a map in an atom. The detector function uses the <code class="literal">density-detector</code> function that we previously defined to initialize the state of every new feature in the input values that are fed to it. Note that this function returns a function that will accept a map with key-value parameters as the features. We can inspect the behavior of the implemented anomaly detector in the REPL as shown in the following code and output:</p><div><pre class="programlisting">user&gt; (def d (detector))
#'user/d
user&gt; (d {:x 10 :y 10 :z 10})
1.0
user&gt; (d {:x 10 :y 10 :z 10})
1.0</pre></div><p>As shown in the preceding code and output, we created a new instance of our anomaly detector by using the <code class="literal">detector</code> function. The <code class="literal">detector</code> function returns a function that accepts a map of key-value pairs of features. When we feed the map with <code class="literal">{:x 10 :y 10 :z 10}</code>, the anomaly detector returns <a id="id755" class="indexterm"/>a PDF of <code class="literal">1.0</code> since all samples in the data so far have the same feature values. The anomaly detector will always return this value as long as the number of features and the values of these features remains the same in all sample inputs fed to it. </p><p>When we feed the anomaly detector with a set of features with different values, the PDF is observed to change to a finite number, as shown in the following code and output:</p><div><pre class="programlisting">user&gt; (d {:x 11 :y 9 :z 15})
0.0060352535208831985
user&gt; (d {:x 10 :y 10 :z 14})
0.07930301229115849</pre></div><p>When the features show a large degree of variation, the detector has a sudden and large decrease in the PDF of its distribution model, as shown in the following code and output:</p><div><pre class="programlisting">user&gt; (d {:x 100 :y 10 :z 14})
1.9851385000301642E-4
user&gt; (d {:x 101 :y 9 :z 12})
5.589934974999084E-4</pre></div><p>In summary, anomalous sample values can be detected when the PDF of the normal distribution model returned by the anomaly detector described previously has a large difference from its previous values. We can extend this implementation to check some kind of threshold value so that the result is quantized. The system thus detects an anomaly only when this threshold value of the PDF is crossed. When dealing with real-world data, all we would have to do is somehow represent the feature values we are modeling as a map and determine the threshold value to use via trial-and-error method.</p><p>Anomaly detection<a id="id756" class="indexterm"/> can be used in both supervised and unsupervised machine learning environments. In supervised learning, the sample data will be labeled. Interestingly, we could also use binary classification, among other supervised learning techniques, to model this kind of data. We can choose between anomaly detection and classification to model labeled data by using the following guidelines:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Choose binary classification when the number of positive and negative examples in the sample data is almost equal. Conversely, choose anomaly detection if there are a very small number of positive or negative examples in the training data.</li><li class="listitem" style="list-style-type: disc">Choose anomaly detection<a id="id757" class="indexterm"/> when there are many sparse classes and a few dense classes in the training data.</li><li class="listitem" style="list-style-type: disc">Choose supervised learning techniques such as classification when positive samples that may be encountered by the trained model will be similar to positive samples that the model has already seen.</li></ul></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec53"/>Building recommendation systems</h1></div></div></div><p>Recommendation systems are<a id="id758" class="indexterm"/> information filtering systems whose goal is to provide its users with useful recommendations. To determine these recommendations, a recommendation system can use historical data about the user's activity, or it can use recommendations that other users liked (for more information, refer to "A Taxonomy of Recommender Agents on the Internet"). These two approaches are the basis of the two types of algorithms used by recommendation systems—<strong>content-based filtering</strong> and <strong>collaborative filtering</strong>. Interestingly, some recommendation systems even use a combination of these two techniques to provide users with<a id="id759" class="indexterm"/> recommendations. <a id="id760" class="indexterm"/>Both these techniques aim to recommend items, or domain objects that are managed or exchanged by user-centric applications, to its users. Such applications include several websites that provide users with online content and information, such as online shopping and media.</p><p>In <em>content-based filtering</em>, <a id="id761" class="indexterm"/>recommendations are determined by finding similar items by using a particular user's rating. Each item is represented as a set of discrete features or characteristics, and each item is also rated by several users. Thus, for each user, we have several sets of input variables to represent the characteristics of each item and a set of output variables that represent the user's rating for the item. This information can be used to recommend items with similar features or characteristics as items that were previously rated by a user.</p><p>
<em>Collaborative filtering</em> methods<a id="id762" class="indexterm"/> are based on collecting data about a given user's behavior, activities, or preferences and using this information to recommend items to users. The recommendation is based on how similar a user's behavior is to that of other users. In effect, a user's recommendations are based on her past behavior as well as decisions made by other users in the system. A collaborative filtering technique will use the preferences of similar users to determine the features of all available items in the system, and then it will recommend items with similar features as the items that a given set of users are observed to like.</p></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec54"/>Content-based filtering</h1></div></div></div><p>As we mentioned earlier, content-based filtering systems provide users with recommendations based on their past behavior as well as the characteristics of items that are positively rated or<a id="id763" class="indexterm"/> liked by the given user. We can also take into account the items that were disliked by the given user. An item is generally represented by several discrete attributes. These attributes are analogous to the input variables or features of a classification or linear regression based machine learning model.</p><p>For example, suppose we want to build a recommendation system that uses content-based filtering to recommend online products to its users. Each product can be characterized and identified by several known characteristics, and users can provide a rating for each characteristic of every product. The feature values of the products can have values between the 0 and 10, and the ratings provided by users for the products will have values within the range of 0 and 5. We can visualize the sample data for this recommendation system in a tabular representation, as follows:</p><div><img src="img/4351OS_08_08.jpg" alt="Content-based filtering"/></div><p>In the preceding table, the<a id="id764" class="indexterm"/> system has <img src="img/4351OS_08_09.jpg" alt="Content-based filtering"/> products and <img src="img/4351OS_08_10.jpg" alt="Content-based filtering"/> users. Each product is defined by <img src="img/4351OS_08_11.jpg" alt="Content-based filtering"/> features, each of which will have a value in the range of 0 and 10, and each product is also rated by a user. Let the rating of each product <img src="img/4351OS_08_12.jpg" alt="Content-based filtering"/> by a user <img src="img/4351OS_08_13.jpg" alt="Content-based filtering"/> be represented as <img src="img/4351OS_08_14.jpg" alt="Content-based filtering"/>. Using the input values <img src="img/4351OS_08_15.jpg" alt="Content-based filtering"/>, or rather the input vector <img src="img/4351OS_08_16.jpg" alt="Content-based filtering"/>, and the rating <img src="img/4351OS_08_14.jpg" alt="Content-based filtering"/> of a user <img src="img/4351OS_08_13.jpg" alt="Content-based filtering"/>, we can estimate a parameter vector <img src="img/4351OS_08_17.jpg" alt="Content-based filtering"/> that we can use to to predict a user's rating. Thus, content-based filtering in fact applies a copy of linear regression to each user's rating and each product's feature values to estimate a regression model that can in turn be used to estimate the users rating for some unrated products. In effect, we learn the parameter <img src="img/4351OS_08_17.jpg" alt="Content-based filtering"/> using the independent variables <img src="img/4351OS_08_16.jpg" alt="Content-based filtering"/> and the dependent variable <img src="img/4351OS_08_14.jpg" alt="Content-based filtering"/> and for all the users the system. Using the estimated parameter <img src="img/4351OS_08_17.jpg" alt="Content-based filtering"/> and some given values for the independent variables, we can predict the value of the dependent variable for any given user. The optimization problem for content-based filtering can thus be expressed as follows:</p><div><img src="img/4351OS_08_18.jpg" alt="Content-based filtering"/></div><div><img src="img/4351OS_08_19.jpg" alt="Content-based filtering"/></div><p>The optimization problem<a id="id765" class="indexterm"/> defined in the preceding equation can be applied to all users of the system to produce the following optimization problem for U<em> </em>users:</p><div><img src="img/4351OS_08_21.jpg" alt="Content-based filtering"/></div><p>In simple terms, the parameter vector <img src="img/4351OS_08_20.jpg" alt="Content-based filtering"/> tries to scale or transform the input variables to match the output variable of the model. The second term that is added is for <em>regularization</em>. Interestingly, the optimization problem defined in the receding equation is analogous to that of linear regression, and thus content-based filtering can be considered as an extension of linear regression.</p><p>The key issue<a id="id766" class="indexterm"/> with content-based filtering is whether a given recommendation system can learn from a user's preferences or ratings. Direct feedback can be used by asking for the rating of items in the system that they like, although these ratings can also be implied from a user's past behavior. Also, a content-based filtering system that is trained for a set of users and a specific category of items cannot be used to predict the same user's ratings for a different category of items. For example, it's a difficult problem to use a user's preference for news to predict the user's liking for online shopping products.</p></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec55"/>Collaborative filtering</h1></div></div></div><p>The other major form of recommendation is <em>collaborative filtering</em>, in which data about the behavior of several users with similar interests is analyzed and used to predict recommendations for these users. The main advantage<a id="id767" class="indexterm"/> of this technique is that the system does not rely on the values for the feature variables of its items, and consequently such a system does not need to know about the characteristics of the items that are provided by it. The features of the items are in fact determined dynamically using the users rating for these items and the behavior of the system users. We will examine more about the advantage in the latter part of this section.</p><p>An essential part of the model used by collaborative filtering depends on the behavior of its users. To build this part <a id="id768" class="indexterm"/>of the model, we can use the following methods to determine the user's rating for the items in the model in an explicit manner:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Asking the users to rate the items on a specific scale</li><li class="listitem" style="list-style-type: disc">Asking users to mark items as favorites</li><li class="listitem" style="list-style-type: disc">Presenting a small number of items to users and asking them to order them according to how much they like or dislike these items</li><li class="listitem" style="list-style-type: disc">Asking users to create a list of items or the kinds of items that they like.</li></ul></div><p>Alternatively, this information could also be gathered from a user's activity in an implicit fashion. Examples of this method of modeling the behavior of a system's users with a given set of items or products are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Observing the items that a user views</li><li class="listitem" style="list-style-type: disc">Analyzing the number of times a particular user views</li><li class="listitem" style="list-style-type: disc">Analyzing the user's social network and discovering users with similar interests</li></ul></div><p>For example, consider a recommendation system for an online shopping example that we discussed in the previous section. <a id="id769" class="indexterm"/>We we can use collaborative filtering to dynamically determine the feature values of the products available and predict the products that the user will be interested in. The sample data for such a system that uses collaborative filtering can be visualized by using the following table:</p><div><img src="img/4351OS_08_22.jpg" alt="Collaborative filtering"/></div><p>In the data shown in the preceding table, the features of the products are unknown. The only available data is the ratings of the users and the behavior models of the users.</p><p>The optimization problem for collaborative filtering and a product's user can be expressed as follows:</p><div><img src="img/4351OS_08_23.jpg" alt="Collaborative filtering"/></div><div><img src="img/4351OS_08_24.jpg" alt="Collaborative filtering"/></div><p>The preceding equation is seen to be the converse of the optimization problem that we defined for content-based filtering. Instead of estimating the parameter's vector <img src="img/4351OS_08_17.jpg" alt="Collaborative filtering"/>, collaborative filtering seeks to determine the values of the features of a product <img src="img/4351OS_08_16.jpg" alt="Collaborative filtering"/>. Similarly, we can define the optimization problem for multiple users as follows:</p><div><img src="img/4351OS_08_25.jpg" alt="Collaborative filtering"/></div><p>Using collaborative filtering,<a id="id770" class="indexterm"/> we can estimate the features of the products <img src="img/4351OS_08_26.jpg" alt="Collaborative filtering"/>,<img src="img/4351OS_08_27.jpg" alt="Collaborative filtering"/>, and then use these feature values to improve the behavior model of the users <img src="img/4351OS_08_28.jpg" alt="Collaborative filtering"/>. The improved user behavior models can then be used to again produce better feature values of the items. This process is then repeated until the feature values and behavior models converge to some appropriate values.</p><div><div><h3 class="title"><a id="note47"/>Note</h3><p>Note that in this process, the algorithm never needed to know the initial feature values of its items, and it only needed to initially estimate the behavior model of the user to provide the user with useful recommendations.</p></div></div><p>Collaborative filtering can also be combined with content-based filtering<a id="id771" class="indexterm"/> in some special cases. Such approaches are called <strong>hybrid methods</strong><a id="id772" class="indexterm"/> of recommendation. There are several ways in which we can combine or hybridize the two models of recommendation, and they are listed as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Results from the two models can be combined numerically in a weighted manner</li><li class="listitem" style="list-style-type: disc">Either one of these two models can be chosen appropriately at a given time</li><li class="listitem" style="list-style-type: disc">Show users a combined<a id="id773" class="indexterm"/> result of recommendations from the two models</li></ul></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec56"/>Using the Slope One algorithm</h1></div></div></div><p>We will now study the Slope One algorithm<a id="id774" class="indexterm"/> for collaborative filtering. Also, we will demonstrate how we can implement it concisely in Clojure.</p><p>The Slope One algorithm is one<a id="id775" class="indexterm"/> of the simplest forms of <em>item-based collaborative filtering</em>, which is essentially a collaborative filtering technique in which the users explicitly rate each item they like (for more information, refer to <em>Slope One Predictors for Online Rating-Based Collaborative Filtering</em>). Generally, item-based collaborative filtering techniques will use the user's ratings and past behavior of users to estimate a simple regression model for each user. Thus, we estimate a function <img src="img/4351OS_08_29.jpg" alt="Using the Slope One algorithm"/> for all users <img src="img/4351OS_08_13.jpg" alt="Using the Slope One algorithm"/> in the system.</p><p>Slope One algorithm uses a simpler predictor <img src="img/4351OS_08_30.jpg" alt="Using the Slope One algorithm"/> to model the regression pattern of a user's behavior, and is thus less computationally expensive. The parameter <img src="img/4351OS_08_31.jpg" alt="Using the Slope One algorithm"/> can be estimated by calculating the differences in user ratings between two items. Since the definition of the Slope One algorithm is simple, it can be implemented easily and efficiently. Interestingly, this algorithm is less susceptible to overfitting than other collaborative filtering techniques.</p><p>Consider a simple recommendation system with two items and two users. We can visualize this sample data with the following table:</p><div><img src="img/4351OS_08_32.jpg" alt="Using the Slope One algorithm"/></div><p>In the data shown in the preceding table, the difference in the ratings of <strong>Item A</strong> and <strong>Item B</strong> can be found by using the<a id="id776" class="indexterm"/> ratings provided by <strong>User 1</strong>. This difference is found to be <img src="img/4351OS_08_33.jpg" alt="Using the Slope One algorithm"/>. Thus, we can add this difference to the rating of <strong>Item A</strong> by <strong>User 2</strong> to predict his/her rating of <strong>Item B</strong>, which is equal to <img src="img/4351OS_08_34.jpg" alt="Using the Slope One algorithm"/>.</p><p>Let's extend the preceding<a id="id777" class="indexterm"/> example to three items and three users. The table for this data can be visualized as follows:</p><div><img src="img/4351OS_08_35.jpg" alt="Using the Slope One algorithm"/></div><p>In this example, the average difference in ratings between <strong>Item A</strong> and <strong>Item B</strong> for <strong>User 2</strong> (-1) and <strong>User 1</strong> (+2) is <img src="img/4351OS_08_36.jpg" alt="Using the Slope One algorithm"/>. Hence, on average, <strong>Item A</strong> is rated better than <strong>Item B</strong> by <img src="img/4351OS_08_37.jpg" alt="Using the Slope One algorithm"/>. Similarly, the average rating difference between <strong>Item A</strong> and <strong>Item C</strong> is <img src="img/4351OS_08_38.jpg" alt="Using the Slope One algorithm"/>. We can predict the rating of <strong>User 3</strong> for <strong>Item A</strong> using his/her rating for <strong>Item B</strong> and the average difference of ratings for <strong>Item A</strong> and <strong>Item B</strong>. This value comes out to <img src="img/4351OS_08_39.jpg" alt="Using the Slope One algorithm"/>.</p><p>We will now describe a <a id="id778" class="indexterm"/>concise implementation the Slope One algorithm in <a id="id779" class="indexterm"/>Clojure. First off, we need to define our sample data. This can be done using a nested map, as shown in the following code:</p><div><pre class="programlisting">(def ? nil)
(def data
  {"User 1" {"Item A" 5 "Item B" 3 "Item C" 2 "Item D" ?}
   "User 2" {"Item A" 3 "Item B" 4 "Item C" ? "Item D" 4}
   "User 3" {"Item A" ? "Item B" 2 "Item C" 5 "Item D" 3}
   "User 4" {"Item A" 4 "Item B" ? "Item C" 3 "Item D" ?}})</pre></div><p>In the preceding code shown, we bind the value <code class="literal">nil</code> to the <code class="literal">?</code> symbol, and use it to define a nested map <code class="literal">data</code>, in which each key represents a user and its value represents a map of the user's ratings with the item names as the keys. We will define some of the following utility methods to help us manipulate the nested map represented by <code class="literal">data</code>:</p><div><pre class="programlisting">(defn flatten-to-vec [coll]
  (reduce #(apply conj %1 %2)
          []
          coll))</pre></div><p>The <code class="literal">flatten-to-vec</code> function<a id="id780" class="indexterm"/> defined in the preceding code simply converts a map to a flat vector using the <code class="literal">reduce</code> and <code class="literal">conj</code> functions. We can also define <code class="literal">flatten-to-vec</code>, by using a functional composition of the standard <code class="literal">vec</code>, <code class="literal">flatten</code>, and <code class="literal">seq</code> functions, as <code class="literal">(def flatten-to-vec (comp vec flatten seq))</code>. Since we are dealing with maps, we can define some of the following functions to map any function to the values of these maps:</p><div><pre class="programlisting">(defn map-vals [f m]
  (persistent!
    (reduce (fn [m [k v]]
              (assoc! m k (f k v)))
            (transient m) m)))

(defn map-nested-vals [f m]
  (map-vals
   (fn [k1 inner-map]
     (map-vals
      (fn [k2 val] (f [k1 k2] val)) inner-map)) m))</pre></div><p>The <code class="literal">map-vals</code> function<a id="id781" class="indexterm"/> defined in the preceding code can be used to mutate the values of a given map. This function uses the <code class="literal">assoc!</code> function to replace the value stored by a given key in the map, and it uses the <code class="literal">reduce</code> function to compose and apply the <code class="literal">assoc!</code> function to all the key-value pairs in the map. In Clojure, most collections, including maps, are persistent and<a id="id782" class="indexterm"/> immutable. Note the use of the <code class="literal">transient</code> function to convert a persistent and immutable map into a mutable one and the use of the <code class="literal">persistent!</code> function that converts a transient mutable collection to a persistent one. By isolating <a id="id783" class="indexterm"/>mutation, the performance of this function is improved while retaining the guarantee of immutability for the code that uses this function. The <code class="literal">map-nested-vals</code> function<a id="id784" class="indexterm"/> defined in the preceding code simply applies the <code class="literal">map-vals</code> function to the second level values in a nested map.</p><p>We can examine the behavior of the <code class="literal">map-vals</code> and <code class="literal">map-nested-vals</code> functions in the REPL as follows:</p><div><pre class="programlisting">user&gt; (map-vals #(inc %2) {:foo 1 :bar 2})
{:foo 2, :bar 3}

user&gt; (map-nested-vals (fn [keys v] (inc v)) {:foo {:bar 2}})
{:foo {:bar 3}}</pre></div><p>As shown the preceding REPL output, the <code class="literal">inc</code> function is applied to the values of maps <code class="literal">{:foo 1 :bar 2}</code> and <code class="literal">{:foo {:bar 3}}</code>. We now define a function to produce a trained model from the sample data by using the Slope One algorithm, as follows:</p><div><pre class="programlisting">(defn train [data]
  (let [diff-map      (for [[user preferences] data]
                        (for [[i u-i] preferences
                              [j u-j] preferences
                              :when (and (not= i j)
                                         u-i u-j)]
                          [[i j] (- u-i u-j)]))
        diff-vec      (flatten-to-vec diff-map)
        update-fn     (fn [[freqs-so-far diffs-so-far]
                           [item-pair diff]]
                        [(update-in freqs-so-far
                                    item-pair (fnil inc 0))
                         (update-in diffs-so-far
                                    item-pair (fnil + 0) diff)])
        [freqs
         total-diffs] (reduce update-fn
                              [{} {}] diff-vec)
        differences   (map-nested-vals
                       (fn [item-pair diff]
                         (/ diff (get-in freqs item-pair)))
                       total-diffs)]
    {:freqs freqs
     :differences differences}))</pre></div><p>The train function defined in the preceding code first finds the differences between the ratings of all the items in the model using the <code class="literal">for</code> macro, and then it adds the frequencies of ratings of the items and the differences in their ratings using the <code class="literal">update-fn</code> closure.</p><div><div><h3 class="title"><a id="note48"/>Note</h3><p>The main difference between a function and a macro is that a macro doesn't evaluate its parameters while being executed. Also, macros are resolved and expanded at compile time and functions are called at runtime.</p></div></div><p>The <code class="literal">update-fn</code> function<a id="id785" class="indexterm"/> uses the <code class="literal">update-in</code> function<a id="id786" class="indexterm"/> to replace the value of a key in a map. Note the use of the <code class="literal">fnil</code> function, which essentially returns a function that checks for the value <code class="literal">nil</code> and replaces it with <a id="id787" class="indexterm"/>the second argument. This is used to treat the values represented by the <code class="literal">?</code> symbol that has the value <code class="literal">nil</code> in the nested map data.<a id="id788" class="indexterm"/> Lastly, the <code class="literal">train</code> function applies the <code class="literal">map-nested-vals</code> and <code class="literal">get-in</code> functions to the map of rating differences returned in the previous step. Finally, it returns a map with the keys <code class="literal">:freqs</code> and <code class="literal">:differences</code>, which contain maps that represent the frequencies of items and differences in ratings with respect to other items in the model respectively. We can now use this trained model to predict the ratings of the given items by various users. To do this, we will implement a function in the following code that uses the value returned by the <code class="literal">train</code> function defined in the preceding code:</p><div><pre class="programlisting">(defn predict [{:keys [differences freqs]
                :as model}
               preferences
               item]
  (let [get-rating-fn (fn [[num-acc denom-acc]
                           [i rating]]
                        (let [freqs-ji (get-in freqs [item i])]
                          [(+ num-acc
                              (* (+ (get-in differences [item i])
                                    rating)
                                 freqs-ji))
                           (+ denom-acc freqs-ji)]))]
    (-&gt;&gt; preferences
         (filter #(not= (first %) item))
         (reduce get-rating-fn [0 0])
         (apply /))))</pre></div><p>The <code class="literal">predict</code> function<a id="id789" class="indexterm"/> defined in the preceding code uses the <code class="literal">get-in</code> function to retrieve the sum of frequencies and differences of each item in the maps returned by the <code class="literal">train</code> function. This function then<a id="id790" class="indexterm"/> averages these rating differences by using a composition of the <code class="literal">reduce</code> and <code class="literal">/</code> (division) functions. The behavior of the <code class="literal">predict</code> function can be examined in the REPL, as shown in the following code:</p><div><pre class="programlisting">user&gt; (def trained-model (train data))
#'user/trained-model
user&gt; (predict trained-model {"Item A" 2} "Item B")
3/2</pre></div><p>As shown in the preceding<a id="id791" class="indexterm"/> REPL output, the <code class="literal">predict</code> function used the value returned by the <code class="literal">train</code> function to predict the rating of <code class="literal">Item B</code> by a user who has given <code class="literal">Item A</code> a rating of <code class="literal">2</code>. The <code class="literal">predict</code> function estimates the rating of <code class="literal">Item B</code> as <code class="literal">3/2</code>. We can now implement a function in the following code that wraps around the <code class="literal">predict </code>function to find the ratings of all items in our model:</p><div><pre class="programlisting">(defn mapmap
  ([vf s]
     (mapmap identity vf s))
  ([kf vf s]
     (zipmap (map kf s)
             (map vf s))))

(defn known-items [model]
  (-&gt; model :differences keys))

(defn predictions
  ([model preferences]
     (predictions
      model
      preferences
      (filter #(not (contains? preferences %))
              (known-items model))))
  ([model preferences items]
     (mapmap (partial predict model preferences)
             items)))</pre></div><p>The <code class="literal">mapmap</code> function<a id="id792" class="indexterm"/> defined in the preceding code simply applied two functions to a given sequence and returns a map with keys that are created using the first function <code class="literal">kf</code> and with a value generated by the second function <code class="literal">vf</code>. If only a single function is passed to the <code class="literal">mapmap</code> function, it uses the <code class="literal">identity</code> function to generate keys in the map returned by it. The <code class="literal">known-items</code> function defined in the preceding code will determine all items in a model using the keys function on the map represented by the :<code class="literal">differences</code> key in the value returned by the <code class="literal">train</code> function. Finally, the <code class="literal">predictions</code> function uses the value returned by the <code class="literal">train</code> and <code class="literal">known-items</code> functions to determine all items in the model and then predict all unrated items for a particular user. The function also takes an optional third argument, which<a id="id793" class="indexterm"/> is a vector of the item names whose ratings are to be predicted, in order to return the predictions of all items with names present in the vector <code class="literal">items</code>.</p><p>Now, we can examine the behavior of the preceding function in the REPL, as follows:</p><div><pre class="programlisting">user&gt; (known-items trained-model)
("Item D" "Item C" "Item B" "Item A")</pre></div><p>As shown in the preceding<a id="id794" class="indexterm"/> output, the <code class="literal">known-items</code> function<a id="id795" class="indexterm"/> returns the names of all items in the model. We can now try out the predictions function, as follows:</p><div><pre class="programlisting">user&gt; (predictions trained-model {"Item A" 2} ["Item C" "Item D"])
{"Item D" 3, "Item C" 0}
user&gt; (predictions trained-model {"Item A" 2})
{"Item B" 3/2, "Item C" 0, "Item D" 3}</pre></div><p>Note that when we skip the last optional parameter of the <code class="literal">predictions</code> function, the map returned by this function will contain all items that are not previously rated by a particular user. This can be asserted in the REPL by using the <code class="literal">keys</code> function, as follows:</p><div><pre class="programlisting">user&gt; (keys  (predictions trained-model {"Item A" 2}))
("Item B" "Item C" "Item D")</pre></div><p>To conclude, we have demonstrated how we can implement the Slope One algorithm using nested maps and standard Clojure functions.</p></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec57"/>Summary</h1></div></div></div><p>In this chapter, we discussed anomaly detection and recommendation. We also implemented a simple anomaly detector and recommendation engine. The topics covered in this chapter can be summarized as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We explored anomaly detection and how we can implement an anomaly detector using the PDF in Clojure.</li><li class="listitem" style="list-style-type: disc">We studied recommendation systems that use both content-based and collaborative filtering techniques. We also studied the various optimization problems in these techniques.</li><li class="listitem" style="list-style-type: disc">We also studied the Slope One algorithm, which is a form of collaborative filtering, and also described a concise implementation of this algorithm.</li></ul></div><p>In the following chapter, we will discuss more applications of machine learning techniques that can be applied to large and complex data-centric applications.</p></div></body></html>