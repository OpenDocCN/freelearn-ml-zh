- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SageMaker Deployment Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After training our **machine learning** (**ML**) model, we can proceed with
    deploying it to a web API. This API can then be invoked by other applications
    (for example, a mobile application) to perform a “prediction” or inference. For
    example, the ML model we trained in [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*, can be deployed to a web API and then
    be used to predict the likelihood of customers canceling their reservations or
    not, given a set of inputs. Deploying the ML model to a web API allows the ML
    model to be accessible to different applications and systems.
  prefs: []
  type: TYPE_NORMAL
- en: A few years ago, ML practitioners had to spend time building a custom backend
    API to host and deploy a model from scratch. If you were given this requirement,
    you might have used a Python framework such as **Flask**, **Pyramid**, or **Django**
    to deploy the ML model. Building a custom API to serve as an inference endpoint
    can take about a week or so since most of the application logic needs to be coded
    from scratch. If we were to set up **A/B testing**, **auto-scaling**, or **model
    monitoring** for the API, then we may have to spend a few additional weeks on
    top of the initial time spent to set up the base API. ML engineers and software
    developers generally underestimate the amount of work required to build and maintain
    ML inference endpoints. Requirements evolve over time and the custom application
    code becomes harder to manage as the requirements and solutions pile up. At this
    point, you might ask, “Is there a better and faster way to do this?”. The good
    news is that we could do all of it in “less than a day” if we were to use **SageMaker**
    to deploy our model! Instead of building everything from scratch, SageMaker has
    already automated most of the work and all we need to do is specify the right
    configuration parameters. If needed, SageMaker allows us to customize certain
    components and we can easily replace some of the default automated solutions with
    our own custom implementations.
  prefs: []
  type: TYPE_NORMAL
- en: One of the misconceptions when using SageMaker is that ML models need to be
    trained in SageMaker first before they can be deployed in the **SageMaker hosting
    services**. It is important to note that “this is not true” since the service
    was designed and built to support different scenarios, which include deploying
    a pre-trained model straight away. This means that if we have a pre-trained model
    trained outside of SageMaker, then we *can* proceed with deploying it without
    having to go through the training steps again. In this chapter, you’ll discover
    how easy it is to use the **SageMaker Python SDK** when performing model deployments.
    In just a few lines of code, we will show you how to deploy our pre-trained model
    into a variety of inference endpoint types – **real-time**, **serverless**, and
    **asynchronous inference endpoints**. We will also discuss when it’s best to use
    each of these inference endpoint types later in this chapter. At the same time,
    we will discuss the different strategies and best practices when performing model
    deployments in SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with model deployments in SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the pre-trained model artifacts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the SageMaker script mode prerequisites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a pre-trained model to a real-time inference endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a pre-trained model to a serverless inference endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a pre-trained model to an asynchronous inference endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment strategies and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will wrap up with a quick discussion of the other alternatives and options
    when deploying ML models. After you have completed the hands-on solutions in this
    chapter, you will be more confident in deploying different types of ML models
    in SageMaker. Once you reach a certain level of familiarity and mastery using
    the SageMaker Python SDK, you should be able to set up an ML inference endpoint
    in just a few hours, or maybe even in just a few minutes!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start, it is important to have the following ready:'
  prefs: []
  type: TYPE_NORMAL
- en: A web browser (preferably Chrome or Firefox)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the AWS account and **SageMaker Studio** domain used in the first
    chapter of the book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Jupyter notebooks, source code, and other files used for each chapter are
    available in this repository: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS).'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to use an IAM user with limited permissions instead of the
    root account when running the examples in this book. We will discuss this along
    with other security best practices in detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*. If you are just starting with
    using AWS, you may proceed with using the root account in the meantime.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with model deployments in SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 6*](B18638_06.xhtml#_idTextAnchor132), *SageMaker Training and
    Debugging Solutions*, we trained and deployed an image classification model using
    the **SageMaker Python SDK**. We made use of a built-in algorithm while working
    on the hands-on solutions in that chapter. When using a built-in algorithm, we
    just need to prepare the training dataset along with specifying a few configuration
    parameters and we are good to go! Note that if we want to train a custom model
    using our favorite ML framework (such as TensorFlow and PyTorch), then we can
    prepare our custom scripts and make them work in SageMaker using **script mode**.
    This gives us a bit more flexibility since we can tweak how SageMaker interfaces
    with our model through a custom script that allows us to use different libraries
    and frameworks when training our model. If we want the highest level of flexibility
    for the environment where the training scripts will run, then we can opt to use
    our own custom container image instead. SageMaker has its own set of pre-built
    container images used when training models. However, we may decide to build and
    use our own if needed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – The different options when training and deployment models ](img/B18638_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – The different options when training and deployment models
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 7.1*, the options available when training ML models
    in SageMaker are also available when deploying models using the SageMaker hosting
    services. Here, we tag each approach or option with an arbitrary label (for example,
    **T1** or **T2**) to help us discuss these options in more detail. When performing
    model deployments in SageMaker, we can choose to deploy a model using the container
    of a built-in algorithm (**D1**). We also have the option to deploy our deep learning
    models using **script mode** (**D2**). With this option, we need to prepare custom
    scripts that will run inside the pre-built **Deep Learning Containers**. We also
    have the option to provide and use our own custom container images for the environment
    where our ML model will be deployed (**D3**).
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Choosing which combination of options to use generally depends on the level
    of customization required (in the form of custom scripts and container images)
    when performing ML experiments and deployments. When getting started with SageMaker,
    it is recommended to use the SageMaker built-in algorithms to have a better feel
    for how things work when training a model (**T1**) and when deploying a model
    (**D1**). If we need to use frameworks such as TensorFlow, PyTorch, or MXNet on
    top of the managed infrastructure of AWS with SageMaker, we will need to prepare
    a set of custom scripts to be used during training (**T2**) and deployment (**D2**).
    Finally, when we need a much greater level of flexibility, we can prepare custom
    container images and use these when training a model (**T3**) and deploying a
    model (**D3**).
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that we can combine and use different options when training
    and deploying models. For example, we can train an ML model using script mode
    (**T2**) and use a custom container image during model deployment (**D3**). Another
    example involves training a model outside of SageMaker (**T4**) and using the
    pre-built inference container image for a built-in algorithm during model deployment
    (**D1**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s talk about how model deployment works using the SageMaker hosting
    services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Deploying a model using the SageMaker hosting services ](img/B18638_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Deploying a model using the SageMaker hosting services
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.2*, we have a high-level diagram of how model deployment works
    using the SageMaker hosting services. Assuming that a `model.tar.gz` file (containing
    the ML model artifacts and output files) has been uploaded to an S3 bucket after
    the training step, the `model.tar.gz` file is downloaded from the S3 bucket into
    an ML compute instance that serves as the dedicated server for the ML inference
    endpoint. Inside this ML compute instance, the model artifacts stored inside the
    `model.tar.gz` file are loaded inside a running container containing the inference
    code, which can load the model and use it for inference for incoming requests.
    As mentioned earlier, the inference code and the container image used for inference
    can either be provided by AWS (built-in or pre-built) or provided by ML engineers
    using SageMaker (custom).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s show a few sample blocks of code to help us explain these concepts. Our
    first example involves training and deploying a model using the built-in **Principal
    Component Analysis** (**PCA**) algorithm – an algorithm that can be used in use
    cases such as dimensionality reduction and data compression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, SageMaker makes use of a pre-built container image when training and deploying
    the PCA model. This container image has been prepared by the AWS team so that
    we won’t have to worry about implementing this ourselves when using the built-in
    algorithms. Note that we can also skip the training step and proceed with the
    deployment step in SageMaker as long as we have a pre-trained model available
    that is compatible with the pre-built container available for the built-in algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s quickly take a look at an example of how to use custom scripts when
    deploying models in SageMaker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this example, SageMaker makes use of a pre-built Deep Learning Container
    image to deploy PyTorch models. As discussed in [*Chapter 3*](B18638_03.xhtml#_idTextAnchor060),
    *Deep Learning Containers*, the relevant packages and dependencies are already
    installed inside these container images. During the deployment step, the container
    runs the custom code specified in a custom `inference.py` script provided during
    the initialization of the `PyTorchModel` object. The custom code would then load
    the model and use it when processing the requests sent to the SageMaker inference
    endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In the example provided, we initialized a `PyTorchModel` object and used the
    `deploy()` method to deploy the model to a real-time inference endpoint. Inside
    the inference endpoint, a container using the PyTorch inference container image
    will run the inference code that loads the model and uses it for inference. Note
    that we also have the corresponding `Model` classes for the other libraries and
    frameworks such as `TensorFlowModel`, `SKLearnModel`, and `MXNetModel`. Once the
    `deploy()` method is called, the appropriate inference container (with the relevant
    installed packages and dependencies) would be used inside the inference endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to specify and use our own custom container image, we can use the
    following block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this example, SageMaker makes use of the custom container image stored in
    the location specified in the `image_uri` variable. Here, the assumption is that
    we have already prepared and tested the custom container image, and we have pushed
    this container image to an **Amazon Elastic Container Registry** repository before
    we have performed the model deployment step.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It takes a bit of trial and error when preparing custom scripts and custom container
    images (similar to how we prepared and tested our custom container image in [*Chapter
    3*](B18638_03.xhtml#_idTextAnchor060), *Deep Learning Containers*). If you are
    using a Notebook instance, you can use the SageMaker **local mode**, which gives
    us a way to test the custom scripts and custom container images in the local environment
    before running these in the managed ML instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code samples shown in this section assumed that we would be deploying our
    ML model in a real-time inference endpoint. However, there are different options
    to choose from when deploying ML models in SageMaker:'
  prefs: []
  type: TYPE_NORMAL
- en: The first option involves deploying and hosting our model in a **real-time inference
    endpoint**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second option involves tweaking the configuration a bit when using the SageMaker
    Python SDK to deploy our model in a **serverless inference endpoint**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third option is to host our model in an **asynchronous inference endpoint**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will cover these options in the hands-on portion of this chapter and we will
    discuss the relevant use cases and scenarios for each of these as well.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that it is also possible to perform inference with
    a model without having to set up an inference endpoint. This involves using **Batch
    Transform** where a model is loaded and used to process multiple input payload
    values and perform predictions all in one go. To see a working example of Batch
    Transform, feel free to check out the following link: [https://bit.ly/3A9wrVy](https://bit.ly/3A9wrVy).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a better idea of how SageMaker model deployment works, let’s
    proceed with the hands-on portion of this chapter. In the next section, we will
    prepare the `model.tar.gz` file containing the ML model artifacts that we will
    use for the model deployment solutions in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the pre-trained model artifacts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 6*](B18638_06.xhtml#_idTextAnchor132), *SageMaker Training and
    Debugging Solutions*, we created a new folder named `CH06`, along with a new Notebook
    using the `Data Science` image inside the created folder. In this section, we
    will create a new folder (named `CH07`), along with a new Notebook inside the
    created folder. Instead of the `Data Science` image, we will use the `PyTorch
    1.10 Python 3.8 CPU Optimized` image as the image used in the Notebook since we
    will download the model artifacts of a pre-trained `transformers` library. Once
    the Notebook is ready, we will use the Hugging Face `transformers` library to
    download a pre-trained model that can be used for sentiment analysis. Finally,
    we will zip the model artifacts into a `model.tar.gz` file and upload it to an
    S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you have completed the hands-on solutions in the *Getting started
    with SageMaker and SageMaker Studio* section of [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*, before proceeding. It is important to
    note that the hands-on section in this chapter is not a continuation of what we
    completed in [*Chapter 6*](B18638_06.xhtml#_idTextAnchor132), *SageMaker Training
    and Debugging Solutions*. As long as we have SageMaker Studio set up, we should
    be good to go.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will prepare the `model.tar.gz` file containing
    the model artifacts and then upload it to an S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to `sagemaker studio` into the search bar of the AWS Management Console
    and then selecting **SageMaker Studio** from the list of results under **Features**.
    We click **Studio** under **SageMaker Domain** in the sidebar and then we select
    **Studio** from the list of drop-down options under the **Launch app** drop-down
    menu (on the **Users** pane). Wait for a minute or two for the SageMaker Studio
    interface to load.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes that we are using the `us-west-2`) region when using services
    to manage and create different types of resources. You may use a different region
    but make sure to perform any adjustments needed in case certain resources need
    to be transferred to the region of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Right-click on the empty space in the `CH07`. Finally, navigate to the `CH07`
    directory by double-clicking the folder name in the sidebar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new Notebook by clicking the `PyTorch 1.10 Python 3.8 CPU Optimized`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Python 3`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`No script`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Select** button afterward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Wait for the kernel to start. This step may take around 3 to 5 minutes while
    an ML instance is being provisioned to run the Jupyter notebook cells.
  prefs: []
  type: TYPE_NORMAL
- en: Rename the notebook from `Untitled.ipynb` to `01 - Prepare model.tar.gz file.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that our notebook is ready, we can proceed with generating the pre-trained
    model artifacts and storing these inside a `model.tar.gz` file. In the first cell
    of the Jupyter Notebook, let’s run the following, which will install the Hugging
    Face `transformers` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install `ipywidgets` using `pip` as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s run the following block of code to restart the kernel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield an output value similar to `{''status'': ''ok'', ''restart'':
    True}` and restart the kernel accordingly to ensure that we will not encounter
    issues using the packages we just installed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s download a pre-trained model using the `transformers` library. We’ll
    download a model that can be used for sentiment analysis and to classify whether
    a statement is *POSITIVE* or *NEGATIVE*. Run the following block of code to download
    the artifacts of the pre-trained `distilbert` model into the current directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should generate two files in the same directory as the `.ipynb` notebook
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`config.json`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch_model.bin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '*How should this work?* If we have an “`I love reading the book MLE on AWS!`”
    statement, for example, the trained model should classify it as a *POSITIVE* statement.
    If we have a “`This is the worst spaghetti I''ve had`” statement, the trained
    model should then classify it as a *NEGATIVE* statement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the `model.tar.gz` (compressed archive) file containing the model artifact
    files generated in the previous step using the following block of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `rm` command to clean up the model files by deleting the model artifacts
    generated in the previous steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the S3 bucket name and prefix. Make sure to replace the value of `<INSERT
    S3 BUCKET NAME HERE>` with a unique S3 bucket name before running the following
    block of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make sure to specify a bucket name for an S3 bucket that does not exist yet.
    In the case that you want to reuse one of the buckets created in the previous
    chapters, you may do so, but make sure to use an S3 bucket in the same region
    where SageMaker Studio is set up and configured.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new S3 bucket using the `aws s3 mb` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can skip this step if you are planning to reuse one of the existing S3 buckets
    created in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the S3 path where we will upload the model files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that at this point, a `model.tar.gz` file does not exist in the specified
    S3 path yet. Here, we are simply preparing the S3 location (string) where the
    `model.tar.gz` file will be uploaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s use the `aws s3 cp` command to copy and upload the `model.tar.gz`
    file to the S3 bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `%store` magic to store the variable values for `model_data`, `s3_bucket`,
    and `prefix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should allow us to use these variable values for one or more of the succeeding
    sections in this chapter, similar to what we have here in *Figure 7.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – The %store magic ](img/B18638_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – The %store magic
  prefs: []
  type: TYPE_NORMAL
- en: Make sure not to restart the kernel or else we will lose the variable values
    saved using the `%store` magic.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the SageMaker script mode prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be preparing a custom script to use a pre-trained model
    for predictions. Before we can proceed with using the **SageMaker Python SDK**
    to deploy our pre-trained model to an inference endpoint, we’ll need to ensure
    that all the script mode prerequisites are ready.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – The desired file and folder structure ](img/B18638_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – The desired file and folder structure
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.4*, we can see that there are three prerequisites we’ll need to
    prepare:'
  prefs: []
  type: TYPE_NORMAL
- en: '`inference.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requirements.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setup.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will store these prerequisites inside the `scripts` directory. We’ll discuss
    these prerequisites in detail in the succeeding pages of this chapter. Without
    further ado, let’s start by preparing the `inference.py` script file!
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the inference.py file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will prepare a custom Python script that will be used by
    SageMaker when processing inference requests. Here, we can influence how the input
    request is deserialized, how a custom model is loaded, how the prediction step
    is performed, and how the output prediction is serialized and returned as a response.
    To do all of this, we will need to override the following inference handler functions
    inside our script file: `model_fn()`, `input_fn()`, `predict_fn()`, and `output_fn()`.
    We will discuss how these functions work shortly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will prepare our custom Python script and override
    the default implementations of the inference handler functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Right-click on the empty space in the **File Browser** sidebar pane to open
    a context menu similar to that shown in *Figure 7.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Creating a new folder inside the CH07 directory ](img/B18638_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Creating a new folder inside the CH07 directory
  prefs: []
  type: TYPE_NORMAL
- en: Select **New Folder** from the list of options available in the context menu,
    as highlighted in *Figure 7.5*. Note that we can also press the envelope button
    (with a plus) just beside the **+** button to create a new folder as well.
  prefs: []
  type: TYPE_NORMAL
- en: Name the new folder `scripts`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, double-click the `scripts` folder to navigate to the directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new text file by clicking the **File** menu and choosing **Text File**
    from the list of options under the **New** submenu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Right-click on the `inference.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the `inference.py` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Getting ready to add code to the inference.py file in the Editor
    pane ](img/B18638_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Getting ready to add code to the inference.py file in the Editor
    pane
  prefs: []
  type: TYPE_NORMAL
- en: We will add the succeeding blocks of code into the `inference.py` file. Make
    sure that there’s an extra blank line after each block of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `inference.py` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we specify the appropriate tokenizer for the model we will use to perform
    the prediction in a later step.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'What’s a tokenizer? A `"I am hungry"`, then a tokenizer will split it into
    the tokens `"I"`, `"am"`, and `"``hungry"`. Note that this is a simplified example
    and there’s more to tokenization than what can be explained in a few sentences.
    For more details, feel free to check out the following link: [https://huggingface.co/docs/transformers/main_classes/tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `model_fn()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we defined a model function that returns a model object used to perform
    predictions and process inference requests. Since we are planning to load and
    use a pre-trained model, we used the `from_pretrained()` method of `AutoModelForSequenceClassification`
    from the `transformers` library to load the model artifacts in the specified model
    directory. The `from_pretrained()` method then returns a model object that can
    be used during the prediction step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s define the `humanize_prediction()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `humanize_prediction()` function simply accepts the raw output produced
    by the model after processing the input payload during the prediction step. It
    returns either a `"POSITIVE"` or a `"NEGATIVE"` prediction to the calling function.
    We will define this *calling function* in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s define `predict_fn()` using the following block of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `predict_fn()` function accepts the deserialized input request data and
    the loaded model as input. It then uses these two parameter values to produce
    a prediction. How? Since the loaded model is available as the second parameter,
    we simply use it to perform predictions. The input payload to this prediction
    step would be the deserialized request data, which is available as the first parameter
    to the `predict_fn()` function. Before the output is returned, we make use of
    the `humanize_prediction()` function to convert the raw output to either `"POSITIVE"`
    or `"NEGATIVE"`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Why do we have a commented line containing `sleep(30)`? Later in the *Deploying
    a pre-trained model to an asynchronous inference endpoint section*, we will emulate
    an inference endpoint with a relatively long processing time using an artificial
    30-second delay. For now, we’ll keep this line commented out and we will undo
    this later in that section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also define the `input_fn()` function that is used to convert the serialized
    input request data into its deserialized form. This deserialized form will be
    used for prediction at a later stage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the `input_fn()` function, we also ensure that the specified content type
    is within the list of support content types we define by raising `Exception` for
    unsupported content types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s define `output_fn()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The purpose of `output_fn()` is to serialize the prediction result into the
    specified content type. Here, we also ensure that the specified content type is
    within the list of support content types we define by raising `Exception` for
    unsupported content types.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can think of *serialization* and *deserialization* as data transformation
    steps that convert data from one form to another. For example, the input request
    data may be passed as a valid JSON *string* to the inference endpoint. This input
    request data passes through the `input_fn()` function, which converts it into
    a *JSON* or a *dictionary*. This deserialized value is then passed as a payload
    to the `predict_fn()` function. After this, the `predict_fn()` function returns
    a prediction as the result. This result is then converted to the specified content
    type using the `output_fn()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Save the changes by pressing *CTRL* + *S*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a Mac, use *CMD* + *S* instead. Alternatively, you can just
    click **Save Python File** under the list of options under the **File** menu.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you might be wondering how all of these fit together. To help
    us understand how the inference handler functions interact with the data and with
    each other, let’s quickly check out the diagram shown in *Figure 7.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – The inference handler functions ](img/B18638_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – The inference handler functions
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.7*, we can see that the `model_fn()` function is used to load the
    ML model object. This model object will be used by the `predict_fn()` function
    to perform predictions once a request comes in. When a request comes in, the `input_fn()`
    function processes the serialized request data and converts it into its deserialized
    form. This deserialized request data is then passed to the `predict_fn()` function,
    which then uses the loaded ML model to perform a prediction using the request
    data as a payload. The `predict_fn()` function then returns the output prediction,
    which is serialized by the `output_fn()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about this topic, feel free to check out the following
    link: [https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.xhtml](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.xhtml#serve-a-pytorch-model).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our inference script ready, let’s proceed with preparing the
    `requirements.txt` file in the next section!
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the requirements.txt file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the `transformers` package is not included in the SageMaker PyTorch Docker
    containers, we will need to include it through a `requirements.txt` file, which
    is used by SageMaker to install additional packages at runtime. If this is your
    first time dealing with a `requirements.txt` file, it is simply a text file that
    contains a list of packages to be installed using the `pip install` command. If
    your `requirements.txt` file contains a single line (for example, `transformers==4.4.2`),
    then this will map to a `pip install transformers==4.4.2` during the installation
    step. If the `requirements.txt` file contains multiple lines, then each of the
    packages listed will be installed using the `pip install` command.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can optionally *pin* the listed packages and dependencies to a specific version
    using `==` (equal). Alternatively, we can also use `<` (less than), `>` (greater
    than), and other variations to manage the upper-bound and lower-bound values of
    the version numbers of the packages to be installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create and prepare the `requirements.txt`
    file inside the `scripts` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new text file by clicking the **File** menu and choosing **Text File**
    from the list of options under the **New** submenu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Creating a new text file inside the scripts directory ](img/B18638_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Creating a new text file inside the scripts directory
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.8*, we can see that we’re in the `scripts` directory (`scripts`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Rename the file `requirements.txt`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `requirements.txt` file to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make sure to save the changes by pressing *CTRL* + *S*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a Mac, use *CMD* + *S* instead. Alternatively, you can just
    click **Save Python File** under the list of options under the **File** menu.
  prefs: []
  type: TYPE_NORMAL
- en: Wasn’t that easy? Let’s now proceed with the last prerequisite – the `setup.py`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the setup.py file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the `requirements.txt` file, we will prepare a `setup.py` file
    that will contain some additional information and metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'We won’t dive deep into the differences between the usage of `requirements.txt`
    and `setup.py` files. Feel free to check out the following link for more information:
    [https://docs.python.org/3/distutils/setupscript.xhtml](https://docs.python.org/3/distutils/setupscript.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create and prepare the `setup.py` file inside
    the `scripts` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the same set of steps as in the previous section, create a new text file
    and rename it `setup.py`. Make sure that this file is in the same directory (`scripts`)
    as the `inference.py` and `requirements.txt` files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the contents of the `setup.py` file to contain the following block of
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The setup script simply makes use of the `setup()` function to describe the
    module distribution. Here, we specify metadata such as the `name`, `version`,
    and `description` when the `setup()` function is called.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, make sure to save the changes by pressing *CTRL* + *S*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a Mac, use *CMD* + *S* instead. Alternatively, you can just
    click **Save Python File** under the list of options under the **File** menu.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have prepared all the prerequisites needed to run all the
    succeeding sections of this chapter. With this, let’s proceed with deploying our
    pre-trained model to a real-time inference endpoint in the next section!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a pre-trained model to a real-time inference endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use the SageMaker Python SDK to deploy a pre-trained
    model to a real-time inference endpoint. From the name itself, we can tell that
    a real-time inference endpoint can process input payloads and perform predictions
    in real time. If you have built an API endpoint before (which can process GET
    and POST requests, for example), then we can think of an inference endpoint as
    an API endpoint that accepts an input request and returns a prediction as part
    of a response. How are predictions made? The inference endpoint simply loads the
    model into memory and uses it to process the input payload. This will yield an
    output that is returned as a response. For example, if we have a pre-trained sentiment
    analysis ML model deployed in a real-time inference endpoint, then it would return
    a response of either `"POSITIVE"` or `"``NEGATIVE"` depending on the input string
    payload provided in the request.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that our inference endpoint receives the statement `"I love reading
    the book MLE on AWS!"` via a POST request. The inference endpoint would then process
    the request input data and use the ML model for inference. The result of the ML
    model inference step (for example, number values that represent a `"POSITIVE"`
    result) would then be returned as part of the response.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – The desired file and folder structure   ](img/B18638_07_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – The desired file and folder structure
  prefs: []
  type: TYPE_NORMAL
- en: To get this to work, we just need to make sure that the prerequisite files,
    including the inference script file (for example, `inference.py`) and the `requirements.txt`
    file, are ready before using the SageMaker Python SDK to prepare the real-time
    inference endpoint. Make sure to check and review the folder structure in *Figure
    7.9* before proceeding with the hands-on solutions in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will use the SageMaker Python SDK to deploy our
    pre-trained model to a real-time inference endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: In `CH07` directory and create a new Notebook using the `Data Science` image.
    Rename the notebook `02 - Deploying a real-time inference endpoint.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The new notebook should be next to `01 - Prepare model.tar.gz file.ipynb`, similar
    to what is shown in *Figure 7.9*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the following code block in the first cell of the new notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `%store` magic to load the variable values for `model_data`,
    `s3_bucket`, and `prefix`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s prepare the IAM execution role for use by SageMaker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the `PyTorchModel` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s check out *Figure 7.10* to help us visualize what has happened in the
    previous block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Deploying a real-time inference endpoint ](img/B18638_07_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Deploying a real-time inference endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.10*, we can see that we initialized a `Model` object by passing
    several configuration parameters during the initialization step: (1) the model
    data, (2) the framework version, and (3) the path to the `inference.py` script
    file. There are other arguments we can set but we will simplify things a bit and
    focus on these three. In order for SageMaker to know how to use the pre-trained
    model for inference, the `inference.py` script file should contain the custom
    logic, which loads the ML model and uses it to perform predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that we are not limited to naming the inference script
    file `inference.py`. We can use a different naming convention as long as we specify
    the correct `entry_point` value.
  prefs: []
  type: TYPE_NORMAL
- en: This is the case if we are using SageMaker’s script mode when deploying ML models.
    Note that there are other options available, such as using a custom container
    image where instead of passing a script, we’ll be passing a container image that
    we’ve prepared ahead of time. When deploying ML models trained using the **built-in
    algorithms** of SageMaker, we can proceed with deploying these models right away
    without any custom scripts or container images, since SageMaker already has provided
    all the prerequisites needed for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `deploy()` method to deploy the model to a real-time inference endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This step should take around 3 to 8 minutes to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'When using the `deploy()` method to deploy an ML model using the SageMaker
    Python SDK, we are given the ability to specify the instance type. Choosing the
    right instance type for the model is important and finding the optimal balance
    between cost and performance is not a straightforward process. There are many
    instance types and sizes to choose from and ML engineers may end up having a suboptimal
    setup when deploying models in the SageMaker hosting services. The good news is
    that SageMaker has a capability called **SageMaker Inference Recommender**, which
    can help you decide which instance type to use. For more information, you can
    check out the following link: [https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our real-time inference endpoint is running, let’s perform a sample
    prediction using the `predict()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should yield an output value of `'POSITIVE'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also test a negative scenario:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should yield an output value of `'NEGATIVE'`. Feel free to test different
    values before deleting the endpoint in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s delete the inference endpoint using the `delete_endpoint()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will help us avoid any unexpected charges for unused inference endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Wasn’t that easy? Deploying a pre-trained model to a real-time inference endpoint
    (inside an ML instance with the specified instance type) using the SageMaker Python
    SDK is so straightforward! A lot of the engineering work has been automated for
    us and all we need to do is call the `Model` object’s `deploy()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a pre-trained model to a serverless inference endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the initial chapters of this book, we’ve worked with several serverless services
    that allow us to manage and reduce costs. If you are wondering whether there’s
    a serverless option when deploying ML models in SageMaker, then the answer to
    that would be a sweet yes. When you are dealing with intermittent and unpredictable
    traffic, using serverless inference endpoints to host your ML model can be a more
    cost-effective option. Let’s say that we can tolerate **cold starts** (where a
    request takes longer to process after periods of inactivity) and we only expect
    a few requests per day – then, we can make use of a serverless inference endpoint
    instead of the real-time option. Real-time inference endpoints are best used when
    we can maximize the inference endpoint. If you’re expecting your endpoint to be
    utilized most of the time, then the real-time option may do the trick.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – The desired file and folder structure ](img/B18638_07_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – The desired file and folder structure
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying a pre-trained ML model to a serverless inference endpoint using the
    SageMaker Python SDK is similar to how it is done for real-time inference endpoints.
    The only major differences would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The initialization of the `ServerlessInferenceConfig` object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passing this object as an argument when calling the `Model` object’s `deploy()`
    method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will use the SageMaker Python SDK to deploy our
    pre-trained model to a serverless inference endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: In `CH07` directory and create a new Notebook using the `Data Science` image.
    Rename the notebook `03 - Deploying a serverless inference endpoint.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The new notebook should be next to `01 - Prepare model.tar.gz file.ipynb`, similar
    to what is shown in *Figure 7.11*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first cell of the new notebook, let’s run the following block of code
    to load the variable values for `model_data`, `s3_bucket`, and `prefix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you get an error when running this block of code, make sure that you have
    completed the steps specified in the *Preparing the pre-trained model artifacts*
    section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the IAM execution role to be used by SageMaker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize and configure the `ServerlessInferenceConfig` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the `PyTorchModel` object and use the `deploy()` method to deploy
    the model to a serverless inference endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The model deployment should take around 3 to 8 minutes to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our real-time inference endpoint is running, let’s perform a sample
    prediction using the `predict()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should yield an output value of `'POSITIVE'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also test a negative scenario:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should yield an output value of `'NEGATIVE'`. Feel free to test different
    values before deleting the endpoint in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s delete the inference endpoint using the `delete_endpoint()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will help us avoid any unexpected charges for unused inference endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, everything is almost the same, except for the initialization
    and usage of the `ServerlessInferenceConfig` object. When using a serverless endpoint,
    SageMaker manages the compute resources for us and performs the following automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: Auto-assigns compute resources proportional to the `memory_size_in_mb` parameter
    value we specified when initializing `ServerlessInferenceConfig`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses the configured maximum concurrency value to manage how many concurrent
    invocations can happen at the same time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scales down the resources automatically to zero if there are no requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you see more examples of how to use the SageMaker Python SDK, you’ll start
    to realize how well this SDK has been designed and implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a pre-trained model to an asynchronous inference endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to real-time and serverless inference endpoints, SageMaker also
    offers a third option when deploying models – **asynchronous inference endpoints**.
    Why is it called asynchronous? For one thing, instead of expecting the results
    to be available immediately, requests are queued, and results are made available
    *asynchronously*. This works for ML requirements that involve one or more of the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Large input payloads (up to 1 GB)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A long prediction processing duration (up to 15 minutes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A good use case for asynchronous inference endpoints would be for ML models
    that are used to detect objects in large video files (which may take more than
    60 seconds to complete). In this case, an inference may take a few minutes instead
    of a few seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '*How do we use asynchronous inference endpoints?* To invoke an asynchronous
    inference endpoint, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The request payload is uploaded to an Amazon S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The S3 path or location (where the request payload is stored) is used as the
    parameter value when calling the `predict_async()` method of the `AsyncPredictor`
    object (which maps or represents the ML inference endpoint).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upon invocation of the endpoint, the asynchronous inference endpoint queues
    the request for processing (once the endpoint can).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After processing the request, the output inference results are stored and uploaded
    to the output S3 location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An SNS notification (for example, a success or error notification) is sent (if
    set up).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we will deploy our NLP model to an asynchronous inference endpoint.
    To emulate a delay, we’ll call the `sleep()` function in our inference script
    so that the prediction step takes longer than usual. Once we can get this relatively
    simple setup to work, working on more complex requirements such as object detection
    for video files will definitely be easier.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – The file and folder structure ](img/B18638_07_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – The file and folder structure
  prefs: []
  type: TYPE_NORMAL
- en: To get this setup to work, we will need to prepare a file that contains an input
    payload similar to that shown in *Figure 7.12* (for example, *data* or `input.json`).
    Once the input file has been prepared, we will upload it to an Amazon S3 bucket
    and then proceed with deploying our pre-trained ML model to an asynchronous inference
    endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, let’s proceed with creating the input JSON file!
  prefs: []
  type: TYPE_NORMAL
- en: Creating the input JSON file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create a sample file containing the input
    JSON value that will be used when invoking the asynchronous inference endpoint
    in the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Right-click on the empty space in the **File Browser** sidebar pane to open
    a context menu similar to that shown in *Figure 7.13*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Creating a new folder ](img/B18638_07_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Creating a new folder
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you are in the `CH07` directory in **File Browser** before performing
    this step.
  prefs: []
  type: TYPE_NORMAL
- en: Rename the folder `data`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Double-click the `data` folder in the **File Browser** sidebar pane to navigate
    to the directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new text file by clicking the **File** menu and choosing **Text File**
    from the list of options under the **New** submenu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Creating a new text file ](img/B18638_07_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Creating a new text file
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you are in the `data` directory when creating a new text file,
    similar to that in *Figure 7.14*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rename the file `input.json`, as in *Figure 7.15*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Renaming the text file ](img/B18638_07_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Renaming the text file
  prefs: []
  type: TYPE_NORMAL
- en: To rename the `untitled.txt` file, right-click on the file in the `input.json`)
    to replace the default name value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `input.json` file with the following JSON value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make sure to save your changes by pressing *CTRL* + *S*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a Mac, use *CMD* + *S* instead. Alternatively, you can just
    click **Save Python File** under the list of options under the **File** menu.
  prefs: []
  type: TYPE_NORMAL
- en: Again, the input file is only needed when we’re planning to deploy our ML model
    to an asynchronous inference endpoint. With this prepared, we can now proceed
    to the next set of steps.
  prefs: []
  type: TYPE_NORMAL
- en: Adding an artificial delay to the inference script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before proceeding with using the SageMaker Python SDK to deploy our pre-trained
    model to an asynchronous inference endpoint, we will add an artificial delay to
    the prediction step. This will help us emulate inference or prediction requests
    that take a bit of time to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When troubleshooting an asynchronous inference endpoint, you may opt to test
    an ML model that performs predictions within just a few seconds first. This will
    help you know right away if there’s something wrong since the output is expected
    to be uploaded to the S3 output path within a few seconds (instead of a few minutes).
    That said, you may want to remove the artificial delay temporarily if you’re having
    issues getting the setup to work.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will update the `inference.py` script to add a
    30-second delay when performing a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing where we left off in the previous section, let’s navigate to the
    `CH07` directory in **File Browser**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Navigating to the CH07 directory ](img/B18638_07_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Navigating to the CH07 directory
  prefs: []
  type: TYPE_NORMAL
- en: Here, we click the `CH07` link, as highlighted in *Figure 7.16*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Double-click the `scripts` folder, as shown in *Figure 7.17*, to navigate to
    the directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Navigating to the scripts directory ](img/B18638_07_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – Navigating to the scripts directory
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that you have completed the hands-on steps in the *Preparing the
    SageMaker script mode prerequisites* section before proceeding with the next step.
    The `scripts` directory should contain three files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`inference.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requirements.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setup.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Double-click and open the `inference.py` file, as highlighted in *Figure 7.18*.
    Locate the `predict_fn()` function and uncomment the line of code containing `sleep(30)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18638_07_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Updating the inference.py file
  prefs: []
  type: TYPE_NORMAL
- en: To uncomment the line of code, simply remove the hash and the empty space (`#`
    ) before `sleep(30)`, similar to what we can see in *Figure 7.18*.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to save the changes by pressing *CTRL* + *S*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a Mac, use *CMD* + *S* instead. Alternatively, you can just
    click **Save Python File** under the list of options under the **File** menu.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have finished adding an artificial 30-second delay, let’s proceed
    with using the SageMaker Python SDK to deploy our asynchronous inference endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and testing an asynchronous inference endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deploying a pre-trained ML model to an asynchronous inference endpoint using
    the SageMaker Python SDK is similar to how it is done for real-time and serverless
    inference endpoints. The only major differences would be (1) the initialization
    of the `AsyncInferenceConfig` object, and (2) passing this object as an argument
    when calling the `Model` object’s `deploy()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will use the SageMaker Python SDK to deploy our
    pre-trained model to an asynchronous inference endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuing where we left off in the *Adding an artificial delay to the inference
    script section*, let’s navigate to the `CH07` directory in `Data Science` image.
    Rename the notebook `04 - Deploying an asynchronous inference endpoint.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The new notebook should be next to `01 - Prepare model.tar.gz file.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first cell of the new notebook, let’s run the following block of code
    to load the variable values for `model_data`, `s3_bucket`, and `prefix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you get an error when running this block of code, make sure that you have
    completed the steps specified in the *Preparing the pre-trained model artifacts*
    section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the path where we will upload the inference input file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upload the `input.json` file to the S3 bucket using the `aws s3 cp` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the IAM execution role for use by SageMaker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the `AsyncInferenceConfig` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: While initializing the `AsyncInferenceConfig` object, we specify the value for
    the `output_path` parameter where the results will be saved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s initialize the `PyTorchModel` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we specify the configuration values for the parameters, such as `model_data`,
    `role`, `source_dir`, `entry_point`, `framework_version`, and `py_version`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `deploy()` method to deploy the model to an asynchronous inference
    endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we specify the `AsyncInferenceConfig` object we initiated in a previous
    step as the parameter value to `async_inference_config`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Deploying an asynchronous inference endpoint ](img/B18638_07_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Deploying an asynchronous inference endpoint
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.19*, we can see that the `deploy()` method accepts the parameter
    value for SageMaker to configure an asynchronous inference endpoint instead of
    a real-time inference endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The model deployment should take around 3 to 8 minutes to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the inference endpoint is ready, let’s use the `predict_async()` method
    to perform the prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should invoke the asynchronous inference endpoint using the data stored
    in the `input.json` file stored in S3.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – How the predict_async() method works ](img/B18638_07_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – How the predict_async() method works
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.20*, we can see that the input payload for an asynchronous inference
    endpoint comes from an S3 bucket. Then, after the endpoint processes the request,
    the output is saved to S3\. This would probably not make any sense if your input
    payload were small (for example, less than *1 MB*). However, if the input payload
    involves larger files such as video files, then uploading this into S3 and utilizing
    an asynchronous inference endpoint for predictions would make a lot more sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `sleep()` function to wait for 40 seconds before calling the `get_result()`
    function of the `response` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should yield an output value of `'POSITIVE'`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Why wait for 40 seconds? Since we added an artificial 30-second delay in the
    prediction step, we would have to wait for at least 30 seconds before the output
    file is available in the specified S3 location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Store the S3 path string value in the `output_path` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `aws s3 cp` command to download a copy of the output file to the Studio
    notebook instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have downloaded the output file, let’s use the `cat` command to
    check its contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should give us an output value of `'POSITIVE'`, similar to what we obtained
    after using the `get_result()` method in an earlier step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do a quick cleanup by deleting the copy of the output file using the
    `rm` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s delete the inference endpoint using the `delete_endpoint()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will help us avoid any unexpected charges for unused inference endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that in a production setup, it is preferable to update
    the architecture to be more event-driven and utilize the `notification_config`
    parameter value must be updated with the appropriate dictionary of values when
    initializing the `AsyncInferenceConfig` object. For more information, feel free
    to check out the following link: [https://sagemaker.readthedocs.io/en/stable/overview.xhtml#sagemaker-asynchronous-inference](https://sagemaker.readthedocs.io/en/stable/overview.xhtml#sagemaker-asynchronous-inference).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: What’s SNS? SNS is a fully managed messaging service that allows architectures
    to be event-driven. Messages from a source (*publisher*) can fan out and be sent
    across a variety of receivers (*subscribers*). If we were to configure the SageMaker
    asynchronous inference endpoint to push notification messages to SNS, then it
    is best if we also register and set up a subscriber that waits for a success (or
    error) notification message once the prediction step is completed. This subscriber
    then proceeds with performing a pre-defined operation once the results are available.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have completed working on the hands-on solutions of this chapter,
    it is time for us to clean up and turn off any resources we will no longer use.
    In the next set of steps, we will locate and turn off any remaining running instances
    in SageMaker Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **Running Instances and Kernels** icon in the sidebar, as highlighted
    in *Figure 7.21*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Turning off the running instance ](img/B18638_07_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Turning off the running instance
  prefs: []
  type: TYPE_NORMAL
- en: Clicking the **Running Instances and Kernels** icon should open and show the
    running instances, apps, and terminals in SageMaker Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Turn off all running instances under **RUNNING INSTANCES** by clicking the **Shut
    down** button for each of the instances, as highlighted in *Figure 7.21*. Clicking
    the **Shut down** button will open a pop-up window verifying the instance shutdown
    operation. Click the **Shut down all** button to proceed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure to check for and delete all the running inference endpoints under
    **SageMaker resources** as well (if there are any):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.22 – Checking the list of running inference endpoints ](img/B18638_07_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.22 – Checking the list of running inference endpoints
  prefs: []
  type: TYPE_NORMAL
- en: To check whether there are running inference endpoints, click the **SageMaker
    resources** icon as highlighted in *Figure 7.22* and then select **Endpoints**
    from the list of options in the drop-down menu.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that this cleanup operation needs to be performed after
    using SageMaker Studio. These resources are not turned off automatically by SageMaker
    even during periods of inactivity.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are looking into other ways to reduce costs when running ML workloads
    in SageMaker, you can check how you can utilize other features and capabilities
    such as **SageMaker Savings Plans** (which helps reduce costs in exchange for
    a consistent usage commitment for a 1-year or 3-year term), **SageMaker Neo**
    (which helps optimize ML models for deployment, speeding up inference and reducing
    costs), and **SageMaker Inference Recommender** (which helps you select the best
    instance type for the inference endpoint through automated load testing). We won’t
    discuss these in further detail in this book, so feel free to check out the following
    link for more information on these topics: [https://docs.aws.amazon.com/sagemaker/latest/dg/inference-cost-optimization.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-cost-optimization.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment strategies and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will discuss the relevant deployment strategies and best
    practices when using the SageMaker hosting services. Let’s start by talking about
    the different ways we can invoke an existing SageMaker inference endpoint. The
    solution we’ve been using so far involves the usage of the SageMaker Python SDK
    to invoke an existing endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: Here, we initialize a `Predictor` object and point it to an existing inference
    endpoint during the initialization step. We then use the `predict()` method of
    this `Predictor` object to invoke the inference endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can also invoke the same endpoint using the **boto3** library,
    similar to what is shown in the following block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: Here, we use the `invoke_endpoint()` method when performing predictions and
    inference using the existing ML inference endpoint. As you can see, even without
    the SageMaker Python SDK installed, we should be able to invoke an existing ML
    inference endpoint from an `POST` request using the `InvokeEndpoint` API.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If your backend application code makes use of a language other than Python
    (for example, Ruby, Java, or JavaScript), then all you need to do is look for
    an existing SDK for that language along with the corresponding function or method
    to call. For more information, you can check out the following link containing
    the different tools, along with the SDKs available for each language: [https://aws.amazon.com/tools/](https://aws.amazon.com/tools/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several solutions possible if you want to prepare an HTTP API that
    invokes and interfaces with an existing SageMaker inference endpoint. Here’s a
    quick list of possible solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Option 1*: *Amazon API Gateway HTTP API + AWS Lambda function + boto3 + SageMaker
    ML inference endpoint* – The `boto3` library to invoke the SageMaker ML inference
    endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 2*: *AWS Lambda function + boto3 + SageMaker ML inference endpoint
    (Lambda function URLs)* – The AWS Lambda function is invoked directly from a Lambda
    function URL (which is a dedicated endpoint for triggering a Lambda function).
    The AWS Lambda function then uses the `boto3` library to invoke the SageMaker
    ML inference endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 3*: *Amazon API Gateway HTTP API + SageMaker ML inference endpoint
    (API Gateway mapping templates)* – The Amazon API Gateway HTTP API receives the
    HTTP request and invokes the SageMaker ML inference endpoint directly using the
    **API Gateway mapping templates** (without the usage of Lambda functions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 4*: *Custom container-based web application using a web framework (for
    example, Flask or Django) inside an EC2 instance + boto3 + SageMaker ML inference
    endpoint* – The web application (running inside a container in an `boto3` library
    to invoke the SageMaker ML Inference endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 5*: *Custom container-based web application using a web framework (for
    example, Flask or Django) inside an Elastic Container Service (ECS) + boto3 +
    SageMaker ML inference endpoint* – The web application (running inside a container
    using the `boto3` library to invoke the SageMaker ML inference endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 6*: *Custom container-based web application using a web framework (for
    example, Flask or Django) with Elastic Kubernetes Service (EKS) + boto3 + SageMaker
    ML inference endpoint* – The web application (running inside an `boto3` library
    to invoke the SageMaker ML inference endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 7*: *AWS AppSync (GraphQL API) + AWS Lambda function + boto3 + SageMaker
    ML inference endpoint* – The `boto3` library to invoke the SageMaker ML inference
    endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this is not an exhaustive list and there are definitely other ways
    to set up an HTTP API invoking an existing SageMaker inference endpoint. Of course,
    there are scenarios as well where we would want to invoke an existing inference
    endpoint directly from another AWS service resource. This would mean that we no
    longer need to prepare a separate HTTP API that serves as a middleman between
    the two services.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that we can also invoke a SageMaker inference endpoint
    directly from **Amazon Aurora**, **Amazon Athena**, **Amazon Quicksight**, or
    **Amazon Redshift**. In [*Chapter 4*](B18638_04.xhtml#_idTextAnchor079), *Serverless
    Data Management on AWS*, we used Redshift and Athena to query our data. In addition
    to the database queries already available using these services, we can perform
    ML inference directly using a syntax similar to that in the following block of
    code (a sample query for Athena):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define and use a custom function that invokes an existing SageMaker
    inference endpoint for prediction when using Amazon Athena. For more information,
    feel free to check out the following resources and links:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Athena** + **Amazon SageMaker**: [https://docs.aws.amazon.com/athena/latest/ug/querying-mlmodel.xhtml](https://docs.aws.amazon.com/athena/latest/ug/querying-mlmodel.xhtml).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Redshift** + **Amazon SageMaker**: [https://docs.aws.amazon.com/redshift/latest/dg/machine_learning.xhtml](https://docs.aws.amazon.com/redshift/latest/dg/machine_learning.xhtml).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Aurora** + **Amazon SageMaker**: [https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ml.xhtml](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ml.xhtml).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon QuickSight** + **Amazon SageMaker**: [https://docs.aws.amazon.com/quicksight/latest/user/sagemaker-integration.xhtml](https://docs.aws.amazon.com/quicksight/latest/user/sagemaker-integration.xhtml).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we want to deploy a model outside of the SageMaker hosting services, we
    can do that as well. For example, we can train our model using SageMaker and then
    download the `model.tar.gz` file from the S3 bucket containing the model artifact
    files generated during the training process. The model artifact files generated
    can be deployed outside of SageMaker, similar to how we deployed and invoked the
    model in [*Chapter 2*](B18638_02.xhtml#_idTextAnchor041), *Deep Learning AMIs*,
    and [*Chapter 3*](B18638_03.xhtml#_idTextAnchor060), *Deep Learning Containers*.
    At this point, you might ask yourself: why deploy ML models using the SageMaker
    hosting services? Here’s a quick list of things you can easily perform and set
    up if you were to deploy ML models in the SageMaker hosting services:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up automatic scaling (**autoscaling**) of the infrastructure resources
    (ML instances) used to host the ML model. Autoscaling automatically adds new ML
    instances when the traffic or workload increases and reduces the number of provisioned
    ML instances once the traffic or workload decreases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying multiple ML models in a single inference endpoint using the **multi-model
    endpoint** (**MME**) and **multi-container endpoint** (**MCE**) support of SageMaker.
    It is also possible to set up a **serial inference pipeline** behind a single
    endpoint that involves a sequence of containers (for example, pre-processing,
    prediction and post-processing) used to process ML inference requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up **A/B testing** of ML models by distributing traffic to multiple
    variants under a single inference endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up automated model monitoring and monitor (1) data quality, (2) model
    quality, (3) bias drift, and (4) feature attribution drift with just a few lines
    of code using the SageMaker Python SDK. We will dive deeper into model monitoring
    in [*Chapter 8*](B18638_08.xhtml#_idTextAnchor172), *Model Monitoring and Management
    Solutions*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **Elastic Inference** when deploying models to add inference acceleration
    to the SageMaker inference endpoint to improve throughput and decrease latency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a variety of traffic shifting modes when performing blue/green deployments
    when updating the deployed model. We can use the **All-at-once** traffic shifting
    mode if we want to shift all the traffic from the old setup to the new setup all
    in one go. We can use the **Canary** traffic shifting mode if we want to shift
    the traffic from the old setup to the new setup in two steps. This involves only
    shifting a portion of the traffic in the first shift and shifting the remainder
    of the traffic in the second shift. Finally, we can use the **Linear** traffic
    shifting mode to iteratively shift the traffic from the old setup to the new setup
    in a predetermined number of steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up **CloudWatch** alarms along with the SageMaker auto-rollback configuration
    to automate the deployment rollback process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these are relatively easy to set up if we are to use SageMaker for model
    deployment. When using these features and capabilities, all we would need to worry
    about would be the configuration step, as a big portion of the work has already
    been automated by SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve been talking about the different options and solutions when deploying
    ML models in the cloud. Before ending this section, let’s quickly discuss ML model
    deployments on **edge devices** such as mobile devices and smart cameras. There
    are several advantages to this approach, including real-time prediction latency,
    privacy preservation, and cost reduction associated with network connectivity.
    Of course, there are challenges when running and managing ML models on edge devices
    due to the resource limitations involved such as compute and memory. These challenges
    can be solved with **SageMaker Edge Manager**, which is a capability that makes
    use of several other services, capabilities, and features (such as **SageMaker
    Neo**, **IoT Greengrass**, and **SageMaker Model Monitor**) when optimizing, running,
    monitoring, and updating ML models on edge devices. We won’t dive any deeper into
    the details so feel free to check out https://docs.aws.amazon.com/sagemaker/latest/dg/edge.xhtml
    for more information about this topic.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed and focused on several deployment options and
    solutions using SageMaker. We deployed a pre-trained model into three different
    types of inference endpoints – (1) a real-time inference endpoint, (2) a serverless
    inference endpoint, and (3) an asynchronous inference endpoint. We also discussed
    the differences of each approach, along with when each option is best used when
    deploying ML models. Toward the end of this chapter, we talked about some of the
    deployment strategies, along with the best practices when using SageMaker for
    model deployments.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deeper into **SageMaker Model Registry** and
    **SageMaker Model Monitor**, which are capabilities of SageMaker that can help
    us manage and monitor our models in production.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics covered in this chapter, feel free to check
    out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Hugging Face DistilBERT model* ([https://huggingface.co/docs/transformers/model_doc/distilbert](https://huggingface.co/docs/transformers/model_doc/distilbert))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SageMaker – Deploying Models for Inference* (https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SageMaker – Inference Recommender* ([https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SageMaker – Deployment guardrails* ([https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4:Securing, Monitoring, and Managing Machine Learning Systems and Environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, readers will learn how to properly secure, monitor, and manage
    production ML systems and deployed models.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B18638_08.xhtml#_idTextAnchor172), *Model Monitoring and Management
    Solutions*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18638_09.xhtml#_idTextAnchor187), *Security, Governance, and
    Compliance Strategies*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
