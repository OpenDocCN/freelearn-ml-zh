- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: SageMaker Deployment Solutions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SageMaker部署解决方案
- en: After training our **machine learning** (**ML**) model, we can proceed with
    deploying it to a web API. This API can then be invoked by other applications
    (for example, a mobile application) to perform a “prediction” or inference. For
    example, the ML model we trained in [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*, can be deployed to a web API and then
    be used to predict the likelihood of customers canceling their reservations or
    not, given a set of inputs. Deploying the ML model to a web API allows the ML
    model to be accessible to different applications and systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练我们的**机器学习**（**ML**）模型后，我们可以继续将其部署到Web API。然后，其他应用程序（例如，移动应用程序）可以通过调用该API来执行“预测”或推理。例如，我们在[*第1章*](B18638_01.xhtml#_idTextAnchor017)，*AWS上的机器学习工程导论*中训练的ML模型可以被部署到Web
    API，并用于根据一组输入预测客户取消预订的可能性或不会取消预订的可能性。将ML模型部署到Web API使得ML模型能够被不同的应用程序和系统访问。
- en: A few years ago, ML practitioners had to spend time building a custom backend
    API to host and deploy a model from scratch. If you were given this requirement,
    you might have used a Python framework such as **Flask**, **Pyramid**, or **Django**
    to deploy the ML model. Building a custom API to serve as an inference endpoint
    can take about a week or so since most of the application logic needs to be coded
    from scratch. If we were to set up **A/B testing**, **auto-scaling**, or **model
    monitoring** for the API, then we may have to spend a few additional weeks on
    top of the initial time spent to set up the base API. ML engineers and software
    developers generally underestimate the amount of work required to build and maintain
    ML inference endpoints. Requirements evolve over time and the custom application
    code becomes harder to manage as the requirements and solutions pile up. At this
    point, you might ask, “Is there a better and faster way to do this?”. The good
    news is that we could do all of it in “less than a day” if we were to use **SageMaker**
    to deploy our model! Instead of building everything from scratch, SageMaker has
    already automated most of the work and all we need to do is specify the right
    configuration parameters. If needed, SageMaker allows us to customize certain
    components and we can easily replace some of the default automated solutions with
    our own custom implementations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，机器学习从业者必须花费时间构建一个自定义的后端API，从头开始托管和部署模型。如果你有这样的要求，你可能使用Python框架，如**Flask**、**Pyramid**或**Django**来部署ML模型。构建一个自定义API作为推理端点可能需要大约一周的时间，因为大部分应用程序逻辑需要从头开始编码。如果我们需要为API设置**A/B测试**、**自动扩展**或**模型监控**，那么我们可能需要在设置基本API的初始时间基础上再额外花费几周时间。机器学习工程师和软件开发人员通常低估了构建和维护ML推理端点所需的工作量。需求随着时间的推移而演变，随着需求和解决方案的累积，自定义应用程序代码变得越来越难以管理。在这个时候，你可能会问，“有没有更好的更快的方法来做这件事？”好消息是，如果我们使用**SageMaker**来部署我们的模型，我们可以在“不到一天”的时间内完成所有这些工作！我们不必从头开始构建一切，SageMaker已经自动化了大部分工作，我们只需要指定正确的配置参数。如果需要，SageMaker允许我们自定义某些组件，我们可以轻松地将一些默认的自动化解决方案替换为我们自己的自定义实现。
- en: One of the misconceptions when using SageMaker is that ML models need to be
    trained in SageMaker first before they can be deployed in the **SageMaker hosting
    services**. It is important to note that “this is not true” since the service
    was designed and built to support different scenarios, which include deploying
    a pre-trained model straight away. This means that if we have a pre-trained model
    trained outside of SageMaker, then we *can* proceed with deploying it without
    having to go through the training steps again. In this chapter, you’ll discover
    how easy it is to use the **SageMaker Python SDK** when performing model deployments.
    In just a few lines of code, we will show you how to deploy our pre-trained model
    into a variety of inference endpoint types – **real-time**, **serverless**, and
    **asynchronous inference endpoints**. We will also discuss when it’s best to use
    each of these inference endpoint types later in this chapter. At the same time,
    we will discuss the different strategies and best practices when performing model
    deployments in SageMaker.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用SageMaker时存在的一个误解是，机器学习模型需要首先在SageMaker中训练，然后才能部署到**SageMaker托管服务**。重要的是要注意，“这并不正确”，因为该服务是设计和构建来支持不同场景的，其中包括直接部署预训练模型。这意味着，如果我们有一个在SageMaker之外训练的预训练模型，那么我们可以继续部署它，而无需再次经过训练步骤。在本章中，你将发现使用**SageMaker
    Python SDK**进行模型部署是多么简单。只需几行代码，我们就会向你展示如何将我们的预训练模型部署到各种推理端点类型——**实时**、**无服务器**和**异步推理端点**。我们还会在本章后面讨论何时使用这些推理端点类型最为合适。同时，我们还将讨论在SageMaker中进行模型部署时的不同策略和最佳实践。
- en: 'That said, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们将涵盖以下主题：
- en: Getting started with model deployments in SageMaker
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在SageMaker中开始模型部署
- en: Preparing the pre-trained model artifacts
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备预训练模型工件
- en: Preparing the SageMaker script mode prerequisites
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备SageMaker脚本模式的前提条件
- en: Deploying a pre-trained model to a real-time inference endpoint
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将预训练模型部署到实时推理端点
- en: Deploying a pre-trained model to a serverless inference endpoint
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将预训练模型部署到无服务器推理端点
- en: Deploying a pre-trained model to an asynchronous inference endpoint
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将预训练模型部署到异步推理端点
- en: Cleaning up
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理
- en: Deployment strategies and best practices
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署策略和最佳实践
- en: We will wrap up with a quick discussion of the other alternatives and options
    when deploying ML models. After you have completed the hands-on solutions in this
    chapter, you will be more confident in deploying different types of ML models
    in SageMaker. Once you reach a certain level of familiarity and mastery using
    the SageMaker Python SDK, you should be able to set up an ML inference endpoint
    in just a few hours, or maybe even in just a few minutes!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以对其他替代方案和选项的简要讨论来结束。完成本章的动手实践后，你将更有信心在SageMaker中部署不同类型的机器学习模型。一旦你达到使用SageMaker
    Python SDK的熟悉和精通程度，你应该能够在几小时内，甚至几分钟内设置一个机器学习推理端点！
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Before we start, it is important to have the following ready:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，以下准备工作是重要的：
- en: A web browser (preferably Chrome or Firefox)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个网络浏览器（最好是Chrome或Firefox）
- en: Access to the AWS account and **SageMaker Studio** domain used in the first
    chapter of the book
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取书中第一章使用的AWS账户和**SageMaker Studio域**的访问权限
- en: 'The Jupyter notebooks, source code, and other files used for each chapter are
    available in this repository: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每章使用的Jupyter笔记本、源代码和其他文件都可在以下存储库中找到：[https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS)。
- en: Important Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is recommended to use an IAM user with limited permissions instead of the
    root account when running the examples in this book. We will discuss this along
    with other security best practices in detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*. If you are just starting with
    using AWS, you may proceed with using the root account in the meantime.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 建议在运行本书中的示例时使用具有有限权限的IAM用户，而不是根账户。我们将在[*第9章*](B18638_09.xhtml#_idTextAnchor187)“安全、治理和合规策略”中详细讨论这一点，以及其他安全最佳实践。如果你刚开始使用AWS，你可以同时使用根账户。
- en: Getting started with model deployments in SageMaker
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在SageMaker中开始模型部署
- en: In [*Chapter 6*](B18638_06.xhtml#_idTextAnchor132), *SageMaker Training and
    Debugging Solutions*, we trained and deployed an image classification model using
    the **SageMaker Python SDK**. We made use of a built-in algorithm while working
    on the hands-on solutions in that chapter. When using a built-in algorithm, we
    just need to prepare the training dataset along with specifying a few configuration
    parameters and we are good to go! Note that if we want to train a custom model
    using our favorite ML framework (such as TensorFlow and PyTorch), then we can
    prepare our custom scripts and make them work in SageMaker using **script mode**.
    This gives us a bit more flexibility since we can tweak how SageMaker interfaces
    with our model through a custom script that allows us to use different libraries
    and frameworks when training our model. If we want the highest level of flexibility
    for the environment where the training scripts will run, then we can opt to use
    our own custom container image instead. SageMaker has its own set of pre-built
    container images used when training models. However, we may decide to build and
    use our own if needed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第6章*](B18638_06.xhtml#_idTextAnchor132)，“SageMaker训练和调试解决方案”，我们使用**SageMaker
    Python SDK**训练并部署了一个图像分类模型。在那一章的动手解决方案中，我们使用了内置算法。当使用内置算法时，我们只需要准备训练数据集，并指定一些配置参数，就可以开始了！请注意，如果我们想使用我们喜欢的机器学习框架（如TensorFlow和PyTorch）来训练自定义模型，那么我们可以准备我们的自定义脚本，并使用**脚本模式**在SageMaker中运行它们。这给了我们更多的灵活性，因为我们可以通过一个自定义脚本调整SageMaker与我们的模型接口的方式，该脚本允许我们在训练模型时使用不同的库和框架。如果我们希望训练脚本运行的环境具有最高级别的灵活性，那么我们可以选择使用我们自己的自定义容器镜像。SageMaker有一套预构建的容器镜像，用于在训练模型时使用。然而，我们可能会决定在需要时构建并使用我们自己的。
- en: '![Figure 7.1 – The different options when training and deployment models ](img/B18638_07_001.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 训练和部署模型的不同选项](img/B18638_07_001.jpg)'
- en: Figure 7.1 – The different options when training and deployment models
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 训练和部署模型的不同选项
- en: As we can see in *Figure 7.1*, the options available when training ML models
    in SageMaker are also available when deploying models using the SageMaker hosting
    services. Here, we tag each approach or option with an arbitrary label (for example,
    **T1** or **T2**) to help us discuss these options in more detail. When performing
    model deployments in SageMaker, we can choose to deploy a model using the container
    of a built-in algorithm (**D1**). We also have the option to deploy our deep learning
    models using **script mode** (**D2**). With this option, we need to prepare custom
    scripts that will run inside the pre-built **Deep Learning Containers**. We also
    have the option to provide and use our own custom container images for the environment
    where our ML model will be deployed (**D3**).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*图7.1*]所示，在SageMaker中训练机器学习模型时提供的选项，在部署模型使用SageMaker托管服务时也是可用的。在这里，我们给每种方法或选项贴上一个任意的标签（例如，**T1**或**T2**），以帮助我们更详细地讨论这些选项。在SageMaker中执行模型部署时，我们可以选择使用内置算法的容器来部署模型（**D1**）。我们还有选择使用**脚本模式**部署我们的深度学习模型（**D2**）。使用此选项时，我们需要准备将在预构建的**深度学习容器**内运行的定制脚本。我们还有选择为我们的机器学习模型部署的环境提供并使用我们自己的定制容器镜像（**D3**）。
- en: Important Note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Choosing which combination of options to use generally depends on the level
    of customization required (in the form of custom scripts and container images)
    when performing ML experiments and deployments. When getting started with SageMaker,
    it is recommended to use the SageMaker built-in algorithms to have a better feel
    for how things work when training a model (**T1**) and when deploying a model
    (**D1**). If we need to use frameworks such as TensorFlow, PyTorch, or MXNet on
    top of the managed infrastructure of AWS with SageMaker, we will need to prepare
    a set of custom scripts to be used during training (**T2**) and deployment (**D2**).
    Finally, when we need a much greater level of flexibility, we can prepare custom
    container images and use these when training a model (**T3**) and deploying a
    model (**D3**).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 选择使用哪种选项组合通常取决于进行机器学习实验和部署时所需的定制级别（以定制脚本和容器镜像的形式）。在开始使用SageMaker时，建议使用SageMaker内置算法，以便更好地了解训练模型时的情况（**T1**）以及部署模型时的情况（**D1**）。如果我们需要在AWS
    SageMaker托管基础设施上使用TensorFlow、PyTorch或MXNet等框架，我们需要准备一套用于训练（**T2**）和部署（**D2**）的自定义脚本。最后，当我们需要更高的灵活性时，我们可以准备自定义容器镜像，并在训练模型（**T3**）和部署模型（**D3**）时使用这些镜像。
- en: It is important to note that we can combine and use different options when training
    and deploying models. For example, we can train an ML model using script mode
    (**T2**) and use a custom container image during model deployment (**D3**). Another
    example involves training a model outside of SageMaker (**T4**) and using the
    pre-built inference container image for a built-in algorithm during model deployment
    (**D1**).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，我们可以在训练和部署模型时组合并使用不同的选项。例如，我们可以使用脚本模式（**T2**）训练 ML 模型，并在模型部署期间使用自定义容器镜像（**D3**）。另一个例子是在
    SageMaker 之外（**T4**）训练模型，并在模型部署期间使用内置算法的预构建推理容器镜像（**D1**）。
- en: 'Now, let’s talk about how model deployment works using the SageMaker hosting
    services:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈如何使用 SageMaker 主机服务进行模型部署：
- en: '![Figure 7.2 – Deploying a model using the SageMaker hosting services ](img/B18638_07_002.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 使用 SageMaker 主机服务部署模型](img/B18638_07_002.jpg)'
- en: Figure 7.2 – Deploying a model using the SageMaker hosting services
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 使用 SageMaker 主机服务部署模型
- en: In *Figure 7.2*, we have a high-level diagram of how model deployment works
    using the SageMaker hosting services. Assuming that a `model.tar.gz` file (containing
    the ML model artifacts and output files) has been uploaded to an S3 bucket after
    the training step, the `model.tar.gz` file is downloaded from the S3 bucket into
    an ML compute instance that serves as the dedicated server for the ML inference
    endpoint. Inside this ML compute instance, the model artifacts stored inside the
    `model.tar.gz` file are loaded inside a running container containing the inference
    code, which can load the model and use it for inference for incoming requests.
    As mentioned earlier, the inference code and the container image used for inference
    can either be provided by AWS (built-in or pre-built) or provided by ML engineers
    using SageMaker (custom).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.2*中，我们有一个使用 SageMaker 主机服务进行模型部署的高级示意图。假设在训练步骤之后，包含 ML 模型工件和输出文件的 `model.tar.gz`
    文件已上传到 S3 桶中，则从 S3 桶中下载 `model.tar.gz` 文件到作为 ML 推理端点专用服务器的 ML 计算实例中。在这个 ML 计算实例内部，存储在
    `model.tar.gz` 文件中的模型工件被加载到一个包含推理代码的运行容器中，该容器可以加载模型并用于处理传入的请求。如前所述，推理代码和用于推理的容器镜像可以是
    AWS（内置或预构建）提供的，也可以是使用 SageMaker 的 ML 工程师提供的（自定义）。
- en: 'Let’s show a few sample blocks of code to help us explain these concepts. Our
    first example involves training and deploying a model using the built-in **Principal
    Component Analysis** (**PCA**) algorithm – an algorithm that can be used in use
    cases such as dimensionality reduction and data compression:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示一些代码示例，以帮助我们解释这些概念。我们的第一个示例涉及使用内置的**主成分分析**（**PCA**）算法进行模型训练和部署——这是一个可用于降维和数据压缩等用例的算法：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, SageMaker makes use of a pre-built container image when training and deploying
    the PCA model. This container image has been prepared by the AWS team so that
    we won’t have to worry about implementing this ourselves when using the built-in
    algorithms. Note that we can also skip the training step and proceed with the
    deployment step in SageMaker as long as we have a pre-trained model available
    that is compatible with the pre-built container available for the built-in algorithm.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，SageMaker 在训练和部署 PCA 模型时使用预构建的容器镜像。这个容器镜像是由 AWS 团队准备的，这样我们就不必担心在使用内置算法时自己实现它。请注意，只要我们有与内置算法的预构建容器兼容的预训练模型，我们也可以在
    SageMaker 中跳过训练步骤并直接进行部署步骤。
- en: 'Now, let’s quickly take a look at an example of how to use custom scripts when
    deploying models in SageMaker:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们快速看一下如何在 SageMaker 中部署模型时使用自定义脚本的示例：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this example, SageMaker makes use of a pre-built Deep Learning Container
    image to deploy PyTorch models. As discussed in [*Chapter 3*](B18638_03.xhtml#_idTextAnchor060),
    *Deep Learning Containers*, the relevant packages and dependencies are already
    installed inside these container images. During the deployment step, the container
    runs the custom code specified in a custom `inference.py` script provided during
    the initialization of the `PyTorchModel` object. The custom code would then load
    the model and use it when processing the requests sent to the SageMaker inference
    endpoint.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，SageMaker 使用预构建的深度学习容器镜像来部署 PyTorch 模型。如[*第 3 章*](B18638_03.xhtml#_idTextAnchor060)所述，“深度学习容器”，相关的包和依赖项已经安装在这些容器镜像内部。在部署步骤中，容器运行在
    `PyTorchModel` 对象初始化期间提供的自定义 `inference.py` 脚本中指定的自定义代码。然后，自定义代码将加载模型并在处理发送到 SageMaker
    推理端点的请求时使用它。
- en: Note
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In the example provided, we initialized a `PyTorchModel` object and used the
    `deploy()` method to deploy the model to a real-time inference endpoint. Inside
    the inference endpoint, a container using the PyTorch inference container image
    will run the inference code that loads the model and uses it for inference. Note
    that we also have the corresponding `Model` classes for the other libraries and
    frameworks such as `TensorFlowModel`, `SKLearnModel`, and `MXNetModel`. Once the
    `deploy()` method is called, the appropriate inference container (with the relevant
    installed packages and dependencies) would be used inside the inference endpoint.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供的示例中，我们初始化了一个`PyTorchModel`对象，并使用`deploy()`方法将模型部署到实时推理端点。在推理端点内部，将运行使用PyTorch推理容器镜像的容器，该容器将加载模型并用于推理。请注意，我们还有其他库和框架（如`TensorFlowModel`、`SKLearnModel`和`MXNetModel`）的相应`Model`类。一旦调用`deploy()`方法，就会在推理端点内部使用适当的推理容器（带有相关已安装的包和依赖项）。
- en: 'If we want to specify and use our own custom container image, we can use the
    following block of code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要指定并使用自己的自定义容器镜像，我们可以使用以下代码块：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this example, SageMaker makes use of the custom container image stored in
    the location specified in the `image_uri` variable. Here, the assumption is that
    we have already prepared and tested the custom container image, and we have pushed
    this container image to an **Amazon Elastic Container Registry** repository before
    we have performed the model deployment step.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，SageMaker使用存储在`image_uri`变量指定的位置的**自定义容器镜像**。这里，假设我们已经准备并测试了自定义容器镜像，并在执行模型部署步骤之前将此容器镜像推送到**Amazon
    Elastic Container Registry**仓库。
- en: Note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It takes a bit of trial and error when preparing custom scripts and custom container
    images (similar to how we prepared and tested our custom container image in [*Chapter
    3*](B18638_03.xhtml#_idTextAnchor060), *Deep Learning Containers*). If you are
    using a Notebook instance, you can use the SageMaker **local mode**, which gives
    us a way to test the custom scripts and custom container images in the local environment
    before running these in the managed ML instances.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备自定义脚本和自定义容器镜像时需要一些尝试和错误（类似于我们在[*第3章*](B18638_03.xhtml#_idTextAnchor060)，*深度学习容器*中准备和测试自定义容器镜像的方式）。如果您正在使用Notebook实例，您可以使用SageMaker的**本地模式**，这为我们提供了一个在将自定义脚本和自定义容器镜像运行在托管机器学习实例之前在本地环境中测试它们的方法。
- en: 'The code samples shown in this section assumed that we would be deploying our
    ML model in a real-time inference endpoint. However, there are different options
    to choose from when deploying ML models in SageMaker:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中展示的代码示例假设我们将我们的机器学习模型部署在实时推理端点。然而，在SageMaker中部署机器学习模型时，我们有不同的选项可供选择：
- en: The first option involves deploying and hosting our model in a **real-time inference
    endpoint**.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个选项涉及在我们的**实时推理端点**部署和托管我们的模型。
- en: The second option involves tweaking the configuration a bit when using the SageMaker
    Python SDK to deploy our model in a **serverless inference endpoint**.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个选项涉及在使用SageMaker Python SDK部署我们的模型到**无服务器推理端点**时稍微调整配置。
- en: The third option is to host our model in an **asynchronous inference endpoint**.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个选项是将我们的模型托管在**异步推理端点**。
- en: We will cover these options in the hands-on portion of this chapter and we will
    discuss the relevant use cases and scenarios for each of these as well.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的实践部分介绍这些选项，并讨论每个选项的相关用例和场景。
- en: Note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'It is important to note that it is also possible to perform inference with
    a model without having to set up an inference endpoint. This involves using **Batch
    Transform** where a model is loaded and used to process multiple input payload
    values and perform predictions all in one go. To see a working example of Batch
    Transform, feel free to check out the following link: [https://bit.ly/3A9wrVy](https://bit.ly/3A9wrVy).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，也可以在不设置推理端点的情况下执行推理。这涉及到使用**批量转换**，其中模型被加载并用于一次性处理多个输入有效载荷值并执行预测。要查看批量转换的工作示例，请随意查看以下链接：[https://bit.ly/3A9wrVy](https://bit.ly/3A9wrVy)。
- en: Now that we have a better idea of how SageMaker model deployment works, let’s
    proceed with the hands-on portion of this chapter. In the next section, we will
    prepare the `model.tar.gz` file containing the ML model artifacts that we will
    use for the model deployment solutions in this chapter.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对 SageMaker 模型部署的工作原理有了更好的了解，让我们继续本章的动手实践部分。在下一节中，我们将准备包含我们将用于本章模型部署解决方案的
    ML 模型工件 `model.tar.gz` 文件。
- en: Preparing the pre-trained model artifacts
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备预训练模型工件
- en: In [*Chapter 6*](B18638_06.xhtml#_idTextAnchor132), *SageMaker Training and
    Debugging Solutions*, we created a new folder named `CH06`, along with a new Notebook
    using the `Data Science` image inside the created folder. In this section, we
    will create a new folder (named `CH07`), along with a new Notebook inside the
    created folder. Instead of the `Data Science` image, we will use the `PyTorch
    1.10 Python 3.8 CPU Optimized` image as the image used in the Notebook since we
    will download the model artifacts of a pre-trained `transformers` library. Once
    the Notebook is ready, we will use the Hugging Face `transformers` library to
    download a pre-trained model that can be used for sentiment analysis. Finally,
    we will zip the model artifacts into a `model.tar.gz` file and upload it to an
    S3 bucket.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 6 章*](B18638_06.xhtml#_idTextAnchor132) 的 *SageMaker 训练和调试解决方案* 中，我们创建了一个名为
    `CH06` 的新文件夹，并在创建的文件夹中使用 `Data Science` 映像创建了一个新的笔记本。在本节中，我们将创建一个新文件夹（命名为 `CH07`），并在创建的文件夹内创建一个新的笔记本。由于我们将下载预训练的
    `transformers` 库的模型工件，我们将使用 `PyTorch 1.10 Python 3.8 CPU Optimized` 映像作为笔记本中使用的映像。一旦笔记本准备就绪，我们将使用
    Hugging Face 的 `transformers` 库下载一个可用于情感分析的预训练模型。最后，我们将模型工件压缩成 `model.tar.gz`
    文件，并将其上传到 S3 桶。
- en: Note
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure that you have completed the hands-on solutions in the *Getting started
    with SageMaker and SageMaker Studio* section of [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*, before proceeding. It is important to
    note that the hands-on section in this chapter is not a continuation of what we
    completed in [*Chapter 6*](B18638_06.xhtml#_idTextAnchor132), *SageMaker Training
    and Debugging Solutions*. As long as we have SageMaker Studio set up, we should
    be good to go.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保您已经完成了 [*第 1 章*](B18638_01.xhtml#_idTextAnchor017) 的 *SageMaker 和 SageMaker
    Studio 入门* 部分的动手解决方案。需要注意的是，本章的动手部分不是对 [*第 6 章*](B18638_06.xhtml#_idTextAnchor132)
    的 *SageMaker 训练和调试解决方案* 中所完成内容的延续。只要我们设置了 SageMaker Studio，我们就应该可以继续了。
- en: 'In the next set of steps, we will prepare the `model.tar.gz` file containing
    the model artifacts and then upload it to an S3 bucket:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将准备包含模型工件的 `model.tar.gz` 文件，并将其上传到 S3 桶：
- en: Navigate to `sagemaker studio` into the search bar of the AWS Management Console
    and then selecting **SageMaker Studio** from the list of results under **Features**.
    We click **Studio** under **SageMaker Domain** in the sidebar and then we select
    **Studio** from the list of drop-down options under the **Launch app** drop-down
    menu (on the **Users** pane). Wait for a minute or two for the SageMaker Studio
    interface to load.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 管理控制台的搜索栏中导航到 `sagemaker studio`，然后从 **功能** 下的结果列表中选择 **SageMaker Studio**。在侧边栏中，我们点击
    **SageMaker Domain** 下的 **Studio**，然后从 **启动应用** 下拉菜单（在 **用户** 面板中）选择 **Studio**。等待一分钟左右，以加载
    SageMaker Studio 界面。
- en: Important Note
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意
- en: This chapter assumes that we are using the `us-west-2`) region when using services
    to manage and create different types of resources. You may use a different region
    but make sure to perform any adjustments needed in case certain resources need
    to be transferred to the region of choice.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设我们在使用服务管理和创建不同类型的资源时使用的是 `us-west-2` 区域。您可以使用不同的区域，但请确保在需要将某些资源转移到所选区域的情况下进行任何必要的调整。
- en: Right-click on the empty space in the `CH07`. Finally, navigate to the `CH07`
    directory by double-clicking the folder name in the sidebar.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `CH07` 的空白区域右键单击。最后，通过在侧边栏中双击文件夹名称来导航到 `CH07` 目录。
- en: Create a new Notebook by clicking the `PyTorch 1.10 Python 3.8 CPU Optimized`
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击 `PyTorch 1.10 Python 3.8 CPU Optimized` 创建一个新的笔记本
- en: '`Python 3`'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Python 3`'
- en: '`No script`'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`无脚本`'
- en: Click the **Select** button afterward.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **选择** 按钮。
- en: Note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Wait for the kernel to start. This step may take around 3 to 5 minutes while
    an ML instance is being provisioned to run the Jupyter notebook cells.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 等待内核启动。在为运行 Jupyter 笔记本单元格分配 ML 实例时，此步骤可能需要大约 3 到 5 分钟。
- en: Rename the notebook from `Untitled.ipynb` to `01 - Prepare model.tar.gz file.ipynb`.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将笔记本从 `Untitled.ipynb` 重命名为 `01 - 准备 model.tar.gz 文件.ipynb`。
- en: 'Now that our notebook is ready, we can proceed with generating the pre-trained
    model artifacts and storing these inside a `model.tar.gz` file. In the first cell
    of the Jupyter Notebook, let’s run the following, which will install the Hugging
    Face `transformers` library:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在笔记本已经准备好了，我们可以继续生成预训练模型工件并将这些存储在 `model.tar.gz` 文件中。在 Jupyter Notebook 的第一个单元中，让我们运行以下代码，这将安装
    Hugging Face 的 `transformers` 库：
- en: '[PRE3]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Install `ipywidgets` using `pip` as well:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pip` 安装 `ipywidgets`：
- en: '[PRE4]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, let’s run the following block of code to restart the kernel:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们运行以下代码块来重启内核：
- en: '[PRE5]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This should yield an output value similar to `{''status'': ''ok'', ''restart'':
    True}` and restart the kernel accordingly to ensure that we will not encounter
    issues using the packages we just installed.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '这应该产生一个类似于 `{''status'': ''ok'', ''restart'': True}` 的输出值，并相应地重启内核，以确保我们不会在使用我们刚刚安装的包时遇到问题。'
- en: 'Let’s download a pre-trained model using the `transformers` library. We’ll
    download a model that can be used for sentiment analysis and to classify whether
    a statement is *POSITIVE* or *NEGATIVE*. Run the following block of code to download
    the artifacts of the pre-trained `distilbert` model into the current directory:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 `transformers` 库下载一个预训练模型。我们将下载一个可以用于情感分析和分类陈述是否为 *POSITIVE* 或 *NEGATIVE*
    的模型。运行以下代码块将预训练的 `distilbert` 模型工件下载到当前目录：
- en: '[PRE8]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This should generate two files in the same directory as the `.ipynb` notebook
    file:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该在 `.ipynb` 笔记本文件相同的目录中生成两个文件：
- en: '`config.json`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config.json`'
- en: '`pytorch_model.bin`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch_model.bin`'
- en: Note
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*How should this work?* If we have an “`I love reading the book MLE on AWS!`”
    statement, for example, the trained model should classify it as a *POSITIVE* statement.
    If we have a “`This is the worst spaghetti I''ve had`” statement, the trained
    model should then classify it as a *NEGATIVE* statement.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*如何实现这一功能？* 例如，如果我们有一个“`I love reading the book MLE on AWS!`”这样的陈述，训练好的模型应该将其分类为
    *POSITIVE* 陈述。如果我们有一个“`This is the worst spaghetti I''ve had`”这样的陈述，训练好的模型随后应该将其分类为
    *NEGATIVE* 陈述。'
- en: 'Prepare the `model.tar.gz` (compressed archive) file containing the model artifact
    files generated in the previous step using the following block of code:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码块准备包含之前步骤中生成的模型工件文件的 `model.tar.gz`（压缩存档）文件：
- en: '[PRE12]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Use the `rm` command to clean up the model files by deleting the model artifacts
    generated in the previous steps:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `rm` 命令通过删除之前步骤中生成的模型工件来清理模型文件：
- en: '[PRE17]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Specify the S3 bucket name and prefix. Make sure to replace the value of `<INSERT
    S3 BUCKET NAME HERE>` with a unique S3 bucket name before running the following
    block of code:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定 S3 桶名称和前缀。在运行以下代码块之前，请确保将 `<INSERT S3 BUCKET NAME HERE>` 的值替换为一个唯一的 S3 桶名称：
- en: '[PRE20]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Make sure to specify a bucket name for an S3 bucket that does not exist yet.
    In the case that you want to reuse one of the buckets created in the previous
    chapters, you may do so, but make sure to use an S3 bucket in the same region
    where SageMaker Studio is set up and configured.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 确保指定一个尚不存在的 S3 桶的桶名称。如果你想要重用之前章节中创建的其中一个桶，可以这样做，但请确保使用与 SageMaker Studio 设置和配置相同的区域的
    S3 桶。
- en: 'Create a new S3 bucket using the `aws s3 mb` command:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `aws s3 mb` 命令创建一个新的 S3 桶：
- en: '[PRE22]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You can skip this step if you are planning to reuse one of the existing S3 buckets
    created in the previous chapters.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划重用之前章节中创建的现有 S3 桶，则可以跳过此步骤。
- en: 'Prepare the S3 path where we will upload the model files:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备我们将上传模型文件的 S3 路径：
- en: '[PRE23]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that at this point, a `model.tar.gz` file does not exist in the specified
    S3 path yet. Here, we are simply preparing the S3 location (string) where the
    `model.tar.gz` file will be uploaded.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在此阶段，指定 S3 路径中尚不存在 `model.tar.gz` 文件。在这里，我们只是在准备 `model.tar.gz` 文件将要上传的 S3
    位置（字符串）。
- en: 'Now, let’s use the `aws s3 cp` command to copy and upload the `model.tar.gz`
    file to the S3 bucket:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用 `aws s3 cp` 命令来复制并上传 `model.tar.gz` 文件到 S3 桶：
- en: '[PRE26]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Use the `%store` magic to store the variable values for `model_data`, `s3_bucket`,
    and `prefix`:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `%store` 魔法来存储 `model_data`、`s3_bucket` 和 `prefix` 的变量值：
- en: '[PRE27]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This should allow us to use these variable values for one or more of the succeeding
    sections in this chapter, similar to what we have here in *Figure 7.3*:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该允许我们在本章的后续部分中使用这些变量值，类似于我们在 *图 7.3* 中所做的那样：
- en: '![Figure 7.3 – The %store magic ](img/B18638_07_003.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – %store 魔法](img/B18638_07_003.jpg)'
- en: Figure 7.3 – The %store magic
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – %store 魔法
- en: Make sure not to restart the kernel or else we will lose the variable values
    saved using the `%store` magic.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 确保不要重新启动内核，否则我们将丢失使用`%store`魔法保存的变量值。
- en: Preparing the SageMaker script mode prerequisites
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备SageMaker脚本模式的前提条件
- en: In this chapter, we will be preparing a custom script to use a pre-trained model
    for predictions. Before we can proceed with using the **SageMaker Python SDK**
    to deploy our pre-trained model to an inference endpoint, we’ll need to ensure
    that all the script mode prerequisites are ready.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将准备一个自定义脚本，用于使用预训练模型进行预测。在我们可以使用**SageMaker Python SDK**将预训练模型部署到推理端点之前，我们需要确保所有脚本模式的前提条件都已准备好。
- en: '![Figure 7.4 – The desired file and folder structure ](img/B18638_07_004.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 所需的文件和文件夹结构](img/B18638_07_004.jpg)'
- en: Figure 7.4 – The desired file and folder structure
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 所需的文件和文件夹结构
- en: 'In *Figure 7.4*, we can see that there are three prerequisites we’ll need to
    prepare:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.4*中，我们可以看到我们需要准备三个前提条件：
- en: '`inference.py`'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inference.py`'
- en: '`requirements.txt`'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requirements.txt`'
- en: '`setup.py`'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setup.py`'
- en: We will store these prerequisites inside the `scripts` directory. We’ll discuss
    these prerequisites in detail in the succeeding pages of this chapter. Without
    further ado, let’s start by preparing the `inference.py` script file!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些前提条件存储在`scripts`目录中。我们将在本章后续页面中详细讨论这些前提条件。现在，让我们开始准备`inference.py`脚本文件！
- en: Preparing the inference.py file
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备inference.py文件
- en: 'In this section, we will prepare a custom Python script that will be used by
    SageMaker when processing inference requests. Here, we can influence how the input
    request is deserialized, how a custom model is loaded, how the prediction step
    is performed, and how the output prediction is serialized and returned as a response.
    To do all of this, we will need to override the following inference handler functions
    inside our script file: `model_fn()`, `input_fn()`, `predict_fn()`, and `output_fn()`.
    We will discuss how these functions work shortly.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将准备一个自定义Python脚本，该脚本将由SageMaker在处理推理请求时使用。在这里，我们可以影响输入请求的反序列化方式、如何加载自定义模型、如何执行预测步骤以及如何将输出预测序列化并作为响应返回。为了完成所有这些，我们需要在我们的脚本文件中覆盖以下推理处理函数：`model_fn()`、`input_fn()`、`predict_fn()`和`output_fn()`。我们将在稍后讨论这些函数的工作原理。
- en: 'In the next set of steps, we will prepare our custom Python script and override
    the default implementations of the inference handler functions:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将准备我们的自定义Python脚本并覆盖推理处理函数的默认实现：
- en: 'Right-click on the empty space in the **File Browser** sidebar pane to open
    a context menu similar to that shown in *Figure 7.5*:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 右键单击**文件浏览器**侧边栏中的空白区域以打开一个类似于*图7.5*所示的上下文菜单：
- en: '![Figure 7.5 – Creating a new folder inside the CH07 directory ](img/B18638_07_005.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 在CH07目录内创建新文件夹](img/B18638_07_005.jpg)'
- en: Figure 7.5 – Creating a new folder inside the CH07 directory
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 在CH07目录内创建新文件夹
- en: Select **New Folder** from the list of options available in the context menu,
    as highlighted in *Figure 7.5*. Note that we can also press the envelope button
    (with a plus) just beside the **+** button to create a new folder as well.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从上下文菜单中的选项列表中选择**新建文件夹**，如图7.5所示。请注意，我们也可以按下位于**+**按钮旁边的信封按钮（带加号）来创建一个新的文件夹。
- en: Name the new folder `scripts`.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新文件夹命名为`scripts`。
- en: Next, double-click the `scripts` folder to navigate to the directory.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，双击`scripts`文件夹以导航到该目录。
- en: Create a new text file by clicking the **File** menu and choosing **Text File**
    from the list of options under the **New** submenu.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击**文件**菜单并从**新建**子菜单下的选项列表中选择**文本文件**来创建一个新的文本文件。
- en: Right-click on the `inference.py`.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 右键单击`inference.py`。
- en: 'Click on the `inference.py` script:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`inference.py`脚本：
- en: '![Figure 7.6 – Getting ready to add code to the inference.py file in the Editor
    pane ](img/B18638_07_006.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 在编辑器窗格中准备向inference.py文件添加代码](img/B18638_07_006.jpg)'
- en: Figure 7.6 – Getting ready to add code to the inference.py file in the Editor
    pane
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 在编辑器窗格中准备向inference.py文件添加代码
- en: We will add the succeeding blocks of code into the `inference.py` file. Make
    sure that there’s an extra blank line after each block of code.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把接下来的代码块添加到`inference.py`文件中。确保每个代码块后面都有一个额外的空白行。
- en: 'In the `inference.py` file:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`inference.py`文件中：
- en: '[PRE30]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Specify the tokenizer:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定分词器：
- en: '[PRE37]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Here, we specify the appropriate tokenizer for the model we will use to perform
    the prediction in a later step.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定了在后续步骤中用于执行预测的模型的适当分词器。
- en: Note
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'What’s a tokenizer? A `"I am hungry"`, then a tokenizer will split it into
    the tokens `"I"`, `"am"`, and `"``hungry"`. Note that this is a simplified example
    and there’s more to tokenization than what can be explained in a few sentences.
    For more details, feel free to check out the following link: [https://huggingface.co/docs/transformers/main_classes/tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是分词器？例如，对于 `"I am hungry"`，分词器会将其分割成 `"I"`、`"am"` 和 `"hungry"` 这三个标记。请注意，这是一个简化的例子，分词器的工作远不止这些可以在几句话中解释清楚的内容。更多详情，请随时查看以下链接：[https://huggingface.co/docs/transformers/main_classes/tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)。
- en: 'Define the `model_fn()` function:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `model_fn()` 函数：
- en: '[PRE38]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Here, we defined a model function that returns a model object used to perform
    predictions and process inference requests. Since we are planning to load and
    use a pre-trained model, we used the `from_pretrained()` method of `AutoModelForSequenceClassification`
    from the `transformers` library to load the model artifacts in the specified model
    directory. The `from_pretrained()` method then returns a model object that can
    be used during the prediction step.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个模型函数，它返回一个用于执行预测和处理推理请求的模型对象。由于我们计划加载和使用预训练模型，我们使用了 `transformers`
    库中的 `AutoModelForSequenceClassification` 的 `from_pretrained()` 方法来加载指定模型目录中的模型工件。`from_pretrained()`
    方法随后返回一个模型对象，该对象可以在预测步骤中使用。
- en: 'Now, let’s define the `humanize_prediction()` function:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义 `humanize_prediction()` 函数：
- en: '[PRE41]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The `humanize_prediction()` function simply accepts the raw output produced
    by the model after processing the input payload during the prediction step. It
    returns either a `"POSITIVE"` or a `"NEGATIVE"` prediction to the calling function.
    We will define this *calling function* in the next step.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`humanize_prediction()` 函数简单地接受模型在预测步骤中处理输入有效载荷后产生的原始输出。它将返回一个 `"POSITIVE"`
    或 `"NEGATIVE"` 预测给调用函数。我们将在下一步定义这个 *调用函数*。'
- en: 'Next, let’s define `predict_fn()` using the following block of code:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们使用以下代码块定义 `predict_fn()`：
- en: '[PRE52]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The `predict_fn()` function accepts the deserialized input request data and
    the loaded model as input. It then uses these two parameter values to produce
    a prediction. How? Since the loaded model is available as the second parameter,
    we simply use it to perform predictions. The input payload to this prediction
    step would be the deserialized request data, which is available as the first parameter
    to the `predict_fn()` function. Before the output is returned, we make use of
    the `humanize_prediction()` function to convert the raw output to either `"POSITIVE"`
    or `"NEGATIVE"`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict_fn()` 函数接受反序列化的输入请求数据和加载的模型作为输入。然后，它使用这两个参数值来生成一个预测。如何？由于加载的模型作为第二个参数可用，我们只需使用它来执行预测。这个预测步骤的输入有效载荷将是反序列化的请求数据，它是
    `predict_fn()` 函数的第一个参数。在输出返回之前，我们使用 `humanize_prediction()` 函数将原始输出转换为 `"POSITIVE"`
    或 `"NEGATIVE"`。'
- en: Note
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: Why do we have a commented line containing `sleep(30)`? Later in the *Deploying
    a pre-trained model to an asynchronous inference endpoint section*, we will emulate
    an inference endpoint with a relatively long processing time using an artificial
    30-second delay. For now, we’ll keep this line commented out and we will undo
    this later in that section.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们有一个包含 `sleep(30)` 的注释行？在 *将预训练模型部署到异步推理端点部分* 的后面，我们将使用人工的 30 秒延迟来模拟一个相对较长的处理时间。现在，我们将保持这一行被注释，稍后在该部分中我们将取消注释。
- en: 'Let’s also define the `input_fn()` function that is used to convert the serialized
    input request data into its deserialized form. This deserialized form will be
    used for prediction at a later stage:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，我们还需要定义 `input_fn()` 函数，该函数用于将序列化的输入请求数据转换为它的反序列化形式。这种反序列化形式将在稍后的预测阶段使用：
- en: '[PRE68]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: In the `input_fn()` function, we also ensure that the specified content type
    is within the list of support content types we define by raising `Exception` for
    unsupported content types.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `input_fn()` 函数中，我们还确保指定的内容类型在我们定义的支持内容类型列表中，对于不支持的内容类型，我们通过抛出 `Exception`
    来处理。
- en: 'Finally, let’s define `output_fn()`:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们定义 `output_fn()`：
- en: '[PRE75]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: The purpose of `output_fn()` is to serialize the prediction result into the
    specified content type. Here, we also ensure that the specified content type is
    within the list of support content types we define by raising `Exception` for
    unsupported content types.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`output_fn()` 的目的是将预测结果序列化为指定的内容类型。在这里，我们还确保指定的内容类型在我们定义的支持内容类型列表中，对于不支持的内容类型，我们通过抛出
    `Exception` 来处理。'
- en: Note
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: We can think of *serialization* and *deserialization* as data transformation
    steps that convert data from one form to another. For example, the input request
    data may be passed as a valid JSON *string* to the inference endpoint. This input
    request data passes through the `input_fn()` function, which converts it into
    a *JSON* or a *dictionary*. This deserialized value is then passed as a payload
    to the `predict_fn()` function. After this, the `predict_fn()` function returns
    a prediction as the result. This result is then converted to the specified content
    type using the `output_fn()` function.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 *序列化* 和 *反序列化* 视为数据转换步骤，将数据从一种形式转换为另一种形式。例如，输入请求数据可能作为有效的 JSON *字符串* 传递给推理端点。这个输入请求数据通过
    `input_fn()` 函数，该函数将其转换为 *JSON* 或 *字典*。然后，这个反序列化的值作为有效载荷传递给 `predict_fn()` 函数。之后，`predict_fn()`
    函数返回一个预测结果。然后，使用 `output_fn()` 函数将这个结果转换为指定的内容类型。
- en: Save the changes by pressing *CTRL* + *S*.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过按 *CTRL* + *S* 保存更改。
- en: Note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you are using a Mac, use *CMD* + *S* instead. Alternatively, you can just
    click **Save Python File** under the list of options under the **File** menu.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是 Mac，请使用 *CMD* + *S*。或者，你可以在 **文件** 菜单下的选项列表中点击 **保存 Python 文件**。
- en: 'At this point, you might be wondering how all of these fit together. To help
    us understand how the inference handler functions interact with the data and with
    each other, let’s quickly check out the diagram shown in *Figure 7.7*:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能想知道所有这些是如何结合在一起的。为了帮助我们理解推理处理函数如何与数据和彼此交互，让我们快速查看 *图 7.7* 中所示的图表：
- en: '![Figure 7.7 – The inference handler functions ](img/B18638_07_007.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 推理处理函数](img/B18638_07_007.jpg)'
- en: Figure 7.7 – The inference handler functions
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 推理处理函数
- en: In *Figure 7.7*, we can see that the `model_fn()` function is used to load the
    ML model object. This model object will be used by the `predict_fn()` function
    to perform predictions once a request comes in. When a request comes in, the `input_fn()`
    function processes the serialized request data and converts it into its deserialized
    form. This deserialized request data is then passed to the `predict_fn()` function,
    which then uses the loaded ML model to perform a prediction using the request
    data as a payload. The `predict_fn()` function then returns the output prediction,
    which is serialized by the `output_fn()` function.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 7.7* 中，我们可以看到 `model_fn()` 函数用于加载 ML 模型对象。这个模型对象将在收到请求后由 `predict_fn()`
    函数用于执行预测。当收到请求时，`input_fn()` 函数处理序列化的请求数据并将其转换为反序列化形式。然后，这个反序列化的请求数据传递给 `predict_fn()`
    函数，该函数使用加载的 ML 模型，以请求数据作为有效载荷执行预测。然后，`predict_fn()` 函数返回输出预测，该预测由 `output_fn()`
    函数序列化。
- en: Note
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information about this topic, feel free to check out the following
    link: [https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.xhtml](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.xhtml#serve-a-pytorch-model).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个主题的更多信息，请随时查看以下链接：[https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.xhtml](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.xhtml#serve-a-pytorch-model)。
- en: Now that we have our inference script ready, let’s proceed with preparing the
    `requirements.txt` file in the next section!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了推理脚本，接下来让我们在下一节中准备 `requirements.txt` 文件！
- en: Preparing the requirements.txt file
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备 requirements.txt 文件
- en: Since the `transformers` package is not included in the SageMaker PyTorch Docker
    containers, we will need to include it through a `requirements.txt` file, which
    is used by SageMaker to install additional packages at runtime. If this is your
    first time dealing with a `requirements.txt` file, it is simply a text file that
    contains a list of packages to be installed using the `pip install` command. If
    your `requirements.txt` file contains a single line (for example, `transformers==4.4.2`),
    then this will map to a `pip install transformers==4.4.2` during the installation
    step. If the `requirements.txt` file contains multiple lines, then each of the
    packages listed will be installed using the `pip install` command.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `transformers` 包不包括在 SageMaker PyTorch Docker 容器中，我们需要通过 `requirements.txt`
    文件来包含它，该文件由 SageMaker 在运行时用于安装额外的包。如果你第一次处理 `requirements.txt` 文件，它只是一个包含要使用 `pip
    install` 命令安装的包列表的文本文件。如果你的 `requirements.txt` 文件包含单行（例如，`transformers==4.4.2`），那么在安装步骤中这将会映射到
    `pip install transformers==4.4.2`。如果 `requirements.txt` 文件包含多行，那么列出的每个包都将使用 `pip
    install` 命令安装。
- en: Note
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We can optionally *pin* the listed packages and dependencies to a specific version
    using `==` (equal). Alternatively, we can also use `<` (less than), `>` (greater
    than), and other variations to manage the upper-bound and lower-bound values of
    the version numbers of the packages to be installed.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create and prepare the `requirements.txt`
    file inside the `scripts` directory:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new text file by clicking the **File** menu and choosing **Text File**
    from the list of options under the **New** submenu:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Creating a new text file inside the scripts directory ](img/B18638_07_008.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Creating a new text file inside the scripts directory
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.8*, we can see that we’re in the `scripts` directory (`scripts`
    directory.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Rename the file `requirements.txt`
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `requirements.txt` file to the following:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Make sure to save the changes by pressing *CTRL* + *S*.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a Mac, use *CMD* + *S* instead. Alternatively, you can just
    click **Save Python File** under the list of options under the **File** menu.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Wasn’t that easy? Let’s now proceed with the last prerequisite – the `setup.py`
    file.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the setup.py file
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the `requirements.txt` file, we will prepare a `setup.py` file
    that will contain some additional information and metadata.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'We won’t dive deep into the differences between the usage of `requirements.txt`
    and `setup.py` files. Feel free to check out the following link for more information:
    [https://docs.python.org/3/distutils/setupscript.xhtml](https://docs.python.org/3/distutils/setupscript.xhtml).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create and prepare the `setup.py` file inside
    the `scripts` directory:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Using the same set of steps as in the previous section, create a new text file
    and rename it `setup.py`. Make sure that this file is in the same directory (`scripts`)
    as the `inference.py` and `requirements.txt` files.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the contents of the `setup.py` file to contain the following block of
    code:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: The setup script simply makes use of the `setup()` function to describe the
    module distribution. Here, we specify metadata such as the `name`, `version`,
    and `description` when the `setup()` function is called.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Finally, make sure to save the changes by pressing *CTRL* + *S*.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a Mac, use *CMD* + *S* instead. Alternatively, you can just
    click **Save Python File** under the list of options under the **File** menu.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have prepared all the prerequisites needed to run all the
    succeeding sections of this chapter. With this, let’s proceed with deploying our
    pre-trained model to a real-time inference endpoint in the next section!
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a pre-trained model to a real-time inference endpoint
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use the SageMaker Python SDK to deploy a pre-trained
    model to a real-time inference endpoint. From the name itself, we can tell that
    a real-time inference endpoint can process input payloads and perform predictions
    in real time. If you have built an API endpoint before (which can process GET
    and POST requests, for example), then we can think of an inference endpoint as
    an API endpoint that accepts an input request and returns a prediction as part
    of a response. How are predictions made? The inference endpoint simply loads the
    model into memory and uses it to process the input payload. This will yield an
    output that is returned as a response. For example, if we have a pre-trained sentiment
    analysis ML model deployed in a real-time inference endpoint, then it would return
    a response of either `"POSITIVE"` or `"``NEGATIVE"` depending on the input string
    payload provided in the request.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 SageMaker Python SDK 将预训练模型部署到实时推理端点。从其名称本身，我们可以看出实时推理端点可以处理输入有效载荷并在实时进行预测。如果你之前构建过
    API 端点（例如，可以处理 GET 和 POST 请求），那么我们可以将推理端点视为一个接受输入请求并作为响应的一部分返回预测的 API 端点。预测是如何进行的？推理端点只需将模型加载到内存中并使用它来处理输入有效载荷。这将产生一个作为响应返回的输出。例如，如果我们有一个预训练的情感分析
    ML 模型部署在实时推理端点，那么它将根据请求中提供的输入字符串有效载荷返回 `"POSITIVE"` 或 `"NEGATIVE"` 的响应。
- en: Note
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Let’s say that our inference endpoint receives the statement `"I love reading
    the book MLE on AWS!"` via a POST request. The inference endpoint would then process
    the request input data and use the ML model for inference. The result of the ML
    model inference step (for example, number values that represent a `"POSITIVE"`
    result) would then be returned as part of the response.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的推理端点通过 POST 请求接收语句 `"我喜欢阅读 AWS 上的 MLE 书籍！"`。然后，推理端点将处理请求输入数据并使用 ML 模型进行推理。ML
    模型推理步骤的结果（例如，表示 `"POSITIVE"` 结果的数值）将作为响应的一部分返回。
- en: '![Figure 7.9 – The desired file and folder structure   ](img/B18638_07_009.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 所需的文件和文件夹结构   ](img/B18638_07_009.jpg)'
- en: Figure 7.9 – The desired file and folder structure
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 所需的文件和文件夹结构
- en: To get this to work, we just need to make sure that the prerequisite files,
    including the inference script file (for example, `inference.py`) and the `requirements.txt`
    file, are ready before using the SageMaker Python SDK to prepare the real-time
    inference endpoint. Make sure to check and review the folder structure in *Figure
    7.9* before proceeding with the hands-on solutions in this section.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 要使这一切正常工作，我们只需在使用 SageMaker Python SDK 准备实时推理端点之前确保所有先决文件就绪，包括推理脚本文件（例如，`inference.py`）和
    `requirements.txt` 文件。在继续本节的手动解决方案之前，请务必检查并审查 *图 7.9* 中的文件夹结构。
- en: 'In the next set of steps, we will use the SageMaker Python SDK to deploy our
    pre-trained model to a real-time inference endpoint:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将使用 SageMaker Python SDK 将我们的预训练模型部署到实时推理端点：
- en: In `CH07` directory and create a new Notebook using the `Data Science` image.
    Rename the notebook `02 - Deploying a real-time inference endpoint.ipynb`.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `CH07` 目录中创建一个新的笔记本，使用 `Data Science` 映像。将笔记本重命名为 `02 - 部署实时推理端点.ipynb`。
- en: Note
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The new notebook should be next to `01 - Prepare model.tar.gz file.ipynb`, similar
    to what is shown in *Figure 7.9*.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 新的笔记本应该紧挨着 `01 - 准备模型.tar.gz 文件.ipynb`，类似于 *图 7.9* 中所示。
- en: 'Let’s run the following code block in the first cell of the new notebook:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在新笔记本的第一个单元中运行以下代码块：
- en: '[PRE88]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Here, we use the `%store` magic to load the variable values for `model_data`,
    `s3_bucket`, and `prefix`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `%store` 魔法加载 `model_data`、`s3_bucket` 和 `prefix` 的变量值。
- en: 'Next, let’s prepare the IAM execution role for use by SageMaker:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们为 SageMaker 准备 IAM 执行角色：
- en: '[PRE91]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Initialize the `PyTorchModel` object:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 `PyTorchModel` 对象：
- en: '[PRE93]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Let’s check out *Figure 7.10* to help us visualize what has happened in the
    previous block of code:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看 *图 7.10* 以帮助我们可视化之前代码块中发生的情况：
- en: '![Figure 7.10 – Deploying a real-time inference endpoint ](img/B18638_07_010.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – 部署实时推理端点 ](img/B18638_07_010.jpg)'
- en: Figure 7.10 – Deploying a real-time inference endpoint
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 部署实时推理端点
- en: 'In *Figure 7.10*, we can see that we initialized a `Model` object by passing
    several configuration parameters during the initialization step: (1) the model
    data, (2) the framework version, and (3) the path to the `inference.py` script
    file. There are other arguments we can set but we will simplify things a bit and
    focus on these three. In order for SageMaker to know how to use the pre-trained
    model for inference, the `inference.py` script file should contain the custom
    logic, which loads the ML model and uses it to perform predictions.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.10*中，我们可以看到在初始化步骤中通过传递几个配置参数初始化了一个`Model`对象：（1）模型数据，（2）框架版本，以及（3）`inference.py`脚本文件的路径。我们还可以设置其他参数，但为了简化，我们将关注这三个。为了让SageMaker知道如何使用预训练模型进行推理，`inference.py`脚本文件应包含自定义逻辑，该逻辑加载ML模型并使用它进行预测。
- en: Note
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is important to note that we are not limited to naming the inference script
    file `inference.py`. We can use a different naming convention as long as we specify
    the correct `entry_point` value.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，我们不仅限于将推理脚本文件命名为`inference.py`。只要我们指定正确的`entry_point`值，我们就可以使用不同的命名约定。
- en: This is the case if we are using SageMaker’s script mode when deploying ML models.
    Note that there are other options available, such as using a custom container
    image where instead of passing a script, we’ll be passing a container image that
    we’ve prepared ahead of time. When deploying ML models trained using the **built-in
    algorithms** of SageMaker, we can proceed with deploying these models right away
    without any custom scripts or container images, since SageMaker already has provided
    all the prerequisites needed for deployment.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在部署机器学习模型时使用SageMaker的脚本模式，那么这就是这种情况。请注意，还有其他选项可用，例如使用自定义容器镜像，在这种情况下，我们将传递一个我们事先准备好的容器镜像，而不是传递脚本。当使用SageMaker的**内置算法**训练机器学习模型时，我们可以直接部署这些模型，无需任何自定义脚本或容器镜像，因为SageMaker已经提供了所有部署所需的前提条件。
- en: 'Use the `deploy()` method to deploy the model to a real-time inference endpoint:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`deploy()`方法将模型部署到实时推理端点：
- en: '[PRE102]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: This step should take around 3 to 8 minutes to complete.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤应花费大约3到8分钟才能完成。
- en: Note
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'When using the `deploy()` method to deploy an ML model using the SageMaker
    Python SDK, we are given the ability to specify the instance type. Choosing the
    right instance type for the model is important and finding the optimal balance
    between cost and performance is not a straightforward process. There are many
    instance types and sizes to choose from and ML engineers may end up having a suboptimal
    setup when deploying models in the SageMaker hosting services. The good news is
    that SageMaker has a capability called **SageMaker Inference Recommender**, which
    can help you decide which instance type to use. For more information, you can
    check out the following link: [https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml).'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用SageMaker Python SDK的`deploy()`方法部署机器学习模型时，我们被赋予了指定实例类型的权限。为模型选择正确的实例类型很重要，在成本和性能之间找到最佳平衡并不是一个简单的过程。有众多实例类型和大小可供选择，机器学习工程师在SageMaker托管服务中部署模型时可能会遇到次优配置。好消息是，SageMaker有一个名为**SageMaker推理推荐器**的功能，可以帮助您决定使用哪种实例类型。更多信息，您可以查看以下链接：[https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml)。
- en: 'Now that our real-time inference endpoint is running, let’s perform a sample
    prediction using the `predict()` method:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们的实时推理端点正在运行，让我们使用`predict()`方法进行一次样本预测：
- en: '[PRE111]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: This should yield an output value of `'POSITIVE'`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生一个输出值`'POSITIVE'`。
- en: 'Let’s also test a negative scenario:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也测试一个负面场景：
- en: '[PRE115]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: This should yield an output value of `'NEGATIVE'`. Feel free to test different
    values before deleting the endpoint in the next step.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生一个输出值`'NEGATIVE'`。在下一步删除端点之前，您可以自由测试不同的值。
- en: 'Finally, let’s delete the inference endpoint using the `delete_endpoint()`
    method:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们使用`delete_endpoint()`方法删除推理端点：
- en: '[PRE119]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: This will help us avoid any unexpected charges for unused inference endpoints.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这将帮助我们避免任何未使用的推理端点的意外费用。
- en: Wasn’t that easy? Deploying a pre-trained model to a real-time inference endpoint
    (inside an ML instance with the specified instance type) using the SageMaker Python
    SDK is so straightforward! A lot of the engineering work has been automated for
    us and all we need to do is call the `Model` object’s `deploy()` method.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 难道不是很容易吗？使用SageMaker Python SDK将预训练模型部署到实时推理端点（在指定实例类型的ML实例内部）非常简单！大量的工程工作已经为我们自动化了，我们只需要调用`Model`对象的`deploy()`方法。
- en: Deploying a pre-trained model to a serverless inference endpoint
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将预训练模型部署到无服务器推理端点
- en: In the initial chapters of this book, we’ve worked with several serverless services
    that allow us to manage and reduce costs. If you are wondering whether there’s
    a serverless option when deploying ML models in SageMaker, then the answer to
    that would be a sweet yes. When you are dealing with intermittent and unpredictable
    traffic, using serverless inference endpoints to host your ML model can be a more
    cost-effective option. Let’s say that we can tolerate **cold starts** (where a
    request takes longer to process after periods of inactivity) and we only expect
    a few requests per day – then, we can make use of a serverless inference endpoint
    instead of the real-time option. Real-time inference endpoints are best used when
    we can maximize the inference endpoint. If you’re expecting your endpoint to be
    utilized most of the time, then the real-time option may do the trick.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几章中，我们使用了几种无服务器服务，这些服务允许我们管理和降低成本。如果您想知道在SageMaker中部署ML模型时是否有无服务器选项，那么对这个问题的答案将是美妙的肯定。当您处理间歇性和不可预测的流量时，使用无服务器推理端点托管您的ML模型可以是一个更经济的选择。假设我们可以容忍**冷启动**（在一段时间的不活跃后，请求处理时间更长）并且我们每天只期望有少量请求，那么我们可以使用无服务器推理端点而不是实时选项。实时推理端点最好用于我们可以最大化推理端点的情况。如果您预计您的端点大部分时间都会被使用，那么实时选项可能就足够了。
- en: '![Figure 7.11 – The desired file and folder structure ](img/B18638_07_011.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11 – 所需的文件和文件夹结构](img/B18638_07_011.jpg)'
- en: Figure 7.11 – The desired file and folder structure
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 所需的文件和文件夹结构
- en: 'Deploying a pre-trained ML model to a serverless inference endpoint using the
    SageMaker Python SDK is similar to how it is done for real-time inference endpoints.
    The only major differences would be the following:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SageMaker Python SDK将预训练ML模型部署到无服务器推理端点的方式与部署到实时推理端点的方式相似。唯一的主要区别如下：
- en: The initialization of the `ServerlessInferenceConfig` object
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ServerlessInferenceConfig`对象的初始化'
- en: Passing this object as an argument when calling the `Model` object’s `deploy()`
    method
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在调用`Model`对象的`deploy()`方法时传递此对象作为参数
- en: 'In the next set of steps, we will use the SageMaker Python SDK to deploy our
    pre-trained model to a serverless inference endpoint:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将使用SageMaker Python SDK将我们的预训练模型部署到无服务器推理端点：
- en: In `CH07` directory and create a new Notebook using the `Data Science` image.
    Rename the notebook `03 - Deploying a serverless inference endpoint.ipynb`.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`CH07`目录下创建一个新的笔记本，使用`Data Science`镜像。将笔记本重命名为`03 - 部署无服务器推理端点.ipynb`。
- en: Note
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The new notebook should be next to `01 - Prepare model.tar.gz file.ipynb`, similar
    to what is shown in *Figure 7.11*.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 新笔记本应位于`01 - 准备model.tar.gz文件.ipynb`旁边，类似于*图7.11*中所示。
- en: 'In the first cell of the new notebook, let’s run the following block of code
    to load the variable values for `model_data`, `s3_bucket`, and `prefix`:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新笔记本的第一个单元中，让我们运行以下代码块来加载`model_data`、`s3_bucket`和`prefix`变量的值：
- en: '[PRE120]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: If you get an error when running this block of code, make sure that you have
    completed the steps specified in the *Preparing the pre-trained model artifacts*
    section of this chapter.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在运行此代码块时出现错误，请确保您已完成了本章“*准备预训练模型工件*”部分中指定的步骤。
- en: 'Prepare the IAM execution role to be used by SageMaker:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备SageMaker使用的IAM执行角色：
- en: '[PRE123]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'Initialize and configure the `ServerlessInferenceConfig` object:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化和配置`ServerlessInferenceConfig`对象：
- en: '[PRE125]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Initialize the `PyTorchModel` object and use the `deploy()` method to deploy
    the model to a serverless inference endpoint:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`PyTorchModel`对象并使用`deploy()`方法将模型部署到无服务器推理端点：
- en: '[PRE130]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: Note
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The model deployment should take around 3 to 8 minutes to complete.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署应大约需要3到8分钟才能完成。
- en: 'Now that our real-time inference endpoint is running, let’s perform a sample
    prediction using the `predict()` method:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们实时推理端点正在运行，让我们使用`predict()`方法进行一次样本预测：
- en: '[PRE148]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: This should yield an output value of `'POSITIVE'`.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生一个输出值为`'POSITIVE'`。
- en: 'Let’s also test a negative scenario:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE152]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: This should yield an output value of `'NEGATIVE'`. Feel free to test different
    values before deleting the endpoint in the next step.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s delete the inference endpoint using the `delete_endpoint()`
    method:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE156]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: This will help us avoid any unexpected charges for unused inference endpoints.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, everything is almost the same, except for the initialization
    and usage of the `ServerlessInferenceConfig` object. When using a serverless endpoint,
    SageMaker manages the compute resources for us and performs the following automatically:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Auto-assigns compute resources proportional to the `memory_size_in_mb` parameter
    value we specified when initializing `ServerlessInferenceConfig`
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses the configured maximum concurrency value to manage how many concurrent
    invocations can happen at the same time
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scales down the resources automatically to zero if there are no requests
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you see more examples of how to use the SageMaker Python SDK, you’ll start
    to realize how well this SDK has been designed and implemented.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a pre-trained model to an asynchronous inference endpoint
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to real-time and serverless inference endpoints, SageMaker also
    offers a third option when deploying models – **asynchronous inference endpoints**.
    Why is it called asynchronous? For one thing, instead of expecting the results
    to be available immediately, requests are queued, and results are made available
    *asynchronously*. This works for ML requirements that involve one or more of the
    following:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Large input payloads (up to 1 GB)
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A long prediction processing duration (up to 15 minutes)
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A good use case for asynchronous inference endpoints would be for ML models
    that are used to detect objects in large video files (which may take more than
    60 seconds to complete). In this case, an inference may take a few minutes instead
    of a few seconds.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '*How do we use asynchronous inference endpoints?* To invoke an asynchronous
    inference endpoint, we do the following:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: The request payload is uploaded to an Amazon S3 bucket.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The S3 path or location (where the request payload is stored) is used as the
    parameter value when calling the `predict_async()` method of the `AsyncPredictor`
    object (which maps or represents the ML inference endpoint).
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upon invocation of the endpoint, the asynchronous inference endpoint queues
    the request for processing (once the endpoint can).
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After processing the request, the output inference results are stored and uploaded
    to the output S3 location.
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An SNS notification (for example, a success or error notification) is sent (if
    set up).
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we will deploy our NLP model to an asynchronous inference endpoint.
    To emulate a delay, we’ll call the `sleep()` function in our inference script
    so that the prediction step takes longer than usual. Once we can get this relatively
    simple setup to work, working on more complex requirements such as object detection
    for video files will definitely be easier.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – The file and folder structure ](img/B18638_07_012.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – The file and folder structure
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: To get this setup to work, we will need to prepare a file that contains an input
    payload similar to that shown in *Figure 7.12* (for example, *data* or `input.json`).
    Once the input file has been prepared, we will upload it to an Amazon S3 bucket
    and then proceed with deploying our pre-trained ML model to an asynchronous inference
    endpoint.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, let’s proceed with creating the input JSON file!
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: Creating the input JSON file
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create a sample file containing the input
    JSON value that will be used when invoking the asynchronous inference endpoint
    in the next section:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 'Right-click on the empty space in the **File Browser** sidebar pane to open
    a context menu similar to that shown in *Figure 7.13*:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Creating a new folder ](img/B18638_07_013.jpg)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Creating a new folder
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you are in the `CH07` directory in **File Browser** before performing
    this step.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Rename the folder `data`.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Double-click the `data` folder in the **File Browser** sidebar pane to navigate
    to the directory.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new text file by clicking the **File** menu and choosing **Text File**
    from the list of options under the **New** submenu:'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Creating a new text file ](img/B18638_07_014.jpg)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Creating a new text file
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you are in the `data` directory when creating a new text file,
    similar to that in *Figure 7.14*.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'Rename the file `input.json`, as in *Figure 7.15*:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Renaming the text file ](img/B18638_07_015.jpg)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Renaming the text file
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: To rename the `untitled.txt` file, right-click on the file in the `input.json`)
    to replace the default name value.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `input.json` file with the following JSON value:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE157]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: Make sure to save your changes by pressing *CTRL* + *S*.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a Mac, use *CMD* + *S* instead. Alternatively, you can just
    click **Save Python File** under the list of options under the **File** menu.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Again, the input file is only needed when we’re planning to deploy our ML model
    to an asynchronous inference endpoint. With this prepared, we can now proceed
    to the next set of steps.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Adding an artificial delay to the inference script
  id: totrans-429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before proceeding with using the SageMaker Python SDK to deploy our pre-trained
    model to an asynchronous inference endpoint, we will add an artificial delay to
    the prediction step. This will help us emulate inference or prediction requests
    that take a bit of time to complete.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: When troubleshooting an asynchronous inference endpoint, you may opt to test
    an ML model that performs predictions within just a few seconds first. This will
    help you know right away if there’s something wrong since the output is expected
    to be uploaded to the S3 output path within a few seconds (instead of a few minutes).
    That said, you may want to remove the artificial delay temporarily if you’re having
    issues getting the setup to work.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will update the `inference.py` script to add a
    30-second delay when performing a prediction:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing where we left off in the previous section, let’s navigate to the
    `CH07` directory in **File Browser**:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Navigating to the CH07 directory ](img/B18638_07_016.jpg)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Navigating to the CH07 directory
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: Here, we click the `CH07` link, as highlighted in *Figure 7.16*.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'Double-click the `scripts` folder, as shown in *Figure 7.17*, to navigate to
    the directory:'
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Navigating to the scripts directory ](img/B18638_07_017.jpg)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – Navigating to the scripts directory
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that you have completed the hands-on steps in the *Preparing the
    SageMaker script mode prerequisites* section before proceeding with the next step.
    The `scripts` directory should contain three files:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '`inference.py`'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requirements.txt`'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setup.py`'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Double-click and open the `inference.py` file, as highlighted in *Figure 7.18*.
    Locate the `predict_fn()` function and uncomment the line of code containing `sleep(30)`:'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18638_07_018.jpg)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Updating the inference.py file
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: To uncomment the line of code, simply remove the hash and the empty space (`#`
    ) before `sleep(30)`, similar to what we can see in *Figure 7.18*.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to save the changes by pressing *CTRL* + *S*.
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a Mac, use *CMD* + *S* instead. Alternatively, you can just
    click **Save Python File** under the list of options under the **File** menu.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have finished adding an artificial 30-second delay, let’s proceed
    with using the SageMaker Python SDK to deploy our asynchronous inference endpoint.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and testing an asynchronous inference endpoint
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deploying a pre-trained ML model to an asynchronous inference endpoint using
    the SageMaker Python SDK is similar to how it is done for real-time and serverless
    inference endpoints. The only major differences would be (1) the initialization
    of the `AsyncInferenceConfig` object, and (2) passing this object as an argument
    when calling the `Model` object’s `deploy()` method.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will use the SageMaker Python SDK to deploy our
    pre-trained model to an asynchronous inference endpoint:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: Continuing where we left off in the *Adding an artificial delay to the inference
    script section*, let’s navigate to the `CH07` directory in `Data Science` image.
    Rename the notebook `04 - Deploying an asynchronous inference endpoint.ipynb`.
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: The new notebook should be next to `01 - Prepare model.tar.gz file.ipynb`.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first cell of the new notebook, let’s run the following block of code
    to load the variable values for `model_data`, `s3_bucket`, and `prefix`:'
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE158]'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: If you get an error when running this block of code, make sure that you have
    completed the steps specified in the *Preparing the pre-trained model artifacts*
    section of this chapter.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the path where we will upload the inference input file:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE161]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'Upload the `input.json` file to the S3 bucket using the `aws s3 cp` command:'
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE165]'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'Prepare the IAM execution role for use by SageMaker:'
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE166]'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: 'Initialize the `AsyncInferenceConfig` object:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE168]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: '[PRE170]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: While initializing the `AsyncInferenceConfig` object, we specify the value for
    the `output_path` parameter where the results will be saved.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s initialize the `PyTorchModel` object:'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE173]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '[PRE175]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: Here, we specify the configuration values for the parameters, such as `model_data`,
    `role`, `source_dir`, `entry_point`, `framework_version`, and `py_version`.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `deploy()` method to deploy the model to an asynchronous inference
    endpoint:'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE182]'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '[PRE184]'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE184]'
- en: '[PRE185]'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE185]'
- en: '[PRE186]'
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '[PRE187]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '[PRE190]'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE191]'
- en: Here, we specify the `AsyncInferenceConfig` object we initiated in a previous
    step as the parameter value to `async_inference_config`.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Deploying an asynchronous inference endpoint ](img/B18638_07_019.jpg)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Deploying an asynchronous inference endpoint
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.19*, we can see that the `deploy()` method accepts the parameter
    value for SageMaker to configure an asynchronous inference endpoint instead of
    a real-time inference endpoint.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: The model deployment should take around 3 to 8 minutes to complete.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the inference endpoint is ready, let’s use the `predict_async()` method
    to perform the prediction:'
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE192]'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: This should invoke the asynchronous inference endpoint using the data stored
    in the `input.json` file stored in S3.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – How the predict_async() method works ](img/B18638_07_020.jpg)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – How the predict_async() method works
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.20*, we can see that the input payload for an asynchronous inference
    endpoint comes from an S3 bucket. Then, after the endpoint processes the request,
    the output is saved to S3\. This would probably not make any sense if your input
    payload were small (for example, less than *1 MB*). However, if the input payload
    involves larger files such as video files, then uploading this into S3 and utilizing
    an asynchronous inference endpoint for predictions would make a lot more sense.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `sleep()` function to wait for 40 seconds before calling the `get_result()`
    function of the `response` object:'
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE195]'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: This should yield an output value of `'POSITIVE'`.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: Why wait for 40 seconds? Since we added an artificial 30-second delay in the
    prediction step, we would have to wait for at least 30 seconds before the output
    file is available in the specified S3 location.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: 'Store the S3 path string value in the `output_path` variable:'
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE198]'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: 'Use the `aws s3 cp` command to download a copy of the output file to the Studio
    notebook instance:'
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE199]'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: 'Now that we have downloaded the output file, let’s use the `cat` command to
    check its contents:'
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE200]'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: This should give us an output value of `'POSITIVE'`, similar to what we obtained
    after using the `get_result()` method in an earlier step.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do a quick cleanup by deleting the copy of the output file using the
    `rm` command:'
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE201]'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE201]'
- en: 'Finally, let’s delete the inference endpoint using the `delete_endpoint()`
    method:'
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE202]'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: This will help us avoid any unexpected charges for unused inference endpoints.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that in a production setup, it is preferable to update
    the architecture to be more event-driven and utilize the `notification_config`
    parameter value must be updated with the appropriate dictionary of values when
    initializing the `AsyncInferenceConfig` object. For more information, feel free
    to check out the following link: [https://sagemaker.readthedocs.io/en/stable/overview.xhtml#sagemaker-asynchronous-inference](https://sagemaker.readthedocs.io/en/stable/overview.xhtml#sagemaker-asynchronous-inference).'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: What’s SNS? SNS is a fully managed messaging service that allows architectures
    to be event-driven. Messages from a source (*publisher*) can fan out and be sent
    across a variety of receivers (*subscribers*). If we were to configure the SageMaker
    asynchronous inference endpoint to push notification messages to SNS, then it
    is best if we also register and set up a subscriber that waits for a success (or
    error) notification message once the prediction step is completed. This subscriber
    then proceeds with performing a pre-defined operation once the results are available.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up
  id: totrans-539
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have completed working on the hands-on solutions of this chapter,
    it is time for us to clean up and turn off any resources we will no longer use.
    In the next set of steps, we will locate and turn off any remaining running instances
    in SageMaker Studio:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **Running Instances and Kernels** icon in the sidebar, as highlighted
    in *Figure 7.21*:'
  id: totrans-541
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Turning off the running instance ](img/B18638_07_021.jpg)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Turning off the running instance
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: Clicking the **Running Instances and Kernels** icon should open and show the
    running instances, apps, and terminals in SageMaker Studio.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: Turn off all running instances under **RUNNING INSTANCES** by clicking the **Shut
    down** button for each of the instances, as highlighted in *Figure 7.21*. Clicking
    the **Shut down** button will open a pop-up window verifying the instance shutdown
    operation. Click the **Shut down all** button to proceed.
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure to check for and delete all the running inference endpoints under
    **SageMaker resources** as well (if there are any):'
  id: totrans-546
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.22 – Checking the list of running inference endpoints ](img/B18638_07_022.jpg)'
  id: totrans-547
  prefs: []
  type: TYPE_IMG
- en: Figure 7.22 – Checking the list of running inference endpoints
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: To check whether there are running inference endpoints, click the **SageMaker
    resources** icon as highlighted in *Figure 7.22* and then select **Endpoints**
    from the list of options in the drop-down menu.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that this cleanup operation needs to be performed after
    using SageMaker Studio. These resources are not turned off automatically by SageMaker
    even during periods of inactivity.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are looking into other ways to reduce costs when running ML workloads
    in SageMaker, you can check how you can utilize other features and capabilities
    such as **SageMaker Savings Plans** (which helps reduce costs in exchange for
    a consistent usage commitment for a 1-year or 3-year term), **SageMaker Neo**
    (which helps optimize ML models for deployment, speeding up inference and reducing
    costs), and **SageMaker Inference Recommender** (which helps you select the best
    instance type for the inference endpoint through automated load testing). We won’t
    discuss these in further detail in this book, so feel free to check out the following
    link for more information on these topics: [https://docs.aws.amazon.com/sagemaker/latest/dg/inference-cost-optimization.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-cost-optimization.xhtml).'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: Deployment strategies and best practices
  id: totrans-553
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will discuss the relevant deployment strategies and best
    practices when using the SageMaker hosting services. Let’s start by talking about
    the different ways we can invoke an existing SageMaker inference endpoint. The
    solution we’ve been using so far involves the usage of the SageMaker Python SDK
    to invoke an existing endpoint:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE203]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE203]'
- en: Here, we initialize a `Predictor` object and point it to an existing inference
    endpoint during the initialization step. We then use the `predict()` method of
    this `Predictor` object to invoke the inference endpoint.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can also invoke the same endpoint using the **boto3** library,
    similar to what is shown in the following block of code:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE204]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: Here, we use the `invoke_endpoint()` method when performing predictions and
    inference using the existing ML inference endpoint. As you can see, even without
    the SageMaker Python SDK installed, we should be able to invoke an existing ML
    inference endpoint from an `POST` request using the `InvokeEndpoint` API.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: 'If your backend application code makes use of a language other than Python
    (for example, Ruby, Java, or JavaScript), then all you need to do is look for
    an existing SDK for that language along with the corresponding function or method
    to call. For more information, you can check out the following link containing
    the different tools, along with the SDKs available for each language: [https://aws.amazon.com/tools/](https://aws.amazon.com/tools/).'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several solutions possible if you want to prepare an HTTP API that
    invokes and interfaces with an existing SageMaker inference endpoint. Here’s a
    quick list of possible solutions:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: '*Option 1*: *Amazon API Gateway HTTP API + AWS Lambda function + boto3 + SageMaker
    ML inference endpoint* – The `boto3` library to invoke the SageMaker ML inference
    endpoint.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 2*: *AWS Lambda function + boto3 + SageMaker ML inference endpoint
    (Lambda function URLs)* – The AWS Lambda function is invoked directly from a Lambda
    function URL (which is a dedicated endpoint for triggering a Lambda function).
    The AWS Lambda function then uses the `boto3` library to invoke the SageMaker
    ML inference endpoint.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 3*: *Amazon API Gateway HTTP API + SageMaker ML inference endpoint
    (API Gateway mapping templates)* – The Amazon API Gateway HTTP API receives the
    HTTP request and invokes the SageMaker ML inference endpoint directly using the
    **API Gateway mapping templates** (without the usage of Lambda functions).'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 4*: *Custom container-based web application using a web framework (for
    example, Flask or Django) inside an EC2 instance + boto3 + SageMaker ML inference
    endpoint* – The web application (running inside a container in an `boto3` library
    to invoke the SageMaker ML Inference endpoint.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 5*: *Custom container-based web application using a web framework (for
    example, Flask or Django) inside an Elastic Container Service (ECS) + boto3 +
    SageMaker ML inference endpoint* – The web application (running inside a container
    using the `boto3` library to invoke the SageMaker ML inference endpoint.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 6*: *Custom container-based web application using a web framework (for
    example, Flask or Django) with Elastic Kubernetes Service (EKS) + boto3 + SageMaker
    ML inference endpoint* – The web application (running inside an `boto3` library
    to invoke the SageMaker ML inference endpoint.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Option 7*: *AWS AppSync (GraphQL API) + AWS Lambda function + boto3 + SageMaker
    ML inference endpoint* – The `boto3` library to invoke the SageMaker ML inference
    endpoint.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this is not an exhaustive list and there are definitely other ways
    to set up an HTTP API invoking an existing SageMaker inference endpoint. Of course,
    there are scenarios as well where we would want to invoke an existing inference
    endpoint directly from another AWS service resource. This would mean that we no
    longer need to prepare a separate HTTP API that serves as a middleman between
    the two services.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that we can also invoke a SageMaker inference endpoint
    directly from **Amazon Aurora**, **Amazon Athena**, **Amazon Quicksight**, or
    **Amazon Redshift**. In [*Chapter 4*](B18638_04.xhtml#_idTextAnchor079), *Serverless
    Data Management on AWS*, we used Redshift and Athena to query our data. In addition
    to the database queries already available using these services, we can perform
    ML inference directly using a syntax similar to that in the following block of
    code (a sample query for Athena):'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE205]'
  id: totrans-572
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: 'Here, we define and use a custom function that invokes an existing SageMaker
    inference endpoint for prediction when using Amazon Athena. For more information,
    feel free to check out the following resources and links:'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Athena** + **Amazon SageMaker**: [https://docs.aws.amazon.com/athena/latest/ug/querying-mlmodel.xhtml](https://docs.aws.amazon.com/athena/latest/ug/querying-mlmodel.xhtml).'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Redshift** + **Amazon SageMaker**: [https://docs.aws.amazon.com/redshift/latest/dg/machine_learning.xhtml](https://docs.aws.amazon.com/redshift/latest/dg/machine_learning.xhtml).'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Aurora** + **Amazon SageMaker**: [https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ml.xhtml](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ml.xhtml).'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon QuickSight** + **Amazon SageMaker**: [https://docs.aws.amazon.com/quicksight/latest/user/sagemaker-integration.xhtml](https://docs.aws.amazon.com/quicksight/latest/user/sagemaker-integration.xhtml).'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we want to deploy a model outside of the SageMaker hosting services, we
    can do that as well. For example, we can train our model using SageMaker and then
    download the `model.tar.gz` file from the S3 bucket containing the model artifact
    files generated during the training process. The model artifact files generated
    can be deployed outside of SageMaker, similar to how we deployed and invoked the
    model in [*Chapter 2*](B18638_02.xhtml#_idTextAnchor041), *Deep Learning AMIs*,
    and [*Chapter 3*](B18638_03.xhtml#_idTextAnchor060), *Deep Learning Containers*.
    At this point, you might ask yourself: why deploy ML models using the SageMaker
    hosting services? Here’s a quick list of things you can easily perform and set
    up if you were to deploy ML models in the SageMaker hosting services:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: Setting up automatic scaling (**autoscaling**) of the infrastructure resources
    (ML instances) used to host the ML model. Autoscaling automatically adds new ML
    instances when the traffic or workload increases and reduces the number of provisioned
    ML instances once the traffic or workload decreases.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying multiple ML models in a single inference endpoint using the **multi-model
    endpoint** (**MME**) and **multi-container endpoint** (**MCE**) support of SageMaker.
    It is also possible to set up a **serial inference pipeline** behind a single
    endpoint that involves a sequence of containers (for example, pre-processing,
    prediction and post-processing) used to process ML inference requests.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up **A/B testing** of ML models by distributing traffic to multiple
    variants under a single inference endpoint.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up automated model monitoring and monitor (1) data quality, (2) model
    quality, (3) bias drift, and (4) feature attribution drift with just a few lines
    of code using the SageMaker Python SDK. We will dive deeper into model monitoring
    in [*Chapter 8*](B18638_08.xhtml#_idTextAnchor172), *Model Monitoring and Management
    Solutions*.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **Elastic Inference** when deploying models to add inference acceleration
    to the SageMaker inference endpoint to improve throughput and decrease latency.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a variety of traffic shifting modes when performing blue/green deployments
    when updating the deployed model. We can use the **All-at-once** traffic shifting
    mode if we want to shift all the traffic from the old setup to the new setup all
    in one go. We can use the **Canary** traffic shifting mode if we want to shift
    the traffic from the old setup to the new setup in two steps. This involves only
    shifting a portion of the traffic in the first shift and shifting the remainder
    of the traffic in the second shift. Finally, we can use the **Linear** traffic
    shifting mode to iteratively shift the traffic from the old setup to the new setup
    in a predetermined number of steps.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up **CloudWatch** alarms along with the SageMaker auto-rollback configuration
    to automate the deployment rollback process.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these are relatively easy to set up if we are to use SageMaker for model
    deployment. When using these features and capabilities, all we would need to worry
    about would be the configuration step, as a big portion of the work has already
    been automated by SageMaker.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve been talking about the different options and solutions when deploying
    ML models in the cloud. Before ending this section, let’s quickly discuss ML model
    deployments on **edge devices** such as mobile devices and smart cameras. There
    are several advantages to this approach, including real-time prediction latency,
    privacy preservation, and cost reduction associated with network connectivity.
    Of course, there are challenges when running and managing ML models on edge devices
    due to the resource limitations involved such as compute and memory. These challenges
    can be solved with **SageMaker Edge Manager**, which is a capability that makes
    use of several other services, capabilities, and features (such as **SageMaker
    Neo**, **IoT Greengrass**, and **SageMaker Model Monitor**) when optimizing, running,
    monitoring, and updating ML models on edge devices. We won’t dive any deeper into
    the details so feel free to check out https://docs.aws.amazon.com/sagemaker/latest/dg/edge.xhtml
    for more information about this topic.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-588
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed and focused on several deployment options and
    solutions using SageMaker. We deployed a pre-trained model into three different
    types of inference endpoints – (1) a real-time inference endpoint, (2) a serverless
    inference endpoint, and (3) an asynchronous inference endpoint. We also discussed
    the differences of each approach, along with when each option is best used when
    deploying ML models. Toward the end of this chapter, we talked about some of the
    deployment strategies, along with the best practices when using SageMaker for
    model deployments.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deeper into **SageMaker Model Registry** and
    **SageMaker Model Monitor**, which are capabilities of SageMaker that can help
    us manage and monitor our models in production.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-591
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics covered in this chapter, feel free to check
    out the following resources:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: '*The Hugging Face DistilBERT model* ([https://huggingface.co/docs/transformers/model_doc/distilbert](https://huggingface.co/docs/transformers/model_doc/distilbert))'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SageMaker – Deploying Models for Inference* (https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.xhtml)'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SageMaker – Inference Recommender* ([https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml))'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SageMaker – Deployment guardrails* ([https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails.xhtml))'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4:Securing, Monitoring, and Managing Machine Learning Systems and Environments
  id: totrans-597
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, readers will learn how to properly secure, monitor, and manage
    production ML systems and deployed models.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B18638_08.xhtml#_idTextAnchor172), *Model Monitoring and Management
    Solutions*'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18638_09.xhtml#_idTextAnchor187), *Security, Governance, and
    Compliance Strategies*'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
