<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
<meta charset="utf-8"/>
<meta name="generator" content="packt"/>
<title>2 The Makridakis Competitions: M5 on Kaggle for Accuracy and Uncertainty</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet2.css"/>

</head>
<body>
<section id="the-makridakis-competitions-m5-on-kaggle-for-accuracy-and-uncertainty" class="level1 pkt" data-number="3">
<h1 data-number="3">2 The Makridakis Competitions: M5 on Kaggle for Accuracy and Uncertainty</h1>
<section id="join-our-book-community-on-discord-1" class="level2" data-number="3.1">
<h2 data-number="3.1">Join our book community on Discord</h2>
<p><a href="https://packt.link/EarlyAccessCommunity">https://packt.link/EarlyAccessCommunity</a></p>
<figure>
<img src="../media/file1.png" style="width:10em" />
</figure>
<p>Since 1982, Spyros Makridakis (<a href="https://mofc.unic.ac.cy/dr-spyros-makridakis/https://mofc.unic.ac.cy/dr-spyros-makridakis/">https://mofc.unic.ac.cy/dr-spyros-makridakis/https://mofc.unic.ac.cy/dr-spyros-makridakis/</a>) has involved groups of researchers from all over the world in forecasting challenges, called M competitions, in order to conduct comparisons of the efficacy of existing and new forecasting methods against different forecasting problems. For this reason, M competitions have always been completely open to both academics and practitioners. The competitions are probably the most cited and referenced event in the forecasting community and they have always highlighted the changing state of the art in forecasting methods. Each previous M competition has provided both researchers and practitioners not only with useful data to train and test their forecasting tools, but also with a series of discoveries and approaches that are revolutionizing the way forecasting are done.</p>
<p>The recent M5 competition (the M6 is just running as this chapter is being written) has been held on Kaggle and it has proved particularly significant in remarking the usefulness of gradient boosting methods when trying to solve a host of volume forecasts of retail products. In this chapter, focusing on the accuracy track, we deal with a time series problem from Kaggle competitions, and by replicating one of the top, yet simplest and most clear solutions, we intend to provide our readers with code and ideas to successfully handle any future forecasting competition that may appear on Kaggle.</p>
<ul>
<li>Apart from the competition pages, we found a lot of information regarding the competition and its dynamics in the following papers from the International Journal of Forecasting:</li>
<li>Makridakis, Spyros, Evangelos Spiliotis, and Vassilios Assimakopoulos. <em>The M5 competition: Background, organization, and implementation</em>. International Journal of Forecasting (2021).</li>
<li>Makridakis, Spyros, Evangelos Spiliotis, and Vassilios Assimakopoulos. "M5 accuracy competition: Results, findings, and conclusions." International Journal of Forecasting (2022).</li>
<li>Makridakis, Spyros, et al. "The M5 Uncertainty competition: Results, findings and conclusions." International Journal of Forecasting (2021).</li>
</ul>
</section>
<section id="understanding-the-competition-and-the-data-1" class="level2" data-number="3.2">
<h2 data-number="3.2">Understanding the competition and the data</h2>
<p>The competition ran from March to June 2020 and over 7,000 participants took part in it on Kaggle. The organizers arranged it into two separated tracks, one for point-wise prediction (accuracy track) and another one for estimating reliable values at different confidence intervals (uncertainty track)</p>
<p>Walmart provided the data. It consisted of 42,840 daily sales time series of items hierarchically arranged into departments, categories, and stores spread in three U.S. states (the series are somewhat correlated each other). Along with the sales, Walmart also provided accompanying information (exogenous variables, usually not often provided in forecasting problems) such as the prices of items, some calendar information, associated promotions or presence of other events affecting the sales.</p>
<p>Apart from Kaggle, the data is available, together with the datasets from previous M competition, at the address <a href="https://forecasters.org/resources/time-series-data/">https://forecasters.org/resources/time-series-data/</a>.</p>
<p>One interesting aspect of the competitions is that it dealt with consumer goods sales both fast moving and slow moving with many examples of the latest presenting intermittent sales (sales are often zero but for some rare cases). Intermittent series, though common in many industries, are still a challenging case in forecasting for many practitioners.</p>
<p>The competition timeline has been arranged in two parts. In the first, from the beginning of March 2020 to June 1<sup>st</sup>, competitors could train models on the range of days up to day 1,913 and score their submission on the public test set (ranging from day 1,914 to 1,941). After that date, until the end of the competition on July 1<sup>st</sup>, the public test set was made available as part of the training set, allowing participants to tune their models in order to predict from day 1,942 to 1969 (a time windows of 28 days, i.e. four weeks). In that period, submissions were not scored on the leaderboard.</p>
<p>The ratio behind such an arrangement of the competition was to allow teams initially to test their models on the leaderboard and to have grounds to share their best performing methods in notebooks and discussions. After the first phase, the organizers wanted to avoid having the leaderboard used for overfitting purposes or hyperparameter tuning of the models and they wanted to resemble a forecasting situation, as it would happen in the real world. In addition, the requirement to choose only one submission as the final one mirrored the same necessity for realism (in the real world you cannot use two distinct models predictions and choose the one that suits you the best afterwards).</p>
<p>As for as the data, we mentioned that the data has been provided by Walmart and it represents the USA market: it originated from 10 stores in California, Wisconsin and Texas. Specifically, the data it is made up by the sales of 3,049 products, organized into three categories (hobbies, food, and household) that can be divided furthermore into 7 departments each. Such hierarchical structure is certainly a challenge because you can model sale dynamics at the level of USA market, state market, single store, product category, category department and finally specific product. All these levels can also combine as different aggregates, which are something required to be predicted in the second track, the uncertainty track:</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Level id</strong></td>
<td><strong>Level description</strong></td>
<td><strong>Aggregation level</strong></td>
<td><strong>Number of series</strong></td>
</tr>
<tr class="even">
<td>1</td>
<td>All products, aggregated for all stores and states</td>
<td>Total</td>
<td>1</td>
</tr>
<tr class="odd">
<td>2</td>
<td>All products, aggregated for each state</td>
<td>State</td>
<td>3</td>
</tr>
<tr class="even">
<td>3</td>
<td>All products, aggregated for each store</td>
<td>Store</td>
<td>10</td>
</tr>
<tr class="odd">
<td>4</td>
<td>All products, aggregated for each category</td>
<td>Category</td>
<td>3</td>
</tr>
<tr class="even">
<td>5</td>
<td>All products, aggregated for each department</td>
<td>Department</td>
<td>7</td>
</tr>
<tr class="odd">
<td>6</td>
<td>All products, aggregated for each state and category</td>
<td>State-Category</td>
<td>9</td>
</tr>
<tr class="even">
<td>7</td>
<td>All products, aggregated for each state and department</td>
<td>State-Department</td>
<td>21</td>
</tr>
<tr class="odd">
<td>8</td>
<td>All products, aggregated for each store and category</td>
<td>Store-Category</td>
<td>30</td>
</tr>
<tr class="even">
<td>9</td>
<td>All products, aggregated for each store and department</td>
<td>Store-Department</td>
<td>70</td>
</tr>
<tr class="odd">
<td>10</td>
<td>Each product, aggregated for all stores/states</td>
<td>Product</td>
<td>3,049</td>
</tr>
<tr class="even">
<td>11</td>
<td>Each product, aggregated for each state</td>
<td>Product-State</td>
<td>9,147</td>
</tr>
<tr class="odd">
<td>12</td>
<td>Each product, aggregated for each store</td>
<td>Product-Store</td>
<td>30,490</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>Total</td>
<td>42,840</td>
</tr>
</tbody>
</table>
<p>From the point of view of time, the granularity is daily sales record and the covered the period spanning from 29 January 2011 to 19 June 2016 which equals to 1,969 days in total, 1,913 for training, 28 for validation – public leaderboard – 28 for test – private leaderboard. A forecasting horizon of 28 days is actually recognized in the retail sector as the proper horizon for handling stocks and re-ordering operations for most goods.</p>
<p>Let’s examine the different data you receive for the competition. You get <code>sales_train_evaluation.csv</code>, <code>sell_prices.csv</code> and <code>calendar.csv</code>. The one keeping the time series is <code>sales_train_evaluation.csv</code>. It is composed of fields that act as identifiers (<code>item_id</code>, <code>dept_id</code>, <code>cat_id</code>, <code>store_id</code>, and <code>state_id</code>) and columns from <code>d_1</code> to <code>d_1941</code> representing the sales of those days:</p>
<figure>
<img src="../media/file2.png" alt="Figure 2.1: The sales_train_evaluation.csv data" /><figcaption aria-hidden="true">Figure 2.1: The sales_train_evaluation.csv data</figcaption>
</figure>
<p><code>sell_prices.csv</code> contains instead information about the price of the items. The difficulty here is in joining the <code>wm_yr_wk</code> (the id of the week) with the columns in the training data:</p>
<figure>
<img src="../media/file3.png" alt="Figure 2.2: The sell_prices.csv data" /><figcaption aria-hidden="true">Figure 2.2: The sell_prices.csv data</figcaption>
</figure>
<p>The last file, <code>calendar.csv</code>, contains data relative to events that could have affected the sales:</p>
<figure>
<img src="../media/file4.png" alt="Figure 2.3: The calendar.csv data" /><figcaption aria-hidden="true">Figure 2.3: The calendar.csv data</figcaption>
</figure>
<p>Again, the main difficulty seems in joining the data to the columns in the training table. Anyway, here you can get an easy key to connect columns (the d field) with the <code>wm_yr_wk</code>. In addition, in the table we have represented different events that may have occurred on particular days as well as SNAP days that are special days when the nutrition assistance benefits called the Supplement Nutrition Assistance Program (SNAP) can be used.</p>
</section>
<section id="understanding-the-evaluation-metric-1" class="level2" data-number="3.3">
<h2 data-number="3.3">Understanding the Evaluation Metric</h2>
<p>The accuracy competition introduced a new evaluation metric: Weighted Root Mean Squared Scaled Error (WRMSSE). The metric evaluates the deviation of the of the point forecasts around the mean of the realized values of the series being predicted:</p>
<figure>
<img src="../media/file5.png" />
</figure>
<p>where:</p>
<p><em>n</em> is the length of the training sample</p>
<p><em>h</em> is the forecasting horizon (in our case it is <em>h</em>=28)</p>
<em>Y</em><sub><em>t</em></sub> is the sales value at time <em>t</em>,
<figure>
<img src="../media/file6.png" />
</figure>
is the predicted value at time <em>t</em>
<p>In the competition guidelines (<a href="https://mofc.unic.ac.cy/m5-competition/">https://mofc.unic.ac.cy/m5-competition/</a>), in regard of WRMSSE, it is remarked that:</p>
<ul>
<li>The denominator of RMSSE is computed only for the time-periods for which the examined product(s) are actively sold, i.e., the periods following the first non-zero demand observed for the series under evaluation</li>
<li>The measure is scale independent, meaning that it can be effectively used to compare forecasts across series with different scales.</li>
<li>In contrast to other measures, it can be safely computed as it does not rely on divisions with values that could be equal or close to zero (e.g. as done in percentage errors when <em>Y</em><sub><em>t</em></sub> = 0 or relative errors when the error of the benchmark used for scaling is zero).</li>
<li>The measure penalizes positive and negative forecast errors, as well as large and small forecasts, equally, thus being symmetric.</li>
</ul>
<p>A good explanation of the underlying workings of this is provided by this post from Alexander Soare (<a href="https://www.kaggle.com/alexandersoare">https://www.kaggle.com/alexandersoare</a>): <a href="https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273">https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273</a>. After having transformed the evaluation metric, Alexandre attributes improving performances to improving the ratio between the error in the predictions and the day-to-day variation of sales values. If the error is the same as the daily variations (ratio=1), it is likely that the model is not much better than a random guess based on historical variations. If your error is less than that, it is score in a quadratic way (because of the square root) as it approaches zero. Consequently, a WRMSSE of 0.5 corresponds to a ratio of 0.7 and a WRMSSE of 0.25 corresponds to a ratio of 0.5.</p>
<p>During the competition, many attempts have been made at using the metric not only for evaluation besides the leaderboard but also as an objective function. First of all, the Tweedie loss (implemented both in XGBoost and LightGBM) worked quite well for the problem because it could handle the skewed distributions of sales for most products (a lot of them also had intermittent sales and that is also handled finely by the Tweedie loss). The Poisson and Gamma distributions can be considered extreme cases of the Tweedie distribution: based on the parameter power, p, with p=1 you get a Poisson distribution and with p=2 a Gamma one. Such power parameter is actually the glue that keeps the mean and the variance of the distribution connected by the formula variance = k*mean**p. Using a power value between 1 and 2, you actually get a mix of Poisson and Gamma distributions which can fit very well the competition problem. Most of the Kagglers involved in the competition and using a GBM solution, actually have resorted to Tweedie loss.</p>
<p>In spite of Tweedie success, some other Kagglers, however, found interesting ways to implement an objective loss more similar to WRMSSE for their models:</p>
<p>* Martin Kovacevic Buvinic with his asymmetric loss: <a href="https://www.kaggle.com/code/ragnar123/simple-lgbm-groupkfold-cv/notebook">https://www.kaggle.com/code/ragnar123/simple-lgbm-groupkfold-cv/notebook</a></p>
<p>* Timetraveller using PyTorch Autograd to get gradient and hessian for any differentiable continuous loss function to be implemented in LighGBM: <a href="https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/152837">https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/152837</a></p>
</section>
<section id="examining-the-4th-place-solutions-ideas-from-monsaraida" class="level2" data-number="3.4">
<h2 data-number="3.4">Examining the 4th place solution’s ideas from Monsaraida</h2>
<p>There are many solutions available for the competition, to be mostly found on the competition Kaggle discussions pages. The top five methods of both challenges have also been gathered and published (but one because of proprietary rights) by the competition organizers themselves: <a href="https://github.com/Mcompetitions/M5-methods">https://github.com/Mcompetitions/M5-methods</a> (by the way, reproducing the results of the winning submissions was a prerequisite for the collection of a competition prize).</p>
<p>Noticeably, all the Kagglers that have placed in the higher ranks of the competitions have used, as their unique model type or in blended/stacked in ensembles, LightGBM because of its lesser memory usage and speed of computations that gave it an advantage in the competition because of the large amount of times series to process and predict. But there are also other reasons for its success. Contrary to classical methods based on ARIMA, it doesn’t require relying on the analysis of auto-correlation and in specifically figuring out the parameters for each single series in the problem. In addition, contrary to methods based on deep learning, it doesn’t require looking for improving complicated neural architectures or tuning large number of hyperparameters. The strength of the gradient boosting methods in time series problems (for extension of every other gradient boosting algorithm, such as for instance XGBoost) is to rely on feature engineering, creating the right number of features based on time lags, moving averages and averages from groupings of the series attributes. Then choosing the right objective function and doing some hyper-parameter tuning will suffice to obtain excellent results when the time series are enough long (for shorter series, classical statistical methods such as ARIMA or exponential smoothing are still the recommended choice).</p>
<blockquote>
<p>Another advantage of LightGBM and XGBoost against deep learning solutions in the competition was the Tweedie loss, not requiring any feature scaling (deep learning networks are particularly sensible to the scaling you use) and the speed of training that allowed faster iterations while testing feature engineering.</p>
</blockquote>
<p>Among all these available solutions, we found the one proposed by Monsaraida (Masanori Miyahara), a Japanese computer scientist, the most interesting one. He has proposed a simple and straightforward solution that has ranked four on the private leaderboard with a score of 0.53583. The solution uses just general features without prior selection (such as sales statistics, calendar, prices, and identifiers). Moreover, it uses a limited number of models of the same kind, using LightGBM gradient boosting, without resorting to any kind of blending, recursive modelling when predictions feed other hierarchically related predictions or multipliers that is choosing constants to fit the test set better. Here is a scheme taken from his presentation solution presentation to the M (<a href="https://github.com/Mcompetitions/M5-methods/tree/master/Code%20of%20Winning%20Methods/A4">https://github.com/Mcompetitions/M5-methods/tree/master/Code%20of%20Winning%20Methods/A4</a>) where it can be noted that he treats each of the ten stores by each of the four weeks to be looked into the future that in the end corresponds to producing 40 models:</p>
<figure>
<img src="../media/file7.png" alt="Figure 2.4: explanation by Monsaraida about the structure of his solution" /><figcaption aria-hidden="true">Figure 2.4: explanation by Monsaraida about the structure of his solution</figcaption>
</figure>
<p>Given that Monsaraida has kept his solution simple and practical, like in a real-world forecasting project, in this chapter, we will try to replicate his example by refactoring his code in order to run in Kaggle notebooks (we will handle the memory and the running time limitations by splitting the code into multiple notebooks). In this way, we intend to provide the readers with a simple and effective way, based on gradient boosting, to approach forecasting problems.</p>
</section>
<section id="computing-predictions-for-specific-dates-and-time-horizons" class="level2" data-number="3.5">
<h2 data-number="3.5">Computing predictions for specific dates and time horizons</h2>
<p>The plan for replicating Monsaraida’s solution is to create a notebook customizable by input parameters in order to produce the necessary processed data for train and test and the LightGBM models for predictions. The models, given data in the past, will be trained to learn to predict values in a specific number of days in the future. The best results can be obtained by having each model to learn to predict the values in a specific week range in the future. Since we have to predict up to 28 days ahead in the future, we need a model predicting from day +1 to day +7 in the future, then another one able to predict from day +8 to day +14, another from day +15 to +21 and finally another last one capable of handling predictions from day +22 to day +28. We will need a Kaggle notebook for each of these time ranges, thus we need four notebooks. Each of these notebooks will be trained to predict that future time span for each of the ten stores part of the competitions. In total, each notebook will produce ten models. All together, the notebooks will then produce forty models covering all the future range and all the stores.</p>
<p>Since we need to predict both for the public leaderboard and for the private one, it is necessary to repeat this process twice, stopping training at day 1,913 (predicting days from 1,914 to 1,941) for the public test set submission and at day 1,941 (predicting days from 1,942 5o 1,969) for the private one.</p>
<p>Given the current limitations for running Kaggle notebooks based on CPU, all these eight notebooks can be run in parallel (all the process taking almost 6 hours and a half). Each notebook can be distinguishable by others by its name, containing the parameters’ values relative to the last training day and the look-ahead horizon in days. An example of one of these notebooks can be found at: <a href="https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7">https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7</a>.</p>
<p>Let’s now examine together how the code has been arranged and what we can learn from Monsaraida’s solution.</p>
<p>We simply start by importing the necessary packages. You can just notice how, apart from NumPy and pandas, the only data science specialized package is LightGBM. You may also notice that we are going to use gc (garbage collection): that’s because we need to limit the amount of used memory by the script, and we frequently just collect and recycle the unused memory. As part of this strategy, we also frequently store away on disk models and data structures, instead of keeping them in-memory:</p>
<div class="C0-CodePACKT">
<pre><code>import numpy as np
import pandas as pd
import os
import random
import math
from decimal import Decimal as dec
import datetime
import time
import gc
import lightgbm as lgb
import pickle
import warnings
warnings.filterwarnings(“ignore”, category=UserWarning)</code></pre>
</div>
<p>As part of the strategy to limit the memory usage, we resort to the function to reduce pandas DataFrame described in the Kaggle book and initially developed by Arjan Groen during the Zillow competition (read the discussion <a href="https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/discussion/291844">https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/discussion/291844</a>):</p>
<div class="C0-CodePACKT">
<pre><code>def reduce_mem_usage(df, verbose=True):
    numerics = [&#39;int16&#39;, &#39;int32&#39;, &#39;int64&#39;, &#39;float16&#39;, &#39;float32&#39;, &#39;float64&#39;]
    start_mem = df.memory_usage().sum() / 1024**2    
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == &#39;int&#39;:
                if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)    
    end_mem = df.memory_usage().sum() / 1024**2
    if verbose: print(&#39;Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)&#39;.format(end_mem, 100 * (start_mem - end_mem) / start_mem))
    return df</code></pre>
</div>
<p>We keep on defining functions for this solution, because it helps splitting the solution into smaller parts and because it is easier to clean up all the used variables when you just return from a function (you keep only what you saved to disk, and you returned from the function). Our next function helps us to load all the data available and compress it:</p>
<div class="C0-CodePACKT">
<pre><code>def load_data():
    train_df = reduce_mem_usage(pd.read_csv(&quot;../input/m5-forecasting-accuracy/sales_train_evaluation.csv&quot;))
    prices_df = reduce_mem_usage(pd.read_csv(&quot;../input/m5-forecasting-accuracy/sell_prices.csv&quot;))
    calendar_df = reduce_mem_usage(pd.read_csv(&quot;../input/m5-forecasting-accuracy/calendar.csv&quot;))
    submission_df = reduce_mem_usage(pd.read_csv(&quot;../input/m5-forecasting-accuracy/sample_submission.csv&quot;))
    return train_df, prices_df, calendar_df, submission_df
train_df, prices_df, calendar_df, submission_df = load_data() </code></pre>
</div>
<p>After preparing the code to retrieve the data relative to prices, volumes, and calendar information, we proceed to prepare the first processing function that will have the role to create a basic table of information having <code>item_id</code>, <code>dept_id</code>, <code>cat_id</code>, <code>state_id</code> and <code>store_id</code> as row keys, a day column and values column containing the volumes. This is achieved starting from rows having all the days’ data columns by using the pandas command melt (<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html</a>). The command takes as reference the index of the DataFrame and then picks all the remaining features, placing their name on a column and their value on another one (<code>var_name</code> and <code>value_name</code> parameters help you define the name of these new columns). In this way, you can unfold a row representing the sales series of a certain item in a certain store into multiple rows each one representing a single day. The fact that the positional order of the unfolded columns is preserved guarantees that now your time series spans on the vertical axis (you can therefore apply furthermore transformations on it, such as moving means).</p>
<p>To give you an idea of what is happening, here is the <code>train_df</code> before the transformation with <code>pd.melt</code>. Notice how the volumes of the distinct days are column features:</p>
<figure>
<img src="../media/file8.png" alt="Figure 2.5: The training DataFrame" /><figcaption aria-hidden="true">Figure 2.5: The training DataFrame</figcaption>
</figure>
<p>After the transformation, you obtain a <code>grid_df</code> where the days have been distributed on separated days:</p>
<figure>
<img src="../media/file9.png" alt="Figure 2.5: Applying pd.melt to the training DataFrame" /><figcaption aria-hidden="true">Figure 2.5: Applying pd.melt to the training DataFrame</figcaption>
</figure>
<p>The feature d contains the reference to the columns that are not part of the index, in essence, all the features from <code>d_1</code> to <code>d_1935</code>. By simply removing the ‘d_’ prefix from its values and converting them to integers, you now have a day feature.</p>
<p>Apart from this, the code snippet also separates a holdout of the rows (your validation set) based on the time from the training ones. On the training part it will also add the rows necessary for your predictions based on the predict horizon you provide.</p>
<p>Here is the function that creates our basic feature template. As input, it takes the <code>train_df</code> DataFrame, it expects the day the train ends and the predict horizon (the number of days you want to predict in the future):</p>
<div class="C0-CodePACKT">
<pre><code>def generate_base_grid(train_df, end_train_day_x, predict_horizon):
    index_columns = [&#39;id&#39;, &#39;item_id&#39;, &#39;dept_id&#39;, &#39;cat_id&#39;, &#39;store_id&#39;, &#39;state_id&#39;]
    grid_df = pd.melt(train_df, id_vars=index_columns, var_name=&#39;d&#39;, value_name=&#39;sales&#39;)
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    grid_df[&#39;d_org&#39;] = grid_df[&#39;d&#39;]
    grid_df[&#39;d&#39;] = grid_df[&#39;d&#39;].apply(lambda x: x[2:]).astype(np.int16)
    time_mask = (grid_df[&#39;d&#39;] &gt; end_train_day_x) &amp;  (grid_df[&#39;d&#39;] &lt;= end_train_day_x + predict_horizon)
    holdout_df = grid_df.loc[time_mask, [&quot;id&quot;, &quot;d&quot;, &quot;sales&quot;]].reset_index(drop=True)
    holdout_df.to_feather(f&quot;holdout_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    del(holdout_df)
    gc.collect()
    grid_df = grid_df[grid_df[&#39;d&#39;] &lt;= end_train_day_x]
    grid_df[&#39;d&#39;] = grid_df[&#39;d_org&#39;]
    grid_df = grid_df.drop(&#39;d_org&#39;, axis=1)
    add_grid = pd.DataFrame()
    for i in range(predict_horizon):
        temp_df = train_df[index_columns]
        temp_df = temp_df.drop_duplicates()
        temp_df[&#39;d&#39;] = &#39;d_&#39; + str(end_train_day_x + i + 1)
        temp_df[&#39;sales&#39;] = np.nan
        add_grid = pd.concat([add_grid, temp_df])
    
    grid_df = pd.concat([grid_df, add_grid])
    grid_df = grid_df.reset_index(drop=True)
    
    for col in index_columns:
        grid_df[col] = grid_df[col].astype(&#39;category&#39;)
    
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    grid_df.to_feather(f&quot;grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    del(grid_df)
    gc.collect()</code></pre>
</div>
<p>After handling the function to create the basic feature template, we prepare a merge function for pandas DataFrames that will help to save memory space and avoid memory errors when handling large sets of data. Given two DataFrame, df1 and df2 and the set of foreign keys we need them to be merged, the function applies a left outer join between df1 and df2 without creating a new merged object but simply expanding the existent df1 DataFrame.</p>
<p>The function works first by extracting the foreign keys from df1, then merging the extracted keys with df2. In this way, the function creates a new DataFrame, called <code>merged_gf</code>, which is ordered as df1. At this point, we just assign the <code>merged_gf</code> columsn to df1. Internally, df1 will pick the reference to the internal data structures from <code>merged_gf</code>. Such an approach helps minimizing the memory usage because only the necessary used data is created at any time (there are no duplicates that can fill-up the memory). When the function returns df1, <code>merged_gf</code> is cancelled but for the data now used by df1.</p>
<p>Here is the code for this utility function:</p>
<div class="C0-CodePACKT">
<pre><code>def merge_by_concat(df1, df2, merge_on):
    merged_gf = df1[merge_on]
    merged_gf = merged_gf.merge(df2, on=merge_on, how=&#39;left&#39;)
    new_columns = [col for col in list(merged_gf) 
                   if col not in merge_on]
    df1[new_columns] = merged_gf[new_columns]
    return df1</code></pre>
</div>
<p>After this necessary step, we proceed to program a new function to process the data. This time we handle the prices data, a set of data containing the prices of each item by each store for all the weeks. Since it is important to figure out if we are talking about a new product appearing in a store or not, the function picks the first date of price availability (using the <code>wm_yr_wk</code> feature in the price table, representing the id of the week) and it copies it to our feature template.</p>
<p>Here is the code for processing the release dates:</p>
<div class="C0-CodePACKT">
<pre><code>def calc_release_week(prices_df, end_train_day_x, predict_horizon):
    index_columns = [&#39;id&#39;, &#39;item_id&#39;, &#39;dept_id&#39;, &#39;cat_id&#39;, &#39;store_id&#39;, &#39;state_id&#39;]
    
    grid_df = pd.read_feather(f&quot;grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    
    release_df = prices_df.groupby([&#39;store_id&#39;, &#39;item_id&#39;])[&#39;wm_yr_wk&#39;].agg([&#39;min&#39;]).reset_index()
    release_df.columns = [&#39;store_id&#39;, &#39;item_id&#39;, &#39;release&#39;]
    
    grid_df = merge_by_concat(grid_df, release_df, [&#39;store_id&#39;, &#39;item_id&#39;])
    
    del release_df
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    gc.collect()
    
    grid_df = merge_by_concat(grid_df, calendar_df[[&#39;wm_yr_wk&#39;, &#39;d&#39;]], [&#39;d&#39;])
    grid_df = grid_df.reset_index(drop=True)
    grid_df[&#39;release&#39;] = grid_df[&#39;release&#39;] - grid_df[&#39;release&#39;].min()
    grid_df[&#39;release&#39;] = grid_df[&#39;release&#39;].astype(np.int16)
    
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    grid_df.to_feather(f&quot;grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    del(grid_df)
    gc.collect()</code></pre>
</div>
<p>After having handles the day of the product appearance in a store, we definitely proceed to deal with the prices. In regard of each item, by each shop, we prepare basic price features telling:</p>
<ul>
<li>the actual price (normalized by the maximum)</li>
<li>the maximum price</li>
<li>the minimum price</li>
<li>the mean price</li>
<li>the standard deviation of the price</li>
<li>the number of different prices the item has taken</li>
<li>the number of items in the store with the same price</li>
</ul>
<p>Besides these basic descriptive statistics of prices, we also add some features to describe their dynamics for each item in a store based on different time granularities:</p>
<ul>
<li>the day momentum, i.e. the ratio before the actual price and its price the previous day</li>
<li>the month momentum, i.e. the ratio before the actual price and its average price the same month</li>
<li>the year momentum, i.e. the ratio before the actual price and its average price the same year</li>
</ul>
<p>Here we use two interesting and essential pandas methods for time series feature processing:</p>
<p>* shift : that can move the index forward or backward by n steps (<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html</a>)</p>
<p>* transform: that applied to a group by, fills a like-index feature with the transformed values (<a href="https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html">https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html</a>)</p>
<p>In addition, the decimal part of the price is processed as a feature, in order to reveal a situation when the item is sold at psychological pricing thresholds (e.g. $19.99 or £2.98 – see this discussion: <a href="https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/145011">https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/145011</a>). The function <code>math.modf</code> (<a href="https://docs.python.org/3.8/library/math.html#math.modf">https://docs.python.org/3.8/library/math.html#math.modf</a>) helps in doing so because it splits any floating-point number into the fractional and integer parts (a two-item tuple).</p>
<p>Finally, the resulting table is saved onto disk.</p>
<p>Here is the function doing all the feature engineering on prices:</p>
<div class="C0-CodePACKT">
<pre><code>def generate_grid_price(prices_df, calendar_df, end_train_day_x, predict_horizon):
    grid_df = pd.read_feather(f&quot;grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    prices_df[&#39;price_max&#39;] = prices_df.groupby([&#39;store_id&#39;, &#39;item_id&#39;])[&#39;sell_price&#39;].transform(&#39;max&#39;)
    prices_df[&#39;price_min&#39;] = prices_df.groupby([&#39;store_id&#39;, &#39;item_id&#39;])[&#39;sell_price&#39;].transform(&#39;min&#39;)
    prices_df[&#39;price_std&#39;] = prices_df.groupby([&#39;store_id&#39;, &#39;item_id&#39;])[&#39;sell_price&#39;].transform(&#39;std&#39;)
    prices_df[&#39;price_mean&#39;] = prices_df.groupby([&#39;store_id&#39;, &#39;item_id&#39;])[&#39;sell_price&#39;].transform(&#39;mean&#39;)
    prices_df[&#39;price_norm&#39;] = prices_df[&#39;sell_price&#39;] / prices_df[&#39;price_max&#39;]
    prices_df[&#39;price_nunique&#39;] = prices_df.groupby([&#39;store_id&#39;, &#39;item_id&#39;])[&#39;sell_price&#39;].transform(&#39;nunique&#39;)
    prices_df[&#39;item_nunique&#39;] = prices_df.groupby([&#39;store_id&#39;, &#39;sell_price&#39;])[&#39;item_id&#39;].transform(&#39;nunique&#39;)
    calendar_prices = calendar_df[[&#39;wm_yr_wk&#39;, &#39;month&#39;, &#39;year&#39;]]
    calendar_prices = calendar_prices.drop_duplicates(subset=[&#39;wm_yr_wk&#39;])
    prices_df = prices_df.merge(calendar_prices[[&#39;wm_yr_wk&#39;, &#39;month&#39;, &#39;year&#39;]], on=[&#39;wm_yr_wk&#39;], how=&#39;left&#39;)
    
    del calendar_prices
    gc.collect()
    
    prices_df[&#39;price_momentum&#39;] = prices_df[&#39;sell_price&#39;] / prices_df.groupby([&#39;store_id&#39;, &#39;item_id&#39;])[
        &#39;sell_price&#39;].transform(lambda x: x.shift(1))
    prices_df[&#39;price_momentum_m&#39;] = prices_df[&#39;sell_price&#39;] / prices_df.groupby([&#39;store_id&#39;, &#39;item_id&#39;, &#39;month&#39;])[
        &#39;sell_price&#39;].transform(&#39;mean&#39;)
    prices_df[&#39;price_momentum_y&#39;] = prices_df[&#39;sell_price&#39;] / prices_df.groupby([&#39;store_id&#39;, &#39;item_id&#39;, &#39;year&#39;])[
        &#39;sell_price&#39;].transform(&#39;mean&#39;)
    prices_df[&#39;sell_price_cent&#39;] = [math.modf(p)[0] for p in prices_df[&#39;sell_price&#39;]]
    prices_df[&#39;price_max_cent&#39;] = [math.modf(p)[0] for p in prices_df[&#39;price_max&#39;]]
    prices_df[&#39;price_min_cent&#39;] = [math.modf(p)[0] for p in prices_df[&#39;price_min&#39;]]
    del prices_df[&#39;month&#39;], prices_df[&#39;year&#39;]
    prices_df = reduce_mem_usage(prices_df, verbose=False)
    gc.collect()
    
    original_columns = list(grid_df)
    grid_df = grid_df.merge(prices_df, on=[&#39;store_id&#39;, &#39;item_id&#39;, &#39;wm_yr_wk&#39;], how=&#39;left&#39;)
    del(prices_df)
    gc.collect()
    
    keep_columns = [col for col in list(grid_df) if col not in original_columns]
    grid_df = grid_df[[&#39;id&#39;, &#39;d&#39;] + keep_columns]
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    grid_df.to_feather(f&quot;grid_price_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    del(grid_df)
    gc.collect()</code></pre>
</div>
<p>The next function computes the moon phase, giving back one of its eight phases (from new moon to waning crescent). Although moon phases shouldn’t directly influence any sales (weather conditions instead do, but we have no weather information in the data), they represent a periodic cycle of 29 and a half days which can well suit periodic shopping behaviors. There is an interesting discussion, with different hypothesis regarding why moon phases may work as a predictor, in this competition post: <a href="https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/154776">https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/154776</a>:</p>
<div class="C0-CodePACKT">
<pre><code>def get_moon_phase(d):  # 0=new, 4=full; 4 days/phase
    diff = datetime.datetime.strptime(d, &#39;%Y-%m-%d&#39;) - datetime.datetime(2001, 1, 1)
    days = dec(diff.days) + (dec(diff.seconds) / dec(86400))
    lunations = dec(&quot;0.20439731&quot;) + (days * dec(&quot;0.03386319269&quot;))
    phase_index = math.floor((lunations % dec(1) * dec(8)) + dec(&#39;0.5&#39;))
    return int(phase_index) &amp; 7</code></pre>
</div>
<p>The moon phase function is part of a general function for creating time-based features. The function takes the calendar dataset information and places it among the features. Such information contains events and their type as well as indication of the SNAP periods (a nutrition assistance benefit called the Supplement Nutrition Assistance Program – hence SNAP – to help lower-income families) that could drive furthermore sales of basic goods. The function also generates numeric features such as the day, the month, the year, the day of the week, the week in the month, if it is an end of week. Here is the code:</p>
<div class="C0-CodePACKT">
<pre><code>def generate_grid_calendar(calendar_df, end_train_day_x, predict_horizon):
    
    grid_df = pd.read_feather(
                f&quot;grid_df_{end_train_day_x}_to_{end_train_day_x +      
                predict_horizon}.feather&quot;)
    grid_df = grid_df[[&#39;id&#39;, &#39;d&#39;]]
    gc.collect()
    calendar_df[&#39;moon&#39;] = calendar_df.date.apply(get_moon_phase)
    # Merge calendar partly
    icols = [&#39;date&#39;,
             &#39;d&#39;,
             &#39;event_name_1&#39;,
             &#39;event_type_1&#39;,
             &#39;event_name_2&#39;,
             &#39;event_type_2&#39;,
             &#39;snap_CA&#39;,
             &#39;snap_TX&#39;,
             &#39;snap_WI&#39;,
             &#39;moon&#39;,
             ]
    grid_df = grid_df.merge(calendar_df[icols], on=[&#39;d&#39;], how=&#39;left&#39;)
    icols = [&#39;event_name_1&#39;,
             &#39;event_type_1&#39;,
             &#39;event_name_2&#39;,
             &#39;event_type_2&#39;,
             &#39;snap_CA&#39;,
             &#39;snap_TX&#39;,
             &#39;snap_WI&#39;]
    
    for col in icols:
        grid_df[col] = grid_df[col].astype(&#39;category&#39;)
    grid_df[&#39;date&#39;] = pd.to_datetime(grid_df[&#39;date&#39;])
    grid_df[&#39;tm_d&#39;] = grid_df[&#39;date&#39;].dt.day.astype(np.int8)
    grid_df[&#39;tm_w&#39;] = grid_df[&#39;date&#39;].dt.isocalendar().week.astype(np.int8)
    grid_df[&#39;tm_m&#39;] = grid_df[&#39;date&#39;].dt.month.astype(np.int8)
    grid_df[&#39;tm_y&#39;] = grid_df[&#39;date&#39;].dt.year
    grid_df[&#39;tm_y&#39;] = (grid_df[&#39;tm_y&#39;] - grid_df[&#39;tm_y&#39;].min()).astype(np.int8)
    grid_df[&#39;tm_wm&#39;] = grid_df[&#39;tm_d&#39;].apply(lambda x: math.ceil(x / 7)).astype(np.int8)
    grid_df[&#39;tm_dw&#39;] = grid_df[&#39;date&#39;].dt.dayofweek.astype(np.int8)
    grid_df[&#39;tm_w_end&#39;] = (grid_df[&#39;tm_dw&#39;] &gt;= 5).astype(np.int8)
                                                         
    del(grid_df[&#39;date&#39;])
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    grid_df.to_feather(f&quot;grid_calendar_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
                                                         
    del(grid_df)
    del(calendar_df)
    gc.collect()</code></pre>
</div>
<p>The following function instead just removes the <code>wm_yr_wk</code> feature and transforms the d (day) feature into a numeric. It is a necessary step for the following feature transformation functions.</p>
<div class="C0-CodePACKT">
<pre><code>def modify_grid_base(end_train_day_x, predict_horizon):
    grid_df = pd.read_feather(f&quot;grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    grid_df[&#39;d&#39;] = grid_df[&#39;d&#39;].apply(lambda x: x[2:]).astype(np.int16)
    del grid_df[&#39;wm_yr_wk&#39;]
    
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    grid_df.to_feather(f&quot;grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    
    del(grid_df)
    gc.collect()</code></pre>
</div>
<p>Our last two feature creation functions will generate more sophisticated feature engineering for time series. The first function will produce both lagged sales and their moving averages. Fist, using the shift method (<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html</a>) will generate a range of lagged sales up to 15 days in the past. Then using shift in conjunction with rolling (<a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html</a>) will create moving means with windows 7, 14, 30, 60 and 180 days.</p>
<p>The shift command is necessary because it will allow moving the index so that you will always consider available data for your calculations. Hence, if your prediction horizon goes up to seven days, the calculations will consider only the data available seven days before. Then the rolling command will create a moving window observations that can be summarized (in this case by the mean). Having a mean over a period (the moving window) and following its evolutions will help you to detect better any changes in trends because patterns not repeating across the time windows will be leveled off. This a common strategy in time series analysis to remove noise and non-interesting patters. For instance, with a rolling mean of seven days you will cancel all the daily patterns and just represent what happens to your sales on a weekly basis.</p>
<blockquote>
<p>Can you experiment with different moving average windows? Also trying different strategies may help. For instance, by exploring the Tabular Playground of January 2022 (<a href="https://www.kaggle.com/competitions/tabular-playground-series-jan-2022">https://www.kaggle.com/competitions/tabular-playground-series-jan-2022</a>) devoted to time series you may find furthermore idea since most solutions are built using Gradient Boosting.</p>
</blockquote>
<p>Here is the code to generate the lag and rolling mean features:</p>
<div class="C0-CodePACKT">
<pre><code>def generate_lag_feature(end_train_day_x, predict_horizon):
    grid_df = pd.read_feather(f&quot;grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    grid_df = grid_df[[&#39;id&#39;, &#39;d&#39;, &#39;sales&#39;]]
    
    num_lag_day_list = []
    num_lag_day = 15
    for col in range(predict_horizon, predict_horizon + num_lag_day):
        num_lag_day_list.append(col)
   
    grid_df = grid_df.assign(**{
        &#39;{}_lag_{}&#39;.format(col, l): grid_df.groupby([&#39;id&#39;])[&#39;sales&#39;].transform(lambda x: x.shift(l))
        for l in num_lag_day_list
    })
    for col in list(grid_df):
        if &#39;lag&#39; in col:
            grid_df[col] = grid_df[col].astype(np.float16)
    num_rolling_day_list = [7, 14, 30, 60, 180]
    for num_rolling_day in num_rolling_day_list:
        grid_df[&#39;rolling_mean_&#39; + str(num_rolling_day)] = grid_df.groupby([&#39;id&#39;])[&#39;sales&#39;].transform(
            lambda x: x.shift(predict_horizon).rolling(num_rolling_day).mean()).astype(np.float16)
        grid_df[&#39;rolling_std_&#39; + str(num_rolling_day)] = grid_df.groupby([&#39;id&#39;])[&#39;sales&#39;].transform(
            lambda x: x.shift(predict_horizon).rolling(num_rolling_day).std()).astype(np.float16)
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    grid_df.to_feather(f&quot;lag_feature_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    
    del(grid_df)
    gc.collect()</code></pre>
</div>
<p>As for as the second advanced feature engineering function, it is an encoding function, taking specific groupings of variables among state, store, category, department, and sold item and representing their mean and standard deviation. Such embeddings are time independent (time is not part of the grouping) and they have the role to help the training algorithm to distinguish how items, categories, and stores (and their combinations) differentiate among themselves.</p>
<blockquote>
<p>The proposed embeddings are quite easy to compute using target encoding, as described in <em>The Kaggle Book</em> on page 216, can you obtain better results and how?</p>
</blockquote>
<p>The code works by grouping the features, computing their descriptive statistic (the mean or the standard deviation in our case) and then applying the results to the dataset using the transform (<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html</a>) method that we discussed before:</p>
<div class="C0-CodePACKT">
<pre><code>def generate_target_encoding_feature(end_train_day_x, predict_horizon):
    grid_df = pd.read_feather(f&quot;grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    
    grid_df.loc[grid_df[&#39;d&#39;] &gt; (end_train_day_x - predict_horizon), &#39;sales&#39;] = np.nan
    base_cols = list(grid_df)
    icols = [
        [&#39;state_id&#39;],
        [&#39;store_id&#39;],
        [&#39;cat_id&#39;],
        [&#39;dept_id&#39;],
        [&#39;state_id&#39;, &#39;cat_id&#39;],
        [&#39;state_id&#39;, &#39;dept_id&#39;],
        [&#39;store_id&#39;, &#39;cat_id&#39;],
        [&#39;store_id&#39;, &#39;dept_id&#39;],
        [&#39;item_id&#39;],
        [&#39;item_id&#39;, &#39;state_id&#39;],
        [&#39;item_id&#39;, &#39;store_id&#39;]
    ]
    for col in icols:
        col_name = &#39;_&#39; + &#39;_&#39;.join(col) + &#39;_&#39;
        grid_df[&#39;enc&#39; + col_name + &#39;mean&#39;] = grid_df.groupby(col)[&#39;sales&#39;].transform(&#39;mean&#39;).astype(
            np.float16)
        grid_df[&#39;enc&#39; + col_name + &#39;std&#39;] = grid_df.groupby(col)[&#39;sales&#39;].transform(&#39;std&#39;).astype(
            np.float16)
    keep_cols = [col for col in list(grid_df) if col not in base_cols]
    grid_df = grid_df[[&#39;id&#39;, &#39;d&#39;] + keep_cols]
    grid_df = reduce_mem_usage(grid_df, verbose=False)
    grid_df.to_feather(f&quot;target_encoding_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    
    del(grid_df)
    gc.collect()</code></pre>
</div>
<p>Having completed the feature engineering part, we now proceed to put together all the files we have stored away on disk while generating the features. The following function just loads the different datasets of basic features, price features, calendar features, lag/rolling and embedded features, and concatenate all together. The code then filters only the rows relative to a specific shop to be saved as a separated dataset. Such an approach matches the strategy of having a model trained on a specific store aimed at predicting for a specific time interval:</p>
<div class="C0-CodePACKT">
<pre><code>def assemble_grid_by_store(train_df, end_train_day_x, predict_horizon):
    grid_df = pd.concat([pd.read_feather(f&quot;grid_df_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;),
                     pd.read_feather(f&quot;grid_price_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;).iloc[:, 2:],
                     pd.read_feather(f&quot;grid_calendar_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;).iloc[:, 2:]],
                     axis=1)
    gc.collect()
    store_id_set_list = list(train_df[&#39;store_id&#39;].unique())
    index_store = dict()
    for store_id in store_id_set_list:
        extract = grid_df[grid_df[&#39;store_id&#39;] == store_id]
        index_store[store_id] = extract.index.to_numpy()
        extract = extract.reset_index(drop=True)
        extract.to_feather(f&quot;grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    del(grid_df)
    gc.collect()
    
    mean_features = [
        &#39;enc_cat_id_mean&#39;, &#39;enc_cat_id_std&#39;,
        &#39;enc_dept_id_mean&#39;, &#39;enc_dept_id_std&#39;,
        &#39;enc_item_id_mean&#39;, &#39;enc_item_id_std&#39;
        ]
    df2 = pd.read_feather(f&quot;target_encoding_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)[mean_features]
    for store_id in store_id_set_list:
        df = pd.read_feather(f&quot;grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
        df = pd.concat([df, df2[df2.index.isin(index_store[store_id])].reset_index(drop=True)], axis=1)
        df.to_feather(f&quot;grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    del(df2)
    gc.collect()
    
    df3 = pd.read_feather(f&quot;lag_feature_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;).iloc[:, 3:]
    for store_id in store_id_set_list:
        df = pd.read_feather(f&quot;grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
        df = pd.concat([df, df3[df3.index.isin(index_store[store_id])].reset_index(drop=True)], axis=1)
        df.to_feather(f&quot;grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
    del(df3)
    del(store_id_set_list)
    gc.collect()</code></pre>
</div>
<p>The following function, instead, just further processes the selection from the previous one, by removing unused features and reordering the columns and it returns the data for a model to be trained on:</p>
<div class="C0-CodePACKT">
<pre><code>def load_grid_by_store(end_train_day_x, predict_horizon, store_id):
    df = pd.read_feather(f&quot;grid_full_store_{store_id}_{end_train_day_x}_to_{end_train_day_x + predict_horizon}.feather&quot;)
                          
    remove_features = [&#39;id&#39;, &#39;state_id&#39;, &#39;store_id&#39;, &#39;date&#39;, &#39;wm_yr_wk&#39;, &#39;d&#39;, &#39;sales&#39;]
    enable_features = [col for col in list(df) if col not in remove_features]
    df = df[[&#39;id&#39;, &#39;d&#39;, &#39;sales&#39;] + enable_features]
    df = reduce_mem_usage(df, verbose=False)
    gc.collect()
                          
    return df, enable_features</code></pre>
</div>
<p>Finally, we can now deal with the training phase. The following code snippet starts by defining the training parameters as explicated by Monsaraida being the most effective on the problem. For training time reasons, we just modified the boosting type, choosing using goss instead of gbdt because that can really speed up training without much loss in terms of performance. A good speed-up to the model is also provided by the subsample parameter and the feature fraction: at each learning step of the gradient boosting only half of the examples and half of the features will be considered.</p>
<blockquote>
<p>Also compiling LightGBM on your machine with the right compiling options may increase your speed as explained in this interesting competition discussion: <a href="https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273">https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273</a></p>
</blockquote>
<p>The Tweedie loss, with a power value of 1.1 (hence with an underlying distribution quite similar to Poisson) seems particularly effective in modeling intermittent series (where zero sales prevail). The used metric is just the root mean squared error (there is no necessity to use a custom metric for representing the competition metric). We also use the <code>force_row_wise</code> parameter in order to save memory in the Kaggle notebook. All the other parameters are exactly the ones presented by Monsaraida in his solution (apart from the subsampling parameter that has been disabled because of its incompatibility with the goss boosting type).</p>
<blockquote>
<p>In what other Kaggle competition the Tweedie loss has proven useful? Can you find useful discussions about this loss and its usage in Meta Kaggle by exploring the ForumTopics and ForumMessages tables (<a href="https://www.kaggle.com/datasets/kaggle/meta-kaggle">https://www.kaggle.com/datasets/kaggle/meta-kaggle</a>)?</p>
</blockquote>
<p>After defining the training parameters, we just iterate over the stores, each time uploading the training data of a single store and training the LightGBM model. Each model is the pickled. We also extract feature importance from each model in order to consolidate it into a file and then aggregate it resulting into having for each feature the mean importance across all the stores for that prediction horizon.</p>
<p>Here is the complete function for training all the models for a specific prediction horizon:</p>
<div class="C0-CodePACKT">
<pre><code>def train(train_df, seed, end_train_day_x, predict_horizon):
    
    lgb_params = {
            &#39;boosting_type&#39;: &#39;goss&#39;,
            &#39;objective&#39;: &#39;tweedie&#39;,
            &#39;tweedie_variance_power&#39;: 1.1,
            &#39;metric&#39;: &#39;rmse&#39;,
             #&#39;subsample&#39;: 0.5,
             #&#39;subsample_freq&#39;: 1,
            &#39;learning_rate&#39;: 0.03,
            &#39;num_leaves&#39;: 2 ** 11 - 1,
            &#39;min_data_in_leaf&#39;: 2 ** 12 - 1,
            &#39;feature_fraction&#39;: 0.5,
            &#39;max_bin&#39;: 100,
            &#39;boost_from_average&#39;: False,
            &#39;num_boost_round&#39;: 1400,
            &#39;verbose&#39;: -1,
            &#39;num_threads&#39;: os.cpu_count(),
            &#39;force_row_wise&#39;: True,
        }
    random.seed(seed)
    np.random.seed(seed)
    os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
    
    lgb_params[&#39;seed&#39;] = seed
    store_id_set_list = list(train_df[&#39;store_id&#39;].unique())
    print(f&quot;training stores: {store_id_set_list}&quot;)
    
    feature_importance_all_df = pd.DataFrame()
    for store_index, store_id in enumerate(store_id_set_list):
        print(f&#39;now training {store_id} store&#39;)
        grid_df, enable_features = load_grid_by_store(end_train_day_x, predict_horizon, store_id)
        train_mask = grid_df[&#39;d&#39;] &lt;= end_train_day_x
        valid_mask = train_mask &amp; (grid_df[&#39;d&#39;] &gt; (end_train_day_x - predict_horizon))
        preds_mask = grid_df[&#39;d&#39;] &gt; (end_train_day_x - 100)
        train_data = lgb.Dataset(grid_df[train_mask][enable_features],
                                 label=grid_df[train_mask][&#39;sales&#39;])
        valid_data = lgb.Dataset(grid_df[valid_mask][enable_features],
                                 label=grid_df[valid_mask][&#39;sales&#39;])
        # Saving part of the dataset for later predictions
        # Removing features that we need to calculate recursively
        grid_df = grid_df[preds_mask].reset_index(drop=True)
        grid_df.to_feather(f&#39;test_{store_id}_{predict_horizon}.feather&#39;)
        del(grid_df)
        gc.collect()
        
        estimator = lgb.train(lgb_params,
                              train_data,
                              valid_sets=[valid_data],
                              callbacks=[lgb.log_evaluation(period=100, show_stdv=False)],
                              )
        model_name = str(f&#39;lgb_model_{store_id}_{predict_horizon}.bin&#39;)
        feature_importance_store_df = pd.DataFrame(sorted(zip(enable_features, estimator.feature_importance())),
                                                   columns=[&#39;feature_name&#39;, &#39;importance&#39;])
        feature_importance_store_df = feature_importance_store_df.sort_values(&#39;importance&#39;, ascending=False)
        feature_importance_store_df[&#39;store_id&#39;] = store_id
        feature_importance_store_df.to_csv(f&#39;feature_importance_{store_id}_{predict_horizon}.csv&#39;, index=False)
        feature_importance_all_df = pd.concat([feature_importance_all_df, feature_importance_store_df])
        pickle.dump(estimator, open(model_name, &#39;wb&#39;))
        del([train_data, valid_data, estimator])
        gc.collect()
    feature_importance_all_df.to_csv(f&#39;feature_importance_all_{predict_horizon}.csv&#39;, index=False)
    feature_importance_agg_df = feature_importance_all_df.groupby(
        &#39;feature_name&#39;)[&#39;importance&#39;].agg([&#39;mean&#39;, &#39;std&#39;]).reset_index()
    feature_importance_agg_df.columns = [&#39;feature_name&#39;, &#39;importance_mean&#39;, &#39;importance_std&#39;]
    feature_importance_agg_df = feature_importance_agg_df.sort_values(&#39;importance_mean&#39;, ascending=False)
    feature_importance_agg_df.to_csv(f&#39;feature_importance_agg_{predict_horizon}.csv&#39;, index=False)</code></pre>
</div>
<p>With the last function prepared, we got all the necessary code up for our pipeline to work. For the function wrapping the whole operations together, we need the input datasets (the time series dataset, the price dataset, the calendar information) together with the last training day (1,913 for predicting on the public leaderboard, 1,941 for the private one) and the predict horizon (which could be 7, 14, 21 or 28 days).</p>
<div class="C0-CodePACKT">
<pre><code>def train_pipeline(train_df, prices_df, calendar_df,  
                   end_train_day_x_list, prediction_horizon_list):
    
    for end_train_day_x in end_train_day_x_list:
        
        for predict_horizon in prediction_horizon_list:
            
            print(f&quot;end training point day: {end_train_day_x} - prediction horizon: {predict_horizon} days&quot;)
            # Data preparation
            generate_base_grid(train_df, end_train_day_x, predict_horizon)
            calc_release_week(prices_df, end_train_day_x, predict_horizon)
            generate_grid_price(prices_df, calendar_df, end_train_day_x, predict_horizon)
            generate_grid_calendar(calendar_df, end_train_day_x, predict_horizon)
            modify_grid_base(end_train_day_x, predict_horizon)
            generate_lag_feature(end_train_day_x, predict_horizon)
            generate_target_encoding_feature(end_train_day_x, predict_horizon)
            assemble_grid_by_store(train_df, end_train_day_x, predict_horizon)
            # Modelling
            train(train_df, seed, end_train_day_x, predict_horizon)</code></pre>
</div>
<p>Since Kaggle notebook have a limited running time and a limited amount of both memory and disk space, our suggested strategy is to replicate four notebooks with the code hereby presented and train them with different prediction horizon parameters. Using the same name for the notebooks but for a part containing the value of the prediction parameter will help gathering and handling the models later as external datasets in another notebook.</p>
<p>Here is the first notebook, m5-train-day-1941-horizon-7 (<a href="https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7">https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7</a>):</p>
<div class="C0-CodePACKT">
<pre><code>end_train_day_x_list = [1941]
prediction_horizon_list = [7]
seed = 42
train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list)</code></pre>
</div>
<p>The second notebook, m5-train-day-1941-horizon-14 (<a href="https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-14">https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-14</a>):</p>
<div class="C0-CodePACKT">
<pre><code>end_train_day_x_list = [1941]
prediction_horizon_list = [14]
seed = 42
train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list)</code></pre>
</div>
<p>The third notebook, m5-train-day-1941-horizon-21 (<a href="https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-21">https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-21</a>):</p>
<div class="C0-CodePACKT">
<pre><code>end_train_day_x_list = [1941]
prediction_horizon_list = [21]
seed = 42
train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list)</code></pre>
</div>
<p>Finally the last one, m5-train-day-1941-horizon-28 (<a href="https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-28">https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-28</a>):</p>
<div class="C0-CodePACKT">
<pre><code>end_train_day_x_list = [1941]
prediction_horizon_list = [28]
seed = 42
train_pipeline(train_df, prices_df, calendar_df, end_train_day_x_list, prediction_horizon_list)</code></pre>
</div>
<p>If you are working on a local computer with enough disk space and memory resources, you can just run all the four prediction horizons together, by using as an input the list containing them all: [7, 14, 21, 28]. Now the last step before being able to submit our prediction is assembling the predictions.</p>
</section>
<section id="assembling-public-and-private-predictions" class="level2" data-number="3.6">
<h2 data-number="3.6">Assembling public and private predictions</h2>
<p>You can see an example about how we assembled the predictions for both the public and private leaderboards here:</p>
<ul>
<li>Public leaderboard example: <a href="https://www.kaggle.com/lucamassaron/m5-predict-public-leaderboard">https://www.kaggle.com/lucamassaron/m5-predict-public-leaderboard</a></li>
<li>Private leaderboard example: <a href="https://www.kaggle.com/code/lucamassaron/m5-predict-private-leaderboard">https://www.kaggle.com/code/lucamassaron/m5-predict-private-leaderboard</a></li>
</ul>
<p>What changes between the public and private submissions is just the different last training day: it determinates what days we are going to predict.</p>
<p>In this conclusive code snippet, after loading the necessary packages, such as LightGBM, for every end of training day, and for every prediction horizon, we recover the correct notebook with its data. Then, we iterate through all the stores and predict the sales for all the items in the time ranging from the previous prediction horizon up to the present one. In this way, every model will predict on a single week, the one it has been trained on.</p>
<div class="C0-CodePACKT">
<pre><code>import numpy as np
import pandas as pd
import os
import random
import math
from decimal import Decimal as dec
import datetime
import time
import gc
import lightgbm as lgb
import pickle
import warnings
warnings.filterwarnings(&quot;ignore&quot;, category=UserWarning)
store_id_set_list = [&#39;CA_1&#39;, &#39;CA_2&#39;, &#39;CA_3&#39;, &#39;CA_4&#39;, &#39;TX_1&#39;, &#39;TX_2&#39;, &#39;TX_3&#39;, &#39;WI_1&#39;, &#39;WI_2&#39;, &#39;WI_3&#39;]
end_train_day_x_list = [1913, 1941]
prediction_horizon_list = [7, 14, 21, 28]
pred_v_all_df = list()
for end_train_day_x in end_train_day_x_list:
    previous_prediction_horizon = 0
    for prediction_horizon in prediction_horizon_list:
        notebook_name = f&quot;../input/m5-train-day-{end_train_day_x}-horizon-{prediction_horizon}&quot;
        pred_v_df = pd.DataFrame()
        
        for store_index, store_id in enumerate(store_id_set_list):
            
            model_path = str(f&#39;{notebook_name}/lgb_model_{store_id}_{prediction_horizon}.bin&#39;)
            print(f&#39;loading {model_path}&#39;)
            estimator = pickle.load(open(model_path, &#39;rb&#39;))
            base_test = pd.read_feather(f&quot;{notebook_name}/test_{store_id}_{prediction_horizon}.feather&quot;)
            enable_features = [col for col in base_test.columns if col not in [&#39;id&#39;, &#39;d&#39;, &#39;sales&#39;]]
            
            for predict_day in range(previous_prediction_horizon + 1, prediction_horizon + 1):
                print(&#39;[{3} -&gt; {4}] predict {0}/{1} {2} day {5}&#39;.format(
                store_index + 1, len(store_id_set_list), store_id,
                previous_prediction_horizon + 1, prediction_horizon, predict_day))
                mask = base_test[&#39;d&#39;] == (end_train_day_x + predict_day)
                base_test.loc[mask, &#39;sales&#39;] = estimator.predict(base_test[mask][enable_features])
                
            temp_v_df = base_test[
                    (base_test[&#39;d&#39;] &gt;= end_train_day_x + previous_prediction_horizon + 1) &amp;
                    (base_test[&#39;d&#39;] &lt; end_train_day_x + prediction_horizon + 1)
                    ][[&#39;id&#39;, &#39;d&#39;, &#39;sales&#39;]]
            
            if len(pred_v_df)!=0:
                pred_v_df = pd.concat([pred_v_df, temp_v_df])
            else:
                pred_v_df = temp_v_df.copy()
            
            del(temp_v_df)
            gc.collect()
        
        previous_prediction_horizon = prediction_horizon
        pred_v_all_df.append(pred_v_df)
    
pred_v_all_df = pd.concat(pred_v_all_df)</code></pre>
</div>
<p>When all the predictions have been gatherd, we merge them using the sample submission file as a reference, both for the required rows to be predicted and for the columns format (Kaggle expects distinct rows for items in the validation or testing periods with daily sales in progressive columns).</p>
<div class="C0-CodePACKT">
<pre><code>submission = pd.read_csv(&quot;../input/m5-forecasting-accuracy/sample_submission.csv&quot;)
pred_v_all_df.d = pred_v_all_df.d - end_train_day_x_list
pred_h_all_df = pred_v_all_df.pivot(index=&#39;id&#39;, columns=&#39;d&#39;, values=&#39;sales&#39;)
pred_h_all_df = pred_h_all_df.reset_index()
pred_h_all_df.columns = submission.columns
submission = submission[[&#39;id&#39;]].merge(pred_h_all_df, on=[&#39;id&#39;], how=&#39;left&#39;).fillna(0)
submission.to_csv(&quot;m5_predictions.csv&quot;, index=False)</code></pre>
</div>
<p>The solution can reach around 0.54907 in the private leaderboard, resulting in 12<sup>th</sup> position, in the gold medal area. Reverting back to Monsaraida’s LightGBM parameters (for instance using gbdt instead of goss for the boosting parameter) should result even in higher performances (but you would need to run the code in local computer or on the Google Cloud Platform).</p>
<blockquote>
<p><strong>Exercise</strong></p>
<blockquote>
<p>As an exercise, try comparing a training of LightGBM using the same number of iterations with the boosting set to gbdt instead of goss. How much is the difference in performance and training time (you may need to use a local machine or cloud one because the training may exceed the 12 hours)?</p>
</blockquote>
</blockquote>
</section>
<section id="summary-1" class="level2" data-number="3.7">
<h2 data-number="3.7">Summary</h2>
<p>In this second chapter, we took on quite a complex time series competition, hence the easiest top solution we tried, it is actually fairly complex, and it requires coding quite a lot of processing functions. After you went through the chapter, you should have a better idea of how to process time series and have them predicted using gradient boosting. Favoring gradient boosting solutions over traditional methods, when you have enough data, as with this problem, should help you create strong solutions for complex problems with hierarchical correlations, intermittent series and availability of covariates such as events or prices or market conditions. In the following chapters, you will tackle with even more complex Kaggle competitions, dealing with images and texts. You will be amazed at how much you can learn by recreating top-scoring solutions and understanding their inner workings.</p>
</section>
</section>
</body>
</html>
