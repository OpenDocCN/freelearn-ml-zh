<html><head></head><body>
<div class="Basic-Text-Frame" id="_idContainer123">
<h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 class="chapterTitle" id="_idParaDest-182"><span class="koboSpan" id="kobo.2.1">Open-Source ML Platforms</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">In the previous chapter, we covered how Kubernetes can be used as the foundational infrastructure for running ML tasks, such as running model training jobs or building data science environments such as </span><strong class="keyWord"><span class="koboSpan" id="kobo.4.1">Jupyter Notebook</span></strong><span class="koboSpan" id="kobo.5.1"> servers. </span><span class="koboSpan" id="kobo.5.2">However, to perform these tasks at scale and more efficiently for large organizations, you will need to build ML platforms with the capabilities to support the full data science lifecycle. </span><span class="koboSpan" id="kobo.5.3">These capabilities include scalable data science environments, model training services, model registries, and model deployment capabilities.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.6.1">In this chapter, we will discuss the core components of an ML platform and explore additional open-source technologies that can be used for building ML platforms. </span><span class="koboSpan" id="kobo.6.2">We will begin with technologies designed for building a data science environment capable of supporting a large number of users for experimentation. </span><span class="koboSpan" id="kobo.6.3">Subsequently, we will delve into various technologies for model training, model registries, model deployment, and ML pipeline automation.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.7.1">In a nutshell, the following topics are covered:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.8.1">Core components of an ML platform</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.9.1">Open-source technologies for building ML platforms</span></li>
</ul>
<h1 class="heading-1" id="_idParaDest-183"><span class="koboSpan" id="kobo.10.1">Core components of an ML platform</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.11.1">An </span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.12.1">ML platform </span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.13.1">is a complex system encompassing multiple environments for running distinct tasks and orchestrating complex workflow processes. </span><span class="koboSpan" id="kobo.13.2">Furthermore, an ML platform</span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.14.1"> needs to cater to a multitude of roles, including data scientists, ML engineers, infrastructure engineers, operations teams, and security and compliance stakeholders. </span><span class="koboSpan" id="kobo.14.2">To construct an ML platform, several components come into play. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.15.1">These</span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.16.1"> components include: </span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.17.1">Data science environment</span></strong><span class="koboSpan" id="kobo.18.1">: The</span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.19.1"> data science environment provides data analysis and ML tools, such as Jupyter notebooks, data sources and storage, code repositories, and ML frameworks. </span><span class="koboSpan" id="kobo.19.2">Data scientists and ML engineers use the data science environment to perform data analysis, run data science experiments, and build and tune models. </span><span class="koboSpan" id="kobo.19.3">The data science environment also provides collaboration capabilities, allowing data scientists to share and collaborate on code, data, experiments, and models.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.20.1">Model training environment</span></strong><span class="koboSpan" id="kobo.21.1">: The </span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.22.1">model training environment provides a separate infrastructure tailored to meet specific model training requirements. </span><span class="koboSpan" id="kobo.22.2">While data scientists and ML engineers can execute small-scale model training tasks directly within their local Jupyter environment, they need a separate dedicated infrastructure for large-scale model training. </span><span class="koboSpan" id="kobo.22.3">By utilizing a dedicated training infrastructure, organizations can exercise greater control over model training process management and model lineage management processes.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.23.1">Model registry</span></strong><span class="koboSpan" id="kobo.24.1">: Trained models need to be tracked and managed within a model registry. </span><span class="koboSpan" id="kobo.24.2">The </span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.25.1">model registry serves as a centralized repository for inventorying and managing models, ensuring effective lineage management, version control, model discovery, and comprehensive lifecycle management. </span><span class="koboSpan" id="kobo.25.2">This becomes particularly significant when dealing with a large number of models. </span><span class="koboSpan" id="kobo.25.3">Data scientists can register models directly in the registry as they perform experiments in their data science environment. </span><span class="koboSpan" id="kobo.25.4">Additionally, models can be registered as part of automated ML model pipeline executions, enabling streamlined and automated integration of models into the registry.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.26.1">Model serving environment</span></strong><span class="koboSpan" id="kobo.27.1">: To </span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.28.1">serve predictions from trained ML models to client applications, it is necessary to host the models within a dedicated model serving infrastructure that operates behind an API endpoint in real time. </span><span class="koboSpan" id="kobo.28.2">This infrastructure should also provide support for batch transform capabilities, allowing predictions to be processed in large batches. </span><span class="koboSpan" id="kobo.28.3">Several types of model serving frameworks are available to fulfill these requirements.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.29.1">ML pipeline development:</span></strong><span class="koboSpan" id="kobo.30.1"> To </span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.31.1">effectively manage the various ML components and stages in the lifecycle, it is crucial to incorporate capabilities that enable pipeline development to orchestrate ML training and prediction workflows. </span><span class="koboSpan" id="kobo.31.2">These pipelines play an important role in coordinating different stages, such as data preparation, model training, and evaluation.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.32.1">Model monitoring</span></strong><span class="koboSpan" id="kobo.33.1">: Robust model monitoring </span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.34.1">is crucial for maintaining the high performance of ML models in production. </span><span class="koboSpan" id="kobo.34.2">Continuous monitoring tracks metrics like prediction accuracy, data drift, latency, errors, and anomalies over time. </span><span class="koboSpan" id="kobo.34.3">Monitoring enables platform operators to detect production model degradation before it impacts users. </span><span class="koboSpan" id="kobo.34.4">When monitored metrics cross defined thresholds, alerts trigger investigative workflows and mitigation where needed. </span><span class="koboSpan" id="kobo.34.5">Effective monitoring also provides performance dashboards and visibility into all deployed models. </span><span class="koboSpan" id="kobo.34.6">This facilitates continuous improvement of models and allows replacing underperforming models proactively.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.35.1">ML feature management</span></strong><span class="koboSpan" id="kobo.36.1">: Managing features is a key capability in the ML lifecycle. </span><span class="koboSpan" id="kobo.36.2">Feature</span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.37.1"> management entails the ongoing curation, monitoring, and sharing of ML features to accelerate model development. </span><span class="koboSpan" id="kobo.37.2">This includes tools for discovery, lineage tracking, and governance of feature data. </span><span class="koboSpan" id="kobo.37.3">Centralized feature stores democratize access to high-quality features by teams across an organization. </span><span class="koboSpan" id="kobo.37.4">They provide a single source of truth, eliminating duplication of feature engineering efforts.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.38.1">Continuous integration</span></strong><span class="koboSpan" id="kobo.39.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.40.1">CI</span></strong><span class="koboSpan" id="kobo.41.1">)/</span><strong class="keyWord"><span class="koboSpan" id="kobo.42.1">continuous deployment</span></strong><span class="koboSpan" id="kobo.43.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.44.1">CD</span></strong><span class="koboSpan" id="kobo.45.1">) </span><strong class="keyWord"><span class="koboSpan" id="kobo.46.1">and workflow automation</span></strong><span class="koboSpan" id="kobo.47.1">: Finally, to </span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.48.1">streamline the data processing, model training, and model </span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.49.1">deployment processes on an ML platform, it is crucial to establish CI/CD practices, along with workflow automation capabilities. </span><span class="koboSpan" id="kobo.49.2">These practices and tools significantly contribute to increasing the velocity, consistency, reproducibility, and observability of ML deployments.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.50.1">In addition to these core components, there are several other platform architecture factors to consider when building an end-to-end ML platform. </span><span class="koboSpan" id="kobo.50.2">These factors include security and authentication, version control and reproducibility, and data management and governance. </span><span class="koboSpan" id="kobo.50.3">By integrating these additional architectural factors into the ML platform, organizations can enhance security, gain visibility into system operations, and enforce governance policies. </span><span class="koboSpan" id="kobo.50.4">In the following sections, we will explore various open-source technologies that can be used to build an end-to-end ML platform.</span></p>
<h1 class="heading-1" id="_idParaDest-184"><span class="koboSpan" id="kobo.51.1">Open-source technologies for building ML platforms</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.52.1">Managing ML tasks</span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.53.1"> individually by deploying standalone ML containers in a Kubernetes cluster can become challenging </span><a id="_idIndexMarker629"/><span class="koboSpan" id="kobo.54.1">when dealing with a large number of users and workloads. </span><span class="koboSpan" id="kobo.54.2">To address this complexity and enable efficient scaling, many open-source technologies have emerged as viable solutions. </span><span class="koboSpan" id="kobo.54.3">These technologies, including Kubeflow, MLflow, Seldon Core, GitHub, Feast, and Airflow, provide comprehensive support for building data science environments, model training services, model inference services, and ML workflow automation. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.55.1">Before delving into the technical details, let’s first explore why numerous organizations opt for open-source technologies to construct their ML platforms. </span><span class="koboSpan" id="kobo.55.2">For many, the appeal lies in the ability to tailor the platform to specific organizational needs and workflows, with open standards and interoperable components preventing vendor lock-in and allowing the flexibility to adopt new technologies over time. </span><span class="koboSpan" id="kobo.55.3">Leveraging popular open-source ML projects also taps into a rich talent pool, as many practitioners are already proficient with these technologies. </span><span class="koboSpan" id="kobo.55.4">Additionally, open-source allows complete control over the platform roadmap to internal teams, reducing dependence on a vendor’s priorities. </span><span class="koboSpan" id="kobo.55.5">When executed efficiently, an open-source stack can lead to cost savings for organizations, as there are no licensing costs associated with the software.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.56.1">Building an ML platform using open-source technology comes with notable advantages. </span><span class="koboSpan" id="kobo.56.2">However, it’s also crucial to consider the potential drawbacks. </span><span class="koboSpan" id="kobo.56.3">Challenges may arise from integration complexities, a lack of comprehensive support, security vulnerabilities, and potential limitations in features compared to commercial solutions. </span><span class="koboSpan" id="kobo.56.4">Additionally, the resource-intensive nature of maintaining an open-source platform, coupled with a potential learning curve for the team, could impact efficiency and total cost of ownership. </span><span class="koboSpan" id="kobo.56.5">Concerns about documentation quality, the absence of standardization, and the responsibility for updates and maintenance further underscore the need for careful consideration. </span><span class="koboSpan" id="kobo.56.6">You must weigh these factors against the benefits, taking into account their specific requirements, resources, and expertise before opting for an open-source approach.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.57.1">With these considerations in mind, let’s explore the design of core ML platform components using open-source technologies. </span></p>
<h2 class="heading-2" id="_idParaDest-185"><span class="koboSpan" id="kobo.58.1">Implementing a data science environment</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.59.1">Kubeflow </span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.60.1">is an </span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.61.1">open-source ML platform built on top of Kubernetes. </span><span class="koboSpan" id="kobo.61.2">It offers a set of tools and frameworks specifically designed to simplify the deployment, orchestration, and management of ML workloads. </span><span class="koboSpan" id="kobo.61.3">Kubeflow provides features like Jupyter notebooks for interactive data exploration and experimentation, distributed training capabilities, and model serving infrastructure. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.62.1">Core capabilities</span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.63.1"> of Kubeflow include:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.64.1">A central UI dashboard</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.65.1">A Jupyter Notebook server for code authoring and model building</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.66.1">A Kubeflow pipeline for ML pipeline orchestration</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.67.1">KFServing</span></strong><span class="koboSpan" id="kobo.68.1"> for model serving</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.69.1">Training operators for model training support</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.70.1">The following figure illustrates how Kubeflow can provide the various components needed for a data science environment. </span><span class="koboSpan" id="kobo.70.2">Specifically, we will delve into its support for Jupyter Notebook servers as it is the main building block for a data science environment.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.71.1"><img alt="Figure 7.1 – A Kubeflow-based data science environment " src="../Images/B20836_07_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.72.1">Figure 7.1: A Kubeflow-based data science environment</span></p>
<p class="normal"><span class="koboSpan" id="kobo.73.1">Kubeflow </span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.74.1">provides a</span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.75.1"> multi-tenant Jupyter Notebook server environment with built-in authentication and authorization support. </span><span class="koboSpan" id="kobo.75.2">Let’s discuss each of these core components in detail:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.76.1">Jupyter Notebook</span></strong><span class="koboSpan" id="kobo.77.1">: As a </span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.78.1">data scientist, you can take advantage of the Kubeflow Jupyter Notebook server, which offers a platform to author and run your </span><strong class="keyWord"><span class="koboSpan" id="kobo.79.1">Python</span></strong><span class="koboSpan" id="kobo.80.1"> code to explore data and build models inside the Jupyter notebook. </span><span class="koboSpan" id="kobo.80.2">With Kubeflow, you can spawn multiple notebook servers, each server associated with a single Kubernetes namespace that corresponds to a team, project, or individual user. </span><span class="koboSpan" id="kobo.80.3">Each notebook server runs a container inside a </span><strong class="keyWord"><span class="koboSpan" id="kobo.81.1">Kubernetes</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.82.1">Pod</span></strong><span class="koboSpan" id="kobo.83.1">. </span><span class="koboSpan" id="kobo.83.2">By default, a Kubeflow notebook server provides a list of notebook container images hosted in public container image repositories to choose from. </span><span class="koboSpan" id="kobo.83.3">Alternatively, you can create custom notebook container images to tailor to your specific requirements. </span><span class="koboSpan" id="kobo.83.4">To ensure standards and consistency, Kubeflow administrators can provide a list of standard images for users to use. </span><span class="koboSpan" id="kobo.83.5">When creating a notebook server, you select the namespace to run the notebook server in. </span><span class="koboSpan" id="kobo.83.6">Additionally, you specify the </span><strong class="keyWord"><span class="koboSpan" id="kobo.84.1">Universal Resource Identifier</span></strong><span class="koboSpan" id="kobo.85.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.86.1">URI</span></strong><span class="koboSpan" id="kobo.87.1">) of the container image for the notebook </span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.88.1">server. </span><span class="koboSpan" id="kobo.88.2">You also have the flexibility to specify the resource requirements, such as the number of CPUs/GPUs and memory size.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.89.1">Authentication and authorization</span></strong><span class="koboSpan" id="kobo.90.1">: You access the notebook server through the Kubeflow UI dashboard, which provides an authentication service through the Dex </span><strong class="keyWord"><span class="koboSpan" id="kobo.91.1">OpenID Connect</span></strong><span class="koboSpan" id="kobo.92.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.93.1">OIDC</span></strong><span class="koboSpan" id="kobo.94.1">) provider. </span><span class="koboSpan" id="kobo.94.2">Dex is an identity service that </span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.95.1">uses OIDC to provide authentication for other applications. </span><span class="koboSpan" id="kobo.95.2">Dex can federate with </span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.96.1">other authentication services such as the </span><strong class="keyWord"><span class="koboSpan" id="kobo.97.1">Active Directory</span></strong><span class="koboSpan" id="kobo.98.1"> service. </span><span class="koboSpan" id="kobo.98.2">Each notebook is associated with a default Kubernetes service account (</span><code class="inlineCode"><span class="koboSpan" id="kobo.99.1">default-editor</span></code><span class="koboSpan" id="kobo.100.1">) that can be used for entitlement purposes (such as granting the notebook permission to access various resources in the Kubernetes cluster). </span><span class="koboSpan" id="kobo.100.2">Kubeflow uses Istio </span><strong class="keyWord"><span class="koboSpan" id="kobo.101.1">role-based access control</span></strong><span class="koboSpan" id="kobo.102.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.103.1">RBAC</span></strong><span class="koboSpan" id="kobo.104.1">) to </span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.105.1">control in-cluster traffic. </span><span class="koboSpan" id="kobo.105.2">The following </span><strong class="keyWord"><span class="koboSpan" id="kobo.106.1">YAML</span></strong><span class="koboSpan" id="kobo.107.1"> file grants the </span><code class="inlineCode"><span class="koboSpan" id="kobo.108.1">default-editor</span></code><span class="koboSpan" id="kobo.109.1"> service account (which is associated with the Kubeflow notebook) access to the Kubeflow pipeline service by attaching the </span><code class="inlineCode"><span class="koboSpan" id="kobo.110.1">ml-pipeline-services</span></code><span class="koboSpan" id="kobo.111.1"> service role to it:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr"><span class="koboSpan" id="kobo.112.1">apiVersion:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.113.1">rbac.istio.io/v1alpha1</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.114.1">kind:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.115.1">ServiceRoleBinding</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.116.1">metadata:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.117.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.118.1">bind-ml-pipeline-nb-admin</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.119.1">namespace:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.120.1">kubeflow</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.121.1">spec:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.122.1">roleRef:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.123.1">kind:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.124.1">ServiceRole</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.125.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.126.1">ml-pipeline-services</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.127.1">subjects:</span></span>
<span class="hljs-bullet"><span class="koboSpan" id="kobo.128.1">-</span></span> <span class="hljs-attr"><span class="koboSpan" id="kobo.129.1">properties:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.130.1">source.principal:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.131.1">cluster.local/ns/admin/sa/default-editor</span></span>
</code></pre>
</li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.132.1">Multi-tenancy</span></strong><span class="koboSpan" id="kobo.133.1">: Kubeflow offers the capability for multiple users to access a shared Kubeflow environment while ensuring resource isolation. </span><span class="koboSpan" id="kobo.133.2">This is achieved by creating individual namespaces for each user and leveraging Kubernetes RBAC and Istio RBAC to manage access control for these namespaces and their associated resources. </span><span class="koboSpan" id="kobo.133.3">For collaborative work within teams, the owner of a namespace has the ability to grant access to other users directly from the Kubeflow dashboard UI. </span><span class="koboSpan" id="kobo.133.4">Using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.134.1">Manage Contributor</span></code><span class="koboSpan" id="kobo.135.1"> function, the namespace owner can specify which users are granted access to the namespace and its resources. </span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.136.1">In addition to</span><a id="_idIndexMarker640"/><span class="koboSpan" id="kobo.137.1"> the preceding core components, Kubeflow provides a mechanism for onboarding users to access different Kubeflow resources. </span><span class="koboSpan" id="kobo.137.2">To onboard a new Kubeflow user, you create a new user profile, which automatically generates a new namespace for the profile. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.138.1">The following YAML file, once applied using </span><code class="inlineCode"><span class="koboSpan" id="kobo.139.1">kubectl</span></code><span class="koboSpan" id="kobo.140.1">, creates a new user profile called </span><code class="inlineCode"><span class="koboSpan" id="kobo.141.1">test-user</span></code><span class="koboSpan" id="kobo.142.1"> with an email of </span><code class="inlineCode"><span class="koboSpan" id="kobo.143.1">test-user@kubeflow.org</span></code><span class="koboSpan" id="kobo.144.1">, and it also creates a new namespace called </span><code class="inlineCode"><span class="koboSpan" id="kobo.145.1">test-user</span></code><span class="koboSpan" id="kobo.146.1">:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr"><span class="koboSpan" id="kobo.147.1">apiVersion:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.148.1">kubeflow.org/v1beta1</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.149.1">kind:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.150.1">Profile</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.151.1">metadata:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.152.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.153.1">test-user</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.154.1">spec:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.155.1">owner:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.156.1">kind:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.157.1">User</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.158.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.159.1">test-user@kubeflow.org</span></span>
</code></pre>
<p class="normal"><span class="koboSpan" id="kobo.160.1">You can run the </span><code class="inlineCode"><span class="koboSpan" id="kobo.161.1">kubectl get profiles</span></code><span class="koboSpan" id="kobo.162.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.163.1">kubectl get namespaces</span></code><span class="koboSpan" id="kobo.164.1"> commands to verify that the profile and namespaces have been created.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.165.1">After a user is created and added to the Kubeflow Dex authentication service, the new user can log in to the Kubeflow dashboard and access the Kubeflow resources (such as a Jupyter Notebook server) under the newly created namespace.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.166.1">Utilizing Kubeflow for a data science environment poses several key challenges that one must be aware of before committing to its implementation. </span><span class="koboSpan" id="kobo.166.2">Installing Kubeflow on top of Kubernetes, whether on-premises or in the cloud on platforms like AWS, can be complicated, often requiring substantial configuration and debugging. </span><span class="koboSpan" id="kobo.166.3">The many moving parts make installation non-trivial. </span><span class="koboSpan" id="kobo.166.4">Kubeflow consists of many loosely coupled components, each with its own version. </span><span class="koboSpan" id="kobo.166.5">Orchestrating these diverse components across different versions to work together seamlessly as an integrated platform can present difficulties. </span><span class="koboSpan" id="kobo.166.6">There is a lack of documentation on Kubeflow. </span><span class="koboSpan" id="kobo.166.7">Kubeflow documentation often points to older component versions. </span><span class="koboSpan" id="kobo.166.8">The out-of-date documentation makes adoption more difficult as new Kubeflow users battle mismatches between docs and platform versions. </span><span class="koboSpan" id="kobo.166.9">Despite these limitations, Kubeflow is a highly recommended technology for building ML platforms due to its support for end-to-end pipelines, rich support for different ML frameworks, and portability. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.167.1">With that, we have reviewed how Kubeflow can be used to provide a multi-tenant Jupyter Notebook environment for experimentation and model building. </span><span class="koboSpan" id="kobo.167.2">Next, let’s see how to build a model training environment.</span></p>
<h2 class="heading-2" id="_idParaDest-186"><span class="koboSpan" id="kobo.168.1">Building a model training environment</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.169.1">As discussed earlier, within </span><a id="_idIndexMarker641"/><span class="koboSpan" id="kobo.170.1">an ML platform, it is common to provide a dedicated model training service and infrastructure to support large-scale and automated model training in an ML pipeline. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.171.1">This dedicated training service should be easily accessible from different components within the platform, such as the experimentation environment (such as a Jupyter notebook) as well as the ML automation pipeline.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.172.1">In a Kubernetes-based environment, there are two main approaches for model training you can choose from depending on your training needs:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.173.1">Model training using </span><strong class="keyWord"><span class="koboSpan" id="kobo.174.1">Kubernetes</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.175.1">Jobs</span></strong></li>
<li class="bulletList"><span class="koboSpan" id="kobo.176.1">Model training using </span><strong class="keyWord"><span class="koboSpan" id="kobo.177.1">Kubeflow</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.178.1">training operators</span></strong></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.179.1">Let’s take a closer look at each one of these approaches in detail:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.180.1">Model training using Kubernetes Jobs</span></strong><span class="koboSpan" id="kobo.181.1">: As we discussed in </span><em class="chapterRef"><span class="koboSpan" id="kobo.182.1">Chapter 6</span></em><span class="koboSpan" id="kobo.183.1">, </span><em class="italic"><span class="koboSpan" id="kobo.184.1">Kubernetes Container Orchestration Infrastructure Management</span></em><span class="koboSpan" id="kobo.185.1">, a Kubernetes Job creates one or more containers and runs them through to completion. </span><span class="koboSpan" id="kobo.185.2">This pattern is well suited for running certain types of ML model training jobs, as an ML job runs a training loop to completion and does not run forever. </span><span class="koboSpan" id="kobo.185.3">For example, you can package a container with a Python training script and all the dependencies that train a model and use the Kubernetes Job to load the container and kick off the training script. </span><span class="koboSpan" id="kobo.185.4">When the script completes and exits, the Kubernetes Job also ends. </span><span class="koboSpan" id="kobo.185.5">The following sample YAML file kicks off a model training job if submitted with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.186.1">kubectl apply</span></code><span class="koboSpan" id="kobo.187.1"> command:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr"><span class="koboSpan" id="kobo.188.1">apiVersion:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.189.1">batch/v1</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.190.1">kind:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.191.1">Job</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.192.1">metadata:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.193.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.194.1">train-churn-job</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.195.1">spec:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.196.1">template:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.197.1">spec:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.198.1">containers:</span></span>
<span class="hljs-bullet"><span class="koboSpan" id="kobo.199.1">-</span></span> <span class="hljs-attr"><span class="koboSpan" id="kobo.200.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.201.1">train-container</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.202.1">imagePullPolicy:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.203.1">Always</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.204.1">image:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.205.1">&lt;model</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.206.1">training</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.207.1">uri&gt;</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.208.1">command:</span></span><span class="koboSpan" id="kobo.209.1"> [</span><span class="hljs-string"><span class="koboSpan" id="kobo.210.1">"python"</span></span><span class="koboSpan" id="kobo.211.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.212.1">"train.py"</span></span><span class="koboSpan" id="kobo.213.1">]
      </span><span class="hljs-attr"><span class="koboSpan" id="kobo.214.1">restartPolicy:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.215.1">Never</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.216.1">backoffLimit:</span></span> <span class="hljs-number"><span class="koboSpan" id="kobo.217.1">4</span></span>
</code></pre>
<p class="normal"><span class="koboSpan" id="kobo.218.1">To </span><a id="_idIndexMarker642"/><span class="koboSpan" id="kobo.219.1">query the status of the job and see the detailed training logs, you can run the </span><code class="inlineCode"><span class="koboSpan" id="kobo.220.1">kubectl get jobs</span></code><span class="koboSpan" id="kobo.221.1"> command and the </span><code class="inlineCode"><span class="koboSpan" id="kobo.222.1">kubectl logs &lt;pod name&gt;</span></code><span class="koboSpan" id="kobo.223.1"> command, respectively.</span></p></li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.224.1">Model training using Kubeflow training operators</span></strong><span class="koboSpan" id="kobo.225.1">: A Kubernetes Job can launch a model training container and run a training script inside the container to completion. </span><span class="koboSpan" id="kobo.225.2">Since the controller for a Kubernetes Job does not have application-specific knowledge about the training job, it can only handle generic Pod deployment and management for the running jobs, such as running the container in a Pod, monitoring the Pod, and handling generic Pod failure. </span><span class="koboSpan" id="kobo.225.3">However, some model training jobs, such as distributed training jobs in a cluster, require the special deployment, monitoring, and maintenance of stateful communications among various Pods. </span><span class="koboSpan" id="kobo.225.4">This is where the Kubernetes training operator pattern can be applied.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.226.1">Kubeflow offers a list of pre-built training operators (such as the </span><strong class="keyWord"><span class="koboSpan" id="kobo.227.1">TensorFlow</span></strong><span class="koboSpan" id="kobo.228.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.229.1">PyTorch</span></strong><span class="koboSpan" id="kobo.230.1">, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.231.1">XGBoost</span></strong><span class="koboSpan" id="kobo.232.1"> operators) for complex model training jobs. </span><span class="koboSpan" id="kobo.232.2">Each Kubeflow training operator has a </span><strong class="keyWord"><span class="koboSpan" id="kobo.233.1">custom resource</span></strong><span class="koboSpan" id="kobo.234.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.235.1">CR</span></strong><span class="koboSpan" id="kobo.236.1">) (for example, </span><code class="inlineCode"><span class="koboSpan" id="kobo.237.1">TFJob CR</span></code><span class="koboSpan" id="kobo.238.1"> for TensorFlow jobs) that defines the training job’s specific configurations, such as the type of Pod in the training job (for example, </span><code class="inlineCode"><span class="koboSpan" id="kobo.239.1">master</span></code><span class="koboSpan" id="kobo.240.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.241.1">worker</span></code><span class="koboSpan" id="kobo.242.1">, or </span><code class="inlineCode"><span class="koboSpan" id="kobo.243.1">parameter server</span></code><span class="koboSpan" id="kobo.244.1">), or runs policies on how to clean up resources and for how long a job should run. </span><span class="koboSpan" id="kobo.244.2">The controller for the CR is responsible for configuring the training environment, monitoring the training job’s specific status, and maintaining the desired training job’s specific state. </span><span class="koboSpan" id="kobo.244.3">For instance, the controller can set environment variables to make the training cluster specifications (for example, types of Pods and indices) available to the training code running inside the containers. </span><span class="koboSpan" id="kobo.244.4">Additionally, the controller can inspect the exit code of a training process and fail the training job if the exit code indicates a permanent failure. </span><span class="koboSpan" id="kobo.244.5">The following YAML file sample template represents a specification for running training jobs using the TensorFlow operator (</span><code class="inlineCode"><span class="koboSpan" id="kobo.245.1">tf-operator</span></code><span class="koboSpan" id="kobo.246.1">):</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr"><span class="koboSpan" id="kobo.247.1">apiVersion:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.248.1">"kubeflow.org/v1"</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.249.1">kind:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.250.1">"TFJob"</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.251.1">metadata:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.252.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.253.1">"distributed-tensorflow-job"</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.254.1">spec:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.255.1">tfReplicaSpecs:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.256.1">PS:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.257.1">replicas:</span></span> <span class="hljs-number"><span class="koboSpan" id="kobo.258.1">1</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.259.1">restartPolicy:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.260.1">Never</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.261.1">template:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.262.1">spec:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.263.1">containers:</span></span>
<span class="hljs-bullet"><span class="koboSpan" id="kobo.264.1">-</span></span> <span class="hljs-attr"><span class="koboSpan" id="kobo.265.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.266.1">tensorflow</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.267.1">image:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.268.1">&lt;model</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.269.1">training</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.270.1">image</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.271.1">uri&gt;</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.272.1">command:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.273.1">Worker:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.274.1">replicas:</span></span> <span class="hljs-number"><span class="koboSpan" id="kobo.275.1">2</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.276.1">restartPolicy:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.277.1">Never</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.278.1">template:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.279.1">spec:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.280.1">containers:</span></span>
<span class="hljs-bullet"><span class="koboSpan" id="kobo.281.1">-</span></span> <span class="hljs-attr"><span class="koboSpan" id="kobo.282.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.283.1">tensorflow</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.284.1">image:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.285.1">&lt;model</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.286.1">training</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.287.1">image</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.288.1">uri&gt;</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.289.1">command:</span></span>
</code></pre>
<p class="normal"><span class="koboSpan" id="kobo.290.1">In this example template, the specification will create one copy of the parameter servers (which aggregate model parameters across different containers) and two copies of the workers (which run model training loops and communicate with the parameter servers). </span><span class="koboSpan" id="kobo.290.2">The operator will process the </span><code class="inlineCode"><span class="koboSpan" id="kobo.291.1">TFJob</span></code><span class="koboSpan" id="kobo.292.1"> object according to the specification, keep the </span><code class="inlineCode"><span class="koboSpan" id="kobo.293.1">TFJob</span></code><span class="koboSpan" id="kobo.294.1"> object stored in the system with the actual running services and Pods, and replace the actual state with the desired state. </span><span class="koboSpan" id="kobo.294.2">You can submit the training job using </span><code class="inlineCode"><span class="koboSpan" id="kobo.295.1">kubectl apply -f &lt;TFJob specs template&gt;</span></code><span class="koboSpan" id="kobo.296.1"> and can get the status of the </span><code class="inlineCode"><span class="koboSpan" id="kobo.297.1">TFJob</span></code><span class="koboSpan" id="kobo.298.1"> with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.299.1">kubectl get tfjob</span></code><span class="koboSpan" id="kobo.300.1"> command.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.301.1">As a data</span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.302.1"> scientist, you can submit Kubernetes training jobs or Kubeflow training jobs using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.303.1">kubectl</span></code><span class="koboSpan" id="kobo.304.1"> utility, or from your Jupyter Notebook environment using </span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.305.1">the </span><strong class="keyWord"><span class="koboSpan" id="kobo.306.1">Python SDK</span></strong><span class="koboSpan" id="kobo.307.1">. </span><span class="koboSpan" id="kobo.307.2">For example, the </span><code class="inlineCode"><span class="koboSpan" id="kobo.308.1">TFJob</span></code><span class="koboSpan" id="kobo.309.1"> object has a Python SDK called </span><code class="inlineCode"><span class="koboSpan" id="kobo.310.1">kubernet.tfjob</span></code><span class="koboSpan" id="kobo.311.1">, and Kubernetes has a client SDK called </span><code class="inlineCode"><span class="koboSpan" id="kobo.312.1">kubernetes.client</span></code><span class="koboSpan" id="kobo.313.1"> for interacting with the Kubernetes and Kubeflow environments from your Python code. </span><span class="koboSpan" id="kobo.313.2">You can also invoke training jobs using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.314.1">Kubeflow Pipeline</span></code><span class="koboSpan" id="kobo.315.1"> component, which we will cover later, in the </span><em class="italic"><span class="koboSpan" id="kobo.316.1">Kubeflow pipeline</span></em><span class="koboSpan" id="kobo.317.1"> section.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.318.1">Using Kubernetes Jobs for ML training requires the installation and configuration of the necessary ML software components for training different models using different frameworks. </span><span class="koboSpan" id="kobo.318.2">You will also need to build logging and monitoring capabilities to monitor the training progress.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.319.1">The </span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.320.1">adoption of Kubeflow training operators also presents its own set of challenges. </span><span class="koboSpan" id="kobo.320.2">Several operators, including the MPI training operator, are still in the maturing phase and are not yet suitable for production adoption. </span><span class="koboSpan" id="kobo.320.3">While operators provide certain logs and metrics, obtaining a comprehensive view of extensive training runs across Pods remains challenging and necessitates the integration of multiple dashboards. </span><span class="koboSpan" id="kobo.320.4">The existence of separate operators for various ML frameworks with fragmented capabilities and statuses complicates achieving a unified experience. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.321.1">The learning curve for running training operators can be high, as it involves understanding many components, such as the development of training YAML files, distributed training job configuration, and training job monitoring.</span></p>
<h2 class="heading-2" id="_idParaDest-187"><span class="koboSpan" id="kobo.322.1">Registering models with a model registry</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.323.1">A </span><strong class="keyWord"><span class="koboSpan" id="kobo.324.1">model registry</span></strong><span class="koboSpan" id="kobo.325.1"> is an</span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.326.1"> important component in model management and governance, and</span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.327.1"> it is a key link between the model training stage and the model deployment stage, as models need to be properly stored in a managed repository for a governed model deployment. </span><span class="koboSpan" id="kobo.327.2">There are several open-source options for implementing a model registry in an ML platform. </span><span class="koboSpan" id="kobo.327.3">In this section, we will explore MLflow Model Registry for model management. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.328.1">MLflow Model Registry is one of the leading model registry solutions with strong support in model artifacts lifecycle management, versioning support, and a range of deployment targets like Docker, Amazon SageMaker, and Azure ML. </span><span class="koboSpan" id="kobo.328.2">It is designed for managing all stages of the ML lifecycle, including experiment management, model management, reproducibility, and model deployment. </span><span class="koboSpan" id="kobo.328.3">It has the following four main components:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.329.1">Experiment tracking</span></strong><span class="koboSpan" id="kobo.330.1">: During model development, data scientists run many training jobs as experiments using different datasets, algorithms, and configurations to find the best working model. </span><span class="koboSpan" id="kobo.330.2">Tracking the inputs and outputs of these experiments is critical to ensure efficient progress. </span><span class="koboSpan" id="kobo.330.3">Experiment tracking logs the parameters, code versions, metrics, and artifacts when running your ML code and for later visualizing of the results.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.331.1">ML projects</span></strong><span class="koboSpan" id="kobo.332.1">: Projects package data science code in a format to reproduce runs on any platform.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.333.1">Models:</span></strong><span class="koboSpan" id="kobo.334.1"> Models provide a standard unit for packaging and deploying ML models.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.335.1">Model Registry:</span></strong><span class="koboSpan" id="kobo.336.1"> Model Registry stores, annotates, discovers, and manages models in a central repository.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.337.1">The Model Registry</span><a id="_idIndexMarker648"/><span class="koboSpan" id="kobo.338.1"> component of MLflow provides a central model repository for saved models. </span><span class="koboSpan" id="kobo.338.2">It captures model details such as model lineage, model version, annotation, and description, and also captures model stage transitions from staging to production (so the status of the model state is clearly described).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.339.1">To use MLflow Model Registry</span><a id="_idIndexMarker649"/><span class="koboSpan" id="kobo.340.1"> in a team environment, you need to set up an MLflow tracking server with a database as a backend and storage for the model artifacts. </span><span class="koboSpan" id="kobo.340.2">MLflow provides a UI and an API to interact with its core functionality, including Model Registry. </span><span class="koboSpan" id="kobo.340.3">Once the model is registered in Model Registry, you can add, modify, update, transition, or delete the model through the UI or the API. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.341.1">The following figure shows an architecture setup for an MLflow tracking server and its associated Model Registry:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.342.1"><img alt="Figure 7.2 – The MLflow tracking server and model registry " src="../Images/B20836_07_02.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.343.1">Figure 7.2: The MLflow tracking server and Model Registry</span></p>
<p class="normal"><span class="koboSpan" id="kobo.344.1">MLflow supports basic HTTP authentication to enable access control over experiments and registered models. </span><span class="koboSpan" id="kobo.344.2">The MLflow tracking server also supports basic authentication. </span><span class="koboSpan" id="kobo.344.3">However, these security capabilities might not be sufficient for enterprise requirements such as user group </span><a id="_idIndexMarker650"/><span class="koboSpan" id="kobo.345.1">management and integration with third-party authentication providers. </span><span class="koboSpan" id="kobo.345.2">Organizations often need to implement separate security and authentication controls to manage access to resources.</span></p>
<h2 class="heading-2" id="_idParaDest-188"><span class="koboSpan" id="kobo.346.1">Serving models using model serving services</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.347.1">Once a model</span><a id="_idIndexMarker651"/><span class="koboSpan" id="kobo.348.1"> has been trained and saved, utilizing it to generate predictions is a matter of loading the saved model into an ML package and invoking the appropriate model prediction function provided by the package. </span><span class="koboSpan" id="kobo.348.2">However, for large-scale and complex model serving requirements, you will need to consider implementing a dedicated model serving infrastructure to meet those needs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.349.1">In the subsequent sections, we will explore a variety of open-source model serving frameworks that can assist in addressing such needs.</span></p>
<h3 class="heading-3" id="_idParaDest-189"><span class="koboSpan" id="kobo.350.1">The Gunicorn and Flask inference engine</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.351.1">Gunicorn</span></strong><span class="koboSpan" id="kobo.352.1"> and </span><strong class="keyWord"><span class="koboSpan" id="kobo.353.1">Flask</span></strong><span class="koboSpan" id="kobo.354.1"> are </span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.355.1">often used </span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.356.1">for building custom model serving web frameworks. </span><span class="koboSpan" id="kobo.356.2">The following figure shows a typical </span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.357.1">architecture that</span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.358.1"> uses Flask, Gunicorn, and Nginx as the building blocks for a model serving service.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.359.1"><img alt="Figure 7.3 – A model serving architecture using Flask and Gunicorn " src="../Images/B20836_07_03.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.360.1">Figure 7.3: A model serving architecture using Flask and Gunicorn</span></p>
<p class="normal"><span class="koboSpan" id="kobo.361.1">Flask is a</span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.362.1"> Python-based micro web framework </span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.363.1">for building web apps quickly. </span><span class="koboSpan" id="kobo.363.2">It is lightweight and has almost no dependencies on external libraries. </span><span class="koboSpan" id="kobo.363.3">With Flask, you can define different invocation routes and associate handler functions to handle different web calls (such as health check calls and model invocation calls). </span><span class="koboSpan" id="kobo.363.4">To handle model prediction requests, the Flask app would load the model into memory and call the </span><code class="inlineCode"><span class="koboSpan" id="kobo.364.1">predict</span></code><span class="koboSpan" id="kobo.365.1"> function on the model to generate the prediction. </span><span class="koboSpan" id="kobo.365.2">Flask comes with a built-in web server, but it does not scale well as it can only support one request at a time.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.366.1">This is </span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.367.1">where </span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.368.1">Gunicorn can help address the scalability gap. </span><span class="koboSpan" id="kobo.368.2">Gunicorn is a web server for hosting web apps, including Flask apps. </span><span class="koboSpan" id="kobo.368.3">It can handle multiple requests in parallel and distribute the traffic to the hosted web apps efficiently. </span><span class="koboSpan" id="kobo.368.4">When it receives a web request, it will invoke the hosted Flask app to handle the request, such as invoking the function to generate the model prediction.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.369.1">In addition to serving prediction requests as web requests, an enterprise inference engine also needs to handle secure web traffic (such as SSL/TLS traffic), as well as load balancing when there are multiple web servers. </span><span class="koboSpan" id="kobo.369.2">This is where Nginx can play an important role. </span><span class="koboSpan" id="kobo.369.3">Nginx can serve as a load balancer for multiple web servers and can handle termination for SSL/TLS traffic more efficiently, so web servers do not have to handle it.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.370.1">A Flask/Gunicorn-based model serving architecture can be a good option for hosting simple model</span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.371.1"> serving patterns. </span><span class="koboSpan" id="kobo.371.2">But for </span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.372.1">more</span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.373.1"> complicated patterns such as serving </span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.374.1">different versions of models, A/B testing (showing two variants of a model to different user groups and comparing their responses), or large model serving, this architecture will have limitations. </span><span class="koboSpan" id="kobo.374.2">The Flask/Gunicorn architecture pattern also requires custom code (such as the Flask app) to work, as it does not provide built-in support for the different ML models.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.375.1">Next, let’s explore some purpose-built model serving frameworks and see how they are different from the custom Flask-based inference engine.</span></p>
<h3 class="heading-3" id="_idParaDest-190"><span class="koboSpan" id="kobo.376.1">The TensorFlow Serving framework</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.377.1">TensorFlow Serving</span></strong><span class="koboSpan" id="kobo.378.1"> is a </span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.379.1">production-grade, open-source model serving framework, and provides out-of-the-box support </span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.380.1">for serving TensorFlow models behind a RESTFul endpoint. </span><span class="koboSpan" id="kobo.380.2">It manages the model lifecycle for model serving and provides access to versioned and multiple models behind a single endpoint. </span><span class="koboSpan" id="kobo.380.3">There is also built-in support for </span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.381.1">canary deployments. </span><span class="koboSpan" id="kobo.381.2">A </span><strong class="keyWord"><span class="koboSpan" id="kobo.382.1">canary deployment </span></strong><span class="koboSpan" id="kobo.383.1">allows you to deploy a model to support a subset of traffic. </span><span class="koboSpan" id="kobo.383.2">In addition to the real-time inference support, there is also a batch scheduler feature that can batch multiple prediction requests and perform a single joint execution. </span><span class="koboSpan" id="kobo.383.3">With TensorFlow Serving, there is no need to write custom code to serve the model.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.384.1">The following figure </span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.385.1">shows the architecture of TensorFlow Serving:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.386.1"><img alt="Figure 7.4 – TensorFlow Serving architecture " src="../Images/B20836_07_04.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.387.1">Figure 7.4: TensorFlow Serving architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.388.1">Let’s discuss each of the architecture components in more detail:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.389.1">The </span><em class="italic"><span class="koboSpan" id="kobo.390.1">API handler</span></em><span class="koboSpan" id="kobo.391.1"> provides APIs for TensorFlow Serving. </span><span class="koboSpan" id="kobo.391.2">It comes with a built-in, lightweight</span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.392.1"> HTTP server to serve RESTful-based API requests. </span><span class="koboSpan" id="kobo.392.2">It also</span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.393.1"> supports </span><strong class="keyWord"><span class="koboSpan" id="kobo.394.1">gRPC</span></strong><span class="koboSpan" id="kobo.395.1"> (a </span><strong class="keyWord"><span class="koboSpan" id="kobo.396.1">remote procedure call</span></strong><span class="koboSpan" id="kobo.397.1"> protocol) traffic. </span><span class="koboSpan" id="kobo.397.2">gRPC is a more efficient and fast networking protocol; however, it is more complicated to use than the REST protocol. </span><span class="koboSpan" id="kobo.397.3">TensorFlow Serving has a concept called a </span><em class="italic"><span class="koboSpan" id="kobo.398.1">servable</span></em><span class="koboSpan" id="kobo.399.1">, which refers to the actual objects that handle a task, such as model inferences or lookup tables. </span><span class="koboSpan" id="kobo.399.2">For example, a trained model is represented as a </span><em class="italic"><span class="koboSpan" id="kobo.400.1">servable</span></em><span class="koboSpan" id="kobo.401.1">, and it can contain one or more algorithms and lookup tables or embedding tables. </span><span class="koboSpan" id="kobo.401.2">The API handler uses the servable to fulfill client requests.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.402.1">The </span><em class="italic"><span class="koboSpan" id="kobo.403.1">model manager</span></em><span class="koboSpan" id="kobo.404.1"> manages </span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.405.1">the lifecycle of servables, including loading the servables, serving the servables, and unloading the servables. </span><span class="koboSpan" id="kobo.405.2">When a servable is needed to perform a task, the model manager provides the client with a handler to access the servable instances. </span><span class="koboSpan" id="kobo.405.3">The model manager can manage multiple versions of a servable, allowing gradual rollout of different versions of a model.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.406.1">The </span><em class="italic"><span class="koboSpan" id="kobo.407.1">model loader</span></em><span class="koboSpan" id="kobo.408.1"> is</span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.409.1"> responsible </span><a id="_idIndexMarker672"/><span class="koboSpan" id="kobo.410.1">for loading models from different sources, such as </span><strong class="keyWord"><span class="koboSpan" id="kobo.411.1">Amazon</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.412.1">S3</span></strong><span class="koboSpan" id="kobo.413.1">. </span><span class="koboSpan" id="kobo.413.2">When a new model is loaded, the model loader notifies the model manager about the availability of the new model, and the model manager will decide what the next step should be, such as unloading the previous version and loading the new version.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.414.1">TensorFlow Serving</span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.415.1"> can be extended to support non-TensorFlow models. </span><span class="koboSpan" id="kobo.415.2">For example, models trained in other frameworks can be converted to</span><a id="_idIndexMarker674"/><span class="koboSpan" id="kobo.416.1"> the </span><strong class="keyWord"><span class="koboSpan" id="kobo.417.1">ONNX</span></strong><span class="koboSpan" id="kobo.418.1"> format and served using TensorFlow Serving. </span><span class="koboSpan" id="kobo.418.2">ONNX is a common format for representing models to support interoperability across different ML frameworks.</span></p>
<h3 class="heading-3" id="_idParaDest-191"><span class="koboSpan" id="kobo.419.1">The TorchServe serving framework</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.420.1">TorchServe</span></strong><span class="koboSpan" id="kobo.421.1"> is </span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.422.1">an open-source</span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.423.1"> framework for serving trained </span><strong class="keyWord"><span class="koboSpan" id="kobo.424.1">PyTorch</span></strong><span class="koboSpan" id="kobo.425.1"> models. </span><span class="koboSpan" id="kobo.425.2">Similar to TensorFlow Serving, TorchServe provides a REST API for serving models with its built-in web server. </span><span class="koboSpan" id="kobo.425.3">With core features such as multi-model serving, model versioning, server-side request batching, and built-in monitoring, TorchServe can serve production workloads at scale. </span><span class="koboSpan" id="kobo.425.4">There is also no need to write custom code to host PyTorch models with TorchServe. </span><span class="koboSpan" id="kobo.425.5">In addition, TorchServe comes with a built-in web server for hosting the model.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.426.1">The following figure illustrates </span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.427.1">the architecture components of the TorchServe framework:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.428.1"><img alt="Figure 7.5 – TorchServe architecture " src="../Images/B20836_07_05.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.429.1">Figure 7.5: TorchServe architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.430.1">The </span><em class="italic"><span class="koboSpan" id="kobo.431.1">inference API</span></em><span class="koboSpan" id="kobo.432.1"> is</span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.433.1"> responsible </span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.434.1">for handling prediction requests from client applications using loaded PyTorch models. </span><span class="koboSpan" id="kobo.434.2">It supports the REST protocol and provides a prediction API, as well as other supporting APIs such as health check and model explanation APIs. </span><span class="koboSpan" id="kobo.434.3">The inference API can handle prediction requests for multiple models.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.435.1">The model artifacts are packaged into a single archive file and stored in a model store within the TorchServe environment. </span><span class="koboSpan" id="kobo.435.2">You use a </span><strong class="keyWord"><span class="koboSpan" id="kobo.436.1">command-line interface</span></strong><span class="koboSpan" id="kobo.437.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.438.1">CLI</span></strong><span class="koboSpan" id="kobo.439.1">) command called </span><code class="inlineCode"><span class="koboSpan" id="kobo.440.1">torch-mode-archive</span></code><span class="koboSpan" id="kobo.441.1"> to package the model.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.442.1">The TorchServe backend loads the archived models from the model store into different worker processes. </span><span class="koboSpan" id="kobo.442.2">These worker processes interact with the inference API to process requests and send back responses.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.443.1">The management API is</span><a id="_idIndexMarker680"/><span class="koboSpan" id="kobo.444.1"> responsible for handling management tasks such as registering and unregistering PyTorch models, checking the model status, and scaling the worker process. </span><span class="koboSpan" id="kobo.444.2">The management API is normally used by system administrators.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.445.1">TorchServe also provides built-in support for logging and metrics. </span><span class="koboSpan" id="kobo.445.2">The logging component logs both access logs and processing logs. </span><span class="koboSpan" id="kobo.445.3">The TorchServe metrics collect a list of system metrics, such as CPU/GPU utilization and custom model metrics.</span></p>
<h3 class="heading-3" id="_idParaDest-192"><span class="koboSpan" id="kobo.446.1">KFServing framework</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.447.1">TensorFlow Serving</span><a id="_idIndexMarker681"/><span class="koboSpan" id="kobo.448.1"> and TorchServe are</span><a id="_idIndexMarker682"/><span class="koboSpan" id="kobo.449.1"> standalone model serving frameworks for a specific deep learning framework. </span><span class="koboSpan" id="kobo.449.2">In contrast, </span><strong class="keyWord"><span class="koboSpan" id="kobo.450.1">KFServing</span></strong><span class="koboSpan" id="kobo.451.1"> is a general-purpose, multi-framework, model serving framework that supports different ML models. </span><span class="koboSpan" id="kobo.451.2">KFServing uses standalone model serving frameworks such as TensorFlow Serving and TorchServe as the backend model servers. </span><span class="koboSpan" id="kobo.451.3">It is part of the </span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.452.1">Kubeflow project and provides pluggable architecture for different model formats:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.453.1"><img alt="Figure 7.6 – KFServing components " src="../Images/B20836_07_06.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.454.1">Figure 7.6: KFServing components</span></p>
<p class="normal"><span class="koboSpan" id="kobo.455.1">As a</span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.456.1"> general-purpose, multi-framework model serving solution, KFServing provides several out-of-the-box model servers (also known as predictors) for different model types, including </span><a id="_idIndexMarker685"/><span class="koboSpan" id="kobo.457.1">TensorFlow, </span><strong class="keyWord"><span class="koboSpan" id="kobo.458.1">PyTorch</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.459.1">XGBoost</span></strong><span class="koboSpan" id="kobo.460.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.461.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.462.1">, and ONNX. </span><span class="koboSpan" id="kobo.462.2">With KFServing, you can serve models using both REST and gRPC protocols. </span><span class="koboSpan" id="kobo.462.3">To deploy a supported model type, you simply need to define a YAML specification that points to the model artifact in a data store. </span><span class="koboSpan" id="kobo.462.4">Furthermore, you can build your own custom containers to serve models in KFServing. </span><span class="koboSpan" id="kobo.462.5">The container needs to provide a model serving implementation as well as a web server. </span><span class="koboSpan" id="kobo.462.6">The following code shows a sample </span><a id="_idIndexMarker686"/><span class="koboSpan" id="kobo.463.1">YAML specification to deploy a </span><code class="inlineCode"><span class="koboSpan" id="kobo.464.1">tensorflow</span></code><span class="koboSpan" id="kobo.465.1"> model using KFServing:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr"><span class="koboSpan" id="kobo.466.1">apiVersion:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.467.1">"serving.kubeflow.org/v1alpha2"</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.468.1">kind:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.469.1">"InferenceService"</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.470.1">metadata:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.471.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.472.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.473.1">model-name"</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.474.1">spec:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.475.1">default:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.476.1">predictor:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.477.1">tensorflow:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.478.1">storageUri:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.479.1">&lt;uri</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.480.1">to</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.481.1">model</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.482.1">storage</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.483.1">such</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.484.1">as</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.485.1">s3&gt;</span></span>
</code></pre>
<p class="normal"><span class="koboSpan" id="kobo.486.1">KFServing has a transformer component that allows the custom processing of the input payload before it is sent to the predictors, and also allows transforming the response from the predictor before it is sent back to the calling client. </span><span class="koboSpan" id="kobo.486.2">Sometimes, you need to provide an explanation for the model prediction, such as which features have a stronger influence on the prediction, which we will cover in more detail in a later chapter.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.487.1">KFServing is </span><a id="_idIndexMarker687"/><span class="koboSpan" id="kobo.488.1">designed for production deployment and provides a range of production deployment capabilities. </span><span class="koboSpan" id="kobo.488.2">Its auto-scaling feature allows the model server to scale up/down based on the amount of request traffic. </span><span class="koboSpan" id="kobo.488.3">With KFServing, you can deploy both the default model serving endpoint and the canary endpoint, split the traffic between the two, and specify model revisions behind the endpoint. </span><span class="koboSpan" id="kobo.488.4">For operational support, KFServing also has built-in functionality for monitoring (for example, monitoring request data and request latency).</span></p>
<h3 class="heading-3" id="_idParaDest-193"><span class="koboSpan" id="kobo.489.1">Seldon Core</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.490.1">Seldon Core</span></strong><span class="koboSpan" id="kobo.491.1"> is </span><a id="_idIndexMarker688"/><span class="koboSpan" id="kobo.492.1">another multi-framework</span><a id="_idIndexMarker689"/><span class="koboSpan" id="kobo.493.1"> model serving framework for deploying models on Kubernetes. </span><span class="koboSpan" id="kobo.493.2">Compared to KFServing, Seldon Core provides richer model serving features, for example, model serving inference graphs for use cases such as A/B testing and model ensembles. </span><span class="koboSpan" id="kobo.493.3">The following figure shows the core components of the Seldon Core framework:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.494.1"><img alt="Figure 7.7 – The Seldon Core model serving framework architecture " src="../Images/B20836_07_07.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.495.1">Figure 7.7: The Seldon Core model serving framework architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.496.1">Seldon Core</span><a id="_idIndexMarker690"/><span class="koboSpan" id="kobo.497.1"> provides packaged model servers </span><a id="_idIndexMarker691"/><span class="koboSpan" id="kobo.498.1">for some of the common ML libraries, including the </span><code class="inlineCode"><span class="koboSpan" id="kobo.499.1">SKLearn</span></code><span class="koboSpan" id="kobo.500.1"> server for scikit-learn models, the XGBoost server for XGBoost models, TensorFlow Serving for TensorFlow models, and MLflow server-based model serving. </span><span class="koboSpan" id="kobo.500.2">You can also build your own custom serving container for specific model serving needs and host it using Seldon Core.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.501.1">The following</span><a id="_idIndexMarker692"/><span class="koboSpan" id="kobo.502.1"> template shows how to deploy a model using the SKLearn server using Seldon Core. </span><span class="koboSpan" id="kobo.502.2">You simply need to change the </span><code class="inlineCode"><span class="koboSpan" id="kobo.503.1">modelUri</span></code><span class="koboSpan" id="kobo.504.1"> path to point to a saved model on a cloud object</span><a id="_idIndexMarker693"/><span class="koboSpan" id="kobo.505.1"> storage provider</span><a id="_idIndexMarker694"/><span class="koboSpan" id="kobo.506.1"> such as </span><strong class="keyWord"><span class="koboSpan" id="kobo.507.1">Google</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.508.1">Cloud Storage</span></strong><span class="koboSpan" id="kobo.509.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.510.1">Amazon S3 storage</span></strong><span class="koboSpan" id="kobo.511.1">, or </span><strong class="keyWord"><span class="koboSpan" id="kobo.512.1">Azure</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.513.1">Blob storage</span></strong><span class="koboSpan" id="kobo.514.1">. </span><span class="koboSpan" id="kobo.514.2">To test with an example, you can change the following </span><code class="inlineCode"><span class="koboSpan" id="kobo.515.1">modelUri</span></code><span class="koboSpan" id="kobo.516.1"> value to an example provided by Seldon Core – </span><code class="inlineCode"><span class="koboSpan" id="kobo.517.1">gs://seldon-models/sklearn/iris</span></code><span class="koboSpan" id="kobo.518.1">:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr"><span class="koboSpan" id="kobo.519.1">apiVersion:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.520.1">machinelearning.seldon.io/v1alpha2</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.521.1">kind:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.522.1">SeldonDeployment</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.523.1">metadata:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.524.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.525.1">sklearn</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.526.1">spec:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.527.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.528.1">sklearn-model</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.529.1">predictors:</span></span>
<span class="hljs-bullet"><span class="koboSpan" id="kobo.530.1">-</span></span> <span class="hljs-attr"><span class="koboSpan" id="kobo.531.1">graph:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.532.1">children:</span></span><span class="koboSpan" id="kobo.533.1"> []
      </span><span class="hljs-attr"><span class="koboSpan" id="kobo.534.1">implementation:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.535.1">SKLEARN_SERVER</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.536.1">modelUri:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.537.1">&lt;model</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.538.1">uri</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.539.1">to</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.540.1">model</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.541.1">artifacts</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.542.1">on</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.543.1">the</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.544.1">cloud</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.545.1">storage&gt;</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.546.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.547.1">classifier</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.548.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.549.1">default</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.550.1">replicas:</span></span> <span class="hljs-number"><span class="koboSpan" id="kobo.551.1">1</span></span>
</code></pre>
<p class="normal"><span class="koboSpan" id="kobo.552.1">Seldon Core also supports an advanced workflow, known as an inference graph, for serving models. </span><span class="koboSpan" id="kobo.552.2">The </span><em class="italic"><span class="koboSpan" id="kobo.553.1">inference graph</span></em><span class="koboSpan" id="kobo.554.1"> feature allows you to have a graph with different models and other components in a single inference pipeline. </span><span class="koboSpan" id="kobo.554.2">An inference graph can consist of several components:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.555.1">One or more ML models for the different prediction tasks</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.556.1">Traffic routing management for different usage patterns, such as traffic splitting to different models for A/B testing</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.557.1">A component for combining results from multiple models, such as a model ensemble component</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.558.1">Components for transforming the input requests (such as performing feature engineering) or output responses (for example, returning an array format as a </span><strong class="keyWord"><span class="koboSpan" id="kobo.559.1">JSON</span></strong><span class="koboSpan" id="kobo.560.1"> format)</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.561.1">To build</span><a id="_idIndexMarker695"/><span class="koboSpan" id="kobo.562.1"> inference graph specifications in YAML, you need the following key components in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.563.1">seldondeployment</span></code><span class="koboSpan" id="kobo.564.1"> YAML file:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.565.1">A list of predictors, with each predictor having its own </span><code class="inlineCode"><span class="koboSpan" id="kobo.566.1">componentSpecs</span></code><span class="koboSpan" id="kobo.567.1"> section that specifies details such as container images</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.568.1">A graph that describes how the components are linked together for each </span><code class="inlineCode"><span class="koboSpan" id="kobo.569.1">componentSpecs</span></code><span class="koboSpan" id="kobo.570.1"> section</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.571.1">The</span><a id="_idIndexMarker696"/><span class="koboSpan" id="kobo.572.1"> following sample template shows the inference graph for a custom canary deployment to split the traffic into two different versions of a model – one with 75% of the traffic and another one with 25% of the traffic:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr"><span class="koboSpan" id="kobo.573.1">apiVersion:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.574.1">machinelearning.seldon.io/v1alpha2</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.575.1">kind:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.576.1">SeldonDeployment</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.577.1">metadata:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.578.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.579.1">canary-deployment</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.580.1">spec:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.581.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.582.1">canary-deployment</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.583.1">predictors:</span></span>
<span class="hljs-bullet"><span class="koboSpan" id="kobo.584.1">-</span></span> <span class="hljs-attr"><span class="koboSpan" id="kobo.585.1">componentSpecs:</span></span>
<span class="hljs-bullet"><span class="koboSpan" id="kobo.586.1">-</span></span> <span class="hljs-attr"><span class="koboSpan" id="kobo.587.1">spec:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.588.1">containers:</span></span>
<span class="hljs-bullet"><span class="koboSpan" id="kobo.589.1">-</span></span> <span class="hljs-attr"><span class="koboSpan" id="kobo.590.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.591.1">classifier</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.592.1">image:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.593.1">&lt;container</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.594.1">uri</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.595.1">to</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.596.1">model</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.597.1">version</span></span> <span class="hljs-number"><span class="koboSpan" id="kobo.598.1">1</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.599.1">&gt;</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.600.1">graph:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.601.1">children:</span></span><span class="koboSpan" id="kobo.602.1"> []
      </span><span class="hljs-attr"><span class="koboSpan" id="kobo.603.1">endpoint:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.604.1">type:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.605.1">REST</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.606.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.607.1">classifier</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.608.1">type:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.609.1">MODEL</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.610.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.611.1">main</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.612.1">replicas:</span></span> <span class="hljs-number"><span class="koboSpan" id="kobo.613.1">1</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.614.1">traffic:</span></span> <span class="hljs-number"><span class="koboSpan" id="kobo.615.1">75</span></span>
<span class="hljs-bullet"><span class="koboSpan" id="kobo.616.1">-</span></span> <span class="hljs-attr"><span class="koboSpan" id="kobo.617.1">componentSpecs:</span></span>
<span class="hljs-bullet"><span class="koboSpan" id="kobo.618.1">-</span></span> <span class="hljs-attr"><span class="koboSpan" id="kobo.619.1">spec:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.620.1">containers:</span></span>
<span class="hljs-bullet"><span class="koboSpan" id="kobo.621.1">-</span></span> <span class="hljs-attr"><span class="koboSpan" id="kobo.622.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.623.1">classifier</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.624.1">image:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.625.1">&lt;container</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.626.1">uri</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.627.1">to</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.628.1">model</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.629.1">version</span></span> <span class="hljs-number"><span class="koboSpan" id="kobo.630.1">2</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.631.1">&gt;</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.632.1">graph:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.633.1">children:</span></span><span class="koboSpan" id="kobo.634.1"> []
      </span><span class="hljs-attr"><span class="koboSpan" id="kobo.635.1">endpoint:</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.636.1">type:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.637.1">REST</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.638.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.639.1">classifier</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.640.1">type:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.641.1">MODEL</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.642.1">name:</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.643.1">canary</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.644.1">replicas:</span></span> <span class="hljs-number"><span class="koboSpan" id="kobo.645.1">1</span></span>
<span class="hljs-attr"><span class="koboSpan" id="kobo.646.1">traffic:</span></span> <span class="hljs-number"><span class="koboSpan" id="kobo.647.1">25</span></span>
</code></pre>
<p class="normal"><span class="koboSpan" id="kobo.648.1">Once a </span><a id="_idIndexMarker697"/><span class="koboSpan" id="kobo.649.1">deployment manifest is </span><a id="_idIndexMarker698"/><span class="koboSpan" id="kobo.650.1">applied, the Seldon Core operator is responsible for creating all the resources needed to serve an ML model. </span><span class="koboSpan" id="kobo.650.2">Specifically, the operator will create resources defined in the manifest, add orchestrators to the Pods to manage the orchestration of the inference graph, and configure the traffic using ingress gateways such as Istio.</span></p>
<h3 class="heading-3" id="_idParaDest-194"><span class="koboSpan" id="kobo.651.1">Triton Inference Server</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.652.1">Triton Inference Server</span><a id="_idIndexMarker699"/><span class="koboSpan" id="kobo.653.1"> is </span><a id="_idIndexMarker700"/><span class="koboSpan" id="kobo.654.1">open-source software designed to streamline the process of AI inferencing. </span><span class="koboSpan" id="kobo.654.2">It offers a versatile solution for deploying AI models from various deep learning and ML frameworks, including TensorRT, TensorFlow, PyTorch, ONNX, OpenVINO, Python, and more. </span><span class="koboSpan" id="kobo.654.3">Triton is compatible with a wide range of devices, supporting inference across cloud environments, data centers, edge devices, and embedded systems. </span><span class="koboSpan" id="kobo.654.4">Compared to Seldon Core, Triton Inference Server is more focused on performance. </span><span class="koboSpan" id="kobo.654.5">It is designed to be highly scalable and efficient, making it a good choice for high-traffic applications. </span><span class="koboSpan" id="kobo.654.6">The following figure depicts the core components </span><a id="_idIndexMarker701"/><span class="koboSpan" id="kobo.655.1">of Triton Inference Server:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.656.1"><img alt="A picture containing text, screenshot, diagram, number  Description automatically generated" src="../Images/B20836_07_08.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.657.1">Figure 7.8: Triton Inference Server architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.658.1">The </span><a id="_idIndexMarker702"/><span class="koboSpan" id="kobo.659.1">Triton Inference Server architecture encompasses several components that work together to enable efficient and scalable inferencing. </span><span class="koboSpan" id="kobo.659.2">At its core is the backend, which represents specific deep learning or ML frameworks supported by Triton. </span><span class="koboSpan" id="kobo.659.3">Each backend handles the loading and execution of models trained with its corresponding framework. </span><span class="koboSpan" id="kobo.659.4">Triton Inference Server acts as the central hub, receiving and managing inference requests. </span><span class="koboSpan" id="kobo.659.5">It communicates with clients, such as web applications or services, and orchestrates the inferencing process. </span><span class="koboSpan" id="kobo.659.6">The model repository serves as the storage location for trained models. </span><span class="koboSpan" id="kobo.659.7">It contains serialized versions of models compatible with the supported backends. </span><span class="koboSpan" id="kobo.659.8">When requested by clients, the server accesses and loads the models into memory for inferencing.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.660.1">Triton supports multiple inference protocols, including HTTP/REST and gRPC, allowing clients to communicate with the server and make inference requests. </span><span class="koboSpan" id="kobo.660.2">Clients can specify input data and desired output formats using these protocols. </span><span class="koboSpan" id="kobo.660.3">To monitor and optimize performance, Triton provides metrics and monitoring capabilities. </span><span class="koboSpan" id="kobo.660.4">These metrics include GPU utilization, server throughput, latency, and other relevant statistics. </span><span class="koboSpan" id="kobo.660.5">Monitoring these metrics helps administrators optimize resource utilization and identify potential bottlenecks.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.661.1">Triton also </span><a id="_idIndexMarker703"/><span class="koboSpan" id="kobo.662.1">offers</span><a id="_idIndexMarker704"/><span class="koboSpan" id="kobo.663.1"> dynamic batching capability. </span><span class="koboSpan" id="kobo.663.2">This feature allows for efficient processing of multiple inference requests by grouping them together into batches. </span><span class="koboSpan" id="kobo.663.3">This batching mechanism optimizes resource utilization and improves overall inferencing performance.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.664.1">Overall, the architecture of Triton Inference Server is designed to facilitate the efficient deployment and execution of AI models across diverse frameworks and hardware platforms. </span><span class="koboSpan" id="kobo.664.2">It offers flexibility, scalability, and extensibility, enabling organizations to leverage their preferred frameworks while ensuring high-performance inferencing capabilities.</span></p>
<h2 class="heading-2" id="_idParaDest-195"><span class="koboSpan" id="kobo.665.1">Monitoring models in production</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.666.1">Model performance</span><a id="_idIndexMarker705"/><span class="koboSpan" id="kobo.667.1"> can deteriorate over time due to various factors such as changing data patterns, shifts in user behavior, or unforeseen scenarios. </span><span class="koboSpan" id="kobo.667.2">To ensure the ongoing effectiveness of deployed ML models, continuous monitoring of their performance and behavior in production is essential.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.668.1">Model monitoring involves actively tracking and analyzing the performance of deployed ML models. </span><span class="koboSpan" id="kobo.668.2">This process includes collecting data on different metrics and indicators, comparing them to predefined thresholds or baselines, and identifying anomalies or deviations from expected behavior. </span><span class="koboSpan" id="kobo.668.3">Two critical aspects of model monitoring are data drift and model drift:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.669.1">Data drift</span></strong><span class="koboSpan" id="kobo.670.1">: Data drift </span><a id="_idIndexMarker706"/><span class="koboSpan" id="kobo.671.1">refers to the scenarios where the statistical properties of incoming data change over time. </span><span class="koboSpan" id="kobo.671.2">This can create a disconnect between the data used to train the model and the data it encounters in the production environment. </span><span class="koboSpan" id="kobo.671.3">Data drift significantly impacts the performance and reliability of ML models, as they may struggle to adapt to new and evolving patterns in the data.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.672.1">Model drift</span></strong><span class="koboSpan" id="kobo.673.1">: Model drift</span><a id="_idIndexMarker707"/><span class="koboSpan" id="kobo.674.1"> refers to the degradation of an ML model’s performance over time due to changes in underlying patterns or relationships in the data. </span><span class="koboSpan" id="kobo.674.2">When the assumptions made during model training no longer hold true in the production environment, model drift occurs. </span><span class="koboSpan" id="kobo.674.3">It can lead to decreased accuracy, increased errors, and suboptimal decision-making.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.675.1">To support model monitoring efforts, there are several open-source and commercial products available in the market. </span><span class="koboSpan" id="kobo.675.2">These tools provide capabilities for monitoring model performance, detecting data drift, identifying model drift, and generating insights to help </span><a id="_idIndexMarker708"/><span class="koboSpan" id="kobo.676.1">organizations take necessary corrective actions. </span><span class="koboSpan" id="kobo.676.2">Some popular examples include Evidently AI, Arize AI, Seldon Core, Fiddler, and Author AI.</span></p>
<h2 class="heading-2" id="_idParaDest-196"><span class="koboSpan" id="kobo.677.1">Managing ML features</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.678.1">As </span><a id="_idIndexMarker709"/><span class="koboSpan" id="kobo.679.1">organizations increasingly adopt ML solutions, they recognize the need to standardize and share commonly used data and code throughout the ML lifecycle. </span><span class="koboSpan" id="kobo.679.2">One crucial element that organizations seek to manage centrally is ML features, which are commonly used data attributes that serve as inputs to ML models. </span><span class="koboSpan" id="kobo.679.3">To enable standardization and reuse of these features, organizations often turn to a feature store.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.680.1">A feature store acts as a centralized repository for storing and managing ML features. </span><span class="koboSpan" id="kobo.680.2">It provides a dedicated platform for organizing, validating, and sharing features across different ML projects and teams within an organization. </span><span class="koboSpan" id="kobo.680.3">By consolidating features in a single location, the feature store promotes consistency and facilitates collaboration among data scientists and ML practitioners.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.681.1">The concept of a feature store has gained significant attention in the ML community due to its numerous benefits. </span><span class="koboSpan" id="kobo.681.2">Firstly, it enhances productivity by eliminating the need to recreate and engineer features for each ML project. </span><span class="koboSpan" id="kobo.681.3">Instead, data scientists can readily access precomputed and validated features from the store, saving time and effort. </span><span class="koboSpan" id="kobo.681.4">Additionally, a feature store improves model performance by ensuring the consistency and quality of features used in ML models. </span><span class="koboSpan" id="kobo.681.5">By centralizing feature management, organizations can enforce data governance practices, perform feature validation, and monitor feature quality, leading to more reliable and accurate ML models.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.682.1">Several open-source feature store frameworks are available in the market, such as Feast and Hopsworks Feature Store, offering organizations flexible options for managing their ML features. </span><span class="koboSpan" id="kobo.682.2">Let’s take a closer look at Feast as an example to get a deep understanding of how a feature store works.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.683.1">Feast is an open-source feature store that enables organizations to manage, discover, and serve features for ML applications. </span><span class="koboSpan" id="kobo.683.2">Developed by Tecton, Feast is designed to handle large-scale, real-time feature data. </span><span class="koboSpan" id="kobo.683.3">It supports feature ingestion from various sources, including batch pipelines and streaming systems like Apache Kafka. </span><span class="koboSpan" id="kobo.683.4">Feast integrates well with popular ML frameworks such as TensorFlow and PyTorch, allowing seamless </span><a id="_idIndexMarker710"/><span class="koboSpan" id="kobo.684.1">integration into ML workflows. </span><span class="koboSpan" id="kobo.684.2">With features like feature versioning, data validation, and online and offline serving capabilities, Feast provides a comprehensive solution for feature management.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.685.1"><img alt="A picture containing text, screenshot, diagram, font  Description automatically generated" src="../Images/B20836_07_09.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.686.1">Figure 7.9: Feast feature store</span></p>
<p class="normal"><span class="koboSpan" id="kobo.687.1">At the core of the Feast architecture is the online and offline feature storage, which serves as the centralized storage for feature data. </span><span class="koboSpan" id="kobo.687.2">The feature repository stores feature data in a distributed storage system, allowing for scalable and efficient storage and retrieval of feature values.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.688.1">Feast employs a decoupled architecture, where the ingestion of feature data and the serving of features are separated. </span><span class="koboSpan" id="kobo.688.2">The data ingestion component is responsible for extracting feature data from various sources, such as data warehouses, databases, and streaming platforms. </span><span class="koboSpan" id="kobo.688.3">It then transforms and loads the feature data into the feature storage, ensuring data quality and consistency.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.689.1">The feature serving component is responsible for providing low-latency access to feature data for ML models during training and inference. </span><span class="koboSpan" id="kobo.689.2">The feature serving component also supports online and offline serving modes, allowing for real-time and batch feature serving.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.690.1">To enable efficient data discovery, Feast employs a feature registry. </span><span class="koboSpan" id="kobo.690.2">The feature registry allows for the fast lookup and retrieval of feature values based on different feature combinations and time ranges.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.691.1">Feast </span><a id="_idIndexMarker711"/><span class="koboSpan" id="kobo.692.1">also integrates with popular ML frameworks, such as TensorFlow and PyTorch, through its SDKs and client libraries. </span><span class="koboSpan" id="kobo.692.2">These integrations enable seamless integration of Feast into ML pipelines and workflows, making it easy for data scientists and ML engineers to access and utilize feature data in their models.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.693.1">Overall, the Feast feature store architecture provides a robust and scalable solution for managing and serving ML features. </span><span class="koboSpan" id="kobo.693.2">By centralizing feature data management, Feast enables organizations to enhance productivity, improve model performance, and promote collaboration in ML development.</span></p>
<h2 class="heading-2" id="_idParaDest-197"><span class="koboSpan" id="kobo.694.1">Automating ML pipeline workflows</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.695.1">To </span><a id="_idIndexMarker712"/><span class="koboSpan" id="kobo.696.1">automate the core ML platform components we have discussed so far, we need to build pipelines</span><a id="_idIndexMarker713"/><span class="koboSpan" id="kobo.697.1"> that can orchestrate different steps using these components. </span><span class="koboSpan" id="kobo.697.2">Automation brings efficiency, productivity, and consistency while enabling reproducibility and minimizing human errors. </span><span class="koboSpan" id="kobo.697.3">There are several open-source technologies available to automate ML workflows, with Apache Airflow and Kubeflow Pipelines being prominent examples.</span></p>
<h3 class="heading-3" id="_idParaDest-198"><span class="koboSpan" id="kobo.698.1">Apache Airflow</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.699.1">Apache</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.700.1">Airflow</span></strong><span class="koboSpan" id="kobo.701.1"> is </span><a id="_idIndexMarker714"/><span class="koboSpan" id="kobo.702.1">an open-source software package for programmatically authoring, scheduling, and monitoring multi-step workflows. </span><span class="koboSpan" id="kobo.702.2">It is a general-purpose</span><a id="_idIndexMarker715"/><span class="koboSpan" id="kobo.703.1"> workflow orchestration tool that can be leveraged to define workflows for a wide range of tasks, including ML tasks. </span><span class="koboSpan" id="kobo.703.2">First, let’s explore some core Airflow concepts:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.704.1">Directed Acyclic Graph</span></strong><span class="koboSpan" id="kobo.705.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.706.1">DAG</span></strong><span class="koboSpan" id="kobo.707.1">): A DAG </span><a id="_idIndexMarker716"/><span class="koboSpan" id="kobo.708.1">defines independent tasks that are executed independently in a pipeline. </span><span class="koboSpan" id="kobo.708.2">The sequences of the execution can be visualized like a graph.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.709.1">Tasks</span></strong><span class="koboSpan" id="kobo.710.1">: Tasks </span><a id="_idIndexMarker717"/><span class="koboSpan" id="kobo.711.1">are basic units of execution in Airflow. </span><span class="koboSpan" id="kobo.711.2">Tasks have dependencies between them during executions.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.712.1">Operators</span></strong><span class="koboSpan" id="kobo.713.1">: Operators </span><a id="_idIndexMarker718"/><span class="koboSpan" id="kobo.714.1">are DAG components that describe a single task in the pipeline. </span><span class="koboSpan" id="kobo.714.2">An operator implements the task execution logic. </span><span class="koboSpan" id="kobo.714.3">Airflow provides a list of operators for common tasks, such </span><a id="_idIndexMarker719"/><span class="koboSpan" id="kobo.715.1">as a Python operator for running Python code, or an Amazon S3 operator to interact with the S3 service. </span><span class="koboSpan" id="kobo.715.2">Tasks are created when operators are instantiated.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.716.1">Scheduling</span></strong><span class="koboSpan" id="kobo.717.1">: A DAG </span><a id="_idIndexMarker720"/><span class="koboSpan" id="kobo.718.1">can run on demand or on a predetermined schedule.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.719.1">Sensors</span></strong><span class="koboSpan" id="kobo.720.1">: Sensors </span><a id="_idIndexMarker721"/><span class="koboSpan" id="kobo.721.1">are a special type of operator that are designed to wait for something to occur. </span><span class="koboSpan" id="kobo.721.2">They can then help trigger a downstream task to happen.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.722.1">Airflow can</span><a id="_idIndexMarker722"/><span class="koboSpan" id="kobo.723.1"> run on a single machine or in a cluster. </span><span class="koboSpan" id="kobo.723.2">Additionally, it can be deployed on the Kubernetes infrastructure. </span><span class="koboSpan" id="kobo.723.3">The following figure shows a multi-node Airflow deployment:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.724.1"><img alt="Figure 7.8 – Apache Airflow architecture " src="../Images/B20836_07_10.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.725.1">Figure 7.10: Apache Airflow architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.726.1">The </span><em class="italic"><span class="koboSpan" id="kobo.727.1">master node</span></em><span class="koboSpan" id="kobo.728.1"> mainly </span><a id="_idIndexMarker723"/><span class="koboSpan" id="kobo.729.1">runs the </span><em class="italic"><span class="koboSpan" id="kobo.730.1">web server</span></em><span class="koboSpan" id="kobo.731.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.732.1">scheduler</span></em><span class="koboSpan" id="kobo.733.1">. </span><span class="koboSpan" id="kobo.733.2">The scheduler is responsible for scheduling the execution of the DAGs. </span><span class="koboSpan" id="kobo.733.3">It sends tasks to a queue, and the worker nodes retrieve the tasks from the queue and run them. </span><span class="koboSpan" id="kobo.733.4">The metadata store is used to store the metadata of the Airflow cluster and processes, such as task instance details or user data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.734.1">You can author the Airflow DAGs using Python. </span><span class="koboSpan" id="kobo.734.2">The following sample code shows how to author a basic Airflow DAG in Python with two Bash operators in a sequence:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.735.1">from</span></span><span class="koboSpan" id="kobo.736.1"> airflow </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.737.1">import</span></span><span class="koboSpan" id="kobo.738.1"> DAG
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.739.1">from</span></span><span class="koboSpan" id="kobo.740.1"> airflow.operators.bash_operator </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.741.1">import</span></span><span class="koboSpan" id="kobo.742.1"> BashOperator
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.743.1">from</span></span><span class="koboSpan" id="kobo.744.1"> datetime </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.745.1">import</span></span><span class="koboSpan" id="kobo.746.1"> datetime, timedelta
default_args = {
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.747.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.748.1">owner'</span></span><span class="koboSpan" id="kobo.749.1">: myname,
}
dag = DAG(</span><span class="hljs-string"><span class="koboSpan" id="kobo.750.1">'test'</span></span><span class="koboSpan" id="kobo.751.1">, default_args=default_args, schedule_interval=timedelta(days=</span><span class="hljs-number"><span class="koboSpan" id="kobo.752.1">1</span></span><span class="koboSpan" id="kobo.753.1">))
t1 = BashOperator(
    task_id=</span><span class="hljs-string"><span class="koboSpan" id="kobo.754.1">'print_date'</span></span><span class="koboSpan" id="kobo.755.1">,
    bash_command=</span><span class="hljs-string"><span class="koboSpan" id="kobo.756.1">'date'</span></span><span class="koboSpan" id="kobo.757.1">,
    dag=dag)
t2 = BashOperator(
    task_id=</span><span class="hljs-string"><span class="koboSpan" id="kobo.758.1">'sleep'</span></span><span class="koboSpan" id="kobo.759.1">,
    bash_command=</span><span class="hljs-string"><span class="koboSpan" id="kobo.760.1">'sleep 5'</span></span><span class="koboSpan" id="kobo.761.1">,
    retries=</span><span class="hljs-number"><span class="koboSpan" id="kobo.762.1">3</span></span><span class="koboSpan" id="kobo.763.1">,
    dag=dag)
t2.set_upstream(t1)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.764.1">Airflow can connect to many different sources and has built-in operators for many external </span><a id="_idIndexMarker724"/><span class="koboSpan" id="kobo.765.1">services, such as </span><strong class="keyWord"><span class="koboSpan" id="kobo.766.1">Amazon</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.767.1">EMR</span></strong><span class="koboSpan" id="kobo.768.1"> and </span><strong class="keyWord"><span class="koboSpan" id="kobo.769.1">Amazon</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.770.1">SageMaker</span></strong><span class="koboSpan" id="kobo.771.1">. </span><span class="koboSpan" id="kobo.771.2">It </span><a id="_idIndexMarker725"/><span class="koboSpan" id="kobo.772.1">has been widely adopted by many organizations to run large-scale workflow orchestration jobs in production, such as coordinating ETL jobs and ML data processing jobs. </span><span class="koboSpan" id="kobo.772.2">AWS even has a managed Airflow offering to help reduce the operational overhead of running Airflow infrastructure.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.773.1">Airflow also </span><a id="_idIndexMarker726"/><span class="koboSpan" id="kobo.774.1">comes with some limitations. </span><span class="koboSpan" id="kobo.774.2">Airflow</span><a id="_idIndexMarker727"/><span class="koboSpan" id="kobo.775.1"> does not offer a UI designer for DAG development, which can be a challenge for users without Python programming skills to design workflows. </span><span class="koboSpan" id="kobo.775.2">The lack of versioning control with DAG pipelines also poses some challenges with managing and understanding the evolving variations of pipelines. </span><span class="koboSpan" id="kobo.775.3">Operating Airflow on Kubernetes can be complex, which is why many organizations opt for managed offerings. </span><span class="koboSpan" id="kobo.775.4">Despite these limitations, however, Airflow has emerged as a highly popular workflow orchestration tool due to its enterprise-ready capability, strong community support, and rich ecosystem. </span></p>
<h3 class="heading-3" id="_idParaDest-199"><span class="koboSpan" id="kobo.776.1">Kubeflow Pipelines</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.777.1">Kubeflow Pipelines</span></strong><span class="koboSpan" id="kobo.778.1"> is a </span><a id="_idIndexMarker728"/><span class="koboSpan" id="kobo.779.1">Kubeflow component, and</span><a id="_idIndexMarker729"/><span class="koboSpan" id="kobo.780.1"> it is purpose-built for authoring and orchestrating end-to-end ML workflows on Kubernetes. </span><span class="koboSpan" id="kobo.780.2">First, let’s review some core concepts </span><a id="_idIndexMarker730"/><span class="koboSpan" id="kobo.781.1">of Kubeflow Pipelines:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.782.1">Pipeline</span></strong><span class="koboSpan" id="kobo.783.1">: A pipeline describes an ML workflow, all the components in the workflow, and how the components are related to each other in the pipeline.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.784.1">Pipeline components</span></strong><span class="koboSpan" id="kobo.785.1">: A pipeline component performs a task in the pipeline. </span><span class="koboSpan" id="kobo.785.2">An example of a pipeline component could be a data processing component or a model training component.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.786.1">Experiment</span></strong><span class="koboSpan" id="kobo.787.1">: An experiment organizes different trial runs (model training) for an ML project so you can easily inspect and compare the different runs and their results.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.788.1">Step</span></strong><span class="koboSpan" id="kobo.789.1">: The execution of one component in a pipeline is called a step.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.790.1">Run trigger</span></strong><span class="koboSpan" id="kobo.791.1">: You use a run trigger to kick off the execution of a pipeline. </span><span class="koboSpan" id="kobo.791.2">A run trigger can be a periodic trigger (for example, to run every 2 hours), or a scheduled trigger (for example, run at a specific date and time).</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.792.1">Output artifacts</span></strong><span class="koboSpan" id="kobo.793.1">: Output artifacts are the outputs from the pipeline components. </span><span class="koboSpan" id="kobo.793.2">Examples of output artifacts could be model training metrics or visualizations of datasets.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.794.1">Kubeflow Pipelines</span><a id="_idIndexMarker731"/><span class="koboSpan" id="kobo.795.1"> is installed as part of the Kubeflow installation. </span><span class="koboSpan" id="kobo.795.2">It comes with its own UI, which is part of the overall Kubeflow dashboard UI. </span><span class="koboSpan" id="kobo.795.3">The Pipelines service manages the pipelines and their run status and stores them in a metadata database. </span><span class="koboSpan" id="kobo.795.4">There is an orchestration and workflow controller that manages the actual execution of the pipelines and the components. </span><span class="koboSpan" id="kobo.795.5">The following</span><a id="_idIndexMarker732"/><span class="koboSpan" id="kobo.796.1"> figure illustrates the core architecture components in a Kubeflow pipeline:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.797.1"><img alt="Figure 7.9 – Kubeflow Pipelines architecture " src="../Images/B20836_07_11.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.798.1">Figure 7.11: Kubeflow Pipelines architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.799.1">You</span><a id="_idIndexMarker733"/><span class="koboSpan" id="kobo.800.1"> author the</span><a id="_idIndexMarker734"/><span class="koboSpan" id="kobo.801.1"> pipeline using the Pipeline SDK in Python. </span><span class="koboSpan" id="kobo.801.2">To create and run a pipeline, follow these steps:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.802.1">Create a pipeline definition using the Kubeflow SDK. </span><span class="koboSpan" id="kobo.802.2">The pipeline definition specifies a list of components and how they are joined together in a graph.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.803.1">Compile the definition into a static YAML specification to be executed by the Kubeflow Pipelines service.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.804.1">Register the specification with the Kubeflow Pipelines service and call the pipeline to run from the static definition.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.805.1">The Kubeflow Pipelines service calls the API server to create resources to run the pipeline.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.806.1">Orchestration controllers execute various containers to complete the pipeline run.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.807.1">It is important to note that running pipelines using Kubeflow Pipelines requires a high degree of competency with Kubernetes, which could be challenging for people without deep Kubernetes skills. </span><span class="koboSpan" id="kobo.807.2">Building the workflow in Python can be a complex task that involves writing a Dockerfile or YAML file for each component, and Python scripts for the workflow and execution. </span><span class="koboSpan" id="kobo.807.3">Kubeflow Pipelines mainly works within the Kubeflow environment, with very limited integration with external tools and services. </span><span class="koboSpan" id="kobo.807.4">It also lacks native pipeline versioning capability. </span><span class="koboSpan" id="kobo.807.5">Despite these challenges, Kubeflow Pipelines is still widely adopted due to its support for end-to-end ML management, workflow visualization, portability, and reproducibility.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.808.1">Now that we have explored various open-source tools for building ML platforms, let’s delve into end-to-end architecture using these open-source frameworks and components.</span></p>
<h1 class="heading-1" id="_idParaDest-200"><span class="koboSpan" id="kobo.809.1">Designing an end-to-end ML platform</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.810.1">After</span><a id="_idIndexMarker735"/><span class="koboSpan" id="kobo.811.1"> discussing several open-source technologies individually, let’s now delve into their integration and see how these components come together. </span><span class="koboSpan" id="kobo.811.2">The architecture patterns and technology stack selection may vary based on specific needs and requirements. </span><span class="koboSpan" id="kobo.811.3">The following diagram presents the conceptual building blocks of an ML platform architecture:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.812.1"><img alt="A picture containing text, screenshot, diagram, design  Description automatically generated" src="../Images/B20836_07_12.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.813.1">Figure 7.12: ML platform architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.814.1">Next, let’s delve into different strategies to implement this architecture concept with different combinations of open-source technologies.</span></p>
<h2 class="heading-2" id="_idParaDest-201"><span class="koboSpan" id="kobo.815.1">ML platform-based strategy</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.816.1">When </span><a id="_idIndexMarker736"/><span class="koboSpan" id="kobo.817.1">designing an ML platform using</span><a id="_idIndexMarker737"/><span class="koboSpan" id="kobo.818.1"> open-source technologies, one effective strategy is to utilize an ML platform framework as a base platform and then integrate additional open-source components to address specific requirements. </span><span class="koboSpan" id="kobo.818.2">One such ML platform framework is Kubeflow, which provides a robust foundation with its built-in building blocks for an ML platform. </span><span class="koboSpan" id="kobo.818.3">By leveraging Kubeflow, you can benefit from its core components while extending the platform’s capabilities through the integration</span><a id="_idIndexMarker738"/><span class="koboSpan" id="kobo.819.1"> of complementary open-source tools.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.820.1">This </span><a id="_idIndexMarker739"/><span class="koboSpan" id="kobo.821.1">strategy allows for flexibility and customization by seamlessly integrating a range of open-source ML components into the platform. </span><span class="koboSpan" id="kobo.821.2">You would choose this approach if the base ML platform framework meets most of your requirements, or if you can work within the limitations of the framework. </span><span class="koboSpan" id="kobo.821.3">The following table outlines key ML platform components and their corresponding open-source frameworks and tools:</span></p>
<table class="table-container" id="table001">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.822.1">ML Platform Component</span></strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.823.1">Open-Source Framework</span></strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.824.1">Code repository</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.825.1">GitHub</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.826.1">Experimentation and model development</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.827.1">Kubeflow Jupyter Notebook</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.828.1">Experiment tracking</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.829.1">MLflow experiment tracker</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.830.1">Feature store</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.831.1">Feast feature store</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.832.1">Data annotation</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.833.1">Computer Vision Annotation Tool (CVAT)</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.834.1">Training</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.835.1">Kubeflow training operators</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.836.1">Data and model testing</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.837.1">Deepchecks</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.838.1">Model repository</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.839.1">MLflow Model Repository</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.840.1">ML pipeline development</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.841.1">Kubeflow Pipelines</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.842.1">Model inference</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.843.1">Kubeflow KFServing (Seldon Core, TFServing, Triton)</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.844.1">Docker image repository</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.845.1">Docker Hub</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.846.1">CI/CD</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.847.1">GitHub Actions</span></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.848.1">Drift monitoring</span></p>
</td>
<td class="table-cell">
<p class="normal"><span class="koboSpan" id="kobo.849.1">Deepchecks</span></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref"><span class="koboSpan" id="kobo.850.1">Table 7.1: ML platform components and their corresponding open-source frameworks</span></p>
<p class="normal"><span class="koboSpan" id="kobo.851.1">By incorporating these frameworks and tools into the architectural conceptual diagram, we can visualize the resulting diagram as follows:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.852.1"><img alt="A picture containing text, screenshot, diagram, design  Description automatically generated" src="../Images/B20836_07_13.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.853.1">Figure 7.13: Kubeflow-based ML platform</span></p>
<p class="normal"><span class="koboSpan" id="kobo.854.1">Using this</span><a id="_idIndexMarker740"/><span class="koboSpan" id="kobo.855.1"> architecture, data scientists utilize Kubeflow Jupyter Notebook for conducting experiments and building models. </span><span class="koboSpan" id="kobo.855.2">Experiment runs and relevant details, such as data statistics, hyperparameters, and model metrics, are tracked and saved in the MLflow experiment tracking component.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.856.1">Common </span><a id="_idIndexMarker741"/><span class="koboSpan" id="kobo.857.1">ML features are stored in the Feast feature store. </span><span class="koboSpan" id="kobo.857.2">When there is a need for data annotation, data annotators can employ open-source data </span><a id="_idIndexMarker742"/><span class="koboSpan" id="kobo.858.1">annotation tools like the </span><strong class="keyWord"><span class="koboSpan" id="kobo.859.1">Computer Vision Annotation Tool</span></strong><span class="koboSpan" id="kobo.860.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.861.1">CVAT</span></strong><span class="koboSpan" id="kobo.862.1">) and Label Studio to label data for model training. </span><span class="koboSpan" id="kobo.862.2">Data scientists can utilize features from the feature store and the labeled dataset as part of their experimentation and model building.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.863.1">GitHub serves as the code repository for data scientists. </span><span class="koboSpan" id="kobo.863.2">They save all source code, including training scripts, algorithm code, and data transformation scripts, in the code repository. </span><span class="koboSpan" id="kobo.863.3">Model training and inference Docker images are stored in Docker Hub. </span><span class="koboSpan" id="kobo.863.4">A Docker image build process can be deployed to create new Docker images for training and inference purposes.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.864.1">For formal model training, the training script is pulled from the GitHub repository, and the training Docker image is pulled from Docker Hub into the Kubeflow training operator to initiate the model training, along with the training dataset. </span><span class="koboSpan" id="kobo.864.2">Deepchecks can be utilized to</span><a id="_idIndexMarker743"/><span class="koboSpan" id="kobo.865.1"> perform data validation and model performance checks. </span><span class="koboSpan" id="kobo.865.2">Once the model is trained, the model artifacts, along with any metadata such as model metrics and evaluation graphs, are stored in MLflow Model Registry.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.866.1">When it is </span><a id="_idIndexMarker744"/><span class="koboSpan" id="kobo.867.1">time to deploy the model, models are fetched from MLflow Model Registry and loaded into KFServing for model inference, along with the model inference Docker image and inference script. </span><span class="koboSpan" id="kobo.867.2">KFServing offers the flexibility to choose different inference servers, including Seldon Core, TFServing, and Triton.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.868.1">Prediction logs can be sent to the model monitoring component for detecting data drift and model drift. </span><span class="koboSpan" id="kobo.868.2">Open-source software tools like Evidently AI can be employed for data drift and model drift detection.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.869.1">To orchestrate various tasks such as data processing, feature engineering, model training, and model validation, Kubeflow Pipelines can be developed. </span><span class="koboSpan" id="kobo.869.2">For </span><strong class="keyWord"><span class="koboSpan" id="kobo.870.1">CI/CD</span></strong><span class="koboSpan" id="kobo.871.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.872.1">Continuous Integration/Continuous Deployment</span></strong><span class="koboSpan" id="kobo.873.1">), GitHub Actions can be used as a triggering mechanism to initiate different pipelines.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.874.1">Overall, this approach allows you to combine the benefits of a base ML platform framework with the flexibility provided by a wide range of open-source components.</span></p>
<h2 class="heading-2" id="_idParaDest-202"><span class="koboSpan" id="kobo.875.1">ML component-based strategy</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.876.1">An alternative </span><a id="_idIndexMarker745"/><span class="koboSpan" id="kobo.877.1">approach is to</span><a id="_idIndexMarker746"/><span class="koboSpan" id="kobo.878.1"> build the ML platform using individual components rather than relying on a base ML platform framework. </span><span class="koboSpan" id="kobo.878.2">This strategy offers the advantage of selecting the best-in-class components for each aspect of the platform, allowing organizations to adhere to their existing open-source standards for core components like pipeline development or notebook IDEs. </span><span class="koboSpan" id="kobo.878.3">The following architectural pattern illustrates this approach.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.879.1"><img alt="A picture containing text, screenshot, diagram, design  Description automatically generated" src="../Images/B20836_07_14.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.880.1">Figure 7.14: Component-based architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.881.1">In this</span><a id="_idIndexMarker747"/><span class="koboSpan" id="kobo.882.1"> architecture</span><a id="_idIndexMarker748"/><span class="koboSpan" id="kobo.883.1"> pattern, alternative technologies and tools are utilized for pipeline development, notebook environments, and training infrastructure management. </span><span class="koboSpan" id="kobo.883.2">Additionally, Kubernetes serves as the infrastructure management framework. </span><span class="koboSpan" id="kobo.883.3">This approach allows organizations to leverage specific technologies and tools that align with their unique requirements and preferences.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.884.1">One notable aspect is the use of Airflow as the standard orchestration and pipeline tool across various technical disciplines, including ML and data management. </span><span class="koboSpan" id="kobo.884.2">Airflow’s widespread adoption within organizations enables it to serve as a unifying pipeline management tool, facilitating seamless integration between different components and workflows.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.885.1">Moreover, this architecture pattern emphasizes the building of custom training and inference infrastructure on top of Kubernetes. </span><span class="koboSpan" id="kobo.885.2">By leveraging Kubernetes, organizations gain the flexibility to create customized training and inference environments tailored to their specific needs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.886.1">In addition to the availability of free open-source tools that meet ML platform requirements, it is important to consider the integration of commercial components into the open-source architecture. </span><span class="koboSpan" id="kobo.886.2">These commercial offerings can enhance specific aspects of the ML platform and provide additional capabilities.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.887.1">For instance, when deploying this architecture pattern on AWS, it is advisable to explore the use of Amazon Elastic Container Registry (ECR) as the Docker image repository. </span><span class="koboSpan" id="kobo.887.2">Amazon ECR provides a managed and secure solution for storing and managing container images, integrating seamlessly with other AWS services.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.888.1">When it </span><a id="_idIndexMarker749"/><span class="koboSpan" id="kobo.889.1">comes to monitoring, there are commercial products like Fiddler and Author AI that can offer advanced features and insights. </span><span class="koboSpan" id="kobo.889.2">These tools can enhance the monitoring capabilities of the ML platform, providing in-depth analysis, model explainability, and visualization of model behavior and performance.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.890.1">Overall, the </span><a id="_idIndexMarker750"/><span class="koboSpan" id="kobo.891.1">advantages of this architecture pattern include the ability to choose alternative technologies and tools for different aspects of the ML platform, leveraging Airflow as a unifying pipeline management tool, and building custom training and inference infrastructure on Kubernetes. </span><span class="koboSpan" id="kobo.891.2">These choices enable organizations to create a tailored and optimized ML platform that aligns precisely with their requirements and allows for highly customized training and inference processes.</span></p>
<h1 class="heading-1" id="_idParaDest-203"><span class="koboSpan" id="kobo.892.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.893.1">In this chapter, you have gained an understanding of the core architecture components of a typical ML platform and their capabilities. </span><span class="koboSpan" id="kobo.893.2">We have explored various open-source technologies such as Kubeflow, MLflow, TensorFlow Serving, Seldon Core, Triton Inference Server, Apache Airflow, and Kubeflow Pipelines. </span><span class="koboSpan" id="kobo.893.3">Additionally, we have discussed different strategies for approaching the design of an ML platform using open-source frameworks and tools.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.894.1">While these open-source technologies offer powerful features for building sophisticated ML platforms, it is important to acknowledge that constructing and maintaining such environments requires substantial engineering effort and expertise, especially when dealing with large-scale ML platforms.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.895.1">In the next chapter, we will delve into fully managed, purpose-built ML solutions that are specifically designed to facilitate the development and operation of ML environments. </span><span class="koboSpan" id="kobo.895.2">These managed solutions aim to simplify the complexities of building and managing ML platforms, providing preconfigured and scalable infrastructure, as well as additional features tailored for ML workflows.</span></p>
<h1 class="heading-1"><span class="koboSpan" id="kobo.896.1">Leave a review!</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.897.1">Enjoying this book? </span><span class="koboSpan" id="kobo.897.2">Help readers like you by leaving an Amazon review. </span><span class="koboSpan" id="kobo.897.3">Scan the QR code below to get a free eBook of your choice.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.898.1"><img alt="" role="presentation" src="../Images/Review_Copy.png"/></span></p>
<p class="normal"><em class="italic"><span class="koboSpan" id="kobo.899.1">*Limited Offer</span></em></p>
</div>
</body></html>