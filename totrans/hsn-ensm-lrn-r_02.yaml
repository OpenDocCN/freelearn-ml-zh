- en: Chapter 2. Bootstrapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen in the previous chapter, statistical inference is enhanced to a very
    large extent with the use of computational power. We also looked at the process
    of permutation tests, wherein the same test is applied multiple times for the
    resamples of the given data under the (null) hypothesis. The rationale behind
    resampling methods is also similar; we believe that if the sample is truly random
    and the observations are generated from the same identical distribution, we have
    a valid reason to resample the same set of observations with replacements. This
    is because any observation might as well occur multiple times rather than as a
    single instance.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will begin with a formal definition of resampling, followed by
    a look at the jackknife technique. This will be applied to multiple, albeit relatively
    easier, problems, and we will look at the definition of the pseudovalues first.
    The bootstrap method, invented by Efron, is probably the most useful resampling
    method. We will study this concept thoroughly and vary the applications from simple
    cases to regression models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The jackknife technique**: Our first resampling method that enables bias
    reduction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bootstrap**: A statistical method and generalization of the jackknife method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The boot package**: The main R package for bootstrap methods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bootstrap and testing hypothesis**: Using the bootstrap method for hypothesis
    testing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bootstrapping regression models**: Applying the bootstrap method to the general
    regression model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bootstrapping survival models**: Applying the bootstrap method for the survival
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bootstrapping time series models**: The bootstrap method for the time series
    data – observations are dependent here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the following libraries in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ACSWR`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boot`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`car`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gee`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mvtnorm`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pseudo`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RSADBE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`survival`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The jackknife technique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quenouille (1949) invented the jackknife technique. The purpose of this was
    to reduce bias by looking at multiple samples of data in a methodical way. The
    name jackknife seems to have been coined by the well-known statistician John W.
    Tukey. Due mainly to the lack of computational power, the advances and utility
    of the jackknife method were restricted. Efron invented the bootstrap method in
    1979 (see the following section for its applications) and established the connection
    with the jackknife method. In fact, these two methods have a lot in common and
    are generally put under the umbrella of *resampling methods*.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we draw a random sample ![The jackknife technique](img/00044.jpeg)of
    size *n* from a probability distribution *F*, and we denote by ![The jackknife
    technique](img/00045.jpeg) the parameter of interest. Let ![The jackknife technique](img/00046.jpeg)
    be an estimator of ![The jackknife technique](img/00047.jpeg), and here we don't
    have the probability distribution of ![The jackknife technique](img/00048.jpeg)
    for a given ![The jackknife technique](img/00049.jpeg). Resampling methods will
    help in carrying out statistical inference when the probability distribution is
    unknown. A formal definition of the concept is in order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition: *Resampling methods* are ways of estimating the bias and variance
    of the estimator ![The jackknife technique](img/00050.jpeg) that uses the values
    of ![The jackknife technique](img/00051.jpeg) based on subsamples from the available
    observations ![The jackknife technique](img/00052.jpeg).'
  prefs: []
  type: TYPE_NORMAL
- en: The **jackknife technique** is a resampling method, and we will lay down its
    general procedure in the ensuing discussion. As stated previously, ![The jackknife
    technique](img/00053.jpeg) is an estimator of ![The jackknife technique](img/00054.jpeg).
    For simplicity, we define the vector of the given observations by ![The jackknife
    technique](img/00055.jpeg). The important quantity in setting up this procedure
    is the *pseudovalue*, and we will define this mathematically next.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**: Let ![The jackknife technique](img/00056.jpeg), that is, ![The
    jackknife technique](img/00057.jpeg) is the vector ![The jackknife technique](img/00058.jpeg)
    without the *i-th* observation. The *i-th pseudovalue* of ![The jackknife technique](img/00059.jpeg)
    is then defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The jackknife technique](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It can be mathematically demonstrated that the *pseudovalue* is equivalent
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The jackknife technique](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the *pseudovalue* is seen as the bias-corrected version of ![The jackknife
    technique](img/00062.jpeg). The pseudovalues defined here are also referred to
    as *delete-one* jackknife. The jackknife method treats the pseudovalues as independent
    observations with mean ![The jackknife technique](img/00063.jpeg), and then applies
    the *central limit theorem* for carrying out the statistical inference. The mean
    and (sampling) variance of the pseudovalues is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The jackknife technique](img/00064.jpeg)![The jackknife technique](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The jackknife method for mean and variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose the probability distribution is unknown, and the histogram and other
    visualization techniques suggest that the assumption of normal distribution is
    not appropriate. However, we don't have rich information either to formulate a
    reasonable probability model for the problem at hand. Here, we can put the jackknife
    technique to good use.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define mean and variance estimators as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The jackknife method for mean and variance](img/00066.jpeg)![The jackknife
    method for mean and variance](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The pseudovalues associated with ![The jackknife method for mean and variance](img/00068.jpeg)
    and ![The jackknife method for mean and variance](img/00069.jpeg) are respectively
    given in the following expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The jackknife method for mean and variance](img/00070.jpeg)![The jackknife
    method for mean and variance](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The mean of ![The jackknife method for mean and variance](img/00072.jpeg) will
    be the sample mean, and the mean of ![The jackknife method for mean and variance](img/00073.jpeg)
    will be sampling variance. However, the application of the jackknife method lies
    in the details. Based on the estimated mean alone, we would not be able to infer
    about the population mean, and based on the sample variance, we would not be able
    to exact inference about the population variance. To see what is happening with
    these formulas of pseudovalues and how their variances will be useful, we will
    set up an elegant R program next.
  prefs: []
  type: TYPE_NORMAL
- en: We will simulate *n = 1000* observations from the Weibull distribution with
    some scale and shape parameters. In the standard literature, we will be able to
    find the estimates of these two parameters. However, a practitioner is seldom
    interested in these parameters and would prefer to infer about the mean and variance
    of the lifetimes. The density function is a complex form. Furthermore, the theoretical
    mean and variance of a Weibull random variable in terms of the scale and shape
    parameter is easily found to be too complex, and the expressions involving Gamma
    integrals do not help the case any further. If the reader tries to search for
    the string *statistical inference for the mean of Weibull distribution* in a search
    engine, the results will not be satisfactory, and it won't be easy to proceed
    any further, except for individuals who are mathematically adept. In this complex
    scenario, we will look at how the jackknife method saves the day for us.
  prefs: []
  type: TYPE_NORMAL
- en: A note is in order before we proceed. The reader might wonder, *who cares about
    Weibull distribution in this era of brawny super-computational machines?* However,
    any reliability engineer will vouch for the usefulness of lifetime distributions,
    and Weibull is an important member of this class. The second point might be that
    the normal approximation will hold well for large samples. However, when we have
    moderate samples to carry out the inference, the normal approximation for a highly
    skewed distribution such as Weibull might lose out on the power and confidence
    of the tests. Besides, the question is that if we firmly believe that the underlying
    distribution is Weibull (without parameters) it remains a monumental mathematical
    task to obtain the exact distributions of the mean and variance of the Weibull
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R program will implement the jackknife technique for the mean and variance
    for given raw data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in earlier simulation scenarios, we plant the seed for the sake
    of reproducible results. The `rweibull` function helps to enact the task of simulating
    observations from the Weibull distribution. We calculate the mean, standard deviation,
    and variance of the sample. Next, we define the `pv_mean` function that will enable
    computation of `pseudovalues` of mean and variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the values and `pseudovalues` of the mean and the value of the observation
    are the same for all observations. In fact, this is anticipated, as the statistic
    we are looking at is the mean, which is simply the average. Removing the average
    of other observations from that should return the value. Consequently, the mean
    of the `pseudovalues` and the sample mean would be the same too. However, that
    does not imply that the efforts are futile. We will continue with the computations
    for the variance term as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, there is no counterpart to the `pseudovalue` of the observation in the
    actual data. Here, the mean of the `pseudovalues` will approximately equal the
    sample variance. This is the standard deviation `sd(pv_var)` which will help in
    carrying out the inference related to the variance or standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how the jackknife is useful in inferring the mean and variance.
    In the next part of this section, we will see how the `pseudovalues` can be useful
    in solving problems in the context of a survival regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: Pseudovalues method for survival data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary biliary cirrhosis data, `pbc`, was introduced in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    in *Section 2*, and we made a note that the data is special in that it is survival
    data and the variable of interest time is subject to censoring, which complicates
    further analysis. Specialized methods for dealing with survival data will be dealt
    with in [Chapter 10](part0070_split_000.html#22O7C2-2006c10fab20488594398dc4871637ee
    "Chapter 10. Ensembling Survival Models"), *Ensembling Survival Models*. The specialized
    methods include the hazards regression, and the impact of covariates is measured
    on the hazard rate and not on the lifetime. It has been observed that practitioners
    find these concepts a tad difficult, and hence we will briefly discuss an alternative
    approach based on the pseudovalues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Andersen and Klein have effectively used the notion of pseudovalues for various
    problems in a series of papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Bootstrap – a statistical method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore complex statistical functional. What is the
    statistical distribution of the correlation between two random variables? If normality
    assumption does not hold for the multivariate data, then what is an alternative
    way to obtain the standard error and confidence interval? Efron (1979) invented
    the *bootstrap technique*, which provides the solutions that enable statistical
    inference related to complex statistical functionals. In [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    the permutation test, which repeatedly draws samples of the given sample and carries
    out the test for each of the resamples, was introduced. In theory, the permutation
    test requires ![Bootstrap – a statistical method](img/00074.jpeg) number of resamples,
    where *m* and *n* are the number of observations in the two samples, though one
    does take their foot off the pedal after having enough resamples. The bootstrap
    method works in a similar way and is an important resampling method.
  prefs: []
  type: TYPE_NORMAL
- en: Let ![Bootstrap – a statistical method](img/00075.jpeg) be an independent random
    sample from a probability distribution F, the parameter of interest be ![Bootstrap
    – a statistical method](img/00076.jpeg), and an estimator of the parameter be
    denoted by ![Bootstrap – a statistical method](img/00077.jpeg). If the probability
    distribution of ![Bootstrap – a statistical method](img/00077.jpeg) for the ![Bootstrap
    – a statistical method](img/00078.jpeg) parameter is either unknown or intractable,
    then the statistical inference about the parameter can't be carried out. Consequently,
    we need a generic technique that will aid in the inference.
  prefs: []
  type: TYPE_NORMAL
- en: Efron's method unfolds as follows. The estimate provided by ![Bootstrap – a
    statistical method](img/00077.jpeg) is a single value. Given the data and based
    on the assumption that we have an IID sample, the bootstrap method explores the
    possibility that any observed value is as likely as any other observed value.
    Thus, a random sample of size *n* drawn *with replacement* is intuitively expected
    to carry the same information as the actual sample, and we can obtain an estimate
    of the ![Bootstrap – a statistical method](img/00079.jpeg) parameter based on
    this sample. This step can then be repeated a large number of times, and we will
    produce a varied number of estimates of the parameter. Using this distribution
    of estimates, statistical inference can then be performed. A formal description
    of this method is in order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Draw a random sample with replacement of size *n* from ![Bootstrap – a statistical
    method](img/00080.jpeg) and denote it by ![Bootstrap – a statistical method](img/00081.jpeg).
    The sample ![Bootstrap – a statistical method](img/00082.jpeg) is referred to
    as the *first bootstrap sample*. Compute the estimate ![Bootstrap – a statistical
    method](img/00083.jpeg) for this sample ![Bootstrap – a statistical method](img/00084.jpeg)
    and denote it by ![Bootstrap – a statistical method](img/00085.jpeg). Repeat the
    steps a large number of times and obtain ![Bootstrap – a statistical method](img/00086.jpeg).
    The inference for ![Bootstrap – a statistical method](img/00087.jpeg) can then
    be based on the bootstrap estimates ![Bootstrap – a statistical method](img/00088.jpeg).
    We can put this description in the form of an algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Given data ![Bootstrap – a statistical method](img/00089.jpeg), the parameter
    of interest ![Bootstrap – a statistical method](img/00090.jpeg), calculate the
    estimate ![Bootstrap – a statistical method](img/00091.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a bootstrap sample of size *n* with replacement from ![Bootstrap – a statistical
    method](img/00092.jpeg), and denote it by ![Bootstrap – a statistical method](img/00093.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the statistic ![Bootstrap – a statistical method](img/00094.jpeg)
    for the bootstrap sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Repeat Steps 2 and 3 'B – 1'* number of times to produce ![Bootstrap
    – a statistical method](img/00095.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use ![Bootstrap – a statistical method](img/00096.jpeg) to carry out the statistical
    inference related to ![Bootstrap – a statistical method](img/00097.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What does *Step 5* convey here? We have each of the ![Bootstrap – a statistical
    method](img/00098.jpeg) values estimate the parameter of interest, *B* estimates
    to be precise. Since the estimate (based on the sample) is ![Bootstrap – a statistical
    method](img/00099.jpeg), we (intuitively) expect that the average of the bootstrap
    estimates ![Bootstrap – a statistical method](img/00100.jpeg) will be very close
    to ![Bootstrap – a statistical method](img/00101.jpeg), hence the variance of
    the bootstrap estimates also gives a ''good'' measure of the variance of the estimator
    too. The bootstrap mean and standard deviation are then computed as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bootstrap – a statistical method](img/00102.jpeg)![Bootstrap – a statistical
    method](img/00103.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using ![Bootstrap – a statistical method](img/00104.jpeg) and ![Bootstrap –
    a statistical method](img/00105.jpeg), we can carry out inference for ![Bootstrap
    – a statistical method](img/00106.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the bootstrap method is a very generic algorithm, and
    the execution of this is illustrated to address occasions when faced with certain
    interesting problems.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of sampling with replacement needs to be elucidated here. For the purpose
    of simplicity, assume that we have only five observations, for example ![Bootstrap
    – a statistical method](img/00107.jpeg). Now, when we draw the first bootstrap
    sample with a replacement size of 5, we might get the labels *2*, *4*, *4*, *1*,
    *3*. This means that, from the original sample, we select ![Bootstrap – a statistical
    method](img/00108.jpeg), and consequently the observations labeled *2*, *1*, and
    *3* are selected once and *4* is selected twice. This is the same as ![Bootstrap
    – a statistical method](img/00109.jpeg). In the bootstrap notation, it would be
    ![Bootstrap – a statistical method](img/00110.jpeg). The second bootstrap might
    be ![Bootstrap – a statistical method](img/00111.jpeg), and the third might be
    ![Bootstrap – a statistical method](img/00112.jpeg), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will illustrate this technique and clarify its implementation. The
    bootstrap method will be applied to two problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Standard error of correlation coefficient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvalue of the covariance/correlation matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standard error of correlation coefficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider a hypothetical scenario where we are trying to study the relationship
    between two variables. The historical information, intuition, and scatterplot
    all align, showing that there is a linear relationship between the two variables,
    and the only problem is that the histogram of each of the two variables suggests
    a shape that is anything but a bell shape. In other words, the assumption of normal
    distribution looks very unlikely, and since it fails in the univariate cases (of
    each variable), the analyst is skeptical about the joint bivariate normality holding
    true for the two variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![The standard error of correlation coefficient](img/00113.jpeg) be *n*
    pairs of observations. The sample correlation coefficient is easily calculated
    using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The standard error of correlation coefficient](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here we have ![The standard error of correlation coefficient](img/00115.jpeg).
    An estimate of any parameter is no good if we can't carry out the relevant statistical
    inference. A confidence interval suffices with the relevant results to perform
    the statistical inference. We will now learn how the bootstrap method helps us
    to do that. We will use the vector notation to maintain consistency, and toward
    this define ![The standard error of correlation coefficient](img/00116.jpeg),
    that is, ![The standard error of correlation coefficient](img/00117.jpeg) is a
    vector now. The first bootstrap sample is obtained by selecting n pairs of observations
    randomly and with replacement, and we will denote the first bootstrap sample by
    ![The standard error of correlation coefficient](img/00118.jpeg). Now, using the
    bootstrap sample, we will estimate the correlation coefficient by ![The standard
    error of correlation coefficient](img/00119.jpeg). Repeating the process of obtaining
    the bootstrap samples *B – 1* more times, we will compute the correlation coefficients
    ![The standard error of correlation coefficient](img/00120.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: 'The law school data is used here, drawn from Table 3.1 of Efron and Tibshirani
    (1990, p.19). In this study, fifteen schools are randomly selected from a pool
    of 82 law schools. Two variables measured for the schools include the average
    score for the class on a national law test (LSAT) and the average undergraduate
    grade point (GPA). We will first import the data from the CSV file, display it,
    and then visualize the histograms of the two variables along with the scatterplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will look at the code first. The `read.csv` file helps in importing the
    dataset from the chapter''s `Data` folder, as unzipped from the code bundle and
    stored in the `LS` object. The `LS` is then displayed in the console. Here, we
    give the first and last three observations. The `windows` function creates a new
    graphical device with specified `height` and `weight`. Note that this function
    will only work on the Windows OS. Next, we specify the `layout` for the graphical
    device. To confirm that it''s working, running the line `matrix(c(1,2,3,3),byrow=TRUE,
    nrow=2)` in R terminal gives the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that the first graphical output, as a consequence of running any
    code that results in a graph, is displayed in region 1 (upper left) of the device.
    The second graphical output is displayed in the right upper part, while the third
    will be spread across the lower part. It is a convenient manipulation for visual
    displays. The histogram of the two variables does not suggest normal distribution,
    though it may be argued that the number of observations is much less; fifteen,
    in this case. However, the scatter plot suggests that as `LSAT` increases, the
    `GPA` does too. Consequently, the correlation coefficient is a meaningful measure
    of the linear relationship between the two variables. However, the normal distribution
    assumption is not suitable here, or at least we need more observations that are
    not available as of now, and hence it remains a challenge to carry out the statistical
    inference. To overcome this, we will use the bootstrap technique. Take a look
    at the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The standard error of correlation coefficient](img/00121.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: LSAT and GPA variables visualization'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imitating Efron and Tibshirani''s illustration, we fix the number of bootstrap
    samples at 3200, and we will be interested when the number of bootstrap samples
    is at 25, 50, 100, 200, 400, 800, 1600, and 3200\. The R program approach is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: After finding the number of observations, fixing the number of bootstrap samples,
    and bootstrap samples of interest, we will initialize the bootstrap mean and standard
    vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the purpose of replicating the results, we will fix the initial seed at
    54321\. The seed will be stepped up by an increment of 1 for obtaining the bootstrap
    sample, which will ensure that all the bootstrap samples are different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value of the correlation coefficient is computed for each of the bootstrap
    samples, and thus we will have B = 3200 correlation coefficients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mean and standard deviation of the correlation coefficients up to the desired
    number of bootstrap samples is calculated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For comparisons with Efron and Tibshirani (1990, p.50), the results are reported
    for bootstrap samples 25, 50, 100, 200, 400, 800, 1600, and 3200.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The R program along with the output is given next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The time series plot is displayed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The standard error of correlation coefficient](img/00122.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The nonparametric bootstrap standard error for correlation coefficient'
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard error of the correlation coefficient can be seen to stabilize
    at around `0.13`. Finally, to carry out the statistical inference, we can use
    the bootstrap confidence intervals. A `naïve` method is to simply obtain the 95%
    coverage of the correlation coefficient estimates in the bootstrap samples. This
    is easily achieved in the software. We will use the quantile function to achieve
    the result, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We have learned how to carry out statistical inference using the bootstrap technique.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main reasons for carrying out the bootstrap method here is that we
    can't assume bivariate normal distribution for the LSAT and GPA variables. Now,
    if we are told that the distribution of LSAT and GPA historically follows bivariate
    normal distribution, then technically the probability distribution of the sample
    correlation coefficient ![The standard error of correlation coefficient](img/00123.jpeg)
    can be derived. However, as a practitioner, suppose that you are unable to derive
    the probability distribution of the sample correlation coefficient. How do you
    then carry out the statistical inference? Using the same technique as discussed
    here might seem tempting. We will continue to explore this matter in the next
    subsection.
  prefs: []
  type: TYPE_NORMAL
- en: The parametric bootstrap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the previous subsection, it might be tempting to carry out the
    usual *nonparametric* bootstrap method. However, nonparametric methods are traditionally
    known to be somewhat inefficient compared with the parametric methods. We will
    now look at a mixture of both of these methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen that the bootstrap method relies heavily on the resamples. The
    bootstrap samples and the consequent estimates are then expected to meet the true
    underlying probability distributions. However, we might occasionally know more
    about the shape of the underlying probability distributions, except for a few
    parameters. The approach for mixing up these two methods will require a modification.
    The parametric bootstrap method is set up and run as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let ![The parametric bootstrap](img/00124.jpeg) be the IID sample from ![The
    parametric bootstrap](img/00125.jpeg), and let ![The parametric bootstrap](img/00126.jpeg)
    denote an estimator of the parameter based on an appropriate method, say maximum
    likelihood estimation or method of moments, for example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulate the first bootstrap sample ![The parametric bootstrap](img/00127.jpeg)
    of size *n* from ![The parametric bootstrap](img/00128.jpeg), and obtain the first
    bootstrap estimate ![The parametric bootstrap](img/00129.jpeg) based on ![The
    parametric bootstrap](img/00130.jpeg) using the same estimation technique as used
    in the previous step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the previous step *B – 1* number of times to obtain ![The parametric
    bootstrap](img/00131.jpeg) respectively, based on the bootstrap samples ![The
    parametric bootstrap](img/00132.jpeg).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carry out the inference based on the *B* bootstrap estimates ![The parametric
    bootstrap](img/00133.jpeg).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The parametric bootstrap technique will be illustrated using the earlier example
    of `LSAT` and `GPA` variables. For the bivariate normal distribution, the mean
    vector is an estimator of the population mean, and it enjoys statistical properties
    as being the unbiased estimator and MLE. Similarly, the sample variance-covariance
    matrix also gives an important estimate of the population variance-covariance
    matrix. The `colMeans` is applied on the data frame to obtain the vector mean
    and the `var` function to compute the sample variance-covariance matrix. The R
    code block easily follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, we have the mean and variance-covariance matrix estimators. We now look
    at the parametric bootstrap computations. Now, using the `rmvnorm` function from
    the `mvtnorm` package, we are able to simulate observations from a multivariate
    (bivariate) normal distribution. With the (parametric) bootstrap sample available,
    the rest of the program and conclusion is similar. The complete R program with
    the resulting diagram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The difference between parametric and nonparametric bootstrap can easily be
    seen. The confidence intervals are very short, and the standard error decreases
    to zero as the number of bootstrap samples increases. In spite of the advantage,
    we generally need bootstrap methods when the parametric methods fail. Take a look
    at the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The parametric bootstrap](img/00134.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The parametric bootstrap standard error for correlation coefficient'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will consider a slightly complex problem for the application of the
    bootstrap method.
  prefs: []
  type: TYPE_NORMAL
- en: Eigen values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multivariate statistics is the arm of Statistics which deals with a vector of
    random variables. In the previous example, we have bivariate data, where LSAT
    and GPA scores are obtained for fifteen schools. Now we will consider another
    example, where we have more than two variables; namely we have five observations
    here. The description and bootstrap technique-related details are drawn from [Chapter
    7](part0051_split_000.html#1GKCM1-2006c10fab20488594398dc4871637ee "Chapter 7. The
    General Ensemble Technique"), *The General Ensemble Technique*, of Efron and Tibshirani
    (1990). The chapter discusses the score data from the classic multivariate book
    by Mardia, Kent, and Bibby (1979).
  prefs: []
  type: TYPE_NORMAL
- en: A quick brief of notation is as follows. We will denote the vector of random
    variables by ![Eigen values](img/00135.jpeg), and for the *ith* observation, the
    vector will be ![Eigen values](img/00136.jpeg). Here, each component *Xi* is assumed
    to be a continuous random variable. Most often, and for practical and theoretical
    purposes, we assume that the random vector follows a multivariate normal distribution
    with mean vector ![Eigen values](img/00137.jpeg) and variance-covariance matrix
    ![Eigen values](img/00138.jpeg). Since it is not feasible to go into the details
    of multivariate statistics here, the interested reader might simply consult Mardia,
    Kent, and Bibby (1979).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, *n = 88* students'' scores are noted for the five subjects
    of mechanics, vectors, algebra, analysis, and statistics, and a further difference
    in the test is that the first two subjects, mechanics and vectors, were closed-book
    tests while algebra, analysis, and statistics were open-book exams. We will first
    perform the simple preliminary task here of calculating the mean vector, the variance-covariance
    matrix, and the correlation matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the data is imported from a `.csv` file, and using the `colMeans`, `cov`,
    and `cor` functions, we obtain the mean vector, variance-covariance matrix, and
    correlation matrix. Clearly, we can see from the output of the correlation matrix
    that a strong association exists between all variables. The visual depiction of
    the data is obtained by the `pairs` function, which gives us a matrix of scatter
    plots. This plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigen values](img/00139.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Matrix of scatter plots for the five subjects scores'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dimensionality reduction is one of the goals of multivariate statistics. Given
    a large number of variables, the intent of dimensionality reduction is to find
    a set of variables that will explain most of the variability in the overall data.
    A method of dimensionality reduction is *principal component analysis*. Here,
    we try to find a new random vector ![Eigen values](img/00140.jpeg), which is a
    *vector of principal components*. Each component of this new random vector is
    some linear combination of the original variables which will achieve two objectives:
    (a) the components ![Eigen values](img/00141.jpeg) will be ordered in the sense
    that the first component will have variance larger than the second, the second
    larger than the third, and so on, and (b) each principal component is uncorrelated
    with the others. The core working of the principal components is tied with the
    `eigen` values of the variance-covariance matrix or the correlation matrix. The
    `eigen` values of the variance-covariance matrix indicates the importance of the
    associated principal components. Consequently, if have p related random variables,
    and the estimated variance-covariance matrix is not singular, the normalized p
    `eigen` values will give us the fraction of the variation explained by the principal
    component. For the purpose of the data, we will explain this here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The first `eigen` value is `686.9898`, the second one is `202.1111`, and so
    on. Now, these values divided by their cumulative sum gives the percentage of
    variation in the data explained by the principal component. Thus, the total variation
    of data explained by the first principal component is 61.91%, while 18.21% is
    explained by the second principal component. Here comes the important question
    then: how do we conduct the statistical inference related to this quantity? Naturally,
    we will provide the answer using the bootstrap method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Eigen values](img/00142.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Bootstrap standard error of the variance explained by the first principal
    component'
  prefs: []
  type: TYPE_NORMAL
- en: 'The 95% bootstrap confidence interval is obtained in the usual way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Rule of thumb
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generally, the number of B = 25 bootstrap replications is enough and one rarely
    requires more than `200` replications. For more information on this, see Efron
    and Tibshirani (1990, p.52).
  prefs: []
  type: TYPE_NORMAL
- en: Thus far we have used simulation, resampling, and loops to carry out the bootstrap
    inference. However, earlier in the chapter we mentioned the `boot` package. In
    the following section, we will use the package for some samples and illustrate
    its use.
  prefs: []
  type: TYPE_NORMAL
- en: The boot package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `boot` package is one of the core R packages, and it is optimized for the
    implementation of bootstrap methods. In the previous examples, we mostly used
    loops for carrying out the resampling technique. Here, we will look at how to
    use the `boot` R package.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main structure of the boot function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The central arguments of the function are `data`, `statistic`, `R`, and `stype`.
    The `data` argument is the standard one, as with most R functions. The `statistic`
    is the most important argument for the implementation of the `boot` function and
    it is this function that will be applied on the bootstrap samples obtained from
    the `data` frame. The argument `R` (and not the software) is used to specify the
    number of bootstrap samples to be drawn, and `stype` will indicate the second
    argument of `statistic`. For any inference to be completed using the `boot` function,
    the critical task is to define the function for the statistic. We will continue
    the illustration using earlier examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the study of correlation between the `LSAT` and `GPA` variables, the trick
    is to define the function that will include the correlation coefficient function
    and the data with index specified in a manner that will give us the bootstrap
    sample. After declaring the function for the computation of the correlation coefficient
    for the bootstrap sample, we use the boot function, introduce the function, and
    specify the resampling type as well as the number of required bootstrap samples.
    The `boot` function will be in action now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The correlation function is defined through `corx`, and the boot function is
    applied on it with the data frame `LS`. The number of bootstrap samples is `200`
    and the resampling will occur until the next iteration. From the preceding output
    we can obtain the value of the statistic as `0.7763745`, the bias as `-0.01791293`,
    and the bootstrap standard error as `0.1357282`. But what about bias? We have
    made almost no mention of bias in our discussion thus far. To understand what
    the bootstrap bias is, we will first look at the components of the fitted `corboot
    boot` object. The value of the statistic, correlation coefficient here, is stored
    as `t0`, the bootstrap sample estimates (`R` of them) in `t`, and using these
    two quantities we will find the bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see how useful the `boot` function is for the applications. The `confint`
    function can be slapped on the `corboot` object to obtain the bootstrap confidence
    interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will apply the `boot` function for the problem of obtaining the confidence
    interval of the variation explained by the first principal component. To that
    end, we first create the necessary `R` function that can be supplied to the boot
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the boot package can be effectively used without having the need to write
    loops. We have used the bootstrap method for the main purpose of estimating the
    parameters and their functions. Hypothesis testing based on bootstrap will be
    covered next.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap and testing hypotheses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We begin the bootstrap hypothesis testing problems with the t-test to compare
    means and the F-test to compare variances. It is understood that, since we are
    assuming normal distribution for the two populations under comparison, the distributional
    properties of the test statistics are well known. To carry out the nonparametric
    bootstrap for the t-statistic based on the t-test, we first define the function,
    and then run the bootstrap function boot on the Galton dataset. The Galton dataset
    is available in the `galton data.frame` from the `RSADBE` package. The `galton`
    dataset consists of `928` pairs of observations, with the pair consisting of the
    height of the parent and the height of their child. First, we define the `t2`
    function, load the Galton dataset, and run the boot function as the following
    unfolds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The reader should compare the bootstrap confidence interval and the confidence
    interval given by the t-statistic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will carry out the bootstrap hypothesis testing for the variances.
    The variance function is defined for the `var.test` function and it will then
    be used in the `boot` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The confidence interval and the bootstrap confidence interval can be compared
    by the reader. The bootstrap methods have been demonstrated for different estimation
    and hypothesis testing scenarios. In the remaining sections, we will consider
    some regression models where we have additional information on the observations
    in terms of the explanatory variables.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `US Crime` dataset introduced in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    is an example of why the linear regression model might be a good fit. In this
    example, we are interested in understanding the crime rate (R) as a function of
    thirteen related variables such as average age, the southern state indicator,
    and so on. Mathematically, the linear regression model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bootstrapping regression models](img/00143.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Bootstrapping regression models](img/00144.jpeg) are the p-covariates,
    ![Bootstrapping regression models](img/00145.jpeg) is the intercept term, ![Bootstrapping
    regression models](img/00146.jpeg) are the regression coefficients, and ![Bootstrapping
    regression models](img/00147.jpeg) is the error term assumed to follow a normal
    distribution ![Bootstrapping regression models](img/00148.jpeg). The covariates
    can be written in a vector form and the *ith* observation can be summarized as
    ![Bootstrapping regression models](img/00149.jpeg), where ![Bootstrapping regression
    models](img/00150.jpeg). The *n* observations ![Bootstrapping regression models](img/00151.jpeg),
    are assumed to be stochastically independent. The linear regression model has
    been detailed in many classical regression books; see Draper and Smith (1999),
    for instance. A recent book that details the implementation of the linear regression
    model in R is Ciaburro (2018). As the reader might have guessed, we will now fit
    a linear regression model to the US Crime dataset to kick off the discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: It can be seen from the `summary` output that a lot of information is displayed
    about the fitted linear regression model. From the output, we can find the estimated
    regression coefficients in `Estimate`. The standard error of these estimators
    is in `Std`. `Error`, the corresponding value of the t-statistic in `t value`,
    and the p-values in `Pr(>|t|)`. We can further estimate the residual standard
    deviation ![Bootstrapping regression models](img/00152.jpeg) in `Residual standard
    error`. Similarly, we can obtain the respective multiple and adjusted R-square
    values in `Multiple R-squared` and `Adjusted R-squared`, the overall F-statistic
    in `F-statistic`, and finally, the model p-value in `p-value`. Many of these statistics/quantities/summaries
    have clean statistical properties and as such, exact statistical inference regarding
    the parameters can be carried out. However, this is not the case for a few of
    them. For instance, if one asks for a confidence interval of the adjusted R-square
    value, the author is not able to recollect the corresponding statistical distribution.
    Hence, using the convenience of the bootstrap technique, we can obtain the bootstrap
    confidence interval for Adjusted R-square. The reason the confidence interval
    of the Adjusted R-square might be sought is that it has a very good interpretation
    of explaining the variance explained in the Y's by the model. Let us look at its
    implementation in the R software.
  prefs: []
  type: TYPE_NORMAL
- en: 'With complex problems there will be many solutions, and none with a proven
    advantage over the other. Nevertheless, we have two main ways of carrying out
    the bootstrap for the linear regression model: (i) bootstrapping the residuals,
    and (ii) bootstrapping the observations. The two methods can work for any general
    regression scenario too. Before we describe the two methods, let ![Bootstrapping
    regression models](img/00153.jpeg) denote the least squares estimated of ![Bootstrapping
    regression models](img/00154.jpeg), and the fitted model will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bootstrapping regression models](img/00155.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Consequently, we will also have an estimate of the variance term of the error
    distribution and will denote it by ![Bootstrapping regression models](img/00156.jpeg).
    Define the vector of residual by ![Bootstrapping regression models](img/00157.jpeg).
    The **residual bootstrapping** method is then carried out in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Draw a sample of size *n* with replacement from ![Bootstrapping regression models](img/00158.jpeg)
    and denote it by ![Bootstrapping regression models](img/00159.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the resampled ![Bootstrapping regression models](img/00159.jpeg), obtain
    the new regressands using ![Bootstrapping regression models](img/00160.jpeg).
    That is, ![Bootstrapping regression models](img/00161.jpeg) is the (first) bootstrap
    sample **Y** value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using ![Bootstrapping regression models](img/00162.jpeg) and the covariate matrix
    ![Bootstrapping regression models](img/00163.jpeg), obtain the first bootstrap
    estimate of the regression coefficient vector ![Bootstrapping regression models](img/00164.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the process a large number of times, say *B*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bootstrapping the observations is the usual bootstrapping method, which does
    not require any further explanation. However, the rank of ![Bootstrapping regression
    models](img/00165.jpeg) might be affected, especially if a covariate is a discrete
    variable and only one factor is chosen. Hence, for any regression problem, bootstrapping
    the residual is the best approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'The regular `boot` package won''t be useful, and we will instead use the `Boot`
    function from the `car` package to perform the bootstrap analysis on the linear
    regression model. The `Boot` function will also be required to be a specified
    function whose output will give the value of the required statistic. Consequently,
    we will first define a function `f`, which will return the adjusted R-square value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Thus, a 95% bootstrap confidence interval for the adjusted R-square is `(0.5244243,
    0.7639986)`. Similarly, inference related to any other parameter of the linear
    regression model can be carried out using the bootstrap technique.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping survival models*
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first section, we looked at the role of pseudovalues in carrying out
    inference related to survival data. The main idea behind the use of pseudovalues
    is to replace the incomplete observations with an appropriate (expected) value
    and then use the flexible framework of the generalized estimating equation. Survival
    analysis and the related specialized methods for it will be detailed in [Chapter
    10](part0070_split_000.html#22O7C2-2006c10fab20488594398dc4871637ee "Chapter 10. Ensembling
    Survival Models"), *Ensembling Survival Models*, of the book. We will briefly
    introduce the notation here as required to set up the parameters. Let *T* denote
    the survival time, or the time to the event of interest, and we naturally have
    ![Bootstrapping survival models*](img/00166.jpeg), which is a continuous random
    variable. Suppose that the lifetime cumulative distribution is F and the associated
    density function is *f*. Since the lifetimes *T* are incomplete for some of the
    observations and subject to censoring, we will not be able to properly infer about
    interesting parameters such as mean survival time or median survival time. Since
    there are additional complications because of censoring, it suffices to note here
    that we will be borrowing heavily from the material in [Chapter 10](part0070_split_000.html#22O7C2-2006c10fab20488594398dc4871637ee
    "Chapter 10. Ensembling Survival Models"), *Ensembling Survival Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* Asterisked sections can be omitted on the first reading, or you can continue
    if you are already familiar with the related concepts and terminologies.'
  prefs: []
  type: TYPE_NORMAL
- en: The `censboot` function from the `boot` package is developed to handle survival
    data. In the `pbc` dataset, the time to event of interest is the variable named
    `time` and the completeness of the observation is indicated by `status==2`. The
    package `survival` is required to create the `Surv` objects tenable to handle
    the survival data. The `survfit` function would then give us an estimate of the
    survival function, which is the complement of the cumulative distribution function
    1-F. It is well known that the mean of a continuous non-negative random variable
    is ![Bootstrapping survival models*](img/00167.jpeg), and that the median survival
    time is that time point u which satisfies the condition of ![Bootstrapping survival
    models*](img/00168.jpeg). Since the `summary` of the `survfit` object can be used
    to obtain the survival probabilities at the desired time, we will use it to find
    the median survival time. All these arguments are built in the `Med_Surv` function,
    which will return the median survival time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `Med_Surv` function as the formula/statistic for the `censboot` function,
    we will be able to obtain the bootstrap estimates of the median survival time;
    subsequently, using the bootstrap estimates, we obtain the confidence interval
    for the median survival time. The R program and the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: For the actual data, the estimated median survival time is `3395` days. The
    95% bootstrap confidence interval for the median survival time is `(3090, 3853)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To carry out the inference about the mean survival time, we need to use the
    `survmean` function from the `survival` package and appropriately extract the
    estimated mean survival time. The `Mean_Surv` function delivers this task. The
    R program and its output are given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The reader is left with the task of obtaining the bootstrap confidence interval
    for the mean survival time. The following section will discuss using the bootstrap
    method for time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping time series models*
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An example of the time series data was seen in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    in the `New Zealand Overseas` dataset. See [Chapter 10](part0070_split_000.html#22O7C2-2006c10fab20488594398dc4871637ee
    "Chapter 10. Ensembling Survival Models"), *Ensembling Survival Models*, of Tattar
    et al. (2016). Time series is distinctive in that the observations are not stochastically
    independent of each other. For example, the maximum temperature of the day is
    very unlikely to be independent of the previous day''s maximum temperature. However,
    we are likely to believe that the maximum temperature of a block of ten previous
    days is mostly independent of a ten-day block six months ago. Thus, the `bootstrap`
    method is modified to the `block bootstrap` method. The `tsboot` function from
    the `boot` package is useful to bootstrap time series data. The main structure
    of the `tsboot` function appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `statistic`, `tseries` is the time series data, which is the usual function
    of interest to us. `R` is the number of bootstrap replicates, and `l` is the length
    of the block, which we draw from the time series data. Now, we consider the problem
    of estimating the variance for an autoregressive (AR) time series model and we
    will consider a maximum order, `order.max`, of the AR model at 25\. The `Var.fun`
    function will fit the best AR model and obtain the variance. This function will
    then be fed to the `tsboot` and, using the statistic calculated for each bootstrap
    sample, we will obtain the 95% bootstrap confidence interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Consequently, we have been able to apply the bootstrap methods for the time
    series data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main purpose of dealing with the bootstrap method in detail is that it
    lays the foundation for the resampling methods. We began the chapter with a very
    early resampling method: the jackknife method. This method is illustrated for
    the purpose of multiple scenarios, including survival data, which is inherently
    complex. The bootstrap method kicked off for seemingly simpler problems, and then
    we immediately applied it to complex problems, such as principal components and
    regression data. For the regression data, we also illustrated the bootstrap method
    for survival data and time series data. In the next chapter, we will look at the
    central role the bootstrap method plays in resampling decision trees, a quintessential
    machine learning tool.'
  prefs: []
  type: TYPE_NORMAL
