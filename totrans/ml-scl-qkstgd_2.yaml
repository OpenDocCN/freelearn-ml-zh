- en: Scala for Regression Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn regression analysis in detail. We will start
    learning from the regression analysis workflow followed by the **linear regression**
    (**LR**) and **generalized linear regression** (**GLR**) algorithms. Then we will
    develop a regression model for predicting slowness in traffic using LR and GLR
    algorithms and their Spark ML-based implementation in Scala. Finally, we will
    learn the hyperparameter tuning with cross-validation and the grid searching techniques.
    Concisely, we will learn the following topics throughout this end-to-end project:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: An overview of regression analysis
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression analysis algorithms
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning regression analysis through examples
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized linear regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning and cross-validation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapters can be found on GitHub:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter02](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter02)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2GLlQTl](http://bit.ly/2GLlQTl)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: An overview of regression analysis
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we already gained some basic understanding of the **machine
    learning** (**ML**) process, as we have seen the basic distinction between regression
    and classification. Regression analysis is a set of statistical processes for
    estimating the relationships between a set of variables called a dependent variable
    and one or multiple independent variables. The values of dependent variables depend
    on the values of independent variables.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'A regression analysis technique helps us to understand this dependency, that
    is, how the value of the dependent variable changes when any one of the independent
    variables is changed, while the other independent variables are held fixed. For
    example, let''s assume that there will be more savings in someone''s bank when
    they grow older. Here, the amount of **Savings** (say in million $) depends on
    age (that is, **Age** in years, for example):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '| **Age (years)** | **Savings (million $)** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
- en: '| 40 | 1.5 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
- en: '| 50 | 5.5 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: '| 60 | 10.8 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
- en: '| 70 | 6.7 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
- en: So, we can plot these two values in a 2D plot, where the dependent variable
    (**Savings**) is plotted on the *y*-axis and the independent variable (**Age**)
    should be plotted on the *x*-axis. Once these data points are plotted, we can
    see correlations. If the theoretical chart indeed represents the impact of getting
    older on savings, then we'll be able to say that the older someone gets, the more
    savings there will be in their bank account.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Now the question is how can we tell the degree to which age helps someone to
    get more money in their bank account? To answer this question, one can draw a
    line through the middle of all of the data points on the chart. This line is called
    the regression line, which can be calculated precisely using a regression analysis
    algorithm. A regression analysis algorithm takes either discrete or continuous
    (or both) input features and produces continuous values.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: A classification task is used for predicting the label of the class attribute,
    while a regression task is used for making a numeric prediction of the class attribute.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Making a prediction using such a regression model on unseen and new observations
    is like creating a data pipeline with multiple components working together, where
    we observe an algorithm''s performance in two stages: learning and inference.
    In the whole process and for making the predictive model a successful one, data
    acts as the first-class citizen in all ML tasks.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Learning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the important task at the learning stage is to prepare and convert the
    data into feature vectors (vectors of numbers out of each feature). Training data
    in feature vector format can be fed into the learning algorithms to train the
    model, which can be used for inferencing. Typically, and of course based on data
    size, running an algorithm may take hours (or even days) so that the features
    converge into a useful model as shown in the following diagram:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e9cb443-9b8c-406f-b6ad-bd6718fbba96.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Learning and training a predictive model—it shows how to generate the feature
    vectors from the training data to train the learning algorithm that produces a
    predictive model
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Inferencing
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the inference stage, the trained model is used for making intelligent use
    of the model, such as predicting from never-before-seen data, making recommendations,
    and deducing future rules. Typically, it takes less time compared to the learning
    stage and sometimes even in real time. Thus, inferencing is all about testing
    the model against new (that is, unobserved) data and evaluating the performance
    of the model itself, as shown in the following diagram:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c8a4f15-9b49-4993-8a42-50525807292c.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Inferencing from an existing model towards predictive analytics (feature vectors
    are generated from unknown data for making predictions)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: In summary, when using regression analysis the goal is to predict a continuous
    target variable. Now that we know how to construct a basic workflow for a supervised
    learning task, knowing a little about available regression algorithms will provide
    a bit more concrete information on how to apply these regression algorithms.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis algorithms
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are numerous algorithms proposed and available, which can be used for
    the regression analysis. For example, LR tries to find relationships and dependencies
    between variables. It models the relationship between a continuous dependent variable
    *y* (that is, a label or target) and one or more independent variables, *x*, using
    a linear function. Examples of regression algorithms include the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear regression** (**LR**)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalized linear regression** (**GLR**)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Survival regression** (**SR**)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Isotonic regression** (**IR**)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision tree regressor** (**DTR**)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forest regression** (**RFR**)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient boosted trees regression** (**GBTR**)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We start by explaining regression with the simplest LR algorithm, which models
    the relationship between a dependent variable, *y*, which involves a linear combination
    of interdependent variables, *x*:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11963a25-c48f-40da-a3e7-5f14b4f4ff65.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation letters, *β[0]* and *β[1]* are two constants for *y*-axis
    intercept and the slope of the line, respectively. LR is about learning a model,
    which is a linear combination of features of the input example (data points).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following graph and imagine that the red line is not there.
    We have a few dotted blue points (data points). Can we reasonably develop a machine
    learning (regression) model to separate most of them? Now, if we draw a straight
    line between two classes of data, those get almost separated, don''t they? Such
    a line (red in our case) is called the decision boundary, which is also called
    the regression line in the case of regression analysis (see the following example
    for more):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed09f3bd-c70c-461d-96d9-ceab36e4465d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: 'If we are given a collection of labeled examples, say ![](img/4bbeb78b-a1d8-46cf-a394-d26d3489e864.png), where
    *N* is the number of samples in the dataset, *x[i]* is the *D*-dimensional feature
    vector of the samples *i = 1, 2… N*, and *y[i]* is a real-valued *y ∈ R*, where
    *R* denotes the set of all real numbers called the target variable and every feature *x[i ]*is
    a real number. Then combining these, the next step is to build the following mathematical
    model, *f*:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/000a67b5-7349-4298-8ca3-3c8e63c0d435.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Here, *w* is a *D*-dimensional parameterized vector and *b* is a real number.
    The notation *f[w,b]* signifies that the model *f* is parameterized by values
    *w* and *b*. Once we have a well-defined model, it can now be used for making
    a prediction of unknown *y* for a given *x,* that is, *y ← f[w,b ](x)*. However,
    there is an issue, as since the model is parametrized with two different values
    (*w*, *b*), this will mean the model tends to produce two different predictions
    when applied to the same sample, even when coming from the same distribution.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Literally, it can be referred as an optimization problem—where the objective
    is to find the optimal (that is, minimum, for example) values ![](img/ee4deb14-39bd-475d-b9d7-77be4da025a6.png) such
    that the optimal values of parameters will mean the model tends to make more accurate
    predictions. In short, in the LR model, we intend to find the optimal values for ![](img/d17f18d1-2969-40c3-bc5c-9d45e3f90ca0.png)
    and ![](img/14c7a856-3f2d-47a4-9510-8404fb177d51.png) to minimize the following
    objective function:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0097bfe-fc96-41dc-8b43-7da46a2d8f17.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the expression *(f [w,b] (X[i]) - y[i])²* is called
    the **loss function**, which is a measure of penalty (that is, error or loss)
    for giving the wrong prediction for sample *i*. This loss function is in the form
    of squared error loss. However, other loss functions can be used too, as outlined
    in the following equations:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11416c4c-02dc-44cb-8170-db2194fe7101.png)![](img/c9bbc527-8893-436e-9f81-be0e0fc582b7.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: The **squared error** (**SE**) in equation 1 is called *L[2]* loss, which is
    the default loss function for the regression analysis task. On the other hand,
    the **absolute error** (**AE**) in equation (*2)* is called *L[1]* loss.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: In cases where the dataset has many outliers, using *L[1]* loss is recommend
    more than *L[2]*, because *L[1] *is more robust against outliers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: All model-based learning algorithms have a loss function associated with them.
    Then we try to find the best model by minimizing the cost function. In our LR
    case, the cost function is defined by the average loss (also called empirical
    risk), which can be formulated as the average of all penalties obtained by fitting
    the model to the training data, which may contain many samples.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4* shows an example of simple linear regression. Let''s say the idea
    is to predict the amount of **Savings** versus **Age**. So, in this case, we have
    one independent variable *x* (that is, a set of 1D data points and, in our case,
    the **Age**) and one dependent variable, *y* (amount of **Savings (in millions
    $)**). Once we have a trained regression model, we can use this line to predict
    the value of the target *y[l]* for a new unlabeled input example, *x[l].* However,
    in the case of *D* -dimensional feature vectors (for example, *2D* or *3D*), it
    would be a plane (for *2D*) or a hyperplane (for *>=3D*):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/113ca892-0d37-4fd7-8dc6-0e149613c560.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: A regression line separates data points to solve Age versus the amount
    of Savings: i) the left model separates data points based on training data: ii)
    the right model predicts for an unknown observation'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you see why it is important to have the requirement that the regression
    hyperplane lies as close to the training examples as possible: if the blue line
    in *Figure 4* (the model on the right) is far away from the blue dots, the prediction
    *y[l]* is less likely to be correct. The best fit line, which is expected to pass
    through most of the data points, is the result of the regression analysis. However,
    in practice it does not pass through all of the data points because of the existence
    of regression errors.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以看到为什么要求回归超平面尽可能靠近训练样本是如此重要：如果 *图 4*（右侧的模型）中的蓝色线远离蓝色点，预测值 *y[l]* 就不太可能是正确的。最佳拟合线，预期将通过大多数数据点，是回归分析的结果。然而，在实践中，由于回归误差的存在，它并不通过所有数据点。
- en: Regression error is the distance between any data points (actual) and the line
    (predicted).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 回归误差是任何数据点（实际）与线（预测）之间的距离。
- en: Since solving a regression problem is itself an optimization problem, we expect
    a smaller margin for errors as possible because smaller errors contribute towards
    higher predictive accuracy, while predicting unseen observations. Although an
    LR algorithm is not so efficient in many cases, the nicest thing is that an LR
    model usually does not overfit, which is unlikely for a more complex model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于解决回归问题本身就是一个优化问题，我们期望尽可能小的误差范围，因为较小的误差有助于提高预测准确性，同时预测未见过的观测值。尽管 LR 算法在许多情况下并不高效，但最好的一点是
    LR 模型通常不会过拟合，这对于更复杂的模型来说是不太可能的。
- en: In the previous chapter, we discussed overfitting (a phenomenon whereby a model
    that shows a model predicts very well during the training but makes more errors
    when applied to test set) and underfitting (if your training error is low and
    your validation error is high, then your model is most likely overfitting your
    training data). Often these two phenomena occur due to bias and variance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了过拟合（一个现象，即模型在训练期间预测得非常好，但在应用于测试集时却犯了更多错误）和欠拟合（如果您的训练误差低而验证误差高，那么您的模型很可能是过拟合了训练数据）。这两种现象通常是由于偏差和方差引起的。
- en: Performance metrics
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能指标
- en: 'To measure the predictive performance of a regression model, several metrics
    are proposed and in use in terms of regression errors, which can be outlined as
    follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量回归模型的预测性能，提出了几个基于回归误差的指标，可以概括如下：
- en: '**Mean squared error (MSE)**: It is the measure of the difference between the
    predicted and estimated values, that is, how close a fitted line is to data points.
    The smaller the MSE, the closer the fit is to the data.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差 (MSE)**: 它是预测值和估计值之间差异的度量，即拟合线与数据点之间的接近程度。MSE 越小，拟合线与数据越接近。'
- en: '**Root mean squared error (RMSE)**: It is the square root of the MSE but has
    the same units as the quantity plotted on the vertical axis.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方根误差 (RMSE)**: 它是 MSE 的平方根，但具有与垂直轴上绘制的数量相同的单位。'
- en: '**R-squared**: It is the coefficient of determination for assessing how close
    the data is to the fitted regression line ranges between 0 and 1\. The higher
    the R-squared, the better the model fits your data.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R²**: 它是确定系数，用于评估数据与拟合回归线之间的接近程度，范围在 0 到 1 之间。R² 越高，模型与数据的拟合度越好。'
- en: '**Mean absolute error (MAE)**: It is a measure of *accuracy* for continuous
    variables without considering their direction. The smaller the MAE, the better
    the model fits your data.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差 (MAE)**: 它是连续变量精度的度量，不考虑其方向。MAE 越小，模型与数据的拟合度越好。'
- en: Now that we know how a regression algorithm works and how to evaluate the performance
    using several metrics, the next important task is to apply this knowledge to solve
    a real-life problem.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了回归算法的工作原理以及如何使用几个指标来评估性能，下一个重要的任务是应用这些知识来解决现实生活中的问题。
- en: Learning regression analysis through examples
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过示例学习回归分析
- en: In the previous section, we discussed a simple real-life problem (that is, **Age**
    versus **Savings**). However, in practice, there are several real-life problems
    where more factors and parameters (that is, data properties) are involved, where
    regression can be applied too. Let's first introduce a real-life problem. Imagine
    that you live in Sao Paulo, a city in Brazil, where every day several hours of
    your valuable time are wasted because of unavoidable reasons such as an immobilized
    bus, broken truck, vehicle excess, accident victim, overtaking, fire vehicles,
    incident involving dangerous freight, lack of electricity, fire, and flooding.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Now, to measure how many man hours get wasted, we can we develop an automated
    technique, which will predict the slowness of traffic such that you can avoid
    certain routes or at least get some rough estimation of how long it'll take you
    to reach some point in the city. A predictive analytics application using machine
    learning is probably one of the preferred solutions for predicting such slowness.
    Yes, for that we'll use the behavior of the urban traffic of the city of Sao Paulo
    in Brazil dataset in the next section.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Description of the dataset
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset is downloaded from [https://archive.ics.uci.edu/ml/datasets/Behavior+of+the+urban+traffic+of+the+city+of+Sao+Paulo+in+Brazil](https://archive.ics.uci.edu/ml/datasets/Behavior+of+the+urban+traffic+of+the+city+of+Sao+Paulo+in+Brazil).
    It contains the records of behavior of the urban traffic of the city of Sao Paulo
    in Brazil between December 14, 2009 and December 18, 2009\. The dataset has the
    following features:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Hour**: Total hours spent on the road'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Immobilized bus**: Number of immobilized buses'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Broken truck**: Number of broken trucks'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vehicle excess**: Number of redundant vehicles'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accident victim**: Number of accident victims on the road or road side'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Running over**: Number of running over or taking over cases'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fire vehicles**: Number of fire trucks and vehicles'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Occurrence involving freight**: Number of goods transported in bulk by trucks'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incident involving dangerous freight**: Number of transporter bulk trucks
    involved in accident'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of electricity**: Number of hours without electricity in the affected
    areas'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fire**: Number of fire incidents'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Point of flooding**: Number of points of flooding areas'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manifestations**: Number of places showing construction work ongoing or dangerous
    signs'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Defect in the network of trolleybuses**: Number of defects in the network
    of trolley buses'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tree on the road**: Number of trees on the road or road side that create
    obstacles'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semaphore off**: Number of mechanical gadgets with arms, lights, or flags
    that are used as a signal'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intermittent semaphore**: Number of mechanical gadgets with arms, lights,
    or flags that are used as a signal for a specific period of time'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slowness in traffic**: Number of average hours people got stuck in traffic
    because of the preceding reasons'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The last feature is the target column, which we want to predict. Since I used
    this dataset, I would like to acknowledge the following publication:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Ferreira, R. P., Affonso, C., & Sassi, R. J. (2011, November). Combination of
    Artificial Intelligence Techniques for Prediction the Behavior of Urban Vehicular
    Traffic in the City of Sao Paulo. In 10th Brazilian Congress on Computational
    Intelligence (CBIC) - Fortaleza, Brazil. (pp.1-7), 2011.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory analysis of the dataset
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we read the training set for the **exploratory data analysis** (**EDA**).
    Readers can refer to the `EDA.scala` file for this. Once extracted, there will
    be a CSV file named `Behavior of the urban traffic of the city of Sao Paulo in
    Brazil.csv`. Let''s rename the file as `UrbanTraffic.csv`. Also, `Slowness in
    traffic (%)`, which is the last column, represents the percentage of slowness
    in an unusual format: it represents the real number with a comma (`,`), for example,
    `4,1` instead of `4.1`. So I replaced all instances of a comma (`,`) in that column
    with a period (`.`). Otherwise, the Spark CSV reader will treat the column as
    a `String` type:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'First, let''s load, parse, and create a DataFrame using the `read.csv()` method
    but with the Databricks CSV format (also known as `com.databricks.spark.csv`)
    by setting it to read the header of the CSV file, which is directly applied to
    the columns'' names of the DataFrame created; and the `inferSchema` property is
    set to `true`, because if you don''t specify the `inferSchema` configuration explicitly,
    the float values would be treated as strings*.* This might cause `VectorAssembler`
    to raise an exception such as `java.lang.IllegalArgumentException: Data type StringType
    is not supported`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now let''s print the schema of the DataFrame we just created to check to make
    sure the structure is preserved:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As seen from the following screenshot, the schema of the Spark DataFrame has
    been correctly identified. Also, as expected, all the features of my ML algorithms
    are numeric (in other words, in integer or double format):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d2ffd9c-b6b3-41e5-a06e-ef7868fed189.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'You can see that none of the columns are categorical features. So, we don''t
    need any numeric transformation. Now let''s see how many rows there are in the
    dataset using the `count()` method:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives a 135 sample count**.** Now let''s see a snapshot of the dataset
    using the `show()` method, but with only some selected columns so that it can
    make more sense rather than showing all of them. But feel free to use `rawTrafficDF.show()`
    to see all columns:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As the `Slowness in traffic (%)` column contains continuous values, we have
    to deal with a regression task. Now that we have seen a snapshot of the dataset,
    it would be worth seeing some other statistics such as average claim or loss,
    minimum, and maximum loss of Spark SQL using the `sql()` interface:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a076e247-3e92-4e9f-9f63-9848530ada55.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'However, before that, let''s rename the last column from `Slowness in traffic
    (%)` to `label`, since the ML model will complain about it. Even after using `setLabelCol`
    on the regression model, it still looks for a column called `label`. This introduces
    a disgusting error saying `org.apache.spark.sql.AnalysisException: cannot resolve
    ''label'' given input columns`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Since we want to execute some SQL query, we need to create a temporary view
    so that the operation can be performed in-memory:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let''s average the slowness in the form of a percentage (the deviation
    with standard hours):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding line of code should show a 10% delay on average every day across
    routes and based on other factors:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Also, we can see the number of flood points in the city. However, for that
    we might need some extra effort by changing the column name into a single string
    since it''s a multi-string containing spaces, so SQL won''t be able to resolve
    it:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This should show as many as seven flood points that could be very dangerous:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'However, the `describe()` method will give these types of statistics more flexibly.
    Let''s do it for the selected columns:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'So, we can see that the slowness varies between `3.4` and `23.4`, which is
    quite high. This is why we need efficient data processing steps so that such a
    relation can be preserved. Now let''s focus on the data preprocessing instead:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f6af703-9e8a-4f0c-a96b-09601ef91887.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: Feature engineering and data preparation
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen some properties of the dataset and since there're no null
    values or categorical features, we don't need any other preprocessing or intermediate
    transformations. We just need to do some feature engineering before we can have
    our training and test sets.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: The first step before getting these sets is to prepare training data that is
    consumable by the Spark regression model. For this, Spark classification and regression
    algorithms expect two components called `features` and `label`. Fortunately, we
    already have the `label` column. Next, the `features` column has to contain the
    data from all the columns except the `label` column, which can be achieved using
    the `VectorAssembler()` transformer.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'Since all the columns are numeric, we can use `VectorAssembler()` directly
    from the Spark ML library to transform a given list of columns into a single vector
    column. So, let''s collect the list of desirable columns. As you may have guessed,
    we''ll have to exclude the `label` column, which can be done using the `dropRight()`
    method of standard Scala:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that we have the `VectorAssembler()` estimator, we now call the `transform()`
    method, which will embed selected columns into a single vector column:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As expected, the last line of the preceding code segment shows the assembled
    DataFrame having `label` and `features`, which are needed to train an ML algorithm:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0b68aa4-2797-4b5e-9a8f-644d1d4edf84.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: 'We can now proceed to generate separate training and test sets. Additionally,
    we can cache both the sets for faster in-memory access. We use 60% of the data
    to train the model and the other 40% will be used to evaluate the model:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: That is all we need before we start training the regression models. At first,
    we start training the LR model and evaluate the performance.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will develop a predictive analytics model for predicting
    slowness in traffic for each row of the data using an LR algorithm. First, we
    create an LR estimator as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then we invoke the `fit()` method to perform the training on the training set
    as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we have the fitted model, which means it is now capable of making predictions.
    So, let''s start evaluating the model on the training and validation sets and
    calculating the RMSE, MSE, MAE, R squared, and so on:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Great! We have managed to compute the raw prediction on the training and the
    test sets. Now that we have both the performance metrics on both training and
    validation sets, let''s observe the results of the training and the validation
    sets:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding code segment should show something similar. Although, because
    of the randomness, you might experience slightly different output:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now that we have the prediction on the test set as well, however, we can't directly
    say if it's a good or optimal regression model. To improve the result further
    with lower MAE, Spark also provides the generalized version of linear regression
    implementation called GLR.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear regression (GLR)
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In an LR, the output is assumed to follow a Gaussian distribution. In contrast,
    in **generalized linear models** (**GLMs**), the response variable *Y[i]* follows
    some random distribution from a parametric set of probability distributions of
    a certain form. As we have seen in the previous example, following and creating
    a GLR estimator will not be difficult:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For the GLR-based prediction, the following response and identity link functions
    are supported based on data types (source: [https://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression)):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbc3b29b-50ad-4c25-9c92-8a25f3fdd716.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: 'Then we invoke the `fit()` method to perform the training on the training set
    as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The current implementation through the `GeneralizedLinearRegression` interface
    in Spark supports up to 4,096 features only. Now that we have the fitted model
    (which means it is now capable of making predictions), let''s start evaluating
    the model on training and validation sets and calculating the RMSE, MSE, MAE,
    R squared, and so on:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Great! We have managed to compute the raw prediction on the training and the
    test sets. Now that we have both the performance metrics on both training and
    test sets, let''s observe the result on the train and the validation sets:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding code segment should show similar results. Although, because of
    the randomness, you might experience slightly different output:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Using GLR, we can see a slightly worse MAE value and also the RMSE is higher.
    If you see these two examples, we have not got to tune the hyperparameters but
    simply let the models train and evaluate a single value of each parameter. We
    could even use a regularization parameter for reducing overfitting. However, the
    performance of an ML pipeline often improves with the hyperparameter tuning, which
    is usually done with grid search and cross-validation. In the next section, we
    will discuss how we can get even better performance with the cross-validated models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning and cross-validation
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, the term hyperparameter refers to those parameters that
    cannot be learned from the regular training process directly. These are the various
    knobs that you can tweak on your machine learning algorithms. Hyperparameters
    are usually decided by training the model with different combinations of the parameters
    and deciding which ones work best by testing them. Ultimately, the combination
    that provides the best model would be our final hyperparameters. Setting hyperparameters
    can have a significant influence on the performance of the trained models.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, cross-validation is often used in conjunction with hyperparameter
    tuning. Cross-validation (also know asrotation estimation) is a model validation
    technique for assessing the quality of the statistical analysis and results. Cross-validation
    helps to describe a dataset to test the model in the training phase using the
    validation set.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unfortunately, there is no shortcut or straightforward way of choosing the
    right combination of hyperparameters based on a clear recipe—of course, experience
    helps. For example, while training a random forest, Matrix factorization, k-means,
    or a logistic/LR algorithm might be appropriate. Here are some typical examples
    of such hyperparameters:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Number of leaves, bins, or depth of a tree in tree-based algorithms
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of iterations
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization values
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of latent factors in a matrix factorization
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of clusters in a k-means clustering and so on
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technically, hyperparameters form an *n*-dimensional space called a param-grid,
    where *n* is the number of hyperparameters. Every point in this space is one particular
    hyperparameter configuration, which is a hyperparameter vector.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 1](33fe7442-ce44-4a18-bac6-0e08e9b1ae1e.xhtml), *Introduction
    to Machine Learning with Scala*, overfitting and underfitting are two problematic
    phenomena in machine learning. Therefore, sometimes full convergence to a best
    model parameter set is often not necessary and can be even preferred, because
    an almost-best-fitting model tends to perform better on new data or settings.
    In other words, if you care for a best fitting model, you really don't need the
    best parameter set.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we cannot explore every point in this space, so the grid search
    over a subset in that space is commonly used. The following diagram shows some
    high-level idea:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01205c83-200d-47ce-aa8a-d7c03f2909ea.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Hyperparameter tuning of ML models'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there are several approaches for such a scheme, random search or grid
    search are probably the most well-known techniques used:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '**Grid search**: Using this approach, different hyperparameters are defined
    in a dictionary that you want to test. Then a param-grid is constructed before
    feeding them into the ML model such that the training can be performed with the
    different combinations. Finally, the algorithm tells you for which combination
    of the hyperparameters you have the highest accuracy.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random search**: As you can understand, training an ML model with all possible
    combinations of hyperparameters is a very expensive and time consuming operation.
    However, often we don''t have that much flexibility but still we want to tune
    those parameters. In such a situation, random search could be a workaround. Random
    search is performed through evaluating *n* uniformly random points in the hyperparameter
    space, and selecting the right combination for which the model gives the best
    performance.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two types of cross-validation, called exhaustive cross-validation,
    which includes leave-p-out cross-validation and leave-one-out cross-validation,
    and non-exhaustive cross-validation, which is based on K-fold cross-validation
    and repeated random sub-sampling cross-validation, for example, 5-fold or 10-fold
    cross-validation, is very common.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: In most of the cases, 10-fold cross-validation is used instead of testing on
    a validation set. Also, the training set should be as large as possible (because
    more data with quality features are good to train the model) not only to train
    the model but because about 5 to 10% of the training set can be used for the cross-validation.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Using the K-fold cross-validation technique, the complete training data is split
    into K subsets. The model is trained on K-1 subsets; hold the last one for the
    validation. This process is repeated K times so that each time, one of the K subsets
    is used as the validation set and the other K-1 subsets are used to form the training
    set. This way, each of the subsets (fold) is used at least once for both training
    and validation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, different machine learning models that have been obtained are joined
    by a bagging (or boosting) scheme for classifiers or by averaging (that is, regression).
    The following diagram explains the 10-fold cross-validation technique:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/460bc239-46e0-4317-8bfa-48b326d6ae45.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: Figure 6: 10-fold cross-validation technique
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Tuning and cross-validation in Spark ML
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Spark ML, before performing the cross-validation, we need to have a `paramGrid`
    (that is a grid of parameters). The `ParamGridBuilder` interface is used in order
    to define the hyperparameter space where `CrossValidator` has to search and finally, `CrossValidator()`
    takes our pipeline, the hyperparameter space of our LR regressor, and the number
    of folds for the cross-validation as parameters.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s start creating `paramGrid` by specifying the number of maximum iterations,
    the value of regularization parameter, the value of tolerance, and the elastic
    network parameters, as follows for the LR model (since we observed lower MAE for
    this):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The regularization parameter reduces overfitting by reducing the variance of
    your estimated regression parameters. Now, for a better and more stable performance,
    we can perform 10-fold cross-validation. Since our task is predicting continuous
    values, we need to define `RegressionEvaluator`, that is, the evaluator for regression,
    which expects two input columns—`prediction` and `label`—and evaluates the training
    based on MSE, RMSE, R-squared, and MAE:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Fantastic, we have created the cross-validation estimator. Now it''s time to
    train the LR model:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'By the way, Spark provides a way to save a trained ML model using the `save()`
    method:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then the same model can be restored from the disk using the `load()` method:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then we compute the model''s metrics on the test set similar to the LR and
    GLR models:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we gather the metrics and print to get some insights:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding code segment should show something similar. Although, because
    of the randomness, you might experience slightly different output:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As we can see, both the RMSE and MAE are slightly lower than the non-cross validated
    LR model. Ideally, we should have experienced even lower values for these metrics.
    However, due to the small size of the training as well as test sets, probably
    both the LR and GLR models overfitted. Still, we will try to use robust regression
    analysis algorithms in [Chapter 4](6730e23e-eabb-4628-934a-7ac609049563.xhtml),
    *Scala for Tree-Based Ensemble Techniques*. More specifically, we will try to
    solve the same problem with decision trees, random forest, and GBTRs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen how to develop a regression model for analyzing
    insurance severity claims using LR and GLR algorithms. We have also seen how to
    boost the performance of the GLR model using cross-validation and grid search
    techniques, which give the best combination of hyperparameters. Finally, we have
    seen some frequently asked questions so that the similar regression techniques
    can be applied for solving other real-life problems.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see another supervised learning technique called
    classification through a real-life problem called analyzing outgoing customers
    through churn prediction. Several classification algorithms will be used for making
    the prediction in Scala. Churn prediction is essential for businesses as it helps
    you detect customers who are likely to cancel a subscription, product, or service,
    which also minimizes customer defection by predicting which customers are likely
    to cancel a subscription to a service.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
