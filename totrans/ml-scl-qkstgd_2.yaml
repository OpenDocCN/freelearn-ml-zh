- en: Scala for Regression Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn regression analysis in detail. We will start
    learning from the regression analysis workflow followed by the **linear regression**
    (**LR**) and **generalized linear regression** (**GLR**) algorithms. Then we will
    develop a regression model for predicting slowness in traffic using LR and GLR
    algorithms and their Spark ML-based implementation in Scala. Finally, we will
    learn the hyperparameter tuning with cross-validation and the grid searching techniques.
    Concisely, we will learn the following topics throughout this end-to-end project:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of regression analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression analysis algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning regression analysis through examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning and cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapters can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter02](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter02)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2GLlQTl](http://bit.ly/2GLlQTl)'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of regression analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we already gained some basic understanding of the **machine
    learning** (**ML**) process, as we have seen the basic distinction between regression
    and classification. Regression analysis is a set of statistical processes for
    estimating the relationships between a set of variables called a dependent variable
    and one or multiple independent variables. The values of dependent variables depend
    on the values of independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'A regression analysis technique helps us to understand this dependency, that
    is, how the value of the dependent variable changes when any one of the independent
    variables is changed, while the other independent variables are held fixed. For
    example, let''s assume that there will be more savings in someone''s bank when
    they grow older. Here, the amount of **Savings** (say in million $) depends on
    age (that is, **Age** in years, for example):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Age (years)** | **Savings (million $)** |'
  prefs: []
  type: TYPE_TB
- en: '| 40 | 1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 5.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 60 | 10.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 70 | 6.7 |'
  prefs: []
  type: TYPE_TB
- en: So, we can plot these two values in a 2D plot, where the dependent variable
    (**Savings**) is plotted on the *y*-axis and the independent variable (**Age**)
    should be plotted on the *x*-axis. Once these data points are plotted, we can
    see correlations. If the theoretical chart indeed represents the impact of getting
    older on savings, then we'll be able to say that the older someone gets, the more
    savings there will be in their bank account.
  prefs: []
  type: TYPE_NORMAL
- en: Now the question is how can we tell the degree to which age helps someone to
    get more money in their bank account? To answer this question, one can draw a
    line through the middle of all of the data points on the chart. This line is called
    the regression line, which can be calculated precisely using a regression analysis
    algorithm. A regression analysis algorithm takes either discrete or continuous
    (or both) input features and produces continuous values.
  prefs: []
  type: TYPE_NORMAL
- en: A classification task is used for predicting the label of the class attribute,
    while a regression task is used for making a numeric prediction of the class attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Making a prediction using such a regression model on unseen and new observations
    is like creating a data pipeline with multiple components working together, where
    we observe an algorithm''s performance in two stages: learning and inference.
    In the whole process and for making the predictive model a successful one, data
    acts as the first-class citizen in all ML tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the important task at the learning stage is to prepare and convert the
    data into feature vectors (vectors of numbers out of each feature). Training data
    in feature vector format can be fed into the learning algorithms to train the
    model, which can be used for inferencing. Typically, and of course based on data
    size, running an algorithm may take hours (or even days) so that the features
    converge into a useful model as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e9cb443-9b8c-406f-b6ad-bd6718fbba96.png)'
  prefs: []
  type: TYPE_IMG
- en: Learning and training a predictive model—it shows how to generate the feature
    vectors from the training data to train the learning algorithm that produces a
    predictive model
  prefs: []
  type: TYPE_NORMAL
- en: Inferencing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the inference stage, the trained model is used for making intelligent use
    of the model, such as predicting from never-before-seen data, making recommendations,
    and deducing future rules. Typically, it takes less time compared to the learning
    stage and sometimes even in real time. Thus, inferencing is all about testing
    the model against new (that is, unobserved) data and evaluating the performance
    of the model itself, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c8a4f15-9b49-4993-8a42-50525807292c.png)'
  prefs: []
  type: TYPE_IMG
- en: Inferencing from an existing model towards predictive analytics (feature vectors
    are generated from unknown data for making predictions)
  prefs: []
  type: TYPE_NORMAL
- en: In summary, when using regression analysis the goal is to predict a continuous
    target variable. Now that we know how to construct a basic workflow for a supervised
    learning task, knowing a little about available regression algorithms will provide
    a bit more concrete information on how to apply these regression algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are numerous algorithms proposed and available, which can be used for
    the regression analysis. For example, LR tries to find relationships and dependencies
    between variables. It models the relationship between a continuous dependent variable
    *y* (that is, a label or target) and one or more independent variables, *x*, using
    a linear function. Examples of regression algorithms include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear regression** (**LR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalized linear regression** (**GLR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Survival regression** (**SR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Isotonic regression** (**IR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision tree regressor** (**DTR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forest regression** (**RFR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient boosted trees regression** (**GBTR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We start by explaining regression with the simplest LR algorithm, which models
    the relationship between a dependent variable, *y*, which involves a linear combination
    of interdependent variables, *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11963a25-c48f-40da-a3e7-5f14b4f4ff65.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation letters, *β[0]* and *β[1]* are two constants for *y*-axis
    intercept and the slope of the line, respectively. LR is about learning a model,
    which is a linear combination of features of the input example (data points).
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following graph and imagine that the red line is not there.
    We have a few dotted blue points (data points). Can we reasonably develop a machine
    learning (regression) model to separate most of them? Now, if we draw a straight
    line between two classes of data, those get almost separated, don''t they? Such
    a line (red in our case) is called the decision boundary, which is also called
    the regression line in the case of regression analysis (see the following example
    for more):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed09f3bd-c70c-461d-96d9-ceab36e4465d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we are given a collection of labeled examples, say ![](img/4bbeb78b-a1d8-46cf-a394-d26d3489e864.png), where
    *N* is the number of samples in the dataset, *x[i]* is the *D*-dimensional feature
    vector of the samples *i = 1, 2… N*, and *y[i]* is a real-valued *y ∈ R*, where
    *R* denotes the set of all real numbers called the target variable and every feature *x[i ]*is
    a real number. Then combining these, the next step is to build the following mathematical
    model, *f*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/000a67b5-7349-4298-8ca3-3c8e63c0d435.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *w* is a *D*-dimensional parameterized vector and *b* is a real number.
    The notation *f[w,b]* signifies that the model *f* is parameterized by values
    *w* and *b*. Once we have a well-defined model, it can now be used for making
    a prediction of unknown *y* for a given *x,* that is, *y ← f[w,b ](x)*. However,
    there is an issue, as since the model is parametrized with two different values
    (*w*, *b*), this will mean the model tends to produce two different predictions
    when applied to the same sample, even when coming from the same distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Literally, it can be referred as an optimization problem—where the objective
    is to find the optimal (that is, minimum, for example) values ![](img/ee4deb14-39bd-475d-b9d7-77be4da025a6.png) such
    that the optimal values of parameters will mean the model tends to make more accurate
    predictions. In short, in the LR model, we intend to find the optimal values for ![](img/d17f18d1-2969-40c3-bc5c-9d45e3f90ca0.png)
    and ![](img/14c7a856-3f2d-47a4-9510-8404fb177d51.png) to minimize the following
    objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0097bfe-fc96-41dc-8b43-7da46a2d8f17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the expression *(f [w,b] (X[i]) - y[i])²* is called
    the **loss function**, which is a measure of penalty (that is, error or loss)
    for giving the wrong prediction for sample *i*. This loss function is in the form
    of squared error loss. However, other loss functions can be used too, as outlined
    in the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11416c4c-02dc-44cb-8170-db2194fe7101.png)![](img/c9bbc527-8893-436e-9f81-be0e0fc582b7.png)'
  prefs: []
  type: TYPE_IMG
- en: The **squared error** (**SE**) in equation 1 is called *L[2]* loss, which is
    the default loss function for the regression analysis task. On the other hand,
    the **absolute error** (**AE**) in equation (*2)* is called *L[1]* loss.
  prefs: []
  type: TYPE_NORMAL
- en: In cases where the dataset has many outliers, using *L[1]* loss is recommend
    more than *L[2]*, because *L[1] *is more robust against outliers.
  prefs: []
  type: TYPE_NORMAL
- en: All model-based learning algorithms have a loss function associated with them.
    Then we try to find the best model by minimizing the cost function. In our LR
    case, the cost function is defined by the average loss (also called empirical
    risk), which can be formulated as the average of all penalties obtained by fitting
    the model to the training data, which may contain many samples.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4* shows an example of simple linear regression. Let''s say the idea
    is to predict the amount of **Savings** versus **Age**. So, in this case, we have
    one independent variable *x* (that is, a set of 1D data points and, in our case,
    the **Age**) and one dependent variable, *y* (amount of **Savings (in millions
    $)**). Once we have a trained regression model, we can use this line to predict
    the value of the target *y[l]* for a new unlabeled input example, *x[l].* However,
    in the case of *D* -dimensional feature vectors (for example, *2D* or *3D*), it
    would be a plane (for *2D*) or a hyperplane (for *>=3D*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/113ca892-0d37-4fd7-8dc6-0e149613c560.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: A regression line separates data points to solve Age versus the amount
    of Savings: i) the left model separates data points based on training data: ii)
    the right model predicts for an unknown observation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you see why it is important to have the requirement that the regression
    hyperplane lies as close to the training examples as possible: if the blue line
    in *Figure 4* (the model on the right) is far away from the blue dots, the prediction
    *y[l]* is less likely to be correct. The best fit line, which is expected to pass
    through most of the data points, is the result of the regression analysis. However,
    in practice it does not pass through all of the data points because of the existence
    of regression errors.'
  prefs: []
  type: TYPE_NORMAL
- en: Regression error is the distance between any data points (actual) and the line
    (predicted).
  prefs: []
  type: TYPE_NORMAL
- en: Since solving a regression problem is itself an optimization problem, we expect
    a smaller margin for errors as possible because smaller errors contribute towards
    higher predictive accuracy, while predicting unseen observations. Although an
    LR algorithm is not so efficient in many cases, the nicest thing is that an LR
    model usually does not overfit, which is unlikely for a more complex model.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed overfitting (a phenomenon whereby a model
    that shows a model predicts very well during the training but makes more errors
    when applied to test set) and underfitting (if your training error is low and
    your validation error is high, then your model is most likely overfitting your
    training data). Often these two phenomena occur due to bias and variance.
  prefs: []
  type: TYPE_NORMAL
- en: Performance metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To measure the predictive performance of a regression model, several metrics
    are proposed and in use in terms of regression errors, which can be outlined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean squared error (MSE)**: It is the measure of the difference between the
    predicted and estimated values, that is, how close a fitted line is to data points.
    The smaller the MSE, the closer the fit is to the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Root mean squared error (RMSE)**: It is the square root of the MSE but has
    the same units as the quantity plotted on the vertical axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**R-squared**: It is the coefficient of determination for assessing how close
    the data is to the fitted regression line ranges between 0 and 1\. The higher
    the R-squared, the better the model fits your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean absolute error (MAE)**: It is a measure of *accuracy* for continuous
    variables without considering their direction. The smaller the MAE, the better
    the model fits your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know how a regression algorithm works and how to evaluate the performance
    using several metrics, the next important task is to apply this knowledge to solve
    a real-life problem.
  prefs: []
  type: TYPE_NORMAL
- en: Learning regression analysis through examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we discussed a simple real-life problem (that is, **Age**
    versus **Savings**). However, in practice, there are several real-life problems
    where more factors and parameters (that is, data properties) are involved, where
    regression can be applied too. Let's first introduce a real-life problem. Imagine
    that you live in Sao Paulo, a city in Brazil, where every day several hours of
    your valuable time are wasted because of unavoidable reasons such as an immobilized
    bus, broken truck, vehicle excess, accident victim, overtaking, fire vehicles,
    incident involving dangerous freight, lack of electricity, fire, and flooding.
  prefs: []
  type: TYPE_NORMAL
- en: Now, to measure how many man hours get wasted, we can we develop an automated
    technique, which will predict the slowness of traffic such that you can avoid
    certain routes or at least get some rough estimation of how long it'll take you
    to reach some point in the city. A predictive analytics application using machine
    learning is probably one of the preferred solutions for predicting such slowness.
    Yes, for that we'll use the behavior of the urban traffic of the city of Sao Paulo
    in Brazil dataset in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Description of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset is downloaded from [https://archive.ics.uci.edu/ml/datasets/Behavior+of+the+urban+traffic+of+the+city+of+Sao+Paulo+in+Brazil](https://archive.ics.uci.edu/ml/datasets/Behavior+of+the+urban+traffic+of+the+city+of+Sao+Paulo+in+Brazil).
    It contains the records of behavior of the urban traffic of the city of Sao Paulo
    in Brazil between December 14, 2009 and December 18, 2009\. The dataset has the
    following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hour**: Total hours spent on the road'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Immobilized bus**: Number of immobilized buses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Broken truck**: Number of broken trucks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vehicle excess**: Number of redundant vehicles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accident victim**: Number of accident victims on the road or road side'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Running over**: Number of running over or taking over cases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fire vehicles**: Number of fire trucks and vehicles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Occurrence involving freight**: Number of goods transported in bulk by trucks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incident involving dangerous freight**: Number of transporter bulk trucks
    involved in accident'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of electricity**: Number of hours without electricity in the affected
    areas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fire**: Number of fire incidents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Point of flooding**: Number of points of flooding areas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manifestations**: Number of places showing construction work ongoing or dangerous
    signs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Defect in the network of trolleybuses**: Number of defects in the network
    of trolley buses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tree on the road**: Number of trees on the road or road side that create
    obstacles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semaphore off**: Number of mechanical gadgets with arms, lights, or flags
    that are used as a signal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intermittent semaphore**: Number of mechanical gadgets with arms, lights,
    or flags that are used as a signal for a specific period of time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slowness in traffic**: Number of average hours people got stuck in traffic
    because of the preceding reasons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The last feature is the target column, which we want to predict. Since I used
    this dataset, I would like to acknowledge the following publication:'
  prefs: []
  type: TYPE_NORMAL
- en: Ferreira, R. P., Affonso, C., & Sassi, R. J. (2011, November). Combination of
    Artificial Intelligence Techniques for Prediction the Behavior of Urban Vehicular
    Traffic in the City of Sao Paulo. In 10th Brazilian Congress on Computational
    Intelligence (CBIC) - Fortaleza, Brazil. (pp.1-7), 2011.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory analysis of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we read the training set for the **exploratory data analysis** (**EDA**).
    Readers can refer to the `EDA.scala` file for this. Once extracted, there will
    be a CSV file named `Behavior of the urban traffic of the city of Sao Paulo in
    Brazil.csv`. Let''s rename the file as `UrbanTraffic.csv`. Also, `Slowness in
    traffic (%)`, which is the last column, represents the percentage of slowness
    in an unusual format: it represents the real number with a comma (`,`), for example,
    `4,1` instead of `4.1`. So I replaced all instances of a comma (`,`) in that column
    with a period (`.`). Otherwise, the Spark CSV reader will treat the column as
    a `String` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s load, parse, and create a DataFrame using the `read.csv()` method
    but with the Databricks CSV format (also known as `com.databricks.spark.csv`)
    by setting it to read the header of the CSV file, which is directly applied to
    the columns'' names of the DataFrame created; and the `inferSchema` property is
    set to `true`, because if you don''t specify the `inferSchema` configuration explicitly,
    the float values would be treated as strings*.* This might cause `VectorAssembler`
    to raise an exception such as `java.lang.IllegalArgumentException: Data type StringType
    is not supported`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s print the schema of the DataFrame we just created to check to make
    sure the structure is preserved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen from the following screenshot, the schema of the Spark DataFrame has
    been correctly identified. Also, as expected, all the features of my ML algorithms
    are numeric (in other words, in integer or double format):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d2ffd9c-b6b3-41e5-a06e-ef7868fed189.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see that none of the columns are categorical features. So, we don''t
    need any numeric transformation. Now let''s see how many rows there are in the
    dataset using the `count()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives a 135 sample count**.** Now let''s see a snapshot of the dataset
    using the `show()` method, but with only some selected columns so that it can
    make more sense rather than showing all of them. But feel free to use `rawTrafficDF.show()`
    to see all columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As the `Slowness in traffic (%)` column contains continuous values, we have
    to deal with a regression task. Now that we have seen a snapshot of the dataset,
    it would be worth seeing some other statistics such as average claim or loss,
    minimum, and maximum loss of Spark SQL using the `sql()` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a076e247-3e92-4e9f-9f63-9848530ada55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, before that, let''s rename the last column from `Slowness in traffic
    (%)` to `label`, since the ML model will complain about it. Even after using `setLabelCol`
    on the regression model, it still looks for a column called `label`. This introduces
    a disgusting error saying `org.apache.spark.sql.AnalysisException: cannot resolve
    ''label'' given input columns`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we want to execute some SQL query, we need to create a temporary view
    so that the operation can be performed in-memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s average the slowness in the form of a percentage (the deviation
    with standard hours):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code should show a 10% delay on average every day across
    routes and based on other factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we can see the number of flood points in the city. However, for that
    we might need some extra effort by changing the column name into a single string
    since it''s a multi-string containing spaces, so SQL won''t be able to resolve
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show as many as seven flood points that could be very dangerous:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the `describe()` method will give these types of statistics more flexibly.
    Let''s do it for the selected columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we can see that the slowness varies between `3.4` and `23.4`, which is
    quite high. This is why we need efficient data processing steps so that such a
    relation can be preserved. Now let''s focus on the data preprocessing instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f6af703-9e8a-4f0c-a96b-09601ef91887.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature engineering and data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen some properties of the dataset and since there're no null
    values or categorical features, we don't need any other preprocessing or intermediate
    transformations. We just need to do some feature engineering before we can have
    our training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: The first step before getting these sets is to prepare training data that is
    consumable by the Spark regression model. For this, Spark classification and regression
    algorithms expect two components called `features` and `label`. Fortunately, we
    already have the `label` column. Next, the `features` column has to contain the
    data from all the columns except the `label` column, which can be achieved using
    the `VectorAssembler()` transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since all the columns are numeric, we can use `VectorAssembler()` directly
    from the Spark ML library to transform a given list of columns into a single vector
    column. So, let''s collect the list of desirable columns. As you may have guessed,
    we''ll have to exclude the `label` column, which can be done using the `dropRight()`
    method of standard Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the `VectorAssembler()` estimator, we now call the `transform()`
    method, which will embed selected columns into a single vector column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the last line of the preceding code segment shows the assembled
    DataFrame having `label` and `features`, which are needed to train an ML algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0b68aa4-2797-4b5e-9a8f-644d1d4edf84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now proceed to generate separate training and test sets. Additionally,
    we can cache both the sets for faster in-memory access. We use 60% of the data
    to train the model and the other 40% will be used to evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: That is all we need before we start training the regression models. At first,
    we start training the LR model and evaluate the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will develop a predictive analytics model for predicting
    slowness in traffic for each row of the data using an LR algorithm. First, we
    create an LR estimator as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we invoke the `fit()` method to perform the training on the training set
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have the fitted model, which means it is now capable of making predictions.
    So, let''s start evaluating the model on the training and validation sets and
    calculating the RMSE, MSE, MAE, R squared, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! We have managed to compute the raw prediction on the training and the
    test sets. Now that we have both the performance metrics on both training and
    validation sets, let''s observe the results of the training and the validation
    sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment should show something similar. Although, because
    of the randomness, you might experience slightly different output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the prediction on the test set as well, however, we can't directly
    say if it's a good or optimal regression model. To improve the result further
    with lower MAE, Spark also provides the generalized version of linear regression
    implementation called GLR.
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear regression (GLR)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In an LR, the output is assumed to follow a Gaussian distribution. In contrast,
    in **generalized linear models** (**GLMs**), the response variable *Y[i]* follows
    some random distribution from a parametric set of probability distributions of
    a certain form. As we have seen in the previous example, following and creating
    a GLR estimator will not be difficult:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'For the GLR-based prediction, the following response and identity link functions
    are supported based on data types (source: [https://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbc3b29b-50ad-4c25-9c92-8a25f3fdd716.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we invoke the `fit()` method to perform the training on the training set
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The current implementation through the `GeneralizedLinearRegression` interface
    in Spark supports up to 4,096 features only. Now that we have the fitted model
    (which means it is now capable of making predictions), let''s start evaluating
    the model on training and validation sets and calculating the RMSE, MSE, MAE,
    R squared, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! We have managed to compute the raw prediction on the training and the
    test sets. Now that we have both the performance metrics on both training and
    test sets, let''s observe the result on the train and the validation sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment should show similar results. Although, because of
    the randomness, you might experience slightly different output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Using GLR, we can see a slightly worse MAE value and also the RMSE is higher.
    If you see these two examples, we have not got to tune the hyperparameters but
    simply let the models train and evaluate a single value of each parameter. We
    could even use a regularization parameter for reducing overfitting. However, the
    performance of an ML pipeline often improves with the hyperparameter tuning, which
    is usually done with grid search and cross-validation. In the next section, we
    will discuss how we can get even better performance with the cross-validated models.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning and cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, the term hyperparameter refers to those parameters that
    cannot be learned from the regular training process directly. These are the various
    knobs that you can tweak on your machine learning algorithms. Hyperparameters
    are usually decided by training the model with different combinations of the parameters
    and deciding which ones work best by testing them. Ultimately, the combination
    that provides the best model would be our final hyperparameters. Setting hyperparameters
    can have a significant influence on the performance of the trained models.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, cross-validation is often used in conjunction with hyperparameter
    tuning. Cross-validation (also know asrotation estimation) is a model validation
    technique for assessing the quality of the statistical analysis and results. Cross-validation
    helps to describe a dataset to test the model in the training phase using the
    validation set.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unfortunately, there is no shortcut or straightforward way of choosing the
    right combination of hyperparameters based on a clear recipe—of course, experience
    helps. For example, while training a random forest, Matrix factorization, k-means,
    or a logistic/LR algorithm might be appropriate. Here are some typical examples
    of such hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of leaves, bins, or depth of a tree in tree-based algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of latent factors in a matrix factorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of clusters in a k-means clustering and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technically, hyperparameters form an *n*-dimensional space called a param-grid,
    where *n* is the number of hyperparameters. Every point in this space is one particular
    hyperparameter configuration, which is a hyperparameter vector.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 1](33fe7442-ce44-4a18-bac6-0e08e9b1ae1e.xhtml), *Introduction
    to Machine Learning with Scala*, overfitting and underfitting are two problematic
    phenomena in machine learning. Therefore, sometimes full convergence to a best
    model parameter set is often not necessary and can be even preferred, because
    an almost-best-fitting model tends to perform better on new data or settings.
    In other words, if you care for a best fitting model, you really don't need the
    best parameter set.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we cannot explore every point in this space, so the grid search
    over a subset in that space is commonly used. The following diagram shows some
    high-level idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01205c83-200d-47ce-aa8a-d7c03f2909ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Hyperparameter tuning of ML models'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there are several approaches for such a scheme, random search or grid
    search are probably the most well-known techniques used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Grid search**: Using this approach, different hyperparameters are defined
    in a dictionary that you want to test. Then a param-grid is constructed before
    feeding them into the ML model such that the training can be performed with the
    different combinations. Finally, the algorithm tells you for which combination
    of the hyperparameters you have the highest accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random search**: As you can understand, training an ML model with all possible
    combinations of hyperparameters is a very expensive and time consuming operation.
    However, often we don''t have that much flexibility but still we want to tune
    those parameters. In such a situation, random search could be a workaround. Random
    search is performed through evaluating *n* uniformly random points in the hyperparameter
    space, and selecting the right combination for which the model gives the best
    performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two types of cross-validation, called exhaustive cross-validation,
    which includes leave-p-out cross-validation and leave-one-out cross-validation,
    and non-exhaustive cross-validation, which is based on K-fold cross-validation
    and repeated random sub-sampling cross-validation, for example, 5-fold or 10-fold
    cross-validation, is very common.
  prefs: []
  type: TYPE_NORMAL
- en: In most of the cases, 10-fold cross-validation is used instead of testing on
    a validation set. Also, the training set should be as large as possible (because
    more data with quality features are good to train the model) not only to train
    the model but because about 5 to 10% of the training set can be used for the cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Using the K-fold cross-validation technique, the complete training data is split
    into K subsets. The model is trained on K-1 subsets; hold the last one for the
    validation. This process is repeated K times so that each time, one of the K subsets
    is used as the validation set and the other K-1 subsets are used to form the training
    set. This way, each of the subsets (fold) is used at least once for both training
    and validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, different machine learning models that have been obtained are joined
    by a bagging (or boosting) scheme for classifiers or by averaging (that is, regression).
    The following diagram explains the 10-fold cross-validation technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/460bc239-46e0-4317-8bfa-48b326d6ae45.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6: 10-fold cross-validation technique
  prefs: []
  type: TYPE_NORMAL
- en: Tuning and cross-validation in Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Spark ML, before performing the cross-validation, we need to have a `paramGrid`
    (that is a grid of parameters). The `ParamGridBuilder` interface is used in order
    to define the hyperparameter space where `CrossValidator` has to search and finally, `CrossValidator()`
    takes our pipeline, the hyperparameter space of our LR regressor, and the number
    of folds for the cross-validation as parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s start creating `paramGrid` by specifying the number of maximum iterations,
    the value of regularization parameter, the value of tolerance, and the elastic
    network parameters, as follows for the LR model (since we observed lower MAE for
    this):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The regularization parameter reduces overfitting by reducing the variance of
    your estimated regression parameters. Now, for a better and more stable performance,
    we can perform 10-fold cross-validation. Since our task is predicting continuous
    values, we need to define `RegressionEvaluator`, that is, the evaluator for regression,
    which expects two input columns—`prediction` and `label`—and evaluates the training
    based on MSE, RMSE, R-squared, and MAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Fantastic, we have created the cross-validation estimator. Now it''s time to
    train the LR model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'By the way, Spark provides a way to save a trained ML model using the `save()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the same model can be restored from the disk using the `load()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we compute the model''s metrics on the test set similar to the LR and
    GLR models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we gather the metrics and print to get some insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment should show something similar. Although, because
    of the randomness, you might experience slightly different output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, both the RMSE and MAE are slightly lower than the non-cross validated
    LR model. Ideally, we should have experienced even lower values for these metrics.
    However, due to the small size of the training as well as test sets, probably
    both the LR and GLR models overfitted. Still, we will try to use robust regression
    analysis algorithms in [Chapter 4](6730e23e-eabb-4628-934a-7ac609049563.xhtml),
    *Scala for Tree-Based Ensemble Techniques*. More specifically, we will try to
    solve the same problem with decision trees, random forest, and GBTRs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen how to develop a regression model for analyzing
    insurance severity claims using LR and GLR algorithms. We have also seen how to
    boost the performance of the GLR model using cross-validation and grid search
    techniques, which give the best combination of hyperparameters. Finally, we have
    seen some frequently asked questions so that the similar regression techniques
    can be applied for solving other real-life problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see another supervised learning technique called
    classification through a real-life problem called analyzing outgoing customers
    through churn prediction. Several classification algorithms will be used for making
    the prediction in Scala. Churn prediction is essential for businesses as it helps
    you detect customers who are likely to cancel a subscription, product, or service,
    which also minimizes customer defection by predicting which customers are likely
    to cancel a subscription to a service.
  prefs: []
  type: TYPE_NORMAL
