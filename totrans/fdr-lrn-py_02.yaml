- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: What Is Federated Learning?
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是联邦学习？
- en: In [*Chapter 1*](B18369_01.xhtml#_idTextAnchor017), *Challenges in Big Data
    and Traditional AI*, we examined how shifting tides in big data and **machine
    learning** (**ML**) have set the stage for a novel approach to practical ML applications.
    This chapter frames **federated learning** (**FL**) as the answer to the desire
    for this new ML approach. In a nutshell, FL is an approach to ML that allows models
    to be trained in parallel across data sources without the transmission of any
    data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B18369_01.xhtml#_idTextAnchor017)《大数据和传统人工智能的挑战》中，我们探讨了大数据和**机器学习（ML**）的潮流如何为新的实用ML应用方法奠定了基础。本章将**联邦学习（FL**）视为满足这种新ML方法需求的答案。简而言之，FL是一种ML方法，允许模型在数据源之间并行训练，而不需要传输任何数据。
- en: The goal of this chapter is to build up the case for the FL approach, with explanations
    of the necessary conceptual building blocks in order to ensure that you can achieve
    a similar understanding of the technical aspects and practical usage of FL.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是建立FL方法的论据，解释必要的概念构建块，以确保你可以达到对FL的技术方面和实际应用的类似理解。
- en: After reading the chapter, you should have a high-level understanding of the
    FL process and should be able to visualize where the approach slots into real-world
    problem domains.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，你应该对FL过程有一个高级别的理解，并且能够可视化这种方法在现实世界问题领域中的位置。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding the current state of ML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解当前机器学习（ML）的状态
- en: Distributed learning nature – toward scalable AI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式学习特性——走向可扩展人工智能
- en: Understanding FL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解联邦学习（FL）
- en: FL system considerations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FL系统考虑因素
- en: Understanding the current state of ML
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解当前机器学习（ML）的状态
- en: To understand why the benefits derived from the application of FL can outweigh
    the increased complexity of this approach, it is necessary to understand how ML
    is currently practiced and the associated limitations. The goal of this section
    is to provide you with this context.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解从应用FL中获得的利益为何可能超过这种方法增加的复杂性，有必要了解ML当前的实践及其相关的限制。本节的目标是为你提供这个背景。
- en: What is a model?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是模型？
- en: The term “model” finds usage across numerous different disciplines; however,
    the generalized definition we are interested in can be narrowed down to a working
    representation of the dynamics within some desired system. Simply put, we develop
    a model B of some phenomenon A as a means of better understanding A through the
    increased interactability offered by B. Consider the phenomenon of an object being
    dropped from some point in a vacuum. Using kinematic equations, we can compute
    exactly how long it will take for the object to hit the ground – this is a model
    of the aforementioned phenomenon.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 术语“模型”在众多不同学科中都有应用；然而，我们感兴趣的广义定义可以缩小到某些所需系统内动态的工作表示。简单来说，我们通过B模型更好地理解现象A，A是现象B的某种现象，这是通过B提供的增加的交互性来实现的。考虑一个物体从真空中某个点掉落的现象。使用运动方程，我们可以精确计算出物体落地所需的时间——这就是上述现象的模型。
- en: The power of this approach is the ability to observe results from the created
    model without having to explicitly interact with the phenomenon in question. For
    example, the model of the falling object allows us to determine the difference
    in fall time between a 10 kg object and a 50 kg object at some height without
    having to physically drop real objects from said height in a real vacuum. Evidently,
    the modeling of natural phenomena plays a key role in being able to claim a true
    understanding of said phenomena. Removing the need for the comprehensive observation
    of a phenomenon allows for true generalization in the decision-making process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的强大之处在于，可以在不与所讨论的现象进行明确交互的情况下观察创建的模型的结果。例如，下落物体的模型使我们能够在某个高度上确定10公斤物体和50公斤物体下落时间的差异，而无需在真实真空中从该高度物理地释放真实物体。显然，对自然现象的建模在能够声称真正理解这些现象方面发挥着关键作用。消除对现象进行全面观察的需要，使得在决策过程中实现真正的泛化成为可能。
- en: The concept of a model is greatly narrowed down within the context of computer
    science. In this context, models are algorithms that allow for some key values
    of a phenomenon to be output given some initial characterization of the phenomenon
    in question. Going back to the falling object example, a computer science model
    could entail the computation of values such as the time to hit the ground and
    the maximum speed given the mass of the object and the height from which it is
    dropped. These computer science models are uniquely powerful due to the superhuman
    ability of computers to compute the output from countless starting phenomenon
    configurations, offering us even greater understanding and generalization.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学领域，模型的概念被大大缩小了。在这种情况下，模型是算法，它允许根据对所讨论现象的某些初始描述输出该现象的一些关键值。回到落体例子，计算机科学模型可能包括根据物体的质量和下落的高度计算值，如击中地面的时间和最大速度。这些计算机科学模型由于计算机在从无数起始现象配置中计算输出的超人类能力而独具特色，为我们提供了更深入的理解和更广泛的概括。
- en: So, how do we create such models? The first and simplest approach is building
    rule-based systems or **white-box** models. A white-box (also known as glass-box
    or clear-box) model is made by writing down the underlying functions of a system
    of interest explicitly. This is only possible when information about the system
    is available *a priori*. Naturally, in this case, the underlying functions are
    relatively simple. One such example is the problem of classifying a randomly selected
    integer as odd or even; we can easily write an algorithm to do this by *checking
    the remainder after dividing the integer by two*. If you want to see how much
    it costs to fill up your gas tank, given how empty the tank is and the price per
    gallon, you can just multiply those values together. Despite their simplicity,
    these examples illustrate that simple models can have a lot of practical applications
    in various fields.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何创建这样的模型呢？第一个也是最简单的方法是构建基于规则的系统或**白盒**模型。白盒模型（也称为玻璃盒或透明盒）是通过明确写出感兴趣系统的底层函数来构建的。这只有在系统信息可预先获得的情况下才可能。自然地，在这种情况下，底层函数相对简单。一个这样的例子是分类随机选择的整数是奇数还是偶数的问题；我们可以很容易地编写一个算法通过*检查整数除以二的余数*来完成这个任务。如果你想看看加满油箱需要花多少钱，给定油箱的空余量和每加仑的价格，你只需将这些值相乘即可。尽管这些例子很简单，但它们说明了简单的模型可以在各个领域有大量的实际应用。
- en: 'Unfortunately, the white-box modeling of underlying functions can quickly become
    too complex to perform directly. In general, systems are often too complex for
    us to be able to construct a white-box model for. For example, let’s say you want
    to predict the future values of your property. You have a lot of metrics about
    the property, such as the area, how old it is, its location, and interest rate
    to name but a few. You believe that there is likely a linear relationship between
    the property value and all of those metrics, such that the weighted sum of all
    of them would give you the property value. Now, if you actually try to build a
    white-box model under that assumption, you will have to directly figure out what
    the parameter (weight) for each metric is, which implies that you must know the
    underlying function of the real estate pricing system. Usually, this is not the
    case. Therefore, we need another approach: **black box** modeling.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，底层函数的白盒建模很快就会变得过于复杂而无法直接执行。一般来说，系统通常过于复杂，我们无法构建白盒模型。例如，假设你想预测你财产的未来价值。你有很多关于财产的指标，比如面积、它的年龄、位置，以及利率等等。你相信财产价值和所有这些指标之间可能存在线性关系，即所有这些指标的加权总和将给出财产价值。现在，如果你实际上基于这个假设尝试构建白盒模型，你必须直接确定每个指标的参数（权重），这意味着你必须知道房地产定价系统的底层函数。通常情况下，这不是事实。因此，我们需要另一种方法：**黑盒**建模。
- en: ML – automating the model creation process
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML – 自动化模型创建过程
- en: The concept of a black box system was first developed in the field of electric
    circuits during the WWII period. It was the famous cybernetician Norbert Wiener
    who began treating the black box as an abstract concept, and a general theory
    was established by Mario Augusto Bunge in the 1960s. The function for estimating
    future property values, as illustrated earlier, is a good example of a black box.
    As you might expect, the function is complex enough that it is unreasonable for
    us to try to write a white-box model to represent it. This is where ML comes in,
    allowing us to create a model as a black box.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒系统的概念最早在二战期间的电气电路领域发展起来。正是著名的控制论学家诺伯特·维纳开始将黑盒视为一个抽象概念，并在20世纪60年代由马里奥·奥古斯特·邦格建立了通用理论。如前所述，估计未来房地产价值的函数是一个很好的黑盒例子。正如你所期望的，这个函数足够复杂，我们尝试编写一个白盒模型来表示它是不可行的。这就是机器学习发挥作用的地方，它允许我们创建一个作为黑盒的模型。
- en: Reference
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献
- en: You might be aware that black box modeling has been criticized for its lack
    of interpretability, an important concept outside the scope of this book; we recommend
    Packt’s *Interpretable Machine Learning with Python* by Serg Masís as a reference
    on the subject.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能知道，黑盒建模因其缺乏可解释性而受到批评，这是本书范围之外的一个重要概念；我们推荐Serg Masís所著的Packt出版的《Python可解释机器学习》作为该主题的参考。
- en: 'ML is a type of artificial intelligence that is used to automatically generate
    model parameters for making decisions and predictions. *Figure 2.1* illustrates
    this in a very simple way: those cases where the known values and the unknown
    value have a linear relationship allow a popular algorithm, called **ordinary
    least squares** (**OLS**), to be applied. OLS computes the unknown parameters
    of the linear relationship by finding the set of parameters that produces the
    closest predictions on some set of known examples (pairs of input feature value
    sets and the true output value):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一种人工智能，用于自动生成用于决策和预测的模型参数。*图 2.1* 以非常简单的方式说明了这一点：那些已知值和未知值之间存在线性关系的案例允许应用一个流行的算法，称为**普通最小二乘法**（**OLS**）。OLS通过找到产生最接近预测的一组参数来计算线性关系的未知参数，这些参数是在一些已知示例（输入特征值集和真实输出值对）上进行的：
- en: '![Figure 2.1 – ML determining model parameters'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1 – 机器学习确定模型参数]'
- en: '](img/B18369_02_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18369_02_01.jpg]'
- en: Figure 2.1 – ML determining model parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1 – 机器学习确定模型参数]'
- en: The preceding diagram displays a simple two-dimensional linear regression problem
    with one feature/input variable and one output variable. In this toy two-dimensional
    case, it might be relatively straightforward for us to come up with the parameters
    representing the best-fit relationship directly, either through implicit knowledge
    or through testing different values. However, it should be clear that this approach
    quickly becomes intractable as the number of feature variables increases.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了一个简单的二维线性回归问题，包含一个特征/输入变量和一个输出变量。在这个简单的二维案例中，我们可能相对容易直接提出代表最佳拟合关系的参数，无论是通过隐含知识还是通过测试不同的值。然而，很明显，随着特征变量的数量增加，这种方法很快就会变得难以处理。
- en: 'OLS allows us to attack this problem from the reverse direction: instead of
    producing linear relationships and evaluating them on the data, we can use the
    data to compute the parameters of the best-fit relationship directly instead.
    Revisiting the real estate problem, let’s assume that we have collected a large
    number of property valuation data points, consisting of the associated metric
    values and the sale price. We can apply OLS to take these points and find the
    relationship between each metric and the sale price for any property (still under
    the assumption that the true relationship is linear). From this, we can pass in
    the metric values of our property and get the predicted sale price.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘法（OLS）允许我们从相反的方向解决这个问题：我们不是产生线性关系并在数据上评估它们，而是可以直接使用数据来计算最佳拟合关系的参数。回顾房地产问题，假设我们已经收集了大量房地产估值数据点，包括相关的指标值和销售价格。我们可以应用OLS来取这些点，并找出每个指标与任何房地产销售价格之间的关系（仍然假设真实关系是线性的）。据此，我们可以输入我们房地产的指标值，并得到预测的销售价格。
- en: The power of this approach is the abstraction of this relationship computation
    from any implicit knowledge of the problem. The OLS algorithm doesn’t care what
    the data represents – it just finds the best line for the data it is given. This
    class of approaches is exactly what ML entails, granting the power to create models
    of phenomena without any required knowledge of the internal relationship, given
    a sufficient amount of data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的力量在于将这种关系计算从对问题的任何隐含知识中抽象出来。最小二乘法（OLS）算法并不关心数据代表什么——它只是找到给定数据的最佳直线。这类方法正是机器学习所包含的，它赋予了在没有内部关系知识的情况下，仅凭足够的数据创建现象模型的能力。
- en: In a nutshell, ML lets us program algorithms that can learn to create models
    from data, and our motivation to do so is to approximate complex systems. It is
    important to keep in mind that the underlying functions of a complex system can
    change over time due to outside factors, quickly making models created from old
    data obsolete. For example, the preceding linear regression model might not work
    to estimate property values in a far distant future or a faraway district. Variance
    in such a macroscopic scale is not taken into account in a model containing only
    a few dozen parameters, and we would need different models for separate groups
    of adjacent data points – unless we employ even more sophisticated ML approaches
    such as **deep learning**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，机器学习（ML）让我们能够编写算法，这些算法可以从数据中学习创建模型，而我们这样做的原因是为了近似复杂系统。重要的是要记住，由于外部因素的影响，复杂系统的底层函数可能会随时间而改变，这使得从旧数据中创建的模型很快就会过时。例如，前面的线性回归模型可能无法用于估计遥远未来或遥远地区的财产价值。在只包含几十个参数的模型中，并没有考虑到这种宏观尺度上的变化，我们需要为相邻数据点的不同组别使用不同的模型——除非我们采用更复杂的机器学习方法，如**深度学习**。
- en: Deep learning
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习
- en: So, how did deep learning become synonymous with ML in common usage? Deep learning
    involves the application of a **deep neural network** (**DNN**), which is a type
    of highly-parameterized model inspired by the transmission of signals between
    neurons in the brain. The foundation of deep learning was established in the early
    1960s by Frank Rosenblatt, who is known as the *father of deep learning*. His
    work was further developed in the 1970s and 1980s by computer scientists including
    Geoffrey Hinton, Yann LeCun, and Yoshua Bengio, and the term *deep learning* was
    popularized by the University of California, Irvine’s distinguished Professor
    Rina Dechter. Deep learning can conduct much more complex tasks compared to simpler
    ML algorithms such as linear regression. While the specifics are beyond the scope
    of this book, the key problem that deep learning was able to solve was the modeling
    of complex non-linear relationships, pushing ML as a whole to the forefront of
    numerous fields due to the increased modeling ability it provided.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，深度学习是如何在普通用法中与机器学习（ML）同义的？深度学习涉及应用**深度神经网络**（DNN），这是一种受大脑中神经元之间信号传递启发的超参数化模型。深度学习的基础是在20世纪60年代初由Frank
    Rosenblatt建立的，他被誉为**深度学习之父**。他的工作在20世纪70年代和80年代得到了包括Geoffrey Hinton、Yann LeCun和Yoshua
    Bengio在内的计算机科学家的进一步发展，而“深度学习”这一术语是由加州大学欧文分校的杰出教授Rina Dechter普及的。与简单的机器学习算法（如线性回归）相比，深度学习可以执行更复杂的任务。虽然具体内容超出了本书的范围，但深度学习能够解决的关键问题是复杂非线性关系的建模，由于它提供的建模能力增强，推动了机器学习作为整体在众多领域的最前沿。
- en: This ability has been mathematically proven via specific universal approximation
    theorems for different model size cases. For those of you who are new to deep
    learning or ML in general, the third edition of *Python Machine Learning* by Sebastian
    Raschka and Vahid Mirjalili is a good reference to learn more about the subject.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能力已经通过针对不同模型大小案例的具体通用逼近定理在数学上得到证明。对于那些对深度学习或机器学习（ML）总体上都是新手的人来说，Sebastian
    Raschka和Vahid Mirjalili合著的《Python机器学习》第三版是一个很好的参考资料，可以了解更多关于这个主题的内容。
- en: Over the past decade, ever-increasingly powerful models have been built by tech
    giants against the backdrop of big data, as discussed in [*Chapter 1*](B18369_01.xhtml#_idTextAnchor017),
    *Challenges in Big Data and Traditional AI*. If we look at the state-of-the-art
    deep learning models today, they could have up to trillions of parameters; expectedly,
    this gives them unparalleled flexibility in modeling complex functions. The reason
    deep learning models can be scaled up to arbitrarily increase performance, unlike
    other ML model types used previously, is due to a phenomenon called **double descent**.
    This refers to the ability for a certain parameterization/training threshold to
    overcome the standard bias-variance trade-off (where increasing complexity leads
    to fine-tuning on training data, reducing bias but increasing variance) and continuing
    to increase performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway is that the performance of deep learning models can be considered
    to be limited by just the available compute power and data, two factors that have
    surged in growth in the past 10 years due to advances in computing and the ever-increasing
    number of devices and software collecting data, respectively. Deep learning has
    become intertwined with ML, with deep learning playing a significant role within
    the current state of ML and big data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: This section focused on establishing a case for the importance of the modeling
    performed by current ML techniques. In a sense, this can be considered the *what*
    – what exactly FL is trying to do. Next, we will focus on the *where* in terms
    of the desired setting for numerous ML applications.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Distributed learning nature – toward scalable AI
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we introduce the distributed computing setting and discuss
    the intersection of this setting with ML approaches to fully establish the support
    for why FL is necessary. The goal of the section is for the user to understand
    both the benefits and limitations imposed by the distributed computing setting,
    in order to understand how FL addresses some of these limitations.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Distributed computing
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The past several years have shown a large but predictable rise in the development
    of new approaches and the conversion of existing server infrastructure within
    the lens of distributed computing. To generalize further, distributed approaches
    themselves have shifted more and more from research implementations to extensive
    use in production settings; one significant example of this phenomenon is the
    usage of cloud computing platforms such as AWS from Amazon, **Google Cloud Platform**
    (**GCP**) from Google, and Azure from Microsoft. It turns out that the flexibility
    of on-demand resources allows for cost-saving and efficiency in numerous applications
    that would, otherwise, be bottlenecked by on-premise servers and computational
    power.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'While a parallel cannot exactly be drawn between cloud computing and the concept
    of distributed computing, the key benefits stemming from the distributed nature
    are similar. At a high level, distributed computing involves spreading the work
    necessary for some computational task over a number of computational agents in
    a way that allows each to act near-autonomously. The following figure shows the
    difference between centralized and distributed approaches in the high-level context
    of answering questions:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不能完全将云计算与分布式计算的概念相提并论，但由此产生的关键好处在本质上相似。从高层次来看，分布式计算涉及将某些计算任务的必要工作分散到多个计算代理中，以便每个代理都能近乎自主地行动。以下图显示了在回答问题的高层次上下文中，集中式和分布式方法之间的差异：
- en: '![Figure 2.2 – Centralized versus distributed question answering'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2 – 集中式与分布式问答'
- en: '](img/B18369_02_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18369_02_02.jpg)'
- en: Figure 2.2 – Centralized versus distributed question answering
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 集中式与分布式问答
- en: In this toy example, the centralized approach involves processing the input
    questions sequentially, whereas the distributed approach is able to process each
    question at the same time. It should be clear that the parallel approach trades
    off computational resource usage for increased answering speed. The natural question,
    then, is whether this trade-off is beneficial for real-world applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个玩具示例中，集中式方法涉及按顺序处理输入问题，而分布式方法能够同时处理每个问题。应该清楚的是，并行方法是在计算资源使用和增加回答速度之间进行权衡。那么，这个权衡对于现实世界的应用是否有益，自然就成为了一个问题。
- en: Real-world example – e-commerce
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现实世界案例 – 电子商务
- en: To understand the practical benefits of distributed computing approaches, let’s
    analyze an example business problem through a traditional and a distributed computing
    lens. Consider an e-commerce business that is trying to host its website using
    on-premise servers. The traditional way to do this would be to perform enough
    analysis on the business side to determine the expected volume of traffic at some
    future time and invest in one or a couple of server machines large enough to handle
    that calculated volume.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解分布式计算方法的实际好处，让我们通过传统计算和分布式计算的角度分析一个业务问题。考虑一个试图使用本地服务器托管其网站的电子商务业务。传统的方法是在业务方面进行足够分析，以确定未来某个时间点的预期流量量，并投资购买一台或几台足够处理该计算出的流量的服务器机器。
- en: Several cases immediately lend themselves to showing the flaws of such an approach.
    Consider a scenario where usage of the websites greatly exceeds the initial projections.
    A fixed number of servers means that all upgrades must be hardware upgrades, resulting
    in old hardware that had to be purchased and is no longer used. Going further,
    there are no guarantees that the now-increased usage will stay fixed. Further
    increases in usage will result in more scaling-up costs, while decreases in usage
    will lead to wasted resources (maintaining large servers when smaller machines
    would be sufficient). A key point is that the integration of additional servers
    is non-trivial due to the single-machine approach used to manage hosting. Additionally,
    we have to consider the hardware limitations of handling large numbers of requests
    in parallel with one or a few machines. The ability to handle requests in parallel
    is limited for each machine – significant volumes of traffic would be almost guaranteed
    to eventually be bottlenecked regardless of the power available to each server.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 几个案例立即揭示了这种方法的缺陷。考虑一种情况，即网站的使用量远远超过了最初的预测。固定数量的服务器意味着所有升级都必须是硬件升级，导致必须购买且不再使用的旧硬件。进一步来说，现在增加的使用量是否保持不变也没有保证。使用量的进一步增加将导致更多的扩展成本，而使用量的减少将导致资源浪费（当小型机器就足够时，仍需维护大型服务器）。一个关键点是，由于使用单机方法来管理托管，额外服务器的集成并不简单。此外，我们必须考虑处理大量并发请求的硬件限制。每个机器处理并发请求的能力有限——大量的流量几乎肯定会最终成为瓶颈，无论每个服务器的可用功率如何。
- en: In comparison, consider the distributed computing-based solution for this problem.
    Based on the initial business projections, a number of smaller server machines
    are purchased and each is set up to handle some fixed volume of traffic. If the
    scenario of incoming traffic exceeding projects arises, no modification to the
    existing machines is necessary; instead, more similarly-sized servers can be purchased
    and configured to handle their designated volume of new traffic. If the incoming
    traffic decreases, the equivalent number of servers can be shut down or shifted
    to handle other tasks. This means that the same hardware can be used for variable
    volumes of traffic.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，考虑基于分布式计算的解决方案。根据初步的商业预测，购买了一些较小的服务器机器，并且每台机器都设置好了以处理一些固定的流量量。如果出现超出预期的流量情况，不需要对现有机器进行修改；相反，可以购买更多类似尺寸的服务器，并配置它们来处理指定的新流量量。如果流量减少，相应数量的服务器可以被关闭或转移到处理其他任务。这意味着相同的硬件可以用于可变流量的处理。
- en: This ability to scale quickly to handle the necessary computational task at
    any moment is precisely due to how distributed computing approaches allow for
    computational agents to seamlessly start and stop working on said task. In addition,
    the use of many smaller machines in parallel, versus using fewer larger machines,
    means that the number of requests that can be handled at the same time is notably
    higher. It is clear that a distributed computing approach, in this case, lends
    itself to cost-saving and flexibility that cannot be matched with more traditional
    methods.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能力能够快速扩展以处理任何时刻必要的计算任务，这正是分布式计算方法允许计算代理无缝开始和停止工作在所述任务上的原因。此外，与使用较少的大型机器相比，并行使用许多较小的机器意味着可以同时处理的请求数量显著更高。很明显，在这种情况下，分布式计算方法本身具有节省成本和灵活性的优势，这是更传统的方法无法比拟的。
- en: Benefits of distributed computing
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式计算的好处
- en: In general, distributed computing approaches offer three main benefits for any
    computational task – scalability, throughput, and resilience. In the previous
    case of web hosting, scalability referred to the ability to scale the number of
    servers deployed based on the amount of incoming traffic, whereas throughput refers
    to the ability to reduce request processing latency through the inherent parallelism
    of smaller servers. In this example, resilience could refer to the ability of
    other deployed servers to take on the load from a server that stops working, allowing
    the hosting to continue relatively unfazed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，分布式计算方法为任何计算任务提供了三个主要好处——可扩展性、吞吐量和弹性。在前面的网页托管案例中，可扩展性指的是根据进入的流量量调整部署的服务器数量，而吞吐量指的是通过较小服务器的固有并行性来减少请求处理延迟的能力。在这个例子中，弹性可能指的是其他部署的服务器能够承担停止工作的服务器的负载，从而使得托管能够相对不受影响地继续进行。
- en: Distributed computing often finds uses when working with large stores of data,
    especially when attempting to perform analyses on the data using a single machine
    would be computationally infeasible or otherwise undesirable. In these cases,
    scalability allows for the deployment of a variable number of agents based on
    factors such as the desired runtime and amount of data at any given time, whereas
    the ability of each agent to autonomously work on processing a subset of the data
    in parallel allows for processing throughput that would be impossible for a single
    high-power machine to achieve. It turns out that this lack of reliance on cutting-edge
    hardware leads to further cost savings, as hardware price-to-performance ratios
    are often not linear.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算通常在处理大量数据时得到应用，尤其是在使用单一机器进行数据分析在计算上不可行或不太理想的情况下。在这些情况下，可扩展性允许根据所需的运行时间和任何给定时间的数据量等因素部署可变数量的代理，而每个代理能够自主地并行处理数据子集的能力，使得处理吞吐量对于单一高性能机器来说是不可能的。结果发现，这种不依赖于尖端硬件的做法进一步降低了成本，因为硬件价格与性能比通常不是线性的。
- en: While the development of parallelized software to operate in a distributed computing
    setting is non-trivial, hopefully, it is clear that many practical computational
    tasks greatly benefit from the scalability and throughput achieved by such approaches.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然开发用于在分布式计算环境中运行的并行化软件并非易事，但希望这清楚地表明，许多实际计算任务都极大地受益于这种方法实现的可扩展性和吞吐量。
- en: Distributed ML
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式机器学习
- en: When thinking about the types of computational tasks that have proven to be
    valuable in practical applications and that might be directly benefited from increased
    scalability and throughput, it is clear that the rapidly growing field of ML is
    near the top. In fact, we can frame ML tasks as a specific example of the aforementioned
    tasks of analyzing large stores of data, placing emphasis on the data being processed
    and the nature of the analysis being performed. The joint growth of cheap computational
    power (for example, smart devices) and the proven benefits of data analysis and
    modeling have led to companies with both the storage of excessive amounts of data
    and the desire to extract meaningful insights and predictions from said data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑那些在实用应用中已被证明有价值且可能直接受益于增加可扩展性和吞吐量的计算任务类型时，很明显，快速增长的机器学习领域几乎处于顶端。事实上，我们可以将机器学习任务视为上述分析大量数据任务的具体例子，强调正在处理的数据和分析所执行的性质。廉价计算能力（例如，智能设备）的联合增长以及数据分析建模的既定益处，导致了一些公司拥有大量数据存储，并希望从这些数据中提取有意义的见解和预测。
- en: The second part is exactly what ML is geared to solve, and large amounts of
    work have already been completed to do so in various domains. However, like other
    computational tasks, performing ML on large stores of data often leads to a time-computational
    power trade-off in which more powerful machines are needed to perform such tasks
    in reasonable amounts of time. As ML algorithms become more computationally and
    memory-intensive, such as recent state-of-the-art deep learning models with billions
    of parameters, hardware bottlenecks make increasing the computational power infeasible.
    As a result, current ML tasks must apply distributed computing approaches to stay
    cutting-edge while producing results in usable timeframes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分正是机器学习旨在解决的问题，已经在各个领域完成了大量工作。然而，与其他计算任务一样，在大量数据上执行机器学习往往会导致时间-计算能力权衡，需要更强大的机器在合理的时间内完成这些任务。随着机器学习算法在计算和内存方面变得更加密集，例如最近具有数十亿参数的最新深度学习模型，硬件瓶颈使得增加计算能力变得不可行。因此，当前的机器学习任务必须应用分布式计算方法，以保持前沿地位，同时在使用时间内产生结果。
- en: Edge inference
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 边缘推理
- en: Although the prevalence of deep learning described earlier, besides the paradigm
    shift from big data to collective intelligence discussed in [*Chapter 1*](B18369_01.xhtml#_idTextAnchor017),
    *Challenges in Big Data and Traditional AI*, gives enough motivation for distributed
    ML, its physical foundation came from the recent development of **edge computing**.
    The *edge* represents the close proximity around deployed solutions; it follows
    that edge computing refers to processing data at or near the location of the data
    source. Extending the concept of computation to ML leads to the idea of **edge
    AI**, where models are integrated directly into edge devices. A few popular examples
    would be Amazon Alexa, where edge AI takes care of speech recognition, and self-driving
    cars that collect real-world data and incrementally improve with edge AI.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面描述的深度学习的普及，以及在第[*第一章*](B18369_01.xhtml#_idTextAnchor017)中讨论的从大数据到集体智能的范式转变——“大数据与传统人工智能的挑战”，为分布式机器学习提供了足够的动力，但其物理基础却源于最近边缘计算的快速发展。这里的*边缘*指的是部署解决方案的附近区域；因此，边缘计算指的是在数据源附近或其处处理数据。将计算的概念扩展到机器学习导致**边缘人工智能**（Edge
    AI）的想法，其中模型直接集成到边缘设备中。一些流行的例子包括亚马逊Alexa，其中边缘人工智能负责语音识别，以及自动驾驶汽车，它们收集真实世界的数据，并通过边缘人工智能逐步改进。
- en: The most ubiquitous example is the smartphone – some potential uses are the
    recommendation of content to the user, searches with voice assistance and auto-complete,
    auto-sorting of pictures into an album and gallery search, and more. To capitalize
    on this potential, smartphone manufacturers have already begun integrating ML-focused
    processor components into the chips they integrate with their newest phones, such
    as the *Neural Processing Unit* from *Samsung* and the *Tensor Processing Unit*
    on the *Google Tensor chip*. Google has also worked to develop ML-focused APIs
    for Android applications through their *Android ML Kit SDK*. From this, it should
    be clear that ML applications are shifting toward the edge computing paradigm.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say that smartphones need to use a deep learning model for word recommendation.
    This is so that when you type words on your phone, it gives you suggestions for
    the next word, with the goal being to save you some time. In the scheme of a centralized
    computing process, the central server is the only component that has access to
    this text prediction model and none of the phones have the model stored locally.
    The central server handles all of the requests sent from the phones to return
    word recommendations. As you type, your phone has to send what has been typed
    along with some personal information about you, all the way to the central server.
    The server receives this information, makes a prediction using the deep learning
    model, and then sends the result back to the phone. The following figure reflects
    this scenario:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Centralized inference scenario'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_02_03.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 – Centralized inference scenario
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: There are a few problems that become apparent when you look at this scenario.
    First, even a half to one second of latency makes the recommendation slower than
    typing everything yourself, making the system useless. Furthermore, if there is
    no internet connection, the recommendation simply does not work. Another restriction
    of this scheme is the need for the central server to process all of these requests.
    Imagine how many smartphones are being used in the world, and you will realize
    a lack of feasibility due to the extreme scale of this solution.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the same problem from the edge computing perspective. What
    if the smartphones themselves contain the deep learning model? The central server
    is only in charge of managing the latest trained model and communicating this
    model with each phone. Now, whenever you start typing, your phone can use the
    received model locally to make recommendations from what you typed. The following
    figure reflects this scenario:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Edge inference scenario'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_02_04.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 – Edge inference scenario
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'This removes both the latency problem and prevents the need to handle the incoming
    inference requests at a central location. In addition, the phones no longer have
    to maintain a connection with the server to make a recommendation. Each phone
    is in charge of fulfilling requests from its user. This is the core benefit of
    edge computing: *we have moved the computing load from the central server to the
    edge devices/servers*.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Edge training
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The distinction between centralized and decentralized computing can be extended
    to the concept of model training. Let’s stick to the smartphone example but think
    about how we would train the predictive model instead. First, in the centralized
    ML process, all of the data used to train the recommendation model must be collected
    from the users’ devices and stored on the central server. Then, the collected
    data is used to train a model, which is eventually sent to all the phones. This
    means that the central server still has to be able to handle the large volume
    of user data coming in and store it in an efficient way to be able to train the
    model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'This design leads to the problems found in the centralized computing approach:
    as the number of phones connected to the server increases, the server’s ability
    to work with the incoming data needs to scale in order to maintain the training
    process. In addition, since the data needs to be transmitted and stored centrally
    in this approach, there is always the possibility of the interception of transmissions
    or even attacks on the stored data. There are several cases where data confidentiality
    and privacy are required or strongly desired; for example, applications in the
    financial and medical industries. *Centralized model training thus limits use
    cases, and an alternative way to work with data directly on edge devices is required*.
    This exact setting is the motivation for FL.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Understanding FL
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section focuses on providing a high-level technical understanding of how
    FL actually slots in as a solution to the problem setting described in the previous
    section. The goal of this section is for you to understand how FL fits as a solution,
    and to provide a conceptual basis that will be filled in by the subsequent chapters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Defining FL
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Federated learning is a method to synthesize global models from local models
    trained on the edge*. FL was first developed by Google in 2016 for their Gboard
    application, which incorporates the context of an Android user’s typing history
    to suggest corrections and propose candidates for subsequent words. Indeed, this
    is the exact word recommendation problem discussed in the *Edge inference* and
    *Edge training* sections. The solution that Google produced was a decentralized
    training approach where an iterative process would compute model training updates
    at the edge, aggregating these updates to produce the global update to be applied
    to the model. This core concept of aggregating model updates was key in allowing
    for a single, performant model to be produced from edge training.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Let’s break this concept down further. The desired model is distributed across
    the edge and is trained on data collected locally at the edge. Of course, we can
    expect that a model trained on one specific data source is not going to be representative
    of the entire dataset. As a result, we dub such models trained with limited data
    **local models**. One immediate benefit of this approach is the enabling of ML
    on data that would otherwise not be collected in the centralized case, due to
    issues with privacy and efficiency.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation, the key theoretical step of FL, allows for our desired single **global
    model** to be created from the set of local models produced at some iteration.
    The most well-known aggregation algorithm, popular for its simplicity and surprising
    performance, is called **federated averaging** (**FedAvg**). FedAvg is performed
    on a set of local models by computing the parameter-wise arithmetic mean across
    the models, producing an aggregate model. It is important to understand that performing
    aggregation once is not enough to produce a good global aggregate model; instead,
    it is the iterative process of locally training the previous global model and
    aggregating the produced local models into a new global model that allows for
    global training progress to be made.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: The FL process
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To better understand FL from an iterative process perspective, we break it down
    into the core constituent steps of a single iteration, or *round*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps for a round can be described as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The aggregate global model parameters are sent to each user’s device.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The received ML models located on the user devices are trained with local data.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After a certain amount of training, the local model parameters are sent to the
    central server.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The central server aggregates the local models by applying an aggregation function,
    producing a new aggregate global model.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These steps are depicted in *Figure 2.5*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – FL steps'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_02_05.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 – FL steps
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The flow from *steps 1 to 4* constitutes a single round of FL. The next round
    begins as the user servers/devices receive the newly created aggregate model and
    start training on the local data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Let’s revisit Google’s word recommendation for Gboard. At some point in time,
    each phone stores a sufficient amount of its user’s typing data. The edge training
    process can create a local model from it, and the parameters will be sent to the
    central server. After receiving parameters from a certain number of phones, the
    server aggregates them to create a global model and sends it to the phones. This
    way, every phone connected to the server receives a model that reflects local
    data in all of the phones without ever transmitting the data from them. In turn,
    each phone retrains the model when another batch of sufficient data is collected,
    sends the model to the server, and receives a new global model. This cycle repeats
    itself over and over according to the configuration of the FL system, resulting
    in the continuous monitoring and updating of the global model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Note that the user data never leaves the edge, only the model parameters; nor
    is there a need to put all the data in a central server to generate a global model,
    allowing for data minimalism. Moreover, model bias can be mitigated with FL methods,
    as discussed in [*Chapter 3*](B18369_03.xhtml#_idTextAnchor058), *Workings of
    the Federated Learning System*. That is why FL can be regarded as a solution to
    the three issues of big data, which were introduced in [*Chapter 1*](B18369_01.xhtml#_idTextAnchor017),
    *Challenges in Big Data and Traditional AI*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FL is closely related to an ML concept called **transfer learning** (**TL**).
    TL allows us to use large deep learning models that have been trained by researchers
    using plentiful compute power and resources on very generalized datasets. These
    models can be applied to more specific problems.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can take an object detection model trained to locate and name
    specific objects in images and retrain it on a limited dataset containing specific
    objects we are interested in, which were not included in the original data. If
    you were to take the original data, add to it the data of those objects of our
    interest, and then train a model from scratch, a lot of computational time and
    power would be required. With TL, you can quicken the process by leveraging a
    key fact about those existing large, generalized models. There is a tendency for
    the intermediate layers of large DNNs to be excellent at extracting features,
    used by the following layers for the specific ML task. We can maintain its learned
    ability to extract features by preserving the parameters in those layers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: In other words, parameters in certain layers of existing pre-trained models
    can be preserved and used to detect new objects – we do not need to reinvent the
    wheel. This technique is called **parameter freezing**. In FL, model training
    often takes place in local devices/servers with limited computational power. One
    example using the Gboard scenario is performing parameter freezing on a pretrained
    word embedding layer to allow training to focus on task-specific information,
    leveraging prior training of the embeddings to greatly reduce the trainable parameter
    count.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Taking this concept further, the intersection of FL and TL is called **federated
    transfer learning** (**FTL**). FTL allows for the FL approach to be applied in
    cases where the local datasets differ in structure by performing FL on a shared
    subset of the model that can later be extended for specific tasks. For example,
    a sentiment analysis model and a text summarization model could both share a sentence
    encoding component, which can be trained using FL and used for both tasks. TL
    (and, by extension, FTL) are key concepts that allow for training efficiency and
    incremental improvement to be realized in FL.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Personalization
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When edge devices are dealing with data that is not **independent and identically
    distributed** (**IID**), each device can customize the global model. This is an
    idea called **personalization**, which can be considered as fine-tuning the global
    model with local data, or the strategic use of bias in the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a shopping mall chain that operates in two areas with
    distinct local demographics (that is, the chain deals with non-IID data). If the
    chain seeks tenant recommendations for both locations using FL, each of the locations
    can be better served with personalized models than a single global model, helping
    attract local customers. Since the personalized model is fine-tuned or *biased*
    with local data, we can expect that its performance on general data would not
    be as good as that of the global model. On the other hand, we can also expect
    that the personalized model performs better than the global model on the local
    data for which the model is personalized. There is a trade-off between user-specific
    performance and generalizability, and the power of an FL system comes from its
    flexibility to balance them according to the requirements.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal and vertical FL
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two types of FL: **horizontal** or **homogeneous FL** and **vertical**
    or **heterogeneous FL**. Horizontal FL, also called **sample-based FL**, is applicable
    when all local datasets connected with the aggregator server have the same features
    but contain different samples. The Gboard application discussed earlier is a good
    example of horizontal FL in the form of **cross-device FL**, that is, local training
    taking place in edge devices. The datasets in all Android phones have identical
    formats but unique contents that reflect their user’s typing history. On the other
    hand, vertical FL, or **feature-based FL**, is a more advanced technology that
    allows parties holding different features for the same samples to cooperatively
    generate a global model.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a bank and an e-commerce company might both store the data of
    residents in a city but their features would differ: the former knows the credit
    and expenditure patterns of the citizens, the latter their shopping behavior.
    Both of them can benefit by sharing valuable insights without sharing customer
    data. First, the bank and e-commerce company can identify their common users with
    a technique called **private set intersection** (**PSI**) while preserving data
    privacy using **Rivest-Shamir-Adleman** (**RSA**) encryption. Next, each party
    trains a preliminary model with local data containing unique features. Those models
    are then aggregated to construct a global model. Usually, vertical FL involves
    multiple data silos, and when that is the case, it is also called **cross-silo
    FL**. In China, **federated Ai ecosystem** (**FATE**) is well known for its seminal
    demonstration of vertical FL involving WeBank. If you are interested in further
    conceptual details of FL, there is a very illustrative and well-written report
    by Cloudera Fast Forward Labs, at https://federated.fastforwardlabs.com/.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The information on FL contained in this section should be sufficient to understand
    the following chapters, which examine, in further depth, some of the key concepts
    introduced here. The final section of the chapter aims to cover some of the auxiliary
    concepts focused on the practical application of FL.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: FL system considerations
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section mainly focuses on the multi-party computation aspects of FL, including
    theoretical security measures and full decentralization approaches. The goal of
    this section is for you to be aware of some of the more practical considerations
    that should be taken into account for practical FL applications.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Security for FL systems
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the nascency of the technology, experimental usage of FL has emerged
    in a few sectors. Specifically, **anti-money laundering** (**AML**) in the financial
    industry and drug discovery and diagnosis in the medical industry have seen promising
    results, as proofs of concepts in those fields have been successfully conducted
    by companies such as Consilient and Owkin. In AML use cases, banks can cooperate
    with one another to identify fraudulent transactions efficiently without sharing
    their account data; and hospitals can keep their patient data to themselves while
    improving ML models for detecting health issues.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: These solutions exploit the power of relatively simple horizontal cross-silo
    FL, as explained in the *Understanding FL* section, and its application is spreading
    to other areas. For example, Edgify is a UK-based company contributing to the
    automation of cashiers at retail stores in collaboration with Intel and Hewlett
    Packard. In Munich, Germany, another UK-based company, Fetch.ai, is developing
    a smart city infrastructure with their FL-based technology. It is clear that the
    practical application of FL is rapidly growing.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Although FL can circumvent the concern over data privacy thanks to its privacy-by-design
    (model parameters do not expose privacy) and data minimalist (data is not collected
    in the central server) approach, as discussed in [*Chapter 1*](B18369_01.xhtml#_idTextAnchor017),
    *Challenges in Big Data and Traditional AI*, there are potential obstructions
    against its implementation; one such example is *mistrust* among the participants
    of an FL project. Consider a situation where *Bank A* and *Bank B* agree to use
    FL for developing a collaborative AML solution. They decide on the common model
    architecture so that each can train a local model with their own data and aggregate
    the results to create a global model to be used by both.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Naïve implementations of FL might allow for one bank to reconstruct the local
    model from the other bank, using their local model and the aggregate model. From
    this, the bank might be able to extract key information on the data used to train
    the other bank’s model. As a result, there might be a dispute regarding which
    party should host the server to aggregate the local models. A possible solution
    is having a third party host the server and take responsibility for model aggregation.
    Yet, how would *Bank A* know that the third party is not colluding with *Bank
    B*, and vice versa? Going further, the integration of an FL system into a security-focused
    domain leads to new concerns regarding the security and stability of each system
    component. Known security issues tied to different FL system approaches might
    incur an additional potential weakness to adversarial attacks that outweighs the
    benefits of the approach.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: There are several security measures to allow FL collaboration without forcing
    the participants to trust one another. With a statistical method called **differential
    privacy** (**DP**), each participant can add random noise to their local model
    parameters to prevent the ability to glean information on the training data distribution
    or specific elements from the transmitted parameters. By sampling the random noise
    from a symmetric distribution with zero mean and relatively low variance (for
    example, Gaussian, Laplace), the random differences added to the local models
    are expected to cancel out when aggregation is performed. As a result, the global
    model is expected to be very similar to what would have been generated without
    DP.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a critical limitation to this approach; for the sum of the
    added random noise to converge to zero, a sufficient number of parties must participate
    in the coalition. This might not be the case for projects involving only a few
    banks or hospitals, and using DP in such cases would harm the global model’s integrity.
    Some additional measures would be necessary, for example, each participant sending
    multiple copies of their local model to increase the number of models so that
    the noise will be offset. Another possibility in certain fully-decentralized FL
    systems is **secure multi-party computation** (**MPC**). MPC-based aggregation
    allows agents to communicate among themselves and compute the aggregate model
    without involving a trusted third-party server, maintaining model parameter privacy.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'How could the participants secure the system from outside attacks? **Homomorphic
    encryption** (**HE**), which preserves the effects of addition and multiplication
    on data across encryption, allows the local models to be aggregated into the global
    model in an encrypted form. This precludes the exposure of model parameters to
    outsiders who do not possess the key for decryption. Yet, HE’s effectiveness in
    securing the communication between the participants comes with a prohibitively
    high computational cost: processing the operation on data with the HE algorithm
    can take hundreds of trillions of times longer than otherwise!'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: A solution to mitigate this challenge is the use of partial HE, which is compatible
    with only one of the additive or multiplicative operations across encryption;
    therefore it is computationally much lighter than the fully homomorphic counterpart.
    Using this scheme, each participant in a coalition can encrypt and send their
    local model to the aggregator, which then sums up all local models and sends the
    aggregated model back to the participants, who, in turn, decrypt the model and
    divide its parameters by the number of participants to receive the global model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Both HE and DP are essential technology for the practical application of FL.
    Those interested in the implementation of FL in real-world scenarios can learn
    a great deal from *Federated AI for Real-World Business Scenarios* written by
    IBM Research Fellow Dinesh C. Verma.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Decentralized FL and blockchain
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The architecture of FL discussed so far is based on client-server networks,
    that is, edge devices exchanging models with a central aggregator server. Due
    to the issues surrounding trust between the participants of FL coalitions discussed
    earlier; however, building a system with an aggregator as a separate and central
    entity can be problematic. It can be difficult for the host of an aggregator to
    be impartial and unbiased toward their own data. Also, having a central server
    inevitably leads to a single point of failure in the FL system, which results
    in low resilience. Furthermore, if the aggregator is set up in a cloud server,
    the implementation of such an FL system would require a skilled DevOps engineer,
    who might be difficult to find and expensive to hire.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Given these concerns, Kiyoshi Nakayama (the primary author of this book) co-authored
    an article about the first-ever experimentation of a fully decentralized FL system
    using blockchain technology ([http://www.kiyoshi-nakayama.com/publications/BAFFLE.pdf](http://www.kiyoshi-nakayama.com/publications/BAFFLE.pdf)).
    Leveraging **smart contracts** to coordinate model updates and aggregation, a
    private Ethereum network was constructed to perform FL in a serverless manner.
    The results of the experiment showed that a peer-to-peer, decentralized FL can
    be much more efficient and scalable than an aggregator-based, centralized FL.
    The superiority of decentralized architecture was confirmed in a more recent experiment
    conducted by Hewlett Packard and German research institutes who gave a unique
    name to decentralized FL with blockchain technology: **swarm learning**.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: While research and development in the field of FL are shifting to a decentralized
    model, the rest of this book assumes centralized architecture with an aggregator
    server. There are two reasons for this design. First, blockchain is still a nascent
    technology that AI and ML researchers are not necessarily familiar with. Incorporating
    a peer-to-peer communication scheme can overcomplicate the subject matter. And
    second, the logic of FL itself is independent of the network architecture, and
    there is no problem with the centralized model to illustrate how FL works.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the two key developments that have resulted from
    the recent growth in accessible computational power at all levels. First, we looked
    at the importance of models and how this has enabled ML to grow considerably in
    practical usage, with increases in computational power allowing stronger models
    that surpass manually created white-box systems to continuously be produced. We
    called this the *what* of FL – ML is what we are trying to perform using FL.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Then, we took a step back to look at how edge devices are reaching a stage where
    complex computations can be performed within reasonable timeframes for real-world
    applications, such as the text recommendation models on our phones. We called
    this the *where* of FL – the setting where we want to perform ML.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: From the what and the where, we get the intersection of these two developments
    – the usage of ML models directly on edge devices. Remember that the standard
    central training approach for ML models greatly suffers from the need to centrally
    collect all of the data in the edge ML case, as this prevents applications requiring
    efficient communication or data privacy from being possible. We showed that *FL*
    directly addresses this problem by performing all training at the edge to produce
    *local models*, at the same location as the requisite data stores. *Aggregation*
    algorithms take these local models and produce a *global model*. By iteratively
    switching between local training and aggregation, FL allows for the creation of
    a model that has effectively been trained across all data stores without ever
    needing to centrally collect the data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: We concluded the chapter by stepping outside the theory behind effective aggregation,
    looking at system and architecture design considerations regarding aspects such
    as model privacy and full decentralization. After reading the chapter, it should
    be clear that the current state of ML, edge computing, and fledgling growth in
    practical FL applications makes it clear that FL is poised for serious growth
    in the near future.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will examine the implementation of FL from a system-level
    perspective.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, please take
    a look at the following references:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Ruiz, N. W. (2020, December 22). *The Future is Federated*. Medium.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contessa, G. (2010). *Scientific Models and Fictional Objects*. Synthese, 172(2):
    215–229.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frigg, R. and Hartmann, S. (2020). *Models in Science*. In Zalta, E. N. (ed.).
    The Stanford Encyclopedia of Philosophy.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bunge, M. (1963). *A General Black Box Theory. Philosophy of Science*, 30(4):
    346-358.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moore, S. K., Schneider, D. and Strickland, E. (2021, September 28). *How Deep
    Learning Works: Inside the neural networks that power today’s AI*. IEEE Spectrum.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raschka, S. and Mirjalili, V. (2019). *Python Machine Learning: Machine Learning
    and Deep Learning with Python, scikit-learn, and TensorFlow 2 (3rd Ed.)*. Birmingham:
    Packt Publishing.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. (2016).
    *Communication-efficient learning of deep networks from decentralized data*. JMLR
    WandCP, 54.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang, X., Ding, Y., Jiang, Z. L. and Qi, S. (2020). *DP-FL: a novel differentially
    private federated learning framework for the unbalanced data*. World Wide Web,
    23: 2529–254.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang, Q., Liu, Y., Chen, T., and Tong, Y. (2019). *Federated Machine Learning:
    Concept and Applications. ACM Transactions on Intelligent Systems and Technology
    (TIST)*, 10(2), 1-19.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Verma, D. C. (2021). *Federated AI for Real-World Business Scenarios*. Florida:
    CRC Press.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramanan, P., and Nakayama, K. (2020). *Baffle: Blockchain Based Aggregator
    Free Federated Learning*. 2020 IEEE International Conference on Blockchain, 72-81.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warnat-Herresthal, S., Schultze, H., Shastry, K. L., Manamohan, S., Mukherjee,
    S., Garg, V., and Schultze, J. L. (2021). *Swarm Learning for decentralized and
    confidential clinical machine learning. Nature*, 594(7862), 265-270.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
