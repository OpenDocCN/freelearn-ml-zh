["```py\nJupyter==1.0.0\nnetworkx==2.5\nmatplotlib==3.2.2\nnode2vec==0.3.3\nkarateclub==1.0.19\nscikit-learn==0.24.0\npandas==1.1.3\nnumpy==1.19.2\ntensorflow==2.4.1\nneural-structured-learning==1.3.1\nstellargraph==1.2.1\n```", "```py\n    from stellargraph import datasets\n    from IPython.display import display, HTML\n    dataset = datasets.PROTEINS()\n    graphs, graph_labels = dataset.load()\n    ```", "```py\n    # convert from StellarGraph format to numpy adj matrices\n    adjs = [graph.to_adjacency_matrix().A for graph in graphs]\n    # convert labels from Pandas.Series to numpy array\n    labels = graph_labels.to_numpy(dtype=int)\n    ```", "```py\n    import numpy as np\n    import networkx as nx\n    metrics = []\n    for adj in adjs:\n      G = nx.from_numpy_matrix(adj)\n      # basic properties\n      num_edges = G.number_of_edges()\n      # clustering measures\n      cc = nx.average_clustering(G)\n      # measure of efficiency\n      eff = nx.global_efficiency(G)\n      metrics.append([num_edges, cc, eff]) \n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(metrics, labels, test_size=0.3, random_state=42)\n    ```", "```py\n    from sklearn import svm\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n    clf = svm.SVC()\n    clf.fit(X_train, y_train)\n     y_pred = clf.predict(X_test)\n    print('Accuracy', accuracy_score(y_test,y_pred))\n     print('Precision', precision_score(y_test,y_pred))\n     print('Recall', recall_score(y_test,y_pred))\n     print('F1-score', f1_score(y_test,y_pred))\n    ```", "```py\n    Accuracy 0.7455\n    Precision 0.7709\n    Recall 0.8413\n    F1-score 0.8045\n    ```", "```py\nclass GraphLabelPropagation(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):\n\n     def fit(self, X, y):\n        X, y = self._validate_data(X, y)\n        self.X_ = X\n        check_classification_targets(y)\n        D = [X.degree(n) for n in X.nodes()]\n        D = np.diag(D)\n        # label construction\n        # construct a categorical distribution for classification only\n       unlabeled_index = np.where(y==-1)[0]\n       labeled_index = np.where(y!=-1)[0]\n       unique_classes = np.unique(y[labeled_index])\n       self.classes_ = unique_classes\n       Y0 = np.array([self.build_label(y[x], len(unique_classes)) if x in labeled_index else np.zeros(len(unique_classes)) for x in range(len(y))])\n\n       A = inv(D)*nx.to_numpy_matrix(G)\n       Y_prev = Y0\n       it = 0\n       c_tool = 10\n       while it < self.max_iter & c_tool > self.tol:\n           Y = A*Y_prev\n           #force labeled nodes\n           Y[labeled_index] = Y0[labeled_index]\n           it +=1\n           c_tol = np.sum(np.abs(Y-Y_prev))\n           Y_prev = Y\n       self.label_distributions_ = Y\n       return self\n```", "```py\nglp = GraphLabelPropagation()\ny = np.array([-1 for x in range(len(G.nodes()))])\ny[0] = 0\ny[6] = 1\nglp.fit(G,y)\n glp.predict_proba(G)\n```", "```py\nfrom scipy.linalg import fractional_matrix_power\nD_inv = fractional_matrix_power(D, -0.5)\n L = D_inv*nx.to_numpy_matrix(G)*D_inv\n```", "```py\nclass GraphLabelSpreading(GraphLabelPropagation):\n    def fit(self, X, y):\n        X, y = self._validate_data(X, y)\n        self.X_ = X\n        check_classification_targets(y)\n        D = [X.degree(n) for n in X.nodes()]\n        D = np.diag(D)\n        D_inv = np.matrix(fractional_matrix_power(D,-0.5))\n        L = D_inv*nx.to_numpy_matrix(G)*D_inv\n        # label construction\n        # construct a categorical distribution for classification only\n        labeled_index = np.where(y!=-1)[0]\n        unique_classes = np.unique(y[labeled_index])\n        self.classes_ = unique_classes\n         Y0 = np.array([self.build_label(y[x], len(unique_classes)) if x in labeled_index else np.zeros(len(unique_classes)) for x in range(len(y))])\n\n        Y_prev = Y0\n        it = 0\n        c_tool = 10\n        while it < self.max_iter & c_tool > self.tol:\n           Y = (self.alpha*(L*Y_prev))+((1-self.alpha)*Y0)\n            it +=1\n            c_tol = np.sum(np.abs(Y-Y_prev))\n            Y_prev = Y\n        self.label_distributions_ = Y\n        return self\n```", "```py\ngls = GraphLabelSpreading()\ny = np.array([-1 for x in range(len(G.nodes()))])\ny[0] = 0\ny[6] = 1\ngls.fit(G,y)\n gls.predict_proba(G)\n```", "```py\nfrom stellargraph import datasets\ndataset = datasets.Cora()\ndataset.download()\nG, labels = dataset.load()\n```", "```py\n    ['Neural_Networks', 'Rule_Learning', 'Reinforcement_Learning', \n    'Probabilistic_Methods', 'Theory', 'Genetic_Algorithms', 'Case_Based']\n    ```", "```py\nadjMatrix = pd.DataFrame.sparse.from_spmatrix(\n        G.to_adjacency_matrix(), \n        index=G.nodes(), columns=G.nodes()\n)\nfeatures = pd.DataFrame(G.node_features(), index=G.nodes())\n```", "```py\ndef getNeighbors(idx, adjMatrix, topn=5):\n    weights = adjMatrix.loc[idx]\n    neighbors = weights[weights>0]\\\n         .sort_values(ascending=False)\\\n         .head(topn)\n    return [(k, v) for k, v in neighbors.iteritems()]\n```", "```py\ndataset = {\n    index: {\n        \"id\": index,\n        \"words\": [float(x) \n                  for x in features.loc[index].values], \n        \"label\": label_index[label],\n        \"neighbors\": getNeighbors(index, adjMatrix, topn)\n    }\n    for index, label in labels.items()\n}\ndf = pd.DataFrame.from_dict(dataset, orient=\"index\")\n```", "```py\ndef getFeatureOrDefault(ith, row):\n    try:\n        nodeId, value = row[\"neighbors\"][ith]\n        return {\n            f\"{GRAPH_PREFIX}_{ith}_weight\": value,\n            f\"{GRAPH_PREFIX}_{ith}_words\": df.loc[nodeId][\"words\"]\n        } \n     except:\n        return {\n            f\"{GRAPH_PREFIX}_{ith}_weight\": 0.0,\n            f\"{GRAPH_PREFIX}_{ith}_words\": [float(x) for x in np.zeros(1433)]\n        } \ndef neighborsFeatures(row):\n    featureList = [getFeatureOrDefault(ith, row) for ith in range(topn)]\n    return pd.Series(\n        {k: v \n         for feat in featureList for k, v in feat.items()}\n    )\n```", "```py\nneighbors = df.apply(neighborsFeatures, axis=1)\nallFeatures = pd.concat([df, neighbors], axis=1)\n```", "```py\nn = int(np.round(len(labels)*ratio))  \nlabelled, unlabelled = model_selection.train_test_split(\n    allFeatures, train_size=n, test_size=None, stratify=labels\n)\n```", "```py\ntrain_base = {\n    \"words\": tf.constant([\n         tuple(x) for x in labelled[\"words\"].values\n    ]),\n    \"label\": tf.constant([\n         x for x in labelled[\"label\"].values\n    ])\n }\ntrain_neighbor_words = {\n    k: tf.constant([tuple(x) for x in labelled[k].values])\n    for k in neighbors if \"words\" in k\n}\ntrain_neighbor_weights = {\n^    k: tf.constant([tuple([x]) for x in labelled[k].values])\n    for k in neighbors if \"weight\" in k\n} \n```", "```py\ntrainSet = tf.data.Dataset.from_tensor_slices({\n    k: v\n    for feature in [train_base, train_neighbor_words,\n                    train_neighbor_weights]\n    for k, v in feature.items()\n})\n```", "```py\nvalidSet = tf.data.Dataset.from_tensor_slices({\n    \"words\": tf.constant([\n       tuple(x) for x in unlabelled[\"words\"].values\n    ]),\n    \"label\": tf.constant([\n       x for x in unlabelled[\"label\"].values\n    ])\n })\n```", "```py\ndef split(features):\n    labels=features.pop(\"label\")\n    return features, labels\ntrainSet = trainSet.map(f)\n validSet = validSet.map(f)\n```", "```py\nfor features, labels in trainSet.batch(2).take(1):\n    print(features)\n    print(labels)\n```", "```py\ninputs = tf.keras.Input(\n    shape=(vocabularySize,), dtype='float32', name='words'\n)\ncur_layer = inputs\nfor num_units in [50, 50]:\n    cur_layer = tf.keras.layers.Dense(\n        num_units, activation='relu'\n    )(cur_layer)\n    cur_layer = tf.keras.layers.Dropout(0.8)(cur_layer)\noutputs = tf.keras.layers.Dense(\n    len(label_index), activation='softmax',\n    name=\"label\"\n)(cur_layer)\nmodel = tf.keras.Model(inputs, outputs=outputs)\n```", "```py\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n```", "```py\nfrom tensorflow.keras.callbacks import TensorBoard\nmodel.fit(\n    trainSet.batch(128), epochs=200, verbose=1,\n    validation_data=validSet.batch(128),\n    callbacks=[TensorBoard(log_dir='/tmp/base)]\n)\n```", "```py\nEpoch 200/200\nloss: 0.7798 – accuracy: 06795 – val_loss: 1.5948 – val_accuracy: 0.5873\n```", "```py\nimport neural_structured_learning as nsl\ngraph_reg_config = nsl.configs.make_graph_reg_config(\n    max_neighbors=2,\n    multiplier=0.1,\n    distance_type=nsl.configs.DistanceType.L2,\n    sum_over_axis=-1)\ngraph_reg= nsl.keras.GraphRegularization(\n     model, graph_reg_config)\n```", "```py\ngraph_reg.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',    metrics=['accuracy']\n)\nmodel.fit(\n    trainSet.batch(128), epochs=200, verbose=1,\n    validation_data=validSet.batch(128),\n    callbacks=[TensorBoard(log_dir='/tmp/nsl)]\n)\n```", "```py\nEpoch 200/200\nloss: 0.9136 – accuracy: 06405 – scaled_graph_loss: 0.0328 - val_loss: 1.2526 – val_accuracy: 0.6320\n```", "```py\nimport pandas as pd\nfrom stellargraph import datasets\ndataset = datasets.PROTEINS()\ngraphs, graph_labels = dataset.load()\n# necessary for converting default string labels to int\nlabels = pd.get_dummies(graph_labels, drop_first=True)\n```", "```py\n    from stellargraph.mapper import PaddedGraphGenerator\n    generator = PaddedGraphGenerator(graphs=graphs)\n    ```", "```py\n    from stellargraph.layer import DeepGraphCNN\n    from tensorflow.keras import Model\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n    from tensorflow.keras.losses import binary_crossentropy\n    import tensorflow as tf\n    nrows = 35  # the number of rows for the output tensor\n    layer_dims = [32, 32, 32, 1]\n    # backbone part of the model (Encoder)\n     dgcnn_model = DeepGraphCNN(\n        layer_sizes=layer_dims,\n        activations=[\"tanh\", \"tanh\", \"tanh\", \"tanh\"],\n        k=nrows,\n        bias=False,\n        generator=generator,\n    )\n    ```", "```py\n    # necessary for connecting the backbone to the head\n    gnn_inp, gnn_out = dgcnn_model.in_out_tensors()\n    # head part of the model (classification)\n     x_out = Conv1D(filters=16, kernel_size=sum(layer_dims), strides=sum(layer_dims))(gnn_out)\n    x_out = MaxPool1D(pool_size=2)(x_out)\n     x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)\n    x_out = Flatten()(x_out)\n     x_out = Dense(units=128, activation=\"relu\")(x_out)\n     x_out = Dropout(rate=0.5)(x_out)\n    predictions = Dense(units=1, activation=\"sigmoid\")(x_out)\n    ```", "```py\n    model = Model(inputs=gnn_inp, outputs=predictions)\n    model.compile(optimizer=Adam(lr=0.0001), loss=binary_crossentropy, metrics=[\"acc\"])\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    train_graphs, test_graphs = train_test_split(\n    graph_labels, test_size=.3, stratify=labels,)\n    gen = PaddedGraphGenerator(graphs=graphs)\n    train_gen = gen.flow(\n        list(train_graphs.index - 1),\n        targets=train_graphs.values,\n        symmetric_normalization=False,\n        batch_size=50,\n    )\n    test_gen = gen.flow(\n        list(test_graphs.index - 1),\n        targets=test_graphs.values,\n        symmetric_normalization=False,\n        batch_size=1,\n    )\n    ```", "```py\n    epochs = 100\n    history = model.fit(train_gen, epochs=epochs, verbose=1,\n     validation_data=test_gen, shuffle=True,)\n    ```", "```py\n    Epoch 100/100\n    loss: 0.5121 – acc: 0.7636 – val_loss: 0.5636 – val_acc: 0.7305\n    ```", "```py\ndataset = datasets.Cora()\nG, nodes = dataset.load()\n```", "```py\n    train_nodes, test_nodes = train_test_split(nodes, train_size=0.1,test_size=None, stratify=nodes)\n    ```", "```py\n    from sklearn import preprocessing\n    label_encoding = preprocessing.LabelBinarizer()\n    train_labels = label_encoding.fit_transform(train_nodes)\n     test_labels = label_encoding.transform(test_nodes)\n    ```", "```py\n    from stellargraph.mapper import GraphSAGENodeGenerator\n    batchsize = 50\n    n_samples = [10, 5, 7]\n     generator = GraphSAGENodeGenerator(G, batchsize, n_samples)\n    train_gen = generator.flow(train_nodes.index, train_labels, shuffle=True)\n     test_gen = generator.flow(test_labels.index, test_labels)\n    ```", "```py\n    from stellargraph.layer import GraphSAGE\n    from tensorflow.keras.losses import categorical_crossentropy\n    graphsage_model = GraphSAGE(layer_sizes=[32, 32, 16], generator=generator, bias=True, dropout=0.6,)\n    gnn_inp, gnn_out = graphsage_model.in_out_tensors()\n    outputs = Dense(units=train_labels.shape[1], activation=\"softmax\")(gnn_out)\n    # create the model and compile\n    model = Model(inputs=gnn_inp, outputs=outputs)\n    model.compile(optimizer=Adam(lr=0.003), loss=categorical_crossentropy, metrics=[\"acc\"],)\n    ```", "```py\n    model.fit(train_gen, epochs=20, validation_data=test_gen, verbose=2, shuffle=False)\n    ```", "```py\n    Epoch 20/20\n    loss: 0.8252 – acc: 0.8889 – val_loss: 0.9070 – val_acc: 0.8011\n    ```"]