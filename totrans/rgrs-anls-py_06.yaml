- en: Chapter 6. Achieving Generalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have to confess that, until this point, we've delayed the crucial moment
    of truth when our linear model has to be put to the test and verified as effectively
    predicting its target. Up to now, we have just considered whether we were doing
    a good modeling job by naively looking at a series of good-fit measures, all just
    telling us if the linear model could be apt at predicting based solely on the
    information in our training data.
  prefs: []
  type: TYPE_NORMAL
- en: Unless you love sink-or-swim situations, in much the same procedure you'd employ
    with new software before going into production, you need to apply the correct
    tests to your model and to be able to anticipate its live performance.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, no matter your level of skill and experience with such types of models,
    you can easily be misled into thinking you're building a good model just on the
    basis of the same data you used to define it. We will therefore introduce you
    to the fundamental distinction between in-sample and out-of-sample statistics
    and demonstrate how they risk diverging when you use too many predictors, too
    few predictors, or simply just the wrong ones.
  prefs: []
  type: TYPE_NORMAL
- en: Here we are then, ready at last to check whether we have done a good job or
    have to rethink everything from scratch. In this pivotal chapter of the book,
    before proceeding to more complex techniques, we will introduce you to key data
    science recipes to thoroughly test your model, fine-tune it optimally, make it
    economical, and pit it against real, fresh data without any concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you''ll get to know how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Test your models, using the most appropriate cost measure, on a validation/test
    set or using cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the best features on the basis of statistical tests and experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make your model more economical by tweaking the cost function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use stability selection, an almost automated method for variable selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking on out-of-sample data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until this point in the book, we have striven to make the regression model fit
    data, even by modifying the data itself (inputting missing data, removing outliers,
    transforming for non-linearity, or creating new features). By keeping an eye on
    measures such as R-squared, we have tried our best to reduce prediction errors,
    though we have no idea to what extent this was successful.
  prefs: []
  type: TYPE_NORMAL
- en: The problem we face now is that we shouldn't expect a well fit model to automatically
    perform well on any new data during production.
  prefs: []
  type: TYPE_NORMAL
- en: While defining and explaining the problem, we recall what we said about underfitting.
    Since we are working with a linear model, we are actually expecting to apply our
    work to data that has a linear relationship with the response variable. Having
    a linear relationship means that, with respect to the level of the response variable,
    our predictors always tend to constantly increase (or decrease) at the same rate.
    Graphically, on a scatterplot, this is refigured by a straight and very elongated
    cloud of points that could be crossed by a straight regression line with little
    or minimal prediction error.
  prefs: []
  type: TYPE_NORMAL
- en: When the relationship is instead non-linear, the rate of change and direction
    are mutable (alternatively increasing or decreasing). In such a situation, in
    order to have the linear model work better, we will have to try to make the relationship
    straight by opportune transformations. Otherwise we will have to try guessing
    the response by a not-always-successful approximation of a non-linear shape to
    a linear one.
  prefs: []
  type: TYPE_NORMAL
- en: If for instance the relationship is quadratic (so the functional shape is that
    of a parabola), using a line will pose the problem of a systematic underestimation
    or overestimation of the predicted values at certain ranges in the predictor's
    values. This systematic error is called bias and it is typical of simple models
    such as linear regression. A prediction model with a high bias will systematically
    tend to generate erroneous predictions in certain situations. Since inaccuracy
    of predictions is an undesirable characteristic for a tool that should be able
    to provide effective forecasts, we have to strive to achieve a better fit to the
    response by adding new variables and transforming the present ones by polynomial
    expansion or other transformations. Such efforts constitute the so-called **feature
    creation phase**.
  prefs: []
  type: TYPE_NORMAL
- en: By doing so, we may find ourselves in a different but no less problematic situation.
    In fact, when we render our model more and more complex, it will not just better
    fit the response by catching more and more parts of the unknown function that
    ties it to the predictors, but also, by adding more and more terms, we are enabling
    our model to receive that part of the information that is exclusively specific
    to the data at hand (we call this noise), making it more and more unable to work
    properly with different data.
  prefs: []
  type: TYPE_NORMAL
- en: You could think about it as a *power of memorization* so that, the more complex
    the learning algorithm, the more space there will be to fit not-so-useful information
    from the data we are using for learning. This memorization brings very inconvenient
    consequences. Though our model appears to have a good fit on our data, as soon
    as it is applied to a different set, it reveals its inability to predict correctly.
    In such a situation, contrary to before when the errors were systematic (systematic
    under- or over-estimation), errors will appear to be erratic, depending on the
    dataset. This is called variance of the estimates and it could prove more of a
    problem to you because it can leave you unaware of its existence until you test
    it against real data. It tends to strike in more complex algorithms and, in its
    simplest form, linear regression tends to present a higher bias on the estimates
    than variance. Anyway, adding too many terms and interactions or resorting to
    polynomial expansion does expose linear models to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Testing by sample split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we expect the ability to generalize to new data from a model and since
    we are seldom interested in just fitting or simply memorizing the present data,
    we need to take some cautionary steps as we build our model. To fight against
    this problem, the practice of learning from data has defined over the years a
    series of procedures, based on the scientific method of validating and testing,
    that we are going to illustrate and practice ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: First, if we want our model to generalize well on new data, we have to test
    it in such a situation. This means that, if getting new data is not an easy task
    or a feasible one, we should reserve some data for our tests from the beginning.
    We can achieve that by randomly splitting our data into two parts, a training
    set and a test set, using 70–80 percent of the data for the training part and
    the residual 20–30 percent for testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn''s `cross_validation` module offers a series of methods that can
    help us in dealing with all these operations. Let''s try it by operating on our
    usual Boston Housing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After having loaded it, let''s first split it into train and test parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`train_test_split` will separate the data according to the specified quota
    for testing indicated in the `test_size` parameter. The split will be a random
    one, and you can deterministically control the results (for replication purposes)
    using a specific numeric seed in the `random_state` parameter (our choice for
    the seed is `101`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, reserving an out-of-sample (comprising what is not in-sample—that
    is, used as a sample for learning from training activity data) is not enough,
    because we may have to tune some parameters or make specific choices and we want
    to test the alternatives without having to use the test data. The solution is
    to reserve another part of our data for validation purposes, which implies checking
    what parameters could be optimal for our model. We can achieve that using `train_test_split`
    in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Though helpful in measuring the true error of an hypothesis, dividing your
    data into train and test (and sometimes also into validation) sets presents some
    risks that you have to take into account:'
  prefs: []
  type: TYPE_NORMAL
- en: Since it involves sub-sampling (you casually draw out a part of your initial
    sample), you may incur the risk of drawing sets that are too favorable or unfavorable
    for training and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By leaving aside a portion of your sample, you reduce the number of examples
    to learn from, whereas linear models need as many as possible in order to reduce
    the variance of the estimates, disambiguate collinear variables, and properly
    model non-linearity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though we always suggest drawing a small test sample (say 10% of the data) as
    a final check of the validity of your work, the best way to avoid the aforementioned
    problems, and easily manage different comparisons of models and parameters, is
    to apply cross-validation, which requires you to split your data for both training
    and testing but it does so repetitively until every observation has played the
    role of training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, you decide how many mutually exclusive parts to split your data
    into, then you repeatedly keep on training your model using all the folds but
    a different one every time; this plays the role of a test set.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The number of parts you split your data into is usually set to 3, 5, 10, or
    20 and you decide on a large number of splits (each one called a **fold**) when
    you have little training data.
  prefs: []
  type: TYPE_NORMAL
- en: When you have completed the validation, using every single split available as
    the test set, you first take the average of the results, which tells you with
    a good degree of accuracy the overall performance of your model when faced with
    new data (new but not too dissimilar from the one you have at hand). Then you
    also notice the standard deviation of the cross-validated performances. This is
    important because, if there's a high deviation (over half of the average performance
    value), it can indicate that the model has a high variance of the estimates and
    that it needs more data to work well.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, you can look at how `KFold` and `StratifiedKFold`
    (from the Scikit-learn's `cross_validation` module) work.
  prefs: []
  type: TYPE_NORMAL
- en: 'They are both iterators: you draw the indices for training and testing for
    each round of cross validation, with the sole difference that `KFold` just applies
    a random draw. Instead, `StratifiedKFold` takes account of the distribution of
    a target variable that you want distributed in your training and test samples
    as if it were on the original set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As parameters to both classes, you should provide:'
  prefs: []
  type: TYPE_NORMAL
- en: The count of observations to `KFold` and the target vector to `StratifiedKFold`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of folds (10 is usually the standard choice, but you can decrease
    the number of folds if you have many observations, or you can increase it if your
    dataset is small)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You should also decide:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether to shuffle the data or take it as it is (shuffling is always recommended)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether to apply a random seed and make the results replicable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `n_jobs` parameter will set the number of threads involved in the computation
    of the results by leveraging parallel computations. When it is set to `−1` it
    will automatically use all the available threads, speeding up the calculations
    to the maximum on your computer. Anyway, depending on the system you are working
    on, sometimes setting the parameter to something different than `1` will cause
    problems, slowing down the results. In our examples, as a precautionary measure,
    it is always set to `1`, but you can change its value if you need to cut short
    the computational time.
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, we try to get the cross-validation score of an over-parameterized
    model (a second-degree polynomial expansion of the original features of the Boston
    dataset). Please notice that the results are negative (though they are squared
    errors) because of the internals of the automatic function for computing the cross-validation
    of a model, `cross_val_score`, from Scikit-learn. This function requires the model,
    the features, and the target variable as input. It also accepts a cross validation
    iterator of your choice for the parameter `cv`, a string for `scoring` indicating
    the name of the scoring function to be used (more on this can be found at: [http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html));
    and finally the number of threads working in parallel on your PC by specifying
    `n_jobs` (`1` indicates that only one thread is working whereas `−1` indicates
    all the available threads in the system are used):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The mean squared error is negative because of the internals of the function,
    which can only maximize, whereas our cost metric has to be minimized; this is
    why it has become negative
  prefs: []
  type: TYPE_NORMAL
- en: 'After removing the sign, we can take both the average and the standard deviation.
    Here, we can also notice that the standard deviation is high, and maybe we should
    then try to control the distribution of the target variable, since in the real
    estate business there are outlying observations due to very rich residential areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply such a control, we stratify the target variable; that is, we divide
    it into bins and we expect the bin distribution to be kept during the cross-validation
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the end, controlling for the response distribution really lowers the standard
    deviation of the estimated error (and our expected average). A successful stratification
    attempt in cross-validation suggests that we should train on a correctly distributed
    training sample, otherwise we may achieve an outcome model not always working
    properly due to bad sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a final remark on the topic of cross-validation, we suggest using it mostly
    for evaluating parameters, and always relying on a small drawn out test set for
    performance validation. In fact, it is a bit tricky, but if you cross-validate
    too many times (for example changing the seed) looking for the best performance,
    you will end up with the best result, which is another form of overfitting called
    snooping (this also happens if you do the same with the test set). Instead, when
    you use cross-validation to choose between parameters, you just decide on the
    best among the options, not on the absolute cross-validation value.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, if the training data is really small, even dividing into folds can
    penalize how the model is trained. The statistical technique of bootstrapping
    allows repeating the training and testing validation sequence (allowing precise
    estimations of both the mean and standard deviation of expected results) for a
    large number of times by trying to replicate the underlying distribution of the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bootstrapping is based on sampling with repetition, which implies allowing
    an observation to be drawn multiple times. Usually bootstraps draw the number
    of observations equivalent to the original size of the dataset. Also, there''s
    always a part of the observations that it is left untouched, equivalent to a third
    of the available observations, which can be used for validating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be shown as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bootstrapping](img/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As illustrated by the preceding example (unfortunately, this method is not
    part of Scikit-learn, having being recently deprecated), in a set of 10 observations,
    on average four observations are left available for testing purposes. However,
    in a bootstrapping process, it is not just the left out cases that provide insight.
    A model is in fact fitted to the training dataset, and we can also inspect how
    the coefficients are determined in the bootstrap replications, thus allowing us
    to figure out how stable each coefficient is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, the tenth coefficient index (PTRATIO) is quite stable in both
    sign and value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Whereas the sixth coefficient (AGE) has great variability, often even changing
    sign:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In conclusion, bootstrap is a form of replication that can be run as many times
    as you decide, and this allows you to create multiple models and evaluate their
    results in a similar way to a cross-validation procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Greedy selection of features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By following our experiments throughout the book, you may have noticed that
    adding new variables is always a great success in a linear regression model. That's
    especially true for training errors and it happens not just when we insert the
    right variables but also when we place the wrong ones. Puzzlingly, when we add
    redundant or non-useful variables, there is always a more or less positive impact
    on the fit of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The reason is easily explained; since regression models are high-bias models,
    they find it beneficial to augment their complexity by increasing the number of
    coefficients they use. Thus, some of the new coefficients can be used to fit the
    noise and other details present in data. It is precisely the memorization/overfitting
    effect we discussed before. When you have as many coefficients as observations,
    your model can become saturated (that's the technical term used in statistics)
    and you could have a perfect prediction because basically you have a coefficient
    to learn every single response in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make this concept more concrete with a quick example using a training
    set (in-sample observations) and a test set (out-sample observations). Let''s
    start by finding out how many cases and features we have and what the baseline
    performance is (for both in-sample and out-sample):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The best approach would be to use a cross validation or bootstrap for such an
    experiment, not just a plain train/test split, but we want to make it fast, and
    that's the reason why we decided on such a solution. We assure you that using
    more sophisticated estimation techniques doesn't change the results of the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we have similar in-sample and out-sample errors. We can start working
    on improving our model using polynomial expansions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we apply the second-order polynomial expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It seems that the good in-sample results have little correspondence with the
    out-sample test. Though the out-sample performance has improved, the lack of comparability
    in results is a clear sign of overfitting; there are some more useful coefficients
    in the model but most of them are just there to catch noise in data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now go to extremes and we test the third-degree polynomial expansion (using
    only interactions though):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now, clearly something very bad has happened to our model. Having more coefficients
    than observations (`p>n`), we achieved a perfect fit on our training set. However,
    on the out-sample validation, it seems that our model achieved the same performance
    as a random number generator. In the next few paragraphs, we will show you how
    to take advantage of an increased number of features without incurring any of
    the problems demonstrated by the previous code snippets.
  prefs: []
  type: TYPE_NORMAL
- en: The Madelon dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the task of selecting the best subset of variables among many noisy and
    collinear ones, we decided to accompany our usual Boston house dataset with a
    tricky one, the Madelon dataset ([https://archive.ics.uci.edu/ml/datasets/Madelon](https://archive.ics.uci.edu/ml/datasets/Madelon)).
    It is an artificial dataset (that is generated using an algorithm) presented at
    the NIPS 2003 (the seventh Annual Conference on Neural Information Processing
    Systems) during a contest on feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is particularly challenging because it has been generated by placing
    32 distinct clusters of points (16 from the positive group, 16 from the negative
    one) on the vertices of a five-dimension hypercube. The resulting 500 features
    and 2,000 cases have been extracted from various transformations of the five metric
    dimensions. To make things harder, some random numbers have been added to the
    features to act as noise and a few responses have been flipped (the flipped ones
    amount to 1%). All these intricate transformations make dealing with the modeling
    quite difficult, especially for linear models, since the relationship of most
    of the features with the response is definitely non-linear. This is really helpful
    for our exemplification because it clearly demonstrates how a direct inclusion
    of all the features is detrimental to the accuracy of out-of-sample predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download and make available on your computer such an interesting and challenging
    dataset, please carry out the following instructions and allow some time for your
    computer to download the data from the external website where it is stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After finishing loading both the training and validation sets, we can start
    exploring some of the information available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Naturally, we won''t touch the validation set (we won''t even glance at it
    or it would be snooping), but we can try to figure out the situation with the
    training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is quite lengthy and it is put in matrix form (therefore it is not
    reported here), but it really tells us everything about the mean, min, max, variance,
    skewness, and kurtosis for each feature in the dataset. A fast glance through
    it doesn''t reveal anything special; however, it explicits that all the variables
    have an approximately normal distribution and that they have a limited range of
    values. We can proceed with our exploration using a graphical representation of
    correlations among the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Madelon dataset](img/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: After a glance at a portion of the features and their respective correlation,
    we can notice that just a couple of them have a significant correlation, whereas
    the others are mildly related. This gives the impression of noisy relationships
    between them, thus rendering an effective selection quite complicated.
  prefs: []
  type: TYPE_NORMAL
- en: As a last step, we check how a simple logistic regression model would score
    in terms of the error measured using the area under the curve metric.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Area under the curve (AUC) is a measure derived from comparing the rate of correct
    positive results against the rate of incorrect ones at different classification
    thresholds. It is a bit tricky to calculate, so we suggest always relying on the
    `roc_auc_score` function from the `sklearn.metrics` module.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression classifies an observation as positive if the threshold is
    over 0.5 since such a split is always proved to be optimal, but we can freely
    change that threshold. To increase the precision at a top selection of results
    we just raise the threshold from 0.5 to 1.0 (raising the threshold increases the
    accuracy in the selected range). Instead, if we intend to increase the total number
    of guessed positive cases we just choose a threshold inferior to 0.5 down to almost
    0.0 (lowering the threshold increases the coverage of positive cases in the selected
    range).
  prefs: []
  type: TYPE_NORMAL
- en: The AUC error measure helps us determine whether our predictions are ordered
    properly, no matter their effective precision in terms of value. Thus, AUC is
    the ideal error measure to evaluate an algorithm for selection. If you order results
    properly based on probability, no matter if the guessed probability is correct
    or not, you can simply pick the correct selection to be used by your project by
    changing the 0.5 threshold—that is, by taking a certain number of the top results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, the baseline AUC measure is `0.602`, a quite disappointing value
    since a random selection should bring us a `0.5` value (`1.0` is the maximum possible):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Univariate selection of features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Feature selection can help in both increasing the model out-sample performance
    and its human readability by retaining only the most predictive set of variables
    in the model, in some cases just the best ones and in others the set that works
    the best in unison.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few feature selection methods. The simplest approach is the
    univariate method, which evaluates how good a variable is by estimating its predictive
    value when taken alone in respect of the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'This usually involves using statistical tests, and Scikit-learn offers three
    possible tests:'
  prefs: []
  type: TYPE_NORMAL
- en: The `f_regression` class, which works out an F-test (a statistical test for
    comparing different regression solutions) and a p-value (interpretable as the
    probability value in which we observed a difference by chance) and reveals the
    best features for a regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `f_class`, which is an Anova F-test (a statistical test for comparing differences
    among classes), another statistical and related method that will prove useful
    for classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Chi2` class, which is a chi-squared test (a statistical test on count data),
    a good choice when your problem is classification and your answer variable is
    a count or a binary (in every case, a positive number such as units sold or money
    earned)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All such tests output a score and a statistical test expressed by a p-value.
    High scores, confirmed by small p-values (under 0.05, indicating a low probability
    that the score has been obtained by luck), will provide you with confirmation
    that a certain variable is useful for predicting your target.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we will use `f_class` (since we are working on a classification
    problem now) and we will have the `SelectPercentile` function help us by selecting
    a certain percentage of high-scoring features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After selecting the upper half, hoping to have cut off the most irrelevant
    features and to have kept the important ones, we plot our results on an histogram
    to reveal the distribution of the scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Univariate selection of features](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Noticeably, most scores are near zero, with a few high-ranking ones. Now we
    are going to pick the features we assume to be important by directly selecting
    a threshold empirically chosen for its convenience:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have reduced our dataset to just the core features. At this point,
    it does make sense to test a polynomial expansion and try to automatically catch
    any relevant non-linear relationship in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The resulting validation score (out-sample) is about 0.81, a very promising
    value given our initial overfitted score of 0.82 on the training set. Of course,
    we can decide to stop here or try to go on filtering even the polynomial expansion;
    feature selection is really a never-ending job, though after a certain point you
    have to realize that only slightly incremental results are possible from further
    tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Recursive feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The only problem with univariate selection is that it will decide the best features
    by considering each feature separately from the others, not verifying how they
    work together in unison. Consequently, redundant variables are not infrequently
    picked (due to collinearity).
  prefs: []
  type: TYPE_NORMAL
- en: A multivariate approach, such as recursive elimination, can avoid this problem;
    however, it is more computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Recursive elimination works by starting with the full model and by trying to
    exclude each variable in turn, evaluating the removal effect by cross-validation
    estimation. If certain variables have a negligible effect on the model's performance,
    then the elimination algorithm just prunes them. The process stops when any further
    removal is proven to hurt the ability of the model to predict correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a demonstration of how `RFECV`, Scikit-learn''s implementation of recursive
    elimination, works. We will use the Boston dataset enhanced by second-degree polynomial
    expansion, thus working on a regression problem this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Given an estimator (our model), a cross validation iterator, and an error measure,
    `RFECV` will find out after a while that half of the features can be dropped from
    the model without fear of worsening its performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'A test-based check will reveal that now the out-sample performance is 11.5\.
    For further confirmation, we can also run a cross-validation and obtain a similar
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Regularization optimized by grid-search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularization is another way to modify the role of variables in a regression
    model to prevent overfitting and to achieve simpler functional forms. The interesting
    aspect of this alternative approach is that it actually doesn't require manipulating
    your original dataset, making it suitable for systems that learn and predict online
    from large amounts of features and observations, without human intervention. Regularization
    works by enriching the learning process using a penalization for too complex models
    to shrink (or reduce to zero) coefficients relative to variables that are irrelevant
    for your prediction term or are redundant, as they are highly correlated with
    others present in the model (the collinearity problem seen before).
  prefs: []
  type: TYPE_NORMAL
- en: Ridge (L2 regularization)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea behind ridge regression is simple and straightforward: if the problem
    is the presence of many variables, which affect the regression model because of
    their coefficient, all we have to do is reduce their coefficient so their contribution
    is minimized and they do not influence the result so much.'
  prefs: []
  type: TYPE_NORMAL
- en: Such a result is easily achieved by working out a different cost function. Working
    on the error in respect of the answer, the cost function can be balanced by imposing
    a penalization value depending on how large the coefficients are.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following formula, a reprisal of the formula in the [Chapter 2](part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 2. Approaching Simple Linear Regression"), *Approaching Simple Linear
    Regression* paragraph *Gradient descent at work*, the weight update is modified
    by the presence of a negative term, which is the square of the weight reduced
    by a factor expressed by lambda. Consequently, the larger the coefficient, the
    more it will be reduced during the update phase of the gradient descent optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ridge (L2 regularization)](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, each single coefficient `j`, whose value is represented
    by `w[j]`, is updated by the gradient descent learning rate α `/ n`, where `n`
    is the number of observations. The learning rate is multiplied by the summed deviance
    of the prediction (the gradient). The novelty is the presence in the gradient
    of a penalization, calculated as the squared coefficient multiplied by a `λ` lambda
    coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the error will be propagated to the coefficients only if there
    is an advantage (a large deviance in predictions), otherwise the coefficients
    will be reduced in value. The advantage is controlled by the `λ` lambda value,
    which has to be found empirically according to the specific model that we are
    building.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example will clarify how this new approach works. First, we have to use
    the `Ridge` class from Scikit-learn, if our problem is a regression, or we use
    the penalty parameter in the `LogisticRegression` specification (`LogisticRegression(C=1.0,
    penalty=''l2'', tol=0.01)`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The impact of regularization on the model is controlled by the `alpha` parameter
    in the `Ridge`, and by the `C` parameter in `LogisticRegression`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The smaller the value of `alpha`, the less the coefficient values are controlled
    by the regularization, the higher its value with increased regularization, the
    more the coefficients are shrunk. Its functioning can be easily memorized as a
    shrinkage parameter: the higher the value, the higher the shrinkage of the complexity
    of the model. However, the C parameter in `LogisticRegression` is exactly the
    inverse, with smaller values corresponding to high regularization (*alpha = 1
    / 2C*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After having completely fitted the model, we can have a look at how the values
    of coefficients are defined now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now, the average coefficient value is almost near zero and the values are placed
    in a much shorter range than before. In the regularized form, no single coefficient
    has the weight to influence or, worse, disrupt a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Grid search for optimal parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, we haven't had much to decide about the model itself, no matter whether
    we decided on a logistic or a linear regression. All that mattered was to properly
    transform our variables (and actually, we have learned that this is not an easy
    task either); however, the introduction of the L2 parameter brings forth much
    more complexity since we also have to heuristically set a value to maximize the
    performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping on working with cross-validation, which ensures we evaluate the performance
    of our model in a realistic way, a good solution to this problem is to check systematically
    the result of our model given a range of possible values of our parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `GridSearchCV` class in the Scikit-learn package can be set using our preferred
    `cv` iterator and scoring after setting a dictionary explicating what parameters
    have to be changed in the model (the key) and a range of values to be evaluated
    (a list of values related to the key), finally assigning it to the `param_grid`
    parameter of the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the search, which can take some time when there are many possible
    model variations to test, can be explored using the attribute `grid_scores_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The maximum scoring value (actually using RMSE we should minimize the result,
    so the grid search works with the negative value of RMSE) is achieved when alpha
    is `0.001`. In addition, the standard deviation of the cross-validation score
    is minimal in respect of our possible solutions, confirming to us that it is the
    best solution available at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you would like to further optimize the results, just explore, using a second
    grid search, the range of values around the winning solution—that is, in our specific
    case from `0.0001` to `0.01`, you may find a slightly better value in terms of
    expected results or stability of the solution (expressed by the standard deviation).
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, `GridSearchCV` can be used effectively when more parameters to be
    optimized are involved. Please be aware that the more parameters, the more trials
    have to be made, and the resulting number is a combination—that is, a multiplication—of
    all the possible values to be tested. Consequently, if you are testing four values
    of a hypermeter and four of another one, in the end you will need *4 × 4* trials
    and, depending on the cross-validation folds, let's say in our case 10, you'll
    have your CPU compute *4 × 4 × 10 = 160* models. Searches that are more complex
    may even involve testing thousands of models, and although `GridSearchCV` can
    parallelize all its computations, in certain cases it can still be a problem.
    We are going to address a possible solution in the next paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have illustrated how to grid-search using the more general `GridSearchCV`.
    There is anyway a specialized function for automatically creating out-of-the-box
    a cross-validated optimized ridge regression using Scikit-learn: `RidgeCV`. There
    are automated classes also for the other regularization variants we are going
    to illustrate, `LassoCV` and `ElasticNetCV`. Actually, these classes, apart from
    being more synthetic than the approach we described, are much faster in finding
    the best parameter because they follow an optimization path (so they actually
    do not exhaustively search along the grid).'
  prefs: []
  type: TYPE_NORMAL
- en: Random grid search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Searching for good combinations of hyper-parameters in a grid is a really time-consuming
    task, especially if there are many parameters; the number of combinations can
    really explode and thus your CPU can take a long time to compute the results.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it is often the case that not all hyper-parameters are important;
    in such a case, when grid-searching, you are really wasting time checking on a
    large number of solutions that aren't really distinguishable from one another,
    while instead omitting to check important values on critical parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The solution is a random grid search, which is not only much speedier than the
    grid search, but it is also much more efficient, as pointed out in a paper by
    the scholars James Bergstra and Yoshua Bengio ([http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: Random search works by sampling possible parameters from ranges or distribution
    that you point out (the `NumPy` package has quite a lot of distributions that
    can be used, but for this test we found that `logspace` function is ideal for
    systematically exploring the L1/L2 range). Given a certain number of trials, there
    is a high chance that you can get the right hyper-parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we try using just `10` values sampled from `100` possible ones (so reducing
    our running time to `1/10` in respect of a grid search):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a heuristic, the number of trials of a random search depends on the number
    of possible combinations that may be tried under a grid search. As a matter of
    statistical probability, it has been empirically observed that the most efficient
    number of random trials should be between 30 and 60\. More than 60 random trials
    is unlikely to bring many more performance improvements from tuning hyper parameters
    than previously assessed.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso (L1 regularization)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ridge regression is not really a selection method. Penalizing the useless coefficients
    through keeping them all in the model won't provide much clarity about what variables
    work the best in your linear regression and won't improve its comprehensibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'The lasso regularization, a recent addition by Rob Tibshirani, using the absolute
    value instead of the quadratic one in the regularization penalization, does help
    to shrink many coefficient values to zero, thus making your vector of resulting
    coefficients sparse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Lasso (L1 regularization)](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Again, we have a formula similar to the previous one for L2 regularization but
    now the penalization term is made up of `λ` lambda multiplied by the absolute
    value of the coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'The procedure is the same as in the ridge regression; you just have to use
    a different class called `Lasso`. If instead your problem is a classification
    one, in your logistic regression you just have to specify that the parameter `penalty`
    is `''l1''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check what happens to the previously seen regularization of the linear
    regression on the Boston dataset when using `Lasso`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: From the viewpoint of performance, we obtained a slightly worse but comparable
    mean squared error value.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You will have noticed that using the `Lasso` regularization takes more time
    (there are usually more iterations) than applying the ridge one. A good strategy
    for speeding up things is to apply the lasso only on a subset of the data (which
    should take less time), find out the best alpha, and then apply it directly to
    your complete sample to verify whether the performance results are consistent.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, what is most interesting is evaluating what coefficients have been
    reduced to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Now, our second-degree polynomial expansion has been reduced to just `20` working
    variables, as if the model has been reduced by a recursive selection, with the
    advantage that you don't have to change the dataset structure; you just apply
    your data to the model and only the right variables will work out the prediction
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are wondering what kind of regularization to use first, `ridge` or `lasso`,
    a good rule of thumb is to first run a linear regression without any regularization
    and check the distribution of the standardized coefficients. If there are many
    with similar values, then `ridge` is the best choice; if instead you notice that
    there are a few important coefficients and many lesser ones, using `lasso` is
    advisable to remove the unimportant ones. In any case, when you have more variables
    than observations, you should always use `lasso`.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic net
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lasso can rapidly and without much hassle reduce the number of working variables
    in a prediction model, rendering it simpler and much more generalizable. Its strategy
    is simple: it aims to retain only the variables that contribute to the solution.
    Consequently, if, by chance, among your features you have a couple of strongly
    collinear variables, an L1 regularization will keep just one of them, on the basis
    of the characteristics of the data itself (noise and correlation with other variables
    contribute to the choice).'
  prefs: []
  type: TYPE_NORMAL
- en: Such a characteristic anyway may prove undesirable because of the instability
    of the L1 solution (the noise and strength of correlations may change with the
    data) since having all the correlated variables in the model guarantees a more
    reliable model (especially if they all depend on a factor that is not included
    into the model). Thus, the alternative elastic net approach has been devised by
    combining the effects of L1 and L2 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: In elastic net (Scikit-learn's `ElasticNet` class), you always have an `alpha`
    parameter that controls the impact of regularization on the determination of the
    model's coefficients, plus a `l1_ratio` parameter that helps weight the combination
    between the L1 and L2 parts of the regularization part of the cost function. When
    the parameter is `0.0`, there is no role for L1 so it is equivalent to a ridge.
    When it is set to `1.0`, you have a lasso regression. Intermediate values act
    by mixing the effects of both types of regularizations; thus, while some variables
    will still be reduced to zero value coefficients, collinear variables will be
    reduced to the same coefficient, allowing them all to be still present in the
    model formulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we try solving our model with elastic net regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'By introspecting the solution, we realize that this is achieved by excluding
    a larger number of variables than a pure L1 solution; however, the resulting performance
    is similar to a L2 solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Stability selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As presented, L1-penalty offers the advantage of rendering your coefficients'
    estimates sparse, and effectively it acts as a variable selector since it tends
    to leave only essential variables in the model. On the other hand, the selection
    itself tends to be unstable when data changes and it requires a certain effort
    to correctly tune the C parameter to make the selection most effective. As we
    have seen while discussing elastic net, the peculiarity resides in the behavior
    of Lasso when there are two highly correlated variables; depending on the structure
    of the data (noise and correlation with other variables), L1 regularization will
    choose just one of the two.
  prefs: []
  type: TYPE_NORMAL
- en: In the field of studies related to bioinformatics (DNA, molecular studies),
    it is common to work with a large number of variables based on a few observations.
    Typically, such problems are denominated p >> n (features are much more numerous
    than cases) and they present the necessity to select what features to use for
    modeling. Because the variables are numerous and also are quite correlated among
    themselves, resorting to variable selection, whether by greedy selection or L1-penalty,
    can lead to more than one outcome taken from quite a large range of possible solutions.
    Two scholars, Nicolai Meinshausen and Peter Buhlmann, respectively from the University
    of Oxford and ETH Zurich, have come up with the idea of trying to leverage this
    instability and turn it into a surer selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Their idea is straightforward: since L1-penalty is influenced by the cases
    and variables present in the dataset to choose a certain variable over others
    in the case of multicollinearity, we can subsample the cases and the variables
    to involve and fit with them a L1-penalized model repetitively. Then, for each
    run, we can record the features that got a zero coefficient and the one that didn''t.
    By pooling these multiple results, we can calculate a frequency statistic of how
    many times each feature got a non-zero value. In such a fashion, even if the results
    are unstable and uncertain, the most informative features will score a non-zero
    coefficient more often than less informative ones. In the end, a threshold can
    help to exactly retain the important variables and discard the unimportant ones
    and the collinear, but not so relevant, ones.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The scoring can also be interpreted as a ranking of each variable's role in
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn offers two implementations of stability selection: `RandomizedLogisticRegression`
    for classification tasks and `RandomizedLasso` as a regressor. They are both in
    the `linear_model` module.'
  prefs: []
  type: TYPE_NORMAL
- en: 'They also both share the same key hyper-parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`C` : is the regularization parameter, by default set to `1.0`. If you can
    manage to find a good C on all the data by cross-validation, put that figure in
    the parameter. Otherwise, start confidently using the default value; it is a good
    compromise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scaling` : is the percentage of feature to be kept at every iteration, the
    default value of `0.5` is a good figure; lower the number if there are many redundant
    variables in your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_fraction` : is the percentage of observations to be kept; the default
    value of `0.75` should be decreased if you suspect outliers in your data (so they
    will less likely be drawn).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_resampling` : the number of iterations; the more the better, but 200-300
    resamples should bear good results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with the Madelon
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From our past experimentations, stability selection does help to quickly fix
    any problem inherent to variable selection, even when dealing with sparse variables
    such as textual data rendered into indicator variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate its effectiveness, we are going to apply it to the Madelon dataset,
    trying to get a better AUC score after stability selection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Since it is a classification problem, we are going to use the `RandomizedLogisticRegression`
    class, setting `300` resamples and subsampling 15% of variables and 50% of observations.
    As a threshold, we are going to retain all those features that appear significant
    in the model at least 3% of the time. Such settings are quite strict, but they
    are due to the presence of high redundancy in the dataset and extreme instability
    of L1 solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the solution using a `make_pipeline` command allows us to create a sequence
    of actions to be first fitted and used on training data and then reapplied, using
    the same configuration, to the validation data. The idea is to first select the
    important and relevant features based on stability selection and then to create
    interactions (just multiplicative terms) using polynomial expansion to catch the
    non-linear components in the data with new derived features. If we were to create
    polynomial expansion without first selecting which variables we should use, then
    our dataset would exponentially grow in the number of variables and it could prove
    impossible even to store it in-memory.
  prefs: []
  type: TYPE_NORMAL
- en: '`RandomizedLogisticRegression` acts more as a pre-processing filter than a
    predictive model: after fitting, though allowing us to have a glance at the produced
    scores, it won''t allow any prediction on the basis of the host of created models,
    but it will allow us to transform any dataset similar to ours (the same number
    of columns), keeping only the columns whose score is above the threshold that
    we initially defined when we instantiated the class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, after having the resamples run, and it may take some time, we
    can try to figure out how many variables have been retained by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Here, 19 variables constitute a small set, which can be expanded into four-way
    interactions of the type `var1 × var2 × var3 × var4`, allowing us to better map
    the unknown transformations at the origin of the Madelon dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: A final test on the obtained probability estimates reveals to us that we reached
    an AUC value of `0.885`, a fairly good improvement from the initial `0.602` baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During this chapter, we have covered quite a lot of ground, finally exploring
    the most experimental and scientific part of the task of modeling linear regression
    or classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with the topic of generalization, we explained what can go wrong in
    a model and why it is always important to check the true performances of your
    work by train/test splits and by bootstraps and cross-validation (though we recommend
    using the latter more for validation work than general evaluation itself).
  prefs: []
  type: TYPE_NORMAL
- en: Model complexity as a source of variance in the estimate gave us the occasion
    to introduce variable selection, first by greedy selection of features, univariate
    or multivariate, then using regularization techniques, such as Ridge, Lasso and
    Elastic Net.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we demonstrated a powerful application of Lasso, called stability selection,
    which, in the light of our experience, we recommend you try for many feature selection
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will deal with the problem of incrementally growing
    datasets, proposing solutions that may work well even if your problem is that
    of datasets too large to easily and timely fit into the memory of your working
    computer.
  prefs: []
  type: TYPE_NORMAL
