<html><head></head><body>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04" class="calibre1"/>Chapter 4. Logistic Regression</h1></div></div></div><p class="calibre8">In this chapter, another supervised method is introduced: classification. We will introduce the simplest classifier, the Logistic Regressor, which shares the same foundations as the Linear Regressor, but it targets classification problems.</p><p class="calibre8">In the following chapter, you'll find:</p><div><ul class="itemizedlist"><li class="listitem">A formal and mathematical definition of the classification problem, for both binary and multiclass problems</li><li class="listitem">How to evaluate classifier performances—that is, their metrics</li><li class="listitem">The math behind Logistic Regression</li><li class="listitem">A revisited formula for SGD, specifically built for Logistic Regression</li><li class="listitem">The multiclass case, with Multiclass Logistic Regression</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch04lvl1sec23" class="calibre1"/>Defining a classification problem</h1></div></div></div><p class="calibre8">Although the <a id="id315" class="calibre1"/>name Logistic Regression suggests a regression operation, the goal of Logistic Regression is classification. In a very rigorous world such as statistics, why is this technique ambiguously named? Simple, the name is not wrong at all, and it makes perfect sense: it just requires a bit of an introduction and investigation. After that you'll fully understand why it's named Logistic Regression, and you'll no longer think that it's a wrong name.</p><p class="calibre8">First, let's introduce what a classification problem is, what a classifier is, how it operates, and what its output is.</p><p class="calibre8">In the previous chapter, we presented regression as the operation of estimating a continuous value in a target variable; mathematically speaking, the predicted variable is a real number in the range (<em class="calibre9">−∞</em>, <em class="calibre9">+∞</em>). Classification, instead, predicts a class, that is, an index in a finite set of classes. The simplest case is named binary classification, and the output is typically a Boolean value (<code class="email">true</code>/<code class="email">false</code>). If the class is <code class="email">true</code> the sample is typically called a <em class="calibre9">positive sample</em>; otherwise it's a <em class="calibre9">negative sample</em>.</p><p class="calibre8">To state some examples, here are some questions that refer to a binary classification problem:</p><div><ul class="itemizedlist"><li class="listitem">Is this email spam?</li><li class="listitem">Is my house worth at least $200,000?</li><li class="listitem">Is the banner/email clicked/opened by the user?</li><li class="listitem">Is the current document about finance?</li><li class="listitem">Is there a person in the image? Is it a man or a woman?</li></ul></div><div><h3 class="title2"><a id="tip19" class="calibre1"/>Tip</h3><p class="calibre8">Putting a threshold on the output of a regression problem, to determine whether the value is greater or lower than a fixed threshold, is actually a binary classification problem.</p></div><p class="calibre8">When the output<a id="id316" class="calibre1"/> can have multiple values (that is, the predicted label is a categorical variable), the classification is named a multiclass one. Usually, the possible labels are named levels or classes, and the list of them should be finite and known in advance (or else it will be an unsupervised problem, not a supervised one).</p><p class="calibre8">Examples of multiclass classification problems are:</p><div><ul class="itemizedlist"><li class="listitem">Which kind of flower is this?</li><li class="listitem">What's the primary topic of this webpage?</li><li class="listitem">Which kind of network attack am I experiencing?</li><li class="listitem">Which digit/letter is drawn in the image?</li></ul></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec47" class="calibre1"/>Formalization of the problem: binary classification</h2></div></div></div><p class="calibre8">Let's start now <a id="id317" class="calibre1"/>with the simplest type of classification: the <strong class="calibre2">binary classification</strong>. Don't worry; in a few pages things are going to be more <a id="id318" class="calibre1"/>complex when we focus on the multiclass classification.</p><p class="calibre8">Formally, the generic observation is an <em class="calibre9">n</em>-dimensional feature vector (<em class="calibre9">x<sub class="calibre20">i</sub></em>) paired with its label: the generic <em class="calibre9">i</em>-th can be written as:</p><div><img src="img/00056.jpeg" alt="Formalization of the problem: binary classification" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The model underneath the classifier is a function <a id="id319" class="calibre1"/>and is called a <strong class="calibre2">classification function</strong>, which can be either linear or non linear. The form of the function is the following:</p><div><img src="img/00057.jpeg" alt="Formalization of the problem: binary classification" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">During the prediction task, the classification function is applied to a new feature vector, and the output of the classifier represents the class to which the input sample is classified, that is, the predicted label. A perfect classifier predicts, for every possible input, the correct class <code class="email">y</code>.</p><p class="calibre8">The feature vector <em class="calibre9">x</em> should comprise numbers. If you're dealing with categorical features (such as gender, membership, and words), you should be able to take that variable to one or more numeric variables (usually binary). We'll see more about this point later on in the book, in <a class="calibre1" title="Chapter 5. Data Preparation" href="part0035_split_000.html#11C3M2-a2faae6898414df7b4ff4c9a487a20c6">Chapter 5</a>, <em class="calibre9">Data Preparation</em>, which is devoted to data preparation of variables into the most suitable form for regression.</p><p class="calibre8">To have a visual<a id="id320" class="calibre1"/> understanding of what's going on, let's<a id="id321" class="calibre1"/> consider now a binary classification problem, where every feature has two dimensions (a 2-D problem). Let's first define the input dataset; here the <code class="email">make_classifier</code> method of the Scikit-learn library comes in very handy. It creates a dummy dataset for classification, providing the number of classes, the dimensionality of the problem, and the number of observations as parameters. Additionally, you should specify that each feature is informative (and there are no redundancies) and each class is composed of a single cluster of points:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">%matplotlib inline</strong>

<strong class="calibre2">import matplotlib.pyplot as plt</strong>
<strong class="calibre2">from sklearn.datasets import make_classification</strong>

<strong class="calibre2">X, y = make_classification(n_samples=100, n_features=2,</strong>
<strong class="calibre2">                           n_informative=2, n_redundant=0,</strong>
<strong class="calibre2">                           n_clusters_per_class=1,</strong>
<strong class="calibre2">                           class_sep = 2.0, random_state=101)</strong>

<strong class="calibre2">plt.scatter(X[:, 0], X[:, 1], marker='o', c=y,</strong>
<strong class="calibre2">            linewidth=0, edgecolor=None)</strong>
<strong class="calibre2">plt.xlabel('Feature 1')</strong>
<strong class="calibre2">plt.ylabel('Feature 2')</strong>
<strong class="calibre2">plt.show()</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00058.jpeg" alt="Formalization of the problem: binary classification" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch04lvl2sec48" class="calibre1"/>Assessing the classifier's performance</h2></div></div></div><p class="calibre8">To understand if a classifier is a good one, or equivalently, to identify the classifier with the best performance in the classification task, we need to define some metrics. There is no single <a id="id322" class="calibre1"/>metric since the classification goal can be different—for example, the correctness or completeness of a defined label, minimization of the number of misclassifications, correct ordering in respect of the likelihood of having a certain label, and quite a few others. All the measures can be derived from the classification matrix after having applied a cost matrix: the outcome highlights which errors are more expensive and which are not so expensive in terms of results.</p><p class="calibre8">Metrics exposed here can be used for both binary and multiclass classification. Although it is not a measure of <a id="id323" class="calibre1"/>performance, let's start from the confusion matrix, the simplest metric that gives us a visual impact of the correct classifications and the misclassification errors for each class. On the rows there are the true labels, on the column the predicted one. Let's also create a dummy label set and a predicted set for the following experiments. In our example the original labels are six <code class="email">0</code> and four <code class="email">1</code>; the classifier misclassified entries are two <code class="email">0</code> and one <code class="email">1</code>:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">y_orig = [0,0,0,0,0,0,1,1,1,1]</strong>
<strong class="calibre2">y_pred = [0,0,0,0,1,1,1,1,1,0]</strong>
</pre></div><p class="calibre8">Let's now create the confusion matrix for this experiment:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.metrics import confusion_matrix</strong>
<strong class="calibre2">confusion_matrix(y_orig, y_pred)</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">array([[4, 2],</strong>
<strong class="calibre2">       [1, 3]])</strong>
</pre></div><p class="calibre8">From this matrix we can extract some evidence:</p><div><ul class="itemizedlist"><li class="listitem">The number of samples is <code class="email">10</code> (the sum of the whole matrix).</li><li class="listitem">The number of samples labeled <code class="email">0</code> in the original is <code class="email">6</code>; <code class="email">1</code>s are <code class="email">4</code> (the sum for the lines). These numbers are named support.</li><li class="listitem">The number of samples labeled <code class="email">0</code> in the predicted dataset is <code class="email">5</code>; <code class="email">1</code>s are <code class="email">5</code> (the sum as columns).</li><li class="listitem">Correct classifications are <code class="email">7</code> (the sum of the diagonal).</li><li class="listitem">Misclassifications are <code class="email">3</code> (the sum of all numbers not on the diagonal).</li></ul></div><p class="calibre8">A perfect <a id="id324" class="calibre1"/>classification example would have had all the numbers on the diagonal, and <code class="email">0</code> elsewhere.</p><p class="calibre8">This matrix can also be represented graphically, using a heatmap. This is a very impactful representation, especially when dealing with multiclass problems:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">plt.matshow(confusion_matrix(y_orig, y_pred))</strong>
<strong class="calibre2">plt.title('Confusion matrix')</strong>
<strong class="calibre2">plt.colorbar()</strong>
<strong class="calibre2">plt.ylabel('True label')</strong>
<strong class="calibre2">plt.xlabel('Predicted label')</strong>
<strong class="calibre2">plt.show()</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00059.jpeg" alt="Assessing the classifier's performance" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The first measure we're going to explore to evaluate the classifier's performance is accuracy. Accuracy is the <a id="id325" class="calibre1"/>percentage of correct classifications, over the total number of samples. You<a id="id326" class="calibre1"/> can derive this error measure directly from the confusion matrix by dividing the sum over the diagonal by the sum of the elements in the matrix. The best possible accuracy is <code class="email">1.0</code> and the worst one is <code class="email">0.0</code>. In the preceding example, accuracy amounts to <em class="calibre9">7/10 = 0.7</em>.</p><p class="calibre8">Using Python, this becomes:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.metrics import accuracy_score</strong>
<strong class="calibre2">accuracy_score(y_orig, y_pred)</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">0.69999999999999996</strong>
</pre></div><p class="calibre8">Another very popular measure is precision. It considers only one label and counts the percentage of correct classifications on that label. While considering our label "1", the precision is the number in the bottom right of the confusion matrix, divided by the sum of the elements in the second column—that is, <em class="calibre9">3/5=0.6</em>. Values are bounded between 0 and 1, where 1 is the best possible result and 0 the worst.</p><p class="calibre8">Note that this function in Scikit-learn expects a binary input, where only the class under examination is marked as <code class="email">true</code> (this is sometime named a <em class="calibre9">class indicator</em>). To extract a precision score for each <a id="id327" class="calibre1"/>label, you should then make each class a binary vector:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.metrics import precision_score</strong>
<strong class="calibre2">precision_score(y_orig, y_pred)</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">0.59999999999999998</strong>
</pre></div><p class="calibre8">Paired with precision you'll frequently find another error measure, recall. If precision is about the quality of what you got (that is, the quality of the results marked with the label <code class="email">1</code>), recall is about<a id="id328" class="calibre1"/> the quality of what you could have gotten—that is, how many instances of <code class="email">1</code> you've been able to extract properly. Also, here, this measure is class-based, and to compute the recall score for class <code class="email">1</code> you should divide the bottom right number in the confusion matrix by <a id="id329" class="calibre1"/>the sum of the second line, that is, <em class="calibre9">3/4=0.75</em>. Recall is bounded <code class="email">0</code> and <code class="email">1</code>; the best score is <code class="email">1</code> and means that all the instances of "1" in the original dataset have been correctly classified as "1"; a score equal to <code class="email">0</code> means that no "1"s have been classified properly:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.metrics import recall_score</strong>
<strong class="calibre2">recall_score(y_orig, y_pred)</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">0.75</strong>
</pre></div><p class="calibre8">Precision and recall are two metrics that indicate how well the classifier performed on a class. Merge their score, using a harmonic average, and you'll get the comprehensive f1-score, helping you to figure out at a glance the performance on both error measures.</p><p class="calibre8">Mathematically:</p><div><img src="img/00060.jpeg" alt="Assessing the classifier's performance" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In Python this is easier:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.metrics import f1_score</strong>
<strong class="calibre2">f1_score(y_orig, y_pred)</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">0.66666666666666652</strong>
</pre></div><p class="calibre8">In conclusion, if there are so many error scores, which is the best to use? The solution is not very easy, and often it is better to have and evaluate the classifier on all of them. How can we do that? Is it a long function to write? No, Scikit-learn here comes to help us here, providing a method to compute all these scores for each class (this is really handy). Here is how it works:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.metrics import classification_report</strong>
<strong class="calibre2">print(classification_report(y_orig, y_pred))</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00061.jpeg" alt="Assessing the classifier's performance" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec24" class="calibre1"/>Defining a probability-based approach</h1></div></div></div><p class="calibre8">Let's gradually introduce how logistic regression works. We said that it's a classifier, but its name recalls a regressor. The element we need to join the pieces is the probabilistic interpretation.</p><p class="calibre8">In a binary <a id="id330" class="calibre1"/>classification problem, the output can be either "0" or "1". What if we check the probability of the label belonging to class "1"? More specifically, a classification problem can be seen as: given the feature vector, find the class (either 0 or 1) that maximizes the conditional probability:</p><div><img src="img/00062.jpeg" alt="Defining a probability-based approach" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here's the connection: if we compute a probability, the classification problem <em class="calibre9">looks like</em> a regression problem. Moreover, in a binary classification problem, we just need to compute the probability of membership of class "1", and therefore it looks like a well-defined regression problem. In the regression problem, classes are no longer "1" or "0" (as strings), but 1.0 and 0.0 (as the probability of belonging to class "1").</p><p class="calibre8">Let's now try fitting a multiple linear regressor on a dummy classification problem, using a probabilistic interpretation. We reuse the same dataset we created earlier in this chapter, but first we split the dataset into train and test sets, and we convert the <code class="email">y</code> vector to floating point values:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.cross_validation import train_test_split</strong>

<strong class="calibre2">X_train, X_test, y_train, \y_test = train_test_split(X, y.astype(float),\test_size=0.33, random_state=101)</strong>

<strong class="calibre2">In:</strong>
<strong class="calibre2">y_test.dtype</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">dtype('float64')</strong>
<strong class="calibre2">In:</strong>
<strong class="calibre2">y_test</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00063.jpeg" alt="Defining a probability-based approach" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, with these few methods, we split the datasets into two folds, (train and test) and we converted all the <a id="id331" class="calibre1"/>numbers in the <em class="calibre9">y</em> array to floating point. In the last cell, we effectively check the operation. Now, if <em class="calibre9">y = 1.0</em>, it means that the relative observation is 100% class "1"; <em class="calibre9">y = 0.0</em> implies that the observation is 0% class "1". Since it's a binary classification task, it implies that it's also 100% class "0" (note that the percentages here refer to probability).</p><p class="calibre8">Let's now proceed with the regression:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.linear_model import LinearRegression</strong>
<strong class="calibre2">regr = LinearRegression()</strong>
<strong class="calibre2">regr.fit(X_train, y_train)</strong>
<strong class="calibre2">regr.predict(X_test)</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00064.jpeg" alt="Defining a probability-based approach" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The output—that is, the prediction of the regressor—should be the probability of belonging to class 1. As you can see in the last cell output, that's not a proper probability, since it contains values below 0 and greater than 1. The simplest idea here is clipping results between 0 and 1, and putting a threshold at <code class="email">0.5</code>: if the value is <em class="calibre9">&gt;0.5</em>, then the predicted class is "1"; otherwise the predicted class is "0".</p><p class="calibre8">This procedure works, but we can do better. We've seen how easy it is to transit from a classification problem to a regression one, and then go back with predicted values to predicted classes. With this process in mind, let's again start the analysis, digging further in its core algorithm while introducing some changes.</p><p class="calibre8">In our dummy<a id="id332" class="calibre1"/> problem, we applied the linear regression model to estimate the probability of the observation belonging to class "1". The regression model was (as we've seen in the previous chapter):</p><div><img src="img/00065.jpeg" alt="Defining a probability-based approach" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, we've seen that the output is not a proper probability. To be a probability, we need to do the following:</p><div><ol class="orderedlist"><li class="listitem" value="1">Bound the output between 0.0 and 1.0 (clipping).</li><li class="listitem" value="2">If the prediction is equal to the threshold (we chose 0.5 previously), the probability should be 0.5 (symmetry).</li></ol><div></div><p class="calibre8">To have both conditions <code class="email">true</code>, the best we could do is to send the output of the regressor through a sigmoid curve, or an S-shaped curve. A sigmoid generically maps values in R (the field of real numbers) to values in the range <code class="email">[0,1]</code>, and its value when mapping <code class="email">0</code> is <code class="email">0.5</code>.</p><p class="calibre8">On the basis of such a hypothesis, we can now write (for the first time) the formula underneath the logistic regression algorithm.</p><div><img src="img/00066.jpeg" alt="Defining a probability-based approach" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note also that the weight <code class="email">W[0]</code> (the bias weight) will take care of the misalignment of the central point of the sigmoid (it's in 0, whereas the threshold is in 0.5).</p><p class="calibre8">That's all. That's the logistic regression algorithm. There is just one thing missing: why logistic? What's the <em class="calibre9">σ</em> function?</p><p class="calibre8">Well, the answer to both questions is trivial: the standard choice of sigma is the logistic function, also <a id="id333" class="calibre1"/>named the inverse-logit function:</p><div><img src="img/00067.jpeg" alt="Defining a probability-based approach" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Although there are infinite functions that satisfy the sigmoid constraints, the logistic has been chosen because it's continuous, easily differentiable, and quick to compute. If the results are not satisfactory, always consider that, by introducing a couple of parameters, you can change<a id="id334" class="calibre1"/> the steepness and the center of the function.</p><p class="calibre8">The sigmoid function is quickly drawn:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">import numpy as np</strong>

<strong class="calibre2">def model(x):</strong>
<strong class="calibre2">    return 1 / (1 + np.exp(-x))</strong>

<strong class="calibre2">X_vals = np.linspace(-10, 10, 1000)</strong>
<strong class="calibre2">plt.plot(X_vals, model(X_vals), color='blue', linewidth=3)</strong>
<strong class="calibre2">plt.ylabel('sigma(t)')</strong>
<strong class="calibre2">plt.xlabel('t')</strong>

<strong class="calibre2">plt.show()</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00068.jpeg" alt="Defining a probability-based approach" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">You can immediately see that, for a very low <strong class="calibre2">t</strong>, the function tends to the value <strong class="calibre2">0</strong>; for a very high <strong class="calibre2">t</strong>, the function tends to be <strong class="calibre2">1</strong>, and, in the center, where <strong class="calibre2">t</strong> is <strong class="calibre2">0</strong>, the function is <strong class="calibre2">0.5</strong>. Exactly the sigmoid function we were looking for.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec49" class="calibre1"/>More on the logistic and logit functions</h2></div></div></div><p class="calibre8">Now, why<a id="id335" class="calibre1"/> did <a id="id336" class="calibre1"/>we<a id="id337" class="calibre1"/> use<a id="id338" class="calibre1"/> the inverse of the logit function? Isn't there anything better than that? The answer to this question comes from statistics: we're dealing with probabilities, and the logit function is a great fit. In statistics, the logit function applied to a probability, returns the log-odds:</p><div><img src="img/00069.jpeg" alt="More on the logistic and logit functions" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This function transforms numbers from range <code class="email">[0,1]</code> to numbers in (<em class="calibre9">−∞</em>, <em class="calibre9">+∞</em>).</p><p class="calibre8">Now, let's <a id="id339" class="calibre1"/>see if you can intuitively understand the<a id="id340" class="calibre1"/> logic behind the selection of the<a id="id341" class="calibre1"/> inverse-logit function as the sigmoid function for the <a id="id342" class="calibre1"/>logistic regression. Let's first write down the probabilities for both classes, according to this logistic regression equation:</p><div><img src="img/00070.jpeg" alt="More on the logistic and logit functions" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Let's now compute the log-odds:</p><div><img src="img/00071.jpeg" alt="More on the logistic and logit functions" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">However, not surprisingly, that's also the <strong class="calibre2">logit</strong> function, applied to the probability of getting a "1":</p><div><img src="img/00072.jpeg" alt="More on the logistic and logit functions" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The chain of our reasoning is finally closed, and here's why logistic regression is based on, as the definition implies, the logistic function. Actually, logistic regression is a model of the big category of the GLM: the generalized linear model. Each model has a different function, a different<a id="id343" class="calibre1"/> formulation, a different <a id="id344" class="calibre1"/>operative<a id="id345" class="calibre1"/> hypothesis, and <a id="id346" class="calibre1"/>not, surprisingly, a different goal.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec50" class="calibre1"/>Let's see some code</h2></div></div></div><p class="calibre8">First, we<a id="id347" class="calibre1"/> start with the dummy dataset we created at the beginning of the chapter. Creating and fitting a logistic regressor classifier is really easy: thanks to Scikit-learn, it just requires a couple of lines of Python code. As for regressors, to train the model you need to call the <code class="email">fit</code> method, whereas for predicting the class you just need to call the <code class="email">predict</code> method:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.linear_model import LogisticRegression</strong>

<strong class="calibre2">clf = LogisticRegression()</strong>
<strong class="calibre2">clf.fit(X_train, y_train.astype(int))</strong>
<strong class="calibre2">y_clf = clf.predict(X_test)</strong>

<strong class="calibre2">print(classification_report(y_test, y_clf))</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00073.jpeg" alt="Let's see some code" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note that here we're not making a regression operation; that's why the label vector must comprise integers (or class indexes). The report shown at the bottom shows a very accurate prediction: all the scores are close to 1 for all classes. Since we have <code class="email">33</code> samples in the test set, <code class="email">0.97</code> means just one case misclassified. That's almost perfect in this dummy example!</p><p class="calibre8">Now, let's try to dig under the hood even more. First, we would like to check the decision boundary of the <a id="id348" class="calibre1"/>classifier: which part of the bidimensional space has points being classified as "1"; and where are the "0"s? Let's see how you can visually see the decision boundary here:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2"># Example based on:</strong>
<strong class="calibre2"># Code source: Gaël Varoquaux, Modified for documentation by Jaques Grobler, License: BSD 3 clause</strong>

<strong class="calibre2">h = .02  # step size in the mesh</strong>

<strong class="calibre2"># Plot the decision boundary. For that, we will assign a color to each</strong>
<strong class="calibre2"># point in the mesh [x_min, m_max]x[y_min, y_max].</strong>
<strong class="calibre2">x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5</strong>
<strong class="calibre2">y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5</strong>
<strong class="calibre2">xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</strong>
<strong class="calibre2">Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</strong>

<strong class="calibre2"># Put the result into a color plot</strong>
<strong class="calibre2">Z = Z.reshape(xx.shape)</strong>
<strong class="calibre2">plt.pcolormesh(xx, yy, Z, cmap=plt.cm.autumn)</strong>

<strong class="calibre2"># Plot also the training points</strong>
<strong class="calibre2">plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', linewidth=0, cmap=plt.cm.Paired)</strong>

<strong class="calibre2">plt.xlim(xx.min(), xx.max())</strong>
<strong class="calibre2">plt.ylim(yy.min(), yy.max())</strong>
<strong class="calibre2">plt.xticks(())</strong>
<strong class="calibre2">plt.yticks(())</strong>

<strong class="calibre2">plt.show()</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00074.jpeg" alt="Let's see some code" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The separation is almost vertical. "1"s are on the left (yellow) side; "0"s on the right (red). From the earlier screenshot, you can immediately perceive the misclassification: it's pretty close to the boundary. Therefore, its probability of belonging to class "1" will be very close<a id="id349" class="calibre1"/> to 0.5.</p><p class="calibre8">Let's now see the bare probabilities and the weight vector. To compute the probability, you need to use the <code class="email">predict_proba</code> method of the classifier. It returns two values for each observation: the first is the probability of being of class "0"; the second the probability for class "1". Since we're interested in class "1", here we just select the second value for all the observations:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]</strong>
<strong class="calibre2">Z = Z.reshape(xx.shape)</strong>
<strong class="calibre2">plt.pcolormesh(xx, yy, Z, cmap=plt.cm.autumn)</strong>

<strong class="calibre2">ax = plt.axes()</strong>
<strong class="calibre2">ax.arrow(0, 0, clf.coef_[0][0], clf.coef_[0][1], head_width=0.5, </strong>
<strong class="calibre2">head_length=0.5, fc='k', ec='k')</strong>
<strong class="calibre2">plt.scatter(0, 0, marker='o', c='k')</strong>

<strong class="calibre2">plt.xlim(xx.min(), xx.max())</strong>
<strong class="calibre2">plt.ylim(yy.min(), yy.max())</strong>

<strong class="calibre2">plt.show()</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00075.jpeg" alt="Let's see some code" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In the screenshot, pure yellow and pure red are where the predicted probability is very close to 1 and 0 respectively. The black dot is the origin <em class="calibre9">(0,0)</em> of the Cartesian bidimensional space, and the arrow is the representation of the weight vector of the classifier. As you <a id="id350" class="calibre1"/>can see, it's orthogonal to the decision boundary, and it's <em class="calibre9">pointing</em> toward the "1" class. The weight vector is actually the model itself: if you need to store it in a file, consider that it's just a couple of floating point numbers and nothing more.</p><p class="calibre8">Lastly, I'd want to focus on speed. Let's now see how much time the classifier takes to train and to predict the labels:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">%timeit clf.fit(X, y)</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">1000 loops, best of 3: 291 µs per loop</strong>
<strong class="calibre2">In:</strong>
<strong class="calibre2">%timeit clf.predict(X)</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">10000 loops, best of 3: 45.5 µs per loop</strong>
<strong class="calibre2">In:</strong>
<strong class="calibre2">%timeit clf.predict_proba(X)</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">10000 loops, best of 3: 74.4 µs per loop</strong>
</pre></div><p class="calibre8">Although timings are computer-specific (here we're training it and predicting using the full 100-point dataset), you can see that Logistic Regression is a very fast technique both during training and when predicting the class and the probability for all classes.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch04lvl2sec51" class="calibre1"/>Pros and cons of logistic regression</h2></div></div></div><p class="calibre8">Logistic<a id="id351" class="calibre1"/> regression is a very popular algorithm because of the following:</p><div><ul class="itemizedlist"><li class="listitem">It's linear: it's<a id="id352" class="calibre1"/> the equivalent of the linear regression for classification.</li><li class="listitem">It's very simple to understand, and the output can be the most likely class, or the probability of membership.</li><li class="listitem">It's simple to train: it has very few coefficients (one coefficient for each feature, plus one bias). This makes the model very small to store (you just need to store a vector of weights).</li><li class="listitem">It's computationally efficient: using some special tricks (see later in the chapter), it can be trained very quickly.</li><li class="listitem">It has an extension for multiclass classification.</li></ul></div><p class="calibre8">Unfortunately, it's not a perfect classifier and has some drawbacks:</p><div><ul class="itemizedlist"><li class="listitem">It's often not very performant, compared to most advanced algorithms, because it tends to underfit (no flexibility: the boundary has to be a line or a hyperplane)</li><li class="listitem">It's linear: if the problem<a id="id353" class="calibre1"/> is non-linear, there is no way to<a id="id354" class="calibre1"/> properly fit this classifier onto the dataset</li></ul></div></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec25" class="calibre1"/>Revisiting gradient descent</h1></div></div></div><p class="calibre8">In the previous chapter, we introduced the gradient descent technique to speed up processing. As we've seen with Linear Regression, the fitting of the model can be made in two ways: closed form or iterative form. Closed form gives the best possible solution in one step (but it's a very complex and time-demanding step); iterative algorithms, instead, reach the minima step by step with few calculations for each update and can be stopped at any time.</p><p class="calibre8">Gradient descent is <a id="id355" class="calibre1"/>a very popular choice for fitting the Logistic Regression model; however, it shares its popularity with Newton's methods. Since Logistic Regression is the base of the iterative optimization, and we've already introduced it, we will focus on it in this section. Don't worry, there is no winner or any best algorithm: all of them can reach the very same model eventually, following different paths in the coefficients' space.</p><p class="calibre8">First, we should compute the derivate of the loss function. Let's make it a bit longer, and let's start deriving the logistic function:</p><div><img src="img/00076.jpeg" alt="Revisiting gradient descent" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Its first-order derivative is as follows:</p><div><img src="img/00077.jpeg" alt="Revisiting gradient descent" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This is another reason why logistic regression used the logistic function: its derivate is computationally light. Now, let's assume that the training observations are independent. Computing the likelihood, with respect to the set of weights, is as follows:</p><div><img src="img/00078.jpeg" alt="Revisiting gradient descent" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note that, in the<a id="id356" class="calibre1"/> last row, we used a trick based on the fact that <em class="calibre9">y<sub class="calibre20">i</sub></em> can be either 0 or 1. If <em class="calibre9">y<sub class="calibre20">i</sub>=1</em>, only the first factor of the multiplication is computed; otherwise it's the second factor.</p><p class="calibre8">Let's now compute the log-likelihood: it will make things easier:</p><div><img src="img/00079.jpeg" alt="Revisiting gradient descent" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, we have two considerations to make. First: the SGD works with one point at a time; therefore, the log-likelihood, step-by-step, is just a function of one point. Hence, we can remove the sum over all the points, and name <em class="calibre9">(x,y)</em> the point under observation. Second, we need to maximize the likelihood: to do so, we need to extract its partial derivative with respect to the generic <em class="calibre9">k</em>-th coefficient of <em class="calibre9">W</em>.</p><p class="calibre8">The math here becomes a bit complex; therefore we will just write the last result (this is the thinking we will use in our model). Deriving and understanding the equations in the middle is given to the reader as homework:</p><div><img src="img/00080.jpeg" alt="Revisiting gradient descent" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Since we're trying to maximize the likelihood (and its log version), the right formula for updating the weights is the Stochastic Gradient Ascent:</p><div><img src="img/00081.jpeg" alt="Revisiting gradient descent" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">That's the generic formula. In our case, the update step for each coefficient composing <em class="calibre9">W</em> is as follows:</p><div><img src="img/00082.jpeg" alt="Revisiting gradient descent" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, <em class="calibre9">(x,y)</em> is the (stochastic) random observation chosen for the update step, and  the learning step.</p><p class="calibre8">To see a real <a id="id357" class="calibre1"/>example of what the SGD produces, check the last section of this chapter.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec26" class="calibre1"/>Multiclass Logistic Regression</h1></div></div></div><p class="calibre8">The extension to<a id="id358" class="calibre1"/> Logistic Regression, for classifying more than two classes, is Multiclass Logistic Regression. Its foundation is actually a generic approach: it doesn't just work for Logistic Regressors, it also works with other binary<a id="id359" class="calibre1"/> classifiers. The base algorithm is named <strong class="calibre2">One-vs-rest</strong>, or <strong class="calibre2">One-vs-all</strong>, and <a id="id360" class="calibre1"/>it's simple to grasp and apply.</p><p class="calibre8">Let's describe it with an example: we have to classify three kinds of flowers and, given some features, the possible outputs are three classes: <code class="email">f1</code>, <code class="email">f2</code>, and <code class="email">f3</code>. That's not what we've seen so far; in fact, this is not a binary classification problem. Instead, it seems very easy to break down this problem into three simpler problems:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">Problem #1</strong>: Positive examples (that is, the ones that get the label "1") are <code class="email">f1</code>; negative examples are all the others</li><li class="listitem"><strong class="calibre2">Problem #2</strong>: Positive examples are <code class="email">f2</code>; negative examples are <code class="email">f1</code> and <code class="email">f3</code></li><li class="listitem"><strong class="calibre2">Problem #3</strong>: Positive examples are <code class="email">f3</code>; negative examples are <code class="email">f1</code> and <code class="email">f2</code></li></ul></div><p class="calibre8">For all three problems, we can use a binary classifier, as Logistic Regressor, and, unsurprisingly, the first classifier will output <em class="calibre9">P(y = f1|x)</em>; the second and the third will output respectively <em class="calibre9">P(y = f2|x) and P(y = f3|x)</em>.</p><p class="calibre8">To make the final prediction, we just need to select the classifier that emitted the highest probability. Having trained three classifiers, the feature space is not divided in two subplanes, but according to the decision boundary of the three classifiers.</p><p class="calibre8">The approach of One-vs-all is very convenient, in fact:</p><div><ul class="itemizedlist"><li class="listitem">The number of classifiers to fit is exactly the same as the number of classes. Therefore, the model will be composed by <em class="calibre9">N</em> (where <em class="calibre9">N</em> is the number of classes) weight vectors.</li><li class="listitem">Moreover, this operation is embarrassingly parallel and the training of the <em class="calibre9">N</em> classifiers can be made simultaneously, using multiple threads (up to <em class="calibre9">N</em> threads).</li><li class="listitem">If the classes are balanced, the training time for each classifier is similar, and the predicting time is the same (even for unbalanced classes).</li></ul></div><p class="calibre8">For a better<a id="id361" class="calibre1"/> understanding, let's make a multiclass classification example, creating a dummy three-class dataset, splitting it as training and test sets, training a Multiclass Logistic Regressor, applying it on the training set, and finally visualizing the boundaries:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">%reset -f</strong>
<strong class="calibre2">In:</strong>
<strong class="calibre2">%matplotlib inline</strong>

<strong class="calibre2">import matplotlib.pyplot as plt</strong>
<strong class="calibre2">from sklearn.datasets import make_classification</strong>

<strong class="calibre2">X, y = make_classification(n_samples=200, n_features=2,</strong>
<strong class="calibre2">                           n_classes=3, n_informative=2,</strong>
<strong class="calibre2">                           n_redundant=0, n_clusters_per_class=1,</strong>
<strong class="calibre2">                           class_sep = 2.0, random_state=101)</strong>

<strong class="calibre2">plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, linewidth=0, edgecolor=None)</strong>
<strong class="calibre2">plt.show()</strong>
<strong class="calibre2">Out:</strong>
</pre></div><p class="calibre8"> </p><div><img src="img/00083.jpeg" alt="Multiclass Logistic Regression" class="calibre10"/></div><p class="calibre11"> </p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.cross_validation import train_test_split</strong>

<strong class="calibre2">X_train, X_test, y_train, y_test = train_test_split(X, y.astype(float),</strong>
<strong class="calibre2">                                   test_size=0.33, random_state=101)</strong>
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.linear_model import LogisticRegression</strong>

<strong class="calibre2">clf = LogisticRegression()</strong>
<strong class="calibre2">clf.fit(X_train, y_train.astype(int))</strong>
<strong class="calibre2">y_clf = clf.predict(X_test)</strong>
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.metrics import classification_report</strong>
<strong class="calibre2">print(classification_report(y_test, y_clf))</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00084.jpeg" alt="Multiclass Logistic Regression" class="calibre10"/></div><p class="calibre11"> </p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">import numpy as np</strong>

<strong class="calibre2">h = .02  # step size in the mesh</strong>

<strong class="calibre2">x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5</strong>
<strong class="calibre2">y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5</strong>
<strong class="calibre2">xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</strong>
<strong class="calibre2">Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</strong>

<strong class="calibre2"># Put the result into a color plot</strong>
<strong class="calibre2">Z = Z.reshape(xx.shape)</strong>
<strong class="calibre2">plt.pcolormesh(xx, yy, Z, cmap=plt.cm.autumn)</strong>

<strong class="calibre2"># Plot also the training points</strong>
<strong class="calibre2">plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)</strong>
<strong class="calibre2">plt.xlim(xx.min(), xx.max())</strong>
<strong class="calibre2">plt.ylim(yy.min(), yy.max())</strong>
<strong class="calibre2">plt.xticks(())</strong>
<strong class="calibre2">plt.yticks(())</strong>

<strong class="calibre2">plt.show()</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00085.jpeg" alt="Multiclass Logistic Regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">On this dummy dataset, the classifier has achieved a perfect classification (precision, recall, and f1-score are all 1.0). In the last picture, you can see that the decision boundaries define three areas, and create a non-linear division.</p><p class="calibre8">Finally, let's observe the first feature vector, its original label, and its predicted label (both reporting class "0"):</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">print(X_test[0])</strong>
<strong class="calibre2">print(y_test[0])</strong>
<strong class="calibre2">print(y_clf[0])</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">[ 0.73255032  1.19639333]</strong>
<strong class="calibre2">0.0</strong>
<strong class="calibre2">0</strong>
</pre></div><p class="calibre8">To get its probabilities to belong to each of the three classes, you can simply apply the <code class="email">predict_proba</code> method (exactly as in the binary case), and the classifier will output the three probabilities. Of<a id="id362" class="calibre1"/> course, their sum is 1.0, and the highest value is, naturally, one for class "0".</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">clf.predict_proba(X_test[0])</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">array([[ 0.72797056,  0.06275109,  0.20927835]])</strong>
</pre></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec27" class="calibre1"/>An example</h1></div></div></div><p class="calibre8">We now look at a <a id="id363" class="calibre1"/>practical example, containing what we've seen so far in this chapter.</p><p class="calibre8">Our dataset is an artificially created one, composed of 10,000 observations and 10 features, all of them informative (that is, no redundant ones) and labels "0" and "1" (binary classification). Having all the informative features is not an unrealistic hypothesis in machine learning, since usually the feature selection or feature reduction operation selects non-related features.</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">X, y = make_classification(n_samples=10000, n_features=10,</strong>
<strong class="calibre2">                           n_informative=10, n_redundant=0,</strong>
<strong class="calibre2">                           random_state=101)</strong>
</pre></div><p class="calibre8">Now, we'll show you how to use different libraries, and different modules, to perform the classification task, using logistic regression. We won't focus here on how to measure the performance, but on how the coefficients can compose the model (what we've named in the previous chapters).</p><p class="calibre8">As a first step, we will use Statsmodel. After having loaded the right modules, we need to add an additional feature to the input set in order to have the bias weight <code class="email">W[0]</code>. After that, training the model is really simple: we just need to instantiate a <code class="email">logit</code> object and use its <code class="email">fit</code> method. Statsmodel will train the model and will show whether it was able to train a model (<em class="calibre9">Optimization terminated successfully</em>) or it failed:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">import statsmodels.api as sm</strong>
<strong class="calibre2">import statsmodels.formula.api as smf</strong>
<strong class="calibre2">In:</strong>
<strong class="calibre2">Xc = sm.add_constant(X)</strong>
<strong class="calibre2">logistic_regression = sm.Logit(y,Xc)</strong>
<strong class="calibre2">fitted_model = logistic_regression.fit()</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">Optimization terminated successfully.</strong>
<strong class="calibre2">         Current function value: 0.438685</strong>
<strong class="calibre2">         Iterations 7</strong>
</pre></div><p class="calibre8">To get a<a id="id364" class="calibre1"/> detailed insight into the model, use the method summary:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">fitted_model.summary()</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00086.jpeg" alt="An example" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Two tables are returned: the first one is about the dataset and model performances; the second is about the weights of the model. Statsmodel provides a lot of information on the model; some<a id="id365" class="calibre1"/> of it has been shown in <a class="calibre1" title="Chapter 2. Approaching Simple Linear Regression" href="part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6">Chapter 2</a>, <em class="calibre9">Approaching Simple Linear Regression</em>, about a trained regressor. Here, instead, we have a brief description of the information shown for the classifier:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">Converged</strong>: This tells whether the classification model has reached convergence while being trained. Use the model only if this parameter is <code class="email">true</code>.</li><li class="listitem"><strong class="calibre2">Log-Likelihood</strong>: This is the logarithm of the likelihood. It's what we previously named.</li><li class="listitem"><strong class="calibre2">LL-Null</strong>: This is the Log-Likelihood when only the intercept is used as a predictor.</li><li class="listitem"><strong class="calibre2">LLR p-value</strong>: This is the chi-squared probability of getting a log-likelihood ratio statistically greater than LLR. Basically, it shows how the model is better than guessing with a constant value. LLR is the log-likelihood ratio, that is, the logarithm of the likelihood of the null model (intercept only), divided by the likelihood of the alternate model (full model).</li><li class="listitem"><strong class="calibre2">Pseudo R-squared</strong>: This can be seen as the proportion of the total variability unexplained by the model. It's computed as <em class="calibre9">1-Log-likelihood/LL-Null</em>.</li></ul></div><p class="calibre8">As for the coefficient table, there is one line for each coefficient: <code class="email">const</code> is the weight associated to the intercept term (that is, the bias weight); <code class="email">x1</code>, <code class="email">x2</code>, … <code class="email">x10</code> are the weights associated to the 10 features composing the model. For each of them there are a few values:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">Coef</strong>: This is the weight in the model associated to that feature.</li><li class="listitem"><strong class="calibre2">Std err</strong>: This is the standard error of the coefficient, that is its (predicted) standard deviation (across all observations) divided by the square root of the sample size.</li><li class="listitem"><strong class="calibre2">Z</strong>: This is the ratio between the standard error and the coefficient (it's the stat t-value).</li><li class="listitem"><strong class="calibre2">P&gt;|z|</strong>: This is the probability of obtaining a t-value greater than z, while sampling from the same population.</li><li class="listitem"><strong class="calibre2">[95.0% Conf. Int.]</strong>: This is the interval where, with 95% confidence, the real value of the coefficient is. It is computed as <em class="calibre9">coefficient +/- 1.96 * std err</em>.</li></ul></div><p class="calibre8">An alternate method to obtain the same result (often used when the model contains a small number of features) is to write down the formula involved in the regression. This is possible thanks to the Statsmodel formula API, which makes the fitting operation similar to what you would use in R. We first need to name the features, then we write down the formula (using the names we set), and lastly we fit the model. With this method, the intercept <a id="id366" class="calibre1"/>term is automatically added to the model. Its output, then, is the same as the preceding output:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">import pandas as pd</strong>

<strong class="calibre2">Xd = pd.DataFrame(X)</strong>
<strong class="calibre2">Xd.columns = ['VAR'+str(i+1) for i in range(10)]</strong>
<strong class="calibre2">Xd['response'] = y</strong>

<strong class="calibre2">logistic_regression = smf.logit(formula = </strong>
<strong class="calibre2">     'response ~ VAR1+ VAR2 + VAR3 + VAR4 + \</strong>
<strong class="calibre2">     VAR5 + VAR6 + VAR7 + VAR8 + VAR9 + VAR10', data=Xd)</strong>

<strong class="calibre2">fitted_model = logistic_regression.fit()</strong>
<strong class="calibre2">fitted_model.summary()</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">[same output as above]</strong>
</pre></div><p class="calibre8">Let's change our approach, and let's now fully implement the stochastic gradient descent formula. Each piece of the formula has a function, and the <code class="email">main</code> function is optimization. With respect to the linear regression, here the big difference is the <code class="email">loss</code> function, which is the logistic (that is, the sigmoid):</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.preprocessing import StandardScaler</strong>
<strong class="calibre2">import numpy as np</strong>
<strong class="calibre2">observations = len(X)</strong>
<strong class="calibre2">variables = ['VAR'+str(i+1) for i in range(10)]</strong>
<strong class="calibre2">In:</strong>
<strong class="calibre2">import random</strong>

<strong class="calibre2">def random_w( p ):</strong>
<strong class="calibre2">    return np.array([np.random.normal() for j in range(p)])</strong>

<strong class="calibre2">def sigmoid(X,w):</strong>
<strong class="calibre2">    return 1./(1.+np.exp(-np.dot(X,w)))</strong>

<strong class="calibre2">def hypothesis(X,w):</strong>
<strong class="calibre2">    return np.dot(X,w)</strong>

<strong class="calibre2">def loss(X,w,y):</strong>
<strong class="calibre2">    return hypothesis(X,w) - y</strong>

<strong class="calibre2">def logit_loss(X,w,y):</strong>
<strong class="calibre2">    return sigmoid(X,w) - y</strong>

<strong class="calibre2">def squared_loss(X,w,y):</strong>
<strong class="calibre2">    return loss(X,w,y)**2</strong>

<strong class="calibre2">def gradient(X,w,y,loss_type=squared_loss):</strong>
<strong class="calibre2">    gradients = list()</strong>
<strong class="calibre2">    n = float(len( y ))</strong>
<strong class="calibre2">    for j in range(len(w)):</strong>
<strong class="calibre2">        gradients.append(np.sum(loss_type(X,w,y) * X[:,j]) / n)</strong>
<strong class="calibre2">    return gradients</strong>

<strong class="calibre2">def update(X,w,y, alpha=0.01, loss_type=squared_loss):</strong>
<strong class="calibre2">    return [t - alpha*g for t, g in zip(w, gradient(X,w,y,loss_type))]</strong>

<strong class="calibre2">def optimize(X,y, alpha=0.01, eta = 10**-12, loss_type=squared_loss, iterations = 1000):</strong>
<strong class="calibre2">    standardization = StandardScaler()</strong>
<strong class="calibre2">    Xst = standardization.fit_transform(X)</strong>
<strong class="calibre2">    original_means, originanal_stds = standardization.mean_, standardization.std_</strong>
<strong class="calibre2">    Xst = np.column_stack((Xst,np.ones(observations)))</strong>
<strong class="calibre2">    w = random_w(Xst.shape[1])</strong>
<strong class="calibre2">    path = list()</strong>
<strong class="calibre2">    for k in range(iterations):</strong>
<strong class="calibre2">        SSL = np.sum(squared_loss(Xst,w,y))</strong>
<strong class="calibre2">        new_w = update(Xst,w,y, alpha=alpha, loss_type=logit_loss)</strong>
<strong class="calibre2">        new_SSL = np.sum(squared_loss(Xst,new_w,y))</strong>
<strong class="calibre2">        w = new_w</strong>
<strong class="calibre2">        if k&gt;=5 and (new_SSL - SSL &lt;= eta and new_SSL - SSL &gt;= -eta):</strong>
<strong class="calibre2">            path.append(new_SSL)</strong>
<strong class="calibre2">            break</strong>
<strong class="calibre2">        if k % (iterations / 20) == 0:</strong>
<strong class="calibre2">            path.append(new_SSL)</strong>
<strong class="calibre2">    unstandardized_betas = w[:-1] / originanal_stds</strong>
<strong class="calibre2">    unstandardized_bias = w[-1]-np.sum((original_means / </strong>
<strong class="calibre2">originanal_stds) * w[:-1])</strong>
<strong class="calibre2">return np.insert(unstandardized_betas, 0, unstandardized_bias), </strong>
<strong class="calibre2">path,k</strong>
<strong class="calibre2">                                 </strong>
<strong class="calibre2">alpha = 0.5</strong>
<strong class="calibre2">w, path, iterations = optimize(X, y, alpha, eta = 10**-5, loss_type=logit_loss, iterations = 100000)</strong>
<strong class="calibre2">print ("These are our final standardized coefficients: %s" % w)</strong>
<strong class="calibre2">print ("Reached after %i iterations" % (iterations+1))</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">These are our final standardized coefficients: [ 0.42991407  0.0670771  -0.78279578  0.12208733  0.28410285  0.14689341</strong>
<strong class="calibre2"> -0.34143436  0.05031078 -0.1393206   0.11267402 -0.47916908]</strong>
<strong class="calibre2">Reached after 868 iterations	</strong>
</pre></div><p class="calibre8">The coefficients <a id="id367" class="calibre1"/>produced with the stochastic gradient descent approach are the same as the ones Statsmodels derived previously. The code implementation, as seen before, is not best optimized; though reasonably efficient at working out the solution, it's just an instructive way to understand how SGD works under the hood in the logistic regression task. Try to play around, checking the relation between the number of iterations, alpha, eta, and the final outcome: you'll understand how these parameters are connected, as well as how to select the best settings.</p><p class="calibre8">Finally, we switch to the Scikit-learn library, and its implementation of Logistic Regression. Scikit-learn has two implementations: one based on the <em class="calibre9">classic</em> solution of the logistic regression optimization, and the other one based on a quick SGD implementation. We'll explore them both.</p><p class="calibre8">First, we start with the classic Logistic Regression implementation. The training is really simple, and just requires a couple of parameters. We will set its parameters to the extreme, so the solution is not regularized (C is very high) and the stopping criterion on tolerance is very low. We do that in this example to get the same weights in the model; in a real experiment, these parameters will guide hyperparameter optimization. For more information about regularization, please refer to <a class="calibre1" title="Chapter 6. Achieving Generalization" href="part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6">Chapter 6</a>, <em class="calibre9">Achieving Generalization</em>:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.linear_model import LogisticRegression</strong>

<strong class="calibre2">clf = LogisticRegression(C=1E4, tol=1E-25, random_state=101)</strong>
<strong class="calibre2">clf.fit(X,y)</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">LogisticRegression(C=10000.0, class_weight=None, dual=False,</strong>
<strong class="calibre2">          fit_intercept=True, intercept_scaling=1, max_iter=100,</strong>
<strong class="calibre2">          multi_class='ovr', penalty='l2', random_state=101,</strong>
<strong class="calibre2">          solver='liblinear', tol=1e-25, verbose=0)</strong>
<strong class="calibre2">In:</strong>
<strong class="calibre2">coeffs = [clf.intercept_[0]]</strong>
<strong class="calibre2">coeffs.extend(clf.coef_[0])</strong>
<strong class="calibre2">coeffs</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00087.jpeg" alt="An example" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As the last model, we try the Scikit-learn implementation of the SGD. Getting the same weights is really<a id="id368" class="calibre1"/> tricky, since the model is really complex, and the parameters should be optimized for performance, not for obtaining the same result as for the closed form approach. So, use this example to understand the coefficients in the model, but not for training a real-world model:</p><div><pre class="programlisting">
<strong class="calibre2">In:</strong>
<strong class="calibre2">from sklearn.linear_model import SGDClassifier</strong>

<strong class="calibre2">clf = SGDClassifier(loss='log', alpha=1E-4, n_iter=1E2, random_state=101)</strong>
<strong class="calibre2">clf.fit(X,y)</strong>
<strong class="calibre2">Out:</strong>
<strong class="calibre2">SGDClassifier(alpha=0.0001, average=False, class_weight=None,</strong>
<strong class="calibre2">       epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,</strong>
<strong class="calibre2">       learning_rate='optimal', loss='log', n_iter=100.0,</strong>
<strong class="calibre2">       n_jobs=1, penalty='l2', power_t=0.5, random_state=101,</strong>
<strong class="calibre2">       shuffle=True, verbose=0, warm_start=False)</strong>
<strong class="calibre2">In:</strong>
<strong class="calibre2">coeffs = [clf.intercept_[0]]</strong>
<strong class="calibre2">coeffs.extend(clf.coef_[0])</strong>
<strong class="calibre2">coeffs</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00088.jpeg" alt="An example" class="calibre10"/></div><p class="calibre11"> </p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec28" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">We've seen in this chapter how to build a binary classifier based on Linear Regression and the logistic function. It's fast, small, and very effective, and can be trained using an incremental technique based on SGD. Moreover, with very little effort (the One-vs-Rest approach), the Binary Logistic Regressor can become multiclass.</p><p class="calibre8">In the next chapter, we will focus on how to prepare data: to obtain the maximum from the supervised algorithm, the input dataset must be carefully cleaned and normalized. In fact, real world datasets can have missing data, errors, and outliers, and variables can be categorical and with different ranges of values. Fortunately, some popular algorithms deal with these problems, transforming the dataset in the best way possible for the machine learning algorithm.</p></div></body></html>