<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Feature Transformations</h1>
                </header>
            
            <article>
                
<p class="mce-root">So far, in this text, we have encountered feature engineering tools from what seems like all possible angles of data. From analyzing tabular data in order to ascertain levels of data to constructing and selecting columns using statistical measures in order to optimize our machine learning pipelines, we have been on a remarkable journey of dealing with features in our data.</p>
<p class="mce-root">It is worth mentioning once more that enhancements of machine learning come in many forms. We generally consider our two main metrics as accuracy and prediction/fit times. This means that if we can utilize feature engineering tools to make our pipeline have higher accuracy in a cross-validated setting, or be able to fit and/or predict data quicker, then we may consider that a success. Of course, our ultimate hope is to optimize for both accuracy and time, giving us a much better pipeline to work with.</p>
<p class="mce-root">The past five chapters have dealt with what is considered classical feature engineering. We have looked at five main categories/steps in feature engineering so far:</p>
<ul>
<li><strong>Exploratory data analysis</strong>: In the beginning of our work with machine learning pipelines, before even touching machine learning algorithms or feature engineering tools, it is encouraged to perform some basic descriptive statistics on our datasets and create visualizations to better understand the nature of the data</li>
<li><strong>Feature understanding</strong>: Once we have a sense of the size and shape of the data, we should take a closer look at each of the columns in our dataset (if possible) and outline characteristics, including the level of data, as that will dictate how to clean specific columns if necessary</li>
<li><strong>Feature improvement</strong>: This phase is about altering data values and entire columns by imputing missing values depending on the level of the columns and performing dummy variable transformations and scaling operations if possible</li>
<li><strong>Feature construction</strong>: Once we have the best possible dataset at our disposal, we can think about constructing new columns to account for feature interaction</li>
<li><strong>Feature selection</strong>: In the selection phase of our pipeline, we take all original and constructed columns and perform (usually univariate) statistical tests in order to isolate the best performing columns for the purpose of removing noise and speeding up calculations</li>
</ul>
<p>The following figure sums up this procedure and shows us how to think about each step in the process:</p>
<div class="CDPAlignCenter CDPAlign"><img height="316" src="assets/84d94f72-4d12-4648-a745-0c7b592d5869.png" width="163"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Machine learning pipeline</span></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">This is an example of a machine learning pipeline using methods from earlier in this text. It consists of five main steps: analysis, understanding, improvement, construction, and selection. In the upcoming chapters, we will be focusing on a new method of transforming data that partly breaks away from this classical notion.</p>
<p>At this stage of the book, the reader is more than ready to start tackling the datasets of the world with reasonable confidence and expectations of performance. The following two <a href="8dc49afd-2a3a-4063-9c38-ac6a049bbfe6.xhtml" target="_blank">Chapters 6</a>, <em>Feature Transformations</em>, and <a href="e1c6751c-a892-4cf3-9c54-53e9bb3e1431.xhtml" target="_blank">Chapter 7</a>, <em>Feature Learning</em><em>, </em>will focus on two subsets of feature engineering that are quite heavy in both programming and mathematics, specifically linear algebra. We will, as always, do our best to explain all lines of code used in this chapter and only describe mathematical procedures where necessary.</p>
<p>This chapter will deal with <strong>feature transformations</strong>, a suite of algorithms designed to alter the internal structure of data to produce mathematically superior <em>super-columns, </em>while the following chapter will focus on feature learning using non-parametric algorithms (those that do not depend on the shape of the data) to automatically learn new features. The final chapter of this text contains several worked out case studies to show the end-to-end process of feature engineering and its effects on machine learning pipelines.</p>
<p>For now, let us begin with our discussion of feature transformation. As we mentioned before, feature transformations are a set of matrix algorithms that will structurally alter our data and produce what is essentially a brand new matrix of data. The basic idea is that original features of a dataset are the descriptors/characteristics of data-points and we should be able to create a new set of features that explain the data-points just as well, perhaps even better, with fewer columns.</p>
<p>Imagine a simple, rectangular room. The room is empty except for a single mannequin standing in the center. The mannequin never moves and is always facing the same way. You have been charged with the task of monitoring that room 24/7. Of course, you come up with the idea of adding security cameras to the room to make sure that all activity is captured and recorded. You place a single camera in a top corner of the room, facing down to look at the face of the mannequin and, in the process, catch a large part of the room on camera. With one camera, you are able to see virtually all aspects of the room. The problem is that the camera has blind spots. For example, you won't be able to see directly below the camera (due to its physical inability to see there) and behind the mannequin (as the dummy itself is blocking the camera's view). Being brilliant, you add a second camera to the opposite top corner, behind the mannequin, to compensate for the blind spots of the first camera. Using two cameras, you can now see greater than 99% of the room from a security office.</p>
<p>In this example, the room represents the original feature space of data and the mannequin represents a data-point, standing at a certain section of the feature space. More formally, I'm asking you to consider a three-dimensional feature space with a single data-point:</p>
<p style="padding-left: 180px"><em>[X, Y, Z]</em></p>
<p>To try and capture this data-point with a single camera is like squashing down our dataset to have only one new dimension, namely, the data seen by camera one:</p>
<p style="padding-left: 180px"><em>[X, Y, Z] ≈ [C1]</em></p>
<p>However, only using one dimension likely will not be enough, as we were able to conceive blind spots for that single camera so we added a second camera:</p>
<p style="padding-left: 180px"><em>[X, Y, Z] ≈ [C1, C2]</em></p>
<p>These two cameras (new dimensions produced by feature transformations) capture the data in a new way, but give us enough of the information we needed with only two columns instead of three. The toughest part of feature transformations is the suspension of our belief that the original feature space is the best. We must be open to the fact that there may be other mathematical axes and systems that describe our data just as well with fewer features, or possibly even better.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dimension reduction – feature transformations versus feature selection versus feature construction</h1>
                </header>
            
            <article>
                
<p>In the last section, I mentioned how we could squish datasets to have fewer columns to describe data in new ways. This sounds similar to the concept of feature selection: removing columns from our original dataset to create a different, potentially better, views of our dataset by cutting out the noise and enhancing signal columns. While both feature selection and feature transformation are methods of performing dimension reduction, it is worth mentioning that they could not be more different in their methodologies. </p>
<p>Feature selection processes are limited to only being able to select features from the original set of columns, while feature transformation algorithms use these original columns and combine them in useful ways to create new columns that are better at describing the data than any single column from the original dataset. Therefore, feature selection methods reduce dimensions by isolating signal columns and ignoring noise columns.</p>
<p>Feature transformation methods create new columns using hidden structures in the original datasets to produce an entirely new, structurally different dataset. These algorithms create brand new columns that are so powerful that we only need a few of them to explain our entire dataset accurately.</p>
<p>We also mentioned that feature transformation works by producing new columns that capture the essence (variance) of the data. This is similar to the crux of feature construction: creating new features for the purpose of capturing latent structures in data. Again, we should mention that these two different processes achieve similar results using vastly different methods.</p>
<p>Feature construction is again limited to constructing new columns using simple operations (addition, multiplication, and so on) between a few columns at a time. This implies that any constructed features using classical feature construction are constructed using only a few columns from the original dataset at a time. If our goal is to create enough features to capture all possible feature interactions, that might take an absurd number of additional columns. For example, if given a dataset had 1,000 features or more, we would need to create tens of thousands of columns to construct enough features to capture even a subset of all possible feature interactions.</p>
<p>Feature transformation methods are able to utilize small bits of information from all columns in every new super-column, so we do not need to create an inordinate amount of new columns to capture latent feature interactions. Due to the nature of feature transformation algorithms and its use of matrixes/linear algebra, feature transformation methods never create more columns than we start with, and are still able to extract the latent structure that features construction columns attempt to extract.</p>
<p>Feature transformation algorithms are able to <em>construct</em> new features by <em>selecting</em> the best of all columns and combining this latent structure with a few brand new columns. In this way, we may consider feature transformation as one of the most powerful sets of algorithms that we will discuss in this text. That being said, it is time to introduce our first algorithm and dataset in the book: <strong>Principal Components Analysis</strong> (<strong>PCA</strong>) and the <kbd>iris</kbd> dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Principal Component Analysis</h1>
                </header>
            
            <article>
                
<p>Principal Component Analysis is a technique that takes datasets that have several correlated features and projects them onto a coordinate (axis) system that has fewer correlated features. These new, uncorrelated features (which I referred to before as a super-columns) are called <strong>principal components</strong><em>.</em> The principal components serve as an alternative coordinate system to the original feature space that requires fewer features and captures as much variance as possible. If we refer back to our example with the cameras, the principal components are exemplified by the cameras themselves.</p>
<p>Put another way, the goal of the PCA is to identify patterns and latent structures within datasets in order to create new columns and use these columns instead of the original features. Just as in feature selection, if we start with a data matrix of size <em>n x d</em> where <em>n</em> is the number of observations and <em>d</em> is the number of original features, we are projecting this matrix onto a matrix of size <em>n x k </em>(where <em>k &lt; d</em>).</p>
<p>Our principal components give rise to new columns that maximize the variance in our data. This means that each column is trying to explain the shape of our data. Principal components are ordered by variance explained so that the first principal component does the most to explain the variance of the data, while the second component does the second most to explain the variance of the data. The goal is to utilize as many components as we need in order to optimize the machine learning task, whether it be supervised or unsupervised learning:</p>
<div class="CDPAlignCenter CDPAlign"><img height="301" src="assets/ea40a6ea-891d-4855-b4a2-cc59691284da.png" width="501"/></div>
<p class="packt_figref">Feature transformation is about transforming datasets into matrices with the same number of rows with a reduced number of features. This is similar to the point of feature selection but in this case, we are concerned with the creation of brand new features.</p>
<p>PCA is itself an unsupervised task, meaning that it does not utilize a response column in order to make the projection/transformation. This matters because the second feature transformation algorithm that we will work with will be supervised and will utilize the response variable in order to create super-columns in a different way that optimizes predictive tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How PCA works</h1>
                </header>
            
            <article>
                
<p>A PCA works by invoking a process called the <strong>eigenvalue decomposition</strong> of the covariance of a matrix. The mathematics behind this was first published in the 1930s and involves a bit of multivariable calculus and linear algebra. For the purposes of this text, we will skip over that and get to the good part.</p>
<div class="packt_infobox">PCA may also work on the correlation matrix. You may choose to use the correlation matrix if the features are on a similar scale while covariance matrices are more useful when using different scales. We generally recommend using the covariance matrix with scaled data.</div>
<p>This process happens in four steps:</p>
<ol>
<li>Create the covariance matrix of the dataset</li>
<li>Calculate the eigenvalues of the covariance matrix</li>
<li>Keep the top <em>k</em> eigenvalues (sorted by the descending eigenvalues)</li>
<li>Use the kept eigenvectors to transform new data-points</li>
</ol>
<p>Let's look at an example of this using a dataset called the <kbd>iris</kbd> dataset. In this fairly small dataset, we will take a look at a step by step performance of a PCA followed by the scikit-learn implementation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PCA with the Iris dataset – manual example</h1>
                </header>
            
            <article>
                
<p>The <kbd>iris</kbd> dataset consists of 150 rows and four columns. Each row/observation represents a single flower while the columns/features represent four different quantitative characteristics about the flower. The goal of the dataset is to fit a classifier that attempts to predict one of three types of <kbd>iris</kbd> given the four features. The flower may be considered either a setosa, a virginica, or a versicolor.</p>
<p>This dataset is so common in the field of machine learning instruction, scikit-learn has a built-in module for downloading the dataset:</p>
<ol>
<li>Let's first import the module and then extract the dataset into a variable called <kbd>iris</kbd>:</li>
</ol>
<pre style="padding-left: 60px"># import the Iris dataset from scikit-learn<br/>from sklearn.datasets import load_iris<br/># import our plotting module<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/><br/># load the Iris dataset<br/>iris = load_iris()</pre>
<ol start="2">
<li>Now, let's store the extracted data matrix and response variables into two new variables, <kbd>iris_X</kbd> and <kbd>iris_y</kbd>, respectively:</li>
</ol>
<pre style="padding-left: 60px"># create X and y variables to hold features and response column<br/>iris_X, iris_y = iris.data, iris.target</pre>
<ol start="3">
<li>Let's take a look at the names of the flowers that we are trying to predict:</li>
</ol>
<pre style="padding-left: 60px"># the names of the flower we are trying to predict.<br/>iris.target_names<br/><br/>array(['setosa', 'versicolor', 'virginica'], dtype='|S10')</pre>
<ol start="4">
<li>Along with the names of the flowers, we can also look at the names of the features that we are utilizing to make these predictions:</li>
</ol>
<pre style="padding-left: 60px"># Names of the features<br/>iris.feature_names<br/><br/>['sepal length (cm)',<br/> 'sepal width (cm)',
 'petal length (cm)',
 'petal width (cm)']</pre>
<ol start="5">
<li>To get a sense of what our data looks like, let's write some code that will display the data-points of two of the four features:</li>
</ol>
<pre style="padding-left: 60px"># for labelling<br/>label_dict = {i: k for i, k in enumerate(iris.target_names)}<br/># {0: 'setosa', 1: 'versicolor', 2: 'virginica'}<br/><br/>def plot(X, y, title, x_label, y_label):<br/> ax = plt.subplot(111)<br/> for label,marker,color in zip(<br/> range(3),('^', 's', 'o'),('blue', 'red', 'green')):<br/><br/> plt.scatter(x=X[:,0].real[y == label],<br/> y=X[:,1].real[y == label],<br/> color=color,<br/> alpha=0.5,<br/> label=label_dict[label]<br/> )<br/><br/> plt.xlabel(x_label)<br/> plt.ylabel(y_label)<br/><br/> leg = plt.legend(loc='upper right', fancybox=True)<br/> leg.get_frame().set_alpha(0.5)<br/> plt.title(title)<br/><br/>plot(iris_X, iris_y, "Original Iris Data", "sepal length (cm)", "sepal width (cm)")</pre>
<p class="mce-root">The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/354912f3-dd99-44ba-85b7-2c7c2c9faa88.png"/></div>
<p>Let us now perform a PCA of the <kbd>iris</kbd> dataset in order to obtain our principal components. Recall that this happens in four steps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the covariance matrix of the dataset</h1>
                </header>
            
            <article>
                
<p>To calculate the covariance matrix of <kbd>iris</kbd>, we will first calculate the feature-wise mean vector (for use in the future) and then calculate our covariance matrix using NumPy.</p>
<p>The <span>covariance matrix is a <em>d x d</em> matrix (square matrix with the same number of features as the number of rows and columns) that represents feature interactions between each feature. It is quite similar to a correlation matrix:</span></p>
<pre># Calculate a PCA manually<br/><br/># import numpy<br/>import numpy as np<br/><br/># calculate the mean vector<br/>mean_vector = iris_X.mean(axis=0)<br/>print mean_vector<br/>[ 5.84333333  3.054       3.75866667  1.19866667]
<br/># calculate the covariance matrix<br/>cov_mat = np.cov((iris_X-mean_vector).T)<br/>print cov_mat.shape<br/>(4, 4)</pre>
<p>The variable <kbd>cov_mat</kbd> stores our 4 x 4 covariance matrix.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating the eigenvalues of the covariance matrix</h1>
                </header>
            
            <article>
                
<p>NumPy is a handy function that computes eigenvectors and eigenvalues that we can use in order to get the principal components of our <kbd>iris</kbd> dataset:</p>
<pre># calculate the eigenvectors and eigenvalues of our covariance matrix of the iris dataset<br/>eig_val_cov, eig_vec_cov = np.linalg.eig(cov_mat)<br/><br/># Print the eigen vectors and corresponding eigenvalues<br/># in order of descending eigenvalues<br/>for i in range(len(eig_val_cov)):<br/> eigvec_cov = eig_vec_cov[:,i]<br/> print 'Eigenvector {}: \n{}'.format(i+1, eigvec_cov)<br/> print 'Eigenvalue {} from covariance matrix: {}'.format(i+1, eig_val_cov[i])<br/> print 30 * '-'<br/><br/>Eigenvector 1: <br/>[ 0.36158968 -0.08226889  0.85657211  0.35884393]
Eigenvalue 1 from covariance matrix: 4.22484076832
------------------------------
Eigenvector 2: 
[-0.65653988 -0.72971237  0.1757674   0.07470647]
Eigenvalue 2 from covariance matrix: 0.242243571628
------------------------------
Eigenvector 3: 
[-0.58099728  0.59641809  0.07252408  0.54906091]
Eigenvalue 3 from covariance matrix: 0.0785239080942
------------------------------
Eigenvector 4: 
[ 0.31725455 -0.32409435 -0.47971899  0.75112056]
Eigenvalue 4 from covariance matrix: 0.023683027126
------------------------------</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keeping the top k eigenvalues (sorted by the descending eigenvalues)</h1>
                </header>
            
            <article>
                
<p>Now that we have our four eigenvalues, we will choose the appropriate number of them to keep to consider them principal components. We can choose all four if we wish, but we generally wish to choose a number less than the original number of features. But what is the right number? We could grid search and find the answer using the brute-force method, however, we have another tool in our arsenal, called the<em> </em><strong>scree plot</strong>.</p>
<p>A scree plot is a simple line graph that shows the percentage of total variance explained in the data by each principal component. To build this plot, we will sort the eigenvalues in order of descending value and plot the <em>cumulative</em> variance explained by each component and all components prior. In the case of <kbd>iris</kbd>, we will have four points on our scree plot, one for each principal component. Each component on its own explains a percentage of the total variance captured, and all components, when the percentages are added up, should account for 100% of the total variance in the dataset.</p>
<p>Let's calculate the percentage of variance explained by each eigenvector (principal component) by taking the eigenvalue associated with that eigenvector and dividing it by the sum of all eigenvalues:</p>
<pre># the percentages of the variance captured by each eigenvalue<br/># is equal to the eigenvalue of that components divided by<br/># the sum of all eigen values<br/><br/>explained_variance_ratio = eig_val_cov/eig_val_cov.sum()<br/>explained_variance_ratio<br/><br/>array([ 0.92461621,  0.05301557,  0.01718514,  0.00518309])</pre>
<p>What this is telling us is that our four principal components differ vastly in the amount of variance that they account for. The first principal component, as a single feature/column, is able to account for over 92% of the variance in the data. That is astonishing! This means that this single super-column theoretically can do nearly all of the work of the four original columns.</p>
<p>To visualize our scree plot, let's create a plot with the four principal components on the <em>x</em> axis and the cumulative variance explained on the <em>y</em> axis. For every data-point, the <em>y</em> position will represent the total percentage of variance explained using all principal components up until that one:</p>
<pre># Scree Plot<br/><br/>plt.plot(np.cumsum(explained_variance_ratio))<br/>plt.title('Scree Plot')<br/>plt.xlabel('Principal Component (k)')<br/>plt.ylabel('% of Variance Explained &lt;= k')</pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="317" src="assets/33a082c7-98b2-495e-92ea-26d6c7aff81a.png" width="450"/></div>
<p>This is telling us that the first two components, by themselves, account for nearly 98% of the total variance of the original dataset, meaning that if we only used the first two eigenvectors and used them as our new principal components, then we would be in good shape. We would be able to shrink the size of our dataset by half (from four to two columns) while maintaining integrity in performance and speeding up performance. We will taking a closer look at examples of machine learning to validate these theoretical notions in the upcoming sections.</p>
<div class="packt_infobox">An eigenvalue decomposition will always result in as many eigenvectors as we have features. It is up to us to choose how many principal components we wish to use once they are all calculated. This highlights the fact that PCA, like most other algorithms in this text, is semi-supervised and require some human input.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the kept eigenvectors to transform new data-points</h1>
                </header>
            
            <article>
                
<p>Once we decide to keep two principal components (whether we use a grid search module or the analysis of a scree plot to find the optimal number doesn't matter), we have to be able to use these components to transform incoming, out of sample data-points. To do this, let's first isolate the top two eigenvectors and store them in a new variable called <kbd>top_2_eigenvectors</kbd>:</p>
<pre class="mce-root"># store the top two eigenvectors in a variable<br/><span>top_2_eigenvectors</span> = eig_vec_cov[:,:2].T<br/><br/># show the transpose so that each row is a principal component, we have two rows == two components<br/><span>top_2_eigenvectors</span><br/><br/>array([[ 0.36158968, -0.08226889,  0.85657211,  0.35884393],
       [-0.65653988, -0.72971237,  0.1757674 ,  0.07470647]])</pre>
<p>This array represents the top two eigenvectors:</p>
<ul>
<li><kbd>[ 0.36158968, -0.08226889, 0.85657211, 0.35884393]</kbd></li>
<li>
<p><kbd>[-0.65653988, -0.72971237, 0.1757674 , 0.07470647]]</kbd></p>
</li>
</ul>
<p>With these vectors in place, we can use them to project our data into the new and improved super-dataset by multiplying the two matrices together: <kbd>iris_X</kbd> and <kbd>top_2_eigenvectors</kbd>. The following image shows us how we are going to make sure that the numbers work out:</p>
<div class="CDPAlignCenter CDPAlign"><img height="330" src="assets/d1f93502-76db-48cc-9102-f480ad539c78.png" width="430"/></div>
<p class="packt_figref">The preceding figure shows how to utilize principal components to transform datasets from their original feature spaces to the new coordinate systems. In the case of <kbd>iris</kbd>, we take our original 150 x 4 dataset and multiply it by the transpose of the top two eigenvectors. We utilize the transpose to ensure that the matrix sizes match up. The result is a matrix with the same number of rows but a reduced number of columns. Each row is multiplied by the two principal components.</p>
<p>By multiplying these matrices together, we are <em>projecting</em> our original dataset onto this new space of two dimensions:</p>
<pre class="mce-root"># to transform our data from having shape (150, 4) to (150, 2)<br/># we will multiply the matrices of our data and our eigen vectors together<br/><br/>np.dot(iris_X, top_2_eigenvectors.T)[:5,]<br/><br/>array([[ 2.82713597, -5.64133105],
       [ 2.79595248, -5.14516688],
       [ 2.62152356, -5.17737812],
       [ 2.7649059 , -5.00359942],
       [ 2.78275012, -5.64864829]])</pre>
<p>And that's it. We have transformed our four-dimensional iris data into a new matrix with only two columns. This new matrix may serve in place of the original dataset in our machine learning pipeline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scikit-learn's PCA</h1>
                </header>
            
            <article>
                
<p>As usual, scikit-learn saves the day by implementing this procedure in an easy to use transformer so that we don't have to go through that manual process each time we wish to use this powerful process:</p>
<ol>
<li>We can import it from scikit-learn's decomposition module:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># scikit-learn's version of PCA<br/>from sklearn.decomposition import PCA</pre>
<ol start="2">
<li>To mimic the process we performed with the <kbd>iris</kbd> dataset, let's instantiate a <kbd>PCA</kbd> object with only two components:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># Like any other sklearn module, we first instantiate the class<br/>pca = PCA(n_components=2)</pre>
<ol start="3">
<li>Now, we can fit our PCA to the data:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># fit the PCA to our data<br/>pca.fit(iris_X)</pre>
<ol start="4">
<li>Let's take a look at some of the attributes of the PCA object to see if they match up with what we achieved in our manual process. Let's take a look at the <kbd>components_ attribute</kbd> of our object to see if this matches up without the <kbd>top_2_eigenvectors</kbd> variable:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">pca.components_<br/><br/>array([[ 0.36158968, -0.08226889,  0.85657211,  0.35884393],
       [ 0.65653988,  0.72971237, -0.1757674 , -0.07470647]])<br/><br/># note that the second column is the negative of the manual process<br/># this is because eignevectors can be positive or negative<br/># It should have little to no effect on our machine learning pipelines</pre>
<ol start="5">
<li>Our two components match, almost exactly, our previous variable, <kbd>top_2_eigenvectors</kbd>. We say almost because the second component is actually the negative of the eigenvector we calculated. This is fine because, mathematically, both eigenvectors are 100% valid and still achieve the primary goal of creating uncorrelated columns.</li>
<li>So far, this process is much less painful than what we were doing before. To complete the process, we need to use the transform method of the PCA object to project data onto our new two-dimensional plane:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><span>pca.transform(iris_X)[:5,]</span><br/><br/>array([[-2.68420713,  0.32660731],
       [-2.71539062, -0.16955685],
       [-2.88981954, -0.13734561],
       [-2.7464372 , -0.31112432],
       [-2.72859298,  0.33392456]])<br/><br/><span># sklearn PCA centers the data first while transforming, so these numbers won't match our manual process.</span></pre>
<div class="packt_infobox">Notice that our projected data does not match up with the projected data we got before at all. This is because the scikit-learn version of PCA automatically centers data in the prediction phase, which changes the outcome.</div>
<ol start="7">
<li class="mce-root">We can mimic this by altering a single line in our version to match:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><span># manually centering our data to match scikit-learn's implementation of PCA</span><br/><span>np.dot(iris_X-mean_vector, top_2_eigenvectors.T)[:5,]</span><br/><br/>array([[-2.68420713, -0.32660731],
       [-2.71539062,  0.16955685],
       [-2.88981954,  0.13734561],
       [-2.7464372 ,  0.31112432],
       [-2.72859298, -0.33392456]])</pre>
<ol start="8">
<li>Let's make a quick plot of the projected <kbd>iris</kbd> data and compare what the dataset looks like before and after projecting onto our new coordinate system:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># Plot the original and projected data<br/>plot(iris_X, iris_y, "Original Iris Data", "sepal length (cm)", "sepal width (cm)")<br/>plt.show()<br/>plot(pca.transform(iris_X), iris_y, "Iris: Data projected onto first two PCA components", "PCA1", "PCA2")</pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="453" src="assets/188b6248-8cc6-4c3e-aeec-a99011012f84.png" width="341"/></div>
<p>In our original dataset, we can see the irises in their original feature space along the first two columns. Notice that in our projected space, the flowers are much more separated from one another and also rotated on their axis a bit. It looks like the data clusters are <em>standing upright</em>. This phenomenon is because our principal components are working to capture variance in our data, and it shows in our plots.</p>
<p>We can extract the amount of variance explained by each component as we did in our manual example:</p>
<pre class="mce-root"># percentage of variance in data explained by each component<br/># same as what we calculated earlier<br/>pca.explained_variance_ratio_<br/><br/>array([ 0.92461621,  0.05301557])</pre>
<p>Now, that we can perform all of the basic functions with scikit-learn's PCA, let's use this information to display one of the main benefits of PCA: de-correlating features.</p>
<div class="packt_infobox">By nature, in the <span>eigenvalue decomposition</span> procedure, the resulting principal components are perpendicular to each other, meaning that they are linearly independent of one another.</div>
<p>This is a major benefit because many machine learning models and preprocessing techniques make the assumption that inputted features are independent, and utilizing PCA ensures this for us.</p>
<p>To show this, let's create the correlation matrix of the original <kbd>iris</kbd> dataset and find the average linear correlation coefficient between each of the features. Then, we will do the same for a PCA projected dataset and compare the values. We expect that the average correlation of the projected dataset should be much closer to zero, implying that they are all linearly independent. </p>
<p>Let's begin by calculating the correlation matrix of the original <kbd>iris</kbd> dataset:</p>
<ol>
<li>It will be a 4 x 4 matrix where the values represent the correlation coefficient of every feature versus each other:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># show how pca attempts to eliminate dependence between columns<br/><br/># show the correlation matrix of the original dataset<br/>np.corrcoef(iris_X.T)<br/><br/><br/>array([[ 1.        , -0.10936925,  0.87175416,  0.81795363],
       [-0.10936925,  1.        , -0.4205161 , -0.35654409],
       [ 0.87175416, -0.4205161 ,  1.        ,  0.9627571 ],
       [ 0.81795363, -0.35654409,  0.9627571 ,  1.        ]])</pre>
<ol start="2">
<li>We will then extract all values above the diagonal of 1s to use them to find the average correlation between all of the features:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># correlation coefficients above the diagonal<br/> np.corrcoef(iris_X.T)[[0, 0, 0, 1, 1], [1, 2, 3, 2, 3]]<br/> <br/> <br/> array([-0.10936925, 0.87175416, 0.81795363, -0.4205161 , -0.35654409])</pre>
<ol start="3">
<li class="mce-root">Finally, we will take the mean of this array:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># average correlation of original iris dataset.<br/> np.corrcoef(iris_X.T)[[0, 0, 0, 1, 1], [1, 2, 3, 2, 3]].mean()<br/> <br/> 0.16065567094168495</pre>
<ol start="4">
<li>The average correlation coefficient of the original features is <kbd>.16</kbd>, which is pretty small, but definitely not zero. Now, let's create a full PCA that captures all four principal components:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># capture all four principal components<br/> full_pca = PCA(n_components=4)<br/> <br/> # fit our PCA to the iris dataset<br/> full_pca.fit(iris_X)</pre>
<ol start="5">
<li>Once we've done this, we will use the same method as before and calculate the average correlation coefficient between the new, supposedly linearly independent columns:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"> pca_iris = full_pca.transform(iris_X)<br/> # average correlation of PCAed iris dataset.<br/> np.corrcoef(pca_iris.T)[[0, 0, 0, 1, 1], [1, 2, 3, 2, 3]].mean()<br/> # VERY close to 0 because columns are independent from one another<br/> # This is an important consequence of performing an eigenvalue decomposition<br/> <br/> 7.2640855025557061e-17 # very close to 0</pre>
<p>This shows how data projected onto principal components end up having fewer correlated features, which is helpful in general in machine learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How centering and scaling data affects PCA</h1>
                </header>
            
            <article>
                
<p>As with many of the transformations that we have worked with previously in this text, the scaling of features tends to matter a great deal to the transformations. PCA is no different. Previously, we mentioned that the scikit-learn version of PCA automatically centers data in the prediction phase, but why doesn't it do so at the fitting time? If the scikit-learn PCA module goes through the trouble of centering data in the predict method, why doesn't it do so while calculating the eigenvectors? The hypothesis here is that centering data doesn't affect the principal components. Let's test this:</p>
<ol>
<li>Let's import out <kbd>StandardScaler</kbd> module from scikit-learn and center the <kbd>iris</kbd> dataset:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># import our scaling module<br/> from sklearn.preprocessing import StandardScaler<br/> # center our data, not a full scaling<br/> X_centered = StandardScaler(with_std=False).fit_transform(iris_X)<br/> <br/> X_centered[:5,]<br/> <br/> array([[-0.74333333, 0.446 , -2.35866667, -0.99866667], [-0.94333333, -0.054 , -2.35866667, -0.99866667], [-1.14333333, 0.146 , -2.45866667, -0.99866667], [-1.24333333, 0.046 , -2.25866667, -0.99866667], [-0.84333333, 0.546 , -2.35866667, -0.99866667]])</pre>
<ol start="2">
<li>Let's take a look at the now centered dataset:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># Plot our centered data<br/> plot(X_centered, iris_y, "Iris: Data Centered", "sepal length (cm)", "sepal width (cm)")</pre>
<p>We get the following output for the code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="288" src="assets/4ee17ee2-bd94-4bc9-b58d-cd0ec31ee608.png" width="412"/></div>
<ol start="3">
<li>We can then fit the PCA class that we instanstiated before, with <kbd>n_components</kbd> set to <kbd>2</kbd>, to our centered <kbd>iris</kbd> dataset:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># fit our PCA (with n_components still set to 2) on our centered data<br/> pca.fit(X_centered)</pre>
<ol start="4">
<li>Once this is done, we can call the <kbd>components_ attribute</kbd> of the PCA module and compare the resulting principal components with the PCs that we got with the original <kbd>iris</kbd> dataset:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># same components as before<br/> pca.components_<br/> <br/> array([[ 0.36158968, -0.08226889, 0.85657211, 0.35884393], [ 0.65653988, 0.72971237, -0.1757674 , -0.07470647]])</pre>
<ol start="5">
<li>It seems that the PCs that resulted from the centered data are exactly the same as the PCs that we saw earlier. To clarify this, let's transform the centered data using the PCA module and look at the first five rows and see if they match up with the previously obtained projection:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># same projection when data are centered because PCA does this automatically<br/> pca.transform(X_centered)[:5,]<br/> <br/> array([[-2.68420713, 0.32660731], [-2.71539062, -0.16955685], [-2.88981954, -0.13734561], [-2.7464372 , -0.31112432], [-2.72859298, 0.33392456]])</pre>
<ol start="6">
<li>The rows match up! <span>If we look at the graph of the projected centered data and the explained variance ratios, we will that these also match up:</span></li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># Plot PCA projection of centered data, same as previous PCA projected data<br/> plot(pca.transform(X_centered), iris_y, "Iris: Data projected onto first two PCA components with centered data", "PCA1", "PCA2")<br/> </pre>
<p>We get the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img height="275" src="assets/ef82f7e6-d094-41f5-9679-68ef402b5118.png" width="433"/></div>
<p>For percentage variance, we implement the following: </p>
<pre class="mce-root"> # percentage of variance in data explained by each component<br/> pca.explained_variance_ratio_<br/> <br/> array([ 0.92461621, 0.05301557])</pre>
<p><span>The reason this is happening is because matrices have the same covariance matrix as their centered counterparts. If two matrices have the same covariance matrix, then they will have the same eignenvalue decomposition. This is why the scikit-learn version of PCA doesn't bother to center data while finding the eigenvalues and eigenvectors, because they would have found the same ones regardless of centering, so why add an extra, unnecessary step? </span></p>
<p>Now, let's take a look at what happens to our principal components when we scale data using standard z-score scaling:</p>
<pre class="mce-root"># doing a normal z score scaling<br/> X_scaled = StandardScaler().fit_transform(iris_X)<br/> <br/> # Plot scaled data<br/> plot(X_scaled, iris_y, "Iris: Data Scaled", "sepal length (cm)", "sepal width (cm)")</pre>
<p>We get the output, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="282" src="assets/75c9d3bc-0aa6-46b2-b17b-17142b8738d6.png" width="395"/></div>
<p>It is worth mentioning that at this point, we have plotted the iris data in its original format, centered, and now scaled completely. In each graph, the data-points are exactly the same, but the axes are different. This is expected. Centering and scaling data doesn't change the shape of the data, but it does effect feature interaction for our feature engineering and machine learning pipelines.</p>
<p>Let's fit our PCA module on our newly scaled data and see if our PCs are different:</p>
<pre class="mce-root"># fit our 2-dimensional PCA on our scaled data<br/> pca.fit(X_scaled)<br/> <br/> # different components as cenetered data<br/> pca.components_<br/> <br/> array([[ 0.52237162, -0.26335492, 0.58125401, 0.56561105], [ 0.37231836, 0.92555649, 0.02109478, 0.06541577]])</pre>
<p>These are different components, as before. PCA is scale-invariant, meaning that scale affects the components. Note that when we say scaling, we mean centering<em> </em>and dividing by the standard deviation. Let's project our dataset onto our new components and ensure that the newly projected data is indeed different:</p>
<pre class="mce-root"><span># different projection when data are scaled<br/> pca.transform(X_scaled)[:5,]</span><br/> <br/> array([[-2.26454173, 0.5057039 ], [-2.0864255 , -0.65540473], [-2.36795045, -0.31847731], [-2.30419716, -0.57536771], [-2.38877749, 0.6747674 ]])</pre>
<p class="mce-root">Finally, let's take a look at our explained variance ratios:</p>
<pre class="mce-root"># percentage of variance in data explained by each component<br/> pca.explained_variance_ratio_<br/> <br/> array([ 0.72770452, 0.23030523])</pre>
<p>This is interesting. <span>Scaling our data is usually a good idea when performing feature engineering/machine learning and u</span>sually we recommend it to our readers, but why does our first component have a much lower explained variance ratio than it did before?</p>
<p>It's because once we scaled our data, the columns' covariance with one another became more consistent and the variance explained by each principal component was spread out instead of being solidified in a single PC. In practice and production, we generally recommend scaling, but it is a good idea to test your pipeline's performance on both scaled and un-scaled data.</p>
<p>Let's top off this section with a look at the projected iris data <span>on</span> our scaled data:</p>
<pre class="mce-root"># Plot PCA projection of scaled data<br/> plot(pca.transform(X_scaled), iris_y, "Iris: Data projected onto first two PCA components", "PCA1", "PCA2")</pre>
<p class="mce-root">We get the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img height="238" src="assets/4566df9f-079e-4813-9bcd-7d20710b4f9d.png" width="333"/></div>
<p>It is subtle, but if you look at this graph and compare it to the previous plots of projected data under the original and centered data, you will notice slight differences between them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A deeper look into the principal components</h1>
                </header>
            
            <article>
                
<p><span>Before we take a look at our second feature transformation algorithm, it is important to take a look at how principal components are interpreted:</span></p>
<ol>
<li><span>Our <kbd>iris</kbd> dataset is a</span> 150 x 4 <span>matrix, and when we calculated our PCA components when <kbd>n_components</kbd> was set to <kbd>2</kbd>, we obtained a components matrix of size <kbd>2 x 4</kbd>:</span></li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><span># how to interpret and use components</span><br/> <span>pca.components_ # a 2 x 4 matrix</span><br/> <br/> array([[ 0.52237162, -0.26335492, 0.58125401, 0.56561105], [ 0.37231836, 0.92555649, 0.02109478, 0.06541577]])</pre>
<ol start="2">
<li>Just like in our manual example of calculating eigenvectors, the <kbd>components_</kbd> attribute can be used to project data using matrix multiplication. We do so by multiplying our original dataset with the transpose of the <kbd>components_ matrix</kbd>:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><span># Multiply original matrix (150 x 4) by components transposed (4 x 2) to get new columns (150 x 2)</span><br/> <span>np.dot(X_scaled, pca.components_.T)[:5,]</span><br/> <br/> array([[-2.26454173, 0.5057039 ], [-2.0864255 , -0.65540473], [-2.36795045, -0.31847731], [-2.30419716, -0.57536771], [-2.38877749, 0.6747674 ]])</pre>
<ol start="3">
<li>We invoke the transpose function here so that the matrix dimensions match up. What is happening at a low level is that for every row, we are calculating the dot product between the original row and each of the principal components. The results of the dot product become the elements of the new row:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># extract the first row of our scaled data<br/> first_scaled_flower = X_scaled[0]<br/> <br/> # extract the two PC's<br/> first_Pc = pca.components_[0]<br/> second_Pc = pca.components_[1]<br/> <br/> first_scaled_flower.shape # (4,)<br/> print first_scaled_flower # array([-0.90068117, 1.03205722, -1.3412724 , -1.31297673])<br/> <br/> <br/> # same result as the first row of our matrix multiplication<br/> np.dot(first_scaled_flower, first_Pc), np.dot(first_scaled_flower, second_Pc)<br/> <br/> (-2.2645417283949003, 0.50570390277378274)</pre>
<ol start="4">
<li>Luckily, we can rely on the built-in transform method to do this work for us:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># This is how the transform method works in pca<br/> pca.transform(X_scaled)[:5,]<br/> <br/> array([[-2.26454173, 0.5057039 ], [-2.0864255 , -0.65540473], [-2.36795045, -0.31847731], [-2.30419716, -0.57536771], [-2.38877749, 0.6747674 ]])</pre>
<p>Put another way, we can interpret each component as being a<em> </em>combination of the original columns. In this case, our first principal component is:</p>
<pre>[ 0.52237162, -0.26335492, 0.58125401, 0.56561105]</pre>
<p>The first scaled flower is:</p>
<pre>[-0.90068117, 1.03205722, -1.3412724 , -1.31297673]</pre>
<p>To get the first element of the first row of our projected data, we can use the following formula:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" height="23" src="assets/b58fbd99-ad4b-48bb-b7c1-2b599036fc5c.png" width="1194"/></div>
<p>In fact, in general, for any flower with the coordinates (a, b, c, d), where a is the sepal length of the iris, b the sepal width, c the petal length, and d the petal width (this order was taken from <kbd>iris.feature_names</kbd> from before), the first value of the new coordinate system can be calculated by the following:</p>
<p style="padding-left: 60px"><img class="fm-editor-equation" height="18" src="assets/cf9df89f-785d-4c53-bb39-8deb95e36aba.png" width="486"/></p>
<p>Let's take this a step further and visualize the components in space alongside our data. We will truncate our original data to only keep two of its original features, sepal length and sepal width. The reason we are doing this is so that we can visualize the data easier without having to worry about four dimensions:</p>
<pre class="mce-root"># cut out last two columns of the original iris dataset<br/> iris_2_dim = iris_X[:,2:4]<br/> <br/> # center the data<br/> iris_2_dim = iris_2_dim - iris_2_dim.mean(axis=0)<br/> <br/> plot(iris_2_dim, iris_y, "Iris: Only 2 dimensions", "sepal length", "sepal width")</pre>
<p class="mce-root">We get the output, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="274" src="assets/0d09ec40-d4da-42b9-9a18-9df50d0e0677.png" width="392"/></div>
<p>We can see a cluster of flowers (<strong>setosas</strong>) on the bottom left and a larger cluster of both <strong>versicolor</strong> and <strong>virginicia</strong> flowers on the top right. It appears obvious right away that the data, as a whole, is stretched along a diagonal line stemming from the bottom left to the top right. The hope is that our principal components also pick up on this and rearrange our data accordingly.</p>
<p>Let's instantiate a PCA class that keeps two principal components and then use that class to transform our truncated <strong>iris data</strong> into new columns:</p>
<pre class="mce-root"># instantiate a PCA of 2 components<br/> twodim_pca = PCA(n_components=2)<br/> <br/> # fit and transform our truncated iris data<br/> iris_2_dim_transformed = twodim_pca.fit_transform(iris_2_dim)<br/> <br/> plot(iris_2_dim_transformed, iris_y, "Iris: PCA performed on only 2 dimensions", "PCA1", "PCA2")</pre>
<p class="mce-root">We get the output, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ae0c4b07-f56d-4e03-aafc-8479da51f616.png"/></div>
<p><strong>PCA 1</strong>, our first principal component, should be carrying the majority of the variance within it, which is why the projected data is spread out mostly across the new <em>x</em> axis. Notice how the scale of the <em>x</em> axis is between -3 and 3 while the <em>y</em> axis is only between -0.4 and 0.6. To further clarify this, the following code block will graph both the original and projected iris scatter plots, as well as an overlay the principal components of <kbd>twodim_pca</kbd> <span>on top of them, </span>in both the original coordinate system as well as the new coordinate system.</p>
<p>The goal is to interpret the components as being guiding vectors, showing the way in which the data is moving and showing how these guiding vectors become perpendicular coordinate systems:</p>
<pre class="mce-root"># This code is graphing both the original iris data and the projected version of it using PCA.<br/> # Moreover, on each graph, the principal components are graphed as vectors on the data themselves<br/> # The longer of the arrows is meant to describe the first principal component and<br/> # the shorter of the arrows describes the second principal component<br/> def draw_vector(v0, v1, ax):<br/> arrowprops=dict(arrowstyle='-&gt;',linewidth=2,<br/> shrinkA=0, shrinkB=0)<br/> ax.annotate('', v1, v0, arrowprops=arrowprops)<br/> <br/> fig, ax = plt.subplots(2, 1, figsize=(10, 10))<br/> fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)<br/> <br/> # plot data<br/> ax[0].scatter(iris_2_dim[:, 0], iris_2_dim[:, 1], alpha=0.2)<br/> for length, vector in zip(twodim_pca.explained_variance_, twodim_pca.components_):<br/> v = vector * np.sqrt(length) # elongdate vector to match up to explained_variance<br/> draw_vector(twodim_pca.mean_,<br/> twodim_pca.mean_ + v, ax=ax[0])<br/> ax[0].set(xlabel='x', ylabel='y', title='Original Iris Dataset',<br/> xlim=(-3, 3), ylim=(-2, 2))<br/> <br/> <br/> ax[1].scatter(iris_2_dim_transformed[:, 0], iris_2_dim_transformed[:, 1], alpha=0.2)<br/> for length, vector in zip(twodim_pca.explained_variance_, twodim_pca.components_):<br/> transformed_component = twodim_pca.transform([vector])[0] # transform components to new coordinate system<br/> v = transformed_component * np.sqrt(length) # elongdate vector to match up to explained_variance<br/> draw_vector(iris_2_dim_transformed.mean(axis=0),<br/> iris_2_dim_transformed.mean(axis=0) + v, ax=ax[1])<br/> ax[1].set(xlabel='component 1', ylabel='component 2',<br/> title='Projected Data',<br/> xlim=(-3, 3), ylim=(-1, 1))<br/> </pre>
<p class="mce-root">This is the <strong>Original Iris Dataset</strong> and <strong>Projected Data</strong> using PCA:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b7706a4e-311a-4d62-b81b-852c2153ed34.png"/></div>
<p>The top graph is showing the principal components as they exist in the original data's axis system. They are not perpendicular and they are pointing in the direction that the data naturally follows. We can see that the longer of the two vectors, the first principal component, is clearly following that diagonal direction that the iris data is following the most.</p>
<p>The secondary principal component is pointing in a direction of variance that explains a portion of the shape of the data, but not all of it. The bottom graph shows the projected iris data onto these new components accompanied by the same components, but acting as perpendicular coordinate systems. They have become the new <em>x</em> and <em>y</em> axes.</p>
<p>The PCA is a feature transformation tool that allows us to construct brand new super-features as linear combinations of previous features. We have seen that these components carry the maximum amount of variance within them, and act as new coordinate systems for our data. Our next feature transformation algorithm is similar in that it, too, will extract components from our data, but it does so in a machine learning-type manner.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear Discriminant Analysis</h1>
                </header>
            
            <article>
                
<p><span><strong>Linear Discriminant Analysis</strong> (<strong>LDA</strong>) is a feature transformation technique as well as a supervised classifier. It is commonly used as a preprocessing step for classification pipelines. The goal of LDA, like PCA, is to extract a new coordinate system and project datasets onto a lower-dimensional space. The main difference between LDA and PCA is that instead of focusing on the variance of the data as a whole like PCA, LDA optimizes the lower-dimensional space for the best class separability. This means that the new coordinate system is more useful in finding decision boundaries for classification models, which is perfect for us when building classification pipelines.</span></p>
<div class="packt_infobox">The reason that LDA is extremely useful is that separating based on class separability helps us avoid overfitting in our machine learning pipelines. This is also known as <em>preventing the curse of dimensionality</em>. LDA also reduces computational costs.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How LDA works</h1>
                </header>
            
            <article>
                
<p>LDA works as a dimensionality reduction tool, just like PCA, however instead of calculating the eigenvalues of the covariance matrix of the data as a whole, LDA calculates eigenvalues and eigenvectors of within-class and between-class <span>scatter matrices</span>. Performing LDA can be broken down into five steps:</p>
<ol>
<li>
<p>Calculate mean vectors of each class</p>
</li>
<li>
<p>Calculate within-class and between-class scatter matrices</p>
</li>
<li>
<p> Calculate eigenvalues and eigenvectors for <img class="fm-editor-equation" height="25" src="assets/5f94c65e-d2c4-4dfe-99a3-7651808d28cc.png" width="54"/></p>
</li>
<li>
<p>Keep the top k eigenvectors by ordering them by descending eigenvalues</p>
</li>
<li>
<p>Use the top eigenvectors to project onto the new space</p>
</li>
</ol>
<p>Let's look at an example.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating the mean vectors of each class</h1>
                </header>
            
            <article>
                
<p>First, we need to calculate a column-wise mean vector for each of our classes. One for <kbd>setosa</kbd>, one for <kbd>versicolor</kbd>, and another for <kbd>virginica</kbd>:</p>
<pre class="mce-root"># calculate the mean for each class<br/> # to do this we will separate the iris dataset into three dataframes<br/> # one for each flower, then we will take one's mean columnwise<br/> mean_vectors = []<br/> for cl in [0, 1, 2]:<br/> class_mean_vector = np.mean(iris_X[iris_y==cl], axis=0)<br/> mean_vectors.append(class_mean_vector)<br/> print label_dict[cl], class_mean_vector<br/> <br/> setosa [ 5.006 3.418 1.464 0.244]<br/> versicolor [ 5.936 2.77 4.26 1.326]<br/> virginica [ 6.588 2.974 5.552 2.026]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating within-class and between-class scatter matrices</h1>
                </header>
            
            <article>
                
<p>We will now calculate a <strong>within-class</strong> scatter matrix, defined by the following: </p>
<p style="padding-left: 180px"><img class="fm-editor-equation" height="53" src="assets/df75dfcc-4a6c-404a-9943-42dfacfee667.png" width="98"/></p>
<p><span>Where we define <em>S<sub>i</sub></em> as:</span></p>
<p style="padding-left: 180px"><img class="fm-editor-equation" height="52" src="assets/535dfd0c-a501-43e8-a657-a23ac23c137c.png" width="219"/></p>
<p>Here, <em>m</em><sub><em>i</em> </sub>represents the mean vector for the <em>i</em> class, and a <strong>between-class scatter </strong>matrix defined by the following:</p>
<p style="padding-left: 180px"><img class="fm-editor-equation" height="57" src="assets/c2f68b67-ceba-4e44-8cc7-ea5b743fdf4a.png" width="282"/></p>
<p><em><span class="MathJax"><span class="MJX_Assistive_MathML">m</span></span></em><span> </span>is the overall mean of the dataset,<span> <em>m<sub>i</sub></em></span><span> is the sample mean for each class, </span>and<span> </span><em><span class="MathJax"><span class="math"><span class="mrow"><span class="msubsup"><span class="mi">N</span><sub><span class="texatom"><span class="mi">i</span></span></sub></span></span></span></span></em><span> is the sample size</span> for each class (number of observations per class):</p>
<pre class="mce-root"># Calculate within-class scatter matrix<br/> S_W = np.zeros((4,4))<br/> # for each flower<br/> for cl,mv in zip([0, 1, 2], mean_vectors):<br/> # scatter matrix for every class, starts with all 0's<br/> class_sc_mat = np.zeros((4,4))<br/> # for each row that describes the specific flower<br/> for row in iris_X[iris_y == cl]:<br/> # make column vectors<br/> row, mv = row.reshape(4,1), mv.reshape(4,1)<br/> # this is a 4x4 matrix<br/> class_sc_mat += (row-mv).dot((row-mv).T)<br/> # sum class scatter matrices<br/> S_W += class_sc_mat<br/> <br/> S_W<br/> <br/> array([[ 38.9562, 13.683 , 24.614 , 5.6556], [ 13.683 , 17.035 , 8.12 , 4.9132], [ 24.614 , 8.12 , 27.22 , 6.2536], [ 5.6556, 4.9132, 6.2536, 6.1756]])<br/> <br/> # calculate the between-class scatter matrix<br/> <br/> # mean of entire dataset<br/> overall_mean = np.mean(iris_X, axis=0).reshape(4,1)<br/> <br/> # will eventually become between class scatter matrix<br/> S_B = np.zeros((4,4))<br/> for i,mean_vec in enumerate(mean_vectors):<br/> # number of flowers in each species<br/> n = iris_X[iris_y==i,:].shape[0]<br/> # make column vector for each specied<br/> mean_vec = mean_vec.reshape(4,1)<br/> S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)<br/> <br/> S_B<br/> <br/> array([[ 63.2121, -19.534 , 165.1647, 71.3631], [ -19.534 , 10.9776, -56.0552, -22.4924], [ 165.1647, -56.0552, 436.6437, 186.9081], [ 71.3631, -22.4924, 186.9081, 80.6041]])</pre>
<div class="packt_tip">Within-class and between-class scatter matrices are generalizations of a step in the ANOVA test (mentioned in the previous chapter). The idea here is to decompose our iris dataset into two distinct parts.</div>
<p>Once we have calculated these matrices, we can move onto the next step, which uses matrix algebra to extract linear discriminants.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"> Calculating eigenvalues and eigenvectors for SW-1SB </h1>
                </header>
            
            <article>
                
<p>Just as we did in PCA, we rely on eigenvalue decompositions of a specific matrix. In the case of LDA, we will be decomposing the matrix <strong><img class="fm-editor-equation" height="25" src="assets/f9376114-0b5d-4325-b3af-b5f0bb83ac01.png" width="54"/></strong>:</p>
<pre class="mce-root"># calculate eigenvalues and eigenvectors of S−1W x SB<br/> eig_vals, eig_vecs = np.linalg.eig(np.dot(np.linalg.inv(S_W), S_B))<br/> eig_vecs = eig_vecs.real<br/> eig_vals = eig_vals.real<br/> <br/> for i in range(len(eig_vals)):<br/> eigvec_sc = eig_vecs[:,i]<br/> print 'Eigenvector {}: {}'.format(i+1, eigvec_sc)<br/> print 'Eigenvalue {:}: {}'.format(i+1, eig_vals[i])<br/> print<br/> <br/> Eigenvector 1: [-0.2049 -0.3871 0.5465 0.7138]<br/> Eigenvalue 1: 32.2719577997 Eigenvector 2: [ 0.009 0.589 -0.2543 0.767 ] Eigenvalue 2: 0.27756686384 Eigenvector 3: [ 0.2771 -0.3863 -0.4388 0.6644] Eigenvalue 3: -6.73276389619e-16 . # basically 0 Eigenvector 4: [ 0.2771 -0.3863 -0.4388 0.6644] Eigenvalue 4: -6.73276389619e-16 . # basically 0</pre>
<p>Note that the third and fourth eigenvalues are basically zero. This is because the way LDA is trying to work is by drawing decision boundaries between our classes. Because we only have three classes in the iris, we may only draw up to two decision boundaries. In general, fitting LDA to a dataset with n classes will only produce up to n-1 components.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keeping the top k eigenvectors by ordering them by descending eigenvalues</h1>
                </header>
            
            <article>
                
<p>As in PCA, we only wish to keep the eigenvectors that are doing most of the work:</p>
<pre class="mce-root"># keep the top two linear discriminants<br/> linear_discriminants = eig_vecs.T[:2]<br/> <br/> linear_discriminants<br/> <br/> array([[-0.2049, -0.3871, 0.5465, 0.7138], [ 0.009 , 0.589 , -0.2543, 0.767 ]])</pre>
<p>We can look at the ratio of explained variance in each component/linear discriminant by dividing each eigenvalue by the sum total of all eigenvalues:</p>
<pre class="mce-root">#explained variance ratios<br/> eig_vals / eig_vals.sum()<br/> <br/> array([ .99147, .0085275, -2.0685e-17, -2.0685e-17])</pre>
<p>It appears that the first component is doing a vast majority of the work and holding over 99% of the information on its own. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the top eigenvectors to project onto the new space</h1>
                </header>
            
            <article>
                
<p>Now that we have our components, let's plot the projected iris data by first using the eigenvectors to project the original data onto the new space and then calling our plot function:</p>
<pre class="mce-root"># LDA projected data<br/> lda_iris_projection = np.dot(iris_X, linear_discriminants.T)<br/> lda_iris_projection[:5,]<br/> <br/> plot(lda_iris_projection, iris_y, "LDA Projection", "LDA1", "LDA2")</pre>
<p class="mce-root">We get the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6860bc3a-3cd5-4604-95d1-175c6e72de30.png"/></div>
<p><span>Notice that in this graph, the data is <em>standing</em> almost fully upright (even more than PCA projected data), as if the LDA components are trying to help machine learning models separate the flowers as much as possible by drawing these decision boundaries and providing eigenvectors/linear discriminants. This helps us project data into a space that separates classes as much as possible.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to use LDA in scikit-learn</h1>
                </header>
            
            <article>
                
<p>LDA has an implementation in scikit-learn to avoid this very laborious process. It is easily imported:</p>
<pre class="mce-root">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis</pre>
<p>From there, let's use it to fit and transform our original iris data and plot the resulting projected dataset so that we may compare it to the projection using PCA. The biggest thing to notice in the following code block is that the fit function requires two inputs.</p>
<p>Recall how we mentioned that LDA is actually a classifier disguised as a feature transformation algorithm. Unlike PCA, which finds components in an unsupervised manner (without a response variable), LDA will attempt to find the best coordinate system <em>with respect to</em> a response variable that optimizes for class separability. This implies that LDA only works if we have a response variable. If we do, we input the response as a second input to our fit method and let LDA do its thing:</p>
<pre class="mce-root"># instantiate the LDA module<br/> lda = LinearDiscriminantAnalysis(n_components=2)<br/> <br/> # fit and transform our original iris data<br/> X_lda_iris = lda.fit_transform(iris_X, <strong>iris_y</strong>)<br/> <br/> # plot the projected data<br/> plot(X_lda_iris, iris_y, "LDA Projection", "LDA1", "LDA2")</pre>
<p class="mce-root">We get the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img height="284" src="assets/66fb18f8-cbba-4bf9-a562-31c5492fc672.png" width="396"/></div>
<p>This graph is a mirror image of the manual LDA that we performed. This is OK. Recall in PCA that the manual version we had contained eigenvectors that had opposite signs (positive versus negative). This does not affect our machine learning pipelines. <span>Within the LDA module, we have some differences to keep note of. Instead of</span> a <kbd>.components_ attribute</kbd><span>, we have a <kbd>.scalings_ attribute</kbd>, which acts almost the same:</span></p>
<pre class="mce-root"># essentially the same as pca.components_, but transposed (4x2 instead of 2x4)<br/> lda.scalings_<br/> <br/> <br/> array([[ 0.81926852, 0.03285975], [ 1.5478732 , 2.15471106], [-2.18494056, -0.93024679], [-2.85385002, 2.8060046 ]])<br/> <br/> # same as manual calculations<br/> lda.explained_variance_ratio_<br/> array([ 0.9915, 0.0085])</pre>
<p>The explained variance ratios for the two linear discriminants are exactly the same as the ones we calculated before and notice that they omit the third and fourth eigenvalues because they are virtually zero.</p>
<p>These components, however, at first glance, look nothing like the manual eigenvectors that we achieved before. The reason for this is that the way that scikit-learn calculates the eigenvectors produces the same eigenvectors, but scaled by a scalar, as follows:</p>
<pre class="mce-root"># show that the sklearn components are just a scalar multiplication from the manual components we calculateda<br/> for manual_component, sklearn_component in zip(eig_vecs.T[:2], lda.scalings_.T):<br/> print sklearn_component / manual_component<br/> <br/> [-3.9982 -3.9982 -3.9982 -3.9982] [ 3.6583 3.6583 3.6583 3.6583]</pre>
<p>The scikit-learn linear discriminants are a scalar multiplication of the manual eigenvectors, which means that they are both valid eigenvectors. The only difference is in the scaling of the projected data.</p>
<p>These components are organized as a 4 x 2 matrix, instead of the PCA components, which was given to us as a 2 x 4 matrix. This was a choice when developing the module and doesn't affect the math at all for us. LDA, like PCA, scales invariant, so scaling the data matters.</p>
<p>Let's fit the LDA module to scaled iris data and look at the components to see the difference:</p>
<pre class="mce-root"># fit our LDA to scaled data<br/> X_lda_iris = lda.fit_transform(X_scaled, iris_y)<br/> <br/> lda.scalings_ # different scalings when data are scaled<br/> <br/> array([[ 0.67614337, 0.0271192 ], [ 0.66890811, 0.93115101], [-3.84228173, -1.63586613], [-2.17067434, 2.13428251]])</pre>
<p>The <kbd>scalings_ attribute</kbd> (akin to PCA's <kbd>components_ attribute</kbd>) is showing us different arrays, which means that the projection will also be different. To finish our (briefer) description of LDA, let's apply the same code block that we did with PCA and interpret the <kbd>scalings_ arrays</kbd> as we did with the <kbd>components_ attribute</kbd> of PCA.</p>
<p>Let's first fit and transform LDA on our truncated iris dataset, where we have only kept the first two features:</p>
<pre class="mce-root"># fit our LDA to our truncated iris dataset<br/> iris_2_dim_transformed_lda = lda.fit_transform(iris_2_dim, iris_y)</pre>
<p class="mce-root">Let's take a look at the first five rows of our projected dataset:</p>
<pre class="mce-root"># project data<br/> iris_2_dim_transformed_lda[:5,]<br/> <br/> array([[-6.04248571, 0.07027756], [-6.04248571, 0.07027756], [-6.19690803, 0.28598813], [-5.88806338, -0.14543302], [-6.04248571, 0.07027756]])</pre>
<p>Our <kbd>scalings_ matrix</kbd> is now a 2 x 2 matrix (2 rows and 2 columns) where the columns are the components (instead of the rows being components in PCA). To adjust for this, let's make a new variable called components that holds the transposed version of the <kbd>scalings_ attribute</kbd>:</p>
<pre class="mce-root"># different notation<br/> components = lda.scalings_.T # transposing gives same style as PCA. We want rows to be components<br/> <br/> print components<br/> [[ 1.54422328 2.40338224] [-2.15710573 5.02431491]]<br/> <br/> <br/> np.dot(iris_2_dim, components.T)[:5,] # same as transform method<br/> <br/> <br/> array([[-6.04248571, 0.07027756], [-6.04248571, 0.07027756], [-6.19690803, 0.28598813], [-5.88806338, -0.14543302], [-6.04248571, 0.07027756]])</pre>
<p>We can see that se uses the components variable in the same way that we did the PCA <kbd>components_ attribute</kbd>. This implies that the projection is another linear combination of original columns, just as they were in PCA. It is also worth noting that LDA still de-correlates features, just as PCA did. To show this, let us calculate the correlation coefficient matrix of both the original truncated iris data and the correlation matrix of the projected data:</p>
<pre class="mce-root"><span># original features are highly correlated<br/></span><span>np.corrcoef(iris_2_dim.T)</span><br/> array([[ 1. , <strong>0.9627571</strong>],<br/> [ 0.9627571, 1. ]])<br/> <br/> # new LDA features are highly uncorrelated, like in PCA<br/> np.corrcoef(iris_2_dim_transformed_lda.T)<br/> array([[ 1.00000000e+00, <strong>1.03227536e-15</strong>], [ 1.03227536e-15, 1.00000000e+00]])</pre>
<p>Note that in the top right value in each matrix, the original matrix is showing <em>highly</em> correlated features, while the projected data using LDA has highly independent features (given the close to zero correlation coefficient). To wrap up our interpretation of LDA before we move onto the real fun (using both PCA and LDA for machine learning), let's take a look at a visualization of the <kbd>scalings_ attribute</kbd> of LDA, just as we did for PCA:</p>
<pre class="mce-root"># This code is graphing both the original iris data and the projected version of it using LDA.<br/> # Moreover, on each graph, the scalings of the LDA are graphed as vectors on the data themselves<br/> # The longer of the arrows is meant to describe the first scaling vector and<br/> # the shorter of the arrows describes the second scaling vector<br/> def draw_vector(v0, v1, ax):<br/> arrowprops=dict(arrowstyle='-&gt;',<br/> linewidth=2,<br/> shrinkA=0, shrinkB=0)<br/> ax.annotate('', v1, v0, arrowprops=arrowprops)<br/> <br/> fig, ax = plt.subplots(2, 1, figsize=(10, 10))<br/> fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)<br/> <br/> # plot data<br/> ax[0].scatter(iris_2_dim[:, 0], iris_2_dim[:, 1], alpha=0.2)<br/> for length, vector in zip(lda.explained_variance_ratio_, components):<br/> v = vector * .5<br/> draw_vector(lda.xbar_, lda.xbar_ + v, ax=ax[0]) # lda.xbar_ is equivalent to pca.mean_<br/> ax[0].axis('equal')<br/> ax[0].set(xlabel='x', ylabel='y', title='Original Iris Dataset',<br/> xlim=(-3, 3), ylim=(-3, 3))<br/> <br/> ax[1].scatter(iris_2_dim_transformed_lda[:, 0], iris_2_dim_transformed_lda[:, 1], alpha=0.2)<br/> for length, vector in zip(lda.explained_variance_ratio_, components):<br/> transformed_component = lda.transform([vector])[0]<br/> v = transformed_component * .1<br/> draw_vector(iris_2_dim_transformed_lda.mean(axis=0), iris_2_dim_transformed_lda.mean(axis=0) + v, ax=ax[1])<br/> ax[1].axis('equal')<br/> ax[1].set(xlabel='lda component 1', ylabel='lda component 2',<br/> title='Linear Discriminant Analysis Projected Data',<br/> xlim=(-10, 10), ylim=(-3, 3))</pre>
<p class="mce-root">We get the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img height="393" src="assets/52940eb2-fb8c-4065-8dac-fca01817fe43.png" width="457"/></div>
<div class="mce-root CDPAlignLeft CDPAlign packt_infobox">Notice how the component, instead of going with the variance of the data, goes almost perpendicular to it; it's following the separation of the classes instead. Also, note how it's almost parallel with the gap between the flowers on the left and right side. LDA is trying to capture the separation between classes</div>
<p>In the top graph, we can see the original iris dataset with the <kbd>scalings_ vectors</kbd> overlaid on top of the data-points. The longer vector is pointing almost parallel to the large gap between the setosas on the bottom left and the rest of the flowers on the top right. This is indicative that the LDA is trying to point out the best directions to look in to separate the classes of flowers in the original coordinate system.</p>
<p>It is important to note here that the <kbd>scalings_ attribute</kbd> of LDA does not correlate <span>1:1</span> to the new coordinate system as it did in PCA. This is because the goal of <kbd>scalings_</kbd> is not to create a new coordinate system, but just to point in the direction of boundaries in the data that optimizes for class separability. We will not go into detail about the calculation of these new coordinate systems as we did with PCA. It is sufficient to understand that the main difference between PCA and LDA is that PCA is an unsupervised method that captures the variance of the data as a whole whereas LDA, a supervised method, uses the response variable to capture class separability.</p>
<div class="packt_tip">Limitations of supervised feature transformations like LDA mean that they cannot help with tasks such as clustering, whereas PCA can help. This is because clustering is an unsupervised task and does not have a response variable for LDA to use.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LDA versus PCA – iris dataset</h1>
                </header>
            
            <article>
                
<p>Finally, we arrive at the moment where we can try using both PCA and LDA in our machine learning pipelines. Because we have been working with the <kbd>iris</kbd> dataset extensively in this chapter, we will continue to demonstrate the utility of both LDA and PCA as feature transformational pre-processing steps for supervised and unsupervised machine learning.</p>
<p>We will start with supervised machine learning and attempt to build a classifier to recognize the species of flower given the four quantitative flower traits:</p>
<ol>
<li>We begin by importing three modules from scikit-learn:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.model_selection import cross_val_score</pre>
<p>We will use KNN as our supervised model and the pipeline module to combine our KNN model with our feature transformation tools to create machine learning pipelines that can be cross-validated using the <kbd>cross_val_score</kbd> module. We will try a few different machine learning pipelines and record their performance:</p>
<ol start="2">
<li>Let's begin by creating three new variables, one to hold our LDA, one to hold our PCA, and another to hold a KNN model:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># Create a PCA module to keep a single component<br/> single_pca = PCA(n_components=1)<br/> <br/> # Create a LDA module to keep a single component<br/> single_lda = LinearDiscriminantAnalysis(n_components=1)<br/> <br/> # Instantiate a KNN model<br/> knn = KNeighborsClassifier(n_neighbors=3)</pre>
<ol start="3">
<li>Let's invoke the KNN model without any transformational techniques to get the baseline accuracy. We will use this to compare the two feature transformation algorithms:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># run a cross validation on the KNN without any feature transformation<br/> knn_average = cross_val_score(knn, iris_X, iris_y).mean()<br/> <br/> # This is a baseline accuracy. If we did nothing, KNN on its own achieves a 98% accuracy<br/> knn_average<br/> <br/> 0.98039215686274517</pre>
<ol start="4">
<li>The baseline accuracy to beat is 98.04%. Let's use our LDA, which keeps only the most powerful component:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">lda_pipeline = Pipeline([('lda', single_lda), ('knn', knn)])<br/> lda_average = cross_val_score(lda_pipeline, iris_X, iris_y).mean()<br/> <br/> # better prediction accuracy than PCA by a good amount, but not as good as original<br/> lda_average<br/> <br/> 0.9673202614379085</pre>
<ol start="5">
<li>It seems that only using a single linear discriminant isn't enough to beat our baseline accuracy. Let us now try the PCA. Our hypothesis here is that the PCA will not outperform the LDA for the sole reason that the PCA is not trying to optimize for class separation as LDA is:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># create a pipeline that performs PCA<br/> pca_pipeline = Pipeline([('pca', single_pca), ('knn', knn)])<br/> <br/> pca_average = cross_val_score(pca_pipeline, iris_X, iris_y).mean()<br/> <br/> pca_average<br/> <br/> 0.8941993464052288</pre>
<p class="prompt input_prompt">Definitely the worst so far. </p>
<p class="prompt input_prompt">It is worth exploring whether adding another LDA component will help us:</p>
<pre class="input"># try LDA with 2 components<br/> lda_pipeline = Pipeline([('lda', LinearDiscriminantAnalysis(n_components=2)),<br/> ('knn', knn)])<br/> <br/> lda_average = cross_val_score(lda_pipeline, iris_X, iris_y).mean()<br/> <br/> # Just as good as using original data<br/> lda_average<br/> <br/> 0.98039215686274517</pre>
<p class="inner_cell">With two components, we are able to achieve the original accuracy! This is great, but we want to do better than our baseline. Let's see if a feature selection module from the last chapter can help us. Let's import and use the <kbd>SelectKBest</kbd> module and see if statistical feature selection would best our LDA module:</p>
<pre class="inner_cell"># compare our feature transformation tools to a feature selection tool<br/> from sklearn.feature_selection import SelectKBest<br/> # try all possible values for k, excluding keeping all columns<br/> for k in [1, 2, 3]:<br/> # make the pipeline<br/> select_pipeline = Pipeline([('select', SelectKBest(k=k)), ('knn', knn)])<br/> # cross validate the pipeline<br/> select_average = cross_val_score(select_pipeline, iris_X, iris_y).mean()<br/> print k, "best feature has accuracy:", select_average<br/><br/> # LDA is even better than the best selectkbest<br/> 1 best feature has accuracy: 0.953839869281 2 best feature has accuracy: 0.960784313725 3 best feature has accuracy: 0.97385620915</pre>
<p class="mce-root">Our LDA with two components is so far winning. In production, it is quite common to use both unsupervised and supervised feature transformations. Let's set up a <kbd>GridSearch</kbd> module to find the best combination across:</p>
<ul>
<li>Scaling data (with or without mean/std)</li>
<li>PCA components</li>
<li>LDA components</li>
<li>KNN neighbors</li>
</ul>
<p>The following code block is going to set up a function called <kbd>get_best_model_and_accuracy</kbd><span> which will take in a model (scikit-learn pipeline or other), a parameter grid in the form of a dictionary, our <kbd>X</kbd> and <kbd>y</kbd> datasets, and output the result of the grid search module. The output will be the model's best performance (in terms of accuracy), the best parameters that led to the best performance, the average time it took to fit, and the average time it took to predict:</span></p>
<pre class="input">def get_best_model_and_accuracy(model, params, X, y):<br/>    grid = GridSearchCV(model, # the model to grid search<br/>                        params, # the parameter set to try<br/>                        error_score=0.) # if a parameter set raises an error, continue and set the performance as a big, fat 0<br/><br/>    grid.fit(X, y) # fit the model and parameters<br/>    # our classical metric for performance<br/>    print "Best Accuracy: {}".format(grid.best_score_)<br/>    # the best parameters that caused the best accuracy<br/>    print "Best Parameters: {}".format(grid.best_params_)<br/>    # the average time it took a model to fit to the data (in seconds)<br/>    avg_time_fit = round(grid.cv_results_['mean_fit_time'].mean(), 3)<br/>    print "Average Time to Fit (s): {}".format(avg_time_fit)<br/>    # the average time it took a model to predict out of sample data (in seconds)<br/>    # this metric gives us insight into how this model will perform in real-time analysis<br/>    print "Average Time to Score (s): {}".format(round(grid.cv_results_['mean_score_time'].mean(), 3))</pre>
<p>Once we have our function set up to take in models and parameters, let's use it to test our pipeline with our combinations of scaling, PCA, LDA, and KNN:</p>
<pre class="input"><br/>from sklearn.model_selection import GridSearchCV<br/>iris_params = {<br/>     'preprocessing__scale__with_std': [True, False],<br/>     'preprocessing__scale__with_mean': [True, False],<br/>     'preprocessing__pca__n_components':[1, 2, 3, 4], # according to scikit-learn docs, max allowed n_components for LDA is number of classes - 1<br/>     'preprocessing__lda__n_components':[1, 2],<br/><br/>     'clf__n_neighbors': range(1, 9)<br/>}<br/># make a larger pipeline<br/>preprocessing = Pipeline([('scale', StandardScaler()), ('pca', PCA()), ('lda', LinearDiscriminantAnalysis())])<br/><br/><br/>iris_pipeline = Pipeline(steps=[('preprocessing', preprocessing),('clf', KNeighborsClassifier())])<br/><br/><br/>get_best_model_and_accuracy(iris_pipeline, iris_params, iris_X, iris_y)<br/><br/>Best Accuracy: 0.986666666667 Best Parameters: {'preprocessing__scale__with_std': False, 'preprocessing__pca__n_components': 3, 'preprocessing__scale__with_mean': True, 'preprocessing__lda__n_components': 2, 'clf__n_neighbors': 3} Average Time to Fit (s): 0.002 Average Time to Score (s): 0.001</pre>
<div class="input_area">
<p class="CodeMirror cm-s-ipython">The best accuracy so far (near 99%) uses a combination of scaling, PCA, and LDA. It is common to correctly use all three of these algorithms in the same pipelines and perform hyper-parameter tuning to fine-tune the process. This shows us that more often than not, the best production-ready machine learning pipelines are in fact a combination of multiple feature engineering methods.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>To summarize our findings, both PCA and LDA are feature transformation tools in our arsenal that are used to find optimal new features to use. LDA specifically optimizes for class separation while PCA works in an unsupervised way to capture variance in the data in fewer columns. Usually, the two are used in conjunction with supervised pipelines, as we showed in the iris pipeline. In the final chapter, we will go through two longer case studies that utilize both PCA and LDA for text clustering and facial recognition software.</p>
<p>PCA and LDA are extremely powerful tools, but have limitations. Both of them are linear transformations, which means that they can only create linear boundaries and capture linear qualities in our data. They are also static transformations. No matter what data we input into a PCA or LDA, the output is expected and mathematical. If the data we are using isn't a good fit for PCA or LDA (they exhibit non-linear qualities, for example, they are circular), then the two algorithms will not help us, no matter how much we grid search.</p>
<p>The next chapter will focus on feature learning algorithms. These are arguably the most powerful feature engineering algorithms. They are built to learn new features based on the input data without assuming qualities such as PCA and LDA. In this chapter, we will use complex structures including neural networks to achieve the highest level of feature engineering yet.</p>


            </article>

            
        </section>
    </body></html>