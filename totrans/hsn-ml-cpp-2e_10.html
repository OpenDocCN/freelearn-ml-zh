<html><head></head><body>
		<div id="_idContainer936">
			<h1 class="chapter-number" id="_idParaDest-203"><a id="_idTextAnchor539"/>10</h1>
			<h1 id="_idParaDest-204"><a id="_idTextAnchor540"/>Neural Networks for Image Classification</h1>
			<p>In recent years, we have seen huge interest in <strong class="bold">neural networks</strong>, which are successfully used in various areas, such as business, medicine, technology, geology, and physics. Neural networks have come into play wherever it is necessary to solve problems of forecasting, classification, or control. Neural networks are intuitive as they are based on a simplified biological model of the human nervous system. They arose from research in the field of artificial intelligence, namely, from attempts to reproduce the ability of biological nervous systems to learn and correct mistakes by modeling the low-level structure of the brain. Neural networks are compelling modeling methods that allow us to reproduce extremely complex dependencies because they are non-linear. Neural networks also cope better with the <em class="italic">curse of dimensionality</em> than other methods that don’t allow modeling dependencies for a large number <span class="No-Break">of variables.</span></p>
			<p>In this chapter, we’ll look at the basic concepts of artificial neural networks and show you how to implement neural networks with different C++ libraries. We’ll also go through the implementation of the multilayer perceptron and simple convolutional networks and find out what deep learning is and what its <span class="No-Break">applications are.</span></p>
			<p>The following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>An overview of <span class="No-Break">neural networks</span></li>
				<li>Delving into <span class="No-Break">convolutional networks</span></li>
				<li>What is <span class="No-Break">deep learning?</span></li>
				<li>Examples of using C++ libraries to create <span class="No-Break">neural networks</span></li>
				<li>Understanding image classification using the <span class="No-Break">LeNet architecture</span><a id="_idTextAnchor541"/></li>
			</ul>
			<h1><a id="_idTextAnchor542"/></h1>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor543"/>Technical requirements</h1>
			<ul>
				<li>You will need the following to complete <span class="No-Break">this chapter:</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">Dlib</strong></span><span class="No-Break"> library</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">mlpack</strong></span><span class="No-Break"> library</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">Flashlight</strong></span><span class="No-Break"> library</span></li>
				<li>The <span class="No-Break">PyTorch library</span></li>
				<li>Modern C++ compiler with <span class="No-Break">C++20 support</span></li>
				<li>CMake build system version &gt;= <span class="No-Break">3.22</span></li>
			</ul>
			<p>The code files for this chapter can be found in the following GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter10"><span class="No-Break">https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter10</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor544"/>An overview of neural networks</h1>
			<p>In this section, we will discuss what artificial neural networks are and their building blocks. We will learn how artificial neurons work and how they relate to their biological analogs. We will also discuss how to train <a id="_idIndexMarker1152"/>neural networks with the backpropagation method, as well as how to deal with the <span class="No-Break">overfitting problem.</span></p>
			<p>A neural network is a sequence of neurons interconnected by synapses. The structure of the neural network came into the world of programming directly from biology. Thanks to this structure, computers can analyze and even remember information. Neural networks are based on the human brain, which contains millions of neurons that transmit information in the form of <span class="No-Break">electrical impulses.</span></p>
			<p>Artificial neural networks are inspired by biology because they are composed of elements with similar functionalities to those of biological neurons. These elements can be organized in a way that corresponds to the anatomy of the brain, and they demonstrate a large number of properties that are inherent to the brain. For example, they can learn from experience, generalize previous precedents to new cases, and identify significant features from input data that contains <span class="No-Break">redundant information.</span></p>
			<p>Now, let’s understand the process of a <span class="No-Break">single neuron<a id="_idTextAnchor545"/>.</span><a id="_idTextAnchor546"/></p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor547"/>Neurons</h2>
			<p>The <strong class="bold">biological neuron</strong> consists of a<a id="_idIndexMarker1153"/> body and processes that connect it to the outside world. The processes along which a<a id="_idIndexMarker1154"/> neuron receives excitation are called <strong class="bold">dendrites</strong>. The process through which a neuron transmits excitation is <a id="_idIndexMarker1155"/>called an <strong class="bold">axon</strong>. Each neuron has only one<a id="_idIndexMarker1156"/> axon. Dendrites and axons <a id="_idIndexMarker1157"/>have a rather complex branching structure. The junction of the axon and a<a id="_idIndexMarker1158"/> dendrite is called a <strong class="bold">synapse</strong>. The following figure shows the biological <span class="No-Break">neuron scheme:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer764">
					<img alt="Figure 10.1 – Biological neuron scheme" src="image/B19849_10_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Biological neuron scheme</p>
			<p>The main functionality of a neuron is to transfer excitation from dendrites to an axon. However, signals that come from different dendrites can affect the signal in the axon. A neuron gives off a signal if the total excitation exceeds a certain limit value, which varies within certain limits. If the signal is not sent to the axon, the neuron does not respond to excitation. The intensity of the signal that the neuron receives (and therefore the activation possibility) strongly depends on synapse activity. A synapse is a contact for transmitting this information. Each synapse has a length, and special chemicals transmit a signal along it. This basic circuit has many simplifications and exceptions compared to a biological system, but most neural networks model themselves on these <span class="No-Break">simple properties.</span></p>
			<p>The artificial neuron receives a specific<a id="_idIndexMarker1159"/> set of signals as input, each of which is the output of another neuron. Each<a id="_idIndexMarker1160"/> input is multiplied by the corresponding weight, which is equivalent to its synaptic power. Then, all the products are summed up, and the result of this summation is used to determine the level of neuron activation. The following diagram shows a model that demonstrates <span class="No-Break">this idea:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer765">
					<img alt="Figure 10.2 – Mathematical neuron scheme" src="image/B19849_10_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Mathematical neuron scheme</p>
			<p>Here, a set of input signals, denoted by <img alt="" role="presentation" src="image/B19849_Formula_0012.png"/>, go to an artificial neuron. These input signals correspond to the signals that arrive at the synapses of a biological neuron. Each signal is multiplied by the corresponding weight, <img alt="" role="presentation" src="image/B19849_Formula_0022.png"/>, and passed to the summing block. Each weight corresponds to the strength of one biological synaptic connection. The summing block, which corresponds to the body of the biological neuron, algebraically combines the <span class="No-Break">weighted inputs.</span></p>
			<p>The <img alt="" role="presentation" src="image/B19849_Formula_0032.png"/> signal, which is called bias, displays<a id="_idIndexMarker1161"/> the function of the limit value, known as the <strong class="bold">shift</strong>. This signal allows<a id="_idIndexMarker1162"/> us to shift the origin of the activation function, which subsequently leads to an increase in the neuron’s learning speed. The bias signal is added to each neuron. It learns like all the other weights, except it connects to the signal, <em class="italic">+1</em>, instead of to the output of the previous neuron. The received signal is processed by the activation function, <em class="italic">f</em>, and gives a neural signal, <em class="italic">y</em>, as output. The activation function is a way to normalize the input data. It narrows the range of <strong class="source-inline">sum</strong> so that the values of <strong class="source-inline">f (sum)</strong> belong to<a id="_idIndexMarker1163"/> a specific interval. That is, if we have a large input number, passing it through the activation function gets us output in the required range. There are many <a id="_idIndexMarker1164"/>activation functions, and we’ll go through them later in this chapter. To learn more about neural networks, we’ll have a look at a few more of <span class="No-Break">their com<a id="_idTextAnchor548"/>p<a id="_idTextAnchor549"/>onents.</span></p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor550"/>The perceptron and neural networks</h2>
			<p>The first appearance of artificial <a id="_idIndexMarker1165"/>neural networks can be traced to the article <em class="italic">A logical calculus of the ideas immanent in nervous activity</em>, which was published in 1943 where an early model of an artificial neuron was proposed. Later, American neurophysiologist Frank<a id="_idIndexMarker1166"/> Rosenblatt invented the perceptron concept in 1957 as a mathematical model of the human brain’s information<a id="_idIndexMarker1167"/> perception. Currently, terms such as <strong class="bold">single-layer perceptron </strong>(<strong class="bold">SLP</strong>), or just perceptron, and <strong class="bold">multilayer perceptron </strong>(<strong class="bold">MLP</strong>) are used. Usually, under the layers<a id="_idIndexMarker1168"/> in the perceptron is a sequence of neurons, located at the same level and not connected. The following diagram shows <span class="No-Break">this model:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer769">
					<img alt="Figure 10.3 – One layer of perceptrons" src="image/B19849_10_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – One layer of perceptrons</p>
			<p>Typically, we can distinguish between the following types of neural <span class="No-Break">network layers:</span></p>
			<ul>
				<li><strong class="bold">Input</strong>: This is just the source data or signals arriving as the input of the system (model). For example, these <a id="_idIndexMarker1169"/>can be individual components of a specific vector from the training <span class="No-Break">set, <img alt="" role="presentation" src="image/B19849_Formula_0042.png"/>.</span></li>
				<li><strong class="bold">Hidden</strong>: This is a layer of <a id="_idIndexMarker1170"/>neurons located between the input and output layers. There can be more than one <span class="No-Break">hidden layer.</span></li>
				<li><strong class="bold">Output</strong>: This is the last layer of <a id="_idIndexMarker1171"/>neurons that aggregates the model’s work, and its outputs are used as the result of the <span class="No-Break">model’s work.</span></li>
			</ul>
			<p>The term <strong class="bold">single-layer perceptron</strong> is often<a id="_idIndexMarker1172"/> understood to describe a model that consists of an input layer and an artificial neuron that aggregates the input data. This term is sometimes used in conjunction with the term <strong class="bold">Rosenblatt’s perceptron</strong>, but this is <a id="_idIndexMarker1173"/>not entirely correct since Rosenblatt used a randomized procedure to set up connections between input data and neurons to transfer data to a different dimension, which made it possible to solve the problems that arose when classifying linearly non-separable data. In Rosenblatt’s work, a perceptron consists of <em class="italic">S</em> and <em class="italic">A</em> neuron types, and an <em class="italic">R</em> adder. <em class="italic">S</em> neurons are the input layers, <em class="italic">A</em> neurons are the hidden layers, and the <em class="italic">R</em> neuron generates the model’s result. The terminology’s ambiguity arose because the weights were used only for the <em class="italic">R</em> neuron, while constant weights were used between the <em class="italic">S</em> and <em class="italic">A</em> neuron types. However, note that connections between these types of neurons were established according to a particular <span class="No-Break">randomized procedure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer771">
					<img alt="Figure 10.4 – Rosenblatt’s perceptron scheme" src="image/B19849_10_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Rosenblatt’s perceptron scheme</p>
			<p>The term MLP refers to a<a id="_idIndexMarker1174"/> model that consists of an input layer, a certain number of hidden layers, and an output layer. This can be seen in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer772">
					<img alt="Figure 10.5 – MLP" src="image/B19849_10_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – MLP</p>
			<p>It should be noted that the architecture of a perceptron (or neural network) includes the direction of signal propagation. In the preceding examples, all communications are directed from the input neurons to <a id="_idIndexMarker1175"/>the output ones—this is called a feedforward network. Other network <a id="_idIndexMarker1176"/>architectures may also include feedback <span class="No-Break">between neurons.</span></p>
			<p>The second point that we need to pay attention to in the architecture of a perceptron is the number of connections between neurons. In the preceding diagram, we can see that each neuron in one layer connects to all the neurons in the next layer—this is called a <strong class="bold">fully connected layer</strong>. This is not a <a id="_idIndexMarker1177"/>requirement, but we can see an example of a layer with different types of connections in the <em class="italic">Rosenblatt’s perceptron</em> scheme in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
			<p>Now, let’s learn about one of the ways in which artificial neural network<a id="_idTextAnchor551"/>s<a id="_idTextAnchor552"/> can <span class="No-Break">be trained.</span></p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor553"/>Training with the backpropagation method</h2>
			<p>Let’s consider the most common method that’s used to train a feedforward neural network: the <strong class="bold">error backpropagation method</strong>. It is related to supervised methods. Therefore, it requires target values in the <span class="No-Break">training</span><span class="No-Break"><a id="_idIndexMarker1178"/></span><span class="No-Break"> examples.</span></p>
			<p>The algorithm uses the output error of a neural network. At each iteration of the algorithm, there are two <a id="_idIndexMarker1179"/>network passes—forward and backward. On a forward pass, an input vector is propagated<a id="_idIndexMarker1180"/> from the network inputs to its outputs and forms a specific output vector corresponding to the current (actual) state of the weights. Then, the neural network error is calculated. On the backward pass, this error propagates from the network output to its inputs, and the neuron weights <span class="No-Break">are corrected.</span></p>
			<p>The function that’s used to calculate the network error is <a id="_idIndexMarker1181"/>called the <strong class="bold">loss function</strong>. An example of such a function is the square of the difference between the actual and <span class="No-Break">target values:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer773">
					<img alt="" role="presentation" src="image/B19849_Formula_005.jpg"/>
				</div>
			</div>
			<p>Here, <em class="italic">k</em> is the number of output neurons in the network, <em class="italic">y’</em> is the target value, and <em class="italic">y</em> is the actual output value. The algorithm is iterative and uses the principle of <em class="italic">step-by-step</em> training; the weights of the neurons of the network are adjusted after one training example is submitted as input. On the backward pass, this error propagates from the network output to its inputs, and the following rule corrects the <span class="No-Break">neuron’s weights:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer774">
					<img alt="" role="presentation" src="image/B19849_Formula_0061.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_0071.png"/> is the weight of the <img alt="" role="presentation" src="image/B19849_Formula_0082.png"/> connection of the <img alt="" role="presentation" src="image/B19849_Formula_0092.png"/> neuron, and <img alt="" role="presentation" src="image/B19849_Formula_0101.png"/> is the learning rate parameter, which allows us to control the value of the correction step, <img alt="" role="presentation" src="image/B19849_Formula_0112.png"/> . To accurately adjust to a minimum of errors, this is selected experimentally in the learning process (it varies in the range from 0 <span class="No-Break">to 1).</span></p>
			<p>Choosing an appropriate learning rate can significantly impact the performance and convergence of machine learning models. If the learning rate is too small, the model may converge slowly or not converge at all. On the other hand, if the learning rate is too large, the model can overshoot the optimal solution and diverge, resulting in poor accuracy and overfitting. To avoid such issues, it is important to carefully choose the learning rate based on the specific problem and dataset. Adaptive learning rates (such as Adam) can help automatically adjust the learning rate during training, making it easier to achieve good results. <img alt="" role="presentation" src="image/B19849_Formula_0122.png"/> is the number of the hierarchy of the algorithm (that is, the step number). Let’s say that the output sum of the <em class="italic">i</em><span class="superscript">th</span> neuron is <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer781">
					<img alt="" role="presentation" src="image/B19849_Formula_013.jpg"/>
				</div>
			</div>
			<p>From this, we can show <span class="No-Break">the following:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer782">
					<img alt="" role="presentation" src="image/B19849_Formula_014.jpg"/>
				</div>
			</div>
			<p>Here, we can see that the differential, <img alt="" role="presentation" src="image/B19849_Formula_0151.png"/>, of the activation function of the neurons of the network, <em class="italic">f (s)</em>, must exist and <a id="_idIndexMarker1182"/>not be equal to zero at any point; that is, the activation function must be differentiable on the entire numerical axis. Therefore, to apply the backpropagation method, sigmoidal activation functions, such as logistic or hyperbolic tangents, are <span class="No-Break">often used.</span></p>
			<p>In practice, training is continued not until the network is precisely tuned to the minimum of the error function, but until a sufficiently accurate approximation is achieved. This process allows us to reduce the number of learning iterations and prevent the network <span class="No-Break">from overfitting.</span></p>
			<p>Currently, many modifications of the backpropagation algorithm have been developed. Let’s look at so<a id="_idTextAnchor554"/>m<a id="_idTextAnchor555"/>e <span class="No-Break">of them.</span></p>
			<h3>Backpropagation method modes</h3>
			<p>There are three main <a id="_idIndexMarker1183"/>modes of the <span class="No-Break">backpropagation method:</span></p>
			<ul>
				<li><span class="No-Break">Stochastic</span></li>
				<li><span class="No-Break">Batch</span></li>
				<li><span class="No-Break">Mini-batch</span></li>
			</ul>
			<p>Let’s see what these modes are and how they differ from<a id="_idTextAnchor556"/> <a id="_idTextAnchor557"/><span class="No-Break">each other.</span></p>
			<h4>Stochastic mode</h4>
			<p>In stochastic mode, the <a id="_idIndexMarker1184"/>backpropagation method introduces corrections to the weight coefficients immediately after calculating the network output<a id="_idIndexMarker1185"/> on one <span class="No-Break">training sample.</span></p>
			<p>The stochastic method is slower than the batch method. Given that it does not carry out an accurate gradient descent, instead introducing some <em class="italic">noise</em> using an undeveloped gradient, it can get out of local minima and produce better results. It is also easier to apply when working with large amounts of <a id="_idTextAnchor558"/><span class="No-Break">t<a id="_idTextAnchor559"/>raining data.</span></p>
			<h4>Batch mode</h4>
			<p>For the batch mode of<a id="_idIndexMarker1186"/> gradient descent, the loss function is calculated immediately for all available training samples, and then corrections of the weight coefficients of the neuron are introduced by the error <span class="No-Break">backpropagation method.</span></p>
			<p>The batch method is faster and<a id="_idIndexMarker1187"/> more stable than stochastic mode, but it tends to stop and get stuck at local minima. Also, when it needs to train large amounts of data, it requires substantial <span class="No-Break">computat<a id="_idTextAnchor560"/>i<a id="_idTextAnchor561"/>onal resources.</span></p>
			<h4>Mini-batch mode</h4>
			<p>In practice, mini-batches are often <a id="_idIndexMarker1188"/>used as a compromise. The weights are adjusted after <a id="_idIndexMarker1189"/>processing several training samples (mini-batches). This is done less often than with stochastic descent, but more often than with <span class="No-Break">batch mode.</span></p>
			<p>Now that we’ve looked at the main backpropagation training modes, let’s discuss the problems of the <span class="No-Break">backp<a id="_idTextAnchor562"/>r<a id="_idTextAnchor563"/>opagation method.</span></p>
			<h3>Backpropagation method problems</h3>
			<p>Despite the mini-batch<a id="_idIndexMarker1190"/> method not being universal, it is widespread at the moment because it provides a compromise between computational scalability and learning effectiveness. It also has individual flaws. Most of its problems come from the indefinitely long learning process. In complex tasks, it may take days or even weeks to train the network. Also, while training the network, the values of the weights can become enormous due to correction. This problem can lead to all or most of the neurons beginning to function at enormous values, in the region where the derivative of the loss function is very small. Since the error that’s sent back during the learning process is proportional to this derivative, the learning process can <span class="No-Break">practically freeze.</span></p>
			<p>The gradient descent method can get stuck in a local minimum without hitting a global minimum. The error backpropagation method uses a kind of gradient descent; that is, it descends along the error surface, continuously adjusting the weights until they reach a minimum. The surface of the <a id="_idIndexMarker1191"/>error of a complex network is rugged and consists of hills, valleys, folds, and ravines in a high-dimensional space. A network can fall into a local minimum when there is a much deeper minimum nearby. At the local minimum point, all directions lead upward, and the network is unable to get out of it. The main difficulty in training neural networks comes down to the methods that are used to exit the local minima: each time we leave a local minimum, the next local minimum is searched by the same method, thereby backpropagating the error until it is no longer possible to find a way out <span class="No-Break">of it.</span></p>
			<p>A careful analysis of the proof of convergence shows that weight corrections are assumed to be infinitesimal. This assumption is not feasible in practice since it leads to an infinite learning time. The step size should be taken as the final size. If the step size is fixed and very small, then the convergence will be too slow, while if it is fixed and too large, then paralysis or permanent instability can occur. Today, many optimization methods have been developed that use a variable correction step size. They adapt the step size depending on the learning process (examples of such algorithms include Adam, Adagrad, RMSProp, Adadelta, and Nesterov <span class="No-Break">Accelerated Gradient).</span></p>
			<p>Notice that there is the possibility of the network overfitting. With too many neurons, the ability of the network to generalize information can be lost. The network can learn an entire set of samples provided for training, but any other images, even very similar ones, may be classified incorrectly. To prevent this problem, we need to use regularization and pay attention to this when designing our <span class="No-Break">n<a id="_idTextAnchor564"/>e<a id="_idTextAnchor565"/>twork architecture.</span></p>
			<h3>The backpropagation method – an example</h3>
			<p>To understand how the <a id="_idIndexMarker1192"/>backpropagation method works, let’s look at <span class="No-Break">an example.</span></p>
			<p>We’ll introduce the following indexing for all expression elements: <img alt="" role="presentation" src="image/B19849_Formula_0161.png"/> is the index of the layer, <img alt="" role="presentation" src="image/B19849_Formula_194.png"/> is the index of the neuron in the layer, and <img alt="" role="presentation" src="image/B19849_Formula_195.png"/> is the index of the current element or connection (for example, weight). We use these indexes <span class="No-Break">as follows:</span></p>
			<ul>
				<li><img alt="" role="presentation" src="image/B19849_Formula_0191.png"/></li>
			</ul>
			<p>This expression<a id="_idIndexMarker1193"/> should be read as the <img alt="" role="presentation" src="image/B19849_Formula_0201.png"/>element of the <img alt="" role="presentation" src="image/B19849_Formula_0213.png"/> neuron in the <img alt="" role="presentation" src="image/B19849_Formula_0222.png"/> <span class="No-Break">layer.</span></p>
			<p>Let’s say we have a network that consists of three layers, each of which contains <span class="No-Break">two neurons:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer791">
					<img alt="Figure 10.6 – Three-layer neural network" src="image/B19849_10_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Three-layer neural network</p>
			<ul>
				<li>As the loss function, we choose the square of the difference between the actual and <span class="No-Break">target values:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer792">
					<img alt="" role="presentation" src="image/B19849_Formula_023.jpg"/>
				</div>
			</div>
			<ul>
				<li>Here, <img alt="" role="presentation" src="image/B19849_Formula_0242.png"/> is the target value of the network output, <img alt="" role="presentation" src="image/B19849_Formula_0251.png"/> is the actual result of the output layer of the network, and <img alt="" role="presentation" src="image/B19849_Formula_0262.png"/> is the number of neurons in the <span class="No-Break">output layer.</span></li>
				<li>This formula calculates the output sum of the neuron, <img alt="" role="presentation" src="image/B19849_Formula_0721.png"/>, in the <span class="No-Break">layer, <img alt="" role="presentation" src="image/B19849_Formula_0281.png"/>:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer798">
					<img alt="" role="presentation" src="image/B19849_Formula_029.jpg"/>
				</div>
			</div>
			<ul>
				<li>Here, <img alt="" role="presentation" src="image/B19849_Formula_0302.png"/> is the number of inputs of a specific neuron and <img alt="" role="presentation" src="image/B19849_Formula_0312.png"/> is the bias value for a <span class="No-Break">specific neuron.</span></li>
				<li>For example, for the first neuron from the second layer, it is equal to <span class="No-Break">the following:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer801">
					<img alt="" role="presentation" src="image/B19849_Formula_0321.jpg"/>
				</div>
			</div>
			<ul>
				<li>Don’t forget that no<a id="_idIndexMarker1194"/> weights for the first layer exist because this layer only represents the <span class="No-Break">input values.</span></li>
				<li>The activation function that determines the output of a neuron should be a sigmoid, <span class="No-Break">as follows:</span><div class="IMG---Figure" id="_idContainer802"><img alt="" role="presentation" src="image/B19849_Formula_0331.jpg"/></div></li>
			</ul>
			<ul>
				<li>Its properties, as well as other activation functions, will be discussed later in this chapter. Accordingly, the output of the <em class="italic">i</em>th neuron in the <em class="italic">l</em>th layer (<img alt="" role="presentation" src="image/B19849_Formula_0342.png"/>) is equal to <span class="No-Break">the following:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer804">
					<img alt="" role="presentation" src="image/B19849_Formula_035.jpg"/>
				</div>
			</div>
			<ul>
				<li>Now, we implement stochastic gradient descent; that is, we correct the weights after each training example and move in a multidimensional space of weights. To get to the minimum of the error, we need to move in the direction opposite to the gradient. We have to add error correction to each weight, <img alt="" role="presentation" src="image/B19849_Formula_0362.png"/>, based on the corresponding output. The following formula shows how we calculate the error correction value, <img alt="" role="presentation" src="image/B19849_Formula_037.png"/>, with respect to the <img alt="" role="presentation" src="image/B19849_Formula_0381.png"/> <span class="No-Break">output:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer808">
					<img alt="" role="presentation" src="image/B19849_Formula_039.jpg"/>
				</div>
			</div>
			<ul>
				<li>Now that we have the formula for the error correction value, we can write a formula for the <span class="No-Break">weight update:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer809">
					<img alt="" role="presentation" src="image/B19849_Formula_040.jpg"/>
				</div>
			</div>
			<ul>
				<li>Here,- <img alt="" role="presentation" src="image/B19849_Formula_0412.png"/> is a learning <span class="No-Break">rate value.</span></li>
				<li>The partial derivative of the error with respect to the weights, <img alt="" role="presentation" src="image/B19849_Formula_0422.png"/>, is calculated using the chain rule, which is applied twice. Note that <img alt="" role="presentation" src="image/B19849_Formula_0432.png"/> affects the error only in the <span class="No-Break">sum, <img alt="" role="presentation" src="image/B19849_Formula_0441.png"/>:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer814">
					<img alt="" role="presentation" src="image/B19849_Formula_045.jpg"/>
				</div>
			</div>
			<ul>
				<li>We start with the output layer and derive an expression that’s used to calculate the correction for the weight, <img alt="" role="presentation" src="image/B19849_Formula_0461.png"/>. To do this, we must sequentially calculate the components. Consider how the error is calculated for <span class="No-Break">our network:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer816">
					<img alt="" role="presentation" src="image/B19849_Formula_047.jpg"/>
				</div>
			</div>
			<ul>
				<li>Here, we can<a id="_idIndexMarker1195"/> see that <img alt="" role="presentation" src="image/B19849_Formula_0482.png"/> does not depend on the weight of <img alt="" role="presentation" src="image/B19849_Formula_0491.png"/>. Its partial derivative with respect to this variable is equal <span class="No-Break">to <img alt="" role="presentation" src="image/B19849_Formula_0501.png"/>:</span><div class="IMG---Figure" id="_idContainer820"><img alt="" role="presentation" src="image/B19849_Formula_0511.jpg"/></div></li>
			</ul>
			<ul>
				<li>Then, the general expression changes to follow the <span class="No-Break">next formula:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer821">
					<img alt="" role="presentation" src="image/B19849_Formula_0521.jpg"/>
				</div>
			</div>
			<ul>
				<li>The first part of the expression is calculated <span class="No-Break">as follows:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer822">
					<img alt="" role="presentation" src="image/B19849_Formula_053.jpg"/>
				</div>
			</div>
			<ul>
				<li>The sigmoid derivative is <img alt="" role="presentation" src="image/B19849_Formula_0541.png"/>, respectively. For the second part of the expression, we get <span class="No-Break">the following:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer824">
					<img alt="" role="presentation" src="image/B19849_Formula_055.jpg"/>
				</div>
			</div>
			<ul>
				<li>The third part is the partial derivative of the sum, which is calculated <span class="No-Break">as follows:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer825">
					<img alt="" role="presentation" src="image/B19849_Formula_0561.jpg"/>
				</div>
			</div>
			<ul>
				<li>Now, we can combine everything into <span class="No-Break">one formula:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer826">
					<img alt="" role="presentation" src="image/B19849_Formula_0571.jpg"/>
				</div>
			</div>
			<ul>
				<li>We can also derive a general formula in order to calculate the error correction for all the weights of the <span class="No-Break">output layer:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer827">
					<img alt="" role="presentation" src="image/B19849_Formula_0581.jpg"/>
				</div>
			</div>
			<ul>
				<li>Here, <img alt="" role="presentation" src="image/B19849_Formula_059.png"/> is the index of the output layer of <span class="No-Break">the network.</span></li>
				<li>Now, we can consider how the corresponding calculations are carried out for the inner (hidden) layers of the network. Let’s take, for example, the weight, <img alt="" role="presentation" src="image/B19849_Formula_0601.png"/>. Here, the approach is the same, but with one significant difference—the output of the neuron of the hidden layer is<a id="_idIndexMarker1196"/> passed to the input of all (or several) of the neurons of the output layer, and this must be taken <span class="No-Break">into account:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer830">
					<img alt="" role="presentation" src="image/B19849_Formula_0611.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer831">
					<img alt="" role="presentation" src="image/B19849_Formula_0621.jpg"/>
				</div>
			</div>
			<ul>
				<li>Here, we can see that <img alt="" role="presentation" src="image/B19849_Formula_0631.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_0642.png"/> have already been calculated in the previous step and that we can use their values to <span class="No-Break">perform calculations:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer834">
					<img alt="" role="presentation" src="image/B19849_Formula_065.jpg"/>
				</div>
			</div>
			<p>By combining the obtained results, we receive the <span class="No-Break">following output:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer835">
					<img alt="" role="presentation" src="image/B19849_Formula_066.jpg"/>
				</div>
			</div>
			<p>Similarly, we can calculate the second component of the sum using the values that were calculated in the previous steps—<img alt="" role="presentation" src="image/B19849_Formula_0672.png"/> <span class="No-Break">and <img alt="" role="presentation" src="image/B19849_Formula_0681.png"/>:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer838">
					<img alt="" role="presentation" src="image/B19849_Formula_0691.jpg"/>
				</div>
			</div>
			<p>The remaining parts of the expression for weight correction, <img alt="" role="presentation" src="image/B19849_Formula_0702.png"/><span class="subscript">,</span> are obtained as follows, similar to how the expressions were obtained for the weights of the <span class="No-Break">output layer:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer840">
					<img alt="" role="presentation" src="image/B19849_Formula_0711.jpg"/>
				</div>
			</div>
			<p>By combining the obtained results, we obtain a general formula that we can use to calculate the magnitude of the adjustment of the weights of the <span class="No-Break">hidden layers:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer841">
					<img alt="" role="presentation" src="image/B19849_Formula_0721.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_0732.png"/> is the index of the hidden layer and <img alt="" role="presentation" src="image/B19849_Formula_0742.png"/> is the number of <span class="No-Break">neurons in</span></p>
			<p>the <span class="No-Break">layer, </span><span class="No-Break"><img alt="" role="presentation" src="image/B19849_Formula_0752.png"/>.</span></p>
			<p>Now, we have all the necessary formulas to describe the main steps of the error <span class="No-Break">backpropagation algorithm:</span></p>
			<ol>
				<li>Initialize <a id="_idIndexMarker1197"/>all weights, <img alt="" role="presentation" src="image/B19849_Formula_0762.png"/>, with small random values (the initialization process will be <span class="No-Break">discussed later).</span></li>
				<li>Repeat this several times, sequentially, for all the training samples, or a mini-batch <span class="No-Break">of samples.</span></li>
				<li>Pass a training sample (or a mini-batch of samples) to the network input and calculate and remember all the outputs of the neurons. These calculate all the sums and values of our <span class="No-Break">activation functions.</span></li>
				<li>Calculate the errors for all the neurons of the <span class="No-Break">output layer:</span><div class="IMG---Figure" id="_idContainer846"><img alt="" role="presentation" src="image/B19849_Formula_077.jpg"/></div></li>
			</ol>
			<ol>
				<li value="5">For each neuron on all <em class="italic">l</em> layers, starting from the penultimate one, calculate <span class="No-Break">the error:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer847">
					<img alt="" role="presentation" src="image/B19849_Formula_078.jpg"/>
				</div>
			</div>
			<p class="list-inset">Here, <em class="italic">Lnext</em> is the number of neurons in the <em class="italic">l + </em><span class="No-Break"><em class="italic">1</em></span><span class="No-Break"> layer.</span></p>
			<ol>
				<li value="6">Update the <span class="No-Break">network weights:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer848">
					<img alt="" role="presentation" src="image/B19849_Formula_079.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_0801.png"/> is the learning <span class="No-Break">rate value.</span></p>
			<p>There are many versions of the backpropagation algorithm that improve the stability and convergence rate of the algorithm. One of the very first proposed improvements was the use of momentum. At each step, the value <img alt="" role="presentation" src="image/B19849_Formula_0811.png"/> is memorized, and at the next step, we use a linear combination of the current gradient value and the <span class="No-Break">previous one:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer851">
					<img alt="" role="presentation" src="image/B19849_Formula_082.jpg"/>
				</div>
			</div>
			<p><img alt="" role="presentation" src="image/B19849_Formula_0832.png"/> is the hyperparameter that’s used for additional algorithm tuning. This algorithm is more common now than the original version because it allows us to achieve better results <span class="No-Break">during training.</span></p>
			<p>The next important<a id="_idIndexMarker1198"/> element that’s used to train the neural network is the <span class="No-Break">loss function.</span></p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor566"/>Loss functions</h2>
			<p>With the loss function, ne<a id="_idTextAnchor567"/>u<a id="_idTextAnchor568"/>ral <a id="_idIndexMarker1199"/>network training is reduced to the process of optimally selecting the coefficients of the matrix of weights in order to minimize the error. This function <a id="_idIndexMarker1200"/>should correspond to the task, for example, categorical cross-entropy for the classification problem or the square of the difference for regression. Differentiability is also an essential property of the loss function if the backpropagation method is used to train the network. Let’s look at some of the popular loss functions that are used in <span class="No-Break">neural networks:</span></p>
			<ul>
				<li>The <strong class="bold">mean squared error </strong>(<strong class="bold">MSE</strong>) loss function is widely used for regression and classification tasks. Classifiers can<a id="_idIndexMarker1201"/> predict continuous <a id="_idIndexMarker1202"/>scores, which are intermediate results that are only converted into class labels (usually by a threshold) as the very last step of the classification process. The MSE can be calculated using these continuous scores rather than the class labels. The advantage of this is that we avoid losing information due to dichotomization. The standard form of the MSE loss function is defined <span class="No-Break">as follows:</span><div class="IMG---Figure" id="_idContainer853"><img alt="" role="presentation" src="image/B19849_Formula_0841.jpg"/></div></li>
			</ul>
			<ul>
				<li>The <strong class="bold">mean squared logarithmic error </strong>(<strong class="bold">MSLE</strong>) loss function is a variant of the MSE and is defined <span class="No-Break">as </span><span class="No-Break"><a id="_idIndexMarker1203"/></span><span class="No-Break">follows:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer854">
					<img alt="" role="presentation" src="image/B19849_Formula_085.jpg"/>
				</div>
			</div>
			<p class="list-inset">By taking the log of the <a id="_idIndexMarker1204"/>predictions and target values, the variance that we are measuring has changed. It is often used when we do not want to penalize considerable differences in the predicted and target values when both the predicted and actual values are big numbers. Also, the MSLE penalizes underestimates more <span class="No-Break">than overestimates.</span></p>
			<ul>
				<li>The <strong class="bold">L2</strong> loss function is the square <a id="_idIndexMarker1205"/>of the L2 norm of the difference<a id="_idIndexMarker1206"/> between the actual value and the target value. It is defined <span class="No-Break">as follows:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer855">
					<img alt="" role="presentation" src="image/B19849_Formula_0861.jpg"/>
				</div>
			</div>
			<ul>
				<li>The <strong class="bold">mean absolute error </strong>(<strong class="bold">MAE</strong>) loss function<a id="_idIndexMarker1207"/> is used to measure how close forecasts or predictions are to the <span class="No-Break">eventual outcomes:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer856">
					<img alt="" role="presentation" src="image/B19849_Formula_087.jpg"/>
				</div>
			</div>
			<p class="list-inset">The MAE requires<a id="_idIndexMarker1208"/> complicated tools such as linear programming to compute the gradient. The MAE is more robust to outliers than the MSE since it does not make use of <span class="No-Break">the square.</span></p>
			<ul>
				<li>The <strong class="bold">L1</strong> loss function is the<a id="_idIndexMarker1209"/> sum of absolute errors of<a id="_idIndexMarker1210"/> the difference between the actual value and target value. Similar to the relationship between the MSE and L2, L1 is mathematically similar to the MAE except it does not have division by <strong class="bold">n</strong>. It is defined <span class="No-Break">as follows:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer857">
					<img alt="" role="presentation" src="image/B19849_Formula_088.jpg"/>
				</div>
			</div>
			<ul>
				<li>The <strong class="bold">cross-entropy</strong> loss function is commonly used for binary classification tasks where labels are<a id="_idIndexMarker1211"/> assumed to<a id="_idIndexMarker1212"/> take values of 0 or 1. It is defined <span class="No-Break">as follows:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer858">
					<img alt="" role="presentation" src="image/B19849_Formula_0891.jpg"/>
				</div>
			</div>
			<p class="list-inset">Cross-entropy measures the divergence between two probability distributions. If the cross-entropy is large, this means that the difference between the two distributions is significant, while if the cross-entropy is small, this means that the two distributions are similar to each other. The cross-entropy loss function has the advantage of faster convergence, and it is more likely to reach global optimization than the quadratic <span class="No-Break">loss function.</span></p>
			<ul>
				<li>The <strong class="bold">negative log-likelihood</strong> loss function is used in neural networks for classification tasks. It is used<a id="_idIndexMarker1213"/> when the model outputs a <a id="_idIndexMarker1214"/>probability for each class rather than the class label. It is defined <span class="No-Break">as follows:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer859">
					<img alt="" role="presentation" src="image/B19849_Formula_0901.jpg"/>
				</div>
			</div>
			<ul>
				<li>The <strong class="bold">cosine proximity</strong> loss function computes the cosine proximity between the predicted <a id="_idIndexMarker1215"/>value and the target value. It is defined <a id="_idIndexMarker1216"/><span class="No-Break">as follows:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer860">
					<img alt="" role="presentation" src="image/B19849_Formula_0912.jpg"/>
				</div>
			</div>
			<p class="list-inset">This function is the same as the cosine similarity, which is a measure of similarity between two non-zero vectors. This is expressed as the cosine of the angle between them. Unit vectors are maximally similar if they are parallel and maximally dissimilar if they <span class="No-Break">are orthogonal.</span></p>
			<ul>
				<li>The <strong class="bold">hinge loss</strong> function is used for training classifiers. The hinge loss is also known as the max-margin<a id="_idIndexMarker1217"/> objective and is used for <em class="italic">maximum-margin</em> classification. It uses<a id="_idIndexMarker1218"/> the raw output of the classifier’s decision function, not<a id="_idIndexMarker1219"/> the predicted class label. It is defined <span class="No-Break">as follows:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer861">
					<img alt="" role="presentation" src="image/B19849_Formula_0921.jpg"/>
				</div>
			</div>
			<p>There are many other loss functions. Complex network architectures often use several loss functions to train different parts of a network. For example, the <em class="italic">Mask RCNN</em> architecture, which is used for predicting object classes and boundaries on images, uses different loss functions: one for regression and another for classifiers. In the next section, we will discuss the neuron’s <span class="No-Break">activation functions.</span></p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor569"/>Activation functions</h2>
			<p>What doe<a id="_idTextAnchor570"/>s<a id="_idTextAnchor571"/> an artificial <a id="_idIndexMarker1220"/>neuron do? Simply put, it calculates the weighted sum of inputs, adds <a id="_idIndexMarker1221"/>the bias, and decides whether to exclude this value or use it further. The artificial neuron doesn’t know of a threshold that can be used to figure out whether the output value switches neurons to the activated state. For this purpose, we add an activation function. It checks the value that’s produced by the neuron for whether external connections should recognize that this neuron is activated or whether it can be ignored. It determines the output value of a neuron, depending on the result of a weighted sum of inputs and a <span class="No-Break">threshold value.</span></p>
			<p>Let’s consider some examples of activation functions and <span class="No-Break">their properties.</span></p>
			<h3>The stepwise activation fun<a id="_idTextAnchor572"/>c<a id="_idTextAnchor573"/>tion</h3>
			<p>The stepwise activation function works<a id="_idIndexMarker1222"/> like this—if the sum value is higher than a particular threshold value, we consider the neuron<a id="_idIndexMarker1223"/> activated. Otherwise, we say that the neuron <span class="No-Break">is inactive.</span></p>
			<p>A graph of this function can be seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer862">
					<img alt="" role="presentation" src="image/B19849_10_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Stepwise activation function</p>
			<p>The function returns <em class="italic">1</em> (the neuron <a id="_idIndexMarker1224"/>has been activated) when the argument is <em class="italic">&gt; 0</em> (the zero value is a threshold), and the<a id="_idIndexMarker1225"/> function returns <em class="italic">0</em> (the neuron hasn’t been activated) otherwise. This approach is easy, but it has flaws. Imagine that we are creating a binary classifier—a model that should say <em class="italic">yes</em> or <em class="italic">no</em> (activated or not). A stepwise function can do this for us—it prints <em class="italic">1</em> or <em class="italic">0</em>. Now, imagine a case when more neurons are required to classify many classes: <em class="italic">class1</em>, <em class="italic">class2</em>, <em class="italic">class3</em>, or even more. What happens if more than one neuron is activated? All the neurons from the activation function <span class="No-Break">derive </span><span class="No-Break"><em class="italic">1</em></span><span class="No-Break">.</span></p>
			<p>In this case, questions arise about what class should ultimately be obtained for a given object. We only want one neuron to be activated, and the activation functions of other neurons should be zero (except in this case, we can be sure that the network correctly defines the class). Such a network is more challenging to train and achieve convergence. If the activation function is not binary, then the possible values are activated at 50%, activated at 20%, and so on. If several neurons are activated, we can find the neuron with the highest value of the activation function. Since there are intermediate values at the output of the neuron, the learning process runs smoother <span class="No-Break">and faster.</span></p>
			<p>In the stepwise activation function, the likelihood of several fully activated neurons appearing during training decreases (although this depends on what we are training and on what data). Also, the stepwise <a id="_idIndexMarker1226"/>activation function is not differentiable at point 0 and its derivative is equal to 0 at <a id="_idIndexMarker1227"/>all other points. This leads to difficulties when we use gradient descent methods <span class="No-Break">for training.</span></p>
			<h3>The linear activation <a id="_idTextAnchor574"/>f<a id="_idTextAnchor575"/>unction</h3>
			<p>The linear activation function, <em class="italic">y = c x</em>, is a straight line and is proportional to the input (that is, the weighted sum on this neuron). Such a choice of activation function allows us to get a range of values, not<a id="_idIndexMarker1228"/> just a binary answer. We can connect several neurons and if <a id="_idIndexMarker1229"/>more than one neuron is activated, the decision is made based on the choice of, for example, the <span class="No-Break">maximum value.</span></p>
			<p>The following diagram shows what the linear activation function <span class="No-Break">looks like:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer863">
					<img alt="Figure 10.8 – Linear activation function" src="image/B19849_10_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Linear activation function</p>
			<p>The derivative of <em class="italic">y = c x</em> with respect to <em class="italic">x</em> is <em class="italic">c</em>. This conclusion means that the gradient has nothing to do with the argument of the function. The gradient is a constant vector, while the descent is made according to a constant gradient. If an erroneous prediction is made, then the backpropagation error’s <a id="_idIndexMarker1230"/>update changes are also constant and do not depend on the change<a id="_idIndexMarker1231"/> that’s made regarding <span class="No-Break">the input.</span></p>
			<p>There is another problem: related layers. A linear function activates each layer. The value from this function goes to the next layer as input while the second layer considers the weighted sum at its inputs and, in turn, includes neurons, depending on another linear activation function. It doesn’t matter how many layers we have. If they are all linear, then the final activation function in the last layer is just a linear function of the inputs on the first layer. This means that two layers (or <em class="italic">N</em> layers) can be replaced with one layer. Due to this, we lose the ability to make sets of layers. The entire neural network is still similar to the one layer with a linear activation function because it's the linear combination of <span class="No-Break">linear functions.</span></p>
			<h3>The sigmoid activat<a id="_idTextAnchor576"/>i<a id="_idTextAnchor577"/>on function</h3>
			<p>The sigmoid activation<a id="_idIndexMarker1232"/> function, <img alt="" role="presentation" src="image/B19849_Formula_0931.png"/>, is a smooth function, similar to a<a id="_idIndexMarker1233"/> <span class="No-Break">stepwise function:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer865">
					<img alt="Figure 10.9 – Sigmoid activation function" src="image/B19849_10_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Sigmoid activation function</p>
			<p>A sigmoid is a non-linear function, and a combination of sigmoids also produces a non-linear function. This allows us to combine neuron layers. A sigmoid activation function is not binary, which makes an activation with a set of values from the range [0,1], in contrast to a stepwise function. A smooth gradient also characterizes a sigmoid. In the range of values of <img alt="" role="presentation" src="image/B19849_Formula_0942.png"/> from -2 to 2, the values, <img alt="" role="presentation" src="image/B19849_Formula_0421.png"/>, change very quickly. This gradient property means that any small change in the value of <img alt="" role="presentation" src="image/B19849_Formula_0911.png"/> in this area entails a significant change in the value of <img alt="" role="presentation" src="image/B19849_Formula_0972.png"/>. This behavior of the function indicates that <img alt="" role="presentation" src="image/B19849_Formula_0421.png"/> tends to cling to one of the edges of <span class="No-Break">the curve.</span></p>
			<p>The sigmoid looks like a <a id="_idIndexMarker1234"/>suitable function for classification tasks. It tries to bring the values to one of the sides of the curve (for example, to the upper edge at <img alt="" role="presentation" src="image/B19849_Formula_0992.png"/> and the lower edge at <img alt="" role="presentation" src="image/B19849_Formula_1001.png"/>). This behavior allows us to find clear boundaries in <span class="No-Break">the prediction.</span></p>
			<p>Another advantage of a <a id="_idIndexMarker1235"/>sigmoid over a linear function is as follows: in the first case, we have a fixed range of function values, [0, 1], while a linear function varies within <img alt="" role="presentation" src="image/B19849_Formula_1013.png"/>. This is advantageous because it does not lead to errors in numerical calculations when dealing with large values on the <span class="No-Break">activation function.</span></p>
			<p>Today, the sigmoid is one of the most popular activation functions in neural networks. But it also has flaws that we have to take into account. When the sigmoid function approaches its maximum or minimum, the output value of <img alt="" role="presentation" src="image/B19849_Formula_0421.png"/> tends to weakly reflect changes in <img alt="" role="presentation" src="image/B19849_Formula_1032.png"/>. This means that<a id="_idIndexMarker1236"/> the gradient in such areas takes small values, and the small values cause the gradient to vanish. The <strong class="bold">vanishing gradient</strong> problem is a situation where a gradient value becomes too small or disappears and the neural network refuses to learn further or learns <span class="No-Break">very slowly.</span></p>
			<h3>The hyperbolic tangent</h3>
			<p>The<a id="_idTextAnchor578"/> <a id="_idTextAnchor579"/>hyperbolic<a id="_idIndexMarker1237"/> tangent is<a id="_idIndexMarker1238"/> another commonly used activation function. It can be represented graphically <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer876">
					<img alt="Figure 10.10 – Hyperbolic tangent activation function" src="image/B19849_10_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – Hyperbolic tangent activation function</p>
			<p>The hyperbolic tangent is very similar to the sigmoid. This is the correct sigmoid function, <img alt="" role="presentation" src="image/B19849_Formula_1042.png"/>. Therefore, such a function has the same characteristics as the sigmoid we looked at earlier. Its nature is non-linear, it is well suited for a combination of layers, and the range of values of the function is <img alt="A black background with a black square&#10;&#10;Description automatically generated with medium confidence" src="image/B19849_Formula_105.png"/>. Therefore, it makes no sense to worry about the values of the activation function leading to computational problems. However, it is worth noting that the gradient of the tangential function has higher values than that of the sigmoid (the derivative is steeper than it is for the sigmoid). Whether we choose a sigmoid or a tangent function depends on the requirements of the gradient’s amplitude. As well as the sigmoid, the hyperbolic tangent has the inherent vanishing <span class="No-Break">gradient</span><span class="No-Break"><a id="_idIndexMarker1239"/></span><span class="No-Break"> problem.</span></p>
			<p>The <strong class="bold">rectified linear unit</strong> (<strong class="bold">ReLU</strong>), <img alt="" role="presentation" src="image/B19849_Formula_106.png"/>, returns <img alt="" role="presentation" src="image/B19849_Formula_0431.png"/> if <img alt="" role="presentation" src="image/B19849_Formula_1082.png"/> is positive, and <img alt="" role="presentation" src="image/B19849_Formula_1091.png"/> <span class="No-Break">otherwise:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer883">
					<img alt="Figure 10.11 – ReLU activation function" src="image/B19849_10_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – ReLU activation function</p>
			<p>At first glance, it seems that ReLU has all the same problems as a linear function since ReLU is linear in the first quadrant. But in fact, ReLU is non-linear, and a combination of ReLU is also non-linear. A combination of ReLU can approximate any function. This property means that we can use layers and they won’t degenerate into a linear combination. The range of permissible values of ReLU is <img alt="" role="presentation" src="image/B19849_Formula_1101.png"/>, which means that its values can be quite high, thus leading to computational problems. However, this same property removes<a id="_idIndexMarker1240"/> the problem of a vanishing gradient. It is recommended to use regularization<a id="_idIndexMarker1241"/> and normalize the input data to solve the problem with large function values (for example, to the range of values [<span class="No-Break">0,1] ).</span></p>
			<p>Let’s look at activation sparseness as a property of neural networks. Imagine a large neural network with many neurons. The use of a sigmoid or hyperbolic tangent entails the activation of all neurons. This action means that almost all activations must be processed to calculate the network output. In other words, activation is dense <span class="No-Break">and costly.</span></p>
			<p>Ideally, we want some neurons not to be activated, and this would make activations sparse and efficient. ReLU allows us to do this. Imagine a network with randomly initialized weights (or normalized) in which approximately 50% of activations are 0 because of the ReLU property, returning 0 for negative values of <img alt="" role="presentation" src="image/B19849_Formula_1112.png"/>. In such a network, fewer neurons are included (sparse activation), and the network itself <span class="No-Break">becomes lightweight.</span></p>
			<p>Since part of the ReLU is a horizontal line (for negative values of <img alt="" role="presentation" src="image/B19849_Formula_1123.png"/>), the gradient on this part is 0. This property leads to the<a id="_idIndexMarker1242"/> fact that weights cannot be adjusted during training. This phenomenon is called the <strong class="bold">dying ReLU problem</strong>. Because of this problem, some neurons are turned off and do not respond, making a significant part of the neural network passive. However, there are variations of ReLU that help solve this problem. For example, it makes sense to replace the horizontal part of the function (the region where <img alt="" role="presentation" src="image/B19849_Formula_1133.png"/>) with the linear one using the expression <img alt="" role="presentation" src="image/B19849_Formula_1142.png"/>. There are other ways to avoid a zero gradient, but the main idea is to make the gradient non-zero and gradually restore it <span class="No-Break">during training.</span></p>
			<p>Also, ReLU is significantly<a id="_idIndexMarker1243"/> less demanding on computational resources than hyperbolic tangent or sigmoid because it performs simpler mathematical operations than the <span class="No-Break">aforementioned functions.</span></p>
			<p>The critical properties of ReLU are<a id="_idIndexMarker1244"/> its small computational complexity, nonlinearity, and unsusceptibility to the vanishing gradient problem. This makes it one of the most frequently used activation functions for creating deep <span class="No-Break">neural networks.</span></p>
			<p>Now that we’ve looked at a number of activation functions, we can highlight their <span class="No-Break">main properties.</span></p>
			<h3>Activation function properties</h3>
			<p><a id="_idTextAnchor580"/>T<a id="_idTextAnchor581"/>he following is a list of activation function properties that are worth considering when deciding which activation function <span class="No-Break">to choose:</span></p>
			<ul>
				<li><strong class="bold">Nonlinearity</strong>: If the activation function is <a id="_idIndexMarker1245"/>non-linear, it can be proved that even a two-level neural network can be a universal approximator of <span class="No-Break">the function.</span></li>
				<li><strong class="bold">Continuous differentiability</strong>: This property is desirable for providing gradient descent <span class="No-Break">optimization methods.</span></li>
				<li><strong class="bold">Value range</strong>: If the set of values for the activation function is limited, gradient-based learning methods are more stable and less prone to calculation errors since there are no large values. If the range of values is infinite, training is usually more effective, but care must be taken to avoid exploding the gradient (it means that gradient values can get extremal values and the learning ability will <span class="No-Break">be lost).</span></li>
				<li><strong class="bold">Monotonicity</strong>: If the activation function is monotonic, the error surface associated with the single-level model is guaranteed to be convex. This allows us to learn <span class="No-Break">more effectively.</span></li>
				<li><strong class="bold">Smooth functions with monotone derivatives</strong>: In some cases, these provide a higher degree <a id="_idIndexMarker1246"/><span class="No-Break">of generality.</span></li>
			</ul>
			<p>Now that we’ve discussed the main components used to train neural networks, it’s time to learn how to deal with the overfitting problem, which regularly appears during the <span class="No-Break">training process.</span></p>
			<h2 id="_idParaDest-212">Regularization in neural netw<a id="_idTextAnchor582"/>o<a id="_idTextAnchor583"/>rks</h2>
			<p>Overfitting is one of the problems of machine learning models and neural networks in particular. The problem is that the model only explains the samples from the training set, thus adapting to the training<a id="_idIndexMarker1247"/> samples instead of learning to classify samples that were not involved in the training process (losing the ability to generalize). Usually, the primary cause of overfitting is the model’s complexity (in terms of the number of <a id="_idIndexMarker1248"/>parameters it has). The complexity can be too high for the training set available and, ultimately, for the problem to be solved. The task of the regularizer is to reduce the model’s complexity, preserving the number of parameters. Let’s consider the most common regularization methods that are used in <span class="No-Break">neural networks.</span></p>
			<p>The most popular regulariza<a id="_idTextAnchor584"/>tion methods are L2 regularization, dropout, and batch normalization. Let’s take a look <span class="No-Break">at each.</span></p>
			<h3>L2 regularization</h3>
			<p><strong class="bold">L2 regularization</strong> (weight decay) is performed <a id="_idIndexMarker1249"/>by penalizing the weights with the highest values. Penalizing is performed by minimizing their <img alt="" role="presentation" src="image/B19849_Formula_1151.png"/>-norm using the <img alt="" role="presentation" src="image/B19849_Formula_1161.png"/> parameter—a regularization coefficient that <a id="_idIndexMarker1250"/>expresses the preference for minimizing the norm when we need to minimize losses on the training set. That is, for each weight, <img alt="" role="presentation" src="image/B19849_Formula_1172.png"/>, we add the term, <img alt="" role="presentation" src="image/B19849_Formula_1182.png"/>, to the loss function, <img alt="" role="presentation" src="image/B19849_Formula_1192.png"/> (the <img alt="" role="presentation" src="image/B19849_Formula_1201.png"/> factor is used so that the gradient of this term with respect to the <img alt="" role="presentation" src="image/B19849_Formula_1212.png"/> parameter is equal to <img alt="" role="presentation" src="image/B19849_Formula_1223.png"/> and not <img alt="" role="presentation" src="image/B19849_Formula_1233.png"/> for the convenience of applying the error backpropagation method). We must select <img alt="" role="presentation" src="image/B19849_Formula_1243.png"/> correctly. If the coefficient is too small, then the effect of regularization is negligible. If it is too large, the model can reset <a id="_idIndexMarker1251"/>all <span class="No-Break">the weights.</span></p>
			<h3>Dropout regularization</h3>
			<p><strong class="bold">Dropout regularization</strong> consists of changing the structure of the network. Each neuron can be excluded from a network structure with some probability, <img alt="" role="presentation" src="image/B19849_Formula_119.png"/>. The exclusion of a neuron means that with <a id="_idIndexMarker1252"/>any input data or parameters, it <span class="No-Break">returns 0.</span></p>
			<p>Excluded neurons do not contribute to <a id="_idIndexMarker1253"/>the learning process at any stage of the backpropagation algorithm. Therefore, the exclusion of at least one of the neurons is equal to learning a new neural network. This <em class="italic">thinning</em> network is used to train the remaining weights. A gradient step is taken, after which all ejected neurons are returned to the neural network. Thus, at each step of the training, we set up one of the possible 2<em class="italic">N</em> network architectures. By architecture, we mean the structure of connections between neurons, and by <em class="italic">N</em>, we’re denoting the total number of neurons. When we are evaluating a neural network, neurons are no longer thrown out. Each neuron output is multiplied by (<em class="italic">1 - p</em>). This means that in the neuron’s output, we receive its response expectation for all 2<em class="italic">N</em> architectures. Thus, a neural network trained using dropout regularization can be considered a result of averaging responses from an ensemble of <span class="No-Break">2</span><span class="No-Break"><em class="italic">N</em></span><span class="No-Break"> networks.</span></p>
			<h3>Batch normalization</h3>
			<p><strong class="bold">Batch normalization</strong> makes sure that the effective learning process of neural networks isn’t impeded. The input signal can be significantly distorted by the mean and variance as the signal propagates through<a id="_idIndexMarker1254"/> the inner layers of a network, even if we initially normalized the signal at the network input. This <a id="_idIndexMarker1255"/>phenomenon is called the internal covariance shift and is fraught with severe discrepancies <a id="_idIndexMarker1256"/>between the gradients at different levels or layers. Therefore, we have to use stronger regularizers, which slows down the pace <span class="No-Break">of learning.</span></p>
			<p>Batch normalization offers a straightforward solution to this problem: normalize the input data in such a way as to obtain zero mean and unit variance. Normalization is performed before entering each layer. During the training process, we normalize the batch samples, and during use, we normalize the statistics obtained based on the entire training set since we cannot see the test data in advance. We calculate the mean and variance for a specific batch, <img alt="" role="presentation" src="image/B19849_Formula_1261.png"/>, <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer901">
					<img alt="" role="presentation" src="image/B19849_Formula_1271.jpg"/>
				</div>
			</div>
			<p>Using these statistical <a id="_idIndexMarker1257"/>characteristics, we transform the activation function in such a way that it has zero mean and unit variance throughout the <span class="No-Break">whole batch:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer902">
					<img alt="" role="presentation" src="image/B19849_Formula_1281.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_1291.png"/> is a parameter that protects us from dividing by 0 in cases where the standard deviation of the batch is very small or even equal to zero. Finally, to get the final activation function, <img alt="" role="presentation" src="image/B19849_Formula_0421.png"/>, we need to make sure that, during normalization, we don’t lose the ability to generalize. Since <a id="_idIndexMarker1258"/>we applied scaling and shifting operations to the original data, we can allow arbitrary scaling and shifting of normalized values, thereby obtaining the final <span class="No-Break">activation function:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer905">
					<img alt="" role="presentation" src="image/B19849_Formula_131.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_1323.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_1331.png"/> are the parameters of batch normalization that the system can be trained with (they can be optimized by the gradient descent method on the training data). This generalization also means that batch normalization can be useful when applying the input of a neural <span class="No-Break">network directly.</span></p>
			<p>This method, when applied to multilayer networks, almost always successfully reaches its goal—it accelerates learning. Moreover, it’s an excellent regularizer, allowing us to choose the learning rate, the power of the <img alt="" role="presentation" src="image/B19849_Formula_1341.png"/> regularizer, and the dropout. The regularization here is a consequence of the fact that the result of the network for a specific sample is no longer deterministic (it depends on the whole batch that this result was obtained from), which simplifies the <span class="No-Break">generalization process.</span></p>
			<p>The next important topic we’ll look at is neural network initialization. This affects the convergence of the training process, training speed, and overall <span class="No-Break">network performance.</span></p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor585"/>Neural network initialization</h2>
			<p>The pri<a id="_idTextAnchor586"/>n<a id="_idTextAnchor587"/>ciple of choosing <a id="_idIndexMarker1259"/>the initial values of weights for the layers that make up the model is very important. Setting all the weights to 0 is a severe obstacle to learning because none of the weights can be active initially. Assigning weights to the random values from the interval, [0, 1], is also usually not the best option. Actually, model performance and learning process convergence can strongly rely on correct weight initialization; however, the initial task and model complexity can also play an<a id="_idIndexMarker1260"/> important role. Even if the task’s solution does not assume a strong dependency on the values of the initial weights, a well-chosen method of initializing weights can significantly affect the model’s ability to learn. This is because it presets the model parameters while taking the loss function into account. Let’s look at two popular methods that are used to <span class="No-Break">initialize weights.</span></p>
			<h3>Xavier initialization method</h3>
			<p>The <strong class="bold">Xa<a id="_idTextAnchor588"/>v<a id="_idTextAnchor589"/>ier initialization method</strong> is used to simplify the signal flow through the layer during both the forward pass and the backward pass of the error for the linear activation function. This method <a id="_idIndexMarker1261"/>also works well for the sigmoid function, since the region where it is unsaturated also has a linear <a id="_idIndexMarker1262"/>character. When calculating weights, this method relies on probability distribution (such as the uniform or the normal ones) with a variance of <img alt="" role="presentation" src="image/B19849_Formula_1351.png"/>, where <img alt="" role="presentation" src="image/B19849_Formula_1361.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_1371.png"/> are the number of neurons in the previous and subsequent <span class="No-Break">layers, respectively.</span></p>
			<h3>He initialization method</h3>
			<p>The <strong class="bold">He init<a id="_idTextAnchor590"/>i<a id="_idTextAnchor591"/>alization method</strong> is a variation of the Xavier method that’s more suitable for ReLU activation functions because it <a id="_idIndexMarker1263"/>compensates for<a id="_idIndexMarker1264"/> the fact that this function returns zero for half of the definition domain. This method of weight calculation relies on a probability distribution with the <span class="No-Break">following variance:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer912">
					<img alt="" role="presentation" src="image/B19849_Formula_138.jpg"/>
				</div>
			</div>
			<p>There are also other methods of weight initialization. Which one you choose is usually determined by the problem being solved, the network topology, the activation functions being used, and the loss function. For example, for recursive networks, the orthogonal initialization method can be used. We’ll provide a concrete programming example of neural network initialization in <a href="B19849_12.xhtml#_idTextAnchor660"><span class="No-Break"><em class="italic">Chapter 12</em></span></a><em class="italic">, Exporting and </em><span class="No-Break"><em class="italic">Importing Models</em></span><span class="No-Break">.</span></p>
			<p>In the previous sections, we looked at the basic components of artificial neural networks, which are common<a id="_idIndexMarker1265"/> to almost all types of networks. In the next section, we will discuss the features of convolutional neural <a id="_idIndexMarker1266"/>networks that are often used for <span class="No-Break">image processing.</span></p>
			<h1 id="_idParaDest-214">Delving into convolutional networ<a id="_idTextAnchor592"/>k<a id="_idTextAnchor593"/>s</h1>
			<p>The MLP is the most powerful feedforward neural network. It consists of several layers, where each neuron receives its copy of all the output from the previous layer of neurons. This model is ideal for <a id="_idIndexMarker1267"/>certain types of tasks, for example, training on a limited number of more or less <span class="No-Break">unstructured parameters.</span></p>
			<p>Nevertheless, let’s see what happens to the number of parameters (weights) in such a model when raw data is used as input. For example, the CIFAR-10 dataset contains 32 x 32 x 32 color images, and if we consider each channel of each pixel as an independent input parameter for the MLP, each neuron in the first hidden layer adds about 3,000 new parameters to the model! With the increase in image size, the situation quickly gets out of hand, producing images that users can’t use for <span class="No-Break">real applications.</span></p>
			<p>One popular solution is to lower the resolution of the images so that an MLP becomes applicable. Nevertheless, when we lower the resolution, we risk losing a large amount of information. It would be great if it were possible to process the information before applying a decrease in quality so that we don’t cause an explosive increase in the number of model parameters. There is a very effective way to solve this problem, which is based on the <span class="No-Break">convolution operation.</span></p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor594"/>Convolution operator</h2>
			<p>This approa<a id="_idTextAnchor595"/>c<a id="_idTextAnchor596"/>h was first used for<a id="_idIndexMarker1268"/> neural networks that worked with images, but it<a id="_idIndexMarker1269"/> has been successfully used to solve problems from other subject areas. Let’s consider using this method for <span class="No-Break">image classification.</span></p>
			<p>Let’s assume that the image pixels that are close to each other interact more closely when forming a feature of interest for us (the feature of an object in the image) than pixels located at a considerable distance. Also, if a small trait is considered very important in the process of image classification, it does not matter in which part of the image this trait <span class="No-Break">is found.</span></p>
			<p>Let’s have a look at the concept of a convolution operator. We have a two-dimensional image of <em class="italic">I</em> and a small <em class="italic">K</em> matrix that has dimensions of <em class="italic">h x w</em> (the so-called convolution kernel) constructed in such a way<a id="_idIndexMarker1270"/> that it graphically encodes a feature. We compute a minimized image of <em class="italic">I * K</em>, superimposing<a id="_idIndexMarker1271"/> the core to the image in all possible ways and recording the sum of the elements of the original image and <span class="No-Break">the kernel:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer913">
					<img alt="" role="presentation" src="image/B19849_Formula_139.jpg"/>
				</div>
			</div>
			<p>An exact definition<a id="_idIndexMarker1272"/> assumes that the kernel matrix is transposed, but for machine learning tasks, it doesn’t matter whether this operation was performed or not. The convolution operator is the basis of the convolutional layer in a CNN. The layer consists of a certain number of kernels, <img alt="" role="presentation" src="image/B19849_Formula_1401.png"/> (with additive displacement components, <img alt="" role="presentation" src="image/B19849_Formula_1412.png"/>, for each kernel), and calculates the convolution of the output image of the previous layer using each of the kernels, each time adding a displacement component. In the end, the activation function, <img alt="" role="presentation" src="image/B19849_Formula_1422.png"/>, can be applied to the entire output image. Usually, the input stream for a convolutional layer consists of <em class="italic">d</em> channels—for example, red/green/blue for the input layer, in which case the kernels are also expanded so that they also consist of <em class="italic">d</em> channels. The following formula is obtained for one channel of the output image of the convolutional layer, where <em class="italic">K</em> is the kernel and <em class="italic">b</em> is the stride (<span class="No-Break">shift) component:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer917">
					<img alt="" role="presentation" src="image/B19849_Formula_143.jpg"/>
				</div>
			</div>
			<p>The following diagram schematically depicts the <span class="No-Break">preceding formula:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer918">
					<img alt="Figure 10.12 – Convolution operation scheme" src="image/B19849_10_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – Convolution operation scheme</p>
			<p>If the additive (stride) component is not equal to 1, then this can be schematically depicted <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer919">
					<img alt="Figure 10.13 – Convolution with the stride equals one" src="image/B19849_10_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.13 – Convolution with the stride equals one</p>
			<p>Please note that since all we are doing here is adding and scaling the input pixels, the kernels can be obtained from the existing training sample using the gradient descent method, similar to calculating weights in an MLP. An MLP could perfectly cope with the functions of the convolutional layer, but it requires a much longer training time, as well as a more significant <a id="_idIndexMarker1273"/>amount of <span class="No-Break">training data.</span></p>
			<p>Notice that the convolution <a id="_idIndexMarker1274"/>operator is not limited to two-dimensional data: most deep learning frameworks provide layers for one-dimensional or <em class="italic">N</em>-dimensional convolutions directly out of the box. It is also worth noting that although the convolutional layer reduces the number of parameters compared to a fully connected layer, it uses more hyperparameters—parameters that are selected <span class="No-Break">before training.</span></p>
			<p>In particular, the following hyperparameters <span class="No-Break">are selected:</span></p>
			<ul>
				<li><strong class="bold">Depth</strong>: How many kernels and bias coefficients will be involved in <span class="No-Break">one layer.</span></li>
				<li>The <strong class="bold">height</strong> and <strong class="bold">width</strong> of <span class="No-Break">each kernel.</span></li>
				<li><strong class="bold">Step (stride)</strong>: How much the kernel is shifted at each step when calculating the next pixel of the resulting image. Usually, the step value that’s taken is equal to 1, and the larger the value is, the smaller the size of the output image <span class="No-Break">that’s produced.</span></li>
				<li><strong class="bold">Padding</strong>: Note that convoluting any kernel of a dimension greater than 1 x 1 reduces the size of the output image. Since it is generally desirable to keep the size of the original image, the pattern is supplemented with zeros along <span class="No-Break">the edges.</span></li>
			</ul>
			<p>One pass of the convolutional<a id="_idIndexMarker1275"/> layer affects the image by reducing the length and width of a particular channel but increasing its <span class="No-Break">value (depth).</span></p>
			<p>Another way to reduce the image<a id="_idIndexMarker1276"/> dimension and save its general properties is to <a id="_idIndexMarker1277"/>downsample the image. Network layers that perform such operations are called <span class="No-Break"><strong class="bold">pooling layers</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor597"/>Pooling operation</h2>
			<p>A pooling l<a id="_idTextAnchor598"/>a<a id="_idTextAnchor599"/>yer receives small, separate<a id="_idIndexMarker1278"/> fragments of the image and combines each<a id="_idIndexMarker1279"/> fragment into one value. There are several possible methods of aggregation. The most straightforward one is to take the maximum from a set of pixels. This method is shown schematically in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer920">
					<img alt="Figure 10.14 – Pooling operation" src="image/B19849_10_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.14 – Pooling operation</p>
			<p>Let’s consider how maximum pooling works. In the preceding diagram, we have a matrix of numbers that’s 6 x 6 in size. The pooling window’s size equals 3, so we can divide this matrix into the four smaller submatrices of size 3 x 3. Then, we can choose the maximum number from each submatrix and make a smaller matrix of size 2 x 2 from <span class="No-Break">these numbers.</span></p>
			<p>The most important characteristic of a convolutional or pooling layer is its receptive field value, which allows<a id="_idIndexMarker1280"/> us to understand how much information is used for processing. Let’s discuss it <span class="No-Break">in detail.</span></p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor600"/>Receptive field</h2>
			<p>An essentia<a id="_idTextAnchor601"/>l<a id="_idTextAnchor602"/> component of the<a id="_idIndexMarker1281"/> convolutional neural network architecture is a reduction in the amount of data from the input to the output of the model while still increasing <a id="_idIndexMarker1282"/>the channel depth. As mentioned earlier, this is usually done by choosing a convolution step (stride) or pooling layers. The receptive field determines how much of the original input from the source is processed at the output. The expansion of the receptive field allows convolutional layers to combine low-level features (lines, edges) to create higher-level features (<span class="No-Break">curves, textures):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer921">
					<img alt="Figure 10.15 – Receptive field concept" src="image/B19849_10_15.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.15 – Receptive field concept</p>
			<p>The receptive field, <img alt="" role="presentation" src="image/B19849_Formula_1441.png"/>, of layer <em class="italic">k</em> can be given by the <span class="No-Break">following formula:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer923">
					<img alt="" role="presentation" src="image/B19849_Formula_145.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_1461.png"/> is the receptive field of the layer, <em class="italic">k - 1</em>, <img alt="" role="presentation" src="image/B19849_Formula_1471.png"/> is the filter size, and <img alt="" role="presentation" src="image/B19849_Formula_1481.png"/> is the stride of layer <em class="italic">i</em>. So, for the preceding example, the input layer has <em class="italic">RF = 1</em>, the hidden layer has <em class="italic">RF = 3</em>, and the last layer has <em class="italic">RF = </em><span class="No-Break"><em class="italic">5</em></span><span class="No-Break">.</span></p>
			<p>Now that we’re acquainted with the basic concepts of CNNs, let’s look at how we can combine them to create a concrete network architecture for <span class="No-Break">image classification.</span></p>
			<h2 id="_idParaDest-218">Convolution network architectur<a id="_idTextAnchor603"/>e<a id="_idTextAnchor604"/></h2>
			<p>The network is developed from a small number of low-level filters in the initial stages to a vast number of filters, each of<a id="_idIndexMarker1283"/> which finds a specific high-level attribute. The transition from level to level provides a hierarchy of <span class="No-Break">pattern recognition.</span></p>
			<p>One of the first convolutional network architectures that was successfully applied to the pattern recognition task was the LeNet-5, which was developed by Yann LeCun, Leon Bottou, Yosuha Bengio, and Patrick Haffner. It was used to recognize handwritten and printed numbers in the 1990s. The following diagram shows <span class="No-Break">this architecture:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer927">
					<img alt="Figure 10.16 – LeNet-5 network architecture" src="image/B19849_10_16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.16 – LeNet-5 network architecture</p>
			<p>The network layers of this architecture are explained in the <span class="No-Break">following table:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table001-4">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Number</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Layer</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Feature </strong><span class="No-Break"><strong class="bold">map (depth)</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Size</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Kernel size</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Stride</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Activation</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Input</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Image</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>32 <span class="No-Break">x 32</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-</p>
						</td>
						<td class="No-Table-Style">
							<p>-</p>
						</td>
						<td class="No-Table-Style">
							<p>-</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Convolution</span></p>
						</td>
						<td class="No-Table-Style">
							<p>6</p>
						</td>
						<td class="No-Table-Style">
							<p>28 <span class="No-Break">x 28</span></p>
						</td>
						<td class="No-Table-Style">
							<p>5 <span class="No-Break">x 5</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">tanh</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Average pool</span></p>
						</td>
						<td class="No-Table-Style">
							<p>6</p>
						</td>
						<td class="No-Table-Style">
							<p>14 <span class="No-Break">x 14</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2 <span class="No-Break">x 2</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">tanh</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Convolution</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">16</span></p>
						</td>
						<td class="No-Table-Style">
							<p>10 <span class="No-Break">x 10</span></p>
						</td>
						<td class="No-Table-Style">
							<p>5 <span class="No-Break">x 5</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">tanh</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Average pool</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">16</span></p>
						</td>
						<td class="No-Table-Style">
							<p>5 <span class="No-Break">x 5</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2 <span class="No-Break">x 2</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">tanh</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>5</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Convolution</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">120</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1 <span class="No-Break">x 1</span></p>
						</td>
						<td class="No-Table-Style">
							<p>5 <span class="No-Break">x 5</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">tanh</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>6</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">FC</span></p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break">84</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-</p>
						</td>
						<td class="No-Table-Style">
							<p>-</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">tanh</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Output</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">FC</span></p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break">10</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-</p>
						</td>
						<td class="No-Table-Style">
							<p>-</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">softmax</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Figure">Table 10.1 – LeNet-5 layer properties</p>
			<p>Notice how the depth and size of the layer are changing toward the final layer. We can see that the depth was increasing and that the size became smaller. This means that toward the final layer, the number<a id="_idIndexMarker1284"/> of features the network can learn increased, but their size became smaller. Such behavior is very common among different convolutional <span class="No-Break">network architectures<a id="_idTextAnchor605"/>.</span></p>
			<p>In the next section, we will discuss deep learning, which is a subset of machine learning that uses artificial neural networks to learn and make decisions. It’s called <em class="italic">deep</em> learning because the neural networks used have multiple layers, allowing them to model complex relationships and patterns <span class="No-Break">in data.</span><a id="_idTextAnchor606"/></p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor607"/>What is deep learning?</h1>
			<p>Most often, the term deep learning is used to describe artificial neural networks that are designed to work with large amounts of data and use complex algorithms to train the model. Algorithms for deep learning can use both supervised and unsupervised algorithms (reinforcement learning). The learning process is <em class="italic">deep</em> because, over time, the neural network covers <a id="_idIndexMarker1285"/>an increasing number of levels. The deeper the network is (that is, the more hidden layers, filters, and levels of feature abstraction it has), the higher the network’s performance. On large datasets, deep learning shows better accuracy than traditional machine <span class="No-Break">learning algorithms.</span></p>
			<p>The real breakthrough that led to the current resurgence of interest in deep neural networks occurred in 2012, after the publication of the article <em class="italic">ImageNet classification with deep convolutional neural networks</em>, by <em class="italic">Alex Krizhevsky</em>, <em class="italic">Ilya Sutskever</em>, and <em class="italic">Geoff Hinton</em> in the <em class="italic">Communications of the ACM</em> magazine. The authors have put together many different learning acceleration techniques. These techniques include convolutional neural networks, the intelligent use of GPUs, and some innovative math tricks: optimized linear neurons (ReLU) and dropout, showing that in a few weeks, they could train a complex neural network to a level that would surpass the result of traditional approaches used in <span class="No-Break">computer vision.</span></p>
			<p>Now, systems based on deep learning are applied in various fields and have successfully replaced the traditional approaches to machine learning. Some examples of areas where deep learning is used are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Speech recognition</strong>: All major commercial speech recognition systems (such as Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu, and iFlytek) are based on <span class="No-Break">deep learning.</span></li>
				<li><strong class="bold">Computer vision</strong>: Today, deep<a id="_idIndexMarker1286"/> learning image recognition systems are already able to give more accurate results than the human eye, for example, when analyzing medical research images (MRI, X-ray, and <span class="No-Break">so on).</span></li>
				<li><strong class="bold">Discovery of new drugs</strong>: For example, the AtomNet neural network was used to predict new biomolecules and was put forward for the treatment of diseases such as the Ebola virus and <span class="No-Break">multiple sclerosis.</span></li>
				<li><strong class="bold">Recommender systems</strong>: Today, deep learning is used to study <span class="No-Break">user preferences.</span></li>
				<li><strong class="bold">Bioinformatics</strong>: It is also used to study the prediction of <span class="No-Break">genetic ontologi<a id="_idTextAnchor608"/>es.</span></li>
			</ul>
			<p>As we delve deeper into the realm of neural network development, we will explore how C++ libraries can be used for creating and training artificial neural <span class="No-Break">network model<a id="_idTextAnchor609"/>s.</span></p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor610"/>Examples of using C++ libraries to create neural networks</h1>
			<p>Many machine learning libraries<a id="_idIndexMarker1287"/> have an API for creating and working with neural networks. All the libraries we used in the previous chapters—<strong class="source-inline">mlpack</strong>, <strong class="source-inline">Dlib</strong>, and <strong class="source-inline">Flashlight</strong>—are supported by neural networks. But there are<a id="_idIndexMarker1288"/> also specialized frameworks for neural networks; for example, one popular one is the PyTorch framework. The difference between a specialized library and a general-purpose library is that a specialized library supports more configurable options and different network types, layers, and loss functions. Also, specialized libraries usually have more modern instruments, and these instruments are introduced to their APIs <span class="No-Break">more quickly.</span></p>
			<p>In this section, we’ll create a simple MLP for a regression task with the <strong class="source-inline">mlpack</strong>, <strong class="source-inline">Dlib</strong>, and <strong class="source-inline">Flashlight</strong> libraries. We’ll also use the PyTorch C++ API to create a more advanced network—a convolutional <a id="_idIndexMarker1289"/>deep neural network with the LeNet5 architecture, which we discussed earlier in the <em class="italic">Convolution network architecture</em> section. We’ll use this network for <span class="No-Break">image </span><span class="No-Break"><a id="_idIndexMarker1290"/></span><span class="No-Break">classifi<a id="_idTextAnchor611"/>cation.</span></p>
			<p>Let’s learn how to use the <strong class="source-inline">mlpack</strong>, <strong class="source-inline">Dlib</strong>, and <strong class="source-inline">Flashlight</strong> libraries to create a simple MLP for a regression task. The task is the same for all series samples—MLP should learn cosine functions at limited intervals. In this book’s code samples, we can find the full program for data generation and MLP training. Here, we’ll discuss the essential parts of the programs that are used for the neural network’s API view. Note that the activation functions we’ll be using for these samples are the Tanh and ReLU functions. We’ve chosen them in order to achieve better convergence for this <span class="No-Break">particula<a id="_idTextAnchor612"/>r<a id="_idTextAnchor613"/> task.</span></p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor614"/>Dlib</h2>
			<p>The <strong class="source-inline">Dlib</strong> library has an API for working with neural networks. It can also be built with Nvidia CUDA support for performance optimization. Using CUDA or OpenCL for GPUs is important if we are planning to <a id="_idIndexMarker1291"/>work with a large amount of data and deep <span class="No-Break">neural networks.</span></p>
			<p>The approach used in the <strong class="source-inline">Dlib</strong> library for <a id="_idIndexMarker1292"/>neural networks is the same as for other machine learning algorithms in this library. We should instantiate and configure an object of the required algorithm class and then use a particular trainer to train it on <span class="No-Break">a dataset.</span></p>
			<p>There is the <strong class="source-inline">dnn_trainer</strong> class for training neural networks in the <strong class="source-inline">Dlib</strong> library. Objects of this class should be initialized with an object of the concrete network and the object of the optimization algorithm. The most popular optimization algorithm is the stochastic gradient descent algorithm with momentum, which we discussed in the <em class="italic">Backpropagation method modes</em> section. This algorithm is implemented in the <strong class="source-inline">sgd</strong> class. Objects of the <strong class="source-inline">sgd</strong> class should be configured with the weight decay regularization and momentum parameter values. The <strong class="source-inline">dnn_trainer</strong> class has the following essential configuration methods: <strong class="source-inline">set_learning_rate</strong>, <strong class="source-inline">set_mini_batch_size</strong>, and <strong class="source-inline">set_max_num_epochs</strong>. These set the learning rate parameter value, the mini-batch size, and the maximum number of training epochs, respectively. Also, this trainer class supports dynamic learning rate change so that we can, for example, make a lower learning rate for later epochs. The learning rate shrink parameter can be configured with the <strong class="source-inline">set_learning_rate_shrink_factor</strong> method. But for the following example, we’ll use the constant learning rate because, for this particular data, it gives<a id="_idIndexMarker1293"/> better <span class="No-Break">training results.</span></p>
			<p>The next essential item for<a id="_idIndexMarker1294"/> instantiating the trainer object is the neural network type object. The <strong class="source-inline">Dlib</strong> library uses a declarative style to define the network architecture, and for this purpose, it uses C++ templates. So, to define the neural network architecture, we should start with the network’s input. In our case, this is of the <strong class="source-inline">matrix&lt;double&gt;</strong> type. We need to pass this as the template argument to the next layer type; in our case, this is the fully connected layer of the <strong class="source-inline">fc</strong> type. The fully connected layer type also takes the number of neurons as the template argument. To define the whole network, we should create the nested type definitions, until we reach the last layer and the loss function. In our case, this is the <strong class="source-inline">loss_mean_squared</strong> type, which implements the mean squared loss function, which is usually used for <span class="No-Break">regression tasks.</span></p>
			<p>The following code snippet shows the network definition with the <strong class="source-inline">Dlib</strong> <span class="No-Break">library API:</span></p>
			<pre class="source-code">
using NetworkType = loss_mean_squared&lt;fc&lt;
    1,
    htan&lt;fc&lt;
        8,
        htan&lt;fc&lt;16,
                htan&lt;fc&lt;32, input&lt;matrix&lt;double&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;;</pre>			<p>This definition can be read in the <span class="No-Break">following order:</span></p>
			<ol>
				<li>We started with the <span class="No-Break">input layer:</span><pre class="source-code">
input&lt;matrix&lt;double&gt;</pre></li>				<li>Then, we added the first hidden layer with <span class="No-Break">32 neurons:</span><pre class="source-code">
fc&lt;32, input&lt;matrix&lt;double&gt;&gt;</pre></li>				<li>After, we added the hyperbolic tangent activation function to the first <span class="No-Break">hidden layer:</span><pre class="source-code">
htan&lt;fc&lt;32, input&lt;matrix&lt;double&gt;&gt;&gt;</pre></li>				<li>Next, we added the second hidden layer with 16 neurons and an <span class="No-Break">activation function:</span><pre class="source-code">
htan&lt;fc&lt;16, htan&lt;fc&lt;32, input&lt;matrix&lt;double&gt;&gt;&gt;&gt;&gt;&gt;</pre></li>				<li>Then, we added the<a id="_idIndexMarker1295"/> third hidden layer with 8 neurons and an <span class="No-Break">activation function:</span><pre class="source-code">
htan&lt;fc&lt;8, htan&lt;fc&lt;16, htan&lt;fc&lt;32,
    input&lt;matrix&lt;double&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</pre></li>				<li>Then, we added the last <a id="_idIndexMarker1296"/>output layer with 1 neuron and without an <span class="No-Break">activation function:</span><pre class="source-code">
fc&lt;1, htan&lt;fc&lt;8, htan&lt;fc&lt;16, htan&lt;fc&lt;32,
    input&lt;matrix&lt;double&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</pre></li>				<li>Finally, we finished with the <span class="No-Break">loss function:</span><pre class="source-code">
loss_mean_squared&lt;...&gt;</pre></li>			</ol>
			<p>The following snippet shows the complete source code example with a <span class="No-Break">network definition:</span></p>
			<pre class="source-code">
size_t n = 10000;
...
std::vector&lt;matrix&lt;double&gt;&gt; x(n);
std::vector&lt;float&gt; y(n);
...
using NetworkType = loss_mean_squared&lt;
fc &lt; 1, htan &lt; fc &lt; 8, htan &lt; fc &lt; 16, htan &lt; fc &lt; 32,
    input &lt; matrix &lt; double &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;;
NetworkType network;
float weight_decay = 0.0001f;
float momentum = 0.5f;
sgd solver(weight_decay, momentum);
dnn_trainer&lt;NetworkType&gt; trainer(network, solver);
trainer.set_learning_rate(0.01);
trainer.set_learning_rate_shrink_factor(1);  // disable learning rate 
                                            //changes
trainer.set_mini_batch_size(64);
trainer.set_max_num_epochs(500);
trainer.be_verbose();
trainer.train(x, y);
network.clean();
auto predictions = network(new_x);</pre>			<p>Now that we’ve configured the trainer object, we can use the <strong class="source-inline">train</strong> method to start the actual training process. This <a id="_idIndexMarker1297"/>method takes two C++ vectors as input parameters. The first one should contain training<a id="_idIndexMarker1298"/> objects of the <strong class="source-inline">matrix&lt;double&gt;</strong> type and the second one should contain the target regression values that are <strong class="source-inline">float</strong> types. We can also call the <strong class="source-inline">be_verbose</strong> method to see the output log of the training process. After the network has been trained, we call the <strong class="source-inline">clean</strong> method to allow the network object to clear the memory from the intermediate training values and therefore reduc<a id="_idTextAnchor615"/>e<a id="_idTextAnchor616"/> <span class="No-Break">memory usage.</span></p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor617"/>mlpack</h2>
			<p>To create the neural <a id="_idIndexMarker1299"/>network with the <strong class="source-inline">mlpack</strong> library, we have<a id="_idIndexMarker1300"/> to start by defining the architecture of the network. We use the <strong class="source-inline">FFN</strong> class in the <strong class="source-inline">mlpack</strong> library to do so, which is used for <a id="_idIndexMarker1301"/>aggregating the network layers. <strong class="bold">FFN</strong> stands for <strong class="bold">feedforward network</strong>. The library has classes for <span class="No-Break">creating layers:</span></p>
			<ul>
				<li><strong class="source-inline">Linear</strong>: The fully connected layer with a value for the <span class="No-Break">output size</span></li>
				<li><strong class="source-inline">Sigmoid</strong>: The sigmoid activation <span class="No-Break">function layer</span></li>
				<li><strong class="source-inline">Convolution</strong>: The 2D <span class="No-Break">convolutional layer</span></li>
				<li><strong class="source-inline">ReLU</strong>: The ReLU activation <span class="No-Break">function layer</span></li>
				<li><strong class="source-inline">MaxPooling</strong>: The maximum <span class="No-Break">pooling layer</span></li>
				<li><strong class="source-inline">Softmax</strong>: The layer with the softmax <span class="No-Break">activation function</span></li>
			</ul>
			<p>There are other <a id="_idIndexMarker1302"/>types of layers in the library. All of them can be<a id="_idIndexMarker1303"/> added to the FFN type object to create a neural network. The first step of neural network creation is <strong class="source-inline">FFN</strong> object instantiation, and it can be done <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
MeanSquaredError loss;
ConstInitialization init(0.);
FFN&lt;MeanSquaredError, ConstInitialization&gt; model( loss, init);</pre>			<p>You can see that <strong class="source-inline">FFN</strong> class constructor takes two arguments. The first one is a loss function object, which in our case is the <strong class="source-inline">MeanSeqaredError</strong> type object. The second one is an initialization object; we used the <strong class="source-inline">ConstantInitialization</strong> type with <strong class="source-inline">0</strong> value. There are other initialization types in the <strong class="source-inline">mlpack</strong> library; for example, you can use the <strong class="source-inline">HeInitialization</strong> or <span class="No-Break"><strong class="source-inline">GlorotInitialization</strong></span><span class="No-Break"> types.</span></p>
			<p>Then, to add a new layer to the network, we can use the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
model.Add&lt;Linear&gt;(8);
model.Add&lt;ReLU&gt;();
...</pre>			<p>The new object layers were added with the <strong class="source-inline">Add</strong> method, and the template parameter was used to specialize the layer type. This method takes as a variable the number of parameters, which depends on the layer type. In this example, we passed a single parameter—the output dimensions of the fully connected linear layer. The <strong class="source-inline">FNN</strong> object automatically configures the <span class="No-Break">input dimension.</span></p>
			<p>Before we will be able to train the network, we have to create an optimization method object. We can do so <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
size_t epochs = 100;
ens::MomentumSGD optimizer(
    /*stepSize=*/0.01,
    /*batchSize=*/ 64,
    /*maxIterations=*/ epochs * x.n_cols,
    /*tolerance=*/1e-10,
    /*shuffle=*/false);</pre>			<p>We created an object that implements stochastic gradient descent optimization with momentum. The optimizer types in the <strong class="source-inline">mlpack</strong> library take not only the optimization parameters, such as the learning<a id="_idIndexMarker1304"/> rate value, but also parameters to<a id="_idIndexMarker1305"/> configure the whole training cycle. We passed the learning rate, the batch size, the number of iterations, the loss value tolerance for early stopping, and the flag to shuffle the dataset as arguments. Notice that we didn’t pass the training epochs number directly; instead, we calculated the <strong class="source-inline">maxIteration</strong> parameter value as a product of the epoch number and the training element number. The <strong class="source-inline">MomentumSGD</strong> class is just the template specialization of the SGD class with the <strong class="source-inline">MomentumUpdate</strong> policy class. So, to update the default momentum value, we have to access the particular field, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
optimizer.UpdatePolicy().Momentum() = 0.5;</pre>			<p>There are various other optimizers in the <strong class="source-inline">mlpack</strong> library that follow the same <span class="No-Break">initialization scheme.</span></p>
			<p>Having the network and the optimizer objects, we can train a model <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
model.Train(x, y, optimizer);</pre>			<p>We passed the <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> matrices with training samples and the optimizer objects as arguments into the <strong class="source-inline">Train</strong> method of the <strong class="source-inline">FFN</strong> type object. There is no special type for a dataset in the <strong class="source-inline">mlpack</strong> library, so the raw <strong class="source-inline">arma::mat</strong> objects are used for this purpose. In the general case, the training is done silently, which is not useful during experiments. So, there are additional parameters in the <strong class="source-inline">Train</strong> method to add verbosity. Following the optimizer parameter, the <strong class="source-inline">Train</strong> method accepts a number of callbacks. For example, if we want to see the training process log with loss values in the console, we can add the <strong class="source-inline">ProgressBar</strong> object callback <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
model.Train(x, y, optimizer, ens::ProgressBar());</pre>			<p>Also, we can add another type<a id="_idIndexMarker1306"/> callback. In the following<a id="_idIndexMarker1307"/> example, we add the early stopping callback and the callback to record the best <span class="No-Break">parameter values:</span></p>
			<pre class="source-code">
ens::StoreBestCoordinates&lt;arma::mat&gt; best_params;
model.Train(scaled_x,
            scaled_y,
            optimizer,
            ens::ProgressBar(),
            ens::EarlyStopAtMinLoss(20),
            best_params);</pre>			<p>We configured the training to stop if the loss value doesn’t change for 20 batches, and to save parameters for the best loss value into the <span class="No-Break"><strong class="source-inline">best_params</strong></span><span class="No-Break"> object.</span></p>
			<p>The complete source code for this example is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
MeanSquaredError loss;
ConstInitialization init(0.);
FFN&lt;MeanSquaredError, ConstInitialization&gt; model( loss, init);
model.Add&lt;Linear&gt;(8);
model.Add&lt;ReLU&gt;();
model.Add&lt;Linear&gt;(16);
model.Add&lt;ReLU&gt;();
model.Add&lt;Linear&gt;(32);
model.Add&lt;ReLU&gt;();
model.Add&lt;Linear&gt;(1);
// Define optimizer
size_t epochs = 100;
ens::MomentumSGD optimizer(
    /*stepSize=*/0.01,
    /*batchSize= */ 64,
    /*maxIterations= */ epochs * x.n_cols,
    /*tolerance=*/1e-10,
    /*shuffle=*/false);
ens::StoreBestCoordinates&lt;arma::mat&gt; best_params;
model.Train(x, y, optimizer, ens::ProgressBar(),
            ens::EarlyStopAtMinLoss(20), best_params);</pre>			<p>After we have a trained model, we can use it for prediction <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
arma::mat predictions;
model.Predict(x, predictions);</pre>			<p>Here we created an output<a id="_idIndexMarker1308"/> variable named <strong class="source-inline">predictions</strong> and <a id="_idIndexMarker1309"/>passed it with the input variable, <strong class="source-inline">x</strong>, to the <strong class="source-inline">Predict</strong> method. The <strong class="source-inline">model</strong> object has all the latest trained weights, but we can replace them with the best weights that we saved in the <strong class="source-inline">best_weights</strong> callback <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
model.Parameters() = best_params.BestCoordinates();</pre>			<p>We just replaced the current weights by using the <strong class="source-inline">Parameters</strong> method of the <span class="No-Break"><strong class="source-inline">model</strong></span><span class="No-Break"> object.</span></p>
			<p>In the next sub-section, we will implement<a id="_idIndexMarker1310"/> the same neural <a id="_idIndexMarker1311"/>network but wi<a id="_idTextAnchor618"/>t<a id="_idTextAnchor619"/>h the <span class="No-Break">Flashlight framework.</span></p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor620"/>Flashlight</h2>
			<p>To create a neural network with the<a id="_idIndexMarker1312"/> Flashlight library, you have to<a id="_idIndexMarker1313"/> follow the same steps as we did with <strong class="source-inline">mlpack</strong> library. The main difference is that you will need to implement the training loop yourself. This gives you more flexibility when you deal with complex architectures and training approaches. Let’s start with a network definition. We create a feedforward model with fully connected linear layers <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
fl::Sequential model;
model.add(fl::View({1, 1, 1, -1}));
model.add(fl::Linear(1, 8));
model.add(fl::ReLU());</pre>			<p>The <strong class="source-inline">Sequential</strong> class was used to create the network object, and then the <strong class="source-inline">add</strong> method was used to populate it with layers. We used the <strong class="source-inline">Linear</strong> and <strong class="source-inline">ReLU</strong> layers as we did in the previous example. The main difference is that the first layer we added was the <strong class="source-inline">View</strong> type object. It was needed to make the model correctly process a batch of input data. The Flashlight tensor<a id="_idIndexMarker1314"/> data layout is <strong class="bold">width, height, channels, batch</strong> (<strong class="bold">WHCN</strong>). So, the view shape <strong class="source-inline">{1,1,1,-1}</strong> means that our input data is single-channel, one-dimensional data, and the batch size should be detected automatically because we used <strong class="source-inline">-1</strong> for the <span class="No-Break">last dimension.</span></p>
			<p>Next, we have to define a loss function object <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
auto loss = fl::MeanSquaredError();</pre>			<p>We used the MSE loss again because we are solving the same regression task. Creating an optimizer object also looks the same as for <span class="No-Break">other frameworks:</span></p>
			<pre class="source-code">
float learning_rate = 0.01;
float momentum = 0.5;
auto sgd = fl::SGDOptimizer(model.params(), learning_rate, momentum);</pre>			<p>We also used stochastic gradient descent with a momentum algorithm. Notice that the optimizer object constructor takes the model parameters as the first argument. It’s a different approach from the <strong class="source-inline">Dlib</strong> and <strong class="source-inline">mlpack</strong> libraries, where the training process is mostly hidden in the top-level <span class="No-Break">training</span><span class="No-Break"><a id="_idIndexMarker1315"/></span><span class="No-Break"> API.</span></p>
			<p>The approach where you pass model<a id="_idIndexMarker1316"/> parameters to an optimizer is more common for frameworks where you configure the training process more precisely; you will see it <span class="No-Break">in PyTorch.</span></p>
			<p>Having all base blocks initialized, we can implement a training loop. Such a loop will contain the following <span class="No-Break">important steps:</span></p>
			<ol>
				<li>Prediction step—the <span class="No-Break">forward propagation.</span></li>
				<li>Loss <span class="No-Break">value calculation.</span></li>
				<li>Gradients <span class="No-Break">calculation—the backpropagation.</span></li>
				<li>Optimization step where the gradient values will <span class="No-Break">be used.</span></li>
				<li><span class="No-Break">Clearing gradients.</span></li>
			</ol>
			<p>We can implement these steps <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
const int epochs = 500;
for (int epoch_i = 0; epoch_i &lt; epochs; ++epoch_i) {
  for (auto&amp; batch : batch_dataset) {
    // Forward propagation
    auto predicted = model(fl::input(batch[0]));
    // Calculate loss
    auto local_batch_size = batch[0].shape().dim(0);
    auto target =
        fl::reshape(batch[1], {1, 1, 1, local_batch_size});
    auto loss_value =
        loss(predicted,
             fl::noGrad(target));  // Backward propagation
    loss_value.backward();
    // Optimization - updating parameters
    sgd.step();
    // clearing graients
    sgd.zeroGrad();
  }
}</pre>			<p>Notice that we made two loops, one over epochs and an internal one over batches, in our training dataset. In the inner loop, we used the <strong class="source-inline">batch_dataset</strong> variable; we assume that it has the <strong class="source-inline">fl::BatchDataset</strong> dataset type, so the <strong class="source-inline">batch</strong> loop variable is the <strong class="source-inline">std::vector</strong> of tensors. Usually, it will<a id="_idIndexMarker1317"/> have only two tensors, for the input data and the target <span class="No-Break">batch data.</span></p>
			<p>We used the <strong class="source-inline">fl::input</strong> function <a id="_idIndexMarker1318"/>to wrap our input batch tensor, <strong class="source-inline">batch[0]</strong>, into the Flashlight <strong class="source-inline">Variable</strong> type with disabled gradient calculations. The <strong class="source-inline">Variable</strong> type is used for the Flashlight auto-gradient mechanism. For the target batch data, <strong class="source-inline">batch[1]</strong>, we used the <strong class="source-inline">fl::noGrad</strong> function to disable <span class="No-Break">gradient calculation.</span></p>
			<p>Our <strong class="source-inline">model</strong> object returns a prediction tensor with a 4D shape in the WHCN format. If didn’t reshape your dataset for a convenient shape, you will have to use the <strong class="source-inline">fl::reshape</strong> function for every batch as we did in this example; otherwise, you will get shape inconsistency errors in the loss <span class="No-Break">value calculation.</span></p>
			<p>After we calculated the loss value with the <strong class="source-inline">loss</strong> object using the predicted and target values, we calculated the gradient values. This was done with the <strong class="source-inline">backward</strong> method of the <strong class="source-inline">loss_value</strong> object, which has the <span class="No-Break"><strong class="source-inline">fl::Variable</strong></span><span class="No-Break"> type.</span></p>
			<p>Having the gradient values calculated, we used the <strong class="source-inline">step</strong> method of the <strong class="source-inline">sgd</strong> object to apply the optimization step for the network parameters. Remember that we initialized the optimization <strong class="source-inline">sgd</strong> object with the model parameters (weights). For the final step, we called the <strong class="source-inline">zeroGrad</strong> method for the optimizer to clear the network <span class="No-Break">parameter gradients.</span></p>
			<p>When the network (model) is trained, it’s easy <a id="_idIndexMarker1319"/>to use it for prediction, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
auto predicted = model(fl::noGrad(x));</pre>			<p><strong class="source-inline">x</strong> should be your input data. Disabling<a id="_idIndexMarker1320"/> the gradient calculation for the model evaluation (prediction) stage is very important because it can save a lot of computational resources and increase overall <span class="No-Break">model throughput.</span></p>
			<p>In the next section, we will implement a more complex neural network to solve an image classific<a id="_idTextAnchor621"/>a<a id="_idTextAnchor622"/>tion task using the <span class="No-Break"><strong class="source-inline">PyTorch</strong></span><span class="No-Break"> library.</span></p>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor623"/>Understanding image classification using the LeNet architecture</h1>
			<p>In this section, we’ll implement<a id="_idIndexMarker1321"/> a CNN for image classification. We are <a id="_idIndexMarker1322"/>going to use the famous<a id="_idIndexMarker1323"/> dataset of handwritten digits called the <strong class="bold">Modified National Institute of Standards and Technology </strong>(<strong class="bold">MNIST</strong>) dataset, which can be found at <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>. The dataset is a standard that was proposed by the <em class="italic">US National Institute of Standards and Technology</em> to calibrate and compare image recognition methods using machine learning, primarily based on <span class="No-Break">neural networks.</span></p>
			<p>The creators of the dataset used a set of samples from the US Census Bureau, with some samples written by students of American universities added later. All the samples are normalized, anti-aliased grayscale images <a id="_idIndexMarker1324"/>of 28 x 28 pixels. The MNIST dataset contains 60,000 images for training <a id="_idIndexMarker1325"/>and 10,000 images for testing. There are <span class="No-Break">four files:</span></p>
			<ul>
				<li><strong class="source-inline">train-images-idx3-ubyte</strong>: Training <span class="No-Break">set images</span></li>
				<li><strong class="source-inline">train-labels-idx1-ubyte</strong>: Training <span class="No-Break">set labels</span></li>
				<li><strong class="source-inline">t10k-images-idx3-ubyte</strong>: Test <span class="No-Break">set images</span></li>
				<li><strong class="source-inline">t10k-labels-idx1-ubyte</strong>: Test <span class="No-Break">set labels</span></li>
			</ul>
			<p>The files that contain labels are in the <span class="No-Break">following format:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table002-2">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Offset</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Type</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Value</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">32-bit integer</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0x00000801(2049)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Magic number (<span class="No-Break">MSB first)</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">32-bit integer</span></p>
						</td>
						<td class="No-Table-Style">
							<p>60,000 <span class="No-Break">or 10,000</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Number <span class="No-Break">of items</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>8</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Unsigned char</span></p>
						</td>
						<td class="No-Table-Style">
							<p>??</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Label</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>9</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Unsigned char</span></p>
						</td>
						<td class="No-Table-Style">
							<p>??</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Label</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>...</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.2 – MNIST labels file format</p>
			<p>The label values are from <strong class="source-inline">0</strong> to <strong class="source-inline">9</strong>. The files that contain images are in the <span class="No-Break">following format:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table003">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Offset</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Type</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Value</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">32-bit integer</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0x00000803(2051)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Magic number (<span class="No-Break">MSB first)</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">32-bit integer</span></p>
						</td>
						<td class="No-Table-Style">
							<p>60,000 <span class="No-Break">or 10,000</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Number <span class="No-Break">of images</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">32-bit integer</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">28</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Number <span class="No-Break">of rows</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">32-bit integer</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">28</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Number <span class="No-Break">of columns</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Unsigned byte</span></p>
						</td>
						<td class="No-Table-Style">
							<p>??</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Pixel</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Unsigned byte</span></p>
						</td>
						<td class="No-Table-Style">
							<p>??</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Pixel</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>...</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Figure">Table 10.3 – MNIST image file format</p>
			<p>Pixels are stored in a row-wise manner, with values in the range of [0, 255]. <strong class="source-inline">0</strong> means background (white), while <strong class="source-inline">255</strong> means <span class="No-Break">foreground (black).</span></p>
			<p>In this example, we are<a id="_idIndexMarker1326"/> using the PyTorch deep learning framework. This framework is primarily used with the <a id="_idIndexMarker1327"/>Python language. However, its core part is written in C++, and it has a well-documented and actively developed C++ client API called <strong class="bold">LibPyTorch</strong>. This<a id="_idIndexMarker1328"/> framework is based on the linear algebra library called <strong class="bold">ATen</strong>, which<a id="_idIndexMarker1329"/> heavily uses the Nvidia CUDA technology for performance improvement. The Python and C++ APIs are pretty much the same but have different language notations, so we can use the official Python documentation to learn how to use the framework. This documentation also contains a section stating the differences between C++ and Python APIs and specific articles about the usage of the <span class="No-Break">C++ API.</span></p>
			<p>The PyTorch framework is widely used for research in deep learning. As we discussed previously, the framework provides functionality for managing big datasets. It can automatically parallelize loading the data from a disk, manage pre-loaded buffers for the data to reduce memory usage, and limit expensive performance disk operations. It provides the <strong class="source-inline">torch::data::Dataset</strong> base class for the implementation of the user custom dataset. We only need to override two methods here: <strong class="source-inline">get</strong> and <strong class="source-inline">size</strong>. These methods are not virtual because we have to use the C++ template’s polymorphism to inherit from <span class="No-Break">this cl<a id="_idTextAnchor624"/>a<a id="_idTextAnchor625"/>ss.</span></p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor626"/>Reading the training dataset</h2>
			<p>Consider the <strong class="source-inline">MNISTDataset</strong> class, which <a id="_idIndexMarker1330"/>provides access to the MNIST dataset. The constructor of this class takes two parameters: one is the name of the file that contains the images, and the other is the name of the file that contains the labels. It loads whole files into its memory, which is not a best practice, but for this dataset, this approach works well because the dataset is small. For bigger datasets, we have to implement another scheme of reading data from the disk because usually, for real tasks, we are unable to load all the data into the <span class="No-Break">computer’s memory.</span></p>
			<p>We use the OpenCV library to<a id="_idIndexMarker1331"/> deal with images, so we store all the loaded images in the C++ <strong class="source-inline">vector</strong> of the <strong class="source-inline">cv::Mat</strong> type. Labels are stored in a vector of the <strong class="source-inline">unsigned char</strong> type. We write two additional helper functions to read images and labels from the disk: <strong class="source-inline">ReadImages</strong> and <strong class="source-inline">ReadLabels</strong>. The following snippet shows the header file for <span class="No-Break">this class:</span></p>
			<pre class="source-code">
#include &lt;torch/torch.h&gt;
#include &lt;opencv2/opencv.hpp&gt;
#include &lt;string&gt;
class MNISTDataset
    : public torch::data::Dataset&lt;MNISTDataset&gt; {
 public:
  MNISTDataset(const std::string&amp; images_file_name,
               const std::string&amp; labels_file_name);
  // torch::data::Dataset implementation
  torch::data::Example&lt;&gt; get(size_t index) override;
  torch::optional&lt;size_t&gt; size() const override;
 private:
  void ReadLabels(const std::string&amp; labels_file_name);
  void ReadImages(const std::string&amp; images_file_name);
  uint32_t rows_ = 0;
  uint32_t columns_ = 0;
  std::vector&lt;unsigned char&gt; labels_;
  std::vector&lt;cv::Mat&gt; images_;
}</pre>			<p>The following snippet shows the implementation of the public interface of <span class="No-Break">the class:</span></p>
			<pre class="source-code">
MNISTDataset::MNISTDataset(
    const std::string&amp; images_file_name,
    const std::string&amp; labels_file_name) {
  ReadLabels(labels_file_name);
  ReadImages(images_file_name);
}</pre>			<p>We can see that the constructor<a id="_idIndexMarker1332"/> passed the filenames to the corresponding loader functions. The <strong class="source-inline">size</strong> method returns the number of items that were loaded from the disk into the <span class="No-Break"><strong class="source-inline">labels</strong></span><span class="No-Break"> container:</span></p>
			<pre class="source-code">
torch::optional&lt;size_t&gt; MNISTDataset::size() const {
    return labels_.size();
}</pre>			<p>The following snippet shows the <strong class="source-inline">get</strong> <span class="No-Break">method’s implementation:</span></p>
			<pre class="source-code">
torch::data::Example&lt;&gt; MNISTDataset::get(size_t index) {
  return {
      CvImageToTensor(images_[index]),
      torch::tensor(static_cast&lt;int64_t&gt;(labels_[index]),
                    torch::TensorOptions()
                        .dtype(torch::kLong)
                        .device(torch::DeviceType::CUDA))};
}</pre>			<p>The <strong class="source-inline">get</strong> method returns an object of the <strong class="source-inline">torch::data::Example&lt;&gt;</strong> class. In general, this type holds two values: the training sample represented with the <strong class="source-inline">torch::Tensor</strong> type and the target value, which is also represented with the <strong class="source-inline">torch::Tensor</strong> type. This method retrieves an image from the corresponding container using a given subscript, converts the image into the <strong class="source-inline">torch::Tensor</strong> type with the <strong class="source-inline">CvImageToTensor</strong> function, and uses the label value converted into the <strong class="source-inline">torch::Tensor</strong> type as a <span class="No-Break">target value.</span></p>
			<p>There is a set of <strong class="source-inline">torch::tensor</strong> functions that are used to convert a C++ variable into the <strong class="source-inline">torch::Tensor</strong> type. They automatically deduce the variable type and create a tensor with corresponding values. In our case, we explicitly convert the label into the <strong class="source-inline">int64_t</strong> type because the<a id="_idIndexMarker1333"/> loss function we’ll be using later assumes that the target values have a <strong class="source-inline">torch::Long</strong> type. Also, notice that we passed <strong class="source-inline">torch::TensorOptions</strong> as a second argument to the <strong class="source-inline">torch::tensor</strong> function. We specified the <strong class="source-inline">torch</strong> type of the tensor values and told the system to place this tensor in the GPU memory by setting the <strong class="source-inline">device</strong> argument to be equal to the <strong class="source-inline">torch::DeviceType::CUDA value</strong> and by using the <strong class="source-inline">torch::TensorOptions</strong> object. When we manually create the PyTorch tensors, we have to explicitly configure where to place them—in the CPU or in the GPU. Tensors that are placed in different types of memory can’t be <span class="No-Break">used together.</span></p>
			<p>To convert the OpenCV image into a tensor, write the <span class="No-Break">following function:</span></p>
			<pre class="source-code">
torch::Tensor CvImageToTensor(const cv::Mat&amp; image) {
  assert(image.channels() == 1);
  std::vector&lt;int64_t&gt; dims{
      static_cast&lt;int64_t&gt;(1),
      static_cast&lt;int64_t&gt;(image.rows),
      static_cast&lt;int64_t&gt;(image.cols)};
  torch::Tensor tensor_image =
      torch::from_blob(image.data, torch::IntArrayRef(dims),
                       // clone is required to copy data
                       // from temporary object
                       torch::TensorOptions()
                           .dtype(torch::kFloat)
                           .requires_grad(false))
          .clone();
  return tensor_image.to(torch::DeviceType::CUDA);
}</pre>			<p>The most important part of this function is the call to the <strong class="source-inline">torch::from_blob</strong> function. This function constructs the tensor from values located in memory that are referenced by the pointer that’s passed as a first argument. A second argument should be a C++ vector with tensor dimension values; in our case, we specified a three-dimensional tensor with one<a id="_idIndexMarker1334"/> channel and two image dimensions. The third argument is the <strong class="source-inline">torch::TensorOptions</strong> object. We specified that the data should be of the floating-point type and that it doesn’t require a <span class="No-Break">gradient calculation.</span></p>
			<p>PyTorch uses the auto-gradient approach for model training, and it means that it doesn’t construct a static network graph with pre-calculated gradient dependencies. Instead, it uses a dynamic network graph, which means that gradient flow paths for modules are connected and calculated dynamically during the backward pass of the training process. Such an architecture allows us to dynamically change the network’s topology and characteristics while running the program. All the libraries we covered previously use a static <span class="No-Break">network graph.</span></p>
			<p>The third interesting PyTorch function that’s used here is the <strong class="source-inline">torch::Tensor::to</strong> function, which allows us to move tensors from CPU memory to GPU memory <span class="No-Break">and back.</span></p>
			<p>Now, let’s learn how to read <span class="No-Break">datas<a id="_idTextAnchor627"/>e<a id="_idTextAnchor628"/>t files.</span></p>
			<h3>Reading dataset files</h3>
			<p>We read the labels file<a id="_idIndexMarker1335"/> with the <span class="No-Break"><strong class="source-inline">ReadLabels</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
void MNISTDataset::ReadLabels(
    const std::string&amp; labels_file_name) {
  std::ifstream labels_file(
      labels_file_name,
      std::ios::binary | std::ios::binary);
  labels_file.exceptions(std::ifstream::failbit |
                         std::ifstream::badbit);
  if (labels_file) {
    uint32_t magic_num = 0;
    uint32_t num_items = 0;
    if (read_header(&amp;magic_num, labels_file) &amp;&amp;
        read_header(&amp;num_items, labels_file)) {
      labels_.resize(static_cast&lt;size_t&gt;(num_items));
      labels_file.read(
          reinterpret_cast&lt;char*&gt;(labels_.data()),
          num_items);
    }
  }
}</pre>			<p>This function opens the file in binary mode and reads the header records, the magic number, and the number of<a id="_idIndexMarker1336"/> items in the file. It also reads all the items directly to the C++ vector. The most important part is to correctly read the header records. To do this, we can use the <span class="No-Break"><strong class="source-inline">read_header</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
template &lt;class T&gt;
bool read_header(T* out, std::istream&amp; stream) {
  auto size = static_cast&lt;std::streamsize&gt;(sizeof(T));
  T value;
  if (!stream.read(reinterpret_cast&lt;char*&gt;(&amp;value), size)) {
    return false;
  } else {
    // flip endianness
    *out = (value &lt;&lt; 24) | ((value &lt;&lt; 8) &amp; 0x00FF0000) |
           ((value &gt;&gt; 8) &amp; 0X0000FF00) | (value &gt;&gt; 24);
    return true;
  }
}</pre>			<p>This function reads the value from the input stream—in our case, the file stream—and flips the endianness. This function also assumes that header records are 32-bit integer values. In a different<a id="_idIndexMarker1337"/> scenario, we would have to think of other ways to flip <span class="No-Break">the<a id="_idTextAnchor629"/> <a id="_idTextAnchor630"/>endianness.</span></p>
			<h3>Reading the image file</h3>
			<p>Reading the images file is also pretty straightforward; we read the header records and sequentially read the images. From the<a id="_idIndexMarker1338"/> header records, we get the total number of images in the file and the image size. Then, we define the OpenCV matrix object that has a corresponding size and type—the one-channel image with the underlying byte <strong class="source-inline">CV_8UC1</strong> type. We read images from the disk in a loop directly to the OpenCV matrix object by passing a pointer, which is returned by the <strong class="source-inline">data</strong> object variable, to the stream read function. The size of the data we need to read is determined by calling the <strong class="source-inline">cv::Mat::size()</strong> function, followed by the call to the <strong class="source-inline">area</strong> function. Then, we use the <strong class="source-inline">convertTo</strong> OpenCV function to convert an image from the <strong class="source-inline">unsigned byte</strong> type to the 32-bit floating-point type. This is important so that we have enough precision while performing math operations in the network layers. We also normalize all the data so that it’s in the range [0, 1] by dividing it <span class="No-Break">by 255.</span></p>
			<p>We resize all the images so that they’re 32 x 32 in size because the LeNet5 network architecture requires us to hold the original dimensions of the <span class="No-Break">convolution filters:</span></p>
			<pre class="source-code">
void MNISTDataset::ReadImages(
    const std::string&amp; images_file_name) {
  std::ifstream images_file(
      images_file_name,
      std::ios::binary | std::ios::binary);
  labels_file.exceptions(std::ifstream::failbit |
                         std::ifstream::badbit);
  if (labels_file) {
    uint32_t magic_num = 0;
    uint32_t num_items = 0;
    rows_ = 0;
    columns_ = 0;
    if (read_header(&amp;magic_num, labels_file) &amp;&amp;
        read_header(&amp;num_items, labels_file) &amp;&amp;
        read_header(&amp;rows_, labels_file) &amp;&amp;
        read_header(&amp;columns_, labels_file)) {
      assert(num_items == labels_.size());
      images_.resize(num_items);
      cv::Mat img(static_cast&lt;int&gt;(rows_),
                  static_cast&lt;int&gt;(columns_), CV_8UC1);
      for (uint32_t i = 0; i &lt; num_items; ++i) {
        images_file.read(reinterpret_cast&lt;char*&gt;(img.data),
                         static_cast&lt;std::streamsize&gt;(
                             img.size().area()));
        img.convertTo(images_[i], CV_32F);
        images_[i] /= 255;  // normalize
        cv::resize(images_[i], images_[i],
                   cv::Size(32, 32));  // Resize to
        // 32x32 size
      }
    }
  }
}</pre>			<p>Now that we’ve loaded the<a id="_idIndexMarker1339"/> training data, we have to define our <span class="No-Break">n<a id="_idTextAnchor631"/>e<a id="_idTextAnchor632"/>ural network.</span></p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor633"/>Neural network definition</h2>
			<p>In this example, we chose the LeNet5 architecture, which was developed by Yann LeCun, Leon Bottou, Yosuha Bengio, and Patrick Haffner (<a href="http://yann.lecun.com/exdb/lenet/">http://yann.lecun.com/exdb/lenet/</a>). The architecture’s details were <a id="_idIndexMarker1340"/>discussed earlier in the <em class="italic">Convolution network architecture </em>section. Here, we’ll show you how to implement it with the <span class="No-Break">PyTorch framework.</span></p>
			<p>All the structural parts of the neural networks in the PyTorch framework should be derived from the <strong class="source-inline">torch::nn::Module</strong> class. The following snippet shows the header file of the <span class="No-Break"><strong class="source-inline">LeNet5</strong></span><span class="No-Break"> class:</span></p>
			<pre class="source-code">
#include &lt;torch/torch.h&gt;
class LeNet5Impl : public torch::nn::Module {
    public:
        LeNet5Impl();
        torch::Tensor forward(torch::Tensor x);
    private:
        torch::nn::Sequential conv_;
        torch::nn::Sequential full_;
};
TORCH_MODULE(LeNet5);</pre>			<p>Notice that we defined the intermediate implementation class, which is called <strong class="source-inline">LeNet5Impl</strong>. This is because PyTorch uses a memory management model based on smart pointers, and all the modules should be wrapped in a special type. There is a special class called <strong class="source-inline">torch::nn::ModuleHolder</strong>, which is a wrapper around <strong class="source-inline">std::shared_ptr</strong> but also defines some additional methods for managing modules. So, if we want to follow all PyTorch conventions and use our module (network) with all PyTorch’s functions without any<a id="_idIndexMarker1341"/> problems, our module class definition should be <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
class Name : public torch::nn::ModuleHolder&lt;Impl&gt; {}</pre>			<p><strong class="source-inline">Impl</strong> is the implementation of our module, which is derived from the <strong class="source-inline">torch::nn::Module</strong> class. There is a special macro that can do this definition for us automatically; it is called <strong class="source-inline">TORCH_MODULE</strong>. We need to specify the name of our module in order to <span class="No-Break">use it.</span></p>
			<p>The most important function in this definition is the <strong class="source-inline">forward</strong> function. This function, in our case, takes the network’s input and passes it through all the network layers until an output value is returned from this function. If we don’t implement a whole network but rather <em class="italic">some</em> custom layers or <em class="italic">some</em> structural parts of a network, this function should assume we take the values from the previous layers or other parts of the network as input. Also, if we are implementing a custom module that isn’t from the PyTorch standard modules, we should define the <strong class="source-inline">backward</strong> function, which should calculate gradients for our <span class="No-Break">custom operations.</span></p>
			<p>The next essential thing in our module definition is the usage of the <strong class="source-inline">torch::nn::Sequential</strong> class. This class is used to group sequential layers in the network and automate the process of forwarding values between them. We broke our network into two parts, one containing convolutional layers and another containing the final fully <span class="No-Break">connected layers.</span></p>
			<p>The PyTorch framework contains many functions for creating layers. For example, the <strong class="source-inline">torch::nn::Conv2d</strong> function created the two-dimensional convolutional layer. Another way to create a layer in PyTorch is to use the <strong class="source-inline">torch::nn::Functional</strong> function to wrap some simple function into the layer, which can then be connected with all the outputs of the previous layer. Notice that activation functions are not part of the neurons in PyTorch and should be connected as a separate layer. The following code snippet shows the definition of <a id="_idIndexMarker1342"/>our <span class="No-Break">network components:</span></p>
			<pre class="source-code">
static std::vector&lt;int64_t&gt; k_size = {2, 2};
static std::vector&lt;int64_t&gt; p_size = {0, 0};
LeNet5Impl::LeNet5Impl() {
  conv_ = torch::nn::Sequential(
      torch::nn::Conv2d(torch::nn::Conv2dOptions(1, 6, 5)),
      torch::nn::Functional(torch::tanh),
      torch::nn::Functional(
          torch::avg_pool2d,
          /*kernel_size*/ 
          /*kernel_size*/ torch::IntArrayRef(k_size),
          /*stride*/ torch::IntArrayRef(k_size),
          /*padding*/ torch::IntArrayRef(p_size),
          /*ceil_mode*/ false,
          /*count_include_pad*/ false),
      torch::nn::Conv2d(torch::nn::Conv2dOptions(6, 16, 5)),
      torch::nn::Functional(torch::tanh),
      torch::nn::Functional(
          torch::avg_pool2d,
          /*kernel_size*/ torch::IntArrayRef(k_size),
          /*stride*/ torch::IntArrayRef(k_size),
          /*padding*/ torch::IntArrayRef(p_size),
          /*ceil_mode*/ false,
          /*count_include_pad*/ false),
      torch::nn::Conv2d(
          torch::nn::Conv2dOptions(16, 120, 5)),
      torch::nn::Functional(torch::tanh));
  register_module("conv", conv_);
  full_ = torch::nn::Sequential(
      torch::nn::Linear(torch::nn::LinearOptions(120, 84)),
      torch::nn::Functional(torch::tanh),
      torch::nn::Linear(torch::nn::LinearOptions(84, 10)));
  register_module("full", full_);
}</pre>			<p>Here, we initialized two <strong class="source-inline">torch::nn::Sequential</strong> modules. They take a variable number of other modules as arguments for constructors. Notice that for the initialization of the <strong class="source-inline">torch::nn::Conv2d</strong> module, we have to pass the instance of the <strong class="source-inline">torch::nn::Conv2dOptions</strong> class, which can be initialized with the number of input channels, the number of output channels, and the kernel size. We used <strong class="source-inline">torch::tanh</strong> as an activation <a id="_idIndexMarker1343"/>function; notice that it is wrapped in the <strong class="source-inline">torch::nn::Functional</strong> <span class="No-Break">class instance.</span></p>
			<p>The average pooling function is also wrapped in the <strong class="source-inline">torch::nn::Functional</strong> class instance because it is not a layer in the PyTorch C++ API; it’s a function. Also, the pooling function takes several arguments, so we bound their fixed values. When a function in PyTorch requires the values of the dimensions, it assumes that we provide an instance of the <strong class="source-inline">torch::IntArrayRef</strong> type. An object of this type behaves as a wrapper for an array with dimension values. We should be careful here because such an array should exist at the same time as the wrapper lifetime; notice that <strong class="source-inline">torch::nn::Functional</strong> stores <strong class="source-inline">torch::IntArrayRef</strong> objects internally. That is why we defined <strong class="source-inline">k_size</strong> and <strong class="source-inline">p_size</strong> as static <span class="No-Break">global variables.</span></p>
			<p>Also, pay attention to the <strong class="source-inline">register_module</strong> function. It associates the string name with the module and registers it in the internals of the parent module. If the module is registered in a certain way, we can use a string-based parameter search later (often used when we need to manually manage weight updates during training) and automatic <span class="No-Break">module serialization.</span></p>
			<p>The <strong class="source-inline">torch::nn::Linear</strong> module defines the fully connected layer and should be initialized with an instance of the <strong class="source-inline">torch::nn::LinearOptions</strong> type, which defines the number of inputs and the number of outputs, that is, a count of the layer’s neurons. Notice that the last layer returns 10 values, not one label, despite us only having a single target label. This is the standard approach in <span class="No-Break">classification tasks.</span></p>
			<p>The following code <a id="_idIndexMarker1344"/>shows the <strong class="source-inline">forward</strong> function’s implementation, which performs <span class="No-Break">model inference:</span></p>
			<pre class="source-code">
torch::Tensor LeNet5Impl::forward(at::Tensor x) {
    auto output = conv_-&gt;forward(x);
    output = output.view({x.size(0), -1});
    output = full_-&gt;forward(output);
    output = torch::log_softmax(output, -1);
    return output;
}</pre>			<p>This function is implemented <span class="No-Break">as follows:</span></p>
			<ol>
				<li>We passed the input tensor (image) to the <strong class="source-inline">forward</strong> function of the sequential <span class="No-Break">convolutional group.</span></li>
				<li>Then, we flattened its output with the <strong class="source-inline">view</strong> tensor method because fully connected layers assume that the input is flat. The <strong class="source-inline">view</strong> method takes the new dimensions for the tensor and returns a tensor view without exactly copying the data; <em class="italic">-1</em> means that we don’t care about the dimension’s value and that it can <span class="No-Break">be flattened.</span></li>
				<li>Then, the flattened output from the convolutional group is passed to the fully <span class="No-Break">connected group.</span></li>
				<li>Finally, we applied the softmax function to the final output. We’re unable to wrap <strong class="source-inline">torch::log_softmax</strong> in the <strong class="source-inline">torch::nn::Functional</strong> class instance because of <span class="No-Break">multiple overrides.</span></li>
			</ol>
			<p>The softmax function converts a vector, <img alt="" role="presentation" src="image/B19849_Formula_093.png"/>, of dimension <img alt="" role="presentation" src="image/B19849_Formula_1512.png"/> into a vector, <img alt="" role="presentation" src="image/B19849_Formula_291.png"/>, of the same dimension, where each coordinate, <img alt="" role="presentation" src="image/B19849_Formula_1531.png"/>, of the resulting vector is represented by a real number in the range <img alt="" role="presentation" src="image/B19849_Formula_154.png"/> and the sum of the coordinates <span class="No-Break">is 1.</span></p>
			<p>The coordinates are calculated <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer933">
					<img alt="" role="presentation" src="image/B19849_Formula_155.jpg"/>
				</div>
			</div>
			<p>The softmax function is used in machine learning for classification problems when the number of possible classes is more than two (for two classes, a logistic function is used). The coordinates, <img alt="" role="presentation" src="image/B19849_Formula_1531.png"/>, of the resulting vector can be interpreted as the probabilities that the object belongs to the class, <img alt="" role="presentation" src="image/B19849_Formula_1571.png"/>. We chose this function because its results can be directly used for the cross-entropy loss function, which measures the difference between two probability distributions. The target distribution can be directly calculated from the target label value—we<a id="_idIndexMarker1345"/> create the 10 value’s vector of zeros and put one in the place indexed by the label value. Now, we have all the required components to train the <span class="No-Break">neu<a id="_idTextAnchor634"/>r<a id="_idTextAnchor635"/>al network.</span></p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor636"/>Network training</h2>
			<p>First, we should create PyTorch data loader objects for the train and test datasets. The data loader object is responsible for<a id="_idIndexMarker1346"/> sampling objects from the dataset and making mini-batches from them. This object can be configured <span class="No-Break">as follows:</span></p>
			<ol>
				<li>First, we initialize the <strong class="source-inline">MNISTDataset</strong> type objects representing <span class="No-Break">our datasets.</span></li>
				<li>Then, we use the <strong class="source-inline">torch::data::make_data_loader</strong> function to create a data loader object. This function takes the <strong class="source-inline">torch::data::DataLoaderOptions</strong> type object with configuration settings for the data loader. We set the mini-batch size equal to 256 items and set 8 parallel data loading threads. We should also configure the sampler type, but in this case, we’ll leave the default one—the <span class="No-Break">random sampler.</span></li>
			</ol>
			<p>The following snippet shows how to initialize the train and test <span class="No-Break">data loaders:</span></p>
			<pre class="source-code">
auto train_images = root_path / "train-images-idx3-ubyte";
auto train_labels = root_path / "train-labels-idx1-ubyte";
auto test_images = root_path / "t10k-images-idx3-ubyte";
auto test_labels = root_path / "t10k-labels-idx1-ubyte";
// initialize train dataset
// ----------------------------------------------
MNISTDataset train_dataset(train_images.native(),
                           train_labels.native());
auto train_loader = torch::data::make_data_loader(
    train_dataset.map(torch::data::transforms::Stack&lt;&gt;()),
    torch::data::DataLoaderOptions()
        .batch_size(256)
        .workers(8));
// initialize test dataset
// ----------------------------------------------
MNISTDataset test_dataset(test_images.native(),
                          test_labels.native());
auto test_loader = torch::data::make_data_loader(
    test_dataset.map(torch::data::transforms::Stack&lt;&gt;()),
    torch::data::DataLoaderOptions()
        .batch_size(1024)
        .workers(8));</pre>			<p>Notice that we didn’t pass our dataset objects <a id="_idIndexMarker1347"/>directly to the <strong class="source-inline">torch::data::make_data_loader</strong> function, but we applied the stacking transformation mapping to it. This transformation allows us to sample mini-batches in the form of the <strong class="source-inline">torch::Tensor</strong> object. If we skip this transformation, the mini-batches will be sampled as the C++ vector of tensors. Usually, this isn’t very useful because we can’t apply linear algebra operations to the whole batch in a <span class="No-Break">vectorized manner.</span></p>
			<p>The next step is to initialize the neural network object of the <strong class="source-inline">LeNet5</strong> type, which we defined previously. We’ll move it to the GPU to improve training and <span class="No-Break">evaluation performance:</span></p>
			<pre class="source-code">
LeNet5 model;
model-&gt;to(torch::DeviceType::CUDA);</pre>			<p>When the model of our neural <a id="_idIndexMarker1348"/>network has been initialized, we can initialize an optimizer. We chose stochastic gradient descent with momentum optimization for this. It is implemented in the <strong class="source-inline">torch::optim::SGD</strong> class. The object of this class should be initialized with model (network) parameters and the <strong class="source-inline">torch::optim::SGDOptions</strong> type object. All <strong class="source-inline">torch::nn::Module</strong> type objects have the <strong class="source-inline">parameters()</strong> method, which returns the <strong class="source-inline">std::vector&lt;Tensor&gt;</strong> object containing all the parameters (weights) of the network. There is also the <strong class="source-inline">named_parameters</strong> method, which returns the dictionary of named parameters. Parameter names are created with the names we used in the <strong class="source-inline">register_module</strong> function call. This method is handy if we want to filter parameters and exclude some of them from the <span class="No-Break">training process.</span></p>
			<p>The <strong class="source-inline">torch::optim::SGDOptions</strong> object can be configured with the values of the learning rate, the weight decay regularization factor, and the momentum <span class="No-Break">value factor:</span></p>
			<pre class="source-code">
double learning_rate = 0.01;
double weight_decay = 0.0001; // regularization parameter
torch::optim::SGD optimizer(
    model-&gt;parameters(),
    torch::optim::SGDOptions(learning_rate)
        .weight_decay(weight_decay)
        .momentum(0.5));</pre>			<p>Now that we have our initialized data loaders, the <strong class="source-inline">network</strong> object, and the <strong class="source-inline">optimizer</strong> object, we are ready to start the training cycle. The following snippet shows the training <span class="No-Break">cycle’s</span><span class="No-Break"><a id="_idIndexMarker1349"/></span><span class="No-Break"> implementation:</span></p>
			<pre class="source-code">
int epochs = 100;
for (int epoch = 0; epoch &lt; epochs; ++epoch) {
  model-&gt;train();  // switch to the training mode
  // Iterate the data loader to get batches from the dataset
  int batch_index = 0;
  for (auto&amp; batch : (*train_loader)) {
    // Clear gradients
    optimizer.zero_grad();
    // Execute the model on the input data
    torch::Tensor prediction = model-&gt;forward(batch.data);
    // Compute a loss value to estimate error of our model
    // target should have size of [batch_size]
    torch::Tensor loss =
        torch::nll_loss(prediction, batch.target);
    // Compute gradients of the loss and parameters of our
    // model
    loss.backward();
    // Update the parameters based on the calculated
    // gradients.
    optimizer.step();
    // Output the loss every 10 batches.
    if (++batch_index % 10 == 0) {
      std::cout &lt;&lt; "Epoch: " &lt;&lt; epoch
                &lt;&lt; " | Batch: " &lt;&lt; batch_index
                &lt;&lt; " | Loss: " &lt;&lt; loss.item&lt;float&gt;()
                &lt;&lt; std::endl;
    }
  }</pre>			<p>We’ve made a loop that repeats the training cycle for 100 epochs. At the beginning of the training cycle, we switched our network object to training mode with <strong class="source-inline">model-&gt;train()</strong>. For one epoch, we iterate over all the mini-batches provided by the data <span class="No-Break">loader object:</span></p>
			<pre class="source-code">
for (auto&amp; batch : (*train_loader)){
...
}</pre>			<p>For every mini-batch, we did the <a id="_idIndexMarker1350"/>next training steps, cleared the previous gradient values by calling the <strong class="source-inline">zero_grad</strong> method for the optimizer object, made a forward step over the network object, <strong class="source-inline">model-&gt;forward(batch.data)</strong>, and computed the loss value with the <strong class="source-inline">nll_loss</strong> function. This function computes the <em class="italic">negative log-likelihood</em> loss. It takes two parameters: the vector containing the probability that a training sample belongs to a class identified by position in the vector and the numeric class label (number). Then, we called the <strong class="source-inline">backward</strong> method of the loss tensor. It recursively computes the gradients for the overall network. Finally, we called the <strong class="source-inline">step</strong> method for the optimizer object, which updated all the parameters (weights) and their corresponding gradient values. The <strong class="source-inline">step</strong> method only updated the parameters that were used <span class="No-Break">for initialization.</span></p>
			<p>It’s common practice to use test or validation data to check the training process after each epoch. We can do this in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
model-&gt;eval(); // switch to the training mode
unsigned long total_correct = 0;
float avg_loss = 0.0;
for (auto&amp; batch : (*test_loader)) {
  // Execute the model on the input data
  torch::Tensor prediction = model-&gt;forward(batch.data);
  // Compute a loss value to estimate error of our model
  torch::Tensor loss =
      torch::nll_loss(prediction, batch.target);
  avg_loss += loss.sum().item&lt;float&gt;();
  auto pred = std::get&lt;1&gt;(prediction.detach_().max(1));
  total_correct += static_cast&lt;unsigned long&gt;(
      pred.eq(batch.target.view_as(pred))
          .sum()
          .item&lt;long&gt;());
}
avg_loss /= test_dataset.size().value();
double accuracy = (static_cast&lt;double&gt;(total_correct) /
                   test_dataset.size().value());
std::cout &lt;&lt; "Test Avg. Loss: " &lt;&lt; avg_loss
          &lt;&lt; " |
    Accuracy : " &lt;&lt; accuracy &lt;&lt; std::endl;</pre>			<p>First, we switched the model to evaluation mode by calling the <strong class="source-inline">eval</strong> method. Then we iterated over all the batches from the test data loader. For each of these batches, we performed a forward pass <a id="_idIndexMarker1351"/>over the network, calculating the loss value in the same way that we did for our training process. To estimate the total loss (error) value for the model, we averaged the loss values for all the batches. To get the total loss for the batch, we used <strong class="source-inline">loss.sum().item&lt;float&gt;()</strong>. Here, we summarized the losses for each training sample in the batch and moved it to the CPU floating-point variable with the <span class="No-Break"><strong class="source-inline">item&lt;float&gt;()</strong></span><span class="No-Break"> method.</span></p>
			<p>Next, we calculate the accuracy value. This is the ratio between correct answers and misclassified ones. Let’s go through this calculation with the following approach. First, we determine the predicted class labels by using the <strong class="source-inline">max</strong> method of the <span class="No-Break">tensor object:</span></p>
			<pre class="source-code">
auto pred = std::get&lt;1&gt;(prediction.detach_().max(1));</pre>			<p>The <strong class="source-inline">max</strong> method returns a tuple, where the values are the maximum value of each row of the input tensor in the given dimension and the location indices of each maximum value the method found. Then, we <a id="_idIndexMarker1352"/>compare the predicted labels with the target ones and calculate the number of <span class="No-Break">correct answers:</span></p>
			<pre class="source-code">
total_correct += static_cast&lt;unsigned long&gt;(
    pred.eq(batch.target.view_as(pred)).sum().item&lt;long&gt;());</pre>			<p>We used the <strong class="source-inline">eq</strong> tensor’s method for our comparison. This method returns a boolean vector whose size is equal to the input vector, with values equal to <strong class="source-inline">1</strong> where the vector element components are equal and with values equal to <strong class="source-inline">0</strong> where they’re not. To perform the comparison operation, we made a view for the target labels tensor with the same dimensions as the predictions tensor. The <strong class="source-inline">view_as</strong> method is used for this comparison. Then, we calculated the sum of <strong class="source-inline">1</strong> values and moved the value to the CPU variable with the <span class="No-Break"><strong class="source-inline">item&lt;long&gt;()</strong></span><span class="No-Break"> method.</span></p>
			<p>By doing this, we can see that the specialized framework has more options we can configure and is more flexible for neural network development. It has more layer types and supports dynamic network graphs. It also has a powerful specialized linear algebra library that can be used to create new layers, as well as new loss and activation functions. It has powerful abstractions that enable us to work with big training data. One more important thing to note is that it has a C++ API very similar to the Python API, so we can easily port Python programs to <a id="_idTextAnchor637"/>C<a id="_idTextAnchor638"/>++ and <span class="No-Break">vice versa.</span></p>
			<h1 id="_idParaDest-228"><a id="_idTextAnchor639"/>Summary</h1>
			<p>In this chapter, we looked at what artificial neural networks are, looked at their history, and examined the reasons for their appearance, rise, and fall and why they have become one of the most actively developed machine learning approaches today. We looked at the difference between biological and artificial neurons before learning the basics of the perceptron concept, which was created by Frank Rosenblatt. Then, we discussed the internal features of artificial neurons and networks, such as activation functions and their characteristics, network topology, and convolution layer concepts. We also learned how to train artificial neural networks with the error backpropagation method. We saw how to choose the right loss function for different types of tasks. Then, we discussed the regularization methods that are used to combat overfitting <span class="No-Break">during training.</span></p>
			<p>Finally, we implemented a simple MLP for a regression task with the mlpack, Dlib, and Flashlight C++ machine learning libraries. Then, we implemented a more advanced convolution network for an image classification task with PyTorch, a specialized neural network framework. This showed us the benefits of specialized frameworks over <span class="No-Break">general-purpose libraries.</span></p>
			<p>In the next chapter, we will discuss how to use pre-trained <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) and adapt them to our particular tasks. We will see how to use the transfer learning technique and the BERT network to perform <span class="No-Break">s<a id="_idTextAnchor640"/>entiment analysis.</span></p>
			<h1 id="_idParaDest-229"><a id="_idTextAnchor641"/>Further reading</h1>
			<ul>
				<li><em class="italic">Loss Functions for Deep Neural Networks in </em><span class="No-Break"><em class="italic">Classification</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/1702.05659.pdf"><span class="No-Break">https://arxiv.org/pdf/1702.05659.pdf</span></a></li>
				<li><em class="italic">Neural Networks and Deep Learning</em>, by Michael <span class="No-Break">Nielsen: </span><a href="http://neuralnetworksanddeeplearning.com/"><span class="No-Break">http://neuralnetwork</span>
<span class="No-Break">sanddeeplearning.com/</span></a></li>
				<li><em class="italic">Principles of Neurodynamics</em>, Rosenblatt, Frank (1962), Washington, DC: <span class="No-Break">Spartan Books</span></li>
				<li><em class="italic">Perceptrons</em>, Minsky M. L. and Papert S. A. 1969. Cambridge, MA: <span class="No-Break">MIT Press</span></li>
				<li><em class="italic">Neural Networks and Learning Machines</em>, Simon O. <span class="No-Break">Haykin 2008</span></li>
				<li><em class="italic">Deep Learning</em>, Ian Goodfellow, Yoshua Bengio, Aaron <span class="No-Break">Courville 2016</span></li>
				<li>The PyTorch GitHub <span class="No-Break">page: </span><a href="https://github.com/pytorch/"><span class="No-Break">https://github.com/pytorch/</span></a></li>
				<li>The PyTorch documentation <span class="No-Break">site: </span><a href="https://pytorch.org/docs/"><span class="No-Break">https://pytorch.org/docs/</span></a></li>
				<li>The LibPyTorch (C++) documentation <span class="No-Break">site: </span><a href="https://pytorch.org/cppdocs/"><span class="No-Break">https://pytorch.org/cppdocs/</span></a></li>
			</ul>
		</div>
	</body></html>