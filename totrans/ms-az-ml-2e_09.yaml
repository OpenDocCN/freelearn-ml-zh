- en: '*Chapter 7*: Advanced Feature Extraction with NLP'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章*：使用NLP的高级特征提取'
- en: In the previous chapters, we learned about many standard transformation and
    preprocessing approaches within the Azure Machine Learning service as well as
    typical labeling techniques using the Azure Machine Learning Data Labeling service.
    In this chapter, we want to go one step further to extract semantic features from
    textual and categorical data—a problem that users often face when training ML
    models. This chapter will describe the foundations of feature extraction with
    **Natural Language Processing** (**NLP**). This will help you to practically implement
    semantic embeddings using NLP for your ML pipelines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了在Azure机器学习服务中许多标准的转换和预处理方法，以及使用Azure机器学习数据标注服务进行典型标注技术的应用。在本章中，我们希望更进一步，从文本和分类数据中提取语义特征——这是用户在训练机器学习模型时经常遇到的问题。本章将描述使用**自然语言处理**（**NLP**）进行特征提取的基础。这将帮助您在实际的机器学习管道中实现使用NLP的语义嵌入。
- en: First, we will take a look at the differences between *textual*, *categorical*,
    *nominal*, and *ordinal* data. This classification will help you to decide the
    best feature extraction and transformation technique per feature type. Later,
    we will look at the most common transformations for categorical values, namely
    **label encoding** and **one-hot encoding**. Both techniques will be compared
    and tested to understand the different use cases and applications for both techniques.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨**文本**、**分类**、**名义**和**有序**数据之间的差异。这种分类将帮助您根据特征类型决定最佳的特征提取和转换技术。稍后，我们将查看分类值最常见的转换方法，即**标签编码**和**独热编码**。这两种技术将被比较和测试，以了解这两种技术的不同用例和应用。
- en: Next, we will tackle the numerical embedding of textual data. To achieve this,
    we will build a simple **bag-of-words** model, using a **count vectorizer**. To
    sanitize the input, we will build an NLP pipeline consisting of a **tokenizer**,
    stop word removal, **stemming**, and **lemmatization**. We will learn how these
    different techniques affect a sample dataset step by step.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将处理文本数据的数值嵌入。为了实现这一点，我们将构建一个简单的**词袋**模型，使用**计数向量器**。为了净化输入，我们将构建一个包含**分词器**、停用词去除、**词干提取**和**词形还原**的NLP管道。我们将逐步学习这些不同的技术如何影响样本数据集。
- en: Following this, we will replace the word count method with a much better word
    frequency weighting approach—the **Term Frequency-Inverse Document Frequency**
    (**TF-IDF**) algorithm. This will help you to compute the importance of words
    when given a whole corpus of documents by weighting the occurrence of a term in
    one document over the frequency in the corpus. Additionally, we will look at **Singular
    Value Decomposition** (**SVD**) for reducing the size of the term dictionary.
    As a next step, we will improve the term embedding quality by leveraging word
    semantics, and we will look under the hood of semantic embeddings such as **Global
    Vectors** (**GloVe**) and **Word2Vec**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，我们将用一种更好的词频加权方法——**词频-逆文档频率**（**TF-IDF**）算法来替换词计数方法。这将帮助您在给定整个文档集合的情况下，通过加权一个文档中术语的出现频率相对于文档集合中的频率来计算单词的重要性。此外，我们将探讨**奇异值分解**（**SVD**）以减少术语字典的大小。作为下一步，我们将通过利用词义来提高术语嵌入的质量，并深入了解语义嵌入，如**全局向量**（**GloVe**）和**Word2Vec**。
- en: In the last section, we will take a look at current state-of-the-art language
    models that are based on sequence-to-sequence deep neural networks with over 100
    million parameters. We will train a small end-to-end model using **Long Short-Term
    Memory** (**LSTM**), perform word embedding and sentiment analysis using **Bidirectional
    Encoder Representations from Transformers** (**BERT**), and compare both custom
    solutions to Azure's text analytics capabilities in Cognitive Services.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节，我们将探讨基于序列到序列深度神经网络且超过一亿参数的当前最先进的语言模型。我们将使用**长短期记忆**（**LSTM**）训练一个小的端到端模型，使用**双向编码器表示从Transformer**（**BERT**）进行词嵌入和情感分析，并将这两种自定义解决方案与Azure认知服务中的文本分析能力进行比较。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding categorical data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分类数据
- en: Building a simple bag-of-words model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建简单的词袋模型
- en: Leveraging term importance and semantics
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用术语重要性和语义
- en: Implementing end-to-end language models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现端到端语言模型
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use the following Python libraries and versions to
    create categorical encodings, create semantic embeddings, train an end-to-end
    model, and perform classic NLP preprocessing steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下Python库和版本来创建分类编码、创建语义嵌入、训练端到端模型以及执行经典的NLP预处理步骤：
- en: '`azureml-sdk 1.34.0`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-sdk 1.34.0`'
- en: '`azureml-widgets 1.34.0`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-widgets 1.34.0`'
- en: '`tensorflow 2.6.0`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow 2.6.0`'
- en: '`numpy 1.19.5`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy 1.19.5`'
- en: '`pandas 1.3.2`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas 1.3.2`'
- en: '`scikit-learn 0.24.2`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn 0.24.2`'
- en: '`nltk 3.6.2`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk 3.6.2`'
- en: '`genism 3.8.3`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gensim 3.8.3`'
- en: Similar to previous chapters, you can execute this code using either a local
    Python interpreter or a notebook environment hosted in Azure Machine Learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，您可以使用本地Python解释器或Azure Machine Learning中托管的笔记本环境来执行此代码。
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有的代码示例都可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07)。
- en: Understanding categorical data
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分类数据
- en: '**Categorical data** comes in many forms, shapes, and meanings. It is extremely
    important to understand what type of data you are dealing with—is it a string,
    text, or numeric value disguised as a categorical value? This information is essential
    for data preprocessing, feature extraction, and model selection.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类数据**以多种形式、形状和意义存在。了解你正在处理的数据类型至关重要——它是一个字符串、文本还是伪装成分类值的数值？这些信息对于数据预处理、特征提取和模型选择至关重要。'
- en: In this section, first, we will take a look at the different types of categorical
    data—namely *ordinal*, *nominal*, and *text*. Depending on the type, you can use
    different methods to extract information or other valuable data from it. Please
    bear in mind that categorical data is ubiquitous, whether it is in an ID column,
    a nominal category, an ordinal category, or a free-text field. It's worth mentioning
    that the more information you have on the data, the easier the preprocessing is.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，首先，我们将查看不同类型的分类数据——即*顺序*、*名义*和*文本*。根据类型，你可以使用不同的方法从中提取信息或其他有价值的数据。请记住，分类数据无处不在，无论是ID列、名义类别、顺序类别还是自由文本字段。值得一提的是，你对数据的了解越多，预处理就越容易。
- en: Next, we will actually preprocess the ordinal and nominal categorical data by
    transforming it into numerical values. This is a required step when you want to
    use an ML algorithm later on that can't interpret categorical data, which is true
    for most algorithms except, for example, decision tree-based approaches. Most
    other algorithms can only operate (for example, compute a loss function) on a
    numeric value and so a transformation is required.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过将其转换为数值来实际预处理顺序和名义分类数据。当你想要使用不能解释分类数据的机器学习算法时，这是一个必要的步骤，这对于大多数算法都是真实的，例如基于决策树的算法。大多数其他算法只能对数值值进行操作（例如，计算损失函数），因此需要进行转换。
- en: Comparing textual, categorical, and ordinal data
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较文本、分类和顺序数据
- en: Many ML algorithms, such as support vector machines, neural networks, linear
    regression, and more, can only be applied to numeric data. However, in real-world
    datasets, we often find non-numeric columns, such as columns that contain textual
    data. The goal of this chapter is to transform textual data into numeric data
    as an advanced feature extraction step, which allows us to plug the processed
    data into any ML algorithm.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法，如支持向量机、神经网络、线性回归等，只能应用于数值数据。然而，在现实世界的数据集中，我们经常发现非数值列，例如包含文本数据的列。本章的目标是将文本数据转换为数值数据，作为高级特征提取步骤，这样我们就可以将处理后的数据插入到任何机器学习算法中。
- en: 'When working with real-world data, you will be confronted with many different
    types of textual and/or categorical data. To optimize ML algorithms, you need
    to understand the differences in order to apply different preprocessing techniques
    to the different types. But first, let''s define the three different textual data
    types:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理现实世界数据时，你将面临许多不同类型的文本和/或分类数据。为了优化机器学习算法，你需要了解这些差异，以便对不同的类型应用不同的预处理技术。但首先，让我们定义三种不同的文本数据类型：
- en: '*Textual data*: Free text'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本数据*：自由文本'
- en: '*Categorical nominal data*: Non-orderable categories'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类名义数据*：不可排序的类别'
- en: '*Categorical ordinal data*: Orderable categories'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有序类别数据*：可排序的类别'
- en: The difference between textual data and categorical data is that, in textual
    data, we want to capture semantic similarities (that is, the similarity in the
    meaning of the words), whereas, in categorical data, we want to differentiate
    between a small number of variables.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据和类别数据之间的区别在于，在文本数据中，我们想要捕捉语义相似性（即词语的意义相似性），而在类别数据中，我们想要区分少数几个变量。
- en: The difference between categorical nominal data and categorical ordinal data
    is that nominal data cannot be ordered (all categories have the same weight),
    whereas ordinal categories can be logically ordered on an ordinal scale.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有序类别数据和有序类别数据之间的区别在于，名义数据不能排序（所有类别具有相同的权重），而有序类别可以在有序尺度上逻辑排序。
- en: '*Figure 7.1* shows an example dataset of comments on news articles, where the
    first column, named `statement`, is a textual field, the column named `topic`
    is a nominal category, and `rating` is an ordinal category:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.1* 展示了一个新闻文章评论的示例数据集，其中第一列，命名为 `statement`，是一个文本字段，名为 `topic` 的列是一个名义类别，而
    `rating` 是一个有序类别：'
- en: '![Figure 7.1 – Comparing different textual data types ](img/B17928_07_001.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 比较不同的文本数据类型](img/B17928_07_001.jpg)'
- en: Figure 7.1 – Comparing different textual data types
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 比较不同的文本数据类型
- en: Understanding the differences between these data representations is essential
    to find the proper embedding technique afterward. It seems quite natural to replace
    ordinal categories with an ordinal numeric scale and to embed nominal categories
    in an orthogonal space. On the contrary, it's not obvious how to embed textual
    data into a numerical space where the semantics are preserved—this will be covered
    in the later sections of this chapter that deal with NLP.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些数据表示之间的差异对于之后找到适当的嵌入技术至关重要。用有序数值尺度替换有序类别似乎很自然，将名义类别嵌入到正交空间中。相反，将文本数据嵌入到保留语义的数值空间中并不明显——这将在本章后面的部分中介绍，这部分内容涉及NLP。
- en: 'Please note that instead of categorical values, you will also see continuous
    numeric variables representing categorical information, for example, IDs from
    a dimension or lookup table. Although these are numeric values, you should consider
    treating them as categorical nominal values, if possible. Here is an example dataset:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了类别值之外，你还会看到表示类别信息的连续数值变量，例如来自维度或查找表的ID。尽管这些是数值，但如果可能的话，你应该考虑将它们作为类别名义值处理。以下是一个示例数据集：
- en: '![Figure 7.2 – Comparing numerical categorical values ](img/B17928_07_002.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 比较数值类别值](img/B17928_07_002.jpg)'
- en: Figure 7.2 – Comparing numerical categorical values
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 比较数值类别值
- en: In this example, we can see that the `sensorId` value is a numeric value that
    should be interpreted as a categorical nominal value instead of a numeric value
    by default because it doesn't have a numeric meaning. What do you get when you
    subtract `sensorId` `2` from `sensorId` `1`? Is `sensorId` `10` 10 times larger
    than `sensorId` `1`? These are the typical questions to ask to discover and encode
    these categorical values. We will discover, in [*Chapter 9*](B17928_09_ePub.xhtml#_idTextAnchor152),
    *Building ML Models Using Azure Machine Learning*, that by specifying that these
    values are categorical, a gradient-boosted tree model can optimize these features
    instead of treating them as continuous variables.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到 `sensorId` 值是一个数值，应该将其解释为类别名义值，而不是默认的数值，因为它没有数值意义。当你从 `sensorId`
    `1` 减去 `sensorId` `2` 时，你得到什么？`sensorId` `10` 是 `sensorId` `1` 的10倍大吗？这些问题是发现和编码这些类别值的典型问题。我们将在
    [*第9章*](B17928_09_ePub.xhtml#_idTextAnchor152)，*使用Azure机器学习构建ML模型* 中发现，通过指定这些值是类别数据，梯度提升树模型可以优化这些特征，而不是将它们作为连续变量处理。
- en: Transforming categories into numeric values
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将类别转换为数值
- en: 'Let''s start by converting categorical variables (both ordinal and nominal)
    into numeric values. In this section, we will look at two common techniques for
    categorical encoding: **label encoding** and **one-hot encoding** (also called
    *dummy coding*). While **label encoding** replaces a categorical feature column
    with a numerical feature column, **one-hot encoding** uses multiple columns (where
    the number of columns equals the number of unique values) to encode a single feature.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从将分类变量（序数和名义）转换为数值开始。在本节中，我们将探讨两种常见的分类编码技术：**标签编码**和**独热编码**（也称为*虚拟编码*）。虽然**标签编码**用一个数值特征列替换分类特征列，**独热编码**则使用多个列（列的数量等于唯一值的数量）来编码一个单一特征。
- en: Both techniques are applied in the same way. During the training iteration,
    these techniques find all of the unique values in a feature column and assign
    them a specific numeric value (multidimensional value for one-hot encoding). As
    a result, a lookup dictionary defining this replacement is stored in the encoder.
    When the encoder is applied, the values in the applied column are transformed
    (replaced) using the lookup dictionary. If the list of possible values is known
    beforehand, most implementations allow the encoder to initialize the lookup dictionary
    directly from the list of known values, rather than finding the unique values
    in the training set. This has the benefit of specifying the order of the values
    in the dictionary, so orders the encoded values.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种技术以相同的方式进行应用。在训练迭代过程中，这些技术会找到特征列中的所有唯一值，并给它们分配一个特定的数值（对于独热编码，是一个多维数值）。结果，一个定义这种替换的查找字典存储在编码器中。当应用编码器时，应用列中的值会使用查找字典进行转换（替换）。如果事先知道可能的值列表，大多数实现允许编码器直接从已知值的列表初始化查找字典，而不是在训练集中找到唯一值。这有利于指定字典中值的顺序，从而对编码值进行排序。
- en: Important Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Please note that it's often possible that certain categorical feature values
    in the test set don't appear in the training set and, hence, are not stored in
    the lookup dictionary. So, you should add a default category to your encoder that
    can also transform unseen values into numeric values.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，通常可能存在某些分类特征值在测试集中没有出现在训练集中，因此没有存储在查找字典中。因此，你应该在你的编码器中添加一个默认类别，该类别也可以将未见过的值转换为数值。 '
- en: 'Now, we will use two different categorical data columns, one ordinal and one
    nominal category, to showcase the different encodings. *Figure 7.3* shows a nominal
    feature, `topic`, which could represent a list of articles by a news agency:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用两个不同的分类数据列，一个是序数类别，另一个是名义类别，来展示不同的编码。*图7.3* 显示了一个名义特征`topic`，它可能代表一个新闻机构的文章列表：
- en: '![Figure 7.3 – Nominal categorical data ](img/B17928_07_003.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 名义分类数据](img/B17928_07_003.jpg)'
- en: Figure 7.3 – Nominal categorical data
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 名义分类数据
- en: '*Figure 7.4* contains the ordinal category of `rating`; it could represent
    a feedback form for purchased articles on a website:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.4* 包含了`rating`的序数类别；它可能代表一个网站购买文章的反馈表单：'
- en: '![Figure 7.4 – Ordinal categorical data ](img/B17928_07_004.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 序列分类数据](img/B17928_07_004.jpg)'
- en: Figure 7.4 – Ordinal categorical data
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 序列分类数据
- en: To preserve the meaning of the categories, we require different preprocessing
    techniques for the different categorical data types. First, we take a look at
    the *label encoder*. The label encoder assigns an incrementing value to each unique
    categorical value in a feature column. So, it transforms categories into a numeric
    value between `0` and `N-1`, where `N` represents the number of unique values.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保留类别的含义，我们需要为不同的分类数据类型采用不同的预处理技术。首先，我们来看一下*标签编码器*。标签编码器为特征列中的每个唯一分类值分配一个递增的值。因此，它将类别转换为介于`0`和`N-1`之间的数值，其中`N`代表唯一值的数量。
- en: 'Let''s test the label encoder in the `topic` column within the first table.
    We train the encoder on the data and replace the `topic` column with a numeric
    topic ID. Here is an example snippet to train the label encoder and transform
    the dataset:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在第一个表中的`topic`列中测试标签编码器。我们在数据上训练编码器，并用数值主题ID替换`topic`列。以下是一个训练标签编码器并转换数据集的示例片段：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Figure 7.5* shows the results of the preceding transformation. Each topic
    was encoded as a numerical increment, `topicId`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.5* 显示了先前转换的结果。每个主题都被编码为一个数值增量，`topicId`：'
- en: '![Figure 7.5 – Label-encoded topics ](img/B17928_07_005.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 标签编码的主题](img/B17928_07_005.jpg)'
- en: Figure 7.5 – Label-encoded topics
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 标签编码的主题
- en: 'The generated lookup table for `topicId` is shown in *Figure 7.6*. This lookup
    dictionary was learned by the encoder during the `fit()` method and can be applied
    to categorical data using the `transform()` method:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`topicId`生成的查找表如图*7.6*所示。这个查找字典是在`fit()`方法期间由编码器学习到的，可以使用`transform()`方法应用于分类数据：'
- en: '![Figure 7.6 – A lookup dictionary for topics ](img/B17928_07_006.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 主题查找字典](img/B17928_07_006.jpg)'
- en: Figure 7.6 – A lookup dictionary for topics
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 主题的查找字典
- en: As you can see in the previous screenshots, encoding nominal data with labels
    is easy and straightforward. However, the resulting numerical data has different
    mathematical properties from the distinct nominal categories. So, let's find out
    how this method works for ordinal data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几个截图所示，使用标签对名义数据进行编码既简单又直接。然而，生成的数值数据具有与不同的名义类别不同的数学属性。因此，让我们找出这种方法对有序数据是如何工作的。
- en: 'In the next example, we naïvely apply the label encoder to the ratings dataset.
    The encoder is trained by iterating the training data in order to create the lookup
    dictionary:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个例子中，我们天真地将标签编码器应用于评分数据集。编码器通过迭代训练数据来训练，以创建查找字典：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Figure 7.7* shows the result of the encoded ratings as `ratingId`, which is
    very similar to the previous example. However, in the case of ratings, the numerical
    properties of the ratings data are similar to the ordinal properties of the categorical
    ratings:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.7*显示了编码后的评分结果作为`ratingId`，这与前面的例子非常相似。然而，在评分的情况下，评分数据的数值属性与分类评分的有序属性相似：'
- en: '![Figure 7.7 – Label-encoded ratings ](img/B17928_07_007.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 标签编码的评分](img/B17928_07_007.jpg)'
- en: Figure 7.7 – Label-encoded ratings
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 标签编码的评分
- en: 'Additionally, let''s look at the lookup dictionary, in *Figure 7.8*, that the
    encoder learned from the input data:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们看看编码器从输入数据中学习到的查找字典，如图*7.8*所示：
- en: '![Figure 7.8 – The lookup dictionary for ratings ](img/B17928_07_008.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 评分的查找字典](img/B17928_07_008.jpg)'
- en: Figure 7.8 – The lookup dictionary for ratings
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 评分的查找字典
- en: 'Do you see something odd in the autogenerated lookup dictionary? Due to the
    order of the categorical values in the training data, we created a numeric list
    with the following order:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你在自动生成的查找字典中看到什么奇怪的地方了吗？由于训练数据中分类值的顺序，我们按照以下顺序创建了一个数字列表：
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is probably not what we anticipated when applying a label encoder to an
    ordinal categorical value. The ordering we would be looking for is similar to
    the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能不是我们在将标签编码器应用于有序分类值时所预期的结果。我们希望寻找的顺序类似于以下内容：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In order to create a label encoder with the right order, we can pass the ordered
    list of categorical values to the encoder. This would create a more meaningful
    encoding, as shown in *Figure 7.9*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建具有正确顺序的标签编码器，我们可以将分类值的有序列表传递给编码器。这将创建一个更有意义的编码，如图*7.9*所示：
- en: '![Figure 7.9 – Label-encoded ratings with custom order ](img/B17928_07_009.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 带有自定义顺序的标签编码的评分](img/B17928_07_009.jpg)'
- en: Figure 7.9 – Label-encoded ratings with custom order
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 带有自定义顺序的标签编码的评分
- en: 'To achieve this in Python, we have to use pandas'' categorical ordinal variable,
    which is a special kind of label encoder that requires a list of ordered categories
    as input:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中实现这一点，我们必须使用pandas的分类顺序变量，这是一种特殊的标签编码器，它需要一个有序分类列表作为输入：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Under the hood, we implicitly created the following lookup dictionary for the
    encoder by passing the categories directly to it in order:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，我们通过直接将类别传递给编码器来隐式地创建了以下查找字典：
- en: '![Figure 7.10 – A lookup dictionary for ratings with custom orders ](img/B17928_07_010.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 带有自定义顺序的评分查找字典](img/B17928_07_010.jpg)'
- en: Figure 7.10 – A lookup dictionary for ratings with custom orders
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 带有自定义顺序的评分的查找字典
- en: As you can see in the preceding example, a label encoder can be quickly applied
    to any categorical data without much afterthought. The result of the label encoder
    is a single numerical feature and a categorical lookup table. Additionally, we
    can see, in the examples with topics and ratings, that label encoding is more
    suitable for ordinal data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，标签编码器可以迅速应用于任何分类数据，无需过多思考。标签编码器的结果是单个数值特征和分类查找表。此外，我们还可以看到，在主题和评分的示例中，标签编码更适合有序数据。
- en: Important Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The key takeaway is that the label encoder is great for encoding ordinal categorical
    data. You also learned that the order of elements matters, and so it is good practice
    to manually pass the categories to the encoder in the correct order.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的收获是标签编码器非常适合编码有序分类数据。你也了解到元素的顺序很重要，因此将类别按正确顺序手动传递给编码器是一个好的实践。
- en: Orthogonal embedding using one-hot encoding
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用独热编码的正交嵌入
- en: 'In the second part of this section, we will take a look at the `N`, where `N`
    represents the number of unique values. This vector contains only zeros, except
    for one column that contains `1` and represents the column for this specific value.
    Here is a code snippet showing you how to apply the one-hot encoder to the `articles`
    dataset:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的第二部分，我们将探讨`N`的含义，其中`N`代表唯一值的数量。这个向量除了包含一个列值为`1`的列，代表这个特定值所在的列外，其余列都包含`0`。以下是一个代码片段，展示了如何将独热编码器应用于`articles`数据集：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the preceding code is shown in *Figure 7.11*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出显示在*图7.11*中：
- en: '![Figure 7.11 – One-hot-encoded articles ](img/B17928_07_011.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11 – 独热编码的文章](img/B17928_07_011.jpg)'
- en: Figure 7.11 – One-hot-encoded articles
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 独热编码的文章
- en: 'The lookup dictionary for one-hot encoding has `N+1` columns, where `N` is
    the number of unique values in the encoded column. As we can see in the lookup
    dictionary in *Figure 7.12*, all N-dimensional vectors in the dictionary are orthogonal
    and of an equal length, `1`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码的查找字典有`N+1`列，其中`N`是编码列中唯一值的数量。正如我们在*图7.12*中的查找字典中可以看到的那样，字典中的所有N维向量都是正交的，长度相等，为`1`：
- en: '![Figure 7.12 – The lookup dictionary for articles ](img/B17928_07_012.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图7.12 – 文章的查找字典](img/B17928_07_012.jpg)'
- en: Figure 7.12 – The lookup dictionary for articles
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 – 文章的查找字典
- en: 'Now, let''s compare this technique with ordinal data and apply one-hot encoding
    to the ratings table. The result is shown in *Figure 7.13*:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这种技术与有序数据进行比较，并将独热编码应用于评分表。结果显示在*图7.13*中：
- en: '![Figure 7.13 – One-hot-encoded ratings ](img/B17928_07_013.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图7.13 – 独热编码的评分](img/B17928_07_013.jpg)'
- en: Figure 7.13 – One-hot-encoded ratings
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – 独热编码的评分
- en: In the preceding figure, we can see that even if the original category values
    are ordinal, the encoded values can no longer be sorted, and so, this property
    is lost after the numeric encoding. Therefore, we can conclude that one-hot encoding
    is great for nominal categorical values where the number of unique values is small.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到，即使原始的类别值是有序的，编码后的值也无法排序，因此，在数值编码后，这个属性就丢失了。因此，我们可以得出结论，独热编码非常适合唯一值数量较少的名称分类值。
- en: So far, we've learned how to embed nominal and ordinal categorical values into
    numeric values by using a lookup dictionary and one-dimensional or N-dimensional
    numeric embedding. However, we discovered that it is somewhat limited in many
    aspects, such as the number of unique categories and capabilities to embed free
    text. In the following sections, we will learn how to extract words using a simple
    NLP pipeline.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何通过使用查找字典和一维或N维数值嵌入将名称和有序分类值嵌入到数值中。然而，我们发现它在许多方面都有一定的局限性，例如唯一类别的数量和嵌入自由文本的能力。在接下来的几节中，我们将学习如何使用简单的NLP管道提取单词。
- en: Semantics and textual values
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义和文本值
- en: It's worth taking the time to understand that a categorical value and a textual
    value are not the same. Although they might both be stored as a string and could
    have the same data type in your dataset, usually, a categorical value represents
    a finite set of categories, whereas a text value can hold any textual information.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 值得花时间去理解的是，分类值和文本值并不相同。尽管它们可能都存储为字符串，并且在你的数据集中可能有相同的数据类型，但通常，分类值代表一组有限的类别，而文本值可以包含任何文本信息。
- en: So, why is this distinction important? Once you preprocess your categorical
    data and embed it into a numerical space, nominal categories will often be implemented
    as orthogonal vectors. You will not automatically be able to compute a distance
    from category A to category B or create a semantic meaning between the categories.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这种区分为什么很重要呢？一旦你预处理了分类数据并将其嵌入到数值空间中，名称类别通常会被实现为正交向量。你将无法自动计算类别A到类别B的距离或创建类别之间的语义意义。
- en: However, with textual data, usually, you start feature extraction with a different
    approach that assumes that you will find similar terms in the same text feature
    of your dataset samples. You can use this information to compute meaningful similarity
    scores between two textual columns; for example, to measure the number of words
    that are in common.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于文本数据，通常您会采用不同的方法来开始特征提取，该方法假设您将在数据集样本的相同文本特征中找到相似术语。您可以使用这些信息来计算两个文本列之间的有意义相似度得分；例如，测量共同单词的数量。
- en: Therefore, we recommend that you thoroughly check what kind of categorical values
    you have and how you are aiming to preprocess them. Also, a great exercise is
    to compute the similarity between two rows and see whether it matches your prediction.
    Let's take a look at a simple textual preprocessing approach using a dictionary-based
    bag-of-words embedding.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们建议您彻底检查您有哪些类型的分类值以及您打算如何预处理它们。此外，一个很好的练习是计算两行之间的相似度，看看它是否与您的预测相符。让我们看看使用基于字典的词袋嵌入的简单文本预处理方法。
- en: Building a simple bag-of-words model
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建简单的词袋模型
- en: In this section, we will look at a surprisingly simple concept to tackle the
    shortcomings of label encoding for textual data using a technique called bag-of-words,
    which will build a foundation for a simple NLP pipeline. Don't worry if these
    techniques look too simple when you read through them; we will gradually build
    on top of them with tweaks, optimizations, and improvements to build a modern
    NLP pipeline.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一个惊人的简单概念，即使用称为词袋的技术来解决标签编码在文本数据中的不足，这将为一个简单的NLP管道打下基础。当您阅读这些技术时，如果它们看起来太简单，请不要担心；我们将通过调整、优化和改进逐步构建现代NLP管道。
- en: A naïve bag-of-words model using counting
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用计数构建的简单词袋模型
- en: In this section, the main concept that we will build is the bag-of-words model.
    It is a very simple concept; that is, it involves modeling any document as a collection
    of words that appear in a given document with the frequency of each word. Hence,
    we throw away sentence structure, word order, punctuation marks, and more and
    reduce the documents to a raw count of words. Following this, we can vectorize
    this word count into a numeric vector representation, which can then be used for
    ML, analysis, document comparisons, and much more. While this word count model
    sounds very simple, we will encounter quite a few language-specific obstacles
    along the way that we will need to resolve.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建的主要概念是词袋模型。这是一个非常简单的概念；也就是说，它涉及将任何文档建模为包含在给定文档中的单词集合，每个单词的频率。因此，我们丢弃句子结构、单词顺序、标点符号等，并将文档简化为单词的原始计数。在此基础上，我们可以将这个单词计数向量化为一个数值向量表示，然后可以用于机器学习、分析、文档比较等等。虽然这个单词计数模型听起来非常简单，但在路上我们将会遇到很多语言特定的障碍，我们需要解决。
- en: 'Let''s get started and define a sample document that we will transform throughout
    this section:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始并定义一个示例文档，我们将在这个部分对其进行转换：
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Applying a naïve word count to the document gives us our first (too simple)
    bag-of-words model:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将简单的单词计数应用于文档为我们提供了我们的第一个（过于简单）词袋模型：
- en: '![Figure 7.14 – A naïve bag-of-words model ](img/B17928_07_014.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图7.14 – 一个简单的词袋模型](img/B17928_07_014.jpg)'
- en: Figure 7.14 – A naïve bag-of-words model
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 - 一个简单的词袋模型
- en: However, there are many problems with a naïve approach such as the preceding
    one. We have mixed different punctuation marks, notations, nouns, verbs, adverbs,
    and adjectives in different declinations, conjugations, tenses, and cases. Therefore,
    we have to build a pipeline to clean and normalize the data using NLP. In this
    section, we will build a pipeline with the following cleaning steps before feeding
    the data into a **count vectorizer** that, ultimately, counts the word occurrences
    and collects them in a feature vector.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，像前面那样简单的方法有很多问题。我们混合了不同的标点符号、符号、名词、动词、副词和形容词的不同变形、屈折、时态和格。因此，我们必须构建一个管道来使用NLP清理和标准化数据。在本节中，在将数据输入到**计数向量器**之前，我们将构建以下清理步骤的管道，该向量器最终会计算单词出现次数并将它们收集到特征向量中。
- en: Tokenization – turning a string into a list of words
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词 - 将字符串转换为单词列表
- en: 'The first step in building the pipeline is to separate a corpus into documents
    and a document into words. This process is called `nltk`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 构建管道的第一步是将语料库分为文档，将文档分为单词。这个过程被称为`nltk`：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code will output a list of tokens that contains words and punctuation
    marks:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码将输出一个包含单词和标点符号的标记列表：
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When you execute the preceding code snippet, `nltk` will download the pretrained
    punctuation model in order to run the word tokenizer. The output of the tokenizer
    is the words and punctuation marks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行前面的代码片段时，`nltk`将下载预训练的标点模型以运行分词器。分词器的输出是单词和标点符号。
- en: 'In the next step, we will remove the punctuation marks as they are not relevant
    for the subsequent *stemming* process. However, we will bring them back for *lemmatization*
    later in this section:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将移除标点符号，因为它们对于随后的*词形还原*过程不相关。然而，我们将在本节稍后将其恢复：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The result will only contain the words of the original document without any
    punctuation marks:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将只包含原始文档中的单词，没有任何标点符号：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding code, we used the `word.islanum()` function to only extract
    alphanumeric tokens and make them all lowercase. The preceding list of words already
    looks much better than the initial naïve model. However, it still contains a lot
    of unnecessary words, such as *the*, *we*, *had*, and more, which don't convey
    any information.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了`word.isalnum()`函数来仅提取字母数字标记并将它们全部转换为小写。前面的单词列表已经比最初的原始模型好得多。然而，它仍然包含许多不必要的词，如*the*、*we*、*had*等，这些词不传达任何信息。
- en: 'In order to filter out the noise for a specific language, it makes sense to
    remove these words that often appear in texts and don''t add any semantic meaning
    to the text. It is common practice to remove these so-called `nltk` library in
    Python:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了过滤掉特定语言的噪声，有道理移除那些经常出现在文本中且不增加任何语义意义的词。在Python中，移除这些词是常见的做法，使用`nltk`库：
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now the resulting list only contains words that are not stop words:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在得到的列表只包含不是停用词的单词：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code gives us a nice pipeline where we end up with only the semantically
    meaningful words. We can take this list of words to the next step and apply a
    more sophisticated transformation/normalization to each word. If we applied the
    count vectorizer at this stage, we would end up with the simple bag-of-words model,
    as shown in *Figure 7.15*:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码为我们提供了一个很好的管道，我们最终只得到具有语义意义的词。我们可以将这个词表带到下一步，并对每个词应用更复杂的转换/归一化。如果我们在这个阶段应用计数向量器，我们最终会得到如图7.15所示的简单词袋模型：
- en: '![Figure 7.15 – A simple bag-of-words model ](img/B17928_07_015.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图7.15 – 一个简单的词袋模型](img/B17928_07_015.jpg)'
- en: Figure 7.15 – A simple bag-of-words model
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – 一个简单的词袋模型
- en: As you can see in the previous figure, the list of terms that are included in
    the bag-of-words model is already far cleaner than the naïve example. This is
    because it doesn't contain any punctuation marks or stop words.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，词袋模型中包含的术语列表已经比原始示例干净得多。这是因为它不包含任何标点符号或停用词。
- en: You might ask what qualifies a word as a stop word other than it occurring relatively
    often in a piece of text? Well, that's an excellent question! We can measure the
    importance of each word in the current context compared to its occurrences across
    the text using the **TF-IDF** method, which will be discussed in the *Measuring
    the importance of words using TF-IDF* section.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，除了在文本中相对频繁出现之外，什么使一个词成为停用词？嗯，这是一个非常好的问题！我们可以使用**TF-IDF**方法来衡量每个词在当前上下文中的重要性，与它在整个文本中的出现频率进行比较，这将在*使用TF-IDF衡量词的重要性*部分进行讨论。
- en: Stemming – the rule-based removal of affixes
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词干提取 – 基于规则的词缀移除
- en: In the next step, we want to normalize affixes—word endings to create plurals
    and conjugations. You can see that with each step, we are diving deeper into the
    concept of a single language—in this case, English. However, when applying these
    steps to a different language, it's likely that completely different transformations
    will need to be used. This is what makes NLP such a difficult field.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们想要归一化词缀——单词的结尾以创建复数和动词变位。你可以看到，随着每一步的进行，我们都在更深入地探讨单一语言的概念——在这个案例中，是英语。然而，当将这些步骤应用于不同的语言时，可能需要使用完全不同的转换。这就是为什么NLP是一个如此困难的领域。
- en: 'Removing the affixes of words to obtain the stem of a word is also called **stemming**.
    Stemming refers to a rule-based (heuristic) approach to transform each occurrence
    of a word into its word stem. Here is a simple example of some expected transformations:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 移除单词的词缀以获得词根也称为**词干提取**。词干提取是指将每个单词的出现转换为它的词根的基于规则（启发式）方法。以下是一些预期的转换示例：
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see in the preceding example, such a heuristic approach for stemming
    has to be built specifically for each language. This is generally true for all
    other NLP algorithms as well. For the sake of brevity, in this book, we will only
    discuss English examples.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'A popular algorithm for stemming in English is Porter''s algorithm, which defines
    five sequential reduction rules, such as removing *-ed*, *-ing*, -*ate*, *-tion*,
    *-ence*, *-ance*, and more, from the end of words. The `nltk` library comes with
    an implementation of Porter''s stemming algorithm:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The resulting list of words after stemming looks like this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code, we simply apply `stemmer` to each word in the tokenized
    document. The bag-of-words model after this step is shown in *Figure 7.16*:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – The bag-of-words model after stemming ](img/B17928_07_016.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – The bag-of-words model after stemming
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: While this algorithm works well with affixes, it can't avoid normalizing conjugations
    and tenses. This will be our next problem to tackle using lemmatization.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization – dictionary-based word normalization
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When looking at the stemming examples, we can already see the limitations of
    that approach. For example, what would happen with irregular verb conjugations—such
    as *are*, *am*, or *is*—that should all be normalized to the same word, *be*?
    This is exactly what lemmatization tries to solve using a pretrained set of vocabulary
    and conversion rules, called lemmas. The **lemmas** are stored in a lookup dictionary
    and look similar to the following transformations:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'There is one very important point to make when discussing lemmatization. Each
    lemma needs to be applied to the correct word type, hence a lemma for nouns, verbs,
    adjectives, and more. The reason for this is that a word can be either a noun
    or a verb in the past tense. In our example, `ground` could come from the noun
    *ground* or the verb *grind*; `left` could be an adjective or the past tense of
    *leave*. So, we also need to extract the word type from the word in a sentence—this
    process is called `nltk` library has us covered once again. To estimate the correct
    POS tag, we also need to provide the punctuation mark:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here are the resulting POS tags:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The POS tags describe the word type of each token in the document. You can
    find a complete list of tags using the `nltk.help.upenn_tagset()` command. Here
    is an example of how to do so from the command line:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding command will print the list of POS tags:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The POS tags also include tenses for verbs and other very useful information.
    However, for the lemmatization in this section, we only need to know the word
    type—*noun*, *verb*, *adjective*, or *adverb*. One possible choice of lemmatizer
    is the WordNet lemmatizer in `nltk`. WordNet is a lexical database of English
    words that groups them into groups of concepts and word types.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply the lemmatizer to the output of the stemming, we need to filter the
    POS tags by punctuation marks and stop words, similar to the previous preprocessing
    step. Then, we can use the word tags for the resulting words. Let''s apply the
    lemmatizer using `nltk`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The code outputs the lemmatized words:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding list of words looks a lot cleaner than what we found in previous
    models. This is because we normalized the tenses of the verbs and transformed
    them into their infinitive form. The resulting bag-of-words model is shown in
    *Figure 7.17*:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – The bag-of-words model after lemmatization ](img/B17928_07_017.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – The bag-of-words model after lemmatization
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: This technique is extremely helpful for cleaning up irregular forms of words
    in your dataset. However, it works based on rules—called lemmas—and, hence, it
    can only be used for languages and words where such lemmas are available.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: A bag-of-words model in scikit-learn
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we can put all our previous steps together to create a state-of-the-art
    NLP preprocessing pipeline to normalize the input documents and run them through
    a count vectorizer so that we can transform them into a numeric feature vector.
    Doing so for multiple documents allows us to easily compare the semantics of the
    document in a numerical space. We could compute cosine similarities on the document's
    feature vectors to compute their similarity, plug them into a supervised classification
    method, or perform clustering on the resulting document concepts.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, let''s take a look at the final pipeline for the simple bag-of-words
    model. I want to emphasize that this model is only the start of our journey in
    feature extraction using NLP. We performed the following steps for normalization:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing punctuation marks
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing stop words
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stemming
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lemmatization with POS tagging
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the last step, we applied `CountVectorizer` in scikit-learn. This will count
    the occurrences of each word, create a global corpus of words, and output a sparse
    feature vector of word frequencies. Here is the sample code to pass the preprocessed
    data from `nltk` to `CountVectorizer`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The transformed bag-of-words model contains coordinates and counts:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The coordinates refer to the `(document id, term id)` pair, whereas the count
    refers to the term frequency. To better understand this output, we can also look
    at the internal vocabulary of the model. The `vocabulary_` parameter contains
    a lookup dictionary for the term ids:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The code outputs the model''s word dictionary:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the preceding example, we transform the preprocessed document back into a
    string before passing it to `CountVectorizer`. The reason for this is that `CountVectorizer`
    comes with some configurable preprocessing techniques out of the box, such as
    tokenization, stop word removal, and more. For this demonstration, we want to
    apply it to the preprocessed data. The output of the transformation is a sparse
    feature vector containing the term frequencies.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Let's find out how we can combine multiple terms with semantic concepts.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging term importance and semantics
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everything we have done up to now has been relatively simple and based on word
    stems or so-called tokens. The bag-of-words model was nothing but a dictionary
    of tokens counting the occurrence of tokens per field. In this section, we will
    take a look at a common technique to further improve matching between documents
    using n-gram and skip-gram combinations of terms.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Combining terms in multiple ways will explode your dictionary. This will turn
    into a problem if you have a large corpus; for instance, 10 million words. Hence,
    we will look at a common preprocessing technique to reduce the dimensionality
    of a large dictionary through SVD.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: While, now, this approach is a lot more complicated, it is still based on a
    bag-of-words model that already works well on a large corpus, in practice. However,
    of course, we can do better and try to understand the importance of words. Therefore,
    we will tackle another popular technique in NLP to compute the importance of terms.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Generalizing words using n-grams and skip-grams
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous pipeline, we considered each word on its own without any context.
    However, as we all know, context matters a lot in language. Sometimes, words belong
    together and only make sense in context rather than on their own. To introduce
    this context into the same type of algorithm, we will introduce **n-grams** and
    **skip-grams**. Both techniques are heavily used in NLP for preprocessing datasets
    and extracting relevant features from text data.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with n-grams. An `N` consecutive entities (that is, characters,
    words, or tokens) of an input dataset. Here are some examples for computing the
    n-grams in a list of characters:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is an example using the built-in `ngram_range` parameter in scikit-learn''s
    `CountVectorizer` to generate multiple n-grams for the input data:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As you can see, the vocabulary now contains both the 1-gram and 2-gram representations
    of each term:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the preceding code, we can see that instead of the original words, we now
    have a combination of two consecutive words in our trained vocabulary.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'We can extend the concept of n-grams to also allow the model to skip words.
    This a great option, if we for example want to perform a 2-gram, but in one of
    our samples there is an adjective in-between two words and in the other those
    words are directly next to each other. To achieve this, we need a method that
    allows us to define how many words it is allowed to skip to find matching words.
    Here is an example using the same characters as before:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Luckily, we find the generalized version of n-grams implemented in `nltk` as
    the `nltk.skipgrams` method. Setting the skip distance to `0` results in the traditional
    n-gram algorithm. We can apply it to our original dataset:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Similar to the 2-gram example, the method produces a list of combinations of
    paired terms. However, in this case, we allowed one skip word to be present between
    those pairs:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the preceding code, we can observe that skip-grams can generate a lot of
    additional useful feature dimensions for the NLP model. In real-world scenarios,
    both techniques are often used because the individual word order plays a big role
    in the semantics.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: However, the explosion of new feature dimensions could be devastating if the
    input documents are, for example, all websites from the web or large documents.
    Therefore, we also need a way to avoid an explosion of the dimensions while capturing
    all of the semantics from the input data. We will tackle this challenge in the
    next section.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Reducing word dictionary size using SVD
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common problem with NLP is the vast number of words in a corpus and, hence,
    exploding dictionary sizes. In the previous example, we saw that the size of the
    dictionary defines the size of the orthogonal term vector. Therefore, a dictionary
    size of 20,000 terms would result in 20,000-dimensional feature vectors. Even
    without any n-gram enrichment, this feature vector dimension is too large to be
    processed on standard PCs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need an algorithm to reduce the dimensions of the generated `CountVectorizer`
    while preserving the present information. Optimally, we would only remove redundant
    information from the input data and project it onto a lower-dimensional space
    while preserving all of the original information.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: The PCA transformation would be a great fit for our solution and help us to
    transform the input data into lower linearly unrelated dimensions. However, computing
    the eigenvalues requires a symmetric matrix (the same number of rows and columns),
    which, in our case, we don't have. Hence, we can use the SVD algorithm, which
    generalizes the eigenvector computation to non-symmetric matrices. Due to its
    numeric stability, it is often used in NLP and information retrieval systems.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The usage of SVD in NLP applications is also called **Latent Semantic Analysis**
    (**LSA**), as the principal components can be interpreted as concepts in a latent
    feature space. The SVD embedding transforms the high-dimensional feature vector
    into a lower-dimensional concept space. Each dimension in the concept space is
    constructed by a linear combination of term vectors. By dropping the concepts
    with the smallest variance, we also reduce the dimensions of the resulting concept
    space to something that is a lot smaller and easier to handle. Typical concept
    spaces have 10s to 100s of dimensions, while word dictionaries usually have over
    100,000.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example using the `TruncatedSVD` implementation from `sklearn`.
    The SVD is implemented as a transformer class, and so, we need to call `fit_transform()`
    to fit a dictionary and transform it using the same step. The SVD is configured
    to only keep the components with the highest variance using the `n_components`
    argument:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the preceding code, we perform the LSA on the `X_train_counts` data and the
    output of `CountVectorizer` using SVD. We configure the SVD to only keep the first
    five components with the highest variance.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'By reducing the dimensionality of your dataset, you lose information. Thankfully,
    we can compute the amount of variance in the remaining dataset using the trained
    SVD object, as shown in the following example:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding command outputs the variance as a number between 0 and 1, where
    1 means that the SVD transformation is an exact lossless mapping of the original
    data into the latent space:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In this case, with only five components, the SVD retained 20% of the variance
    of the original dataset.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the task, we usually aim to preserve more than 80–90% of the original
    variance after the latent transformation.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: In the previous code example, we computed the variance of the data that is preserved
    after the transformation to the configured number of components. Hence, we can
    now increase or reduce the number of components in order to keep a specific percentage
    of the information in the transformed data. This is a very helpful operation and
    is used in many practical NLP implementations.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are still using the original word dictionary from the bag-of-words
    model. One particular downside of this model is that the more often a term occurs,
    the higher its count (and, therefore, weight) will get. This is a problem because,
    now, any term that is not a stop word and appears often in the text will receive
    a high weight—independent of the importance of the term within a certain document.
    Therefore, we introduce another extremely popular preprocessing technique—**TF-IDF**.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the importance of words using TF-IDF
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One particular downside of the bag-of-words approach is that we simply count
    the absolute number of words in a context without checking whether the word generally
    appears frequently across all documents. A term that appears in every document
    might not be relevant for our model, as it contains less information and more
    often it appears across other documents. Hence, an important technique in text
    mining is to compute the importance of a certain word in a given context.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, instead of an absolute count of terms in a context, we want to compute
    the number of terms in the context relative to a corpus of documents. By doing
    so, we will give higher weight to terms that appear only in a certain context,
    and reduce the amount of weight given to terms that appear in many different documents.
    This is exactly what the TF-IDF algorithm does. It is easy to compute a weight
    (*w*) for a term (*t*) in a document (*d*) according to the following equation:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_07_001.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: While the term frequency (*f*t) counts all of the terms in a document, the inverse
    document frequency is computed by dividing the total number of documents (*N*)
    by the counts of a term in all documents (*f*d). The *IDF* term is usually log-transformed,
    as the total count of a term across all documents can get quite large.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will not use the TF-IDF function directly. Instead,
    we will use `TfidfVectorizer`, which does the counting and then applies the TF-IDF
    function to the result in one step. Again, the function is implemented as a `sklearn`
    transformer, and hence, we call `fit_transform()` to train and transform the dataset:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The result is formatted in a similar manner to the earlier example containing
    `(document id, term id)` pairs and their TF-IDF values:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In the preceding code, we apply `TfidfVectorizer` directly, which returns the
    same result as using `CountVectorizer` and `TfidfTransformer` combined. We transform
    a dataset containing the words of the bag-of-words model and return the TF-IDF
    values. We can also return the terms for each TF-IDF value:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code returns the vocabulary of the model:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In this example, we can see that `ground` gets a TF-IDF value of `0.667`, whereas
    all the other terms receive a value of `0.333`. This count will now scale relatively
    when more documents are added to the corpus—hence, if the word `hold` were to
    be included again, the TF-IDF value would decrease.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: In any real-world pipeline, we would always use all the techniques presented
    in this chapter—tokenization, stop word removal, stemming, lemmatization, n-grams/skip-grams,
    TF-IDF, and SVD—combined in a single pipeline. The result would be a numeric representation
    of n-grams/skip-grams of tokens weighted by importance and transformed into a
    latent semantic space. Using these techniques for your first NLP pipeline will
    get you quite far, as you can now capture a lot of information from your textual
    data.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned how to numerically encode many kinds of categorical
    and textual values by using either one-dimensional or N-dimensional labels or
    counting and weighting word stems and character combinations. While many of these
    methods work well in many situations where you require simple numeric embedding,
    they all have a serious limitation—they don't encode semantics. Let's take a look
    at how we can extract the semantic meaning of text in the same pipeline.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Extracting semantics using word embeddings
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When computing the similarity of news, you would imagine that topics such as
    tennis, *Formula 1*, or *soccer* would be semantically more similar to each other
    than topics such as politics, economics, or science. Yet, in terms of the previously
    discussed techniques, all encoded categories are seen as semantically the same.
    In this section, we will discuss a simple method of semantic embedding, which
    is also called **word embedding**.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: The previously discussed pipeline using LSA transforms multiple documents into
    terms and then transforms those terms into semantic concepts that can be compared
    with other documents. However, the semantic meaning is based on the term occurrences
    and importance—there is no measurement of semantics between individual terms.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, what we are looking for is an embedding of terms into numerical multidimensional
    space such that each word represents one point in this space. This allows us to
    compute a numerical distance between multiple words in this space in order to
    compare the semantic meaning of two words. The most interesting benefit of word
    embeddings is that algebra on the word embeddings is not only numerically possible
    but also makes sense. Consider the following example:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We can create such an embedding by mapping a corpus of words on an N-dimensional
    numeric space and optimizing the numeric distance based on the word semantics—for
    example, based on the distance between words in a corpus. The resulting optimization
    outputs a dictionary of words in the corpus and their numeric N-dimensional representation.
    In this numeric space, words have the same, or at least similar, properties as
    in the semantic space. A great benefit is that these embeddings can be trained
    unsupervised, so no training data has to be labeled.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the first embeddings is called **Word2Vec** and is based on a continuous
    bag-of-words model or a continuous skip-gram model to count and measure the words
    in a window. Let''s try this functionality and perform a semantic word embedding
    using Word2Vec:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'The best Python implementation for word embeddings is **Gensim**, which we
    will also use here. We need to feed our tokens into the model in order to train
    it:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In the preceding code, we load the `Word2Vec` model and initialize it with the
    list of tokens from the previous sections, which is stored in the `words` variable.
    The `size` attribute defines the dimension of the resulting vectors, and the `window`
    parameter decides how many words we should consider per window. Once the model
    has been trained, we can simply look up the word embedding in the model's dictionary.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The code will automatically train the embedding on the set of tokens we provided.
    The resulting model stores the word-to-vector mapping in the `wv` property. Optimally,
    we also use a large corpus or pretrained model that is either provided by `gensim`
    or another NLP library, such as `NLTK`, to train the embedding and fine-tune it
    with a smaller dataset.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can use the trained model to embed all the terms from our document
    using the Word2Vec embedding. However, this will result in multiple vectors as
    each word returns its own embedding. Therefore, you need to combine all the vectors
    into a single vector using the mathematical mean of all the embeddings. This procedure
    is quite similar to the one used to generate a concept in LSA. Also, other reduction
    techniques are possible; for example, weighing the individual embedding vectors
    using their TF-IDF values:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In the preceding function, we compute the mean from all the word embedding vectors
    of the terms—this is called a **mean embedding**, and it represents the concept
    of this document in the embedding space. If a word is not found in the embedding,
    we need to replace it with zeros in the computation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: You can use such a semantic embedding for your application by downloading a
    pretrained embedding, for example, on the Wikipedia corpus. Then, you can loop
    through your sanitized input tokens and look up the words in the dictionary of
    the numeric embedding.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'GloVe is another popular technique for encoding words as numerical vectors,
    developed by Stanford University. In contrast to the continuous window-based approach,
    it uses global word-to-word co-occurrence statistics to determine the linear relationships
    between words:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the pretrained 6 B tokens embedding trained on Wikipedia
    and the Gigaword news archive:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In the preceding code, we only open and parse the pretrained word embedding
    in order to store the word and vectors in a lookup dictionary.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we use the dictionary to look up tokens in our training data and merge
    them by computing the mean of all GloVe vectors:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The preceding code works very similar to before and returns one vector per word,
    which is aggregated by taking their mean at the end. Again, this corresponds with
    a semantic concept using all the tokens of the training data.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Gensim provides other popular models for semantic embeddings, such as *doc2word*,
    *fastText*, and *GloVe*. The `gensim` Python library is a great place for utilizing
    these pretrained embeddings or for training your own models. Now you can replace
    your bag-of-words model with a mean embedding of the word vectors to also capture
    word semantics. However, your pipeline will still be built out of many tunable
    components.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will take a look at building end-to-end state-of-the-art
    language models and reusing some of the language features from Azure Cognitive
    Services.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Implementing end-to-end language models
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we trained and concatenated multiple pieces to implement
    a final algorithm where most of the individual steps need to be trained as well.
    Lemmatization contains a dictionary of conversion rules. Stop words are stored
    in the dictionary. Stemming needs rules for each language and word that the embedding
    needs to train—TF-IDF and SVD are only computed on your training data but are
    independent of each other.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: This is a similar problem to the traditional computer vision approach, which
    we will discuss in more depth in [*Chapter 10*](B17928_10_ePub.xhtml#_idTextAnchor165),
    *Training Deep Neural Networks on Azure*, where many classic algorithms are combined
    into a pipeline of feature extractors and classifiers. Similar to breakthroughs
    of end-to-end models trained via gradient descent and backpropagation in computer
    vision, deep neural networks—especially sequence-to-sequence models—have replaced
    the classical approach of performing each step of the transformation and training
    process manually.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: In this section, first, we will take a look at improving our previous model
    using custom embedding and an LSTM implementation to model a token sequence. This
    will give you a good understanding of how we are moving from an individual preprocessor-based
    pipeline to a full end-to-end approach using deep learning.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence models are models based on encoders and decoders that are
    trained on a variable set of inputs. This encoder/decoder architecture is used
    for a variety of tasks, such as machine translation, image captioning, and summarization.
    A nice benefit of these models is that you can reuse the encoder part of this
    network to convert a set of inputs into a fixed-set numerical representation of
    the encoder.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the state-of-the-art language representation models and
    discuss how they can be used for feature engineering and the preprocessing of
    your text data. We will use BERT to perform sentiment analysis and numeric embedding.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will also look at reusing the Azure Cognitive Services APIs for
    text analytics to carry out advanced modeling and feature extraction, such as
    text or sentence sentiment, keywords, or entity recognition. This is a nice approach
    because you can leverage the know-how and amount of training data from Microsoft
    to perform complex text analytics using a simple HTTP request.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: The end-to-end learning of token sequences
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of concatenating different pieces of algorithms into a single pipeline,
    we want to build and train an end-to-end model that can train the word embedding,
    pre-form latent semantic transformation, and capture sequential information in
    the text in a single model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefit of such a model is that each processing step can be fine-tuned
    for the user''s prediction task in a single combined optimization process:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of the pipeline will look extremely similar to the previous
    sections. We will build a tokenizer that converts documents into sequences of
    tokens that are then transformed into a numerical model based on the token sequence.
    Then, we will use `pad_sequences` to align all of the documents to the same length:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In the next step, we will build a simple model using Keras, an embedding layer,
    and an LSTM layer to capture token sequences. The embedding layer will perform
    a similar operation to GloVe, where the words will be embedded into a semantic
    space. The LSTM cell will ensure that we are comparing sequences of words instead
    of single words at a time. Then, we will use a dense layer with a *softmax* activation
    to implement a classifier head:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: As you can see in the preceding function, we build a simple neural network using
    three layers (that is, `Embedding`, `LSTM`, and `Dense`) and a `softmax` activation
    for classification. This means that in order to train this model, we would also
    need a classification problem to be solved at the same time. Hence, we do need
    labeled training data to perform analysis using this approach. In the next section,
    we will examine how sequence-to-sequence models are used in input-output text
    sequences to learn an implicit text representation.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-art sequence-to-sequence models
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, another type of model has replaced the traditional NLP pipelines—transformer-based
    models. These types of models are fully end-to-end and use sequence-to-sequence
    mapping, positional encoding, and multi-head attention layers. This allows the
    models to look forward and backward in a text, pay attention to specific patterns,
    and learn tasks fully end to end. As you might be able to tell, these models have
    complex architectures and usually have well over 100 million or over 1 billion
    parameters.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence models are now state of the art for many complex end-to-end
    NLP problems such as classification (for example, sentiment or text analysis),
    language understanding (for example, entity recognition), translation, text generation,
    summarization, and more.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: One popular sequence-to-sequence model is BERT, which, today, exists in many
    different variations and configurations. Models based on the BERT architecture
    seem to perform particularly well but have already been outperformed by newer
    updated architectures, tuned parameters, or models with more training data.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to get started using these new NLP models is with the *Hugging
    Face* `transformers` library, which provides end-to-end models (or pipelines)
    along with pretrained tokenizers and models. The `transformers` library implements
    all model architectures for both *TensorFlow* and *PyTorch*. The models can be
    easily consumed and used in an application, trained from scratch, or fine-tuned
    using domain-specific custom training data.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows how to implement sentiment analysis using the default
    `sentiment-analysis` pipeline, which, at the time of writing, uses the `TFDistilBertForSequenceClassification`
    model:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As you can see in the previous example, it's very simple to use a pretrained
    model for an end-to-end prediction task. These three lines of code can easily
    be integrated into your feature extraction pipeline to enrich your training data
    with sentiments.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Besides end-to-end models, another popular application of NLP is to provide
    semantic embeddings for textual data during preprocessing. This can also be implemented
    using the `transformers` library and any of the many supported models.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, first, we initialize a pretrained tokenizer for BERT. This will
    help us to split the input data into the correct format for the BERT model:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Once we have transformed the input into a token sequence, we can evaluate the
    BERT model. To retrieve the numerical embedding, we need to understand the latent
    state of the encoder, which we can retrieve using the `last_hidden_state` property:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The last hidden layer contains the latent representation of the model, which
    we can now use as a semantic numerical representation in our model:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The key takeaway from these models is that they use an encoder/decoder-based
    architecture, which allows us to simply borrow the encoder to embed text into
    a semantic numerical feature space. Hence, a common approach is to download the
    pretrained model and perform a forward pass through the encoder part of the network.
    The fixed-sized numerical output can now be used as a feature vector for any other
    model. This is a common preprocessing step and a good trade-off for using a state-of-the-art
    language model for numerical embedding.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Text analytics using Azure Cognitive Services
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A good approach in many engineering disciplines is to not reinvent the wheel
    when many other companies have already solved the same problem far better than
    you will ever be able to solve it. This might be the case for basic text analytics
    and text understanding tasks that Microsoft has developed, implemented, and trained
    and now offers as a service.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: What if I told you that when working with Azure, text understanding features
    such as sentiment analysis, key phrase extraction, language detection, named entity
    recognition, and the extraction of **Personally Identifiable Information** (**PII**)
    is just one request away? Azure provides the Text Analytics API as part of Cognitive
    Services, which will solve all of these problems for you.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: This won't solve the need to transform a piece of text into numerical values,
    but it will make it easier to extract semantics from your text. One example would
    be to perform a key phrase extraction or sentiment analysis using Cognitive Services
    as an additional feature engineering step, instead of implementing your own NLP
    pipeline.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement a function that returns the sentiment for a given document
    using the Text Analytics API of Cognitive Services. This is great when you want
    to enrich your data with additional attributes, such as overall sentiment, in
    the text. Let''s start by setting up all the parameters we will need to call the
    Cognitive Services API:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we define the content and metadata of the request. We create a `payload`
    object that contains a single document and the text we want to analyze:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Finally, we need to send the payload, heads, and parameters to the Cognitive
    Services API:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The preceding code looks very similar to the computer vision example that we
    saw in [*Chapter 2*](B17928_02_ePub.xhtml#_idTextAnchor034), *Choosing the Right
    Machine Learning Service in Azure*. In fact, it uses the same API but just a different
    endpoint for Text Analytics and, in this case, sentiment analysis functionality.
    Let''s run this code and look at the output, which looks very similar to the following
    snippet:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We can observe that the JSON response contains a sentiment classification for
    each document (`positive`, `neutral`, and `negative`) as well as numeric confidence
    scores for each class. Also, you can see that the resulting documents are stored
    in an array and marked with an `id` value. Hence, you can send multiple documents
    to this API using an ID to identify each document.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Using custom pretrained language models is great, but for standardized text
    analytics, we can simply reuse Cognitive Services. Microsoft has invested tons
    of resources into the research and production of these language models, which
    you can use for your own data pipelines for a relatively small amount of money.
    Therefore, if you prefer using a managed service instead of running your customer
    transformer model, you should try this Text Analytics API.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to preprocess textual and categorical nominal
    and ordinal data using state-of-the-art NLP techniques.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: You can now build a classical NLP pipeline with stop word removal, *lemmatization*
    and *stemming*, *n-grams*, and count term occurrences using a *bag-of-words* model.
    We used *SVD* to reduce the dimensionality of the resulting feature vector and
    to generate lower-dimensional topic encoding. One important tweak to the count-based
    bag-of-words model is to compare the relative term frequencies of a document.
    You learned about the *TF-IDF* function and can use it to compute the importance
    of a word in a document compared to the corpus.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we looked at *Word2Vec* and *GloVe*, which are pretrained
    dictionaries of numeric word embeddings. Now you can easily reuse a pretrained
    word embedding for commercial NLP applications with great improvements and accuracy
    due to the semantic embedding of words.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we finished the chapter by looking at a state-of-the-art approach to
    language modeling, using end-to-end language representations, such as *BERT* and
    BERT-based architectures, which are trained as sequence-to-sequence models. The
    benefit of these models is that you can reuse the encoder to transform a sequence
    of text into a numerical representation, which is a very common task during feature
    extraction.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how to train an ML model using Azure Machine
    Learning, applying everything we have learned so far.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
