- en: '*Chapter 7*: Advanced Feature Extraction with NLP'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned about many standard transformation and
    preprocessing approaches within the Azure Machine Learning service as well as
    typical labeling techniques using the Azure Machine Learning Data Labeling service.
    In this chapter, we want to go one step further to extract semantic features from
    textual and categorical data—a problem that users often face when training ML
    models. This chapter will describe the foundations of feature extraction with
    **Natural Language Processing** (**NLP**). This will help you to practically implement
    semantic embeddings using NLP for your ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will take a look at the differences between *textual*, *categorical*,
    *nominal*, and *ordinal* data. This classification will help you to decide the
    best feature extraction and transformation technique per feature type. Later,
    we will look at the most common transformations for categorical values, namely
    **label encoding** and **one-hot encoding**. Both techniques will be compared
    and tested to understand the different use cases and applications for both techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will tackle the numerical embedding of textual data. To achieve this,
    we will build a simple **bag-of-words** model, using a **count vectorizer**. To
    sanitize the input, we will build an NLP pipeline consisting of a **tokenizer**,
    stop word removal, **stemming**, and **lemmatization**. We will learn how these
    different techniques affect a sample dataset step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Following this, we will replace the word count method with a much better word
    frequency weighting approach—the **Term Frequency-Inverse Document Frequency**
    (**TF-IDF**) algorithm. This will help you to compute the importance of words
    when given a whole corpus of documents by weighting the occurrence of a term in
    one document over the frequency in the corpus. Additionally, we will look at **Singular
    Value Decomposition** (**SVD**) for reducing the size of the term dictionary.
    As a next step, we will improve the term embedding quality by leveraging word
    semantics, and we will look under the hood of semantic embeddings such as **Global
    Vectors** (**GloVe**) and **Word2Vec**.
  prefs: []
  type: TYPE_NORMAL
- en: In the last section, we will take a look at current state-of-the-art language
    models that are based on sequence-to-sequence deep neural networks with over 100
    million parameters. We will train a small end-to-end model using **Long Short-Term
    Memory** (**LSTM**), perform word embedding and sentiment analysis using **Bidirectional
    Encoder Representations from Transformers** (**BERT**), and compare both custom
    solutions to Azure's text analytics capabilities in Cognitive Services.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding categorical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a simple bag-of-words model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging term importance and semantics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing end-to-end language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the following Python libraries and versions to
    create categorical encodings, create semantic embeddings, train an end-to-end
    model, and perform classic NLP preprocessing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`azureml-sdk 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`azureml-widgets 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensorflow 2.6.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy 1.19.5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas 1.3.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn 0.24.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nltk 3.6.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`genism 3.8.3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to previous chapters, you can execute this code using either a local
    Python interpreter or a notebook environment hosted in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding categorical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Categorical data** comes in many forms, shapes, and meanings. It is extremely
    important to understand what type of data you are dealing with—is it a string,
    text, or numeric value disguised as a categorical value? This information is essential
    for data preprocessing, feature extraction, and model selection.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, first, we will take a look at the different types of categorical
    data—namely *ordinal*, *nominal*, and *text*. Depending on the type, you can use
    different methods to extract information or other valuable data from it. Please
    bear in mind that categorical data is ubiquitous, whether it is in an ID column,
    a nominal category, an ordinal category, or a free-text field. It's worth mentioning
    that the more information you have on the data, the easier the preprocessing is.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will actually preprocess the ordinal and nominal categorical data by
    transforming it into numerical values. This is a required step when you want to
    use an ML algorithm later on that can't interpret categorical data, which is true
    for most algorithms except, for example, decision tree-based approaches. Most
    other algorithms can only operate (for example, compute a loss function) on a
    numeric value and so a transformation is required.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing textual, categorical, and ordinal data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many ML algorithms, such as support vector machines, neural networks, linear
    regression, and more, can only be applied to numeric data. However, in real-world
    datasets, we often find non-numeric columns, such as columns that contain textual
    data. The goal of this chapter is to transform textual data into numeric data
    as an advanced feature extraction step, which allows us to plug the processed
    data into any ML algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with real-world data, you will be confronted with many different
    types of textual and/or categorical data. To optimize ML algorithms, you need
    to understand the differences in order to apply different preprocessing techniques
    to the different types. But first, let''s define the three different textual data
    types:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Textual data*: Free text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Categorical nominal data*: Non-orderable categories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Categorical ordinal data*: Orderable categories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between textual data and categorical data is that, in textual
    data, we want to capture semantic similarities (that is, the similarity in the
    meaning of the words), whereas, in categorical data, we want to differentiate
    between a small number of variables.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between categorical nominal data and categorical ordinal data
    is that nominal data cannot be ordered (all categories have the same weight),
    whereas ordinal categories can be logically ordered on an ordinal scale.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.1* shows an example dataset of comments on news articles, where the
    first column, named `statement`, is a textual field, the column named `topic`
    is a nominal category, and `rating` is an ordinal category:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Comparing different textual data types ](img/B17928_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Comparing different textual data types
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the differences between these data representations is essential
    to find the proper embedding technique afterward. It seems quite natural to replace
    ordinal categories with an ordinal numeric scale and to embed nominal categories
    in an orthogonal space. On the contrary, it's not obvious how to embed textual
    data into a numerical space where the semantics are preserved—this will be covered
    in the later sections of this chapter that deal with NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that instead of categorical values, you will also see continuous
    numeric variables representing categorical information, for example, IDs from
    a dimension or lookup table. Although these are numeric values, you should consider
    treating them as categorical nominal values, if possible. Here is an example dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Comparing numerical categorical values ](img/B17928_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Comparing numerical categorical values
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we can see that the `sensorId` value is a numeric value that
    should be interpreted as a categorical nominal value instead of a numeric value
    by default because it doesn't have a numeric meaning. What do you get when you
    subtract `sensorId` `2` from `sensorId` `1`? Is `sensorId` `10` 10 times larger
    than `sensorId` `1`? These are the typical questions to ask to discover and encode
    these categorical values. We will discover, in [*Chapter 9*](B17928_09_ePub.xhtml#_idTextAnchor152),
    *Building ML Models Using Azure Machine Learning*, that by specifying that these
    values are categorical, a gradient-boosted tree model can optimize these features
    instead of treating them as continuous variables.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming categories into numeric values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start by converting categorical variables (both ordinal and nominal)
    into numeric values. In this section, we will look at two common techniques for
    categorical encoding: **label encoding** and **one-hot encoding** (also called
    *dummy coding*). While **label encoding** replaces a categorical feature column
    with a numerical feature column, **one-hot encoding** uses multiple columns (where
    the number of columns equals the number of unique values) to encode a single feature.'
  prefs: []
  type: TYPE_NORMAL
- en: Both techniques are applied in the same way. During the training iteration,
    these techniques find all of the unique values in a feature column and assign
    them a specific numeric value (multidimensional value for one-hot encoding). As
    a result, a lookup dictionary defining this replacement is stored in the encoder.
    When the encoder is applied, the values in the applied column are transformed
    (replaced) using the lookup dictionary. If the list of possible values is known
    beforehand, most implementations allow the encoder to initialize the lookup dictionary
    directly from the list of known values, rather than finding the unique values
    in the training set. This has the benefit of specifying the order of the values
    in the dictionary, so orders the encoded values.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Please note that it's often possible that certain categorical feature values
    in the test set don't appear in the training set and, hence, are not stored in
    the lookup dictionary. So, you should add a default category to your encoder that
    can also transform unseen values into numeric values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will use two different categorical data columns, one ordinal and one
    nominal category, to showcase the different encodings. *Figure 7.3* shows a nominal
    feature, `topic`, which could represent a list of articles by a news agency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Nominal categorical data ](img/B17928_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Nominal categorical data
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.4* contains the ordinal category of `rating`; it could represent
    a feedback form for purchased articles on a website:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Ordinal categorical data ](img/B17928_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Ordinal categorical data
  prefs: []
  type: TYPE_NORMAL
- en: To preserve the meaning of the categories, we require different preprocessing
    techniques for the different categorical data types. First, we take a look at
    the *label encoder*. The label encoder assigns an incrementing value to each unique
    categorical value in a feature column. So, it transforms categories into a numeric
    value between `0` and `N-1`, where `N` represents the number of unique values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test the label encoder in the `topic` column within the first table.
    We train the encoder on the data and replace the `topic` column with a numeric
    topic ID. Here is an example snippet to train the label encoder and transform
    the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 7.5* shows the results of the preceding transformation. Each topic
    was encoded as a numerical increment, `topicId`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Label-encoded topics ](img/B17928_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Label-encoded topics
  prefs: []
  type: TYPE_NORMAL
- en: 'The generated lookup table for `topicId` is shown in *Figure 7.6*. This lookup
    dictionary was learned by the encoder during the `fit()` method and can be applied
    to categorical data using the `transform()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – A lookup dictionary for topics ](img/B17928_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – A lookup dictionary for topics
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the previous screenshots, encoding nominal data with labels
    is easy and straightforward. However, the resulting numerical data has different
    mathematical properties from the distinct nominal categories. So, let's find out
    how this method works for ordinal data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next example, we naïvely apply the label encoder to the ratings dataset.
    The encoder is trained by iterating the training data in order to create the lookup
    dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 7.7* shows the result of the encoded ratings as `ratingId`, which is
    very similar to the previous example. However, in the case of ratings, the numerical
    properties of the ratings data are similar to the ordinal properties of the categorical
    ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Label-encoded ratings ](img/B17928_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Label-encoded ratings
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, let''s look at the lookup dictionary, in *Figure 7.8*, that the
    encoder learned from the input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – The lookup dictionary for ratings ](img/B17928_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – The lookup dictionary for ratings
  prefs: []
  type: TYPE_NORMAL
- en: 'Do you see something odd in the autogenerated lookup dictionary? Due to the
    order of the categorical values in the training data, we created a numeric list
    with the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This is probably not what we anticipated when applying a label encoder to an
    ordinal categorical value. The ordering we would be looking for is similar to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to create a label encoder with the right order, we can pass the ordered
    list of categorical values to the encoder. This would create a more meaningful
    encoding, as shown in *Figure 7.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Label-encoded ratings with custom order ](img/B17928_07_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Label-encoded ratings with custom order
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this in Python, we have to use pandas'' categorical ordinal variable,
    which is a special kind of label encoder that requires a list of ordered categories
    as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Under the hood, we implicitly created the following lookup dictionary for the
    encoder by passing the categories directly to it in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – A lookup dictionary for ratings with custom orders ](img/B17928_07_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – A lookup dictionary for ratings with custom orders
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding example, a label encoder can be quickly applied
    to any categorical data without much afterthought. The result of the label encoder
    is a single numerical feature and a categorical lookup table. Additionally, we
    can see, in the examples with topics and ratings, that label encoding is more
    suitable for ordinal data.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway is that the label encoder is great for encoding ordinal categorical
    data. You also learned that the order of elements matters, and so it is good practice
    to manually pass the categories to the encoder in the correct order.
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonal embedding using one-hot encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the second part of this section, we will take a look at the `N`, where `N`
    represents the number of unique values. This vector contains only zeros, except
    for one column that contains `1` and represents the column for this specific value.
    Here is a code snippet showing you how to apply the one-hot encoder to the `articles`
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is shown in *Figure 7.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – One-hot-encoded articles ](img/B17928_07_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – One-hot-encoded articles
  prefs: []
  type: TYPE_NORMAL
- en: 'The lookup dictionary for one-hot encoding has `N+1` columns, where `N` is
    the number of unique values in the encoded column. As we can see in the lookup
    dictionary in *Figure 7.12*, all N-dimensional vectors in the dictionary are orthogonal
    and of an equal length, `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – The lookup dictionary for articles ](img/B17928_07_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – The lookup dictionary for articles
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s compare this technique with ordinal data and apply one-hot encoding
    to the ratings table. The result is shown in *Figure 7.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – One-hot-encoded ratings ](img/B17928_07_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – One-hot-encoded ratings
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see that even if the original category values
    are ordinal, the encoded values can no longer be sorted, and so, this property
    is lost after the numeric encoding. Therefore, we can conclude that one-hot encoding
    is great for nominal categorical values where the number of unique values is small.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we've learned how to embed nominal and ordinal categorical values into
    numeric values by using a lookup dictionary and one-dimensional or N-dimensional
    numeric embedding. However, we discovered that it is somewhat limited in many
    aspects, such as the number of unique categories and capabilities to embed free
    text. In the following sections, we will learn how to extract words using a simple
    NLP pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Semantics and textual values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's worth taking the time to understand that a categorical value and a textual
    value are not the same. Although they might both be stored as a string and could
    have the same data type in your dataset, usually, a categorical value represents
    a finite set of categories, whereas a text value can hold any textual information.
  prefs: []
  type: TYPE_NORMAL
- en: So, why is this distinction important? Once you preprocess your categorical
    data and embed it into a numerical space, nominal categories will often be implemented
    as orthogonal vectors. You will not automatically be able to compute a distance
    from category A to category B or create a semantic meaning between the categories.
  prefs: []
  type: TYPE_NORMAL
- en: However, with textual data, usually, you start feature extraction with a different
    approach that assumes that you will find similar terms in the same text feature
    of your dataset samples. You can use this information to compute meaningful similarity
    scores between two textual columns; for example, to measure the number of words
    that are in common.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we recommend that you thoroughly check what kind of categorical values
    you have and how you are aiming to preprocess them. Also, a great exercise is
    to compute the similarity between two rows and see whether it matches your prediction.
    Let's take a look at a simple textual preprocessing approach using a dictionary-based
    bag-of-words embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Building a simple bag-of-words model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at a surprisingly simple concept to tackle the
    shortcomings of label encoding for textual data using a technique called bag-of-words,
    which will build a foundation for a simple NLP pipeline. Don't worry if these
    techniques look too simple when you read through them; we will gradually build
    on top of them with tweaks, optimizations, and improvements to build a modern
    NLP pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: A naïve bag-of-words model using counting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, the main concept that we will build is the bag-of-words model.
    It is a very simple concept; that is, it involves modeling any document as a collection
    of words that appear in a given document with the frequency of each word. Hence,
    we throw away sentence structure, word order, punctuation marks, and more and
    reduce the documents to a raw count of words. Following this, we can vectorize
    this word count into a numeric vector representation, which can then be used for
    ML, analysis, document comparisons, and much more. While this word count model
    sounds very simple, we will encounter quite a few language-specific obstacles
    along the way that we will need to resolve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started and define a sample document that we will transform throughout
    this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Applying a naïve word count to the document gives us our first (too simple)
    bag-of-words model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – A naïve bag-of-words model ](img/B17928_07_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – A naïve bag-of-words model
  prefs: []
  type: TYPE_NORMAL
- en: However, there are many problems with a naïve approach such as the preceding
    one. We have mixed different punctuation marks, notations, nouns, verbs, adverbs,
    and adjectives in different declinations, conjugations, tenses, and cases. Therefore,
    we have to build a pipeline to clean and normalize the data using NLP. In this
    section, we will build a pipeline with the following cleaning steps before feeding
    the data into a **count vectorizer** that, ultimately, counts the word occurrences
    and collects them in a feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization – turning a string into a list of words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step in building the pipeline is to separate a corpus into documents
    and a document into words. This process is called `nltk`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will output a list of tokens that contains words and punctuation
    marks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: When you execute the preceding code snippet, `nltk` will download the pretrained
    punctuation model in order to run the word tokenizer. The output of the tokenizer
    is the words and punctuation marks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we will remove the punctuation marks as they are not relevant
    for the subsequent *stemming* process. However, we will bring them back for *lemmatization*
    later in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will only contain the words of the original document without any
    punctuation marks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we used the `word.islanum()` function to only extract
    alphanumeric tokens and make them all lowercase. The preceding list of words already
    looks much better than the initial naïve model. However, it still contains a lot
    of unnecessary words, such as *the*, *we*, *had*, and more, which don't convey
    any information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to filter out the noise for a specific language, it makes sense to
    remove these words that often appear in texts and don''t add any semantic meaning
    to the text. It is common practice to remove these so-called `nltk` library in
    Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the resulting list only contains words that are not stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code gives us a nice pipeline where we end up with only the semantically
    meaningful words. We can take this list of words to the next step and apply a
    more sophisticated transformation/normalization to each word. If we applied the
    count vectorizer at this stage, we would end up with the simple bag-of-words model,
    as shown in *Figure 7.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – A simple bag-of-words model ](img/B17928_07_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – A simple bag-of-words model
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the previous figure, the list of terms that are included in
    the bag-of-words model is already far cleaner than the naïve example. This is
    because it doesn't contain any punctuation marks or stop words.
  prefs: []
  type: TYPE_NORMAL
- en: You might ask what qualifies a word as a stop word other than it occurring relatively
    often in a piece of text? Well, that's an excellent question! We can measure the
    importance of each word in the current context compared to its occurrences across
    the text using the **TF-IDF** method, which will be discussed in the *Measuring
    the importance of words using TF-IDF* section.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming – the rule-based removal of affixes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next step, we want to normalize affixes—word endings to create plurals
    and conjugations. You can see that with each step, we are diving deeper into the
    concept of a single language—in this case, English. However, when applying these
    steps to a different language, it's likely that completely different transformations
    will need to be used. This is what makes NLP such a difficult field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Removing the affixes of words to obtain the stem of a word is also called **stemming**.
    Stemming refers to a rule-based (heuristic) approach to transform each occurrence
    of a word into its word stem. Here is a simple example of some expected transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding example, such a heuristic approach for stemming
    has to be built specifically for each language. This is generally true for all
    other NLP algorithms as well. For the sake of brevity, in this book, we will only
    discuss English examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'A popular algorithm for stemming in English is Porter''s algorithm, which defines
    five sequential reduction rules, such as removing *-ed*, *-ing*, -*ate*, *-tion*,
    *-ence*, *-ance*, and more, from the end of words. The `nltk` library comes with
    an implementation of Porter''s stemming algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting list of words after stemming looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we simply apply `stemmer` to each word in the tokenized
    document. The bag-of-words model after this step is shown in *Figure 7.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – The bag-of-words model after stemming ](img/B17928_07_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – The bag-of-words model after stemming
  prefs: []
  type: TYPE_NORMAL
- en: While this algorithm works well with affixes, it can't avoid normalizing conjugations
    and tenses. This will be our next problem to tackle using lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization – dictionary-based word normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When looking at the stemming examples, we can already see the limitations of
    that approach. For example, what would happen with irregular verb conjugations—such
    as *are*, *am*, or *is*—that should all be normalized to the same word, *be*?
    This is exactly what lemmatization tries to solve using a pretrained set of vocabulary
    and conversion rules, called lemmas. The **lemmas** are stored in a lookup dictionary
    and look similar to the following transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'There is one very important point to make when discussing lemmatization. Each
    lemma needs to be applied to the correct word type, hence a lemma for nouns, verbs,
    adjectives, and more. The reason for this is that a word can be either a noun
    or a verb in the past tense. In our example, `ground` could come from the noun
    *ground* or the verb *grind*; `left` could be an adjective or the past tense of
    *leave*. So, we also need to extract the word type from the word in a sentence—this
    process is called `nltk` library has us covered once again. To estimate the correct
    POS tag, we also need to provide the punctuation mark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the resulting POS tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The POS tags describe the word type of each token in the document. You can
    find a complete list of tags using the `nltk.help.upenn_tagset()` command. Here
    is an example of how to do so from the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will print the list of POS tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The POS tags also include tenses for verbs and other very useful information.
    However, for the lemmatization in this section, we only need to know the word
    type—*noun*, *verb*, *adjective*, or *adverb*. One possible choice of lemmatizer
    is the WordNet lemmatizer in `nltk`. WordNet is a lexical database of English
    words that groups them into groups of concepts and word types.
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply the lemmatizer to the output of the stemming, we need to filter the
    POS tags by punctuation marks and stop words, similar to the previous preprocessing
    step. Then, we can use the word tags for the resulting words. Let''s apply the
    lemmatizer using `nltk`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The code outputs the lemmatized words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding list of words looks a lot cleaner than what we found in previous
    models. This is because we normalized the tenses of the verbs and transformed
    them into their infinitive form. The resulting bag-of-words model is shown in
    *Figure 7.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – The bag-of-words model after lemmatization ](img/B17928_07_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – The bag-of-words model after lemmatization
  prefs: []
  type: TYPE_NORMAL
- en: This technique is extremely helpful for cleaning up irregular forms of words
    in your dataset. However, it works based on rules—called lemmas—and, hence, it
    can only be used for languages and words where such lemmas are available.
  prefs: []
  type: TYPE_NORMAL
- en: A bag-of-words model in scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we can put all our previous steps together to create a state-of-the-art
    NLP preprocessing pipeline to normalize the input documents and run them through
    a count vectorizer so that we can transform them into a numeric feature vector.
    Doing so for multiple documents allows us to easily compare the semantics of the
    document in a numerical space. We could compute cosine similarities on the document's
    feature vectors to compute their similarity, plug them into a supervised classification
    method, or perform clustering on the resulting document concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, let''s take a look at the final pipeline for the simple bag-of-words
    model. I want to emphasize that this model is only the start of our journey in
    feature extraction using NLP. We performed the following steps for normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing punctuation marks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lemmatization with POS tagging
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the last step, we applied `CountVectorizer` in scikit-learn. This will count
    the occurrences of each word, create a global corpus of words, and output a sparse
    feature vector of word frequencies. Here is the sample code to pass the preprocessed
    data from `nltk` to `CountVectorizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The transformed bag-of-words model contains coordinates and counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The coordinates refer to the `(document id, term id)` pair, whereas the count
    refers to the term frequency. To better understand this output, we can also look
    at the internal vocabulary of the model. The `vocabulary_` parameter contains
    a lookup dictionary for the term ids:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The code outputs the model''s word dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we transform the preprocessed document back into a
    string before passing it to `CountVectorizer`. The reason for this is that `CountVectorizer`
    comes with some configurable preprocessing techniques out of the box, such as
    tokenization, stop word removal, and more. For this demonstration, we want to
    apply it to the preprocessed data. The output of the transformation is a sparse
    feature vector containing the term frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: Let's find out how we can combine multiple terms with semantic concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging term importance and semantics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everything we have done up to now has been relatively simple and based on word
    stems or so-called tokens. The bag-of-words model was nothing but a dictionary
    of tokens counting the occurrence of tokens per field. In this section, we will
    take a look at a common technique to further improve matching between documents
    using n-gram and skip-gram combinations of terms.
  prefs: []
  type: TYPE_NORMAL
- en: Combining terms in multiple ways will explode your dictionary. This will turn
    into a problem if you have a large corpus; for instance, 10 million words. Hence,
    we will look at a common preprocessing technique to reduce the dimensionality
    of a large dictionary through SVD.
  prefs: []
  type: TYPE_NORMAL
- en: While, now, this approach is a lot more complicated, it is still based on a
    bag-of-words model that already works well on a large corpus, in practice. However,
    of course, we can do better and try to understand the importance of words. Therefore,
    we will tackle another popular technique in NLP to compute the importance of terms.
  prefs: []
  type: TYPE_NORMAL
- en: Generalizing words using n-grams and skip-grams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous pipeline, we considered each word on its own without any context.
    However, as we all know, context matters a lot in language. Sometimes, words belong
    together and only make sense in context rather than on their own. To introduce
    this context into the same type of algorithm, we will introduce **n-grams** and
    **skip-grams**. Both techniques are heavily used in NLP for preprocessing datasets
    and extracting relevant features from text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with n-grams. An `N` consecutive entities (that is, characters,
    words, or tokens) of an input dataset. Here are some examples for computing the
    n-grams in a list of characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example using the built-in `ngram_range` parameter in scikit-learn''s
    `CountVectorizer` to generate multiple n-grams for the input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the vocabulary now contains both the 1-gram and 2-gram representations
    of each term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we can see that instead of the original words, we now
    have a combination of two consecutive words in our trained vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can extend the concept of n-grams to also allow the model to skip words.
    This a great option, if we for example want to perform a 2-gram, but in one of
    our samples there is an adjective in-between two words and in the other those
    words are directly next to each other. To achieve this, we need a method that
    allows us to define how many words it is allowed to skip to find matching words.
    Here is an example using the same characters as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Luckily, we find the generalized version of n-grams implemented in `nltk` as
    the `nltk.skipgrams` method. Setting the skip distance to `0` results in the traditional
    n-gram algorithm. We can apply it to our original dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the 2-gram example, the method produces a list of combinations of
    paired terms. However, in this case, we allowed one skip word to be present between
    those pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we can observe that skip-grams can generate a lot of
    additional useful feature dimensions for the NLP model. In real-world scenarios,
    both techniques are often used because the individual word order plays a big role
    in the semantics.
  prefs: []
  type: TYPE_NORMAL
- en: However, the explosion of new feature dimensions could be devastating if the
    input documents are, for example, all websites from the web or large documents.
    Therefore, we also need a way to avoid an explosion of the dimensions while capturing
    all of the semantics from the input data. We will tackle this challenge in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing word dictionary size using SVD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common problem with NLP is the vast number of words in a corpus and, hence,
    exploding dictionary sizes. In the previous example, we saw that the size of the
    dictionary defines the size of the orthogonal term vector. Therefore, a dictionary
    size of 20,000 terms would result in 20,000-dimensional feature vectors. Even
    without any n-gram enrichment, this feature vector dimension is too large to be
    processed on standard PCs.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need an algorithm to reduce the dimensions of the generated `CountVectorizer`
    while preserving the present information. Optimally, we would only remove redundant
    information from the input data and project it onto a lower-dimensional space
    while preserving all of the original information.
  prefs: []
  type: TYPE_NORMAL
- en: The PCA transformation would be a great fit for our solution and help us to
    transform the input data into lower linearly unrelated dimensions. However, computing
    the eigenvalues requires a symmetric matrix (the same number of rows and columns),
    which, in our case, we don't have. Hence, we can use the SVD algorithm, which
    generalizes the eigenvector computation to non-symmetric matrices. Due to its
    numeric stability, it is often used in NLP and information retrieval systems.
  prefs: []
  type: TYPE_NORMAL
- en: The usage of SVD in NLP applications is also called **Latent Semantic Analysis**
    (**LSA**), as the principal components can be interpreted as concepts in a latent
    feature space. The SVD embedding transforms the high-dimensional feature vector
    into a lower-dimensional concept space. Each dimension in the concept space is
    constructed by a linear combination of term vectors. By dropping the concepts
    with the smallest variance, we also reduce the dimensions of the resulting concept
    space to something that is a lot smaller and easier to handle. Typical concept
    spaces have 10s to 100s of dimensions, while word dictionaries usually have over
    100,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example using the `TruncatedSVD` implementation from `sklearn`.
    The SVD is implemented as a transformer class, and so, we need to call `fit_transform()`
    to fit a dictionary and transform it using the same step. The SVD is configured
    to only keep the components with the highest variance using the `n_components`
    argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we perform the LSA on the `X_train_counts` data and the
    output of `CountVectorizer` using SVD. We configure the SVD to only keep the first
    five components with the highest variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'By reducing the dimensionality of your dataset, you lose information. Thankfully,
    we can compute the amount of variance in the remaining dataset using the trained
    SVD object, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command outputs the variance as a number between 0 and 1, where
    1 means that the SVD transformation is an exact lossless mapping of the original
    data into the latent space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In this case, with only five components, the SVD retained 20% of the variance
    of the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the task, we usually aim to preserve more than 80–90% of the original
    variance after the latent transformation.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous code example, we computed the variance of the data that is preserved
    after the transformation to the configured number of components. Hence, we can
    now increase or reduce the number of components in order to keep a specific percentage
    of the information in the transformed data. This is a very helpful operation and
    is used in many practical NLP implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are still using the original word dictionary from the bag-of-words
    model. One particular downside of this model is that the more often a term occurs,
    the higher its count (and, therefore, weight) will get. This is a problem because,
    now, any term that is not a stop word and appears often in the text will receive
    a high weight—independent of the importance of the term within a certain document.
    Therefore, we introduce another extremely popular preprocessing technique—**TF-IDF**.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the importance of words using TF-IDF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One particular downside of the bag-of-words approach is that we simply count
    the absolute number of words in a context without checking whether the word generally
    appears frequently across all documents. A term that appears in every document
    might not be relevant for our model, as it contains less information and more
    often it appears across other documents. Hence, an important technique in text
    mining is to compute the importance of a certain word in a given context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, instead of an absolute count of terms in a context, we want to compute
    the number of terms in the context relative to a corpus of documents. By doing
    so, we will give higher weight to terms that appear only in a certain context,
    and reduce the amount of weight given to terms that appear in many different documents.
    This is exactly what the TF-IDF algorithm does. It is easy to compute a weight
    (*w*) for a term (*t*) in a document (*d*) according to the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_07_001.png)'
  prefs: []
  type: TYPE_IMG
- en: While the term frequency (*f*t) counts all of the terms in a document, the inverse
    document frequency is computed by dividing the total number of documents (*N*)
    by the counts of a term in all documents (*f*d). The *IDF* term is usually log-transformed,
    as the total count of a term across all documents can get quite large.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will not use the TF-IDF function directly. Instead,
    we will use `TfidfVectorizer`, which does the counting and then applies the TF-IDF
    function to the result in one step. Again, the function is implemented as a `sklearn`
    transformer, and hence, we call `fit_transform()` to train and transform the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is formatted in a similar manner to the earlier example containing
    `(document id, term id)` pairs and their TF-IDF values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we apply `TfidfVectorizer` directly, which returns the
    same result as using `CountVectorizer` and `TfidfTransformer` combined. We transform
    a dataset containing the words of the bag-of-words model and return the TF-IDF
    values. We can also return the terms for each TF-IDF value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code returns the vocabulary of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we can see that `ground` gets a TF-IDF value of `0.667`, whereas
    all the other terms receive a value of `0.333`. This count will now scale relatively
    when more documents are added to the corpus—hence, if the word `hold` were to
    be included again, the TF-IDF value would decrease.
  prefs: []
  type: TYPE_NORMAL
- en: In any real-world pipeline, we would always use all the techniques presented
    in this chapter—tokenization, stop word removal, stemming, lemmatization, n-grams/skip-grams,
    TF-IDF, and SVD—combined in a single pipeline. The result would be a numeric representation
    of n-grams/skip-grams of tokens weighted by importance and transformed into a
    latent semantic space. Using these techniques for your first NLP pipeline will
    get you quite far, as you can now capture a lot of information from your textual
    data.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned how to numerically encode many kinds of categorical
    and textual values by using either one-dimensional or N-dimensional labels or
    counting and weighting word stems and character combinations. While many of these
    methods work well in many situations where you require simple numeric embedding,
    they all have a serious limitation—they don't encode semantics. Let's take a look
    at how we can extract the semantic meaning of text in the same pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting semantics using word embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When computing the similarity of news, you would imagine that topics such as
    tennis, *Formula 1*, or *soccer* would be semantically more similar to each other
    than topics such as politics, economics, or science. Yet, in terms of the previously
    discussed techniques, all encoded categories are seen as semantically the same.
    In this section, we will discuss a simple method of semantic embedding, which
    is also called **word embedding**.
  prefs: []
  type: TYPE_NORMAL
- en: The previously discussed pipeline using LSA transforms multiple documents into
    terms and then transforms those terms into semantic concepts that can be compared
    with other documents. However, the semantic meaning is based on the term occurrences
    and importance—there is no measurement of semantics between individual terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, what we are looking for is an embedding of terms into numerical multidimensional
    space such that each word represents one point in this space. This allows us to
    compute a numerical distance between multiple words in this space in order to
    compare the semantic meaning of two words. The most interesting benefit of word
    embeddings is that algebra on the word embeddings is not only numerically possible
    but also makes sense. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We can create such an embedding by mapping a corpus of words on an N-dimensional
    numeric space and optimizing the numeric distance based on the word semantics—for
    example, based on the distance between words in a corpus. The resulting optimization
    outputs a dictionary of words in the corpus and their numeric N-dimensional representation.
    In this numeric space, words have the same, or at least similar, properties as
    in the semantic space. A great benefit is that these embeddings can be trained
    unsupervised, so no training data has to be labeled.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the first embeddings is called **Word2Vec** and is based on a continuous
    bag-of-words model or a continuous skip-gram model to count and measure the words
    in a window. Let''s try this functionality and perform a semantic word embedding
    using Word2Vec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The best Python implementation for word embeddings is **Gensim**, which we
    will also use here. We need to feed our tokens into the model in order to train
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we load the `Word2Vec` model and initialize it with the
    list of tokens from the previous sections, which is stored in the `words` variable.
    The `size` attribute defines the dimension of the resulting vectors, and the `window`
    parameter decides how many words we should consider per window. Once the model
    has been trained, we can simply look up the word embedding in the model's dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: The code will automatically train the embedding on the set of tokens we provided.
    The resulting model stores the word-to-vector mapping in the `wv` property. Optimally,
    we also use a large corpus or pretrained model that is either provided by `gensim`
    or another NLP library, such as `NLTK`, to train the embedding and fine-tune it
    with a smaller dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can use the trained model to embed all the terms from our document
    using the Word2Vec embedding. However, this will result in multiple vectors as
    each word returns its own embedding. Therefore, you need to combine all the vectors
    into a single vector using the mathematical mean of all the embeddings. This procedure
    is quite similar to the one used to generate a concept in LSA. Also, other reduction
    techniques are possible; for example, weighing the individual embedding vectors
    using their TF-IDF values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding function, we compute the mean from all the word embedding vectors
    of the terms—this is called a **mean embedding**, and it represents the concept
    of this document in the embedding space. If a word is not found in the embedding,
    we need to replace it with zeros in the computation.
  prefs: []
  type: TYPE_NORMAL
- en: You can use such a semantic embedding for your application by downloading a
    pretrained embedding, for example, on the Wikipedia corpus. Then, you can loop
    through your sanitized input tokens and look up the words in the dictionary of
    the numeric embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 'GloVe is another popular technique for encoding words as numerical vectors,
    developed by Stanford University. In contrast to the continuous window-based approach,
    it uses global word-to-word co-occurrence statistics to determine the linear relationships
    between words:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the pretrained 6 B tokens embedding trained on Wikipedia
    and the Gigaword news archive:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we only open and parse the pretrained word embedding
    in order to store the word and vectors in a lookup dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we use the dictionary to look up tokens in our training data and merge
    them by computing the mean of all GloVe vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code works very similar to before and returns one vector per word,
    which is aggregated by taking their mean at the end. Again, this corresponds with
    a semantic concept using all the tokens of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Gensim provides other popular models for semantic embeddings, such as *doc2word*,
    *fastText*, and *GloVe*. The `gensim` Python library is a great place for utilizing
    these pretrained embeddings or for training your own models. Now you can replace
    your bag-of-words model with a mean embedding of the word vectors to also capture
    word semantics. However, your pipeline will still be built out of many tunable
    components.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will take a look at building end-to-end state-of-the-art
    language models and reusing some of the language features from Azure Cognitive
    Services.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing end-to-end language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we trained and concatenated multiple pieces to implement
    a final algorithm where most of the individual steps need to be trained as well.
    Lemmatization contains a dictionary of conversion rules. Stop words are stored
    in the dictionary. Stemming needs rules for each language and word that the embedding
    needs to train—TF-IDF and SVD are only computed on your training data but are
    independent of each other.
  prefs: []
  type: TYPE_NORMAL
- en: This is a similar problem to the traditional computer vision approach, which
    we will discuss in more depth in [*Chapter 10*](B17928_10_ePub.xhtml#_idTextAnchor165),
    *Training Deep Neural Networks on Azure*, where many classic algorithms are combined
    into a pipeline of feature extractors and classifiers. Similar to breakthroughs
    of end-to-end models trained via gradient descent and backpropagation in computer
    vision, deep neural networks—especially sequence-to-sequence models—have replaced
    the classical approach of performing each step of the transformation and training
    process manually.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, first, we will take a look at improving our previous model
    using custom embedding and an LSTM implementation to model a token sequence. This
    will give you a good understanding of how we are moving from an individual preprocessor-based
    pipeline to a full end-to-end approach using deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence models are models based on encoders and decoders that are
    trained on a variable set of inputs. This encoder/decoder architecture is used
    for a variety of tasks, such as machine translation, image captioning, and summarization.
    A nice benefit of these models is that you can reuse the encoder part of this
    network to convert a set of inputs into a fixed-set numerical representation of
    the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the state-of-the-art language representation models and
    discuss how they can be used for feature engineering and the preprocessing of
    your text data. We will use BERT to perform sentiment analysis and numeric embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will also look at reusing the Azure Cognitive Services APIs for
    text analytics to carry out advanced modeling and feature extraction, such as
    text or sentence sentiment, keywords, or entity recognition. This is a nice approach
    because you can leverage the know-how and amount of training data from Microsoft
    to perform complex text analytics using a simple HTTP request.
  prefs: []
  type: TYPE_NORMAL
- en: The end-to-end learning of token sequences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of concatenating different pieces of algorithms into a single pipeline,
    we want to build and train an end-to-end model that can train the word embedding,
    pre-form latent semantic transformation, and capture sequential information in
    the text in a single model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefit of such a model is that each processing step can be fine-tuned
    for the user''s prediction task in a single combined optimization process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of the pipeline will look extremely similar to the previous
    sections. We will build a tokenizer that converts documents into sequences of
    tokens that are then transformed into a numerical model based on the token sequence.
    Then, we will use `pad_sequences` to align all of the documents to the same length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the next step, we will build a simple model using Keras, an embedding layer,
    and an LSTM layer to capture token sequences. The embedding layer will perform
    a similar operation to GloVe, where the words will be embedded into a semantic
    space. The LSTM cell will ensure that we are comparing sequences of words instead
    of single words at a time. Then, we will use a dense layer with a *softmax* activation
    to implement a classifier head:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see in the preceding function, we build a simple neural network using
    three layers (that is, `Embedding`, `LSTM`, and `Dense`) and a `softmax` activation
    for classification. This means that in order to train this model, we would also
    need a classification problem to be solved at the same time. Hence, we do need
    labeled training data to perform analysis using this approach. In the next section,
    we will examine how sequence-to-sequence models are used in input-output text
    sequences to learn an implicit text representation.
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-art sequence-to-sequence models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, another type of model has replaced the traditional NLP pipelines—transformer-based
    models. These types of models are fully end-to-end and use sequence-to-sequence
    mapping, positional encoding, and multi-head attention layers. This allows the
    models to look forward and backward in a text, pay attention to specific patterns,
    and learn tasks fully end to end. As you might be able to tell, these models have
    complex architectures and usually have well over 100 million or over 1 billion
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence models are now state of the art for many complex end-to-end
    NLP problems such as classification (for example, sentiment or text analysis),
    language understanding (for example, entity recognition), translation, text generation,
    summarization, and more.
  prefs: []
  type: TYPE_NORMAL
- en: One popular sequence-to-sequence model is BERT, which, today, exists in many
    different variations and configurations. Models based on the BERT architecture
    seem to perform particularly well but have already been outperformed by newer
    updated architectures, tuned parameters, or models with more training data.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to get started using these new NLP models is with the *Hugging
    Face* `transformers` library, which provides end-to-end models (or pipelines)
    along with pretrained tokenizers and models. The `transformers` library implements
    all model architectures for both *TensorFlow* and *PyTorch*. The models can be
    easily consumed and used in an application, trained from scratch, or fine-tuned
    using domain-specific custom training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows how to implement sentiment analysis using the default
    `sentiment-analysis` pipeline, which, at the time of writing, uses the `TFDistilBertForSequenceClassification`
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the previous example, it's very simple to use a pretrained
    model for an end-to-end prediction task. These three lines of code can easily
    be integrated into your feature extraction pipeline to enrich your training data
    with sentiments.
  prefs: []
  type: TYPE_NORMAL
- en: Besides end-to-end models, another popular application of NLP is to provide
    semantic embeddings for textual data during preprocessing. This can also be implemented
    using the `transformers` library and any of the many supported models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, first, we initialize a pretrained tokenizer for BERT. This will
    help us to split the input data into the correct format for the BERT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have transformed the input into a token sequence, we can evaluate the
    BERT model. To retrieve the numerical embedding, we need to understand the latent
    state of the encoder, which we can retrieve using the `last_hidden_state` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The last hidden layer contains the latent representation of the model, which
    we can now use as a semantic numerical representation in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The key takeaway from these models is that they use an encoder/decoder-based
    architecture, which allows us to simply borrow the encoder to embed text into
    a semantic numerical feature space. Hence, a common approach is to download the
    pretrained model and perform a forward pass through the encoder part of the network.
    The fixed-sized numerical output can now be used as a feature vector for any other
    model. This is a common preprocessing step and a good trade-off for using a state-of-the-art
    language model for numerical embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Text analytics using Azure Cognitive Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A good approach in many engineering disciplines is to not reinvent the wheel
    when many other companies have already solved the same problem far better than
    you will ever be able to solve it. This might be the case for basic text analytics
    and text understanding tasks that Microsoft has developed, implemented, and trained
    and now offers as a service.
  prefs: []
  type: TYPE_NORMAL
- en: What if I told you that when working with Azure, text understanding features
    such as sentiment analysis, key phrase extraction, language detection, named entity
    recognition, and the extraction of **Personally Identifiable Information** (**PII**)
    is just one request away? Azure provides the Text Analytics API as part of Cognitive
    Services, which will solve all of these problems for you.
  prefs: []
  type: TYPE_NORMAL
- en: This won't solve the need to transform a piece of text into numerical values,
    but it will make it easier to extract semantics from your text. One example would
    be to perform a key phrase extraction or sentiment analysis using Cognitive Services
    as an additional feature engineering step, instead of implementing your own NLP
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement a function that returns the sentiment for a given document
    using the Text Analytics API of Cognitive Services. This is great when you want
    to enrich your data with additional attributes, such as overall sentiment, in
    the text. Let''s start by setting up all the parameters we will need to call the
    Cognitive Services API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the content and metadata of the request. We create a `payload`
    object that contains a single document and the text we want to analyze:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to send the payload, heads, and parameters to the Cognitive
    Services API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code looks very similar to the computer vision example that we
    saw in [*Chapter 2*](B17928_02_ePub.xhtml#_idTextAnchor034), *Choosing the Right
    Machine Learning Service in Azure*. In fact, it uses the same API but just a different
    endpoint for Text Analytics and, in this case, sentiment analysis functionality.
    Let''s run this code and look at the output, which looks very similar to the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We can observe that the JSON response contains a sentiment classification for
    each document (`positive`, `neutral`, and `negative`) as well as numeric confidence
    scores for each class. Also, you can see that the resulting documents are stored
    in an array and marked with an `id` value. Hence, you can send multiple documents
    to this API using an ID to identify each document.
  prefs: []
  type: TYPE_NORMAL
- en: Using custom pretrained language models is great, but for standardized text
    analytics, we can simply reuse Cognitive Services. Microsoft has invested tons
    of resources into the research and production of these language models, which
    you can use for your own data pipelines for a relatively small amount of money.
    Therefore, if you prefer using a managed service instead of running your customer
    transformer model, you should try this Text Analytics API.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to preprocess textual and categorical nominal
    and ordinal data using state-of-the-art NLP techniques.
  prefs: []
  type: TYPE_NORMAL
- en: You can now build a classical NLP pipeline with stop word removal, *lemmatization*
    and *stemming*, *n-grams*, and count term occurrences using a *bag-of-words* model.
    We used *SVD* to reduce the dimensionality of the resulting feature vector and
    to generate lower-dimensional topic encoding. One important tweak to the count-based
    bag-of-words model is to compare the relative term frequencies of a document.
    You learned about the *TF-IDF* function and can use it to compute the importance
    of a word in a document compared to the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we looked at *Word2Vec* and *GloVe*, which are pretrained
    dictionaries of numeric word embeddings. Now you can easily reuse a pretrained
    word embedding for commercial NLP applications with great improvements and accuracy
    due to the semantic embedding of words.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we finished the chapter by looking at a state-of-the-art approach to
    language modeling, using end-to-end language representations, such as *BERT* and
    BERT-based architectures, which are trained as sequence-to-sequence models. The
    benefit of these models is that you can reuse the encoder to transform a sequence
    of text into a numerical representation, which is a very common task during feature
    extraction.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how to train an ML model using Azure Machine
    Learning, applying everything we have learned so far.
  prefs: []
  type: TYPE_NORMAL
