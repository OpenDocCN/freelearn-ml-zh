- en: <st c="0">1</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Imputing Missing Data</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="23">Missing data—meaning the absence</st> <st c="56">of values for certain
    observations—is an unavoidable problem in most data sources.</st> <st c="140">Some
    machine learning model implementations can handle missing data out of the box.</st>
    <st c="224">To train other models, we must remove observations with missing data
    or transform them into</st> <st c="316">permitted values.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="333">The act of replacing missing data</st> <st c="367">with their statistical
    estimates is called</st> **<st c="411">imputation</st>**<st c="421">. The goa</st><st
    c="430">l of any imputation technique is to produce a complete dataset.</st> <st
    c="495">There are multiple imputation methods.</st> <st c="534">We select which
    one to use, depending on whether the data is missing at random, the proportion
    of missing values, and the machine learning model we intend to use.</st> <st c="697">In
    this chapter, we will discuss several</st> <st c="738">imputation methods.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="757">This chapter will cover the</st> <st c="786">following recipes:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="804">Removing observations with</st> <st c="832">missing data</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="844">Performing mean or</st> <st c="864">median imputation</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="881">Imputing</st> <st c="891">categorical variables</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="912">Replacing missing values with an</st> <st c="946">arbitrary number</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="962">Finding extreme values</st> <st c="986">for imputation</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1000">Marking</st> <st c="1009">imputed values</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1023">Implementing forward and</st> <st c="1049">backward fill</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1062">Carrying</st> <st c="1072">out interpolation</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1089">Performing multivariate imputation by</st> <st c="1128">chained
    equations</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1145">Estimating missing data with</st> <st c="1175">nearest neighbo</st><st
    c="1190">r</st><st c="1192">s</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1193">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1215">In this chapter, we will use the Python libraries Matplotlib, pandas,
    NumPy, scikit-learn, and Feature-engine.</st> <st c="1327">If you need to install
    Python, the free Anaconda Python distribution (</st>[<st c="1397">https://www.anaconda.com/</st>](https://www.anaconda.com/)<st
    c="1423">) includes most numerical</st> <st c="1450">computing libraries.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="1470">feature-engine</st>` <st c="1485">can be installed with</st>
    `<st c="1508">pip</st>` <st c="1511">as follows:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="1550">If you use Anaconda, you can install</st> `<st c="1588">feature-engine</st>`
    <st c="1602">with</st> `<st c="1608">conda</st>`<st c="1613">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="1659">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1664">The recipes from this chapter were created using the latest versions
    of the Python libraries at the time of publishing.</st> <st c="1785">You can check
    the versions in the</st> `<st c="1819">requirements.txt</st>` <st c="1835">file
    in the accompanying GitHub repository,</st> <st c="1880">at</st> [<st c="1883">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/requirements.txt</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/requirements.txt)<st
    c="1994">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1995">We will use the</st> **<st c="2012">Credit Approval</st>** <st
    c="2027">dataset from the</st> *<st c="2045">UCI Machine Learning Repository</st>*
    <st c="2076">(</st>[<st c="2078">https://archive.ics.uci.edu/</st>](https://archive.ics.uci.edu/)<st
    c="2106">), licensed under the CC BY 4.0 creative commons attribution:</st> [<st
    c="2169">https://creativecommons.org/licenses/by/4.0/legalcode</st>](https://creativecommons.org/licenses/by/4.0/legalcode)<st
    c="2222">. You’ll find the dataset at this</st> <st c="2256">link:</st> [<st c="2262">http://archive.ics.uci.edu/dataset/27/credit+approval</st>](http://archive.ics.uci.edu/dataset/27/credit+approval)<st
    c="2315">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2316">I downloaded and modified the data as shown in this</st> <st c="2369">notebook:</st>
    [<st c="2379">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputation/credit-approval-dataset.ipynb</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputation/credit-approval-dataset.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2532">We will also use the</st> **<st c="2554">air passenger</st>** <st
    c="2567">dataset located in Facebook’s Prophet GitHub repository (</st>[<st c="2625">https://github.com/facebook/prophet/blob/main/examples/example_air_passengers.csv</st>](https://github.com/facebook/prophet/blob/main/examples/example_air_passengers.csv)<st
    c="2707">), licensed under the MIT</st> <st c="2734">license:</st> [<st c="2743">https://github.com/facebook/prophet/blob/main/LICENSE</st>](https://github.com/facebook/prophet/blob/main/LICENSE)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2796">I modified the data as shown in this</st> <st c="2834">notebook:</st>
    [<st c="2844">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputation/air-passengers-dataset.ipynb</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputation/air-passengers-dataset.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2996">You’ll find a copy of the modified data sets in the accompanying
    GitHub</st> <st c="3069">repository:</st> [<st c="3081">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputatio</st><st
    c="3203">n</st><st c="3205">/</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputation/)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3206">Removing observations with missing data</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**<st c="3245">Complete Case Analysis</st>** <st c="3268">(</st>**<st c="3270">CCA</st>**<st
    c="3273">), als</st><st c="3280">o called list-wis</st><st c="3298">e deletion
    of cases, consists</st> <st c="3328">of discarding</st> <st c="3342">observations</st>
    <st c="3355">with missing data.</st> <st c="3375">CCA can be applied to both categorical</st>
    <st c="3413">and numerical variables.</st> <st c="3439">With CCA, we pre</st><st
    c="3455">serve the distribution o</st><st c="3480">f the variables after the imputation,
    provided the data is missing at random and only in a small proportion of observations.</st>
    <st c="3606">However, if data is missing across many variables, CCA may lead to
    the removal of a large portion of</st> <st c="3707">the dataset.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3719">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3724">Use CCA only when a small number of observations are missing and
    you have good reasons to believe that they are not important to</st> <st c="3854">your
    m</st><st c="3860">odel.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3866">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="3882">Let’s begin by making some imports and loading</st> <st c="3930">the
    dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3942">Let’s import</st> `<st c="3956">pandas</st>`<st c="3962">,</st>
    `<st c="3964">matplotlib</st>`<st c="3974">, and the train/test split function</st>
    <st c="4010">from scikit-learn:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="4133">Let’s load and display the dataset described in the</st> *<st c="4186">Technical</st>*
    *<st c="4196">requirements</st>* <st c="4208">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="4275">In the following image, we see the first 5 rows</st> <st c="4324">of
    data:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.1 – First 5 rows of the dataset](img/B22396_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="4633">Figure 1.1 – First 5 rows of the dataset</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4673">Let’s proceed as we normally</st> <st c="4702">would if we were
    preparing</st> <st c="4729">the data to train machine learning models; by splitting
    the data into a training and a</st> <st c="4817">test set:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="4959">Let’s now make a bar plot with the proportion of missing data per
    variable in the training and</st> <st c="5055">te</st><st c="5057">st sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5395">The previous code block returns</st> <st c="5427">the following
    bar plots</st> <st c="5451">with the fraction of missing data per variable in
    the training (top) and test</st> <st c="5530">sets (b</st><st c="5537">ottom):</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Proportion of missing data per variable](img/B22396_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="5717">Figure 1.2 – Proportion of missing data per variable</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5769">Now</st><st c="5773">, we’ll remove obser</st><st c="5793">vations
    if they have missing values in</st> <st c="5833">any variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5902">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5907">pandas’</st> `<st c="5916">dropna()</st>`<st c="5924">drops observations
    with any missing value by default.</st> <st c="5979">We can remove observations
    with missing data in a subset of variables like this:</st> `<st c="6060">data.dropna(subset=["A3",
    "A4"])</st>`<st c="6092">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6093">Let’s print and compare the size of the original and complete</st>
    <st c="6156">case datasets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6268">We removed more than 200 observations with missing data from the
    training set, as shown in the</st> <st c="6364">following output:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6435">After removing</st> <st c="6450">observations from the training</st>
    <st c="6481">and test sets, we need to align the</st> <st c="6518">target variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6618">Now, the datasets and target variables contain the rows without</st>
    <st c="6683">missing data.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="6696">To drop observations with missing data utilizing</st> `<st c="6746">feature-engine</st>`<st
    c="6760">, let’s import the</st> <st c="6779">required transformer:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6854">Let</st><st c="6858">’s set up the imputer to auto</st><st c="6888">matically
    find the variables with</st> <st c="6923">missing data:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6993">Let’s fit the transformer so that it finds the variables with</st>
    <st c="7056">missing data:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7086">Let’s inspect the variables with NAN that the</st> <st c="7133">transformer
    found:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7166">The previous command returns the names of the variables with</st>
    <st c="7228">missing data:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7310">Let’s remove the rows with missing data in the training and</st>
    <st c="7371">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7449">Use</st> `<st c="7454">train_cca.isnull().sum()</st>` <st c="7478">to
    corroborate</st> <st c="7493">the absence of missing data in the complete</st>
    <st c="7537">case dataset.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`<st c="7551">DropMissingData</st>` <st c="7567">can automatically adjust the
    target after removing missing data from the</st> <st c="7641">training set:</st>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7765">The previous code removed rows with</st> `<st c="7802">nan</st>`
    <st c="7805">from the training and test sets and then re-aligned the</st> <st
    c="7862">target variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7879">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7884">To remove observations with missing data in a subset of variables,
    use</st> `<st c="7956">DropMissingData(variables=['A3', 'A4'])</st>`<st c="7995">.
    To remove rows with</st> `<st c="8017">nan</st>` <st c="8020">in at least 5% of
    the variables,</st> <st c="8054">use</st> `<st c="8058">DropMissingData(thres</st><st
    c="8079">hold=0.95)</st>`<st c="8090">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8091">How it works</st><st c="8104">...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="8107">In this recipe,</st> <st c="8123">we plotted</st> <st c="8134">the
    proportion of missing data</st> <st c="8165">in each variable and then removed
    all observations with</st> <st c="8222">missing values.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8237">We used</st> `<st c="8246">pandas</st>` `<st c="8252">isnull()</st>`
    <st c="8261">and</st> `<st c="8266">mean()</st>` <st c="8272">methods to determine
    the proportion of missing observations in each variable.</st> <st c="8351">The</st>
    `<st c="8355">isnull()</st>` <st c="8363">method created a Boolean vector per
    variable with</st> `<st c="8414">True</st>` <st c="8418">and</st> `<st c="8423">False</st>`
    <st c="8428">values indicating whether a value was missing.</st> <st c="8476">The</st>
    `<st c="8480">mean()</st>` <st c="8486">method took the average of these values
    and returned the proportion of</st> <st c="8558">missing data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8571">We used</st> `<st c="8580">pandas</st>` `<st c="8586">plot.bar()</st>`
    <st c="8597">to create a bar plot of the fraction of missing data per variable.</st>
    <st c="8665">In</st> *<st c="8668">Figure 1</st>**<st c="8676">.2</st>*<st c="8678">,
    we saw the fraction of</st> `<st c="8703">nan</st>` <st c="8706">per variable
    in the training and</st> <st c="8740">test sets.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8750">To remove observations with missing values in</st> *<st c="8797">any</st>*
    <st c="8800">variable, we used pandas’</st> `<st c="8827">dropna()</st>`<st c="8835">,
    thereby obtaining a complete</st> <st c="8866">case dataset.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8879">Finally, we removed missing data using Feature-engine’s</st> `<st
    c="8936">DropMissingData()</st>`<st c="8953">. This imputer automatically identified
    and stored the variables with missing data from the train set when we called the</st>
    `<st c="9074">fit()</st>` <st c="9079">method.</st> <st c="9088">With the</st>
    `<st c="9097">transform()</st>` <st c="9108">method, the imputer removed observations
    with</st> `<st c="9154">nan</st>` <st c="9158">in those variables.</st> <st c="9179">With</st>
    `<st c="9184">transform_x_y()</st>`<st c="9199">, the imputer removed rows with</st>
    `<st c="9231">nan</st>` <st c="9234">from the data sets and then realigned the</st>
    <st c="9277">target variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9293">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="9302">If you want to use</st> `<st c="9322">DropMissingData()</st>` <st
    c="9339">within a pipeline together with other Feature-engine or scikit-learn
    transformers, check</st> <st c="9428">out Feature-engine’s</st> `<st c="9450">Pipeline</st>`<st
    c="9458">:</st> [<st c="9461">https://Feature-engine.trainindata.com/en/latest/user_guide/pipeline/Pipeline.html</st>](https://Feature-engine.trainindata.com/en/latest/user_guide/pipeline/Pipeline.html)<st
    c="9543">. This pipeline can align the target with the training and test sets
    after</st> <st c="9618">removing rows.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9632">Performing mean or median impu</st><st c="9663">tation</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="9670">Mean or median imputation</st> <st c="9696">consists of replaci</st><st
    c="9716">ng missing data</st> <st c="9732">with the variable’s mean or median
    value.</st> <st c="9775">To avoid data leakage, we determine the mean or median
    using the train set, and then use these values to impute the train and test sets,
    and all</st> <st c="9920">future data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9932">Scikit-learn and Feature-engine learn the mean or median from the
    train set and store these parameters for future use out of</st> <st c="10058">the
    box.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10066">In this recipe, we will perform mean and median imputation using</st>
    `<st c="10132">pandas</st>`<st c="10138">,</st> `<st c="10140">scikit</st>`<st
    c="10146">-</st>`<st c="10148">learn</st>`<st c="10153">,</st> <st c="10155">and</st>
    `<st c="10159">feature-engine</st>`<st c="10173">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10174">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10179">Use mean imputation if variables are normally distributed and
    median imputation otherwise.</st> <st c="10271">Mean and median imputation may
    distort the variable distribution if there is a high percentage</st> <st c="10366">of</st>
    <st c="10369">missing data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10382">How t</st><st c="10388">o do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="10399">Let’s begin</st> <st c="10412">this reci</st><st c="10421">pe:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10425">First, we’ll import</st> `<st c="10446">pandas</st>` <st c="10452">and
    the required functions and classes from</st> `<st c="10496">scikit-learn</st>`
    <st c="10509">and</st> `<st c="10514">feature-engine</st>`<st c="10528">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="10746">Let’s load the dataset that we prepared in the</st> *<st c="10794">Technical</st>*
    *<st c="10804">requirements</st>* <st c="10816">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="10871">Let’s split the data into train and test sets with their</st>
    <st c="10929">respective targets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11080">Let’s make a list with</st> <st c="11103">the numerical variables
    by excluding</st> <st c="11140">variables of</st> <st c="11154">type object:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11235">If you execute</st> `<st c="11251">numeric_vars</st>`<st c="11263">,
    you will see the names of the numerical variables:</st> `<st c="11316">['A2',
    'A3', 'A8', 'A11', '</st>``<st c="11343">A14', 'A15']</st>`<st c="11356">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="11357">Let’s capture the variables’ median values in</st> <st c="11404">a
    dictionary:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11475">Tip</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11479">Note how we calculate the median using the train set.</st> <st
    c="11534">We will use these values to replace missing data in the train and test
    sets.</st> <st c="11611">To calculate the mean, use pandas</st> `<st c="11645">mean()</st>`
    <st c="11651">instead</st> <st c="11660">of</st> `<st c="11663">median()</st>`<st
    c="11671">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="11672">If you execute</st> `<st c="11688">median_values</st>`<st c="11701">,
    you will see a dictionary with the median value per variable:</st> `<st c="11765">{''A2'':
    28.835, ''A3'': 2.75, ''A8'': 1.0, ''A11'': 0.0, ''A14'': 160.0, ''</st>``<st
    c="11830">A15'': 6.0}.</st>`'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11842">Let’s replace missing data with</st> <st c="11875">the median:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11980">If you execute</st> `<st c="11996">X_train_t[numeric_vars].isnull().sum()</st>`
    <st c="12034">after the imputation, the number of missing values in the numerical
    variables should</st> <st c="12120">be</st> `<st c="12123">0</st>`<st c="12124">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="12125">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="12130">pandas</st>` `<st c="12137">fillna()</st>` <st c="12146">returns
    a new dataset with imputed values by default.</st> <st c="12201">To replace missing
    data in the original DataFrame, set the</st> `<st c="12260">inplace</st>` <st
    c="12267">parameter to</st> `<st c="12281">True</st>`<st c="12285">:</st> `<st
    c="12288">X_train.fillna(value=median_values, inplace</st><st c="12331">=True)</st>`<st
    c="12338">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12339">Now, let’s impute missing valu</st><st c="12370">es with the median</st>
    <st c="12390">using</st> `<st c="12396">scikit-learn</st>`<st c="12408">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12409">Let’s set up the imputer</st> <st c="12434">to replace missing
    data with</st> <st c="12463">the median:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12518">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12523">To perform mean imputation, set</st> `<st c="12556">SimpleImputer()</st>`
    <st c="12571">as follows:</st> `<st c="12584">imputer =</st>` `<st c="12593">SimpleImputer(strategy
    = "</st>``<st c="12620">mean")</st>`<st c="12627">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12628">We restrict the imputation to the numerical variables by</st>
    <st c="12686">using</st> `<st c="12692">ColumnTransformer()</st>`<st c="12711">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12865">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12870">Scikit-learn can return</st> `<st c="12895">numpy</st>` <st c="12900">arrays,</st>
    `<st c="12909">pandas</st>` <st c="12915">DataFrames, or</st> `<st c="12931">polar</st>`
    <st c="12936">frames, depending on how we set out the transform output.</st> <st
    c="12995">By default, it returns</st> `<st c="13018">numpy</st>` <st c="13023">arrays.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13031">Let’s fit the imputer</st> <st c="13053">to the train set so that
    it learns the</st> <st c="13093">median</st> <st c="13099">values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13124">Let’s check out the learned</st> <st c="13153">median values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13211">The previous command returns the median values</st> <st c="13259">per
    variable:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13313">Let’s replace missing values with</st> <st c="13348">the median:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13425">Let’s display the resulting</st> <st c="13454">training set:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13491">We see the resulting DataFrame in the</st> <st c="13530">following
    image:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Training set after the imputation. The imputed variables are
    marked by the imputer prefix; the untransformed variables show the prefix remainder](img/B22396_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="14090">Figure 1.3 – Training set after the imputation.</st> <st c="14138">The
    imputed variables are marked by the imputer prefix; the untransformed variables
    show the prefix remainder</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14247">Finally, let’s perform median imputation</st> <st c="14289">using</st>
    `<st c="14295">feature-engine</st>`<st c="14309">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14310">Let’s set up the imputer</st> <st c="14335">to replace missing
    data in numerical variables</st> <st c="14382">with</st> <st c="14388">the median:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14482">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14487">To perform mean imputation, change</st> `<st c="14523">imputation_method</st>`
    <st c="14540">to</st> `<st c="14544">"mean"</st>`<st c="14550">. By default</st>
    `<st c="14563">MeanMedianImputer()</st>` <st c="14582">will impute all numerical
    variables in the DataFrame, ignoring categorical variables.</st> <st c="14669">Use
    the</st> `<st c="14677">variables</st>` <st c="14686">argument to restrict the
    imputation to a subset of</st> <st c="14738">numer</st><st c="14743">ical variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14759">Fit the imputer so th</st><st c="14781">at it learns the</st>
    <st c="14799">median values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14834">Inspect the</st> <st c="14847">learned medians:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14885">The previous command returns the median values in</st> <st c="14936">a
    dictionary:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="15025">Finally, let’s replace the missing values with</st> <st c="15073">the
    median:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="15156">Feature-engine’s</st> `<st c="15174">MeanMedianImputer()</st>`
    <st c="15193">returns a</st> `<st c="15204">DataFrame</st>`<st c="15213">. You
    can check that the imputed variables do not contain missing values</st> <st c="15286">using</st>
    `<st c="15292">X_train[numeric</st><st c="15307">_vars].isnull().mean()</st>`<st
    c="15330">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15331">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="15347">In this recipe, we replaced</st> <st c="15375">missing data with
    the variable’s median values</st> <st c="15422">using</st> `<st c="15429">pandas</st>`<st
    c="15435">,</st> `<st c="15437">scikit-learn</st>`<st c="15449">,</st> <st c="15451">and</st>
    `<st c="15455">feature-engine</st>`<st c="15469">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15470">We divided the dataset into train and test sets using scikit-learn’s</st>
    `<st c="15540">train_test_split()</st>` <st c="15558">function.</st> <st c="15569">The
    function takes the predictor variables, the target, the fraction of observations
    to retain in the test set, and a</st> `<st c="15687">random_state</st>` <st c="15699">value
    for reproducibility, as arguments.</st> <st c="15741">It returned a train set
    with 70% of the original observations and a test set with 30% of the original
    observations.</st> <st c="15857">The 70:30 split was done</st> <st c="15882">at
    random.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15892">To impute missing data with pandas, in</st> *<st c="15932">step
    5</st>*<st c="15938">, we created a dictionary with the numerical variable names
    as keys and their medians as values.</st> <st c="16035">The median values were
    learned from the training set to avoid data leakage.</st> <st c="16111">To replace
    missing data, we applied</st> `<st c="16147">pandas</st>`<st c="16153">’</st>
    `<st c="16156">fillna()</st>` <st c="16164">to train and test sets, passing the
    dictionary with the median values per variable as</st> <st c="16251">a parameter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16263">To replace the missing values with the median using</st> `<st
    c="16316">scikit-learn</st>`<st c="16328">, we used</st> `<st c="16338">SimpleImputer()</st>`
    <st c="16353">with the</st> `<st c="16363">strategy</st>` <st c="16371">set to</st>
    `<st c="16379">"median"</st>`<st c="16387">. To restrict the imputation to numerical
    variables, we used</st> `<st c="16448">ColumnTransformer()</st>`<st c="16467">.
    With the</st> `<st c="16478">remainder</st>` <st c="16487">argument set to</st>
    `<st c="16504">passthrough</st>`<st c="16515">, we made</st> `<st c="16525">ColumnTransformer()</st>`
    <st c="16544">return</st> *<st c="16552">all the variables</st>* <st c="16569">seen
    in the training set in the transformed output; the imputed ones followed by those
    that were</st> <st c="16667">not transformed.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16683">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="16688">ColumnTransformer()</st>` <st c="16708">changes the names of
    the variables in the output.</st> <st c="16759">The transformed variables show
    the prefix</st> `<st c="16801">imputer</st>` <st c="16808">and the unchanged variables
    show the</st> <st c="16846">prefix</st> `<st c="16853">remainder</st>`<st c="16862">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16863">In</st> *<st c="16867">step 8</st>*<st c="16873">, we set the
    output of the column transformer to</st> `<st c="16922">pandas</st>` <st c="16928">to
    obtain a DataFrame as a result.</st> <st c="16964">By default,</st> `<st c="16976">ColumnTransformer()</st>`
    <st c="16995">returns</st> `<st c="17004">numpy</st>` <st c="17009">arrays.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17017">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17022">From version 1.4.0,</st> `<st c="17043">scikit-learn</st>` <st
    c="17055">transformers can return</st> `<st c="17080">numpy</st>` <st c="17085">arrays,</st>
    `<st c="17094">pandas</st>` <st c="17100">DataFrames, or</st> `<st c="17116">polar</st>`
    <st c="17121">frames as a result of the</st> `<st c="17148">transform()</st>`
    <st c="17159">method.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17167">With</st> `<st c="17173">fit()</st>`<st c="17178">,</st> `<st
    c="17180">SimpleImputer()</st>` <st c="17195">learned the median of each numerical
    variable in the train set and stored them in its</st> `<st c="17282">statistics_</st>`
    <st c="17293">attribute.</st> <st c="17305">With</st> `<st c="17310">transform()</st>`<st
    c="17321">, it replaced the missing values with</st> <st c="17359">the medians.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17371">To replace missing values with the median using Feature-engine,
    we used the</st> `<st c="17448">MeanMedianImputer()</st>` <st c="17467">with the</st>
    `<st c="17477">imputation_method</st>` <st c="17494">set to</st> `<st c="17502">median</st>`<st
    c="17508">. To restrict the imputation to a subset of variables, we passed the
    variable names in a list to the</st> `<st c="17609">variables</st>` <st c="17618">parameter.</st>
    <st c="17630">With</st> `<st c="17635">fit()</st>`<st c="17640">, the transformer
    learned and stored the median values per</st> <st c="17698">variable</st> <st
    c="17707">in a dictionary</st> <st c="17723">in its</st> `<st c="17731">imputer_dict_</st>`
    <st c="17744">attribute.</st> <st c="17756">With</st> `<st c="17761">transform()</st>`<st
    c="17772">, it replaced the missing values, re</st><st c="17808">turning a</st>
    <st c="17819">pandas DataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17836">Imputing categorical variables</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="17867">We typically impute categorical variables</st> <st c="17909">with
    the most frequent category, or with a specific string.</st> <st c="17970">To avoid
    data leakage, we find the frequent categories from the train set.</st> <st c="18045">Then,
    we use these values to impute the train, test, and future datasets.</st> `<st
    c="18119">scikit-learn</st>` <st c="18131">and</st> `<st c="18136">feature-engine</st>`
    <st c="18150">find and store the frequent categories for the imputation, out of</st>
    <st c="18217">the box.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18225">In this recipe, we will replace missing data in categorical variables
    with the most frequent category,</st> <st c="18329">or with an</st> <st c="18340">arbitrary
    string.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18357">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="18373">To begin, let’s make a few imports and prepare</st> <st c="18421">the
    data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18430">Let’s import</st> `<st c="18444">pandas</st>` <st c="18450">and
    the required functions and classes from</st> `<st c="18495">scikit-learn</st>`
    <st c="18507">and</st> `<st c="18512">feature-engine</st>`<st c="18526">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="18745">Let’s load the dataset that we prepared in the</st> *<st c="18793">Technical</st>*
    *<st c="18803">requirements</st>* <st c="18815">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="18871">Let’s split the data into train and test sets and their</st> <st
    c="18928">respective targets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19078">Let’s capture the categorical variables in</st> <st c="19122">a
    list:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19202">Let’s store the variables’ most frequent categories in</st> <st
    c="19258">a dictionary:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19341">Let’s replace missing values</st> <st c="19370">with the</st>
    <st c="19380">frequent categories:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19498">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="19503">fillna()</st>` <st c="19512">returns a new DataFrame with the
    imputed values by default.</st> <st c="19573">We can replace missing data in the
    original DataFrame by executing</st> `<st c="19640">X_train.fillna(value=frequent_valu</st><st
    c="19674">es, inplace=True)</st>`<st c="19692">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19693">To replace missing data with a specific string, let’s create an
    imputation dictionary with the categorical variable names as the keys and an arbitrary
    string as</st> <st c="19855">the values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19929">Now, we can use this dictionary and the code in</st> *<st c="19978">step
    6</st>* <st c="19984">to replace</st> <st c="19996">missing data.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="20009">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20014">With</st> `<st c="20020">pandas</st>` `<st c="20026">value_counts()</st>`
    <st c="20041">we can see the string added by the imputation.</st> <st c="20089">Try
    executing, for</st> <st c="20108">example,</st> `<st c="20117">X_train["A1"].value_counts()</st>`<st
    c="20145">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20146">Now, let’s impute missing values with the most frequent category</st>
    <st c="20212">using</st> `<st c="20218">scikit-learn</st>`<st c="20230">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20231">Let’s set up the imputer to find the most frequent category</st>
    <st c="20292">per variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="20355">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="20360">SimpleImputer()</st>` <st c="20376">will learn the mode for
    numerical and categorical variables alike.</st> <st c="20444">But in practice,
    mode imputation is done for categorical</st> <st c="20501">variables only.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20516">Let’s restrict the imputation</st> <st c="20547">to the</st> <st
    c="20554">categorical variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="20698">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20703">To impute missing data with a string instead of the most frequent
    category, set</st> `<st c="20784">SimpleImputer()</st>` <st c="20799">as follows:</st>
    `<st c="20812">imputer =</st>` `<st c="20822">SimpleImputer(strategy="constant",</st>
    <st c="20857">fill_value="missing")</st>`<st c="20878">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20879">Fit the imputer to the train set so that it learns the most</st>
    <st c="20940">frequent values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="20972">Let’s take a look at the most frequent values learned by</st>
    <st c="21030">the imputer:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="21085">The previous command returns the most frequent values</st> <st
    c="21140">per variable:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="21220">Finally, let’s replace missing values with the</st> <st c="21268">frequent
    categories:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="21354">Make sure to inspect the resulting DataFrames by</st> <st c="21404">executing</st>
    `<st c="21414">X_train_t.head()</st>`<st c="21430">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="21431">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21436">The</st> `<st c="21441">ColumnTransformer()</st>` <st c="21460">changes
    the names of the variables.</st> <st c="21497">The imputed variables show the
    prefix</st> `<st c="21535">imputer</st>` <st c="21542">and the untransformed variables
    the</st> <st c="21579">prefix</st> `<st c="21586">remainder</st>`<st c="21595">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21596">Finally, let’s impute missing value</st><st c="21632">s</st> <st
    c="21635">using</st> `<st c="21641">feature-engine</st>`<st c="21655">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21656">Let’s set up the imputer</st> <st c="21681">to replace the missing
    data in categorical variables with their most</st> <st c="21751">frequent value:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="21856">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21861">With the</st> `<st c="21871">variables</st>` <st c="21880">parameter
    set to</st> `<st c="21898">None</st>`<st c="21902">,</st> `<st c="21904">CategoricalImputer()</st>`
    <st c="21924">will automatically impute all categorical variables found in the
    train set.</st> <st c="22001">Use this parameter to restrict the imputation to
    a subset of categorical variables, as shown in</st> *<st c="22097">step 13</st>*<st
    c="22104">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22105">Fit the imputer to the train set so that it learns the most</st>
    <st c="22166">frequent categories:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="22207">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22212">To impute categorical variables with a specific string, set</st>
    `<st c="22273">imputation_method</st>` <st c="22290">to</st> `<st c="22294">missing</st>`
    <st c="22301">and</st> `<st c="22306">fill_value</st>` <st c="22316">to the</st>
    <st c="22324">desired string.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22339">Let’s check out the</st> <st c="22360">learned categories:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="22401">We can see the dictionary</st> <st c="22427">with the most frequent
    values in the</st> <st c="22465">following output:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="22585">Finally, let’s replace the missing values with</st> <st c="22633">frequent
    categories:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="22729">If you want to impute numerical variables with a string or the
    most frequent value using</st> `<st c="22819">CategoricalImputer()</st>`<st c="22839">,
    set the</st> `<st c="22849">ignore_format</st>` <st c="22862">parameter</st> <st
    c="22873">to</st> `<st c="22876">True</st>`<st c="22880">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`<st c="22881">CategoricalImputer()</st>` <st c="22902">returns</st> <st c="22910">a
    pandas DataFrame as</st> <st c="22933">a result.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22942">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="22958">In this recipe, we replaced missing values in categorical</st>
    <st c="23017">variables with the most frequent categories or an arbitrary string.</st>
    <st c="23085">We used</st> `<st c="23093">pandas</st>`<st c="23099">,</st> `<st
    c="23101">scikit-learn</st>`<st c="23113">,</st> <st c="23115">and</st> `<st c="23119">feature-engine</st>`<st
    c="23133">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23134">In</st> *<st c="23138">step 5</st>*<st c="23144">, we created
    a dictionary</st> <st c="23170">with the variable names as keys and the frequent
    categories as values.</st> <st c="23241">To capture the frequent categories, we
    used pandas</st> `<st c="23292">mode()</st>`<st c="23298">, and to return a dictionary,
    we used pandas</st> `<st c="23343">to_dict()</st>`<st c="23352">. To replace the
    missing data, we used</st> `<st c="23391">pandas</st>` `<st c="23397">fillna()</st>`<st
    c="23406">, passing the dictionary with the variables and their frequent categories
    as parameters.</st> <st c="23495">There can be more than one mode in a variable,
    that’s why we made sure to capture only one of those valu</st><st c="23599">es
    by</st> <st c="23606">using</st> `<st c="23612">.iloc[0]</st>`<st c="23620">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23621">To replace the missing values using</st> `<st c="23658">scikit-learn</st>`<st
    c="23670">, we used</st> `<st c="23680">SimpleImputer()</st>` <st c="23695">with
    the</st> `<st c="23705">strategy</st>` <st c="23713">set to</st> `<st c="23721">most_frequent</st>`<st
    c="23734">. To restrict the imputation to categorical variables, we used</st>
    `<st c="23797">ColumnTransformer()</st>`<st c="23816">. With</st> `<st c="23823">remainder</st>`
    <st c="23832">set to</st> `<st c="23840">passthrough</st>`<st c="23851">, we made</st>
    `<st c="23861">ColumnTransformer()</st>` <st c="23880">return all the variables
    present in the training set as a result of the</st> `<st c="23953">transform()</st>`
    <st c="23964">method .</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23973">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="23978">ColumnTransformer()</st>` <st c="23998">changes the names of
    the variables in the output.</st> <st c="24049">The transformed variables show
    the prefix</st> `<st c="24091">imputer</st>` <st c="24098">and the unchanged variables
    show the</st> <st c="24136">prefix</st> `<st c="24143">remainder</st>`<st c="24152">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24153">With</st> `<st c="24159">fit()</st>`<st c="24164">,</st> `<st
    c="24166">SimpleImputer()</st>` <st c="24181">learned the variables’ most frequent
    categories and stored them in its</st> `<st c="24253">statistics_</st>` <st c="24264">attribute.</st>
    <st c="24276">With</st> `<st c="24281">transform()</st>`<st c="24292">, it replaced
    the missing data with the</st> <st c="24332">learned parameters.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="24351">SimpleImputer()</st>` <st c="24367">and</st> `<st c="24372">ColumnTransformer()</st>`
    <st c="24391">return NumPy arrays by default.</st> <st c="24424">We can change
    this behavior with the</st> `<st c="24461">set_output()</st>` <st c="24473">parameter.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24484">To replace missing values with</st> `<st c="24516">feature-engine</st>`<st
    c="24530">, we used the</st> `<st c="24544">CategoricalImputer()</st>` <st c="24564">with</st>
    `<st c="24570">imputation_method</st>` <st c="24587">set to</st> `<st c="24595">frequent</st>`<st
    c="24603">. With</st> `<st c="24610">fit()</st>`<st c="24615">, the transformer
    learned and stored the most frequent categories in a dictionary in its</st> `<st
    c="24704">imputer_dict_</st>` <st c="24717">attribute.</st> <st c="24729">With</st>
    `<st c="24734">transform()</st>`<st c="24745">, it replaced the missing values
    with the</st> <st c="24787">learned parameters.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24806">Unlike</st> `<st c="24814">SimpleImputer()</st>`<st c="24829">,</st>
    `<st c="24831">CategoricalImputer()</st>` <st c="24851">will only impute categorical
    variables, unless specificall</st><st c="24910">y told not to do so by setting
    the</st> `<st c="24946">ignore_format</st>` <st c="24959">parameter to</st> `<st
    c="24973">True</st>`<st c="24977">. In addition, with</st> `<st c="24997">feature-engine</st>`
    <st c="25011">transformers we can restrict the transformations to a subset of
    variables through the transformer itself.</st> <st c="25118">For</st> `<st c="25122">scikit-learn</st>`
    <st c="25134">transformers, we need the additional</st> `<st c="25172">ColumnTransformer()</st>`
    <st c="25191">class to apply the transformat</st><st c="25222">ion</st> <st c="25226">to
    a subset of</st> <st c="25242">the variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25256">Replacing missing values with an arbitrary number</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="25306">We can replace missing data</st> <st c="25334">with an arbitrary
    value.</st> <st c="25360">Commonly</st> <st c="25368">used values are</st> `<st
    c="25385">999</st>`<st c="25388">,</st> `<st c="25390">9999</st>`<st c="25394">,
    or</st> `<st c="25399">-1</st>` <st c="25401">for positive distributions.</st>
    <st c="25430">This method is used for numerical variables.</st> <st c="25475">For
    categorical variables, the equivalent metho</st><st c="25522">d is to replace
    missing data with an arbitrary string, as described in the</st> *<st c="25598">Imputing
    categorical</st>* *<st c="25619">variables</st>* <st c="25628">recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25636">When replacing missing values with arbitrary numbers, we need
    to be careful not to select a value close to the mean, the median, or any other
    common value of</st> <st c="25795">the distribution.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25812">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25817">We’d use arbitrary number imputation when data is not missing
    at random, use non-linear models, or when the percentage of missing data is high.</st>
    <st c="25962">This imputation technique distorts the original</st> <st c="26010">variable
    distribution.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26032">In this recipe, we will impute missing data with arbitrary numbers
    using</st> `<st c="26106">panda</st><st c="26111">s</st>`<st c="26113">,</st>
    `<st c="26115">scikit-learn</st>`<st c="26127">,</st> <st c="26129">and</st> `<st
    c="26133">feature-engine</st>`<st c="26147">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26148">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="26164">Let’s begin by importing the necessary tools and loading</st>
    <st c="26222">the data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26231">Import</st> `<st c="26239">pandas</st>` <st c="26245">and the
    required functions</st> <st c="26273">and classes:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26460">Let’s load the dataset described in the</st> *<st c="26501">Technical</st>*
    *<st c="26511">requirements</st>* <st c="26523">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26578">Let’s separate</st> <st c="26593">the data into train and</st>
    <st c="26618">test</st> <st c="26622">sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26761">We will select arbitrary values greater than the maximum value
    of</st> <st c="26828">the distribution.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="26845">Let’s find the maximum value of four</st> <st c="26883">numerical
    variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26943">The previous</st> <st c="26956">command returns the</st> <st c="26977">following
    output:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="27165">Let’s make a copy of the</st> <st c="27191">original DataFrames:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="27263">Now, we replace the missing values</st> <st c="27299">with</st>
    `<st c="27304">99</st>`<st c="27306">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="27482">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="27487">To impute different variables with different values using</st>
    `<st c="27546">pandas</st>` `<st c="27552">fillna()</st>`<st c="27561">, use a
    dictionary like this:</st> `<st c="27591">imputation_dict = {"A2": -1, "A3": -</st><st
    c="27627">1, "A8": 999, "</st>``<st c="27643">A11": 9999}</st>`<st c="27655">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27656">Now, we’ll impute missing</st> <st c="27682">values with an arbitrary
    number</st> <st c="27714">using</st> `<st c="27721">scikit-learn</st>`<st c="27733">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27734">Let’s set up</st> `<st c="27748">imputer</st>` <st c="27755">to
    replace missing values</st> <st c="27782">with</st> `<st c="27787">99</st>`<st
    c="27789">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="27851">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27856">If your dataset contains categorical variables,</st> `<st c="27905">SimpleImputer()</st>`
    <st c="27920">will add</st> `<st c="27930">99</st>` <st c="27932">to those variables
    as well if any values</st> <st c="27974">are missing.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27986">Let’s fit</st> `<st c="27997">imputer</st>` <st c="28004">to a
    slice of the train set containing the variables</st> <st c="28058">to impute:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="28128">Replace the missing values with</st> `<st c="28161">99</st>` <st
    c="28163">in the</st> <st c="28171">desired variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="28289">Go ahead and check the lack of missing values by executing</st>
    `<st c="28349">X_test_t[["A2", "A3", "</st>``<st c="28372">A8", "A11"]].isnull().sum()</st>`<st
    c="28400">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="28401">To finish, let’s impute mis</st><st c="28429">sing values</st>
    <st c="28442">using</st> `<st c="28448">f</st><st c="28449">eature-engine</st>`<st
    c="28462">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="28463">Let’s set up the</st> `<st c="28481">imputer</st>` <st c="28488">to
    replace</st> <st c="28499">missing values</st> <st c="28514">with</st> `<st c="28520">99</st>`
    <st c="28522">in 4</st> <st c="28528">specific variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`<st c="28641">Note</st>`'
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="28646">ArbitraryNumberImputer()</st>` <st c="28671">will automatically
    select all numerical variables in the train set for imputation if we set the</st>
    `<st c="28768">variables</st>` <st c="28777">parameter</st> <st c="28788">to</st>
    `<st c="28791">None</st>`<st c="28795">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28796">Finally, let’s replace the missing values</st> <st c="28839">with</st>
    `<st c="28844">99</st>`<st c="28846">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="28924">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="28929">To impute different variables with different numbers, set up</st>
    `<st c="28991">ArbitraryNumberImputer()</st>` <st c="29015">as follows:</st> `<st
    c="29028">ArbitraryNumberImputer(imputater_dict = {"A2": -1, "A3": -1, "A8": 999,
    "</st>``<st c="29101">A11": 9999})</st>`<st c="29114">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29115">We have now replaced missing data with arbitrary numbers us</st><st
    c="29175">ing three different</st> <st c="29196">open-source libraries.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29218">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="29234">In this recipe, we replaced</st> <st c="29262">missing values
    in numerical variables</st> <st c="29300">with an arbitrary number using</st>
    `<st c="29332">pandas</st>`<st c="29338">,</st> `<st c="29340">sc</st><st c="29342">ikit-learn</st>`<st
    c="29353">,</st> <st c="29355">and</st> `<st c="29359">featur</st><st c="29365">e-engine</st>`<st
    c="29374">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29375">To determine which arbitrary value to use, we inspected the maximum
    values of four numerical variables using pandas’</st> `<st c="29493">max()</st>`<st
    c="29498">. We chose</st> `<st c="29509">99</st>` <st c="29511">because it was
    greater than the maximum values of the selected variables.</st> <st c="29586">In</st>
    *<st c="29589">step 5</st>*<st c="29595">, we used</st> `<st c="29605">pandas</st>`
    `<st c="29611">fillna()</st>` <st c="29620">to replace the</st> <st c="29636">missing
    data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29649">To replace missing values using</st> `<st c="29682">scikit-learn</st>`<st
    c="29694">, we utilized</st> `<st c="29708">SimpleImputer()</st>`<st c="29723">,
    with the</st> `<st c="29734">strategy</st>` <st c="29742">set to</st> `<st c="29750">constant</st>`<st
    c="29758">, and specified</st> `<st c="29774">99</st>` <st c="29776">in the</st>
    `<st c="29784">fill_value</st>` <st c="29794">argument.</st> <st c="29805">Next,
    we fitted the imputer to a slice of the train set with the numerical variables
    to impute.</st> <st c="29901">Finally, we replaced missing values</st> <st c="29937">using</st>
    `<st c="29943">transform()</st>`<st c="29954">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29955">To replace missing values with</st> `<st c="29987">feature-engine</st>`
    <st c="30001">we used</st> `<st c="30010">ArbitraryValueImputer()</st>`<st c="30033">,
    specifying the value</st> `<st c="30056">99</st>` <st c="30058">and the variables
    to impute as parameters.</st> <st c="30102">Next, we applied the</st> `<st c="30123">fit_transform()</st>`
    <st c="30138">method to replace missing data in the train set and the</st> `<st
    c="30195">transform()</st>` <st c="30206">met</st><st c="30210">hod to replace
    missing data in the</st> <st c="30246">test set.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30255">Finding extreme values for imputation</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="30293">Replacing missing values</st> <st c="30318">with a value at the
    end of the variable</st> <st c="30358">distribution (extreme values) is like replacing
    them with an arbitra</st><st c="30427">ry value, but instead of setting the arbitrary
    values manually, the values are automatically selected from the end of the</st>
    <st c="30550">variable distribution.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30572">We can replace missing data with a value that is greater or smaller
    than most values in the variable.</st> <st c="30675">To select a value that is
    greater, we can use the mean plus a factor of the standard deviation.</st> <st
    c="30771">Alternatively, we can set it to the 75th quantile + IQR × 1.5\.</st>
    **<st c="30834">IQR</st>** <st c="30837">stands for</st> **<st c="30849">inter-quartile
    range</st>** <st c="30869">and is the difference between</st> <st c="30899">the
    75th and 25th quantile.</st> <st c="30928">To replace missing data with values
    that are smaller than the remaining values, we can use the mean minus a factor
    of the standard deviation, or the 25th quantile – IQR ×</st> <st c="31099">1.5.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31103">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31108">End-of-tail imputation may distort the distribution of the original
    variables, so it may not be suitable for</st> <st c="31218">linear models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31232">In this recipe, we will implement end-of-tail or extreme va</st><st
    c="31292">lue imputation using</st> `<st c="31314">pandas</st>` <st c="31320">and</st>
    `<st c="31325">feature-engine</st>`<st c="31339">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31340">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="31356">To begin this</st> <st c="31370">recipe, let’s import the necessary
    tools</st> <st c="31411">and load</st> <st c="31421">the data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31430">Let’s import</st> `<st c="31444">pandas</st>` <st c="31450">and
    the required function</st> <st c="31477">and class:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31613">Let’s load the dataset we described in the</st> *<st c="31657">Technical</st>*
    *<st c="31667">requirements</st>* <st c="31679">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31734">Let’s capture the numerical variables in a list, excluding</st>
    <st c="31794">the target:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31908">Let’s split the data into train and test sets, keeping only the</st>
    <st c="31973">numerical variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="32116">We’ll now determine</st> <st c="32137">the IQR:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="32199">We can visualize</st> <st c="32216">the IQR values</st> <st c="32231">by
    executing</st> `<st c="32245">IQR</st>` <st c="32248">or</st> `<st c="32252">print(IQR)</st>`<st
    c="32262">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="32346">Let’s create a dictionary with the variable names and the</st>
    <st c="32405">imputation values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="32489">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32494">If we use the inter-quartile range proximity rule, we determine
    the imputation values by adding 1.5 times the IQR to the 75th quantile.</st> <st
    c="32631">If variables are normally distributed, we can calculate the imputation
    values as the mean plus a factor of the standard deviation,</st> `<st c="32762">imputation_dict
    = (X_train.me</st><st c="32791">an() + 3 *</st>` `<st c="32803">X_train.std()).to_dict()</st>`<st
    c="32827">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32828">Finally, let’s replace the</st> <st c="32856">missing data:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="32967">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32972">We can also replace missing data with values at the left tail
    of the distribution using</st> `<st c="33061">value = X_train[var].quantile(0.25)
    - 1.5 * IQR</st>` <st c="33108">or</st> `<st c="33112">value = X_train[var].mean()
    – 3 *</st>` `<st c="33146">X_train[var].std()</st>`<st c="33164">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33165">To finish, let’s impute</st> <st c="33189">missing values</st>
    <st c="33204">using</st> `<st c="33211">feature-engine</st>`<st c="33225">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33226">Let’s set up</st> `<st c="33240">imputer</st>` <st c="33247">to
    estimate a value at the right of the distribution using the IQR</st> <st c="33315">proximity
    rule:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="33421">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33426">To use the mean and standard deviation to calculate the replacement
    values, set</st> `<st c="33507">imputation_method="Gaussian"</st>`<st c="33535">.
    Use</st> `<st c="33541">left</st>` <st c="33545">or</st> `<st c="33549">right</st>`
    <st c="33554">in the</st> `<st c="33562">tail</st>` <st c="33566">argument to
    specify the side of the distribution to consider when finding values for</st>
    <st c="33652">the imputation.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33667">Let’s fit</st> `<st c="33678">EndTailImputer()</st>` <st c="33694">to
    the train set so that it learns the</st> <st c="33734">values for</st> <st c="33745">the
    imputation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="33781">Let’s inspect the</st> <st c="33800">learned values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="33837">The previous command</st> <st c="33858">returns a dictionary with
    the values</st> <st c="33895">to use to impute</st> <st c="33913">each variable:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="34022">Finally, let’s replace the</st> <st c="34050">missing values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="34137">Remember that you can corroborate that the missing values were
    replaced by using</st> `<st c="34219">X_train[[</st><st c="34228">'A2','A3', 'A8',
    'A11', '</st>``<st c="34254">A14', 'A15']].isnull().mean()</st>`<st c="34284">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34285">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="34301">In this recipe, we replaced missing values in numerical variables
    with a number at the end</st> <st c="34393">of the distribution using</st> `<st
    c="34419">pandas</st>` <st c="34425">and</st> `<st c="34430">feature-engine</st>`<st
    c="34444">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34445">We determined the imputation values according to the formulas
    described in the introduction to this recipe.</st> <st c="34554">We used pandas</st>
    `<st c="34569">quantile()</st>` <st c="34579">to find specific quantile values,
    or</st> `<st c="34617">pandas</st>` `<st c="34623">mean()</st>` <st c="34630">and</st>
    `<st c="34635">std()</st>` <st c="34640">for the mean and standard deviation.</st>
    <st c="34678">With pandas</st> `<st c="34690">fillna()</st>` <st c="34698">we
    replaced the</st> <st c="34715">missing values.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34730">To replace missing values with</st> `<st c="34762">EndTailImputer()</st>`
    <st c="34778">from</st> `<st c="34784">feature-engine</st>`<st c="34798">, we
    set</st> `<st c="34807">distribution</st>` <st c="34819">to</st> `<st c="34823">iqr</st>`
    <st c="34826">to calculate the values based on the IQR proximity rule.</st> <st
    c="34884">With</st> `<st c="34889">tail</st>` <st c="34893">set to</st> `<st c="34901">right</st>`
    <st c="34906">the transformer found the imputation values from the right of the
    distribution.</st> <st c="34987">With</st> `<st c="34992">fit()</st>`<st c="34997">,
    the imputer learned and stored the values for the imputation in a dictionary in
    the</st> `<st c="35083">imputer_dict_</st>` <st c="35096">attribute.</st> <st
    c="35108">With</st> `<st c="35113">transform()</st>`<st c="35124">, we replaced</st>
    <st c="35137">the missing</st> <st c="35149">values,</st> <st c="35158">returning
    DataFrames.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35179">Marking imputed values</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="35202">In the previous recipes, we focused</st> <st c="35239">on replacing
    missing data with estimates of their values.</st> <st c="35297">In addition, we
    can add missing indicators to</st> *<st c="35343">mark</st>* <st c="35347">observations
    where values</st> <st c="35374">were missing.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35387">A missing indicator is a binary var</st><st c="35423">iable that
    takes the value</st> `<st c="35451">1</st>` <st c="35452">or</st> `<st c="35456">True</st>`
    <st c="35460">to indicate whether a value was missing, and</st> `<st c="35506">0</st>`
    <st c="35507">or</st> `<st c="35511">False</st>` <st c="35516">otherwise.</st>
    <st c="35528">It is common practice to replace missing observations with the mean,
    median, or most frequent category while simultaneously marking those missing observations
    with missing indicators.</st> <st c="35712">In this recipe, we will learn how
    to add missing</st> <st c="35761">indicators using</st> `<st c="35778">pandas</st>`<st
    c="35784">,</st> `<st c="35786">scikit-learn</st>`<st c="35798">,</st> <st c="35800">and</st>
    `<st c="35804">feature-engine</st>`<st c="35818">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35819">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="35835">Let’s begin by making some imports and loading</st> <st c="35883">the
    data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35892">Let’s import the required libraries, functions,</st> <st c="35941">and
    classes:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36270">Let’s load and split the dataset</st> <st c="36303">described
    in the</st> *<st c="36321">Technical</st>* *<st c="36331">requirements</st>* <st
    c="36343">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36529">Let’s capture the variable names in</st> <st c="36566">a list:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36627">Let’s create names for the missing indicators and store them in</st>
    <st c="36692">a list:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36746">If we execute</st> `<st c="36761">indicators</st>`<st c="36771">,
    we will see the names we will use for the new variables:</st> `<st c="36830">['A1_na',
    'A3_na', 'A4_na', 'A5_na', 'A6_na', '</st>``<st c="36877">A7_na', 'A8_na']</st>`<st
    c="36894">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="36895">Let’s make a copy of the</st> <st c="36921">original DataFrames:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36994">Let’s add the</st> <st c="37009">missing indicators:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37149">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37154">If you want the indicators to have</st> `<st c="37190">True</st>`
    <st c="37194">and</st> `<st c="37199">False</st>` <st c="37204">as values instead
    of</st> `<st c="37226">0</st>` <st c="37227">and</st> `<st c="37232">1</st>`<st
    c="37233">, remove</st> `<st c="37242">astype(int)</st>` <st c="37253">in</st>
    *<st c="37257">step 6</st>*<st c="37263">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37264">Let’s inspect the</st> <st c="37283">resulting</st> <st c="37292">DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37320">We can see the newly added vari</st><st c="37352">ables at the
    right of the DataFrame in the</st> <st c="37396">following image:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.4 – DataFrame with the missing indicators](img/B22396_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="37815">Figure 1.4 – DataFrame with the missing indicators</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37865">Now, let’s add missing indicators using</st> <st c="37906">Feature-engine
    instead.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37929">Set up the imputer to add binary indicators to every variable
    with</st> <st c="37997">missing data:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38078">Fit the imputer to the train set so that it finds the variables
    with</st> <st c="38148">missing data:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38182">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38187">If we execute</st> `<st c="38202">imputer.variables_</st>`<st
    c="38220">, we will find the variables for which missing indicators will</st>
    <st c="38283">be added.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38292">Finally, let’s add the</st> <st c="38316">missing indicators:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38411">So far, we just added missing indicators.</st> <st c="38454">But
    we still have the missing data in our variables.</st> <st c="38507">We need to
    replace them with numbers.</st> <st c="38545">In the rest of this recipe, we will
    combine the use of missing indicators with mean and</st> <st c="38633">mode imputation.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="38649">Let’s create a pipeline</st> <st c="38673">to add missing indicators
    to categorical and numerical variables, then impute categorical variables with
    the most frequent category, and numerical variables with</st> <st c="38836">the
    mean:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39026">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="39031">feature-engine</st>` <st c="39046">imputers automatically identify
    numerical or categorical variables.</st> <st c="39115">So there is no need to
    slice the data or pass the variabl</st><st c="39172">e names as arguments to the
    transformers in</st> <st c="39217">this case.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39227">Let’s add the indicators and impute</st> <st c="39264">missing
    values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39353">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39358">Use</st> `<st c="39363">X_train_t.isnull().sum()</st>` <st c="39387">to
    corroborate that there is no data missing.</st> <st c="39434">Execute</st> `<st
    c="39442">X_train_t.head()</st>` <st c="39458">to get a view of the</st> <st c="39480">resulting
    datafame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39499">Finally, let’s add missing indicators and simultaneously impute
    numerical and categorical variables with the mean and most frequent categories
    respectively,</st> <st c="39657">utilizing scikit-learn.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39680">Let’s make a list with the names</st> <st c="39713">of the numerical
    and</st> <st c="39735">categorical variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39885">Let’s set up a pipeline to perform mean and frequent category
    imputation while marking the</st> <st c="39977">missing data:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40218">Now, let’s carry out</st> <st c="40240">the imputation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40329">Make sure to explore</st> `<st c="40350">X_train_t.head()</st>`
    <st c="40367">to get familiar with the</st> <st c="40393">pipeline’s output.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40411">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="40427">To add missing</st> <st c="40442">indicators</st> <st c="40453">using
    pandas, we used</st> `<st c="40476">isna()</st>`<st c="40482">, which created
    a new vector assigning the value of</st> `<st c="40534">True</st>` <st c="40538">if
    there was a missing value or</st> `<st c="40571">False</st>` <st c="40576">otherwise.</st>
    <st c="40588">We used</st> `<st c="40596">astype(int)</st>` <st c="40607">to convert
    the Boolean vectors into binary vectors with values</st> `<st c="40671">1</st>`
    <st c="40672">and</st> `<st c="40676">0</st>`<st c="40677">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40678">To add a missing indicator with</st> `<st c="40711">feature-engine</st>`<st
    c="40725">, we used</st> `<st c="40735">AddMissingIndicator()</st>`<st c="40756">.
    With</st> `<st c="40763">fit()</st>` <st c="40768">the transformer found the variables
    with missing data.</st> <st c="40824">With</st> `<st c="40829">transform()</st>`
    <st c="40840">it appended the missing indicators to the right of the train and</st>
    <st c="40906">test sets.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40916">To sequentially add missing indicators and then replace the</st>
    `<st c="40977">nan</st>` <st c="40980">values with the most frequent category
    or the mean, we lined up Feature-engine’s</st> `<st c="41062">AddMissingIndicator()</st>`<st
    c="41083">,</st> `<st c="41085">CategoricalImputer()</st>`<st c="41105">, and</st>
    `<st c="41111">MeanMedianImputer()</st>` <st c="41130">within a</st> `<st c="41140">pipeline</st>`<st
    c="41148">. The</st> `<st c="41154">fit()</st>` <st c="41159">method from the</st>
    `<st c="41176">pipeline</st>` <st c="41184">made the transformers find the variables
    with</st> `<st c="41231">nan</st>` <st c="41234">and calculate the mean of the
    numerical variables and the mode of the categorical variables.</st> <st c="41328">The</st>
    `<st c="41332">transform()</st>` <st c="41343">method from the</st> `<st c="41360">pipeline</st>`
    <st c="41368">made the transformers add the missing indicators and then replace
    the missing values with</st> <st c="41459">their estimates.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41475">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41480">Feature-engine transformations return DataFrames respecting the
    original names and order of the variables.</st> <st c="41588">Scikit-learn’s</st>
    `<st c="41603">ColumnTransformer()</st>`<st c="41622">, on the other hand, changes
    the variable’s names and order in the</st> <st c="41689">resulting data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41704">Finally, we added missing indicators and replaced missing data
    with the mean and most frequent category using</st> `<st c="41815">scikit-learn</st>`<st
    c="41827">. We lined up two instances of</st> `<st c="41858">SimpleImputer()</st>`<st
    c="41873">, the first to impute data with the mean and the second to impute data
    with the most frequent category.</st> <st c="41977">In both cases, we set the</st>
    `<st c="42003">add_indicator</st>` <st c="42016">parameter to</st> `<st c="42030">True</st>`
    <st c="42034">to add the missing indicators.</st> <st c="42066">We wrapped</st>
    `<st c="42077">SimpleImputer()</st>` <st c="42092">with</st> `<st c="42098">ColumnTransformer()</st>`
    <st c="42117">to specifically modify numerical or categorical variables.</st>
    <st c="42177">Then we used the</st> `<st c="42194">fit()</st>` <st c="42199">and</st>
    `<st c="42204">transform()</st>` <st c="42215">methods from the</st> `<st c="42233">pipeline</st>`
    <st c="42241">to train the transformers and then add the indicators and replace
    the</st> <st c="42312">missing data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42325">When returning DataFrames,</st> `<st c="42353">ColumnTransformer()</st>`
    <st c="42372">changes the names of the variables and their order.</st> <st c="42425">Take
    a look at the result from</st> *<st c="42456">step 15</st>* <st c="42463">by executing</st>
    `<st c="42477">X_train_t.head()</st>`<st c="42493">. You’ll see that the name
    given to each step of the pipeline is added as a prefix to the variables to flag
    which variable was modified with each transformer.</st> <st c="42652">Then,</st>
    `<st c="42658">num_imputer__A2</st>` <st c="42673">was returned by the first step
    of the pipeline, while</st> `<st c="42728">cat_imputer__A12</st>` <st c="42744">was
    returned</st> <st c="42757">by the second step of</st> <st c="42780">the pipeline.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42793">There’s more…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="42807">Scikit-learn has</st> <st c="42824">the</st> `<st c="42829">MissingIndicator()</st>`
    <st c="42847">transformer that just adds missing indicators.</st> <st c="42895">Check
    it out in the documentation:</st> [<st c="42930">https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html</st>](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html)
    <st c="43016">and find an example in the accompanying GitHub repository</st> <st
    c="43075">at</st> [<st c="43078">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01</st><st
    c="43177">-missing-data-imputation/Recipe-06-Marking-imputed-values.ipynb</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputation/Recipe-06-Marking-imputed-values.ipynb)<st
    c="43241">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43242">Implementing forward and backward fill</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="43281">Time series data also show missing values.</st> <st c="43325">To
    impute</st> <st c="43334">missing data in time series, we use specific methods.</st>
    <st c="43389">Forward fill imputation involves filling missing values in a dataset
    with the most recent non-missing value that precedes it in the data sequence.</st>
    <st c="43536">In other words, we carry forward the last seen value to the next
    valid value.</st> <st c="43614">Backward fill imputation involves filling missing
    values with the next non-missing value that follows it in the data sequence.</st>
    <st c="43741">In other words, we carry the last valid value backward to its preceding</st>
    <st c="43813">valid value.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43825">In this recipe, we will replace missing data in a time series
    with forward and</st> <st c="43905">backward fills.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43920">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="43936">Let’s begin by importing the required</st> <st c="43974">libraries
    and time</st> <st c="43994">series dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44009">Let’s import</st> `<st c="44023">pandas</st>` <st c="44029">and</st>
    `<st c="44034">matplotlib</st>`<st c="44044">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="44098">Let’s load the air passengers dataset that we described in the</st>
    *<st c="44162">Technical requirements</st>* <st c="44184">section and display
    the first five rows of the</st> <st c="44232">time series:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="44341">We see the time series in the</st> <st c="44372">following output:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="44478">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44483">You can determine the percentage of missing data by</st> <st c="44536">executing</st>
    `<st c="44546">df.isnull().mean()</st>`<st c="44564">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44565">Let’s plot the time series to spot any obvious</st> <st c="44613">data
    gaps:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="44769">The previous code returns</st> <st c="44795">the following plot,
    where we see intervals of time where data</st> <st c="44858">is missing:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Time series data showing missing values](img/B22396_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="44967">Figure 1.5 – Time series data showing missing values</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45019">Let’s impute missing data by carrying the last observed value
    in any interval to the next</st> <st c="45110">valid value:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="45146">You can verify the absence of missing data by</st> <st c="45193">executing</st>
    `<st c="45203">df_imputed.isnull().sum()</st>`<st c="45228">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="45229">Let’s now plot the complete dataset and overlay as a dotted line
    the values used for</st> <st c="45315">the imputation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="45560">The previous code returns the following plot, where we see the
    values used to replace</st> `<st c="45647">nan</st>` <st c="45650">as dotted lines</st>
    <st c="45666">overlaid in between the continuous time</st> <st c="45707">series
    lines:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Time series data where missing values were replaced by the last
    seen observations (dotted line)](img/B22396_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="45819">Figure 1.6 – Time series data where missing values were replaced
    by the last seen observations (dotted line)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45927">Alternatively, we can impute missing data using</st> <st c="45976">backward
    fill:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="46014">If we plot the imputed dataset and overlay the imputation values
    as we did in</st> *<st c="46093">step 5</st>*<st c="46099">, we’ll see the</st>
    <st c="46115">following plot:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Time series data where missing values were replaced by backward
    fill (dotted line)](img/B22396_01_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="46225">Figure 1.7 – Time series data where missing values were replaced
    by backward fill (dotted line)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46320">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46325">The heights of the values used in the imputation are different
    in</st> *<st c="46392">Figures 1.6 and 1.7</st>*<st c="46411">. In</st> *<st c="46416">Figure
    1</st>**<st c="46424">.6</st>*<st c="46426">, we carry the last value forward,
    hence the height is lower.</st> <st c="46488">In</st> *<st c="46491">Figure 1</st>**<st
    c="46499">.7</st>*<st c="46501">, we carry the next value backward, hence the
    height</st> <st c="46554">is higher.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46564">We’ve now obtained complete</st> <st c="46592">datasets that we
    can use for time series analysis</st> <st c="46643">and modeling.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46656">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`<st c="46672">pandas</st>` `<st c="46679">ffill()</st>` <st c="46687">takes
    the last seen value in any temporal gap in a time series and propagates it forward
    to the next observed value.</st> <st c="46805">Hence, in</st> *<st c="46815">Figure
    1</st>**<st c="46823">.6</st>* <st c="46825">we see the dotted overlay corresponding
    to the imputation values at the height of the last</st> <st c="46917">seen observation.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="46934">pandas</st>` `<st c="46941">bfill()</st>` <st c="46949">takes
    the next valid value in any temporal gap in a time series and propagates it backward
    to the previously observed value.</st> <st c="47075">Hence, in</st> *<st c="47085">Figure
    1</st>**<st c="47093">.7</st>* <st c="47095">we see the dotted overlay corresponding
    to the imputation values at the height of the next observation in</st> <st c="47202">the
    gap.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47210">By default,</st> `<st c="47223">ffill()</st>` <st c="47230">and</st>
    `<st c="47235">bfill()</st>` <st c="47242">will impute all values between valid
    observations.</st> <st c="47294">We can restrict the imputation to a maximum number
    of data points in any interval by setting a limit, using the</st> `<st c="47406">limit</st>`
    <st c="47411">parameter in both methods.</st> <st c="47439">For example,</st>
    `<st c="47452">ffill(limit=10)</st>` <st c="47467">will only replace</st> <st
    c="47485">the first 10 data points in</st> <st c="47514">any gap.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47522">Carrying out interpolation</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="47549">We can impute missing</st> <st c="47571">data in time series by
    using interpolation between two non-missing data points.</st> <st c="47652">Interpolation
    is the estimation of one or more values in a range by means of a function.</st>
    <st c="47741">In linear interpolation, we fit a linear function between the last
    observed value and the next valid point.</st> <st c="47849">In spline interpolation,
    we fit a low-degree polynomial between the last and next observed values.</st>
    <st c="47948">The idea of using interpolation is to obtain better estimates of
    the</st> <st c="48017">missing data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48030">In this recipe, we’ll carry out linear and spline interpolation
    in a</st> <st c="48100">time series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48112">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="48128">Let’s begin by importing the required libraries and time</st>
    <st c="48186">series dataset.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48201">Let’s import</st> `<st c="48215">pandas</st>` <st c="48221">and</st>
    `<st c="48226">matplotlib</st>`<st c="48236">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="48290">Let’s load the time series data described in the</st> *<st c="48340">Technical</st>*
    *<st c="48350">requirements</st>* <st c="48362">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="48451">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48456">You can plot the time series to find data gaps as we did in</st>
    *<st c="48517">step 3</st>* <st c="48523">of the</st> *<st c="48531">Implementing
    forward and backward</st>* *<st c="48565">fill</st>* <st c="48569">recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48577">Let’s impute the missing</st> <st c="48602">data by</st> <st c="48611">linear
    interpolation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="48677">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48682">If the time intervals between rows are not uniform then the</st>
    `<st c="48743">method</st>` <st c="48749">should be set to</st> `<st c="48767">time</st>`
    <st c="48771">to achieve a</st> <st c="48785">linear fit.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48796">You can verify the absence of missing data by</st> <st c="48843">executing</st>
    `<st c="48853">df_imputed.isnull().sum()</st>`<st c="48878">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48879">Let’s now plot the complete dataset and overlay as a dotted line
    the values used for</st> <st c="48965">the imputation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="49210">The previous code returns the following plot, where we see the
    values used to replace</st> `<st c="49297">nan</st>` <st c="49300">as dotted lines
    in between the continuous line of the</st> <st c="49355">time series:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Time series data where missing values were replaced by linear
    interpolation between the last and next valid data points (dotted line)](img/B22396_01_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="49462">Figure 1.8 – Time series data where missing values were replaced
    by linear interpolation between the last and next valid data points (dotted line)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49608">Alternatively, we can impute</st> <st c="49637">missing data by
    doing spline interpolation.</st> <st c="49682">We’ll use a polynomial of the</st>
    <st c="49712">second degree:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="49780">If we plot the imputed dataset and overlay the imputation values
    as we did in</st> *<st c="49859">step 4</st>*<st c="49865">, we’ll see the</st>
    <st c="49881">following plot:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Time series data where missing values were replaced by fitting
    a second-degree polynomial between the last and next valid data points (dotted
    line)](img/B22396_01_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="49997">Figure 1.9 – Time series data where missing values were replaced
    by fitting a second-degree polynomial between the last and next valid data points
    (dotted line)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50157">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50162">Change the degree of the polynomial used in the interpolation
    to see how the replacement</st> <st c="50252">values vary.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50264">We’ve now obtained complete</st> <st c="50292">datasets that we
    can use for analysis</st> <st c="50331">and modeling.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50344">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`<st c="50360">pandas</st>` `<st c="50367">interpolate()</st>` <st c="50381">fills
    missing values in a range by using an interpolation method.</st> <st c="50448">When
    we set the</st> `<st c="50464">method</st>` <st c="50470">to</st> `<st c="50474">linear</st>`<st
    c="50480">,</st> `<st c="50482">interpolate()</st>` <st c="50495">treats all data
    points as equidistant and fits a line between the last and next valid points in
    an interval with</st> <st c="50609">missing data.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50622">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50627">If you want to perform linear interpolation, but your data points
    are not equally distanced, set</st> `<st c="50725">method</st>` <st c="50731">to</st>
    `<st c="50735">time</st>`<st c="50739">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50740">We then performed spline interpolation with a second-degree polynomial
    by setting</st> `<st c="50823">method</st>` <st c="50829">to</st> `<st c="50833">spline</st>`
    <st c="50839">and</st> `<st c="50844">order</st>` <st c="50849">to</st> `<st c="50853">2</st>`<st
    c="50854">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="50855">pandas</st>` `<st c="50862">interpolate()</st>` <st c="50876">uses</st>
    `<st c="50882">scipy.interpolate.interp1d</st>` <st c="50908">and</st> `<st c="50913">scipy.interpolate.UnivariateSpline</st>`
    <st c="50947">under the hood, and can therefore implement other interpolation</st>
    <st c="51011">methods.</st> <st c="51021">Check out pandas</st> <st c="51037">documentation
    for more details</st> <st c="51069">at</st> [<st c="51072">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html</st>](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html)<st
    c="51164">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51165">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="51174">While interpolation aims to get better estimates of the missing
    data compared to forward and backward fill, these estimates may still not be accurate
    if the times series show strong trend and seasonality.</st> <st c="51380">To obtain
    better estimates of the missing data in these types of time series, check out
    time series decomposition</st> <st c="51493">followed by interpolation in the</st>
    *<st c="51527">Feature Engineering for Time Series Course</st>* <st c="51569">at</st>
    [<st c="51573">https://www.trainindata.com/p/feature-engineering-for-forecasting</st>](https://www.trainindata.com/p/feature-engineering-for-forecasting)<st
    c="51638">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51639">Performing multivariate imputation by chained equations</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="51695">Multivariate imputat</st><st c="51716">ion</st> <st c="51720">methods,
    as opposed</st> <st c="51740">to univariate imputation, use multiple variables
    to estimate the missing values.</st> **<st c="51822">Multivariate Imputation by
    Chained Equations</st>** <st c="51866">(</st>**<st c="51868">MICE</st>**<st c="51872">)
    models each variable with missing</st> <st c="51908">values as a function of the
    remaining variables in the dataset.</st> <st c="51973">The output of that function
    is used to replace</st> <st c="52020">missing data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52033">MICE involves the</st> <st c="52052">following steps:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52068">First, it performs a simple univariate imputation to every variable
    with missing data.</st> <st c="52156">For example,</st> <st c="52169">median imputation.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="52187">Next, it selects one specific variable, say,</st> `<st c="52233">var_1</st>`<st
    c="52238">, and sets the missing values back</st> <st c="52273">to missing.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="52284">It trains a model to predict</st> `<st c="52314">var_1</st>` <st
    c="52319">using the other variables as</st> <st c="52349">input features.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="52364">Finally, it replaces the missing values of</st> `<st c="52408">var_1</st>`
    <st c="52413">with the output of</st> <st c="52433">the model.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="52443">MICE repeats</st> *<st c="52457">steps 2</st>* <st c="52464">to</st>
    *<st c="52468">4</st>* <st c="52469">for each of the</st> <st c="52486">remaining
    variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52506">An imputation cycle</st> <st c="52526">concludes once all the
    variables</st> <st c="52559">have been modeled.</st> <st c="52579">MICE carries
    out multiple imputation cycles, typically 10\.</st> <st c="52638">That is, we
    repeat</st> *<st c="52657">steps 2</st>* <st c="52664">to</st> *<st c="52668">4</st>*
    <st c="52669">for each variable 10 times.</st> <st c="52698">The idea is th</st><st
    c="52712">at by the end of the cycles, we should have found the best possible
    estimates of the missing data for</st> <st c="52815">each variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52829">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52834">Multivariate imputation</st> <st c="52858">can be a useful alternative
    to univariate imputation in situations where we don’t want to distort the variable
    distributions.</st> <st c="52985">Multivariate imputation is also useful when
    we are interested in having good estimates o</st><st c="53073">f the</st> <st
    c="53080">missing data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53093">In th</st><st c="53099">is recipe, we will implement MICE</st>
    <st c="53134">using scikit-learn.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53153">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="53169">To begin the recipe, let’s import the required libraries and load</st>
    <st c="53236">the data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53245">Let’s import the required Python libraries, classes,</st> <st
    c="53299">and functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="53590">Let’s load some numerical</st> <st c="53616">variables from the
    dataset</st> <st c="53643">described in the</st> *<st c="53661">Technical</st>*
    *<st c="53671">requirements</st>* <st c="53683">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="53821">Let’s divide the data into train and</st> <st c="53859">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="54001">Let’s create a MICE imputer using Bayes regression, specifying
    the number of iteration cycles and setting</st> `<st c="54108">random_state</st>`
    <st c="54120">for reproducibility:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="54259">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="54264">IterativeImputer()</st>` <st c="54283">contains other useful
    arguments.</st> <st c="54317">For example, we can specify the first imputation
    strategy using the</st> `<st c="54385">initial_strategy</st>` <st c="54401">parameter.</st>
    <st c="54413">We can choose from the mean, median, mode, or arbitrary imputation.</st>
    <st c="54481">We can also specify how we want to cycle over the variables, either
    randomly or from the one with the fewest missing values to the one with</st> <st
    c="54621">the most.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54630">Let’s fit</st> `<st c="54641">IterativeImputer()</st>` <st c="54659">so
    that it trains</st> <st c="54677">the estimators</st> <st c="54692">to predict
    the missing values in</st> <st c="54726">each variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="54761">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54766">We can use any regression model to estimate the missing data</st>
    <st c="54828">with</st> `<st c="54833">IterativeImputer()</st>`<st c="54851">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54852">Finally, let’s fill in the missing values in both the train and</st>
    <st c="54917">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="55003">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55008">To corroborate the lack of missing data, we can</st> <st c="55057">execute</st>
    `<st c="55065">X_train_t.isnull().sum()</st>`<st c="55089">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55090">To wrap up the recipe, let’s impute the variables with a simple
    univariate imputation method and compare the effect on the</st> <st c="55214">variables’
    distribution.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55238">Let’s set up scikit-learn’s</st> `<st c="55267">SimpleImputer()</st>`
    <st c="55282">to perform mean imputation, and then transform</st> <st c="55330">the
    datasets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="55517">Let’s now make a histogram</st> <st c="55544">of the</st> `<st
    c="55552">A3</st>` <st c="55554">variable</st> <st c="55563">after MICE imputation,
    followed by a histogram of the same variable after</st> <st c="55638">mean imputation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="56070">In the following plot, we see that mean imputation distorts the
    variable distribution, with more observations toward the</st> <st c="56192">mean
    value:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.10 –  Histogram of variable A3 after mice imputation (top) or mean
    imputation (bottom), showing the distortion in the variable distribution caused
    by the latter](img/B22396_01_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="56285">Figure 1.10 – Histogram of variable A3 after mice imputation (top)
    or mean imputation (bottom), showing the distortion in the variable distribution
    caused by the latter</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56453">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="56469">In this recipe, we performed</st> <st c="56498">multivariate imputation</st>
    <st c="56522">using</st> `<st c="56529">IterativeImputer()</st>` <st c="56547">from</st>
    `<st c="56553">scikit-learn</st>`<st c="56565">. When we fit the model,</st> `<st
    c="56590">IterativeImputer()</st>` <st c="56608">carried out the steps that we
    described in the introduction of the recipe.</st> <st c="56684">That is, it imputed
    all variables with the mean.</st> <st c="56733">Then it selected one variable
    and set its missing values back to missing.</st> <st c="56807">And finally, it
    fitted a Bayes regressor to estimate that variable based on the others.</st> <st
    c="56895">It repeated this procedure for each variable.</st> <st c="56941">That
    was one cycle of imputation.</st> <st c="56975">We set it to repeat this process
    10 times.</st> <st c="57018">By the end of this procedure,</st> `<st c="57048">IterativeImputer()</st>`
    <st c="57066">had one Bayes regressor trained to predict the values of each variable
    based on the other variables in the dataset.</st> <st c="57183">With</st> `<st
    c="57188">transform()</st>`<st c="57199">, it uses the predictions of these Bayes
    models to impute the</st> <st c="57261">missing d</st><st c="57270">ata.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="57275">Iter</st><st c="57280">ativeImputer()</st>` <st c="57295">can
    only impute missing data in numerical variables based on numerical variables.</st>
    <st c="57378">If you want to use categorical variables as input, you need to encode
    them first.</st> <st c="57460">However, keep in mind</st> <st c="57481">that it
    will only carry</st> <st c="57506">out regression.</st> <st c="57522">Hence it
    is</st> <st c="57534">not suitable to estimate missing data in discrete or</st>
    <st c="57587">categorical variables.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57609">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="57618">To learn more about MICE, take a look at the</st> <st c="57664">following
    resources:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57684">A multivariate technique for multiplying imputing missing values
    using a sequence of regression</st> <st c="57781">models:</st> [<st c="57789">https://www.researchgate.net/publication/244959137</st>](https://www.researchgate.net/publication/244959137_A_Multivariate_Technique_for_Multiply_Imputing_Missing_Values_Using_a_Sequence_of_Regression_Models)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="57839">Multiple Imputation by Chained Equations: W</st><st c="57883">hat
    is it and how does it</st>* *<st c="57910">work?</st>*<st c="57915">:</st> [<st
    c="57918">https://www.jstatsoft.org/article/download/v045i03/550</st>](https://www.jstatsoft.org/article/download/v045i03/550)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="57972">Estimating missing data with nearest neighbors</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="58019">Imputation with</st> **<st c="58036">K-Nearest Neighbors</st>**
    <st c="58055">(</st>**<st c="58057">KNN</st>**<st c="58060">) involves estimating
    missing values</st> <st c="58097">in a dataset by considering</st> <st c="58125">the
    values of their nearest</st> <st c="58154">neighbors, where similarity between
    data points is determined based on a distance metric, such as the Euclidean distance.</st>
    <st c="58276">It assigns the missing value the average of the nearest neighbors’
    values, weighted by</st> <st c="58363">their distance.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58378">Consider the following data set containing 4 variables (columns)
    and 11 observations (rows).</st> <st c="58472">We want to impute the dark value
    in the fifth row of the second variable.</st> <st c="58546">First, we find the
    row’s k-nearest neighbors, where</st> *<st c="58598">k=3</st>* <st c="58601">in
    our example, and they are highlighted by the rectangular boxes (middle panel).</st>
    <st c="58684">Next, we take the average value shown by the closest neighbors for</st>
    <st c="58751">variable 2.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – Diagram showing a value to impute (dark box), the three closest
    rows to the value to impute (square boxes), and the values considered to take
    the average for the imputation](img/B22396_01_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="58870">Figure 1.11 – Diagram showing a value to impute (dark box), the
    three closest rows to the value to impute (square boxes), and the values considered
    to take the average for the imputation</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59056">The value for the imputation</st> <st c="59085">is given by (value1
    × w1 + value2 × w2 + value3 × w3) / 3, where w1, w2, and w3 are proportional to
    the distance</st> <st c="59198">of the neighbo</st><st c="59213">r to the data</st>
    <st c="59228">to impute.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59238">In this recipe, we will perform KNN imputation</st> <st c="59286">using
    scikit-learn.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59305">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="59321">To proceed with the recipe, let’s import the required libraries
    and prepare</st> <st c="59398">the data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59407">Let’s import the required libraries, classes,</st> <st c="59454">and
    functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="59611">Let’s load the dataset described in the</st> *<st c="59652">Technical
    requirements</st>* <st c="59674">section (only some</st> <st c="59694">numerical
    variables):</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="59847">Let’s divide the data into train and</st> <st c="59885">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="60026">Let’s set up the imputer</st> <st c="60051">to replace missing
    data</st> <st c="60075">with the weighted mean of its closest</st> <st c="60114">five
    neighbors:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="60219">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60224">The replacement values can be calculated as the uniform mean of
    the k-nearest neighbors, by setting</st> `<st c="60325">weights</st>` <st c="60332">to</st>
    `<st c="60336">uniform</st>` <st c="60343">or as the weighted average, as we do
    in the recipe.</st> <st c="60396">The weight is based on the distance of the neighbor
    to the observation to impute.</st> <st c="60478">The nearest neighbors carry</st>
    <st c="60506">more weight.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60518">Find the</st> <st c="60528">nearest neighbors:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="60567">Replace the missing values with the weighted mean of the values
    shown by</st> <st c="60641">the neighbors:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="60733">The result is a</st> <st c="60749">pandas DataFrame with the missing</st>
    <st c="60784">data replaced.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60798">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="60814">In this recipe, we replaced</st> <st c="60842">missing data with
    the average</st> <st c="60872">value shown by each observation’s k-nearest neighbors.</st>
    <st c="60928">We set up</st> `<st c="60938">KNNImputer()</st>` <st c="60950">to
    find each observation’s five closest neighbors based on the Euclidean distance.</st>
    <st c="61034">The replacement values were estimated as the weighted average of
    the values shown by the five closest neighbors for the variable to impute.</st>
    <st c="61174">With</st> `<st c="61179">transform()</st>`<st c="61190">, the imputer
    calculated the replacement value and replaced the</st> <st c="61254">missing data.</st>
  prefs: []
  type: TYPE_NORMAL
