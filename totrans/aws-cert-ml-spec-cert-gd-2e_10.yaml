- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you explored various aspects of Amazon SageMaker, including
    different instances, data preparation in Jupyter Notebook, model training with
    built-in algorithms, and crafting custom code for training and inference. Now,
    your focus shifts to diverse model deployment choices using AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: If you are navigating the landscape of model deployment on AWS, understanding
    the options is crucial. One standout service is Amazon SageMaker – a fully managed
    solution that streamlines the entire **machine learning** (**ML**) life cycle,
    especially when it comes to deploying models. There are several factors that influence
    the model deployment options. As you go ahead in this chapter you will learn different
    options to deploy models using SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Factors influencing model deployment options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the primary factors that play a crucial role in determining a model
    deployment option:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability requirements**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High traffic:** Imagine you are developing a recommendation system for a
    popular e-commerce platform expecting fluctuating traffic throughout the day.
    If the application anticipates high traffic and varying loads, services such as
    Amazon SageMaker with autoscaling capabilities or AWS Lambda may be preferable.
    This is crucial to maintain performance during peak hours.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time versus** **batch inference**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time inference**: Consider a fraud detection system for a financial
    institution where immediate decisions are essential for transaction approval or
    denial. For such real-time predictions, services such as Amazon SageMaker and
    AWS Lambda are suitable. For fraud detection, these services provide low-latency
    responses, enabling quick decisions on the legitimacy of transactions. Real-time
    transactions trigger immediate predictions through SageMaker’s inference endpoints
    in this case.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch inference:** In a healthcare setting, you may need to process a large
    volume of patient data periodically to update predictive models for disease diagnosis.
    This is a batch processing use case, and a combination of SageMaker and services
    such as Amazon S3 can be employed. You can efficiently handle large datasets,
    perform periodic model updates, and ensure that predictions align with the latest
    information, which is crucial for maintaining accuracy in healthcare predictions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Infrastructure** **management complexity**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Managed deployment versus custom deployment:** Suppose you are developing
    a computer vision application for analyzing satellite images, requiring specialized
    configurations and dependencies. In cases where custom configurations are necessary,
    opting for custom deployments using EC2 instances allows more control over the
    infrastructure. SageMaker’s managed services are ideal for scenarios where you
    prioritize ease of management and do not want to delve into intricate infrastructure
    details.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost considerations**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pay-per-use:** Consider a weather forecasting application where the computational
    demand varies based on weather events. AWS Lambda’s pay-per-use model is advantageous
    in situations where the workload fluctuates. You pay only for the compute time
    consumed, making it cost-effective for applications with sporadic, unpredictable
    usage patterns compared to alternatives with fixed costs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SageMaker deployment options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon SageMaker offers diverse deployment options to deploy ML models effectively.
    In this section, you will explore different ways of deploying models using SageMaker,
    providing technology solutions with scenarios and examples.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time endpoint deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this **scenario**, you have a trained image classification model, and you
    want to deploy it to provide real-time predictions for incoming images.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a SageMaker model and deploy it to a real-time endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Train your model using SageMaker training jobs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a SageMaker model from the trained model artifacts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the model to a real-time endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example code snippet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Batch transform job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this **scenario**, you have a large dataset, and you want to perform batch
    inference on the entire dataset using a trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use SageMaker batch transform to process the entire dataset in a batch.
  prefs: []
  type: TYPE_NORMAL
- en: Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a SageMaker transformer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the batch transform job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example code snippet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Multi-model endpoint deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this **scenario**, you have multiple versions of a model, and you want to
    deploy them on a single endpoint for A/B testing or gradual rollout.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use SageMaker multi-model endpoints to deploy and manage multiple models on
    a single endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create and train multiple models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a SageMaker multi-model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the multi-model to an endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example code snippet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Endpoint autoscaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this **scenario**, your application experiences varying workloads, and you
    want to automatically adjust the number of instances based on traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enable auto-scaling for SageMaker endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Configure the SageMaker endpoint to use autoscaling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the minimum and maximum instance counts based on the expected workload.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example code snippet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Serverless APIs with AWS Lambda and SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this **scenario**, you want to create a serverless API using AWS Lambda to
    interact with your SageMaker model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use AWS Lambda to invoke the SageMaker endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create an AWS Lambda function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Integrate the Lambda function with the SageMaker endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example code snippet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In the realm of deploying ML models, Amazon SageMaker emerges as a robust choice.
    Its managed environment, scalability features, and seamless integration with other
    AWS services make it an efficient and reliable solution for businesses navigating
    diverse deployment scenarios. Whether you are anticipating high traffic or need
    a hassle-free deployment experience, SageMaker is your ally in the journey of
    model deployment excellence. In the next section, you will learn about creating
    pipelines with Lambda functions.
  prefs: []
  type: TYPE_NORMAL
- en: Creating alternative pipelines with Lambda Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Indeed, SageMaker is an awesome platform that you can use to create training
    and inference pipelines. However, you can always work with different services
    to come up with similar solutions. One of these services, which you will learn
    about next, is known as **Lambda functions**.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda is a serverless compute service where you can run a function as a
    service. In other words, you can concentrate your efforts on just writing your
    function. Then, you just need to tell AWS how to run it (that is, the environment
    and resource configurations), so all the necessary resources will be provisioned
    to run your code and then discontinued once it is completed.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout [*Chapter 3*](B21197_03.xhtml#_idTextAnchor337), *AWS Services for
    Data Migration and Processing*, you explored how Lambda functions integrate with
    many different services, such as Kinesis and AWS Batch. Indeed, AWS did a very
    good job of integrating Lambda with 140+ services (and the list is constantly
    increasing). That means that when you work with a specific AWS service, you will
    remember that it is likely to integrate with Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to bear this in mind because Lambda functions can really expand
    your possibilities to create scalable and integrated architectures. For example,
    you can trigger a Lambda function when a file is uploaded to S3 in order to preprocess
    your data before loading it to Redshift. Alternatively, you can create an API
    that triggers a Lambda function at each endpoint execution. Again, the possibilities
    are endless with this powerful service.
  prefs: []
  type: TYPE_NORMAL
- en: It is also useful to know that you can write your function in different programming
    languages, such as Node.js, Python, Go, Java, and more. Your function does not
    necessarily have to be triggered by another AWS service – that is, you can trigger
    it manually for your web or mobile application, for example.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to deployment, you can upload your function as a ZIP file or as
    a container image. Although this is not ideal for an automated deployment process,
    coding directly into the AWS Lambda console is also possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with any other service, this one also has some downsides that you should
    be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory allocation for your function**: This ranges from 128 MB to 10,240
    MB (AWS has recently increased this limit from 3 GB to 10 GB, as stated previously)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Function timeout**: This is a maximum of 900 seconds (15 minutes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Function layer**: This is a maximum of five layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Burst concurrency**: This is from 500 to 3,000, depending on the AWS Region'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment package size**: This is 250 MB unzipped, including layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container image code package size**: This is 10 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Available space in the /tmp directory**: This is 512 MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before opting for Lambda functions, make sure these restrictions fit your use
    case. By bringing Lambda functions closer to your scope of alternative pipelines
    for SageMaker, you can leverage one potential use of Lambda, which is to create
    inference pipelines for the models.
  prefs: []
  type: TYPE_NORMAL
- en: As you know, SageMaker has a very handy `.deploy()` method that will create
    endpoints for model inference. This is so that you can call to pass the input
    data to receive predictions back. Here, you can create this inference endpoint
    by using the API gateway and Lambda functions.
  prefs: []
  type: TYPE_NORMAL
- en: If you do not need an inference endpoint and you just want to make predictions
    and store results somewhere (in a batch fashion), then all you need is a Lambda
    function, which is able to fetch the input data, instantiate the model object,
    make predictions, and store the results in the appropriate location. Of course,
    it does this by considering all the limitations that you discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Alright, now that you have a good background about Lambda and some use cases,
    you can take a look at the most important configurations that you should be aware
    of for the exam.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and configuring a Lambda Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, you should know that you can create a Lambda function in different
    ways, such as via the AWS CLI (a Lambda API reference), the AWS Lambda console,
    or even deployment frameworks (for example, *the* *serverless framework*).
  prefs: []
  type: TYPE_NORMAL
- en: Serverless frameworks are usually provider and programming language-independent.
    In other words, they usually allow you to choose where you want to deploy a serverless
    infrastructure from a varied list of cloud providers and programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The concept of serverless architecture is not specific to AWS. In fact, many
    cloud providers offer other services similar to AWS Lambda functions. That is
    why these serverless frameworks have been built – to help developers and engineers
    to deploy their services, wherever they want, including AWS. This is unlikely
    to come up in your exam, but it is something that you should know so that you
    are aware of different ways in which to solve your challenges as a data scientist
    or data engineer.
  prefs: []
  type: TYPE_NORMAL
- en: Since you want to pass the AWS Certified Machine Learning Specialty exam, here,
    you will walk through the AWS Lambda console. This is so that you become more
    familiar with its interface and the most important configuration options.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you navigate to the Lambda console and request a new Lambda function,
    AWS will provide you with some starting options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Author from scratch**: This is if you want to create your function from scratch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use a blueprint**: This is if you want to create your function from sample
    code and a configuration preset for common use cases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container image**: This is if you want to select a container image to deploy
    your function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Browse serverless app repository**: This is if you want to deploy a sample
    Lambda application from the AWS Serverless Application Repository'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Starting from scratch, the next step is to set up your Lambda configurations.
    AWS splits these configurations between basic and advanced settings. In the basic
    configuration, you will set your function name, runtime environment, and permissions.
    *Figure 10**.1* shows these configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Creating a new Lambda function from the AWS Lambda console](img/B21197_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Creating a new Lambda function from the AWS Lambda console
  prefs: []
  type: TYPE_NORMAL
- en: Here, you have a very important configuration that you should remember during
    your exam – the **execution role**. Your Lambda function might need permissions
    to access other AWS resources, such as S3, Redshift, and more. The execution role
    grants permissions to your Lambda function so that it can access resources as
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: You have to remember that your VPC and security group configurations will also
    interfere with how your Lambda function runs. For example, if you want to create
    a function that needs internet access to download something, then you have to
    deploy this function in a VPC with internet access. The same logic applies to
    other resources, such as access to relational databases, Kinesis, and Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, to properly configure a Lambda function, you have to, at least,
    write its code, set the execution role, and make sure the VPC and security group
    configurations match your needs. Next, you will take a look at other configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Completing your configurations and deploying a Lambda function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once your Lambda is created in the AWS console, you can set additional configurations
    before deploying the function. One of these configurations is the event trigger.
    As mentioned earlier, your Lambda function can be triggered from a variety of
    services or even manually.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A very common example of a trigger is **Amazon EventBridge**. This is an AWS
    service where you can schedule the execution of your function.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the event trigger you choose, your function will have access to
    different event metadata. For example, if your function is triggered by a `PUT`
    event on S3 (for example, someone uploads a file to a particular S3 bucket), then
    your function will receive the metadata associated with this event – for example,
    the bucket name and object key. Other types of triggers will give you different
    types of event metadata!
  prefs: []
  type: TYPE_NORMAL
- en: 'You have access to that metadata through the event parameter that belongs to
    the signature of the entry point of your function. Not clear enough? You will
    now look at how your function code should be declared, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Here, `lambda_handler` is the method that represents the entry point of your
    function. When it is triggered, this method will be called, and it will receive
    the event metadata associated with the event trigger (through the `event` parameter).
    That is how you have access to the information associated with the underlying
    event that has triggered your function! The `event` parameter is a JSON-like object.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to test your function but do not want to trigger it directly from
    the underlying event, that is no problem; you can use **test events**. They simulate
    the underlying event by preparing a JSON object that will be passed to your function.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.2* shows a very intuitive example. Suppose you have created a
    function that is triggered when a user uploads a file to S3, and now, you want
    to test your function. You can either upload a file to S3 (which forces the trigger)
    or create a test event.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By creating a test event, you can prepare a JSON object that simulates the
    **S3-put** event and then pass this object to your function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Creating a test event from the Lambda console](img/B21197_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Creating a test event from the Lambda console
  prefs: []
  type: TYPE_NORMAL
- en: 'Another type of configuration that you can set is an **environment variable**,
    which will be available on your function. *Figure 10**.3* shows how to add environment
    variables in a Lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Adding environment variables to a Lambda function](img/B21197_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Adding environment variables to a Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: 'You can always come back to these basic settings to make adjustments as necessary.
    *Figure 10**.4* shows what you will find in the basic settings section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Changing the basic settings of a Lambda function](img/B21197_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Changing the basic settings of a Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: In terms of monitoring, by default, Lambda functions produce a `Lambda function
    monitoring` section and clicking on **View logs** **in CloudWatch**.
  prefs: []
  type: TYPE_NORMAL
- en: In CloudWatch, each Lambda function will have a **log group** and, inside that
    log group, many **log streams**. Log streams store the execution logs of the associated
    function. In other words, a log stream is a sequence of logs that share the same
    source, which, in this case, is your Lambda function. A log group is a group of
    log streams that share the same retention, monitoring, and access control settings.
  prefs: []
  type: TYPE_NORMAL
- en: You are now reaching the end of this section, but not the end of this topic
    on Lambda functions. As mentioned earlier, this AWS service has a lot of use cases
    and integrates with many other services. In the next section, you will look at
    another AWS service that will help orchestrate executions of Lambda functions.
    This is known as **AWS** **Step Functions**.
  prefs: []
  type: TYPE_NORMAL
- en: Working with step functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Step Functions is an AWS service that allows you to create workflows to orchestrate
    the execution of Lambda functions. This is so that you can connect them in a sort
    of event sequence, known as **steps**. These steps are grouped in a **state machine**.
  prefs: []
  type: TYPE_NORMAL
- en: Step Functions incorporates retry functionality so that you can configure your
    pipeline to proceed only after a particular step has succeeded. The way you set
    these retry configurations is by creating a **retry policy**.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Just like the majority of AWS services, AWS Step Functions also integrates with
    other services, not only AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a state machine is relatively simple. All you have to do is navigate
    to the AWS Step Functions console and then create a new state machine. On the
    **Create state machine** page, you can specify whether you want to create your
    state machine from scratch or from a template, or whether you just want to run
    a sample project.
  prefs: []
  type: TYPE_NORMAL
- en: AWS will help you with this state machine creation, so even if you choose to
    create it from scratch, you will find code snippets for a various list of tasks,
    such as AWS Lambda invocation, SNS topic publication, and running Athena queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of demonstration, you will now create a very simple, but still
    helpful, example of how to use Step Functions to execute a Lambda function with
    the `retry` option activated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, you created a state machine with two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Invoke a Lambda function**: This will start the execution of your underlying
    Lambda'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execute** *Example*: This is a simple pass task just to show you how to connect
    a second step in the pipeline'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the first step, you have also set up a retry policy, which will try to re-execute
    this task if there are any failures. You set up the interval (in seconds) to try
    again and to show the number of attempts. *Figure 10**.5* shows the state machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – The state machine](img/B21197_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – The state machine
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will explore various autoscaling scenarios and different
    ways to handle them.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling applications with SageMaker deployment and AWS Autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoscaling is a crucial aspect of deploying ML models in production environments,
    ensuring that applications can handle varying workloads efficiently. Amazon SageMaker,
    combined with AWS Auto Scaling, provides a robust solution for automatically adjusting
    resources based on demand. In this section, you will explore different scenarios
    where autoscaling is essential and how to achieve it, using SageMaker model deployment
    options and AWS Auto Scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 1 – Fluctuating inference workloads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a retail application, the number of users making product recommendation requests
    can vary throughout the day, with peak loads during specific hours.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implement autoscaling for SageMaker real-time endpoints to dynamically adjust
    the number of instances, based on the inference request rate.
  prefs: []
  type: TYPE_NORMAL
- en: Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Configure the SageMaker endpoint to use autoscaling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up minimum and maximum instance counts based on expected workload variations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example code snippet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Scenario 2 – The batch processing of large datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performing batch inference on a large dataset periodically may lead to resource
    constraints if not managed dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Utilize AWS batch transform with SageMaker and configure autoscaling for efficient
    resource utilization during batch processing.
  prefs: []
  type: TYPE_NORMAL
- en: Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Set up an AWS batch transform job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable autoscaling for the underlying infrastructure to handle varying batch
    sizes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example code snippet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Scenario 3 – A multi-model endpoint with dynamic traffic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multiple models are deployed on a single endpoint for A/B testing, and the traffic
    distribution between models is dynamic.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Leverage SageMaker multi-model endpoints with autoscaling to handle varying
    traffic loads across different model versions.
  prefs: []
  type: TYPE_NORMAL
- en: Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create and deploy multiple models to a SageMaker multi-model endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable autoscaling to adjust the instance count based on traffic distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example code snippet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Scenario 4 – Continuous Model Monitoring with drift detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You monitor models for concept drift or data quality issues and automatically
    adjust resources if model performance degrades.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Integrate SageMaker Model Monitor with AWS CloudWatch alarms to trigger autoscaling
    when drift or degradation is detected.
  prefs: []
  type: TYPE_NORMAL
- en: Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Set up CloudWatch alarms to monitor model quality metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure autoscaling policies to trigger when specific alarm thresholds are
    breached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scaling applications with SageMaker model deployment options and AWS Auto Scaling
    provides a flexible and efficient solution for handling varying workloads and
    ensuring optimal resource utilization. By understanding the different scenarios
    requiring autoscaling and following the outlined steps, you can seamlessly integrate
    autoscaling into your ML deployment strategy, enhancing the scalability and reliability
    of your applications. In the next section, you will learn and explore different
    ways of securing AWS SageMaker applications.
  prefs: []
  type: TYPE_NORMAL
- en: Securing SageMaker applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As ML applications become integral to business operations, securing AWS SageMaker
    applications is paramount to safeguard sensitive data, maintain regulatory compliance,
    and prevent unauthorized access. In this section, you will first dive into the
    reasons for securing SageMaker applications and then explore different strategies
    to achieve security:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reasons to secure** **SageMaker applications**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data protection**: ML models trained on sensitive data, such as customer
    information or financial records, pose a significant security risk if not adequately
    protected. Securing SageMaker ensures that data confidentiality and integrity
    are maintained throughout the ML life cycle.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compliance requirements**: Industries such as healthcare and finance are
    subject to stringent data protection regulations. Securing SageMaker helps organizations
    comply with standards such as the **Health Insurance Portability and Accountability
    Act (HIPAA)** or the **General Data Protection Regulation (GDPR)**, avoiding legal
    ramifications and reputational damage.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preventing unauthorized access**: SageMaker instances and endpoints should
    only be accessible to authorized personnel. Unauthorized access can lead to data
    breaches or misuse of ML capabilities. Robust authentication mechanisms are crucial
    for preventing such security lapses.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model intellectual property protection**: ML models represent intellectual
    property. Securing SageMaker ensures that the models, algorithms, and methodologies
    developed remain confidential and are not susceptible to intellectual property
    theft or reverse engineering.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`from sagemaker import get_execution_role`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`{`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`from sagemaker import get_execution_role`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`from sagemaker.model_monitor import ModelQualityMonitor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Securing AWS SageMaker applications is not just a best practice; it is a critical
    imperative in the era of data-driven decision-making. By implementing robust security
    measures, such as utilizing VPC endpoints, IAM roles, encryption, and continuous
    monitoring, organizations can fortify their SageMaker applications against potential
    threats and ensure the integrity of their ML workflows. As SageMaker continues
    to empower businesses with ML capabilities, a proactive approach to security becomes
    indispensable for sustained success. You have now reached the end of this section
    and the end of this chapter. Next, take a look at a summary of what you have learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you dived into deploying ML models with Amazon SageMaker, exploring
    factors influencing deployment options. You looked at real-world scenarios and
    dissected them to try out hands-on solutions and code snippets for diverse use
    cases. You emphasized the crucial integration of SageMaker deployment with AWS
    Auto Scaling, dynamically adjusting resources based on workload variations. You
    focused on securing SageMaker applications, presenting practical strategies such
    as VPC endpoints, IAM roles, and encryption practices. Referring to the AWS documentation
    for clarifying any doubts is also the best option. It is always important to design
    your solutions in a cost-effective way, so exploring the cost-effective way to
    use these services is equally important as building the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How To Access These Resources
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/MLSC01E2_CH10](https://packt.link/MLSC01E2_CH10).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following **QR code** (*Figure 10**.6*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.6 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – QR code that opens Chapter Review Questions for logged-in users
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 10**.7*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Chapter Review Questions for Chapter 10](img/B21197_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Chapter Review Questions for Chapter 10
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  prefs: []
  type: TYPE_TB
- en: Table 10.1 – Sample timing practice drills on the online platform
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  prefs: []
  type: TYPE_NORMAL
