<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer094">
<h1 class="chapter-number" id="_idParaDest-227"><a id="_idTextAnchor230"/>14</h1>
<h1 id="_idParaDest-228"><a id="_idTextAnchor231"/>Synthetic-to-Real Domain Adaptation</h1>
<p>This chapter introduces you to a well-known issue that usually limits the usability of synthetic data, called the domain gap problem. In this chapter, you will learn various approaches to bridge this gap, which will help you to better leverage synthetic data. At the same time, the chapter discusses current state-of-the-art research on synthetic-to-real domain adaptation. Thus, you will learn which methods you may use for your own problems. Then, it represents the challenges and issues in this context to better comprehend <span class="No-Break">the problem.</span></p>
<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>The domain gap problem <span class="No-Break">in ML</span></li>
<li>Approaches for synthetic-to-real <span class="No-Break">domain adaptation</span></li>
<li>Synthetic-to-real domain adaptation – issues <span class="No-Break">and challenges</span></li>
</ul>
<h1 id="_idParaDest-229"><a id="_idTextAnchor232"/>The domain gap problem in ML</h1>
<p>In this section, we will understand <a id="_idIndexMarker577"/>what the domain gap is and why it is a problem in ML. The domain gap is one of the main issues that limit the usability of synthetic data in practice. It usually refers to the dissimilarity between the distributions and properties of data in two or more domains. It is not just associated with synthetic data. However, it is a common problem in ML. It is very common to notice a degradation in the performance of ML models when tested on similar but slightly different datasets. For more information, please refer to <em class="italic">Who is closer: A computational method for domain gap </em><span class="No-Break"><em class="italic">evaluation</em></span><span class="No-Break"> (</span><a href="https://doi.org/10.1016/j.patcog.2021.108293"><span class="No-Break">https://doi.org/10.1016/j.patcog.2021.108293</span></a><span class="No-Break">).</span></p>
<p>The main reasons for the domain gap between datasets can be linked to <span class="No-Break">the following:</span></p>
<ul>
<li>Sensitivity to <span class="No-Break">sensors’ variations</span></li>
<li>Discrepancy in class and <span class="No-Break">feature distributions</span></li>
<li><span class="No-Break">Concept drift</span></li>
</ul>
<p>Let’s discuss each of these points in <span class="No-Break">more detail.</span></p>
<h2 id="_idParaDest-230"><a id="_idTextAnchor233"/>Sensitivity to sensors’ variations</h2>
<p>In computer vision, your ML model<a id="_idIndexMarker578"/> may perform well on images captured using certain cameras, setups, and parameters but drastically fail under similar inputs captured with different cameras or using different parameters. For instance, your computer vision model may work well on videos captured from a first-person view but drastically fail on videos captured from a third-person view. Therefore, we can see that even the same task, such as action recognition, is usually studied with an emphasis on the person’s viewpoint, as in <em class="italic">First-person Activity Recognition by Modelling Subject-Action Relevance</em> (<a href="https://doi.org/10.1109/IJCNN55064.2022.9892547">https://doi.org/10.1109/IJCNN55064.2022.9892547</a>). Another example is the camera’s <strong class="bold">Field of View</strong> (<strong class="bold">FoV</strong>). Some computer vision tasks, such as semantic segmentation, are also <a id="_idIndexMarker579"/>studied under certain camera FoVs. A semantic segmentation method trained on the Cityscapes and Synscapes datasets, which are captured under a standard FoV, will fail at segmenting fisheye images captured by a super-wide fisheye lens. For an example, please refer to <em class="italic">FPDM: Fisheye Panoptic segmentation dataset for Door </em><span class="No-Break"><em class="italic">Monitoring</em></span><span class="No-Break"> (</span><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9959151"><span class="No-Break">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9959151</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-231"><a id="_idTextAnchor234"/>Discrepancy in class and feature distributions</h2>
<p>Discrepancy or inconsistency<a id="_idIndexMarker580"/> in the distribution of attributes, classes, and features is one of the main reasons for the domain gap between synthetic and real domains or even in the same real domain when you train and test on different datasets. This can be clearly observed when working on time series-based problems. Many times, the training data becomes outdated and does not come from the same distribution as the test or evaluation data. For example, an ML model trained to predict inflation rates based on data collected one year ago may not perform well once applied to current data because of the domain gap problem. This is because the source and target data distributions and characteristics are now different<a id="_idIndexMarker581"/> <span class="No-Break">from expected.</span></p>
<h2 id="_idParaDest-232"><a id="_idTextAnchor235"/>Concept drift</h2>
<p>Concept drift<a id="_idIndexMarker582"/> refers to the change in the relation between<a id="_idIndexMarker583"/> the input (features) and output (target). Let’s take an illustrative example. Assume we have designed an object classifier and the “printer” is one of our objects of interest. As you can see in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.1</em>, the shape, color, appearance, and other features of the “printer” concept have drastically changed over time, from early typewriters, to inkjet printers, to laser printers. Thus, if your training data contained only old printers (typewriters), it would simply struggle to accurately classify modern printers because of the domain <span class="No-Break">gap problem.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="Figure 14.1 – An example of concept drift (source: Pixabay)" height="240" src="image/B18494_14_01.jpg" width="791"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1 – An example of concept drift (source: Pixabay)</p>
<p>Next, let’s explore the main solutions to mitigate the domain gap problem specifically between synthetic and <span class="No-Break">real domains.</span></p>
<h1 id="_idParaDest-233"><a id="_idTextAnchor236"/>Approaches for synthetic-to-real domain adaptation</h1>
<p>In this section, you will learn<a id="_idIndexMarker584"/> the key approaches for synthetic-to-real domain adaptation. We will discuss the <span class="No-Break">following methods:</span></p>
<ul>
<li><span class="No-Break">Domain randomization</span></li>
<li>Adversarial <span class="No-Break">domain adaptation</span></li>
<li>Feature-based <span class="No-Break">domain adaptation</span></li>
</ul>
<p>Let’s start with one of the most<a id="_idIndexMarker585"/> commonly used approaches for <span class="No-Break">domain adaptation.</span></p>
<h2 id="_idParaDest-234"><a id="_idTextAnchor237"/>Domain randomization</h2>
<p><strong class="bold">Domain randomization</strong> is a mechanism or procedure usually used<a id="_idIndexMarker586"/> to mitigate the domain gap problem and improve the performance<a id="_idIndexMarker587"/> of ML models on the target domain. This approach aims at randomizing the main properties and attributes of the training data or environment, such as simulators to increase the diversity of the scenarios the ML model is exposed to in the training stage. Thus, we can increase the robustness of the ML model for scenarios that it may encounter in the future. For more information, please refer to <em class="italic">Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real </em><span class="No-Break"><em class="italic">World</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/pdf/1703.06907.pdf"><span class="No-Break">https://arxiv.org/pdf/1703.06907.pdf</span></a><span class="No-Break">).</span></p>
<p>Let’s examine the main elements that are usually randomized in two interesting fields: computer vision <span class="No-Break">and NLP.</span></p>
<h3>Computer vision</h3>
<p>In almost any computer<a id="_idIndexMarker588"/> vision problem, task, or system, we have the<a id="_idIndexMarker589"/> following four main elements, as shown in <span class="No-Break"><em class="italic">Figure 14</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<img alt="Figure 14.2 – Main elements to randomize in computer vision" height="165" src="image/B18494_14_02.jpg" width="830"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2 – Main elements to randomize in computer vision</p>
<p>Next, let’s discuss each of these elements in <span class="No-Break">more detail.</span></p>
<ul>
<li><strong class="bold">Objects</strong>: They are usually the primary<a id="_idIndexMarker590"/> focus of computer vision. They populate the scene and interact with each other, the lighting, and the environment. Many major tasks in computer vision, such as object detection, recognition, segmentation, and tracking, are fundamentally related to the appearance of these images in the 3D world and how the world is projected by camera as <span class="No-Break">2D images.</span></li>
</ul>
<p>Utilizing domain randomization to mitigate the domain gap, we can, for instance, randomize the following elements to diversify the <span class="No-Break">object’s appearance:</span></p>
<ul>
<li>Textures <span class="No-Break">and materials</span></li>
<li><span class="No-Break">Colors</span></li>
<li><span class="No-Break">Shapes</span></li>
<li><span class="No-Break">Dimensions</span></li>
<li>Deformation <span class="No-Break">and animation</span></li>
</ul>
<p>As you can see, deciding which factors are more relevant depends on the task and <span class="No-Break">the problem.</span></p>
<ul>
<li><strong class="bold">Lighting</strong>: Although lighting is necessary<a id="_idIndexMarker591"/> to make the objects<a id="_idIndexMarker592"/> visible to the observer, it creates a daunting problem for almost all computer vision tasks. A slight variation in lighting conditions drastically changes the appearance of objects, thus changing the pixels’ intensity, which makes these objects rather hard for ML-based computer vision models to recognize, detect, <span class="No-Break">or track.</span></li>
</ul>
<p>Thus, to make our ML model robust under various lighting conditions, we may need to diversify the <span class="No-Break">following elements:</span></p>
<ul>
<li><span class="No-Break">Lighting intensity</span></li>
<li>Lighting color <span class="No-Break">and temperature</span></li>
<li><span class="No-Break">Light sources</span></li>
<li>Lighting anomalies: flares, glares, <span class="No-Break">and</span><span class="No-Break"><a id="_idIndexMarker593"/></span><span class="No-Break"> flickering</span></li>
</ul>
<ul>
<li><strong class="bold">Camera and sensor characteristics</strong>: A camera captures visual data from the scene, which is the most generic<a id="_idIndexMarker594"/> and conventional<a id="_idIndexMarker595"/> input for computer vision models. Thus, to ensure that our ML model generalizes well even under new camera setups, we need to diversify, for instance, the camera setups in the training stage. This can be done by varying <span class="No-Break">the following:</span><ul><li><span class="No-Break">Camera position</span></li><li><span class="No-Break">Orientation</span></li><li><span class="No-Break">Altitude</span></li><li><span class="No-Break">Viewpoints</span></li><li>Aperture, exposure, <span class="No-Break">and focus</span></li></ul></li>
</ul>
<p>Additionally, a change<a id="_idIndexMarker596"/> in camera parameters such as focal length may also drastically change how the world is captured and perceived. Thus, it will directly affect how the computer vision system recognizes the world, too. Therefore, we may need to consider the following sensor characteristic variations in our <span class="No-Break">training data:</span></p>
<ul>
<li><span class="No-Break">Lens distortion</span></li>
<li><span class="No-Break">Vignetting</span></li>
<li><span class="No-Break">Chromatic aberration</span></li>
<li><span class="No-Break">Scratches</span></li>
<li><span class="No-Break">Haze</span></li>
<li><span class="No-Break">Light leakage</span></li>
</ul>
<ul>
<li><strong class="bold">Environmental factors</strong>: Even when the scene, lighting, and camera parameters<a id="_idIndexMarker597"/> are the same, environmental factors can substantially change how the scene may look. For example, a weather condition such as fog works as a low-pass filter that removes details of objects that are far from the camera. Thus, it makes extracting robust features for these scenarios harder. Consequently, many ML models may fail or struggle in <span class="No-Break">similar cases.</span></li>
</ul>
<p>In many situations, we cannot clearly identify the environmental factors our computer vision system will work under, therefore we may need to randomize the <span class="No-Break">following factors:</span></p>
<ul>
<li><span class="No-Break">Weather conditions</span></li>
<li>Time <span class="No-Break">of day</span></li>
<li>Indoor and <span class="No-Break">outdoor environment</span></li>
<li><span class="No-Break">Pollution level</span></li>
<li><span class="No-Break">Wind effects</span></li>
<li>Terrain <span class="No-Break">and landscapes</span></li>
<li><span class="No-Break">Road conditions</span></li>
<li><span class="No-Break">Crowd density</span></li>
<li><span class="No-Break">Background clutter</span></li>
<li><span class="No-Break">Geographical locations</span></li>
</ul>
<p>Therefore, and based<a id="_idIndexMarker598"/> on what we have learned in this section, to mitigate the domain gap between source (training) and target (evaluation) domains, we need to diversify and randomize the scenarios that our ML model will learn during training. Next, let’s delve into utilizing domain randomization for <span class="No-Break">NLP problems.</span></p>
<h3>NLP</h3>
<p>Similar to what we<a id="_idIndexMarker599"/> discussed in computer vision, domain randomization can<a id="_idIndexMarker600"/> also be utilized for NLP problems. Usually, it can be deployed to make the NLP models more robust and accurate. For more information, please refer to <em class="italic">Scaling Up and Distilling Down: Language-Guided Robot Skill </em><span class="No-Break"><em class="italic">Acquisition</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/2307.14535"><span class="No-Break">https://arxiv.org/abs/2307.14535</span></a><span class="No-Break">).</span></p>
<p>As you can see in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.3</em>, there are four key elements that can be randomized to improve the generalizability of NLP models <span class="No-Break">in practice:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<img alt="Figure 14.3 – Main elements to randomize in NLP" height="220" src="image/B18494_14_03.jpg" width="834"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.3 – Main elements to randomize in NLP</p>
<p>Let’s discuss these elements in detail <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Textual variations</strong>: To improve the NLP<a id="_idIndexMarker601"/> model’s generalizability<a id="_idIndexMarker602"/> and robustness to real-world problems, many textual variations are introduced to augment and complement the training data. This is usually done by varying the <span class="No-Break">following elements:</span><ul><li><span class="No-Break">Vocabulary</span></li><li><span class="No-Break">Sentence structure</span></li><li><span class="No-Break">Sentence length</span></li></ul></li>
<li><strong class="bold">Context</strong>: NLP models can be utilized<a id="_idIndexMarker603"/> in different contexts and for various applications. For example, ChatGPT can be used to answer questions about various topics, such as healthcare, math, finance, and history. It can be utilized to propose travel plans or even to summarize text. Thus, it is crucial to diversify these elements in the <span class="No-Break">training data:</span><ul><li><span class="No-Break">Topic</span></li><li><span class="No-Break">Domain</span></li><li>Textual genres, such as social<a id="_idIndexMarker604"/> media, research papers, <span class="No-Break">and novels</span></li></ul></li>
<li><strong class="bold">Linguistic factors</strong>: An ideal NLP model should be able<a id="_idIndexMarker605"/> to handle and respond to queries with different formats and styles. Additionally, it should be capable of understanding sentiments in various applications. Thus, the following factors should be randomized during the training stage to ensure better performance <span class="No-Break">in practice:</span><ul><li>Style, such as formal, informal, technical, <span class="No-Break">or colloquial</span></li><li>Sentiment, such as positive, negative, or <span class="No-Break">neutral expressions</span></li></ul></li>
<li><strong class="bold">Noise and perturbations</strong>: Introducing perturbations in the training<a id="_idIndexMarker606"/> data and guiding the NLP model on how to perform these scenarios will ensure that your ML model learns how to respond correctly to these issues. The real world is noisy and thus it is crucial to pay attention to the following factors in the training stage to cover these well-known imperfections usually observed in <span class="No-Break">textual data:</span><ul><li><span class="No-Break">Spelling errors</span></li><li><span class="No-Break">Grammar errors</span></li><li><span class="No-Break">Punctuation </span><span class="No-Break"><a id="_idIndexMarker607"/></span><span class="No-Break">errors</span></li></ul></li>
</ul>
<p>Next, let’s explore another interesting domain <span class="No-Break">adaptation method.</span></p>
<h2 id="_idParaDest-235"><a id="_idTextAnchor238"/>Adversarial domain adaptation</h2>
<p><strong class="bold">Adversarial domain adaptation</strong> is another powerful technique used to bridge the gap between synthetic<a id="_idIndexMarker608"/> and real domains based<a id="_idIndexMarker609"/> on GANs. In this domain adaptation method, the generator tries to extract domain-independent features while the discriminator tries to identify the source of the data: synthetic or real. Once the model is trained and the discriminator can no longer identify the source of the data domain, the generator can then generate domain-invariant features. For more information, please refer to <em class="italic">Adversarial Discriminative Domain </em><span class="No-Break"><em class="italic">Adaptation</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/pdf/1702.05464.pdf"><span class="No-Break">https://arxiv.org/pdf/1702.05464.pdf</span></a><span class="No-Break">).</span></p>
<p>Let’s briefly discuss one example from computer vision that illustrates how this synthetic-to-real domain adaptation method can be utilized. For example, let’s consider the vehicle re-identification problem. It was shown that utilizing synthetic and real data for training using adversarial domain adaptation improved the performance on two evaluation real datasets, <em class="italic">CityFlow-ReID</em> (<a href="https://paperswithcode.com/dataset/cityflow">https://paperswithcode.com/dataset/cityflow</a>) and <em class="italic">VeRi</em> (<a href="https://github.com/VehicleReId/VeRi">https://github.com/VehicleReId/VeRi</a>), with a good margin compared to other solutions. The approach was trained on a mixture of synthetic and real vehicle re-identification datasets. The ML model was guided to learn discriminative features, especially from the real training images. At the same time, it was directed to learn the features that are common between synthetic<a id="_idIndexMarker610"/> and real domains. For more details, please refer to <em class="italic">StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle Re-Identification</em> (<a href="https://arxiv.org/abs/2004.12032">https://arxiv.org/abs/2004.12032</a>). Next, let’s look at another approach used for <span class="No-Break">domain</span><span class="No-Break"><a id="_idIndexMarker611"/></span><span class="No-Break"> adaptation.</span></p>
<h2 id="_idParaDest-236"><a id="_idTextAnchor239"/>Feature-based domain adaptation</h2>
<p>Unlike the previous approach, <strong class="bold">feature-based domain adaptation</strong> aims at learning a transformation that extracts<a id="_idIndexMarker612"/> domain-independent<a id="_idIndexMarker613"/> features across synthetic and real domains. This approach transfers the features of both domains into a new representation and then minimizes the discrepancy between the synthetic and real domains. In other words, this method tries to align the feature distributions in both domains with each other. Thus, the approach urges the ML model to learn essential features while discarding domain-specific details and variations. For instance, if the ML model is learning semantic segmentation<a id="_idIndexMarker614"/> by being trained on the <em class="italic">Synscapes </em>(<a href="https://synscapes.on.liu.se">https://synscapes.on.liu.se</a>) and <em class="italic">Cityscapes</em> (<a href="https://www.cityscapes-dataset.com">https://www.cityscapes-dataset.com</a>) datasets, we want<a id="_idIndexMarker615"/> the model to learn how to segment humans, cars, traffic lights, and other objects by somehow learning the meaning of these objects. For instance, humans usually walk on pedestrian areas or sidewalks and they usually have a capsule-like shape. These are the sorts of high-level and domain-invariant features that we want the model <span class="No-Break">to learn.</span></p>
<p>Finally, for a detailed overview of domain adaption methods, please refer to <em class="italic">A Brief Review of Domain Adaptation</em> (<a href="https://arxiv.org/pdf/2010.03978v1.pdf">https://arxiv.org/pdf/2010.03978v1.pdf</a>). Now, we have learned about some<a id="_idIndexMarker616"/> of the key approaches usually utilized<a id="_idIndexMarker617"/> for synthetic-to-real domain adaptation. Next, let’s delve into their common limitations and how to overcome them <span class="No-Break">in practice.</span></p>
<h1 id="_idParaDest-237"><a id="_idTextAnchor240"/>Synthetic-to-real domain adaptation – issues and challenges</h1>
<p>In this section, you will explore the main issues and challenges<a id="_idIndexMarker618"/> of synthetic-to-real domain adaptation. This will help you to understand the limitations of this approach. Additionally, it will give you a better insight into how to overcome these issues in your own problem. Therefore, we will focus on the <span class="No-Break">following issues:</span></p>
<ul>
<li><span class="No-Break">Unseen domain</span></li>
<li>Limited <span class="No-Break">real data</span></li>
<li><span class="No-Break">Computational complexity</span></li>
<li>Synthetic <span class="No-Break">data limitations</span></li>
<li>Multimodal <span class="No-Break">data complexity</span></li>
</ul>
<p>Let’s discuss them in detail in the <span class="No-Break">following subsections.</span></p>
<h2 id="_idParaDest-238"><a id="_idTextAnchor241"/>Unseen domain</h2>
<p>In many cases, the aim is to make sure that your ML model will generalize well to new domains. If we know the domain, domain adaptation methods may work. However, sometimes it is not possible to predict the properties of this new domain. For example, assume you have a computer vision model that works well in Europe but you also want this algorithm to work well in China, Africa, the Middle East, or even on Mars! It is not always possible to have advanced knowledge of the environment or the domain where the ML model will be deployed to make <span class="No-Break">appropriate adaptations.</span></p>
<h2 id="_idParaDest-239"><a id="_idTextAnchor242"/>Limited real data</h2>
<p>As we know, real data is scarce and expensive. Thus, supervised domain adaptation methods cannot be easily applied to all ML problems. Additionally, the limited availability of paired data between synthetic and real domains makes the problem even harder. Thus, it is indeed complex and cumbersome to learn the mapping from a synthetic to a <span class="No-Break">real domain.</span></p>
<h2 id="_idParaDest-240"><a id="_idTextAnchor243"/>Computational complexity</h2>
<p>Domain adaptation methods are computationally expensive and may require resources, time, experience, substantial budget, and domain experts. Thus, it is not easy, and it is sometimes challenging to train these models, especially adversarial domain <span class="No-Break">adaptation ones.</span></p>
<h2 id="_idParaDest-241"><a id="_idTextAnchor244"/>Synthetic data limitations</h2>
<p>Synthetic data has some limitations, especially if the data is not generated appropriately. Synthetic data may lack the diversity and realism of real data. Thus, it makes it even harder for adaptation methods to bridge the gap. Please refer to <a href="B18494_13.xhtml#_idTextAnchor216"><span class="No-Break"><em class="italic">Chapter 13</em></span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-242"><a id="_idTextAnchor245"/>Multimodal data complexity</h2>
<p>Recent state-of-the-art computer vision systems such <em class="italic">Tesla Vision Autopilot</em> utilize data captured from different sources, such as cameras, LiDAR sensors, radar sensors, accelerometers, and gyroscopes. However, generating and annotating the real data and matching it with synthetic data is not a straightforward process. For example, it is possible to generate semantic segmentation, optical flow, or other relevant ground truths, but it is very difficult to simulate the behavior of accelerometers and gyroscopes in virtual worlds. Additionally, it is very complex to develop a simulator that provides all these ground truths together. In parallel to that, it is rather hard for domain adaptation to learn how to appropriately<a id="_idIndexMarker619"/> adapt the data from one domain <span class="No-Break">to another.</span></p>
<h1 id="_idParaDest-243"><a id="_idTextAnchor246"/>Summary</h1>
<p>In this chapter, you learned the essence of the domain gap problem in ML. Additionally, you explored the main solutions to mitigate this problem. We focused on domain randomization in computer vision and NLP. Then, you learned about the main issues and limitations of synthetic-to-real domain adaptation. In the next chapter, we will explore and highlight diversity issues in synthetic data to better comprehend the pros and cons of synthetic data <span class="No-Break">in ML.</span></p>
</div>
</div></body></html>