- en: Chapter 2. Practical Approach to Real-World Supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章. 实际应用中的现实世界监督学习
- en: The ability to learn from observations accompanied by marked targets or labels,
    usually in order to make predictions about unseen data, is known as **supervised
    machine learning**. If the targets are categories, the problem is one of classification
    and if they are numeric values, it is called **regression**. In effect, what is
    being attempted is to infer the function that maps the data to the target. Supervised
    machine learning is used extensively in a wide variety of machine learning applications,
    whenever labeled data is available or the labels can be added manually.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从带有明显目标或标签的观察中学习的能力，通常是为了对未见数据做出预测，这被称为**监督机器学习**。如果目标是类别，则问题属于分类；如果目标是数值，则称为**回归**。实际上，所尝试的是推断将数据映射到目标的功能。监督机器学习在广泛的机器学习应用中被广泛使用，无论是有标签数据可用还是标签可以手动添加。
- en: The core assumption of supervised machine learning is that the patterns that
    are learned from the data used in training will manifest themselves in yet unseen
    data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习的核心假设是，从训练数据中学习到的模式将在未见数据中表现出来。
- en: In this chapter, we will discuss the steps used to explore, analyze, and pre-process
    the data before proceeding to training models. We will then introduce different
    modeling techniques ranging from simple linear models to complex ensemble models.
    We will present different evaluation metrics and validation criteria that allow
    us to compare model performance. Some of the discussions are accompanied by brief
    mathematical explanations that should help express the concepts more precisely
    and whet the appetite of the more mathematically inclined readers. In this chapter,
    we will focus on classification as a method of supervised learning, but the principles
    apply to both classification and regression, the two broad applications of supervised
    learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论在训练模型之前用于探索、分析和预处理数据的步骤。然后，我们将介绍从简单的线性模型到复杂的集成模型的不同建模技术。我们将展示不同的评估指标和验证标准，这些标准使我们能够比较模型性能。一些讨论伴随着简短的数学解释，这有助于更精确地表达概念并激发更多数学倾向读者的兴趣。在本章中，我们将重点关注分类作为监督学习方法，但原则同样适用于分类和回归，这是监督学习的两个广泛应用。
- en: Beginning with this chapter, we will introduce tools to help illustrate how
    the concepts presented in each chapter are used to solve machine learning problems.
    Nothing reinforces the understanding of newly learned material better than the
    opportunity to apply that material to a real-world problem directly. In the process,
    we often gain a clearer and more relatable understanding of the subject than what
    is possible with passive absorption of the theory alone. If the opportunity to
    learn new tools is part of the learning, so much the better! To meet this goal,
    we will introduce a classification dataset familiar to most data science practitioners
    and use it to solve a classification problem while highlighting the process and
    methodologies that guide the solution.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章开始，我们将介绍工具来帮助说明每个章节中提出的概念是如何用于解决机器学习问题的。没有什么比将新学的材料直接应用于现实世界问题更能加强对新材料的理解了。在这个过程中，我们往往比仅仅被动吸收理论更能获得更清晰、更相关的理解。如果学习新工具的机会是学习的一部分，那就更好了！为了达到这个目标，我们将介绍一个大多数数据科学从业者都熟悉的数据集，并使用它来解决一个分类问题，同时突出引导解决方案的过程和方法。
- en: In this chapter, we will use RapidMiner and Weka for building the process by
    which we learn from a single well-known dataset. The workflows and code are available
    on the website for readers to download, execute, and modify.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用RapidMiner和Weka来构建从单个知名数据集中学习的过程。工作流程和代码可在网站上供读者下载、执行和修改。
- en: RapidMiner is a GUI-based Java framework that makes it very easy to conduct
    a data science project, end-to-end, from within the tool. It has a simple drag-and-drop
    interface to build process workflows to ingest and clean data, explore and transform
    features, perform training using a wide selection of machine learning algorithms,
    do validation and model evaluation, apply your best models to test data, and more.
    It is an excellent tool to learn how to make the various parts of the process
    work together and produce rapid results. Weka is another GUI-based framework and
    it has a Java API that we will use to illustrate more of the coding required for
    performing analysis.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: RapidMiner是一个基于GUI的Java框架，它使得在工具内部从头到尾进行数据科学项目变得非常容易。它有一个简单的拖放界面来构建工作流程，用于摄取和清理数据，探索和转换特征，使用广泛的机器学习算法进行训练，进行验证和模型评估，将最佳模型应用于测试数据，等等。它是学习如何使流程的各个部分协同工作并快速产生结果的一个优秀工具。Weka是另一个基于GUI的框架，它有一个我们将用于说明执行分析所需的更多编码的Java
    API。
- en: 'The major topics that we will cover in this chapter are:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖的主要主题包括：
- en: Data quality analysis
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量分析
- en: Descriptive data analysis
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述性数据分析
- en: Visualization analysis
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化分析
- en: Data transformation and preprocessing
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换和预处理
- en: Data sampling
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据采样
- en: Feature relevance analysis and dimensionality reduction
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征相关性分析和降维
- en: Model building
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型构建
- en: Model assessment, evaluation, and comparison
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估、评估和比较
- en: Detailed case study—Horse Colic Classification
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究—马绞痛分类
- en: Formal description and notation
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 形式描述和符号
- en: We would like to introduce some notation and formal definitions for the terms
    used in supervised learning. We will follow this notation through the rest of
    the book when not specified and extend it as appropriate when new concepts are
    encountered. The notation will provide a precise and consistent language to describe
    the terms of art and enable a more rapid and efficient comprehension of the subject.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望介绍一些用于监督学习中使用的术语的符号和形式定义。在不指定的情况下，我们将遵循这种符号贯穿本书的其余部分，并在遇到新概念时适当扩展。这种符号将提供一个精确和一致的语言来描述术语，并使对主题的理解更加快速和高效。
- en: '**Instance**: Every observation is a data instance. Normally the variable *X*
    is used to represent the input space. Each data instance has many variables (also
    called features) and is referred to as **x** (vector representation with bold)
    of dimension *d* where *d* denotes the number of variables or features or attributes
    in each instance. The features are represented as **x** = *(x*[1]*,x*[2]*,…x*[d]*)*^T,
    where each value is a scalar when it is numeric corresponding to the feature value.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实例**：每个观察都是数据实例。通常变量 *X* 用于表示输入空间。每个数据实例有许多变量（也称为特征），被称为 **x**（用粗体表示的向量表示）的维度为
    *d*，其中 *d* 表示每个实例中的变量或特征或属性的数量。特征表示为 **x** = *(x*[1]*,x*[2]*,…x*[d]*)*^T，其中每个值在它是数值时对应于特征值。'
- en: '**Label**: The label (also called target) is the dependent variable of interest,
    generally denoted by *y*. In **classification**, values of the label are well-defined
    categories in the problem domain; they need not be numeric or things that can
    be ordered. In **regression**, the label is real-valued.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签**：标签（也称为目标）是感兴趣的因变量，通常用 *y* 表示。在 **分类** 中，标签的值是问题域中定义良好的类别；它们不需要是数值或可以排序的事物。在
    **回归** 中，标签是实数值。'
- en: '**Binary classification**, where the target takes only two values, it is mathematically
    represented as:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元分类**，其中目标只取两个值，在数学上表示为：'
- en: y ∈ {1,–1}
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: y ∈ {1,–1}
- en: '**Regression**, where the target can take any value in the real number domain,
    is represented as:![Formal description and notation](img/B05137_02_008.jpg)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**，其中目标可以取实数域内的任何值，表示为：![形式描述和符号](img/B05137_02_008.jpg)'
- en: '**Dataset**: Generally, the dataset is denoted by *D* and consists of individual
    data instances and their labels. The instances are normally represented as set
    {**x**[1],**x**[2]…**x**[n]}. The labels for each instance are represented as
    the set **y** = {*y*[1]*,y*[2]*,…y*[n]}. The entire labeled dataset is represented
    as paired elements in a set as given by *D* = {(**x**[1], *y*[1]),(**x**[2], *y*[2])…(**x**[n],
    *y*[n])} where ![Formal description and notation](img/B05137_02_013.jpg) for real-valued
    features.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：通常，数据集用 *D* 表示，由单个数据实例及其标签组成。实例通常表示为集合 {**x**[1],**x**[2]…**x**[n]}。每个实例的标签表示为集合
    **y** = {*y*[1]*,y*[2]*,…y*[n]}。整个标记数据集表示为集合中的配对元素，如 *D* = {(**x**[1], *y*[1]),(**x**[2],
    *y*[2])…(**x**[n], *y*[n])}，其中 ![形式描述和符号](img/B05137_02_013.jpg) 用于实值特征。'
- en: Data quality analysis
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量分析
- en: There are limitations to what can be learned from data that suffers from poor
    quality. Problems with quality can include, among other factors, noisy data, missing
    values, and errors in labeling. Therefore, the first step is to understand the
    data before us in order that we may determine how to address any data quality
    issues. Are the outliers merely noise or indicative of interesting anomalies in
    the population? Should missing data be handled the same way for all features?
    How should sparse features be treated? These and similar questions present themselves
    at the very outset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从质量较差的数据中可以学习到的东西有限。质量问题的因素可能包括，但不仅限于，噪声数据、缺失值和标签错误。因此，第一步是理解我们面前的数据，以便我们确定如何解决任何数据质量问题。异常值仅仅是噪声，还是表明人口中有有趣的异常？对于所有特征，缺失数据是否应该以相同的方式处理？稀疏特征应该如何处理？这些问题以及类似的问题在最初就显现出来。
- en: If we're fortunate, we receive a cleansed, accurately labeled dataset accompanied
    by documentation describing the data elements, the data's pedigree, and what if
    any transformations were already done to the data. Such a dataset would be ready
    to be split into train, validation, and test samples, using methods described
    in the section on Data Sampling. However, if data is not cleansed and suitable
    to be partitioned for our purposes, we must first prepare the data in a principled
    way before sampling can begin. (The significance of partitioning the data is explained
    later in this chapter in a section dedicated to train, validation, and test sets).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们很幸运，我们会收到一个清洗过的、准确标记的数据集，并附带描述数据元素、数据的血统以及是否对数据进行过任何转换的文档。这样的数据集就可以准备好按照数据采样部分描述的方法分成训练、验证和测试样本。然而，如果数据没有清洗并且不适合用于我们的目的，我们必须在采样开始之前以原则性的方式准备数据。（数据分区的意义将在本章的“训练、验证和测试集”部分中解释）。
- en: In the following sections, we will discuss the data quality analysis and transformation
    steps that are needed before we can analyze the features.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论在分析特征之前所需的数据质量分析和转换步骤。
- en: Descriptive data analysis
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述性数据分析
- en: The complete data sample (including train, validation, and test) should be analyzed
    and summarized for the following characteristics. In cases where the data is not
    already split into train, validate, and test, the task of data transformation
    needs to make sure that the samples have similar characteristics and statistics.
    This is of paramount importance to ensure that the trained model can generalize
    over unseen data, as we will learn in the section on data sampling.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 应对以下特征进行分析和总结的完整数据样本（包括训练、验证和测试）。在数据尚未分成训练、验证和测试的情况下，数据转换的任务需要确保样本具有相似的特征和统计信息。这对于确保训练的模型可以泛化到未见过的数据至关重要，正如我们将在数据采样部分所学到的。
- en: Basic label analysis
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本标签分析
- en: The first step of analysis is understanding the distribution of labels in different
    sets as well as in the data as a whole. This helps to determine whether, for example,
    there is imbalance in the distribution of the target variable, and if so, whether
    it is consistent across all the samples. Thus, the very first step is usually
    to find out how many examples in the training and test sets belong to each class.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 分析的第一步是理解不同集合以及整体数据中标签的分布。这有助于确定例如目标变量的分布是否存在不平衡，以及这种不平衡是否在所有样本中一致。因此，第一步通常是找出训练集和测试集中每个类别的例子数量。
- en: Basic feature analysis
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本特征分析
- en: The next step is to calculate the statistics for each feature, such as
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是计算每个特征的统计信息，例如
- en: Number of unique values
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 唯一值的数量
- en: 'Number of missing values: May include counts grouped by different missing value
    surrogates (NA, null, ?, and so on).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值的数量：可能包括按不同缺失值代理（NA、null、?等）分组的计数。
- en: 'For categorical: This counts across feature categories, counts across feature
    categories by label category, most frequently occurring category (mode), mode
    by label category, and so on.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类：这是跨特征类别的计数，按标签类别跨特征类别的计数，最频繁出现的类别（众数），按标签类别的众数等。
- en: 'For numeric: Minimum, maximum, median, standard deviation, variance, and so
    on.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于数值：最小值、最大值、中位数、标准差、方差等。
- en: Feature analysis gives basic insights that can be a useful indicator of missing
    values and noise that can affect the learning process or choice of the algorithms.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 特征分析提供了基本的见解，这些见解可以作为缺失值和噪声的有用指标，这些缺失值和噪声可能会影响学习过程或算法的选择。
- en: Visualization analysis
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化分析
- en: Visualization of the data is a broad topic and it is a continuously evolving
    area in the field of machine learning and data mining. We will only cover some
    of the important aspects of visualization that help us analyze the data in practice.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的可视化是一个广泛的话题，它是在机器学习和数据挖掘领域不断发展的一个领域。我们只将涵盖一些有助于我们在实践中分析数据的可视化重要方面。
- en: Univariate feature analysis
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单变量特征分析
- en: 'The goal here is to visualize one feature at a time, in relation to the label.
    The techniques used are as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是每次可视化一个特征，与标签相关。使用的技术如下：
- en: Categorical features
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分类特征
- en: Stacked bar graphs are a simple way of showing the distribution of each feature
    category among the labels, when the problem is one of classification.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠条形图是一种简单的方式来展示每个特征类别在标签中的分布，当问题是一类分类时。
- en: Continuous features
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 连续特征
- en: Histograms and box plots are two basic visualization techniques for continuous
    features.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图和箱线图是连续特征的两种基本可视化技术。
- en: Histograms have predefined bins whose widths are either fixed intervals or based
    on some calculation used to split the full range of values of the feature. The
    number of instances of data that falls within each bin is then counted and the
    height of the bin is adjusted based on this count. There are variations of histograms
    such as relative or frequency-based histograms, Pareto histograms, two-dimensional
    histograms, and so on; each is a slight variation of the concept and permits a
    different insight into the feature. For those interested in finding out more about
    these variants, the Wikipedia article on histograms is a great resource.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图有预定义的区间，其宽度要么是固定的间隔，要么是基于某种用于分割特征值全范围的计算。然后计算每个区间内的数据实例数量，并根据这个计数调整区间的身高。直方图有各种变体，如相对直方图或基于频率的直方图、帕累托直方图、二维直方图等；每个都是概念的一小部分变化，允许对特征有不同见解。对于那些想了解更多关于这些变体的人来说，维基百科上的直方图文章是一个很好的资源。
- en: Box plots are a key visualization technique for numeric features as they show
    distributions in terms of percentiles and outliers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图是数值特征的键视觉技术，因为它们以百分位数和异常值来展示分布。
- en: Multivariate feature analysis
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多变量特征分析
- en: The idea of multivariate feature analysis is to visualize more than one feature
    to get insights into relationships between them. Some of the well-known plots
    are explained here.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量特征分析的想法是可视化多个特征，以深入了解它们之间的关系。这里解释了一些著名的图表。
- en: '**Scatter plots**: An important technique for understanding the relationship
    between different features and between features and labels. Typically, two-dimensional
    scatter plots are used in practice where numeric features form the dimensions.
    Alignment of data points on some imaginary axis shows correlation while scattering
    of the data points shows no correlation. It can also be useful to identify clusters
    in lower dimensional space. A bubble chart is a variation of a scatter plot where
    two features form the dimensional axes and the third is proportional to the size
    of the data point, with the plot giving the appearance of a field of "bubbles".
    Density charts help visualize even more features together by introducing data
    point color, background color, and so on, to give additional insights.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**散点图**：理解不同特征之间以及特征和标签之间关系的重要技术。在实践中通常使用二维散点图，其中数值特征形成维度。数据点在某些想象轴上的对齐显示相关性，而数据点的散射则显示无相关性。它还可以用于在低维空间中识别簇。气泡图是散点图的变体，其中两个特征形成维度轴，第三个特征与数据点的尺寸成比例，该图呈现出“气泡”场的样子。密度图通过引入数据点颜色、背景颜色等，帮助可视化更多特征，从而提供额外的见解。'
- en: '**ScatterPlot Matrix**: ScatterPlot Matrix is an extension of scatter plots
    where pair-wise scatter plots for each feature (and label) is visualized. It gives
    a way to compare and perform multivariate analysis of high dimensional data in
    an effective way.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**散点图矩阵**：散点图矩阵是散点图的扩展，其中为每个特征（和标签）可视化了成对的散点图。它提供了一种有效的方式，以比较和执行高维数据的多元分析。'
- en: '**Parallel Plots**: In this visualization, each feature is linearly arranged
    on the x-axis and the ranges of values for each feature form the *y* axis. So
    each data element is represented as a line with values for each feature on the
    parallel axis. Class labels, if available, are used to color the lines. Parallel
    plots offer a great understanding of features that are effective in separating
    the data. Deviation charts are variations of parallel plots, where instead of
    showing actual data points, mean and standard deviations are plotted. Andrews
    plots are another variation of parallel plots where data is transformed using
    Fourier series and the function values corresponding to each is projected.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行图**：在这种可视化中，每个特征线性排列在x轴上，每个特征的值域形成y轴。因此，每个数据元素都表示为一条线，该线上的每个特征值都位于平行轴上。如果可用，类标签用于着色线条。并行图提供了对有效分离数据的特征的良好理解。偏差图是并行图的变体，其中不是显示实际数据点，而是绘制均值和标准偏差。Andrews图是并行图的另一种变体，其中使用傅里叶级数转换数据，并将对应于每个数据的函数值投影。'
- en: Data transformation and preprocessing
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换和预处理
- en: In this section, we will cover the broad topic of data transformation. The main
    idea of data transformation is to take the input data and transform it in careful
    ways so as to clean it, extract the most relevant information from it, and to
    turn it into a usable form for further analysis and learning. During these transformations,
    we must only use methods that are designed while keeping in mind not to add any
    bias or artifacts that would affect the integrity of the data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖数据转换的广泛主题。数据转换的主要思想是将输入数据以谨慎的方式进行转换，以便对其进行清理，从中提取最相关的信息，并将其转换为可用于进一步分析和学习的可用形式。在这些转换过程中，我们必须只使用在设计时考虑到不添加任何可能影响数据完整性的偏差或伪影的方法。
- en: Feature construction
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征构建
- en: In the case of some datasets, we need to create more features from features
    we are already given. Typically, some form of aggregation is done using common
    aggregators such as average, sum, minimum, or maximum to create additional features.
    In financial fraud detection, for example, Card Fraud datasets usually contain
    transactional behaviors of accounts over various time periods during which the
    accounts were active. Performing behavioral synthesis such as by capturing the
    "Sum of Amounts whenever a Debit transaction occurred, for each Account, over
    One Day" is an example of feature construction that adds a new dimension to the
    dataset, built from existing features. In general, designing new features that
    enhance the predictive power of the data requires domain knowledge and experience
    with data, making it as much an art as a science.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些数据集的情况下，我们需要从已给出的特征中创建更多特征。通常，会使用常见的聚合器，如平均值、总和、最小值或最大值，以创建额外的特征。例如，在金融欺诈检测中，卡欺诈数据集通常包含账户在活跃期间不同时间段的交易行为。通过捕获“每天发生借记交易时的金额总和，对于每个账户，一天之内”的行为合成，就是一个特征构建的例子，它为数据集添加了一个新的维度，这个维度是由现有特征构建的。一般来说，设计新的特征以增强数据的预测能力需要领域知识和对数据的经验，这使得它既是一门艺术也是一门科学。
- en: Handling missing values
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: In real-world datasets, often, many features have missing values. In some cases,
    they are missing due to errors in measurement, lapses in recording, or because
    they are not available due to various circumstances; for example, individuals
    may choose not to disclose age or occupation. Why care about missing values at
    all? One extreme and not uncommon way to deal with it is to ignore the records
    that have any missing features, in other words, retain only examples that are
    "whole". This approach may severely reduce the size of the dataset when missing
    features are widespread in the data. As we shall see later, if the system we are
    dealing with is complex, dataset size can afford us precious advantage. Besides,
    there is often predictive value that can be exploited even in the "un-whole" records,
    despite the missing values, as long as we use appropriate measures to deal with
    the problem. On the other hand, one may unwittingly be throwing out key information
    when the omission of the data itself is significant, as in the case of deliberate
    misrepresentation or obfuscation on a loan application by withholding information
    that could be used to conclusively establish bone fides.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的数据集中，通常许多特征都有缺失值。在某些情况下，它们是因测量错误、记录失误或由于各种情况下的不可用而缺失；例如，个人可能选择不透露年龄或职业。为什么要在乎缺失值呢？一种极端且不罕见的方法是忽略任何具有缺失特征的记录，换句话说，只保留“完整”的例子。当数据中广泛存在缺失特征时，这种方法可能会严重减少数据集的大小。正如我们稍后将要看到的，如果我们处理的是一个复杂系统，数据集的大小可以给我们带来宝贵的优势。此外，即使存在缺失值，只要我们使用适当的措施来处理问题，即使在“不完整”的记录中，也常常可以挖掘出有预测价值的值。另一方面，当数据本身的省略本身具有重要意义时，如贷款申请中故意隐瞒信息以掩盖事实的情况，可能会无意中丢弃关键信息。
- en: Suffice it to say, that an important step in the learning process is to adopt
    some systematic way to handle missing values and understand the consequences of
    the decision in each case. There are some algorithms such as Naïve Bayes that
    are less sensitive to missing values, but in general, it is good practice to handle
    these missing values as a pre-processing step before any form of analysis is done
    on the data. Here are some of the ways to handle missing values.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是学习过程中的一个重要步骤，即采用某种系统的方式来处理缺失值，并理解每个案例中决策的后果。有一些算法，如朴素贝叶斯，对缺失值不太敏感，但通常，在数据上进行分析之前，将这些缺失值作为预处理步骤处理是良好的实践。以下是一些处理缺失值的方法。
- en: '**Replacing by means and modes**: When we replace the missing value of a continuous
    value feature with the mean value of that feature, the new mean clearly remains
    the same. But if the mean is heavily influenced by outliers, a better approach
    may be to use the mean after dropping the outliers from the calculation, or use
    the median or mode, instead. Likewise, when a feature is sparsely represented
    in the dataset, the mean value may not be meaningful. In the case of features
    with categorical values, replacing the missing value with the one that occurs
    with the highest frequency in the sample makes for a reasonable choice.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用均值和众数替换**：当我们用特征的均值替换连续值特征的缺失值时，新的均值显然保持不变。但如果均值受到异常值的影响很大，更好的方法可能是使用计算中去除异常值后的均值，或者使用中位数或众数，而不是均值。同样，当特征在数据集中稀疏表示时，均值可能没有意义。对于具有分类值的特征，用样本中出现频率最高的值替换缺失值是一个合理的选择。'
- en: '**Replacing by imputation**: When we impute a missing value, we are in effect
    constructing a classification or regression model of the feature and making a
    prediction based on the other features in the record in order to classify or estimate
    the missing value.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过插补替换**：当我们插补一个缺失值时，实际上是在构建一个关于该特征的分类或回归模型，并基于记录中的其他特征进行预测，以便对缺失值进行分类或估计。'
- en: '**Nearest Neighbor imputation**: For missing values of a categorical feature,
    we consider the feature in question to be the target and train a KNN model with
    k taken to be the known number of distinct categories. This model is then used
    to predict the missing values. (A KNN model is non-parametric and assigns a value
    to the "incoming" data instance based on a function of its neighbors—the algorithm
    is described later in this chapter when we talk about non-linear models).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最近邻插补法**：对于分类特征的缺失值，我们将该特征视为目标，并使用已知的类别数量k训练一个KNN模型。然后使用此模型来预测缺失值。（KNN模型是非参数的，根据其邻居的函数为其“传入”数据实例分配一个值——该算法将在本章后面关于非线性模型时进行描述）。'
- en: '**Regression-based imputation**: In the case of continuous value variables,
    we use linear models like Linear Regression to estimate the missing data—the principle
    is the same as for categorical values.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于回归的插补**：在连续值变量的情况下，我们使用线性模型如线性回归来估计缺失数据——原理与分类值相同。'
- en: '**User-defined imputation**: In many cases, the most suitable value for imputing
    missing values must come from the problem domain. For instance, a pH value of
    7.0 is neutral, higher is basic, and lower is acidic. It may make most sense to
    impute a neutral value for pH than either mean or median, and this insight is
    an instance of a user-defined imputation. Likewise, in the case of substitution
    with normal body temperature or resting heart rate—all are examples from medicine.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户定义的插补**：在许多情况下，用于插补缺失值的最合适的值必须来自问题域。例如，pH值为7.0是中性的，更高的是碱性，更低的是酸性。对于pH值，插补一个中性值可能比均值或中位数更有意义，这种洞察力是用户定义插补的一个例子。同样，在用正常体温或静息心率进行替换的情况下——所有这些都是来自医学的例子。'
- en: Outliers
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常值
- en: Handling outliers requires a lot of care and analysis. Outliers can be noise
    or errors in the data, or they can be anomalous behavior of particular interest.
    The latter case is treated in depth in [Chapter 3](ch03.html "Chapter 3. Unsupervised
    Machine Learning Techniques"), *Unsupervised Machine Learning Techniques*. Here
    we assume the former case, that the domain expert is satisfied that the values
    are indeed outliers in the first sense, that is, noise or erroneously acquired
    or recorded data that needs to be handled appropriately.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 处理异常值需要大量的注意和分析。异常值可能是数据中的噪声或错误，也可能是特定感兴趣的特殊行为。后一种情况在[第3章](ch03.html "第3章。无监督机器学习技术")“无监督机器学习技术”中进行了深入讨论。在这里，我们假设前一种情况，即领域专家满意地认为这些值确实是第一种意义上的异常值，即噪声或错误获取或记录的数据，需要适当处理。
- en: Following are different techniques in detecting outliers in the data
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是检测数据中异常值的不同技术
- en: '**Interquartile Ranges (IQR)**: Interquartile ranges are a measure of variability
    in the data or, equivalently, the statistical dispersion. Each numeric feature
    is sorted based on its value in the dataset and the ordered set is then divided
    into quartiles. The median value is generally used to measure central tendency.
    IQR is measured as the difference between upper and lower quartiles, Q3-Q1\. The
    outliers are generally considered to be data values above Q3 + 1.5 * IQR and below
    Q1 - 1.5 * IQR.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**四分位数范围（IQR）**：四分位数范围是数据变异性的度量，或者说，是统计分散度的度量。每个数值特征根据其在数据集中的值进行排序，然后有序集被分为四分位数。通常使用中位数来衡量集中趋势。IQR被测量为上四分位数和下四分位数的差，即Q3-Q1。通常认为，异常值是高于Q3
    + 1.5 * IQR和低于Q1 - 1.5 * IQR的数据值。'
- en: '**Distance-based Methods**: The most basic form of distance-based methods uses
    **k-Nearest Neighbors** (**k-NN**) and distance metrics to score the data points.
    The usual parameter is the value *k* in k-NN and a distance metric such as Euclidean
    distance. The data points at the farthest distance are considered outliers. There
    are many variants of these that use local neighborhoods, probabilities, or other
    factors, which will all be covered in [Chapter 3](ch03.html "Chapter 3. Unsupervised
    Machine Learning Techniques"), *Unsupervised Machine Learning Techniques*. Mixed
    datasets, which have both categorical and numeric features, can skew distance-based
    metrics.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于距离的方法**：基于距离的最基本方法使用**k-最近邻**（**k-NN**）和距离度量来评分数据点。通常的参数是k-NN中的值*k*和一个距离度量，如欧几里得距离。距离最远的数据点被认为是异常值。有许多变体使用局部邻域、概率或其他因素，这些将在[第3章](ch03.html
    "第3章。无监督机器学习技术")，*无监督机器学习技术*中全部涵盖。混合数据集，既有分类特征又有数值特征，可能会扭曲基于距离的度量。'
- en: '**Density-based methods**: Density-based methods calculate the proportion of
    data points within a given distance *D*, and if the proportion is less than the
    specified threshold p, it is considered an outlier. The parameter p and D are
    considered user-defined values; the challenge of selecting these values appropriately
    presents one of the main hurdles in using these methods in the preprocessing stage.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于密度的方法**：基于密度的方法计算给定距离*D*内数据点的比例，如果比例小于指定的阈值*p*，则被认为是异常值。参数*p*和*D*被认为是用户定义的值；选择这些值适当的挑战是使用这些方法在预处理阶段的主要障碍之一。'
- en: '**Mathematical transformation of feature**: With non-normal data, comparing
    the mean value is highly misleading, as in the case when outliers are present.
    Non-parametric statistics allow us to make meaningful observations about highly
    skewed data. Transformation of such values using the logarithm or square root
    function tends to normalize the data in many cases, or make them more amenable
    to statistical tests. These transformations alter the shape of the distribution
    of the feature drastically—the more extreme an outlier, the greater the effect
    of the log transformation, for example.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征的数学变换**：对于非正态数据，比较平均值是非常误导的，例如在存在异常值的情况下。非参数统计使我们能够对高度偏斜的数据做出有意义的观察。使用对数或平方根函数对这些值进行变换往往会在许多情况下使数据归一化，或者使它们更容易进行统计测试。这些变换会极大地改变特征的分布形状——异常值越极端，对数变换的影响就越大，例如。'
- en: '**Handling outliers using robust statistical algorithms in machine learning
    models**: Many classification algorithms which we discuss in the next section
    on modeling, implicitly or explicitly handle outliers. Bagging and Boosting variants,
    which work as meta-learning frameworks, are generally resilient to outliers or
    noisy data points and may not need a preprocessing step to handle them.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在机器学习模型中使用鲁棒统计算法处理异常值**：我们将在下一节建模中讨论的许多分类算法，隐式或显式地处理异常值。作为元学习框架的Bagging和Boosting变体，通常对异常值或噪声数据点具有弹性，可能不需要预处理步骤来处理它们。'
- en: '**Normalization**: Many algorithms—distance-based methods are a case in point—are
    very sensitive to the scale of the features. Preprocessing the numeric features
    makes sure that all of them are in a well-behaved range. The most well-known techniques
    of normalization of features are given here:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化**：许多算法——基于距离的方法就是一个例子——对特征的规模非常敏感。预处理数值特征确保它们都在一个良好的行为范围内。这里给出了特征归一化的最知名技术：'
- en: '**Min-Max Normalization**: In this technique, given the range *[L,U]*, which
    is typically *[0,1]*, each feature with value *x* is normalized in terms of the
    minimum and maximum values, *x*[max] and *x*[min], respectively, using the formula:![Outliers](img/B05137_02_016.jpg)'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小-最大标准化**：在这种技术中，给定范围 *[L,U]*，通常是 *[0,1]*，每个具有值 *x* 的特征分别使用最小值和最大值 *x*[max]
    和 *x*[min] 进行归一化，使用以下公式：![异常值](img/B05137_02_016.jpg)'
- en: '**Z-Score Normalization**: In this technique, also known as standardization,
    the feature values get auto-transformed so that the mean is 0 and standard deviation
    is 1\. The technique to transform is as follows: for each feature *f*, the mean
    value µ(*f*) and standard deviation σ(*f*) are computed and then the feature with
    value *x* is transformed as:'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Z分数标准化**：在这种技术中，也称为标准化，特征值会自动转换，使得平均值是0，标准差是1。转换的技术如下：对于每个特征 *f*，计算其均值 µ(*f*)
    和标准差 σ(*f*)，然后将具有值 *x* 的特征转换如下：'
- en: '![Outliers](img/B05137_02_019.jpg)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![异常值](img/B05137_02_019.jpg)'
- en: Discretization
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离散化
- en: 'Many algorithms can only handle categorical values or nominal values to be
    effective, for example Bayesian Networks. In such cases, it becomes imperative
    to discretize the numeric features into categories using either supervised or
    unsupervised methods. Some of the techniques discussed are:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法只能处理分类值或名义值才能有效，例如贝叶斯网络。在这种情况下，将数值特征通过监督或无监督方法离散化为类别变得至关重要。讨论的一些技术包括：
- en: '**Discretization by binning**: This technique is also referred to as equal
    width discretization. The entire scale of data for each feature *f*, ranging from
    values *x*[max] and *x*[min] is divided into a predefined number, *k*, of equal
    intervals, each having the width ![Discretization](img/B05137_02_020.jpg). The
    "cut points" or discretization intervals are:![Discretization](img/B05137_02_021.jpg)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过分箱进行离散化**：这种技术也被称为等宽离散化。对于每个特征 *f* 的整个数据范围，从值 *x*[max] 到 *x*[min]，被划分为预定义的
    *k* 个等间隔，每个间隔的宽度为 ![离散化](img/B05137_02_020.jpg)。"切割点"或离散化间隔如下：![离散化](img/B05137_02_021.jpg)'
- en: '**Discretization by frequency**: This technique is also referred to as equal
    frequency discretization. The feature is sorted and then the entire data is discretized
    into predefined *k* intervals, such that each interval contains the same proportion.
    Both the techniques, discretization by binning and discretization by frequency,
    suffer from loss of information due to the predefined value of *k*.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过频率进行离散化**：这种技术也被称为等频率离散化。特征被排序后，整个数据被离散化为预定义的 *k* 个间隔，使得每个间隔包含相同比例的数据。这两种技术，通过分箱和通过频率的离散化，由于预定义的
    *k* 值而损失信息。'
- en: '**Discretization by entropy**: Given the labels, the entropy is calculated
    over the split points where the value changes in an iterative way, so that the
    bins of intervals are as pure or discriminating as possible. Refer to the *Feature
    evaluation techniques* section for entropy-based (information gain) theory and
    calculations.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过熵进行离散化**：给定标签，计算在值变化的迭代点处的熵，使得区间的箱子尽可能纯净或具有区分性。请参阅*特征评估技术*部分，了解基于熵（信息增益）的理论和计算。'
- en: Data sampling
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据采样
- en: The dataset one receives may often require judicious sampling in order to effectively
    learn from the data. The characteristics of the data as well as the goals of the
    modeling exercise determine whether sampling is needed, and if so, how to go about
    it. Before we begin to learn from this data it is crucially important to create
    train, validate, and test data samples, as explained in this section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接收到的数据集可能经常需要谨慎采样，以便有效地从数据中学习。数据的特征以及建模练习的目标决定了是否需要采样，以及如何进行。在我们开始从这些数据中学习之前，创建训练、验证和测试数据样本至关重要，如本节所述。
- en: Is sampling needed?
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 是否需要采样？
- en: When the dataset is large or noisy, or skewed towards one type, the question
    as to whether to sample or not to sample becomes important. The answer depends
    on various aspects such as the dataset itself, the objective and the evaluation
    criteria used for selecting the models, and potentially other practical considerations.
    In some situations, algorithms have scalability issues in memory and space, but
    work effectively on samples, as measured by model performance with respect to
    the regression or classification goals they are expected to achieve. For example,
    SVM scales as *O(n*²*)* and *O(n*³*)* in memory and training times, respectively.
    In other situations, the data is so imbalanced that many algorithms are not robust
    enough to handle the skew. In the literature, the step intended to re-balance
    the distribution of classes in the original data extract by creating new training
    samples is also called **resampling**.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集很大或噪声较多，或者偏向于某一类型时，是否进行采样的问题变得重要。答案取决于各种方面，如数据集本身、目标以及用于选择模型的评估标准，以及可能的其他实际考虑。在某些情况下，算法在内存和空间方面存在可扩展性问题，但通过模型性能（根据它们预期实现的回归或分类目标）在样本上工作得很好。例如，SVM在内存和训练时间上的可扩展性分别为*O(n²)*和*O(n³)*。在其他情况下，数据的不平衡程度如此之高，以至于许多算法不足以处理偏斜。在文献中，通过创建新的训练样本来重新平衡原始数据集中类别分布的步骤也被称为**重采样**。
- en: Undersampling and oversampling
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欠采样和过采样
- en: Datasets exhibiting a marked imbalance in the distribution of classes can be
    said to contain a distinct minority class. Often, this minority class is the set
    of instances that we are especially interested in precisely because its members
    occur in such rare cases. For example, in credit card fraud, less than 0.1% of
    the data belongs to fraud. This skewness is not conducive to learning; after all,
    when we seek to minimize the total error in classification, we give equal weight
    to all classes regardless of whether one class is underrepresented compared to
    another. In binary classification problems, we call the minority class the positive
    class and the majority class as the negative class, a convention that we will
    follow in the following discussion.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在类别分布上存在明显不平衡的数据集可以称为包含一个独特的少数类。通常，这个少数类是我们特别感兴趣的实例集合，因为其成员在如此罕见的情况下出现。例如，在信用卡欺诈中，不到0.1%的数据属于欺诈。这种偏斜不利于学习；毕竟，当我们寻求最小化分类的总误差时，我们给予所有类别相同的权重，无论一个类别相对于另一个类别是否代表性不足。在二元分类问题中，我们将少数类称为正类，将多数类称为负类，这是我们将在以下讨论中遵循的惯例。
- en: Undersampling of the majority class is a technique that is commonly used to
    address skewness in data. Taking credit-card fraud as an example, we can create
    different training samples from the original dataset such that each sample has
    all the fraud cases from the original dataset, whereas the non-fraud instances
    are distributed across all the training samples in some fixed ratios. Thus, in
    a given training set created by this method, the majority class is now underrepresented
    compared to the original skewed dataset, effectively balancing out the distribution
    of classes. Training samples with labeled positive and labeled negative instances
    in ratios of, say, 1:20 to 1:50 can be created in this way, but care must be taken
    that the sample of negative instances used should have similar characteristics
    to the data statistics and distributions of the main datasets. The reason for
    using multiple training samples, and in different proportions of positive and
    negative instances, is so that any sampling bias that may be present becomes evident.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 大类欠采样是一种常用的技术，用于解决数据中的偏斜问题。以信用卡欺诈为例，我们可以从原始数据集中创建不同的训练样本，使得每个样本都包含原始数据集中的所有欺诈案例，而非欺诈实例则以某种固定的比例分布在所有训练样本中。因此，通过这种方法创建的给定训练集中，与原始偏斜数据集相比，多数类现在代表性不足，从而有效地平衡了类别的分布。可以通过这种方式创建带有标记的正例和负例实例，比例为，例如，1:20到1:50，但必须注意，使用的负例样本应具有与主要数据集的数据统计和分布相似的特征。使用多个训练样本，以及正负实例的不同比例，是为了使任何可能存在的采样偏差变得明显。
- en: Alternatively, we may choose to oversample the minority class. As before, we
    create multiple samples wherein instances from the minority class have been selected
    by either sampling with replacement or without replacement from the original dataset.
    When sampling without replacement, there are no replicated instances across samples.
    With replacement, some instances may be found in more than one sample. After this
    initial seeding of the samples, we can produce more balanced distributions of
    classes by random sampling with replacement from within the minority class in
    each sample until we have the desired ratios of positive to negative instances.
    Oversampling can be prone to over-fitting as classification decision boundaries
    tend to become more specific due to replicated values. **SMOTE** (**Synthetic
    Minority Oversampling Technique**) is a technique that alleviates this problem
    by creating synthetic data points in the interstices of the feature space by interpolating
    between neighboring instances of the positive class (*References* [20]).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以选择对少数类进行过采样。和之前一样，我们创建多个样本，其中少数类的实例是通过从原始数据集中有放回或无放回地采样选出的。当无放回地采样时，样本之间没有重复的实例。有放回地采样时，某些实例可能出现在多个样本中。在完成样本的初始播种后，我们可以通过在每个样本中对少数类进行随机有放回采样，直到我们得到所需的正负实例比例，从而产生更平衡的类别分布。过采样可能导致过拟合，因为分类决策边界由于重复值而变得更加具体。**SMOTE（合成少数类过采样技术**）是一种通过在特征空间中创建合成数据点来缓解此问题的技术，它通过在正类相邻实例之间进行插值来实现（*参考文献*
    [20]）。
- en: Stratified sampling
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分层采样
- en: Creating samples so that data with similar characteristics is drawn in the same
    proportion as they appear in the population is known as stratified sampling. In
    multi-class classification, if there are *N* classes each in a certain proportion,
    then samples are created such that they represent each class in the same proportion
    as in the original dataset. Generally, it is good practice to create multiple
    samples to train and test the models to validate against biases of sampling.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 创建样本，使得具有相似特征的数据以与它们在总体中出现的相同比例被抽取，这被称为分层采样。在多类分类中，如果有 *N* 个类别，每个类别以一定的比例存在，那么创建的样本将代表原始数据集中每个类别的相同比例。通常，创建多个样本来训练和测试模型，以验证采样偏差是良好的实践。
- en: Training, validation, and test set
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练集、验证集和测试集
- en: The Holy Grail of creating good classification models is to train on a set of
    good quality, representative, (training data), tune the parameters and find effective
    models (validation data), and finally, estimate the model's performance by its
    behavior on unseen data (test data).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 创建良好的分类模型的圣杯是在一组高质量、具有代表性的（训练数据）上训练，调整参数并找到有效的模型（验证数据），最后，通过其在未见数据上的行为来估计模型的表现（测试数据）。
- en: The central idea behind the logical grouping is to make sure models are validated
    or tested on data that has not been seen during training. Otherwise, a simple
    "rote learner" can outperform the algorithm. The generalization capability of
    the learning algorithm must be evaluated on a dataset which is different from
    the training dataset, but comes from the same population (*References* [11]).
    The balance between removing too much data from training to increase the budget
    of validation and testing can result in models which suffer from "underfitting",
    that is, not having enough examples to build patterns that can help in generalization.
    On the other hand, the extreme choice of allocating all the labeled data for training
    and not performing any validation or testing can lead to "overfitting", that is,
    models that fit the examples too faithfully and do not generalize well enough.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑分组背后的核心思想是确保模型在训练期间未见过的数据上进行验证或测试。否则，一个简单的“死记硬背的学习者”可能会优于算法。学习算法的泛化能力必须在不同于训练数据集但来自同一总体的数据集上评估（*参考文献*
    [11]）。在从训练数据中移除过多数据以增加验证和测试预算之间取得平衡可能导致模型“欠拟合”，即没有足够的例子来构建有助于泛化的模式。另一方面，将所有标记数据分配给训练，而不进行任何验证或测试的极端选择可能导致“过拟合”，即模型过于忠实于例子，泛化能力不足。
- en: Typically, in most machine learning challenges and real world customer problems,
    one is given a training set and testing set upfront for evaluating the performance
    of the models. In these engagements, the only question is how to validate and
    find the most effective parameters given the training set. In some engagements,
    only the labeled dataset is given and you need to consider the training, validation,
    and testing sets to make sure your models do not overfit or underfit the data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在大多数机器学习挑战和现实世界的客户问题中，人们一开始就得到一个训练集和一个测试集来评估模型的性能。在这些合作中，唯一的问题是如何在给定的训练集的基础上验证并找到最有效的参数。在某些合作中，只提供了标记的数据集，你需要考虑训练、验证和测试集，以确保你的模型不会过度拟合或欠拟合数据。
- en: Three logical processes are needed for modeling and hence three logical datasets
    are needed, namely, training, validation, and testing. The purpose of the training
    dataset is to give labeled data to the learning algorithm to build the models.
    The purpose of the validation set is to see the effect of the parameters of the
    training model being evaluated by training on the validation set. Finally, the
    best parameters or models are retrained on the combination of the training and
    validation sets to find an optimum model that is then tested on the blind test
    set.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 建模需要三个逻辑过程，因此需要三个逻辑数据集，即训练、验证和测试。训练数据集的目的是向学习算法提供标记数据以构建模型。验证集的目的是通过在验证集上训练来观察被评估的训练模型参数的效果。最后，在训练和验证集的组合上重新训练最佳参数或模型，以找到一个最优模型，然后在该模型上测试盲测试集。
- en: '![Training, validation, and test set](img/B05137_02_022.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![训练、验证和测试集](img/B05137_02_022.jpg)'
- en: 'Figure 1: Training, Validation, and Test data and how to use them'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：训练、验证和测试数据及其使用方法
- en: 'Two things affect the learning or the generalization capability: the choice
    of the algorithm (and its parameters) and number of training data. This ability
    to generalize can be estimated by various metrics including the prediction errors.
    The overall estimate of unseen error or risk of the model is given by:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 两个因素会影响学习或泛化能力：算法的选择（及其参数）和训练数据的数量。这种泛化能力可以通过包括预测误差在内的各种指标来估计。模型对未见数据的总体误差估计或风险由以下给出：
- en: '![Training, validation, and test set](img/B05137_02_023.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![训练、验证和测试集](img/B05137_02_023.jpg)'
- en: Here, *Noise* is the stochastic noise, *Var (G,n)* is called the variance error
    and is a measure of how susceptible our hypothesis or the algorithm *(G)* is,
    if given different datasets. ![Training, validation, and test set](img/B05137_02_023a.jpg)
    is called the bias error and represents how far away the best algorithm in the
    model (average learner over all possible datasets) is from the optimal one.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*噪声*是随机噪声，*Var (G,n)*被称为方差误差，它是衡量我们的假设或算法*(G)*如果给定不同的数据集有多敏感的度量。![训练、验证和测试集](img/B05137_02_023a.jpg)被称为偏差误差，表示模型中最佳算法（所有可能数据集上的平均学习者）与最优算法的距离。
- en: Learning curves as shown in *Figure 2* and *Figure 3*—where training and testing
    errors are plotted keeping either the algorithm with its parameters constant or
    the training data size constant—give an indication of underfitting or overfitting.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图2*和*图3*所示的学习曲线——其中训练和测试错误是固定的算法及其参数或训练数据大小——可以提供对欠拟合或过拟合的指示。
- en: When the training data size is fixed, different algorithms or the same algorithms
    with different parameter choices can exhibit different learning curves. The *Figure
    2* shows two cases of algorithms on the same data size giving two different learning
    curves based on bias and variance.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练数据量固定时，不同的算法或具有不同参数选择的相同算法可以表现出不同的学习曲线。*图2*显示了在相同数据量下，两种算法给出基于偏差和方差的两种不同学习曲线的情况。
- en: '![Training, validation, and test set](img/B05137_02_028.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![训练、验证和测试集](img/B05137_02_028.jpg)'
- en: 'Figure 2: The training data relationship with error rate when the model complexity
    is fixed indicates different choices of models.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：当模型复杂度固定时，训练数据与错误率的关系表明了不同模型的选择。
- en: 'The algorithm or model choice also impacts model performance. A complex algorithm,
    with more parameters to tune, can result in overfitting, while a simple algorithm
    with less parameters might be underfitting. The classic figure to illustrate the
    model performance and complexity when the training data size is fixed is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 算法或模型选择也会影响模型性能。一个复杂的算法，具有更多可调整的参数，可能导致过拟合，而一个简单的算法，参数较少，可能存在欠拟合。当训练数据量固定时，以下经典图示了模型性能和复杂性的关系：
- en: '![Training, validation, and test set](img/B05137_02_030.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![训练、验证和测试集](img/B05137_02_030.jpg)'
- en: 'Figure 3: The Model Complexity relationship with Error rate, over the training
    and the testing data when training data size is fixed.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：当训练数据量固定时，模型复杂性与训练和测试数据中的错误率的关系。
- en: 'Validation allows for exploring the parameter space to find the model that
    generalizes best. Regularization (will be discussed in linear models) and validation
    are two mechanisms that should be used for preventing overfitting. Sometimes the
    "k-fold cross-validation" process is used for validation, which involves creating
    *k* samples of the data and using *(k – 1)* to train on and the remaining one
    to test, repeated *k* times to give an average estimate. The following figure
    shows 5-fold cross-validation as an example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 验证允许探索参数空间以找到最佳泛化模型。正则化（将在线性模型中讨论）和验证是两种用于防止过拟合的机制。有时使用“k折交叉验证”过程进行验证，这涉及到创建*k*个数据样本，使用*(k
    – 1)*个进行训练，剩余的一个进行测试，重复*k*次以给出平均估计。以下图示了5折交叉验证作为示例：
- en: '![Training, validation, and test set](img/B05137_02_034.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![训练、验证和测试集](img/B05137_02_034.jpg)'
- en: 'Figure 4: 5-fold cross-validation.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：5折交叉验证。
- en: 'The following are some commonly used techniques to perform data sampling, validation,
    and learning:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些常用的数据采样、验证和学习的技巧：
- en: '**Random split of training, validation, and testing**: 60, 20, 20\. Train on
    60%, use 20% for validation, and then combine the train and validation datasets
    to train a final model that is used to test on the remaining 20%. Split may be
    done randomly, based on time, based on region, and so on.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练、验证和测试的随机分割**：60%，20%，20%。在60%的数据上训练，使用20%进行验证，然后将训练集和验证集合并以训练一个最终模型，用于在剩余的20%上进行测试。分割可以是随机的，基于时间、基于地区等。'
- en: '**Training, cross-validation, and testing**: Split into Train and Test two
    to one, do validation using cross-validation on the train set, train on whole
    two-thirds and test on one-third. Split may be done randomly, based on time, based
    on region, and so on.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练、交叉验证和测试**：分为训练集和测试集各占三分之一，在训练集上使用交叉验证进行验证，在全部三分之二的数据上训练，在三分之一的数据上测试。分割可以是随机的，基于时间、基于地区等。'
- en: '**Training and cross-validation**: When the training set is small and only
    model selection can be done without much parameter tuning. Run cross-validation
    on the whole dataset and chose the best models with learning on the entire dataset.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练和交叉验证**：当训练集较小时，只能进行模型选择而不需要大量参数调整。在整个数据集上运行交叉验证，并选择在整个数据集上学习的最佳模型。'
- en: Feature relevance analysis and dimensionality reduction
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征相关性分析和降维
- en: The goal of feature relevance and selection is to find the features that are
    discriminating with respect to the target variable and help reduce the dimensions
    of the data [1,2,3]. This improves the model performance mainly by ameliorating
    the effects of the curse of dimensionality and by removing noise due to irrelevant
    features. By carefully evaluating models on the validation set with and without
    features removed, we can see the impact of feature relevance. Since the exhaustive
    search for *k* features involves 2^k – 1 sets (consider all combinations of *^k*
    features where each feature is either retained or removed, disregarding the degenerate
    case where none is present) the corresponding number of models that have to be
    evaluated can become prohibitive, so some form of heuristic search techniques
    are needed. The most common of these techniques are described next.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 特征相关性和选择的目标是找到对目标变量有区分性的特征，并帮助减少数据的维度[1,2,3]。这主要通过改善维度灾难的影响和去除无关特征的噪声来提高模型性能。通过仔细评估在验证集上添加和去除特征时的模型，我们可以看到特征相关性的影响。由于对*k*个特征的穷举搜索涉及2^k
    – 1个集合（考虑所有*k*个特征的组合，每个特征要么保留要么去除，不考虑一个特征都不存在的退化情况），因此需要评估的模型数量可能变得不可接受，因此需要某种启发式搜索技术。以下将描述这些技术中最常见的一些。
- en: Feature search techniques
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征搜索技术
- en: 'Some of the very common search techniques employed to find feature sets are:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一些非常常见的搜索技术被用来寻找特征集：
- en: '**Forward or hill climbing**: In this search, one feature is added at a time
    until the evaluation module outputs no further change in performance.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前向或爬山搜索**：在这种搜索中，每次添加一个特征，直到评估模块输出性能不再进一步变化。'
- en: '**Backward search**: Starting from the whole set, one feature at a time is
    removed until no performance improvement occurs. Some applications interleave
    both forward and backward techniques to search for features.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向搜索**：从整个集合开始，每次移除一个特征，直到不再发生性能改进。某些应用程序交替使用前向和反向技术来搜索特征。'
- en: '**Evolutionary search**: Various evolutionary techniques such as genetic algorithms
    can be used as a search mechanism and the evaluation metrics from either filter-
    or wrapper-based methods can be used as fitness criterion to guide the process.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进化搜索**：可以使用各种进化技术，如遗传算法，作为搜索机制，并且可以使用基于过滤器或包装器的评估指标作为适应度标准来指导过程。'
- en: Feature evaluation techniques
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征评估技术
- en: At a high level, there are three basic methods to evaluate features.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，评估特征有三个基本方法。
- en: Filter approach
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤器方法
- en: This approach refers to the use of techniques without using machine learning
    algorithms for evaluation. The basic idea of the filter approach is to use a search
    technique to select a feature (or subset of features) and measure its importance
    using some statistical measure until a stopping criterion is reached.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法指的是使用技术而不使用机器学习算法进行评估。过滤器方法的基本思想是使用搜索技术选择一个特征（或特征子集）并使用某种统计量来衡量其重要性，直到达到停止标准。
- en: Univariate feature selection
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单变量特征选择
- en: This search is as simple as ranking each feature based on the statistical measure
    employed.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这种搜索与根据使用的统计量对每个特征进行排名一样简单。
- en: Information theoretic approach
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 信息论方法
- en: All information theoretic approaches use the concept of entropy mechanism at
    their core. The idea is that if the feature is randomly present in the dataset,
    there is maximum entropy, or, equivalently, the ability to compress or encode
    is low, and the feature may be irrelevant. On the other hand, if the distribution
    of the feature value is such some range of values are more prevalent in one class
    relative to the others, then the entropy is minimized and the feature is discriminating.
    Casting the problem in terms of entropy in this way requires some form of discretization
    to convert the numeric features into categories in order to compute the probabilities.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 所有信息论方法都以熵机制为核心概念。其思想是，如果特征在数据集中随机出现，则熵最大，或者说，压缩或编码的能力低，特征可能是不相关的。另一方面，如果特征值的分布使得某些值在一个类别中相对于其他类别更普遍，那么熵最小化，特征具有区分性。以这种方式将问题表述为熵需要某种形式的离散化，以便将数值特征转换为类别，以便计算概率。
- en: 'Consider a binary classification problem with training data *D*[X]. If *X*[i]
    is the *i*^(th) feature with *v* distinct categorical values such that *D*[Xi]
    *= {D*[1]*, D*[2]*… D*[v]*}*, then information or the entropy in feature *X*[i]
    is:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有训练数据 *D*[X] 的二分类问题。如果 *X*[i] 是具有 *v* 个不同分类值的 *i*^(th) 特征，使得 *D*[Xi] *=
    {D*[1]*, D*[2]*… D*[v]*}*，那么特征 *X*[i] 中的信息或熵为：
- en: '![Information theoretic approach](img/B05137_02_041.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![信息论方法](img/B05137_02_041.jpg)'
- en: 'Here, *Info(D*[j]*)* is the entropy of the partition and is calculated as:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*Info(D*[j]*)* 是分割的熵，其计算如下：
- en: '![Information theoretic approach](img/B05137_02_043.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![信息论方法](img/B05137_02_043.jpg)'
- en: Here, *p*[+]*(D)* is the probability that the data in set *D* is in the positive
    class and *p_(D)* is the probability that it is in the negative class, in that
    sample. Information gain for the feature is calculated in terms of the overall
    information and information of the feature as
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*p*[+]*(D)* 是数据集 *D* 中的数据属于正类的概率，而 *p_(D)* 是它属于负类的概率，在该样本中。特征的信息增益是根据整体信息和特征信息来计算的
- en: '*InfoGain(X*[i]*) = Info(D) – Info(D*[Xi]*)*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*InfoGain(X*[i]*) = Info(D) – Info(D*[Xi]*)*'
- en: For numeric features, the values are sorted in ascending order and split points
    between neighboring values are considered as distinct values.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值特征，值按升序排序，并且考虑相邻值之间的分割点作为不同的值。
- en: 'The greater the decrease in entropy, the higher the relevance of the feature.
    Information gain has problems when the feature has a large number of values; that
    is when Gain Ratio comes in handy. Gain Ratio corrects the information gain over
    large splits by introducing Split Information. Split Information for feature *X*[i]
    and *GainRatio* is given by:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 熵的减少越大，特征的相关性就越高。当特征具有大量值时，信息增益存在问题；这时增益比率就派上用场。增益比率通过引入分割信息来纠正大分割的信息增益。特征*X*[i]和*GainRatio*的分割信息如下：
- en: '![Information theoretic approach](img/B05137_02_048.jpg)![Information theoretic
    approach](img/B05137_02_049.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![信息论方法](img/B05137_02_048.jpg)![信息论方法](img/B05137_02_049.jpg)'
- en: There are other impurity measures such as Gini Impurity Index (as described
    in the section on the *Decision Tree* algorithm) and Uncertainty-based measures
    to compute feature relevance.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他杂质度量，如基尼杂质指数（如*决策树*算法部分所述）和基于不确定性的度量来计算特征相关性。
- en: Statistical approach
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 统计方法
- en: 'Chi-Squared feature selection is one of the most common feature selection methods
    that has statistical hypothesis testing as its base. The null hypothesis is that
    the feature and the class variable are independent of each other. The numeric
    features are discretized so that all features have categorical values. The contingency
    table is calculated as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方特征选择是最常见的特征选择方法之一，其基础是统计假设检验。零假设是特征和类别变量相互独立。数值特征被离散化，以便所有特征都具有分类值。列联表的计算如下：
- en: '| Feature Values | Class=P | Class=N | Sum over classes *niP* + *niN* |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 特征值 | 类别=P | 类别=N | 对类别 *niP* + *niN* 求和 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *X*[1] | (*n*[1P]&#124;*µ*[1P]) | (*n*[1N]&#124;*µ*[1N]) | *n*[1] |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| *X*[1] | (*n*[1P]&#124;*µ*[1P]) | (*n*[1N]&#124;*µ*[1N]) | *n*[1] |'
- en: '| …. | … | …. | … |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| …. | … | …. | … |'
- en: '| *X*[m] | (*n*[mP]&#124;*µ*[mP]) | (*n*[mN]&#124;*µ*[mN]) | *n*[m] |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| *X*[m] | (*n*[mP]&#124;*µ*[mP]) | (*n*[mN]&#124;*µ*[mN]) | *n*[m] |'
- en: '|   | *n*[*P] | *n*[*P] | *n* |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|   | *n*[*P] | *n*[*P] | *n* |'
- en: '*Contingency Table 1: Showing feature values and class distribution for binary
    class.*'
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*列联表1：显示二元类别的特征值和类别分布。*'
- en: In the preceding table, *n*[ij] is a count of the number of features with value—after
    discretization—equal to *x*[i] and class value of *j*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表中，*n*[ij]是具有值——在离散化后——等于*x*[i]和类别值*j*的特征的数量。
- en: 'The value summations are:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 值求和如下：
- en: '![Statistical approach](img/B05137_02_065.jpg)![Statistical approach](img/B05137_02_066.jpg)![Statistical
    approach](img/B05137_02_067.jpg)![Statistical approach](img/B05137_02_068.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![统计方法](img/B05137_02_065.jpg)![统计方法](img/B05137_02_066.jpg)![统计方法](img/B05137_02_067.jpg)![统计方法](img/B05137_02_068.jpg)'
- en: Here *n* is number of data instances, *j = P, N* is the class value and *i =1,2,
    … m* indexes the different discretized values of the feature and the table has
    *m – 1* degrees of freedom.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*n*是数据实例的数量，*j = P, N*是类别值，*i =1,2, … m*是特征的不同离散化值的索引，该表有*m – 1*个自由度。
- en: 'The Chi-Square Statistic is given by:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方统计量由以下公式给出：
- en: '![Statistical approach](img/B05137_02_072.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![统计方法](img/B05137_02_072.jpg)'
- en: The Chi-Square value is compared to confidence level thresholds for testing
    significance. For example, for *i = 2*, the Chi-Squared value at threshold of
    5% is 3.84; if our value is smaller than the table value of 3.83, then we know
    that the feature is interesting and the null hypothesis is rejected.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方值用于与置信水平阈值进行比较以测试显著性。例如，对于*i = 2*，5%置信水平下的卡方值为3.84；如果我们的值小于表格中的3.83，那么我们知道该特征是有趣的，并且零假设被拒绝。
- en: Multivariate feature selection
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多变量特征选择
- en: 'Most multivariate methods of feature selection have two goals:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数多变量特征选择方法有两个目标：
- en: Reduce the redundancy between the feature and other selected features
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少特征与其他选定的特征之间的冗余
- en: Maximize the relevance or correlation of the feature with the class label
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化特征与类别标签的相关性或相关性
- en: The task of finding such subsets of features cannot be exhaustive as the process
    can have a large search space. Heuristic search methods such as backward search,
    forward search, hill-climbing, and genetic algorithms are typically used to find
    a subset of features. Two very well-known evaluation techniques for meeting the
    preceding goals are presented next.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找此类特征子集的任务不能穷尽，因为该过程可能具有很大的搜索空间。通常使用启发式搜索方法，如回溯搜索、前向搜索、爬山法和遗传算法来找到特征子集。接下来将介绍两种非常著名的评估技术，以实现上述目标。
- en: Minimal redundancy maximal relevance (mRMR)
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最小冗余最大相关性（mRMR）
- en: In this technique, numeric features are often discretized—as done in univariate
    pre-processing—to get distinct categories of values.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种技术中，数值特征通常被离散化——就像在单变量预处理中做的那样——以获得不同的值类别。
- en: 'For each subset *S*, the redundancy between two features *X*[i] and *X*[j]
    can be measured as:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个子集 *S*，两个特征 *X*[i] 和 *X*[j] 之间的冗余可以测量如下：
- en: '![Minimal redundancy maximal relevance (mRMR)](img/B05137_02_077.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![最小冗余最大相关性 (mRMR)](img/B05137_02_077.jpg)'
- en: 'Here, *MI (X*[i]*, X*[j]*)* = measure of mutual information between two features
    *X*[i] and *X*[j]. Relevance between feature *X*[i] and class *C* can be measured
    as:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*MI (X*[i]*, X*[j]*)* = 两个特征 *X*[i] 和 *X*[j] 之间的互信息度量。特征 *X*[i] 与类别 *C* 之间的相关性可以测量如下：
- en: '![Minimal redundancy maximal relevance (mRMR)](img/B05137_02_081.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![最小冗余最大相关性 (mRMR)](img/B05137_02_081.jpg)'
- en: 'Also, the two goals can be combined to find the best feature subset using:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以将这两个目标结合起来，使用以下方法找到最佳特征子集：
- en: '![Minimal redundancy maximal relevance (mRMR)](img/B05137_02_082.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![最小冗余最大相关性 (mRMR)](img/B05137_02_082.jpg)'
- en: Correlation-based feature selection (CFS)
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于相关性的特征选择 (CFS)
- en: 'The basic idea is similar to the previous example; the overall merit of subset
    *S* is measured as:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想与前面的例子相似；子集 *S* 的整体优点被测量为：
- en: '![Correlation-based feature selection (CFS)](img/B05137_02_083.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![基于相关性的特征选择 (CFS)](img/B05137_02_083.jpg)'
- en: Here, *k* is the total number of features, ![Correlation-based feature selection
    (CFS)](img/B05137_02_084.jpg) is the average feature class correlation and ![Correlation-based
    feature selection (CFS)](img/B05137_02_085.jpg) is the average feature-feature
    inter correlation. The numerator gives the relevance factor while the denominator
    gives the redundancy factor and hence the goal of the search is to maximize the
    overall ratio or the *Merit (S)*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*k* 是特征的总数，![基于相关性的特征选择 (CFS)](img/B05137_02_084.jpg) 是平均特征类别相关性，![基于相关性的特征选择
    (CFS)](img/B05137_02_085.jpg) 是平均特征间互相关性。分子给出相关性因子，分母给出冗余因子，因此搜索的目标是最大化整体比率或*优点
    (S)*。
- en: There are other techniques such as Fast-Correlation-based feature selection
    that is based on the same principles, but with variations in computing the metrics.
    Readers can experiment with this and other techniques in Weka.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他技术，如基于快速相关性的特征选择，它基于相同的原则，但在计算指标方面有所变化。读者可以在Weka中尝试这些和其他技术。
- en: The advantage of the Filter approach is that its methods are independent of
    learning algorithms and hence one is freed from choosing the algorithms and parameters.
    They are also faster than wrapper-based approaches.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤方法的优势在于其方法与学习算法无关，因此可以免除选择算法和参数的麻烦。它们也比基于包装的方法更快。
- en: Wrapper approach
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 包装方法
- en: The search technique remains the same as discussed in the feature search approach;
    only the evaluation method changes. In the wrapper approach, a machine learning
    algorithm is used to evaluate the subset of features that are found to be discriminating
    based on various metrics. The machine learning algorithm used as the wrapper approach
    may be the same or different from the one used for modeling.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索技术与在特征搜索方法中讨论的方法相同；只是评估方法不同。在包装方法中，使用机器学习算法来评估基于各种指标发现的具有区分性的特征子集。用作包装方法的机器学习算法可能与用于建模的算法相同或不同。
- en: Most commonly, cross-validation is used in the learning algorithm. Performance
    metrics such as area under curve or F-score, obtained as an average on cross-validation,
    guide the search process. Since the cost of training and evaluating models is
    very high, we choose algorithms that have fast training speed, such as Linear
    Regression, linear SVM, or ones that are Decision Tree-based.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的交叉验证用于学习算法。性能指标，如曲线下面积或F分数，作为交叉验证的平均值，指导搜索过程。由于训练和评估模型的成本非常高，我们选择具有快速训练速度的算法，如线性回归、线性SVM或基于决策树的算法。
- en: Some wrapper approaches have been very successful using specific algorithms
    such as Random Forest to measure feature relevance.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一些包装方法使用特定的算法，如随机森林来衡量特征相关性，已经非常成功。
- en: Embedded approach
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入方法
- en: This approach does not require feature search techniques. Instead of performing
    feature selection as preprocessing, it is done in the machine learning algorithm
    itself. Rule Induction, Decision Trees, Random Forest, and so on, perform feature
    selection as part of the training algorithm. Some algorithms such as regression
    or SVM-based methods, known as **shrinking methods**, can add a regularization
    term in the model to overcome the impact of noisy features in the dataset. Ridge
    and lasso-based regularization are well-known techniques available in regressions
    to provide feature selection implicitly.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不需要特征搜索技术。不是在预处理中进行特征选择，而是在机器学习算法本身中进行。规则归纳、决策树、随机森林等算法在训练算法中执行特征选择。一些算法，如回归或基于SVM的方法，被称为**收缩方法**，可以在模型中添加正则化项以克服数据集中噪声特征的影响。岭回归和lasso正则化是回归中提供隐式特征选择的知名技术。
- en: There are other techniques using unsupervised algorithms that will be discussed
    in [Chapter 3](ch03.html "Chapter 3. Unsupervised Machine Learning Techniques"),
    *Unsupervised Machine Learning Techniques*, that can be used effectively in a
    supervised setting too, for example, **Principal Component Analysis** (**PCA**).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html "第3章。无监督机器学习技术")“无监督机器学习技术”中，将讨论其他使用无监督算法的技术，这些技术也可以在监督环境中有效地使用，例如，**主成分分析**（**PCA**）。
- en: Model building
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型构建
- en: In real-world problems, there are many constraints on learning and many ways
    to assess model performance on unseen data. Each modeling algorithm has its strengths
    and weaknesses when applied to a given problem or to a class of problems in a
    particular domain. This is articulated in the famous **No Free Lunch Theorem**
    (**NFLT**), which says—for the case of supervised learning—that averaged over
    all distributions of data, every classification algorithm performs about as well
    as any other, including one that always picks the same class! Application of NFLT
    to supervised learning and search and optimization can be found at [http://www.no-free-lunch.org/](http://www.no-free-lunch.org/).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的问题中，学习有许多约束，评估模型在未见数据上的性能也有许多方法。每个建模算法在应用于特定问题或特定领域的问题类时都有其优势和劣势。这一点在著名的**无免费午餐定理**（**NFLT**）中得到了阐述，该定理指出——对于监督学习的情况——在所有数据分布的平均情况下，每个分类算法的表现几乎与其他任何算法一样好，包括总是选择同一类的算法！监督学习、搜索和优化的NFLT应用可以在[http://www.no-free-lunch.org/](http://www.no-free-lunch.org/)找到。
- en: In this section, we will discuss the most commonly used practical algorithms,
    giving the necessary details to answer questions such as what are the algorithm's
    inputs and outputs? How does it work? What are the advantages and limitations
    to consider while choosing the algorithm? For each model, we will include sample
    code and outputs obtained from testing the model on the chosen dataset. This should
    provide the reader with insights into the process. Some algorithms such as neural
    networks and deep learning, Bayesian networks, stream-based earning, and so on,
    will be covered separately in their own chapters.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论最常用的实用算法，提供必要的细节来回答诸如算法的输入和输出是什么？它是如何工作的？在选择算法时需要考虑哪些优势和局限性？对于每个模型，我们将包括测试所选数据集时获得的示例代码和输出。这应该为读者提供对过程的洞察。一些算法，如神经网络和深度学习、贝叶斯网络、基于流的机器学习等，将在各自的章节中单独介绍。
- en: Linear models
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性模型
- en: Linear models work well when the data is linearly separable. This should always
    be the first thing to establish.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据线性可分时，线性模型效果很好。这始终应该是首先要确定的事情。
- en: Linear Regression
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归
- en: Linear Regression can be used for both classification and estimation problems.
    It is one of the most widely used methods in practice. It consists of finding
    the best-fitting hyperplane through the data points.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归可用于分类和估计问题。它是实践中最广泛使用的方法之一。它包括通过数据点找到最佳拟合超平面。
- en: Algorithm input and output
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法输入和输出
- en: Features must be numeric. Categorical features are transformed using various
    pre-processing techniques, as when a categorical value becomes a feature with
    1 and 0 values. Linear Regression models output a categorical class in classification
    or numeric values in regression. Many implementations also give confidence values.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 特征必须是数值型的。分类特征使用各种预处理技术进行转换，例如，当分类值成为具有1和0值的特征时。线性回归模型在分类中输出一个分类类别，在回归中输出数值。许多实现还给出置信度值。
- en: How does it work?
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: The model tries to learn a "hyperplane" in the input space that minimizes the
    error between the data points of each class (*References* [4]).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 模型试图在输入空间中学习一个“超平面”，以最小化每个类别的数据点之间的误差（*参考文献* [4]）。
- en: 'A hyperplane in d-dimensional inputs that linear model learns is given by:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型在d维输入空间中学习的超平面由以下公式给出：
- en: '![How does it work?](img/B05137_02_087.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_087.jpg)'
- en: 'The two regions (binary classification) the model divides the input space into
    are ![How does it work?](img/B05137_02_088.jpg) and ![How does it work?](img/B05137_02_089.jpg).
    Associating a value of 1 to the coordinate of feature 0, that is, *x*0=1, the
    vector representation of hypothesis space or the model is:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将输入空间分割成两个区域（二元分类），分别是![如何工作？](img/B05137_02_088.jpg)和![如何工作？](img/B05137_02_089.jpg)。将特征0的坐标赋值为1，即*x*0=1，假设空间或模型的向量表示为：
- en: '![How does it work?](img/B05137_02_091.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_091.jpg)'
- en: 'The weight matrix can be derived using various methods such as ordinary least
    squares or iterative methods using matrix notation as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵可以通过各种方法推导出来，例如普通最小二乘法或使用矩阵表示的迭代方法，如下所示：
- en: '![How does it work?](img/B05137_02_092.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_092.jpg)'
- en: 'Here **X** is the input matrix and **y** is the label. If the matrix **X**^T**X**
    in the least squares problem is not of full rank or if encountering various numerical
    stability issues, the solution is modified as:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在最小二乘问题中，如果矩阵**X**^T**X**不是满秩的，或者遇到各种数值稳定性问题，解决方案将进行修改如下：
- en: '![How does it work?](img/B05137_02_096.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_096.jpg)'
- en: Here, ![How does it work?](img/B05137_02_097.jpg) is added to the diagonal of
    an identity matrix **I**[n] of size (*n* + 1, *n* + 1) with the rest of the values
    being set to 0\. This solution is called **ridge regression** and parameter λ
    theoretically controls the trade-off between the square loss and low norm of the
    solution. The constant λ is also known as regularization constant and helps in
    preventing "overfitting".
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![如何工作？](img/B05137_02_097.jpg)被添加到大小为(*n* + 1, *n* + 1)的单位矩阵**I**[n]的对角线上，其余值被设置为0。这种解决方案被称为**岭回归**，参数λ理论上控制了平方损失和解决方案低范数的权衡。常数λ也称为正则化常数，有助于防止“过拟合”。
- en: Advantages and limitations
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: It is an appropriate method to try and get insights when there are less than
    100 features and a few thousand data points.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当特征少于100个且数据点只有几千个时，这是一个尝试获取洞察力的合适方法。
- en: Interpretable to some level as the weights give insights on the impact of each
    feature.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一定程度上是可解释的，因为权重提供了对每个特征影响的洞察。
- en: Assumes linear relationship, additive and uncorrelated features, hence it doesn't
    model complex non-linear real-world data. Some implementations of Linear Regression
    allow removing collinear features to overcome this issue.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设线性关系、可加性和不相关特征，因此它不模拟复杂的非线性现实世界数据。一些线性回归的实现允许移除共线性特征来克服这个问题。
- en: Very susceptible to outliers in the data, if there are huge outliers, they have
    to be treated prior to performing Linear Regression.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常容易受到数据中的异常值的影响，如果有巨大的异常值，在进行线性回归之前必须先处理这些异常值。
- en: Heteroskedasticity, that is, unequal training point variances, can affect the
    simple least square regression models. Techniques such as weighted least squares
    are employed to overcome this situation.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异方差性，即训练点的方差不等，可能会影响简单的最小二乘回归模型。采用加权最小二乘等技术来克服这种情况。
- en: Naïve Bayes
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Based on the Bayes rule, the Naïve Bayes classifier assumes the features of
    the data are independent of each other (*References* [9]). It is especially suited
    for large datasets and frequently performs better than other, more elaborate techniques,
    despite its naïve assumption of feature independence.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝叶斯规则，朴素贝叶斯分类器假设数据的特征相互独立（*参考文献* [9]）。它特别适合于大型数据集，尽管它对特征独立性的假设是朴素的，但通常比其他更复杂的技术表现更好。
- en: Algorithm input and output
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法输入和输出
- en: The Naïve Bayes model can take features that are both categorical and continuous.
    Generally, the performance of Naïve Bayes models improves if the continuous features
    are discretized in the right format. Naïve Bayes outputs the class and the probability
    score for all class values, making it a good classifier for scoring models.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型可以接受既是分类又是连续的特征。通常，如果连续特征以正确的格式离散化，朴素贝叶斯模型的性能会得到提高。朴素贝叶斯输出所有类别的类别和概率分数，使其成为评分模型的良好分类器。
- en: How does it work?
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: It is a probability-based modeling algorithm. The basic idea is using Bayes'
    rule and measuring the probabilities of different terms, as given here. Measuring
    probabilities can be done either using pre-processing such as discretization,
    assuming a certain distribution, or, given enough data, mapping the distribution
    for numeric features.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个基于概率的建模算法。基本思想是使用贝叶斯定理并测量不同项的概率，如这里所示。测量概率可以通过预处理（如离散化）、假设某种分布或，如果数据足够多，映射数值特征的分布来完成。
- en: 'Bayes'' rule is applied to get the posterior probability as predictions and
    *k* represents *k*^(th) class.:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 应用贝叶斯定理以获得后验概率作为预测，其中*k*代表*k*^(th)类别。
- en: '![How does it work?](img/B05137_02_101.jpg)![How does it work?](img/B05137_02_102.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_101.jpg)![如何工作？](img/B05137_02_102.jpg)'
- en: Advantages and limitations
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: It is robust against isolated noisy data points because such points are averaged
    when estimating the probabilities of input data.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对孤立噪声数据点具有鲁棒性，因为在估计输入数据的概率时，这些点会被平均。
- en: Probabilistic scores as confidence values from Bayes classification can be used
    as scoring models.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从贝叶斯分类中得到的概率分数可以作为评分模型。
- en: Can handle missing values very well as they are not used in estimating probabilities.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够很好地处理缺失值，因为它们在估计概率时没有被使用。
- en: Also, it is robust against irrelevant attributes. If the features are not useful
    the probability distribution for the classes will be uniform and will cancel itself
    out.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，它对无关属性具有鲁棒性。如果特征没有用，类别的概率分布将是均匀的，并且会相互抵消。
- en: Very good in training speed and memory, it can be parallelized as each computation
    of probability in the equation is independent of the other.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练速度和内存方面非常好，它可以并行化，因为方程中每个概率的计算都是相互独立的。
- en: Correlated features can be a big issue when using Naïve Bayes because the conditional
    independence assumption is no longer valid.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用朴素贝叶斯时，相关特征可能成为一个大问题，因为条件独立性假设不再有效。
- en: Normal distribution of errors is an assumption in most optimization algorithms.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数优化算法都假设误差是正态分布的。
- en: Logistic Regression
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: If we employ Linear Regression model using, say, the least squares regression
    method, the outputs have to be converted to classes, say 0 and 1\. Many Linear
    Regression algorithms output class and confidence as probability. As a rule of
    thumb, if we see that the probabilities of Linear Regression are mostly beyond
    the ranges of 0.2 to 0.8, then logistic regression algorithm may be a better choice.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用线性回归模型，比如使用最小二乘回归方法，输出必须转换为类别，比如0和1。许多线性回归算法输出类别和置信度作为概率。一般来说，如果我们看到线性回归的概率大多超出了0.2到0.8的范围，那么逻辑回归算法可能是一个更好的选择。
- en: Algorithm input and output
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法输入和输出
- en: Similar to Linear Regression, all features must be numeric. Categorical features
    have to be transformed to numeric. Like in Naïve Bayes, this algorithm outputs
    class and probability for each class and can be used as a scoring model.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归类似，所有特征都必须是数值的。分类特征必须转换为数值。像在朴素贝叶斯中一样，此算法为每个类别输出类别和概率，可以用作评分模型。
- en: How does it work?
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: Logistic regression models the posterior probabilities of classes using linear
    functions in the input features.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归使用输入特征的线性函数来模拟类别的后验概率。
- en: 'The logistic regression model for a binary classification is given as:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类的逻辑回归模型如下所示：
- en: '![How does it work?](img/B05137_02_104.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_104.jpg)'
- en: The model is a log-odds or logit transformation of linear models (*References*
    [6]). The weight vector is generally computed using various optimization methods
    such as **iterative reweighted least squares** (**IRLS**) or the **Broyden–Fletcher–Goldfarb–Shanno**
    (**BFGS**) method, or variants of these methods.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是线性模型的对数几率或logit转换（*参考文献* [6]）。权重向量通常使用各种优化方法计算，如**迭代加权最小二乘法**（**IRLS**）或**Broyden-Fletcher-Goldfarb-Shanno**（**BFGS**）方法，或这些方法的变体。
- en: Advantages and limitations
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Overcomes the issue of heteroskedasticity and some non-linearity between inputs
    and outputs.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克服了输入和输出之间异方差性和某些非线性问题。
- en: No need of normal distribution assumptions in the error estimates.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在误差估计中不需要正态分布的假设。
- en: It is interpretable, but less so than Linear Regression models as some understanding
    of statistics is required. It gives information such as odds ratio, *p* values,
    and so on, which are useful in understanding the effects of features on the classes
    as well as doing implicit feature relevance based on significance of *p* values.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是可解释的，但不如线性回归模型那么可解释，因为需要一些统计学知识。它提供诸如优势比、*p* 值等信息，这些信息有助于理解特征对类别的影响，以及根据 *p*
    值的重要性进行隐式特征相关性。
- en: L1 or L2 regularization has to be employed in practice to overcome overfitting
    in the logistic regression models.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实践中，必须使用 L1 或 L2 正则化来克服逻辑回归模型中的过拟合问题。
- en: Many optimization algorithms are available for improving speed of training and
    robustness.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多优化算法可用于提高训练速度和鲁棒性。
- en: Non-linear models
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非线性模型
- en: Next, we will discuss some of the well-known, practical, and most commonly used
    non-linear models.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论一些知名、实用且最常用的非线性模型。
- en: Decision Trees
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision Trees are also known as **Classification and Regression Trees** (**CART**)
    (*References* [5]). Their representation is a binary tree constructed by evaluating
    an inequality in terms of a single attribute at each internal node, with each
    leaf-node corresponding to the output value or class resulting from the decisions
    in the path leading to it. When a new input is provided, the output is predicted
    by traversing the tree starting at the root.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树也被称为**分类和回归树**（**CART**）（*参考文献* [5]）。它们的表示是通过在每个内部节点评估一个关于单个属性的不等式来构建的二叉树，每个叶子节点对应于从其路径到达的决策产生的输出值或类别。当提供新的输入时，通过从根节点开始遍历树来预测输出。
- en: Algorithm inputs and outputs
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法输入和输出
- en: Features can be both categorical and numeric. It generates class as an output
    and most implementations give a score or probability using frequency-based estimation.
    Decision Trees probabilities are not smooth functions like Naïve Bayes and Logistic
    Regression, though there are extensions that are.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 特征可以是分类的也可以是数值的。它生成类别作为输出，大多数实现使用基于频率的估计给出分数或概率。尽管有一些扩展，但决策树的概率并不是像朴素贝叶斯和逻辑回归那样的平滑函数。
- en: How does it work?
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: Generally, a single tree is created, starting with single features at the root
    with decisions split into branches based on the values of the features while at
    the leaf there is either a class or more features. There are many choices to be
    made, such as how many trees, how to choose features at the root level or at subsequent
    leaf level, and how to split the feature values when not categorical. This has
    resulted in many different algorithms or modifications to the basic Decision Tree.
    Many techniques to split the feature values are similar to what was discussed
    in the section on discretization. Generally, some form of pruning is applied to
    reduce the size of the tree, which helps in addressing overfitting.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，从根节点开始创建单个树，根节点使用单个特征进行决策，根据特征的值将决策分支到各个分支，而在叶子节点处则是一个类别或更多特征。有许多选择要做，例如树的数量、如何选择根级别或后续叶子级别的特征，以及如何在不分类的情况下分割特征值。这导致了许多不同的算法或对基本决策树的修改。许多分割特征值的技术与在离散化部分讨论的内容相似。通常，会应用某种形式的剪枝来减少树的大小，这有助于解决过拟合问题。
- en: Gini index is another popular technique used to split the features. Gini index
    of data in set *S* of all the data points is ![How does it work?](img/B05137_02_106.jpg)
    where *p*[1], *p*[2] … *p*[k] are probability distribution for each class.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Gini指数是另一种用于分割特征的流行技术。所有数据点的集合 *S* 中的数据点的 Gini 指数如下 ![如何工作？](img/B05137_02_106.jpg)，其中
    *p*[1]，*p*[2] … *p*[k] 是每个类别的概率分布。
- en: 'If *p* is the fraction or probability of data in set *S* of all the data points
    belonging to say class positive, then 1 – *p* is the fraction for the other class
    or the error rate in binary classification. If the dataset *S* is split in *r*
    ways *S*[1]*, S*[2]*, …S*[r] then the error rate of each set can be quantified
    as |*S*[i]|. Gini index for an *r* way split is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *p* 是所有数据点集合 *S* 中属于正类别的数据点的分数或概率，那么 1 – *p* 是其他类别的分数或二分类中的错误率。如果数据集 *S*
    以 *r* 种方式分割为 *S*[1]*， S*[2]*，…S*[r]，那么每个集合的错误率可以量化为 |*S*[i]|。*r* 种分割的 Gini 指数如下：
- en: '![How does it work?](img/B05137_02_113.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_113.jpg)'
- en: The split with the lowest Gini index is used for selection. The CART algorithm,
    a popular Decision Tree algorithm, uses Gini index for split criteria.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最低 Gini 指数的分割用于选择。流行的决策树算法 CART 使用 Gini 指数作为分割标准。
- en: 'The entropy of the set of data points *S* can similarly be computed as:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点集S的熵可以类似地计算如下：
- en: '![How does it work?](img/B05137_02_114.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_114.jpg)'
- en: 'Similarly, entropy-based split is computed as:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，基于熵的分割计算如下：
- en: '![How does it work?](img/B05137_02_115.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_115.jpg)'
- en: The lower the value of the entropy split, the better the feature, and this is
    used in ID3 and C4.5 Decision Tree algorithms (*References* [12]).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 熵分割的值越低，特征越好，这一点在ID3和C4.5决策树算法中得到了应用（*参考文献* [12]）。
- en: The stopping criteria and pruning criteria are related. The idea behind stopping
    the growth of the tree early or pruning is to reduce the "overfitting" and it
    works similar to regularization in linear and logistic models. Normally, the training
    set is divided into tree growing sets and pruning sets so that pruning uses different
    data to overcome any biases from the growing set. **Minimum Description Length**
    (**MDL**), which penalizes the complexity of the tree based on number of nodes
    is a popular methodology used in many Decision Tree algorithms.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 停止标准与剪枝标准相关。提前停止树的生长或剪枝的目的是减少“过拟合”，这与线性模型和逻辑模型中的正则化类似。通常，训练集被分为树生长集和剪枝集，这样剪枝就可以使用不同的数据来克服生长集的任何偏差。**最小描述长度**（**MDL**），根据节点数量惩罚树的复杂性，是许多决策树算法中常用的方法。
- en: '![How does it work?](img/B05137_02_116.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_116.jpg)'
- en: 'Figure 5: Shows a two-dimensional binary classification problem and a Decision
    Tree induced using splits at thresholds *X*[1t] and *X*[1t], respectively'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：显示了一个二维二分类问题和一个通过在阈值*X*[1t]和*X*[1t]处进行分割诱导的决策树
- en: Advantages and limitations
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: The main advantages of Decision Trees are they are quite easily interpretable.
    They can be understood in layman's terms and are especially suited for business
    domain experts to easily understand the exact model.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的主要优点是它们很容易解释。它们可以用通俗易懂的语言来理解，并且特别适合商业领域的专家轻松理解精确模型。
- en: If there are a large number of features, then building Decision Tree can take
    lots of training time as the complexity of the algorithm increases.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特征数量很多，那么构建决策树可能需要大量的训练时间，因为算法的复杂性会增加。
- en: Decision Trees have an inherent problem with overfitting. Many tree algorithms
    have pruning options to reduce the effect. Using pruning and validation techniques
    can alleviate the problem to a large extent.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树固有的问题是过拟合。许多树算法都有剪枝选项来减少这种影响。使用剪枝和验证技术可以在很大程度上减轻这个问题。
- en: Decision Trees work well when there is correlation between the features.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当特征之间存在相关性时，决策树工作得很好。
- en: Decision Trees build axis-parallel boundaries across classes, the bias of which
    can introduce errors, especially in a complex, smooth, non-linear boundary.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树在类别之间构建轴平行的边界，这种偏差可能会引入错误，尤其是在复杂、平滑、非线性边界的情况下。
- en: K-Nearest Neighbors (KNN)
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-近邻（KNN）
- en: K-Nearest Neighbors falls under the branch of non-parametric and lazy algorithms.
    K-Nearest neighbors doesn't make any assumptions on the underlying data and doesn't
    build and generalize models from training data (*References* [10]).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: K-近邻属于非参数和懒惰算法的分支。K-近邻不对底层数据做出任何假设，也不从训练数据中构建和泛化模型（*参考文献* [10]）。
- en: Algorithm inputs and outputs
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法输入和输出
- en: Though KNN's can work with categorical and numeric features, the distance computation,
    which is the core of finding the neighbors, works better with numeric features.
    Normalization of numeric features to be in the same ranges is one of the mandatory
    steps required. KNN's outputs are generally the classes based on the neighbors'
    distance calculation.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然KNN可以处理分类和数值特征，但寻找邻居的核心——距离计算，与数值特征配合得更好。将数值特征归一化到相同的范围是必须的步骤之一。KNN的输出通常是基于邻居距离计算的类别。
- en: How does it work?
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'KNN uses the entire training data to make predictions on unseen test data.
    When unseen test data is presented KNN finds the K "nearest neighbors" using some
    distance computation and based on the neighbors and the metric of deciding the
    category it classifies the new point. If we consider two vectors represented by
    **x**[1] and **x**[2] corresponding to two data points the distance is calculated
    as:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: KNN使用整个训练数据对未见过的测试数据进行预测。当出现未见过的测试数据时，KNN通过某种距离计算找到K个“最近的邻居”，并根据邻居和决定类别的度量标准对新的点进行分类。如果我们考虑由**x**[1]和**x**[2]表示的两个数据点，对应的向量，距离计算如下：
- en: Euclidean Distance:![How does it work?](img/B05137_02_121.jpg)
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧几里得距离：![如何工作？](img/B05137_02_121.jpg)
- en: Cosine Distance or similarity:![How does it work?](img/B05137_02_122.jpg)
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦距离或相似度：![如何工作？](img/B05137_02_122.jpg)
- en: The metric used to classify an unseen may simply be the majority class among
    the *K* neighbors.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 用于对未见数据分类的度量可能仅仅是*K*个邻居中的多数类。
- en: The training time is small as all it has to do is build data structures to hold
    the data in such a way that the computation of the nearest neighbor is minimized
    when unseen data is presented. The algorithm relies on choices of how the data
    is stored from training data points for efficiency of searching the neighbors,
    which distance computation is used to find the nearest neighbor, and which metrics
    are used to categorize based on classes of all neighbors. Choosing the value of
    "*K*" in KNN by using validation techniques is critical.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间很小，因为它只需构建数据结构以以最小化未见面数据时最近邻的计算方式来存储数据。该算法依赖于从训练数据点存储数据的选择，以实现搜索邻居的高效性，使用哪种距离计算来找到最近邻，以及使用哪种度量来根据所有邻居的类别进行分类。通过使用验证技术来选择KNN中的"*K*"值是至关重要的。
- en: '![How does it work?](img/B05137_02_123.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_123.jpg)'
- en: 'Figure 6: K-Nearest Neighbor illustrated using two-dimensional data with different
    choices of k.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：使用二维数据以及不同的k值选择来展示K-最近邻。
- en: Advantages and limitations
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: No assumption on underlying data distribution and minimal training time makes
    it a very attractive method for learning.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对底层数据分布没有假设，并且训练时间最小，这使得它成为学习的一个非常有吸引力的方法。
- en: KNN uses local information for computing the distances and in certain domains
    can yield highly adaptive behaviors.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNN使用局部信息来计算距离，在特定领域可以产生高度自适应的行为。
- en: It is robust to noisy training data when *K* is effectively chosen.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当*K*值选择得当，对噪声训练数据具有鲁棒性。
- en: Holding the entire training data for classification can be problematic depending
    on the number of data points and hardware constraints
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据数据点的数量和硬件限制，保留整个训练数据用于分类可能会出现问题。
- en: Number of features and the curse of dimensionality affects this algorithm more
    hence some form of dimensionality reduction or feature selection has to be done
    prior to modeling in KNN.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征数量和维度的诅咒会影响此算法，因此在KNN建模之前必须进行某种形式的维度缩减或特征选择。
- en: Support vector machines (SVM)
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）
- en: SVMs, in simple terms, can be viewed as linear classifiers that maximize the
    margin between the separating hyperplane and the data by solving a constrained
    optimization problem. SVMs can even deal with data that is not linearly separable
    by invoking transformation to a higher dimensional space using kernels described
    later.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，SVM可以被视为通过解决约束优化问题来最大化分离超平面和数据之间边界的线性分类器。SVM甚至可以通过使用后面描述的核将数据转换到更高维空间来处理非线性可分的数据。
- en: Algorithm inputs and outputs
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法输入和输出
- en: SVM is effective with numeric features only, though most implementations can
    handle categorical features with transformation to numeric or binary. Normalization
    is often a choice as it helps the optimization part of the training. Outputs of
    SVM are class predictions. There are implementations that give probability estimates
    as confidences, but this requires considerable training time as they use k-fold
    cross-validation to build the estimates.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: SVM仅对数值特征有效，尽管大多数实现可以通过将特征转换为数值或二进制来处理分类特征。归一化通常是选择之一，因为它有助于训练中的优化部分。SVM的输出是类别预测。有一些实现提供概率估计作为置信度，但这需要相当多的训练时间，因为它们使用k折交叉验证来构建估计。
- en: How does it work?
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: In its linear form, SVM works similar to Linear Regression classifier, where
    a linear decision boundary is drawn between the two classes. The difference between
    the two is that with SVM, the boundary is drawn in such a way that the "margin"
    or the distance between the points near the boundary is maximized. The points
    on the boundaries are known as "support vectors" (*References* [13 and 8]).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在其线性形式中，SVM的工作方式类似于线性回归分类器，在两个类别之间绘制线性决策边界。两者的区别在于，在SVM中，边界是以最大化边界附近点之间的“间隔”或距离的方式绘制的。边界上的点被称为“支持向量”（*参考文献*
    [13和8]）。
- en: 'Thus, SVM tries to find the weight vector in linear models similar to Linear
    Regression model as given by the following:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，支持向量机（SVM）试图在类似于线性回归模型的线性模型中找到权重向量，如下所示：
- en: '![How does it work?](img/B05137_02_124.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_124.jpg)'
- en: 'The weight *w*[0] is represented by *b* here. SVM for a binary class y ∈{1,-1}
    tries to find a hyperplane:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 权重 *w*[0] 在这里表示为 *b*。对于二类分类问题，SVM试图找到一个超平面：
- en: '![How does it work?](img/B05137_02_127.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_127.jpg)'
- en: 'The hyperplane tries to separate the data points such that all points with
    the class lie on the side of the hyperplane as:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 超平面试图将数据点分离，使得属于同一类别的所有点都位于超平面的同一侧，如下所示：
- en: '![How does it work?](img/B05137_02_128.jpg)![How does it work?](img/B05137_02_129.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_128.jpg)![如何工作？](img/B05137_02_129.jpg)'
- en: 'The models are subjected to maximize the margin using constraint-based optimization
    with a penalty function denoted by *C* for overcoming the errors denoted by ![How
    does it work?](img/B05137_02_131.jpg):'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通过约束优化和惩罚函数 *C*（用于克服误差![如何工作？](img/B05137_02_131.jpg)）来最大化间隔：
- en: '![How does it work?](img/B05137_02_132.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_132.jpg)'
- en: Such that ![How does it work?](img/B05137_02_133.jpg) and ![How does it work?](img/B05137_02_134.jpg).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 使得 ![如何工作？](img/B05137_02_133.jpg) 和 ![如何工作？](img/B05137_02_134.jpg)。
- en: They are also known as large margin classifiers for the preceding reason. The
    kernel-based SVM transforms the input data into a hypothetical feature space where
    SV machinery works in a linear way and the boundaries are drawn in the feature
    spaces.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个原因，它们也被称为大间隔分类器。基于核的SVM将输入数据转换到一个假设的特征空间，在这个空间中，SV机制以线性方式工作，边界在特征空间中绘制。
- en: 'A kernel function on the transformed representation is given by:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换后的表示上，核函数如下给出：
- en: '![How does it work?](img/B05137_02_135.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_135.jpg)'
- en: Here Φ is a transformation on the input space. It can be seen that the entire
    optimization and solution of SVM remains the same with the only exception that
    the dot-product **x**[i] · **x**[j] is replaced by the kernel function *k*(**x**[i],
    **x**[j]), which is a function involving the two vectors in a different space
    without actually transforming to that space. This is known as the **kernel trick**.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 Φ 是输入空间上的一个变换。可以看出，SVM的整个优化和求解过程保持不变，唯一的例外是点积 **x**[i] · **x**[j] 被核函数 *k*(**x**[i],
    **x**[j]) 所替代，这是一个涉及不同空间中两个向量的函数，实际上并没有转换到那个空间。这被称为**核技巧**。
- en: 'The most well-known kernels that are normally used are:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用的最著名的核函数包括：
- en: '**Gaussian Radial Basis Kernel**:![How does it work?](img/B05137_02_139.jpg)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯径向基核**：![如何工作？](img/B05137_02_139.jpg)'
- en: '**Polynomial Kernel**:![How does it work?](img/B05137_02_140.jpg)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多项式核**：![如何工作？](img/B05137_02_140.jpg)'
- en: '**Sigmoid Kernel**:![How does it work?](img/B05137_02_141.jpg)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid核**：![如何工作？](img/B05137_02_141.jpg)'
- en: SVM's performance is very sensitive to some of the parameters of optimization
    and the kernel parameters and the core SV parameter such as the cost function
    *C*. Search techniques such as grid search or evolutionary search combined with
    validation techniques such as cross-validation are generally used to find the
    best parameter values.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的性能对一些优化参数、核参数以及核心SV参数（如代价函数 *C*）非常敏感。通常使用网格搜索或进化搜索等技术，结合交叉验证等验证技术来寻找最佳参数值。
- en: '![How does it work?](img/B05137_02_142.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_142.jpg)'
- en: 'Figure 7: SVM Linear Hyperplane learned from training data that creates a maximum
    margin separation between two classes.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：从训练数据中学习得到的SVM线性超平面，它能在两个类别之间创建最大间隔分离。
- en: '![How does it work?](img/B05137_02_144.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_144.jpg)'
- en: 'Figure 8: Kernel transformation illustrating how two-dimensional input space
    can be transformed using a polynomial transformation into a three-dimensional
    feature space where data is linearly separable.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：核变换说明如何使用多项式变换将二维输入空间转换为三维特征空间，其中数据是线性可分的。
- en: Advantages and limitations
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: SVMs are among the best in generalization, low overfitting, and have a good
    theoretical foundation for complex non-linear data if the parameters are chosen
    judiciously.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM在泛化、低过拟合方面表现优异，如果参数选择得当，对于复杂非线性数据有良好的理论基础。
- en: SVMs work well even with a large number of features and less training data.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM在具有大量特征和较少训练数据的情况下也能很好地工作。
- en: SVMs are less sensitive to noisy training data.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM对噪声训练数据不太敏感。
- en: The biggest disadvantage of SVMs is that they are not interpretable.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM的最大缺点是它们不可解释。
- en: Another big issue with SVM is its training time and memory requirements. They
    are *O(n*²*)* and *O(n*³*)* and can result in major scalability issues when the
    data is large or there are hardware constraints. There are some modifications
    that help in reducing both.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM（支持向量机）的另一个大问题是其训练时间和内存需求。它们是 *O(n²)* 和 *O(n³)*，当数据量大或存在硬件限制时，可能会导致严重的可扩展性问题。有一些修改可以帮助减少这两者。
- en: SVM generally works well for binary classification problems, but for multiclass
    classification problems, though there are techniques such as one versus many and
    one versus all, it is not as robust as some other classifiers such as Decision
    Trees.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM通常在二分类问题上表现良好，但对于多分类问题，尽管有诸如一对多和一对一等技术，但它的鲁棒性不如决策树等一些其他分类器。
- en: Ensemble learning and meta learners
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成学习和元学习器
- en: 'Combining multiple algorithms or models to classify instead of relying on just
    one is known as ensemble learning. It helps to combine various models as each
    model can be considered—at a high level—as an expert in detecting specific patterns
    in the whole dataset. Each base learner can be made to learn on slightly different
    datasets too. Finally, the results from all models are combined to perform prediction.
    Based on how similar the algorithms used in combination are, how the training
    dataset is presented to each algorithm, and how the algorithms combine the results
    to finally classify the unseen dataset, there are many branches of ensemble learning:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 将多个算法或模型组合起来进行分类，而不是仅仅依赖一个，这被称为集成学习。它有助于结合各种模型，因为每个模型都可以被视为——在高级别上——在整个数据集中检测特定模式的专家。每个基学习器也可以在略微不同的数据集上学习。最后，将所有模型的结果结合起来进行预测。根据组合中使用的算法的相似程度、如何向每个算法展示训练数据集以及算法如何组合结果以最终对未见数据集进行分类，集成学习有许多分支：
- en: '![Ensemble learning and meta learners](img/B05137_02_145.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![集成学习和元学习器](img/B05137_02_145.jpg)'
- en: 'Figure 9: Illustration of ensemble learning strategies'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：集成学习策略的说明
- en: 'Some common types of ensemble learning are:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的集成学习方法包括：
- en: Different learning algorithms
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的学习算法
- en: Same learning algorithms, but with different parameter choices
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同的学习算法，但具有不同的参数选择
- en: Different learning algorithms on different feature sets
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同特征集上的不同学习算法
- en: Different learning algorithms with different training data
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同训练数据的不同学习算法
- en: Bootstrap aggregating or bagging
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自助聚合或 bagging
- en: It is one of the most commonly used ensemble methods for dividing the data in
    different samples and building classifiers on each sample.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 它是用于在不同样本中划分数据并在每个样本上构建分类器的最常用的集成方法之一。
- en: Algorithm inputs and outputs
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法输入和输出
- en: The input is constrained by the choice of the base learner used—if using Decision
    Trees there are basically no restrictions. The method outputs class membership
    along with the probability distribution for classes.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 输入受所使用的基学习器的选择限制——如果使用决策树，基本上没有限制。该方法输出类成员资格以及类的概率分布。
- en: How does it work?
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: The core idea of bagging is to apply the bootstrapping estimation to different
    learners that have high variance, such as Decision Trees. Bootstrapping is any
    statistical measure that depends on random sampling with replacement. The entire
    data is split into different samples using bootstrapping and for each sample,
    a model is built using the base learner. Finally, while predicting, the average
    prediction is arrived at using a majority vote—this is one technique to combine
    over all the learners.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: bagging 的核心思想是对具有高方差的不同学习器应用自助估计，例如决策树。自助估计是任何依赖于有放回随机抽样的统计量。整个数据被使用自助估计分成不同的样本，并对每个样本使用基学习器构建模型。最后，在预测时，通过多数投票得到平均预测——这是结合所有学习器的一种技术。
- en: Random Forest
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 随机森林
- en: Random Forest is an improvement over basic bagged Decision Trees. Even with
    bagging, the basic Decision Tree has a choice of all the features at every split
    point in creating a tree. Because of this, even with different samples, many trees
    can form highly correlated submodels, which causes the performance of bagging
    to deteriorate. By giving random features to different models in addition to a
    random dataset, the correlation between the submodels reduces and Random Forest
    shows much better performance compared to basic bagged trees. Each tree in Random
    Forest grows its structure on the random features, thereby minimizing the bias;
    combining many such trees on decision reduces the variance (*References* [15]).
    Random Forest is also used to measure feature relevance by averaging the impurity
    decrease in the trees and ranking them across all the features to give the relative
    importance of each.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是基本Bagged决策树的改进。即使在Bagging的情况下，基本的决策树在创建树时在每个分割点都有所有特征的选择。正因为如此，即使有不同的样本，许多树也可以形成高度相关的子模型，这导致Bagging的性能下降。通过给不同的模型提供随机特征以及随机数据集，子模型之间的相关性降低，与基本Bagged树相比，随机森林表现出更好的性能。随机森林中的每棵树都在随机特征上生长其结构，从而最小化偏差；在决策上结合许多这样的树可以减少方差（*参考文献*
    [15]）。随机森林还用于通过平均树中的不纯度减少来衡量特征相关性，并对所有特征进行排名，以给出每个特征的相对重要性。
- en: Advantages and limitations
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Better generalization than the single base learner. Overcomes the issue of overfitting
    of base learners.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比单个基学习器有更好的泛化能力。克服了基学习器过拟合的问题。
- en: Interpretability of bagging is very low as it works as meta learner combining
    even the interpretable learners.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于Bagging作为元学习器结合了甚至可解释的学习器，其可解释性非常低。
- en: Like most other ensemble learners, Bagging is resilient to noise and outliers.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与大多数其他集成学习器一样，Bagging对噪声和异常值具有弹性。
- en: Random Forest generally does not tend to overfit given the training data is
    iid.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林通常不会对训练数据是独立同分布(iid)的情况下产生过拟合。
- en: Boosting
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Boosting
- en: Boosting is another popular form of ensemble learning, which is based on using
    a weak learner and iteratively learning the points that are "misclassified" or
    difficult to learn. Thus, the idea is to "boost" the difficult to learn instances
    and making the base learners learn the decision boundaries more effectively. There
    are various flavors of boosting such as AdaBoost, LogitBoost, ConfidenceBoost,
    Gradient Boosting, and so on. We present a very basic form of AdaBoost here (*References*
    [14]).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting是另一种流行的集成学习方法，它基于使用弱学习器并迭代地学习“错误分类”或难以学习的点。因此，想法是“提升”难以学习的实例，并使基学习器更有效地学习决策边界。Boosting有各种变体，如AdaBoost、LogitBoost、ConfidenceBoost、Gradient
    Boosting等。我们在这里介绍一个非常基本的AdaBoost形式（*参考文献* [14]）。
- en: Algorithm inputs and outputs
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法输入和输出
- en: The input is constrained by the choice of the base learner used—if using Decision
    Trees there are basically no restrictions. Outputs class membership along with
    probability distribution for classes.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 输入受所使用的基学习器的选择约束——如果使用决策树，基本上没有限制。输出类成员资格以及类别的概率分布。
- en: How does it work?
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: The basic idea behind boosting is iterative reweighting of input samples to
    create new distribution of the data for learning a model from a simple base learner
    in every iteration.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting背后的基本思想是迭代地重新加权输入样本，以创建新的数据分布，以便在每次迭代中从简单的基学习器中学习模型。
- en: Initially, all the instances are uniformly weighted with weights ![How does
    it work?](img/B05137_02_146.jpg) and at every iteration *t*, the population is
    resampled or reweighted as ![How does it work?](img/B05137_02_148.jpg) where ![How
    does it work?](img/B05137_02_149.jpg) and *Z*t is the normalization constant.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，所有实例都使用![如何工作？](img/B05137_02_146.jpg)的权重均匀加权，并且在每次迭代*t*时，种群被重新抽样或重新加权，如![如何工作？](img/B05137_02_148.jpg)所示，其中![如何工作？](img/B05137_02_149.jpg)和*Z*t是归一化常数。
- en: 'The final model works as a linear combination of all the models learned in
    the iteration:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 最终模型作为迭代中学习到的所有模型的线性组合工作：
- en: '![How does it work?](img/B05137_02_151.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_02_151.jpg)'
- en: The reweighting or resampling of the data in each iteration is based on "errors";
    the data points that result in errors are sampled more or have larger weights.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代的重新加权或重新抽样数据基于“错误”；导致错误的点被抽样更多或具有更大的权重。
- en: Advantages and limitations
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Better generalization than the base learner and overcomes the issue of overfitting
    very effectively.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比基学习器有更好的泛化能力，并且非常有效地克服了过拟合的问题。
- en: Some boosting algorithms such as AdaBoost can be susceptible to uniform noise.
    There are variants of boosting such as "GentleBoost" and "BrownBoost" that decrease
    the effect of outliers.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些提升算法，如AdaBoost，可能对均匀噪声敏感。存在一些提升算法的变体，如“GentleBoost”和“BrownBoost”，它们可以减少异常值的影响。
- en: Boosting has a theoretical bounds and guarantee on the error estimation making
    it a statistically robust algorithm.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升算法在误差估计上有理论上的界限和保证，使其成为一个统计上稳健的算法。
- en: Model assessment, evaluation, and comparisons
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估、评估和比较
- en: 'The key ideas discussed here are:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论的关键思想包括：
- en: How to assess or estimate the performance of the classifier on unseen datasets
    that it will be predicting on future unseen datasets.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何评估或估计分类器在未见数据集上的性能，这些数据集将是它未来预测未见数据集的。
- en: What are the metrics that we should use to assess the performance of the model?
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该使用哪些指标来评估模型的性能？
- en: How do we compare algorithms if we have to choose between them?
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们必须在它们之间进行选择，我们如何比较算法？
- en: Model assessment
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估
- en: In order to train the model(s), tune the model parameters, select the models,
    and finally estimate the predictive behavior of models on unseen data, we need
    many datasets. We cannot train the model on one set of data and estimate its behavior
    on the same set of data, as it will have a clear optimistic bias and estimations
    will be unlikely to match the behavior in the unseen data. So at a minimum, there
    is a need to partition data available into training sets and testing sets. Also,
    we need to tune the parameters of the model and test the effect of the tuning
    on a separate dataset before we perform testing on the test set. The same argument
    of optimistic bias and wrong estimation applies if we use the same dataset for
    training, parameter tuning, and testing. Thus there is a theoretical and practical
    need to have three datasets, that is, training, validation, and testing.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型（s），调整模型参数，选择模型，并最终估计模型在未见数据上的预测行为，我们需要许多数据集。我们不能在单一数据集上训练模型并估计其在该数据集上的行为，因为这会产生明显的乐观偏差，估计结果不太可能匹配未见数据的行为。因此，至少需要将可用的数据进行分区，形成训练集和测试集。此外，在我们对测试集进行测试之前，我们需要调整模型的参数，并在单独的数据集上测试调整的效果。如果我们使用同一数据集进行训练、参数调整和测试，乐观偏差和错误估计的问题同样适用。因此，从理论和实践上讲，需要有三个数据集，即训练集、验证集和测试集。
- en: The models are trained on the training set, the effect of different parameters
    on the training set are validated on the validation set, and the finalized model
    with the selected parameters is run on the test set to gauge the performance of
    the model on future unseen data. When the dataset is not large enough, or is large
    but the imbalance between classes is wide, that is, one class is present only
    in a small fraction of the total population, we cannot create too many samples.
    Recall that one of the steps described in our methodology is to create different
    data samples and datasets. If the total training data is large and has a good
    proportion of data and class ratios, then creating these three sets using random
    stratified partitioning is the most common option employed. In certain datasets
    that show seasonality and time-dependent behaviors, creating datasets based on
    time bounds is a common practice. In many cases, when the dataset is not large
    enough, only two physical partitions, that is, training and testing may be created.
    The training dataset ranges roughly from 66% to 80% while the rest is used for
    testing. The validation set is then created from the training dataset using the
    k-fold cross-validation technique. The training dataset is split *k* times, each
    time producing *k-1/k* random training *1/k* testing data samples, and the average
    metrics of performance needed is generated. This way the limited training data
    is partitioned *k* times and average performance across different split of training/testing
    is used for gauging the effect of the parameters. Using 10-fold cross-validation
    is the most common practice employed in cross-validation.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在训练集上训练，不同参数对训练集的影响在验证集上得到验证，最终选择参数的模型在测试集上运行以评估模型在未见数据上的性能。当数据集不够大，或者数据集很大但类别之间的不平衡性很大，即一个类别只占总人口的很小一部分时，我们无法创建太多的样本。记住，我们方法中描述的步骤之一是创建不同的数据样本和数据集。如果总训练数据量很大，并且数据量和类别比例良好，那么使用随机分层划分来创建这三个集合是最常见的选项。在某些显示季节性和时间相关行为的特定数据集中，基于时间界限创建数据集是一种常见的做法。在许多情况下，当数据集不够大时，可能只创建两个物理分区，即训练和测试。训练数据集大约占66%到80%，其余用于测试。然后使用k折交叉验证技术从训练数据集中创建验证集。训练数据集被分成*k*次，每次产生*k-1/k*随机训练*1/k*测试数据样本，并生成所需的性能平均指标。这样，有限的训练数据被分成*k*次，并使用不同训练/测试分割的平均性能来评估参数的影响。在交叉验证中，使用10折交叉验证是最常见的做法。
- en: Model evaluation metrics
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估指标
- en: The next important decision when tuning parameters or selecting models is to
    base your decision on certain performance metrics. In classification learning,
    there are different metrics available on which you can base your decision, depending
    on the business requirement. For example, in certain domains, not missing a single
    true positive is the most important concern, while in other domains where humans
    are involved in adjudicating results of models, having too many false positives
    is the greater concern. In certain cases, having overall good accuracy is considered
    more vital. In highly imbalanced datasets such as fraud or cyber attacks, there
    are just a handful of instances of one class and millions of the other classes.
    In such cases accuracy gives a wrong indication of model performance and some
    other metrics such as precision, true positive ratio, or area under the curve
    are used as metrics.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整参数或选择模型时，下一个重要的决定是基于某些性能指标做出决策。在分类学习中，有不同可用的指标，你可以根据业务需求做出决策。例如，在某个领域，不漏掉任何一个真正的阳性是最重要的关注点，而在其他领域，人类参与判断模型结果时，过多的假阳性是更大的关注点。在某些情况下，整体良好的准确率被认为更为重要。在高度不平衡的数据集，如欺诈或网络攻击中，一个类别的实例只有几个，而另一个类别的实例有数百万。在这种情况下，准确率给出了模型性能的错误指示，因此使用一些其他指标，如精确度、真正阳性比率或曲线下面积作为指标。
- en: We will now discuss the most commonly employed metrics in classification algorithms
    evaluation (*References* [16, 17, and 19]).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将讨论在分类算法评估中最常使用的指标（*参考文献* [16, 17和19]）。
- en: '![Model evaluation metrics](img/B05137_02_152.jpg)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![模型评估指标](img/B05137_02_152.jpg)'
- en: 'Figure 10: Model evaluation metrics for classification models'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：分类模型的模型评估指标
- en: Confusion matrix and related metrics
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混淆矩阵和相关指标
- en: '![Confusion matrix and related metrics](img/B05137_02_153.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![混淆矩阵和相关指标](img/B05137_02_153.jpg)'
- en: 'Figure 11: Confusion Matrix'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：混淆矩阵
- en: The confusion matrix is central to the definition of a number of model performance
    metrics. The proliferation of metrics and synonymous terms is a result of the
    utility of different quantities derived from the elements of the matrix in various
    disciplines, each emphasizing a different aspect of the model's behavior.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是许多模型性能指标定义的核心。指标和同义词的激增是矩阵元素在不同学科中衍生出的不同数量的效用结果，每个学科都强调模型行为的不同方面。
- en: The four elements of the matrix are raw counts of the number of False Positives,
    False Negatives, True Positives, and True Negatives. Often more interesting are
    the different ratios of these quantities, the True Positive Rate (or Sensitivity,
    or Recall), and the False Positive Rate (FPR, or 1—Specificity, or Fallout). Accuracy
    reflects the percentage of correct predictions, whether Class 1 or Class 0\. For
    skewed datasets, accuracy is not particularly useful, as even a constant prediction
    can appear to perform well.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的四个元素是假阳性、假阴性、真阳性和真阴性的数量原始计数。通常，这些数量的不同比率更有趣，例如真阳性率（或灵敏度、或召回率）和假阳性率（FPR，或1—特异性，或漏报率）。准确率反映了正确预测的百分比，无论是类别1还是类别0。对于倾斜的数据集，准确率并不特别有用，因为即使是恒定的预测也可能看起来表现良好。
- en: ROC and PRC curves
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ROC和PRC曲线
- en: The previously mentioned metrics such as accuracy, precision, recall, sensitivity,
    and specificity are aggregates, that is, they describe the behavior of the entire
    dataset. In many complex problems it is often valuable to see the trade-off between
    metrics such as TPs and say FPs.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的指标，如准确率、精确率、召回率、灵敏度和特异性，是汇总指标，即它们描述了整个数据集的行为。在许多复杂问题中，观察TP和FP等指标之间的权衡通常很有价值。
- en: Many classifiers, mostly probability-based classifiers, give confidence or probability
    of the prediction, in addition to giving classification. The process to obtain
    the ROC or PRC curves is to run the unseen validation or test set on the learned
    models, and then obtain the prediction and the probability of prediction. Sort
    the predictions based on the confidences in decreasing order. For every probability
    or confidence calculate two metrics, the fraction of FP (FP rate) and the fraction
    of TP (TP rate).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 许多分类器，主要是基于概率的分类器，除了给出分类外，还给出预测的置信度或概率。获得ROC或PRC曲线的过程是在学习模型上运行未见验证或测试集，然后获得预测和预测概率。根据置信度按降序对预测进行排序。对于每个概率或置信度，计算两个指标，即FP的分数（FP率）和TP的分数（TP率）。
- en: 'Plotting the TP rate on the *y* axis and FP rate on the *x* axis gives the
    ROC curves. ROC curves of random classifiers lie close to the diagonal while the
    ROC curves of good classifiers tend towards the upper left of the plot. The **area
    under the curve** (**AUC**) is the area measured under the ROC curve by using
    the trapezoidal area from 0 to 1 of ROC curves. While running cross-validation
    for instance there can be many ROC curves. There are two ways to get "average"
    ROC curves: first, using vertical averaging, that is, TPR average is plotted at
    different FP rate or second, using horizontal averaging, that is, FPR average
    is plotted at different TP rate. The classifiers that have area under curves greater
    than 0.8, as a rule-of-thumb are considered good for prediction for unseen data.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 将TP率绘制在y轴上，FP率绘制在x轴上，可以得到ROC曲线。随机分类器的ROC曲线靠近对角线，而优秀分类器的ROC曲线倾向于图表的左上角。**曲线下面积**（**AUC**）是通过使用ROC曲线从0到1的梯形面积来测量的面积。例如，在运行交叉验证时，可能会有许多ROC曲线。有两种方法可以得到“平均”ROC曲线：首先，使用垂直平均，即TPR平均在不同FP率下绘制；其次，使用水平平均，即在不同的TP率下绘制FPR平均。根据经验法则，曲线下面积大于0.8的分类器被认为对未见数据具有良好的预测能力。
- en: Precision Recall curves or PRC curves are similar to ROC curves, but instead
    of TPR versus FPR, metrics Precision and Recall are plotted on the *y* and *x*
    axis, respectively. When the data is highly imbalanced, that is, ROC curves don't
    really show the impact while PRC curves are more reliable in judging performance.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率-召回率曲线或PRC曲线与ROC曲线类似，但与TPR对FPR不同，分别将精确率和召回率绘制在y轴和x轴上。当数据高度不平衡时，即ROC曲线并不能真正显示影响，而PRC曲线在判断性能方面更为可靠。
- en: Gain charts and lift curves
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收益图和提升曲线
- en: Lift and Gain charts are more biased towards sensitivity or true positives.
    The whole purpose of these two charts is to show how instead of random selection,
    the models prediction and confidence can detect better quality or true positives
    in the sample of unseen data.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 提升和增益图更偏向于敏感性或真正阳性。这两个图表的全部目的在于展示，与随机选择相比，模型的预测和置信度可以检测到未见数据样本中更好的质量或真正阳性。
- en: This is usually very appealing for detection engines that are used in detecting
    fraud in financial crime or threats in cyber security. The gain charts and lift
    curves give exact estimates of real true positives that will be detected at different
    quartiles or intervals of total data. This will give insight to the business decision
    makers on how many investigators would be needed or how many hours would be spent
    towards detecting fraudulent actions or cyber attacks and thus can give real ROI
    of the models.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常对用于检测金融犯罪中的欺诈或网络安全中的威胁的检测引擎非常有吸引力。增益图和提升曲线给出了在不同四分位数或总数据区间的不同区间内将被检测到的真正阳性的精确估计。这将给业务决策者提供洞察，了解需要多少调查人员或需要花费多少小时来检测欺诈行为或网络攻击，从而可以给出模型的实际投资回报率。
- en: The process for generating gain charts or lift curves has a similar process
    of running unseen validation or test data through the models and getting the predictions
    along with the confidences or probabilities. It involves ranking the probabilities
    in decreasing order and keeping count of TPs per quartile of the dataset. Finally,
    the histogram of counts per quartile give the lift curve, while the cumulative
    count of TPs added over quartile gives the gains chart. In many tools such as
    RapidMiner, instead of coarse intervals such as quartiles, fixed larger intervals
    using binning is employed for obtaining the counts and cumulative counts.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 生成增益图或提升曲线的过程与通过模型运行未见过的验证或测试数据并获取预测以及置信度或概率的过程类似。这涉及到按降序排列概率，并计算数据集每个四分位数的TPs数量。最后，每个四分位数的计数直方图给出提升曲线，而每个四分位数的TPs累计计数给出增益图。在许多工具，如RapidMiner中，为了获得计数和累计计数，使用分箱技术而不是粗略的区间（如四分位数），而是使用固定的较大区间。
- en: Model comparisons
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型比较
- en: When it comes to choosing between algorithms, or the right parameters for a
    given algorithm, we make the comparison either on different datasets, or, as in
    the case of cross-validation, on different splits of the same dataset. Measures
    of statistical testing are employed in decisions involved in these comparisons.
    The basic idea of using hypothesis testing from classical statistics is to compare
    the two metrics from the algorithms. The null hypothesis is that there is no difference
    between the algorithms based on the measured metrics and so the test is done to
    validate or reject the null hypothesis based on the measured metrics (*References*
    [16]). The main question answered by statistical tests is- are the results or
    metrics obtained by the algorithm its real characteristics, or is it by chance?
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到选择算法，或者给定算法的正确参数时，我们会在不同的数据集上，或者在交叉验证的情况下，在同一数据集的不同分割上进行比较。在这些比较中，会使用统计测试的度量来做出决策。使用经典统计学中的假设检验的基本思想是比较算法中的两个度量。零假设是算法基于测量的度量之间没有差异，因此测试是为了根据测量的度量来验证或拒绝零假设（*参考文献*
    [16]）。统计测试回答的主要问题是——算法获得的结果或度量是其真实特征，还是偶然得到的？
- en: In this section, we will discuss the most common methods for comparing classification
    algorithms used in practical scenarios.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论在实际情况中使用的比较分类算法的最常见方法。
- en: Comparing two algorithms
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较两个算法
- en: The general process is to train the algorithms on the same training set and
    run the models on either multiple validation sets, different test sets, or cross-validation,
    gauge the metrics of interest discussed previously, such as error rate or area
    under curve, and then get the statistics of the metrics for each of the algorithms
    to decide which worked better. Each method has its advantages and disadvantages.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 一般过程是在相同的训练集上训练算法，然后在多个验证集、不同的测试集或交叉验证上运行模型，衡量之前讨论的兴趣度量，如错误率或曲线下面积，然后获取每个算法的度量统计，以决定哪个效果更好。每种方法都有其优点和缺点。
- en: McNemar's Test
  id: totrans-389
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: McNemar测试
- en: 'This is a non-parametric test and thus it makes no assumptions on data and
    distribution. McNemar''s test builds a contingency table of a performance metric
    such as "misclassification or errors" with:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非参数测试，因此它不对数据分布做出假设。McNemar 的测试构建了一个关于性能度量值（如“误分类或错误”）的列联表，例如：
- en: Count of misclassification by both algorithms (*c*[00])
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 两个算法误分类的数量(*c*[00])
- en: Count of misclassification by algorithm *G1*, but correctly classified by algorithm
    *G2*(*c*[01])
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法 *G1* 误分类但算法 *G2* 正确分类的数量(*c*[01])
- en: Count of misclassification by algorithm *G2*, but correctly classified by algorithm
    *G1* (*c*[10])
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法 *G2* 误分类但算法 *G1* 正确分类的数量(*c*[10])
- en: Count of correctly classified by both *G1* and *G2*(*c*[11])![McNemar's Test](img/B05137_02_162.jpg)
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时被 *G1* 和 *G2* 正确分类的数量(*c*[11])![McNemar's Test](img/B05137_02_162.jpg)
- en: If χ² exceeds ![McNemar's Test](img/B05137_02_164.jpg) statistic then we can
    reject the null hypothesis that the two performance metrics on algorithms *G1*
    and *G2* were equal under the confidence value of 1 – α.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 χ² 超过 ![McNemar's Test](img/B05137_02_164.jpg) 统计量，则可以在 1 – α 的置信水平下拒绝两个算法
    *G1* 和 *G2* 的性能度量值相等的零假设。
- en: Paired-t test
  id: totrans-396
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 配对 t 检验
- en: This is a parametric test and an assumption of normally distributed computed
    metrics becomes valid. Normally it is coupled with cross-validation processes
    and results of metrics such as area under curve or precision or error rate is
    computed for each and then the mean and standard deviations are measured. Apart
    from normal distribution assumption, the additional assumption that two metrics
    come from a population of equal variance can be a big disadvantage for this method.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个参数测试，并且假设计算出的度量值呈正态分布是有效的。通常，它与交叉验证过程结合使用，并计算曲线下面积、精确度或错误率等度量值的结果，然后测量均值和标准差。除了正态分布假设之外，两个度量值来自具有相等方差的总体这一额外假设可能对这种方法是一个很大的缺点。
- en: '![Paired-t test](img/B05137_02_166.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![Paired-t test](img/B05137_02_166.jpg)'
- en: '![Paired-t test](img/B05137_02_167.jpg) is difference of means in performance
    metrics of two algorithms *G1* and *G2*.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '![Paired-t test](img/B05137_02_167.jpg) 是两个算法 *G1* 和 *G2* 性能度量值的均值差异。'
- en: '![Paired-t test](img/B05137_02_168.jpg)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![Paired-t test](img/B05137_02_168.jpg)'
- en: Here, *d*i is the difference between the performance metrics of two algorithms
    *G1* and *G2* in the trial and there are *n* trials.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*d*i 是试验中两个算法 *G1* 和 *G2* 性能度量值的差异，并且有 *n* 次试验。
- en: 'The *t*-statistic is computed using the mean differences and the standard errors
    from the standard deviation as follows and is compared to the table for the right
    alpha value to check for significance:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '*t*-统计量使用均值差异和标准误差（从标准差计算得出）计算，并与表格中的正确 alpha 值进行比较，以检查显著性：'
- en: '![Paired-t test](img/B05137_02_170.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![Paired-t test](img/B05137_02_170.jpg)'
- en: Wilcoxon signed-rank test
  id: totrans-404
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Wilcoxon 符号秩检验
- en: 'The most popular non-parametric method of testing two metrics over datasets
    is to use the Wilcoxon signed-rank test. The algorithms are trained on the same
    training data and metrics such as error rate or area under accuracy are calculated
    over different validation or test sets. Let *d*[i] be the difference between the
    performance metrics of two classifiers in the *i*^(th) trial for *N* datasets.
    Differences are then ranked according to their absolute values, and mean ranks
    associated for ties. Let *R*^+ be the sum of ranks where the second algorithm
    outperformed the first and R^– be the sum of ranks where the first outperformed
    the second:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集上测试两个度量值最流行的非参数方法是使用 Wilcoxon 符号秩检验。算法在相同的训练数据和度量值（如错误率或准确率下的面积）上训练，并在不同的验证或测试集上计算。设
    *d*[i] 为 *N* 个数据集的第 *i* 次试验中两个分类器性能度量值的差异。差异根据其绝对值进行排名，并关联平均秩以处理平局。设 *R*^+ 为第二个算法优于第一个算法的秩之和，R^–
    为第一个算法优于第二个算法的秩之和：
- en: '![Wilcoxon signed-rank test](img/B05137_02_174.jpg)![Wilcoxon signed-rank test](img/B05137_02_175.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![Wilcoxon signed-rank test](img/B05137_02_174.jpg)![Wilcoxon signed-rank test](img/B05137_02_175.jpg)'
- en: The statistic ![Wilcoxon signed-rank test](img/B05137_02_176.jpg) is then compared
    to threshold value at an alpha, ![Wilcoxon signed-rank test](img/B05137_02_177.jpg)
    to reject the hypothesis.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 统计量 ![Wilcoxon signed-rank test](img/B05137_02_176.jpg) 然后与 alpha 值的阈值进行比较，![Wilcoxon
    signed-rank test](img/B05137_02_177.jpg) 以拒绝假设。
- en: Comparing multiple algorithms
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较多个算法
- en: We will now discuss the two most common techniques used when there are more
    than two algorithms involved and we need to perform comparison across many algorithms
    for evaluation metrics.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论在涉及两个以上算法且需要跨多个算法进行评估指标比较时使用的两种最常见技术。
- en: ANOVA test
  id: totrans-410
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ANOVA测试
- en: These are parametric tests that assume normal distribution of the samples, that
    is, metrics we are calculating for evaluations. ANOVA test follows the same process
    as others, that is, train the models/algorithms on similar training sets and run
    it on different validation or test sets. The main quantities computed in ANOVA
    are the metric means for each algorithm performance and then compute the overall
    metric means across all algorithms.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是参数测试，假设样本呈正态分布，即我们用于评估的指标。方差分析（ANOVA）测试遵循与其他测试相同的过程，即使用相似的训练集训练模型/算法，并在不同的验证或测试集上运行。ANOVA测试计算的主要量包括每个算法性能的指标均值，然后计算所有算法的整体指标均值。
- en: 'Let *p*[ij] be the performance metric for *i = 1,2… k* and *j = 1,2 …l* for
    *k* trials and *l* classifiers. The mean performance of classifier *j* on all
    trials and overall mean performance is:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 设*p*[ij]为*i = 1,2… k*和*j = 1,2 …l*的试验和分类器的性能指标。分类器*j*在所有试验中的平均性能和整体平均性能如下：
- en: '![ANOVA test](img/B05137_02_182.jpg)![ANOVA test](img/B05137_02_183.jpg)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![ANOVA测试](img/B05137_02_182.jpg)![ANOVA测试](img/B05137_02_183.jpg)'
- en: 'Two types of variation are evaluated. The first is within-group variation,
    that is, total deviation of each algorithm from the overall metric mean, and the
    second is between-group variation, that is, deviation of each algorithm metric
    mean. Within-group variation and between-group variation are used to compute the
    respective within- and between- sum of squares as:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 评估两种类型的变异。第一种是组内变异，即每个算法与整体指标均值的总偏差，第二种是组间变异，即每个算法指标均值的偏差。组内变异和组间变异用于计算相应的组内和组间平方和，如下：
- en: '![ANOVA test](img/B05137_02_184.jpg)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![ANOVA测试](img/B05137_02_184.jpg)'
- en: 'Using the two sum of squares and a computation such as F-statistic, which is
    the ratio of the two, the significance test can be done at alpha values to accept
    or reject the null hypothesis:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两个平方和以及如F统计量这样的计算，即两者的比率，可以在alpha值上进行显著性测试以接受或拒绝零假设：
- en: '![ANOVA test](img/B05137_02_185.jpg)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![ANOVA测试](img/B05137_02_185.jpg)'
- en: ANOVA tests have the same limitations as paired-t tests on the lines of assumptions
    of normal distribution of metrics and assuming the variances being equal.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: ANOVA测试在假设指标的正态分布和方差相等方面与配对t测试具有相同的局限性。
- en: Friedman's test
  id: totrans-419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 弗里德曼测试
- en: 'Friedman''s test is a non-parametric test for multiple algorithm comparisons
    and it has no assumption on the data distribution or variances of metrics that
    ANOVA does. It uses ranks instead of the performance metrics directly for its
    computation. On each dataset or trials, the algorithms are sorted and the best
    one is ranked 1 and so on for all classifiers. The average rank of an algorithm
    over *n* datasets is computed, say *R*[j]. The Friedman''s statistic over *l*
    classifiers is computed as follows and compared to alpha values to accept or reject
    the null hypothesis:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 弗里德曼测试是一种针对多个算法比较的非参数测试，它对数据分布或指标的方差没有假设，ANOVA测试有这些假设。它使用排名而不是直接使用性能指标进行计算。在每个数据集或试验中，算法被排序，最好的算法排名为1，依此类推，对所有分类器都如此。计算算法在*n*个数据集上的平均排名，记为*R*[j]。弗里德曼统计量在*l*个分类器上的计算如下，并与alpha值比较以接受或拒绝零假设：
- en: '![Friedman''s test](img/B05137_02_187.jpg)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![弗里德曼测试](img/B05137_02_187.jpg)'
- en: Case Study – Horse Colic Classification
  id: totrans-422
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 – 马肠阻塞分类
- en: To illustrate the different steps and methodologies described in [Chapter 1](ch01.html
    "Chapter 1. Machine Learning Review"), *Machine Learning Review*, from data analysis
    to model evaluation, a representative dataset that has real-world characteristics
    is essential.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明[第1章](ch01.html "第1章. 机器学习回顾")中描述的不同步骤和方法，从数据分析到模型评估，需要一个具有现实世界特征的代表性数据集。
- en: 'We have chosen "Horse Colic Dataset" from the UCI Repository available at the
    following link: [https://archive.ics.uci.edu/ml/datasets/Horse+Colic](https://archive.ics.uci.edu/ml/datasets/Horse+Colic)'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从以下链接可用的UCI存储库中选择“马肠阻塞数据集”:[https://archive.ics.uci.edu/ml/datasets/Horse+Colic](https://archive.ics.uci.edu/ml/datasets/Horse+Colic)
- en: The dataset has 23 features and has a good mix of categorical and continuous
    features. It has a large number of features and instances with missing values,
    hence understanding how to replace these missing values and using it in modeling
    is made more practical in this treatment. The large number of missing data (30%)
    is in fact a notable feature of this dataset. The data consists of attributes
    that are continuous, as well as nominal in type. Also, the presence of self-predictors
    makes working with this dataset instructive from a practical standpoint.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集有23个特征，具有类别和连续特征的混合。它包含大量具有缺失值的特征和实例，因此在本处理中，理解如何替换这些缺失值并在建模中使用它们变得更加实用。实际上，大量缺失数据（30%）是该数据集的一个显著特征。数据由连续属性以及类型为名义的属性组成。此外，存在自预测因子使得从实际角度处理此数据集具有指导意义。
- en: The goal of the exercise is to apply the techniques of supervised learning that
    we have assimilated so far. We will do this using a real dataset and by working
    with two open source toolkits—WEKA and RapidMiner. With the help of these tools,
    we will construct the pipeline that will allow us to start with the ingestion
    of the data file through data cleansing, the learning process, and model evaluation.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的目的是应用我们迄今为止已吸收的监督学习技术。我们将使用真实数据集并通过使用两个开源工具包——WEKA和RapidMiner来完成。借助这些工具，我们将构建一个管道，使我们能够从数据文件的摄取开始，通过数据清洗、学习过程和模型评估。
- en: Weka is a Java framework for machine learning—we will see how to use this framework
    to solve a classification problem from beginning to end in a few lines of code.
    In addition to a Java API, Weka also has a GUI.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: Weka是一个机器学习的Java框架——我们将看到如何使用这个框架通过几行代码从头到尾解决一个分类问题。除了Java API之外，Weka还有一个GUI。
- en: RapidMiner is a graphical environment with drag and drop capability and a large
    suite of algorithms and visualization tools that makes it extremely easy to quickly
    run experiments with data and different modeling techniques.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: RapidMiner是一个具有拖放功能和大量算法及可视化工具的图形化环境，这使得快速运行数据实验和不同建模技术变得极其简单。
- en: Business problem
  id: totrans-429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 业务问题
- en: The business problem is to determine given values for the well-known variables
    of the dataset—if the lesion of the horse was surgical. We will use the test set
    as the unseen data that must be classified.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 业务问题是确定数据集中已知变量的值——如果马的病变是手术性的。我们将使用测试集作为必须分类的未见数据。
- en: Machine learning mapping
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习映射
- en: Based on the data and labels, this is a binary classification problem. The data
    is already split into training and testing data. This makes the evaluation technique
    simpler as all methodologies from feature selection to models can be evaluated
    on the same test data.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 基于数据和标签，这是一个二元分类问题。数据已经分为训练数据和测试数据。这使得评估技术更简单，因为从特征选择到模型的所有方法都可以在相同的测试数据上评估。
- en: The dataset contains 300 training and 68 test examples. There are 28 attributes
    and the target corresponds to whether or not a lesion is surgical.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含300个训练样本和68个测试样本。有28个属性，目标对应于病变是否为手术性的。
- en: Data analysis
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分析
- en: After looking at the distribution of the label categories over the training
    and test samples, we combine the 300 training samples and the 68 test samples
    prior to feature analyzes.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看标签类别在训练和测试样本中的分布后，我们在特征分析之前将300个训练样本和68个测试样本合并。
- en: Label analysis
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标签分析
- en: 'The ratio of the No Class to Yes Class is 109/191 = 0.57 in the Training set
    and 0.66 in the Test set:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集中无类别与有类别的比例是109/191 = 0.57，测试集中为0.66：
- en: '| Training dataset |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据集 |'
- en: '| --- |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Surgical Lesion? | 1 (Yes) | 2 (No) |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 手术病变？ | 1 (是) | 2 (否) |'
- en: '| Number of examples | 191 | 109 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 示例数量 | 191 | 109 |'
- en: '| Testing dataset |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 测试数据集 |'
- en: '| Surgical Lesion? | 1 (Yes) | 2 (No) |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 手术病变？ | 1 (是) | 2 (否) |'
- en: '| Number of examples | 41 | 27 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 示例数量 | 41 | 27 |'
- en: '*Table 2: Label analysis*'
  id: totrans-445
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表2：标签分析*'
- en: Features analysis
  id: totrans-446
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征分析
- en: 'The following is a screenshot of top features with characteristics of types,
    missing values, basic statistics of minimum, maximum, modes, and standard deviations
    sorted by missing values. Observations are as follows:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个截图，展示了具有类型、缺失值、最小值、最大值、众数和标准差基本统计特征的顶级功能，并按缺失值排序。观察结果如下：
- en: There are no categorical or continuous features with non-missing values; the
    least is the feature "pulse" with 74 out of 368 missing, that is, 20% values missing,
    which is higher than general noise threshold!
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有非缺失值的分类或连续特征；最少的特征是“脉搏”，有74个缺失值，共368个，即20%的值缺失，这高于一般的噪声阈值！
- en: Most numeric features have missing values, for example, "nasogastric reflux
    PH" has 247 out of 368 values missing, that is, 67% values are missing!
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数数值特征也存在缺失值，例如，“鼻胃反流PH”中有247个缺失值，共368个，即67%的值缺失！
- en: Many categorical features have missing values, for example, "abidominocentesis
    appearance" have 165 out of 368 missing, that is, 45% values are missing!
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多分类特征存在缺失值，例如，“腹部穿刺外观”中有165个缺失值，共368个，即45%的值缺失！
- en: Missing values have to be handled in some way to overcome the noise created
    by such large numbers!![Features analysis](img/B05137_02_188.jpg)
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须以某种方式处理缺失值，以克服由如此大量缺失值产生的噪声！！[特征分析](img/B05137_02_188.jpg)
- en: 'Figure 12: Basic statistics of features from datasets.'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12：数据集特征的基本统计信息。
- en: Supervised learning experiments
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习实验
- en: In this section, we will cover supervised learning experiments using two different
    tools—highlighting coding and analysis in one tool and the GUI framework in the
    other. This gives the developers the opportunity to explore whichever route they
    are most comfortable with.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍使用两种不同工具进行的监督学习实验——突出一种工具的编码和分析，另一种工具的GUI框架。这为开发者提供了探索他们最舒适路径的机会。
- en: Weka experiments
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Weka实验
- en: In this section, we have given the entire code and will walk through the process
    from loading data, transforming the data, selecting features, building sample
    models, evaluating them on test data, and even comparing the algorithms for statistical
    significance.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们给出了整个代码，并将从加载数据、转换数据、选择特征、构建样本模型、在测试数据上评估它们，甚至比较算法的统计显著性等方面进行过程讲解。
- en: Sample end-to-end process in Java
  id: totrans-457
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Java中的端到端流程示例
- en: 'In each algorithm, the same training/testing data is used and evaluation is
    performed for all the metrics as follows. The training and testing file is loaded
    in memory as follows:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个算法中，使用相同的训练/测试数据，并对所有指标进行评估，如下所示。训练和测试文件按如下方式加载到内存中：
- en: '[PRE0]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The generic code, using WEKA, is shown here, where each classifier is wrapped
    by a filtered classifier for replacing missing values:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 使用WEKA的通用代码如下所示，其中每个分类器都被一个过滤分类器包裹，用于替换缺失值：
- en: '[PRE1]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When the classifier needs to perform Feature Selection, in Weka, `AttributeSelectedClassifier`
    further wraps the `FilteredClassifier` as shown in the following listing:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 当分类器需要执行特征选择时，在Weka中，`AttributeSelectedClassifier`进一步包裹了`FilteredClassifier`，如下所示：
- en: '[PRE2]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The sample output of evaluation is given here:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 评估的样本输出如下所示：
- en: '[PRE3]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Weka experimenter and model selection
  id: totrans-466
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Weka实验者和模型选择
- en: As explained in the *Model evaluation metrics* section, to select models, we
    need to validate which one will work well on unseen datasets. Cross-validation
    must be done on the training set and the performance metric of choice needs to
    be analyzed using standard statistical testing metrics. Here we show an example
    using the same training data, 10-fold cross validation, performing 30 experiments
    on two models, and comparison of results using paired-t tests.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*模型评估指标*部分所述，为了选择模型，我们需要验证哪个模型在未见过的数据集上表现良好。必须在训练集上进行交叉验证，并使用标准的统计测试指标来分析所选的性能指标。在此，我们展示了使用相同训练数据、10折交叉验证、对两个模型进行30次实验，并使用配对t检验比较结果的示例。
- en: One uses Naïve Bayes with preprocessing that includes replacing missing values
    and performing feature selection by removing any features with a score below 0.0.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是使用预处理过的朴素贝叶斯，包括替换缺失值并通过移除任何得分低于0.0的特征进行特征选择。
- en: Another uses the same preprocessing and AdaBoostM1 with Naïve Bayes.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用相同的预处理和AdaBoostM1与朴素贝叶斯。
- en: '![Weka experimenter and model selection](img/B05137_02_189.jpg)'
  id: totrans-470
  prefs: []
  type: TYPE_IMG
  zh: '![Weka实验者和模型选择](img/B05137_02_189.jpg)'
- en: 'Figure 13: WEKA experimenter showing the process of using cross-validation
    runs with 30 repetitions with two algorithms.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：WEKA实验者显示了使用30次重复的交叉验证运行两个算法的过程。
- en: '![Weka experimenter and model selection](img/B05137_02_190.jpg)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
  zh: '![Weka实验者和模型选择](img/B05137_02_190.jpg)'
- en: 'Figure 14: WEKA Experimenter results showing two algorithms compared on metric
    of percent correct or accuracy using paired-t test.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：WEKA实验者结果显示了使用配对t检验比较两个算法在百分比正确或准确度指标上的结果。
- en: RapidMiner experiments
  id: totrans-474
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RapidMiner实验
- en: Let's now run some experiments using the Horse-colic dataset in RapidMiner.
    We will again follow the methodology presented in the first part of the chapter.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用RapidMiner中的马绞痛数据集进行一些实验。我们将再次遵循章节第一部分介绍的方法。
- en: Note
  id: totrans-476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This section is not intended as a tutorial on the RapidMiner tool. The experimenter
    is expected to read the excellent documentation and user guide to familiarize
    themselves with the use of the tool. There is a tutorial dedicated to every operator
    in the software—we recommend you make use of these tutorials whenever you want
    to learn how a particular operator is to be used.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目的不是作为RapidMiner工具的教程。实验者应阅读优秀的文档和用户指南，以便熟悉工具的使用。软件中每个操作员都有一个专门的教程——我们建议您在想要了解如何使用特定操作员时使用这些教程。
- en: Once we have imported the test and training data files using the data access
    tools, we will want to visually explore the dataset to familiarize ourselves with
    the lay of the land. Of particular importance is to recognize whether each of
    the 28 attributes are continuous (numeric, integer, or real in RapidMiner) or
    categorical (nominal, binominal, or polynominal in RapidMiner).
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们使用数据访问工具导入测试和训练数据文件，我们就会想要直观地探索数据集，以便熟悉情况。特别重要的是要识别出这28个属性中的每一个是连续的（在RapidMiner中为数值、整数或实数）还是分类的（在RapidMiner中为名义、二项或多项）。
- en: Visualization analysis
  id: totrans-479
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可视化分析
- en: From the **Results** panel of the tool, we perform univariate, bivariate, and
    multivariate analyses of the data. The Statistics tool gives a short summary for
    each feature—min, max, mean, and standard deviation for continuous types and least,
    most, and frequency by category for nominal types.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 从工具的**结果**面板中，我们执行数据的单变量、双变量和多变量分析。统计工具为每个特征提供简短的摘要——对于连续类型，有最小值、最大值、平均值和标准差；对于名义类型，有最少、最多和频率。
- en: 'Interesting characteristics of the data begin to show themselves as we get
    into bivariate analysis. In the Quartile Color Matrix, the color represents the
    two possible target values. As seen in the box plots, we immediately notice some
    attributes discriminate between the two target values more clearly than others.
    Let''s examine a few:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行双变量分析时，数据的有趣特征开始显现。在四分位数颜色矩阵中，颜色代表两个可能的目标值。正如箱线图所示，我们立即注意到一些属性比其他属性更明显地区分这两个目标值。让我们检查几个：
- en: '![Visualization analysis](img/B05137_02_191.jpg)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![可视化分析](img/B05137_02_191.jpg)'
- en: 'Figure 15: Quartile Color Matrix'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：四分位数颜色矩阵
- en: 'Peristalsis: This feature shows a marked difference in distribution when separated
    by target value. There is almost no overlap in the inter-quartile regions between
    the two. This points to the discriminating power of this feature with respect
    to the target.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 蠕动：这个特征在按目标值分开时显示出明显的分布差异。两个四分位数区域之间几乎没有重叠。这表明这个特征在目标方面的区分能力。
- en: The plot for Rectal Temperature, on the other hand, shows no perceptible difference
    in the distributions. This suggests that this feature has low correlation with
    the target. A similar inference may be drawn from the feature Pulse. We expect
    these features to rank fairly low when we evaluate the features for their discriminating
    power with respect to the target.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，直肠温度的图表在分布上没有明显的差异。这表明这个特征与目标的相关性较低。从脉搏这个特征也可以得出类似的推断。我们预计这些特征在评估它们相对于目标区分能力时排名相当低。
- en: Lastly, the plot for Pain has a very different characteristic. It is also discriminating
    of the target, but in a very different way than Peristalsis. In the case of Pain,
    the variance in data for Class 2 is much larger than Class 1\. Abdominal Distension
    also has markedly dissimilar variance across the classes, except with the larger
    variance in Class 2 compared to Class 1.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，疼痛的图表具有非常不同的特征。它也是区分目标的，但与蠕动的区分方式非常不同。在疼痛的情况下，类别2的数据方差比类别1大得多。腹部膨胀在类别之间也有明显不同的方差，除了类别2比类别1有更大的方差。
- en: '![Visualization analysis](img/B05137_02_192.jpg)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
  zh: '![可视化分析](img/B05137_02_192.jpg)'
- en: 'Figure 16: Scatter plot matrix'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：散点图矩阵
- en: An important part of exploring the data is understanding how different attributes
    correlate with each other and with the target. Here we consider pairs of features
    and see if the occurrence of values *in combination* tells us something about
    the target. In these plots, the color of the data points is the target.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 探索数据的一个重要部分是理解不同的属性如何相互关联以及与目标的相关性。在这里，我们考虑特征对，并查看值的组合发生是否告诉我们有关目标的信息。在这些图中，数据点的颜色代表目标。
- en: '![Visualization analysis](img/B05137_02_193.jpg)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![可视化分析](img/B05137_02_193.jpg)'
- en: 'Figure 17: Bubble chart'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：气泡图
- en: In the bubble chart we can visualize four features at once by using the graphing
    tools to specify the *x* and *y* axes as well as a third dimension expressed as
    the size of bubble representing the feature. The target class is denoted by the
    color.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 在气泡图中，我们可以通过使用绘图工具指定x轴和y轴以及表示特征大小的第三维（气泡大小）来同时可视化四个特征。目标类别由颜色表示。
- en: At the low end of total protein, we see higher pH values in the mid-range of
    rectal temperature values. In this cluster, high pH values appear to show a stronger
    correlation to lesions that were surgical. Another cluster with wider variance
    in total protein is also found for values of total protein greater than 50\. The
    variance in pH is also low in this cluster.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 在总蛋白的低端，我们看到直肠温度中值范围内的pH值较高。在这个簇中，高pH值似乎与手术后的病变有更强的相关性。对于总蛋白大于50的值，也发现了一个总蛋白变化范围更宽的簇。这个簇中的pH值变化也较低。
- en: Feature selection
  id: totrans-494
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征选择
- en: Having gained some insight into the data, we are ready to use some of the techniques
    presented in the theory that evaluate feature relevance.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据有了初步了解之后，我们准备使用理论中提出的评估特征相关性的技术。
- en: 'Here we use two techniques: one that calculates the weights for features based
    on Chi-squared statistics with respect to the target attribute and the other based
    on the Gini Impurity Index. The results are shown in the table. Note that as we
    inferred while doing analysis of the features via visualization, both Pulse and
    Rectal Temperature prove to have low relevance as shown by both techniques.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了两种技术：一种基于与目标属性相关的卡方统计量来计算特征权重，另一种基于吉尼不纯度指数。结果如表所示。请注意，正如我们在通过可视化分析特征时推断的那样，脉搏和直肠温度都证明与两种技术显示的低相关性。
- en: '| Chi-squared | Gini index |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| 卡方检验 | 吉尼指数 |'
- en: '| --- | --- |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Attribute | Weight | Attribute | Weight |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 权重 | 属性 | 权重 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Pain | 54.20626 | Pain | 0.083594 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| 疼痛 | 54.20626 | 疼痛 | 0.083594 |'
- en: '| Abdomen | 53.93882 | Abdomen | 0.083182 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| 腹部 | 53.93882 | 腹部 | 0.083182 |'
- en: '| Peristalsis | 38.73474 | Peristalsis | 0.059735 |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| 蠕动 | 38.73474 | 蠕动 | 0.059735 |'
- en: '| AbdominalDistension | 35.11441 | AbdominalDistension | 0.054152 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| 腹部膨胀 | 35.11441 | 腹部膨胀 | 0.054152 |'
- en: '| PeripheralPulse | 23.65301 | PeripheralPulse | 0.036476 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| 周围脉搏 | 23.65301 | 周围脉搏 | 0.036476 |'
- en: '| AbdominocentesisAppearance | 20.00392 | AbdominocentesisAppearance | 0.030849
    |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| 腹腔穿刺外观 | 20.00392 | 腹腔穿刺外观 | 0.030849 |'
- en: '| TemperatureOfExtremeties | 17.07852 | TemperatureOfExtremeties | 0.026338
    |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| 四肢温度 | 17.07852 | 四肢温度 | 0.026338 |'
- en: '| MucousMembranes | 15.0938 | MucousMembranes | 0.023277 |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| 黏膜 | 15.0938 | 黏膜 | 0.023277 |'
- en: '| NasogastricReflux | 14.95926 | NasogastricReflux | 0.023069 |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 鼻胃管反流 | 14.95926 | 鼻胃管反流 | 0.023069 |'
- en: '| PackedCellVolume | 13.5733 | PackedCellVolume | 0.020932 |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| 聚集细胞体积 | 13.5733 | 聚集细胞体积 | 0.020932 |'
- en: '| RectalExamination-Feces | 11.88078 | RectalExamination-Feces | 0.018322 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| 直肠指检-粪便 | 11.88078 | 直肠指检-粪便 | 0.018322 |'
- en: '| CapillaryRefillTime | 8.078319 | CapillaryRefillTime | 0.012458 |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| 毛细血管充盈时间 | 8.078319 | 毛细血管充盈时间 | 0.012458 |'
- en: '| RespiratoryRate | 7.616813 | RespiratoryRate | 0.011746 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| 呼吸频率 | 7.616813 | 呼吸频率 | 0.011746 |'
- en: '| TotalProtein | 5.616841 | TotalProtein | 0.008662 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| 总蛋白 | 5.616841 | 总蛋白 | 0.008662 |'
- en: '| NasogastricRefluxPH | 2.047565 | NasogastricRefluxPH | 0.003158 |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| 鼻胃管反流PH | 2.047565 | 鼻胃管反流PH | 0.003158 |'
- en: '| Pulse | 1.931511 | Pulse | 0.002979 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 脉搏 | 1.931511 | 脉搏 | 0.002979 |'
- en: '| Age | 0.579216 | Age | 8.93E-04 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 年龄 | 0.579216 | 年龄 | 8.93E-04 |'
- en: '| NasogastricTube | 0.237519 |   |   |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| 鼻胃管 | 0.237519 |   |   |'
- en: '| AbdomcentecisTotalProtein | 0.181868 |   |   |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| 腹腔穿刺总蛋白 | 0.181868 |   |   |'
- en: '| RectalTemperature | 0.139387 |   |   |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| 直肠温度 | 0.139387 |   |   |'
- en: '*Table 3: Relevant features determined by two different techniques, Chi-squared
    and Gini index.*'
  id: totrans-521
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表3：由两种不同技术（卡方检验和吉尼指数）确定的关联特征。*'
- en: Model process flow
  id: totrans-522
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型流程
- en: 'In RapidMiner you can define a pipeline of computations using operators with
    inputs and outputs that can be chained together. The following process represents
    the flow used to perform the entire set of operations starting with loading the
    training and test data, handling missing values, weighting features by relevance,
    filtering out low scoring features, training an ensemble model that uses Bagging
    with Random Forest as the algorithm, and finally applying the learned model to
    the test data and outputting the performance metrics. Note that all the preprocessing
    steps that are applied to the training dataset must also be applied, in the same
    order, to the test set by means of the Group Models operator:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 在RapidMiner中，您可以使用具有输入和输出的运算符定义计算流程，这些运算符可以串联在一起。以下流程表示执行整个操作集的流程，从加载训练和测试数据，处理缺失值，按相关性加权特征，过滤掉得分低的特征，训练使用Bagging和随机森林作为算法的集成模型，最后将学习模型应用于测试数据并输出性能指标。请注意，应用于训练数据集的所有预处理步骤必须以相同的顺序通过Group
    Models运算符应用于测试集：
- en: '![Model process flow](img/B05137_02_194.jpg)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
  zh: '![模型流程图](img/B05137_02_194.jpg)'
- en: 'Figure 18: RapidMiner process diagram'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：RapidMiner流程图
- en: Following the top of the process, the training set is ingested in the left-most
    operator, followed by the exclusion of non-predictors (Hospital Number, CP data)
    and self-predictors (Lesion 1). This is followed by the operator that replaces
    missing values with the mean and mode for continuous and categorical attributes,
    respectively. Next, the Feature Weights operator evaluates weights for each feature
    based on the Chi-squared statistic, which is followed by a filter that ignores
    low-weighted features. This pre-processed dataset is then used to train a model
    using Bagging with a Random Forest classifier.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在流程的顶部，训练集由最左侧的运算符摄取，随后排除非预测因子（医院编号、CP数据）和自预测因子（病变1）。这之后是替换缺失值（对于连续属性使用均值，对于分类属性使用众数）的运算符。接下来，特征权重运算符根据卡方统计量评估每个特征的权重，随后是一个忽略低权重特征的过滤器。然后，使用Bagging和随机森林分类器训练模型，使用预处理后的数据集。
- en: The preprocessing steps used on the training data are grouped together in the
    appropriate order via the Group Models operator and applied to the test data in
    the penultimate step. Finally, the predictions of the target variable on the test
    examples accompanied by the confusion matrix and other performance metrics are
    made evaluated and presented in the last step.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据上使用的预处理步骤通过Group Models运算符按适当顺序分组，并在倒数第二步应用于测试数据。最后，在最后一步，对测试示例上的目标变量的预测以及混淆矩阵和其他性能指标进行评估和展示。
- en: Model evaluation metrics
  id: totrans-528
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型评估指标
- en: We are now ready to compare the results from the various models. If you have
    followed along you may find that your results vary from what's presented here—that
    may be due to the stochastic nature of some learning algorithms, or differences
    in the values of some hyper-parameters used in the models.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以比较各种模型的结果。如果您一直跟随，您可能会发现您的结果与这里展示的不同——这可能是由于某些学习算法的随机性质，或者模型中使用的某些超参数值的差异。
- en: 'We have considered three different training datasets:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了三个不同的训练数据集：
- en: Original training data with missing values
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始训练数据包含缺失值
- en: Training data transformed with missing values handled
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失值的转换后的训练数据
- en: Training data transformed with missing values handled and with feature selection
    (Chi-Square) applied to select features that are highly discriminatory.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失值并应用特征选择（卡方检验）以选择高度区分性特征的转换后的训练数据
- en: 'We have considered three different sets of algorithms on each of the datasets:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每个数据集上考虑了三组不同的算法：
- en: Linear algorithms (Naïve Bayes and Logistic Regression)
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性算法（朴素贝叶斯和逻辑回归）
- en: Non-linear algorithms (Decision Tree and KNN)
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性算法（决策树和KNN）
- en: Ensemble algorithms (Bagging, Ada Boost, and Random Forest).
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成算法（Bagging、Ada Boost和随机森林）
- en: Evaluation on Confusion Metrics
  id: totrans-538
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 混淆度指标评估
- en: '| Models | TPR | FPR | Precision | Specificity | Accuracy | AUC |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 真阳性率 | 假阳性率 | 精确率 | 特异性 | 准确率 | AUC |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Naïve Bayes | 68.29% | 14.81% | 87.50% | 85.19% | 75.00% | 0.836 |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 68.29% | 14.81% | 87.50% | 85.19% | 75.00% | 0.836 |'
- en: '| Logistic Regression | 78.05% | 14.81% | 88.89% | 85.19% | 80.88% | 0.856
    |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 78.05% | 14.81% | 88.89% | 85.19% | 80.88% | 0.856 |'
- en: '| Decision Tree | 68.29% | 33.33% | 75.68% | 66.67% | 67.65% | 0.696 |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 68.29% | 33.33% | 75.68% | 66.67% | 67.65% | 0.696 |'
- en: '| k-NN | 90.24% | 85.19% | 61.67% | 14.81% | 60.29% | 0.556 |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| k-NN | 90.24% | 85.19% | 61.67% | 14.81% | 60.29% | 0.556 |'
- en: '| Bagging (GBT) | 90.24% | 74.07% | 64.91% | 25.93% | 64.71% | 0.737 |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| Bagging (GBT) | 90.24% | 74.07% | 64.91% | 25.93% | 64.71% | 0.737 |'
- en: '| Ada Boost (Naïve Bayes) | 63.41% | 48.15% | 66.67% | 51.85% | 58.82% | 0.613
    |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| Ada Boost (朴素贝叶斯) | 63.41% | 48.15% | 66.67% | 51.85% | 58.82% | 0.613 |'
- en: '*Table 4: Results on unseen (Test) data for models trained on Horse-colic data
    with missing values*'
  id: totrans-547
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表 4：在具有缺失值的马肠炎数据上训练的模型在未见（测试）数据上的结果*'
- en: '| Models | TPR | FPR | Precision | Specificity | Accuracy | AUC |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 真阳性率 | 假阳性率 | 精确度 | 特异性 | 准确率 | AUC |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Naïve Bayes | 68.29% | 66.67% | 60.87% | 33.33% | 54.41% | 0.559 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 68.29% | 66.67% | 60.87% | 33.33% | 54.41% | 0.559 |'
- en: '| Logistic Regression | 78.05% | 62.96% | 65.31% | 37.04% | 61.76% | 0.689
    |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 78.05% | 62.96% | 65.31% | 37.04% | 61.76% | 0.689 |'
- en: '| Decision Tree | 97.56% | 96.30% | 60.61% | 3.70% | 60.29% | 0.812 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 97.56% | 96.30% | 60.61% | 3.70% | 60.29% | 0.812 |'
- en: '| k-NN | 75.61% | 48.15% | 70.45% | 51.85% | 66.18% | 0.648 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| k-NN | 75.61% | 48.15% | 70.45% | 51.85% | 66.18% | 0.648 |'
- en: '| Bagging (Random Forest) | 97.56% | 74.07% | 66.67% | 25.93% | 69.12% | 0.892
    |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| Bagging (随机森林) | 97.56% | 74.07% | 66.67% | 25.93% | 69.12% | 0.892 |'
- en: '| Bagging (GBT) | 82.93% | 18.52% | 87.18% | 81.48% | 82.35% | 0.870 |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| Bagging (GBT) | 82.93% | 18.52% | 87.18% | 81.48% | 82.35% | 0.870 |'
- en: '| Ada Boost (Naïve Bayes) | 68.29% | 7.41% | 93.33% | 92.59% | 77.94% | 0.895
    |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| Ada Boost (朴素贝叶斯) | 68.29% | 7.41% | 93.33% | 92.59% | 77.94% | 0.895 |'
- en: '*Table 5: Results on unseen (Test) data for models trained on Horse-colic data
    with missing values replaced*'
  id: totrans-557
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表 5：在替换缺失值后训练的模型在马肠炎数据上的未见（测试）数据结果*'
- en: '| Models | TPR | FPR | Precision | Specificity | Accuracy | AUC |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 真阳性率 | 假阳性率 | 精确度 | 特异性 | 准确率 | AUC |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Naïve Bayes | 75.61% | 77.78% | 59.62% | 29.63% | 54.41% | 0.551 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 75.61% | 77.78% | 59.62% | 29.63% | 54.41% | 0.551 |'
- en: '| Logistic Regression | 82.93% | 62.96% | 66.67% | 37.04% | 64.71% | 0.692
    |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 82.93% | 62.96% | 66.67% | 37.04% | 64.71% | 0.692 |'
- en: '| Decision Tree | 95.12% | 92.59% | 60.94% | 7.41% | 60.29% | 0.824 |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 95.12% | 92.59% | 60.94% | 7.41% | 60.29% | 0.824 |'
- en: '| k-NN | 75.61% | 48.15% | 70.45% | 51.85% | 66.18% | 0.669 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| k-NN | 75.61% | 48.15% | 70.45% | 51.85% | 66.18% | 0.669 |'
- en: '| Bagging (Random Forest) | 92.68% | 33.33% | 80.85% | 66.67% | 82.35% | 0.915
    |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| Bagging (随机森林) | 92.68% | 33.33% | 80.85% | 66.67% | 82.35% | 0.915 |'
- en: '| Bagging (GBT) | 78.05% | 22.22% | 84.21% | 77.78% | 77.94% | 0.872 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| Bagging (GBT) | 78.05% | 22.22% | 84.21% | 77.78% | 77.94% | 0.872 |'
- en: '| Ada Boost (Naïve Bayes) | 68.29% | 18.52% | 84.85% | 81.48% | 73.53% | 0.848
    |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| Ada Boost (朴素贝叶斯) | 68.29% | 18.52% | 84.85% | 81.48% | 73.53% | 0.848 |'
- en: '*Table 6: Results on unseen (Test) data for models trained on Horse-colic data
    using features selected by Chi-squared statistic technique*'
  id: totrans-567
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表 6：使用卡方统计技术选择的特征在马肠炎数据上训练的模型在未见（测试）数据上的结果*'
- en: ROC Curves, Lift Curves, and Gain Charts
  id: totrans-568
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: ROC 曲线、提升曲线和增益图
- en: The performance plots enable us to visually assess the models used in two of
    the three experiments—without any replacement of missing data, and with using
    features from Chi-squared weighting after replacing missing data—and to compare
    them against each other. Pairs of plots display the performance curves of each
    Linear (Logistic Regression), Non-linear (Decision Tree), and Ensemble (Bagging,
    using Gradient Boosted Tree) technique we learned about earlier in the chapter,
    drawn from results of the two experiments.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 性能图使我们能够直观地评估三个实验中使用的两个模型——在没有替换缺失数据的情况下，以及在用卡方加权特征替换缺失数据后——并将它们相互比较。成对的图显示了我们在本章
    earlier 中了解到的每个线性（逻辑回归）、非线性（决策树）和集成（Bagging，使用梯度提升树）技术的性能曲线，这些曲线来自两个实验的结果。
- en: '![ROC Curves, Lift Curves, and Gain Charts](img/B05137_02_195.jpg)'
  id: totrans-570
  prefs: []
  type: TYPE_IMG
  zh: '![ROC 曲线、提升曲线和增益图](img/B05137_02_195.jpg)'
- en: 'Figure 19: ROC Performance curves for experiment using Missing Data'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：使用缺失数据进行的实验的 ROC 性能曲线
- en: '![ROC Curves, Lift Curves, and Gain Charts](img/B05137_02_196.jpg)'
  id: totrans-572
  prefs: []
  type: TYPE_IMG
  zh: '![ROC 曲线、提升曲线和增益图](img/B05137_02_196.jpg)'
- en: 'Figure 20: Cumulative Gains performance curves for experiment using Missing
    Data'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：使用缺失数据进行的实验的累积增益性能曲线
- en: '![ROC Curves, Lift Curves, and Gain Charts](img/B05137_02_197.jpg)'
  id: totrans-574
  prefs: []
  type: TYPE_IMG
  zh: '![ROC 曲线、提升曲线和增益图](img/B05137_02_197.jpg)'
- en: 'Figure 21: Lift performance curves for experiment using Missing Data'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：使用缺失数据进行的实验的升力性能曲线
- en: Results, observations, and analysis
  id: totrans-576
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果、观察和分析
- en: The impact of handling missing values is significant. Of the seven classifiers,
    with the exception of Naïve Bayes and Logistic Regression, all show remarkable
    improvement when missing values are handled as indicated by various metrics, including
    AUC, precision, accuracy, and specificity. This tells us that handling missing
    values that can be "noisy" is an important aspect of data transformation. Naive
    Bayes has its own internal way of managing missing values and the results from
    our experiments show that it does a better job of null-handling than our external
    transformations. But in general, the idea of transforming missing values seems
    beneficial when you consider all of the classifiers.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失值的影响是显著的。在七个分类器中，除了朴素贝叶斯和逻辑回归之外，当按照各种指标（包括AUC、精确度、准确率和特异性）处理缺失值时，所有分类器都表现出显著的改进。这告诉我们，处理可能“噪声”的缺失值是数据转换的重要方面。朴素贝叶斯有其处理缺失值的内部方式，而我们实验的结果显示，它在处理空值方面比我们的外部转换做得更好。但总的来说，当考虑到所有分类器时，转换缺失值的概念似乎是有益的。
- en: As discussed in the section on modeling, some of the algorithms require the
    right handling of missing values and feature selection to get optimum performance.
    From the results, we can see that the performance of Decision Trees, for example,
    improved incrementally from 0.696 with missing data, 0.812 with managed missing
    data, and for the best performance of 0.824 with missing data handled together
    with feature selection. Six out of seven classifiers improve the performance in
    AUC (and in others metrics) when both the steps are performed; comparing *Table
    5* and *Table 6* for AUC gives us these quick insights. This demonstrates the
    importance of doing preprocessing such as missing value handling along with feature
    selection before performing modeling.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 如建模部分所述，一些算法需要正确处理缺失值和特征选择才能获得最佳性能。从结果中我们可以看到，例如，决策树的性能从缺失数据的0.696逐步提高到管理缺失数据的0.812，以及与特征选择一起处理缺失数据时的最佳性能0.824。七个分类器中有六个在执行这两个步骤时，AUC（以及其他指标）的性能都有所提高；比较*AUC*的*表5*和*表6*提供了这些快速见解。这证明了在进行建模之前进行预处理（如处理缺失值和特征选择）的重要性。
- en: A major conclusion from the results is that the problem is highly non-linear
    and therefore most non-linear classifiers from the simplest Decision Trees to
    ensemble Random Forest perform very well. The best performance comes from the
    meta-learning algorithm Random Forest, with missing values properly handled and
    the most relevant features used in training. The best linear model performance
    measured by AUC was 0.856 for Logistic Regression with data as-is (that is, with
    missing values), whereas Random Forest achieved AUC performance of 0.915 with
    proper handling of missing data accompanied by feature selection. Generally, as
    evident from *Table 3*, the non-linear classifiers or meta-learners performed
    better than linear classifiers by most performance measures.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中得出的一个主要结论是，问题高度非线性，因此从最简单的决策树到集成随机森林的多数非线性分类器都表现出色。最佳性能来自元学习算法随机森林，在适当处理缺失值并使用最相关特征进行训练的情况下。以AUC衡量的最佳线性模型性能是逻辑回归的0.856（数据如原样，即包含缺失值），而随机森林通过适当处理缺失数据并伴随特征选择实现了AUC性能的0.915。一般而言，如*表3*所示，非线性分类器或元学习者在大多数性能指标上都优于线性分类器。
- en: Handling missing values, which can be thought as "noise", in the appropriate
    manner improves the performance of AdaBoost by a significant amount. The AUC improves
    from 0.613 to 0.895 and FPR reduces from 48.15 to 7.41%. This indeed conforms
    to the expected theoretical behavior for this technique.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 正确处理缺失值，这可以被视为“噪声”，可以显著提高AdaBoost的性能。AUC从0.613提高到0.895，FPR从48.15%降低到7.41%。这确实符合该技术的预期理论行为。
- en: Meta-learning techniques, which use concepts of boosting and bagging, are relatively
    more effective when dealing with real-world data, when compared to other common
    techniques. This seems to be justified by the results since AdaBoost with Naïve
    Bayes as base learner trained on data that has undergone proper handling of noise
    outperforms Naive Bayes in most of the metrics, as shown in *Table 5* and *Table
    6*. Random Forest and GBTs also show the best performance along with AdaBoost
    as compared to base classifiers in *Table 6*, again confirming that the right
    process and ensemble learning can produce the most optimum results in real-world
    noisy datasets.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他常见技术相比，元学习技术，即使用提升和袋装概念的技巧，在处理现实世界数据时相对更有效。这似乎由结果得到证实，因为基于经过适当噪声处理的AdaBoost（以朴素贝叶斯作为基学习器）在大多数指标上优于朴素贝叶斯，如*表5*和*表6*所示。随机森林和GBTs与AdaBoost相比，在*表6*中也表现出最佳性能，再次证实了正确的过程和集成学习可以在现实世界的噪声数据集中产生最优化结果。
- en: Note
  id: totrans-582
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'All data, models, and results for both WEKA and RapidMiner process files from
    this chapter are available at: [https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter2](https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter2).'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中WEKA和RapidMiner处理文件的全部数据、模型和结果均可在以下网址找到：[https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter2](https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter2)。
- en: Summary
  id: totrans-584
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Supervised learning is the predominant technique used in machine learning applications.
    The methodology consists of a series of steps beginning with data exploration,
    data transformation, and data sampling, through feature reduction, model building,
    and ultimately, model assessment and comparison. Each step of the process involves
    some decision making which must answer key questions: How should we impute missing
    values? What data sampling strategy should we use? What is the most appropriate
    algorithm given the amount of noise in the dataset and the prescribed goal of
    interpretability? This chapter demonstrated the application of these processes
    and techniques to a real-world problem—the classification problem using the UCI
    Horse Colic dataset.'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是机器学习应用中占主导地位的技术。该方法包括一系列步骤，从数据探索、数据转换和数据采样开始，通过特征减少、模型构建，最终到模型评估和比较。过程中的每一步都涉及一些决策，这些决策必须回答关键问题：我们应该如何插补缺失值？我们应该使用什么数据采样策略？在数据集中的噪声量和规定的可解释性目标下，最合适的算法是什么？本章展示了将这些过程和技术应用于现实世界问题——使用UCI马绞痛数据集的分类问题。
- en: Whether the problem is one of classification, when the target is a categorical
    value, or Regression, when it is a real-valued continuous variable, the methodology
    used for supervised learning is similar. In this chapter, we have used classification
    for illustration.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 无论问题是分类问题，目标是一个分类值，还是回归问题，目标是一个实值连续变量，用于监督学习的方法都是相似的。在本章中，我们使用了分类进行说明。
- en: The first step is data quality analysis, which includes descriptive statistics
    of the features, visualization analysis using univariate, and multivariate feature
    analysis. With the help of various plot types, we can uncover different trends
    in the data and examine how certain features may or may not correlate with the
    label values and with each other. Data analysis is followed by data pre-processing,
    where the techniques include ways to address noise, as in the case of missing
    data, and outliers, as well as preparing the data for modeling techniques through
    normalization and discretization.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是数据分析，这包括特征描述性统计、使用单变量和多变量特征分析的可视化分析。借助各种绘图类型，我们可以揭示数据中的不同趋势，并检查某些特征可能与标签值以及彼此相关或无关。数据分析之后是数据预处理，其中技术包括处理噪声的方法，例如缺失数据的情况，以及异常值，以及通过归一化和离散化准备数据以供建模技术使用。
- en: Following pre-processing, we must suitably split the data into train, validation,
    and test samples. Different sampling strategies may be used depending on the characteristics
    of the data and the problem at hand, for example, when the data is skewed or when
    we have a multi-class classification problem. Depending on data size, cross-validation
    is a common alternative to creating a separate validation set.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理之后，我们必须适当地将数据分成训练、验证和测试样本。根据数据和问题的特性，可能使用不同的采样策略，例如，当数据有偏斜或当我们有一个多类分类问题时。根据数据大小，交叉验证是创建单独验证集的常见替代方案。
- en: The next step is the culling of irrelevant features. In the filter approach,
    techniques that use univariate analysis are either entropy-based (Information
    Gain, Gains Ratio) or based on statistical hypothesis testing (Chi-Squared). With
    the main multivariate methods, the aim is reduction of redundant features when
    considered together, or using the ones that correlate most closely with the target
    label. In the wrapper approach, we use machine learning algorithms to tell us
    about the more discriminating features. Finally, some learning techniques have
    feature selection embedded in the algorithm in the form of a regularization term,
    typically using ridge or lasso techniques. These represent the embedded approach.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是剔除无关特征。在过滤方法中，使用单变量分析的技术要么是基于熵的（信息增益、增益比率）要么是基于统计假设检验的（卡方检验）。在主要的多变量方法中，目标是当考虑在一起时减少冗余特征，或者使用与目标标签相关性最高的特征。在包装方法中，我们使用机器学习算法来告诉我们哪些特征更具区分性。最后，一些学习技术将特征选择嵌入到算法中，通常使用岭回归或Lasso技术作为正则化项。这些代表嵌入式方法。
- en: Modeling techniques are broadly classified into linear, non-linear, and ensemble
    methods. Among linear algorithms, the type of features can determine the algorithms
    to use—Linear Regression (numeric features only), Naïve Bayes (numeric or categorical),
    and logistic regression (numeric features only, or categorical transformed to
    numeric) are the work-horses. The outlined advantages and disadvantages of each
    method must be understood when choosing between them or interpreting the results
    of learning using these models.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 建模技术可以广泛地分为线性、非线性以及集成方法。在线性算法中，特征类型可以决定使用哪种算法——线性回归（仅限数值特征）、朴素贝叶斯（数值或分类）和逻辑回归（仅限数值特征，或分类转换为数值）是主要的工具。在选择它们或解释使用这些模型进行学习的结果时，必须理解每种方法的优缺点。
- en: Decision Tree, k-NN, and SVM are non-linear techniques, each with their own
    strengths and limitations. For example, interpretability is the main advantage
    of Decision Tree. k-NN is robust in the face of noisy data, but it does poorly
    with high-dimensional data. SVM suffers from poor interpretability, but shines
    even when the dataset is limited, and the number of features is large.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树、k-NN和SVM都是非线性技术，每种技术都有其自身的优势和局限性。例如，可解释性是决策树的主要优势。k-NN在面对噪声数据时具有鲁棒性，但在处理高维数据时表现不佳。SVM的可解释性较差，但当数据集有限且特征数量较多时，其表现依然出色。
- en: With a number of different models collaborating, ensemble methods can leverage
    the best of all. Bagging and boosting both are techniques that generalize better
    in the ensemble compared to the base learner they use and are popular in many
    applications.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个不同模型协作的情况下，集成方法可以充分利用所有模型的优势。Bagging和Boosting都是相对于它们所使用的基学习器在集成中表现更好的技术，并且在许多应用中都很受欢迎。
- en: Finally, what are the strategies and methods that can be used in evaluating
    model performance and comparing models to each other? The role of validation sets
    or cross-validation is essential to the ability to generalize over unseen data.
    Performance evaluation metrics derived from the confusion matrix are used universally
    to evaluate classifiers; some are used more commonly in certain domains and disciplines
    than others. ROC, Gain, and Lift curves are great visual representations of the
    range of model performance as the classification threshold is varied. When comparing
    models in pairs, several metrics based on statistical hypothesis testing are used.
    Wilcoxon and McNemar's are two non-parametric tests; Paired-t test is an example
    of a parametric method. Likewise, when comparing multiple algorithms, a common
    non-parametric test that does not make assumptions about the data distribution
    is Friedman's test. ANOVA, which are parametric tests, assume normal distribution
    of the metrics and equal variances.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，评价模型性能和比较模型之间有哪些策略和方法可以使用？验证集或交叉验证在泛化到未见数据的能力中起着至关重要的作用。从混淆矩阵中衍生出的性能评估指标被普遍用于评估分类器；某些指标在某些领域和学科中比其他指标更常用。ROC、增益和提升曲线是随着分类阈值变化时模型性能范围的优秀可视化表示。在成对比较模型时，使用基于统计假设检验的几个指标。Wilcoxon和McNemar的检验是两种非参数检验；配对t检验是参数方法的一个例子。同样，在比较多个算法时，Friedman检验是一种常用的非参数检验，它不对数据分布做出假设。ANOVA是参数检验，它假设指标服从正态分布且方差相等。
- en: The final sections of the chapter present the process undertaken using the RapidMiner
    tool to develop and evaluate models generated to classify test data from the UCI
    Horse-colic dataset. Three experiments are designed to compare and contrast the
    performance of models under different data pre-processing conditions, namely,
    without handling missing data, with replacement of missing data using standard
    techniques, and finally, with feature selection following null replacement. In
    each experiment we choose multiple linear, non-linear, and ensemble methods. As
    part of the overall process, we illustrate how the modeling environment is used.
    We can draw revealing conclusions from the results, which give us insights into
    the data as well as demonstrating the relative strengths and weakness of the various
    classes of techniques in different situations. We conclude that the data is highly
    non-linear and that ensemble learning demonstrates clear advantages over other
    techniques.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后几节介绍了使用RapidMiner工具开发并评估用于从UCI马绞痛数据集测试数据分类生成的模型的流程。设计了三个实验来比较和对比在不同数据预处理条件下的模型性能，即不处理缺失数据、使用标准技术替换缺失数据，以及最终在空替换后进行特征选择。在每个实验中，我们选择了多个线性、非线性以及集成方法。作为整体流程的一部分，我们说明了如何使用建模环境。我们可以从结果中得出有意义的结论，这些结论不仅让我们对数据有了深入了解，还展示了不同情况下各种技术类别的相对优势和劣势。我们得出结论，数据高度非线性，集成学习在与其他技术相比时显示出明显的优势。
- en: References
  id: totrans-595
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: D. Bell and H. Wang (2000). *A Formalism for Relevance and its Application in
    Feature Subset Selection. Machine Learning*, 41(2):175–195.
  id: totrans-596
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D. Bell和H. Wang (2000). *相关性形式化及其在特征子集选择中的应用. 机器学习*, 41(2):175–195.
- en: 'J. Doak (1992). *An Evaluation of Feature Selection Methods and their Application
    to Computer Security*. Technical Report CSE–92–18, Davis, CA: University of California,
    Department of Computer Science.'
  id: totrans-597
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: J. Doak (1992). *特征选择方法及其在计算机安全中的应用评估*. 技术报告CSE–92–18, 加州大学戴维斯分校，计算机科学系.
- en: M. Ben-Bassat (1982). *Use of Distance Measures, Information Measures and Error
    Bounds in Feature Evaluation*. In P. R. Krishnaiah and L. N. Kanal, editors, Handbook
    of Statistics, volume 2, pages 773–791, North Holland.
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M. Ben-Bassat (1982). *在特征评估中使用距离度量、信息度量以及误差界限. 在P. R. Krishnaiah和L. N. Kanal编辑的《统计学手册》，第2卷，第773–791页，North
    Holland.
- en: Littlestone N, Warmuth M (1994) *The weighted majority algorithm*. Information
    Computing 108(2):212–261
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Littlestone N, Warmuth M (1994) *加权多数算法*. 《信息计算》. 108(2):212–261
- en: Breiman L., Friedman J.H., Olshen R.A., Stone C.J. (1984) *Classification and
    Regression Trees*, Wadsforth International Group.
  id: totrans-600
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Breiman L., Friedman J.H., Olshen R.A., Stone C.J. (1984) *分类与回归树*. Wadsforth国际集团.
- en: B. Ripley(1996), *Pattern recognition and neural networks*. Cambridge University
    Press, Cambridge.
  id: totrans-601
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B. Ripley(1996), *模式识别与神经网络*. 剑桥大学出版社，剑桥.
- en: Breiman, L., (1996). *Bagging Predictors, Machine Learning*, 24 123-140.
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Breiman, L., (1996). *Bagging预测器，机器学习*. 24 123-140.
- en: Burges, C. (1998). *A tutorial on support vector machines for pattern recognition.
    Data Mining and Knowledge Discovery*. 2(2):1-47.
  id: totrans-603
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Burges, C. (1998). *支持向量机在模式识别中的教程*. 《数据挖掘与知识发现》. 2(2):1-47.
- en: Bouckaert, R. (2004), *Naive Bayes Classifiers That Perform Well with Continuous
    Variables, Lecture Notes in Computer Science*, Volume 3339, Pages 1089 – 1094.
  id: totrans-604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bouckaert, R. (2004), *在连续变量上表现良好的朴素贝叶斯分类器，计算机科学讲义*, 第3339卷，第1089 – 1094页.
- en: Aha D (1997). *Lazy learning*, Kluwer Academic Publishers, Dordrecht
  id: totrans-605
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Aha D (1997). *懒惰学习*, Kluwer学术出版社，多德雷赫特
- en: Nadeau, C. and Bengio, Y. (2003), *Inference for the generalization error*.
    In Machine Learning 52:239– 281.
  id: totrans-606
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nadeau, C. and Bengio, Y. (2003), *泛化误差的推理*. 在《机器学习》52:239– 281.
- en: 'Quinlan, J.R. (1993). C4.5: *Programs for machine learning*, Morgan Kaufmann,
    San Francisco.'
  id: totrans-607
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Quinlan, J.R. (1993). C4.5: *机器学习程序*, Morgan Kaufmann, 旧金山.'
- en: Vapnik, V. (1995), *The Nature of Statistical Learning Theory*. Springer Verlag.
  id: totrans-608
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vapnik, V. (1995), *统计学习理论的本质*. Springer Verlag.
- en: 'Schapire RE, Singer Y, Singhal A (1998). *Boosting and Rocchio applied to text
    filtering*. In SIGIR ''98: Proceedings of the 21st Annual International Conference
    on Research and Development in Information Retrieval, pp 215–223'
  id: totrans-609
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Schapire RE, Singer Y, Singhal A (1998). *Boosting和Rocchio应用于文本过滤*. 在SIGIR
    ''98: 第21届国际信息检索研究与发展年度会议论文集，第215–223页'
- en: Breiman L.(2001). *Random Forests*. Machine Learning, 45 (1), pp 5-32.
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Breiman L.(2001). *随机森林*. 《机器学习》, 45 (1), pp 5-32.
- en: 'Nathalie Japkowicz and Mohak Shah (2011). *Evaluating Learning Algorithms:
    A Classification Perspective*. Cambridge University Press.'
  id: totrans-611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nathalie Japkowicz 和 Mohak Shah (2011). *评估学习算法：从分类的角度*. 剑桥大学出版社.
- en: Hanley, J. & McNeil, B. (1982). *The meaning and use of the area under a receiver
    operating characteristic (ROC) curve*. Radiology 143, 29–36.
  id: totrans-612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hanley, J. & McNeil, B. (1982). *受试者工作特征曲线下面积的意义和使用*. 放射学 143, 29–36.
- en: 'Tjen-Sien, L., Wei-Yin, L., Yu-Shan, S. (2000). *A Comparison of Prediction
    Accuracy, Complexity, and Training Time of Thirty-Three Old and New Classification
    Algorithms*. Machine Learning 40: 203–228.'
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Tjen-Sien, L., Wei-Yin, L., Yu-Shan, S. (2000). *比较三十三种旧的和新的分类算法的预测准确性、复杂性和训练时间*.
    机器学习 40: 203–228.'
- en: A. W. Moore and M. S. Lee (1994). *Efficient Algorithms for Minimizing Cross
    Validation Error*. In Proc. of the 11th Int. Conf. on Machine Learning, pages
    190–198, New Brunswick, NJ. Morgan Kaufmann.
  id: totrans-614
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. W. Moore 和 M. S. Lee (1994). *最小化交叉验证错误的有效算法*. 在第11届国际机器学习会议论文集中，第190–198页，新不伦瑞克州新布鲁斯威克，Morgan
    Kaufmann出版社.
- en: Nitesh V. Chawla et. al. (2002). *Synthetic Minority Over-sampling Technique*.
    Journal of Artificial Intelligence Research. 16:321-357.
  id: totrans-615
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nitesh V. Chawla 等人 (2002). *合成少数类过采样技术*. 人工智能研究杂志. 16:321-357.
