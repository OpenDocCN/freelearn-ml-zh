- en: Chapter 2. Practical Approach to Real-World Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability to learn from observations accompanied by marked targets or labels,
    usually in order to make predictions about unseen data, is known as **supervised
    machine learning**. If the targets are categories, the problem is one of classification
    and if they are numeric values, it is called **regression**. In effect, what is
    being attempted is to infer the function that maps the data to the target. Supervised
    machine learning is used extensively in a wide variety of machine learning applications,
    whenever labeled data is available or the labels can be added manually.
  prefs: []
  type: TYPE_NORMAL
- en: The core assumption of supervised machine learning is that the patterns that
    are learned from the data used in training will manifest themselves in yet unseen
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the steps used to explore, analyze, and pre-process
    the data before proceeding to training models. We will then introduce different
    modeling techniques ranging from simple linear models to complex ensemble models.
    We will present different evaluation metrics and validation criteria that allow
    us to compare model performance. Some of the discussions are accompanied by brief
    mathematical explanations that should help express the concepts more precisely
    and whet the appetite of the more mathematically inclined readers. In this chapter,
    we will focus on classification as a method of supervised learning, but the principles
    apply to both classification and regression, the two broad applications of supervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Beginning with this chapter, we will introduce tools to help illustrate how
    the concepts presented in each chapter are used to solve machine learning problems.
    Nothing reinforces the understanding of newly learned material better than the
    opportunity to apply that material to a real-world problem directly. In the process,
    we often gain a clearer and more relatable understanding of the subject than what
    is possible with passive absorption of the theory alone. If the opportunity to
    learn new tools is part of the learning, so much the better! To meet this goal,
    we will introduce a classification dataset familiar to most data science practitioners
    and use it to solve a classification problem while highlighting the process and
    methodologies that guide the solution.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use RapidMiner and Weka for building the process by
    which we learn from a single well-known dataset. The workflows and code are available
    on the website for readers to download, execute, and modify.
  prefs: []
  type: TYPE_NORMAL
- en: RapidMiner is a GUI-based Java framework that makes it very easy to conduct
    a data science project, end-to-end, from within the tool. It has a simple drag-and-drop
    interface to build process workflows to ingest and clean data, explore and transform
    features, perform training using a wide selection of machine learning algorithms,
    do validation and model evaluation, apply your best models to test data, and more.
    It is an excellent tool to learn how to make the various parts of the process
    work together and produce rapid results. Weka is another GUI-based framework and
    it has a Java API that we will use to illustrate more of the coding required for
    performing analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The major topics that we will cover in this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: Data quality analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Descriptive data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformation and preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature relevance analysis and dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model assessment, evaluation, and comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detailed case study—Horse Colic Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formal description and notation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We would like to introduce some notation and formal definitions for the terms
    used in supervised learning. We will follow this notation through the rest of
    the book when not specified and extend it as appropriate when new concepts are
    encountered. The notation will provide a precise and consistent language to describe
    the terms of art and enable a more rapid and efficient comprehension of the subject.
  prefs: []
  type: TYPE_NORMAL
- en: '**Instance**: Every observation is a data instance. Normally the variable *X*
    is used to represent the input space. Each data instance has many variables (also
    called features) and is referred to as **x** (vector representation with bold)
    of dimension *d* where *d* denotes the number of variables or features or attributes
    in each instance. The features are represented as **x** = *(x*[1]*,x*[2]*,…x*[d]*)*^T,
    where each value is a scalar when it is numeric corresponding to the feature value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Label**: The label (also called target) is the dependent variable of interest,
    generally denoted by *y*. In **classification**, values of the label are well-defined
    categories in the problem domain; they need not be numeric or things that can
    be ordered. In **regression**, the label is real-valued.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binary classification**, where the target takes only two values, it is mathematically
    represented as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: y ∈ {1,–1}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Regression**, where the target can take any value in the real number domain,
    is represented as:![Formal description and notation](img/B05137_02_008.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset**: Generally, the dataset is denoted by *D* and consists of individual
    data instances and their labels. The instances are normally represented as set
    {**x**[1],**x**[2]…**x**[n]}. The labels for each instance are represented as
    the set **y** = {*y*[1]*,y*[2]*,…y*[n]}. The entire labeled dataset is represented
    as paired elements in a set as given by *D* = {(**x**[1], *y*[1]),(**x**[2], *y*[2])…(**x**[n],
    *y*[n])} where ![Formal description and notation](img/B05137_02_013.jpg) for real-valued
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data quality analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are limitations to what can be learned from data that suffers from poor
    quality. Problems with quality can include, among other factors, noisy data, missing
    values, and errors in labeling. Therefore, the first step is to understand the
    data before us in order that we may determine how to address any data quality
    issues. Are the outliers merely noise or indicative of interesting anomalies in
    the population? Should missing data be handled the same way for all features?
    How should sparse features be treated? These and similar questions present themselves
    at the very outset.
  prefs: []
  type: TYPE_NORMAL
- en: If we're fortunate, we receive a cleansed, accurately labeled dataset accompanied
    by documentation describing the data elements, the data's pedigree, and what if
    any transformations were already done to the data. Such a dataset would be ready
    to be split into train, validation, and test samples, using methods described
    in the section on Data Sampling. However, if data is not cleansed and suitable
    to be partitioned for our purposes, we must first prepare the data in a principled
    way before sampling can begin. (The significance of partitioning the data is explained
    later in this chapter in a section dedicated to train, validation, and test sets).
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will discuss the data quality analysis and transformation
    steps that are needed before we can analyze the features.
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The complete data sample (including train, validation, and test) should be analyzed
    and summarized for the following characteristics. In cases where the data is not
    already split into train, validate, and test, the task of data transformation
    needs to make sure that the samples have similar characteristics and statistics.
    This is of paramount importance to ensure that the trained model can generalize
    over unseen data, as we will learn in the section on data sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Basic label analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step of analysis is understanding the distribution of labels in different
    sets as well as in the data as a whole. This helps to determine whether, for example,
    there is imbalance in the distribution of the target variable, and if so, whether
    it is consistent across all the samples. Thus, the very first step is usually
    to find out how many examples in the training and test sets belong to each class.
  prefs: []
  type: TYPE_NORMAL
- en: Basic feature analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next step is to calculate the statistics for each feature, such as
  prefs: []
  type: TYPE_NORMAL
- en: Number of unique values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of missing values: May include counts grouped by different missing value
    surrogates (NA, null, ?, and so on).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For categorical: This counts across feature categories, counts across feature
    categories by label category, most frequently occurring category (mode), mode
    by label category, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For numeric: Minimum, maximum, median, standard deviation, variance, and so
    on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature analysis gives basic insights that can be a useful indicator of missing
    values and noise that can affect the learning process or choice of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualization of the data is a broad topic and it is a continuously evolving
    area in the field of machine learning and data mining. We will only cover some
    of the important aspects of visualization that help us analyze the data in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate feature analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal here is to visualize one feature at a time, in relation to the label.
    The techniques used are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Categorical features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Stacked bar graphs are a simple way of showing the distribution of each feature
    category among the labels, when the problem is one of classification.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Histograms and box plots are two basic visualization techniques for continuous
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Histograms have predefined bins whose widths are either fixed intervals or based
    on some calculation used to split the full range of values of the feature. The
    number of instances of data that falls within each bin is then counted and the
    height of the bin is adjusted based on this count. There are variations of histograms
    such as relative or frequency-based histograms, Pareto histograms, two-dimensional
    histograms, and so on; each is a slight variation of the concept and permits a
    different insight into the feature. For those interested in finding out more about
    these variants, the Wikipedia article on histograms is a great resource.
  prefs: []
  type: TYPE_NORMAL
- en: Box plots are a key visualization technique for numeric features as they show
    distributions in terms of percentiles and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate feature analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea of multivariate feature analysis is to visualize more than one feature
    to get insights into relationships between them. Some of the well-known plots
    are explained here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scatter plots**: An important technique for understanding the relationship
    between different features and between features and labels. Typically, two-dimensional
    scatter plots are used in practice where numeric features form the dimensions.
    Alignment of data points on some imaginary axis shows correlation while scattering
    of the data points shows no correlation. It can also be useful to identify clusters
    in lower dimensional space. A bubble chart is a variation of a scatter plot where
    two features form the dimensional axes and the third is proportional to the size
    of the data point, with the plot giving the appearance of a field of "bubbles".
    Density charts help visualize even more features together by introducing data
    point color, background color, and so on, to give additional insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ScatterPlot Matrix**: ScatterPlot Matrix is an extension of scatter plots
    where pair-wise scatter plots for each feature (and label) is visualized. It gives
    a way to compare and perform multivariate analysis of high dimensional data in
    an effective way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel Plots**: In this visualization, each feature is linearly arranged
    on the x-axis and the ranges of values for each feature form the *y* axis. So
    each data element is represented as a line with values for each feature on the
    parallel axis. Class labels, if available, are used to color the lines. Parallel
    plots offer a great understanding of features that are effective in separating
    the data. Deviation charts are variations of parallel plots, where instead of
    showing actual data points, mean and standard deviations are plotted. Andrews
    plots are another variation of parallel plots where data is transformed using
    Fourier series and the function values corresponding to each is projected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformation and preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the broad topic of data transformation. The main
    idea of data transformation is to take the input data and transform it in careful
    ways so as to clean it, extract the most relevant information from it, and to
    turn it into a usable form for further analysis and learning. During these transformations,
    we must only use methods that are designed while keeping in mind not to add any
    bias or artifacts that would affect the integrity of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Feature construction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the case of some datasets, we need to create more features from features
    we are already given. Typically, some form of aggregation is done using common
    aggregators such as average, sum, minimum, or maximum to create additional features.
    In financial fraud detection, for example, Card Fraud datasets usually contain
    transactional behaviors of accounts over various time periods during which the
    accounts were active. Performing behavioral synthesis such as by capturing the
    "Sum of Amounts whenever a Debit transaction occurred, for each Account, over
    One Day" is an example of feature construction that adds a new dimension to the
    dataset, built from existing features. In general, designing new features that
    enhance the predictive power of the data requires domain knowledge and experience
    with data, making it as much an art as a science.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In real-world datasets, often, many features have missing values. In some cases,
    they are missing due to errors in measurement, lapses in recording, or because
    they are not available due to various circumstances; for example, individuals
    may choose not to disclose age or occupation. Why care about missing values at
    all? One extreme and not uncommon way to deal with it is to ignore the records
    that have any missing features, in other words, retain only examples that are
    "whole". This approach may severely reduce the size of the dataset when missing
    features are widespread in the data. As we shall see later, if the system we are
    dealing with is complex, dataset size can afford us precious advantage. Besides,
    there is often predictive value that can be exploited even in the "un-whole" records,
    despite the missing values, as long as we use appropriate measures to deal with
    the problem. On the other hand, one may unwittingly be throwing out key information
    when the omission of the data itself is significant, as in the case of deliberate
    misrepresentation or obfuscation on a loan application by withholding information
    that could be used to conclusively establish bone fides.
  prefs: []
  type: TYPE_NORMAL
- en: Suffice it to say, that an important step in the learning process is to adopt
    some systematic way to handle missing values and understand the consequences of
    the decision in each case. There are some algorithms such as Naïve Bayes that
    are less sensitive to missing values, but in general, it is good practice to handle
    these missing values as a pre-processing step before any form of analysis is done
    on the data. Here are some of the ways to handle missing values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Replacing by means and modes**: When we replace the missing value of a continuous
    value feature with the mean value of that feature, the new mean clearly remains
    the same. But if the mean is heavily influenced by outliers, a better approach
    may be to use the mean after dropping the outliers from the calculation, or use
    the median or mode, instead. Likewise, when a feature is sparsely represented
    in the dataset, the mean value may not be meaningful. In the case of features
    with categorical values, replacing the missing value with the one that occurs
    with the highest frequency in the sample makes for a reasonable choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replacing by imputation**: When we impute a missing value, we are in effect
    constructing a classification or regression model of the feature and making a
    prediction based on the other features in the record in order to classify or estimate
    the missing value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nearest Neighbor imputation**: For missing values of a categorical feature,
    we consider the feature in question to be the target and train a KNN model with
    k taken to be the known number of distinct categories. This model is then used
    to predict the missing values. (A KNN model is non-parametric and assigns a value
    to the "incoming" data instance based on a function of its neighbors—the algorithm
    is described later in this chapter when we talk about non-linear models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression-based imputation**: In the case of continuous value variables,
    we use linear models like Linear Regression to estimate the missing data—the principle
    is the same as for categorical values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User-defined imputation**: In many cases, the most suitable value for imputing
    missing values must come from the problem domain. For instance, a pH value of
    7.0 is neutral, higher is basic, and lower is acidic. It may make most sense to
    impute a neutral value for pH than either mean or median, and this insight is
    an instance of a user-defined imputation. Likewise, in the case of substitution
    with normal body temperature or resting heart rate—all are examples from medicine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Handling outliers requires a lot of care and analysis. Outliers can be noise
    or errors in the data, or they can be anomalous behavior of particular interest.
    The latter case is treated in depth in [Chapter 3](ch03.html "Chapter 3. Unsupervised
    Machine Learning Techniques"), *Unsupervised Machine Learning Techniques*. Here
    we assume the former case, that the domain expert is satisfied that the values
    are indeed outliers in the first sense, that is, noise or erroneously acquired
    or recorded data that needs to be handled appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Following are different techniques in detecting outliers in the data
  prefs: []
  type: TYPE_NORMAL
- en: '**Interquartile Ranges (IQR)**: Interquartile ranges are a measure of variability
    in the data or, equivalently, the statistical dispersion. Each numeric feature
    is sorted based on its value in the dataset and the ordered set is then divided
    into quartiles. The median value is generally used to measure central tendency.
    IQR is measured as the difference between upper and lower quartiles, Q3-Q1\. The
    outliers are generally considered to be data values above Q3 + 1.5 * IQR and below
    Q1 - 1.5 * IQR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distance-based Methods**: The most basic form of distance-based methods uses
    **k-Nearest Neighbors** (**k-NN**) and distance metrics to score the data points.
    The usual parameter is the value *k* in k-NN and a distance metric such as Euclidean
    distance. The data points at the farthest distance are considered outliers. There
    are many variants of these that use local neighborhoods, probabilities, or other
    factors, which will all be covered in [Chapter 3](ch03.html "Chapter 3. Unsupervised
    Machine Learning Techniques"), *Unsupervised Machine Learning Techniques*. Mixed
    datasets, which have both categorical and numeric features, can skew distance-based
    metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Density-based methods**: Density-based methods calculate the proportion of
    data points within a given distance *D*, and if the proportion is less than the
    specified threshold p, it is considered an outlier. The parameter p and D are
    considered user-defined values; the challenge of selecting these values appropriately
    presents one of the main hurdles in using these methods in the preprocessing stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mathematical transformation of feature**: With non-normal data, comparing
    the mean value is highly misleading, as in the case when outliers are present.
    Non-parametric statistics allow us to make meaningful observations about highly
    skewed data. Transformation of such values using the logarithm or square root
    function tends to normalize the data in many cases, or make them more amenable
    to statistical tests. These transformations alter the shape of the distribution
    of the feature drastically—the more extreme an outlier, the greater the effect
    of the log transformation, for example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling outliers using robust statistical algorithms in machine learning
    models**: Many classification algorithms which we discuss in the next section
    on modeling, implicitly or explicitly handle outliers. Bagging and Boosting variants,
    which work as meta-learning frameworks, are generally resilient to outliers or
    noisy data points and may not need a preprocessing step to handle them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normalization**: Many algorithms—distance-based methods are a case in point—are
    very sensitive to the scale of the features. Preprocessing the numeric features
    makes sure that all of them are in a well-behaved range. The most well-known techniques
    of normalization of features are given here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Min-Max Normalization**: In this technique, given the range *[L,U]*, which
    is typically *[0,1]*, each feature with value *x* is normalized in terms of the
    minimum and maximum values, *x*[max] and *x*[min], respectively, using the formula:![Outliers](img/B05137_02_016.jpg)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Z-Score Normalization**: In this technique, also known as standardization,
    the feature values get auto-transformed so that the mean is 0 and standard deviation
    is 1\. The technique to transform is as follows: for each feature *f*, the mean
    value µ(*f*) and standard deviation σ(*f*) are computed and then the feature with
    value *x* is transformed as:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Outliers](img/B05137_02_019.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Discretization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many algorithms can only handle categorical values or nominal values to be
    effective, for example Bayesian Networks. In such cases, it becomes imperative
    to discretize the numeric features into categories using either supervised or
    unsupervised methods. Some of the techniques discussed are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discretization by binning**: This technique is also referred to as equal
    width discretization. The entire scale of data for each feature *f*, ranging from
    values *x*[max] and *x*[min] is divided into a predefined number, *k*, of equal
    intervals, each having the width ![Discretization](img/B05137_02_020.jpg). The
    "cut points" or discretization intervals are:![Discretization](img/B05137_02_021.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discretization by frequency**: This technique is also referred to as equal
    frequency discretization. The feature is sorted and then the entire data is discretized
    into predefined *k* intervals, such that each interval contains the same proportion.
    Both the techniques, discretization by binning and discretization by frequency,
    suffer from loss of information due to the predefined value of *k*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discretization by entropy**: Given the labels, the entropy is calculated
    over the split points where the value changes in an iterative way, so that the
    bins of intervals are as pure or discriminating as possible. Refer to the *Feature
    evaluation techniques* section for entropy-based (information gain) theory and
    calculations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset one receives may often require judicious sampling in order to effectively
    learn from the data. The characteristics of the data as well as the goals of the
    modeling exercise determine whether sampling is needed, and if so, how to go about
    it. Before we begin to learn from this data it is crucially important to create
    train, validate, and test data samples, as explained in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Is sampling needed?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the dataset is large or noisy, or skewed towards one type, the question
    as to whether to sample or not to sample becomes important. The answer depends
    on various aspects such as the dataset itself, the objective and the evaluation
    criteria used for selecting the models, and potentially other practical considerations.
    In some situations, algorithms have scalability issues in memory and space, but
    work effectively on samples, as measured by model performance with respect to
    the regression or classification goals they are expected to achieve. For example,
    SVM scales as *O(n*²*)* and *O(n*³*)* in memory and training times, respectively.
    In other situations, the data is so imbalanced that many algorithms are not robust
    enough to handle the skew. In the literature, the step intended to re-balance
    the distribution of classes in the original data extract by creating new training
    samples is also called **resampling**.
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling and oversampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets exhibiting a marked imbalance in the distribution of classes can be
    said to contain a distinct minority class. Often, this minority class is the set
    of instances that we are especially interested in precisely because its members
    occur in such rare cases. For example, in credit card fraud, less than 0.1% of
    the data belongs to fraud. This skewness is not conducive to learning; after all,
    when we seek to minimize the total error in classification, we give equal weight
    to all classes regardless of whether one class is underrepresented compared to
    another. In binary classification problems, we call the minority class the positive
    class and the majority class as the negative class, a convention that we will
    follow in the following discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling of the majority class is a technique that is commonly used to
    address skewness in data. Taking credit-card fraud as an example, we can create
    different training samples from the original dataset such that each sample has
    all the fraud cases from the original dataset, whereas the non-fraud instances
    are distributed across all the training samples in some fixed ratios. Thus, in
    a given training set created by this method, the majority class is now underrepresented
    compared to the original skewed dataset, effectively balancing out the distribution
    of classes. Training samples with labeled positive and labeled negative instances
    in ratios of, say, 1:20 to 1:50 can be created in this way, but care must be taken
    that the sample of negative instances used should have similar characteristics
    to the data statistics and distributions of the main datasets. The reason for
    using multiple training samples, and in different proportions of positive and
    negative instances, is so that any sampling bias that may be present becomes evident.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we may choose to oversample the minority class. As before, we
    create multiple samples wherein instances from the minority class have been selected
    by either sampling with replacement or without replacement from the original dataset.
    When sampling without replacement, there are no replicated instances across samples.
    With replacement, some instances may be found in more than one sample. After this
    initial seeding of the samples, we can produce more balanced distributions of
    classes by random sampling with replacement from within the minority class in
    each sample until we have the desired ratios of positive to negative instances.
    Oversampling can be prone to over-fitting as classification decision boundaries
    tend to become more specific due to replicated values. **SMOTE** (**Synthetic
    Minority Oversampling Technique**) is a technique that alleviates this problem
    by creating synthetic data points in the interstices of the feature space by interpolating
    between neighboring instances of the positive class (*References* [20]).
  prefs: []
  type: TYPE_NORMAL
- en: Stratified sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Creating samples so that data with similar characteristics is drawn in the same
    proportion as they appear in the population is known as stratified sampling. In
    multi-class classification, if there are *N* classes each in a certain proportion,
    then samples are created such that they represent each class in the same proportion
    as in the original dataset. Generally, it is good practice to create multiple
    samples to train and test the models to validate against biases of sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Training, validation, and test set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Holy Grail of creating good classification models is to train on a set of
    good quality, representative, (training data), tune the parameters and find effective
    models (validation data), and finally, estimate the model's performance by its
    behavior on unseen data (test data).
  prefs: []
  type: TYPE_NORMAL
- en: The central idea behind the logical grouping is to make sure models are validated
    or tested on data that has not been seen during training. Otherwise, a simple
    "rote learner" can outperform the algorithm. The generalization capability of
    the learning algorithm must be evaluated on a dataset which is different from
    the training dataset, but comes from the same population (*References* [11]).
    The balance between removing too much data from training to increase the budget
    of validation and testing can result in models which suffer from "underfitting",
    that is, not having enough examples to build patterns that can help in generalization.
    On the other hand, the extreme choice of allocating all the labeled data for training
    and not performing any validation or testing can lead to "overfitting", that is,
    models that fit the examples too faithfully and do not generalize well enough.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, in most machine learning challenges and real world customer problems,
    one is given a training set and testing set upfront for evaluating the performance
    of the models. In these engagements, the only question is how to validate and
    find the most effective parameters given the training set. In some engagements,
    only the labeled dataset is given and you need to consider the training, validation,
    and testing sets to make sure your models do not overfit or underfit the data.
  prefs: []
  type: TYPE_NORMAL
- en: Three logical processes are needed for modeling and hence three logical datasets
    are needed, namely, training, validation, and testing. The purpose of the training
    dataset is to give labeled data to the learning algorithm to build the models.
    The purpose of the validation set is to see the effect of the parameters of the
    training model being evaluated by training on the validation set. Finally, the
    best parameters or models are retrained on the combination of the training and
    validation sets to find an optimum model that is then tested on the blind test
    set.
  prefs: []
  type: TYPE_NORMAL
- en: '![Training, validation, and test set](img/B05137_02_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Training, Validation, and Test data and how to use them'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two things affect the learning or the generalization capability: the choice
    of the algorithm (and its parameters) and number of training data. This ability
    to generalize can be estimated by various metrics including the prediction errors.
    The overall estimate of unseen error or risk of the model is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training, validation, and test set](img/B05137_02_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *Noise* is the stochastic noise, *Var (G,n)* is called the variance error
    and is a measure of how susceptible our hypothesis or the algorithm *(G)* is,
    if given different datasets. ![Training, validation, and test set](img/B05137_02_023a.jpg)
    is called the bias error and represents how far away the best algorithm in the
    model (average learner over all possible datasets) is from the optimal one.
  prefs: []
  type: TYPE_NORMAL
- en: Learning curves as shown in *Figure 2* and *Figure 3*—where training and testing
    errors are plotted keeping either the algorithm with its parameters constant or
    the training data size constant—give an indication of underfitting or overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: When the training data size is fixed, different algorithms or the same algorithms
    with different parameter choices can exhibit different learning curves. The *Figure
    2* shows two cases of algorithms on the same data size giving two different learning
    curves based on bias and variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Training, validation, and test set](img/B05137_02_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The training data relationship with error rate when the model complexity
    is fixed indicates different choices of models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm or model choice also impacts model performance. A complex algorithm,
    with more parameters to tune, can result in overfitting, while a simple algorithm
    with less parameters might be underfitting. The classic figure to illustrate the
    model performance and complexity when the training data size is fixed is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training, validation, and test set](img/B05137_02_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The Model Complexity relationship with Error rate, over the training
    and the testing data when training data size is fixed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Validation allows for exploring the parameter space to find the model that
    generalizes best. Regularization (will be discussed in linear models) and validation
    are two mechanisms that should be used for preventing overfitting. Sometimes the
    "k-fold cross-validation" process is used for validation, which involves creating
    *k* samples of the data and using *(k – 1)* to train on and the remaining one
    to test, repeated *k* times to give an average estimate. The following figure
    shows 5-fold cross-validation as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training, validation, and test set](img/B05137_02_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: 5-fold cross-validation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some commonly used techniques to perform data sampling, validation,
    and learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random split of training, validation, and testing**: 60, 20, 20\. Train on
    60%, use 20% for validation, and then combine the train and validation datasets
    to train a final model that is used to test on the remaining 20%. Split may be
    done randomly, based on time, based on region, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training, cross-validation, and testing**: Split into Train and Test two
    to one, do validation using cross-validation on the train set, train on whole
    two-thirds and test on one-third. Split may be done randomly, based on time, based
    on region, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training and cross-validation**: When the training set is small and only
    model selection can be done without much parameter tuning. Run cross-validation
    on the whole dataset and chose the best models with learning on the entire dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature relevance analysis and dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of feature relevance and selection is to find the features that are
    discriminating with respect to the target variable and help reduce the dimensions
    of the data [1,2,3]. This improves the model performance mainly by ameliorating
    the effects of the curse of dimensionality and by removing noise due to irrelevant
    features. By carefully evaluating models on the validation set with and without
    features removed, we can see the impact of feature relevance. Since the exhaustive
    search for *k* features involves 2^k – 1 sets (consider all combinations of *^k*
    features where each feature is either retained or removed, disregarding the degenerate
    case where none is present) the corresponding number of models that have to be
    evaluated can become prohibitive, so some form of heuristic search techniques
    are needed. The most common of these techniques are described next.
  prefs: []
  type: TYPE_NORMAL
- en: Feature search techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some of the very common search techniques employed to find feature sets are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward or hill climbing**: In this search, one feature is added at a time
    until the evaluation module outputs no further change in performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backward search**: Starting from the whole set, one feature at a time is
    removed until no performance improvement occurs. Some applications interleave
    both forward and backward techniques to search for features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evolutionary search**: Various evolutionary techniques such as genetic algorithms
    can be used as a search mechanism and the evaluation metrics from either filter-
    or wrapper-based methods can be used as fitness criterion to guide the process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature evaluation techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At a high level, there are three basic methods to evaluate features.
  prefs: []
  type: TYPE_NORMAL
- en: Filter approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This approach refers to the use of techniques without using machine learning
    algorithms for evaluation. The basic idea of the filter approach is to use a search
    technique to select a feature (or subset of features) and measure its importance
    using some statistical measure until a stopping criterion is reached.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate feature selection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This search is as simple as ranking each feature based on the statistical measure
    employed.
  prefs: []
  type: TYPE_NORMAL
- en: Information theoretic approach
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: All information theoretic approaches use the concept of entropy mechanism at
    their core. The idea is that if the feature is randomly present in the dataset,
    there is maximum entropy, or, equivalently, the ability to compress or encode
    is low, and the feature may be irrelevant. On the other hand, if the distribution
    of the feature value is such some range of values are more prevalent in one class
    relative to the others, then the entropy is minimized and the feature is discriminating.
    Casting the problem in terms of entropy in this way requires some form of discretization
    to convert the numeric features into categories in order to compute the probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a binary classification problem with training data *D*[X]. If *X*[i]
    is the *i*^(th) feature with *v* distinct categorical values such that *D*[Xi]
    *= {D*[1]*, D*[2]*… D*[v]*}*, then information or the entropy in feature *X*[i]
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Information theoretic approach](img/B05137_02_041.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *Info(D*[j]*)* is the entropy of the partition and is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Information theoretic approach](img/B05137_02_043.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *p*[+]*(D)* is the probability that the data in set *D* is in the positive
    class and *p_(D)* is the probability that it is in the negative class, in that
    sample. Information gain for the feature is calculated in terms of the overall
    information and information of the feature as
  prefs: []
  type: TYPE_NORMAL
- en: '*InfoGain(X*[i]*) = Info(D) – Info(D*[Xi]*)*'
  prefs: []
  type: TYPE_NORMAL
- en: For numeric features, the values are sorted in ascending order and split points
    between neighboring values are considered as distinct values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The greater the decrease in entropy, the higher the relevance of the feature.
    Information gain has problems when the feature has a large number of values; that
    is when Gain Ratio comes in handy. Gain Ratio corrects the information gain over
    large splits by introducing Split Information. Split Information for feature *X*[i]
    and *GainRatio* is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Information theoretic approach](img/B05137_02_048.jpg)![Information theoretic
    approach](img/B05137_02_049.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are other impurity measures such as Gini Impurity Index (as described
    in the section on the *Decision Tree* algorithm) and Uncertainty-based measures
    to compute feature relevance.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical approach
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Chi-Squared feature selection is one of the most common feature selection methods
    that has statistical hypothesis testing as its base. The null hypothesis is that
    the feature and the class variable are independent of each other. The numeric
    features are discretized so that all features have categorical values. The contingency
    table is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature Values | Class=P | Class=N | Sum over classes *niP* + *niN* |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *X*[1] | (*n*[1P]&#124;*µ*[1P]) | (*n*[1N]&#124;*µ*[1N]) | *n*[1] |'
  prefs: []
  type: TYPE_TB
- en: '| …. | … | …. | … |'
  prefs: []
  type: TYPE_TB
- en: '| *X*[m] | (*n*[mP]&#124;*µ*[mP]) | (*n*[mN]&#124;*µ*[mN]) | *n*[m] |'
  prefs: []
  type: TYPE_TB
- en: '|   | *n*[*P] | *n*[*P] | *n* |'
  prefs: []
  type: TYPE_TB
- en: '*Contingency Table 1: Showing feature values and class distribution for binary
    class.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the preceding table, *n*[ij] is a count of the number of features with value—after
    discretization—equal to *x*[i] and class value of *j*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value summations are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Statistical approach](img/B05137_02_065.jpg)![Statistical approach](img/B05137_02_066.jpg)![Statistical
    approach](img/B05137_02_067.jpg)![Statistical approach](img/B05137_02_068.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here *n* is number of data instances, *j = P, N* is the class value and *i =1,2,
    … m* indexes the different discretized values of the feature and the table has
    *m – 1* degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Chi-Square Statistic is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Statistical approach](img/B05137_02_072.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Chi-Square value is compared to confidence level thresholds for testing
    significance. For example, for *i = 2*, the Chi-Squared value at threshold of
    5% is 3.84; if our value is smaller than the table value of 3.83, then we know
    that the feature is interesting and the null hypothesis is rejected.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate feature selection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Most multivariate methods of feature selection have two goals:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the redundancy between the feature and other selected features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximize the relevance or correlation of the feature with the class label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The task of finding such subsets of features cannot be exhaustive as the process
    can have a large search space. Heuristic search methods such as backward search,
    forward search, hill-climbing, and genetic algorithms are typically used to find
    a subset of features. Two very well-known evaluation techniques for meeting the
    preceding goals are presented next.
  prefs: []
  type: TYPE_NORMAL
- en: Minimal redundancy maximal relevance (mRMR)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this technique, numeric features are often discretized—as done in univariate
    pre-processing—to get distinct categories of values.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each subset *S*, the redundancy between two features *X*[i] and *X*[j]
    can be measured as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Minimal redundancy maximal relevance (mRMR)](img/B05137_02_077.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *MI (X*[i]*, X*[j]*)* = measure of mutual information between two features
    *X*[i] and *X*[j]. Relevance between feature *X*[i] and class *C* can be measured
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Minimal redundancy maximal relevance (mRMR)](img/B05137_02_081.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, the two goals can be combined to find the best feature subset using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Minimal redundancy maximal relevance (mRMR)](img/B05137_02_082.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Correlation-based feature selection (CFS)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The basic idea is similar to the previous example; the overall merit of subset
    *S* is measured as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlation-based feature selection (CFS)](img/B05137_02_083.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *k* is the total number of features, ![Correlation-based feature selection
    (CFS)](img/B05137_02_084.jpg) is the average feature class correlation and ![Correlation-based
    feature selection (CFS)](img/B05137_02_085.jpg) is the average feature-feature
    inter correlation. The numerator gives the relevance factor while the denominator
    gives the redundancy factor and hence the goal of the search is to maximize the
    overall ratio or the *Merit (S)*.
  prefs: []
  type: TYPE_NORMAL
- en: There are other techniques such as Fast-Correlation-based feature selection
    that is based on the same principles, but with variations in computing the metrics.
    Readers can experiment with this and other techniques in Weka.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of the Filter approach is that its methods are independent of
    learning algorithms and hence one is freed from choosing the algorithms and parameters.
    They are also faster than wrapper-based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapper approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The search technique remains the same as discussed in the feature search approach;
    only the evaluation method changes. In the wrapper approach, a machine learning
    algorithm is used to evaluate the subset of features that are found to be discriminating
    based on various metrics. The machine learning algorithm used as the wrapper approach
    may be the same or different from the one used for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Most commonly, cross-validation is used in the learning algorithm. Performance
    metrics such as area under curve or F-score, obtained as an average on cross-validation,
    guide the search process. Since the cost of training and evaluating models is
    very high, we choose algorithms that have fast training speed, such as Linear
    Regression, linear SVM, or ones that are Decision Tree-based.
  prefs: []
  type: TYPE_NORMAL
- en: Some wrapper approaches have been very successful using specific algorithms
    such as Random Forest to measure feature relevance.
  prefs: []
  type: TYPE_NORMAL
- en: Embedded approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This approach does not require feature search techniques. Instead of performing
    feature selection as preprocessing, it is done in the machine learning algorithm
    itself. Rule Induction, Decision Trees, Random Forest, and so on, perform feature
    selection as part of the training algorithm. Some algorithms such as regression
    or SVM-based methods, known as **shrinking methods**, can add a regularization
    term in the model to overcome the impact of noisy features in the dataset. Ridge
    and lasso-based regularization are well-known techniques available in regressions
    to provide feature selection implicitly.
  prefs: []
  type: TYPE_NORMAL
- en: There are other techniques using unsupervised algorithms that will be discussed
    in [Chapter 3](ch03.html "Chapter 3. Unsupervised Machine Learning Techniques"),
    *Unsupervised Machine Learning Techniques*, that can be used effectively in a
    supervised setting too, for example, **Principal Component Analysis** (**PCA**).
  prefs: []
  type: TYPE_NORMAL
- en: Model building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In real-world problems, there are many constraints on learning and many ways
    to assess model performance on unseen data. Each modeling algorithm has its strengths
    and weaknesses when applied to a given problem or to a class of problems in a
    particular domain. This is articulated in the famous **No Free Lunch Theorem**
    (**NFLT**), which says—for the case of supervised learning—that averaged over
    all distributions of data, every classification algorithm performs about as well
    as any other, including one that always picks the same class! Application of NFLT
    to supervised learning and search and optimization can be found at [http://www.no-free-lunch.org/](http://www.no-free-lunch.org/).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss the most commonly used practical algorithms,
    giving the necessary details to answer questions such as what are the algorithm's
    inputs and outputs? How does it work? What are the advantages and limitations
    to consider while choosing the algorithm? For each model, we will include sample
    code and outputs obtained from testing the model on the chosen dataset. This should
    provide the reader with insights into the process. Some algorithms such as neural
    networks and deep learning, Bayesian networks, stream-based earning, and so on,
    will be covered separately in their own chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Linear models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear models work well when the data is linearly separable. This should always
    be the first thing to establish.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear Regression can be used for both classification and estimation problems.
    It is one of the most widely used methods in practice. It consists of finding
    the best-fitting hyperplane through the data points.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm input and output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Features must be numeric. Categorical features are transformed using various
    pre-processing techniques, as when a categorical value becomes a feature with
    1 and 0 values. Linear Regression models output a categorical class in classification
    or numeric values in regression. Many implementations also give confidence values.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The model tries to learn a "hyperplane" in the input space that minimizes the
    error between the data points of each class (*References* [4]).
  prefs: []
  type: TYPE_NORMAL
- en: 'A hyperplane in d-dimensional inputs that linear model learns is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_087.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The two regions (binary classification) the model divides the input space into
    are ![How does it work?](img/B05137_02_088.jpg) and ![How does it work?](img/B05137_02_089.jpg).
    Associating a value of 1 to the coordinate of feature 0, that is, *x*0=1, the
    vector representation of hypothesis space or the model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_091.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The weight matrix can be derived using various methods such as ordinary least
    squares or iterative methods using matrix notation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_092.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here **X** is the input matrix and **y** is the label. If the matrix **X**^T**X**
    in the least squares problem is not of full rank or if encountering various numerical
    stability issues, the solution is modified as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_096.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![How does it work?](img/B05137_02_097.jpg) is added to the diagonal of
    an identity matrix **I**[n] of size (*n* + 1, *n* + 1) with the rest of the values
    being set to 0\. This solution is called **ridge regression** and parameter λ
    theoretically controls the trade-off between the square loss and low norm of the
    solution. The constant λ is also known as regularization constant and helps in
    preventing "overfitting".
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is an appropriate method to try and get insights when there are less than
    100 features and a few thousand data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretable to some level as the weights give insights on the impact of each
    feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumes linear relationship, additive and uncorrelated features, hence it doesn't
    model complex non-linear real-world data. Some implementations of Linear Regression
    allow removing collinear features to overcome this issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very susceptible to outliers in the data, if there are huge outliers, they have
    to be treated prior to performing Linear Regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heteroskedasticity, that is, unequal training point variances, can affect the
    simple least square regression models. Techniques such as weighted least squares
    are employed to overcome this situation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the Bayes rule, the Naïve Bayes classifier assumes the features of
    the data are independent of each other (*References* [9]). It is especially suited
    for large datasets and frequently performs better than other, more elaborate techniques,
    despite its naïve assumption of feature independence.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm input and output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Naïve Bayes model can take features that are both categorical and continuous.
    Generally, the performance of Naïve Bayes models improves if the continuous features
    are discretized in the right format. Naïve Bayes outputs the class and the probability
    score for all class values, making it a good classifier for scoring models.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is a probability-based modeling algorithm. The basic idea is using Bayes'
    rule and measuring the probabilities of different terms, as given here. Measuring
    probabilities can be done either using pre-processing such as discretization,
    assuming a certain distribution, or, given enough data, mapping the distribution
    for numeric features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayes'' rule is applied to get the posterior probability as predictions and
    *k* represents *k*^(th) class.:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_101.jpg)![How does it work?](img/B05137_02_102.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is robust against isolated noisy data points because such points are averaged
    when estimating the probabilities of input data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probabilistic scores as confidence values from Bayes classification can be used
    as scoring models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle missing values very well as they are not used in estimating probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, it is robust against irrelevant attributes. If the features are not useful
    the probability distribution for the classes will be uniform and will cancel itself
    out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very good in training speed and memory, it can be parallelized as each computation
    of probability in the equation is independent of the other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlated features can be a big issue when using Naïve Bayes because the conditional
    independence assumption is no longer valid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal distribution of errors is an assumption in most optimization algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we employ Linear Regression model using, say, the least squares regression
    method, the outputs have to be converted to classes, say 0 and 1\. Many Linear
    Regression algorithms output class and confidence as probability. As a rule of
    thumb, if we see that the probabilities of Linear Regression are mostly beyond
    the ranges of 0.2 to 0.8, then logistic regression algorithm may be a better choice.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm input and output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to Linear Regression, all features must be numeric. Categorical features
    have to be transformed to numeric. Like in Naïve Bayes, this algorithm outputs
    class and probability for each class and can be used as a scoring model.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Logistic regression models the posterior probabilities of classes using linear
    functions in the input features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic regression model for a binary classification is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_104.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The model is a log-odds or logit transformation of linear models (*References*
    [6]). The weight vector is generally computed using various optimization methods
    such as **iterative reweighted least squares** (**IRLS**) or the **Broyden–Fletcher–Goldfarb–Shanno**
    (**BFGS**) method, or variants of these methods.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Overcomes the issue of heteroskedasticity and some non-linearity between inputs
    and outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No need of normal distribution assumptions in the error estimates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is interpretable, but less so than Linear Regression models as some understanding
    of statistics is required. It gives information such as odds ratio, *p* values,
    and so on, which are useful in understanding the effects of features on the classes
    as well as doing implicit feature relevance based on significance of *p* values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 or L2 regularization has to be employed in practice to overcome overfitting
    in the logistic regression models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many optimization algorithms are available for improving speed of training and
    robustness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-linear models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we will discuss some of the well-known, practical, and most commonly used
    non-linear models.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decision Trees are also known as **Classification and Regression Trees** (**CART**)
    (*References* [5]). Their representation is a binary tree constructed by evaluating
    an inequality in terms of a single attribute at each internal node, with each
    leaf-node corresponding to the output value or class resulting from the decisions
    in the path leading to it. When a new input is provided, the output is predicted
    by traversing the tree starting at the root.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Features can be both categorical and numeric. It generates class as an output
    and most implementations give a score or probability using frequency-based estimation.
    Decision Trees probabilities are not smooth functions like Naïve Bayes and Logistic
    Regression, though there are extensions that are.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generally, a single tree is created, starting with single features at the root
    with decisions split into branches based on the values of the features while at
    the leaf there is either a class or more features. There are many choices to be
    made, such as how many trees, how to choose features at the root level or at subsequent
    leaf level, and how to split the feature values when not categorical. This has
    resulted in many different algorithms or modifications to the basic Decision Tree.
    Many techniques to split the feature values are similar to what was discussed
    in the section on discretization. Generally, some form of pruning is applied to
    reduce the size of the tree, which helps in addressing overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Gini index is another popular technique used to split the features. Gini index
    of data in set *S* of all the data points is ![How does it work?](img/B05137_02_106.jpg)
    where *p*[1], *p*[2] … *p*[k] are probability distribution for each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *p* is the fraction or probability of data in set *S* of all the data points
    belonging to say class positive, then 1 – *p* is the fraction for the other class
    or the error rate in binary classification. If the dataset *S* is split in *r*
    ways *S*[1]*, S*[2]*, …S*[r] then the error rate of each set can be quantified
    as |*S*[i]|. Gini index for an *r* way split is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_113.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The split with the lowest Gini index is used for selection. The CART algorithm,
    a popular Decision Tree algorithm, uses Gini index for split criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entropy of the set of data points *S* can similarly be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_114.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, entropy-based split is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The lower the value of the entropy split, the better the feature, and this is
    used in ID3 and C4.5 Decision Tree algorithms (*References* [12]).
  prefs: []
  type: TYPE_NORMAL
- en: The stopping criteria and pruning criteria are related. The idea behind stopping
    the growth of the tree early or pruning is to reduce the "overfitting" and it
    works similar to regularization in linear and logistic models. Normally, the training
    set is divided into tree growing sets and pruning sets so that pruning uses different
    data to overcome any biases from the growing set. **Minimum Description Length**
    (**MDL**), which penalizes the complexity of the tree based on number of nodes
    is a popular methodology used in many Decision Tree algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_116.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Shows a two-dimensional binary classification problem and a Decision
    Tree induced using splits at thresholds *X*[1t] and *X*[1t], respectively'
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main advantages of Decision Trees are they are quite easily interpretable.
    They can be understood in layman's terms and are especially suited for business
    domain experts to easily understand the exact model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are a large number of features, then building Decision Tree can take
    lots of training time as the complexity of the algorithm increases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Trees have an inherent problem with overfitting. Many tree algorithms
    have pruning options to reduce the effect. Using pruning and validation techniques
    can alleviate the problem to a large extent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Trees work well when there is correlation between the features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Trees build axis-parallel boundaries across classes, the bias of which
    can introduce errors, especially in a complex, smooth, non-linear boundary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Nearest Neighbors (KNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: K-Nearest Neighbors falls under the branch of non-parametric and lazy algorithms.
    K-Nearest neighbors doesn't make any assumptions on the underlying data and doesn't
    build and generalize models from training data (*References* [10]).
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Though KNN's can work with categorical and numeric features, the distance computation,
    which is the core of finding the neighbors, works better with numeric features.
    Normalization of numeric features to be in the same ranges is one of the mandatory
    steps required. KNN's outputs are generally the classes based on the neighbors'
    distance calculation.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'KNN uses the entire training data to make predictions on unseen test data.
    When unseen test data is presented KNN finds the K "nearest neighbors" using some
    distance computation and based on the neighbors and the metric of deciding the
    category it classifies the new point. If we consider two vectors represented by
    **x**[1] and **x**[2] corresponding to two data points the distance is calculated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean Distance:![How does it work?](img/B05137_02_121.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine Distance or similarity:![How does it work?](img/B05137_02_122.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metric used to classify an unseen may simply be the majority class among
    the *K* neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: The training time is small as all it has to do is build data structures to hold
    the data in such a way that the computation of the nearest neighbor is minimized
    when unseen data is presented. The algorithm relies on choices of how the data
    is stored from training data points for efficiency of searching the neighbors,
    which distance computation is used to find the nearest neighbor, and which metrics
    are used to categorize based on classes of all neighbors. Choosing the value of
    "*K*" in KNN by using validation techniques is critical.
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_123.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: K-Nearest Neighbor illustrated using two-dimensional data with different
    choices of k.'
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: No assumption on underlying data distribution and minimal training time makes
    it a very attractive method for learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN uses local information for computing the distances and in certain domains
    can yield highly adaptive behaviors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is robust to noisy training data when *K* is effectively chosen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holding the entire training data for classification can be problematic depending
    on the number of data points and hardware constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of features and the curse of dimensionality affects this algorithm more
    hence some form of dimensionality reduction or feature selection has to be done
    prior to modeling in KNN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines (SVM)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SVMs, in simple terms, can be viewed as linear classifiers that maximize the
    margin between the separating hyperplane and the data by solving a constrained
    optimization problem. SVMs can even deal with data that is not linearly separable
    by invoking transformation to a higher dimensional space using kernels described
    later.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SVM is effective with numeric features only, though most implementations can
    handle categorical features with transformation to numeric or binary. Normalization
    is often a choice as it helps the optimization part of the training. Outputs of
    SVM are class predictions. There are implementations that give probability estimates
    as confidences, but this requires considerable training time as they use k-fold
    cross-validation to build the estimates.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In its linear form, SVM works similar to Linear Regression classifier, where
    a linear decision boundary is drawn between the two classes. The difference between
    the two is that with SVM, the boundary is drawn in such a way that the "margin"
    or the distance between the points near the boundary is maximized. The points
    on the boundaries are known as "support vectors" (*References* [13 and 8]).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, SVM tries to find the weight vector in linear models similar to Linear
    Regression model as given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_124.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The weight *w*[0] is represented by *b* here. SVM for a binary class y ∈{1,-1}
    tries to find a hyperplane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_127.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The hyperplane tries to separate the data points such that all points with
    the class lie on the side of the hyperplane as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_128.jpg)![How does it work?](img/B05137_02_129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The models are subjected to maximize the margin using constraint-based optimization
    with a penalty function denoted by *C* for overcoming the errors denoted by ![How
    does it work?](img/B05137_02_131.jpg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_132.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Such that ![How does it work?](img/B05137_02_133.jpg) and ![How does it work?](img/B05137_02_134.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: They are also known as large margin classifiers for the preceding reason. The
    kernel-based SVM transforms the input data into a hypothetical feature space where
    SV machinery works in a linear way and the boundaries are drawn in the feature
    spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'A kernel function on the transformed representation is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_135.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here Φ is a transformation on the input space. It can be seen that the entire
    optimization and solution of SVM remains the same with the only exception that
    the dot-product **x**[i] · **x**[j] is replaced by the kernel function *k*(**x**[i],
    **x**[j]), which is a function involving the two vectors in a different space
    without actually transforming to that space. This is known as the **kernel trick**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most well-known kernels that are normally used are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gaussian Radial Basis Kernel**:![How does it work?](img/B05137_02_139.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial Kernel**:![How does it work?](img/B05137_02_140.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sigmoid Kernel**:![How does it work?](img/B05137_02_141.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM's performance is very sensitive to some of the parameters of optimization
    and the kernel parameters and the core SV parameter such as the cost function
    *C*. Search techniques such as grid search or evolutionary search combined with
    validation techniques such as cross-validation are generally used to find the
    best parameter values.
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_142.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: SVM Linear Hyperplane learned from training data that creates a maximum
    margin separation between two classes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_144.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Kernel transformation illustrating how two-dimensional input space
    can be transformed using a polynomial transformation into a three-dimensional
    feature space where data is linearly separable.'
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SVMs are among the best in generalization, low overfitting, and have a good
    theoretical foundation for complex non-linear data if the parameters are chosen
    judiciously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVMs work well even with a large number of features and less training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVMs are less sensitive to noisy training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The biggest disadvantage of SVMs is that they are not interpretable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another big issue with SVM is its training time and memory requirements. They
    are *O(n*²*)* and *O(n*³*)* and can result in major scalability issues when the
    data is large or there are hardware constraints. There are some modifications
    that help in reducing both.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM generally works well for binary classification problems, but for multiclass
    classification problems, though there are techniques such as one versus many and
    one versus all, it is not as robust as some other classifiers such as Decision
    Trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble learning and meta learners
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Combining multiple algorithms or models to classify instead of relying on just
    one is known as ensemble learning. It helps to combine various models as each
    model can be considered—at a high level—as an expert in detecting specific patterns
    in the whole dataset. Each base learner can be made to learn on slightly different
    datasets too. Finally, the results from all models are combined to perform prediction.
    Based on how similar the algorithms used in combination are, how the training
    dataset is presented to each algorithm, and how the algorithms combine the results
    to finally classify the unseen dataset, there are many branches of ensemble learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ensemble learning and meta learners](img/B05137_02_145.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Illustration of ensemble learning strategies'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common types of ensemble learning are:'
  prefs: []
  type: TYPE_NORMAL
- en: Different learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Same learning algorithms, but with different parameter choices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different learning algorithms on different feature sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different learning algorithms with different training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bootstrap aggregating or bagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is one of the most commonly used ensemble methods for dividing the data in
    different samples and building classifiers on each sample.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The input is constrained by the choice of the base learner used—if using Decision
    Trees there are basically no restrictions. The method outputs class membership
    along with the probability distribution for classes.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The core idea of bagging is to apply the bootstrapping estimation to different
    learners that have high variance, such as Decision Trees. Bootstrapping is any
    statistical measure that depends on random sampling with replacement. The entire
    data is split into different samples using bootstrapping and for each sample,
    a model is built using the base learner. Finally, while predicting, the average
    prediction is arrived at using a majority vote—this is one technique to combine
    over all the learners.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Random Forest is an improvement over basic bagged Decision Trees. Even with
    bagging, the basic Decision Tree has a choice of all the features at every split
    point in creating a tree. Because of this, even with different samples, many trees
    can form highly correlated submodels, which causes the performance of bagging
    to deteriorate. By giving random features to different models in addition to a
    random dataset, the correlation between the submodels reduces and Random Forest
    shows much better performance compared to basic bagged trees. Each tree in Random
    Forest grows its structure on the random features, thereby minimizing the bias;
    combining many such trees on decision reduces the variance (*References* [15]).
    Random Forest is also used to measure feature relevance by averaging the impurity
    decrease in the trees and ranking them across all the features to give the relative
    importance of each.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Better generalization than the single base learner. Overcomes the issue of overfitting
    of base learners.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretability of bagging is very low as it works as meta learner combining
    even the interpretable learners.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like most other ensemble learners, Bagging is resilient to noise and outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest generally does not tend to overfit given the training data is
    iid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Boosting is another popular form of ensemble learning, which is based on using
    a weak learner and iteratively learning the points that are "misclassified" or
    difficult to learn. Thus, the idea is to "boost" the difficult to learn instances
    and making the base learners learn the decision boundaries more effectively. There
    are various flavors of boosting such as AdaBoost, LogitBoost, ConfidenceBoost,
    Gradient Boosting, and so on. We present a very basic form of AdaBoost here (*References*
    [14]).
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The input is constrained by the choice of the base learner used—if using Decision
    Trees there are basically no restrictions. Outputs class membership along with
    probability distribution for classes.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The basic idea behind boosting is iterative reweighting of input samples to
    create new distribution of the data for learning a model from a simple base learner
    in every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, all the instances are uniformly weighted with weights ![How does
    it work?](img/B05137_02_146.jpg) and at every iteration *t*, the population is
    resampled or reweighted as ![How does it work?](img/B05137_02_148.jpg) where ![How
    does it work?](img/B05137_02_149.jpg) and *Z*t is the normalization constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final model works as a linear combination of all the models learned in
    the iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_02_151.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The reweighting or resampling of the data in each iteration is based on "errors";
    the data points that result in errors are sampled more or have larger weights.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Better generalization than the base learner and overcomes the issue of overfitting
    very effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some boosting algorithms such as AdaBoost can be susceptible to uniform noise.
    There are variants of boosting such as "GentleBoost" and "BrownBoost" that decrease
    the effect of outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting has a theoretical bounds and guarantee on the error estimation making
    it a statistically robust algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model assessment, evaluation, and comparisons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key ideas discussed here are:'
  prefs: []
  type: TYPE_NORMAL
- en: How to assess or estimate the performance of the classifier on unseen datasets
    that it will be predicting on future unseen datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the metrics that we should use to assess the performance of the model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we compare algorithms if we have to choose between them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model assessment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to train the model(s), tune the model parameters, select the models,
    and finally estimate the predictive behavior of models on unseen data, we need
    many datasets. We cannot train the model on one set of data and estimate its behavior
    on the same set of data, as it will have a clear optimistic bias and estimations
    will be unlikely to match the behavior in the unseen data. So at a minimum, there
    is a need to partition data available into training sets and testing sets. Also,
    we need to tune the parameters of the model and test the effect of the tuning
    on a separate dataset before we perform testing on the test set. The same argument
    of optimistic bias and wrong estimation applies if we use the same dataset for
    training, parameter tuning, and testing. Thus there is a theoretical and practical
    need to have three datasets, that is, training, validation, and testing.
  prefs: []
  type: TYPE_NORMAL
- en: The models are trained on the training set, the effect of different parameters
    on the training set are validated on the validation set, and the finalized model
    with the selected parameters is run on the test set to gauge the performance of
    the model on future unseen data. When the dataset is not large enough, or is large
    but the imbalance between classes is wide, that is, one class is present only
    in a small fraction of the total population, we cannot create too many samples.
    Recall that one of the steps described in our methodology is to create different
    data samples and datasets. If the total training data is large and has a good
    proportion of data and class ratios, then creating these three sets using random
    stratified partitioning is the most common option employed. In certain datasets
    that show seasonality and time-dependent behaviors, creating datasets based on
    time bounds is a common practice. In many cases, when the dataset is not large
    enough, only two physical partitions, that is, training and testing may be created.
    The training dataset ranges roughly from 66% to 80% while the rest is used for
    testing. The validation set is then created from the training dataset using the
    k-fold cross-validation technique. The training dataset is split *k* times, each
    time producing *k-1/k* random training *1/k* testing data samples, and the average
    metrics of performance needed is generated. This way the limited training data
    is partitioned *k* times and average performance across different split of training/testing
    is used for gauging the effect of the parameters. Using 10-fold cross-validation
    is the most common practice employed in cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next important decision when tuning parameters or selecting models is to
    base your decision on certain performance metrics. In classification learning,
    there are different metrics available on which you can base your decision, depending
    on the business requirement. For example, in certain domains, not missing a single
    true positive is the most important concern, while in other domains where humans
    are involved in adjudicating results of models, having too many false positives
    is the greater concern. In certain cases, having overall good accuracy is considered
    more vital. In highly imbalanced datasets such as fraud or cyber attacks, there
    are just a handful of instances of one class and millions of the other classes.
    In such cases accuracy gives a wrong indication of model performance and some
    other metrics such as precision, true positive ratio, or area under the curve
    are used as metrics.
  prefs: []
  type: TYPE_NORMAL
- en: We will now discuss the most commonly employed metrics in classification algorithms
    evaluation (*References* [16, 17, and 19]).
  prefs: []
  type: TYPE_NORMAL
- en: '![Model evaluation metrics](img/B05137_02_152.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Model evaluation metrics for classification models'
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix and related metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Confusion matrix and related metrics](img/B05137_02_153.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Confusion Matrix'
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix is central to the definition of a number of model performance
    metrics. The proliferation of metrics and synonymous terms is a result of the
    utility of different quantities derived from the elements of the matrix in various
    disciplines, each emphasizing a different aspect of the model's behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The four elements of the matrix are raw counts of the number of False Positives,
    False Negatives, True Positives, and True Negatives. Often more interesting are
    the different ratios of these quantities, the True Positive Rate (or Sensitivity,
    or Recall), and the False Positive Rate (FPR, or 1—Specificity, or Fallout). Accuracy
    reflects the percentage of correct predictions, whether Class 1 or Class 0\. For
    skewed datasets, accuracy is not particularly useful, as even a constant prediction
    can appear to perform well.
  prefs: []
  type: TYPE_NORMAL
- en: ROC and PRC curves
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previously mentioned metrics such as accuracy, precision, recall, sensitivity,
    and specificity are aggregates, that is, they describe the behavior of the entire
    dataset. In many complex problems it is often valuable to see the trade-off between
    metrics such as TPs and say FPs.
  prefs: []
  type: TYPE_NORMAL
- en: Many classifiers, mostly probability-based classifiers, give confidence or probability
    of the prediction, in addition to giving classification. The process to obtain
    the ROC or PRC curves is to run the unseen validation or test set on the learned
    models, and then obtain the prediction and the probability of prediction. Sort
    the predictions based on the confidences in decreasing order. For every probability
    or confidence calculate two metrics, the fraction of FP (FP rate) and the fraction
    of TP (TP rate).
  prefs: []
  type: TYPE_NORMAL
- en: 'Plotting the TP rate on the *y* axis and FP rate on the *x* axis gives the
    ROC curves. ROC curves of random classifiers lie close to the diagonal while the
    ROC curves of good classifiers tend towards the upper left of the plot. The **area
    under the curve** (**AUC**) is the area measured under the ROC curve by using
    the trapezoidal area from 0 to 1 of ROC curves. While running cross-validation
    for instance there can be many ROC curves. There are two ways to get "average"
    ROC curves: first, using vertical averaging, that is, TPR average is plotted at
    different FP rate or second, using horizontal averaging, that is, FPR average
    is plotted at different TP rate. The classifiers that have area under curves greater
    than 0.8, as a rule-of-thumb are considered good for prediction for unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: Precision Recall curves or PRC curves are similar to ROC curves, but instead
    of TPR versus FPR, metrics Precision and Recall are plotted on the *y* and *x*
    axis, respectively. When the data is highly imbalanced, that is, ROC curves don't
    really show the impact while PRC curves are more reliable in judging performance.
  prefs: []
  type: TYPE_NORMAL
- en: Gain charts and lift curves
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lift and Gain charts are more biased towards sensitivity or true positives.
    The whole purpose of these two charts is to show how instead of random selection,
    the models prediction and confidence can detect better quality or true positives
    in the sample of unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: This is usually very appealing for detection engines that are used in detecting
    fraud in financial crime or threats in cyber security. The gain charts and lift
    curves give exact estimates of real true positives that will be detected at different
    quartiles or intervals of total data. This will give insight to the business decision
    makers on how many investigators would be needed or how many hours would be spent
    towards detecting fraudulent actions or cyber attacks and thus can give real ROI
    of the models.
  prefs: []
  type: TYPE_NORMAL
- en: The process for generating gain charts or lift curves has a similar process
    of running unseen validation or test data through the models and getting the predictions
    along with the confidences or probabilities. It involves ranking the probabilities
    in decreasing order and keeping count of TPs per quartile of the dataset. Finally,
    the histogram of counts per quartile give the lift curve, while the cumulative
    count of TPs added over quartile gives the gains chart. In many tools such as
    RapidMiner, instead of coarse intervals such as quartiles, fixed larger intervals
    using binning is employed for obtaining the counts and cumulative counts.
  prefs: []
  type: TYPE_NORMAL
- en: Model comparisons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to choosing between algorithms, or the right parameters for a
    given algorithm, we make the comparison either on different datasets, or, as in
    the case of cross-validation, on different splits of the same dataset. Measures
    of statistical testing are employed in decisions involved in these comparisons.
    The basic idea of using hypothesis testing from classical statistics is to compare
    the two metrics from the algorithms. The null hypothesis is that there is no difference
    between the algorithms based on the measured metrics and so the test is done to
    validate or reject the null hypothesis based on the measured metrics (*References*
    [16]). The main question answered by statistical tests is- are the results or
    metrics obtained by the algorithm its real characteristics, or is it by chance?
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss the most common methods for comparing classification
    algorithms used in practical scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing two algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The general process is to train the algorithms on the same training set and
    run the models on either multiple validation sets, different test sets, or cross-validation,
    gauge the metrics of interest discussed previously, such as error rate or area
    under curve, and then get the statistics of the metrics for each of the algorithms
    to decide which worked better. Each method has its advantages and disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: McNemar's Test
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is a non-parametric test and thus it makes no assumptions on data and
    distribution. McNemar''s test builds a contingency table of a performance metric
    such as "misclassification or errors" with:'
  prefs: []
  type: TYPE_NORMAL
- en: Count of misclassification by both algorithms (*c*[00])
  prefs: []
  type: TYPE_NORMAL
- en: Count of misclassification by algorithm *G1*, but correctly classified by algorithm
    *G2*(*c*[01])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Count of misclassification by algorithm *G2*, but correctly classified by algorithm
    *G1* (*c*[10])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Count of correctly classified by both *G1* and *G2*(*c*[11])![McNemar's Test](img/B05137_02_162.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If χ² exceeds ![McNemar's Test](img/B05137_02_164.jpg) statistic then we can
    reject the null hypothesis that the two performance metrics on algorithms *G1*
    and *G2* were equal under the confidence value of 1 – α.
  prefs: []
  type: TYPE_NORMAL
- en: Paired-t test
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is a parametric test and an assumption of normally distributed computed
    metrics becomes valid. Normally it is coupled with cross-validation processes
    and results of metrics such as area under curve or precision or error rate is
    computed for each and then the mean and standard deviations are measured. Apart
    from normal distribution assumption, the additional assumption that two metrics
    come from a population of equal variance can be a big disadvantage for this method.
  prefs: []
  type: TYPE_NORMAL
- en: '![Paired-t test](img/B05137_02_166.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![Paired-t test](img/B05137_02_167.jpg) is difference of means in performance
    metrics of two algorithms *G1* and *G2*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Paired-t test](img/B05137_02_168.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *d*i is the difference between the performance metrics of two algorithms
    *G1* and *G2* in the trial and there are *n* trials.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *t*-statistic is computed using the mean differences and the standard errors
    from the standard deviation as follows and is compared to the table for the right
    alpha value to check for significance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Paired-t test](img/B05137_02_170.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Wilcoxon signed-rank test
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The most popular non-parametric method of testing two metrics over datasets
    is to use the Wilcoxon signed-rank test. The algorithms are trained on the same
    training data and metrics such as error rate or area under accuracy are calculated
    over different validation or test sets. Let *d*[i] be the difference between the
    performance metrics of two classifiers in the *i*^(th) trial for *N* datasets.
    Differences are then ranked according to their absolute values, and mean ranks
    associated for ties. Let *R*^+ be the sum of ranks where the second algorithm
    outperformed the first and R^– be the sum of ranks where the first outperformed
    the second:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wilcoxon signed-rank test](img/B05137_02_174.jpg)![Wilcoxon signed-rank test](img/B05137_02_175.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The statistic ![Wilcoxon signed-rank test](img/B05137_02_176.jpg) is then compared
    to threshold value at an alpha, ![Wilcoxon signed-rank test](img/B05137_02_177.jpg)
    to reject the hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing multiple algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will now discuss the two most common techniques used when there are more
    than two algorithms involved and we need to perform comparison across many algorithms
    for evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: ANOVA test
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These are parametric tests that assume normal distribution of the samples, that
    is, metrics we are calculating for evaluations. ANOVA test follows the same process
    as others, that is, train the models/algorithms on similar training sets and run
    it on different validation or test sets. The main quantities computed in ANOVA
    are the metric means for each algorithm performance and then compute the overall
    metric means across all algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let *p*[ij] be the performance metric for *i = 1,2… k* and *j = 1,2 …l* for
    *k* trials and *l* classifiers. The mean performance of classifier *j* on all
    trials and overall mean performance is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ANOVA test](img/B05137_02_182.jpg)![ANOVA test](img/B05137_02_183.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Two types of variation are evaluated. The first is within-group variation,
    that is, total deviation of each algorithm from the overall metric mean, and the
    second is between-group variation, that is, deviation of each algorithm metric
    mean. Within-group variation and between-group variation are used to compute the
    respective within- and between- sum of squares as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ANOVA test](img/B05137_02_184.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the two sum of squares and a computation such as F-statistic, which is
    the ratio of the two, the significance test can be done at alpha values to accept
    or reject the null hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ANOVA test](img/B05137_02_185.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ANOVA tests have the same limitations as paired-t tests on the lines of assumptions
    of normal distribution of metrics and assuming the variances being equal.
  prefs: []
  type: TYPE_NORMAL
- en: Friedman's test
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Friedman''s test is a non-parametric test for multiple algorithm comparisons
    and it has no assumption on the data distribution or variances of metrics that
    ANOVA does. It uses ranks instead of the performance metrics directly for its
    computation. On each dataset or trials, the algorithms are sorted and the best
    one is ranked 1 and so on for all classifiers. The average rank of an algorithm
    over *n* datasets is computed, say *R*[j]. The Friedman''s statistic over *l*
    classifiers is computed as follows and compared to alpha values to accept or reject
    the null hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Friedman''s test](img/B05137_02_187.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Case Study – Horse Colic Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate the different steps and methodologies described in [Chapter 1](ch01.html
    "Chapter 1. Machine Learning Review"), *Machine Learning Review*, from data analysis
    to model evaluation, a representative dataset that has real-world characteristics
    is essential.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have chosen "Horse Colic Dataset" from the UCI Repository available at the
    following link: [https://archive.ics.uci.edu/ml/datasets/Horse+Colic](https://archive.ics.uci.edu/ml/datasets/Horse+Colic)'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has 23 features and has a good mix of categorical and continuous
    features. It has a large number of features and instances with missing values,
    hence understanding how to replace these missing values and using it in modeling
    is made more practical in this treatment. The large number of missing data (30%)
    is in fact a notable feature of this dataset. The data consists of attributes
    that are continuous, as well as nominal in type. Also, the presence of self-predictors
    makes working with this dataset instructive from a practical standpoint.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the exercise is to apply the techniques of supervised learning that
    we have assimilated so far. We will do this using a real dataset and by working
    with two open source toolkits—WEKA and RapidMiner. With the help of these tools,
    we will construct the pipeline that will allow us to start with the ingestion
    of the data file through data cleansing, the learning process, and model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Weka is a Java framework for machine learning—we will see how to use this framework
    to solve a classification problem from beginning to end in a few lines of code.
    In addition to a Java API, Weka also has a GUI.
  prefs: []
  type: TYPE_NORMAL
- en: RapidMiner is a graphical environment with drag and drop capability and a large
    suite of algorithms and visualization tools that makes it extremely easy to quickly
    run experiments with data and different modeling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Business problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The business problem is to determine given values for the well-known variables
    of the dataset—if the lesion of the horse was surgical. We will use the test set
    as the unseen data that must be classified.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the data and labels, this is a binary classification problem. The data
    is already split into training and testing data. This makes the evaluation technique
    simpler as all methodologies from feature selection to models can be evaluated
    on the same test data.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset contains 300 training and 68 test examples. There are 28 attributes
    and the target corresponds to whether or not a lesion is surgical.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After looking at the distribution of the label categories over the training
    and test samples, we combine the 300 training samples and the 68 test samples
    prior to feature analyzes.
  prefs: []
  type: TYPE_NORMAL
- en: Label analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The ratio of the No Class to Yes Class is 109/191 = 0.57 in the Training set
    and 0.66 in the Test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training dataset |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Surgical Lesion? | 1 (Yes) | 2 (No) |'
  prefs: []
  type: TYPE_TB
- en: '| Number of examples | 191 | 109 |'
  prefs: []
  type: TYPE_TB
- en: '| Testing dataset |'
  prefs: []
  type: TYPE_TB
- en: '| Surgical Lesion? | 1 (Yes) | 2 (No) |'
  prefs: []
  type: TYPE_TB
- en: '| Number of examples | 41 | 27 |'
  prefs: []
  type: TYPE_TB
- en: '*Table 2: Label analysis*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Features analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following is a screenshot of top features with characteristics of types,
    missing values, basic statistics of minimum, maximum, modes, and standard deviations
    sorted by missing values. Observations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: There are no categorical or continuous features with non-missing values; the
    least is the feature "pulse" with 74 out of 368 missing, that is, 20% values missing,
    which is higher than general noise threshold!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most numeric features have missing values, for example, "nasogastric reflux
    PH" has 247 out of 368 values missing, that is, 67% values are missing!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many categorical features have missing values, for example, "abidominocentesis
    appearance" have 165 out of 368 missing, that is, 45% values are missing!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing values have to be handled in some way to overcome the noise created
    by such large numbers!![Features analysis](img/B05137_02_188.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 12: Basic statistics of features from datasets.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Supervised learning experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will cover supervised learning experiments using two different
    tools—highlighting coding and analysis in one tool and the GUI framework in the
    other. This gives the developers the opportunity to explore whichever route they
    are most comfortable with.
  prefs: []
  type: TYPE_NORMAL
- en: Weka experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we have given the entire code and will walk through the process
    from loading data, transforming the data, selecting features, building sample
    models, evaluating them on test data, and even comparing the algorithms for statistical
    significance.
  prefs: []
  type: TYPE_NORMAL
- en: Sample end-to-end process in Java
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In each algorithm, the same training/testing data is used and evaluation is
    performed for all the metrics as follows. The training and testing file is loaded
    in memory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The generic code, using WEKA, is shown here, where each classifier is wrapped
    by a filtered classifier for replacing missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When the classifier needs to perform Feature Selection, in Weka, `AttributeSelectedClassifier`
    further wraps the `FilteredClassifier` as shown in the following listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The sample output of evaluation is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Weka experimenter and model selection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As explained in the *Model evaluation metrics* section, to select models, we
    need to validate which one will work well on unseen datasets. Cross-validation
    must be done on the training set and the performance metric of choice needs to
    be analyzed using standard statistical testing metrics. Here we show an example
    using the same training data, 10-fold cross validation, performing 30 experiments
    on two models, and comparison of results using paired-t tests.
  prefs: []
  type: TYPE_NORMAL
- en: One uses Naïve Bayes with preprocessing that includes replacing missing values
    and performing feature selection by removing any features with a score below 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: Another uses the same preprocessing and AdaBoostM1 with Naïve Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Weka experimenter and model selection](img/B05137_02_189.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: WEKA experimenter showing the process of using cross-validation
    runs with 30 repetitions with two algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Weka experimenter and model selection](img/B05137_02_190.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: WEKA Experimenter results showing two algorithms compared on metric
    of percent correct or accuracy using paired-t test.'
  prefs: []
  type: TYPE_NORMAL
- en: RapidMiner experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's now run some experiments using the Horse-colic dataset in RapidMiner.
    We will again follow the methodology presented in the first part of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section is not intended as a tutorial on the RapidMiner tool. The experimenter
    is expected to read the excellent documentation and user guide to familiarize
    themselves with the use of the tool. There is a tutorial dedicated to every operator
    in the software—we recommend you make use of these tutorials whenever you want
    to learn how a particular operator is to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have imported the test and training data files using the data access
    tools, we will want to visually explore the dataset to familiarize ourselves with
    the lay of the land. Of particular importance is to recognize whether each of
    the 28 attributes are continuous (numeric, integer, or real in RapidMiner) or
    categorical (nominal, binominal, or polynominal in RapidMiner).
  prefs: []
  type: TYPE_NORMAL
- en: Visualization analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From the **Results** panel of the tool, we perform univariate, bivariate, and
    multivariate analyses of the data. The Statistics tool gives a short summary for
    each feature—min, max, mean, and standard deviation for continuous types and least,
    most, and frequency by category for nominal types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interesting characteristics of the data begin to show themselves as we get
    into bivariate analysis. In the Quartile Color Matrix, the color represents the
    two possible target values. As seen in the box plots, we immediately notice some
    attributes discriminate between the two target values more clearly than others.
    Let''s examine a few:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization analysis](img/B05137_02_191.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Quartile Color Matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'Peristalsis: This feature shows a marked difference in distribution when separated
    by target value. There is almost no overlap in the inter-quartile regions between
    the two. This points to the discriminating power of this feature with respect
    to the target.'
  prefs: []
  type: TYPE_NORMAL
- en: The plot for Rectal Temperature, on the other hand, shows no perceptible difference
    in the distributions. This suggests that this feature has low correlation with
    the target. A similar inference may be drawn from the feature Pulse. We expect
    these features to rank fairly low when we evaluate the features for their discriminating
    power with respect to the target.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the plot for Pain has a very different characteristic. It is also discriminating
    of the target, but in a very different way than Peristalsis. In the case of Pain,
    the variance in data for Class 2 is much larger than Class 1\. Abdominal Distension
    also has markedly dissimilar variance across the classes, except with the larger
    variance in Class 2 compared to Class 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization analysis](img/B05137_02_192.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Scatter plot matrix'
  prefs: []
  type: TYPE_NORMAL
- en: An important part of exploring the data is understanding how different attributes
    correlate with each other and with the target. Here we consider pairs of features
    and see if the occurrence of values *in combination* tells us something about
    the target. In these plots, the color of the data points is the target.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization analysis](img/B05137_02_193.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Bubble chart'
  prefs: []
  type: TYPE_NORMAL
- en: In the bubble chart we can visualize four features at once by using the graphing
    tools to specify the *x* and *y* axes as well as a third dimension expressed as
    the size of bubble representing the feature. The target class is denoted by the
    color.
  prefs: []
  type: TYPE_NORMAL
- en: At the low end of total protein, we see higher pH values in the mid-range of
    rectal temperature values. In this cluster, high pH values appear to show a stronger
    correlation to lesions that were surgical. Another cluster with wider variance
    in total protein is also found for values of total protein greater than 50\. The
    variance in pH is also low in this cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Having gained some insight into the data, we are ready to use some of the techniques
    presented in the theory that evaluate feature relevance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we use two techniques: one that calculates the weights for features based
    on Chi-squared statistics with respect to the target attribute and the other based
    on the Gini Impurity Index. The results are shown in the table. Note that as we
    inferred while doing analysis of the features via visualization, both Pulse and
    Rectal Temperature prove to have low relevance as shown by both techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Chi-squared | Gini index |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Attribute | Weight | Attribute | Weight |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Pain | 54.20626 | Pain | 0.083594 |'
  prefs: []
  type: TYPE_TB
- en: '| Abdomen | 53.93882 | Abdomen | 0.083182 |'
  prefs: []
  type: TYPE_TB
- en: '| Peristalsis | 38.73474 | Peristalsis | 0.059735 |'
  prefs: []
  type: TYPE_TB
- en: '| AbdominalDistension | 35.11441 | AbdominalDistension | 0.054152 |'
  prefs: []
  type: TYPE_TB
- en: '| PeripheralPulse | 23.65301 | PeripheralPulse | 0.036476 |'
  prefs: []
  type: TYPE_TB
- en: '| AbdominocentesisAppearance | 20.00392 | AbdominocentesisAppearance | 0.030849
    |'
  prefs: []
  type: TYPE_TB
- en: '| TemperatureOfExtremeties | 17.07852 | TemperatureOfExtremeties | 0.026338
    |'
  prefs: []
  type: TYPE_TB
- en: '| MucousMembranes | 15.0938 | MucousMembranes | 0.023277 |'
  prefs: []
  type: TYPE_TB
- en: '| NasogastricReflux | 14.95926 | NasogastricReflux | 0.023069 |'
  prefs: []
  type: TYPE_TB
- en: '| PackedCellVolume | 13.5733 | PackedCellVolume | 0.020932 |'
  prefs: []
  type: TYPE_TB
- en: '| RectalExamination-Feces | 11.88078 | RectalExamination-Feces | 0.018322 |'
  prefs: []
  type: TYPE_TB
- en: '| CapillaryRefillTime | 8.078319 | CapillaryRefillTime | 0.012458 |'
  prefs: []
  type: TYPE_TB
- en: '| RespiratoryRate | 7.616813 | RespiratoryRate | 0.011746 |'
  prefs: []
  type: TYPE_TB
- en: '| TotalProtein | 5.616841 | TotalProtein | 0.008662 |'
  prefs: []
  type: TYPE_TB
- en: '| NasogastricRefluxPH | 2.047565 | NasogastricRefluxPH | 0.003158 |'
  prefs: []
  type: TYPE_TB
- en: '| Pulse | 1.931511 | Pulse | 0.002979 |'
  prefs: []
  type: TYPE_TB
- en: '| Age | 0.579216 | Age | 8.93E-04 |'
  prefs: []
  type: TYPE_TB
- en: '| NasogastricTube | 0.237519 |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| AbdomcentecisTotalProtein | 0.181868 |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| RectalTemperature | 0.139387 |   |   |'
  prefs: []
  type: TYPE_TB
- en: '*Table 3: Relevant features determined by two different techniques, Chi-squared
    and Gini index.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Model process flow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In RapidMiner you can define a pipeline of computations using operators with
    inputs and outputs that can be chained together. The following process represents
    the flow used to perform the entire set of operations starting with loading the
    training and test data, handling missing values, weighting features by relevance,
    filtering out low scoring features, training an ensemble model that uses Bagging
    with Random Forest as the algorithm, and finally applying the learned model to
    the test data and outputting the performance metrics. Note that all the preprocessing
    steps that are applied to the training dataset must also be applied, in the same
    order, to the test set by means of the Group Models operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model process flow](img/B05137_02_194.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: RapidMiner process diagram'
  prefs: []
  type: TYPE_NORMAL
- en: Following the top of the process, the training set is ingested in the left-most
    operator, followed by the exclusion of non-predictors (Hospital Number, CP data)
    and self-predictors (Lesion 1). This is followed by the operator that replaces
    missing values with the mean and mode for continuous and categorical attributes,
    respectively. Next, the Feature Weights operator evaluates weights for each feature
    based on the Chi-squared statistic, which is followed by a filter that ignores
    low-weighted features. This pre-processed dataset is then used to train a model
    using Bagging with a Random Forest classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The preprocessing steps used on the training data are grouped together in the
    appropriate order via the Group Models operator and applied to the test data in
    the penultimate step. Finally, the predictions of the target variable on the test
    examples accompanied by the confusion matrix and other performance metrics are
    made evaluated and presented in the last step.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We are now ready to compare the results from the various models. If you have
    followed along you may find that your results vary from what's presented here—that
    may be due to the stochastic nature of some learning algorithms, or differences
    in the values of some hyper-parameters used in the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have considered three different training datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Original training data with missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training data transformed with missing values handled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training data transformed with missing values handled and with feature selection
    (Chi-Square) applied to select features that are highly discriminatory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have considered three different sets of algorithms on each of the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear algorithms (Naïve Bayes and Logistic Regression)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-linear algorithms (Decision Tree and KNN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble algorithms (Bagging, Ada Boost, and Random Forest).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation on Confusion Metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Models | TPR | FPR | Precision | Specificity | Accuracy | AUC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 68.29% | 14.81% | 87.50% | 85.19% | 75.00% | 0.836 |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic Regression | 78.05% | 14.81% | 88.89% | 85.19% | 80.88% | 0.856
    |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 68.29% | 33.33% | 75.68% | 66.67% | 67.65% | 0.696 |'
  prefs: []
  type: TYPE_TB
- en: '| k-NN | 90.24% | 85.19% | 61.67% | 14.81% | 60.29% | 0.556 |'
  prefs: []
  type: TYPE_TB
- en: '| Bagging (GBT) | 90.24% | 74.07% | 64.91% | 25.93% | 64.71% | 0.737 |'
  prefs: []
  type: TYPE_TB
- en: '| Ada Boost (Naïve Bayes) | 63.41% | 48.15% | 66.67% | 51.85% | 58.82% | 0.613
    |'
  prefs: []
  type: TYPE_TB
- en: '*Table 4: Results on unseen (Test) data for models trained on Horse-colic data
    with missing values*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| Models | TPR | FPR | Precision | Specificity | Accuracy | AUC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 68.29% | 66.67% | 60.87% | 33.33% | 54.41% | 0.559 |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic Regression | 78.05% | 62.96% | 65.31% | 37.04% | 61.76% | 0.689
    |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 97.56% | 96.30% | 60.61% | 3.70% | 60.29% | 0.812 |'
  prefs: []
  type: TYPE_TB
- en: '| k-NN | 75.61% | 48.15% | 70.45% | 51.85% | 66.18% | 0.648 |'
  prefs: []
  type: TYPE_TB
- en: '| Bagging (Random Forest) | 97.56% | 74.07% | 66.67% | 25.93% | 69.12% | 0.892
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bagging (GBT) | 82.93% | 18.52% | 87.18% | 81.48% | 82.35% | 0.870 |'
  prefs: []
  type: TYPE_TB
- en: '| Ada Boost (Naïve Bayes) | 68.29% | 7.41% | 93.33% | 92.59% | 77.94% | 0.895
    |'
  prefs: []
  type: TYPE_TB
- en: '*Table 5: Results on unseen (Test) data for models trained on Horse-colic data
    with missing values replaced*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| Models | TPR | FPR | Precision | Specificity | Accuracy | AUC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 75.61% | 77.78% | 59.62% | 29.63% | 54.41% | 0.551 |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic Regression | 82.93% | 62.96% | 66.67% | 37.04% | 64.71% | 0.692
    |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 95.12% | 92.59% | 60.94% | 7.41% | 60.29% | 0.824 |'
  prefs: []
  type: TYPE_TB
- en: '| k-NN | 75.61% | 48.15% | 70.45% | 51.85% | 66.18% | 0.669 |'
  prefs: []
  type: TYPE_TB
- en: '| Bagging (Random Forest) | 92.68% | 33.33% | 80.85% | 66.67% | 82.35% | 0.915
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bagging (GBT) | 78.05% | 22.22% | 84.21% | 77.78% | 77.94% | 0.872 |'
  prefs: []
  type: TYPE_TB
- en: '| Ada Boost (Naïve Bayes) | 68.29% | 18.52% | 84.85% | 81.48% | 73.53% | 0.848
    |'
  prefs: []
  type: TYPE_TB
- en: '*Table 6: Results on unseen (Test) data for models trained on Horse-colic data
    using features selected by Chi-squared statistic technique*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ROC Curves, Lift Curves, and Gain Charts
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The performance plots enable us to visually assess the models used in two of
    the three experiments—without any replacement of missing data, and with using
    features from Chi-squared weighting after replacing missing data—and to compare
    them against each other. Pairs of plots display the performance curves of each
    Linear (Logistic Regression), Non-linear (Decision Tree), and Ensemble (Bagging,
    using Gradient Boosted Tree) technique we learned about earlier in the chapter,
    drawn from results of the two experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '![ROC Curves, Lift Curves, and Gain Charts](img/B05137_02_195.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: ROC Performance curves for experiment using Missing Data'
  prefs: []
  type: TYPE_NORMAL
- en: '![ROC Curves, Lift Curves, and Gain Charts](img/B05137_02_196.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: Cumulative Gains performance curves for experiment using Missing
    Data'
  prefs: []
  type: TYPE_NORMAL
- en: '![ROC Curves, Lift Curves, and Gain Charts](img/B05137_02_197.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Lift performance curves for experiment using Missing Data'
  prefs: []
  type: TYPE_NORMAL
- en: Results, observations, and analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The impact of handling missing values is significant. Of the seven classifiers,
    with the exception of Naïve Bayes and Logistic Regression, all show remarkable
    improvement when missing values are handled as indicated by various metrics, including
    AUC, precision, accuracy, and specificity. This tells us that handling missing
    values that can be "noisy" is an important aspect of data transformation. Naive
    Bayes has its own internal way of managing missing values and the results from
    our experiments show that it does a better job of null-handling than our external
    transformations. But in general, the idea of transforming missing values seems
    beneficial when you consider all of the classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the section on modeling, some of the algorithms require the
    right handling of missing values and feature selection to get optimum performance.
    From the results, we can see that the performance of Decision Trees, for example,
    improved incrementally from 0.696 with missing data, 0.812 with managed missing
    data, and for the best performance of 0.824 with missing data handled together
    with feature selection. Six out of seven classifiers improve the performance in
    AUC (and in others metrics) when both the steps are performed; comparing *Table
    5* and *Table 6* for AUC gives us these quick insights. This demonstrates the
    importance of doing preprocessing such as missing value handling along with feature
    selection before performing modeling.
  prefs: []
  type: TYPE_NORMAL
- en: A major conclusion from the results is that the problem is highly non-linear
    and therefore most non-linear classifiers from the simplest Decision Trees to
    ensemble Random Forest perform very well. The best performance comes from the
    meta-learning algorithm Random Forest, with missing values properly handled and
    the most relevant features used in training. The best linear model performance
    measured by AUC was 0.856 for Logistic Regression with data as-is (that is, with
    missing values), whereas Random Forest achieved AUC performance of 0.915 with
    proper handling of missing data accompanied by feature selection. Generally, as
    evident from *Table 3*, the non-linear classifiers or meta-learners performed
    better than linear classifiers by most performance measures.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values, which can be thought as "noise", in the appropriate
    manner improves the performance of AdaBoost by a significant amount. The AUC improves
    from 0.613 to 0.895 and FPR reduces from 48.15 to 7.41%. This indeed conforms
    to the expected theoretical behavior for this technique.
  prefs: []
  type: TYPE_NORMAL
- en: Meta-learning techniques, which use concepts of boosting and bagging, are relatively
    more effective when dealing with real-world data, when compared to other common
    techniques. This seems to be justified by the results since AdaBoost with Naïve
    Bayes as base learner trained on data that has undergone proper handling of noise
    outperforms Naive Bayes in most of the metrics, as shown in *Table 5* and *Table
    6*. Random Forest and GBTs also show the best performance along with AdaBoost
    as compared to base classifiers in *Table 6*, again confirming that the right
    process and ensemble learning can produce the most optimum results in real-world
    noisy datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All data, models, and results for both WEKA and RapidMiner process files from
    this chapter are available at: [https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter2](https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter2).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supervised learning is the predominant technique used in machine learning applications.
    The methodology consists of a series of steps beginning with data exploration,
    data transformation, and data sampling, through feature reduction, model building,
    and ultimately, model assessment and comparison. Each step of the process involves
    some decision making which must answer key questions: How should we impute missing
    values? What data sampling strategy should we use? What is the most appropriate
    algorithm given the amount of noise in the dataset and the prescribed goal of
    interpretability? This chapter demonstrated the application of these processes
    and techniques to a real-world problem—the classification problem using the UCI
    Horse Colic dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Whether the problem is one of classification, when the target is a categorical
    value, or Regression, when it is a real-valued continuous variable, the methodology
    used for supervised learning is similar. In this chapter, we have used classification
    for illustration.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is data quality analysis, which includes descriptive statistics
    of the features, visualization analysis using univariate, and multivariate feature
    analysis. With the help of various plot types, we can uncover different trends
    in the data and examine how certain features may or may not correlate with the
    label values and with each other. Data analysis is followed by data pre-processing,
    where the techniques include ways to address noise, as in the case of missing
    data, and outliers, as well as preparing the data for modeling techniques through
    normalization and discretization.
  prefs: []
  type: TYPE_NORMAL
- en: Following pre-processing, we must suitably split the data into train, validation,
    and test samples. Different sampling strategies may be used depending on the characteristics
    of the data and the problem at hand, for example, when the data is skewed or when
    we have a multi-class classification problem. Depending on data size, cross-validation
    is a common alternative to creating a separate validation set.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is the culling of irrelevant features. In the filter approach,
    techniques that use univariate analysis are either entropy-based (Information
    Gain, Gains Ratio) or based on statistical hypothesis testing (Chi-Squared). With
    the main multivariate methods, the aim is reduction of redundant features when
    considered together, or using the ones that correlate most closely with the target
    label. In the wrapper approach, we use machine learning algorithms to tell us
    about the more discriminating features. Finally, some learning techniques have
    feature selection embedded in the algorithm in the form of a regularization term,
    typically using ridge or lasso techniques. These represent the embedded approach.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling techniques are broadly classified into linear, non-linear, and ensemble
    methods. Among linear algorithms, the type of features can determine the algorithms
    to use—Linear Regression (numeric features only), Naïve Bayes (numeric or categorical),
    and logistic regression (numeric features only, or categorical transformed to
    numeric) are the work-horses. The outlined advantages and disadvantages of each
    method must be understood when choosing between them or interpreting the results
    of learning using these models.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree, k-NN, and SVM are non-linear techniques, each with their own
    strengths and limitations. For example, interpretability is the main advantage
    of Decision Tree. k-NN is robust in the face of noisy data, but it does poorly
    with high-dimensional data. SVM suffers from poor interpretability, but shines
    even when the dataset is limited, and the number of features is large.
  prefs: []
  type: TYPE_NORMAL
- en: With a number of different models collaborating, ensemble methods can leverage
    the best of all. Bagging and boosting both are techniques that generalize better
    in the ensemble compared to the base learner they use and are popular in many
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, what are the strategies and methods that can be used in evaluating
    model performance and comparing models to each other? The role of validation sets
    or cross-validation is essential to the ability to generalize over unseen data.
    Performance evaluation metrics derived from the confusion matrix are used universally
    to evaluate classifiers; some are used more commonly in certain domains and disciplines
    than others. ROC, Gain, and Lift curves are great visual representations of the
    range of model performance as the classification threshold is varied. When comparing
    models in pairs, several metrics based on statistical hypothesis testing are used.
    Wilcoxon and McNemar's are two non-parametric tests; Paired-t test is an example
    of a parametric method. Likewise, when comparing multiple algorithms, a common
    non-parametric test that does not make assumptions about the data distribution
    is Friedman's test. ANOVA, which are parametric tests, assume normal distribution
    of the metrics and equal variances.
  prefs: []
  type: TYPE_NORMAL
- en: The final sections of the chapter present the process undertaken using the RapidMiner
    tool to develop and evaluate models generated to classify test data from the UCI
    Horse-colic dataset. Three experiments are designed to compare and contrast the
    performance of models under different data pre-processing conditions, namely,
    without handling missing data, with replacement of missing data using standard
    techniques, and finally, with feature selection following null replacement. In
    each experiment we choose multiple linear, non-linear, and ensemble methods. As
    part of the overall process, we illustrate how the modeling environment is used.
    We can draw revealing conclusions from the results, which give us insights into
    the data as well as demonstrating the relative strengths and weakness of the various
    classes of techniques in different situations. We conclude that the data is highly
    non-linear and that ensemble learning demonstrates clear advantages over other
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: D. Bell and H. Wang (2000). *A Formalism for Relevance and its Application in
    Feature Subset Selection. Machine Learning*, 41(2):175–195.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'J. Doak (1992). *An Evaluation of Feature Selection Methods and their Application
    to Computer Security*. Technical Report CSE–92–18, Davis, CA: University of California,
    Department of Computer Science.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: M. Ben-Bassat (1982). *Use of Distance Measures, Information Measures and Error
    Bounds in Feature Evaluation*. In P. R. Krishnaiah and L. N. Kanal, editors, Handbook
    of Statistics, volume 2, pages 773–791, North Holland.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Littlestone N, Warmuth M (1994) *The weighted majority algorithm*. Information
    Computing 108(2):212–261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Breiman L., Friedman J.H., Olshen R.A., Stone C.J. (1984) *Classification and
    Regression Trees*, Wadsforth International Group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B. Ripley(1996), *Pattern recognition and neural networks*. Cambridge University
    Press, Cambridge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Breiman, L., (1996). *Bagging Predictors, Machine Learning*, 24 123-140.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Burges, C. (1998). *A tutorial on support vector machines for pattern recognition.
    Data Mining and Knowledge Discovery*. 2(2):1-47.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bouckaert, R. (2004), *Naive Bayes Classifiers That Perform Well with Continuous
    Variables, Lecture Notes in Computer Science*, Volume 3339, Pages 1089 – 1094.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aha D (1997). *Lazy learning*, Kluwer Academic Publishers, Dordrecht
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nadeau, C. and Bengio, Y. (2003), *Inference for the generalization error*.
    In Machine Learning 52:239– 281.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quinlan, J.R. (1993). C4.5: *Programs for machine learning*, Morgan Kaufmann,
    San Francisco.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vapnik, V. (1995), *The Nature of Statistical Learning Theory*. Springer Verlag.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Schapire RE, Singer Y, Singhal A (1998). *Boosting and Rocchio applied to text
    filtering*. In SIGIR ''98: Proceedings of the 21st Annual International Conference
    on Research and Development in Information Retrieval, pp 215–223'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Breiman L.(2001). *Random Forests*. Machine Learning, 45 (1), pp 5-32.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Nathalie Japkowicz and Mohak Shah (2011). *Evaluating Learning Algorithms:
    A Classification Perspective*. Cambridge University Press.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hanley, J. & McNeil, B. (1982). *The meaning and use of the area under a receiver
    operating characteristic (ROC) curve*. Radiology 143, 29–36.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tjen-Sien, L., Wei-Yin, L., Yu-Shan, S. (2000). *A Comparison of Prediction
    Accuracy, Complexity, and Training Time of Thirty-Three Old and New Classification
    Algorithms*. Machine Learning 40: 203–228.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. W. Moore and M. S. Lee (1994). *Efficient Algorithms for Minimizing Cross
    Validation Error*. In Proc. of the 11th Int. Conf. on Machine Learning, pages
    190–198, New Brunswick, NJ. Morgan Kaufmann.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nitesh V. Chawla et. al. (2002). *Synthetic Minority Over-sampling Technique*.
    Journal of Artificial Intelligence Research. 16:321-357.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
