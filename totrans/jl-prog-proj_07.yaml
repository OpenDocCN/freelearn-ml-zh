- en: Machine Learning for Recommender Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope that you are now excited about the amazing possibilities offered by the
    recommender systems that we've built. The techniques we've learned will provide
    you with a tremendous amount of data-taming prowess and practical abilities that
    you can already apply in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is more to recommendation systems than that. Due to their large-scale
    applications in recent years, as an efficient solution to the information overload
    caused by the abundance of offerings on online platforms, recommenders have received
    a lot of attention, with new algorithms being developed at a rapid pace. In fact,
    all the algorithms that we studied in the previous chapter are part of a single
    category, called **memory-based** **recommenders**. Besides these, there's another
    very important class or recommender, which is known as** model-based**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll learn about them. We will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory-based versus model-based recommendation systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data processing for training a model-based recommender
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a model-based recommender
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid recommendation systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Julia package ecosystem is under continuous development and new package
    versions are released on a daily basis. Most of the times this is great news,
    as new releases bring new features and bug fixes. However, since many of the packages
    are still in beta (version 0.x), any new release can introduce breaking changes.
    As a result, the code presented in the book can stop working. In order to ensure
    that your code will produce the same results as described in the book, it is recommended
    to use the same package versions. Here are the external packages used in this
    chapter and their specific versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to install a specific version of a package you need to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively you can install all the used packages by downloading the `Project.toml`
    file provided with the chapter and using `pkg>` instantiate as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Comparing the memory-based versus model-based recommenders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to understand the strengths and weaknesses of both memory-based
    and model-based recommenders so that we can make the right choice according to
    the available data and the business requirements. As we saw in the previous chapter,
    we can classify recommender systems according to the data they are using and the
    algorithms that are employed.
  prefs: []
  type: TYPE_NORMAL
- en: First, we can talk about non-personalized versus personalized recommenders.
    Non-personalized recommenders do not take into account user preferences, but that
    doesn't make them less useful. They are successfully employed when the relevant
    data is missing, for example, for a user that is new to the system or just not
    logged in. Such recommendations can include the best apps of the week on the Apple
    App Store, trending movies on Netflix, songs of the day on Spotify, NY Times bestsellers,
    Billboard Top 10, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to personalized recommender systems, these can be further split into
    content-based and collaborative system. A content-based system makes recommendations
    by matching an item, specifications. A famous example of this category is Pandora
    and its Music Genome Project. The Music Genome Project, which powers Pandora,
    is the most comprehensive analysis of music ever undertaken. They worked with
    trained musicologists who listened to music across all genres and decades, studying
    and collecting musical details on every track—450 musical attributes altogether.
    Pandora makes recommendations by picking other songs from its catalog that closely
    match the features (*features* is data-science language for attributes, properties,
    or tags) of the tracks that the user previously liked.
  prefs: []
  type: TYPE_NORMAL
- en: As for collaborative filtering, the idea behind it is that we can identify a
    metric that correctly reflects a user's tastes and then exploit it in combination
    with a dataset of other users, whose preferences were already collected. The underlying
    supposition is that if we have a pool of users that enjoy many of the same things,
    we can recommend to one of them some items from another's user list, which were
    not yet discovered by the targeted user. Any item in the list of options that
    is not part of the targeted user's list can readily be offered as a recommendation
    because similar preferences will lead to other similar choices.
  prefs: []
  type: TYPE_NORMAL
- en: This specific type of collaborative filtering was named user-based since the
    primary focus of the algorithm is the similarity between the target user and other
    users.
  prefs: []
  type: TYPE_NORMAL
- en: Another variation of the collaborative algorithm is **item-based filtering**.
    The main difference between this and user-based filtering is that the focus is
    on similar items. Which approach is the best depends on the specific use case—item-based
    recommendations are more efficient when the product catalog is considerably smaller
    and changes less often than the number of users and their preferences.
  prefs: []
  type: TYPE_NORMAL
- en: The last of the commonly accepted typologies divides the recommender systems
    into memory-based and model-based. *Memory-based* refers to the fact that the
    system requires the whole dataset to be loaded into working memory (the RAM).
    The algorithms rely on mapping to and from memory to consequently calculate the
    similarity between two users or items, and produce a prediction for the user by
    taking the weighted average of all the ratings. A few ways of computing the correlation
    can be used, such as *Pearson's r*. There are certain advantages to this approach,
    like the simplicity of the implementation, the easy facilitation of new data,
    or the fact that the results can be easily explained. But, unsurprisingly, it
    does come with significant performance downsides, creating problems when the data
    is sparse and the datasets are large.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the limitations of the memory-based recommender systems, alternative
    solutions were needed, mainly driven by the continuous growth of online businesses
    and their underlying data. These were characterized by large volumes of users
    and an increasing number of products. The most famous example is Netflix's one
    million dollar competition—in 2006, Netflix offered a one million dollar prize
    to the individual or team that could improve their existing recommendations algorithm,
    called **Cinematch**, by at least 10%. It took three years for this feat to be
    achieved, and it was done by a joint team of initial competitors, who ultimately
    decided to join forces to grab the prize.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about the model-based approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This innovative approach to recommender systems was named *model-based*, and
    it made extensive use of matrix factorization techniques. In this approach, models
    are developed using different machine learning algorithms to predict a user's
    ratings. In a way, the model-based approach can be seen as a complementary technique
    to improve memory-based recommendations. They address the matrix sparsity problem
    by guessing how much a user will like a new item. Machine learning algorithms
    are used to train on the existing vector of ratings of a specific user, and then
    build a model that can predict the user's score for an item that the user hasn't
    tried yet. Popular model-based techniques are Bayesian Networks, **singular value
    decomposition** (**SVD**), and **Probabilistic Latent Semantic Analysis** (**PLSA**)
    or **Probabilistic Latent Semantic Indexing** (**PLSI**).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of popular approaches for building the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Probability**: Making a recommendation is framed as a problem of predicting
    the probability of a rating being of a particular value. Bayesian networks are
    often used with this implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced ****memory-based**: This uses a model to represent the similarities
    between users or items and then predicts the ratings. The Netflix prize-winning
    ALS-WR algorithm represents this type of implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear algebra**: Finally, recommendations can be made by performing linear
    algebra operations on the matrices of users and ratings. A commonly used algorithm
    is SVD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we'll implement a model-based recommender. We'll
    use a third-party Julia package and code our business logic around it.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding our data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get conclusive results from our **Machine Learning** (**ML**) models, we
    need data—and plenty of it. There are many open source datasets available online.
    Kaggle, for example, provides a large collection of high quality and anonymized
    data dumps that can be used for training and experimenting, and is available for
    download at [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets).
    Another famous data repository is provided by FiveThirtyEight, at [https://github.com/fivethirtyeight/data](https://github.com/fivethirtyeight/data).
    Buzzfeed also makes a large treasure of data public at [https://github.com/BuzzFeedNews](https://github.com/BuzzFeedNews).
  prefs: []
  type: TYPE_NORMAL
- en: For our project, we'll create a book recommendation system. We'll use the *Book-Crossing
    Dataset*, which is available for download at [http://www2.informatik.uni-freiburg.de/~cziegler/BX/](http://www2.informatik.uni-freiburg.de/~cziegler/BX/).
    This data was collected during the months of August and September 2004, under
    permission, from the Book-Crossing community ([https://www.bookcrossing.com/](https://www.bookcrossing.com/)).
    It includes over 1.1 million book ratings, for more than 270,000 books, from 278,000
    users. The user data is anonymized, but still includes demographic information
    (location and age, where available). We'll use this data to train our recommendation
    system and then ask it for interesting new books for our users.
  prefs: []
  type: TYPE_NORMAL
- en: A first look at the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset is composed of three tables—one for users, one for books, and one
    for ratings. The `BX-Users` table contains the users' data. The `User-ID` is a
    sequential integer value, as the original user ID has been anonymized. The `Location`
    and `Age` columns contain the corresponding demographic information. This is not
    available for all the users and in these cases, we'll encounter the `NULL` value
    (as the `NULL` string).
  prefs: []
  type: TYPE_NORMAL
- en: The `BX-Books` table stores the information about the books. For the unique
    identifier, we have the standard ISBN book code. Besides this, we are also provided
    with the book's title (the `Book-Title` column), author (`Book-Author`), publishing
    year (`Year-of-Publication`), and the publisher (`Publisher`). URLs of thumbnail
    cover images are also provided, corresponding to three sizes—small (`Image-URL-S`),
    medium (`Image-URL-M`), and large (`Image-URL-L`).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `BX-Book-Ratings` table contains the actual ratings. The table
    has a simple structure, with three columns—`User-ID`, for the user making the
    rating; the ISBN of the book; and `Book-Rating`, which is the score. The ratings
    are expressed on a scale from 1 to 10, where higher is better. The value `0` signifies
    an implicit rating.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is available in SQL and CSV formats, packaged as ZIP archives.
    Please download the CSV version from [http://www2.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip](http://www2.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip).
  prefs: []
  type: TYPE_NORMAL
- en: Unzip the file somewhere on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Loading this dataset is going to be a bit more challenging, as we have to work
    with three distinct files, and due to the particularities of the data itself.
    Here is the head of the `BX-Users.csv` file, in a plain text editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ffff291-6cef-498c-b11d-dac6f092589e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have to explicitly handle the following formatting particularities, which
    will otherwise cause the import to fail:'
  prefs: []
  type: TYPE_NORMAL
- en: The columns are separated by `;` instead of the more customary comma or *Tab*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing values are represented by the string `NULL`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first row is the header, representing the column names
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data is enclosed in double quotes `" "`, and double quotes within the data
    itself are escaped by backslashes, for example, `"1273";"valladolid, \"n/a\",
    spain";"27"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fortunately, the CSV package provides additional options for passing in all
    of this information when reading in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: It might take a bit of time to load the table, but eventually, we'll get the
    sweet taste of success—`278858` rows loaded into memory!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/585c070e-919b-4120-8947-7a9ebd932893.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll use the same approach to load the books and rankings tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Excellent! We now have all three tables loaded into memory as `DataFrames`.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In data science, missing values occur when no data value is stored for a field
    in a record—in other words, when we don't have a value for a column in a row.
    It is a common scenario, but nonetheless, it can have a significant negative effect
    on the usefulness of the data, so it needs to be explicitly handled.
  prefs: []
  type: TYPE_NORMAL
- en: The approach in `DataFrames` is to mark the missing value by using the `Missing`
    type. The default behavior is the propagation of the missing values, thus *poisoning* the
    data operations that involve `missing`—that is, operations involving valid input,
    and `missing` will return `missing` or `fail`. Hence, in most cases, the missing
    values need to be addressed in the data-cleaning phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common techniques for handling missing values are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deletion**: The rows containing the missing variables are deleted (also called
    **listwise deletion**). The downside of this approach is that it leads to loss
    of information. However, if we have plenty of data and not many incomplete records
    (say, under 10%), this is the simplest approach and the most commonly used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imputation**: The `missing` values are inferred using some technique, usually
    `mean`, `median`, or `mode`. However, you need to be careful, as this artificially
    reduces the variation of the dataset. As an alternative, a predictive model could
    be used to infer the missing value by applying statistical methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can read more about Julia's treatment of missing values in the documentation
    at [https://docs.julialang.org/en/v1.0/manual/missing/](https://docs.julialang.org/en/v1.0/manual/missing/),
    while a more advanced discussion of the theoretical aspects of handling missing
    data can be found at [https://datascience.ibm.com/blog/missing-data-conundrum-exploration-and-imputation-techniques/](https://datascience.ibm.com/blog/missing-data-conundrum-exploration-and-imputation-techniques/).
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get a feel of the data, starting with the users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70cbc219-b188-4fc6-a176-9ab636781edb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We chose a few key stats—the minimum and maximum values, the number of missing
    and unique values, and the type of data. Unsurprisingly, the `User-ID` column,
    which is the table''s primary key, starts at `1` and goes all the way up to `278858`
    with no missing values. However, the `Age` column shows a clear sign of data errors—the
    maximum age is `244` years! Let''s see what we have there by plotting the data
    with `Gadfly`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/498825bb-b25d-4f9e-99d9-78c129837a0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We rendered a histogram of the ages, splitting the data into 15 intervals.
    We have some outliers indicating incorrect ages, but most of the data is distributed
    within the expected range, up to 80-90 years old. Since anything after **100**
    years old is highly unlikely to be correct, let''s get rid of it. The simplest
    way is to filter out all the rows where the age is greater than **100**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Oops! Our `Age` column has `missing` values that cannot be compared. We could
    remove these as well, but in this case, the missing age seems to be more of a
    symptom of the user not disclosing the information, rather than a data error.
    Therefore, I''m more inclined to keep the rows while replacing the missing data
    with valid values. The question is, what values? Imputation using the `mean` seems
    like a good option. Let''s compute it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We used the `skipmissing` function to iterate over all the non-missing `Age`
    values and compute the `mean`. Now, we can use this in conjunction with `coalesce`
    to replace the missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We are effectively replacing the `Age` column of the `users` `DataFrame` with
    a new array, resulting from the application of `coalesce` to the same `Age` column.
    Please notice the dot in the invocation of `coalesce`, indicating that it is applied
    element-wise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Great—finally, we need to get rid of those erroneous ages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f902d24-567b-478f-a590-6b8cf26c6159.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking good!
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re done with the users, so let''s move on to the books data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3de5518e-aeda-441a-bd2c-7b1db4d01217.png)'
  prefs: []
  type: TYPE_IMG
- en: The data looks much cleaner—first of all, there's no missing values. Then, looking
    at the counts for `nunique`, we can tell that some of the books have identical
    titles and that there's a considerable amount of authors that have published more
    than one book. Finally, the books come from almost 17,000 publishers.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, so good, but let''s take a look at the `Year-Of-Publication`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Something''s not right here—we have some publishing years that don''t make
    sense. Some are too far in the past, while others are way in the future. I wonder
    what the distribution looks like. Let''s render another histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be6a1974-795e-4fbe-9b19-bd91a4adbf8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Most of the data seems to be correct, but there are some faulty outliers. We
    can take a look at the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'At first sight, we can get rid of the rows that have the publishing year equal
    to `0`. We can also safely assume that all the rows where the publishing date
    is greater than the year when the data was collected (`2004`) are also wrong,
    and so they can be removed. It''s difficult to say what to do about the rest,
    but still, it''s hard to believe that people have ranked books that were published in
    the Middle Ages. Let''s just keep the books that were published between `1970`
    and `2004`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab4f1089-c149-4fb8-9722-682870fa0892.png)'
  prefs: []
  type: TYPE_IMG
- en: This is much better and entirely plausible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s check the ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66b8603b-1bbb-49e5-9f74-efe4af383200.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There''s no missing values, which is great. The `Book-Rating` values are between
    `0` (implicit rating) and `10`, where `1` to `10` represent explicit ratings.
    The median of `0.0` is a bit of a concern though, so let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48211541-9958-405a-808a-a6ab7b121f2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It turns out that most of the ratings are implicit, thus set to `0`. These
    are not relevant to our recommender, so let''s get rid of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/617829d8-33c4-4844-ba12-cda577faf730.png)'
  prefs: []
  type: TYPE_IMG
- en: We're doing great! There's one more step in our **extract, transform, load** (**ETL**)
    process—let's put the three `DataFrames` together by joining them on the matching
    columns, thus removing the various orphan entries (the ones that don't have corresponding
    rows in all the other tables).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll join book ratings and books:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We're using the `join` method, indicating the two `DataFrames` we want to join,
    plus the join column and the kind of join we want. An inner join requires that
    the result contains rows for values of the key that exist in both the first and
    second `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s join with the user''s data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Our dataset now contains only the valid data, nicely packed in a single `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As our ratings are on a scale between `1` and `10`, not all of these ratings
    can be considered an endorsement for the book. It''s true that the vast majority
    of the rankings are above `5`, but a `5` is still not good enough for a useful
    recommendation. Let''s simplify our data a bit to make the computations faster
    by assuming that any ranking starting with `8` represents a positive review and
    would make for a strong recommendation. Therefore, we''ll keep only these rows
    and discard the rest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This is looking good, but it will look even better with just a small tweak
    to make the column names more Julia-friendly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We will iterate over each column name and remove the dashes. This way, we''ll
    be able to use the names without having to explicitly use the `Symbol` constructor
    every time. We''ll end up with the following names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re getting closer—the last step in our data processing workflow is to check
    the number of reviews per user. The more reviews we have from a user, the better
    the preference profile we can create, leading to more relevant and better quality
    recommendations. Basically, we want to get a count of ratings, per user, and then
    get a count of each count (that is, how many rating of ones, twos, threes, and
    so on, up to ten ratings we have):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we group the `top_ratings` data by `UserID` and use the `size` function
    as our `aggregation` function, which returns a tuple of dimensions—out of which
    we retrieve just its first dimension. We''ll get the following result, where the
    `x1` column contains the number of ratings provided by the corresponding user:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b094b39c-89ef-45ac-b617-8929f72888a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Wondering what this data will reveal? Let''s find out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc620be1-d226-4a70-a555-d4685ad1e86d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The minimum number of ratings is `1`, while the most productive user has provided
    no less than `5491`, with a mean of around `5` reviews per user. Considering that
    the recommendations for a user with less than `5` reviews would be pretty weak
    anyway, we''re better off removing the users without enough data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re only keeping the users that have at least `5` ratings. Let''s see how
    the number of ratings is distributed now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f96dcf0d-a017-4ebe-a612-878a4f3104bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Looks like the vast majority of users have up to `1000` ratings. What about
    the outliers with lots of reviews?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6075a11b-184a-4e9e-9e64-d846bbfacbf4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There''s only `3` users. We''d better remove them so that they don''t skew
    our results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the list of final users, the next step is to remove all the
    others from the `top_ratings` `DataFrame`. Again, let''s use an inner join—it''s
    pretty straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: That's it, our data is ready. Great job!
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want, you can save this data to file by using `CSV.write`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: If you've had problems following along, don't worry. In a few paragraphs, I'll
    explain how you can load a ready-made dataset, which is provided in this chapter's
    support files.
  prefs: []
  type: TYPE_NORMAL
- en: Training our data models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning can be divided into four main types, depending on the methodology
    and the type of data that is used:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semi-supervised
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In supervised learning, we start with a dataset that contains training (or teaching)
    data, where each record is labeled, representing both input (let's call it *X*),
    and output values (named *Y*). Then, the algorithm's job is to identify a function
    *f* from input to output, so that *Y = f(X)*. Once this function is identified,
    it can be used on new data (that is, new inputs that are not labeled) to predict
    the output. Depending on the type of output that needs to be computed, if the
    output has to be assigned to a certain class of entities (as in, it represents
    categorical data), then a classification algorithm will be used. Alternatively,
    if the type of output is a numeric value, we'll be dealing with a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: With unsupervised machine learning, we have the inputs, but not the outputs.
    In such a scenario, once we use the learning dataset to train our system, the
    main goal will be data clustering, that is, generating different clusters of inputs
    and being able to assign new data to the most appropriate cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised, as the name suggests, represents a mixture of the two previously
    described approaches, both of which are applicable when our data contains both
    labeled and unlabeled records.
  prefs: []
  type: TYPE_NORMAL
- en: In reinforcement learning, the algorithm is informed about the success of its
    previous decisions. Based on this, the algorithm modifies its strategy in order
    to maximize the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the learning style and the specific problem that's meant to be
    solved, there are a multitude of algorithms that can be applied. For supervised
    learning, we can use regression (linear or logistic), decision trees, or neural
    networks, to name just a few. With unsupervised learning, we could choose k-means
    clustering or `Apriori` algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Since our data is tagged (we have the rating for each user), we are dealing
    with a supervised machine learning problem. For our test case, since our data
    is represented as a matrix, we'll employ an algorithm called **Matrix Factorization**
    (**MF**).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about the various types of ML algorithms and how to choose
    them at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice](https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice)
    [https://blog.statsbot.co/machine-learning-algorithms-183cc73197c](https://blog.statsbot.co/machine-learning-algorithms-183cc73197c)
    [https://elitedatascience.com/machine-learning-algorithms](https://elitedatascience.com/machine-learning-algorithms)
    [https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling down our dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training machine learning models at scale usually requires (lots of) powerful
    computers and plenty of time. If you have neither of these while reading this
    book, I have prepared a smaller dataset so that you can go through our project.
  prefs: []
  type: TYPE_NORMAL
- en: Training the recommender on the full `top_ratings` data took over 24 hours on
    my quad-core, 16 GB RAM laptop. If you're so inclined, feel free to try it. It
    is also available for download at [https://github.com/PacktPublishing/Julia-Projects/blob/master/Chapter07/data/large/top_ratings.csv.zip](https://github.com/PacktPublishing/Julia-Projects/blob/master/Chapter07/data/large/top_ratings.csv.zip).
  prefs: []
  type: TYPE_NORMAL
- en: However, if you'd like to follow through the code while reading this chapter,
    please download the `top_ratings.csv` file that's provided with this chapter's
    support files at [https://github.com/PacktPublishing/Julia-Projects/blob/master/Chapter07/data/top_ratings.csv](https://github.com/PacktPublishing/Julia-Projects/blob/master/Chapter07/data/top_ratings.csv).
    I will be using the data from this smaller file for the remainder of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve downloaded the file, you can load its content into the `top_ratings`
    variable by using the `CSV.read` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Training versus testing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A common strategy in machine learning implementations is to split the data
    into training (some 80-90%) and testing (the remaining 10-20%) datasets. First,
    we''ll initialize two empty `DataFrames` to store this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll iterate through our `top_ratings` and put the contents into the
    corresponding `DataFrame`. We''ll go with 10% of data for testing—so with each
    iteration, we''ll generate a random integer between `1` and `10`. The chances
    of getting a `10` are, obviously, one in ten, so when we get it, we put the corresponding
    row into the test dataset. Otherwise, it goes into the training one, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: There's no canonical way for pushing a `DataFrameRow` onto another `DataFrame`,
    so we're using one of the recommended approaches, which is to convert the row
    into an `Array` and `push!` it to the `DataFrame`. Our training and testing datasets
    are now ready.
  prefs: []
  type: TYPE_NORMAL
- en: 'For me, they look like this, but since the data was generated randomly, it
    will be different for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'If you prefer for us to work with the same datasets, you can download the data
    dump from this chapter''s support files (available at [https://github.com/PacktPublishing/Julia-Projects/blob/master/Chapter07/data/training_data.csv](https://github.com/PacktPublishing/Julia-Projects/blob/master/Chapter07/data/training_data.csv) and
    [https://github.com/PacktPublishing/Julia-Projects/blob/master/Chapter07/data/test_data.csv](https://github.com/PacktPublishing/Julia-Projects/blob/master/Chapter07/data/test_data.csv),
    respectively) and read them in as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Machine learning-based recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Julia's ecosystem provides access to `Recommendation.jl`, a package that implements
    a multitude of algorithms for both personalized and non-personalized recommendations.
    For model-based recommenders, it has support for SVD, MF, and content-based recommendations
    using TF-IDF scoring algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: There's also another very good alternative—the `ScikitLearn.jl` package ([https://github.com/cstjean/ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl)).
    This implements Python's very popular scikit-learn interface and algorithms in
    Julia, supporting both models from the Julia ecosystem and those of the scikit-learn
    library (via `PyCall.jl`). The Scikit website and documentation can be found at
    [http://scikit-learn.org/stable/](http://scikit-learn.org/stable/). It is very
    powerful and definitely worth keeping in mind, especially for building highly
    efficient recommenders for production usage. For learning purposes, we'll stick
    to `Recommendation`, as it provides for a simpler implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Making recommendations with Recommendation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our learning example, we'll use `Recommendation`. It is the simplest of
    the available options, and it's a good teaching device, as it will allow us to
    further experiment with its plug-and-play algorithms and configurable model generators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can do anything interesting, though, we need to make sure that we
    have the package installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Please note that I'm using the `#master` version, because the tagged version,
    at the time of writing this book, was not yet fully updated for Julia 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow for setting up a recommender with `Recommendation` involves three
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the training data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiating and training a recommender using one of the available algorithms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the training is complete, asking for recommendations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's implement these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Recommendation` uses a `DataAccessor` object to set up the training data.
    This can be instantiated with a set of `Event` objects. A `Recommendation.Event`
    is an object that represents a user-item interaction. It is defined like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, the `user` field will represent the `UserID`, the `item` field
    will map to the ISBN, and the `value` field will store the `Rating`. However,
    a bit more work is needed to bring our data in the format required by `Recommendation`:'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, our ISBN data is stored as a string and not as an integer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, internally, `Recommendation` builds a sparse matrix of `user` * ` item`
    and stores the corresponding values, setting up the matrix using sequential IDs.
    However, our actual user IDs are large numbers, and `Recommendation` will set
    up a very large, sparse matrix, going all the way from the minimum to the maximum
    user IDs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What this means is that, for example, we only have 69 users in our dataset (as
    confirmed by `unique(training_data[:UserID]) |> size`), with the largest ID being
    277,427, while for books we have 9,055 unique ISBNs. If we go with this, `Recommendation`
    will create a 277,427 x 9,055 matrix instead of a 69 x 9,055 matrix. This matrix
    would be very large, sparse, and inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we'll need to do a bit more data processing to map the original user
    IDs and the ISBNs to sequential integer IDs, starting from 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use two `Dict` objects that will store the mappings from the `UserID`
    and `ISBN` columns to the recommender''s sequential user and book IDs. Each entry
    will be of the form `dict[original_id] = sequential_id`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll also need two counters to keep track of, and increment, the sequential
    IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now prepare the `Event` objects for our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This will fill up the events array with instances of `Recommendation.Event`,
    which represent a unique `UserID`, `ISBN`, and `Rating` combination. To give you
    an idea, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Please remember this very important aspect—in Julia, the `for` loop defines
    a new scope. This means that variables defined outside the `for` loop are not
    accessible inside it. To make them visible within the loop's body, we need to
    declare them as `global`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are ready to set up our `DataAccessor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Building and training the recommender
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we have all that we need to instantiate our recommender. A very
    efficient and common implementation uses MF—unsurprisingly, this is one of the
    options provided by the `Recommendation` package, so we'll use it.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Factorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The idea behind MF is that, if we''re starting with a large sparse matrix like
    the one used to represent *user x profile* ratings, then we can represent it as
    the product of multiple smaller and denser matrices. The challenge is to find
    these smaller matrices so that their product is as close to our original matrix
    as possible. Once we have these, we can fill in the blanks in the original matrix
    so that the predicted values will be consistent with the existing ratings in the
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c67372c9-cbfd-4365-af58-5a1ba2baf472.png)'
  prefs: []
  type: TYPE_IMG
- en: Our *user x books* rating matrix can be represented as the product between smaller
    and denser users and books matrices.
  prefs: []
  type: TYPE_NORMAL
- en: To perform the matrix factorization, we can use a couple of algorithms, among
    which the most popular are SVD and **Stochastic Gradient Descent** (**SGD**).
    `Recommendation` uses SGD to perform matrix factorization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We instantiate a new MF recommender and then we build it—that is, train it.
    The build step might take a while (a few minutes on a high-end computer using
    the small dataset that's provided in this chapter's support files).
  prefs: []
  type: TYPE_NORMAL
- en: If we want to tweak the training process, since SGD implements an iterative
    approach for matrix factorization, we can pass a `max_iter` argument to the build
    function, asking it for a maximum number of iterations. The more iterations we
    do, in theory, the better the recommendations—but the longer it will take to train
    the model. If you want to speed things up, you can invoke the build function with
    a `max_iter` of `30` or less—`build(recommender, max_iter = 30)`.
  prefs: []
  type: TYPE_NORMAL
- en: We can pass another optional argument for the learning rate, for example, `build
    (recommender, learning_rate=15e-4, max_iter=100)`. The learning rate specifies
    how aggressively the optimization technique should vary between each iteration.
    If the learning rate is too small, the optimization will need to be run a lot
    of times. If it's too big, then the optimization might fail, generating worse
    results than the previous iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Making recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have successfully built and trained our model, we can ask it for
    recommendations. These are provided by the `recommend` function, which takes an
    instance of a recommender, a user ID (from the ones available in the training
    matrix), the number of recommendations, and an array of books ID from which to
    make recommendations as its arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: With this line of code, we retrieve the recommendations for the user with the
    recommender ID `1`, which corresponds to the `UserID` `277427` in the original
    dataset. We're asking for up to `20` recommendations that have been picked from
    all the available books.
  prefs: []
  type: TYPE_NORMAL
- en: 'We get back an array of a `Pair` of book IDs and recommendation scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Testing the recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, our machine learning-based recommender system is ready. It will provide
    a significant boost in user experience for any bookshop, for sure. But before
    we start advertising it, we should make sure that it's reliable. Remember that
    we put aside 10% of our dataset for testing purposes. The idea is to compare the
    recommendations with actual ratings from the test data to see what degree of similarity
    exists between the two; that is, how many of the actual ratings from the dataset
    were in fact recommended. Depending on the data that's used for the training,
    you may want to test that both correct recommendations are made, but also that
    bad recommendations are not included (that is, the recommender does not suggest
    items that got low ratings, indicating a dislike). Since we only used ratings
    of 8, 9, and 10, we won't check if low-ranked recommendations were provided. We'll
    just focus on checking how many of the recommendations are actually part of the
    user's data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the test data uses the original user and profile IDs, and our recommender
    uses the normalized, sequential IDs, we''ll need a way to convert the data between
    the two. We already have the `user_mappings` and `book_mappings` dictionaries,
    which map from the original IDs to recommender IDs. However, we''ll also need
    the reverse. So, let''s start by defining a helper function for reversing a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This is simple, but very useful—we can now use this function to look up the
    original IDs based on the recommender IDs. For instance, if we want to test the
    recommendations for user `1`, we''ll need to retrieve this user''s actual ratings,
    so we''ll need the original ID. We can easily get it with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The same applies to the books mappings—for instance, the recommendation with
    ID `5081` corresponds to ISBN `981013004X` from the original dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'All right, let''s check the test data that we put aside for `UserID` `277427`
    (recommender user `1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21d47dc0-094c-4399-9181-b28e96e92e49.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'To check for recommended versus actually rated profiles, the easiest approach
    is to intersect the vector of recommendations with the vector of ratings. So,
    the first thing to do is put the test ratings into a vector, out of the `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We just select the ISBN column data, for all the rows, as an `Array`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Doing the same for the recommendations is a bit more involved. Also, since
    I expect we''ll want to test with various recommender settings and with different
    numbers of recommendations, it''s best to define a function that converts the
    recommendations to a vector of ISBNs, so that we can easily reuse the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The `recommendations_to_books` function takes the vector of `id => score` pairs
    generated by the recommender as its only argument and converts it into a vector
    of original ISBNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The `recommendations_to_books` function outputs the ISBNs for the `20` recommended
    books.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have all of the pieces to check recommendations versus ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We use the intersect function to check what elements from the first vector—the
    list of books we put away for testing—also show up in the second vector, that
    is, the recommendations. We had to ask for `500` recommendations as the chances
    of hitting one of the eight test books in a pool of 9,055 books were very slim.
    This is due to the fact that we worked with very little data, but in a production
    environment and potentially billions of rows, we would get a lot more overlapping
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what the top five recommendations were:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'In an IJulia Notebook, we can even look at the covers, thus rendering a small
    piece of HTML using the cover''s URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60e8823b-ff1e-4113-b583-1d6a9e89ca3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Excellent! We did a great job. We tamed a very complex dataset, performed advanced
    analysis, and then we optimized it for usage in our recommender. We then successfully
    trained our recommender and used it to generate book recommendations for our users.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and working with the `Recommendation` package is very straightforward,
    as I'm sure you've come to appreciate. Again, as in most data science projects,
    the ETL step was the most involved.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about hybrid recommender systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some clear advantages when using model-based recommenders. As mentioned
    already, scalability is one of the most important. Usually, the models are much
    smaller than the initial dataset, so that even for very large data samples, the
    models are small enough to allow efficient usage. Another benefit is the speed.
    The time required to query the model, as opposed to querying the whole dataset,
    is usually considerably smaller.
  prefs: []
  type: TYPE_NORMAL
- en: These advantages stem from the fact that the models are generally prepared offline,
    allowing for almost instantaneous recommendations. But since there's no such thing
    as free performance, this approach also comes with a few significant negatives—on
    one hand, it is less flexible, because building the models takes considerable
    time and resources, making the updates difficult and costly; on the other hand,
    because it does not use the whole dataset, the predictions can be less accurate.
  prefs: []
  type: TYPE_NORMAL
- en: As with everything, there's no silver bullet, and the best approach depends
    on the data you have at hand and the problem you need to solve. However, it doesn't
    always have to be memory-based versus model-based. Even more, it doesn't have
    to be just one recommender system. It turns out that multiple algorithms and approaches
    can be efficiently combined to compensate for the limitations of one type of recommender.
    Such architectures are called **hybrid**. Due to space limitations, we won't cover
    any implementations of hybrid recommender systems, but I want to give you an idea
    of the possible approaches. I'm just going to refer you to Robin Burke's classification
    from *Chapter 12* of *The Adaptive Web*, entitled *Hybrid Web Recommender Systems*.
    The whole chapter is available online for free at [https://www.researchgate.net/publication/200121024_Hybrid_Web_Recommender_Systems](https://www.researchgate.net/publication/200121024_Hybrid_Web_Recommender_Systems).
    If you're interested in this topic, I highly recommended it.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recommender systems represent a very active and dynamic field of study. They
    started initially as a marginal application of machine learning algorithms and
    techniques, but due to their practical business value, they have become mainstream
    in recent years. These days, almost all major programming languages provide powerful
    recommendations systems libraries—and all major online businesses employ recommenders
    in one form or another.
  prefs: []
  type: TYPE_NORMAL
- en: Julia is a great language for building recommenders due to its excellent performance.
    Despite the fact that the language is still young, we already have a couple of
    interesting packages to choose from.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you have a solid understanding of the model-based recommendation systems
    and of their implementation workflow—both on a theoretical and practical level.
    Plus, throughout our journey, we've also been exposed to more advanced data wrangling
    using `DataFrames`, an invaluable tool in Julia's data science arsenal.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll further improve our mastery of `DataFrames`, as we'll
    learn the secrets of metaprogramming in Julia, while developing an unsupervised
    machine learning system.
  prefs: []
  type: TYPE_NORMAL
