- en: Exploring Structure from Motion Using OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the notion of **Structure from Motion** (**SfM**),or
    better put, extracting geometric structures from images taken with a camera under
    motion, using OpenCV's API to help us. First, let's constrain the otherwise very
    b road approach to SfM using a single camera, usually called a **monocular** approach,
    and a discrete and sparse set of frames rather than a continuous video stream.
    These two constrains will greatly simplify the system we will sketch out in the
    coming pages, and help us understand the fundamentals of any SfM method. To implement
    our method, we will follow in the footsteps of Hartley and Zisserman (hereafter
    referred to as H&Z, for brevity), as documented in Chapters 9 through 12 of their
    seminal book *Multiple View Geometry in Computer Vision*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Structure from Motion concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the camera motion from a pair of images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reconstructing the scene
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reconstructing from many views
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refining the reconstruction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout the chapter, we assume the use of a calibrated camera, one that was
    calibrated beforehand. *Calibration* is a ubiquitous operation in Computer Vision,
    fully supported in OpenCV using command-line tools, and was discussed in previous
    chapters. We, therefore, assume the existence of the camera's **intrinsic parameters**
    embodied in the K matrix and distortionn coefficients vector - the outputs from
    the calibration process.
  prefs: []
  type: TYPE_NORMAL
- en: To make things clear in terms of language, from this point on, we will refer
    to a camera as a single view of the scene rather than to the optics and hardware
    taking the image. A camera has a 3D position in space (translation) and a 3D direction
    of view (orientation). In general, we describe this as the 6 **Degree of Freedom**
    (**DOF**) camera pose, sometimes referred to as **extrinsic parameters**. Between
    two cameras, therefore, there is a 3D translation element (movement through space)
    and a 3D rotation of the direction of view.
  prefs: []
  type: TYPE_NORMAL
- en: We will also unify the terms for the point in the scene, world, real, or 3D
    to be the same thing, a point that exists in our real world. The same goes for
    points in an image or 2D, which are points in the image coordinates of some real
    3D point that was projected on the camera sensor at that location and time.
  prefs: []
  type: TYPE_NORMAL
- en: In the chapter's code sections, you will notice references to *Multiple View
    Geometry in Computer Vision*, for example `// HZ 9.12`. This refers to equation
    number 12 of Chapter 9 of the book. Also, the text will include excerpts of code
    only; while the complete runnable code is included in the material accompanied
    with the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following flow diagram describes the process in the SfM pipeline we will
    implement. We begin by triangulating an initial reconstructed point cloud of the
    scene, using 2D features matched across the image set and a calculation of two
    camera poses. We then add more views to the reconstruction by matching more points
    into the forming point cloud, calculating camera poses and triangulating their
    matching points. In between, we will also perform bundle adjustment to minimize
    the error in the reconstruction. All the steps are detailed in the next sections
    of this chapter, with relevant code excerpts, pointers to useful OpenCV functions,
    and mathematical reasoning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05389_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Structure from Motion concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first discrimination we should make is the difference between stereo (or
    indeed any multiview) and 3D reconstruction using calibrated rigs and SfM. A rig
    of two or more cameras assumes that we already know the *motion* between the cameras,
    while in SfM, we don't know what this motion is and we wish to find it. Calibrated
    rigs, from a simplistic point of view, allow a much more accurate reconstruction
    of 3D geometry because there is no error in estimating the distance and rotation
    between the cameras, it is already known. The first step in implementing an SfM
    system is finding the motion between the cameras. OpenCV may help us in a number
    of ways to obtain this motion, specifically using the `findFundamentalMat` and
    `findEssentialMat` functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s think for one moment of the goal behind choosing an SfM algorithm. In
    most cases, we wish to obtain the geometry of the scene, for example, where objects
    are in relation to the camera and what their form is. Having found the motion
    between the cameras picturing the same scene, from a reasonably similar point
    of view, we would now like to reconstruct the geometry. In Computer Vision jargon,
    this is known as **triangulation**, and there are plenty of ways to go about it.
    It may be done by way of ray intersection, where we construct two rays-one from
    each camera''s center of projection and a point on each of the image planes. The
    intersection of these rays in space will, ideally, intersect at one 3D point in
    the real world that is imaged in each camera, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05389_04_30-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In reality, ray intersection is highly unreliable; H&Z recommend against it.
    This is because the rays usually do not intersect, making us fall back to using
    the middle point on the shortest segment connecting the two rays. OpenCV contains
    a simple API for a more accurate form of triangulation--the `triangulatePoints`
    function--so we do not need to code this part on our own.
  prefs: []
  type: TYPE_NORMAL
- en: After you learn how to recover 3D geometry from two views, we will see how we
    can incorporate more views of the same scene to get an even richer reconstruction.
    At that point, most SfM methods try to optimize the bundle of estimated positions
    of our cameras and 3D points by means of **Bundle Adjustment**, in the *Refinement
    of the reconstruction* section. OpenCV contains the means for Bundle Adjustment
    in its new Image Stitching Toolbox. However, the beauty of working with OpenCV
    and C++ is the abundance of external tools that can be easily integrated into
    the pipeline. We will, therefore, see how to integrate an external bundle adjuster,
    the Ceres nonlinear optimization package.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have sketched an outline of our approach to SfM using OpenCV, we
    will see how each element can be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the camera motion from a pair of images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we set out to actually find the motion between two cameras, let's examine
    the inputs and the tools we have at hand to perform this operation. First, we
    have two images of the same scene from (hopefully not extremely) different positions
    in space. This is a powerful asset, and we will make sure that we use it. As for
    tools, we should take a look at mathematical objects that impose constraints over
    our images, cameras, and the scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two very useful mathematical objects are the fundamental matrix (denoted by
    `F`) and the essential matrix (denoted by `E`), which impose a constraint over
    corresponding 2D points in two images of the scene. They are mostly similar, except
    that the essential matrix is assuming usage of calibrated cameras; this is the
    case for us, so we will choose it. OpenCV allows us to find the fundamental matrix
    via the `findFundamentalMat` function and the essential matrix via the `findEssentialMatrix`
    function. Finding the essential matrix can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This function makes use of matching points in the left-hand side image, `leftPoints`,
    and right-hand side image, `rightPoints`, which we will discuss shortly, as well
    as two additional pieces of information from the camera''s calibration: the focal
    length, `focal`, and principal point, `pp`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The essential matrix E is a 3x3 sized matrix, which imposes the following constraint
    on a point *x *in one image and a point and a point *x'' *corresponding image:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x''K^TEKx = 0*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *K* is the calibration matrix.
  prefs: []
  type: TYPE_NORMAL
- en: This is extremely useful, as we are about to see. Another important fact we
    use is that the essential matrix is all we need in order to recover the two cameras'
    positions from our images, although only up to an arbitrary unit of scale. So,
    if we obtain the essential matrix, we know where each camera is positioned in
    space, and where it is looking. We can easily calculate the matrix if we have
    enough of those constraint equations, simply because each equation can be used
    to solve for a small part of the matrix. In fact, OpenCV internally calculates
    it using just five point-pairs, but through the **Random Sample Consensus algorithm
    (RANSAC)**, many more pairs can be used and they make for a more robust solution.
  prefs: []
  type: TYPE_NORMAL
- en: Point matching using rich feature descriptors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will make use of our constraint equations to calculate the essential
    matrix. To get our constraints, remember that for each point in image A, we must
    find a corresponding point in image B. We can achieve such a matching using OpenCV's
    extensive 2D feature-matching framework, which has greatly matured in the past
    few years.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction and descriptor matching is an essential process in Computer
    Vision, and is used in many methods to perform all sorts of operations, for example,
    detecting the position and orientation of an object in an image or searching a
    big database of images for similar images through a given query. In essence, *feature
    extraction* means selecting points in the image that would make for good features
    and computing a descriptor for them. A *descriptor* is a vector of numbers that
    describes the surrounding environment around a feature point in an image. Different
    methods have different length and data types for their descriptor vectors. **Descriptor
    Matching** is the process of finding a corresponding feature of one set in another
    using its descriptor. OpenCV provides very easy and powerful methods to support
    feature extraction and matching.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine a very simple feature extraction and matching scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You may have already seen similar OpenCV code, but let''s review it quickly.
    Our goal is to obtain three elements: feature points for two images, descriptors
    for them, and a matching between the two sets of features. OpenCV provides a range
    of feature detectors, descriptor extractors, and matchers. In this simple example,
    we use the ORB class to get both the 2D location of **Oriented BRIEF (ORB)**(where,
    **BRIEF** stands for **Binary Robust Independent Elementary Features**) feature
    points and their respective descriptors. ORB may be preferred over traditional
    2D features such as the **Speeded-Up Robust Features (SURF)** or **Scale Invariant
    Feature Transform (SIFT)** because it is unencumbered with intellectual property
    and shown to be faster to detect, compute, and match.'
  prefs: []
  type: TYPE_NORMAL
- en: We use a *bruteforce* binary matcher to get the matching, which simply matches
    two feature sets by comparing each feature in the first set to each feature in
    the second set (hence the phrasing *bruteforce*).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following image, we will see a matching of feature points on two images
    from the Fountain P11 sequence can be found at [h t t p ://c v l a b . e p f l
    . c h /~s t r e c h a /m u l t i v i e w /d e n s e M V S . h t m l](http://cvlab.epfl.ch/~strecha/multiview/denseMVS.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05389_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Practically, raw matching like we just performed is good only up to a certain
    level, and many matches are probably erroneous. For that reason, most SfM methods
    perform some form of filtering on the matches to ensure correctness and reduce
    errors. One form of filtering, which is built into OpenCV's brute-force matcher,
    is **cross-check filtering**. That is, a match is considered true if a feature
    of the first image matches a feature of the second image, and the reverse check
    also matches the feature of the second image with the feature of the first image.
    Another common filtering mechanism, used in the provided code, is to filter based
    on the fact that the two images are of the same scene and have a certain stereo-view
    relationship between them. In practice, the filter tries to robustly calculate
    the fundamental or essential matrix which we will learn about in the *Finding
    camera matrices* section and retain those feature pairs that correspond with this
    calculation with small errors.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to using rich features, such as ORB, is to use **optical flow**.
    The following information box provides a short overview of optical flow. It is
    possible to use optical flow instead of descriptor matching to find the required
    point matching between two images, while the rest of the SfM pipeline remains
    the same. OpenCV recently extended its API to get the flow field from two images
    and now it is faster and more powerful.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optical flow** is the process of matching selected points from one image
    to another, assuming both images are part of a sequence and relatively close to
    one another. Most optical flow methods compare a small region, known as the **search
    window** or patch, around each point from *image A* to the same area in *image
    B*. Following a very common rule in Computer Vision, called the **brightness constancy
    constraint** (and other names), the small patches of the image will not change
    drastically from one image to the other, and therefore the magnitude of their
    subtraction should be close to zero. In addition to matching patches, newer methods
    of optical flow use a number of additional methods to get better results. One
    is using image pyramids, which are smaller and smaller resized versions of the
    image, which allow for working *from-coarse-to-fine*, a very well-used trick in
    Computer Vision. Another method is to define global constraints on the flow field,
    assuming that the points close to each other move together in the same direction.
    A more in-depth review of optical flow methods in OpenCV can be found in a chapter
    named *Developing Fluid Wall Using the Microsoft Kinect* which is available on
    the Packt website.'
  prefs: []
  type: TYPE_NORMAL
- en: Finding camera matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have obtained matches between keypoints, we can calculate the essential
    matrix. However, we must first align our matching points into two arrays, where
    an index in one array corresponds to the same index in the other. This is required
    by the `findEssentialMat` function as we''ve seen in the *Estimating Camera Motion *section.
    We would also need to convert the `KeyPoint` structure to a `Point2f` structure.
    We must pay special attention to the `queryIdx` and `trainIdx` member variables
    of `DMatch`, the OpenCV struct that holds a match between two keypoints, as they
    must align with the way we used the `DescriptorMatcher::match()` function. The
    following code section shows how to align a matching into two corresponding sets
    of 2D points, and how these can be used to find the essential matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We may, later, use the `status` binary vector to prune those points that align
    with the recovered essential matrix. Look at the following image for an illustration
    of point matching after pruning. The red arrows mark feature matches that were
    removed in the process of finding the matrix, and the green arrows are feature
    matches that were retained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05389_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we are ready to find the camera matrices. This process is described at
    length in a chapter of H&Z''s book; however, the new OpenCV 3 API makes things
    very easy for us by introducing the `recoverPose` function. First, we will briefly
    examine the structure of the camera matrix we are going to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05389_04_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the model for our camera pose, which consists of two elements: rotation
    (denoted by **R**) and translation (denoted by **t**). The interesting thing is
    that it holds a very essential equation: *x = PX*, where *x* is a 2D point on
    the image and *X* is a 3D point in space. There is more to it, but this matrix
    gives us a very important relationship between the image points and the scene
    points. So, now that we have a motivation for finding the camera matrices, we
    will see how it can be done. The following code section shows how to decompose
    the essential matrix into the rotation and translation elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Very simple. Without going too deeply into the mathematical interpretation,
    this conversion of the essential matrix to rotation and translation is possible
    because the essential matrix was originally composed by these two elements. Strictly
    for satisfying our curiosity, we can look at the following equation for the essential
    matrix, which appears in the literature: *E=[t][x]R*. We see it is composed of
    (some form of) a translation element *t* and a rotational element *R*.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that a *cheirality check* is internally performed in the `recoverPose`
    function. The cheirality check makes sure that all triangulated 3D points are
    *in front* of the reconstructed camera. H&Z show that camera matrix recovery from
    the essential matrix has in fact four possible solutions, but the only correct
    solution is the one that will produce triangulated points in front of the camera,
    hence the need for a cheirality check. We will learn about triangulation and 3D
    reconstruction in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note what we just did only gives us one camera matrix, and for triangulation,
    we require two camera matrices. This operation assumes that one camera matrix
    is fixed and canonical (no rotation and no translation, placed at the *world origin*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05389_04_19.png)'
  prefs: []
  type: TYPE_IMG
- en: The other camera that we recovered from the essential matrix has moved and rotated
    in relation to the fixed one. This also means that any of the 3D points that we
    recover from these two camera matrices will have the first camera at the world
    origin point (0, 0, 0). The assumption of a canonical camera is just how `cv::recoverPose`
    works; however in other situations, the *origin* camera pose matrix may be different
    than the canonical and still be valid for 3D points' triangulation, as we will
    see later when we will not use `cv::recoverPose` to get a new camera pose matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'One more thing we can think of adding to our method is error checking. Many
    times, the calculation of an essential matrix from point matching is erroneous,
    and this affects the resulting camera matrices. Continuing to triangulate with
    faulty camera matrices is pointless. We can install a check to see if the rotation
    element is a valid rotation matrix. Keeping in mind that rotation matrices must
    have a determinant of 1 (or -1), we can simply do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Think of `EPS` (from Epsilon) as a very small number that helps us cope with
    numerical calculation limits of our CPU. In reality, we may define the following
    in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now see how all these elements combine into a function that recovers
    the `P` matrices. First, we will introduce some convenience data structures and
    type shorthand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can write the camera matrix finding function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we have the two cameras that we need in order to reconstruct
    the scene. The canonical first camera in the `Pleft` variable, and the second
    camera we calculated form the essential matrix in the `Pright` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the image pair to use first
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given we have more than just two image views of the scene, we must choose which
    two views we will start the reconstruction from. In their paper, *Snavely et al.* suggest
    to picking the two views that have the least number of **homography** inliers.
    A homography is a relationship between two images or sets of points that lie on
    a plane; the **homography matrix** defines the transformation from one plane to
    another. In case of an image or a set of 2D points, the homography matrix is of
    size 3x3.
  prefs: []
  type: TYPE_NORMAL
- en: When *Snavely et al.* look for the lowest inlier ratio, they essentially suggest
    that you calculate the homography matrix between all pairs of images and pick
    the pair whose points mostly do not correspond with the homography matrix. This
    means that the geometry of the scene in these two views is not planar, or at least,
    not the same plane in both views, which helps when doing 3D reconstruction. For
    reconstruction, it is best to look at a complex scene with non-planar geometry,
    with things closer and farther away from the camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how to use OpenCV''s `findHomography` function
    to count the number of inliers between two views whose features were already extracted
    and matched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to perform this operation on all pairs of image views in our
    bundle and sort them based on the ratio of homography inliers to outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `std::map<float, ImagePair>` will internally sort the pairs based
    on the map''s key: the inliers ratio. We then simply need to traverse this map
    from the beginning to find the image pair with least inlier ratio, and if that
    pair cannot be used, we can easily skip ahead to the next pair. The next section
    will reveal how we use these cameras pair to obtain a 3D structure of the scene.'
  prefs: []
  type: TYPE_NORMAL
- en: Reconstructing the scene
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we look into the matter of recovering the 3D structure of the scene from
    the information we have acquired so far. As we had done before, we should look
    at the tools and information we have at hand to achieve this. In the preceding
    section, we obtained two camera matrices from the essential matrix; we already
    discussed how these tools would be useful for obtaining the 3D position of a point
    in space. Then, we can go back to our matched point pairs to fill in our equations
    with numerical data. The point pairs will also be useful in calculating the error
    we get from all our approximate calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the time to see how we can perform triangulation using OpenCV. Luckily,
    OpenCV supplies us with a number of functions that make this process easy to implement:
    `triangulatePoints`, `undistortPoints`, and `convertPointsFromHomogeneous`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember we had two key equations arising from the 2D point matching and *P*
    matrices: *x=PX* and *x''= P''X*, where *x* and *x''* are matching 2D points and
    X is a real-world 3D point imaged by the two cameras. If we examine these equations,
    we will see that the x vector that represents a 2D point should be of size (*3x1*)
    and X that represents a 3D point should be (*4x1*). Both points received an extra
    entry in the vector; this is called **Homogeneous Coordinates**. We use these
    coordinates to streamline the triangulation process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation *x = PX* (where *x* is a 2D image point, *X* is a world 3D point,
    and *P* is a camera matrix) is missing a crucial element: the camera calibration
    parameters matrix, *K*. The matrix K is used to transform 2D image points from
    pixel coordinates to **normalized coordinates** (in the [-1, 1] range) removing
    the dependency on the size of the image in pixels, which is absolutely necessary.
    For example, a 2D point *x[1] = (160, 120)* in a 320x240 image, may transform
    to *x[1]'' = (0, 0)* under certain circumstances. To that end, we use the `undistortPoints`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to triangulate the normalized 2D image points into 3D world
    points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following image, we can see a triangulation result of two images out
    of the Fountain P-11 sequence at [http://cvlabwww.epfl.ch/data/multiview/denseMVS.html](http://cvlabwww.epfl.ch/data/multiview/denseMVS.html).
    The two images at the top are the original two views of the scene, and the bottom
    pair is the view of the reconstructed point cloud from the two views, including
    the estimated cameras looking at the fountain. We can see how the right-hand side
    section of the red brick wall was reconstructed, and also the fountain that protrudes
    from the wall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05389_04_26.png)'
  prefs: []
  type: TYPE_IMG
- en: However, as we discussed earlier, we have an issue with the reconstruction being
    only up to scale. We should take a moment to understand what up to scale means.
    The motion we obtained between our two cameras is going to have an arbitrary unit
    of measurement that is, it is not in centimeters or inches, but simply a given
    unit of scale. Our reconstructed cameras we will be one unit of scale distance
    apart. This has big implications, should we decide to recover more cameras later,
    as each pair of cameras will have their own units of scale, rather than a common
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now discuss how the error measure that we set up may help us in finding
    a more robust reconstruction. First, we should note that reprojection means we
    simply take the triangulated 3D point and reimage it on a camera to get a reprojected
    2D point, we then compare the distance between the original 2D point and the reprojected
    2D point. If this distance is large, this means we may have an error in triangulation,
    so we may not want to include this point in the final result. Our global measure
    is the average reprojection distance and may give us a hint to how our triangulation
    performed overall. High average reprojection rates may point to a problem with
    the *P* matrices, and therefore a possible problem with the calculation of the
    essential matrix or the matched feature points. To reproject points, OpenCV offers
    the `projectPoints` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will take a look at recovering more cameras looking at the same scene,
    and combining the 3D reconstruction results.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstruction from many views
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to recover the motion and scene geometry from two cameras,
    it would seem simple to get the parameters of additional cameras and more scene
    points simply by applying the same process. This matter is in fact not so simple,
    as we can only get a reconstruction that is upto scale, and each pair of pictures
    has a different scale.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of ways to correctly reconstruct the 3D scene data from multiple
    views. One way to achieve **camera pose estimation **or **camera resectioning**,
    is the **Perspective N-Point**(**PnP**) algorithm, where we try to solve for the
    position of a new camera using *N* 3D scene points, which we have already found
    and their respective 2D image points. Another way is to triangulate more points
    and see how they fit into our existing scene geometry; this will tell us the position
    of the new camera by means of **point cloud registration**. In this section, we
    will discuss using OpenCV's `solvePnP` functions that implements the first method.
  prefs: []
  type: TYPE_NORMAL
- en: The first step we choose in this kind of reconstruction, incremental 3D reconstruction
    with camera resection, is to get a baseline scene structure. As we will look for
    the position of any new camera based on a known structure of the scene, we need
    to find an initial structure to work with. We can use the method we previously
    discussed-for example, between the first and second frames, to get a baseline
    by finding the camera matrices (using the `findCameraMatricesFromMatch` function)
    and triangulate the geometry (using `triangulatePoints`).
  prefs: []
  type: TYPE_NORMAL
- en: Having found an initial structure, we may continue; however, our method requires
    quite a bit of bookkeeping. First we should note that the `solvePnP` function
    needs aligned vectors of 3D and 2D points. Aligned vectors mean that the ith position
    in one vector aligns with the i^(th) position in the other. To obtain these vectors
    we need to find those points among the 3D points that we recovered earlier, which
    align with the 2D points in our new frame. A simple way to do this is to attach,
    for each 3D point in the cloud, a vector denoting the 2D points it came from.
    We can then use feature matching to get a matching pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s introduce a new structure for a 3D point as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It holds, on top of the 3D point, an index to the 2D point inside the vector
    of 2D points that each frame has, which had contributed to this 3D point. The
    information for `Point3DInMap::originatingViews` must be initialized when triangulating
    a new 3D point, recording which cameras were involved in the triangulation. We
    can then use it to trace back from our 3D point cloud to the 2D point in each
    frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add some convenience definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see how to get aligned 2D-3D point vectors to use with `solvePnP`.
    The following code segment illustrates the process of finding 2D points in a new
    image from the existing 3D point cloud augmented with the originating 2D views.
    Simply put, the algorithm scans the existing 3D points in the cloud, looks at
    their originating 2D points, and tries to find a match (via the feature descriptors)
    to 2D points in the new image. If such a match is found, it may indicate that
    this 3D point also appears in the new image at a specific 2D point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have aligned the pairing of 3D points in the scene to the 2D points
    in a new frame, and we can use them to recover the camera position as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are using the `solvePnPRansac` function rather than the `solvePnP`
    function as it is more robust to outliers. Now that we have a new `P` matrix,
    we can simply use the `triangulatePoints` function as we did earlier and populate
    our point cloud with more 3D points.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following image, we see an incremental reconstruction of the Fountain-P11
    scene at [http://cvlabwww.epfl.ch/data/multiview/denseMVS.html](http://cvlabwww.epfl.ch/data/multiview/denseMVS.html),
    starting from the fourth image. The top-left image is the reconstruction after
    four images were used; the participating cameras are shown as red pyramids with
    a white line showing the direction. The other images show how more cameras add
    more points to the cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05389_04_27.png)'
  prefs: []
  type: TYPE_IMG
- en: Refinement of the reconstruction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important parts of an SfM method is refining and optimizing
    the reconstructed scene, also known as the process of **Bundle Adjustment** (**BA**).
    This is an optimization step where all the data we gathered is fitted to a monolithic
    model. Both the position of the recovered 3D points and the positions of the cameras
    are optimized, so re-projection errors are minimized. In other words, recovered
    3D points that are re-projected on the image are expected to lie close to the
    position of originating 2D feature points that generated them. The BA process
    we use will try to minimize this error for all 3D points together, making for
    a very big system of simultaneous linear equations with on the order of thousands
    of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We will implement a BA algorithm using the **Ceres** library, a well-known optimization
    package from Google. Ceres has built-in tools to help with BA, such as automatic
    differentiation and many flavors of linear and nonlinear optimization schemes,
    which result in less code and more flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: To make things simple and easy to implement, we will make a few assumptions,
    whereas in a real SfM system, these things cannot be neglected. Firstly, we will
    assume a simple intrinsic model for our cameras, specifically that the focal length
    in *x* and *y* is the same and the center of projection is exactly the middle
    of the image. We further assume that all cameras share the same intrinsic parameters,
    meaning that the same camera takes all the images in the bundle with the exact
    configuration (for example, zoom). These assumptions greatly reduce the number
    of parameters to optimize, which in turn makes the optimization not only easier
    to code but also faster to converge.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we will model the *error function*, sometimes also called the **cost
    function**, which is, simply put, the way the optimization knows how good the
    new parameters are and also which way to go to get even better parameters. We
    can write the following functor that makes use of Ceres'' Auto Differentiation
    mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This functor calculates the deviation a 3D point has from its originating 2D
    point by re-projecting it using simplified extrinsic and intrinsic camera parameters.
    The error in *x* and *y* is saved as the residual, which guides the optimization.
  prefs: []
  type: TYPE_NORMAL
- en: There is quite a bit of additional code that goes into the BA implementation,
    but it primarily handles bookkeeping of cloud 3D points, originating 2D points,
    and their respective cameras. The readers may wish to review how this is done
    in the code attached to the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows the effects of BA. The two images on the left are
    the points of the point cloud before adjustment from two perspectives, and the
    images on the right show the optimized cloud. The change is quite dramatic, and
    many misalignments between points triangulated from different views are now mostly
    consolidated. We can also notice how the adjustment created a far better reconstruction
    of flat surfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05389_04_28.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the example code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can find the example code for SfM with the supporting material of this book.
    We will now see how we can build, run, and make use of it. The code makes use
    of **CMake**, a cross-platform build environment similar to Maven or SCons. We
    should also make sure that we have all the following prerequisites to build the
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV v3.0 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ceres v1.11 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boost v1.54 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we must set up the build environment. To that end, we may create a folder
    named `build` in which all build-related files will go; we will now assume that
    all command-line operations are within the `build/` folder, although the process
    is similar (up to the locations of the files) even if not using the `build` folder.
    We should also make sure that CMake can find boost and Ceres.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are using Windows as the operating system, we can use Microsoft Visual
    Studio to build; therefore, we should run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If we are using Linux, Mac OS, or another Unix-like operating system, we execute
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If we prefer to use XCode on Mac OS, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: CMake also has the ability to build macros for Eclipse, Codeblocks, and more.
  prefs: []
  type: TYPE_NORMAL
- en: After CMake is done creating the environment, we are ready to build. If we are
    using a Unix-like system, we can simply execute the `make` utility, else we should
    use our development environment's building process.
  prefs: []
  type: TYPE_NORMAL
- en: After the build has finished, we should be left with an executable named `ExploringSfM`,
    which runs the SfM process. Running it with no arguments
  prefs: []
  type: TYPE_NORMAL
- en: 'will result in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: To execute the process over a set of images, we should supply a location on
    the drive to find image files. If a valid location is supplied, the process should
    start and we should see the progress and debug information on the screen. If no
    errors arise, the process will end with a message stating that the point cloud
    that arises from the images was saved to PLY files, which can be opened in most
    3D editing software.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how OpenCV v3 can help us approach Structure from Motion
    in a manner that is both simple to code and simple to understand. OpenCV v3's
    new API contains a number of useful functions and data structures that make our
    lives easier and also assist in a cleaner implementation.
  prefs: []
  type: TYPE_NORMAL
- en: However, the state-of-the-art SfM methods are far more complex. There are many
    issues we choose to disregard in favor of simplicity, and plenty more error examinations
    that are usually in place. Our chosen methods for the different elements of SfM
    can also be revisited. For one, H&Z propose a highly accurate triangulation method
    that minimizes the reprojection error in the image domain. Some methods even use
    the N-view triangulation once they understand the relationship between the features
    in multiple images.
  prefs: []
  type: TYPE_NORMAL
- en: If we would like to extend and deepen our familiarity with SfM, we will certainly
    benefit from looking at other open source SfM libraries. One particularly interesting
    project is libMV, which implements a vast array of SfM elements that may be interchanged
    to get the best results. There is a great body of work from University of Washington
    that provides tools for many flavors of SfM (Bundler and VisualSfM). This work
    inspired an online product from Microsoft called **PhotoSynth** and **123D Catch**
    from Adobe. There are many more implementations of SfM readily available online,
    and one must only search to find quite a lot of them.
  prefs: []
  type: TYPE_NORMAL
- en: Another important relationship we have not discussed in depth is that of SfM
    and Visual Localization and Mapping, better known as **Simultaneous Localization
    and Mapping (SLAM)** methods. In this chapter, we dealt with a given dataset of
    images and a video sequence, and using SfM is practical in those cases; however,
    some applications have no prerecorded dataset and must bootstrap the reconstruction
    on the fly. This process is better known as **Mapping**, and it is done while
    we are creating a 3D map of the world, using feature matching and tracking in
    2D, and after triangulation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how OpenCV can be used for extracting license
    plate numbers from images, using various techniques in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Hartley, Richard, and Andrew Zisserman, Multiple View Geometry in Computer
    Vision, Cambridge University Press, 2003*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hartley, Richard I., and Peter Sturm; Triangulation, Computer Vision and image
    understanding 68.2 (1997): 146-157*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Snavely, Noah, Steven M. Seitz, and Richard Szeliski; Photo Tourism: Exploring
    Photo Collections in 3D, ACM Transactions on Graphics (TOG). Vol. 25\. No. 3\.
    ACM, 2006*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Strecha, Christoph, et al, On Benchmarking Camera Calibration and Multi-view
    Stereo for High Resolution Imagery, IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR) 2008*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[h t t p ://c v l a b w w w . e p f l . c h /d a t a /m u l t i v i e w /d
    e n s e M V S . h t m l h t t p s ://d e v e l o p e r . b l e n d e r . o r g
    /t a g /l i b m v /](http://cvlabwww.epfl.ch/data/multiview/denseMVS.htmlhttps://developer.blender.org/tag/libmv/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[h t t p ://c c w u . m e /v s f m /](http://ccwu.me/vsfm/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[h t t p ://w w w . c s . c o r n e l l . e d u /~s n a v e l y /b u n d l
    e r /](http://www.cs.cornell.edu/~snavely/bundler/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[h t t p ://p h o t o s y n t h . n e t](http://photosynth.net)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[h t t p ://e n . w i k i p e d i a . o r g /w i k i /S i m u l t a n e o u
    s _ l o c a l i z a t i o n _ a n d _ m a p p i n g](http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[h t t p ://w w w . c m a k e . o r g](http://www.cmake.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[h t t p ://c e r e s - s o l v e r . o r g](http://ceres-solver.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[h t t p ://w w w . 123d a p p . c o m /c a t c h](http://www.123dapp.com/catch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
