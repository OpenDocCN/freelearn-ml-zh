<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Feature Understanding – What&amp;#x27;s in My Dataset?</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Finally! We can start to jump into some real data, some real code, and some real results. </span><span>Specifically, we will be diving deeper into the following ideas:</span><br/></p>
<ul>
<li>Structured versus unstructured data</li>
<li><span>Quantitative versus qualitative data</span></li>
<li><span>The four levels of data</span></li>
<li><span>Exploratory data analysis and data visualizations</span></li>
<li>Descriptive statistics</li>
</ul>
<p>Each of these topics will give us a better sense of the data given to us, what is present within the dataset, what is not present within the dataset, and some basic notions on how to proceed from there.</p>
<p>If you're familiar with, <em>Principles of Data Science</em>, much of this echoes <em>Chapter 2, Types of Data</em> of that book. That being said, in this chapter, we will specifically look at our data less from a holistic standpoint, and more from a machine-learning standpoint.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The structure, or lack thereof, of data</h1>
                </header>
            
            <article>
                
<p>When given a new dataset, it is first important to recognize whether or not your data is structured or unstructured<em>:</em></p>
<ul>
<li>
<p><strong>Structured (organized) data</strong>: <span>Data that can be broken down into observations and characteristics. They are generally organized using a tabular method</span> <span>(where rows are observations and columns are characteristics).</span></p>
</li>
<li>
<p><strong>Unstructured (unorganized) data</strong><span>: Data that exists as a free-flowing entity and does not follow standard organizational hierarchy such as tabularity. Often, unstructured data appears to us as a <em>blob</em> of data, or as a single characteristic (column).</span></p>
</li>
</ul>
<p><span>A few examples that highlight the difference between structured and unstructured data are as follows:</span></p>
<ul>
<li>
<p><span>Data that exists in a raw free-text form, including server logs and tweets, are unstructured</span></p>
</li>
<li>
<p>Meteorological data, as reported by scientific instruments in precise movements, would be considered highly structured as they exist in a tabular row/column structure</p>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An example of unstructured data – server logs</h1>
                </header>
            
            <article>
                
<div class="column">
<p>As an example of unstructured data, we have pulled some sample server logs from a public source and included them in a text document. We can take a glimpse of what this unstructured data looks like, so we can recognize it in the future:</p>
<pre># Import our data manipulation tool, Pandas<br/>import pandas as pd<br/># Create a pandas DataFrame from some unstructured Server Logs<br/>logs = pd.read_table('../data/server_logs.txt', header=None, names=['Info'])<br/><br/># header=None, specifies that the first line of data is the first data point, not a column name<br/># names=['Info] is me setting the column name in our DataFrame for easier access</pre>
<p>We created a DataFrame in pandas called <span><kbd>logs</kbd> </span>that hold our server logs. To take a look, let's call the <kbd>.head()</kbd> method to look at the first few rows:</p>
</div>
<pre class="mce-root"># Look at the first 5 rows<br/>logs.head()<br/></pre>
<p>This will show us a table of the first 5 rows in our logs DataFrame as follows:</p>
<table>
<tbody>
<tr>
<td>
<p><strong>Info</strong></p>
</td>
<td/>
</tr>
<tr>
<td>
<p>0</p>
</td>
<td>
<p>64.242.88.10 - - [07/Mar/2004:16:05:49 -0800] ...</p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>64.242.88.10 - - [07/Mar/2004:16:06:51 -0800] ...</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>64.242.88.10 - - [07/Mar/2004:16:10:02 -0800] ...</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>64.242.88.10 - - [07/Mar/2004:16:11:58 -0800] ...</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>64.242.88.10 - - [07/Mar/2004:16:20:55 -0800] ...</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We can see in our logs that each row represents a single log and there is only a single column, the text of the log itself. Not exactly a characteristic or anything, just the raw log is taken directly from the server. This is a great example of unstructured data. Most often, data in the form of text is usually unstructured.</p>
<div class="packt_tip">It is im<span>portant</span> to recognize that most unstructured data can be transformed into structured data through a few manipulations, but this is something that we will tackle in the next chapter.</div>
<p>Most of the data that we will be working on the book will be structured. That means that there will be a sense of rows and columns. Given this, we can start to look at the types of values in the cells of our tabular data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quantitative versus qualitative data</h1>
                </header>
            
            <article>
                
<p><span>To accomplish our diagnoses of the various types of data, we will begin with the highest order of separation. When dealing with structured, tabular data (which we usually will be doing), the first question we generally ask ourselves is whether the values are of a numeric or categorical nature.</span></p>
<p><strong>Quantitative data </strong>are data that are numerical in nature. They should be measuring the quantity of something.</p>
<p><strong>Qualitative data</strong> are data that are categorical in nature. They should be describing the quality of something.</p>
<p>Basic examples:</p>
<ul>
<li>Weather measured as temperature in Fahrenheit or Celsius would be quantitative</li>
<li>Weather measured as cloudy or sunny would be qualitative</li>
<li>The name of a person visiting the White House would be qualitative</li>
<li>The amount of blood you donate at a blood drive is quantitative</li>
</ul>
<p>The first two examples show that we can describe similar systems using data from both the qualitative and quantitative side. In fact, in most datasets, we will be working with both qualitative and quantitative data.</p>
<p>Sometimes, data can, arguably, be either quantitative or qualitative. The ranking you would give a restaurant (one through five stars) could be considered quantitative or qualitative, for example. While they are numbers, the numbers themselves might also represent categories. For example, if the restaurant rating app asked you to rate the restaurant using a quantitative star system, then feasibly the restaurant's average ranking might be a decimal, like 4.71 stars, making the data quantitative. At the same time, if the app asked you if you <em>hated it</em>, <em>thought it was OK</em>, <em>liked it</em>, <em>loved it</em>, or <em>really loved it</em>, then these are now categories. As a result of these ambiguities between quantitative and qualitative data, we employ an even <span>deeper method</span> called the four levels of data. Before we do that, let's introduce our first dataset for the chapter and really solidify some examples of qualitative and quantitative data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Salary ranges by job classification</h1>
                </header>
            
            <article>
                
<p>Let's first do some import statements:</p>
<pre># import packages we need for exploratory data analysis (EDA)<br/># to store tabular data<br/>import pandas as pd<br/># to do some math<br/>import numpy as np <br/># a popular data visualization tool<br/>import matplotlib.pyplot as plt <br/># another popular data visualization tool<br/>import seaborn as sns<br/># allows the notebook to render graphics<br/>%matplotlib inline <br/># a popular data visualization theme<br/>plt.style.use('fivethirtyeight')</pre>
<p>And then, let's import our first dataset, which will explore salaries of different job titles in San Francisco. This dataset is available publicly and so you are encouraged to play around with it as much as you want:</p>
<pre># load in the data set<br/># https://data.sfgov.org/City-Management-and-Ethics/Salary-Ranges-by-Job-Classification/7h4w-reyq<br/>salary_ranges = pd.read_csv('../data/Salary_Ranges_by_Job_Classification.csv')<br/><br/># view the first few rows and the headers<br/>salary_ranges.head()</pre>
<p>Let us have a look at the following table to understand better:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>SetID</strong></p>
</td>
<td>
<p><strong>Job Code</strong></p>
</td>
<td>
<p><strong>Eff Date</strong></p>
</td>
<td>
<p><strong>Sal End Date</strong></p>
</td>
<td>
<p><strong>Salary SetID</strong></p>
</td>
<td>
<p><strong>Sal Plan</strong></p>
</td>
<td>
<p><strong>Grade</strong></p>
</td>
<td>
<p><strong>Step</strong></p>
</td>
<td>
<p><strong>Biweekly High Rate</strong></p>
</td>
<td>
<p><strong>Biweekly Low Rate</strong></p>
</td>
<td>
<p><strong>Union Code</strong></p>
</td>
<td>
<p><strong>Extended Step</strong></p>
</td>
<td>
<p><strong>Pay Type</strong></p>
</td>
</tr>
<tr>
<td>
<p>0</p>
</td>
<td>
<p>COMMN</p>
</td>
<td>
<p>0109</p>
</td>
<td>
<p>07/01/2009 12:00:00 AM</p>
</td>
<td>
<p>06/30/2010 12:00:00 AM</p>
</td>
<td>
<p>COMMN</p>
</td>
<td>
<p>SFM</p>
</td>
<td>
<p>00000</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>$0.00</p>
</td>
<td>
<p>$0.00</p>
</td>
<td>
<p>330</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>C</p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>COMMN</p>
</td>
<td>
<p>0110</p>
</td>
<td>
<p>07/01/2009 12:00:00 AM</p>
</td>
<td>
<p>06/30/2010 12:00:00 AM </p>
</td>
<td>
<p>COMMN</p>
</td>
<td>
<p>SFM</p>
</td>
<td>
<p>00000</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>$15.00</p>
</td>
<td>
<p>$15.00</p>
</td>
<td>
<p>323</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>D</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>COMMN</p>
</td>
<td>
<p>0111</p>
</td>
<td>
<p>07/01/2009 12:00:00 AM</p>
</td>
<td>
<p>06/30/2010 12:00:00 AM</p>
</td>
<td>
<p>COMMN</p>
</td>
<td>
<p>SFM</p>
</td>
<td>
<p>00000</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>$25.00</p>
</td>
<td>
<p>$25.00</p>
</td>
<td>
<p>323</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>D</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>COMMN</p>
</td>
<td>
<p>0112</p>
</td>
<td>
<p>07/01/2009 12:00:00 AM</p>
</td>
<td>
<p>06/30/2010 12:00:00 AM</p>
</td>
<td>
<p>COMMN</p>
</td>
<td>
<p>SFM</p>
</td>
<td>
<p>00000</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>$50.00</p>
</td>
<td>
<p>$50.00</p>
</td>
<td>
<p>323</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>D</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>COMMN</p>
</td>
<td>
<p>0114</p>
</td>
<td>
<p>07/01/2009 12:00:00 AM</p>
</td>
<td>
<p>06/30/2010 12:00:00 AM</p>
</td>
<td>
<p>COMMN</p>
</td>
<td>
<p>SFM</p>
</td>
<td>
<p>00000</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>$100.00</p>
</td>
<td>
<p>$100.00</p>
</td>
<td>
<p>323</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>M</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We can see that we have a bunch of columns, and some already start to jump out at us as being quantitative or qualitative. Let's get a sense of how many rows of data there are using the <kbd>.info()</kbd> command:</p>
<pre># get a sense of how many rows of data there are, if there are any missing values, and what data type each column has<br/>salary_ranges.info()<br/><br/><br/>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1356 entries, 0 to 1355
Data columns (total 13 columns):
SetID                 1356 non-null object
Job Code              1356 non-null object
Eff Date              1356 non-null object
Sal End Date          1356 non-null object
Salary SetID          1356 non-null object
Sal Plan              1356 non-null object
Grade                 1356 non-null object
Step                  1356 non-null int64
Biweekly High Rate    1356 non-null object
Biweekly Low Rate     1356 non-null object
Union Code            1356 non-null int64
Extended Step         1356 non-null int64
Pay Type              1356 non-null object
dtypes: int64(3), object(10)
memory usage: 137.8+ KB</pre>
<p>So, we have <kbd>1356 entries</kbd> (rows) and <kbd>13 columns</kbd>. The <kbd>.info()</kbd> command also tells us the number of <kbd>non-null</kbd> items in each column. This is important because missing data is by far one of the most common issues in feature engineering. Sometimes, we are working with datasets that are just incomplete. In pandas, we have many ways of figuring out if we are working with missing data, and many ways of dealing with them. A very quick and common way to count the number of missing values is to run:</p>
<pre><span># another method to check for missing values</span><br/><span>salary_ranges.isnull().sum()</span><br/><br/>SetID                 0
Job Code              0
Eff Date              0
Sal End Date          0
Salary SetID          0
Sal Plan              0
Grade                 0
Step                  0
Biweekly High Rate    0
Biweekly Low Rate     0
Union Code            0
Extended Step         0
Pay Type              0
dtype: int64</pre>
<p>So, we see we are not missing any pieces of data in this one, phew (for now). Moving on, let's run the <kbd>describe</kbd> method to check out some descriptive statistics of our quantitative columns (which we should have). Note that the <kbd>describe</kbd> method will default to describing quantitative columns, but will describe qualitative columns if there are no quantitative columns:</p>
<pre># show descriptive stats:<br/>salary_ranges.describe()</pre>
<p>Let us have a look at the following table for a better understanding here:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>Step</strong></p>
</td>
<td>
<p><strong>Union Code</strong></p>
</td>
<td>
<p><strong>Extended Step</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>count</strong></p>
</td>
<td>
<p><span>1356.000000</span></p>
</td>
<td>
<p><span>1356.000000</span></p>
</td>
<td>
<p><span>1356.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>mean</strong></p>
</td>
<td>
<p><span>1.294985</span></p>
</td>
<td>
<p><span>392.676991</span></p>
</td>
<td>
<p><span>0.150442</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>std</strong></p>
</td>
<td>
<p><span>1.045816</span></p>
</td>
<td>
<p><span>338.100562</span></p>
</td>
<td>
<p><span>1.006734</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>min</strong></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>25%</strong></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>21.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>50%</strong></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>351.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>75%</strong></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>790.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>max</strong></p>
</td>
<td>
<p><span>5.000000</span></p>
</td>
<td>
<p><span>990.000000</span></p>
</td>
<td>
<p><span>11.000000</span></p>
</td>
</tr>
</tbody>
</table>
<p>According to pandas, we only have three quantitative columns: <kbd>Step</kbd>, <kbd>Union Code</kbd>, and <kbd>Extended Step</kbd>. Let's ignore <kbd>Step</kbd> and <kbd>Extended Step</kbd> for now, and also notice that <kbd>Union Code</kbd> isn't really quantitative. While it is a number, it doesn't really represent a quantity of something, it's merely describing the union through a unique coding. So, we have some work to do here to even understand the features that we are more interested in. Most notably, let's say we wish to pull out a single quantitative column, the <kbd>Biweekly High Rate</kbd>, and a single qualitative column, <kbd>Grade</kbd> (the type of job):</p>
<pre>salary_ranges = salary_ranges[['Biweekly High Rate', 'Grade']]<br/>salary_ranges.head()</pre>
<p>The following is the result of the preceding code:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/0e3e0c58-4b55-44c1-b323-5e01abc6065b.png"/></div>
<p>To do some cleaning up on these columns, let's remove those dollar signs (<kbd>$</kbd>) from the salary rate and ensure that the columns are of the correct type. When working with quantitative columns, we generally want them to be integer or floats (floats are preferred), while qualitative columns are usually strings or Unicode objects:</p>
<pre># Rate has dollar signs in a few of them, we need to clean that up..<br/>salary_ranges['Biweekly High Rate'].describe()<br/><br/>count         1356
unique         593
top       $3460.00
freq            12
Name: Biweekly High Rate, dtype: object</pre>
<p>To clean up this column, let's use the map feature in pandas to efficiently map a function to an entire series of data:</p>
<pre># need to clean our Biweekly High columns to remove the dollar sign in order to visualize<br/>salary_ranges['Biweekly High Rate'] = salary_ranges['Biweekly High Rate'].map(lambda value: value.replace('$',''))<br/><br/># Check to see the '$' has been removed<br/>salary_ranges.head()</pre>
<p>The following table gives us a better understanding here:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>Biweekly High Rate</strong></p>
</td>
<td>
<p><strong>Grade</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p><span>0.00</span></p>
</td>
<td>
<p><span>00000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p><span>15.00</span></p>
</td>
<td>
<p><span>00000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p><span>25.00</span></p>
</td>
<td>
<p><span>00000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p><span>50.00</span></p>
</td>
<td>
<p><span>00000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p><span>100.00</span></p>
</td>
<td>
<p><span>00000</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>To finish our transformation of the <kbd>Biweekly High Rate</kbd> column, we will cast the whole thing as a <kbd>float</kbd>:</p>
<pre># Convert the Biweeky columns to float<br/>salary_ranges['Biweekly High Rate'] = salary_ranges['Biweekly High Rate'].astype(float)</pre>
<p>While we are casting, let's also cast the <kbd>Grade</kbd> column as a string:</p>
<pre># Convert the Grade columns to str<br/>salary_ranges['Grade'] = salary_ranges['Grade'].astype(str)<br/><br/># check to see if converting the data types worked<br/>salary_ranges.info()<br/><br/><br/>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1356 entries, 0 to 1355
Data columns (total 2 columns):
Biweekly High Rate    1356 non-null float64
Grade                 1356 non-null object
dtypes: float64(1), object(1)
memory usage: 21.3+ KB</pre>
<p>We see that we now have a total of:</p>
<ul>
<li>1,356 rows (like we started with)</li>
<li>Two columns (that we selected):
<ul>
<li><strong>Biweekly High Rate</strong>: A quantitative column that refers to the average high weekly salary for a specified department:
<ul>
<li>This column is quantitative because the the values are numerical in nature and describe the quantity of money that the person earns weekly</li>
<li>It is of type float, which we cast it to</li>
</ul>
</li>
<li><strong>Grade</strong>: The department that the salary is in reference to:<br/>
<ul>
<li>This column is definitely qualitative because the codes refer to a department and not a quantity of any kind</li>
<li>It is of type object, which is the type pandas will stipulate if it is a string</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>To break quantitative and qualitative data even further, let's dive into the four levels of data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The four levels of data</h1>
                </header>
            
            <article>
                
<p>We already know that we can identify data as being either qualitative or quantitative. But, from there, we can go further. The four levels of data are:</p>
<ul>
<li>The nominal level</li>
<li>The ordinal level</li>
<li>The interval level</li>
<li>The ratio level</li>
</ul>
<p>Each level comes with a varying level of control and mathematical possibilities. It is crucial to know which level data lives on because it will dictate the types of visualizations and operations you are allowed to perform.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The nominal level</h1>
                </header>
            
            <article>
                
<p><span>The first level of data, the</span> <span>nominal</span> <span>level, has the weakest structure. It</span> <span>consists of data that are purely described by name. Basic examples include blood type (A, O, AB), species of animal, or names of people. These types of data are all qualitative.</span></p>
<p><span>Some other examples include:</span></p>
<ul>
<li><span>In the <kbd>SF Job Salary</kbd> dataset, the</span> <kbd>Grade</kbd> c<span>olumn would be nominal</span></li>
<li>Given visitor logs of a company, the first and last names of the visitors would be nominal</li>
<li>Species of animals in a lab experiment <span>would be nominal</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mathematical operations allowed </h1>
                </header>
            
            <article>
                
<div class="page">
<div class="layoutArea">
<div class="column">
<div class="layoutArea">
<div class="column">
<p><span>At each level, we will describe briefly the type of math that is allowed, and more importantly, not allowed. At this level, we cannot perform any quantitative mathematical operations, such as addition or division. These would not make any sense. Due to the lack of addition and division, we obviously cannot find an average value at the nominal level. There is no average name or average job department.</span></p>
<p>We can, however, do basic counts using pandas' <kbd>value_counts</kbd> methods:</p>
<pre># Basic Value Counts of the Grade column<br/>salary_ranges['Grade'].value_counts().head()<br/><br/><br/>00000    61
07450    12
06870     9
07170     9
07420     9
Name: Grade, dtype: int64</pre></div>
</div>
</div>
</div>
</div>
<div>
<div>
<div>
<div class="container">
<div class="cell code_cell rendered selected">
<p class="output_subarea output_html rendered_html output_result">The most commonly occurring <kbd>Grade</kbd> is <kbd>00000</kbd>, meaning that that is our <strong>mode</strong> or most commonly occurring category. Because of our ability to count at the nominal level, graphs, like bar charts, are available to us:</p>
<pre># Bar Chart of the Grade column salary_ranges['Grade'].value_counts().sort_values(ascending=False).head(20).plot(kind='bar')</pre>
<p>The following is the result of the preceding code:</p>
<div class="output_area CDPAlignCenter CDPAlign"><img height="252" src="assets/2f219471-ae45-4ba2-9da7-789fd421dd6a.png" width="357"/></div>
</div>
</div>
</div>
</div>
</div>
<p>At the nominal level, we may also utilize pie charts:</p>
<pre class="mce-root"># Bar Chart of the Grade column as a pie chart (top 5 values only)<br/>salary_ranges['Grade'].value_counts().sort_values(ascending=False).head(5).plot(kind='pie')</pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="162" src="assets/60033570-ccb5-4bb3-a54f-98e6f1266cfc.png" width="279"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The ordinal level</h1>
                </header>
            
            <article>
                
<p>The nominal level provided us with much in the way of capabilities for further exploration. Moving one level up, we are now on the ordinal scale. The ordinal scale inherits all of the properties of the nominal level, but has important additional properties:</p>
<ul>
<li>Data <span>at the ordinal level can be <strong>naturally ordered</strong></span></li>
<li>This implies that some data values in the column can be considered better than or greater than others</li>
</ul>
<p>As with the nominal level, data at the ordinal level is still categorical in nature, even if numbers are used to represent the categories.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mathematical operations allowed</h1>
                </header>
            
            <article>
                
<p class="container"><span>We have a few new abilities to work with at the ordinal level compared to the nominal level. At the ordinal level, we may still do basic counts as we did at the nominal level, but we can also introduce comparisons and orderings into the mix. For this reason, we may utilize new graphs at this level. We may use bar and pie charts like we did at the nominal level, but because we now have ordering and comparisons, we can calculate medians and percentiles. With medians and percentiles, stem-and-leaf plots, as well as box plots, are possible.</span></p>
<div class="cell code_cell rendered selected">
<div class="output_wrapper CDPAlignLeft CDPAlign">
<p>Some examples of data at the ordinal level include:</p>
<ul>
<li>
<p>Using a Likert scale (rating something on a scale from one to ten, for example)</p>
</li>
<li>
<p>Grade levels on an exam (F, D, C, B, A)</p>
</li>
</ul>
<p>For a real-world example of data at the ordinal scale, let's bring in a new dataset. <span>This dataset holds key insights into how much people enjoy the San Francisco International Airport or SFO. This dataset is also publicly available on SF's open database (<a href="https://data.sfgov.org/Transportation/2013-SFO-Customer-Survey/mjr8-p6m5" target="_blank">https://data.sfgov.org/Transportation/2013-SFO-Customer-Survey/mjr8-p6m5</a>):</span></p>
</div>
</div>
<pre># load in the data set<br/>customer = pd.read_csv('../data/2013_SFO_Customer_survey.csv')</pre>
<p>This CSV has many, many columns:</p>
<pre><span>customer.shape</span><br/><br/>(3535, 95)</pre>
<p><kbd>95</kbd> columns, to be exact. For more information on the columns available for this dataset, check out the data dictionary on the website (<a href="https://data.sfgov.org/api/views/mjr8-p6m5/files/FHnAUtMCD0C8CyLD3jqZ1-Xd1aap8L086KLWQ9SKZ_8?download=true&amp;filename=AIR_DataDictionary_2013-SFO-Customer-Survey.pdf" target="_blank">https://data.sfgov.org/api/views/mjr8-p6m5/files/FHnAUtMCD0C8CyLD3jqZ1-Xd1aap8L086KLWQ9SKZ_8?download=true&amp;filename=AIR_DataDictionary_2013-SFO-Customer-Survey.pdf</a>)</p>
<p>For now, let's focus on a single column, <kbd>Q7A_ART</kbd>. As described by the publicly available data dictionary, <span><kbd>Q7A_ART</kbd> is about artwork and exhibitions. The possible choices are 0, 1, 2, 3, 4, 5, 6 and each number has a meaning:</span></p>
<ul>
<li><span><strong>1</strong>: Unacceptable</span></li>
<li><span><strong>2</strong>: Below Average</span></li>
<li><span><strong>3</strong>: Average</span></li>
<li><span><strong>4</strong>: Good</span></li>
<li><span><strong>5</strong>: Outstanding</span></li>
<li><span><strong>6</strong>: Have Never Used or Visited</span></li>
<li><span><strong>0</strong>: Blank</span></li>
</ul>
<p>We can represent it as follows:</p>
<pre>art_ratings = customer['Q7A_ART']<br/>art_ratings.describe()<br/><br/><br/>count    3535.000000
mean        4.300707
std         1.341445
min         0.000000
25%         3.000000
50%         4.000000
75%         5.000000
max         6.000000
Name: Q7A_ART, dtype: float64</pre>
<p><span>The pandas is considering the column numerical because it is full of numbers, however, we must remember that even though the cells' values are numbers, those numbers represent a category, and therefore this data belongs to the qualitative side, and more specifically, ordinal. If we remove the</span> <kbd>0</kbd> <span>and</span> <kbd>6</kbd> <span>category, we are left with five ordinal categories which basically resemble the star rating of restaurant ratings:</span></p>
<pre># only consider ratings 1-5<br/>art_ratings = art_ratings[(art_ratings &gt;=1) &amp; (art_ratings &lt;=5)]</pre>
<p>We will then cast the values as strings:</p>
<pre># cast the values as strings<br/>art_ratings = art_ratings.astype(str)<br/><br/>art_ratings.describe()<br/><br/>count     2656
unique       5
top          4
freq      1066
Name: Q7A_ART, dtype: object</pre>
<p>Now that we have our ordinal data in the right format, let's look at some visualizations:</p>
<pre class="mce-root"># Can use pie charts, just like in nominal level<br/>art_ratings.value_counts().plot(kind='pie')</pre>
<p class="mce-root">The following is the result of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="202" src="assets/ed1e4cc9-73d6-4fa4-87e7-77de7dec49fb.png" width="325"/></div>
<p>We can also visualize this as a bar chart as follows:</p>
<pre class="mce-root"># Can use bar charts, just like in nominal level<br/>art_ratings.value_counts().plot(kind='bar')</pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="183" src="assets/451880ea-7f27-4f98-8156-96c9d03af6ff.png" width="312"/></div>
<p>However, now we can also introduce box plots since we are at the ordinal level:</p>
<pre># Boxplots are available at the ordinal level<br/>art_ratings.value_counts().plot(kind='box')</pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="218" src="assets/9ce8981c-244b-4e42-9989-6368745ebcc0.png" width="349"/></div>
<p>This box plot would not be possible for the <kbd>Grade</kbd> column in the salary data, as finding a median would not be possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The interval level</h1>
                </header>
            
            <article>
                
<p><span>We are starting to cook with gas now. At the nominal and ordinal level, we were working with data that was qualitative in nature. There was data that did not describe a true quantity. At the interval level, we move away from this notion and move into quantitative data. At the interval data level, we are working with numerical data that not only has ordering like at the ordinal level, but also has meaningful differences between values. This means that at the interval level, not only may we order and compare values, we may also <strong>add</strong> and <strong>subtract</strong> values.</span></p>
<p>Example:</p>
<p>A classic example of data at the interval level is temperature. If it is 90 degrees in Texas, and 40 degrees in Alaska, then we may calculate a 90-40 = 50 degrees difference between the locations. This may seem like a very simple example, but thinking back on the last two levels, we have never had this amount of control over our data before.</p>
<p>Non-example:</p>
<p>A classic non-example of data that is not at the interval level are Likert scales. We have identified <span>Likert at the ordinal levels for their ability to be ordered, but it is important to notice that subtractions do not have a true consistent meaning. If we subtract a 5-3 on a Likert scale, the resulting 2 doesn't actually mean the number 2, nor does it represent the category 2. Thus, subtraction in a Likert scale is difficult.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mathematical operations allowed</h1>
                </header>
            
            <article>
                
<p>Remember, at the interval level, we have addition and subtraction to work with. This is a real game-changer. With the ability to add values together, we may introduce two familiar concepts, the <strong>arithmetic mean</strong> (referred to simply as the mean) and <strong>standard deviation</strong>. At the interval level, both of these are available to us. To see a great example of this, let's pull in a new dataset, one about climate change:</p>
<pre># load in the data set<br/>climate = pd.read_csv('../data/GlobalLandTemperaturesByCity.csv')<br/>climate.head()</pre>
<p>Let us have a look at the following table for a better understanding:</p>
<table>
<tbody>
<tr>
<td>
<p><strong>dt</strong></p>
</td>
<td>
<p><strong>AverageTemperature</strong></p>
</td>
<td>
<p><strong>AverageTemperatureUncertainty</strong></p>
</td>
<td>
<p><strong>City</strong></p>
</td>
<td>
<p><strong>Country</strong></p>
</td>
<td>
<p><strong>Latitude</strong></p>
</td>
<td>
<p><strong>Longitude</strong></p>
</td>
<td/>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p><span>1743-11-01</span></p>
</td>
<td>
<p><span>6.068</span></p>
</td>
<td>
<p><span>1.737</span></p>
</td>
<td>
<p><span>Århus</span></p>
</td>
<td>
<p><span>Denmark</span></p>
</td>
<td>
<p><span>57.05N</span></p>
</td>
<td>
<p><span>10.33E</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p><span>1743-12-01</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>Århus</span></p>
</td>
<td>
<p><span>Denmark</span></p>
</td>
<td>
<p><span>57.05N</span></p>
</td>
<td>
<p><span>10.33E</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p><span>1744-01-01</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>Århus</span></p>
</td>
<td>
<p><span>Denmark</span></p>
</td>
<td>
<p><span>57.05N</span></p>
</td>
<td>
<p><span>10.33E</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p><span>1744-02-01</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>Århus</span></p>
</td>
<td>
<p><span>Denmark</span></p>
</td>
<td>
<p><span>57.05N</span></p>
</td>
<td>
<p><span>10.33E</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p><span>1744-03-01</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p><span>Århus</span></p>
</td>
<td>
<p><span>Denmark</span></p>
</td>
<td>
<p><span>57.05N</span></p>
</td>
<td>
<p><span>10.33E</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>This dataset has 8.6 million rows, where each row quantifies the average temperature of cities around the world by the month, going back to the 18th century. Note that just by looking at the first five rows, we already have some missing values. Let's remove them for now in order to get a better look:</p>
<pre># remove missing values<br/>climate.dropna(axis=0, inplace=True)<br/><br/>climate.head() . # check to see that missing values are gone</pre>
<p>The following table gives us a better understanding here:</p>
<table>
<tbody>
<tr>
<td>
<p><strong>dt</strong></p>
</td>
<td>
<p><strong>AverageTemperature</strong></p>
</td>
<td>
<p><strong>AverageTemperatureUncertainty</strong></p>
</td>
<td>
<p><strong>City</strong></p>
</td>
<td>
<p><strong>Country</strong></p>
</td>
<td>
<p><strong>Latitude</strong></p>
</td>
<td>
<p><strong>Longitude</strong></p>
</td>
<td/>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p><span>1743-11-01</span></p>
</td>
<td>
<p><span>6.068</span></p>
</td>
<td>
<p><span>1.737</span></p>
</td>
<td>
<p><span>Århus</span></p>
</td>
<td>
<p><span>Denmark</span></p>
</td>
<td>
<p><span>57.05N</span></p>
</td>
<td>
<p><span>10.33E</span></p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p><span>1744-04-01</span></p>
</td>
<td>
<p><span>5.788</span></p>
</td>
<td>
<p><span>3.624</span></p>
</td>
<td>
<p><span>Århus</span></p>
</td>
<td>
<p><span>Denmark</span></p>
</td>
<td>
<p><span>57.05N</span></p>
</td>
<td>
<p><span>10.33E</span></p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p><span>1744-05-01</span></p>
</td>
<td>
<p><span>10.644</span></p>
</td>
<td>
<p><span>1.283</span></p>
</td>
<td>
<p><span>Århus</span></p>
</td>
<td>
<p><span>Denmark</span></p>
</td>
<td>
<p><span>57.05N</span></p>
</td>
<td>
<p><span>10.33E</span></p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p><span>1744-06-01</span></p>
</td>
<td>
<p><span>14.051</span></p>
</td>
<td>
<p><span>1.347</span></p>
</td>
<td>
<p><span>Århus</span></p>
</td>
<td>
<p><span>Denmark</span></p>
</td>
<td>
<p><span>57.05N</span></p>
</td>
<td>
<p><span>10.33E</span></p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p><span>1744-07-01</span></p>
</td>
<td>
<p><span>16.082</span></p>
</td>
<td>
<p><span>1.396</span></p>
</td>
<td>
<p><span>Århus</span></p>
</td>
<td>
<p><span>Denmark</span></p>
</td>
<td>
<p><span>57.05N</span></p>
</td>
<td>
<p><span>10.33E</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Let's see if we have any missing values with the following line of code:</p>
<pre>climate.isnull().sum()<br/><br/>dt                               0
AverageTemperature               0
AverageTemperatureUncertainty    0
City                             0
Country                          0
Latitude                         0
Longitude                        0
year                             0
dtype: int64<br/><br/># All good</pre>
<p>The column in question is called <kbd>AverageTemperature</kbd>. One quality of data at the interval level, which temperature is, is that we cannot use a bar/pie chart here because we have too many values:</p>
<pre><span># show us the number of unique items<br/></span><span>climate['AverageTemperature'].nunique()</span><br/> <br/> 111994</pre>
<p>111,994 values is absurd to plot, and also absurd because we know that the data is quantitative. Likely, the most common graph to utilize starting at this level would be the <strong>histogram</strong>. This graph is a cousin of the bar graph, and visualizes buckets of quantities and shows frequencies of these buckets.</p>
<p>Let's see a histogram for the AverageTemperature around the world, to see the distribution of temperatures in a very holistic view:</p>
<pre>climate['AverageTemperature'].hist()</pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="215" src="assets/34557624-2946-4bdc-8af7-54c0746a4efd.png" width="367"/></div>
<p>Here, we can see that we have an average value of 20°C. Let's confirm this:</p>
<pre>climate['AverageTemperature'].describe()<br/> <br/> count 8.235082e+06 mean 1.672743e+01 std 1.035344e+01 min -4.270400e+01 25% 1.029900e+01 50% 1.883100e+01 75% 2.521000e+01 max 3.965100e+01 Name: AverageTemperature, dtype: float64</pre>
<p>We were close. The mean seems to be around 17°. Let's make this a bit more fun and add new columns called <kbd>year</kbd> and <kbd>century</kbd>, and also subset the data to only be the temperatures recorded in the US:</p>
<pre># Convert the dt column to datetime and extract the year<br/> climate['dt'] = pd.to_datetime(climate['dt'])<br/> climate['year'] = climate['dt'].map(lambda value: value.year)<br/> <br/> climate_sub_us['century'] = climate_sub_us['year'].map(lambda x: x/100+1)<br/> # 1983 would become 20<br/> # 1750 would become 18<br/> <br/> # A subset the data to just the US<br/> climate_sub_us = climate.loc[climate['Country'] == 'United States']</pre>
<p class="mce-root">With the new column <kbd>century</kbd>, let's plot four histograms of temperature, one for each century:</p>
<pre>climate_sub_us['AverageTemperature'].hist(by=climate_sub_us['century'],<br/> sharex=True, sharey=True,<br/> figsize=(10, 10),<br/> bins=20)</pre>
<p class="mce-root">The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="365" src="assets/aa154be5-faa1-40ed-8330-502c5549ed6d.png" width="389"/></div>
<p>Here, we have our four histograms, showing that the <kbd>AverageTemperature</kbd> is going up slightly. Let's confirm this:</p>
<pre>climate_sub_us.groupby('century')['AverageTemperature'].mean().plot(kind='line')</pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="197" src="assets/f0569aa0-3de6-49a3-86ea-e5ebbee765e5.png" width="313"/></div>
<p>Interesting! And because differences are significant at this level, we can answer the question of how much, on average, the temperature has risen since the 18th century in the US. Let's store the changes over the centuries as its own pandas Series object first:</p>
<pre>century_changes = climate_sub_us.groupby('century')['AverageTemperature'].mean()<br/> <br/>century_changes<br/> <br/>century 18 12.073243 19 13.662870 20 14.386622 21 15.197692 Name: AverageTemperature, dtype: float64</pre>
<p>And now, let's use the indices in the Series to subtract the value in the 21st century minus the value in the 18th century, to get the difference in temperature:</p>
<pre># 21st century average temp in US minus 18th century average temp in US<br/> century_changes[21] - century_changes[18]<br/> <br/> # average difference in monthly recorded temperature in the US since the 18th century<br/> 3.12444911546</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting two columns at the interval level</h1>
                </header>
            
            <article>
                
<p>One large advantage of having two columns of data at the interval level, or higher, is that it opens us up to using scatter plots where we can graph two columns of data on our axes and visualize data-points as literal points on the graph. The <kbd>year</kbd> and <kbd>averageTemperature</kbd> column of our <kbd>climate change</kbd> dataset are both at the interval level, as they both have meaning differences, so let's take a crack at plotting all of the monthly recorded US temperatures as a scatter plot, where the <em>x</em> axis will be the year and the <em>y</em> axis will be the temperature. We hope to notice a trending increase in temperature, as the line graph previously suggested:</p>
<pre>x = climate_sub_us['year']<br/> y = climate_sub_us['AverageTemperature']<br/> fig, ax = plt.subplots(figsize=(10,5))<br/> ax.scatter(x, y)<br/> plt.show()</pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="202" src="assets/a0cb0ab6-e844-4bf8-b39b-e065cf236925.png" width="423"/></div>
<p>Oof, that's not pretty. There seems to be a lot of noise, and that is to be expected. Every year has multiple towns reporting multiple average temperatures, so it makes sense that we see many vertical points at each year.</p>
<p>Let's employ a <kbd>groupby</kbd> the year column to remove much of this noise:</p>
<pre># Let's use a groupby to reduce the amount of noise in the US<br/> climate_sub_us.groupby('year').mean()['AverageTemperature'].plot() </pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="198" src="assets/2690320e-c787-443c-8983-c28834545e5e.png" width="284"/></div>
<p>Better! We can definitely see the increase over the years, but let's smooth it out slightly by taking a rolling mean over the years:</p>
<pre># A moving average to smooth it all out:<br/> climate_sub_us.groupby('year').mean()['AverageTemperature'].rolling(10).mean().plot()<br/> </pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="197" src="assets/bb05280f-619e-4eab-8138-2b36106e54d2.png" width="284"/></div>
<p>So, our ability to plot two columns of data at the interval level has re-confirmed what the previous line graph suggested; that there does seem to be a general trend upwards in average temperature across the US.</p>
<p>The interval level of data provides a whole new level of understanding of our data, but we aren't done yet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The ratio level</h1>
                </header>
            
            <article>
                
<p>Finally, we move up to the highest level, the ratio level. At this level, we arguably have the highest degree of control and mathematics at our disposal. At the ratio level, like the interval level, we are still working with quantitative data. We inherit addition and subtraction from the interval level, but now we have a notion of <em>true zero</em> which gives us the ability to multiply and divide values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mathematical operations allowed</h1>
                </header>
            
            <article>
                
<p><span>At the ratio level, we may multiply and divide values together. This may not seem like a big deal, but it does allow us to make unique observations about data at this level that we cannot do at lower levels. Let's jump into a few examples to see exactly what this means.</span></p>
<p>Example:</p>
<p>When working with financial data, we almost always have to work with some monetary value. Money is at the ratio level because we have a concept of having "zero money". For this reason, we may make statements such as:</p>
<ul>
<li>
<p>$100 is <em>twice </em>as much as $50 because 100/50 = 2</p>
</li>
<li>
<p>10mg of penicillin is <em>half </em>as much as 20mg of penicillin because 10/20 = .5</p>
</li>
</ul>
<p>It is because of the existence of zero that ratios have meaning at this level.</p>
<p>Non-example:</p>
<p>We generally consider temperature to be at the interval level and not the ratio level, because it doesn't make sense to say something like 100 degree<sup> </sup>is twice as hot as 50 degree. That doesn't quite make sense. Temperature is quite subjective and this is not objectively correct.</p>
<div class="layoutArea">
<div class="column packt_tip packt_infobox">
<p><span>It can be argued that Celsius and Fahrenheit have a starting point mainly because we can convert them into Kelvin, which does boast a true zero. In reality, because Celsius and Fahrenheit allow negative values, while Kelvin does not; both Celsius and Fahrenheit do not have a real <em>true zero</em>, while Kelvin does.<br/></span></p>
</div>
<p>Going back to the salary data from San Francisco, we now see that the salary weekly rate is at the ratio level, and there we can start making new observations. Let's begin by looking at the highest paid salaries:</p>
</div>
<pre class="mce-root"># Which Grade has the highest Biweekly high rate<br/># What is the average rate across all of the Grades<br/>fig = plt.figure(figsize=(15,5))<br/>ax = fig.gca()<br/><br/>salary_ranges.groupby('Grade')[['Biweekly High Rate']].mean().sort_values(<br/> 'Biweekly High Rate', ascending=False).head(20).plot.bar(stacked=False, ax=ax, color='darkorange')<br/>ax.set_title('Top 20 Grade by Mean Biweekly High Rate')</pre>
<p class="mce-root">The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/188ffdf5-9fa2-4672-af3a-c1f81a9c6d14.png"/></div>
<p>If we look up the highest-paid salary in a San Francisco public record found at:</p>
<p> <a href="http://sfdhr.org/sites/default/files/documents/Classification%20and%20Compensation/Archives/Compensation-Manual-FY09-10.pdf" target="_blank">http://sfdhr.org/sites/default/files/documents/Classification%20and%20Compensation/Archives/Compensation-Manual-FY09-10.pdf</a></p>
<p>We see that it is the <strong>General Manager</strong>, <strong>Public Transportation Dept.</strong>. Let's take a look at the lowest-paid jobs by employing a similar strategy:</p>
<pre># Which Grade has the lowest Biweekly high rate<br/> fig = plt.figure(figsize=(15,5))<br/> ax = fig.gca()<br/> <br/> salary_ranges.groupby('Grade')[['Biweekly High Rate']].mean().sort_values(<br/> 'Biweekly High Rate', ascending=False).tail(20).plot.bar(stacked=False, ax=ax, color='darkorange')<br/> ax.set_title('Bottom 20 Grade by Mean Biweekly High Rate')</pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6ff7f1f3-ea27-4106-b9b2-84627f788e4e.png"/></div>
<p>Again, looking up the lowest-paid job, we see that it is a <strong>Camp Assistant</strong>.</p>
<p>Because money is at the ratio level, we can also find the ratio of the highest-paid employee to the lowest-paid employee:</p>
<pre><span>sorted_df = salary_ranges.groupby('Grade')[['Biweekly High Rate']].mean().sort_values(</span><br/> <span>'Biweekly High Rate', ascending=False)</span><br/> <span>sorted_df.iloc[0][0] / sorted_df.iloc[-1][0]</span><br/> <br/> 13.931919540229886</pre>
<p>The highest-paid employee makes 14x the lowest city employee. Thanks, ratio level!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recap of the levels of data</h1>
                </header>
            
            <article>
                
<p><span>Understanding the various levels of data is necessary to perform feature engineering. When it comes time to build new features, or fix old ones, we must have ways of identifying how to work with every column.</span></p>
<p>Here is a quick table to summarize what is and isn't possible at every level:</p>
<table>
<tbody>
<tr>
<td>
<p class="table"><strong>Level of Measurement</strong></p>
</td>
<td>
<p class="table"><strong>Properties</strong></p>
</td>
<td>
<p class="table"><strong>Examples</strong></p>
</td>
<td>
<p class="table"><strong>Descriptive statistics</strong></p>
</td>
<td>
<p class="table"><strong>Graphs</strong></p>
</td>
</tr>
<tr>
<td>
<p class="table">Nominal</p>
</td>
<td>
<p class="table">Discrete</p>
<p class="table">Orderless</p>
</td>
<td>
<p class="table">Binary Responses (True or False)</p>
<p class="table">Names of People</p>
<p>Colors of paint</p>
</td>
<td>
<p class="table">Frequencies/Percentages<br/>
Mode</p>
</td>
<td>
<p class="table">Bar</p>
<p class="table">Pie</p>
</td>
</tr>
<tr>
<td>
<p class="table">Ordinal</p>
</td>
<td>
<p class="table">Ordered categories</p>
<p class="table">Comparisons</p>
</td>
<td>
<p class="table">Likert Scales</p>
<p>Grades on an exam</p>
</td>
<td>
<p class="table">Frequencies</p>
<p class="table">Mode</p>
<p class="table">Median</p>
<p class="table">Percentiles</p>
</td>
<td>
<p class="table">Bar</p>
<p class="table">Pie</p>
<p class="table">Stem and leaf</p>
</td>
</tr>
<tr>
<td>
<p class="table">Interval</p>
</td>
<td>
<p class="table">Differences between ordered values have meaning</p>
<p class="table"/>
</td>
<td>
<p class="table">Deg. C or F</p>
<p>Some Likert Scales (must be specific)</p>
</td>
<td>
<p class="table">Frequencies</p>
<p class="table">Mode</p>
<p class="table">Median</p>
<p class="table">Mean</p>
<p class="table">Standard Deviation</p>
<p class="table"/>
</td>
<td>
<p class="table">Bar<br/>
Pie<br/>
<span class="SpellE">Stem and leaf</span></p>
<p class="table">Box plot</p>
<p class="table">Histogram</p>
</td>
</tr>
<tr>
<td>
<p class="table">Ratio</p>
</td>
<td>
<p class="table">Continuous</p>
<p class="table">True 0 allows ratio statements<br/>
(for example, $100 is twice as much as $50)</p>
</td>
<td>
<p class="table">Money</p>
<p>Weight</p>
</td>
<td>
<p class="table">Mean</p>
<p class="table">Standard Deviation</p>
<p class="table"/>
</td>
<td>
<p class="table">Histogram</p>
<p class="table">Box plot</p>
</td>
</tr>
</tbody>
</table>
<p>The following is a table showing the types of statistics allowed at each level:</p>
<table>
<tbody>
<tr>
<td>
<p class="table"><strong>Statistic</strong></p>
</td>
<td>
<p class="table"><strong>Nominal</strong></p>
</td>
<td>
<p class="table"><strong>Ordinal</strong></p>
</td>
<td>
<p class="table"><strong>Interval</strong></p>
</td>
<td>
<p class="table"><strong>Ratio</strong></p>
</td>
</tr>
<tr>
<td>
<p class="table">Mode</p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p><span>Sometimes</span></p>
</td>
</tr>
<tr>
<td>
<p class="table">Median</p>
</td>
<td>
<p class="table"><span>X</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
</tr>
<tr>
<td>
<p class="table">Range, Min. Max</p>
</td>
<td>
<p class="table"><span>X</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
</tr>
<tr>
<td>
<p class="table">Mean</p>
</td>
<td>
<p class="table"><span>X</span></p>
</td>
<td>
<p class="table"><span>X</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
</tr>
<tr>
<td>
<p class="table">SD</p>
</td>
<td>
<p class="table"><span>X</span></p>
</td>
<td>
<p class="table"><span>X</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="table">And finally, the following is a table showing purely the graphs that are and are not possible at each level:</p>
<table>
<tbody>
<tr>
<td>
<p class="table"><strong>Graph</strong></p>
</td>
<td>
<p class="table"><strong>Nominal</strong></p>
</td>
<td>
<p class="table"><strong>Ordinal</strong></p>
</td>
<td>
<p class="table"><strong>Interval</strong></p>
</td>
<td>
<p class="table"><strong>Ratio</strong></p>
</td>
</tr>
<tr>
<td>
<p class="table">Bar/Pie</p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>Sometimes</span></p>
</td>
<td>
<p class="table"><span>X</span></p>
</td>
</tr>
<tr>
<td>
<p class="table">Stem and Leaf</p>
</td>
<td>
<p class="table"><span>X</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
</tr>
<tr>
<td>
<p class="table">Boxplot</p>
</td>
<td>
<p class="table"><span>X</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
</tr>
<tr>
<td>
<p class="table">Histogram</p>
</td>
<td>
<p class="table"><span>X</span></p>
</td>
<td>
<p class="table"><span>X</span></p>
</td>
<td>
<p class="table"><span>Sometimes</span></p>
</td>
<td>
<p class="table"><span>√</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p><span>Whenever you are faced with a new dataset, here is a basic workflow to follow:</span></p>
<ol>
<li><span>Is the data organized or unorganized? D</span><span>oes our data exist in a tabular format with distinct rows and columns, or does it exist as a mess of text in an unstructured format?</span></li>
<li>Is each column quantitative or qualitative? <span>Are the values in the cells numbers that represent quantity, or strings that do not?</span></li>
<li>At what level of data is each column? <span>Are the values at the nominal, ordinal, interval, or ratio level?</span></li>
<li>What graphs can I utilize to visualize my data—bar, pie, box, histogram, and so on?</li>
</ol>
<div class="layoutArea CDPAlignCenter CDPAlign">
<p>Here is a visualization of this flow:</p>
<p style="padding-left: 120px"><img height="249" src="assets/7839f08c-8139-4885-bf43-be312bf00679.png" width="351"/></p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Understanding the features that we are working with is step <span>zero</span> of feature engineering. If we cannot understand the data given to us, we will never hope to fix, create, and utilize features in order to create well-performing, machine-learning pipelines. In this chapter, we were able to recognize, and extract the levels of data from our datasets and use that information to create useful and meaningful visuals that shine new lights on our data.</p>
<p>In the next chapter, we will use all of this new-found knowledge of the levels of data to start improving our features, and we will start to use machine-learning to effectively measure the impact of our feature engineering pipelines.</p>


            </article>

            
        </section>
    </body></html>