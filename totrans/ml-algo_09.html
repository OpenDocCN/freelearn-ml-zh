<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Clustering Fundamentals</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to introduce the basic concepts of clustering and the structure of k-means, a quite common algorithm that can solve many problems efficiently. However, its assumptions are very strong, in particular those concerning the convexity of the clusters, and this can lead to some limitations in its adoption. We're going to discuss its mathematical foundation and how it can be optimized. Moreover, we're going to analyze two alternatives that can be employed when k-means fails to cluster a dataset. These alternatives are DBSCAN, (which works by considering the differences of sample density), and spectral clustering, a very powerful approach based on the affinity among points.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering basics</h1>
                </header>
            
            <article>
                
<p>Let's consider a dataset of points:</p>
<div class="CDPAlignCenter CDPAlign"><img height="43" width="240" src="assets/166885de-3d63-48d5-a7a0-d08c5ec35c22.png"/></div>
<p>We assume that it's possible to find a criterion (not unique) so that each sample can be associated with a specific group:</p>
<div class="CDPAlignCenter CDPAlign"><img height="27" width="217" src="assets/c8142ca3-03ec-4ebb-ac1a-42b0e66b3145.png"/></div>
<p>Conventionally, each group is called a <strong>cluster</strong> and the process of finding the function <em>G</em> is called <strong>clustering</strong>. Right now, we are not imposing any restriction on the clusters; however, as our approach is unsupervised, there should be a similarity criterion to join some elements and separate other ones. Different clustering algorithms are based on alternative strategies to solve this problem, and can yield very different results. In the following figure, there's an example of clustering based on four sets of bidimensional samples; the decision to assign a point to a cluster depends only on its features and sometimes on the position of a set of other points (neighborhood): </p>
<div class="CDPAlignCenter CDPAlign"><br/>
<img class="image-border" src="assets/ae488a1f-8aac-4104-94c1-81059dc3a61f.png"/></div>
<p>In this book, we're going to discuss <strong>hard clustering</strong> techniques, where each element must belong to a single cluster. The alternative approach, called <strong>soft clustering</strong> (or <strong>fuzzy clustering</strong>), is based on a membership score that defines how much the elements are "compatible" with each cluster. The generic clustering function becomes:</p>
<div class="CDPAlignCenter CDPAlign"><img height="26" width="364" src="assets/8b957908-ca3d-4ae6-be64-a65745af825d.png"/></div>
<p>A vector <em>m<sub>i</sub></em> represents the relative membership of <em>x<sub>i</sub></em>, and it's often normalized as a probability distribution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means</h1>
                </header>
            
            <article>
                
<p>The k-means algorithm is based on the (strong) initial condition to decide the number of clusters through the assignment of k initial <strong>centroids</strong> or <strong>means</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="34" width="175" src="assets/57f9c0a1-3a73-47ff-a35e-ee9e4fe2b787.png"/></div>
<p>Then the distance between each sample and each centroid is computed and the sample is assigned to the cluster where the distance is minimum. This approach is often called <strong>minimizing the inertia</strong> of the clusters, which is defined as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="57" width="225" src="assets/ffa4f61c-5be4-48ea-87f1-52e27213501b.png"/></div>
<p>The process is iterative—once all the samples have been processed, a new set of centroids <em>K<sup>(1)</sup></em> is computed (now considering the actual elements belonging to the cluster), and all the distances are recomputed. The algorithm stops when the desired tolerance is reached, or in other words, when the centroids become stable and, therefore, the inertia is minimized.<br/>
Of course, this approach is quite sensitive to the initial conditions, and some methods have been studied to improve the convergence speed. One of them is called <strong>k-means++</strong> (Karteeka Pavan K., Allam Appa Rao, Dattatreya Rao A. V., and Sridhar G.R., <em>Robust Seed Selection Algorithm for K-Means Type Algorithms</em>, International Journal of Computer Science and Information Technology 3, no. 5, October 30, 2011), which selects the initial centroids so that they are statistically close to the final ones. The mathematical explanation is quite difficult; however, this method is the default choice for scikit-learn, and it's normally the best choice for any clustering problem solvable with this algorithm.<br/>
Let's consider a simple example with a dummy dataset:</p>
<pre><strong>from sklearn.datasets import make_blobs</strong><br/><br/><strong>nb_samples = 1000</strong><br/><strong>X, _ = make_blobs(n_samples=nb_samples, n_features=2, centers=3, cluster_std=1.5)</strong></pre>
<p>We expect to have three clusters with bidimensional features and a partial overlap due to the standard deviation of each blob. In our example, we won't use the <em>Y</em> variable (which contains the expected cluster) because we want to generate only a set of locally coherent points to try our algorithms.</p>
<p>The resultant plot is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="423" width="513" class="image-border" src="assets/ae60d470-3a18-4476-9072-f17138b8f586.png"/></div>
<p>In this case, the problem is quite simple to solve, so we expect k-means to separate the three groups with minimum error in the region of <em>X</em> bounded between [-5, 0]. Keeping the default values, we get:</p>
<pre><strong>from sklearn.cluster import KMeans</strong><br/><br/><strong>&gt;&gt;&gt; km = KMeans(n_clusters=3)</strong><br/><strong>&gt;&gt;&gt; km.fit(X)</strong><br/><strong>KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,</strong><br/><strong>    n_clusters=3, n_init=10, n_jobs=1, precompute_distances='auto',</strong><br/><strong>    random_state=None, tol=0.0001, verbose=0)</strong><br/><br/><strong>&gt;&gt;&gt; print(km.cluster_centers_)</strong><br/><strong>[[ 1.39014517,  1.38533993]</strong><br/><strong> [ 9.78473454,  6.1946332 ]</strong><br/><strong> [-5.47807472,  3.73913652]]</strong></pre>
<p>Replotting the data using three different markers, it's possible to verify how k-means successfully separated the data:</p>
<div class="CDPAlignCenter CDPAlign"><img height="423" width="519" class="image-border" src="assets/86c72a22-3ffd-4b06-ac81-6c4ff4ded9d3.png"/></div>
<p>In this case, the separation was very easy because k-means is based on Euclidean distance, which is radial, and therefore the clusters are expected to be convex. When this doesn't happen, the problem cannot be solved using this algorithm. Most of the time, even if the convexity is not fully guaranteed, k-means can produce good results, but there are several situations when the expected clustering is impossible and letting k-means find out the centroid can lead to completely wrong solutions.<br/>
Let's consider the case of concentric circles. scikit-learn provides a built-in function to generate such datasets:</p>
<pre><strong>from sklearn.datasets import make_circles</strong><br/><br/><strong>&gt;&gt;&gt; nb_samples = 1000</strong><br/><strong>&gt;&gt;&gt; X, Y = make_circles(n_samples=nb_samples, noise=0.05)</strong></pre>
<p>The plot of this dataset is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="400" width="503" class="image-border" src="assets/dba7d538-1ae1-491d-87c7-40988971e54c.png"/></div>
<p>We would like to have an internal cluster (corresponding to the samples depicted with triangular markers) and an external one (depicted by dots). However, such sets are not convex, and it's impossible for k-means to separate them correctly (the means should be the same!). In fact, suppose we try to apply the algorithm to two clusters:</p>
<pre><strong>&gt;&gt;&gt; km = KMeans(n_clusters=2)</strong><br/><strong>&gt;&gt;&gt; km.fit(X)</strong><br/><strong>KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,</strong><br/><strong>    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',</strong><br/><strong>    random_state=None, tol=0.0001, verbose=0)</strong></pre>
<p>We get the separation shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="411" width="516" class="image-border" src="assets/3ce4374e-9ffc-40ae-86d2-250a23da572f.png"/></div>
<p>As expected, k-means converged on the two centroids in the middle of the two half-circles, and the resulting clustering is quite different from what we expected. Moreover, if the samples must be considered different according to the distance from the common center, this result will lead to completely wrong predictions. It's obvious that another method must be employed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding the optimal number of clusters</h1>
                </header>
            
            <article>
                
<p>One of the most common disadvantages of k-means is related to the choice of the optimal number of clusters. An excessively small value will determine large groupings that contain heterogeneous elements, while a large number leads to a scenario where it can be difficult to identify the differences among clusters. Therefore, we're going to discuss some methods that can be employed to determine the appropriate number of splits and to evaluate the corresponding performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing the inertia</h1>
                </header>
            
            <article>
                
<p>The first method is based on the assumption that an appropriate number of clusters must produce a small inertia. However, this value reaches its minimum (0.0) when the number of clusters is equal to the number of samples; therefore, we can't look for the minimum, but for a value which is a trade-off between the inertia and the number of clusters.</p>
<p>Let's suppose we have a dataset of 1,000 elements. We can compute and collect the inertias (scikit-learn stores these values in the instance variable <kbd>inertia_</kbd>) for a different number of clusters:</p>
<pre class="mce-root"><strong>&gt;&gt;&gt; nb_clusters = [2, 3, 5, 6, 7, 8, 9, 10]</strong><br/><br/><strong>&gt;&gt;&gt; inertias = []</strong><br/><br/><strong>&gt;&gt;&gt; for n in nb_clusters:</strong><br/><strong>&gt;&gt;&gt;    km = KMeans(n_clusters=n)</strong><br/><strong>&gt;&gt;&gt;    km.fit(X)</strong><br/><strong>&gt;&gt;&gt;    inertias.append(km.inertia_)</strong></pre>
<p>Plotting the values, we get the result shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="401" width="520" class="image-border" src="assets/09130468-38ab-4b44-a9ce-c7d700bb673d.png"/></div>
<p>As you can see, there's a dramatic reduction between 2 and 3 and then the slope starts flattening. We want to find a value that, if reduced, leads to a great inertial increase and, if increased, produces a very small inertial reduction. Therefore, a good choice could be 4 or 5, while greater values are likely to produce unwanted intracluster splits (till the extreme situation where each point becomes a single cluster). This method is very simple, and can be employed as the first approach to determine a potential range. The next strategies are more complex, and can be used to find the final number of clusters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Silhouette score</h1>
                </header>
            
            <article>
                
<p>The silhouette score is based on the principle of "maximum internal cohesion and maximum cluster separation". In other words, we would like to find the number of clusters that produce a subdivision of the dataset into dense blocks that are well separated from each other. In this way, every cluster will contain very similar elements and, selecting two elements belonging to different clusters, their distance should be greater than the maximum intracluster one.  </p>
<p>After defining a distance metric (Euclidean is normally a good choice), we can compute the average intracluster distance for each element:</p>
<div class="CDPAlignCenter CDPAlign"><img height="39" width="239" src="assets/52a32904-d907-4c36-9912-5d96f96b5af3.png"/></div>
<p>We can also define the average nearest-cluster distance (which corresponds to the lowest intercluster distance):</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="40" width="439" src="assets/79c11243-250f-4c9c-b722-62686b2f9665.png"/></div>
<p>The silhouette score for an element <em>x<sub>i</sub></em> is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="54" width="176" src="assets/33afd3c2-4b5b-44fb-8592-f83970dce3ae.png"/></div>
<p>This value is bounded between -1 and 1, with the following interpretation:</p>
<ul>
<li>A value close to 1 is good (1 is the best condition) because it means that <em>a(x<sub>i</sub>) &lt;&lt; b(x<sub>i</sub>)</em></li>
<li>A value close to 0 means that the difference between intra and inter cluster measures is almost null and therefore there's a cluster overlap</li>
<li>A value close to -1 means that the sample has been assigned to a wrong cluster because <em>a(x<sub>i</sub>) &gt;&gt; b(x<sub>i</sub>)</em></li>
</ul>
<p>scikit-learn allows computing the average silhouette score to have an immediate overview for different numbers of clusters:</p>
<pre><strong>from sklearn.metrics import silhouette_score</strong><br/><br/><strong>&gt;&gt;&gt; nb_clusters = [2, 3, 5, 6, 7, 8, 9, 10]</strong><br/><br/><strong>&gt;&gt;&gt; avg_silhouettes = []</strong><br/><br/><strong>&gt;&gt;&gt; for n in nb_clusters:</strong><br/><strong>&gt;&gt;&gt;    km = KMeans(n_clusters=n)</strong><br/><strong>&gt;&gt;&gt;    Y = km.fit_predict(X)</strong><br/><strong>&gt;&gt;&gt;    avg_silhouettes.append(silhouette_score(X, Y))</strong></pre>
<p>The corresponding plot is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="411" width="516" class="image-border" src="assets/4a1a75ed-58d7-457f-98f6-82a890aedc23.png"/></div>
<p>The best value is 3 (which is very close to 1.0), however, bearing in mind the previous method, 4 clusters provide a smaller inertia, together with a reasonable silhouette score. Therefore, a good choice could be 4 instead of 3. However, the decision between 3 and 4 is not immediate and should be evaluated by also considering the nature of the dataset. The silhouette score indicates that there are 3 dense agglomerates, but the inertia diagram suggests that one of them (at least) can probably be split into two clusters. To have a better understanding of how the clustering is working, it's also possible to graph the silhouette plots, showing the sorted score for each sample in all clusters. In the following snippet we create the plots for a number of clusters equal to 2, 3, 4, and 8:</p>
<pre class="mce-root"><strong>from sklearn.metrics import silhouette_samples<br/><br/>&gt;&gt;&gt; fig, ax = subplots(2, 2, figsize=(15, 10))</strong><br/><br/><strong>&gt;&gt;&gt; nb_clusters = [2, 3, 4, 8]</strong><br/><strong>&gt;&gt;&gt; mapping = [(0, 0), (0, 1), (1, 0), (1, 1)]</strong><br/><br/><strong>&gt;&gt;&gt; for i, n in enumerate(nb_clusters):</strong><br/><strong>&gt;&gt;&gt;    km = KMeans(n_clusters=n)</strong><br/><strong>&gt;&gt;&gt;    Y = km.fit_predict(X)</strong><br/><br/><strong>&gt;&gt;&gt;    silhouette_values = silhouette_samples(X, Y)</strong><br/>    <br/><strong>&gt;&gt;&gt;    ax[mapping[i]].set_xticks([-0.15, 0.0, 0.25, 0.5, 0.75, 1.0])</strong><br/><strong>&gt;&gt;&gt;    ax[mapping[i]].set_yticks([])</strong><br/><strong>&gt;&gt;&gt;    ax[mapping[i]].set_title('%d clusters' % n)<br/>&gt;&gt;&gt;    ax[mapping[i]].set_xlim([-0.15, 1])</strong><br/><strong>&gt;&gt;&gt;    ax[mapping[i]].grid()</strong><br/><strong>&gt;&gt;&gt;    y_lower = 20</strong><br/><br/><strong>&gt;&gt;&gt;    for t in range(n):</strong><br/><strong>&gt;&gt;&gt;        ct_values = silhouette_values[Y == t]</strong><br/><strong>&gt;&gt;&gt;        ct_values.sort()</strong><br/>        <br/><strong>&gt;&gt;&gt;        y_upper = y_lower + ct_values.shape[0]</strong><br/><br/><strong>&gt;&gt;&gt;        color = cm.Accent(float(t) / n)</strong><br/><strong>&gt;&gt;&gt;        ax[mapping[i]].fill_betweenx(np.arange(y_lower, y_upper), 0, </strong><br/><strong>&gt;&gt;&gt;                                     ct_values, facecolor=color, edgecolor=color)</strong><br/><br/><strong>&gt;&gt;&gt;        y_lower = y_upper + 20</strong></pre>
<p>The silhouette coefficients for each sample are computed using the function <kbd>silhouette_values</kbd> (which are always bounded between -1 and 1). In this case, we are limiting the graph between -0.15 and 1 because there are no smaller values. However, it's important to check the whole range before restricting it.</p>
<p>The resulting graph is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="349" width="524" class="image-border" src="assets/94cbeff8-0256-4d3d-a26d-8945814fba7d.png"/></div>
<p>The width of each silhouette is proportional to the number of samples belonging to a specific cluster, and its shape is determined by the scores of each sample. An ideal plot should contain homogeneous and long silhouettes without peaks (they must be similar to trapezoids rather than triangles) because we expect to have a very low score variance among samples in the same cluster. For 2 clusters, the shapes are acceptable, but one cluster has an average score of 0.5, while the other has a value greater than 0.75; therefore, the first cluster has a low internal coherence. A completely different situation is shown in the plot corresponding to 8 clusters. All the silhouettes are triangular and their maximum score is slightly greater than 0.5. It means that all the clusters are internally coherent, but the separation is unacceptable. With 3 clusters, the plot is almost perfect, except for the width of the second silhouette. Without further metrics, we could consider this number as the best choice (confirmed also by the average score), but the inertia is lower for a higher numbers of clusters. With 4 clusters, the plot is slightly worse, with two silhouettes having a maximum score of about 0.5. This means that two clusters are perfectly coherent and separated, while the remaining two are rather coherent, but they aren't probably well separated. Right now, our choice should be made between 3 and 4. The next methods will help us in banishing all doubts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calinski-Harabasz index</h1>
                </header>
            
            <article>
                
<p>Another method that is based on the concept of dense and well-separated clusters is the Calinski-Harabasz index. To build it, we need first to define the inter cluster dispersion. If we have k clusters with their relative centroids and the global centroid, the inter-cluster dispersion (BCD) is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="48" width="352" src="assets/61698f0e-0e3d-430c-81e7-ca1ff5689493.png"/></div>
<p>In the above expression, <em>n<sub>k</sub></em> is the number of elements belonging to the cluster k, <em>mu</em> (the Greek letter in the formula) is the global centroid, and <em>mu</em><sub>i</sub> is the centroid of cluster <em>i</em>. The intracluster dispersion (WCD) is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="64" width="403" src="assets/318e76f2-f257-43fa-b84c-873789494b11.png"/></div>
<p>The Calinski-Harabasz index is defined as the ratio between <em>BCD(k)</em> and <em>WCD(k)</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="38" width="156" src="assets/9e3c3dde-0942-4d7d-a775-941a6683a9b7.png"/></div>
<p>As we look for a low intracluster dispersion (dense agglomerates) and a high intercluster dispersion (well-separated agglomerates), we need to find the number of clusters that maximizes this index. We can obtain a graph in a way similar to what we have already done for the silhouette score:</p>
<pre><strong>from sklearn.metrics import calinski_harabaz_score</strong><br/><br/><strong>&gt;&gt;&gt; nb_clusters = [2, 3, 5, 6, 7, 8, 9, 10]</strong><br/><br/><strong>&gt;&gt;&gt; ch_scores = []</strong><br/><br/><strong>&gt;&gt;&gt; km = KMeans(n_clusters=n)</strong><br/><strong>&gt;&gt;&gt; Y = km.fit_predict(X)</strong><br/><br/><strong>&gt;&gt;&gt; for n in nb_clusters:</strong><br/><strong>&gt;&gt;&gt;    km = KMeans(n_clusters=n)</strong><br/><strong>&gt;&gt;&gt;    Y = km.fit_predict(X)</strong><br/><strong>&gt;&gt;&gt;    ch_scores.append(calinski_harabaz_score(X, Y))</strong></pre>
<p>The resulting plot is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="410" width="514" class="image-border" src="assets/bedfcc53-43b8-4b6c-be5c-4c35cf2ea966.png"/></div>
<p>As expected, the highest value (5,500) is obtained with 3 clusters, while 4 clusters yield a value slightly below 5,000. Considering only this method, there's no doubt that the best choice is 3, even if 4 is still a reasonable value. Let's consider the last method, which evaluates the overall stability.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cluster instability</h1>
                </header>
            
            <article>
                
<p>Another approach is based on the concept of cluster instability defined in Von Luxburg U., <em>Cluster stability: an overview</em>, arXiv 1007:1075v1, 7 July 2010<em>. </em>Intuitively, we can say that a clustering approach is stable if perturbed versions of the same dataset produce very similar results. More formally, if we have a dataset <em>X</em>, we can define a set of <em>m</em> perturbed (or noisy) versions:</p>
<div class="CDPAlignCenter CDPAlign"><img height="35" width="153" src="assets/69ae3442-57db-4512-b20d-e6a63f7a1ff5.png"/></div>
<p>Considering a distance metric <em>d(C(X<sub>1</sub>), C(X<sub>2</sub>))</em> between two clusterings with the same number (k) of clusters, the instability is defined as the average distance between couples of clusterings of noisy versions:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="50" width="274" src="assets/8fb57b45-0c9a-4d8d-9e6e-d10689759aee.png"/></div>
<p class="mce-root">For our purposes, we need to find the value of k that minimizes <em>I(C)</em> (and therefore maximizes the stability). First of all, we need to produce some noisy versions of the dataset. Let's suppose that <em>X</em> contains 1,000 bidimensional samples with a standard deviation of 10.0. We can perturb <em>X</em> by adding a uniform random value (in the range [-2.0, 2.0]) with a probability of 0.25:</p>
<pre class="mce-root"><strong>&gt;&gt;&gt; nb_noisy_datasets = 4</strong><br/><br/><strong>&gt;&gt;&gt; X_noise = []</strong><br/><br/><strong>&gt;&gt;&gt; for _ in range(nb_noisy_datasets):</strong><br/><strong>&gt;&gt;&gt;    Xn = np.ndarray(shape=(1000, 2))</strong><br/><strong>&gt;&gt;&gt;    for i, x in enumerate(X):</strong><br/><strong>&gt;&gt;&gt;        if np.random.uniform(0, 1) &lt; 0.25:</strong><br/><strong>&gt;&gt;&gt;            Xn[i] = X[i] + np.random.uniform(-2.0, 2.0)</strong><br/><strong>&gt;&gt;&gt;        else:</strong><br/><strong>&gt;&gt;&gt;            Xn[i] = X[i]</strong><br/><strong>&gt;&gt;&gt;    X_noise.append(Xn)</strong></pre>
<p class="mce-root">Here we are assuming to have four perturbed versions. As a metric, we adopt the Hamming distance, which is proportional (if normalized) to the number of output elements that disagree. At this point, we can compute the instabilities for various numbers of clusters:</p>
<pre><strong>from sklearn.metrics.pairwise import pairwise_distances</strong><br/><br/><strong>&gt;&gt;&gt; instabilities = []</strong><br/><br/><strong>&gt;&gt;&gt; for n in nb_clusters:</strong><br/><strong>&gt;&gt;&gt;    Yn = []</strong><br/><strong>&gt;&gt;&gt;    </strong><br/><strong>&gt;&gt;&gt;    for Xn in X_noise:</strong><br/><strong>&gt;&gt;&gt;        km = KMeans(n_clusters=n)</strong><br/><strong>&gt;&gt;&gt;        Yn.append(km.fit_predict(Xn))</strong><br/><br/><strong>&gt;&gt;&gt; distances = []</strong><br/><br/><strong>&gt;&gt;&gt; for i in range(len(Yn)-1):</strong><br/><strong>&gt;&gt;&gt;        for j in range(i, len(Yn)):</strong><br/><strong>&gt;&gt;&gt;            d = pairwise_distances(Yn[i].reshape(-1, 1), Yn[j].reshape(-1, -1), 'hamming')</strong><br/><strong>&gt;&gt;&gt;            distances.append(d[0, 0])</strong><br/><strong>                </strong><br/><strong>&gt;&gt;&gt;    instability = (2.0 * np.sum(distances)) / float(nb_noisy_datasets ** 2)</strong><br/><strong>&gt;&gt;&gt;    instabilities.append(instability)</strong></pre>
<p>As the distances are symmetrical, we compute them only for the upper triangular part of the matrix. The result is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="383" width="481" class="image-border" src="assets/7f2485c9-b154-444a-8823-1a3adde2a9be.png"/></div>
<p>Excluding the configuration with 2 clusters, where the inertia is very high, we have a minimum for 3 clusters, a value that has already been confirmed by the three previous methods. Therefore, we can finally decide to set <kbd>n_clusters=3</kbd>, excluding the options of 4 or more clusters.  This method is very powerful, but it's important to evaluate the stability with a reasonable number of noisy datasets, taking care not to excessively alter the original geometry. A good choice is to use Gaussian noise with a variance set to a fraction (for example 1/10) of the dataset variance. Alternative approaches are presented in Von Luxburg U., <em>Cluster stability: an overview</em>, arXiv 1007:1075v1, 7 July 2010.</p>
<div class="packt_infobox">Even if we have presented these methods with k-means, they can be applied to any clustering algorithm to evaluate the performance and compare them.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DBSCAN</h1>
                </header>
            
            <article>
                
<p>DBSCAN or <span><strong>Density-Based Spatial Clustering of Applications with Noise</strong> is a powerful algorithm that can easily solve non-convex problems where k-means fails. The idea is simple: A cluster is a high-density area (there are no restrictions on its shape) surrounded by a low-density one. This statement is generally true, and doesn't need an initial declaration about the number of expected clusters. The procedure starts by analyzing a small area (formally, a point surrounded by a minimum number of other samples). If the density is enough, it is considered part of a cluster. At this point, the neighbors are taken into account. If they also have a high density, they are merged with the first area; otherwise, they determine a topological separation. When all the areas have been scanned, the clusters have also </span><span>been</span><span> </span><span>determined because they are islands surrounded by empty space.</span></p>
<p><span>scikit-learn allows us to control this procedure with two parameters:</span></p>
<ul>
<li><kbd>eps</kbd>: Responsible for defining the maximum distance between two neighbors. Higher values will aggregate more points, while smaller ones will create more clusters.</li>
<li> <kbd>min_samples</kbd><span>: This determines how many surrounding points are necessary to define an area (also known as the core-point).</span></li>
</ul>
<p>Let's try DBSCAN with a very hard clustering problem, called half-moons. The dataset can be created using a built-in function:</p>
<pre><strong>from sklearn.datasets import make_moons</strong><br/><br/><strong>&gt;&gt;&gt; nb_samples = 1000</strong><br/><strong>&gt;&gt;&gt; X, Y = make_moons(n_samples=nb_samples, noise=0.05)</strong></pre>
<p>A plot of the dataset is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="291" width="363" class="image-border" src="assets/252726e9-66ee-44b4-88cb-594b961906ab.png"/></div>
<p>Just to understand, k-means will cluster by finding the optimal convexity, and the result is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="279" width="358" class="image-border" src="assets/49b7dea9-457b-407f-bb37-a803af3e5a4b.png"/></div>
<p>Of course, this separation is unacceptable, and there's no way to improve the accuracy. Let's try it with DBSCAN (with <kbd>eps</kbd> set to 0.1 and the default value of 5 for <kbd>min_samples</kbd>):</p>
<pre><strong>from sklearn.cluster import DBSCAN</strong><br/><br/><strong>&gt;&gt;&gt; dbs = DBSCAN(eps=0.1)</strong><br/><strong>&gt;&gt;&gt; Y = dbs.fit_predict(X)</strong></pre>
<p>In a different manner than other implementations, DBSCAN predicts the label during the training process, so we already have an array <kbd>Y</kbd> containing the cluster assigned to each sample. In the following figure, there's a representation with two different markers:</p>
<div class="CDPAlignCenter CDPAlign"><img height="316" width="402" class="image-border" src="assets/281c2a7b-b947-49a7-b3c6-2fe0e88a43d0.png"/></div>
<p>As you can see, the accuracy is very high and only three isolated points are misclassified (in this case, we know their class, so we can use this term even if it's a clustering process). However, by performing a grid search, it's easy to find the best values that optimize the clustering process. It's important to tune up those parameters to avoid two common problems: few big clusters and many small ones. This problem can be easily avoided using the following method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spectral clustering</h1>
                </header>
            
            <article>
                
<p>Spectral clustering is a more sophisticated approach based on a symmetric affinity matrix:</p>
<div class="CDPAlignCenter CDPAlign"><img height="64" width="181" src="assets/78106f0d-bdd9-47d8-9516-a352132c5fea.png"/></div>
<p>Here, each element <em>a<sub>ij</sub></em> represents a measure of affinity between two samples. The most diffused measures (<span>also</span><span> </span><span>supported by scikit-learn) are radial basis function and nearest neighbors. However, any kernel can be used if it produces measures that have the same features of a distance (non-negative, symmetric, and increasing).  </span></p>
<p>The Laplacian matrix is computed and a standard clustering algorithm is applied to a subset of eigenvectors (this element is strictly related to each single strategy).<br/>
scikit-learn implements the Shi-Malik algorithm (<em>Shi J., Malik J., Normalized Cuts and Image Segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 22, 08/2000</em>), <span>also known as normalized-cuts, which partitions the samples into two sets (<em>G<sub>1</sub></em> and <em>G<sub>2</sub></em>, which are formally graphs where each point is a vertex and the edges are derived from the normalized Laplacian matrix) so that the weights corresponding to the points inside a cluster are quite higher than the one belonging to the cut. A complete mathematical explanation is beyond the scope of this book; however, in </span><span><em>Von Luxburg U., A Tutorial on Spectral Clustering, 2007</em>, </span><span>you can read a full explanation of many alternative spectral approaches.</span></p>
<p>Let's consider the previous half-moon example. In this case, the affinity (just like for DBSCAN) should be based on the nearest neighbors function; however, it's useful to compare different kernels. In the first experiment, we use an RBF kernel with different values for the <kbd>gamma</kbd> parameter:</p>
<pre><strong>from sklearn.cluster import SpectralClustering</strong><br/><br/><strong>&gt;&gt;&gt; Yss = []</strong><br/><strong>&gt;&gt;&gt; gammas = np.linspace(0, 12, 4)</strong><br/> <br/><strong>&gt;&gt;&gt; for gamma in gammas:</strong><br/><strong>       sc = SpectralClustering(n_clusters=2, affinity='rbf', gamma=gamma)</strong><br/><strong>       Yss.append(sc.fit_predict(X))</strong></pre>
<p>In this algorithm, we need to specify how many clusters we want, so we set the value to 2. The resulting plots are shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="193" width="518" class="image-border" src="assets/6d730f90-a7b3-40fa-8363-db26464084b2.png"/></div>
<p>As you can see, when the scaling factor gamma is increased the separation becomes more accurate; however, considering the dataset, using the nearest neighbors kernel is not necessary in any search:</p>
<pre><strong>&gt;&gt;&gt; sc = SpectralClustering(n_clusters=2, affinity='nearest_neighbors')</strong><br/><strong>&gt;&gt;&gt; Ys = sc.fit_predict(X)</strong></pre>
<p>The resulting plot is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="291" width="371" class="image-border" src="assets/54637c5e-b476-439e-b043-9aefef953a45.png"/></div>
<p>As for many other kernel-based methods, spectral clustering needs a previous analysis to detect which kernel can provide the best values for the affinity matrix. scikit-learn <span>also</span><span> </span><span>allows us to define custom kernels for those tasks that cannot easily be solved using the standard ones.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation methods based on the ground truth</h1>
                </header>
            
            <article>
                
<p>In this section, we present some evaluation methods that require the knowledge of the ground truth. This condition is not always easy to obtain because clustering is normally applied as an unsupervised method; however, in some cases, the training set has been manually (or automatically) labeled, and it's useful to evaluate a model before predicting the clusters of new samples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Homogeneity </h1>
                </header>
            
            <article>
                
<p>An important requirement for a clustering algorithm (given the ground truth) is that each cluster should contain only samples belonging to a single class. In <a href="c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml" target="_blank">Chapter 2</a>, <em>Important Elements in Machine Learning</em>, we have defined the concepts of entropy <em>H(X)</em> and conditional entropy <em>H(X|Y)</em>, which measures the uncertainty of <em>X</em> given the knowledge of <em>Y</em>. Therefore, if the class set is denoted as <em>C</em> and the cluster set as <em>K</em>, <em>H(C|K)</em> is a measure of the uncertainty in determining the right class after having clustered the dataset. To have a homogeneity score, it's necessary to normalize this value considering the initial entropy of the class set <em>H(C)</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="38" width="104" src="assets/741df5c8-8901-4a85-b75e-d9c9f3c50316.png"/></div>
<p>In scikit-learn, there's the built-in function <kbd>homogeneity_score()</kbd> that can be used to compute this value. For this and the next few examples, we assume that we have a labeled dataset <em>X</em> (with true labels <em>Y</em>):</p>
<pre><strong>from sklearn.metrics import homogeneity_score</strong><br/><br/><strong>&gt;&gt;&gt; km = KMeans(n_clusters=4)</strong><br/><strong>&gt;&gt;&gt; Yp = km.fit_predict(X)</strong><br/><strong>&gt;&gt;&gt; print(homogeneity_score(Y, Yp))</strong><br/><strong>0.806560739827</strong></pre>
<p>A value of 0.8 means that there's a residual uncertainty of about 20% because one or more clusters contain some points belonging to a secondary class. As with the other methods shown in the previous section, it's possible to use the homogeneity score to determine the optimal number of clusters. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Completeness</h1>
                </header>
            
            <article>
                
<p>A complementary requirement is that each sample belonging to a class is assigned to the same cluster. This measure can be determined using the conditional entropy <em>H(K|C)</em>, which is the uncertainty in determining the right cluster given the knowledge of the class. Like for the homogeneity score, we need to normalize this using the entropy <em>H(K)</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="34" width="92" src="assets/4cb96f4f-63b6-4ca5-9f76-491a9c46d069.png"/></div>
<p>We can compute this score (on the same dataset) using the function <kbd>completeness_score()</kbd>:</p>
<pre><strong>from sklearn.metrics import completeness_score</strong><br/><br/><strong>&gt;&gt;&gt; km = KMeans(n_clusters=4)</strong><br/><strong>&gt;&gt;&gt; Yp = km.fit_predict(X)</strong><br/><strong>&gt;&gt;&gt; print(completeness_score(Y, Yp))</strong><br/><strong>0.807166746307</strong></pre>
<p>Also, in this case, the value is rather high, meaning that the majority of samples belonging to a class have been assigned to the same cluster. This value can be improved using a different number of clusters or changing the algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adjusted rand index</h1>
                </header>
            
            <article>
                
<p>The adjusted rand index measures the similarity between the original class partitioning (<em>Y</em>) and the clustering. Bearing in mind the same notation adopted in the previous scores, we can define:</p>
<ul>
<li><strong>a</strong>: The number of pairs of elements belonging to the same partition in the class set <em>C</em> and to the same partition in the clustering set <em>K</em></li>
<li><strong>b</strong>: The number of pairs of elements belonging to different partitions in the class set <em>C</em> and to different partitions in the clustering set <em><em>K</em></em></li>
</ul>
<p>If we total number of samples in the dataset is <em>n</em>, the rand index is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="64" width="87" src="assets/bc2c20a7-dd7c-4db5-b25a-621ecb046789.png"/></div>
<p>The <em>Corrected for Chance</em> version is the adjusted rand index, defined as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="42" width="128" src="assets/492e610a-b5ee-4914-94d8-8a4f3a96fc36.png"/></div>
<p>We can compute the adjusted rand score using the function <kbd>adjusted_rand_score()</kbd>:</p>
<pre><strong>from sklearn.metrics import adjusted_rand_score</strong><br/><br/><strong>&gt;&gt;&gt; km = KMeans(n_clusters=4)</strong><br/><strong>&gt;&gt;&gt; Yp = km.fit_predict(X)</strong><br/><strong>&gt;&gt;&gt; print(adjusted_rand_score(Y, Yp))</strong><br/><strong>0.831103137285</strong></pre>
<p>As the adjusted rand score is bounded between -1.0 and 1.0, with negative values representing a bad situation (the assignments are strongly uncorrelated), a score of 0.83 means that the clustering is quite similar to the ground truth. Also, in this case, it's possible to optimize this value by trying different numbers of clusters or clustering strategies.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Karteeka Pavan K., Allam Appa Rao, Dattatreya Rao A. V., and Sridhar G.R., <em>Robust seed selection algorithm for k-means type algorithms</em>, International Journal of Computer Science and Information Technology 3, no. 5 (October 30, 2011)</li>
<li>Shi J., Malik J., <em>Normalized Cuts and Image Segmentation</em>, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, Vol. 22 (08/2000)</li>
<li><span>Von Luxburg U., <em>A Tutorial on Spectral Clustering</em>, 2007</span></li>
<li>Von Luxburg U., <em>Cluster stability: an overview</em>, arXiv 1007:1075v1, 7 July 2010</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced the k-means algorithm, which is based on the idea of defining (randomly or according to some criteria) k centroids that represent the clusters and optimize their position so that the sum of squared distances for every point in each cluster <span>and the centroid</span> is minimum. As the distance is a radial function, k-means assumes clusters to be convex and cannot solve problems where the shapes have deep concavities (like the half-moon problem).<br/>
In order to solve such situations, we presented two alternatives. The first one is called DBSCAN and is a simple algorithm that analyzes the difference between points surrounded by other samples and boundary samples. In this way, it can easily determine high-density areas (which become clusters) and low-density spaces among them. There are no assumptions about the shape or the number of clusters, so it's necessary to tune up the other parameters so as to generate the right number of clusters.<br/>
Spectral clustering is a family of algorithms based on a measure of affinity among samples. They use a classical method (such as k-means) on subspaces generated by the Laplacian of the affinity matrix. In this way, it's possible to exploit the power of many kernel functions to determine the affinity between points, which a simple distance cannot classify correctly. This kind of clustering is particularly efficient for image segmentation, but it can also be a good choice whenever the other methods fail to separate a dataset correctly.<br/>
In the next chapter, we're going to discuss another approach called hierarchical clustering. It allows us to segment data by splitting and merging clusters until a final configuration is reached.</p>


            </article>

            
        </section>
    </body></html>