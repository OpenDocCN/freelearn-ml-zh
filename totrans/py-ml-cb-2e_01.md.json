["```py\n>> import numpy as np\n```", "```py\n>> data = np.array([[3, -1.5, 2, -5.4], [0, 4, -0.3, 2.1], [1, 3.3, -1.9, -4.3]])\n```", "```py\n>> print(data)\n```", "```py\n[[ 3\\. -1.5  2\\.  -5.4]\n [ 0\\.  4\\.  -0.3  2.1]\n [ 1\\.  3.3 -1.9 -4.3]]\n```", "```py\n>> NpArray1 = np.arange(10)\n>> print(NpArray1)\n```", "```py\n[0 1 2 3 4 5 6 7 8 9]\n```", "```py\n>> NpArray2 = np.arange(10, 100, 5)\n>> print(NpArray2)\n```", "```py\n[10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95]\n```", "```py\n>> NpArray3 = np.linspace(0, 10, 50)\n>> print(NpArray3)\n```", "```py\n[ 0\\. 0.20408163 0.40816327 0.6122449 0.81632653 1.02040816\n 1.2244898 1.42857143 1.63265306 1.83673469 2.04081633 2.24489796\n 2.44897959 2.65306122 2.85714286 3.06122449 3.26530612 3.46938776\n 3.67346939 3.87755102 4.08163265 4.28571429 4.48979592 4.69387755\n 4.89795918 5.10204082 5.30612245 5.51020408 5.71428571 5.91836735\n 6.12244898 6.32653061 6.53061224 6.73469388 6.93877551 7.14285714\n 7.34693878 7.55102041 7.75510204 7.95918367 8.16326531 8.36734694\n 8.57142857 8.7755102 8.97959184 9.18367347 9.3877551 9.59183673\n 9.79591837 10\\. ]\n```", "```py\n>> from sklearn import preprocessing\n```", "```py\n>> print(\"Mean: \",data.mean(axis=0))\n>> print(\"Standard Deviation: \",data.std(axis=0))\n```", "```py\nMean: [ 1.33333333 1.93333333 -0.06666667 -2.53333333]\nStandard Deviation: [1.24721913 2.44449495 1.60069429 3.30689515]\n```", "```py\n>> data_standardized = preprocessing.scale(data)\n```", "```py\n>> print(\"Mean standardized data: \",data_standardized.mean(axis=0))\n>> print(\"Standard Deviation standardized data: \",data_standardized.std(axis=0))\n```", "```py\nMean standardized data: [ 5.55111512e-17 -1.11022302e-16 -7.40148683e-17 -7.40148683e-17]\nStandard Deviation standardized data: [1\\. 1\\. 1\\. 1.]\n```", "```py\n>> data_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n```", "```py\n>> data_scaled = data_scaler.fit_transform(data)\n```", "```py\n>> print(\"Min: \",data.min(axis=0))\n>> print(\"Max: \",data.max(axis=0))\n```", "```py\nMin: [ 0\\. -1.5 -1.9 -5.4]\nMax: [3\\. 4\\. 2\\. 2.1]\n```", "```py\n>> print(\"Min: \",data_scaled.min(axis=0))\n>> print(\"Max: \",data_scaled.max(axis=0))\n```", "```py\nMin: [0\\. 0\\. 0\\. 0.]\nMax: [1\\. 1\\. 1\\. 1.]\n```", "```py\n>> print(data_scaled)\n```", "```py\n[[ 1\\.          0\\.          1\\.          0\\.        ] \n [ 0\\.          1\\.          0.41025641  1\\.        ]\n [ 0.33333333  0.87272727  0\\.          0.14666667]]\n```", "```py\n>> data_normalized = preprocessing.normalize(data, norm='l1', axis=0)\n```", "```py\n>> print(data_normalized)\n```", "```py\n[[ 0.75 -0.17045455  0.47619048  -0.45762712]\n [ 0\\.    0.45454545 -0.07142857   0.1779661 ]\n [ 0.25  0.375      -0.45238095  -0.36440678]]\n```", "```py\n>> data_norm_abs = np.abs(data_normalized)\n>> print(data_norm_abs.sum(axis=0))\n```", "```py\n[1\\. 1\\. 1\\. 1.]\n```", "```py\n>> data_binarized = preprocessing.Binarizer(threshold=1.4).transform(data)\n```", "```py\n>> print(data_binarized)\n```", "```py\n[[ 1\\.  0\\.  1\\.  0.]\n [ 0\\.  1\\.  0\\.  1.]\n [ 0\\.  1\\.  0\\.  0.]]\n```", "```py\n>> data = np.array([[1, 1, 2], [0, 2, 3], [1, 0, 1], [0, 1, 0]])\n>> print(data)\n```", "```py\n[[1 1 2]\n [0 2 3]\n [1 0 1]\n [0 1 0]]\n```", "```py\n>> encoder = preprocessing.OneHotEncoder()\n>> encoder.fit(data)\n```", "```py\n>> encoded_vector = encoder.transform([[1, 2, 3]]).toarray()\n```", "```py\n[[0\\. 1\\. 0\\. 0\\. 1\\. 0\\. 0\\. 0\\. 1.]]\n```", "```py\n>> from sklearn import preprocessing\n```", "```py\n>> label_encoder = preprocessing.LabelEncoder()\n```", "```py\n>> input_classes = ['audi', 'ford', 'audi', 'toyota', 'ford', 'bmw']\n```", "```py\n>> label_encoder.fit(input_classes)\n>> print(\"Class mapping: \")\n>> for i, item in enumerate(label_encoder.classes_):\n...    print(item, \"-->\", i)\n```", "```py\nClass mapping:\naudi --> 0\nbmw --> 1\nford --> 2\ntoyota --> 3\n```", "```py\n>> labels = ['toyota', 'ford', 'audi']\n>> encoded_labels = label_encoder.transform(labels)\n>> print(\"Labels =\", labels)\n>> print(\"Encoded labels =\", list(encoded_labels))\n```", "```py\nLabels = ['toyota', 'ford', 'audi']\nEncoded labels = [3, 2, 0]\n```", "```py\n>> encoded_labels = [2, 1, 0, 3, 1]\n>> decoded_labels = label_encoder.inverse_transform(encoded_labels)\n>> print(\"Encoded labels =\", encoded_labels)\n>> print(\"Decoded labels =\", list(decoded_labels))\n```", "```py\nEncoded labels = [2, 1, 0, 3, 1]\nDecoded labels = ['ford', 'bmw', 'audi', 'toyota', 'bmw']\n```", "```py\n1 --> 2\n3 --> 6\n4.3 --> 8.6\n7.1 --> 14.2\n```", "```py\nfilename = \"VehiclesItaly.txt\"\nX = []\ny = []\nwith open(filename, 'r') as f:\n    for line in f.readlines():\n        xt, yt = [float(i) for i in line.split(',')]\n        X.append(xt)\n        y.append(yt)\n```", "```py\nnum_training = int(0.8 * len(X))\nnum_test = len(X) - num_training\n\nimport numpy as np\n\n# Training data\nX_train = np.array(X[:num_training]).reshape((num_training,1))\ny_train = np.array(y[:num_training])\n\n# Test data\nX_test = np.array(X[num_training:]).reshape((num_test,1))\ny_test = np.array(y[num_training:])\n```", "```py\nfrom sklearn import linear_model\n\n# Create linear regression object\nlinear_regressor = linear_model.LinearRegression()\n\n# Train the model using the training sets\nlinear_regressor.fit(X_train, y_train)\n```", "```py\ny_train_pred = linear_regressor.predict(X_train)\n```", "```py\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.scatter(X_train, y_train, color='green')\nplt.plot(X_train, y_train_pred, color='black', linewidth=4)\nplt.title('Training data')\nplt.show()\n```", "```py\ny_test_pred = linear_regressor.predict(X_test)\nplt.figure()\nplt.scatter(X_test, y_test, color='green')\nplt.plot(X_test, y_test_pred, color='black', linewidth=4)\nplt.title('Test data')\nplt.show()\n```", "```py\nimport sklearn.metrics as sm\nprint(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, y_test_pred), 2)) \nprint(\"Mean squared error =\", round(sm.mean_squared_error(y_test, y_test_pred), 2)) \nprint(\"Median absolute error =\", round(sm.median_absolute_error(y_test, y_test_pred), 2)) \nprint(\"Explain variance score =\", round(sm.explained_variance_score(y_test, y_test_pred), 2)) \nprint(\"R2 score =\", round(sm.r2_score(y_test, y_test_pred), 2))\n```", "```py\nMean absolute error = 241907.27\nMean squared error = 81974851872.13\nMedian absolute error = 240861.94\nExplain variance score = 0.98\nR2 score = 0.98\n```", "```py\nimport pickle\n\noutput_model_file = \"3_model_linear_regr.pkl\"\n\nwith open(output_model_file, 'wb') as f:\n    pickle.dump(linear_regressor, f) \n```", "```py\nwith open(output_model_file, 'rb') as f:\n    model_linregr = pickle.load(f)\n\ny_test_pred_new = model_linregr.predict(X_test)\nprint(\"New mean absolute error =\", round(sm.mean_absolute_error(y_test, y_test_pred_new), 2))\n```", "```py\nNew mean absolute error = 241907.27\n```", "```py\nfrom sklearn import linear_model\nridge_regressor = linear_model.Ridge(alpha=0.01, fit_intercept=True, max_iter=10000)\n```", "```py\nridge_regressor.fit(X_train, y_train)\ny_test_pred_ridge = ridge_regressor.predict(X_test)\nprint( \"Mean absolute error =\", round(sm.mean_absolute_error(y_test, y_test_pred_ridge), 2))\nprint( \"Mean squared error =\", round(sm.mean_squared_error(y_test, y_test_pred_ridge), 2))\nprint( \"Median absolute error =\", round(sm.median_absolute_error(y_test, y_test_pred_ridge), 2))\nprint( \"Explain variance score =\", round(sm.explained_variance_score(y_test, y_test_pred_ridge), 2))\nprint( \"R2 score =\", round(sm.r2_score(y_test, y_test_pred_ridge), 2))\n```", "```py\nimport numpy as np\n\nTime = np.array([6, 8, 11, 14, 16, 18, 19])\nTemp = np.array([4, 7, 10, 12, 11.5, 9, 7])\n```", "```py\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.plot(Time, Temp, 'bo')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Temp\")\nplt.title('Temperature versus time')\nplt.show()\n```", "```py\nbeta = np.polyfit(Time, Temp, 2)\n```", "```py\np = np.poly1d(beta)\n```", "```py\nxp = np.linspace(6, 19, 100)\nplt.figure()\nplt.plot(Time, Temp, 'bo', xp, p(xp), '-')\nplt.show()\n```", "```py\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn import datasets\nfrom sklearn.metrics import mean_squared_error, explained_variance_score\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\n```", "```py\nhousing_data = datasets.load_boston()\n```", "```py\nX, y = shuffle(housing_data.data, housing_data.target, random_state=7)\n```", "```py\nnum_training = int(0.8 * len(X))\nX_train, y_train = X[:num_training], y[:num_training]\nX_test, y_test = X[num_training:], y[num_training:]\n```", "```py\ndt_regressor = DecisionTreeRegressor(max_depth=4)\ndt_regressor.fit(X_train, y_train)\n```", "```py\nab_regressor = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=400, random_state=7)\nab_regressor.fit(X_train, y_train)\n```", "```py\ny_pred_dt = dt_regressor.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_dt)\nevs = explained_variance_score(y_test, y_pred_dt)\nprint(\"#### Decision Tree performance ####\")\nprint(\"Mean squared error =\", round(mse, 2))\nprint(\"Explained variance score =\", round(evs, 2))\n```", "```py\ny_pred_ab = ab_regressor.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_ab)\nevs = explained_variance_score(y_test, y_pred_ab)\nprint(\"#### AdaBoost performance ####\")\nprint(\"Mean squared error =\", round(mse, 2))\nprint(\"Explained variance score =\", round(evs, 2))\n```", "```py\n#### Decision Tree performance ####\nMean squared error = 14.79\nExplained variance score = 0.82\n\n#### AdaBoost performance ####\nMean squared error = 7.54\nExplained variance score = 0.91\n```", "```py\nDTFImp= dt_regressor.feature_importances_\nDTFImp= 100.0 * (DTFImp / max(DTFImp))\nindex_sorted = np.flipud(np.argsort(DTFImp))\npos = np.arange(index_sorted.shape[0]) + 0.5\n```", "```py\nplt.figure()\nplt.bar(pos, DTFImp[index_sorted], align='center')\nplt.xticks(pos, housing_data.feature_names[index_sorted])\nplt.ylabel('Relative Importance')\nplt.title(\"Decision Tree regressor\")\nplt.show()\n```", "```py\nABFImp= ab_regressor.feature_importances_ \nABFImp= 100.0 * (ABFImp / max(ABFImp))\nindex_sorted = np.flipud(np.argsort(ABFImp))\npos = np.arange(index_sorted.shape[0]) + 0.5\n```", "```py\nplt.figure()\nplt.bar(pos, ABFImp[index_sorted], align='center')\nplt.xticks(pos, housing_data.feature_names[index_sorted])\nplt.ylabel('Relative Importance')\nplt.title(\"AdaBoost regressor\")\nplt.show()\n```", "```py\nimport csv\nimport numpy as np\n```", "```py\nfilename=\"bike_day.csv\"\nfile_reader = csv.reader(open(filename, 'r'), delimiter=',')\nX, y = [], []\nfor row in file_reader:\n    X.append(row[2:13])\n    y.append(row[-1])\n```", "```py\nfeature_names = np.array(X[0])\n```", "```py\nX=np.array(X[1:]).astype(np.float32)\ny=np.array(y[1:]).astype(np.float32)\n```", "```py\nfrom sklearn.utils import shuffle\nX, y = shuffle(X, y, random_state=7)  \n```", "```py\nnum_training = int(0.9 * len(X))\nX_train, y_train = X[:num_training], y[:num_training]\nX_test, y_test = X[num_training:], y[num_training:]\n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor\nrf_regressor = RandomForestRegressor(n_estimators=1000, max_depth=10, min_samples_split=2)\nrf_regressor.fit(X_train, y_train)\n```", "```py\ny_pred = rf_regressor.predict(X_test)\nfrom sklearn.metrics import mean_squared_error, explained_variance_score\nmse = mean_squared_error(y_test, y_pred)\nevs = explained_variance_score(y_test, y_pred)\nprint( \"#### Random Forest regressor performance ####\")\nprint(\"Mean squared error =\", round(mse, 2))\nprint(\"Explained variance score =\", round(evs, 2))\n```", "```py\n#### Random Forest regressor performance ####\nMean squared error = 357864.36\nExplained variance score = 0.89\n```", "```py\nRFFImp= rf_regressor.feature_importances_ \nRFFImp= 100.0 * (RFFImp / max(RFFImp))\nindex_sorted = np.flipud(np.argsort(RFFImp))\npos = np.arange(index_sorted.shape[0]) + 0.5\n```", "```py\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.bar(pos, RFFImp[index_sorted], align='center')\nplt.xticks(pos, feature_names[index_sorted])\nplt.ylabel('Relative Importance')\nplt.title(\"Random Forest regressor\")\nplt.show()\n```", "```py\nX.append(row[2:15])\n```", "```py\n#### Random Forest regressor performance ####\nMean squared error = 22552.26\nExplained variance score = 0.99\n```", "```py\nfilename=\"bike_hour.csv\"\nfile_reader = csv.reader(open(filename, 'r'), delimiter=',')\nX, y = [], []\nfor row in file_reader:\n    X.append(row[2:14])\n    y.append(row[-1])\n```", "```py\n#### Random Forest regressor performance ####\nMean squared error = 2613.86\nExplained variance score = 0.92\n```"]