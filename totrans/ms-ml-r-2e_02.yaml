- en: Linear Regression - The Blocking and Tackling of Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Some people try to find things in this game that don''t exist, but football
    is only two things - blocking and tackling."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Vince Lombardi, Hall of Fame Football Coach'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important that we get started with a simple, yet extremely effective
    technique that has been used for a long time: **linear regression**. Albert Einstein
    is believed to have remarked at one time or another that things should be made
    as simple as possible, but no simpler. This is sage advice and a good rule of
    thumb in the development of algorithms for machine learning. Considering the other
    techniques that we will discuss later, there is no simpler model than tried and
    tested linear regression, which uses the **least squares approach** to predict
    a quantitative outcome. In fact, one can consider it to be the foundation of all
    the methods that we will discuss later, many of which are mere extensions. If
    you can master the linear regression method, well, then quite frankly, I believe
    you can master the rest of this book. Therefore, let us consider this a good starting point
    for our journey towards becoming a machine learning guru.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers introductory material, and an expert in this subject can
    skip ahead to the next topic. Otherwise, ensure that you thoroughly understand
    this topic before venturing to other, more complex learning methods. I believe
    you will discover that many of your projects can be addressed by just applying
    what is discussed in the following section. Linear regression is probably the
    easiest model to explain to your customers, most of whom will have at least a
    cursory understanding of **R-squared**. Many of them will have been exposed to
    it at great depth and thus be comfortable with variable contribution, collinearity,
    and the like.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin by looking at a simple way to predict a quantitative response, *Y*,
    with one predictor variable, *x*, assuming that *Y* has a linear relationship
    with *x*. The model for this can be written as, *Y = B0 + B1x + e*. We can state
    it as the expected value of *Y* being a function of the parameters *B0* (the intercept)
    plus *B1* (the slope) times *x*, plus an error term *e*. The least squares approach
    chooses the model parameters that minimize the **Residual Sum of Squares** (**RSS**)
    of the predicted *y* values versus the actual *Y* values. For a simple example,
    let's say we have the actual values of *Y1* and *Y2* equal to *10* and *20* respectively,
    along with the predictions of *y1* and *y2* as *12* and *18*. To calculate RSS,
    we add the squared differences *RSS = (Y1 - y1)² + (Y2 - y2)²*, which, with simple
    substitution, yields *(10 - 12)² + (20 - 18)² = 8*.
  prefs: []
  type: TYPE_NORMAL
- en: I once remarked to a peer during our Lean Six Sigma Black Belt training that
    it's all about the sum of squares; understand the sum of squares and the rest
    will flow naturally. Perhaps that is true, at least to some extent.
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin with an application, I want to point out that, if you read the
    headlines of various research breakthroughs, you should do so with a jaded eye
    and a skeptical mind as the conclusion put forth by the media may not be valid.
    As we shall see, R, and any other software for that matter, will give us a solution
    regardless of the inputs. However, just because the math makes sense and a high
    correlation or R-squared statistic is reported doesn't mean that the conclusion
    is valid.
  prefs: []
  type: TYPE_NORMAL
- en: 'To drive this point home, let''s have a look at the famous `Anscombe` dataset,
    which is available in R. The statistician Francis Anscombe produced this set to
    highlight the importance of data visualization and outliers when analyzing data.
    It consists of four pairs of *X* and *Y* variables that have the same statistical
    properties but when plotted show something very different. I have used the data
    to train colleagues and to educate business partners on the hazards of fixating
    on statistics without exploring the data and checking assumptions. I think this
    is a good place to start should you have a similar need. It is a brief digression
    before moving on to serious modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As we shall see, each of the pairs has the same correlation coefficient: `0.816`.
    The first two are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The real insight here, as `Anscombe` intended, is when we plot all the four
    pairs together, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Downloading the example code
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, **Plot 1** appears to have a true linear relationship, **Plot
    2** is curvilinear, **Plot 3** has a dangerous outlier, and **Plot 4** is driven
    by one outlier. There you have it, a cautionary tale about  the dangers of solely
    relying on correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our first case focuses on the goal of predicting the water yield (in inches)
    of the Snake River Watershed in Wyoming, USA, as a function of the water content
    of the year''s snowfall. This forecast will be useful in managing the water flow
    and reservoir levels as the Snake River provides much-needed irrigation water
    for the farms and ranches of several western states. The `snake` dataset is available
    in the `alr3` package (note that alr stands for applied linear regression):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have `17` observations, data exploration can begin. But first,
    let''s change `X` and `Y` to meaningful variable names, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_02.png)'
  prefs: []
  type: TYPE_IMG
- en: This is an interesting plot as the data is linear and has a slight curvilinear
    shape driven by two potential outliers at both ends of the extreme. As a result,
    transforming the data or deleting an outlying observation may be warranted.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform a linear regression in R, one uses the `lm()` function to create
    a model in the standard form of *fit = lm(Y ~ X)*. You can then test your assumptions
    using various functions on your fitted model by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With the `summary()` function, we can examine a number of items including the
    model specification, descriptive statistics about the residuals, the coefficients,
    codes to model significance, and a summary on model error and fit. Right now,
    let's focus on the parameter coefficient estimates, see if our predictor variable
    has a significant p-value, and if the overall model F-test has a significant p-value.
    Looking at the parameter estimates, the model tells us that the `yield` is equal
    to `0.72538` plus `0.49808` times the `content`. It can be stated that, for every
    1 unit change in the content, the yield will increase by `0.49808` units. The `F-statistic`
    is used to test the null hypothesis that the model coefficients are all 0.
  prefs: []
  type: TYPE_NORMAL
- en: Since the `p-value` is highly significant, we can reject the null and move on
    to the t-test for content, which tests the null hypothesis that it is 0\. Again,
    we can reject the null. Additionally, we can see `Multiple R-squared` and `Adjusted
    R-squared` values. `Adjusted R-squared` will be covered under the multivariate
    regression topic, so let's zero in on `Multiple R-squared`; here we see that it
    is `0.8709`. In theory, it can range from 0 to 1 and is a measure of the strength
    of the association between *X* and *Y*. The interpretation in this case is that
    87 percent of the variation in the **water yield** can be explained by the **water
    content of snow**. On a side note, R-squared is nothing more than the correlation
    coefficient of [X, Y] squared.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can recall our scatterplot and now add the best fit line produced by our
    model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A linear regression model is only as good as the validity of its assumptions,
    which can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linearity**: This is a linear relationship between the predictor and the
    response variables. If this relationship is not clearly present, transformations
    (log, polynomial, exponent, and so on) of *X* or *Y* may solve the problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-correlation of errors**: A common problem in time series and panel data
    where *e[n] = beta[n-1]*; if the errors are correlated, you run the risk of creating
    a poorly specified model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Homoscedasticity**: Normally the distributed and constant variance of errors,
    which means that the variance of errors is constant across different values of
    inputs. Violations of this assumption can create biased coefficient estimates,
    leading to statistical tests for significance that can be either too high or too
    low. This, in turn, leads to a wrong conclusion. This violation is referred to
    as **heteroscedasticity.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No collinearity**: No linear relationship between two predictor variables,
    which is to say that there should be no correlation between the features. This,
    again, can lead to biased estimates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Presence of outliers**: Outliers can severely skew the estimation, and ideally
    they must be removed prior to fitting a model using linear regression; As we saw
    in the Anscombe example, this can lead to a biased estimate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we are building a univariate model independent of time, we will concern
    ourselves only with linearity and heteroscedasticity. The other assumptions will
    become important in the next section. The best way to initially check the assumptions
    is by producing plots. The `plot()` function, when combined with a linear model
    fit, will automatically produce four plots allowing you to examine the assumptions.
    R produces the plots one at a time and you advance through them by hitting the
    *Enter* key. It is best to examine all four simultaneously and we do it in the
    following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_04.png)'
  prefs: []
  type: TYPE_IMG
- en: The two plots on the left allow us to examine the homoscedasticity of errors
    and non-linearity. What we are looking for is some type of pattern or, more importantly,
    that no pattern exists. Given the sample size of only 17 observations, nothing
    obvious can be seen. Common heteroscedastic errors will appear to be u-shaped,
    inverted u-shaped, or clustered close together on the left of the plot. They will
    become wider as the fitted values increase (a funnel shape). It is safe to conclude
    that no violation of homoscedasticity is apparent in our model.
  prefs: []
  type: TYPE_NORMAL
- en: The **Normal Q-Q** plot in the upper-right corner helps us to determine if the
    residuals are normally distributed. The **Quantile-Quantile** (**Q-Q**) represents
    the quantile values of one variable plotted against the quantile values of another.
    It appears that the outliers (observations **7**, **9**, and **10**), may be causing
    a violation of the assumption. The **Residuals vs Leverage** plot can tell us
    what observations, if any, are unduly influencing the model; in other words, if
    there are any outliers we should be concerned about. The statistic is **Cook's
    distance** or **Cook's D**, and it is generally accepted that a value greater
    than 1 should be worthy of further inspection.
  prefs: []
  type: TYPE_NORMAL
- en: What exactly is further inspection? This is where art meets science. The easy
    way out would be to simply delete the observation, in this case number **9**,
    and redo the model. However, a better option may be to transform the predictor
    and/or the response variables. If we just delete observation **9**, then maybe
    observations **10** and **13** would fall outside the band for greater than 1\.
    I believe that this is where domain expertise can be critical. More times than
    I can count, I have found that exploring and understanding outliers can yield
    valuable insights. When we first examined the previous scatterplot, I pointed
    out the potential outliers and these happen to be observations number **9** and
    number **13**. As an analyst, it would be critical to discuss with the appropriate
    subject matter experts to understand why this is the case. Is it a measurement
    error? Is there a logical explanation for these observations? I certainly don't
    know, but this is an opportunity to increase the value that you bring to an organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having said that, we can drill down on the current model by examining, in more
    detail, the **Normal Q-Q** plot. R does not provide confidence intervals to the
    default Q-Q plot, and given our concerns in looking at the base plot, we should
    check the confidence intervals. The `qqPlot()` function of the `car` package automatically
    provides these confidence intervals. Since the `car` package is loaded along with
    the `alr3` package, I can produce the plot with one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_05.png)'
  prefs: []
  type: TYPE_IMG
- en: According to the plot, the residuals are normally distributed. I think this
    can give us the confidence to select the model with all the observations. A clear
    rationale and judgment would be needed to attempt other models. If we could clearly
    reject the assumption of normally distributed errors, then we would probably have
    to examine the variable transformations and/or observation deletion.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may be asking yourself whether you will ever have just one predictor variable
    in the real world. That is indeed a fair question and certainly a very rare case
    (time series can be a common exception). Most likely, several, if not many, predictor
    variables or features--as they are affectionately termed in machine learning--will
    have to be included in your model. And with that, let's move on to multivariate
    linear regression and a new business case.
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In keeping with the water conservation/prediction theme, let's look at another
    dataset in the `alr3` package, appropriately named `water`. During the writing
    of the first edition of this book, the severe drought in Southern California caused
    much alarm. Even the Governor, Jerry Brown, began to take action with a call to
    citizens to reduce water usage by 20 percent. For this exercise, let's say we
    have been commissioned by the state of California to predict water availability.
    The data provided to us contains 43 years of snow precipitation, measured at six
    different sites in the Owens Valley. It also contains a response variable for
    water availability as the stream runoff volume near Bishop, California, which
    feeds into the Owens Valley aqueduct, and eventually the Los Angeles aqueduct.
    Accurate predictions of the stream runoff will allow engineers, planners, and
    policy makers to plan conservation measures more effectively. The model we are
    looking to create will consist of the form *Y = B0 + B1x1 +...Bnxn + e*, where
    the predictor variables (features) can be from 1 to *n*.
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding and preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin, we will load the dataset named `water` and define the structure of
    the `str()` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we have eight features and one response variable, `BSAAM`. The observations
    start in 1943 and run for 43 consecutive years. Since for this exercise we are
    not concerned with what year the observations occurred in, it makes sense to create
    a new data frame excluding the year vector. This is quite easy to do. With one
    line of code, we can create the new data frame, and then verify that it works
    with the `head()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: With all the features being quantitative, it makes sense to look at the correlation
    statistics and then produce a matrix of scatterplots. The correlation coefficient
    or **Pearson's r**, is a measure of both the strength and direction of the linear
    relationship between two variables. The statistic will be a number between -1
    and 1, where -1 is the total negative correlation and +1 is the total positive
    correlation. The calculation of the coefficient is the covariance of the two variables
    divided by the product of their standard deviations. As previously discussed,
    if you square the correlation coefficient, you will end up with R-squared.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of ways to produce a matrix of correlation plots. Some prefer
    to produce **heatmaps**, but I am a big fan of what is produced with the `corrplot`
    package. It can produce a number of different variations including ellipse, circle,
    square, number, shade, color, and pie. I like the `ellipse` method, but feel free
    to experiment with the others. Let''s load the `corrplot` package, create a correlation
    object using the base `cor()` function, and examine the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'So, what does this tell us? First of all, the response variable is highly and
    positively correlated with the OP features with `OPBPC` as `0.8857`, `OPRC` as
    `0.9196`, and `OPSLAKE` as `0.9384`. Also note that the AP features are highly
    correlated with each other and the OP features as well. The implication is that
    we may run into the issue of multi-collinearity. The correlation plot matrix provides
    a nice visual of the correlations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another popular visual is a scatterplot matrix. This can be called with the
    `pairs()` function. It reinforces what we saw in the correlation plot in the previous
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Modeling and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key elements that we will cover here is the very important task of
    feature selection. In this chapter, we will discuss the best subsets regression
    methods stepwise, using the `leaps` package. Later chapters will cover more advanced
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward stepwise selection** starts with a model that has zero features;
    it then adds the features one at a time until all the features are added. A selected
    feature is added in the process that creates a model with the lowest RSS. So in
    theory, the first feature selected should be the one that explains the response
    variable better than any of the others, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that adding a feature will always decrease RSS and increase
    R-squared, but it will not necessarily improve the model `fit` and interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: '**Backward stepwise regression** begins with all the features in the model
    and removes the least useful, one at a time. A hybrid approach is available where
    the features are added through forward stepwise regression, but the algorithm
    then examines if any features that no longer improve the model fit can be removed.
    Once the model is built, the analyst can examine the output and use various statistics
    to select the features they believe provide the best fit.'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to add here that stepwise techniques can suffer from serious
    issues. You can perform a forward stepwise on a dataset, then a backward stepwise,
    and end up with two completely conflicting models. The bottomline is that stepwise
    can produce biased regression coefficients; in other words, they are too large
    and the confidence intervals are too narrow (Tibshirani, 1996).
  prefs: []
  type: TYPE_NORMAL
- en: Best subsets regression can be a satisfactory alternative to the stepwise methods
    for feature selection. In best subsets regression, the algorithm fits a model
    for all the possible feature combinations; so if you have 3 features, 7 models
    will be created. As with stepwise regression, the analyst will need to apply judgment
    or statistical analysis to select the optimal model. Model selection will be the
    key topic in the discussion that follows. As you might have guessed, if your dataset
    has many features, this can be quite a task, and the method does not perform well
    when you have more features than observations (`p` is greater than `n`).
  prefs: []
  type: TYPE_NORMAL
- en: Certainly, these limitations for best subsets do not apply to our task at hand.
    Given its limitations, we will forgo stepwise, but please feel free to give it
    a try. We will begin by loading the `leaps` package. In order that we may see
    how feature selection works, we will first build and examine a model with all
    the features, then drill down with best subsets to select the best fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a linear model with all the features, we can again use the `lm()`
    function. It will follow the form: *fit = lm(y ~ x1 + x2 + x3...xn)*. A neat shortcut,
    if you want to include all the features, is to use a period after the tilde symbol
    instead of having to type them all in. For starters, let''s load the `leaps` package
    and build a model with all the features for examination as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Just like univariate regression, we examine the `p-value` on the `F-statistic`
    to verify that at least one of the coefficients is not zero.  Indeed, the `p-value`
    is highly significant. We should also have significant `p-values` for the `OPRC`
    and `OPSLAKE` parameters. Interestingly, `OPBPC` is not significant despite being
    highly correlated with the response variable. In short, when we control for the
    other OP features, `OPBPC` no longer explains any meaningful variation of the
    predictor, which is to say that the feature `OPBPC` adds nothing from a statistical
    standpoint with `OPRC` and `OPSLAKE` in the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the first model built, let''s move on to best subsets. We create the `sub.fit`
    object using the `regsubsets()` function of the `leaps` package as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create the `best.summary` object to examine the models further. As
    with all R objects, you can use the `names()` function to list what outputs are
    available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Other valuable functions in model selection include `which.min()` and `which.max()`.
    These functions will provide the model that has the minimum or maximum value respectively,
    as shown in following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The code tells us that the model with six features has the smallest RSS, which
    it should have, as that is the maximum number of inputs and more inputs mean a
    lower RSS. An important point here is that adding features will always decrease
    RSS! Furthermore, it will always increase R-squared. We could add a completely
    irrelevant feature such as the number of wins for the Los Angeles Lakers and RSS
    would decrease and R-squared would increase. The amount would likely be miniscule,
    but present nonetheless. As such, we need an effective method to properly select
    the relevant features.
  prefs: []
  type: TYPE_NORMAL
- en: 'For feature selection, there are four statistical methods that we will talk
    about in this chapter: **Aikake''s Information** **Criterion** (**AIC**), **Mallow''s
    Cp** (**Cp**), **Bayesian Information Criterion** (**BIC**), and the adjusted
    R-squared. With the first three, the goal is to minimize the value of the statistic;
    with adjusted R-squared, the goal is to maximize the statistics value. The purpose
    of these statistics is to create as parsimonious a model as possible, in other
    words, to penalize model complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The formulation of these four statistics is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_08-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a linear model, `AIC` and `Cp` are proportional to each other, so we will
    only concern ourselves with `Cp`, which follows the output available in the `leaps`
    package. `BIC` tends to select the models with fewer variables than `Cp`, so we
    will compare both. To do so, we can create and analyze two plots side by side.
    Let''s do this for `Cp`, followed by `BIC`,with the help of the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the plot on the left-hand side, the model with three features has the lowest
    **cp**. The plot on the right-hand side displays those features that provide the
    lowest **Cp**. The way to read this plot is to select the lowest **Cp** value
    at the top of the y axis, which is **1.2**. Then, move to the right and look at
    the colored blocks corresponding to the x axis. Doing this, we see that **APSLAKE**,
    **OPRC**, and **OPSLAKE** are the features included in this specific model. By
    using the `which.min()` and `which.max()` functions, we can identify how **cp**
    compares to BIC and the adjusted R-squared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, BIC and adjusted R-squared match the **Cp** for the optimal
    model. Now, just as with univariate regression, we need to examine the model and
    test the assumptions. We''ll do this by creating a linear model object and examining
    the plots much as we did earlier, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With the three-feature model, `F-statistic` and all the t-tests have significant
    p-values. Having passed the first test, we can produce our diagnostic plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the plots, it seems safe to assume that the residuals have a constant
    variance and are normally distributed. There is nothing in the leverage plot that
    would indicate a requirement for further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: To investigate the issue of collinearity, one can call up the **Variance Inflation
    Factor** (**VIF**) statistic. VIF is the ratio of the variance of a feature's
    coefficient, when fitting the full model, divided by the feature's coefficient
    variance when fitted by itself. The formula is *1 / (1-R²[i])*, where `R2i` is
    the R-squared for our feature of interest, `i`, being regressed by all the other
    features. The minimum value that the VIF can take is 1, which means no collinearity
    at all. There are no hard and fast rules, but in general a VIF value that exceeds
    5 (or some say 10) indicates a problematic amount of collinearity (James, p.101,
    2013). A precise value is difficult to select, because there is no hard statistical
    cut-off point for when multi-collinearity makes your model unacceptable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `vif()` function in the `car` package is all that is needed to produce
    the values, as can be seen in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It shouldn''t be surprising that we have a potential collinearity problem with
    **OPRC** and **OPSLAKE** (values greater than 5) based on the correlation analysis.
    A plot of the two variables drives the point home, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The simple solution to address collinearity is to drop the variables to remove
    the problem without compromising the predictive ability. If we look at the adjusted
    R-squared from the best subsets, we can see that the two-variable model of APSLAKE
    and OPSLAKE has produced a value of `0.90`, while adding OPRC has only marginally
    increased it to `0.92`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s have a look at the two-variable model and test its assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The model is significant, and the diagnostics do not seem to be a cause for
    concern. This should take care of our collinearity problem as well and we can
    check that using the `vif()` function again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As I stated previously, I don''t believe the plot of fits versus residuals
    is of concern, but if you have any doubts you can formally test the assumption
    of the constant variance of errors in R. This test is known as the **Breusch-Pagan**
    (**BP**) test. For this, we need to load the `lmtest` package, and run one line
    of code. The BP test has the null hypothesis that the error variances are zero
    versus the alternative of not zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We do not have evidence to reject the null that implies that the error variances
    are zero because `p-value = 0.9977`. The `BP = 0.0046` value in the summary of
    the test is the chi-squared value.
  prefs: []
  type: TYPE_NORMAL
- en: 'All things considered, it appears that the best predictive model is with the
    two features APSLAKE and OPSLAKE. The model can explain 90 percent of the variation
    in the stream runoff volume. To forecast the runoff, it would be equal to 19,145
    (the intercept) plus 1,769 times the measurement at APSLAKE plus 3,690 times the
    measurement at OPSLAKE. A scatterplot of the `Predicted vs. Actual` values can
    be done in base R using the fitted values from the model and the response variable
    values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Although informative, the base graphics of R are not necessarily ready for
    a presentation to be made to business partners. However, we can easily spruce
    up this plot in R. Several packages to improve graphics are available for this
    example, I will use `ggplot2`. Before producing the plot, we must put the predicted
    values into our data frame, `socal.water`. I also want to rename `BSAAM` as `Actual` and
    put in a new vector within the data frame, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we will load the `ggplot2` package and produce a nicer graphic with just
    one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s examine one final model selection technique before moving on. In the
    upcoming chapters, we will be discussing cross-validation at some length. Cross-validation
    is a widely used and effective method of model selection and testing. Why is this
    necessary at all? It comes down to the bias-variance trade-off. Professor Tarpey
    of Wright State University has a nice quote on the subject:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Often we use regression models to predict future observations. We can use
    our data to fit the model. However, it is cheating to then access how well the
    model predicts responses using the same data that was used to estimate the model
    - this will tend to give overly optimistic results in terms of how well a model
    is able to predict future observations. If we leave out an observation, fit the
    model and then predict the left out response, then this will give a less biased
    idea of how well the model predicts."'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cross-validation technique discussed by Professor Tarpey in the preceding
    quote is known as the **Leave-One-Out Cross-Validation** (**LOOCV**). In linear
    models, you can easily perform an LOOCV by examining the **Prediction Error Sum
    of Squares** (**PRESS**) statistic and selecting the model that has the lowest
    value. The R library `MPV` will calculate the statistic for you, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'By this statistic alone, we could select our `best.fit` model. However, as
    described previously, I still believe that the more parsimonious model is better
    in this case. You can build a simple function to calculate the statistic on your
    own, taking advantage of some elegant matrix algebra as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '"What are `hatvalues`?" you might ask. Well, if we take our linear model *Y
    = B0 + B1x + e*, we can turn this into a matrix notation: *Y = XB + E*. In this
    notation, `Y` remains unchanged, `X` is the matrix of the input values, `B` is
    the coefficient, and `E` represents the errors. This linear model solves for the
    value of `B`. Without going into the painful details of matrix multiplication,
    the regression process yields what is known as a **Hat Matrix**. This matrix maps,
    or as some say projects, the calculated values of your model to the actual values;
    as a result, it captures how influential a specific observation is in your model.
    So, the sum of the squared residuals divided by 1 minus `hatvalues` is the same
    as LOOCV.'
  prefs: []
  type: TYPE_NORMAL
- en: Other linear model considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before moving on, there are two additional linear model topics that we need
    to discuss. The first is the inclusion of a qualitative feature, and the second
    is an interaction term; both are explained in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A qualitative feature, also referred to as a factor, can take on two or more
    levels such as Male/Female or Bad/Neutral/Good. If we have a feature with two
    levels, say gender, then we can create what is known as an indicator or dummy
    feature, arbitrarily assigning one level as `0` and the other as `1`. If we create
    a model with just the indicator, our linear model would still follow the same
    formulation as before, that is, *Y = B0 + B1x + e*. If we code the feature as
    male being equal to 0 and female equal to 1, then the expectation for male would
    just be the intercept *B0*, while for female it would be *B0 + B1x*. In the situation
    where you have more than two levels of the feature, you can create n-1 indicators;
    so, for three levels you would have two indicators. If you created as many indicators
    as levels, you would fall into the dummy variable trap, which results in perfect
    multi-collinearity.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine a simple example to learn how to interpret the output. Let''s
    load the `ISLR` package and build a model with the `Carseats` dataset using the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'For this example, we will predict the sales of `Carseats` using just `Advertising`,
    a quantitative feature and the qualitative feature `ShelveLoc`, which is a factor
    of three levels: `Bad`, `Good`, and `Medium`. With factors, R will automatically
    code the indicators for the analysis. We build and analyze the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If the shelving location is good, the estimate of sales is almost double of
    that when the location is bad, given an intercept of `4.89662`. To see how R codes
    the indicator features, you can use the `contrasts()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Interaction terms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Interaction terms are similarly easy to code in R. Two features interact if
    the effect on the prediction of one feature depends on the value of the other
    feature. This would follow the formulation, *Y = B0 + B1x + B2x + B1B2x + e*.
    An example is available in the `MASS` package with the `Boston` dataset. The response
    is the median home value, which is `medv` in the output. We will use two features:
    the percentage of homes with a low socioeconomic status, which is termed `lstat`,
    and the age of the home in years, which is termed `age` in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Using *feature1*feature2* with the `lm()` function in the code puts both the
    features as well as their interaction term in the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Examining the output, we can see that, while the socioeconomic status is a highly
    predictive feature, the age of the home is not. However, the two features have
    a significant interaction to positively explain the home value.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of machine learning, we train a model and test it to predict
    or forecast an outcome. In this chapter, we had an in-depth look at the simple
    yet extremely effective method of linear regression to predict a quantitative
    response. Later chapters will cover more advanced techniques, but many of them
    are mere extensions of what we have learned in this chapter. We discussed the
    problem of not visually inspecting the dataset and simply relying on the statistics
    to guide you in model selection.
  prefs: []
  type: TYPE_NORMAL
- en: With just a few lines of code, you can make powerful and insightful predictions
    to support decision-making. Not only is it simple and effective, but also you
    can include quantitative variables and interaction terms among the features. Indeed,
    this is a method that anyone delving into the world of machine learning must master.
  prefs: []
  type: TYPE_NORMAL
