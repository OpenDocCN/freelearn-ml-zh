- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning Pipelines and MLOps with LightGBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter shifts the focus from data science and modeling problems to building
    production services for our ML solutions. We introduce the concept of machine
    learning pipelines, a systematic approach to processing data, and building models
    that ensure consistency and correctness.
  prefs: []
  type: TYPE_NORMAL
- en: We also introduce the concept of MLOps, a practice that blends DevOps and ML
    and addresses the need to deploy and maintain production-capable ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter includes an example of building an ML pipeline using scikit-learn,
    encapsulating data processing, model building, and tuning. We show how to wrap
    the pipeline in a web API, exposing a secure endpoint for prediction. Finally,
    we also look at the containerization of the system and deployment to Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics of this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of MLOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying an ML pipeline for customer churn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The chapter includes examples of creating scikit-learn pipelines, training LightGBM
    models, and building a FastAPI application. The requirements for setting up your
    environment can be found alongside the complete code examples at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-8](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-8).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing machine learning pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 6*](B16690_06.xhtml#_idTextAnchor094), *Solving Real-World Data
    Science Problems with LightGBM*, we gave a detailed overview of the data science
    life cycle, which includes various steps to train an ML model. If we were to focus
    only on the steps required to train a model, given data that has already been
    collected, those would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning and preparation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model training and tuning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model evaluation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In previous case studies, we applied these steps manually while working through
    a Jupyter notebook. However, what would happen if we shifted the context to a
    long-term ML project? If we had to repeat the process when new data becomes available,
    we’d have to follow the same procedure to build a model successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, when we want to use the model to score new data, we must apply the
    steps correctly and with the correct parameters and configuration every time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a sense, these steps form a pipeline for data: data enters the pipeline,
    and a deployable model results from its completion.'
  prefs: []
  type: TYPE_NORMAL
- en: Formally, an ML pipeline is a systematic and automated process that guides the
    workflow of an ML project. It involves several interconnected stages, encapsulating
    the steps listed previously.
  prefs: []
  type: TYPE_NORMAL
- en: An ML pipeline aims to ensure that these tasks are structured, reproducible,
    and efficient, making it easier to manage complex ML tasks. Pipelines are particularly
    beneficial when working with large datasets or when the steps to transform raw
    data into usable inputs for ML models are complex and must be repeated frequently,
    such as in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is some fluidity in the steps involved in a pipeline: steps may be added
    or removed depending on how the pipeline is utilized. Some pipelines include a
    data collection step, pulling data from various data sources or databases, and
    staging the data for ML modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: Many ML services and frameworks provide functionality and utilities to implement
    ML pipelines. Scikit-learn provides this functionality through its `Pipeline`
    class, which we will look at next.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scikit-learn provides the `Pipeline` class as a tool to implement ML pipelines.
    The `Pipeline` class provides a unified interface to perform a sequence of data-
    and model-related tasks. Pipelines rely on scikit-learn’s standard `fit` and `transform`
    interfaces to enable the chaining of operations. Each pipeline consists of any
    number of intermediate steps, which must be `transforms`. A transform must implement
    both `fit` and `transform`, and the `Pipeline` class each transform in turn, first
    passing the data to `fit` and then to `transform`. The final step, which usually
    entails fitting the model to the data, only needs to implement the `fit` method.
    The transformations are usually preprocessing steps that transform or augment
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: The primary advantage of using scikit-learn pipelines is ensuring that the workflow
    is implemented clearly and in a reproducible manner. It helps avoid common mistakes,
    such as leaking statistics from the test data into the trained model during the
    preprocessing steps. By including the preprocessing steps within the pipeline,
    we ensure that the same steps are applied consistently during training and when
    the model is used to predict new data.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, scikit-learn pipelines can be combined with tools for model selection
    and hyperparameter tuning, such as grid search and cross-validation. We can use
    grid search to automatically select the best parameters across the entire pipeline
    by defining a grid of parameters for the preprocessing steps and the final estimator.
    This can significantly simplify the code and reduce errors in a complex ML workflow.
    Tools such as FLAML also feature integration with scikit-learn pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a simple pipeline can be created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the pipeline consists of two steps: a scaling step that standardizes
    the data and a final step that fits a linear regression model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The power of scikit-learn’s `Pipeline` is that it can, in turn, be used as
    we would any other estimator or model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This provides us with a unified interface to all the steps encapsulated in
    the pipeline and makes reproducibility trivial. Furthermore, we can export the
    pipeline as we do for other scikit-learn models to enable deployment of the pipeline
    and all the steps it encapsulates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will show more examples of using scikit-learn pipelines next.
  prefs: []
  type: TYPE_NORMAL
- en: Although we have spent much time addressing the concerns of working with data
    and building and tuning models, we haven’t yet taken an in-depth look at what
    happens after a model is trained. It’s here that the world of MLOps comes into
    play. The next section provides a detailed overview.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MLOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine Learning Operations** (**MLOps**) is a practice that blends the fields
    of ML and system operations. It is designed to standardize and streamline the
    life cycle of ML model development and deployment, thus increasing the efficiency
    and effectiveness of ML solutions within a business setting. In many ways, MLOps
    can be considered a response to the challenges associated with operationalizing
    ML, bringing DevOps principles into the ML world.'
  prefs: []
  type: TYPE_NORMAL
- en: MLOps aims to bring together data scientists, who typically focus on model creation,
    experimentation, and evaluation, and operations professionals, who deal with deployment,
    monitoring, and maintenance. The goal is to facilitate better collaboration between
    these groups, leading to faster, more robust model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of MLOps is underscored by the unique challenges presented by
    ML systems. Machine learning systems are more dynamic and less predictable than
    traditional software systems, leading to potential challenges in reliability and
    robustness, especially in a rapidly changing production environment.
  prefs: []
  type: TYPE_NORMAL
- en: A central goal of MLOps is to accelerate the ML life cycle, facilitating faster
    experimentation and deployment. This is achieved through the automation of ML
    pipelines. **Automation** can cover various stages, including data preprocessing,
    feature engineering, model training, model validation, and deployment. Another
    crucial aspect of MLOps is ensuring reproducibility. Given the dynamic nature
    of ML models, it can be challenging to replicate results exactly, especially when
    models are retrained with new data. MLOps emphasizes the importance of versioning
    code, data, and model configurations, which ensures that every experiment can
    be precisely reproduced, which is crucial for debugging and auditing.
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitoring** is also a vital part of MLOps. Once a model is deployed, *monitoring
    its performance and continuously validating its predictions is critical*. MLOps
    emphasizes the need for robust monitoring tools that can track model performance,
    input data quality, and other vital metrics. Anomalies in these metrics may indicate
    that a model needs to be retrained or debugged.'
  prefs: []
  type: TYPE_NORMAL
- en: MLOps also encourages the use of robust testing practices for ML. ML testing
    includes traditional software testing practices, such as unit tests and integration
    tests, but also more ML-specific tests, such as validating the statistical properties
    of model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps also focuses on managing and scaling ML deployments. In the real world,
    ML models may need to serve thousands or even millions of predictions per second.
    DevOps practices such as containerization and serverless computing platforms come
    into play here to facilitate deployment and scaling automation.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note how MLOps fits into the broader software ecosystem. Just
    like DevOps has bridged the gap between development and operations in software
    engineering, MLOps aims to do the same for ML. By promoting shared understanding
    and responsibilities, MLOps can lead to more successful ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps is a rapidly evolving field becoming increasingly important as more businesses
    adopt ML. By applying principles from DevOps to the unique challenges of ML, MLOps
    provides a framework for managing the end-to-end ML life cycle, from initial experimentation
    to robust, scalable deployment. MLOps emphasizes standardization, automation,
    reproducibility, monitoring, testing, and collaboration to enable high-throughput
    ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll now look at a practical example of creating an ML pipeline using scikit-learn
    and deploying the pipeline behind a REST API.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an ML pipeline for customer churn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our practical example, we’ll use the telecommunication (**telco**) Customer
    Churn dataset we worked with in [*Chapter 5*](B16690_05.xhtml#_idTextAnchor083),
    *LightGBM Parameter Optimization with Optuna*. The dataset consists of descriptive
    information for each customer (such as gender, billing information, and charges)
    and whether the customer has left the telco provider (churn is *yes* or *no*).
    Our task is to build a classification model to predict churn.
  prefs: []
  type: TYPE_NORMAL
- en: Further, we’d like to deploy the model behind a REST API such that it can be
    integrated into a more extensive software system. The REST API should have an
    endpoint that makes predictions for data passed to the API.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use **FastAPI**, a modern, high-performance Python web framework, to build
    our API. Finally, we’ll deploy our model and API to Google Cloud Platform using
    Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Building an ML pipeline using scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start by building an ML pipeline using scikit-learn’s `Pipeline` toolset.
    Our pipeline should encapsulate data cleaning and feature engineering steps, then
    build and tune an appropriate model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll evaluate two algorithms for modeling: LightGBM and random forest, and
    as such, it’s unnecessary to perform any scaling or normalization of the data.
    However, the dataset has a unique identifier for each customer, `customerID`,
    which we need to remove.'
  prefs: []
  type: TYPE_NORMAL
- en: Further, the dataset consists of numerical and categorical features, and we
    must implement one-hot encoding for the categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline preprocessing steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To perform these steps within a scikit-learn pipeline, we’ll use `ColumnTransformer`.
    `ColumnTransformer` is a `Pipeline` transformer that operates only on a subset
    of the columns in the dataset. The transformer accepts a list of tuples in the
    form (`name, transformer, columns`). It applies the sub-transformers to the specified
    columns and concatenates the results such that all resultant features form part
    of the same result set.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following DataFrame and `ColumnTransformer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have a DataFrame with two columns: `price` and `description`. A column
    transformer is created with two sub-transformers: a scaling transformer and a
    vectorizer. The scaling transformer applies min-max scaling only to the `price`
    column. The vectorizer applies TF-IDF vectorization only to the `description`
    column. When `fit_transform` is called, a *single* array is returned with a column
    for the scaled price and the columns representing the word vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '**TF-IDF**, or **term frequency-inverse document frequency**, is just one way
    of extracting a feature from text. Analyzing and extracting features from text,
    and natural language processing in general, is a broad field within ML that we
    won’t be delving into deeply here. You are encouraged to read further on the topic
    at [https://scikit-learn.org/stable/modules/feature_extraction.xhtml#text-feature-extraction](https://scikit-learn.org/stable/modules/feature_extraction.xhtml#text-feature-extraction).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can set up our preprocessing for the Customer Churn dataset as a single
    `ColumnTransformer`. We first define the two individual transformers, `id_transformer`
    and `encode_transformer`, that apply to the ID columns and the categorical features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And then combine the separate transformers into `ColumnTransformer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`ColumnTransformer` is defined with the `remainder=''passthrough''` parameter.
    The `remainder` parameter specifies what happens to the columns that `ColumnTransformer`
    does not transform. These columns are dropped by default, but we would like to
    pass them through, untouched, to include them in the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The encoding transformer creates and applies `OneHotEncoder` to the categorical
    features.
  prefs: []
  type: TYPE_NORMAL
- en: For illustrative purposes, we have created a custom transformer class to maintain
    the list of ID columns and drop them from the data during transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the class extends `BaseEstimator` and `TranformerMixin` from
    the scikit-learn base classes and must implement `fit` and `transform`. Implementing
    these methods also makes it suitable as a transformer in a pipeline. Implementing
    `fit` is optional if required; in our case, nothing is done during `fit`. Our
    transformation step drops the relevant columns.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to encapsulate the functionality to drop irrelevant columns (in
    this case, the ID column) within the pipeline itself. When deploying the pipeline
    for production use, we expect these columns to be passed to the pipeline when
    making a prediction. Removing them as part of the pipeline simplifies our pipeline’s
    usage for our model’s consumers and reduces the chance of user mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This completes the transformations required for preprocessing, and we are ready
    to move on to the following steps: fitting and tuning the models.'
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline modeling steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the pipeline modeling part, we’ll use FLAML. We’ll also use the opportunity
    to show how parameters may be passed to steps within a pipeline. First, we define
    the settings for our AutoML model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code sets our time budget, optimization metric, and classification
    task for AutoML. We also limit the estimators to LightGBM and a random forest
    model. Finally, we customize the search space by specifying that `n_estimators`
    should be uniformly sampled between 20 and 500.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline requires the parameter for constituent steps to be prefixed with
    the step’s name and a double underscore. We can set up a dictionary to pass these
    parameters to the AutoML class within our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `automl` is the name of the step in the pipeline. As such, for example,
    the parameters for time budget and metric are set as `automl__time_budget: 120`
    and `automl__metric:` `accuracy`, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can add FLAML’s AutoML estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The final pipeline is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Final ML pipeline for Customer Churn prediction](img/B16690_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Final ML pipeline for Customer Churn prediction
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.1* shows a *ColumnTransformer* that consists of two sub-transformers,
    feeding into the AutoML estimator.'
  prefs: []
  type: TYPE_NORMAL
- en: Model training and validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are now ready to fit the pipeline to our data, passing the pipeline settings
    we set up earlier. Pipelines support the standard scikit-learn API so that we
    can call on the pipeline itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Running `fit` executes all the preprocessing steps and then passes the data
    for AutoML modeling and tuning. The single `Pipeline` object illustrates the power
    of an ML pipeline: the entire end-to-end     process, including a trained and tuned model, is encapsulated and portable, and
    we can utilize the pipeline as we could a single model. For example, the following
    code performs F1 scoring for the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To export the pipeline, we `joblib` to serialize the model to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Exporting the pipeline allows us to re-instantiate and use it within our production
    code. Next, we’ll look at building an API for our model.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, our pipeline (which encapsulates preprocessing, training, optimization,
    and validation) is defined, and we are ready to deploy it to a system. We’ll accomplish
    this by wrapping our model in an API with FastAPI.
  prefs: []
  type: TYPE_NORMAL
- en: Building an ML API using FastAPI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now look at building a REST API around our pipeline, enabling consumers
    of our pipeline to get predictions via web requests. Building a web API for a
    model also simplifies integration with other systems and services and is the standard
    method for integration in a microservices architecture.
  prefs: []
  type: TYPE_NORMAL
- en: To build the API, we use the Python library FastAPI.
  prefs: []
  type: TYPE_NORMAL
- en: FastAPI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FastAPI is a modern, high-performance web framework for building APIs with Python
    3.6+. It was designed from the ground up to be easy to use and enable high-performance
    API development. The key features of FastAPI are its speed and ease of use, making
    it an excellent choice for developing robust, production-ready APIs. FastAPI widely
    adopts Python’s type checking, which aids in catching errors early in the development
    process. It also uses these type hints to provide data validation, serialization,
    and documentation, reducing the boilerplate code developers need to write.
  prefs: []
  type: TYPE_NORMAL
- en: The performance of FastAPI is one of its defining features. It is on par with
    Node.js and significantly faster than traditional Python frameworks. This speed
    is achieved due to its use of Starlette for the web parts and Pydantic for the
    data parts, and its non-blocking nature makes it suitable for handling many concurrent
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: FastAPI provides automatic interactive API documentation, a considerable advantage
    while developing complex APIs. Using FastAPI, developers gain access to automatically
    generated interactive API docs via Swagger UI. Swagger UI also provides functionality
    to interact with the REST resources without writing code or using external tooling.
    This feature makes FastAPI very developer-friendly and accelerates the development
    process.
  prefs: []
  type: TYPE_NORMAL
- en: FastAPI also supports industry-standard security protocols, such as OAuth2,
    and provides tooling to ease implementation. Much of FastAPI’s tooling relies
    on its dependency injection system, allowing developers to manage dependencies
    and handle shared resources efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: FastAPI is well suited to building web APIs and microservices for ML models
    due to its ease of use and high performance, allowing ML engineers to focus on
    the myriad of other concerns surrounding production ML deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Building with FastAPI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To create a REST API with FastAPI, we can create a new Python script and instantiate
    the FastAPI instance. After the instance starts, we can load our model from the
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Loading the model at the start of the application increases the startup time
    but ensures that the API is ready to serve requests when the application startup
    completes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to implement a REST endpoint to make predictions. Our endpoint
    accepts input data and returns the predictions as JSON. The input JSON is an array
    of JSON objects, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'With FastAPI, we implement a REST endpoint by creating a function that takes
    the input data as parameters. FastAPI serializes the preceding JSON structure
    to a Python list of dictionaries. Therefore, our function signature is implemented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We decorate the function using a FastAPI `post` decorator, specifying the endpoint
    path (`'/predict'`).
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the actual predictions for the model, we convert the dictionaries to
    a DataFrame and perform the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We use `predict_proba` to get the probabilities for each class (Yes or No) since
    we want to send this additional information to the consumers of our API. Returning
    probabilities alongside predictions is a recommended practice, as this affords
    the API consumer more control over the use of the predictions. API consumers can
    decide what probability threshold is good enough for their application based on
    how the predictions are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'To return the results as JSON, we construct a dictionary that FastAPI then
    serializes to JSON. We use NumPy’s `argmax` to get the index of the highest probability
    to determine the predicted class and `amax` to get the highest probability itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces a `prediction` object for each data instance in
    the input list, using the position in the list as an index. When the endpoint
    is called, the following JSON is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We have now built the core of the API endpoint. However, we must also pay attention
    to non-functional concerns such as security. Often, ML engineers neglect aspects
    such as security or performance and focus only on ML concerns. We mustn’t make
    this mistake and must ensure we give these concerns the necessary attention.
  prefs: []
  type: TYPE_NORMAL
- en: Securing the API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To secure our endpoint, we’ll make use of HTTP Basic authentication. We use
    a preset username and password, which we read from the environment. This allows
    us to securely pass these credentials to the application during deployment and
    avoids pitfalls such as hardcoding the credentials. Our endpoint also needs to
    be enhanced to accept credentials from the user. HTTP Basic authentication credentials
    are sent as an HTTP header.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement this as follows. We first set up security for FastAPI and
    read the credentials from the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We then add the following to the `endpoint` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `authenticate` function validates the received credentials against the
    API credentials we got from the environment. We can use Python’s secrets library
    to do the validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: If the credentials are invalid, we throw an exception with HTTP status code
    `401`, signaling that the consumer is not authorized.
  prefs: []
  type: TYPE_NORMAL
- en: Our API endpoint is now fully implemented, secured, and ready for deployment.
    To deploy our API, we’ll containerize it using Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Containerizing our API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can build a Docker container for our API with the following Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The Dockerfile is straightforward: we start with a base Python 3.10 image and
    install some OS dependencies that LightGBM needs (`libgomp1`). We then set up
    the FastAPI app: we copy the Python `requirements` file, install all of them,
    and then copy the necessary source files (using `COPY . .` ).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we run a Uvicorn server, listening on all addresses on port `8080`.
    Uvicorn is an ASGI web server implementation for Python that supports async I/O,
    significantly increasing the web server’s throughput. We bind to port `8080`,
    our deployment platform’s default port.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can build and run the Docker image using the following commands, passing
    the username and password environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The API should now be available on your localhost, on port `8080`, secured behind
    the credentials you provide in the environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: With our application containerized, we are ready to deploy our application to
    any platform that supports containers. For the churn application, we’ll deploy
    it to Google Cloud Platform.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying LightGBM to Google Cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll leverage the **Google Cloud Run** platform to deploy our application to
    Google Cloud Platform.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Run is a serverless platform that allows you to develop and run
    applications without worrying about infrastructure management. Cloud Run allows
    developers to run their applications in a secure, scalable, and zero-ops environment.
    Cloud Run is fully managed, meaning all infrastructure (such as servers and load
    balancers) is abstracted away, allowing users to focus on running their applications.
    Cloud Run also supports full autoscaling, and the number of running containers
    automatically increases to respond to increasing load. Cloud Run is also very
    cost-effective, as you are only charged when the container runs and serves requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Cloud Run, you need a Google Cloud account and need to create a Google
    Cloud project, enable billing, and set up and initialize the **Google Cloud CLI**.
    The following resources guide you through these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://console.cloud.google.com/getting-started](https://console.cloud.google.com/getting-started)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/resource-manager/docs/creating-managing-projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/sdk/docs/initializing](https://cloud.google.com/sdk/docs/initializing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the Google Cloud setup is completed, we can deploy our API using the CLI.
    This can be accomplished using a single command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Running the command prompts you for a service name and a region to deploy your
    service. We also set the environment variables needed for the security credentials.
    For deployment, Cloud Run creates Cloud Build for you, which automatically builds
    and stores the Docker container and then deploys it to Cloud Run.
  prefs: []
  type: TYPE_NORMAL
- en: Once the Cloud Run command completes, we have deployed a secure, scalable, RESTful
    web API serving our customer churn ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced ML pipelines, illustrating their advantages in enabling
    consistency, correctness, and portability when implementing ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: An overview was given on the nascent MLOps field, a practice combining DevOps
    and ML to realize tested, scalable, secure, and observable production ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: Further, we discussed the scikit-learn `Pipeline` class, a toolset to implement
    ML pipelines using the familiar scikit-learn API.
  prefs: []
  type: TYPE_NORMAL
- en: A practical, end-to-end example of implementing an ML pipeline for customer
    churn was also given. We showed how to create a scikit-learn pipeline that performs
    preprocessing, modeling, and tuning and is exportable for a software system. We
    then built a secure RESTful web API using FastAPI that provides an endpoint for
    getting predictions from our customer churn pipeline. Finally, we deployed our
    API to Google Cloud Platform using the Cloud Run service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although our deployment is secure and fully scalable, with observability, metrics,
    and logs provided by Cloud Run, there are some ML-specific aspects our deployment
    does not address: model drift, model performance monitoring, and retraining.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we look at a specialized ML cloud service with AWS SageMaker,
    which provides a platform-specific solution for building and hosting cloud-based
    ML pipelines.
  prefs: []
  type: TYPE_NORMAL
