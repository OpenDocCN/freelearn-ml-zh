- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finding Groups of Data – Clustering with k-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever spent time watching a crowd? If so, you have likely seen some
    recurring personalities. Perhaps a certain type of person, identified by a freshly
    pressed suit and a briefcase, comes to typify the “fat cat” business executive.
    A 20-something wearing skinny jeans, a flannel shirt, and sunglasses might be
    dubbed a “hipster,” while a woman unloading children from a minivan may be labeled
    a “soccer mom.”
  prefs: []
  type: TYPE_NORMAL
- en: Of course, these types of stereotypes are dangerous to apply to individuals,
    as no two people are exactly alike. Yet, understood as a way to describe a collective,
    the labels capture some underlying aspect of similarity shared among the individuals
    within the group.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you will soon learn, the act of clustering, or spotting patterns in data,
    is not much different from spotting patterns in groups of people. This chapter
    describes:'
  prefs: []
  type: TYPE_NORMAL
- en: The ways clustering tasks differ from the classification tasks we examined previously
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How clustering defines a group and how such groups are identified by k-means,
    a classic and easy-to-understand clustering algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The steps needed to apply clustering to a real-world task of identifying marketing
    segments among teenage social media users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before jumping into action, we’ll begin by taking an in-depth look at exactly
    what clustering entails.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is an unsupervised machine learning task that automatically divides
    the data into **clusters**, or groups of similar items. It does this without having
    been told how the groups should look ahead of time. Because we do not tell the
    machine specifically what we’re looking for, clustering is used for knowledge
    discovery rather than prediction. It provides an insight into the natural groupings
    found within data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without advanced knowledge of what comprises a cluster, how can a computer
    possibly know where one group ends and another begins? The answer is simple: clustering
    is guided by the principle that items inside a cluster should be very similar
    to each other, but very different from those outside. The definition of similarity
    might vary across applications, but the basic idea is always the same: group the
    data such that related elements are placed together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting clusters can then be used for action. For instance, you might
    find clustering methods employed in applications such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting customers into groups with similar demographics or buying patterns
    for targeted marketing campaigns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting anomalous behavior, such as unauthorized network intrusions, by identifying
    patterns of use falling outside known clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplifying extremely “wide” datasets—those with a large number of features—by
    creating a small number of categories to describe rows with relatively homogeneous
    values of the features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, clustering is useful whenever diverse and varied data can be exemplified
    by a much smaller number of groups. It results in meaningful and actionable data
    structures, which reduce complexity and provide insight into patterns of relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering as a machine learning task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering is somewhat different from the classification, numeric prediction,
    and pattern detection tasks we’ve examined so far. In each of these tasks, the
    goal was to build a model that relates features to an outcome, or to relate some
    features to other features. Each of these tasks describes existing patterns within
    data. In contrast, the goal of clustering is to create new data. In clustering,
    unlabeled examples are given a new cluster label, which has been inferred entirely
    from the relationships within the data. For this reason, you will sometimes see
    a clustering task referred to as **unsupervised classification** because, in a
    sense, it classifies unlabeled examples.
  prefs: []
  type: TYPE_NORMAL
- en: The catch is that the class labels obtained from an unsupervised classifier
    are without intrinsic meaning. Clustering will tell you which groups of examples
    are closely related—for instance, it might return groups A, B, and C—but it’s
    up to you to apply an actionable and meaningful label, and to tell the story of
    what makes an “A” different from a “B.” To see how this impacts the clustering
    task, let’s consider a simple hypothetical example.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you were organizing a conference on the topic of data science. To facilitate
    professional networking and collaboration, you planned to seat people at one of
    three tables according to their research specialties. Unfortunately, after sending
    out the conference invitations, you realize that you forgot to include a survey
    asking which discipline the attendee would prefer to be seated within.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a stroke of brilliance, you realize that you might be able to infer each
    scholar’s research specialty by examining their publication history. To this end,
    you begin collecting data on the number of articles each attendee has published
    in computer science-related journals and the number of articles published in math-
    or statistics-related journals. Using the data collected for the scholars, you
    create a scatterplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17290_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Visualizing scholars by their math and computer science publication
    data'
  prefs: []
  type: TYPE_NORMAL
- en: As expected, there seems to be a pattern. We might guess that the upper-left
    corner, which represents people with many computer science publications but few
    articles on math, is a cluster of computer scientists. Following this logic, the
    lower-right corner might be a group of mathematicians or statisticians. Similarly,
    the upper-right corner, those with both math and computer science experience,
    may be machine learning experts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying these labels results in the following visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Clusters can be identified based on presumptions about the scholars
    in each group'
  prefs: []
  type: TYPE_NORMAL
- en: Our groupings were formed visually; we simply identified clusters as closely
    grouped data points. Yet, despite the seemingly obvious groupings, without personally
    asking each scholar about their academic specialty, we have no way to know whether
    the groups are truly homogeneous. The labels are qualitative, presumptive judgments
    about the types of people in each group, based on a limited set of quantitative
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than defining the group boundaries subjectively, it would be nice to
    use machine learning to define them objectively. Given the axis-parallel splits
    in the previous figure, our problem seems like an obvious application for decision
    trees, as described in *Chapter 5*, *Divide and Conquer – Classification Using
    Decision Trees and Rules*. This would provide us with a clean rule like “if a
    scholar has few math publications, then they are a computer science expert.” Unfortunately,
    there’s a problem with this plan. Without data on the true class value for each
    point, a supervised learning algorithm would have no ability to learn such a pattern,
    as it would have no way of knowing what splits would result in homogenous groups.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to supervised learning, clustering algorithms use a process very
    similar to what we did by visually inspecting the scatterplot. Using a measure
    of how closely the examples are related, homogeneous groups can be identified.
    In the next section, we’ll start looking at how clustering algorithms are implemented.
  prefs: []
  type: TYPE_NORMAL
- en: This example highlights an interesting application of clustering. If you begin
    with unlabeled data, you can use clustering to create class labels. From there,
    you could apply a supervised learner such as decision trees to find the most important
    predictors of these classes. This is an example of semi-supervised learning as
    described in *Chapter 1*, *Introducing Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Clusters of clustering algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just as there are many approaches to building a predictive model, there are
    multiple approaches to performing the descriptive task of clustering. Many such
    methods are listed on the following site, the CRAN task view for clustering: [https://cran.r-project.org/view=Cluster](https://cran.r-project.org/view=Cluster).
    Here, you will find numerous R packages used for discovering natural groupings
    in data. The various algorithms are distinguished mainly by two characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The **similarity metric**, which provides the quantitative measure of how closely
    two examples are related
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **agglomeration function**, which governs the process of assigning examples
    to clusters based on their similarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even though there may be subtle differences between the approaches, they can
    of course be clustered in various ways. Multiple such typologies exist, but a
    simple three-part framework helps understand the main distinctions. Using this
    approach, the three main clusters of clustering algorithms, listed from simplest
    to most sophisticated, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hierarchical methods**, which create a family tree-style hierarchy that positions
    the most similar examples more closely in the graph structure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition-based methods**, which treat the examples as points in multidimensional
    space, and attempt to find boundaries in this space that lead to relatively homogenous
    groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model or density-based methods**, which rely on statistical principles and/or
    the density of points to discover fuzzy boundaries between the clusters; in some
    cases, examples may be partially assigned to multiple clusters, or even to no
    cluster at all'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite **hierarchical clustering** being the simplest of the methods, it is
    not without a pair of interesting upsides. Firstly, it results in a hierarchical
    graph visualization called a **dendrogram**, which depicts the associations between
    examples such that the most similar examples are positioned more closely in the
    hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: This can be a useful tool to understand which examples and subsets of examples
    are the most tightly grouped. Secondly, hierarchical clustering does not require
    a predefined expectation of how many clusters exist in the dataset. Instead, it
    implements a process in which, at one extreme, every example is included in a
    single, giant cluster with all other examples; at the other extreme, every example
    is found in a tiny cluster containing only itself; and in between, examples may
    be included in other clusters of varying sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.3* illustrates a hypothetical dendrogram for a simple dataset containing
    eight examples, labeled A through H. Notice that the most closely related examples
    (depicted via proximity on the *x* axis) are linked more closely as siblings in
    the diagram. For instance, examples D and E are the most similar and thus are
    the first to be grouped. However, all eight examples are eventually linked to
    one large cluster, or may be included in any number of clusters in between. Slicing
    the dendrogram horizontally at different positions creates varying numbers of
    clusters, as shown for three and five clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, diagram, box and whisker chart  Description automatically generated](img/B17290_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Hierarchical clustering produces a dendrogram that depicts the
    natural groupings for the desired number of clusters'
  prefs: []
  type: TYPE_NORMAL
- en: The dendrogram for hierarchical clustering can be grown using “bottom-up” or
    “top-down” approaches. The former is called **agglomerative clustering**, and
    begins with each example in its own cluster, then connects the most similar examples
    first until all examples are connected in a single cluster. The latter is called
    **divisive clustering**, and begins with a single large cluster and ends with
    all examples in their own individual clusters.
  prefs: []
  type: TYPE_NORMAL
- en: When connecting examples to groups of examples, different metrics may be used,
    such as the example’s similarity to the most similar, least similar, or average
    member of the group. A more complex metric known as **Ward’s method** does not
    use similarity between examples, but instead considers a measure of cluster homogeneity
    to construct the linkages. In any case, the result is a hierarchy that aims to
    group the most similar examples into any number of subgroups.
  prefs: []
  type: TYPE_NORMAL
- en: The flexibility of the hierarchical clustering technique comes at a cost, which
    is computational complexity due to the need to compute the similarity between
    each example and every other. As the number of examples (*N*) grows, the number
    of calculations grows as *N*N = N*², as does the memory needed for the similarity
    matrix that stores the result. For this reason, hierarchical clustering is used
    only on very small datasets and is not demonstrated in this chapter. However,
    the `hclust()` function included in R’s `stats` package provides a simple implementation,
    which is installed with R by default.
  prefs: []
  type: TYPE_NORMAL
- en: Clever implementations of divisive clustering have the potential to be slightly
    more computationally efficient than agglomerative clustering as the algorithm
    may stop early if it is unnecessary to create larger numbers of clusters. This
    being said, both agglomerative and divisive clustering are examples of “greedy”
    algorithms as defined in *Chapter 5*, *Divide and Conquer – Classification Using
    Decision Trees and Rules*, because they use data on a first-come, first-served
    basis and are thus not guaranteed to produce the overall optimal set of clusters
    for a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Partition-based clustering** methods have a distinct efficiency advantage
    over hierarchical clustering in that they apply heuristic methods to divide the
    data into clusters without the need to evaluate the similarity between every pair
    of examples. We’ll explore a widely used partition-based method in greater detail
    shortly, but for now it suffices to understand that this method is concerned with
    finding boundaries between clusters rather than connecting examples to one another—an
    approach that requires far fewer comparisons between examples. This heuristic
    can be quite computationally efficient, but one caveat is that it is somewhat
    rigid or even arbitrary when it comes to group assignments. For example, if five
    clusters are requested, it will partition examples into all five clusters; if
    some examples fall on the boundary between two clusters, these will be placed
    somewhat arbitrarily yet firmly into one cluster or the other. Similarly, if four
    or six clusters might have split the data better, this would not be as apparent
    as it would be with the hierarchical clustering dendrogram.'
  prefs: []
  type: TYPE_NORMAL
- en: The more sophisticated **model-based and density-based clustering** methods
    address some of these issues of inflexibility by estimating the probability that
    an example belongs to each cluster, rather than merely assigning it to a single
    cluster. Some of them may allow the cluster boundaries to follow the natural patterns
    identified in the data rather than forcing a strict divide between the groups.
    Model-based approaches often assume a statistical distribution from which the
    examples are believed to have been pulled.
  prefs: []
  type: TYPE_NORMAL
- en: One such approach, known as **mixture modeling**, attempts to disentangle datasets
    composed of examples pulled from a mixture of statistical distributions—typically
    Gaussian (the normal bell curve). For example, imagine you have a dataset composed
    of voice data from a mixture of male and female vocal registers, as depicted in
    *Figure 9.4* (note that the distributions are hypothetical and are not based on
    real-world data). Although there is some overlap between the two, the average
    male tends to have a lower register than the average female.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Mixture modeling assigns each example a probability of belonging
    to one of the underlying distributions'
  prefs: []
  type: TYPE_NORMAL
- en: Given the unlabeled overall distribution (the bottom part of the figure), a
    mixture model would be capable of assigning a probability that any given example
    belongs to the cluster of males or the cluster of females, incredibly, without
    ever having been trained separately on male or female voices in the top part of
    the figure! This is possible by discovering the statistical parameters like the
    mean and standard deviation that are most likely to have generated the observed
    overall distribution, under the assumption that a specific number of distinct
    distributions were involved—in this case, two Gaussian distributions.
  prefs: []
  type: TYPE_NORMAL
- en: As an unsupervised method, the mixture model would have no way of knowing that
    the left distribution is males and the right distribution is females, but this
    would be readily apparent to a human observer comparing the records with a high
    likelihood of males being in the left cluster versus the right one. The downside
    of this technique is that not only does it require knowledge of how many distributions
    are involved but it also requires an assumption of the types of distributions.
    This may be too rigid for many real-world clustering tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Another powerful clustering technique called **DBSCAN** is named after the “density-based
    spatial clustering of applications with noise” approach it uses to identify natural
    clusters in data. This award-winning technique is incredibly flexible and does
    well with many of the challenges of clustering, such as adapting to the dataset’s
    natural number of clusters, being flexible about the boundaries between clusters,
    and not assuming a particular statistical distribution for the data.
  prefs: []
  type: TYPE_NORMAL
- en: While the implementation details are beyond the scope of this book, the DBSCAN
    algorithm can be understood intuitively as a process of creating neighborhoods
    of examples that are all within a given radius of other examples in the cluster.
    A predefined number of **core points** within a specified radius forms the initial
    cluster nucleus, and points that are within a specified radius of any of the core
    points are then added to the cluster and comprise the outermost boundary of the
    cluster. Unlike many other clustering algorithms, some examples will not be assigned
    any cluster at all, as any points that are not close enough to a core point will
    be treated as noise.
  prefs: []
  type: TYPE_NORMAL
- en: Although DBSCAN is powerful and flexible, it may require experimentation to
    optimize the parameters to fit the data, such as the number of points comprising
    the core, or the allowed radius between points, which adds time complexity to
    the machine learning project. Of course, just because model-based methods are
    more sophisticated does not imply they are the best fit for every clustering project.
    As we will see throughout the remainder of this chapter, a simpler partition-based
    method can perform surprisingly well on a challenging real-world clustering task.
  prefs: []
  type: TYPE_NORMAL
- en: Although mixture modeling and DBSCAN are not demonstrated in this chapter, there
    are R packages that can be used to apply these methods to your own data. The `mclust`
    package fits a model to mixtures of Gaussian distributions, and the `dbscan` package
    provides a fast implementation of the DBSCAN algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The k-means clustering algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **k-means algorithm** is perhaps the most often used clustering method and
    is an example of a partition-based clustering approach. Having been studied for
    several decades, it serves as the foundation for many more sophisticated clustering
    techniques. If you understand the simple principles it uses, you will have the
    knowledge needed to understand nearly any clustering algorithm in use today.
  prefs: []
  type: TYPE_NORMAL
- en: As k-means has evolved over time, there are many implementations of the algorithm.
    An early approach is described in *A k-means clustering algorithm, Hartigan, J.A.,
    Wong, M.A., Applied Statistics, 1979, Vol. 28, pp. 100-108*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though clustering methods have evolved since the inception of k-means,
    this is not to imply that k-means is obsolete. In fact, the method may be more
    popular now than ever. The following table lists some reasons why k-means is still
    used widely:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Strengths** | **Weaknesses** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Uses simple principles that can be explained in non-statistical terms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly flexible and can be adapted with simple adjustments to address many of
    its shortcomings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs well enough under many real-world use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Not as sophisticated as more modern clustering algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because it uses an element of random chance, it is not guaranteed to find the
    optimal set of clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires a reasonable guess as to how many clusters naturally exist in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not ideal for non-spherical clusters or clusters of widely varying density
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: If the name k-means sounds familiar to you, you may be recalling the **k-nearest
    neighbors** (**k-NN**) algorithm presented in *Chapter 3*, *Lazy Learning – Classification
    Using Nearest Neighbors*. As you will soon see, k-means has more in common with
    k-NN than just the letter k.
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm assigns each of the *n* examples to one of the *k* clusters,
    where *k* is a number that has been determined ahead of time. The goal is to minimize
    the differences in feature values of examples within each cluster and maximize
    the differences between clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Unless *k* and *n* are extremely small, it is not feasible to compute the optimal
    clusters across all possible combinations of examples. Instead, the algorithm
    uses a heuristic process that finds **locally optimal** solutions. Put simply,
    this means that it starts with an initial guess for the cluster assignments and
    then modifies the assignments slightly to see if the changes improve the homogeneity
    within the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover the process in depth shortly, but the algorithm essentially involves
    two phases. First, it assigns examples to an initial set of *k* clusters. Then,
    it updates the assignments by adjusting the cluster boundaries according to the
    examples that currently fall into the cluster. The process of updating and assigning
    occurs several times until changes no longer improve the cluster fit. At this
    point, the process stops, and the clusters are finalized.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the heuristic nature of k-means, you may end up with somewhat different
    results by making only slight changes to the starting conditions. If the results
    vary dramatically, this could indicate a problem. For instance, the data may not
    have natural groupings, or the value of *k* has been poorly chosen. With this
    in mind, it’s a good idea to try a cluster analysis more than once to test the
    robustness of your findings.
  prefs: []
  type: TYPE_NORMAL
- en: To see how the process of assigning and updating works in practice, let’s revisit
    the case of the hypothetical data science conference. Though this is a simple
    example, it will illustrate the basics of how k-means operates under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Using distance to assign and update clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with k-NN, k-means treats feature values as coordinates in a multidimensional
    feature space. For the conference data, there are only two features, so we can
    represent the feature space as a two-dimensional scatterplot as depicted previously.
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm begins by choosing *k* points in the feature space to
    serve as the cluster centers. These centers are the catalyst that spurs the remaining
    examples to fall into place. Often, the points are chosen by selecting *k* random
    examples from the training dataset. Because we hope to identify three clusters,
    using this method, *k = 3* points will be selected at random.
  prefs: []
  type: TYPE_NORMAL
- en: 'These points are indicated by the star, triangle, and diamond in *Figure 9.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17290_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: k-means clustering begins by selecting k random cluster centers'
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that although the three cluster centers in the preceding diagram
    happen to be widely spaced apart, this will not always necessarily be the case.
    Because the starting points are selected at random, the three centers could have
    just as easily been three adjacent points. Combined with the fact that the k-means
    algorithm is highly sensitive to the starting position of the cluster centers,
    a good or bad set of initial cluster centers may have a substantial impact on
    the final set of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: To address this problem, k-means can be modified to use different methods for
    choosing the initial centers. For example, one variant chooses random values occurring
    anywhere in the feature space rather than only selecting among values observed
    in the data. Another option is to skip this step altogether; by randomly assigning
    each example to a cluster, the algorithm can jump ahead immediately to the update
    phase. Each of these approaches adds a particular bias to the final set of clusters,
    which you may be able to use to improve your results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2007, an algorithm called **k-means++** was introduced, which proposes an
    alternative method for selecting the initial cluster centers. It purports to be
    an efficient way to get much closer to the optimal clustering solution while reducing
    the impact of random chance. For more information, see *k-means++: The advantages
    of careful seeding, Arthur, D, Vassilvitskii, S, Proceedings of the eighteenth
    annual ACM-SIAM symposium on discrete algorithms, 2007, pp. 1,027–1,035*.'
  prefs: []
  type: TYPE_NORMAL
- en: After choosing the initial cluster centers, the other examples are assigned
    to the cluster center that is nearest according to a distance function, which
    is used as a measure of similarity. You may recall that we used distance functions
    as similarity measures while learning about the k-NN supervised learning algorithm.
    Like k-NN, k-means traditionally uses Euclidean distance, but other distance functions
    can be used if desired.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, any function that returns a numeric measure of similarity could
    be used instead of a traditional distance function. In fact, k-means could even
    be adapted to cluster images or text documents by using a function that measures
    the similarity of pairs of images or texts.
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply the distance function, recall that if *n* indicates the number of
    features, the formula for Euclidean distance between example *x* and example *y*
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_09_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For instance, to compare a guest with five computer science publications and
    one math publication to a guest with zero computer science papers and two math
    papers, we could compute this in R as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using the distance function in this way, we find the distance between each example
    and each cluster center. Each example is then assigned to the nearest cluster
    center.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that because we are using distance calculations, all the features
    need to be numeric, and the values should be normalized to a standard range ahead
    of time. The methods presented in *Chapter 3*, *Lazy Learning – Classification
    Using Nearest Neighbors*, will prove helpful for this task.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the following figure, the three cluster centers partition the examples
    into three partitions labeled *Cluster A*, *Cluster B*, and *Cluster C*. The dashed
    lines indicate the boundaries for the **Voronoi diagram** created by the cluster
    centers. The Voronoi diagram indicates the areas that are closer to one cluster
    center than any other; the vertex where all three boundaries meet is the maximal
    distance from all three cluster centers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these boundaries, we can easily see the regions claimed by each of the
    initial k-means seeds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: The initial cluster centers create three groups of “nearest” points'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the initial assignment phase has been completed, the k-means algorithm
    proceeds to the update phase. The first step of updating the clusters involves
    shifting the initial centers to a new location, known as the **centroid**, which
    is calculated as the average position of the points currently assigned to that
    cluster. The following figure illustrates how as the cluster centers shift to
    the new centroids, the boundaries in the Voronoi diagram also shift, and a point
    that was once in *Cluster B* (indicated by an arrow) is added to *Cluster A*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: The update phase shifts the cluster centers, which causes the reassignment
    of one point'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result of this reassignment, the k-means algorithm will continue through
    another update phase. After shifting the cluster centroids, updating the cluster
    boundaries, and reassigning points into new clusters (as indicated by arrows),
    the figure looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: After another update, two more points are reassigned to the nearest
    cluster center'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because two more points were reassigned, another update must occur, which moves
    the centroids and updates the cluster boundaries. However, because these changes
    result in no reassignments, the k-means algorithm stops. The cluster assignments
    are now final:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated with medium confidence](img/B17290_09_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: Clustering stops after the update phase results in no new cluster
    assignments'
  prefs: []
  type: TYPE_NORMAL
- en: The final clusters can be reported in one of two ways. First, you might simply
    report the cluster assignments of A, B, or C for each example. Alternatively,
    you could report the coordinates of the cluster centroids after the final update.
  prefs: []
  type: TYPE_NORMAL
- en: Given either reporting method, you can compute the other; you can calculate
    the centroids using the coordinates of each cluster’s examples, or you can use
    the centroid coordinates to assign each example to its nearest cluster center.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the appropriate number of clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the introduction to k-means, we learned that the algorithm is sensitive to
    the randomly chosen cluster centers. Indeed, if we had selected a different combination
    of three starting points in the previous example, we may have found clusters that
    split the data differently from what we had expected. Similarly, k-means is sensitive
    to the number of clusters; the choice requires a delicate balance. Setting *k*
    to be very large will improve the homogeneity of the clusters and, at the same
    time, it risks overfitting the data.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, you will have *a priori* knowledge (a prior belief) about the true
    groupings and you can apply this information to choose the number of clusters.
    For instance, if you clustered movies, you might begin by setting *k* equal to
    the number of genres considered for the Academy Awards. In the data science conference
    seating problem that we worked through previously, *k* might reflect the number
    of academic fields of study that invitees belong to.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, the number of clusters is dictated by business requirements or the
    motivation for the analysis. For example, the number of tables in the meeting
    hall might dictate how many groups of people should be created from the data science
    attendee list. Extending this idea to another business case, if the marketing
    department only has the resources to create three distinct advertising campaigns,
    it might make sense to set *k = 3* to assign all the potential customers to one
    of the three appeals.
  prefs: []
  type: TYPE_NORMAL
- en: Without any prior knowledge, one rule of thumb suggests setting *k* equal to
    the square root of *(n / 2)*, where *n* is the number of examples in the dataset.
    However, this rule of thumb is likely to result in an unwieldy number of clusters
    for large datasets. Luckily, there are other quantitative methods that can assist
    in finding a suitable k-means cluster set.
  prefs: []
  type: TYPE_NORMAL
- en: A technique known as the **elbow method** attempts to gauge how the homogeneity
    or heterogeneity within the clusters changes for various values of *k*. As illustrated
    in the following diagrams, the homogeneity within clusters is expected to increase
    as additional clusters are added; similarly, the heterogeneity within clusters
    should decrease with more clusters. Because you could continue to see improvements
    until each example is in its own cluster, the goal is not to maximize homogeneity
    or minimize heterogeneity endlessly, but rather to find *k* such that there are
    diminishing returns beyond that value. This value of *k* is known as the **elbow
    point** because it bends like the elbow joint of the human arm.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated with medium confidence](img/B17290_09_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: The elbow is the point at which increasing k results in relatively
    small improvements'
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous statistics for measuring homogeneity and heterogeneity within
    clusters that can be used with the elbow method (the information box that follows
    provides a citation for more detail). Still, in practice, it is not always feasible
    to iteratively test a large number of *k* values. This is in part because clustering
    large datasets can be fairly time-consuming; clustering the data repeatedly is
    even worse. Furthermore, applications requiring the exact optimal set of clusters
    are rare. In most clustering applications, it suffices to choose a *k* value based
    on convenience rather than the one that creates the most homogenous clusters.
  prefs: []
  type: TYPE_NORMAL
- en: For a thorough review of the vast assortment of cluster performance measures,
    refer to *On Clustering Validation Techniques, Halkidi, M, Batistakis, Y, Vazirgiannis,
    M, Journal of Intelligent Information Systems, 2001, Vol. 17, pp. 107-145*.
  prefs: []
  type: TYPE_NORMAL
- en: The process of setting *k* itself can sometimes lead to interesting insights.
    By observing how the characteristics of the clusters change as *k* changes, one
    might infer where the data has naturally defined boundaries. Groups that are more
    tightly clustered will change very little, while less homogeneous groups will
    form and disband over time.
  prefs: []
  type: TYPE_NORMAL
- en: In general, it may be wise to spend little time worrying about getting *k* exactly
    right. The next example will demonstrate how even a tiny bit of subject-matter
    knowledge borrowed from a Hollywood film can be used to set *k* such that actionable
    and interesting clusters are found. As clustering is unsupervised, the task is
    really about what you make of it; the value is in the insights you take away from
    the algorithm’s findings.
  prefs: []
  type: TYPE_NORMAL
- en: Finding teen market segments using k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interacting with friends on a **social networking service** (**SNS**), such
    as Facebook, TikTok, and Instagram, has become a rite of passage for teenagers
    around the world. Having a relatively large amount of disposable income, these
    adolescents are a coveted demographic for businesses hoping to sell snacks, beverages,
    electronics, entertainment, and hygiene products.
  prefs: []
  type: TYPE_NORMAL
- en: The many millions of teenage consumers using such sites have attracted the attention
    of marketers struggling to find an edge in an increasingly competitive market.
    One way to gain this edge is to identify segments of teenagers who share similar
    tastes, so that clients can avoid targeting advertisements to teens with no interest
    in the product being sold. If it costs 10 dollars to display an advertisement
    to 1,000 website visitors—a measure of **cost per impression**—the advertising
    budget will stretch further if we are selective about who is targeted. For instance,
    an advertisement for sporting apparel should be targeted to clusters of individuals
    more likely to have an interest in sports.
  prefs: []
  type: TYPE_NORMAL
- en: Given the text of teenagers’ SNS posts, we can identify groups that share common
    interests such as sports, religion, or music. Clustering can automate the process
    of discovering the natural segments in this population. However, it will be up
    to us to decide whether the clusters are interesting and how to use them for advertising.
    Let’s try this process from start to finish.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – collecting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this analysis, we will be using a dataset representing a random sample of
    30,000 US high school students who had profiles on a well-known SNS in 2006\.
    To protect the users’ anonymity, the SNS will remain unnamed. However, at the
    time the data was collected, the SNS was a popular web destination for US teenagers.
    Therefore, it is reasonable to assume that the profiles represent a wide cross-section
    of American adolescents in 2006.
  prefs: []
  type: TYPE_NORMAL
- en: I compiled this dataset while conducting my own sociological research on teenage
    identities at the University of Notre Dame. If you use the data for research purposes,
    please cite this book chapter. The full dataset is available in the Packt Publishing
    GitHub repository for this book with the filename `snsdata.csv`. To follow along
    interactively, this chapter assumes you have saved this file to your R working
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: The data was sampled evenly across four high school graduation years (2006 through
    to 2009) representing the senior, junior, sophomore, and freshman classes at the
    time of data collection. Using an automated web crawler, the full text of the
    SNS profiles was downloaded, and each teen’s gender, age, and number of SNS friends
    were recorded.
  prefs: []
  type: TYPE_NORMAL
- en: 'A text-mining tool was used to divide the remaining SNS page content into words.
    From the top 500 words appearing across all pages, 36 words were chosen to represent
    five categories of interests: extracurricular activities, fashion, religion, romance,
    and antisocial behavior. The 36 words include terms such as *football*, *sexy*,
    *kissed*, *bible*, *shopping*, *death*, and *drugs*. The final dataset indicates,
    for each person, how many times each word appeared on the person’s SNS profile.'
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – exploring and preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll use `read.csv()` to load the dataset and convert the character data into
    factor types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also take a quick look at the specifics of the data. The first several
    lines of the `str()` output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As we had expected, the data includes 30,000 teenagers with four variables indicating
    personal characteristics and 36 words indicating interests.
  prefs: []
  type: TYPE_NORMAL
- en: Do you notice anything strange around the `gender` row? If you looked carefully,
    you may have noticed the `NA` value, which is out of place compared to the `1`
    and `2` values. The `NA` is R’s way of telling us that the record has a **missing
    value**—we do not know the person’s gender. Until now, we haven’t dealt with missing
    data, but it can be a significant problem for many types of analyses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how substantial this problem is. One option is to use the `table()`
    command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Although this tells us how many `F` and `M` values are present, the `table()`
    function excluded the `NA` values rather than treating them as a separate category.
    To include the `NA` values (if there are any), we simply need to add an additional
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, we see that 2,724 records (nine percent) have missing gender data. Interestingly,
    there are over four times as many females as males in the SNS data, suggesting
    that males are not as inclined to use this social media website as females.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you examine the other variables in the data frame, you will find that besides
    `gender`, only `age` has missing values. For numeric features, the default output
    for the `summary()` function includes the count of `NA` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: A total of 5,086 records (17 percent) have missing ages. Also concerning is
    the fact that the minimum and maximum values seem to be unreasonable; it is unlikely
    that a three-year-old or a 106-year-old is attending high school. To ensure that
    these extreme values don’t cause problems for the analysis, we’ll need to clean
    them up before moving on.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more plausible range of ages for high school students includes those who
    are at least 13 years old and not yet 20 years old. Any age value falling outside
    this range should be treated the same as missing data—we cannot trust the age
    provided. To recode the `age` variable, we can use the `ifelse()` function, assigning
    `teen$age` the original value of `teen$age` if the age is at least 13 and less
    than 20 years; otherwise, it will receive the value `NA`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'By rechecking the `summary()` output, we see that the range now follows a distribution
    that looks much more like an actual high school:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, now we’ve created an even larger missing data problem. We’ll
    need to find a way to deal with these values before continuing with our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – dummy coding missing values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An easy solution for handling missing values is to exclude any record with a
    missing value. However, if you think through the implications of this practice,
    you might think twice before doing so—just because it is easy does not mean it
    is a good idea! The problem with this approach is that even if the missingness
    is not extensive, you can easily exclude large portions of the data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose that in our data, the people with `NA` values for gender
    are completely different from those with missing age data. This would imply that
    by excluding those missing either gender or age, you would exclude *9% + 17% =
    26%* of the data, or over 7,500 records. And this is for missing data on only
    two variables! The larger the number of missing values present in a dataset, the
    more likely it is that any given record will be excluded. Fairly soon, you will
    be left with a tiny subset of data, or worse, the remaining records will be systematically
    different or non-representative of the full population.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative solution for categorical data like gender is to treat a missing
    value as a separate category. For instance, rather than limiting to female and
    male, we can add an additional category for unknown gender. This allows us to
    utilize dummy coding, which was covered in *Chapter 3*, *Lazy Learning – Classification
    Using Nearest Neighbors*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall, dummy coding involves creating a separate binary (1 or 0) valued
    dummy variable for each level of a nominal feature except one, which is held out
    to serve as the reference group. The reason one category can be excluded is because
    its status can be inferred from the other categories. For instance, if someone
    is not female and not unknown gender, they must be male. Therefore, in this case,
    we need to only create dummy variables for female and unknown gender:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you might expect, the `is.na()` function tests whether the gender is equal
    to `NA`. Therefore, the first statement assigns `teens$female` the value `1` if
    the gender is equal to `F` and the gender is not equal to `NA`; otherwise, it
    assigns the value `0`. In the second statement, if `is.na()` returns `TRUE`, meaning
    the gender is missing, then the `teens$no_gender` variable is assigned `1`; otherwise,
    it is assigned the value `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm that we did the work correctly, let’s compare our constructed dummy
    variables to the original `gender` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The number of `1` values for `teens$female` and `teens$no_gender` matches the
    number of `F` and `NA` values respectively, so the coding has been performed correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – imputing the missing values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, let’s eliminate the 5,523 missing ages. As `age` is a numeric feature,
    it doesn’t make sense to create an additional category for unknown values—where
    would you rank “unknown” relative to the other ages? Instead, we’ll use a different
    strategy known as **imputation**, which involves filling in the missing data with
    a guess as to the true value.
  prefs: []
  type: TYPE_NORMAL
- en: Can you think of a way we might be able to use the SNS data to make an informed
    guess about a teenager’s age? If you are thinking of using the graduation year,
    you’ve got the right idea. Most people in a graduation cohort were born within
    a single calendar year. If we can identify the typical age for each cohort, then
    we will have a reasonable approximation of the age of a student in that graduation
    year.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to find a typical value is by calculating the average, or mean, value.
    If we try to apply the `mean()` function as we have done for previous analyses,
    there’s a problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The issue is that the mean value is undefined for a vector containing missing
    data. As our age data contains missing values, `mean(teens$age)` returns a missing
    value. We can correct this by adding an additional `na.rm` parameter to remove
    the missing values before calculating the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This reveals that the average student in our data is about 17 years old. This
    only gets us part of the way there; we actually need the average age for each
    graduation year. You might first attempt to calculate the mean four times, but
    one of the benefits of R is that there’s usually a way to avoid repeating oneself.
    In this case, the `aggregate()` function is the tool for the job. It computes
    statistics for subgroups of data. Here, it calculates the mean age by graduation
    year after removing the `NA` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `aggregate()` output is in a data frame. This would require extra work to
    merge back into our original data. As an alternative, we can use the `ave()` function,
    which returns a vector with the means of each group repeated such that the resulting
    vector is the same length as the original vector. Where `aggregate()` returns
    one average age for each graduation year (a total of four values), the `ave()`
    function returns a value for all 30,000 teenagers reflecting the average age of
    students in that student’s graduation year (the same four values are repeated
    to reach a total of 30,000 values).
  prefs: []
  type: TYPE_NORMAL
- en: 'When using the `ave()` function, the first parameter is the numeric vector
    for which the group averages are to be computed, the second parameter is the categorical
    vector supplying the group assignments, and the `FUN` parameter is the function
    to be applied to the numeric vector. In our case, we need to define a new function
    that computes the mean with the `NA` values removed. The full command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To impute these means onto the missing values, we need one more `ifelse()`
    call to use the `ave_age` value only if the original age value was `NA`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `summary()` results show that the missing values have now been eliminated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: With the data ready for analysis, we are ready to dive into the interesting
    part of this project. Let’s see if our efforts have paid off.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – training a model on the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To cluster the teenagers into marketing segments, we’ll use an implementation
    of k-means in the `stats` package, which should be included in your R installation
    by default. Although there is no shortage of more sophisticated k-means functions
    available in other R packages, the `kmeans()` function in the default `stats`
    package is widely used and provides a simple yet powerful implementation of the
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17290_09_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11: K-means clustering syntax'
  prefs: []
  type: TYPE_NORMAL
- en: The `kmeans()` function requires a data frame or matrix containing only numeric
    data and a parameter specifying *k*, the desired number of clusters. If you have
    these two things ready, the actual process of building the model is simple. The
    trouble is that choosing the right combination of data and clusters can be a bit
    of an art; sometimes a great deal of trial and error is involved.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start our cluster analysis by considering only the 36 features that measure
    the number of times various interest-based keywords appeared in the text of the
    teenagers’ social media profiles. In other words, we will not cluster based on
    age, graduation year, gender, or number of friends. Of course, we *could* use
    these four features if desired, but *choose* not to, since any clusters built
    upon them would be less insightful than those built upon interests. This is primarily
    because age and gender are already de facto clusters whereas the interest-based
    clusters are yet to be discovered in our data. Secondarily, what will be more
    interesting later is to see whether the interest clusters are associated with
    the gender and popularity features held out from the clustering process. If the
    interest-based clusters are predictive of these individual characteristics, this
    provides evidence that the clusters may be useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid the chance of accidentally including the other features, let’s make
    a data frame called `interests`, by subsetting the data frame to include only
    the 36 keyword columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: If you recall from *Chapter 3*, *Lazy Learning – Classification Using Nearest
    Neighbors*, a common practice employed prior to any analysis using distance calculations
    is to normalize or z-score-standardize the features such that each utilizes the
    same range. By doing so, you can avoid a problem in which some features dominate
    solely because they have a larger range of values than the others.
  prefs: []
  type: TYPE_NORMAL
- en: The process of z-score standardization rescales features such that they have
    a mean of zero and a standard deviation of one. This transformation changes the
    interpretation of the data in a way that may be useful here. Specifically, if
    someone mentions basketball three times on their profile, without additional information,
    we have no idea whether this implies they like basketball more or less than their
    peers. On the other hand, if the z-score is three, we know that they mentioned
    basketball many more times than the average teenager.
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply z-score standardization to the `interests` data frame, we can use
    the `scale()` function with `lapply()`. Since `lapply()` returns a list object,
    it must be coerced back to data frame form using the `as.data.frame()` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm that the transformation worked correctly, we can compare the summary
    statistics of the `basketball` column in the old and new `interests` data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the `interests_z` dataset transformed the basketball feature to
    have a mean of zero and a range that spans above and below zero. Now, a value
    less than zero can be interpreted as a person having fewer-than-average mentions
    of basketball in their profile. A value greater than zero implies that the person
    mentioned basketball more frequently than the average.
  prefs: []
  type: TYPE_NORMAL
- en: Our last decision involves deciding how many clusters to use for segmenting
    the data. If we use too many clusters, we may find them too specific to be useful;
    conversely, choosing too few may result in heterogeneous groupings. You should
    feel comfortable experimenting with the value of *k*. If you don’t like the result,
    you can easily try another value and start over.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the number of clusters is easier if you are familiar with the analysis
    population. Having a hunch about the true number of natural groupings can save
    some trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help choose the number of clusters in the data, I’ll defer to one of my
    favorite films, *The Breakfast Club*, a coming-of-age comedy released in 1985
    and directed by John Hughes. The teenage characters in this movie are self-described
    in terms of five identities:'
  prefs: []
  type: TYPE_NORMAL
- en: A *brain* – also commonly known as a “nerd” or “geek”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An *athlete* – sometimes also known as a “jock” or “prep”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *basket case* – slang terminology for an anxious or neurotic individual, and
    depicted in the film as an anti-social outcast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *princess* – portrayed as a popular, affluent, and stereotypically feminine
    girl
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *criminal* – represents the traditional “burnout” identity described in sociological
    research as engaging in rebellious anti-school and anti-authority behaviors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though the movie depicts five specific identity groups, they have been
    described throughout popular teenage fiction for many years, and although the
    stereotypes have evolved over time, American teenagers are likely to understand
    them intuitively. Thus, five seems like a reasonable starting point for *k*, though
    admittedly, it is unlikely to capture the full spectrum of high-school identities.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the k-means algorithm to divide the teenagers’ interest data into five
    clusters, we use the `kmeans()` function on the `interests` data frame. Note that
    because k-means utilizes random starting points, the `set.seed()` function is
    used to ensure that the results match the output in the examples that follow.
    If you recall from previous chapters, this command initializes R’s random number
    generator to a specific sequence. In the absence of this statement, the results
    may vary each time the k-means algorithm is run. Running the k-means clustering
    process as follows creates a list named `teen_clusters`, which stores the properties
    of each of the five clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Let’s dig in and see how well the algorithm has divided the teenagers’ interest
    data.
  prefs: []
  type: TYPE_NORMAL
- en: If you find that your results differ from those shown in the sections that follow,
    ensure that the `set.seed(2345)` command is run immediately prior to the `kmeans()`
    function. Additionally, because the behavior of R’s random number generator changed
    with R version 3.6, your results may also vary slightly from those shown here
    if you are using an older version of R.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – evaluating model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluating clustering results can be somewhat subjective. Ultimately, the success
    or failure of the model hinges on whether the clusters are useful for their intended
    purpose. As the goal of this analysis was to identify clusters of teenagers with
    similar interests for marketing purposes, we will largely measure our success
    in qualitative terms. For other clustering applications, more quantitative measures
    of success may be needed.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most basic ways to evaluate the utility of a set of clusters is to
    examine the number of examples falling in each of the groups. If some groups are
    too large or too small, then they are less likely to be very useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain the size of the `kmeans()` clusters, simply examine the `teen_clusters$size`
    component as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Here we see the five clusters we requested. The smallest cluster has 601 teenagers
    (2 percent) while the largest has 21,599 (72 percent). Although the large gap
    between the number of people in the largest and smallest clusters is slightly
    concerning, without examining these groups more carefully, we will not know whether
    this indicates a problem. It may be the case that the clusters’ size disparity
    indicates something real, such as a big group of teenagers who share similar interests,
    or it may be a random fluke caused by the initial k-means cluster centers. We’ll
    know more as we start to look at each cluster’s characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, k-means may find extremely small clusters—occasionally as small as
    a single point. This can happen if one of the initial cluster centers happen to
    fall on outliers far from the rest of the data. It is not always clear whether
    to treat such small clusters as a true finding that represents a cluster of extreme
    cases, or a problem caused by random chance. If you encounter this issue, it may
    be worth re-running the k-means algorithm with a different random seed to see
    whether the small cluster is robust to different starting points.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more in-depth look at the clusters, we can examine the coordinates of
    the cluster centroids using the `teen_clusters$centers` component, which is as
    follows for the first four interests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The rows of the output (labeled `1` to `5`) refer to the five clusters, while
    the numbers across each row indicate the cluster’s average value for the interest
    listed at the top of the column. Because the values are z-score-standardized,
    positive values are above the overall mean level for all teenagers and negative
    values are below the overall mean.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the fourth row has the highest value in the `basketball` column,
    which means that cluster `4` has the highest average interest in basketball among
    all the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'By examining whether clusters fall above or below the mean level for each interest
    category, we can discover patterns that distinguish the clusters from one another.
    In practice, this involves printing the cluster centers and searching through
    them for any patterns or extreme values, much like a word search puzzle but with
    numbers. The following annotated screenshot shows a highlighted pattern for each
    of the five clusters, for 18 of the 36 teenager interests:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_09_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: To distinguish clusters, it can be helpful to highlight patterns
    in the coordinates of their centroids'
  prefs: []
  type: TYPE_NORMAL
- en: Given this snapshot of the interest data, we can already infer some characteristics
    of the clusters. Cluster four is substantially above the mean interest level on
    nearly all the sports, which suggests that this may be a group of *athletes* per
    *The* *Breakfast Club* stereotype. Cluster three includes the most mentions of
    cheerleading, dancing, and the word “hot.” Are these the so-called princesses?
  prefs: []
  type: TYPE_NORMAL
- en: By continuing to examine the clusters in this way, it is possible to construct
    a table listing the dominant interests of each of the groups. In the following
    table, each cluster is shown with the features that most distinguish it from the
    other clusters, and *The* *Breakfast Club* identity that seems to most accurately
    capture the group’s characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, cluster five is distinguished by the fact that it is unexceptional:
    its members had lower-than-average levels of interest in every measured activity.
    It is also the single largest group in terms of the number of members. How can
    we reconcile these apparent contradictions? One potential explanation is that
    these users created a profile on the website but never posted any interests.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, table  Description automatically generated](img/B17290_09_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: A table can be used to list important dimensions of each cluster'
  prefs: []
  type: TYPE_NORMAL
- en: When sharing the results of a segmentation analysis with stakeholders, it is
    often helpful to apply memorable and informative labels known as **personas**,
    which simplify and capture the essence of the groups, such as *The* *Breakfast
    Club* typology applied here. The risk in adding such labels is that they can obscure
    the groups’ nuances and possibly even offend the group members if negative stereotypes
    are used. For wider dissemination, provocative labels like “Criminals” and “Princesses”
    might be replaced by more neutral terminology like “Edgy Adolescents” and “Trendy
    Teenagers.” Additionally, because even relatively harmless labels can bias our
    thinking, important patterns can be missed if labels are understood as the whole
    truth rather than a simplification of complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Given memorable labels and a table as depicted in *Figure 9.13*, a marketing
    executive would have a clear mental picture of five types of teenage visitors
    to the social networking website. Based on these personas, the executive could
    sell targeted advertising impressions to businesses with products relevant to
    one or more of the clusters. In the next section, we will see how the cluster
    labels can be applied back to the original population for such uses.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to visualize the results of a cluster analysis using techniques
    that flatten the multidimensional feature data into two dimensions, then color
    the points according to cluster assignment. The `fviz_cluster()` function in the
    `factoextra` package allows such visualizations to be constructed quite easily.
    If this is of interest to you, load the package and try the command `fviz_cluster(teen_clusters,
    interests_z, geom = "point")` to see such a visualization for the teenage SNS
    clusters. Although the visual is of limited use for the SNS example due to the
    large number of overlapping points, sometimes, it can be a helpful tool for presentation
    purposes. To better understand how to create and understand these plots, see *Chapter
    15*, *Making Use of Big Data*.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – improving model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because clustering creates new information, the performance of a clustering
    algorithm depends at least somewhat on both the quality of the clusters themselves
    and what is done with that information. In the preceding section, we demonstrated
    that the five clusters provided useful and novel insights into the interests of
    teenagers. By that measure, the algorithm appears to be performing quite well.
    Therefore, we can now focus our effort on turning these insights into action.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin by applying the clusters back to the full dataset. The `teen_clusters`
    object created by the `kmeans()` function includes a component named `cluster`,
    which contains the cluster assignments for all 30,000 individuals in the sample.
    We can add this as a column to the `teens` data frame with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Given this new data, we can start to examine how the cluster assignment relates
    to individual characteristics. For example, here’s the personal information for
    the first five teenagers in the SNS data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `aggregate()` function, we can also look at the demographic characteristics
    of the clusters. The mean age does not vary much by cluster, which is not too
    surprising, as teen identities are often set well before high school. This is
    depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, there are some substantial differences in the proportion
    of females by cluster. This is a very interesting finding, as we didn’t use gender
    data to create the clusters, yet the clusters are still predictive of gender:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Recall that overall, about 74 percent of the SNS users are female. Cluster three,
    the so-called *princesses*, is nearly 89 percent female, while clusters four and
    five are only about 70 percent female. These disparities imply that there are
    differences in the interests that teenage boys and girls discuss on their social
    networking pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given our success in predicting gender, you might suspect that the clusters
    are also predictive of the number of friends the users have. This hypothesis seems
    to be supported by the data, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'On average, *princesses* have the most friends (38.5), followed by *athletes*
    (35.9) and *brains* (32.8). On the low end are *criminals* (30.7) and *basket
    cases* (27.8). As with gender, the connection between a teenager’s number of friends
    and their predicted cluster is remarkable given that we did not use the friendship
    data as an input to the clustering algorithm. Also interesting is the fact that
    the number of friends seems to be related to the stereotype of each cluster’s
    high-school popularity: the stereotypically popular groups tend to have more friends
    in reality.'
  prefs: []
  type: TYPE_NORMAL
- en: The association between group membership, gender, and number of friends suggests
    that the clusters can be useful predictors of behavior. Validating their predictive
    ability in this way may make the clusters an easier sell when they are pitched
    to the marketing team, ultimately improving the performance of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Just as the characters in *The Breakfast Club* ultimately come to realize that
    “each one of us is a brain, an athlete, a basket case, a princess, and a criminal,”
    it is important for data scientists to realize that the labels or personas we
    attribute to each cluster are stereotypes, and individuals may embody the stereotype
    to a greater or lesser degree. Keep this caveat in mind when acting on the results
    of a cluster analysis; a group may be relatively homogenous, but each member is
    still unique.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our findings support the popular adage that “birds of a feather flock together.”
    By using machine learning methods to cluster teenagers with others who have similar
    interests, we were able to develop a typology of teenage identities, which was
    predictive of personal characteristics such as gender and number of friends. These
    same methods can be applied to other contexts with similar results.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covered only the fundamentals of clustering. There are many variants
    of the k-means algorithm, as well as many other clustering algorithms, which bring
    unique biases and heuristics to the task. Based on the foundation in this chapter,
    you will be able to understand these clustering methods and apply them to new
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin to look at methods for measuring the success
    of a learning algorithm that are applicable across many machine learning tasks.
    While our process has always devoted some effort to evaluating the success of
    learning, in order to obtain the highest degree of performance, it is crucial
    to be able to define and measure it in the strictest terms.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  prefs: []
  type: TYPE_IMG
