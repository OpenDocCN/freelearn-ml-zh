- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Finding Groups of Data – Clustering with k-means
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找数据组——使用k-means进行聚类
- en: Have you ever spent time watching a crowd? If so, you have likely seen some
    recurring personalities. Perhaps a certain type of person, identified by a freshly
    pressed suit and a briefcase, comes to typify the “fat cat” business executive.
    A 20-something wearing skinny jeans, a flannel shirt, and sunglasses might be
    dubbed a “hipster,” while a woman unloading children from a minivan may be labeled
    a “soccer mom.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经花时间观察过人群？如果是的话，你可能已经看到了一些反复出现的个性特征。也许某种类型的人，通过一套新熨烫的西装和公文包，可以代表“肥猫”商业高管。一个二十多岁穿着紧身牛仔裤、法兰绒衬衫和太阳镜的人可能被称为“嬉皮士”，而一个从微型货车卸下孩子的女人可能被贴上“足球妈妈”的标签。
- en: Of course, these types of stereotypes are dangerous to apply to individuals,
    as no two people are exactly alike. Yet, understood as a way to describe a collective,
    the labels capture some underlying aspect of similarity shared among the individuals
    within the group.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，将这些类型的刻板印象应用于个人是危险的，因为没有人是完全相同的。然而，如果将其理解为描述集体的一种方式，这些标签就能捕捉到群体中个体之间共享的一些潜在相似性。
- en: 'As you will soon learn, the act of clustering, or spotting patterns in data,
    is not much different from spotting patterns in groups of people. This chapter
    describes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如你很快就会了解到的那样，聚类或发现数据中的模式这一行为，与发现人群中的模式并没有太大的区别。本章将描述：
- en: The ways clustering tasks differ from the classification tasks we examined previously
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类任务与我们之前考察的分类任务的不同之处
- en: How clustering defines a group and how such groups are identified by k-means,
    a classic and easy-to-understand clustering algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类如何定义一个组以及这些组如何通过k-means，一种经典且易于理解的聚类算法来识别
- en: The steps needed to apply clustering to a real-world task of identifying marketing
    segments among teenage social media users
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将聚类应用于识别青少年社交媒体用户中营销细分市场的实际任务所需的步骤
- en: Before jumping into action, we’ll begin by taking an in-depth look at exactly
    what clustering entails.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在采取行动之前，我们将首先深入探讨聚类究竟意味着什么。
- en: Understanding clustering
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解聚类
- en: Clustering is an unsupervised machine learning task that automatically divides
    the data into **clusters**, or groups of similar items. It does this without having
    been told how the groups should look ahead of time. Because we do not tell the
    machine specifically what we’re looking for, clustering is used for knowledge
    discovery rather than prediction. It provides an insight into the natural groupings
    found within data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督的机器学习任务，它自动将数据划分为**簇**，或相似项的组。它这样做，而无需事先被告知这些组应该如何看起来。因为我们没有告诉机器我们具体在寻找什么，所以聚类用于知识发现而不是预测。它提供了对数据中自然分组洞察。
- en: 'Without advanced knowledge of what comprises a cluster, how can a computer
    possibly know where one group ends and another begins? The answer is simple: clustering
    is guided by the principle that items inside a cluster should be very similar
    to each other, but very different from those outside. The definition of similarity
    might vary across applications, but the basic idea is always the same: group the
    data such that related elements are placed together.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 没有关于构成簇的先进知识，计算机怎么可能知道一个组在哪里结束，另一个组在哪里开始呢？答案是简单的：聚类是由这样一个原则指导的，即簇内的项目应该彼此非常相似，但与簇外的项目非常不同。相似性的定义可能因应用而异，但基本思想始终相同：将数据分组，使得相关元素放在一起。
- en: 'The resulting clusters can then be used for action. For instance, you might
    find clustering methods employed in applications such as:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以使用这些簇进行行动。例如，你可能会发现聚类方法被用于以下应用：
- en: Segmenting customers into groups with similar demographics or buying patterns
    for targeted marketing campaigns
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将客户分成具有相似人口统计或购买模式的小组，以进行定向营销活动
- en: Detecting anomalous behavior, such as unauthorized network intrusions, by identifying
    patterns of use falling outside known clusters
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过识别使用模式落在已知簇之外的模式来检测异常行为，例如未经授权的网络入侵
- en: Simplifying extremely “wide” datasets—those with a large number of features—by
    creating a small number of categories to describe rows with relatively homogeneous
    values of the features
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过创建少量类别来简化极其“宽”的数据集——那些具有大量特征的数据集——以描述具有相对同质特征值的行
- en: Overall, clustering is useful whenever diverse and varied data can be exemplified
    by a much smaller number of groups. It results in meaningful and actionable data
    structures, which reduce complexity and provide insight into patterns of relationships.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，当可以用更少的组来代表多样化和多变的数据时，聚类是有用的。它产生了有意义的可操作数据结构，减少了复杂性，并提供了对关系模式的洞察。
- en: Clustering as a machine learning task
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类作为机器学习任务
- en: Clustering is somewhat different from the classification, numeric prediction,
    and pattern detection tasks we’ve examined so far. In each of these tasks, the
    goal was to build a model that relates features to an outcome, or to relate some
    features to other features. Each of these tasks describes existing patterns within
    data. In contrast, the goal of clustering is to create new data. In clustering,
    unlabeled examples are given a new cluster label, which has been inferred entirely
    from the relationships within the data. For this reason, you will sometimes see
    a clustering task referred to as **unsupervised classification** because, in a
    sense, it classifies unlabeled examples.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类与我们迄今为止考察的分类、数值预测和模式检测任务有所不同。在这些任务中，目标是建立一个将特征与结果相关联的模型，或者将一些特征与另一些特征相关联。这些任务中的每一个都描述了数据中的现有模式。相比之下，聚类的目标是创建新的数据。在聚类中，未标记的示例被赋予一个新的聚类标签，这个标签完全是从数据中的关系推断出来的。因此，有时你会看到聚类任务被称为**无监督分类**，因为在某种程度上，它对未标记的示例进行了分类。
- en: The catch is that the class labels obtained from an unsupervised classifier
    are without intrinsic meaning. Clustering will tell you which groups of examples
    are closely related—for instance, it might return groups A, B, and C—but it’s
    up to you to apply an actionable and meaningful label, and to tell the story of
    what makes an “A” different from a “B.” To see how this impacts the clustering
    task, let’s consider a simple hypothetical example.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但问题是，从无监督分类器获得的类别标签没有内在的意义。聚类会告诉你哪些示例组紧密相关——例如，它可能会返回组A、B和C——但具体应用一个有意义的可操作标签，以及讲述“为什么A与B不同”的故事，取决于你。为了了解这如何影响聚类任务，让我们考虑一个简单的假设例子。
- en: Suppose you were organizing a conference on the topic of data science. To facilitate
    professional networking and collaboration, you planned to seat people at one of
    three tables according to their research specialties. Unfortunately, after sending
    out the conference invitations, you realize that you forgot to include a survey
    asking which discipline the attendee would prefer to be seated within.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在组织一个关于数据科学的会议。为了促进专业网络和协作，你计划根据他们的研究专长将人们安排在三个桌子中的一张。不幸的是，在发出会议邀请后，你意识到你忘记包括一个调查，询问与会者希望坐在哪个学科组内。
- en: 'In a stroke of brilliance, you realize that you might be able to infer each
    scholar’s research specialty by examining their publication history. To this end,
    you begin collecting data on the number of articles each attendee has published
    in computer science-related journals and the number of articles published in math-
    or statistics-related journals. Using the data collected for the scholars, you
    create a scatterplot:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在一次灵光一闪中，你意识到你可能能够通过检查每位学者的出版物历史来推断他们的研究专长。为此，你开始收集每位与会者在计算机科学相关期刊上发表的文章数量以及发表在数学或统计学相关期刊上的文章数量。使用为学者收集的数据，你创建了一个散点图：
- en: '![Chart, scatter chart  Description automatically generated](img/B17290_09_01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  描述自动生成](img/B17290_09_01.png)'
- en: 'Figure 9.1: Visualizing scholars by their math and computer science publication
    data'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：通过数学和计算机科学出版物数据可视化学者
- en: As expected, there seems to be a pattern. We might guess that the upper-left
    corner, which represents people with many computer science publications but few
    articles on math, is a cluster of computer scientists. Following this logic, the
    lower-right corner might be a group of mathematicians or statisticians. Similarly,
    the upper-right corner, those with both math and computer science experience,
    may be machine learning experts.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，似乎存在一种模式。我们可能会猜测左上角，代表那些有众多计算机科学出版物但数学文章很少的人，是一群计算机科学家。按照这个逻辑，右下角可能是一群数学家或统计学家。同样，右上角，那些既有数学又有计算机科学经验的人，可能是机器学习专家。
- en: 'Applying these labels results in the following visualization:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些标签会产生以下可视化效果：
- en: '![Diagram  Description automatically generated](img/B17290_09_02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图示  描述自动生成](img/B17290_09_02.png)'
- en: 'Figure 9.2: Clusters can be identified based on presumptions about the scholars
    in each group'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：可以根据对每组学者的假设来识别集群
- en: Our groupings were formed visually; we simply identified clusters as closely
    grouped data points. Yet, despite the seemingly obvious groupings, without personally
    asking each scholar about their academic specialty, we have no way to know whether
    the groups are truly homogeneous. The labels are qualitative, presumptive judgments
    about the types of people in each group, based on a limited set of quantitative
    data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分组是通过视觉形成的；我们只是将紧密聚集的数据点识别为集群。然而，尽管看似明显的分组，如果没有亲自询问每位学者的学术专长，我们就无法知道这些群体是否真正同质。标签是关于每个群体中人的类型的定性、假设性判断，基于有限的一组定量数据。
- en: Rather than defining the group boundaries subjectively, it would be nice to
    use machine learning to define them objectively. Given the axis-parallel splits
    in the previous figure, our problem seems like an obvious application for decision
    trees, as described in *Chapter 5*, *Divide and Conquer – Classification Using
    Decision Trees and Rules*. This would provide us with a clean rule like “if a
    scholar has few math publications, then they are a computer science expert.” Unfortunately,
    there’s a problem with this plan. Without data on the true class value for each
    point, a supervised learning algorithm would have no ability to learn such a pattern,
    as it would have no way of knowing what splits would result in homogenous groups.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与主观定义群体边界相比，使用机器学习来客观地定义它们会更好。鉴于前一个图中的轴平行分割，我们的问题似乎是一个明显的决策树应用，如*第五章*，*分而治之
    – 使用决策树和规则进行分类*中所述。这将为我们提供一个干净的规则，例如：“如果一个学者数学出版物很少，那么他们是计算机科学专家。”不幸的是，这个计划有问题。没有每个点的真实类别值数据，监督学习算法就无法学习这样的模式，因为它无法知道哪些分割会产生同质群体。
- en: In contrast to supervised learning, clustering algorithms use a process very
    similar to what we did by visually inspecting the scatterplot. Using a measure
    of how closely the examples are related, homogeneous groups can be identified.
    In the next section, we’ll start looking at how clustering algorithms are implemented.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习相反，聚类算法使用的过程与我们通过视觉检查散点图所做的过程非常相似。使用示例之间关系的度量，可以识别出同质群体。在下一节中，我们将开始探讨聚类算法是如何实现的。
- en: This example highlights an interesting application of clustering. If you begin
    with unlabeled data, you can use clustering to create class labels. From there,
    you could apply a supervised learner such as decision trees to find the most important
    predictors of these classes. This is an example of semi-supervised learning as
    described in *Chapter 1*, *Introducing Machine Learning*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子突出了聚类的一个有趣应用。如果您从未标记的数据开始，可以使用聚类来创建类标签。从那里，您可以应用像决策树这样的监督学习器来找到这些类别的最重要的预测因子。这是*第一章*，*介绍机器学习*中描述的半监督学习的一个例子。
- en: Clusters of clustering algorithms
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类算法的集群
- en: 'Just as there are many approaches to building a predictive model, there are
    multiple approaches to performing the descriptive task of clustering. Many such
    methods are listed on the following site, the CRAN task view for clustering: [https://cran.r-project.org/view=Cluster](https://cran.r-project.org/view=Cluster).
    Here, you will find numerous R packages used for discovering natural groupings
    in data. The various algorithms are distinguished mainly by two characteristics:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如构建预测模型有许多方法一样，执行聚类描述性任务的方法也有很多。许多此类方法列在以下网站上，CRAN聚类任务视图：[https://cran.r-project.org/view=Cluster](https://cran.r-project.org/view=Cluster)。在这里，您可以找到用于在数据中发现自然分组的众多R包。不同的算法主要根据两个特征来区分：
- en: The **similarity metric**, which provides the quantitative measure of how closely
    two examples are related
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似度度量**，它提供了两个示例之间关系的定量度量'
- en: The **agglomeration function**, which governs the process of assigning examples
    to clusters based on their similarity
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合函数**，它控制着根据示例之间的相似性将示例分配到集群的过程'
- en: 'Even though there may be subtle differences between the approaches, they can
    of course be clustered in various ways. Multiple such typologies exist, but a
    simple three-part framework helps understand the main distinctions. Using this
    approach, the three main clusters of clustering algorithms, listed from simplest
    to most sophisticated, are as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些方法之间可能存在细微的差异，但它们当然可以以各种方式聚类。存在多种这样的类型，但一个简单的三部分框架有助于理解主要区别。使用这种方法，从最简单到最复杂，以下是聚类算法的三个主要类别：
- en: '**Hierarchical methods**, which create a family tree-style hierarchy that positions
    the most similar examples more closely in the graph structure'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次方法**，这些方法创建了一种家族树式的层次结构，将最相似的示例在图结构中放置得更近。'
- en: '**Partition-based methods**, which treat the examples as points in multidimensional
    space, and attempt to find boundaries in this space that lead to relatively homogenous
    groups'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于划分的方法**，这些方法将示例视为多维空间中的点，并试图找到这个空间中的边界，以形成相对同质的小组。'
- en: '**Model or density-based methods**, which rely on statistical principles and/or
    the density of points to discover fuzzy boundaries between the clusters; in some
    cases, examples may be partially assigned to multiple clusters, or even to no
    cluster at all'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型或密度的方法**，这些方法依赖于统计原理和/或点的密度来发现簇之间的模糊边界；在某些情况下，示例可能部分分配到多个簇中，甚至没有任何簇。'
- en: Despite **hierarchical clustering** being the simplest of the methods, it is
    not without a pair of interesting upsides. Firstly, it results in a hierarchical
    graph visualization called a **dendrogram**, which depicts the associations between
    examples such that the most similar examples are positioned more closely in the
    hierarchy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管层次聚类是这些方法中最简单的，但它并非没有两个有趣的优点。首先，它产生了一种称为**树状图**的层次图可视化，它描绘了示例之间的关联，使得最相似的示例在层次结构中位置更近。
- en: This can be a useful tool to understand which examples and subsets of examples
    are the most tightly grouped. Secondly, hierarchical clustering does not require
    a predefined expectation of how many clusters exist in the dataset. Instead, it
    implements a process in which, at one extreme, every example is included in a
    single, giant cluster with all other examples; at the other extreme, every example
    is found in a tiny cluster containing only itself; and in between, examples may
    be included in other clusters of varying sizes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以是一个有用的工具，用来理解哪些示例和示例的子集是最紧密地分组的。其次，层次聚类不需要预先定义数据集中存在多少簇的期望。相反，它实施了一个过程，在这个过程中，在一种极端情况下，每个示例都包含在一个包含所有其他示例的单个大簇中；在另一种极端情况下，每个示例都发现自己在一个只包含它自己的小簇中；在两者之间，示例可能包含在其他大小不一的簇中。
- en: '*Figure 9.3* illustrates a hypothetical dendrogram for a simple dataset containing
    eight examples, labeled A through H. Notice that the most closely related examples
    (depicted via proximity on the *x* axis) are linked more closely as siblings in
    the diagram. For instance, examples D and E are the most similar and thus are
    the first to be grouped. However, all eight examples are eventually linked to
    one large cluster, or may be included in any number of clusters in between. Slicing
    the dendrogram horizontally at different positions creates varying numbers of
    clusters, as shown for three and five clusters:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.3* 展示了一个包含八个示例的简单数据集的假设树状图，这些示例分别标记为A到H。请注意，最相关的示例（通过*x*轴上的邻近性表示）在图中被更紧密地连接，作为兄弟姐妹。例如，示例D和E是最相似的，因此它们是最先被分组的。然而，所有八个示例最终都会连接到一个大的簇中，或者可能包含在任何数量的簇之间。在树状图的横向不同位置切割，会创建不同数量的簇，如图所示为三个和五个簇：'
- en: '![Chart, diagram, box and whisker chart  Description automatically generated](img/B17290_09_03.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图表、示意图、箱线图 描述自动生成](img/B17290_09_03.png)'
- en: 'Figure 9.3: Hierarchical clustering produces a dendrogram that depicts the
    natural groupings for the desired number of clusters'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：层次聚类产生一个树状图，描绘了所需簇数的自然分组。
- en: The dendrogram for hierarchical clustering can be grown using “bottom-up” or
    “top-down” approaches. The former is called **agglomerative clustering**, and
    begins with each example in its own cluster, then connects the most similar examples
    first until all examples are connected in a single cluster. The latter is called
    **divisive clustering**, and begins with a single large cluster and ends with
    all examples in their own individual clusters.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的树状图可以使用“自下而上”或“自上而下”的方法生成。前者称为**聚合聚类**，它从每个示例自己的簇开始，然后首先连接最相似的示例，直到所有示例都连接到一个单独的簇中。后者称为**分裂聚类**，它从一个大型簇开始，以所有示例都在自己的单独簇中结束。
- en: When connecting examples to groups of examples, different metrics may be used,
    such as the example’s similarity to the most similar, least similar, or average
    member of the group. A more complex metric known as **Ward’s method** does not
    use similarity between examples, but instead considers a measure of cluster homogeneity
    to construct the linkages. In any case, the result is a hierarchy that aims to
    group the most similar examples into any number of subgroups.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在将示例连接到示例组时，可以使用不同的度量标准，例如示例与组中最相似、最不相似或平均成员的相似度。一种更复杂的度量标准称为**沃德方法**，它不使用示例之间的相似度，而是考虑簇同质性的度量来构建链接。无论如何，结果是旨在将最相似的示例分组到任意数量的子组中的层次结构。
- en: The flexibility of the hierarchical clustering technique comes at a cost, which
    is computational complexity due to the need to compute the similarity between
    each example and every other. As the number of examples (*N*) grows, the number
    of calculations grows as *N*N = N*², as does the memory needed for the similarity
    matrix that stores the result. For this reason, hierarchical clustering is used
    only on very small datasets and is not demonstrated in this chapter. However,
    the `hclust()` function included in R’s `stats` package provides a simple implementation,
    which is installed with R by default.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类技术的灵活性是以计算复杂性为代价的，这是由于需要计算每个示例与其他每个示例之间的相似性。随着示例数量（N）的增长，计算数量会增长到N*N = N²，存储结果的相似性矩阵所需的内存也会增加。因此，层次聚类仅用于非常小的数据集，本章没有演示。然而，R的`stats`包中包含的`hclust()`函数提供了一个简单的实现，该实现默认与R一起安装。
- en: Clever implementations of divisive clustering have the potential to be slightly
    more computationally efficient than agglomerative clustering as the algorithm
    may stop early if it is unnecessary to create larger numbers of clusters. This
    being said, both agglomerative and divisive clustering are examples of “greedy”
    algorithms as defined in *Chapter 5*, *Divide and Conquer – Classification Using
    Decision Trees and Rules*, because they use data on a first-come, first-served
    basis and are thus not guaranteed to produce the overall optimal set of clusters
    for a given dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 分裂聚类的巧妙实现可能比聚合聚类在计算上稍微高效一些，因为算法可能会在不需要创建更多簇的情况下提前停止。尽管如此，聚合聚类和分裂聚类都是“贪婪”算法的例子，正如在第5章“分而治之——使用决策树和规则进行分类”中定义的那样，因为它们基于“先来先服务”的原则使用数据，因此不能保证为给定的数据集产生整体最优的簇集。
- en: '**Partition-based clustering** methods have a distinct efficiency advantage
    over hierarchical clustering in that they apply heuristic methods to divide the
    data into clusters without the need to evaluate the similarity between every pair
    of examples. We’ll explore a widely used partition-based method in greater detail
    shortly, but for now it suffices to understand that this method is concerned with
    finding boundaries between clusters rather than connecting examples to one another—an
    approach that requires far fewer comparisons between examples. This heuristic
    can be quite computationally efficient, but one caveat is that it is somewhat
    rigid or even arbitrary when it comes to group assignments. For example, if five
    clusters are requested, it will partition examples into all five clusters; if
    some examples fall on the boundary between two clusters, these will be placed
    somewhat arbitrarily yet firmly into one cluster or the other. Similarly, if four
    or six clusters might have split the data better, this would not be as apparent
    as it would be with the hierarchical clustering dendrogram.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于划分的聚类**方法在效率上具有比层次聚类明显的优势，因为它们通过应用启发式方法将数据划分为集群，而不需要评估每对示例之间的相似性。我们将在稍后更详细地探讨一种广泛使用的基于划分的方法，但就目前而言，只需了解这种方法关注的是寻找集群之间的边界，而不是将示例相互连接——这种方法需要远少于示例之间的比较。这种启发式方法在计算上可能非常高效，但有一个缺点是，在分组分配方面可能有些僵硬甚至任意。例如，如果请求五个集群，它将示例划分为所有五个集群；如果某些示例位于两个集群之间的边界上，这些示例将被随意但坚定地放入一个集群或另一个集群。同样，如果四个或六个集群可能更好地分割数据，这不会像层次聚类树状图那样明显。'
- en: The more sophisticated **model-based and density-based clustering** methods
    address some of these issues of inflexibility by estimating the probability that
    an example belongs to each cluster, rather than merely assigning it to a single
    cluster. Some of them may allow the cluster boundaries to follow the natural patterns
    identified in the data rather than forcing a strict divide between the groups.
    Model-based approaches often assume a statistical distribution from which the
    examples are believed to have been pulled.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的**基于模型和密度聚类**方法通过估计示例属于每个集群的概率来解决了一些这些不灵活的问题，而不是仅仅将其分配到单个集群。其中一些可能允许集群边界遵循在数据中识别出的自然模式，而不是强制在组之间进行严格的划分。基于模型的方法通常假设一个统计分布，认为示例是从该分布中抽取的。
- en: One such approach, known as **mixture modeling**, attempts to disentangle datasets
    composed of examples pulled from a mixture of statistical distributions—typically
    Gaussian (the normal bell curve). For example, imagine you have a dataset composed
    of voice data from a mixture of male and female vocal registers, as depicted in
    *Figure 9.4* (note that the distributions are hypothetical and are not based on
    real-world data). Although there is some overlap between the two, the average
    male tends to have a lower register than the average female.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种方法，称为**混合建模**，试图解开由从统计分布混合中抽取的示例组成的集合数据——通常是高斯分布（正态钟形曲线）。例如，想象你有一个由混合男性和女性音域的语音数据组成的集合数据，如图*图9.4*所示（请注意，分布是假设的，并非基于现实世界的数据）。尽管两者之间有一些重叠，但平均而言，男性的音域通常低于女性的音域。
- en: '![Diagram  Description automatically generated](img/B17290_09_04.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_09_04.png)'
- en: 'Figure 9.4: Mixture modeling assigns each example a probability of belonging
    to one of the underlying distributions'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：混合建模为每个示例分配属于潜在分布之一的概率
- en: Given the unlabeled overall distribution (the bottom part of the figure), a
    mixture model would be capable of assigning a probability that any given example
    belongs to the cluster of males or the cluster of females, incredibly, without
    ever having been trained separately on male or female voices in the top part of
    the figure! This is possible by discovering the statistical parameters like the
    mean and standard deviation that are most likely to have generated the observed
    overall distribution, under the assumption that a specific number of distinct
    distributions were involved—in this case, two Gaussian distributions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到未标记的整体分布（图的下部），混合模型能够为任何给定示例属于男性集群或女性集群的概率分配一个概率，令人难以置信的是，它从未在图的上部单独对男声或女声进行过训练！这是通过发现最有可能生成观察到的整体分布的统计参数，如均值和标准差，在假设涉及特定数量的不同分布的情况下实现的——在这种情况下，是两个高斯分布。
- en: As an unsupervised method, the mixture model would have no way of knowing that
    the left distribution is males and the right distribution is females, but this
    would be readily apparent to a human observer comparing the records with a high
    likelihood of males being in the left cluster versus the right one. The downside
    of this technique is that not only does it require knowledge of how many distributions
    are involved but it also requires an assumption of the types of distributions.
    This may be too rigid for many real-world clustering tasks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种无监督方法，混合模型将无法知道左边的分布是男性，而右边的分布是女性，但一个人类观察者比较记录时，如果左簇中男性出现的可能性高于右簇，这将是显而易见的。这种技术的缺点是，它不仅需要了解涉及多少分布，还需要假设分布的类型。这可能对许多实际聚类任务来说过于僵化。
- en: Another powerful clustering technique called **DBSCAN** is named after the “density-based
    spatial clustering of applications with noise” approach it uses to identify natural
    clusters in data. This award-winning technique is incredibly flexible and does
    well with many of the challenges of clustering, such as adapting to the dataset’s
    natural number of clusters, being flexible about the boundaries between clusters,
    and not assuming a particular statistical distribution for the data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种名为**DBSCAN**的强大聚类技术，其命名来源于它所使用的“基于密度的空间聚类应用噪声”方法，该方法用于在数据中识别自然簇。这项获奖技术极其灵活，在处理许多聚类挑战方面表现良好，例如适应数据集的自然簇数量、对簇之间的边界灵活处理，以及不对数据进行特定的统计分布假设。
- en: While the implementation details are beyond the scope of this book, the DBSCAN
    algorithm can be understood intuitively as a process of creating neighborhoods
    of examples that are all within a given radius of other examples in the cluster.
    A predefined number of **core points** within a specified radius forms the initial
    cluster nucleus, and points that are within a specified radius of any of the core
    points are then added to the cluster and comprise the outermost boundary of the
    cluster. Unlike many other clustering algorithms, some examples will not be assigned
    any cluster at all, as any points that are not close enough to a core point will
    be treated as noise.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然实现细节超出了本书的范围，但DBSCAN算法可以直观地理解为创建一个过程，该过程为簇中的示例创建邻域，这些示例都在给定半径内。在指定半径内预定义的**核心点**形成初始簇核，然后位于任何核心点指定半径内的点被添加到簇中，并构成簇的最外层边界。与许多其他聚类算法不同，一些示例可能根本不会被分配到任何簇中，因为任何距离核心点不够近的点将被视为噪声。
- en: Although DBSCAN is powerful and flexible, it may require experimentation to
    optimize the parameters to fit the data, such as the number of points comprising
    the core, or the allowed radius between points, which adds time complexity to
    the machine learning project. Of course, just because model-based methods are
    more sophisticated does not imply they are the best fit for every clustering project.
    As we will see throughout the remainder of this chapter, a simpler partition-based
    method can perform surprisingly well on a challenging real-world clustering task.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管DBSCAN强大且灵活，但可能需要实验来优化参数以适应数据，例如构成核心点的数量或点之间的允许半径，这增加了机器学习项目的时间复杂度。当然，仅仅因为基于模型的方法更复杂，并不意味着它们适合每个聚类项目。正如我们将在本章剩余部分看到的那样，一个简单的基于分区的方法在具有挑战性的实际聚类任务上可以表现得非常出色。
- en: Although mixture modeling and DBSCAN are not demonstrated in this chapter, there
    are R packages that can be used to apply these methods to your own data. The `mclust`
    package fits a model to mixtures of Gaussian distributions, and the `dbscan` package
    provides a fast implementation of the DBSCAN algorithm.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管混合模型和DBSCAN在本章中没有演示，但有一些R包可以用来将这些方法应用于您自己的数据。`mclust`包可以将模型拟合到高斯分布的混合，而`dbscan`包提供了DBSCAN算法的快速实现。
- en: The k-means clustering algorithm
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means聚类算法
- en: The **k-means algorithm** is perhaps the most often used clustering method and
    is an example of a partition-based clustering approach. Having been studied for
    several decades, it serves as the foundation for many more sophisticated clustering
    techniques. If you understand the simple principles it uses, you will have the
    knowledge needed to understand nearly any clustering algorithm in use today.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-means算法**可能是最常使用的聚类方法，并且是分区聚类方法的一个例子。经过几十年的研究，它成为了许多更复杂聚类技术的基础。如果你理解它使用的简单原则，你将拥有理解今天使用的几乎所有聚类算法所需的知识。'
- en: As k-means has evolved over time, there are many implementations of the algorithm.
    An early approach is described in *A k-means clustering algorithm, Hartigan, J.A.,
    Wong, M.A., Applied Statistics, 1979, Vol. 28, pp. 100-108*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 随着k-means算法随着时间的推移而发展，出现了许多算法的实现。一种早期的方法在*《k-means聚类算法，Hartigan, J.A., Wong,
    M.A., 应用统计学，1979，第28卷，第100-108页》*中进行了描述。
- en: 'Even though clustering methods have evolved since the inception of k-means,
    this is not to imply that k-means is obsolete. In fact, the method may be more
    popular now than ever. The following table lists some reasons why k-means is still
    used widely:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自k-means算法诞生以来聚类方法已经发展，但这并不意味着k-means已经过时。事实上，这个方法可能比以往任何时候都更受欢迎。以下表格列出了k-means仍然被广泛使用的一些原因：
- en: '| **Strengths** | **Weaknesses** |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | **缺点** |'
- en: '|'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Uses simple principles that can be explained in non-statistical terms
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可以非统计术语解释的简单原则
- en: Highly flexible and can be adapted with simple adjustments to address many of
    its shortcomings
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常灵活，可以通过简单的调整来应对许多其不足之处
- en: Performs well enough under many real-world use cases
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在许多实际应用场景下表现良好
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Not as sophisticated as more modern clustering algorithms
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不像更现代的聚类算法那样复杂
- en: Because it uses an element of random chance, it is not guaranteed to find the
    optimal set of clusters
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于它使用随机性的元素，不能保证找到最优的聚类集
- en: Requires a reasonable guess as to how many clusters naturally exist in the data
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要对数据中自然存在的聚类数量进行合理的猜测
- en: Not ideal for non-spherical clusters or clusters of widely varying density
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适合非球形聚类或密度差异很大的聚类
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: If the name k-means sounds familiar to you, you may be recalling the **k-nearest
    neighbors** (**k-NN**) algorithm presented in *Chapter 3*, *Lazy Learning – Classification
    Using Nearest Neighbors*. As you will soon see, k-means has more in common with
    k-NN than just the letter k.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉k-means这个名字，你可能是在回忆*第3章*中提出的**k近邻算法**（**k-NN**）。正如你很快就会看到的，k-means与k-NN的共同之处不仅仅在于字母k。
- en: The k-means algorithm assigns each of the *n* examples to one of the *k* clusters,
    where *k* is a number that has been determined ahead of time. The goal is to minimize
    the differences in feature values of examples within each cluster and maximize
    the differences between clusters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法将每个*n*个示例分配给*k*个聚类中的一个，其中*k*是一个事先确定的数字。目标是使每个聚类内示例的特征值差异最小化，并使聚类之间的差异最大化。
- en: Unless *k* and *n* are extremely small, it is not feasible to compute the optimal
    clusters across all possible combinations of examples. Instead, the algorithm
    uses a heuristic process that finds **locally optimal** solutions. Put simply,
    this means that it starts with an initial guess for the cluster assignments and
    then modifies the assignments slightly to see if the changes improve the homogeneity
    within the clusters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除非*k*和*n*非常小，否则无法计算所有可能的示例组合的最优聚类。相反，算法使用一种启发式过程来找到**局部最优**解。简单来说，这意味着它从一个初始的聚类分配猜测开始，然后稍微修改分配以查看这些变化是否改善了聚类内的同质性。
- en: We will cover the process in depth shortly, but the algorithm essentially involves
    two phases. First, it assigns examples to an initial set of *k* clusters. Then,
    it updates the assignments by adjusting the cluster boundaries according to the
    examples that currently fall into the cluster. The process of updating and assigning
    occurs several times until changes no longer improve the cluster fit. At this
    point, the process stops, and the clusters are finalized.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后深入探讨这个过程，但算法本质上涉及两个阶段。首先，它将示例分配给一组初始的*k*个聚类。然后，根据当前属于聚类的示例调整聚类边界来更新分配。这个过程会多次更新和分配，直到不再通过改变来改善聚类拟合。此时，过程停止，聚类被最终确定。
- en: Due to the heuristic nature of k-means, you may end up with somewhat different
    results by making only slight changes to the starting conditions. If the results
    vary dramatically, this could indicate a problem. For instance, the data may not
    have natural groupings, or the value of *k* has been poorly chosen. With this
    in mind, it’s a good idea to try a cluster analysis more than once to test the
    robustness of your findings.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于k-means的启发式性质，您可能只需对起始条件进行轻微的改变就会得到不同的结果。如果结果差异很大，这可能表明存在问题。例如，数据可能没有自然的分组，或者*k*的值选择不当。考虑到这一点，尝试多次进行聚类分析以测试您发现结果的稳健性是个好主意。
- en: To see how the process of assigning and updating works in practice, let’s revisit
    the case of the hypothetical data science conference. Though this is a simple
    example, it will illustrate the basics of how k-means operates under the hood.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解分配和更新过程在实际中的工作方式，让我们回顾一下假设的数据科学会议案例。虽然这是一个简单的例子，但它将说明k-means在底层是如何工作的。
- en: Using distance to assign and update clusters
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用距离分配和更新聚类
- en: As with k-NN, k-means treats feature values as coordinates in a multidimensional
    feature space. For the conference data, there are only two features, so we can
    represent the feature space as a two-dimensional scatterplot as depicted previously.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与k-NN一样，k-means将特征值视为多维特征空间中的坐标。对于会议数据，只有两个特征，因此我们可以将特征空间表示为之前描述的两个维度的散点图。
- en: The k-means algorithm begins by choosing *k* points in the feature space to
    serve as the cluster centers. These centers are the catalyst that spurs the remaining
    examples to fall into place. Often, the points are chosen by selecting *k* random
    examples from the training dataset. Because we hope to identify three clusters,
    using this method, *k = 3* points will be selected at random.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法首先在特征空间中选择*k*个点作为聚类中心。这些中心是推动剩余示例归位的催化剂。通常，这些点是通过从训练数据集中选择*k*个随机示例来选择的。因为我们希望识别三个聚类，所以使用这种方法，*k
    = 3*个点将被随机选择。
- en: 'These points are indicated by the star, triangle, and diamond in *Figure 9.5*:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些点在*图9.5*中由星号、三角形和菱形表示：
- en: '![Chart, scatter chart  Description automatically generated](img/B17290_09_05.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成的描述](img/B17290_09_05.png)'
- en: 'Figure 9.5: k-means clustering begins by selecting k random cluster centers'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：k-means聚类算法首先通过选择k个随机聚类中心开始
- en: It’s worth noting that although the three cluster centers in the preceding diagram
    happen to be widely spaced apart, this will not always necessarily be the case.
    Because the starting points are selected at random, the three centers could have
    just as easily been three adjacent points. Combined with the fact that the k-means
    algorithm is highly sensitive to the starting position of the cluster centers,
    a good or bad set of initial cluster centers may have a substantial impact on
    the final set of clusters.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管前面图中三个聚类中心恰好分布得很远，但这并不总是必然的情况。因为起始点是随机选择的，三个中心也可能只是三个相邻的点。结合k-means算法对聚类中心起始位置高度敏感的事实，一组好的或坏的初始聚类中心可能会对最终的聚类集产生重大影响。
- en: To address this problem, k-means can be modified to use different methods for
    choosing the initial centers. For example, one variant chooses random values occurring
    anywhere in the feature space rather than only selecting among values observed
    in the data. Another option is to skip this step altogether; by randomly assigning
    each example to a cluster, the algorithm can jump ahead immediately to the update
    phase. Each of these approaches adds a particular bias to the final set of clusters,
    which you may be able to use to improve your results.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，k-means可以被修改为使用不同的方法来选择初始中心。例如，一个变体选择在特征空间中任何地方出现的随机值，而不是仅从数据中观察到的值中选择。另一个选项是完全跳过这一步；通过随机将每个示例分配给一个聚类，算法可以立即跳到更新阶段。这些方法中的每一种都会给最终的聚类集添加特定的偏差，您可能可以利用这些偏差来改进您的结果。
- en: 'In 2007, an algorithm called **k-means++** was introduced, which proposes an
    alternative method for selecting the initial cluster centers. It purports to be
    an efficient way to get much closer to the optimal clustering solution while reducing
    the impact of random chance. For more information, see *k-means++: The advantages
    of careful seeding, Arthur, D, Vassilvitskii, S, Proceedings of the eighteenth
    annual ACM-SIAM symposium on discrete algorithms, 2007, pp. 1,027–1,035*.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在2007年，引入了一种名为**k-means++**的算法，它提出了一种选择初始簇中心的不同方法。它声称这是一种更有效的方法，可以在减少随机机会影响的同时，更接近最优聚类解决方案。更多信息，请参阅*《k-means++：谨慎播种的优势，Arthur,
    D, Vassilvitskii, S, 第十八届ACM-SIAM离散算法年度会议论文集，2007年，第1,027–1,035页》*。
- en: After choosing the initial cluster centers, the other examples are assigned
    to the cluster center that is nearest according to a distance function, which
    is used as a measure of similarity. You may recall that we used distance functions
    as similarity measures while learning about the k-NN supervised learning algorithm.
    Like k-NN, k-means traditionally uses Euclidean distance, but other distance functions
    can be used if desired.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择初始簇中心之后，其他示例根据距离函数分配到最近的簇中心，该距离函数用作相似性度量。你可能还记得，我们在学习k-NN监督学习算法时使用了距离函数作为相似性度量。像k-NN一样，k-means传统上使用欧几里得距离，但如果需要，也可以使用其他距离函数。
- en: Interestingly, any function that returns a numeric measure of similarity could
    be used instead of a traditional distance function. In fact, k-means could even
    be adapted to cluster images or text documents by using a function that measures
    the similarity of pairs of images or texts.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，任何返回相似性数值度量的函数都可以用来代替传统的距离函数。事实上，k-means甚至可以通过使用测量图像或文本对相似性的函数来适应聚类图像或文本文档。
- en: 'To apply the distance function, recall that if *n* indicates the number of
    features, the formula for Euclidean distance between example *x* and example *y*
    is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用距离函数，请记住，如果*n*表示特征的数量，那么示例*x*和示例*y*之间的欧几里得距离的公式如下：
- en: '![](img/B17290_09_001.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_09_001.png)'
- en: 'For instance, to compare a guest with five computer science publications and
    one math publication to a guest with zero computer science papers and two math
    papers, we could compute this in R as:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了比较一个有五个计算机科学出版物和一个数学出版物的访客与一个没有计算机科学论文但有两位数学论文的访客，我们可以在R中这样计算：
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the distance function in this way, we find the distance between each example
    and each cluster center. Each example is then assigned to the nearest cluster
    center.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方式的距离函数，我们可以找到每个示例与每个簇中心的距离。然后，每个示例被分配到最近的簇中心。
- en: Keep in mind that because we are using distance calculations, all the features
    need to be numeric, and the values should be normalized to a standard range ahead
    of time. The methods presented in *Chapter 3*, *Lazy Learning – Classification
    Using Nearest Neighbors*, will prove helpful for this task.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，因为我们使用距离计算，所以所有特征都需要是数值的，并且应该在事先将值归一化到标准范围内。第3章中提出的*《懒惰学习 – 使用最近邻进行分类》*方法将有助于这项任务。
- en: As shown in the following figure, the three cluster centers partition the examples
    into three partitions labeled *Cluster A*, *Cluster B*, and *Cluster C*. The dashed
    lines indicate the boundaries for the **Voronoi diagram** created by the cluster
    centers. The Voronoi diagram indicates the areas that are closer to one cluster
    center than any other; the vertex where all three boundaries meet is the maximal
    distance from all three cluster centers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下图所示，三个簇中心将示例划分为三个分区，分别标记为*Cluster A*、*Cluster B*和*Cluster C*。虚线表示由簇中心创建的**Voronoi图**的边界。Voronoi图表示比其他簇中心更接近一个簇中心的区域；所有三个边界相交的顶点是离所有三个簇中心的最大距离。
- en: 'Using these boundaries, we can easily see the regions claimed by each of the
    initial k-means seeds:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些边界，我们可以轻松地看到每个初始k-means种子的所声称的区域：
- en: '![Diagram  Description automatically generated](img/B17290_09_06.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](img/B17290_09_06.png)'
- en: 'Figure 9.6: The initial cluster centers create three groups of “nearest” points'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6：初始簇中心创建了三个“最近”点的三组
- en: 'Now that the initial assignment phase has been completed, the k-means algorithm
    proceeds to the update phase. The first step of updating the clusters involves
    shifting the initial centers to a new location, known as the **centroid**, which
    is calculated as the average position of the points currently assigned to that
    cluster. The following figure illustrates how as the cluster centers shift to
    the new centroids, the boundaries in the Voronoi diagram also shift, and a point
    that was once in *Cluster B* (indicated by an arrow) is added to *Cluster A*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在初始分配阶段已经完成，k-means算法进入更新阶段。更新簇的第一步是将初始中心移动到新的位置，称为**质心**，它是当前分配给该簇的点的平均位置。以下图示说明了随着簇中心移动到新的质心，Voronoi图中的边界也移动，一个曾经位于*簇B*（由箭头指示）的点被添加到*簇A*：
- en: '![Diagram  Description automatically generated](img/B17290_09_07.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图解描述自动生成](img/B17290_09_07.png)'
- en: 'Figure 9.7: The update phase shifts the cluster centers, which causes the reassignment
    of one point'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7：更新阶段移动簇中心，导致一个点的重新分配
- en: 'As a result of this reassignment, the k-means algorithm will continue through
    another update phase. After shifting the cluster centroids, updating the cluster
    boundaries, and reassigning points into new clusters (as indicated by arrows),
    the figure looks like this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这次重新分配，k-means算法将继续通过另一个更新阶段。在移动簇质心、更新簇边界并将点重新分配到新的簇（如箭头所示）之后，图看起来是这样的：
- en: '![Diagram  Description automatically generated](img/B17290_09_08.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图解描述自动生成](img/B17290_09_08.png)'
- en: 'Figure 9.8: After another update, two more points are reassigned to the nearest
    cluster center'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8：在另一次更新后，另外两个点被重新分配到最近的簇中心
- en: 'Because two more points were reassigned, another update must occur, which moves
    the centroids and updates the cluster boundaries. However, because these changes
    result in no reassignments, the k-means algorithm stops. The cluster assignments
    are now final:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于另外两个点被重新分配，必须进行另一次更新，这将移动质心并更新簇边界。然而，因为这些变化没有导致重新分配，k-means算法停止。簇分配现在是最终的：
- en: '![Chart  Description automatically generated with medium confidence](img/B17290_09_09.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成，置信度中等](img/B17290_09_09.png)'
- en: 'Figure 9.9: Clustering stops after the update phase results in no new cluster
    assignments'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：更新阶段没有导致新的簇分配后，聚类停止
- en: The final clusters can be reported in one of two ways. First, you might simply
    report the cluster assignments of A, B, or C for each example. Alternatively,
    you could report the coordinates of the cluster centroids after the final update.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的簇可以通过两种方式之一进行报告。首先，你可能只是简单地报告每个示例的A、B或C簇的分配。或者，你可以在最终更新后报告簇质心的坐标。
- en: Given either reporting method, you can compute the other; you can calculate
    the centroids using the coordinates of each cluster’s examples, or you can use
    the centroid coordinates to assign each example to its nearest cluster center.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是采用哪种报告方法，你都可以计算另一种方法；你可以使用每个簇示例的坐标来计算质心，或者你可以使用质心坐标来将每个示例分配到其最近的簇中心。
- en: Choosing the appropriate number of clusters
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的簇数量
- en: In the introduction to k-means, we learned that the algorithm is sensitive to
    the randomly chosen cluster centers. Indeed, if we had selected a different combination
    of three starting points in the previous example, we may have found clusters that
    split the data differently from what we had expected. Similarly, k-means is sensitive
    to the number of clusters; the choice requires a delicate balance. Setting *k*
    to be very large will improve the homogeneity of the clusters and, at the same
    time, it risks overfitting the data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-means的介绍中，我们了解到该算法对随机选择的簇中心很敏感。确实，如果我们之前在示例中选择了不同的三个起始点组合，我们可能会找到与我们预期不同的数据分割的簇。同样，k-means对簇的数量也很敏感；选择需要微妙的平衡。将*k*设置得非常大可以提高簇的同质性，同时它也冒着过度拟合数据的风险。
- en: Ideally, you will have *a priori* knowledge (a prior belief) about the true
    groupings and you can apply this information to choose the number of clusters.
    For instance, if you clustered movies, you might begin by setting *k* equal to
    the number of genres considered for the Academy Awards. In the data science conference
    seating problem that we worked through previously, *k* might reflect the number
    of academic fields of study that invitees belong to.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你将拥有关于真实分组的前置知识（即先验信念），并可以应用这些信息来选择簇的数量。例如，如果你对电影进行聚类，你可能首先将*k*设置为考虑的奥斯卡奖项类别数量。在我们之前解决的数据科学会议座位问题中，*k*可能反映了受邀者所属的学术研究领域数量。
- en: Sometimes, the number of clusters is dictated by business requirements or the
    motivation for the analysis. For example, the number of tables in the meeting
    hall might dictate how many groups of people should be created from the data science
    attendee list. Extending this idea to another business case, if the marketing
    department only has the resources to create three distinct advertising campaigns,
    it might make sense to set *k = 3* to assign all the potential customers to one
    of the three appeals.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，簇的数量是由业务需求或分析的动机决定的。例如，会议室中的桌子数量可能决定了从数据科学参会者名单中应该创建多少组人。将这个想法扩展到另一个业务案例，如果营销部门只有资源创建三个不同的广告活动，那么将*k
    = 3*分配所有潜在客户到三个吸引之一可能是有意义的。
- en: Without any prior knowledge, one rule of thumb suggests setting *k* equal to
    the square root of *(n / 2)*, where *n* is the number of examples in the dataset.
    However, this rule of thumb is likely to result in an unwieldy number of clusters
    for large datasets. Luckily, there are other quantitative methods that can assist
    in finding a suitable k-means cluster set.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有任何先验知识，一个经验法则建议将*k*设置为*(n / 2)*的平方根，其中*n*是数据集中示例的数量。然而，这个经验法则很可能会导致大型数据集簇的数量过多。幸运的是，还有其他定量方法可以帮助找到合适的k-means簇集。
- en: A technique known as the **elbow method** attempts to gauge how the homogeneity
    or heterogeneity within the clusters changes for various values of *k*. As illustrated
    in the following diagrams, the homogeneity within clusters is expected to increase
    as additional clusters are added; similarly, the heterogeneity within clusters
    should decrease with more clusters. Because you could continue to see improvements
    until each example is in its own cluster, the goal is not to maximize homogeneity
    or minimize heterogeneity endlessly, but rather to find *k* such that there are
    diminishing returns beyond that value. This value of *k* is known as the **elbow
    point** because it bends like the elbow joint of the human arm.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为**肘部方法**的技术试图评估簇内同质性和异质性与*k*的不同值如何变化。如图中所示，随着额外簇的增加，簇内的同质性预计会增加；同样，簇内的异质性应该随着簇的增加而减少。因为你可能会继续看到改进，直到每个示例都在自己的簇中，所以目标不是无限期地最大化同质性或最小化异质性，而是找到*k*，这样在该值之后就没有递减的回报。这个*k*值被称为**肘点**，因为它像人手臂的肘关节一样弯曲。
- en: '![Diagram  Description automatically generated with medium confidence](img/B17290_09_10.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图描述：自动使用中等置信度生成](img/B17290_09_10.png)'
- en: 'Figure 9.10: The elbow is the point at which increasing k results in relatively
    small improvements'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10：肘部是增加*k*导致相对较小改进的点
- en: There are numerous statistics for measuring homogeneity and heterogeneity within
    clusters that can be used with the elbow method (the information box that follows
    provides a citation for more detail). Still, in practice, it is not always feasible
    to iteratively test a large number of *k* values. This is in part because clustering
    large datasets can be fairly time-consuming; clustering the data repeatedly is
    even worse. Furthermore, applications requiring the exact optimal set of clusters
    are rare. In most clustering applications, it suffices to choose a *k* value based
    on convenience rather than the one that creates the most homogenous clusters.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多用于测量簇内同质性和异质性的统计数据，可以与肘部方法（以下信息框提供了更多细节的引用）一起使用。然而，在实践中，并不总是可行地迭代测试大量*k*值。这部分是因为聚类大型数据集可能相当耗时；重复聚类数据甚至更糟。此外，需要精确最优簇集集的应用很少。在大多数聚类应用中，基于便利性选择*k*值就足够了，而不是创建最同质簇的值。
- en: For a thorough review of the vast assortment of cluster performance measures,
    refer to *On Clustering Validation Techniques, Halkidi, M, Batistakis, Y, Vazirgiannis,
    M, Journal of Intelligent Information Systems, 2001, Vol. 17, pp. 107-145*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大量聚类性能指标的全面回顾，请参阅 *《聚类验证技术》，Halkidi, M, Batistakis, Y, Vazirgiannis, M, 智能信息系统杂志，2001年，第17卷，第107-145页*。
- en: The process of setting *k* itself can sometimes lead to interesting insights.
    By observing how the characteristics of the clusters change as *k* changes, one
    might infer where the data has naturally defined boundaries. Groups that are more
    tightly clustered will change very little, while less homogeneous groups will
    form and disband over time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 *k* 的过程本身有时可以带来有趣的洞察。通过观察随着 *k* 的变化聚类特征如何变化，人们可以推断数据自然定义的边界在哪里。更加紧密聚类的群体变化很小，而不够同质的群体则会在一段时间内形成和解散。
- en: In general, it may be wise to spend little time worrying about getting *k* exactly
    right. The next example will demonstrate how even a tiny bit of subject-matter
    knowledge borrowed from a Hollywood film can be used to set *k* such that actionable
    and interesting clusters are found. As clustering is unsupervised, the task is
    really about what you make of it; the value is in the insights you take away from
    the algorithm’s findings.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，花费少量时间去担心如何精确地确定 *k* 可能是明智的。接下来的例子将展示即使是来自好莱坞电影的一点点主题知识，也可以用来设定 *k*，从而找到可操作且有趣的聚类。由于聚类是无监督的，这项任务实际上取决于你如何利用它；价值在于你从算法的发现中获得的洞察。
- en: Finding teen market segments using k-means clustering
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k-means聚类寻找青少年市场细分
- en: Interacting with friends on a **social networking service** (**SNS**), such
    as Facebook, TikTok, and Instagram, has become a rite of passage for teenagers
    around the world. Having a relatively large amount of disposable income, these
    adolescents are a coveted demographic for businesses hoping to sell snacks, beverages,
    electronics, entertainment, and hygiene products.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与Facebook、TikTok和Instagram等**社交网络服务**（**SNS**）上的朋友互动，已成为全球青少年的一种仪式。这些青少年拥有相当可观的零花钱，因此他们成为了希望销售零食、饮料、电子产品、娱乐和卫生用品的企业所渴望的目标群体。
- en: The many millions of teenage consumers using such sites have attracted the attention
    of marketers struggling to find an edge in an increasingly competitive market.
    One way to gain this edge is to identify segments of teenagers who share similar
    tastes, so that clients can avoid targeting advertisements to teens with no interest
    in the product being sold. If it costs 10 dollars to display an advertisement
    to 1,000 website visitors—a measure of **cost per impression**—the advertising
    budget will stretch further if we are selective about who is targeted. For instance,
    an advertisement for sporting apparel should be targeted to clusters of individuals
    more likely to have an interest in sports.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些网站的数百万青少年消费者吸引了那些在日益竞争激烈的市场中寻找优势的营销人员的注意。获得这种优势的一种方式是识别具有相似口味的青少年群体，这样客户就可以避免向对所售产品不感兴趣的青少年投放广告。如果向1,000名网站访客展示一次广告的成本是10美元——这是一种**每印象成本**的衡量标准——如果我们对目标受众进行选择，广告预算将会更加充裕。例如，运动服装的广告应该针对更有可能对运动感兴趣的个体群体。
- en: Given the text of teenagers’ SNS posts, we can identify groups that share common
    interests such as sports, religion, or music. Clustering can automate the process
    of discovering the natural segments in this population. However, it will be up
    to us to decide whether the clusters are interesting and how to use them for advertising.
    Let’s try this process from start to finish.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析青少年的SNS帖子文本，我们可以识别出具有共同兴趣的群体，如体育、宗教或音乐。聚类可以自动化发现该人群自然段落的流程。然而，我们将决定这些聚类是否有趣以及如何用于广告。让我们从头到尾尝试这个过程。
- en: Step 1 – collecting data
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步 – 收集数据
- en: For this analysis, we will be using a dataset representing a random sample of
    30,000 US high school students who had profiles on a well-known SNS in 2006\.
    To protect the users’ anonymity, the SNS will remain unnamed. However, at the
    time the data was collected, the SNS was a popular web destination for US teenagers.
    Therefore, it is reasonable to assume that the profiles represent a wide cross-section
    of American adolescents in 2006.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次分析中，我们将使用一个数据集，它代表2006年在一家知名SNS上有资料的30,000名美国高中学生的随机样本。为了保护用户的匿名性，SNS将不会被命名。然而，在数据收集时，该SNS是美国青少年非常受欢迎的网站。因此，可以合理地假设这些资料代表了2006年美国青少年的广泛横截面。
- en: I compiled this dataset while conducting my own sociological research on teenage
    identities at the University of Notre Dame. If you use the data for research purposes,
    please cite this book chapter. The full dataset is available in the Packt Publishing
    GitHub repository for this book with the filename `snsdata.csv`. To follow along
    interactively, this chapter assumes you have saved this file to your R working
    directory.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我在圣母大学进行自己的青少年身份社会学研究时编制了这个数据集。如果你用于研究目的，请引用这本书的章节。完整的数据库可以在本书的Packt Publishing
    GitHub存储库中找到，文件名为`snsdata.csv`。为了互动式地跟随，本章假设你已经将此文件保存到你的R工作目录中。
- en: The data was sampled evenly across four high school graduation years (2006 through
    to 2009) representing the senior, junior, sophomore, and freshman classes at the
    time of data collection. Using an automated web crawler, the full text of the
    SNS profiles was downloaded, and each teen’s gender, age, and number of SNS friends
    were recorded.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在四个高中毕业年份（2006年至2009年）之间均匀采样，代表当时的数据收集时的大学一年级、二年级、三年级和四年级学生。使用自动网络爬虫下载了SNS个人资料的全文，并记录了每个青少年的性别、年龄和SNS朋友数量。
- en: 'A text-mining tool was used to divide the remaining SNS page content into words.
    From the top 500 words appearing across all pages, 36 words were chosen to represent
    five categories of interests: extracurricular activities, fashion, religion, romance,
    and antisocial behavior. The 36 words include terms such as *football*, *sexy*,
    *kissed*, *bible*, *shopping*, *death*, and *drugs*. The final dataset indicates,
    for each person, how many times each word appeared on the person’s SNS profile.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本挖掘工具将剩余的SNS页面内容划分为单词。从所有页面中出现的最顶部的500个单词中，选择了36个单词来代表五个兴趣类别：课外活动、时尚、宗教、浪漫和反社会行为。这36个单词包括诸如*足球*、*性感*、*亲吻*、*圣经*、*购物*、*死亡*和*毒品*等术语。最终的数据库表明，对于每个人，每个单词在个人的SNS资料中出现的次数。
- en: Step 2 – exploring and preparing the data
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 – 探索和准备数据
- en: 'We’ll use `read.csv()` to load the dataset and convert the character data into
    factor types:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`read.csv()`来加载数据集并将字符数据转换为因子类型：
- en: '[PRE2]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s also take a quick look at the specifics of the data. The first several
    lines of the `str()` output are as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也快速看一下数据的详细信息。`str()`输出的前几行如下所示：
- en: '[PRE3]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we had expected, the data includes 30,000 teenagers with four variables indicating
    personal characteristics and 36 words indicating interests.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预期的，数据包括30,000名青少年，其中四个变量表示个人特征，36个单词表示兴趣。
- en: Do you notice anything strange around the `gender` row? If you looked carefully,
    you may have noticed the `NA` value, which is out of place compared to the `1`
    and `2` values. The `NA` is R’s way of telling us that the record has a **missing
    value**—we do not know the person’s gender. Until now, we haven’t dealt with missing
    data, but it can be a significant problem for many types of analyses.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到`gender`行周围有什么奇怪的地方吗？如果你仔细观察，你可能已经注意到那个`NA`值，它与`1`和`2`值相比显得格格不入。`NA`是R告诉我们记录有一个**缺失值**的方式——我们不知道这个人的性别。到目前为止，我们还没有处理缺失数据，但它可能对许多类型的分析是一个重大问题。
- en: 'Let’s see how substantial this problem is. One option is to use the `table()`
    command, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个问题有多严重。一个选项是使用`table()`命令，如下所示：
- en: '[PRE5]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Although this tells us how many `F` and `M` values are present, the `table()`
    function excluded the `NA` values rather than treating them as a separate category.
    To include the `NA` values (if there are any), we simply need to add an additional
    parameter:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这告诉我们有多少个`F`和`M`值存在，但`table()`函数排除了`NA`值，而不是将其作为一个单独的分类处理。要包括`NA`值（如果有的话），我们只需要添加一个额外的参数：
- en: '[PRE7]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we see that 2,724 records (nine percent) have missing gender data. Interestingly,
    there are over four times as many females as males in the SNS data, suggesting
    that males are not as inclined to use this social media website as females.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到有2,724条记录（九%）有缺失的性别数据。有趣的是，SNS数据中女性的数量是男性的四倍多，这表明男性不像女性那样倾向于使用这个社交媒体网站。
- en: 'If you examine the other variables in the data frame, you will find that besides
    `gender`, only `age` has missing values. For numeric features, the default output
    for the `summary()` function includes the count of `NA` values:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查数据框中的其他变量，你会发现除了`gender`之外，只有`age`有缺失值。对于数值特征，`summary()`函数的默认输出包括`NA`值的计数：
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: A total of 5,086 records (17 percent) have missing ages. Also concerning is
    the fact that the minimum and maximum values seem to be unreasonable; it is unlikely
    that a three-year-old or a 106-year-old is attending high school. To ensure that
    these extreme values don’t cause problems for the analysis, we’ll need to clean
    them up before moving on.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有5,086条记录（17%）有缺失的年龄。另外，最小值和最大值似乎不合理；一个三岁的孩子或一个106岁的孩子上高中是不太可能的。为了确保这些极端值不会对分析造成问题，我们将在继续之前清理它们。
- en: 'A more plausible range of ages for high school students includes those who
    are at least 13 years old and not yet 20 years old. Any age value falling outside
    this range should be treated the same as missing data—we cannot trust the age
    provided. To recode the `age` variable, we can use the `ifelse()` function, assigning
    `teen$age` the original value of `teen$age` if the age is at least 13 and less
    than 20 years; otherwise, it will receive the value `NA`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 高中生可能更合理的年龄范围包括那些至少13岁且未满20岁的人。任何超出这个范围的年龄值应被视为缺失数据——我们无法相信提供的年龄。为了重新编码`age`变量，我们可以使用`ifelse()`函数，如果年龄至少为13岁且小于20岁，则将`teen$age`赋予原始值；否则，它将接收值`NA`：
- en: '[PRE11]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'By rechecking the `summary()` output, we see that the range now follows a distribution
    that looks much more like an actual high school:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新检查`summary()`输出，我们看到范围现在遵循的分布看起来更像是实际的高中：
- en: '[PRE12]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Unfortunately, now we’ve created an even larger missing data problem. We’ll
    need to find a way to deal with these values before continuing with our analysis.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，现在我们创建了一个更大的缺失数据问题。在我们继续分析之前，我们需要找到处理这些值的方法。
- en: Data preparation – dummy coding missing values
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备 - 虚拟编码缺失值
- en: An easy solution for handling missing values is to exclude any record with a
    missing value. However, if you think through the implications of this practice,
    you might think twice before doing so—just because it is easy does not mean it
    is a good idea! The problem with this approach is that even if the missingness
    is not extensive, you can easily exclude large portions of the data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失值的一个简单方法是不包含任何缺失值的记录。然而，如果你考虑这种做法的后果，你可能在做之前会三思而后行——仅仅因为它容易并不意味着这是一个好主意！这种方法的问题在于，即使缺失并不广泛，你也很容易排除大量数据。
- en: For example, suppose that in our data, the people with `NA` values for gender
    are completely different from those with missing age data. This would imply that
    by excluding those missing either gender or age, you would exclude *9% + 17% =
    26%* of the data, or over 7,500 records. And this is for missing data on only
    two variables! The larger the number of missing values present in a dataset, the
    more likely it is that any given record will be excluded. Fairly soon, you will
    be left with a tiny subset of data, or worse, the remaining records will be systematically
    different or non-representative of the full population.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设在我们的数据中，性别为`NA`的人与缺失年龄数据的人完全不同。这表明，通过排除缺失性别或年龄的人，你会排除*9% + 17% = 26%*的数据，或者超过7,500条记录。而且这仅仅是两个变量的缺失数据！数据集中缺失值的数量越多，任何给定记录被排除的可能性就越大。很快，你将只剩下一个非常小的数据子集，或者更糟糕的是，剩余的记录将系统性地不同或不能代表整个总体。
- en: An alternative solution for categorical data like gender is to treat a missing
    value as a separate category. For instance, rather than limiting to female and
    male, we can add an additional category for unknown gender. This allows us to
    utilize dummy coding, which was covered in *Chapter 3*, *Lazy Learning – Classification
    Using Nearest Neighbors*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于性别等分类数据，一个替代方案是将缺失值视为一个单独的分类。例如，我们不仅可以限制为女性和男性，还可以添加一个未知性别的额外分类。这使我们能够利用虚拟编码，这在*第3章*，*懒惰学习
    - 使用最近邻进行分类*中已有介绍。
- en: 'If you recall, dummy coding involves creating a separate binary (1 or 0) valued
    dummy variable for each level of a nominal feature except one, which is held out
    to serve as the reference group. The reason one category can be excluded is because
    its status can be inferred from the other categories. For instance, if someone
    is not female and not unknown gender, they must be male. Therefore, in this case,
    we need to only create dummy variables for female and unknown gender:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得，虚拟编码涉及为每个名义特征的每个级别创建一个单独的二进制（1或0）值虚拟变量，除了一个作为参考组保留的外。一个类别可以排除的原因是，其状态可以从其他类别中推断出来。例如，如果某人既不是女性也不是未知性别，那么他们一定是男性。因此，在这种情况下，我们只需要为女性和未知性别创建虚拟变量：
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you might expect, the `is.na()` function tests whether the gender is equal
    to `NA`. Therefore, the first statement assigns `teens$female` the value `1` if
    the gender is equal to `F` and the gender is not equal to `NA`; otherwise, it
    assigns the value `0`. In the second statement, if `is.na()` returns `TRUE`, meaning
    the gender is missing, then the `teens$no_gender` variable is assigned `1`; otherwise,
    it is assigned the value `0`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所预期，`is.na()` 函数测试性别是否等于 `NA`。因此，第一个语句在性别等于 `F` 且性别不等于 `NA` 时将 `teens$female`
    赋值为 `1`；否则，它赋值为 `0`。在第二个语句中，如果 `is.na()` 返回 `TRUE`，意味着性别缺失，那么 `teens$no_gender`
    变量被赋值为 `1`；否则，它被赋值为 `0`。
- en: 'To confirm that we did the work correctly, let’s compare our constructed dummy
    variables to the original `gender` variable:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认我们工作正确，让我们将我们构建的虚拟变量与原始的 `gender` 变量进行比较：
- en: '[PRE15]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The number of `1` values for `teens$female` and `teens$no_gender` matches the
    number of `F` and `NA` values respectively, so the coding has been performed correctly.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `teens$female` 和 `teens$no_gender` 的 `1` 值数量与 `F` 和 `NA` 值的数量相匹配，所以编码已经被正确执行。
- en: Data preparation – imputing the missing values
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备 – 填充缺失值
- en: Next, let’s eliminate the 5,523 missing ages. As `age` is a numeric feature,
    it doesn’t make sense to create an additional category for unknown values—where
    would you rank “unknown” relative to the other ages? Instead, we’ll use a different
    strategy known as **imputation**, which involves filling in the missing data with
    a guess as to the true value.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们消除5,523个缺失的年龄。由于 `age` 是一个数值特征，为未知值创建一个额外的类别是没有意义的——你将如何将“未知”与其他年龄进行比较？相反，我们将使用一种称为**插补**的不同策略，它涉及用对真实值的猜测来填充缺失数据。
- en: Can you think of a way we might be able to use the SNS data to make an informed
    guess about a teenager’s age? If you are thinking of using the graduation year,
    you’ve got the right idea. Most people in a graduation cohort were born within
    a single calendar year. If we can identify the typical age for each cohort, then
    we will have a reasonable approximation of the age of a student in that graduation
    year.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想到一种方法，我们可以利用SNS数据来对青少年的年龄做出有根据的猜测吗？如果你想到了使用毕业年份，你的想法是对的。在一个毕业班级中，大多数人都是在同一年出生的。如果我们能确定每个班级的典型年龄，那么我们就能对那个毕业年份的学生年龄有一个合理的估计。
- en: 'One way to find a typical value is by calculating the average, or mean, value.
    If we try to apply the `mean()` function as we have done for previous analyses,
    there’s a problem:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 找到一个典型值的一种方法是通过计算平均值或均值。如果我们尝试像之前分析那样应用 `mean()` 函数，会出现问题：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The issue is that the mean value is undefined for a vector containing missing
    data. As our age data contains missing values, `mean(teens$age)` returns a missing
    value. We can correct this by adding an additional `na.rm` parameter to remove
    the missing values before calculating the mean:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，对于包含缺失数据的向量，平均值是未定义的。由于我们的年龄数据包含缺失值，`mean(teens$age)` 返回一个缺失值。我们可以通过在计算平均值之前添加一个额外的
    `na.rm` 参数来删除缺失值来纠正这个问题：
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This reveals that the average student in our data is about 17 years old. This
    only gets us part of the way there; we actually need the average age for each
    graduation year. You might first attempt to calculate the mean four times, but
    one of the benefits of R is that there’s usually a way to avoid repeating oneself.
    In this case, the `aggregate()` function is the tool for the job. It computes
    statistics for subgroups of data. Here, it calculates the mean age by graduation
    year after removing the `NA` values:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们数据中的平均学生年龄大约是17岁。这只能让我们走了一半的路；我们实际上需要每个毕业年份的平均年龄。你可能会首先尝试计算四次平均值，但R的一个好处通常是有一个避免重复的方法。在这种情况下，`aggregate()`
    函数就是这项工作的工具。它为数据的子组计算统计数据。在这里，它通过删除 `NA` 值来计算按毕业年份的平均年龄：
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `aggregate()` output is in a data frame. This would require extra work to
    merge back into our original data. As an alternative, we can use the `ave()` function,
    which returns a vector with the means of each group repeated such that the resulting
    vector is the same length as the original vector. Where `aggregate()` returns
    one average age for each graduation year (a total of four values), the `ave()`
    function returns a value for all 30,000 teenagers reflecting the average age of
    students in that student’s graduation year (the same four values are repeated
    to reach a total of 30,000 values).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregate()`的输出是一个数据框。这需要额外的工作才能将其合并回我们的原始数据。作为替代方案，我们可以使用`ave()`函数，该函数返回一个向量，其中每个组的平均值被重复，使得结果向量与原始向量长度相同。当`aggregate()`为每个毕业年份返回一个平均年龄（总共四个值）时，`ave()`函数为所有30,000名青少年返回一个值，反映该学生毕业年份的学生平均年龄（相同的四个值被重复以达到总共30,000个值）。'
- en: 'When using the `ave()` function, the first parameter is the numeric vector
    for which the group averages are to be computed, the second parameter is the categorical
    vector supplying the group assignments, and the `FUN` parameter is the function
    to be applied to the numeric vector. In our case, we need to define a new function
    that computes the mean with the `NA` values removed. The full command is as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`ave()`函数时，第一个参数是要计算组平均值的数值向量，第二个参数是提供组分配的类别向量，而`FUN`参数是要应用于数值向量的函数。在我们的情况下，我们需要定义一个新的函数，该函数在计算平均值时移除`NA`值。完整的命令如下：
- en: '[PRE27]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To impute these means onto the missing values, we need one more `ifelse()`
    call to use the `ave_age` value only if the original age value was `NA`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些平均值填充到缺失值中，我们需要再调用一次`ifelse()`函数，仅当原始年龄值为`NA`时才使用`ave_age`值：
- en: '[PRE28]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `summary()` results show that the missing values have now been eliminated:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary()`的结果显示，缺失值现在已经消除：'
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: With the data ready for analysis, we are ready to dive into the interesting
    part of this project. Let’s see if our efforts have paid off.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析准备就绪后，我们就准备好深入这个项目的有趣部分了。让我们看看我们的努力是否得到了回报。
- en: Step 3 – training a model on the data
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 在数据上训练模型
- en: To cluster the teenagers into marketing segments, we’ll use an implementation
    of k-means in the `stats` package, which should be included in your R installation
    by default. Although there is no shortage of more sophisticated k-means functions
    available in other R packages, the `kmeans()` function in the default `stats`
    package is widely used and provides a simple yet powerful implementation of the
    algorithm.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将青少年聚类到营销细分市场，我们将使用`stats`包中的k-means实现，这应该默认包含在您的R安装中。尽管其他R包中提供了许多更复杂的k-means函数，但默认`stats`包中的`kmeans()`函数被广泛使用，并提供了简单而强大的算法实现。
- en: '![Text  Description automatically generated](img/B17290_09_11.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![文本描述自动生成](img/B17290_09_11.png)'
- en: 'Figure 9.11: K-means clustering syntax'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11：K-means聚类语法
- en: The `kmeans()` function requires a data frame or matrix containing only numeric
    data and a parameter specifying *k*, the desired number of clusters. If you have
    these two things ready, the actual process of building the model is simple. The
    trouble is that choosing the right combination of data and clusters can be a bit
    of an art; sometimes a great deal of trial and error is involved.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`kmeans()`函数需要一个只包含数值数据的数据框或矩阵，以及指定*k*，即所需聚类数量的参数。如果您准备好了这两件事，构建模型的实际过程就很简单了。麻烦的是，选择正确的数据和聚类组合可能有点像艺术；有时需要大量的尝试和错误。'
- en: We’ll start our cluster analysis by considering only the 36 features that measure
    the number of times various interest-based keywords appeared in the text of the
    teenagers’ social media profiles. In other words, we will not cluster based on
    age, graduation year, gender, or number of friends. Of course, we *could* use
    these four features if desired, but *choose* not to, since any clusters built
    upon them would be less insightful than those built upon interests. This is primarily
    because age and gender are already de facto clusters whereas the interest-based
    clusters are yet to be discovered in our data. Secondarily, what will be more
    interesting later is to see whether the interest clusters are associated with
    the gender and popularity features held out from the clustering process. If the
    interest-based clusters are predictive of these individual characteristics, this
    provides evidence that the clusters may be useful.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始我们的聚类分析，仅考虑36个特征，这些特征衡量了各种基于兴趣的关键词在青少年社交媒体个人资料文本中出现的次数。换句话说，我们不会基于年龄、毕业年份、性别或朋友数量进行聚类。当然，如果我们愿意，我们可以使用这四个特征，但我们选择不这样做，因为基于这些特征建立的任何聚类都比基于兴趣的聚类缺乏洞察力。这主要是因为年龄和性别已经是事实上的聚类，而基于兴趣的聚类在我们的数据中尚未被发现。其次，稍后更感兴趣的是看看兴趣聚类是否与聚类过程中保留的性别和受欢迎程度特征相关。如果基于兴趣的聚类可以预测这些个体特征，这提供了证据表明聚类可能是有用的。
- en: 'To avoid the chance of accidentally including the other features, let’s make
    a data frame called `interests`, by subsetting the data frame to include only
    the 36 keyword columns:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免意外包含其他特征，让我们创建一个名为 `interests` 的数据框，通过子集化数据框仅包含36个关键词列：
- en: '[PRE31]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: If you recall from *Chapter 3*, *Lazy Learning – Classification Using Nearest
    Neighbors*, a common practice employed prior to any analysis using distance calculations
    is to normalize or z-score-standardize the features such that each utilizes the
    same range. By doing so, you can avoid a problem in which some features dominate
    solely because they have a larger range of values than the others.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得 *第3章*，*懒惰学习 – 使用最近邻进行分类*，在执行任何使用距离计算的分析的任何分析之前，一个常见的做法是对特征进行归一化或 z 分数标准化，以便每个特征都利用相同的范围。通过这样做，你可以避免一个问题，即某些特征仅仅因为它们的值范围比其他特征大而主导。
- en: The process of z-score standardization rescales features such that they have
    a mean of zero and a standard deviation of one. This transformation changes the
    interpretation of the data in a way that may be useful here. Specifically, if
    someone mentions basketball three times on their profile, without additional information,
    we have no idea whether this implies they like basketball more or less than their
    peers. On the other hand, if the z-score is three, we know that they mentioned
    basketball many more times than the average teenager.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: z 分数标准化的过程重新调整特征，使它们具有均值为零和标准差为一。这种转换以可能在这里有用的方式改变了数据的解释。具体来说，如果有人在他们的个人资料中提到篮球三次，没有其他信息，我们无法知道这是否意味着他们比同龄人更喜欢篮球或更不喜欢篮球。另一方面，如果
    z 分数是三，我们知道他们比平均青少年提到了篮球许多次。
- en: 'To apply z-score standardization to the `interests` data frame, we can use
    the `scale()` function with `lapply()`. Since `lapply()` returns a list object,
    it must be coerced back to data frame form using the `as.data.frame()` function,
    as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 z 分数标准化应用于 `interests` 数据框，我们可以使用 `scale()` 函数结合 `lapply()`。由于 `lapply()`
    返回一个列表对象，必须使用 `as.data.frame()` 函数将其转换回数据框形式，如下所示：
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To confirm that the transformation worked correctly, we can compare the summary
    statistics of the `basketball` column in the old and new `interests` data:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认转换是否正确，我们可以比较旧 `interests` 数据中 `basketball` 列的摘要统计：
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As expected, the `interests_z` dataset transformed the basketball feature to
    have a mean of zero and a range that spans above and below zero. Now, a value
    less than zero can be interpreted as a person having fewer-than-average mentions
    of basketball in their profile. A value greater than zero implies that the person
    mentioned basketball more frequently than the average.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，`interests_z` 数据集将篮球特征转换成了均值为零，范围跨越零上和零下的值。现在，一个小于零的值可以解释为一个人在其个人资料中篮球提及次数少于平均水平。一个大于零的值则意味着这个人比平均水平更频繁地提及篮球。
- en: Our last decision involves deciding how many clusters to use for segmenting
    the data. If we use too many clusters, we may find them too specific to be useful;
    conversely, choosing too few may result in heterogeneous groupings. You should
    feel comfortable experimenting with the value of *k*. If you don’t like the result,
    you can easily try another value and start over.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后的决定是决定使用多少个簇来分割数据。如果我们使用太多的簇，我们可能会发现它们过于具体而无法使用；相反，选择太少的簇可能会导致异质分组。你应该对实验*k*的值感到舒适。如果你不喜欢结果，你可以轻松尝试另一个值并重新开始。
- en: Choosing the number of clusters is easier if you are familiar with the analysis
    population. Having a hunch about the true number of natural groupings can save
    some trial and error.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉分析人群，选择簇的数量会更容易。对真实自然分组数量的直觉猜测可以节省一些尝试和错误。
- en: 'To help choose the number of clusters in the data, I’ll defer to one of my
    favorite films, *The Breakfast Club*, a coming-of-age comedy released in 1985
    and directed by John Hughes. The teenage characters in this movie are self-described
    in terms of five identities:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助选择数据中的簇数量，我将参考我最喜欢的电影之一，《早餐俱乐部》，这是一部1985年上映的青春喜剧，由约翰·休斯执导。这部电影中的青少年角色根据以下五个身份进行自我描述：
- en: A *brain* – also commonly known as a “nerd” or “geek”
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*大脑* - 也常被称为“书呆子”或“极客”
- en: An *athlete* – sometimes also known as a “jock” or “prep”
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*运动员* - 有时也被称为“运动健将”或“预备役”
- en: A *basket case* – slang terminology for an anxious or neurotic individual, and
    depicted in the film as an anti-social outcast
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*篮子案* - 指的是焦虑或神经质的人的俚语术语，在电影中被描绘为一个反社会的局外人
- en: A *princess* – portrayed as a popular, affluent, and stereotypically feminine
    girl
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*公主* - 描绘为受欢迎、富有且具有刻板女性形象的女孩
- en: A *criminal* – represents the traditional “burnout” identity described in sociological
    research as engaging in rebellious anti-school and anti-authority behaviors
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*罪犯* - 代表社会学研究中描述的传统“燃尽”身份，参与反学校和反权威的叛逆行为
- en: Even though the movie depicts five specific identity groups, they have been
    described throughout popular teenage fiction for many years, and although the
    stereotypes have evolved over time, American teenagers are likely to understand
    them intuitively. Thus, five seems like a reasonable starting point for *k*, though
    admittedly, it is unlikely to capture the full spectrum of high-school identities.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这部电影描绘了五个具体的身份群体，但它们已经在多年的流行青少年小说中被描述，尽管随着时间的推移，这些刻板印象已经发生了变化，但美国青少年可能仍然会本能地理解它们。因此，五个似乎是一个合理的起始点来选择*k*，尽管诚然，它不太可能捕捉到高中身份的全貌。
- en: 'To use the k-means algorithm to divide the teenagers’ interest data into five
    clusters, we use the `kmeans()` function on the `interests` data frame. Note that
    because k-means utilizes random starting points, the `set.seed()` function is
    used to ensure that the results match the output in the examples that follow.
    If you recall from previous chapters, this command initializes R’s random number
    generator to a specific sequence. In the absence of this statement, the results
    may vary each time the k-means algorithm is run. Running the k-means clustering
    process as follows creates a list named `teen_clusters`, which stores the properties
    of each of the five clusters:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用k-means算法将青少年的兴趣数据划分为五个簇，我们使用`kmeans()`函数对`interests`数据框进行操作。请注意，由于k-means使用随机起始点，因此使用`set.seed()`函数以确保结果与以下示例中的输出相匹配。如果你还记得前面的章节，此命令初始化R的随机数生成器到一个特定的序列。如果没有这个语句，每次运行k-means算法时结果可能会变化。按照以下方式运行k-means聚类过程将创建一个名为`teen_clusters`的列表，该列表存储了五个簇的属性：
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Let’s dig in and see how well the algorithm has divided the teenagers’ interest
    data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一下，看看算法如何将青少年的兴趣数据进行了划分。
- en: If you find that your results differ from those shown in the sections that follow,
    ensure that the `set.seed(2345)` command is run immediately prior to the `kmeans()`
    function. Additionally, because the behavior of R’s random number generator changed
    with R version 3.6, your results may also vary slightly from those shown here
    if you are using an older version of R.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现你的结果与以下章节中显示的结果不同，请确保在运行`kmeans()`函数之前立即运行`set.seed(2345)`命令。此外，由于R的随机数生成器的行为随着R版本3.6而改变，如果你使用的是较旧的R版本，你的结果也可能与这里显示的结果略有不同。
- en: Step 4 – evaluating model performance
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 评估模型性能
- en: Evaluating clustering results can be somewhat subjective. Ultimately, the success
    or failure of the model hinges on whether the clusters are useful for their intended
    purpose. As the goal of this analysis was to identify clusters of teenagers with
    similar interests for marketing purposes, we will largely measure our success
    in qualitative terms. For other clustering applications, more quantitative measures
    of success may be needed.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 评估聚类结果可能具有一定的主观性。最终，模型的成功或失败取决于聚类是否适用于其预期目的。由于本分析的目标是识别具有相似兴趣的青少年聚类以用于营销目的，我们将主要从定性角度衡量我们的成功。对于其他聚类应用，可能需要更多定量成功的衡量标准。
- en: One of the most basic ways to evaluate the utility of a set of clusters is to
    examine the number of examples falling in each of the groups. If some groups are
    too large or too small, then they are less likely to be very useful.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一组聚类的有用性的最基本方法之一是检查每个组中落下的示例数量。如果某些组太大或太小，那么它们不太可能非常有用。
- en: 'To obtain the size of the `kmeans()` clusters, simply examine the `teen_clusters$size`
    component as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取`kmeans()`聚类的尺寸，只需检查`teen_clusters$size`组件，如下所示：
- en: '[PRE38]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here we see the five clusters we requested. The smallest cluster has 601 teenagers
    (2 percent) while the largest has 21,599 (72 percent). Although the large gap
    between the number of people in the largest and smallest clusters is slightly
    concerning, without examining these groups more carefully, we will not know whether
    this indicates a problem. It may be the case that the clusters’ size disparity
    indicates something real, such as a big group of teenagers who share similar interests,
    or it may be a random fluke caused by the initial k-means cluster centers. We’ll
    know more as we start to look at each cluster’s characteristics.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到我们请求的五个聚类。最小的聚类有601名青少年（2%），而最大的有21,599名（72%）。尽管最大和最小聚类人数之间的差距略令人担忧，但如果不仔细检查这些组，我们不会知道这是否表明了问题。可能的情况是，聚类的尺寸差异表明了某些真实情况，例如一个拥有相似兴趣的大青少年群体，或者这可能是由初始k-means聚类中心引起的随机巧合。随着我们开始查看每个聚类的特征，我们将了解更多信息。
- en: Sometimes, k-means may find extremely small clusters—occasionally as small as
    a single point. This can happen if one of the initial cluster centers happen to
    fall on outliers far from the rest of the data. It is not always clear whether
    to treat such small clusters as a true finding that represents a cluster of extreme
    cases, or a problem caused by random chance. If you encounter this issue, it may
    be worth re-running the k-means algorithm with a different random seed to see
    whether the small cluster is robust to different starting points.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，k-means可能会找到极小的聚类——有时小到只有一个点。这可能发生在初始聚类中心之一恰好落在远离其他数据的异常值上。是否将此类小型聚类视为代表极端案例的真正发现，还是随机机会造成的问题，并不总是很清楚。如果你遇到这个问题，可能值得用不同的随机种子重新运行k-means算法，看看小型聚类是否对不同的起始点具有鲁棒性。
- en: 'For a more in-depth look at the clusters, we can examine the coordinates of
    the cluster centroids using the `teen_clusters$centers` component, which is as
    follows for the first four interests:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 要更深入地了解聚类，我们可以检查聚类质心的坐标，使用`teen_clusters$centers`组件，以下是对前四个兴趣的说明：
- en: '[PRE40]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The rows of the output (labeled `1` to `5`) refer to the five clusters, while
    the numbers across each row indicate the cluster’s average value for the interest
    listed at the top of the column. Because the values are z-score-standardized,
    positive values are above the overall mean level for all teenagers and negative
    values are below the overall mean.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的行（标记为`1`到`5`）指的是五个聚类，而每行的数字表示该聚类对列顶部列出的兴趣的平均值。由于这些值是z分数标准化，正值表示所有青少年整体平均水平的上方，而负值表示整体平均水平的下方。
- en: For example, the fourth row has the highest value in the `basketball` column,
    which means that cluster `4` has the highest average interest in basketball among
    all the clusters.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，第四行在“篮球”列中具有最高值，这意味着在所有聚类中，聚类`4`对篮球的平均兴趣最高。
- en: 'By examining whether clusters fall above or below the mean level for each interest
    category, we can discover patterns that distinguish the clusters from one another.
    In practice, this involves printing the cluster centers and searching through
    them for any patterns or extreme values, much like a word search puzzle but with
    numbers. The following annotated screenshot shows a highlighted pattern for each
    of the five clusters, for 18 of the 36 teenager interests:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查集群是否在每个兴趣类别的平均水平之上或之下，我们可以发现区分集群之间的模式。在实践中，这涉及到打印集群中心，并搜索它们以寻找任何模式或极端值，就像一个数字搜索谜题，但使用的是数字。以下标注的屏幕截图显示了五个集群中的每个集群的突出模式，针对36个青少年兴趣中的18个：
- en: '![Table  Description automatically generated](img/B17290_09_12.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![表格 描述自动生成](img/B17290_09_12.png)'
- en: 'Figure 9.12: To distinguish clusters, it can be helpful to highlight patterns
    in the coordinates of their centroids'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12：为了区分集群，突出其质心的坐标中的模式可能很有帮助
- en: Given this snapshot of the interest data, we can already infer some characteristics
    of the clusters. Cluster four is substantially above the mean interest level on
    nearly all the sports, which suggests that this may be a group of *athletes* per
    *The* *Breakfast Club* stereotype. Cluster three includes the most mentions of
    cheerleading, dancing, and the word “hot.” Are these the so-called princesses?
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这个兴趣数据的快照，我们已能推断出一些集群的特征。集群四在几乎所有运动项目上的兴趣水平都显著高于平均水平，这表明这可能是一个*运动员*群体，按照*《早餐俱乐部》*的刻板印象。集群三包括最多的拉拉队、舞蹈和“热”这个词的提及。这些是所谓的公主吗？
- en: By continuing to examine the clusters in this way, it is possible to construct
    a table listing the dominant interests of each of the groups. In the following
    table, each cluster is shown with the features that most distinguish it from the
    other clusters, and *The* *Breakfast Club* identity that seems to most accurately
    capture the group’s characteristics.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 通过继续以这种方式检查这些集群，可以构建一个表格，列出每个群体的主导兴趣。在下面的表格中，每个集群都展示了使其与其他集群最不同的特征，以及似乎最能准确捕捉群体特征的*《早餐俱乐部》*身份。
- en: 'Interestingly, cluster five is distinguished by the fact that it is unexceptional:
    its members had lower-than-average levels of interest in every measured activity.
    It is also the single largest group in terms of the number of members. How can
    we reconcile these apparent contradictions? One potential explanation is that
    these users created a profile on the website but never posted any interests.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，集群五之所以与众不同，是因为它并不出众：其成员在所有测量的活动中兴趣水平都低于平均水平。它也是成员数量最多的单个最大群体。我们如何调和这些明显的矛盾？一个可能的解释是，这些用户在网站上创建了一个个人资料，但从未发布过任何兴趣。
- en: '![Diagram, table  Description automatically generated](img/B17290_09_13.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图表，表格 描述自动生成](img/B17290_09_13.png)'
- en: 'Figure 9.13: A table can be used to list important dimensions of each cluster'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13：可以使用表格列出每个集群的重要维度
- en: When sharing the results of a segmentation analysis with stakeholders, it is
    often helpful to apply memorable and informative labels known as **personas**,
    which simplify and capture the essence of the groups, such as *The* *Breakfast
    Club* typology applied here. The risk in adding such labels is that they can obscure
    the groups’ nuances and possibly even offend the group members if negative stereotypes
    are used. For wider dissemination, provocative labels like “Criminals” and “Princesses”
    might be replaced by more neutral terminology like “Edgy Adolescents” and “Trendy
    Teenagers.” Additionally, because even relatively harmless labels can bias our
    thinking, important patterns can be missed if labels are understood as the whole
    truth rather than a simplification of complexity.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当与利益相关者分享分割分析的结果时，应用易于记忆且信息丰富的标签，即所谓的**角色**，通常很有帮助，这些标签简化并捕捉了群体的本质，例如在此处应用的*《早餐俱乐部》*类型。添加此类标签的风险是，它们可能会掩盖群体的细微差别，甚至如果使用负面刻板印象，可能会冒犯群体成员。为了更广泛的传播，像“罪犯”和“公主”这样的挑衅性标签可能被更中性的术语如“叛逆青少年”和“时尚青少年”所取代。此外，因为即使是相对无害的标签也可能导致我们的思维产生偏见，如果标签被理解为整个真相而不是复杂性的简化，我们可能会错过重要的模式。
- en: Given memorable labels and a table as depicted in *Figure 9.13*, a marketing
    executive would have a clear mental picture of five types of teenage visitors
    to the social networking website. Based on these personas, the executive could
    sell targeted advertising impressions to businesses with products relevant to
    one or more of the clusters. In the next section, we will see how the cluster
    labels can be applied back to the original population for such uses.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 给出如图9.13所示的难忘标签和表格，营销主管会对社交网站上的五种青少年访问者类型有一个清晰的思维图像。基于这些人物角色，主管可以向与一个或多个聚类相关的产品相关的企业销售定向广告印象。在下一节中，我们将看到如何将聚类标签应用于原始人口以实现此类用途。
- en: It is possible to visualize the results of a cluster analysis using techniques
    that flatten the multidimensional feature data into two dimensions, then color
    the points according to cluster assignment. The `fviz_cluster()` function in the
    `factoextra` package allows such visualizations to be constructed quite easily.
    If this is of interest to you, load the package and try the command `fviz_cluster(teen_clusters,
    interests_z, geom = "point")` to see such a visualization for the teenage SNS
    clusters. Although the visual is of limited use for the SNS example due to the
    large number of overlapping points, sometimes, it can be a helpful tool for presentation
    purposes. To better understand how to create and understand these plots, see *Chapter
    15*, *Making Use of Big Data*.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用将多维特征数据展平为二维的技术来可视化聚类分析的结果，然后根据聚类分配给点着色。`factoextra` 包中的 `fviz_cluster()`
    函数允许轻松构建此类可视化。如果您对此感兴趣，请加载该包并尝试以下命令以查看青少年SNS聚类的可视化：`fviz_cluster(teen_clusters,
    interests_z, geom = "point")`。尽管由于重叠点数量众多，这种视觉在SNS示例中用途有限，但有时它可以是演示目的的有用工具。为了更好地理解如何创建和理解这些图表，请参阅*第15章*，*利用大数据*。
- en: Step 5 – improving model performance
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步 – 提高模型性能
- en: Because clustering creates new information, the performance of a clustering
    algorithm depends at least somewhat on both the quality of the clusters themselves
    and what is done with that information. In the preceding section, we demonstrated
    that the five clusters provided useful and novel insights into the interests of
    teenagers. By that measure, the algorithm appears to be performing quite well.
    Therefore, we can now focus our effort on turning these insights into action.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 由于聚类创建了新的信息，聚类算法的性能至少在一定程度上取决于聚类本身的质量以及如何使用这些信息。在前一节中，我们展示了五个聚类为青少年的兴趣提供了有用且新颖的见解。从这个角度来看，算法似乎表现相当好。因此，我们现在可以将精力集中在将这些见解转化为行动上。
- en: 'We’ll begin by applying the clusters back to the full dataset. The `teen_clusters`
    object created by the `kmeans()` function includes a component named `cluster`,
    which contains the cluster assignments for all 30,000 individuals in the sample.
    We can add this as a column to the `teens` data frame with the following command:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将聚类应用于完整的数据集。由 `kmeans()` 函数创建的 `teen_clusters` 对象包含一个名为 `cluster` 的组件，其中包含样本中所有30,000个个体的聚类分配。我们可以使用以下命令将其添加到
    `teens` 数据框中：
- en: '[PRE42]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Given this new data, we can start to examine how the cluster assignment relates
    to individual characteristics. For example, here’s the personal information for
    the first five teenagers in the SNS data:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些新数据，我们可以开始检查聚类分配与个人特征之间的关系。例如，以下是SNS数据中前五个青少年的个人信息：
- en: '[PRE43]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Using the `aggregate()` function, we can also look at the demographic characteristics
    of the clusters. The mean age does not vary much by cluster, which is not too
    surprising, as teen identities are often set well before high school. This is
    depicted as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `aggregate()` 函数，我们还可以查看聚类的人口统计特征。平均年龄在各个聚类之间变化不大，这并不太令人惊讶，因为青少年的身份通常在高中之前就已经确立。这如下所示：
- en: '[PRE45]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'On the other hand, there are some substantial differences in the proportion
    of females by cluster. This is a very interesting finding, as we didn’t use gender
    data to create the clusters, yet the clusters are still predictive of gender:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，各个聚类中女性比例的差异相当显著。这是一个非常有趣的发现，因为我们没有使用性别数据来创建聚类，但聚类仍然可以预测性别：
- en: '[PRE47]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Recall that overall, about 74 percent of the SNS users are female. Cluster three,
    the so-called *princesses*, is nearly 89 percent female, while clusters four and
    five are only about 70 percent female. These disparities imply that there are
    differences in the interests that teenage boys and girls discuss on their social
    networking pages.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，总体而言，大约74%的社交网络用户是女性。第三组，所谓的“公主”，女性比例高达89%，而第四组和第五组女性比例仅为大约70%。这些差异表明，青少年男孩和女孩在社交网络页面讨论的兴趣存在差异。
- en: 'Given our success in predicting gender, you might suspect that the clusters
    are also predictive of the number of friends the users have. This hypothesis seems
    to be supported by the data, which is as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在预测性别方面的成功，你可能会怀疑聚类也可以预测用户拥有的朋友数量。这个假设似乎得到了以下数据的支持：
- en: '[PRE49]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'On average, *princesses* have the most friends (38.5), followed by *athletes*
    (35.9) and *brains* (32.8). On the low end are *criminals* (30.7) and *basket
    cases* (27.8). As with gender, the connection between a teenager’s number of friends
    and their predicted cluster is remarkable given that we did not use the friendship
    data as an input to the clustering algorithm. Also interesting is the fact that
    the number of friends seems to be related to the stereotype of each cluster’s
    high-school popularity: the stereotypically popular groups tend to have more friends
    in reality.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，“公主”拥有最多的朋友（38.5人），其次是“运动员”（35.9人）和“大脑”（32.8人）。在低端是“罪犯”（30.7人）和“疯子”（27.8人）。与性别一样，考虑到我们没有将友谊数据作为聚类算法的输入，一个青少年的朋友数量与其预测的聚类之间的联系是显著的。同样有趣的是，朋友数量似乎与每个聚类的刻板印象中的高中受欢迎程度有关：那些刻板印象中受欢迎的群体在现实中往往有更多的朋友。
- en: The association between group membership, gender, and number of friends suggests
    that the clusters can be useful predictors of behavior. Validating their predictive
    ability in this way may make the clusters an easier sell when they are pitched
    to the marketing team, ultimately improving the performance of the algorithm.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 团体成员资格、性别和朋友的数量之间的关联表明，聚类可以作为行为的有用预测指标。以这种方式验证其预测能力可能会使得在向营销团队推销时，聚类分析结果更容易被接受，从而最终提高算法的性能。
- en: Just as the characters in *The Breakfast Club* ultimately come to realize that
    “each one of us is a brain, an athlete, a basket case, a princess, and a criminal,”
    it is important for data scientists to realize that the labels or personas we
    attribute to each cluster are stereotypes, and individuals may embody the stereotype
    to a greater or lesser degree. Keep this caveat in mind when acting on the results
    of a cluster analysis; a group may be relatively homogenous, but each member is
    still unique.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 正如《早餐俱乐部》中的角色最终意识到的那样，“我们每个人都是一个大脑、一个运动员、一个疯子、一个公主和一个罪犯”，数据科学家意识到我们分配给每个聚类的标签或角色是刻板印象，并且个体可能以不同程度体现这些刻板印象，这一点很重要。在采取聚类分析结果时，请记住这个警告；一个群体可能相对同质，但每个成员仍然是独一无二的。
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Our findings support the popular adage that “birds of a feather flock together.”
    By using machine learning methods to cluster teenagers with others who have similar
    interests, we were able to develop a typology of teenage identities, which was
    predictive of personal characteristics such as gender and number of friends. These
    same methods can be applied to other contexts with similar results.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究支持了那句流行的谚语：“物以类聚，人以群分。”通过使用机器学习方法将具有相似兴趣的青少年进行聚类，我们能够开发出青少年身份类型的分类，这些分类可以预测个人特征，如性别和朋友的数量。这些相同的方法可以应用于其他具有相似结果的环境中。
- en: This chapter covered only the fundamentals of clustering. There are many variants
    of the k-means algorithm, as well as many other clustering algorithms, which bring
    unique biases and heuristics to the task. Based on the foundation in this chapter,
    you will be able to understand these clustering methods and apply them to new
    problems.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 本章仅涵盖了聚类的基础知识。k-means算法有许多变体，以及其他许多聚类算法，它们为任务带来了独特的偏见和启发式方法。基于本章的基础，你将能够理解这些聚类方法并将它们应用于新的问题。
- en: In the next chapter, we will begin to look at methods for measuring the success
    of a learning algorithm that are applicable across many machine learning tasks.
    While our process has always devoted some effort to evaluating the success of
    learning, in order to obtain the highest degree of performance, it is crucial
    to be able to define and measure it in the strictest terms.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始探讨适用于许多机器学习任务的测量学习算法成功的方法。虽然我们的过程一直致力于评估学习的成功，但为了获得最高程度的性能，能够以最严格的标准定义和衡量它是至关重要的。
- en: Join our book’s Discord space
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人相聚，并与超过 4000 人一起学习：
- en: '[https://packt.link/r](https://packt.link/r)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/r](https://packt.link/r)'
- en: '![](img/r.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/r.jpg)'
