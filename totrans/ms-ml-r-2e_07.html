<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Neural Networks and Deep Learning</h1>
            </header>

            <article>
                
<div class="packt_quote">"Forget artificial intelligence - in the brave new world of big data, it's artificial idiocy we should be looking out for."<br/>
                                                                                                              - Tom Chatfield</div>
<p>I recall that at some meeting circa mid-2012, I was part of a group discussing the results of some analysis or other, when one of the people around the table sounded off with a hint of exasperation mixed with a tinge of fright, <em>this</em> <em>isn't</em> <em>one</em> <em>of</em> <em>those</em> <em>neural</em> <em>networks,</em> <em>is</em> <em>it?</em> I knew of his past run-ins with and deep-seated anxiety about neural networks, so I assuaged his fears making some sarcastic comment that neural networks have basically gone the way of the dinosaur. No one disagreed! Several months later, I was gobsmacked when I attended a local meeting where the discussion focused on, of all things, neural networks and this mysterious deep learning. Machine learning pioneers such as Ng, Hinton, Salakhutdinov, and Bengio have revived neural networks and improved their performance.</p>
<p>Much media hype revolves around these methods with high-tech companies such as Facebook, Google, and Netflix investing tens, if not hundreds, of millions of dollars. The methods have yielded promising results in voice recognition, image recognition, and automation. If self-driving cars ever stop running off the road and into each other, it will certainly be from the methods discussed here.</p>
<p>In this chapter, we will discuss how the methods work, their benefits, and inherent drawbacks so that you can become conversationally competent about them. We will work through a practical business application of a neural network. Finally, we will apply the deep learning methodology in a cloud-based application.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Introduction to neural networks</h1>
            </header>

            <article>
                
<p>Neural network is a fairly broad term that covers a number of related methods, but in our case, we will focus on a <strong>feed forward</strong> network that trains with <strong>backpropagation</strong>. I'm not going to waste our time discussing how the machine learning methodology is similar or dissimilar to how a biological brain works. We only need to start with a working definition of what a neural network is. I think the Wikipedia entry is a good start.</p>
<p>In machine learning and cognitive science, <strong>Artificial neural networks</strong> (<strong>ANNs</strong>) are a family of statistical learning models inspired by biological neural networks (the central nervous systems of animals, in particular, the brain) and are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. <span class="URLPACKT"><a href="https://en.wikipedia.org/wiki/Artificial_neural_network">https://en.wikipedia.org/wiki/Artificial_neural_network</a></span></p>
<p>The motivation or benefit of ANNs is that they allow the modeling of highly complex relationships between inputs/features and response variable(s), especially if the relationships are highly nonlinear. No underlying assumptions are required to create and evaluate the model, and it can be used with qualitative and quantitative responses. If this is the yin, then the yang is the common criticism that the results are black box, which means that there is no equation with the coefficients to examine and share with the business partners. In fact, the results are almost not interpretable. The other criticisms revolve around how results can differ by just changing the initial random inputs and that training ANNs is computationally expensive and time-consuming.</p>
<p>The mathematics behind ANNs is not trivial by any measure. However, it is crucial to at least get a working understanding of what is happening. A good way to intuitively develop this understanding is to start a diagram of a simplistic neural network.</p>
<p>In this simple network, the inputs or covariates consist of two nodes or neurons. The neuron labeled <strong>1</strong> represents a constant or more appropriately, the intercept. <strong>X1</strong> represents a quantitative variable. The <strong>W</strong>'s represent the weights that are multiplied by the input node values. These values become <strong>Input Nodes</strong> to <strong>Hidden Node</strong>. You can have multiple hidden nodes, but the principal of what happens in just this one is the same. In the hidden node, <strong>H1</strong>, the <em>weight * value</em> computations are summed. As the intercept is notated as <strong>1</strong>, then that input value is simply the weight, <strong>W1</strong>. Now the magic happens. The summed value is then transformed with the <strong>Activation</strong> function, turning the input signal to an output signal. In this example, as it is the only <strong>Hidden Node</strong>, it is multiplied by <strong>W3</strong> and becomes the estimate of <strong>Y</strong>, our response. This is the feed-forward portion of the algorithm:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="182" width="329" class=" image-border" src="assets/image_07_001.jpg"/></div>
<p>But wait, there's more! To complete the cycle or epoch, as it is known, backpropagation happens and trains the model based on what was learned. To initiate the backpropagation, an error is determined based on a loss function such as <strong>Sum of Squared Error</strong> or <strong>Cross-Entropy</strong>, among others. As the weights, <strong>W1</strong> and <strong>W2</strong>, were set to some initial random values between <em>[-1, 1]</em>, the initial error may be high. Working backward, the weights are changed to minimize the error in the loss function. The following diagram portrays the backpropagation portion:</p>
<div class="CDPAlignCenter CDPAlign"><img height="192" width="346" class=" image-border" src="assets/image_07_002.jpg"/></div>
<p>This completes one epoch. This process continues, using gradient descent (discussed in <a href="a7511867-5362-4215-a7dd-bbdc162740d1.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>More Classification Techniques - K-Nearest Neighbors and Support Vector Machines</em>) until the algorithm converges to the minimum error or prespecified number of epochs. If we assume that our activation function is simply linear, in this example, we would end up with <em>Y = W3(W1(1) + W2(X1))</em>.<br/>
The networks can get complicated if you add numerous input neurons, multiple neurons in a hidden node, and even multiple hidden nodes. It is important to note that the output from a neuron is connected to all the subsequent neurons and has weights assigned to all these connections. This greatly increases the model complexity. Adding hidden nodes and increasing the number of neurons in the hidden nodes has not improved the performance of ANNs as we had hoped. Thus, the development of deep learning occurs, which in part relaxes the requirement of all these neuron connections.</p>
<p>There are a number of activation functions that one can use/try, including a simple linear function, or for a classification problem, the <kbd>sigmoid</kbd> function, which is a special case of the logistic function (<a href="d5d39222-b2f8-4c80-9348-34e075893e47.xhtml"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Logistic Regression and Discriminant Analysis</em>). Other common activation functions are <kbd>Rectifier</kbd>, <kbd>Maxout</kbd>, and <strong>hyperbolic tangent</strong> (<strong>tanh</strong>).</p>
<p>We can plot a <kbd>sigmoid</kbd> function in R, first creating an <kbd>R</kbd> function in order to calculate the <kbd>sigmoid</kbd> function values:</p>
<pre>
    <strong>&gt; sigmoid = function(x) {</strong><br/>    <strong> 1 / ( 1 + exp(-x) )</strong><br/>    <strong> }</strong>
</pre>
<p>Then, it is a simple matter to plot the function over a range of values, say <kbd>-5</kbd> to <kbd>5</kbd>:</p>
<pre>
<strong>    &gt; x &lt;- seq(-5, 5, .1)</strong><strong><br/>    &gt; plot(sigmoid(x))</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="249" width="374" class="image-border" src="assets/image_07_01.png"/></div>
<p>The <kbd>tanh</kbd> function (hyperbolic tangent) is a rescaling of the logistic <kbd>sigmoid</kbd> with the output between <strong>-1</strong> and <strong>1</strong>. The <kbd>tanh</kbd> function relates to <kbd>sigmoid</kbd> as follows, where <strong>x</strong> is the <kbd>sigmoid</kbd> function:</p>
<p><em>tanh(x)</em> <em>=</em> <em>2</em> <em>*</em> <em>sigmoid(2x)</em> <em>-</em> <em>1</em></p>
<p>Let's plot the <kbd>tanh</kbd> and <kbd>sigmoid</kbd> functions for comparison purposes. Let's also use <kbd>ggplot</kbd>:</p>
<pre>
<strong>    &gt; library(ggplot2)</strong><br/><strong>    &gt; s &lt;- sigmoid(x)</strong><br/><strong>    &gt; t &lt;- tanh(x)</strong><br/><strong>    &gt; z &lt;- data.frame(cbind(x, s, t))</strong><br/><strong>    &gt; ggplot(z, aes(x)) +</strong><br/><strong>      geom_line(aes(y = s, color = "sigmoid")) +</strong><br/><strong>      geom_line(aes(y = t, color = "tanh"))<br/></strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="320" width="481" class="image-border" src="assets/image_07_02-1.png"/></div>
<p>So why use the <kbd>tanh</kbd> function versus <kbd>sigmoid</kbd>? It seems there are many opinions on the subject; is <kbd>tanh</kbd> popular in neural networks? In short, assuming you have scaled data with mean 0 and variance 1, the <kbd>tanh</kbd> function permits weights that are on average close to zero (zero-centered). This helps in avoiding bias and improves convergence. Think about the implications of always having positive weights from an output neuron to an input neuron as in a <kbd>sigmoid</kbd> function activation. During backpropagation, the weights will become either all positive or all negative between layers. This may cause performance issues. Also, since the gradient at the tails of a <kbd>sigmoid</kbd> (0 and 1) are almost zero, during backpropagation it can happen that almost no signal will flow between neurons of different layers.  A full discussion of the issue is available, LeCun (1998). Keep in mind it is not a foregone conclusion that <kbd>tanh</kbd> is always better.</p>
<p>This all sounds fascinating, but the ANN almost went the way of disco as it just did not perform as well as advertised, especially when trying to use deep networks with many hidden layers and neurons. It seems that a slow yet gradual revival came about with the seminal paper by Hinton and Salakhutdinov (2006) in the reformulated and, dare I say, rebranded neural network, deep learning.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Deep learning, a not-so-deep overview</h1>
            </header>

            <article>
                
<p>So, what is this deep learning that is grabbing our attention and headlines? Let's turn to Wikipedia again for a working definition: <em>Deep learning is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using model architectures, with complex structures or otherwise, composed of multiple nonlinear transformations</em>. That sounds as if a lawyer wrote it. The characteristics of deep learning are that it is based on ANNs where the machine learning techniques, primarily unsupervised learning, are used to create new features from the input variables. We will dig into some unsupervised learning techniques in the next couple of chapters, but one can think of it as finding structure in data where no response variable is available. A simple way to think of it is the <strong>Periodic Table of Elements</strong>, which is a classic case of finding structure where no response is specified. Pull up this table online and you will see that it is organized based on atomic structure, with metals on one side and non-metals on the other. It was created based on latent classification/structure. This identification of latent structure/hierarchy is what separates deep learning from your run-of-the-mill ANN. Deep learning sort of addresses the question whether there is an algorithm that better represents the outcome than just the raw inputs. In other words, can our model learn to classify pictures other than with just the raw pixels as the only input? This can be of great help in a situation where you have a small set of labeled responses but a vast amount of unlabeled input data. You could train your deep learning model using unsupervised learning and then apply this in a supervised fashion to the labeled data, iterating back and forth.</p>
<p>Identification of these latent structures is not trivial mathematically, but one example is the concept of regularization that we looked at in <a href="04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Advanced Feature Selection in Linear Models</em>. In deep learning, one can penalize weights with regularization methods such as <em>L1</em> (penalize non-zero weights), <em>L2</em> (penalize large weights), and dropout (randomly ignore certain inputs and zero their weight out). In standard ANNs, none of these regularization methods takes place.</p>
<p>Another way is to reduce the dimensionality of the data. One such method is the <kbd>autoencoder</kbd>. This is a neural network where the inputs are transformed into a set of reduced dimension weights. In the following diagram, notice that <strong>Feature A</strong> is not connected to one of the hidden nodes:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="262" width="411" class="image-border" src="assets/image_07_03.png"/></div>
<p>This can be applied recursively and learning can take place over many hidden layers. What you have seen happening in this case is that the network is developing features of features as they are stacked on each other. Deep learning will learn the weights between two layers in sequence first and then only use backpropagation in order to fine-tune these weights. Other feature selection methods include <strong>Restricted Boltzmann Machine</strong> and <strong>Sparse Coding Model</strong>.</p>
<p>The details are beyond our scope, and many resources are available to learn about the specifics. Here are a couple of starting points:<a href="http://www.cs.toronto.edu/~hinton/"> </a></p>
<p><a href="http://www.cs.toronto.edu/~hinton/">http://www.cs.toronto.edu/~hinton/</a></p>
<p><a href="http://deeplearning.net/">http://deeplearning.net/</a></p>
<p>Deep learning has performed well on many classification problems, including winning a Kaggle contest or two. It still suffers from the problems of ANNs, especially the black box problem. Try explaining to the uninformed what is happening inside a neural network. However, it is appropriate for problems where an explanation of How is not a problem and the important question is What. After all, do we really care why an autonomous car avoided running into a pedestrian, or do we care about the fact that it did not? Additionally, the Python community has a bit of a head start on the R community in deep learning usage and packages. As we will see in the practical exercise, the gap is closing.</p>
<p>While deep learning is an exciting undertaking, be aware that to achieve the full benefit of its capabilities, you will need a high degree of computational power along with taking the time to train the best model by fine-tuning the hyperparameters. Here is a list of some that you will need to consider:</p>
<ul>
<li>An activation function</li>
<li>Size and number of the hidden layers</li>
<li>Dimensionality reduction, that is, Restricted Boltzmann versus autoencoder</li>
<li>The number of epochs</li>
<li>The gradient descent learning rate</li>
<li>The loss function</li>
<li>Regularization</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Deep learning resources and advanced methods</h1>
            </header>

            <article>
                
<p>One of the more interesting visual tools you can use for both learning and explaining is the interactive widget provided by TensorFlow<sup>TM</sup>: <a href="http://playground.tensorflow.org/">http://playground.tensorflow.org/</a>. This tool allows you to explore, or <strong>tinker</strong>, as the site calls it, the various parameters and how they impact on the response, be it a classification problem or a regression problem. I could spend, well I have spent hours tinkering with it.</p>
<div class="packt_tip">Here is an interesting task: create your own experimental design and see how the various parameters affect your prediction.</div>
<p>At this point, it seems that the two fastest growing deep learning open-source tools are TensorFlow<sup>TM</sup> and MXNet. I still prefer working with the package we will see, <kbd>h2o</kbd>, but it is important to understand and learn the latest techniques. You can access TensorFlow<sup><span>TM </span></sup><span>with R, but it requires you to install python first. This series of tutorials will walk you through how to get it up and running:</span></p>
<p><a href="https://rstudio.github.io/tensorflow/">https://rstudio.github.io/tensorflow/</a>.</p>
<p>MXNet does not require the installation of Python and is relatively easy to install and make operational. It also offers a number of pretrained models that allow you to start making predictions quickly. Several R tutorials are available:</p>
<p><a href="http://mxnet.io/">http://mxnet.io/</a>.</p>
<p>I now want to take the time to enumerate some of the variations of deep neural networks along with the learning tasks where they have performed well.</p>
<p><strong>Convolutional neural networks</strong> (<strong>CNN</strong>) make the assumption that the inputs are images and create features from slices or small portions of the data, which are combined to create a feature map. Think of these small slices as filters or probably more appropriately, kernels that the network learns during training. The activation function for CNN is a <strong>Rectified</strong> <strong>Linear</strong> <strong>Unit</strong> (<strong>ReLU</strong>). It is simply <em>f(x) = max(0, x)</em>, where <em>x</em> is the input to the neuron. CNNs perform well on image classification, object detection, and even sentence classification.</p>
<p><strong>Recurrent neural networks</strong> (<strong>RNN</strong>) are created to make use of sequential information. In traditional neural networks, the inputs and outputs are independent of each other. With RNN, the output is dependent on the computations of previous layers, permitting information to persist across layers. So, take an output from a neuron (y); it is calculated not only on its input (t) but on all previous layers (t-1, t-n...). It is effective at handwriting and speech detection.</p>
<p><strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) is a special case of RNN. The problem with RNN is that it does not perform well on data with long signals. Thus LSTMs were created to capture complex patterns in data. RNNs combine information during training from previous steps in the same way, regardless of the fact that information in one step is more or less valuable than other steps. LSTMs seek to overcome this limitation by deciding what to remember at each step during training. This multiplication of a weight matrix by the data vector is referred to as a gate, which acts as an information filter. A neuron in LSTM will have two inputs and two outputs. The input from prior outputs and the memory vector passed from the previous gate. Then, it produces the output values and output memory as inputs to the next layer. LSTMs have the limitation of requiring a healthy dose of training data and are computationally intensive. LSTMs have performed well on speech recognition problems.</p>
<div class="packt_tip">I recommend you work with the tutorials on MXNet to help you understand how to develop these models for your own use.</div>
<p> With that, let's move on to some practical applications.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business understanding</h1>
            </header>

            <article>
                
<p>It was a calm, clear night on the 20th of April, 1998. I was a student pilot in a Hughes 500D helicopter on a cross-country flight from the St. Paul, MN downtown airport back home to good old Grand Forks, ND. The flight was my final requirement prior to taking the test to achieve a helicopter instrument rating. My log book shows that we were 35 <strong>Distance Measuring Equipment</strong> (<strong>DME</strong>) or 35 nautical miles from the VOR on Airway Victor 2. This put us somewhere south/southeast of St. Cloud, MN, cruising along at what I recall was 4,500 feet above sea level at approximately 120 knots. Then, it happened...BOOOOM! It is not hyperbole to say that it was a thunderous explosion, followed by a hurricane blast of wind to the face.</p>
<p>It all started when my flight instructor asked a mundane question about our planned instrument approach into Alexandria, MN. We swapped control of the aircraft and I bent over to consult the instrument approach plate on my kneeboard. As I snapped on the red lens flashlight, the explosion happened. Given my face-down orientation, the sound, and ensuing blast of wind, several thoughts crossed my mind: the helicopter is falling apart, I'm plunging to my death, and the Space Shuttle Challenger explosion as an HD quality movie going off in my head. In the 1.359 seconds that it took us to stop screaming, we realized that the Plexiglas windscreen in front of me was essentially gone, but everything else was good to go. After slowing the craft, a cursory inspection revealed that the cockpit was covered in blood, guts, and feathers. We had done the improbable by hitting a Mallard duck over Central Minnesota and in the process, destroyed the windscreen. Had I not been looking at my kneeboard, I would have been covered in pate. We simply declared an emergency and canceled our flight plan with Minneapolis Center and, like the Memphis Belle, limped our way into Alexandria to await rescue from our compatriots at the University of North Dakota (home of the Fighting Sioux).<br/>
So what? Well, I wanted to point out how much of a NASA fan and astronaut I am. In a terrifying moment, where for a split second I thought that I was checking out, my mind drifted to the Space Shuttle. Most males my age wanted to shake the hands of George Brett or Wayne Gretzky. I wanted to, and in fact did, shake the hands of Buzz Aldrin. (he was after all on the North Dakota faculty at the time.) Thus, when I found the <kbd>shuttle</kbd> dataset in the <kbd>MASS</kbd> package, I had to include it in this tome. By the way, if you ever get the chance to see the Space Shuttle Atlantis display at Kennedy Space Center, do not miss it.</p>
<p>For this problem, we will try and develop a neural network to answer the question of whether or not the shuttle should use the autolanding system. The default decision is to let the crew land the craft. However, the autoland capability may be required for situations of crew incapacitation or adverse effects of gravity upon re-entry after extended orbital operations. This data is based on computer simulations, not actual flights. In reality, the autoland system went through some trials and tribulations and, for the most part, the shuttle astronauts were in charge during the landing process. Here are a couple of links for further background information:</p>
<p><a href="http://www.spaceref.com/news/viewsr.html?pid=10518" target="_blank">http://www.spaceref.com/news/viewsr.html?pid=10518</a></p>
<p><a href="https://waynehale.wordpress.com/2011/03/11/breaking-through/">https://waynehale.wordpress.com/2011/03/11/breaking-through/</a></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data understanding and preparation</h1>
            </header>

            <article>
                
<p>To start, we will load these four packages. The data is in the <kbd>MASS</kbd> package:</p>
<pre>
<strong>    &gt; library(caret)</strong><br/><strong>    &gt; library(MASS)</strong><br/><strong>    &gt; library(neuralnet)</strong><br/><strong>    &gt; library(vcd)<br/></strong>
</pre>
<p>The <kbd>neuralnet</kbd> package will be used for the building of the model and <kbd>caret</kbd> for the data preparation. The <kbd>vcd</kbd> package will assist us in data visualization. Let's load the data and examine its structure:</p>
<pre>
    <strong>&gt; data(shuttle)</strong><br/>    <strong>&gt; str(shuttle)</strong><br/>    <strong>'data.frame':256 obs. of  7 variables:</strong><br/>    <strong> $ stability: Factor w/ 2 levepicels "stab","xstab": 2 2 2 2 2 2 2        <br/>       2 2 2 ...</strong><br/>    <strong> $ error    : Factor w/ 4 levels "LX","MM","SS",..: 1 1 1 1 1 1 1 1        <br/>       1 1 ...</strong><br/>    <strong> $ sign     : Factor w/ 2 levels "nn","pp": 2 2 2 2 2 2 1 1 1 1 ...</strong><br/>    <strong> $ wind     : Factor w/ 2 levels "head","tail": 1 1 1 2 2 2 1 1 1 2        <br/>       ...</strong><br/>    <strong> $ magn     : Factor w/ 4 levels "Light","Medium",..: 1 2 4 1 2 4 1        <br/>       2 4 1 ...</strong><br/>    <strong> $ vis      : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 1 1 1        <br/>       ...</strong><br/>    <strong> $ use      : Factor w/ 2 levels "auto","noauto": 1 1 1 1 1 1 1 1 1        <br/>       1 ...</strong>
</pre>
<p>The data consists of <kbd>256</kbd> observations and <kbd>7 variables</kbd>. Notice that all of the variables are categorical and the response is <kbd>use</kbd> with two levels, <kbd>auto</kbd> and <kbd>noauto</kbd>. The covariates are as follows:</p>
<ul>
<li><kbd>stability</kbd>: This is stable positioning or not (<kbd>stab</kbd>/<kbd>xstab</kbd>)</li>
<li><kbd>error</kbd>: This is the size of the error (<kbd>MM</kbd> / <kbd>SS</kbd> / <kbd>LX</kbd>)</li>
<li><kbd>sign</kbd>: This is the sign of the error, positive or negative (<kbd>pp</kbd>/<kbd>nn</kbd>)</li>
<li><kbd>wind</kbd>: This is the <kbd>wind</kbd> sign (<kbd>head</kbd> / <kbd>tail</kbd>)</li>
<li><kbd>magn</kbd>: This is the <kbd>wind</kbd> strength (<kbd>Light</kbd> / <kbd>Medium</kbd> / <kbd>Strong</kbd> / <kbd>Out of Range</kbd>)</li>
<li><kbd>vis</kbd>: This is the visibility (<kbd>yes</kbd> / <kbd>no</kbd>)</li>
</ul>
<p>We will build a number of tables to explore the data, starting with the response/outcome:</p>
<pre>
    <strong>&gt; table(shuttle$use)</strong><br/>    <strong>  auto noauto </strong><br/>    <strong>   145    111</strong>
</pre>
<p>Almost 57 per cent of the time, the decision is to use the autolander. There are a number of possibilities to build tables for categorical data. The <kbd>table()</kbd> function is perfectly adequate to compare one with another, but if you add a third, it can turn into a mess to look at. The <kbd>vcd</kbd> package offers a number of table and plotting functions. One is <kbd>structable()</kbd>. This function will take a formula (<em>column1 + column2 ~ column3</em>), where <em>column3</em> becomes the rows in the table:</p>
<pre>
    <strong>&gt; table1 &lt;- structable(wind + magn ~ use, shuttle)</strong><br/>    <strong>&gt; table1</strong><br/>    <strong>       wind  head                    tail                  </strong><br/>    <strong>       magn Light Medium Out Strong Light Medium Out Strong</strong><br/>    <strong>use                                                        </strong><br/>    <strong>auto           19     19  16     18    19     19  16     19</strong><br/>    <strong>noauto         13     13  16     14    13     13  16     13</strong>
</pre>
<p>Here, we can see that in the cases of a headwind that was <kbd>Light</kbd> in magnitude, <kbd>auto</kbd> occurred <kbd>19</kbd> times and <kbd>noauto</kbd>, <kbd>13</kbd> times. The <kbd>vcd</kbd> package offers the <kbd>mosaic()</kbd> function to plot the table created by <kbd>structable()</kbd> and provide the <strong>p-value</strong> for a chi-squared test:</p>
<pre>
    <strong>&gt; mosaic(table1, shading = T)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="307" width="449" class="image-border" src="assets/image_07_04.png"/></div>
<p>The plot tiles correspond to the proportional size of their respective cells in the table, created by recursive splits. You can also see that the <strong>p-value</strong> is not significant, so the variables are independent, which means that knowing the levels of wind and/or <strong>magn</strong> does not help us predict the use of the autolander. You do not need to include a <kbd>structable()</kbd> object in order to create the plot as it will accept a formula just as well:</p>
<pre>
    <strong>&gt; mosaic(use ~ error + vis, shuttle)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="303" width="474" class="image-border" src="assets/image_07_05.png"/></div>
<p>Note that the shading of the table has changed, reflecting the rejection of the null hypothesis and dependence in the variables. The plot first takes and splits the visibility. The result is that if the visibility is <strong>no</strong>, then the autolander is used. The next split is horizontal by <strong>error</strong>. If <strong>error</strong> is <strong>SS</strong> or <strong>MM</strong> when <strong>vis</strong> is <strong>no</strong>, then the autolander might be recommended, otherwise it is not. A p-value is not necessary as the gray shading indicates significance.</p>
<p>One can also examine proportional tables with the <kbd>prop.table()</kbd> function as a wrapper around <kbd>table()</kbd>:</p>
<pre>
    <strong>&gt; table(shuttle$use, shuttle$stability)</strong><br/>    <strong>         stab xstab</strong><br/>    <strong>  auto     81    64</strong><br/>    <strong>  noauto   47    64</strong><br/>    <strong>&gt; prop.table(table(shuttle$use, shuttle$stability))</strong><br/>    <strong>              stab     xstab</strong><br/>    <strong>  auto   0.3164062 0.2500000</strong><br/>    <strong>  noauto 0.1835938 0.2500000</strong>
</pre>
<p>In case we forget, the chi-squared tests are quite simple:</p>
<pre>
    <strong>&gt; chisq.test(shuttle$use, shuttle$stability)</strong><br/>    <strong>Pearson's Chi-squared test with Yates' continuity</strong><br/>    <strong>correction</strong><br/>    <strong>data:  shuttle$use and shuttle$stability</strong><br/>    <strong>X-squared = 4.0718, df = 1, p-value = 0.0436</strong>
</pre>
<p>Preparing the data for a neural network is very important as all the covariates and responses need to be numeric. In our case, all of the input features are categorical. However, the <kbd>caret</kbd> package allows us to quickly create dummy variables as our input features:</p>
<pre>
    <strong>&gt; dummies &lt;- dummyVars(use ~ .,shuttle, fullRank = T)</strong><br/>    <strong>&gt; dummies</strong><br/>    <strong>Dummy Variable Object</strong><br/>    <strong>Formula: use ~ .</strong><br/>    <strong>7 variables, 7 factors</strong><br/>    <strong>Variables and levels will be separated by '.'</strong><br/>    <strong>A full rank encoding is used</strong>
</pre>
<p>To put this into a data frame, we need to predict the <kbd>dummies</kbd> object to an existing data, either the same or different, in <kbd>as.data.frame()</kbd>. Of course, the same data is needed here:</p>
<pre>
    <strong>&gt; shuttle.2 = as.data.frame(predict(dummies, newdata=shuttle))</strong><br/>    <br/>    <strong>&gt; names(shuttle.2)</strong><br/>    <strong> [1] "stability.xstab" "error.MM"        "error.SS"       </strong><br/>    <strong> [4] "error.XL"        "sign.pp"         "wind.tail"      </strong><br/>    <strong> [7] "magn.Medium"     "magn.Out"        "magn.Strong"    </strong><br/>    <strong>[10] "vis.yes"        </strong><br/>    <br/>    <strong>&gt; head(shuttle.2)</strong><br/>    <strong>  stability.xstab error.MM error.SS error.XL sign.pp wind.tail</strong><br/>    <strong>1               1        0        0        0       1         0</strong><br/>    <strong>2               1        0        0        0       1         0</strong><br/>    <strong>3               1        0        0        0       1         0</strong><br/>    <strong>4               1        0        0        0       1         1</strong><br/>    <strong>5               1        0        0        0       1         1</strong><br/>    <strong>6               1        0        0        0       1         1</strong><br/>    <strong>  magn.Medium magn.Out magn.Strong vis.yes</strong><br/>    <strong>1           0        0           0       0</strong><br/>    <strong>2           1        0           0       0</strong><br/>    <strong>3           0        0           1       0</strong><br/>    <strong>4           0        0           0       0</strong><br/>    <strong>5           1        0           0       0</strong><br/>    <strong>6           0        0           1       0</strong>
</pre>
<p>We now have an input feature space of ten variables. Stability is now either <kbd>0</kbd> for <kbd>stab</kbd> or <kbd>1</kbd> for <kbd>xstab</kbd>. The base error is <kbd>LX</kbd>, and three variables represent the other categories.</p>
<p>The response can be created using the <kbd>ifelse()</kbd> function:</p>
<pre>
    <strong>&gt; shuttle.2$use &lt;- ifelse(shuttle$use == "auto", 1, 0)</strong><br/>    <strong>&gt; table(shuttle.2$use)</strong><br/>    <strong>  0   1 </strong><br/>    <strong>111 145</strong>
</pre>
<p>The <kbd>caret</kbd> package also provides us with the functionality to create the <kbd>train</kbd> and <kbd>test</kbd> sets. The idea is to index each observation as <kbd>train</kbd> or <kbd>test</kbd> and then split the data accordingly. Let's do this with a 70/30 <kbd>train</kbd> to <kbd>test</kbd> split, as follows:</p>
<pre>
    <strong>&gt; set.seed(123)</strong><br/>    <strong>&gt; trainIndex &lt;- createDataPartition(shuttle.2$use, p = .7, list =       <br/>       FALSE)</strong>
</pre>
<p>The values in <kbd>trainIndex</kbd> provide us with the row number; in our case, 70 per cent of the total row numbers in <kbd>shuttle.2</kbd>. It is now a simple case of creating the <kbd>train</kbd>/<kbd>test</kbd> datasets:</p>
<pre>
    <strong>&gt; shuttleTrain &lt;- shuttle.2[trainIndex, ]</strong><br/>    <strong>&gt; shuttleTest  &lt;- shuttle.2[-trainIndex, ]</strong>
</pre>
<p>Nicely done! We are now ready to begin building the neural networks.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling and evaluation</h1>
            </header>

            <article>
                
<p>As mentioned, the package that we will use is <kbd>neuralnet</kbd>. The function in <kbd>neuralnet</kbd> will call for the use of a formula as we used elsewhere, such as <em>y~x1+x2+x3+x4</em>, <em>data = df</em>. In the past, we used <em>y~,</em> to specify all the other variables in the data as inputs. However, <kbd>neuralnet</kbd> does not accommodate this at the time of writing. The way around this limitation is to use the <kbd>as.formula()</kbd> function. After first creating an object of the variable names, we will use this as an input in order to paste the variables properly on the right side of the equation:</p>
<pre>
    <strong>&gt; n &lt;- names(shuttleTrain)</strong><br/>    <strong>&gt; form &lt;- as.formula(paste("use ~", paste(n[!n %in% "use"], <br/>      collapse = " + ")))</strong><br/>    <strong>&gt; form</strong><br/>    <strong>use ~ stability.xstab + error.MM + error.SS + error.XL + sign.pp +       <br/>      wind.tail <br/>       + magn.Medium + magn.Out + magn.Strong + vis.yes</strong>
</pre>
<p>Keep this function in mind for your own use as it may come in quite handy. In the <kbd>neuralnet</kbd> package, the function that we will use is appropriately named <kbd>neuralnet()</kbd>. Other than the formula, there are four other critical arguments that we will need to examine:</p>
<ul>
<li><kbd>hidden</kbd>: This is the number of hidden neurons in each layer, which can be up to three layers; the default is 1</li>
<li><kbd>act.fct</kbd>: This is the activation function with the default logistic and <kbd>tanh</kbd> available</li>
<li><kbd>err.fct</kbd>: This is the function used to calculate the error with the default <kbd>sse</kbd>; as we are dealing with binary outcomes, we will use <kbd>ce</kbd> for cross-entropy</li>
<li><kbd>linear.output</kbd>: This is a logical argument on whether or not to ignore <kbd>act.fct</kbd> with the default TRUE, so for our data, this will need to be <kbd>FALSE</kbd></li>
</ul>
<p>You can also specify the algorithm. The default is resilient with backpropagation and we will use it along with the default of one hidden neuron:</p>
<pre>
    <strong>&gt; fit &lt;- neuralnet(form, data = shuttleTrain, err.fct = "ce", <br/>      linear.output = FALSE)</strong>
</pre>
<p>Here are the overall results:</p>
<pre>
    <strong>&gt; fit$result.matrix</strong><br/>    <strong>  1<br/>    error                         0.009928587504<br/>    reached.threshold             0.009905188403<br/>    steps                       660.000000000000<br/>    Intercept.to.1layhid1        -4.392654985479<br/>    stability.xstab.to.1layhid1   1.957595172393<br/>    error.MM.to.1layhid1         -1.596634090134<br/>    error.SS.to.1layhid1         -2.519372079568<br/>    error.XL.to.1layhid1         -0.371734253789<br/>    sign.pp.to.1layhid1          -0.863963659357<br/>    wind.tail.to.1layhid1         0.102077456260<br/>    magn.Medium.to.1layhid1      -0.018170137582<br/>    magn.Out.to.1layhid1          1.886928834123<br/>    magn.Strong.to.1layhid1       0.140129588700<br/>    vis.yes.to.1layhid1           6.209014123244<br/>    Intercept.to.use             30.721652703205<br/>    1layhid.1.to.use            -65.084168998463</strong>
</pre>
<p>We can see that the error is extremely low at <kbd>0.0099</kbd>. The number of steps required for the algorithm to reach the threshold, which is when the absolute partial derivatives of the error function become smaller than this error (default = 0.1). The highest weight of the first neuron is <kbd>vis.yes.to.1layhid1</kbd> at 6.21.</p>
<p>You can also look at what are known as generalized weights. According to the authors of the <kbd>neuralnet</kbd> package, the generalized weight is defined as the contribution of the <em>i</em>th covariate to the log-odds:</p>
<p><em>The generalized weight expresses the effect of each covariate x<sub>i</sub> and thus has an analogous interpretation as the ith regression parameter in regression models. However, the generalized weight depends on all other covariates</em> (Gunther and Fritsch, 2010).</p>
<div class="packt_tip">The weights can be called and examined. I've abbreviated the output to the first four variables and six observations only. Note that if you sum each row, you will get the same number, which means that the weights are equal for each covariate combination. Please note that your results might be slightly different because of random weight initialization.</div>
<p>The results are as follows:</p>
<pre>
    <strong>&gt; head(fit$generalized.weights[[1]])</strong><br/>          [<strong>,1]             [,2]         [,3]         [,4] </strong><br/><strong>    1 -4.374825405  3.568151106  5.630282059 0.8307501368 </strong><br/><strong>    2 -4.301565756  3.508399808  5.535998871 0.8168386187 </strong><br/><strong>    6 -5.466577583  4.458595039  7.035337605 1.0380665866 </strong><br/><strong>    9 -10.595727733 8.641980909 13.636415225 2.0120579565 </strong><br/><strong>   10 -10.270199330 8.376476707 13.217468969 1.9502422861 </strong><br/><strong>   11 -10.117466745 8.251906491 13.020906259 1.9212393878</strong>
</pre>
<p>To visualize the neural network, simply use the <kbd>plot()</kbd> function:</p>
<pre>
    <strong>&gt; plot(fit)</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="348" width="363" class="image-border" src="assets/image_07_06-1.png"/></div>
<p>This plot shows the weights of the variables and intercepts. You can also examine the generalized weights in a plot. Let's look at <kbd>vis.yes</kbd> versus <kbd>wind.tail</kbd>, which has a low overall synaptic weight. Notice how <kbd>vis.yes</kbd> is skewed and <kbd>wind.tail</kbd> has an even distribution of weights, implying little predictive power:</p>
<pre>
    <strong>&gt; par(mfrow = c(1, 2))</strong><br/>    <strong>&gt; gwplot(fit, selected.covariate = "vis.yes")</strong><br/>    <strong>&gt; gwplot(fit, selected.covariate = "wind.tail")</strong>
</pre>
<p class="packt_figure"><span>The following is the output of the preceding commands:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="298" width="466" class="image-border" src="assets/image_07_07.png"/></div>
<p>We now want to see how well the model performs. This is done with the <kbd>compute()</kbd> function and specifying the fit model and covariates. This syntax will be the same for the predictions on the <kbd>test</kbd> and <kbd>train</kbd> sets. Once computed, a list of the predictions is created with <kbd>$net.result</kbd>:</p>
<pre>
    <strong>&gt; resultsTrain &lt;- compute(fit, shuttleTrain[, 1:10])</strong><br/>    <strong>&gt; predTrain &lt;- resultsTrain$net.result</strong>
</pre>
<p>These results are in probabilities, so let's turn them into <kbd>0</kbd> or <kbd>1</kbd> and follow this up with a confusion matrix:</p>
<pre>
    <strong>&gt; predTrain &lt;- ifelse(predTrain &gt;= 0.5, 1, 0)</strong> <br/>    <strong>&gt; table(predTrain, shuttleTrain$use)</strong>  <br/>    <strong>predTrain  0  1</strong><br/>    <strong>        0 81  0</strong><br/>    <strong>        1  0 99</strong>
</pre>
<p>Lo and behold, the neural network model has achieved 100 per cent accuracy. We will now hold our breath and see how it does on the <kbd>test</kbd> set:</p>
<pre>
<strong>    &gt; resultsTest &lt;- compute(fit, shuttleTest[,1:10])</strong><strong><br/>    &gt; predTest &lt;- resultsTest$net.result</strong><strong><br/>    &gt; predTest &lt;- ifelse(predTest &gt;= 0.5, 1, 0)</strong><strong><br/>    &gt; table(predTest, shuttleTest$use)</strong><br/><strong>    predTest  0  1</strong><br/><strong>           0 29  0</strong><br/><strong>           1  1 46<br/></strong>
</pre>
<p>Only one false positive in the <kbd>test</kbd> set. If you wanted to identify which one this was, use the <kbd>which()</kbd> function to single it out, as follows:</p>
<pre>
    <strong>&gt; which(predTest == 1 &amp; shuttleTest$use == 0)</strong><br/>    <strong>[1] 62</strong>
</pre>
<p>It is row <kbd>62</kbd> in the <kbd>test</kbd> set and observation <kbd>203</kbd> in the full dataset.</p>
<p>I'll leave it to you to see if you can build a neural network that achieves 100% accuracy!</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">An example of deep learning</h1>
            </header>

            <article>
                
<p>Shifting gears away from the Space Shuttle, let's work through a practical example of deep learning, using the <kbd>h2o</kbd> package. We will do this on data I've modified from the UCI Machine Learning Repository. The original data and its description is available at <a href="https://archive.ics.uci.edu/ml/datasets/Bank+Marketing/">https://archive.ics.uci.edu/ml/datasets/Bank+Marketing/</a>.  What I've done is, take the smaller dataset <kbd>bank.csv</kbd>, scale the numeric variables to mean 0 and variance of 1, create dummies for the character variables/sparse numerics, and eliminate near zero variance varaibles.  The data is available on github <a href="https://github.com/datameister66/data/">https://github.com/datameister66/data/</a> named also <kbd>bank_DL.csv</kbd>. In this section, we will focus on how to load the data in the H20 platform and run the deep learning code to build a classifier to predict whether a customer will respond to a marketing campaign.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">H2O background</h1>
            </header>

            <article>
                
<p>H2O is an open source predictive analytics platform with prebuilt algorithms, such as k-nearest neighbor, gradient boosted machines, and deep learning. You can upload data to the platform via Hadoop, AWS, Spark, SQL, noSQL, or your hard drive. The great thing about it is that you can utilize the machine learning algorithms in R and, at a much greater scale, on your local machine. If you are interested in learning more, you can visit the site: <a href="http://h2o.ai/product/"><span class="URLPACKT">http://h2o.ai/product/</span></a>.</p>
<p>The process of installing H2O on R is a little different. I put the code here that gave me the latest update (as of February 25, 2017). You can use it to reinstall the latest version or pull it off of the website: <a href="http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/Ruser/Rinstall.html/">http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/Ruser/Rinstall.html/</a>. The following is the code to install the latest version:</p>
<pre>
<strong>   # The following two commands remove any previously installed H2O      <br/>      packages for <br/>     R.</strong><br/><strong>    if ("package:h2o" %in% search()) { detach("package:h2o",     <br/>      unload=TRUE) }</strong><br/><strong>    if ("h2o" %in% rownames(installed.packages())) {     <br/>      remove.packages("h2o") }</strong><br/><br/><strong>    # Next, we download packages that H2O depends on.</strong><br/><strong>    if (! ("methods" %in% rownames(installed.packages()))) {     <br/>    install.packages("methods") }</strong><br/><strong>    if (! ("statmod" %in% rownames(installed.packages()))) {   <br/>    install.packages("statmod") }</strong><br/><strong>    if (! ("stats" %in% rownames(installed.packages()))) { <br/>    install.packages("stats") }</strong><br/><strong>    if (! ("graphics" %in% rownames(installed.packages()))) { <br/>    install.packages("graphics") }</strong><br/><strong>    if (! ("RCurl" %in% rownames(installed.packages()))) {  <br/>    install.packages("RCurl") }</strong><br/><strong>    if (! ("jsonlite" %in% rownames(installed.packages()))) { <br/>    install.packages("jsonlite") }</strong><br/><strong>    if (! ("tools" %in% rownames(installed.packages()))) { <br/>    install.packages("tools") }</strong><br/><strong>    if (! ("utils" %in% rownames(installed.packages()))) { <br/>    install.packages("utils") }</strong><br/><br/><strong>    # Now we download, install and initialize the H2O package for R.</strong><br/><strong>    install.packages("h2o", type="source", repos=(c("http://h2o-   <br/>    release.s3.amazonaws.com/h2o/rel-tverberg/5/R")))</strong>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data upload to H2O</h1>
            </header>

            <article>
                
<p>Let's assume you have the  <kbd>bank_DL.csv</kbd> file saved in your working directory. Remember, <kbd>getwd()</kbd> will provide you with the path to it. So, let's load the library and create an object with the file path to the data:</p>
<pre>
<strong>    &gt; library(h2o)<br/>    &gt; path &lt;- "C:/.../bank_DL.csv"<br/></strong>
</pre>
<p>We can now connect to H2O and start an instance on the cluster. Specifying <kbd>nthreads = -1</kbd> requests our instance use all CPUs on the cluster:</p>
<pre>
    <strong>&gt; localH2O = h2o.init(nthreads = -1)</strong>
</pre>
<p>The H2O function, <kbd>h2o.uploadFile()</kbd>, allows you to upload/import your file to the H2O cloud. The following functions are also available for uploads:</p>
<ul>
<li><kbd>h2o.importFolder</kbd></li>
<li><kbd>h2o.importURL</kbd></li>
<li><kbd>h2o.importHDFS</kbd></li>
</ul>
<p>It is quite simple to upload the file and a per cent indicator tracks the status:</p>
<pre>
    <strong>&gt; bank &lt;- h2o.uploadFile(path = path)</strong><br/>    <strong>  |=========================================================| 100%</strong>
</pre>
<p>The data is now in <kbd>H2OFrame</kbd>, which you can verify with <kbd>class()</kbd>, as follows:</p>
<pre>
<strong>    &gt; class(bank)</strong><br/><strong>    [1] "H2OFrame"<br/></strong>
</pre>
<p>Many of the R commands in H2O may produce a different output than what you are used to seeing. For instance, look at the structure of our data (abbreviated output):</p>
<pre>
<strong>    &gt; str(bank)</strong><br/><strong>    Class 'H2OFrame' &lt;environment: 0x0000000032d02e80&gt; </strong><br/><strong>     - attr(*, "op")= chr "Parse"</strong><br/><strong>     - attr(*, "id")= chr "bank_DL_sid_95ad_2"</strong><br/><strong>     - attr(*, "eval")= logi FALSE</strong><br/><strong>     - attr(*, "nrow")= int 4521</strong><br/><strong>     - attr(*, "ncol")= int 64</strong><br/><strong>     - attr(*, "types")=List of 64<br/></strong>
</pre>
<p>We see that it consists of 4,521 observations (nrow) and 64 columns (ncol). By the way, the <kbd>head()</kbd> and <kbd>summary()</kbd> functions work exactly the same as in regular R. Before splitting the datasets, let's have a look at our response distribution. It is the column named <strong>y</strong>:</p>
<pre>
<strong>    &gt; h2o.table(bank$y)</strong><br/><strong>       y Count</strong><br/><strong>    1 no  4000</strong><br/><strong>    2 yes  521</strong><br/><strong>    [2 rows x 2 columns]</strong>
</pre>
<p>We see that 521 of the bank's customers responded yes to the offer and 4,000 did not. This response is a bit unbalanced. Techniques that can be used to handle unbalanced response labels are discussed in the chapter on multi-class learning. In this exercise, let's see how deep learning will perform with this lack of label balance.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Create train and test datasets</h1>
            </header>

            <article>
                
<p>You can use H2O's functionality to partition the data into train and test sets. The first thing to do is create a vector of random and uniform numbers for the full data:</p>
<pre>
    <strong>&gt; rand &lt;- h2o.runif(bank, seed = 123)</strong>
</pre>
<p>You can then build your partitioned data and assign it with a desired <kbd>key</kbd> name, as follows:</p>
<pre>
    <strong>&gt; train &lt;- bank[rand &lt;= 0.7, ]<br/>    &gt; train &lt;- h2o.assign(train, key = "train")<br/>    &gt; test &lt;- bank[rand &gt; 0.7, ]<br/>    &gt; test &lt;- h2o.assign(test, key = "test")</strong>
</pre>
<p>With these created, it is probably a good idea that we have a balanced response variable between the <kbd>train</kbd> and <kbd>test</kbd> sets. To do this, you can use the <kbd>h2o.table()</kbd> function and, in our case, it would be column <span><span>64</span></span>:</p>
<pre>
<strong>    &gt; h2o.table(train[, 64])</strong><br/><strong>        y Count</strong><br/><strong>    1  no  2783</strong><br/><strong>    2 yes   396</strong><br/><strong>    [2 rows x 2 columns] </strong><br/><strong><br/>    &gt; h2o.table(test[, 64])</strong><br/><strong>        y Count</strong><br/><strong>    1  no  1217</strong><br/><strong>    2 yes   125</strong><br/><strong>    [2 rows x 2 columns]</strong>
</pre>
<p>This appears all well and good, so let's begin the modeling process:</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling</h1>
            </header>

            <article>
                
<p>As we will see, the deep learning function has quite a few arguments and parameters that you can tune. The thing that I like about the package is the ability to keep it as simple as possible and let the defaults do their thing. If you want to see all the possibilities along with the defaults, see help or run the following command:</p>
<pre>
    <strong>&gt; args(h2o.deeplearning)</strong>
</pre>
<p>Documentation on all the arguments and tuning parameters is available online at <a href="http://h2o.ai/docs/master/model/deep-learning/"><span class="URLPACKT">http://h2o.ai/docs/master/model/deep-learning/</span></a>.</p>
<p>As on a side note, you can run a demo for the various machine learning methods by just running <kbd>demo("method")</kbd>. For instance, you can go through the deep learning demo with <kbd>demo(h2o.deeplearning)</kbd>.</p>
<p>Our next goal is to tune the hyper-parameters using a random search. It takes less time than a full grid search. We will look at <kbd>tanh</kbd>, with and without dropout, three different hidden layer/neuron combinations, two different dropout ratios, and two different learning rates:</p>
<pre>
<strong>    &gt; hyper_params &lt;- list(</strong><br/><strong>       activation = c("Tanh", "TanhWithDropout"),</strong><br/><strong>       hidden = list(c(20,20),c(30, 30),c(30, 30, 30)),</strong><br/><strong>       input_dropout_ratio = c(0, 0.05),</strong><br/><strong>       rate = c(0.01, 0.25)</strong><br/><strong>     )</strong>
</pre>
<p>You now help specify the random search criteria in a list. Since we want a random search we will specify <kbd>RandomDiscrete</kbd>. A full grid search would require <kbd>Cartesian</kbd>. It is recommended to specify one or more early stopping criterion for a random search such as <kbd>max_runtime_secs</kbd>, <kbd>max_models</kbd>. We also specify here that it will stop when the top five models are withing 1% error of each other:</p>
<pre>
<strong>    &gt; search_criteria = list(</strong><br/><strong>       strategy = "RandomDiscrete", max_runtime_secs = 420,</strong><br/><strong>       max_models = 100, seed = 123, stopping_rounds = 5,</strong><br/><strong>       stopping_tolerance = 0.01</strong><br/><strong>     )</strong>
</pre>
<p>Now, this is where the magic should happen using the <kbd>h2o.grid()</kbd> function. We tell it, we want to use the deep learning algorithm, our test data, any validation data (we will use the test set), our input features, and response variable:</p>
<pre>
<strong>    &gt; randomSearch &lt;- h2o.grid(</strong><br/><strong>       algorithm = "deeplearning",</strong><br/><strong>       grid_id = "randomSearch",</strong><br/><strong>       training_frame = train,</strong><br/><strong>       validation_frame = test, </strong><br/><strong>       x = 1:63, </strong><br/><strong>       y = 64,</strong><br/><strong>       epochs = 1,</strong><br/><strong>       stopping_metric = "misclassification",</strong><br/><strong>       hyper_params = hyper_params,</strong><br/><strong>       search_criteria = search_criteria</strong><br/><strong>     )</strong><br/><strong> |===================================================================| 100%<br/></strong>
</pre>
<p>An indicator bar tracks the progress, and with this dataset, it should take less than a few seconds.</p>
<p>We now examine the results of the top five models:</p>
<pre>
<strong>    &gt; grid &lt;- h2o.getGrid("randomSearch",sort_by = "auc", decreasing = <br/>       FALSE)</strong><br/><strong><br/>    &gt; grid</strong><br/><strong>    H2O Grid Details</strong><br/><strong>    ================</strong><br/><br/><strong>    Grid ID: randomSearch </strong><br/><strong>    Used hyper parameters: </strong><br/><strong>     - activation </strong><br/><strong>     - hidden </strong><br/><strong>     - input_dropout_ratio </strong><br/><strong>     - rate </strong><br/><strong>    Number of models: 71 <br/>    Number of failed models: 0 <br/><br/>    Hyper-Parameter Search Summary: ordered by decreasing auc<br/>           activation       hidden input_dropout_ratio rate<br/>    1 TanhWithDropout [30, 30, 30]                0.05 0.25<br/>    2 TanhWithDropout [20, 20]                    0.05 0.01<br/>    3 TanhWithDropout [30, 30, 30]                0.05 0.25<br/>    4 TanhWithDropout [40, 40]                    0.05 0.01<br/>    5 TanhWithDropout [30, 30, 30]                0.0  0.25<br/>                  model_ids                 auc<br/>    1 randomSearch_model_57  0.8636778964667214<br/>    2 randomSearch_model_8   0.8623894823336072<br/>    3 randomSearch_model_10  0.856568611339359<br/>    4 randomSearch_model_39  0.8565258833196385<br/>    5 randomSearch_model_3   0.8544026294165982</strong><strong><br/></strong>
</pre>
<p>So the winning model is <kbd>#57</kbd> with activation of <kbd>TanhWithDropout</kbd>, three hidden layers with 30 neurons each, dropout ratio of 0.05, and learning rate of 0.25, which had an AUC of almost 0.864. <span> </span></p>
<p>We now have a look at our error rates in the validation/test data with a confusion matrix:</p>
<pre>
<strong>    &gt; best_model &lt;- h2o.getModel(grid@model_ids[[1]])</strong><br/><strong>    &gt; h2o.confusionMatrix(best_model, valid = T)</strong><br/><strong>    Confusion Matrix (vertical: actual; across: predicted) for max f1 @ <br/>      threshold = 0.0953170555399435:</strong><br/><strong>             no yes    Error      Rate</strong><br/><strong>    no     1128  89 0.073131 = 89/1217</strong><br/><strong>    yes      60  65 0.480000 =  60/125</strong><br/><strong>    Totals 1188 154 0.111028 = 149/1342<br/></strong>
</pre>
<p>Even though we only have 11% error, we had high errors for the <kbd>yes</kbd> label with high rates of false positives and false negatives. It possibly indicates that class imbalance may be an issue. We also have just started the hyper-parameter tuning process, so much work could be done to improve the outcome. I'll leave that task to you!</p>
<p>Now let's examine how to build a model using cross-validation. Notice how the hyper-parameters are included in the function <kbd>h2o.deeplearning()</kbd> with the exception of learning rate, which is specified as adaptive. I also included the functionality to up-sample the minority class to achieve balanced labels during training. On another note, the folds are a stratified sample based on the response variable:</p>
<pre>
<strong>  &gt; dlmodel &lt;- h2o.deeplearning(</strong><br/><strong>      x = 1:63,</strong><br/><strong>      y = 64, </strong><br/><strong>     training_frame = train,</strong><br/><strong>     hidden = c(30, 30, 30),</strong><br/><strong>     epochs = 3,</strong><br/><strong>     nfolds = 5,</strong><br/><strong>     fold_assignment = "Stratified",</strong><br/><strong>     balance_classes = T,</strong><br/><strong>     activation = "TanhWithDropout",</strong><br/><strong>     seed = 123,</strong><br/><strong>     adaptive_rate = F, </strong><br/><strong>     input_dropout_ratio = 0.05,</strong><br/><strong>     stopping_metric = "misclassification",</strong><br/><strong>     variable_importances = T</strong><br/><strong>  ) </strong>
</pre>
<p>If you call the object <kbd>dlmodel</kbd>, you will receive rather lengthy output. In this instance, let's examine the performance on the holdout folds:</p>
<pre>
<strong>    &gt; dlmodel</strong><br/><strong>    Model Details:</strong><br/><strong>    ==============<br/>    AUC:  0.8571054599<br/>    Gini: 0.7142109198<br/><br/>    Confusion Matrix (vertical: actual; across: predicted) for F1-optimal  <br/>      threshold:<br/>             no yes    Error       Rate<br/>    no     2492 291 0.104563 = 291/2783<br/>    yes     160 236 0.404040 =  160/396<br/>    Totals 2652 527 0.141869 = 451/3179<br/></strong>
</pre>
<p>Given these results, I think more tuning is in order for the hyper-parameters, particularly with the hidden layers/neurons. Examining out of sample performance is a little different, but is quite comprehensive, utilizing the <kbd>h2o.performance()</kbd> function:</p>
<pre>
<strong>    &gt; perf &lt;- h2o.performance(dlmodel, test)</strong><br/><strong>    &gt; perf</strong><br/><strong>    H2OBinomialMetrics: deeplearning</strong><br/><strong>    MSE:                  0.07237450145</strong><br/><strong>    RMSE:                 0.2690250945</strong><br/><strong>    LogLoss:              0.2399027004</strong><br/><strong>    Mean Per-Class Error: 0.2326113394</strong><br/><strong>    AUC:                  0.8319605588</strong><br/><strong>    Gini:                 0.6639211175</strong><br/><br/><strong>    Confusion Matrix (vertical: actual; across: predicted) for F1-<br/>      optimal <br/>    threshold:</strong><br/><strong>             no yes    Error      Rate</strong><br/><strong>        no 1050 167 0.137223 = 167/1217</strong><br/><strong>       yes   41  84 0.328000 =  41/125</strong><br/><strong>    Totals 1091 251 0.154993 = 208/1342</strong><br/><br/><strong>    Maximum Metrics: Maximum metrics at their respective thresholds</strong><br/><strong>      metric                      threshold    value idx</strong><br/><strong>    1 max f1                       0.323529 0.446809  62</strong><br/><strong>    2 max f2                       0.297121 0.612245 166</strong><br/><strong>    3 max f0point5                 0.323529 0.372011  62</strong><br/><strong>    4 max accuracy                 0.342544 0.906110   0</strong><br/><strong>    5 max precision                0.323529 0.334661  62</strong><br/><strong>    6 max recall                   0.013764 1.000000 355</strong><br/><strong>    7 max specificity              0.342544 0.999178   0</strong><br/><strong>    8 max absolute_mcc             0.297121 0.411468 166</strong><br/><strong>    9 max min_per_class_accuracy   0.313356 0.799507 131</strong><br/><strong>   10 max mean_per_class_accuracy  0.285007 0.819730 176</strong>
</pre>
<p>The overall error increased, but we have lower false positive and false negative rates. As before, additional tuning is required.</p>
<p>Finally, the variable importance can be produced. This is calculated based on the so-called <kbd>Gedeon</kbd> Method. Keep in mind that these results can be misleading. In the table, we can see the order of the variable importance, but this importance is subject to the sampling variation, and if you change the seed value, the order of the variable importance could change quite a bit. These are the top five variables by importance:</p>
<pre>
<strong>    &gt; dlmodel@model$variable_importances</strong><br/><strong>    Variable Importances: </strong><br/><strong>              variable relative_importance scaled_importance percentage</strong><br/><strong>    1 duration                    1.000000          1.000000   0.147006</strong><br/><strong>    2 poutcome_success            0.806309          0.806309   0.118532</strong><br/><strong>    3 month_oct                   0.329299          0.329299   0.048409</strong><br/><strong>    4 month_mar                   0.223847          0.223847   0.032907</strong><br/><strong>    5 poutcome_failure            0.199272          0.199272   0.029294<br/></strong>
</pre>
<p>With this, we have completed the introduction to deep learning in R using the capabilities of the <kbd>H2O</kbd> package. It is simple to use while offering plenty of flexibility to tune the hyperparameters and create deep neural networks. Enjoy!</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, the goal was to get you up and running in the exciting world of neural networks and deep learning. We examined how the methods work, their benefits, and their inherent drawbacks with applications to two different datasets. These techniques work well where complex, nonlinear relationships exist in the data. However, they are highly complex, potentially require a ton of hyper-parameter tuning, are the quintessential black boxes, and are difficult to interpret. We don't know why the self-driving car made a right on red, we just know that it did so properly. I hope you will apply these methods by themselves or supplement other methods in an ensemble modeling fashion. Good luck and good hunting! We will now shift gears to unsupervised learning, starting with clustering.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>