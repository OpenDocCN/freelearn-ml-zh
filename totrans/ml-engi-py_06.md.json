["```py\nconda env create –f mlewp-chapter06.yml \n```", "```py\n    from pyspark.sql import SparkSession\n\n    spark = SparkSession\\\n        .builder\\\n        .appName(\"Spark SQL Example\")\\\n        .config(\"spark.some.config.option\", \"some-value\")\\\n        .getOrCreate() \n    ```", "```py\n    spark.sql('''select * from data_table''') \n    ```", "```py\n    data = spark.read.format(\"csv\")\\\n        .option(\"sep\", \";\")\\\n        .option(\"inferSchema\", \"true\")\\\n        .option(\"header\", \"true\").load(\n        \"data/bank/bank.csv\") \n    ```", "```py\n    data.createOrReplaceTempView('data_view') \n    ```", "```py\n    new_data = spark.sql('''select …''') \n    ```", "```py\n    from pyspark.sql import SparkSession\n    from pyspark import SparkContext\n    from pyspark.sql import functions as f\n\n    sc = SparkContext(\"local\", \"Ch6BasicExampleApp\")\n    # Get spark session\n    spark = SparkSession.builder.getOrCreate()\n    # Get the data and place it in a spark dataframe\n    data = spark.read.format(\"csv\").option(\"sep\", \";\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(\n            \"data/bank/bank.csv\") \n    ```", "```py\n    |-- age: integer (nullable = true)\n    |-- job: string (nullable = true)\n    |-- marital: string (nullable = true)\n    |-- education: string (nullable = true)\n    |-- default: string (nullable = true)\n    |-- balance: integer (nullable = true)\n    |-- housing: string (nullable = true)\n    |-- loan: string (nullable = true)\n    |-- contact: string (nullable = true)\n    |-- day: integer (nullable = true)\n    |-- month: string (nullable = true)\n    |-- duration: integer (nullable = true)\n    |-- campaign: integer (nullable = true)\n    |-- pdays: integer (nullable = true)\n    |-- previous: integer (nullable = true)\n    |-- poutcome: string (nullable = true)\n    |-- y: string (nullable = true) \n    ```", "```py\n    import datetime\n\n    def month_as_int(month):\n        month_number = datetime.datetime.strptime(month, \"%b\").month\n        return month_number \n    ```", "```py\n    from pyspark.sql.types import StringType\n\n    spark.udf.register(\"monthAsInt\", month_as_int, StringType()) \n    ```", "```py\n    data.createOrReplaceTempView('bank_data_view')\n    spark.sql('''\n    select *, monthAsInt(month) as month_as_int from bank_data_view\n    ''').show() \n    ```", "```py\n    from pyspark.sql.functions import udf\n\n    month_as_int_udf = udf(month_as_int, StringType())\n    df = spark.table(\"bank_data_view\")\n    df.withColumn('month_as_int', month_as_int_udf(\"month\")).show() \n    ```", "```py\n    @udf(\"string\")\n    def month_as_int_udf(month):\n        month_number = datetime.datetime.strptime(month, \"%b\").month\n        return month_number\n    df.withColumn('month_as_int', month_as_int_udf(\"month\")).show() \n    ```", "```py\n    import sklearn.svm\n    import sklearn.datasets\n\n    clf = sklearn.svm.SVC()\n    X, y = sklearn.datasets.load_wine(return_X_y=True) clf.fit(X, y) \n    ```", "```py\n    df = spark.createDataFrame(X.tolist()) \n    ```", "```py\n    import pandas as pd\n    from pyspark.sql.types import IntegerType\n    from pyspark.sql.functions import pandas_udf\n\n    @pandas_udf(returnType=IntegerType())\n    def predict_pd_udf(*cols):\n        X = pd.concat(cols, axis=1)\n        return pd.Series(clf.predict(X)) \n    ```", "```py\n    col_names = ['_{}'.format(x) for x in range(1, 14)]\n    df_pred = df.select('*', predict_pd_udf(*col_names).alias('class')) \n    ```", "```py\n    def model_bank_data(spark, input_path, output_path):\n        data = spark.read.format(\"csv\")\\\n            .option(\"sep\", \";\")\\\n            .option(\"inferSchema\", \"true\")\\\n            .option(\"header\", \"true\")\\\n            .load(input_path)\n        data = data.withColumn('label', f.when((f.col(\"y\") == \"yes\"),\n                                                1).otherwise(0))\n        # ...\n        data.write.format('parquet')\\\n            .mode('overwrite')\\\n            .save(output_path) \n    ```", "```py\n    def main():\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\n            '--input_path', help='S3 bucket path for the input data.\n            Assume to be csv for this case.'\n        )\n        parser.add_argument(\n            '--output_path', help='S3 bucket path for the output data.\n            Assume to be parquet for this case'\n        )\n        args = parser.parse_args()\n        # Create spark context\n        sc = SparkContext(\"local\", \"pipelines\")\n        # Get spark session\n        spark = SparkSession\\\n            .builder\\\n            .appName('MLEWP Bank Data Classifier EMR Example')\\\n            .getOrCreate()\n        model_bank_data(\n            spark,\n            input_path=args.input_path,,\n            output_path=args.output_path\n        ) \n    ```", "```py\n    import argparse\n    from pyspark.sql import SparkSession\n    from pyspark import SparkContext\n    from pyspark.sql import functions as f\n    from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n    from pyspark.ml.feature import StandardScaler, OneHotEncoder, StringIndexer, Imputer, VectorAssembler\n    from pyspark.ml import Pipeline, PipelineModel\n    from pyspark.ml.classification import LogisticRegression\n\n    def model_bank_data(spark, input_path, output_path):\n        ...\n    def main():\n        ...\n    if __name__ == \"__main__\":\n        main() \n    ```", "```py\n    aws emr list-clusters --cluster-states WAITING \n    ```", "```py\n    aws emr add-steps\\\n    --region eu-west-1 \\\n    --cluster-id <CLUSTER_ID> \\\n    --steps Type=Spark,Name=\"Spark Application Step\",ActionOnFailure=CONTINUE,\\\n    Args=[--files,s3://mlewp-ch6-emr-examples/spark_example_emr.py,\\\n    --input_path,s3://mlewp-ch6-emr-examples/bank.csv,\\\n    --output_path,s3://mleip-emr-ml-simple/results.parquet] \n    ```", "```py\n    Which template source would you like to use?\n            1 - AWS Quick Start Templates\n            2 - Custom Template Location\n    Choice: 1 \n    ```", "```py\n    Choose an AWS Quick Start application template\n        1 - Hello World Example\n        2 - Data processing\n        3 - Hello World Example with Powertools for AWS Lambda\n        4 - Multi-step workflow\n        5 - Scheduled task\n        6 - Standalone function\n        7 - Serverless API\n        8 - Infrastructure event management\n        9 - Lambda Response Streaming\n        10 - Serverless Connector Hello World Example\n        11 - Multi-step workflow with Connectors\n        12 - Full Stack\n        13 - Lambda EFS example\n        14 - DynamoDB Example\n        15 - Machine Learning\n    Template: \n    ```", "```py\n    Which runtime would you like to use?\n        1 - python3.9\n        2 - python3.8\n        3 - python3.10\n    Runtime: \n    ```", "```py\n    Based on your selections, the only Package type available is Image.\n    We will proceed to selecting the Package type as Image.\n    Based on your selections, the only dependency manager available is pip.\n    We will proceed copying the template using pip.\n    Select your starter template\n        1 - PyTorch Machine Learning Inference API\n        2 - Scikit-learn Machine Learning Inference API\n        3 - Tensorflow Machine Learning Inference API\n        4 - XGBoost Machine Learning Inference API\n    Template: 4 \n    ```", "```py\n    Would you like to enable X-Ray tracing on the function(s) in your application? [y/N]: N\n    Would you like to enable monitoring using CloudWatch Application Insights?\n    For more info, please view: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-application-insights.xhtml [y/N]: N\n    Project name [sam-app]: mlewp-sam-ml-api\n    Cloning from https://github.com/aws/aws-sam-cli-app-templates (process may take a moment) \n    ```", "```py\n     -----------------------\n        Generating application:\n        -----------------------\n        Name: mlewp-sam-ml-api\n        Base Image: amazon/python3.10-base\n        Architectures: x86_64\n        Dependency Manager: pip\n        Output Directory: .\n        Configuration file: mlewp-sam-ml-api/samconfig.toml\n        Next steps can be found in the README file at mlewp-sam-ml-api/README.md\n    Commands you can use next\n    =========================\n    [*] Create pipeline: cd mlewp-sam-ml-api && sam pipeline init --bootstrap\n    [*] Validate SAM template: cd mlewp-sam-ml-api && sam validate\n    [*] Test Function in the Cloud: cd mlewp-sam-ml-api && sam sync --stack-name {stack-name} --watch \n    ```", "```py\n    cd mlewp-sam-ml-api\n    ls\n    tree\n    ├── README.md\n    ├── __init__.py\n    ├── app\n    │   ├── Dockerfile\n    │   ├── __init__.py\n    │   ├── app.py\n    │   ├── model\n    │   └── requirements.txt\n    ├── events\n    │   └── event.json\n    ├── samconfig.toml\n    └── template.yaml\n    3 directories, 10 files \n    ```", "```py\n    sam build \n    ```", "```py\n    Build Succeeded\n    Built Artifacts  : .aws-sam/build\n    Built Template   : .aws-sam/build/template.yaml\n    Commands you can use next\n    =========================\n    [*] Validate SAM template: sam validate\n    [*] Invoke Function: sam local invoke\n    [*] Test Function in the Cloud: sam sync --stack-name {{stack-name}} --watch\n    [*] Deploy: sam deploy --guided \n    ```", "```py\n    sam local invoke  --event events/event.json\n    Invoking Container created from inferencefunction:python3.10-v1\n    Building image.................\n    Using local image: inferencefunction:rapid-x86_64.\n    START RequestId: de4a2fe1-be86-40b7-a59d-151aac19c1f0 Version: $LATEST\n    END RequestId: de4a2fe1-be86-40b7-a59d-151aac19c1f0\n    REPORT RequestId: de4a2fe1-be86-40b7-a59d-151aac19c1f0    Init Duration: 1.30 ms    Duration: 1662.13 ms    Billed Duration: 1663 ms    Memory Size: 5000 MB    Max Memory Used: 5000 MB\n    {\"statusCode\": 200, \"body\": \"{\\\"predicted_label\\\": 3}\"}% \n    ```", "```py\n    sam deploy --guided\n    Configuring SAM deploy\n    ======================\n        Looking for config file [samconfig.toml] :  Found\n        Reading default arguments  :  Success\n        Setting default arguments for 'sam deploy'\n        =========================================\n        Stack Name [mlewp-sam-ml-api]: \n    ```", "```py\n    Configuring SAM deploy\n    ======================\n        Looking for config file [samconfig.toml] :  Found\n        Reading default arguments  :  Success\n        Setting default arguments for 'sam deploy'\n        =========================================\n        Stack Name [mlewp-sam-ml-api]:\n        AWS Region [eu-west-2]:\n        #Shows you resources changes to be deployed and require a 'Y' to initiate deploy\n        Confirm changes before deploy [Y/n]: y\n        #SAM needs permission to be able to create roles to connect to the resources in your template\n        Allow SAM CLI IAM role creation [Y/n]: y\n        #Preserves the state of previously provisioned resources when an operation fails\n        Disable rollback [y/N]: y\n        InferenceFunction has no authentication. Is this okay? [y/N]: y\n        Save arguments to configuration file [Y/n]: y\n        SAM configuration file [samconfig.toml]:\n        SAM configuration environment [default]: \n    ```", "```py\n    CloudFormation outputs from deployed stack\n    ---------------------------------------------------------------------------------------------------------------------\n    Outputs\n    ---------------------------------------------------------------------------------------------------------------------\n    Key                 InferenceApi\n    Description         API Gateway endpoint URL for Prod stage for Inference function\n    Value               https://8qg87m9380.execute-api.eu-west-2.\n    amazonaws.com/Prod/classify_digit/\n    Key                 InferenceFunctionIamRole\n    Description         Implicit IAM Role created for Inference function\n    Value               arn:aws:iam::508972911348:role/mlewp-sam-ml-api-InferenceFunctionRole-1UE509ZXC1274\n    Key                 InferenceFunction\n    Description         Inference Lambda Function ARN\n    Value               arn:aws:lambda:eu-west-2:508972911348:function:mlewp-sam-ml-api-InferenceFunction-ueFS1y2mu6Gz\n    --------------------------------------------------------------------------------------------------------------------- \n    ```", "```py\n    curl --location --request POST 'https://8qg87m9380.execute-api.eu-west-2.amazonaws.com/Prod/classify_digit/' \\\n    --header 'Content-Type: raw/json' \\\n    --data '<ENCODED_IMAGE_STRING>' \n    ```", "```py\n    {\n        \"predicted_label\": 3\n    } \n    ```", "```py\ndef add(int: x, int: y) -> int:\n    return x+y \n```", "```py\nimport ray\n\n@remote\ndef add(int: x, int: y) -> int:\n    return x+y\nadd.remote() \n```", "```py\nimport ray\n\n@ray.remote\nclass Counter(object):\n    def __init__(self):\n        self.value = 0\n    def increment(self):\n        self.value += 1\n        return self.value\n    def get_counter(self):\n        return self.value\n# Create an actor from this class.\ncounter = Counter.remote() \n```", "```py\nimport ray\n\nnumerical_array = np.arange(1,10e7)\nobj_numerical_array = ray.put(numerical_array)\nnew_numerical_array = 0.5*ray.get(obj_numerical_array) \n```", "```py\npip install \"ray[air, tune, dashboard]\" \npip install xgboost\npip install xgboost_ray \n```", "```py\npoetry add \"ray[air, tune, dashboard]\"\npoetry add xgboost\npoetry add pytorch \n```", "```py\nimport ray\n\ndataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_\n                             cancer.csv\")\ntrain_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\ntest_dataset = valid_dataset.drop_columns(cols=[\"target\"]) \n```", "```py\nfrom ray.data.preprocessors import StandardScaler\n\npreprocessor = StandardScaler(columns=[\"mean radius\", \"mean texture\"]) \n```", "```py\nfrom ray.air.config import ScalingConfig\nfrom ray.train.xgboost import XGBoostTrainer\n\ntrainer = XGBoostTrainer(\n    scaling_config=ScalingConfig(...),\n    label_column=\"target\",\n    num_boost_round=20,\n    params={...},\n    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n    preprocessor=preprocessor,\n)\nresult = trainer.fit() \n```", "```py\n{'train-logloss': 0.01849572773292735, \n'train-error': 0.0, 'valid-logloss': 0.089797893552767,\n'valid-error': 0.04117647058823529, \n'time_this_iter_s': 0.019704103469848633, \n'should_checkpoint': True, \n'done': True, \n'timesteps_total': None, \n'episodes_total': None, \n'training_iteration': 21, \n'trial_id': '6ecab_00000', \n'experiment_id': '2df66fa1a6b14717bed8b31470d386d4', \n'date': '2023-03-14_20-33-17', \n'timestamp': 1678825997, \n'time_total_s': 6.222438812255859, \n'pid': 1713, \n'hostname': 'Andrews-MacBook-Pro.local', \n'node_ip': '127.0.0.1', \n'config': {}, \n'time_since_restore': 6.222438812255859,\n'timesteps_since_restore': 0, \n'iterations_since_restore': 21, \n'warmup_time': 0.003551006317138672, 'experiment_tag': '0'} \n```", "```py\nscaling_config=ScalingConfig(\n    num_workers=2,\n    use_gpu=False,\n    _max_cpu_fraction_per_node=0.9,\n) \n```", "```py\nparams={\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": [\"logloss\", \"error\"],\n} \n```", "```py\nscaling_config=ScalingConfig(\n    num_workers=20,\n    use_gpu=False,\n    _max_cpu_fraction_per_node=0.9,\n) \n```", "```py\nfrom starlette.requests import Request\nimport ray\nfrom ray import serve \n```", "```py\n@serve.deployment\nclass Classifier:\n    def __init__(self):\n        self.model = get_model()\n    async def __call__(self, http_request: Request) -> str:\n        request_payload = await http_request.json()\n        input_vector = [\n            request_payload[\"mean_radius\"],\n            request_payload[\"mean_texture\"]\n        ]\n        classification = self.model.predict([input_vector])[0]\n        return {\"result\": classification} \n```"]