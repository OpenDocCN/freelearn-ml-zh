<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Image Filtering and Transformations in OpenCV</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image Filtering and Transformations in OpenCV</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, you will learn the basic building blocks for computer vision applications. We are already familiar with digital cameras and smartphone devices with auto image enhancement or color adjustments to make our photographs more pleasing. The techniques behind these originated long ago and have come through several iterations to become better and faster. Many of the techniques explained in this chapter also become major preprocessing techniques for object detection and object classification tasks introduced later. Hence, it is very important to study these techniques and understand their applications.</p>
<p class="mce-root">You will study the basis for these applications with several techniques for filtering an image linearly as well as non-linearly.</p>
<p class="mce-root">Later in the chapter, you will also study transformation techniques and downsampling techniques. A code is provided together with explanations and sample output. Readers are encouraged to write the code and experiment with changing parameters to understand several concepts. There are several colored image results in this chapter, for an effective understanding download the images from book's website.&#160;</p>
<p>In this chapter, we will learn the following topics:</p>
<ul>
<li>Datasets and libraries required</li>
<li>Image manipulation</li>
<li>Introduction to filters</li>
<li>Transformations on an image</li>
<li>Image pyramids</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Datasets and libraries required</h1>
                </header>
            
            <article>
                
<p>We will be using a sample image for most of this task. However, you can try the code with any other image or also use a webcam to see live results. The libraries used in this chapter are <kbd>OpenCV</kbd>, <kbd>NumPy</kbd>, and <kbd>matplotlib</kbd>. Even if you are not acquainted with libraries, you can still understand the code and implement them. There are also remarks for special cases when using a Jupyter notebook for the code written here:</p>
<pre>import numpy as np <br/>import matplotlib.pyplot as plt <br/>import cv2 <br/># With jupyter notebook uncomment below line <br/># %matplotlib inline <br/># This plots figures inside the notebook<span><br/></span></pre>
<p>The sample image used in this chapter can be loaded as follows:</p>
<pre># read an image <br/>img = cv2.imread('flower.png')<span><br/></span></pre>
<p>This image can be plotted either using <kbd>OpenCV</kbd> or <kbd>matplotlib</kbd> libraries. We will be using <kbd>matplotlib</kbd> for the majority of plots as this will be beneficial in plotting other kinds of data as well in later chapters. A plotting function for a colored image read in <kbd>OpenCV</kbd> is defined as follows:</p>
<pre>def plot_cv_img(input_image):     <br/>    """     <br/>    Converts an image from BGR to RGB and plots     <br/>    """   <br/>    # change color channels order for matplotlib     <br/>    plt.imshow(cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB))          <br/><br/>    # For easier view, turn off axis around image     <br/>    plt.axis('off')  <br/>    plt.show()<span><br/></span></pre>
<p>The previously read image can be plotted as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="233" width="351" class="alignnone size-full wp-image-445 image-border" src="images/f28cc829-37ea-4a73-b5bb-4a0074939687.png"/></div>
<p>The image in Python is a NumPy array, so all of the array operations are still valid in the image. For example, you can crop an image by using array slicing:</p>
<pre>plot_cv_img(img[100:400, 100:400]) </pre>
<p class="mce-root">This results in the following:</p>
<div class="CDPAlignCenter CDPAlign"><img height="228" width="227" class="alignnone size-full wp-image-446 image-border" src="images/3dfa0d43-e7b1-4684-8701-ab22ab36d5ef.png"/></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image manipulation</h1>
                </header>
            
            <article>
                
<p>As explained in previous chapters, an image&#160;<span class="md-expand">in the digital domain, such as on a computer, is made up of a grid-like structure with each grid cell termed a <strong>pixel</strong>. These pixels store a value representing information about the image. For a simple grayscale image, these pixels store an integer with range [0, 255].&#160;</span><span class="">Changing these pixel values also changes the image. One of the basic image manipulation techniques is modifying pixel values.</span></p>
<p>Let's start by displaying what is inside an image at pixel level. For simplicity, we will do analysis on a grayscale image:</p>
<pre># read an image <br/>img = cv2.imread('gray_flower.png')</pre>
<p class="mce-root">The earlier&#160;code reads a grayscale image from a file, in this case the image is in PNG&#160;format. We can also convert from one type of image color format to another. In this case, to convert a colored image to grayscale,<span>&#160;</span><kbd>OpenCV</kbd>&#160;provides functions as follows:</p>
<pre># converts rgb image to grayscale <br/>gray_output = cv2.cvtColor(color_input, cv2.COLOR_BGR2GRAY)</pre>
<p>The previously shown code for displaying an image takes only a colored image as input, so to display a grayscale image there needs to be some modification:</p>
<pre>def plot_cv_img(input_image,is_gray=False): <br/>    """ <br/>    Takes in image with flag showing, if gray or not<br/>    Plots image using matplotlib<br/>    """ <br/>    # change color channels order for matplotlib <br/>    if not is_gray:<br/>        plt.imshow(cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB))<br/>    else:<br/>        plt.imshow(input_image, cmap='gray')<br/><br/>    # For easier view, turn off axis around image <br/>    plt.axis('off')<br/>    plt.show()</pre>
<p>The output of the previous code is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="310" width="466" class="alignnone size-full wp-image-447 image-border" src="images/ddb8d425-b42f-4561-886a-0918e0a1e6f9.png"/></div>
<p>We can display a small patch from this image as follows, that shows pixel values:</p>
<pre># read the image<br/>flower = cv2.imread('../figures/flower.png')<br/><br/># convert to gray scale <br/>gray_flower = cv2.cvtColor(flower, cv2.COLOR_BGR2GRAY)<br/><br/># take out a patch of pixels<br/>patch_gray = gray_flower[250:260, 250:260]<br/><br/>#plot the patch as well as print the values <br/>plot_cv_img(patch_gray, is_gray=True)<br/>print(patch_gray)</pre>
<p>This will produce an image of the patch and prints out the value extracted in that patch:</p>
<div class="CDPAlignCenter CDPAlign"><img height="210" width="206" class="alignnone size-full wp-image-521 image-border" src="images/32168a89-43ce-44a6-81bc-0dda59da5563.png"/></div>
<p>Corresponding values are as follows, the lower values represent more darker regions:&#160;</p>
<pre>[[142 147 150 154 164 113  39  40  39  38]
 [146 145 148 152 156  78  42  41  40  40]
 [147 148 147 147 143  62  42  42  44  44]
 [155 148 147 145 142  91  42  44  43  44]
 [156 154 149 147 143 113  43  42  42  48]
 [155 157 152 149 149 133  68  45  47  50]
 [155 154 155 150 152 145  94  48  48  48]
 [152 151 153 151 152 146 106  51  50  47]
 [155 157 152 150 153 145 112  50  49  49]
 [156 154 152 151 149 147 115  49  52  52]]</pre>
<p>&#160;</p>
<p>These are the intensities for a pixel and is represented as a two-dimensional array. The range of each pixel value is 0-255. In order to modify image, we change these pixel values. A simple filtering for images is applying point operation targeted to multiply and add constants to each pixel values. We will see this type of filters in detail in the next section.</p>
<p>In this section, we saw basic IO extending our discussion from <a href="prac-cv_ch01.html" target="_blank">Chapter 1</a>,&#160;<em>A Fast Introduction to Computer Vision</em>. In further section, we will see how to modify these using filters which are used in image editing applications on smartphones, desktops and even on social media applications.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to filters</h1>
                </header>
            
            <article>
                
<p>Filters are operations on an image to modify them so that they are usable for other computer vision tasks or to give us required information. These perform various functions such as removing noise from an image, extracting edges in an image, blurring an image, removing unwanted objects etc. We will see their implementations and understand the results.</p>
<p>Filtering techniques are necessary because there are several factors that may lead to noise in an image or undesired information in an image. Taking a picture in sunlight, induces lots of bright and dark areas in the image or an improper environment like night time, the image captured by a camera may contain a lot of noise. Also, in cases of unwanted objects or colors in an image, these are also considered noise.</p>
<p>An example of salt and pepper noise looks like the following:</p>
<div class="CDPAlignCenter CDPAlign"><img height="250" width="371" class="alignnone size-full wp-image-448 image-border" src="images/832dbbce-3b05-4208-9746-619c5d1d7f35.png"/></div>
<p>The preceding image can be easily generated using OpenCV as follows:</p>
<pre># initialize noise image with zeros<br/>noise = np.zeros((400, 600))<br/><br/># fill the image with random numbers in given range<br/>cv2.randu(noise, 0, 256)</pre>
<p>Let's add weighted noise to a grayscale image (on the left) so the resulting image will look like the one on the right:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="183" width="554" class="alignnone size-full wp-image-450 image-border" src="images/2e1e664e-4a85-47ec-8455-00cab6441e06.png"/></div>
<p>The code for this is as follows:</p>
<pre># add noise to existing image <br/>noisy_gray = gray + np.array(0.2*noise, dtype=np.int)</pre>
<p>Here, <kbd>0.2</kbd> is used as parameter, increase or decrease the value to create different intensity noise.&#160;</p>
<p class="mce-root">In several applications, noise plays an important role in improving a system's capabilities, especially when we will use Deep Learning based models in the upcoming chapters. It is quite crucial for several applications, to know how robust the application is against noise becomes very important. As an example, we would want the model designed for applications like image classification to work with noisy images as well, hence noise is deliberately added in the images to test the application precision.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear filters</h1>
                </header>
            
            <article>
                
<p>To begin with, the simplest kind of filter is a point operator, where each pixel value is multiplied by a scalar value. This operation can be written as follows:</p>
<p style="padding-left: 150px">&#160;&#160;<img height="22" width="157" class="alignnone size-full wp-image-522 image-border" src="images/acf011fc-6afd-45e3-8d9c-5a9ba99c771f.png"/></p>
<p>Here:</p>
<ul>
<li>The input image is F and the value of pixel at <em>(i,j)</em> is denoted as <em>f(i,j)</em></li>
<li>The output image is G and the value of pixel at&#160;<em>(i,j)&#160;</em>is denoted as <em>g(i,j)</em></li>
<li><em>K</em> is scalar constant</li>
</ul>
<p class="mce-root">Such an operation on an image is termed a <strong>linear</strong> <strong>filter</strong>. There are many more kinds of linear filters which you will be reading about further in this section. In addition to multiplication by a scalar value, each pixel can also be increased or decreased by a constant value. So overall point operation can be written as follows:</p>
<p style="padding-left: 150px">&#160;&#160;<img height="22" width="192" class="alignnone size-full wp-image-524 image-border" src="images/60552f1f-9e00-47fc-823f-a5057422b8e4.png"/></p>
<p class="mce-root">This operation can be applied both to grayscale images and RGB images. For RGB images, each channel will be modified with this operation separately. The following is the result of varying both <em>K</em> and <em>L</em>. The first image is input on the left. In the second image, <kbd><em>K</em>=0.5</kbd> and <kbd><em>L</em>=0.0</kbd>, while in the third image, <em>K</em> is set to <kbd>1.0</kbd> and <em>L</em> is <kbd>10</kbd>. For the final image on the right, <kbd><em>K</em>=0.7</kbd> and <kbd><em>L</em>=25</kbd>. As you can see, varying <em>K</em> changes the brightness of the image and varying <em>L</em> changes the contrast of the image:</p>
<div class="CDPAlignCenter CDPAlign"><img height="251" width="1500" class="alignnone size-full wp-image-452 image-border" src="images/3550feb8-129d-42f3-94a1-27361f68de4d.png"/></div>
<p>This image can be generated with the following code:</p>
<pre>import numpy as np <br/>import matplotlib.pyplot as plt <br/>import cv2 <br/><br/>def point_operation(img, K, L):<br/>    """<br/>    Applies point operation to given grayscale image<br/>    """<br/>    img = np.asarray(img, dtype=np.float)<br/>    img = img*K + L<br/>    # clip pixel values<br/>    img[img &gt; 255] = 255 <br/>    img[img &lt; 0] = 0<br/>    return np.asarray(img, dtype = np.int)<br/><br/>def main():<br/>    # read an image <br/>    img = cv2.imread('../figures/flower.png')<br/>    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/>    <br/>    # k = 0.5, l = 0<br/>    out1 = point_operation(gray, 0.5, 0)<br/><br/>    # k = 1., l = 10<br/>    out2 = point_operation(gray, 1., 10)<br/><br/>    # k = 0.8, l = 15<br/>    out3 = point_operation(gray, 0.7, 25)<br/>    <br/>    res = np.hstack([gray,out1, out2, out3])<br/>    plt.imshow(res, cmap='gray')<br/>    plt.axis('off')<br/><br/>    plt.show()<br/><br/><br/>if __name__ == '__main__':<br/>    main()</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">2D linear filters</h1>
                </header>
            
            <article>
                
<p>While the preceding filter is a point-based filter, image pixels have information around the pixel as well. In the previous image of the flower, the pixel values in the petal are all yellow. If we choose a pixel of the petal and move around, the values will be quite close. This gives some more information about the image. To extract this information in filtering, there are several neighborhood filters.&#160;</p>
<p>In neighborhood filters, there is a kernel matrix which captures local region information around a pixel. To explain these filters, let's start with an input image, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="147" width="145" class="alignnone size-full wp-image-453 image-border" src="images/eef640fc-0d99-4136-bd18-bce5b1f26cc9.png"/></div>
<p>This is a simple binary image of the number 2. To get certain information from this image, we can directly use all the pixel values. But instead, to simplify, we can apply filters on this. We define a matrix smaller than the given image which operates in the neighborhood of a target pixel. This matrix is termed <strong>kernel</strong>;&#160;an example is given as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="118" width="115" class="alignnone size-full wp-image-454 image-border" src="images/2b892f25-b7e8-40cb-a8ad-18db2bf256fe.png"/></div>
<p>The operation is defined first by superimposing the kernel matrix on the original image, then taking the product of the corresponding pixels and returning a summation of all the products. In the following figure, the lower 3 x 3 area in the original image is superimposed with the given kernel matrix and the corresponding pixel values from the kernel and image are multiplied. The resulting image is shown on the right and is the summation of all the previous pixel products:&#160;</p>
<div class="CDPAlignCenter CDPAlign"><img height="183" width="478" class="alignnone size-full wp-image-455 image-border" src="images/66b04090-921d-488b-b5c6-37f1fa8e8e7c.png"/></div>
<p>This operation is repeated by sliding the kernel along image rows and then image columns. This can be implemented as in following code. We will see the effects of applying this on an image in coming sections:&#160;</p>
<pre># design a kernel matrix, here is uniform 5x5<br/>kernel = np.ones((5,5),np.float32)/25<br/> <br/># apply on the input image, here grayscale  input<br/>dst = cv2.filter2D(gray,-1,kernel)</pre>
<p>However, as you can see previously, the corner pixel will have a drastic impact and results in a smaller image because the kernel, while overlapping, will be outside the image region. This causes a black region, or holes, along with the boundary of an image. To rectify this, there are some common techniques used:</p>
<ul>
<li>Padding the corners with constant values maybe 0 or 255, by default <kbd>OpenCV</kbd> will use this.</li>
<li>Mirroring the pixel along the edge to the external area&#160;</li>
<li>Creating a pattern of pixels around the image</li>
</ul>
<p>The choice of these will depend on the task at hand. In common cases, padding will be able to generate satisfactory results.&#160;</p>
<p>The effect of the kernel is most crucial as changing these values changes the output significantly. We will first see simple kernel-based filters and also see their effects on the output when changing the size.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Box filters</h1>
                </header>
            
            <article>
                
<p>This filter averages out the pixel value as the kernel matrix is denoted as follows:</p>
<p style="padding-left: 180px"><img height="67" width="86" src="images/a69e58ed-7853-40df-b3e2-5d7f5ce62526.png"/></p>
<p>Applying this filter results in blurring the image. The results are as shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="155" width="394" src="images/1454f664-20c7-49a1-b53b-ac0a35a0153f.png"/></div>
<p>In frequency domain analysis of the image, this filter is a low pass filter. The frequency domain analysis is done using Fourier transformation of the image, which is beyond the scope of this introduction. We can see on changing the kernel size, the image gets more and more blurred:</p>
<div class="CDPAlignCenter CDPAlign"><img height="138" width="375" src="images/f2c49b04-1730-40ea-b3a4-ef664ec4eaec.png"/></div>
<p>As we increase the size of the kernel, we can observe that resulting image gets more blurred. This is due to averaging out of peak values in the small neighborhood where the kernel is applied. The result for applying kernel of size 20 x 20 can be seen in the following image:</p>
<div class="CDPAlignCenter CDPAlign"><img height="144" width="374" src="images/7d799059-e43e-4a7c-bded-a7f40456a585.png"/>&#160;</div>
<p>However, if we use a very small filter of size (3, 3) there is negligible effect on the output, due to the fact that the kernel size is quite small compared to the photo size. In most applications, kernel size is heuristically set according to image size:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="154" width="395" class="alignnone size-full wp-image-456 image-border" src="images/524cdf06-249f-425f-b360-aaa7e1cd0e85.png"/></div>
<p>The complete code to generate box filtered photos is as follows:</p>
<pre>def plot_cv_img(input_image, output_image): <br/>    """ <br/>    Converts an image from BGR to RGB and plots <br/>    """ <br/><br/>    fig, ax = plt.subplots(nrows=1, ncols=2)<br/><br/>    ax[0].imshow(cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)) <br/>    ax[0].set_title('Input Image')<br/>    ax[0].axis('off')<br/>    <br/>    ax[1].imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)) <br/>    ax[1].set_title('Box Filter (5,5)')<br/>    ax[1].axis('off') <br/>    plt.show()<br/>    <br/><br/>def main():<br/>    # read an image <br/>    img = cv2.imread('../figures/flower.png')<br/>    <br/>     <br/>    # To try different kernel, change size here.  <br/>    kernel_size = (5,5)<br/>    <br/>    # opencv has implementation for kernel based box blurring<br/>    blur = cv2.blur(img,kernel_size)<br/> <br/>    # Do plot<br/>    plot_cv_img(img, blur)<br/><br/>if __name__ == '__main__':<br/>    main()</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Properties of linear filters</h1>
                </header>
            
            <article>
                
<p>Several computer vision applications are composed of step by step transformations of an input photo to output. This is easily done due to several properties associated with a common type of filters, that is, linear filters:</p>
<ul>
<li>The linear filters are commutative such that we can perform multiplication operations on filters in any order and the result still remains the same:</li>
</ul>
<p style="padding-left: 210px"><em>a * b = b * a</em>&#160;</p>
<ul>
<li>They are associative in nature, which means the order of applying the filter does not effect the outcome:</li>
</ul>
<p style="padding-left: 210px"><em>(a * b) * c = a * (b * c)</em></p>
<ul>
<li>Even in cases of summing two filters, we can perform the first summation and then apply the filter, or we can also individually apply the filter and then sum the results. The overall outcome still remains the same:</li>
</ul>
<p style="padding-left: 210px"><em>b = (k+l) * a</em></p>
<ul>
<li>Applying a scaling factor to one filter and multiplying to another filter is equivalent to first multiplying both filters and then applying scaling factor</li>
</ul>
<p>These properties play a significant role later, when we look at computer vision tasks such as object detection, segmentation, and so on. A suitable combination of these filters enhances the quality of information extraction and as a result, improves the accuracy.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Non-linear filters&#160;</h1>
                </header>
            
            <article>
                
<p>While in many cases linear filters are sufficient to get the required results, in several other use cases performance can be significantly increased with the use of non-linear filters. As the name suggests, these filters are composed of more complex operations, have some kind of non-linearity, and as a result these filters do not follow some or all of the properties of linear filters.</p>
<p>We will understand these filters with implementations.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Smoothing a photo&#160;</h1>
                </header>
            
            <article>
                
<p>Applying a box filter with hard edges doesn't result in a smooth blur on the output photo.</p>
<p>To improve this, the filter can be made smoother around the edges. One of the popular such filters is a <strong>Gaussian filter</strong>. This is a non-linear filter which enhances the effect of the center pixel and gradually reduces the effects as the pixel gets farther from the center. Mathematically, a Gaussian function is given as:</p>
<p style="padding-left: 180px"><img height="45" width="192" class="alignnone size-full wp-image-525 image-border" src="images/3c1a20e0-6024-4028-9d72-f812f13151ff.png"/></p>
<p>where&#160;μ is mean and&#160;σ is variance.&#160;</p>
<p>An example kernel matrix for this kind of filter in a two-dimensional discrete domain is given as follows:</p>
<p style="padding-left: 180px"><img height="95" width="162" src="images/fa06aa76-4958-476d-836d-aab5b7801689.png"/></p>
<p>This two-dimensional array is used in normalized form and effect of this filter also depends on its width by changing the kernel width has varying effects on the output as discussed in further section.&#160;Applying Gaussian kernel as filter removes high-frequency components which results in removing strong edges and hence a blurred photo:</p>
<div class="CDPAlignCenter CDPAlign"><img height="239" width="679" src="images/560325e5-14f4-44f4-ac22-76439f27394b.png"/></div>
<p>While this filter performs better blurring than a box filter, the implementation is also quite simple with OpenCV:</p>
<pre>def plot_cv_img(input_image, output_image): <br/>    """ <br/>    Converts an image from BGR to RGB and plots <br/>    """ <br/>    fig, ax = plt.subplots(nrows=1, ncols=2)<br/>    <br/>    ax[0].imshow(cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)) <br/>    ax[0].set_title('Input Image')<br/>    ax[0].axis('off')<br/>    <br/>    ax[1].imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)) <br/>    ax[1].set_title('Gaussian Blurred')<br/>    ax[1].axis('off')<br/>    plt.show()<br/>    <br/><br/>def main():<br/>    # read an image <br/>    img = cv2.imread('../figures/flower.png')<br/>    <br/>    # apply gaussian blur,<br/>    <br/>    # kernel of size 5x5, <br/>    # change here for other sizes <br/>    kernel_size = (5,5)<br/>    # sigma values are same in both direction <br/>    blur = cv2.GaussianBlur(img,(5,5),0)<br/>    <br/>    plot_cv_img(img, blur)<br/><br/>if __name__ == '__main__':<br/>    main()</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Histogram equalization</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="md-expand">The basic point operations, to change the brightness and contrast, help in improving photo quality but require manual tuning. Using histogram equalization technique, these can be found algorithmically and create a better-looking photo. Intuitively, this method tries to set the brightest pixels to white and the darker pixels to black. The remaining pixel values are similarly rescaled. This rescaling is performed by transforming original intensity distribution to capture all intensity distribution. An example of this equalization is as following:</span></p>
<div class="CDPAlignCenter CDPAlign">&#160; &#160; &#160; &#160;<img height="356" width="604" class="alignnone size-full wp-image-527 image-border" src="images/a2e536d1-f279-4e31-94bd-d14d78cbb990.png"/></div>
<p class="mce-root"><span class="md-expand">The preceding image is an example of histogram equalization. On the right is the output and, as you can see, the contrast is increased significantly. The input histogram is shown in the bottom figure on the left and it can be observed that not all the colors are observed in the image. After applying equalization, resulting histogram plot is as shown on the right bottom figure. To visualize the results of equalization in the image, the input and results are stacked together in following figure:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="515" width="1500" class="alignnone size-full wp-image-457 image-border" src="images/310afc41-7651-464e-aa2e-0e8d4b2bd65e.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">Code for the preceding photos is as follows:</p>
<pre>def plot_gray(input_image, output_image): <br/>    """ <br/>    Converts an image from BGR to RGB and plots <br/>    """ <br/>    # change color channels order for matplotlib <br/>    fig, ax = plt.subplots(nrows=1, ncols=2)<br/><br/>    ax[0].imshow(input_image, cmap='gray') <br/>    ax[0].set_title('Input Image')<br/>    ax[0].axis('off')<br/>    <br/>    ax[1].imshow(output_image, cmap='gray') <br/>    ax[1].set_title('Histogram Equalized ')<br/>    ax[1].axis('off') <br/>    <br/>    plt.savefig('../figures/03_histogram_equalized.png')<br/><br/>    plt.show()<br/><br/>def main():<br/>    # read an image <br/>    img = cv2.imread('../figures/flower.png')<br/>    <br/>    # grayscale image is used for equalization<br/>    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/>    <br/>    # following function performs equalization on input image     <br/>    equ = cv2.equalizeHist(gray)<br/>    <br/>    # for visualizing input and output side by side<br/>    plot_gray(gray, equ)<br/>    <br/>if __name__ == '__main__':<br/>    main()</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Median filter&#160;</h1>
                </header>
            
            <article>
                
<p>This filter uses the same technique of neighborhood filtering; the key technique in this is the use of a median value. As such, the filter is non-linear. It is quite useful in removing sharp noise such as salt and pepper.</p>
<p>Instead of using a product or sum of neighborhood pixel values, this filter computes a median value of the region. This results in the removal of random peak values in the region, which can be due to noise like salt and pepper noise. This is further shown in the following figure with different kernel size used to create output.&#160;</p>
<p>In this image first input is added with channel wise random noise as:</p>
<pre># read the image<br/>flower = cv2.imread('../figures/flower.png')<br/><br/># initialize noise image with zeros<br/>noise = np.zeros(flower.shape[:2])<br/><br/># fill the image with random numbers in given range<br/>cv2.randu(noise, 0, 256)<br/><br/># add noise to existing image, apply channel wise<br/>noise_factor = 0.1<br/>noisy_flower = np.zeros(flower.shape)<br/>for i in range(flower.shape[2]):<br/>    noisy_flower[:,:,i] = flower[:,:,i] + np.array(noise_factor*noise, dtype=np.int)<br/><br/># convert data type for use<br/>noisy_flower = np.asarray(noisy_flower, dtype=np.uint8)</pre>
<p>The created noisy image is used for median filtering as:</p>
<pre># apply median filter of kernel size 5<br/>kernel_5 = 5<br/>median_5 = cv2.medianBlur(noisy_flower,kernel_5)<br/><br/><br/># apply median filter of kernel size 3<br/>kernel_3 = 3<br/>median_3 = cv2.medianBlur(noisy_flower,kernel_3)</pre>
<p>In the following photo, you can see the resulting photo after varying the kernel size (indicated in brackets). The rightmost photo is the smoothest of them all:</p>
<div class="CDPAlignCenter CDPAlign"><img height="265" width="1500" class="alignnone size-full wp-image-458 image-border" src="images/9e6b982a-c653-4e59-9017-302280a84143.png"/></div>
<p>The most common application for median blur is in smartphone application which filters input image and adds additional artifacts to add artistic effects.&#160;</p>
<p>The code to generate the preceding photograph is as follows:</p>
<pre>def plot_cv_img(input_image, output_image1, output_image2, output_image3): <br/>    """ <br/>    Converts an image from BGR to RGB and plots <br/>    """ <br/><br/>    fig, ax = plt.subplots(nrows=1, ncols=4)<br/><br/>    ax[0].imshow(cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)) <br/>    ax[0].set_title('Input Image')<br/>    ax[0].axis('off')<br/>    <br/>    ax[1].imshow(cv2.cvtColor(output_image1, cv2.COLOR_BGR2RGB)) <br/>    ax[1].set_title('Median Filter (3,3)')<br/>    ax[1].axis('off')<br/><br/>    ax[2].imshow(cv2.cvtColor(output_image2, cv2.COLOR_BGR2RGB)) <br/>    ax[2].set_title('Median Filter (5,5)')<br/>    ax[2].axis('off') <br/><br/>    ax[3].imshow(cv2.cvtColor(output_image3, cv2.COLOR_BGR2RGB)) <br/>    ax[3].set_title('Median Filter (7,7)')<br/>    ax[3].axis('off')<br/>    <br/>    plt.show()<br/>    <br/><br/>def main():<br/>    # read an image <br/>    img = cv2.imread('../figures/flower.png')<br/><br/>    # compute median filtered image varying kernel size<br/>    median1 = cv2.medianBlur(img,3)<br/>    median2 = cv2.medianBlur(img,5)<br/>    median3 = cv2.medianBlur(img,7)<br/>    <br/>    <br/>    # Do plot<br/>    plot_cv_img(img, median1, median2, median3)<br/><br/>if __name__ == '__main__':<br/>    main()</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image gradients</h1>
                </header>
            
            <article>
                
<p>These are more <em>edge detectors</em> or sharp changes in a photograph. Image gradients widely used in object detection and segmentation tasks. In this section, we will look at how to compute image gradients. First, the image derivative is applying the kernel matrix which computes the change in a direction.</p>
<p>The Sobel filter is one such filter and kernel in the <em>x&#160;</em>direction is given as follows:</p>
<p style="padding-left: 180px"><img height="60" width="97" src="images/5a5dd501-0ded-40c5-8978-38357785667b.png"/></p>
<p>Here, in the <em>y&#160;</em>direction:</p>
<p style="padding-left: 180px"><img style="font-size: 1em" height="72" width="133" src="images/b51855e0-69c3-4f41-a6a6-c6cff1f279b1.png"/></p>
<p>This is applied in a similar fashion to the linear box filter by computing values on a superimposed kernel with the photo. The filter is then shifted along the image to compute all values. Following is some example results, where <em>X</em> and <em>Y</em> denote the direction of the Sobel kernel:</p>
<div class="CDPAlignCenter CDPAlign"><img height="186" width="573" src="images/ec732c3c-95a0-4d6f-b690-9ef80b6f877f.png"/></div>
<p><span>This is also termed as an image&#160;derivative with respect to given direction(here X or Y). The lighter resulting photographs (middle and right) are positive gradients, while the darker regions denote negative and gray is zero.</span></p>
<p>While Sobel filters correspond to first order derivatives of a photo, the Laplacian filter gives a second-order derivative of a photo. The Laplacian filter is also applied in a similar way to Sobel:</p>
<div class="CDPAlignCenter CDPAlign"><img height="443" width="1500" class="alignnone size-full wp-image-459 image-border" src="images/bbbbf462-e1ab-419c-8a1f-1c8d9f2933df.png"/></div>
<p>The code to get Sobel and Laplacian filters is as follows:</p>
<pre># sobel <br/>x_sobel = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)<br/>y_sobel = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)<br/><br/># laplacian<br/>lapl = cv2.Laplacian(img,cv2.CV_64F, ksize=5)<br/><br/># gaussian blur<br/>blur = cv2.GaussianBlur(img,(5,5),0)<br/># laplacian of gaussian<br/>log = cv2.Laplacian(blur,cv2.CV_64F, ksize=5)</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Transformation of an image</h1>
                </header>
            
            <article>
                
<p>Transformation operations on an image are usually referred to as geometric transformations, applied on a photo. There are several other kinds of transformations as well but in this section we will refer to geometric transformations. These consist of, but are not limited to, shifting an image, rotating an image along an axis, or projecting it onto different planes.&#160;</p>
<p>At the core of transformation is a matrix multiplication of our image. We will look at different components of this matrix and the resulting image.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Translation</h1>
                </header>
            
            <article>
                
<p>Displacement of an image in any direction can be done by creating a transformation matrix and applying the transformation to our image. The transformation matrix for translation only is given as:</p>
<p style="padding-left: 180px"><img height="43" width="121" class="alignnone size-full wp-image-528 image-border" src="images/c1d16c6a-71f3-4e89-b5a3-259913998bb3.png"/></p>
<p>where <em>t<sub>x</sub></em> is translation in <em>x</em> direction and&#160;<em>t<sub>y</sub></em> in <em>y</em> direction in image reference. On choosing different values of translation matrix, results are shown as follows:&#160;</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="336" width="600" src="images/eb52c7e8-32e0-42db-aa93-ef25e6449def.png"/></div>
<p>In the previous figure, output is images are larger than the input image to show the effects of translation otherwise only visible region of images which are inside the original image size will be shown.</p>
<p>Code for creating this translation is given as follows, here change the values of <kbd>tx</kbd> and <kbd>ty</kbd> to generate different translations:&#160;</p>
<pre># input shape<br/>w, h = flower.shape[1], flower.shape[0]<br/><br/># create translation matrix<br/>tx = w/2 # half of width<br/>ty = h/2 # half of height<br/>translation_matrix = np.float32([[1,0,tx],<br/>                                 [0,1,ty]])<br/><br/># apply translation operation using warp affine function. <br/>output_size = (w*2,h*2)<br/>translated_flower = cv2.warpAffine(flower, translation_matrix, output_size)</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Rotation&#160;</h1>
                </header>
            
            <article>
                
<p>Similar to translation, rotating an image is also possible by creating a transformation matrix. Instead of creating translation matrix, in <kbd>OpenCV</kbd>, given a rotation angle&#160;θ, a rotation matrix is created of the form:</p>
<p style="padding-left: 120px" class="mce-root"><img height="52" width="323" class="alignnone size-full wp-image-529 image-border" src="images/587e03b9-ffdb-4b57-9be5-50036accd864.png"/></p>
<p>where,&#160;</p>
<p style="padding-left: 120px" class="mce-root"><img height="41" width="133" class="alignnone size-full wp-image-530 image-border" src="images/41e76660-f4cb-4a31-b01e-e55cf7a96bdf.png"/></p>
<p>An example of applying result is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="189" width="690" src="images/8018b5d3-c2c3-45e1-9843-cf02e1c30336.png"/></div>
<p>For the previous screenshot, the code is as follows:</p>
<pre># input shape<br/>w, h = flower.shape[1], flower.shape[0]<br/><br/># create rotation matrix<br/>rot_angle = 90 # in degrees<br/>scale = 1 # keep the size same<br/>rotation_matrix = cv2.getRotationMatrix2D((w/2,h/2),rot_angle,1)<br/><br/># apply rotation using warpAffine <br/>output_size = (w*2,h*2)<br/>rotated_flower = cv2.warpAffine(flower,rotation_matrix,output_size)</pre>
<p>Similarly, transformations also can be done by combining both rotation and translation with scaling and as a result, the angle between the lines will be preserved.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Affine transform&#160;</h1>
                </header>
            
            <article>
                
<p>With an Affine transform, only parallel lines will be preserved in the output. An example output image is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="242" width="549" class="alignnone size-full wp-image-460 image-border" src="images/65de58b3-1166-42a6-b0c2-087721d67790.png"/></div>
<p>The code for the preceding image is as follows:</p>
<pre># create transformation matrix form preselected points<br/>pts1 = np.float32([[50,50],[200,50],[50,200]])<br/>pts2 = np.float32([[10,100],[200,50],[100,250]])<br/><br/>affine_tr = cv2.getAffineTransform(pts1,pts2)<br/>transformed = cv2.warpAffine(img, affine_tr, (img.shape[1]*2,img.shape[0]*2))</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image pyramids</h1>
                </header>
            
            <article>
                
<p>Pyramids refer to rescaling a photograph either increasing the resolution or decreasing it. These are often used to increase the computation efficiency of computer vision algorithms such as image matching in a huge database. In such cases, image matching is computed on a downsampled image and later on <span>the search is&#160;</span>iteratively refined for a higher resolution of the image.&#160;</p>
<p>The downsampling and upsampling often depend on the pixel selection process. One of the simplest processes is selecting alternative rows and column pixel values to create a downsampled version of the photo as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="279" width="1500" class="alignnone size-full wp-image-461 image-border" src="images/1f47cd63-6f43-4d7a-852a-60856e46c764.png"/></div>
<p>However, if we try to upsample from the rightmost picture in the previous figure, the results look as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="273" width="1500" class="alignnone size-full wp-image-462 image-border" src="images/c6bdefdf-a24f-4d3d-87a6-4954af6b19ff.png"/></div>
<p>It can easily be seen that the rightmost picture above is not the same as the original one that we started with. This is due to the fact that we lose information during downsampling and hence the image cannot be recreated in the same way as the original one. In <kbd>OpenCV</kbd>, there is also blurring of input image before downsampling or upsampling. This also further makes it harder to keep the resolution intact.&#160;</p>
<p>The codes for downsampling is as follow:</p>
<pre># downsample image by halving both width and height<br/># input:(h, w) --&gt; output:(h/2, w/2)<br/>lower_resolution_img = cv2.pyrDown(img)</pre>
<p>And for upsampling an image to twice its height and width sizes as:</p>
<pre># Upsamples image by doubling both width and height<br/># input:(h, w) --&gt; output:(h*2, w*2)<br/>higher_resolution_img = cv2.pyrUp(img)</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we started with initial image analysis by applying various manipulation. We began discussion with point filters and extending to more complex linear as well as non linear filters. We saw the visualization of results on varying parameters like kernel size, and so on. The non-linear filters, like histogram equalization, can further tune images which are difficult to do with linear filters. Image gradients, introduced in this chapter, are quite common in complex tasks of object detection, image segmentation, and so on. We also saw various transformation methods like translation, rotation and affine transformation with visualization of the output given different choice of parameters. The various transformations can applied in cascaded fashion to create combined transformed results. Lastly, image downsampling and upsampling method is introduced which has crucial role in making computation faster or extracting more richer information respectively.&#160;</p>
<p>In the next chapter, we will be going through different features and feature extraction methods with the importance for each.&#160;</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    </div>
</body>
</html>