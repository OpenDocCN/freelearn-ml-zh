<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Implementing Recommender Systems with Julia</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the previous chapters, we took a deep dive into data mining and web development with Julia. I hope you enjoyed a few relaxing rounds of <em class="calibre16">Six Degrees of Wikipedia</em> while discovering some interesting articles. Randomly poking through the millions of Wikipedia articles as part of a game is a really fun way to stumble upon interesting new content. Although I'm sure that, at times, you've noticed that not all the articles are equally good—maybe they're stubs, or subjective, or not so well written, or simply irrelevant to you. If we were able to learn about each player's individual interests, we could filter out certain Wikipedia articles, which would turn each game session into a wonderful journey of discovery.</p>
<p class="calibre2">It turns out that we're not the only ones struggling with this—information discovery is a multibillion-dollar problem, regardless of whether it's articles, news, books, music, movies, hotels, or really any kind of product or service that can be sold over the internet. As consumers, we are exposed to an immense variety of choices, while at the same time, we have less and less time to review them—and our attention span is getting shorter and shorter. Making relevant recommendations instantly is a key feature of all successful online platforms, from Amazon to Booking.com, to Netflix, to Spotify, to Udemy. All of these companies have invested in building powerful recommender systems, literally inventing new business models together with the accompanying data collection and recommendation algorithms.</p>
<p class="calibre2">In this chapter, we'll learn about recommender systems—the most common and successful algorithms that are used in the wild for addressing a wide variety of business needs. We'll look at the following topics:</p>
<ul class="calibre10">
<li class="calibre11">What recommender systems are and how are they used</li>
<li class="calibre11">Content-based versus collaborative filtering recommender systems</li>
<li class="calibre11">User-based and item-based recommender systems</li>
</ul>
<ul class="calibre10">
<li class="calibre11">More advanced data analysis using <kbd class="calibre12">DataFrames</kbd> and statistical functions</li>
<li class="calibre11">How to roll out our own recommender systems using content-based and collaborative filtering algorithms</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="calibre2">The Julia package ecosystem is under continuous development and new package versions are released on a daily basis. Most of the times this is great news, as new releases bring new features and bug fixes. However, since many of the packages are still in beta (version 0.x), any new release can introduce breaking changes. As a result, the code presented in the book can stop working. In order to ensure that your code will produce the same results as described in the book, it is recommended to use the same package versions. Here are the external packages used in this chapter and their specific versions:</p>
<pre class="calibre17">CSV@v0.4.3<br class="title-page-name"/>DataFrames@v0.15.2<br class="title-page-name"/>Distances@v0.7.4<br class="title-page-name"/>IJulia@v1.14.1<br class="title-page-name"/>Plots@v0.22.0<br class="title-page-name"/>StatPlots@v0.8.2</pre>
<p class="calibre2">In order to install a specific version of a package you need to run:</p>
<pre class="calibre17"><strong class="calibre1">pkg&gt; add PackageName@vX.Y.Z</strong> </pre>
<p class="calibre2">For example:</p>
<pre class="calibre17"><strong class="calibre1">pkg&gt; add IJulia@v1.14.1</strong></pre>
<p class="calibre2">Alternatively you can install all the used packages by downloading the <kbd class="calibre12">Project.toml</kbd> file provided with the chapter and using <kbd class="calibre12">pkg&gt;</kbd> instantiate as follows:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; download("https://raw.githubusercontent.com/PacktPublishing/Julia-Programming-Projects/master/Chapter06/Project.toml", "Project.toml")</strong><br class="title-page-name"/><strong class="calibre1">pkg&gt; activate . </strong><br class="title-page-name"/><strong class="calibre1">pkg&gt; instantiate</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding recommender systems</h1>
                </header>
            
            <article>
                
<p class="calibre2">In its broadest definition, a <strong class="calibre4">recommender system</strong> (<strong class="calibre4">RS</strong>) is a technique that's used for providing suggestions for items that are useful to a person. These suggestions are meant to help in various decision-making processes, usually related to buying or consuming a certain category of products or services. They can be about buying a book, listening to a song, watching a movie, eating out at a certain restaurant, reading a news article, or picking the hotel for your next holiday.</p>
<p class="calibre2">People have relied on recommendations pretty much since the beginning of history. Some RS researchers talk about the first recommendations as being the first orally transmitted information about dangerous plants, animals, or places. Others think that recommendations systems functioned even before language, by simply observing the effects on other humans of consuming plants or unwisely confronting dangerous creatures (that could count as an extreme and possibly violent example of implicit ratings, as we'll see in the following paragraphs).</p>
<p class="calibre2">But we don't have to go that far into human history. In more recent (and less dangerous) times, we can find some great examples of highly successful recommender systems, such as librarians suggesting books based on your tastes and interests, the butcher presenting meat products for your Sunday recipe, your friends' opinion of the latest blockbuster, your neighbor's stories about the kindergarten across the street, and even your MD recommending what treatment to follow to alleviate the symptoms and eliminate the cause of your disease. Other recommender systems are more formal, but equally pervasive and familiar, such as the star category ranking of hotels or the blue flags on top beaches around the world.</p>
<p class="calibre2">For a very long time, the experts in various fields played the part of recommenders, using their expertise in combination with their understanding of our tastes and interests, skillfully probing us for details. However, the rise of the internet and online platforms (e-commerce websites, online radios, movie streaming platforms, and social networks) has replaced the traditional models by making a huge catalog of items (products) available to a potentially very large consumer base (now called <strong class="calibre4">users</strong>). Due to considerations like 24-hour availability, language barriers, and sheer volume, personal recommendations were no longer a feasible option (although in the last couple of years, there was a certain recurrence of human-curated recommendations, from music, to books, to luxury products—but that's a different discussion).</p>
<p class="calibre2">This expansion in the number of choices made finding the right product a very difficult task. At that point, software-based recommender systems entered the stage.</p>
<p class="calibre2">Amazon.com is credited as being the first online business that deployed software recommender systems at scale, with extraordinary business benefits. Later on, Netflix became famous for awarding a one million dollar prize to the team that came up with a recommendation algorithm better than theirs. Nowadays, automated recommender systems power all major platforms, from Spotify's <em class="calibre16">Discover Weekly</em> playlists to Udemy's recommended courses.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying recommender systems</h1>
                </header>
            
            <article>
                
<p class="calibre2">Different business needs—from suggesting related products after buying your new laptop, to compiling the perfect driving playlist, to helping you reconnect with long lost schoolmates—led to the development of different recommendation algorithms. A key part of rolling out a recommender system is picking the right approach for the problem at hand to fully take advantage of the data available. We'll take a look at the most common and most successful algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning about non-personalized, stereotyped, and personalized recommendations</h1>
                </header>
            
            <article>
                
<p class="calibre2">The simplest types of recommendations, from a technical and algorithmic perspective, are the non-personalized ones. That is, they are not customized to take into account specific user preferences. Such recommendations can include best-selling products, various top 10 songs, blockbuster movies, or the most downloaded apps of the week.</p>
<p class="calibre2">Non-personalized recommendations are less challenging technically, but also considerably less powerful. They can be good approximations in certain cases, especially when the product catalog is not very large (there are not that many Hollywood releases, for example). But for an e-commerce retailer like Amazon, with millions of products available at any given time, the chances of getting it right using generic recommendations are slim.</p>
<p class="calibre2">An improvement in non-personalized recommendations comes from combining them with a classification strategy. By stereotyping, we can make the recommended items more relevant, especially when we can identify significantly different user demographics. A good example of this is app store recommendations, which are broken down by country. Take, for instance, the following list of recommended new games. This is what it looks like if you are a user accessing the app store from the US:</p>
<p class="CDPAlignCenter"><img src="assets/65f54a74-a4ab-4792-b873-dd2077dd5775.png" class="calibre66"/></p>
<p class="calibre2">This is what it looks like for a user in Romania, at the exact same time:</p>
<p class="CDPAlignCenter"><img src="assets/5dc64b59-1d9e-438f-842a-6573e4dfeb3a.png" class="calibre67"/></p>
<p class="calibre2">You can easily notice that the top selections vary widely. This is driven by both cultural differences and preferences, but also by availability (copyright and distribution) issues.</p>
<p class="calibre2">We won't focus on non-personalized recommendations in this chapter, since implementing them is quite straightforward. All that is needed for making such recommendations is to identify the relevant metrics and the best performing items, such as the number of downloads for apps, copies sold for a book, volume of streams for a song or movie, and so on. However, non-personalized recommendations, as a business solution, should not be dismissed, as they can be useful when dealing with users that don't present any relevant personal preferences—usually new users.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding personalized recommendations</h1>
                </header>
            
            <article>
                
<p class="calibre2">Both from a business and a technical perspective, the most interesting recommender systems are the ones that take into account the user's preferences (or user's ranking).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Explicit and implicit ratings</h1>
                </header>
            
            <article>
                
<p class="calibre2">When looking for personalization features, we must take into account both explicit data that's willingly provided by the user, as well as relevant information that's generated by their behavior in the app or on the website (or anywhere else where we're tracking user behavior really, since the boundary between the online and physical realms is becoming more blurry, for example, with the introduction of smart cars and autonomous shop checkouts, to name just a few). The explicit rating includes actions such as grading a product or an experience, awarding stars to a movie or purchase, and retweeting or liking a post. On the other hand, not bouncing back to the search results page, sharing a song, or watching a video until the end are all examples of an implicit positive rating, while returning a product, canceling a subscription, or not finishing an online training course or an eBook are instances of negative implicit ranking.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding content-based recommender systems</h1>
                </header>
            
            <article>
                
<p class="calibre2">One of the most common and successful types of recommendations are content-based. The core idea is that if I expressed a preference for a certain set of items, I will most likely be interested in more items that share the same attributes. For example, the fact that I watched <kbd class="calibre12">Finding Nemo (2003)</kbd> can be used as an indication that I will be interested in other movies from the animation and comedy genres.</p>
<p class="calibre2">Alternatively, watching one of the original <em class="calibre16">Star Wars</em> movie can be interpreted as a signal that I like other movies from the franchise, or movies with Harrison Ford, or directed by George Lucas, or science fiction in general. Indeed, Netflix employs such an algorithm, except at a more granular level. Per a recent article, Netflix has a large team that's tasked with watching and tagging movies in detail—later on, matching movie features with users groups. The users themselves are equally carefully classified into thousands of categories.</p>
<p class="calibre2">More advanced content-based recommender systems also take into account the relative weight of the different tags. In the case of the previously mentioned <kbd class="calibre12">Finding Nemo (2003)</kbd>, the suggestions should be less about movies with fish and sharks and more about the fact that it's a funny, light-hearted family movie, so hopefully, the recommendation will be more <kbd class="calibre12">Finding Dory <span>(2016)</span></kbd> and less <em class="calibre16">Jaws</em>.</p>
<p class="calibre2">Let's see how we can build a basic movie recommender using a content-based algorithm. To keep things simple, I have set up a table with the top 10 movies of 2016 and their genres. You can find this file in this book's GitHub repository, as <span class="calibre5"><kbd class="calibre12">top_10_movies.tsv</kbd>, at <a href="https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter06/top_10_movies.tsv" class="calibre9">https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter06/top_10_movies.tsv</a>:</span></p>
<p class="CDPAlignCenter"><img src="assets/8b432b80-4e1f-413f-a382-b89e3c79d870.png" class="calibre18"/></p>
<p class="calibre2">In the preceding screenshot, you can see how we use a binary system to represent whether a movie belongs to a genre (encoded by a <kbd class="calibre12">1</kbd>) or not (a <kbd class="calibre12">0</kbd>).</p>
<p class="calibre2">We can easily load such a table from a CSV/TSV file into Julia by using the <kbd class="calibre12"><span>readdlm</span></kbd> function, which is available in the <kbd class="calibre12">DelimitedFiles</kbd> module. This module comes with the default Julia installation, so there's no need to add it:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; using DelimitedFiles 
Julia&gt; movies = readdlm("top_10_movies.tsv", '\t', skipstart=1)</strong> </pre>
<p class="calibre2">In the preceding snippet, <kbd class="calibre12"><span>skipstart=1</span></kbd> tells Julia to skip the first line when reading the <em class="calibre16">Tab</em> separated <kbd class="calibre12"><span>top_10_movies.tsv</span></kbd> file—otherwise, Julia would interpret the header row as a data row as well.</p>
<p class="calibre2">There is also the option of letting <kbd class="calibre12">readdlm</kbd> know that the first row is the header, passing <kbd class="calibre12">header = true</kbd>. However, this would change the return type of the function invocation to a tuple of <kbd class="calibre12">(data_cells, header_cells)</kbd>, which is not pretty-printed in interactive environments. At this exploratory phase, we're better off with a table-like representation of the data. The result is a tabular data structure that contains our movie titles and their genres:</p>
<pre class="calibre17"> 10×9 Array{Any,2}: 
 "Moonlight (2016)"                   0  0  0  1  0  0  0  0 
 "Zootopia (2016)"                    1  1  1  0  0  0  0  0 
 "Arrival (2016)"                     0  0  0  1  0  1  0  1 
 "Hell or High Water (2016)"          0  0  0  1  0  1  0  0 
 "La La Land (2016)"                  0  0  1  1  0  0  1  0 
 "The Jungle Book (2016)"             1  0  0  0  1  0  0  0 
 "Manchester by the Sea (2016)"       0  0  0  1  0  0  0  0 
 "Finding Dory (2016)"                0  1  0  0  0  0  0  0 
 "Captain America: Civil War (2016)"  1  0  0  0  0  0  0  1 
 "Moana (2016)"                       1  1  0  0  0  0  0  0 </pre>
<p class="calibre2">Let's see what movie from the top 10 list we could recommend to a user who watched the aforementioned movie, <kbd class="calibre12">Finding Nemo (2003)</kbd>. Rotten Tomatoes classifies <kbd class="calibre12">Finding Nemo (2003)</kbd> under the <span class="calibre5"><em class="calibre16">Animation</em></span>, <span class="calibre5"><em class="calibre16">Comedy,</em></span> and <span class="calibre5"><em class="calibre16">Kids</em></span> genres. We can encode this as follows:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; nemo = ["Finding Nemo (2003)", 0, 1, 1, 0, 1, 0, 0, 0] 9-element Array{Any,1}:</strong> 
  "Finding Nemo (2003)" 
 0 
 1 
 1 
 0 
 1 
 0 
 0 
 0 </pre>
<p class="calibre2">To make a movie recommendation based on genre, all we have to do is find the ones that are the most similar, that is, the movies that share the most genres with our <kbd class="calibre12">Finding Nemo (2003)</kbd>.</p>
<p class="calibre2">There is a multitude of algorithms for computing the similarity (or on the contrary, the distance) between items—in our case, as we're dealing with binary values only, the Hamming distance looks like a good choice. The Hamming distance is a number that's used to denote the difference between two binary strings. This distance is calculated by comparing two binary values and taking into account the number of positions at which the corresponding bits are different. We'll compare each bit in succession and record either <kbd class="calibre12">1</kbd> or <kbd class="calibre12">0</kbd>, depending on whether or not the bits are different or the same. If they are the same, we record a <kbd class="calibre12">0</kbd>. For different bits, we record a <kbd class="calibre12">1</kbd>. Then, we add all the 1s and 0s in the record to obtain the Hamming distance.</p>
<p class="calibre2">A function for calculating the Hamming distance is available in the <kbd class="calibre12"><span>Distances</span></kbd> package. This is a third-party Julia package that provides access to a multitude of functions for evaluating distances between vectors, including <span class="calibre5">Euclidian</span>, <span class="calibre5">Jaccard</span>, <span class="calibre5">Hemming</span>, <span class="calibre5">Cosine,</span> and many others. All we need to do to access this treasure of functionality is run the following:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; using Pkg 
pkg&gt; add Distances  
julia&gt; using Distances</strong> </pre>
<p class="calibre2">Then, we need to iterate over our <span class="calibre5">movies</span> matrix and compute the Hamming distance between each movie and <kbd class="calibre12">Finding Nemo (2003)</kbd>:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; distances = Dict{String,Int}() 
Dict{String,Int64} with 0 entries </strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">julia&gt; for i in 1:size(movies, 1) 
            distances[movies[i,:][1]] = hamming(Int[movies[i,2:end]...], Int[nemo[2:end]...]) 
       end</strong> </pre>
<p class="calibre2">In the preceding snippet, we iterated over each movie and calculated the Hamming distance between its genres and the genres of <kbd class="calibre12">Finding Nemo (2003)</kbd>. To do this, we only extracted the genres (leaving off the name of the movie) and converted the list of values into an array of <kbd class="calibre12">Int</kbd>. Finally, we placed the result of the computation into the <kbd class="calibre12">distances</kbd> <kbd class="calibre12">Dict</kbd> we defined previously, which uses the name of the movie as the key, and the distance as the value.</p>
<p class="calibre2">This is the end result:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; distances 
Dict{String,Int64} with 10 entries: 
  "The Jungle Book (2016)"            =&gt; 3 
  "Hell or High Water (2016)"         =&gt; 5 
  "Arrival (2016)"                    =&gt; 6 
  "La La Land (2016)"                 =&gt; 4 
  "Moana (2016)"                      =&gt; 3 
  "Captain America: Civil War (2016)" =&gt; 5 
  "Moonlight (2016)"                  =&gt; 4 
  "Finding Dory (2016)"               =&gt; 2 
  "Zootopia (2016)"                   =&gt; 2 
  "Manchester by the Sea (2016)"      =&gt; 4</strong></pre>
<p class="calibre2">Since we're computing distances, the most similar movies are the ones within the shortest distance. So, according to our recommender, a user who watched <kbd class="calibre12">Finding Nemo (2003)</kbd> should next watch <kbd class="calibre12">Finding Dory (2016)</kbd> or <kbd class="calibre12">Zootopia (2016)</kbd> (distance <kbd class="calibre12">2</kbd>) and when done, should move on to <kbd class="calibre12">The Jungle Book <span>(2016)</span></kbd> and <kbd class="calibre12">Moana <span>(2016)</span></kbd> (both at a distance of <kbd class="calibre12">3</kbd>). If you haven't watched all of these recommended movies already, I can tell you that the suggestions are quite appropriate. Similarly, the least recommended movie is <kbd class="calibre12">Arrival <span>(2016)</span></kbd>, which although is an excellent science fiction drama, has nothing in common with cute Nemo and forgetful Dory.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Beginning with association-based recommendations</h1>
                </header>
            
            <article>
                
<p class="calibre2">Although content-based recommender systems can produce great results, they do have limitations. For starters, they can't be used to recommend new items. Based on my initial <kbd class="calibre12">Finding Nemo (2003)</kbd> ranking alone, I would be stuck getting suggestions for animated movies alone and I'd never get the chance to hear about any new documentaries or car or cooking shows that I sometimes enjoy.</p>
<p class="calibre2">Also, it works best for categories of items that can be purchased repeatedly, like books, apps, songs, or movies, to name a few. But if I'm on Amazon and purchase a new dishwasher from the <em class="calibre16">Home and kitchen</em> category, it doesn't make a lot of sense to get recommendations about products within the same group, such as a fridge or a washing machine, as chances are I'm not replacing all of the expensive kitchen appliances at the same time. However, I will most likely need the corresponding joints and taps and pipes and whatever else is needed to install the dishwasher, together with the recommended detergent and maybe other accessories. Since the e-commerce platform is selling all of these products as well, it's beneficial to order them together and receive them at the same time, saving on transport too.</p>
<p class="calibre2">These bundles of products can form the foundation of a RS based on product association. These types of recommendations are quite common, and are usually presented as <em class="calibre16">frequently bought together</em> on e-commerce platforms. For physical stores, this type of data analysis—also known as <strong class="calibre4">market basket analysis</strong>—is used to place products that are purchased together in close physical proximity. Think, for example, about pasta being side by side with sauces, or shampoo with conditioners.</p>
<p class="calibre2">One of the most popular algorithms used for association based recommendations is the <kbd class="calibre12">Apriori</kbd> algorithm. It is used to identify items that frequently occur together in different scenarios (shopping baskets, web browsing, adverse drug reactions, and so on). The <kbd class="calibre12">Apriori</kbd> algorithm helps us identify correlations through data mining by employing association rules.</p>
<p class="calibre2">Space constraints don't allow us to get into the details of building such as system, but if you would like to dive deeper into this topic, there are many free resources to get you started. I recommend beginning with <em class="calibre16">Movie Recommendation with Market Basket Analysis</em> (<span class="calibre5"><a href="https://rpubs.com/vitidN/203264" class="calibre9">https://rpubs.com/vitidN/203264</a></span>) as it builds a movie recommender that's very similar to ours.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning about collaborative filtering</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5"><strong class="calibre4">Collaborative filtering</strong></span> (<span class="calibre5"><strong class="calibre4">CF</strong></span>) is another very successful and widely used recommendation algorithm. It is based on the idea that people with similar preferences will have similar interests. If two customers, let's call them Annie and Bob, give <kbd class="calibre12">Finding Nemo (2003)</kbd> a good rating and Annie also highly ranks <kbd class="calibre12">Finding Dory (2016)</kbd>, then chances are that Bob will also like <kbd class="calibre12">Finding Dory (2016)</kbd>. Of course, comparing two users and two products may not seem like much, but applied to very large datasets representing both users and products, the recommendations become highly relevant.</p>
<p class="calibre2">If you're confused as to what the difference between CF and content filtering is, since both can be used to infer <kbd class="calibre12">Finding Dory (2016)</kbd> based on <kbd class="calibre12">Finding Nemo (2003)</kbd>, the key point is that CF does not care about item attributes. Indeed, when using CF, we don't need the movie genre information, nor any other tags. The algorithm is not concerned with the classification of the items. It pretty much states that if, for whatever reason, the items were ranked highly by a subset of users, then other items that are highly ranked by the same subset of users will be relevant for our target user, hence making for a good recommendation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding user-item CF</h1>
                </header>
            
            <article>
                
<p class="calibre2">This was the basic idea, and with the advent of big data, the CF technique has become quite powerful. As it's been applied to different business needs and usage scenarios, the algorithm was refined to better address the problems it was attempting to solve. As a consequence, a few other approaches emerged, and the original one became known as <strong class="calibre4">user-item CF</strong>.</p>
<p class="calibre2">It's gotten this name because it takes as its input user data (user preferences, rankings) and outputs item data (item recommendations). It's also known as <strong class="calibre4">user-based CF</strong>.</p>
<p class="calibre2">You can see it illustrated in the following diagram:</p>
<p class="CDPAlignCenter"><img src="assets/b242774a-f1e2-4b9c-8641-6c93227d1fc8.png" class="calibre68"/></p>
<p class="calibre2">The preceding diagram shows that <strong class="calibre4">Annie</strong> likes <strong class="calibre4">A</strong>, <strong class="calibre4">B</strong>, and <strong class="calibre4">E</strong>, while <strong class="calibre4">Bob</strong> likes <strong class="calibre4">A</strong>, <strong class="calibre4">B</strong>, <strong class="calibre4">C</strong>, and <strong class="calibre4">D</strong>.</p>
<p class="calibre2">The <kbd class="calibre12">recommender</kbd> algorithm established that, between <strong class="calibre4">Annie</strong> and <strong class="calibre4">Bob</strong>, there's a high degree of similarity because they both like items <strong class="calibre4">A</strong> and <strong class="calibre4">B</strong>. Next, it will assume that <strong class="calibre4">Annie</strong> will also like other items from Bob's list of preferences that she hasn't discovered yet, and the reverse for <strong class="calibre4">Bob</strong>—he'll like items from Annie's list that he hasn't discovered yet. Thus, since Annie also likes item E, we can recommend it to <strong class="calibre4">Bob</strong>, and since Bob's very fond of <strong class="calibre4">C</strong> and <strong class="calibre4">D</strong> and Annie has no knowledge about these yet, we can confidently suggest that she checks them out.</p>
<p class="calibre2">Let's take another very simple example, also from the realm of movie recommendations. Sticking to our previous list of top 10 movies for the year 2016 on Rotten Tomatoes, this time, let's ignore the classification by genre and imagine that we have user ratings data instead:</p>
<p class="CDPAlignCenter"><img src="assets/855a1095-b43b-486b-9f0a-06ae20a0db88.png" class="calibre69"/></p>
<p class="calibre2">The preceding screenshot shows a table of movie titles and users and their corresponding ratings. As it happens in real life, not all of the users have rated all of the moves—the absence of a rating is indicated by an empty cell.</p>
<p class="calibre2">You will notice in the preceding screenshot that by a strange twist of faith, the user's names provide a hint as to what kind of movies they prefer. Acton is very much into action movies, while Annie loves animations. Comey's favorites are the comedies, while Dean enjoys good dramas. Kit's highest rankings went to kids movies, Missie loves mystery movies, while musical's are Musk reasons for binge watching. Finally, Sam is a science fiction fan.</p>
<p class="calibre2">The dataset is provided in this chapter's files under the name <kbd class="calibre12"><span>top_10_movies_user_rankings.csv</span></kbd>. Please download it from <a href="https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter06/top_10_movies_user_rankings.csv" class="calibre9">https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter06/top_10_movies_user_rankings.csv</a> and place it somewhere on your hard drive where you can easily access it from Julia's REPL.</p>
<p class="calibre2">We can load it into memory using the same <kbd class="calibre12"><span>readdlm</span></kbd> Julia function as before:</p>
<pre class="calibre17">movies = readdlm("/path/to/top_10_movies_user_rankings.csv", ';') </pre>
<p class="calibre2">This file uses the <kbd class="calibre12"><span>;</span></kbd> char as the column separator, so we need to pass that into the <kbd class="calibre12"><span>readdlm</span></kbd> function call. Remember that in Julia, <kbd class="calibre12">";"</kbd> is different from <kbd class="calibre12">':'</kbd>. The first is a <kbd class="calibre12">String</kbd> of length one, while the second is a <kbd class="calibre12">Char</kbd>.</p>
<p class="calibre2">This is the result of the <kbd class="calibre12">.csv</kbd> file being read—a matrix containing movies on rows and people on columns, with each person's rating at the corresponding intersection between rows and columns:</p>
<p class="CDPAlignCenter"><img src="assets/f400846a-c6c9-4597-b0c1-cabaf04cd836.png" class="calibre70"/></p>
<p class="calibre2">It works, but the data doesn't look too good. As usually happens with data in real life, we don't always have ratings from all the users. The <kbd class="calibre12">missing</kbd> values were imported as empty strings <kbd class="calibre12"><span>""</span></kbd>, and the headers were interpreted as entries in the matrix. Julia's <kbd class="calibre12"><span>readdlm</span></kbd> is great for quick data imports, but for more advanced data wrangling, we can benefit considerably from using Julia's powerful <kbd class="calibre12"><span>DataFrames</span></kbd> package.</p>
<p class="calibre2"><span class="calibre5"><kbd class="calibre12">DataFrames</kbd></span> is a third-party Julia package that exposes a rich set of functions for manipulating tabular data. You should remember it from our introduction in <a href="90a7f09d-d63b-45d7-baf5-576470d0910f.xhtml" target="_blank" class="calibre9">Chapter 1</a>,<em class="calibre16"> Getting Started with Julia Programming</em>—if not, please take a few minutes to review that part. The rest of our discussion will assume that you have a basic understanding of <span class="calibre5"><kbd class="calibre12">DataFrames</kbd></span> so that we can now focus on the more advanced features and use cases.</p>
<p class="calibre2">If, for some reason, you no longer have the <kbd class="calibre12"><span>DataFrames</span></kbd> package, <kbd class="calibre12"><span>pkg&gt; add DataFrames</span></kbd> is all we need. While we're at it, let's also install the <kbd class="calibre12"><span>CSV</span></kbd> package—it's a powerful utility library for handling delimited text files. We can add both in one step:</p>
<pre class="calibre17"><strong class="calibre1">pkg&gt; add DataFrames CSV</strong> </pre>
<p class="calibre2">We'll use <kbd class="calibre12">CSV</kbd> to load the comma-separated file and produce a <kbd class="calibre12"><span>DataFrame</span></kbd>:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; movies = CSV.read("top_10_movies_user_rankings.csv", delim = ';')</strong> </pre>
<p class="calibre2">The resulting <kbd class="calibre12">DataFrame</kbd> should look like this:</p>
<p class="CDPAlignCenter"><img src="assets/073d60f6-3d89-4c40-b5c9-94f99efdca91.png" class="calibre71"/></p>
<p class="calibre2">We get a beautifully rendered tabular data structure, with the missing ratings correctly represented as <kbd class="calibre12"><span>missing</span></kbd> data.</p>
<p class="calibre2">We can get a quick summary of our data by using the <kbd class="calibre12"><span>describe</span></kbd> function:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; describe(movies)</strong> </pre>
<p class="calibre2">The output for this is as follows:</p>
<p class="CDPAlignCenter"><img src="assets/f7c3d11a-25f8-43f4-a6b2-a9ea4484c01f.png" class="calibre72"/></p>
<p class="calibre2">Multiple columns have <kbd class="calibre12"><span>missing</span></kbd> values. A <kbd class="calibre12"><span>missing</span></kbd> value represents a value that is absent in the dataset. It is defined in the <kbd class="calibre12"><span>Missings</span></kbd> package (<span class="calibre5"><a href="https://github.com/JuliaData/Missings.jl" class="calibre9">https://github.com/JuliaData/Missings.jl</a></span>), and it's the singleton instance of the <kbd class="calibre12"><span>Missing</span></kbd> type. If you're familiar with <kbd class="calibre12"><span>NULL</span></kbd> in SQL or <kbd class="calibre12"><span>NA</span></kbd> in R, <kbd class="calibre12"><span>missing</span></kbd> is the same in Julia.</p>
<p class="calibre2">Missing values are problematic when working with real-life datasets as they can affect the accuracy of the computations. For this reason, common operations that involve <kbd class="calibre12"><span>missing</span></kbd> values usually propagate <kbd class="calibre12"><span>missing</span></kbd>. For example, <kbd class="calibre12"><span>1 + missing</span></kbd> and <kbd class="calibre12"><span>cos(missing)</span></kbd> will both return <kbd class="calibre12"><span>missing</span></kbd>.</p>
<p class="calibre2">We can check if a value is missing by using the <kbd class="calibre12"><span>ismissing</span></kbd> function:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; movies[1,2] 
missing</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">julia&gt; ismissing(movies[1, 2]) 
true</strong> </pre>
<p class="calibre2">In many cases, <kbd class="calibre12">missing</kbd> values will have to be skipped or replaced with a valid value. What value is appropriate for replacing <kbd class="calibre12"><span>missing</span></kbd> will depend from case to case, as dictated by the business logic. In our case, for the missing ratings, we can use the value <kbd class="calibre12">0</kbd>. By convention, we can agree that valid ratings range from <kbd class="calibre12">1</kbd> to <kbd class="calibre12">10</kbd>, and that a rating of <kbd class="calibre12">0</kbd> corresponds to no rating at all.</p>
<p class="calibre2">One way to do the replacement is to iterate over each column except <kbd class="calibre12">Movie title</kbd> and then over each cell, and if the corresponding value is <span class="calibre5">missing</span>, replace it with <kbd class="calibre12">0</kbd>. Here is the code:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; for c in names(movies)[2:end] 
           movies[ismissing.(movies[c]), c] = 0 
       end</strong> </pre>
<p class="calibre2">We're all done—our data is now clean, with zeroes replacing all the previously missing values:</p>
<p class="CDPAlignCenter"><img src="assets/2e5355cc-0f4a-4ab9-adf0-4ae0f421eb30.png" class="calibre73"/></p>
<p class="calibre2">It would help if you saved this clean version of our data as a <em class="calibre16">Tab</em> separated file, for future reference, with the following code:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; CSV.write("top_10_movies_user_rankings.tsv", movies, delim='\t')</strong> </pre>
<p class="calibre2">Now that we have our ratings loaded into Julia, the next step is to compute the similarity between the various users. The Hamming distance, the formula that we used when computing content based recommendations, would not be a good choice for numerical data. A much better alternative is Pearson's correlation coefficient. This coefficient, also known as <strong class="calibre4"><span class="calibre5"><em class="calibre16">Pearson's r</em></span> or bivariate correlation</strong>, is a measure of the linear correlation between two variables. It has a value between <kbd class="calibre12">+1</kbd> and <kbd class="calibre12">−1</kbd>. A value of <kbd class="calibre12">1</kbd> indicates total positive linear correlation (both values increase together), while <kbd class="calibre12">-1</kbd> represents total negative linear correlation (one value decreases while the other increases). The value <kbd class="calibre12">0</kbd> means that there's no linear correlation.</p>
<p class="calibre2"><span class="calibre5"><span class="calibre5">Here are</span></span> a few examples of scatter diagrams with different visualizations of the correlation coefficient (By Kiatdd—Own work, CC BY-SA 3.0, <a href="https://commons.wikimedia.org/w/index.php?curid=37108966" class="calibre9"><span>https://commons.wikimedia.org/w/index.php?curid=37108966</span></a>):</p>
<p class="CDPAlignCenter"><img src="assets/8c9c0a1a-64df-47c0-92e8-6bf2d380b65d.png" class="calibre74"/></p>
<p class="calibre2">Let's see how we would calculate the similarity between <kbd class="calibre12">Acton</kbd> and <kbd class="calibre12">Annie</kbd>, based on the movie ratings they provided. Let's make things simpler and focus strictly on their data by extracting the <kbd class="calibre12">Movie title</kbd> column, together with the <kbd class="calibre12">Acton</kbd> and <kbd class="calibre12">Annie</kbd> columns:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; acton_and_annie = movies[:, 1:3]</strong> </pre>
<p class="calibre2">The output is as follows:</p>
<p class="CDPAlignCenter"><img src="assets/c61080ab-a58f-4399-89de-7547b64085bd.png" class="calibre75"/></p>
<p class="calibre2">This returns another <kbd class="calibre12"><span>DataFrame</span></kbd>, referenced as <kbd class="calibre12"><span>acton_and_annie</span></kbd>, which corresponds to the columns one to three of the <span class="calibre5"><kbd class="calibre12">movies</kbd> </span><kbd class="calibre12"><span>DataFrame</span></kbd>, representing Acton's and Annie's ratings for each of the movies.</p>
<p class="calibre2">This is good, but we're only interested in the movies that were rated by both users. If you remember from our discussion of <kbd class="calibre12"><span>DataFrame</span></kbd> in <a href="90a7f09d-d63b-45d7-baf5-576470d0910f.xhtml" class="calibre9">Chapter 1</a>, <em class="calibre16">Getting Started with Julia Programming, </em>we can select rows (and columns) by passing a Boolean value—<kbd class="calibre12"><span>true</span></kbd> to select it, <kbd class="calibre12"><span>false</span></kbd> to skip it. We can use this in combination with the dot syntax for element-wise operations to check if the values in the <kbd class="calibre12"><span>:Acton</span></kbd> and <kbd class="calibre12"><span>:Annie</span></kbd> columns are greater than <kbd class="calibre12">0</kbd>. The code will look like this:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; acton_and_annie_in_common = acton_and_annie[(acton_and_annie[:Acton] .&gt; 0) .&amp; (acton_and_annie[:Annie] .&gt; 0), :]</strong> </pre>
<p class="calibre2">Although it might look a bit intimidating, the snippet should be easy to follow: we use the <kbd class="calibre12">(acton_and_annie[:Acton] .&gt; 0) .&amp; (acton_and_annie[:Annie] .&gt; 0)</kbd> expression to check element-wise if the values in the <kbd class="calibre12">Acton</kbd> and <kbd class="calibre12">Annie</kbd> columns are greater than <kbd class="calibre12">0</kbd>. Each comparison will return an array of <kbd class="calibre12">true/false</kbd> values—more exactly two <kbd class="calibre12">10-element BitArrays</kbd>, as follows:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; acton_and_annie[:Acton] .&gt; 0 
10-element BitArray{1}: 
 false 
  true 
  true 
  true 
  true 
  true 
 false 
  true 
  true 
  true 
 
julia&gt; acton_and_annie[:Annie] .&gt; 0 
10-element BitArray{1}: 
  true 
  true 
 false 
 false 
 false 
  true 
 false 
  true 
 false 
  true</strong> </pre>
<p class="calibre2"><span class="calibre5">Next, we apply the bitwise</span> <kbd class="calibre12">&amp;</kbd> <span class="calibre5">operator, which is also element-wise, to</span> the resulting arrays:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; (acton_and_annie[:Acton] .&gt; 0) .&amp; (acton_and_annie[:Annie] .&gt; 0) 
10-element BitArray{1}: 
 false 
  true 
 false 
 false 
 false 
  true 
 false 
  true 
 false 
  true</strong> </pre>
<p class="calibre2">Finally, this array of true/false values is passed into the <kbd class="calibre12">DataFrame</kbd> to filter the rows. The preceding snippet will produce the following output, a new <kbd class="calibre12"><span>DataFrame</span></kbd> that contains only the movies that have been rated by both <kbd class="calibre12">Acton</kbd> and <kbd class="calibre12">Annie</kbd>:</p>
<p class="calibre2">The output is as follows:</p>
<p class="CDPAlignCenter"><img src="assets/4914a5e5-29d2-4d42-9de2-050e2270d142.png" class="calibre76"/></p>
<p class="calibre2">Let's plot the ratings. Julia comes with quite a few options for plotting. We saw some in <a href="90a7f09d-d63b-45d7-baf5-576470d0910f.xhtml" class="calibre9">Chapter 1</a>, <em class="calibre16">Getting Started with Julia Programming</em>, and we'll look at plotting in more detail in <a href="11df7c94-2e9a-4cc5-aba1-b9c9c93800a0.xhtml" class="calibre9">Chapter 9</a>, <em class="calibre16">Working with Dates, Time, and Time Series</em>. For now, we'll use the appropriately named <kbd class="calibre12"><span>Plots</span></kbd> library to quickly visualize our data.</p>
<p class="calibre2"><kbd class="calibre12"><span>Plots</span></kbd> is designed as a higher-level interface to other plotting libraries (named <em class="calibre16">backends</em> in <kbd class="calibre12">Plots</kbd> language), such as <kbd class="calibre12">GR</kbd> or <kbd class="calibre12">PyPlot</kbd>. It basically unifies multiple lower-level plotting packages (backends) under a common API.</p>
<p class="calibre2">As always, start with <kbd class="calibre12"><span>pkg&gt; add Plots</span></kbd> and continue with <kbd class="calibre12"><span>using Plots</span></kbd>.</p>
<p class="calibre2">We're now ready to generate the visualization:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; plot(acton_and_annie_in_common[:,2], acton_and_annie_in_common[:,3], seriestype=:scatter, xticks=0:10, yticks=0:10, lims=(0,11), label="")</strong></pre>
<p class="calibre2">In the preceding snippet, we invoke the <kbd class="calibre12">plot</kbd> function, passing it Acton's and Annie's ratings. As options, we ask it to produce a scatter plot. We also want to make sure that the axes start at <strong class="calibre4">0</strong> and end at 11 (so that value <strong class="calibre4">10</strong> is fully visible), with ticks at each unit. We'll end up with the following plot:</p>
<p class="CDPAlignCenter"><img src="assets/2d60606a-81f7-4823-b621-b1df752e6245.png" class="calibre77"/></p>
<p class="calibre2">By the looks of it, there is a good correlation between the user's movie preferences. But we can do even better.</p>
<p class="calibre2">Julia's ecosystem provides access to yet another powerful package that combines both plotting and statistical features. It's called <kbd class="calibre12"><span>StatPlots</span></kbd> and actually works on top of the <kbd class="calibre12"><span>Plots</span></kbd> package, providing statistical plotting recipes for <kbd class="calibre12"><span>Plots</span></kbd>. It also supports <kbd class="calibre12"><span>DataFrame</span></kbd> visualizations out of the box, so it's a perfect match for our needs.</p>
<p class="calibre2">Let's add it with <kbd class="calibre12"><span>pkg&gt; add StatPlots</span></kbd> and bring it into scope (<kbd class="calibre12"><span>using StatPlots</span></kbd>). We can now use the <kbd class="calibre12"><span>@df</span></kbd> macro that's exposed by <kbd class="calibre12"><span>StatPlots</span></kbd> to generate a scatter plot of our data:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; @df acton_and_annie_in_common scatter([:Acton], [:Annie], smooth = true, line = :red, linewidth = 2, title = "Acton and Annie", legend = false, xlimits = (5, 11), ylimits = (5, 11))</strong> </pre>
<p class="calibre2">The preceding code will produce the following visualization:</p>
<p class="CDPAlignCenter"><img src="assets/bce24b5e-1b1e-4b77-8f99-5269aeb0c174.png" class="calibre78"/></p>
<p class="calibre2">This new plot shows the correlation between the movies, despite the outlier.</p>
<p class="calibre2">Let's compute the Pearson correlation between Acton's and Annie's ratings:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; using Statistics 
julia&gt; cor(acton_and_annie_in_common[:Acton], acton_and_annie_in_common[:Annie]) 
 
0.6324555320336759</strong> </pre>
<p class="calibre2">Pretty much any value over <kbd class="calibre12">0.6</kbd> indicates a good similarity, so it looks like we're onto something.</p>
<p class="calibre2">Now, we can recommend to Annie some of Acton's favorites that she hasn't seen, as follows:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; annies_recommendations = acton_and_annie[(acton_and_annie[:Annie] .== 0) .&amp;  (acton_and_annie[:Acton] .&gt; 0), :]</strong></pre>
<p class="calibre2">This snippet should be easy to understand since it's a slight variation of the common rating formula. From the <span class="calibre5"><kbd class="calibre12">acton_and_annie</kbd> </span><kbd class="calibre12"><span>DataFrame</span></kbd>, we only select the rows where Annie's score is <kbd class="calibre12">0</kbd> (she hasn't rated the movie) and Acton's is greater than <kbd class="calibre12">0</kbd> (he has rated the movie).</p>
<p class="calibre2">We'll get a <kbd class="calibre12"><span>DataFrame</span></kbd> with four rows:</p>
<p class="CDPAlignCenter"><img src="assets/ef28b302-ae0d-4be7-9449-b9460bd13885.png" class="calibre79"/></p>
<p class="calibre2">However, there's a small glitch. We assumed that all the ratings indicate a strong preference, but in this case, many of Acton's ratings are rather an indication of a dislike. With the exception of <kbd class="calibre12">Captain America: Civil War (2016)</kbd>, all the possible recommendations have bad ratings. Luckily, that is easy to fix—we just need to recommend movies that have a high rating, let's say, of at least <kbd class="calibre12">7</kbd>:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; annies_recommendations = acton_and_annie[(acton_and_annie[:Annie] .== 0) .&amp;(acton_and_annie[:Acton] .&gt;= 7 ), :]</strong> </pre>
<p class="calibre2">This leaves us with only one movie, <kbd class="calibre12">Captain America: Civil War (2016)</kbd>:</p>
<p class="CDPAlignCenter"><img src="assets/9266d9fe-9059-42d3-981d-07ec37e7fe1d.png" class="calibre80"/></p>
<p class="calibre2">Now that we understand the logic of user-based recommender systems, let's put all of these steps together to create a simple recommender script.</p>
<p class="calibre2">We'll analyze our users' rating matrix in a script that will take advantage of all the available data to generate recommendations for all of our users.</p>
<p class="calibre2">Here's a possible implementation—please create a u<kbd class="calibre12"><span>ser_based_movie_recommendations.jl</span></kbd> file with the following code. Do make sure that the <kbd class="calibre12"><span>top_10_movies_user_rankings.tsv</span></kbd> file is in the same folder (or update the path in the code to match your location). Here's the code<span class="calibre5">:</span></p>
<pre class="calibre17">using CSV, DataFrames, Statistics<br class="title-page-name"/><br class="title-page-name"/>const minimum_similarity = 0.8<br class="title-page-name"/>const movies = CSV.read("top_10_movies_user_rankings.tsv", delim = '\t')<br class="title-page-name"/><br class="title-page-name"/>function user_similarity(target_user)<br class="title-page-name"/>    similarity = Dict{Symbol,Float64}()<br class="title-page-name"/>    for user in names(movies[:, 2:end])<br class="title-page-name"/>        user == target_user &amp;&amp; continue<br class="title-page-name"/>        ratings = movies[:, [user, target_user]]<br class="title-page-name"/>        common_movies = ratings[(ratings[user] .&gt; 0) .&amp; (ratings[target_user] .&gt; 0), :]<br class="title-page-name"/>        <br class="title-page-name"/>        correlation = try<br class="title-page-name"/>            cor(common_movies[user], common_movies[target_user])<br class="title-page-name"/>        catch<br class="title-page-name"/>            0.0<br class="title-page-name"/>        end<br class="title-page-name"/>        <br class="title-page-name"/>        similarity[user] = correlation<br class="title-page-name"/>    end<br class="title-page-name"/>    <br class="title-page-name"/>    similarity<br class="title-page-name"/>end<br class="title-page-name"/><br class="title-page-name"/>function recommendations(target_user)<br class="title-page-name"/>    recommended = Dict{String,Float64}()<br class="title-page-name"/>    for (user,similarity) in user_similarity(target_user)<br class="title-page-name"/>        similarity &gt; minimum_similarity || continue<br class="title-page-name"/>        ratings = movies[:, [Symbol("Movie title"), user, target_user]]<br class="title-page-name"/>        recommended_movies = ratings[(ratings[user] .&gt;= 7) .&amp; (ratings[target_user] .== 0), :]<br class="title-page-name"/>        <br class="title-page-name"/>        for movie in eachrow(recommended_movies)<br class="title-page-name"/>            recommended[movie[Symbol("Movie title")]] = movie[user] * similarity<br class="title-page-name"/>        end<br class="title-page-name"/>    end<br class="title-page-name"/>    <br class="title-page-name"/>    recommended<br class="title-page-name"/>end<br class="title-page-name"/><br class="title-page-name"/>for user in names(movies)[2:end]<br class="title-page-name"/>    println("Recommendations for $user: $(recommendations(user))")<br class="title-page-name"/>end</pre>
<p class="calibre2">In the preceding snippet, we define two functions, <kbd class="calibre12"><span>user_similarity</span></kbd> and <kbd class="calibre12"><span>recommendations</span></kbd>. They both take, as their single argument, a user's name in the form of a <span class="calibre5">Symbol</span>. This argument matches the column name in our <span class="calibre5"><kbd class="calibre12">movies</kbd> </span><kbd class="calibre12"><span>DataFrame</span></kbd>.</p>
<p class="calibre2">The <kbd class="calibre12"><span>user_similarity</span></kbd> function computes the similarity of our target user (the one passed into the function as the argument) with all the other users and returns a dictionary of the form:</p>
<pre class="calibre17">Dict(<br class="title-page-name"/>    :Comey =&gt; 1.0,<br class="title-page-name"/>    :Dean =&gt; 0.907841,<br class="title-page-name"/>    :Missie =&gt; NaN,<br class="title-page-name"/>    :Kit =&gt; 0.774597,<br class="title-page-name"/>    :Musk =&gt; 0.797512,<br class="title-page-name"/>    :Sam =&gt; 0.0,<br class="title-page-name"/>    :Acton =&gt; 0.632456<br class="title-page-name"/>)</pre>
<p class="calibre2">The <kbd class="calibre12">dict</kbd> represents Annie's similarity with all the other users.</p>
<p class="calibre2">We use the similarities in the <span class="calibre5">recommendations</span> function to pick the relevant users and make recommendations based on their favorite movies, which were not already rated by our target user.</p>
<p class="calibre2">I've also added a little twist to make the recommendations more relevant—a weight factor. This is c<span class="calibre5">omputed by multiplying the user's rating with the user's similarity. If, say, <kbd class="calibre12">Comey</kbd> gives a movie an 8 and is 100% similar to <kbd class="calibre12">Missie</kbd> (c</span><span class="calibre5">orrelation coefficient equals 1), the weight of the recommendation will also be <em class="calibre16">8 (8 * 1)</em>. But if Comey is only 50% similar to Musk (0.5 correlation coefficient), then the weight of the recommendation (corresponding to the estimated rating) will be just <em class="calibre16">4 (8 * 0.5)</em>.</span></p>
<p class="calibre2">At the end of the file, we bootstrap the whole process by looping through an array of all the users, and we produce and print the movie recommendations for each of them.</p>
<p class="calibre2">Running this will output movie recommendations, together with their weights for each of our users:</p>
<pre class="calibre17">Recommendations for Acton: Dict("Moonlight (2016)"=&gt;9.0)<br class="title-page-name"/>Recommendations for Annie: Dict("La La Land (2016)"=&gt;8.0)<br class="title-page-name"/>Recommendations for Comey: Dict("The Jungle Book (2016)"=&gt;7.0,"Moana (2016)"=&gt;7.0,"Moonlight (2016)"=&gt;9.0)<br class="title-page-name"/>Recommendations for Dean: Dict("Moana (2016)"=&gt;10.0,"Zootopia (2016)"=&gt;10.0)<br class="title-page-name"/>Recommendations for Kit: Dict("Hell or High Water (2016)"=&gt;10.0,"Arrival (2016)"=&gt;10.0,"La La Land (2016)"=&gt;9.0,"Moonlight (2016)"=&gt;10.0,"Manchester by the Sea (2016)"=&gt;8.0)<br class="title-page-name"/>Recommendations for Missie: Dict("The Jungle Book (2016)"=&gt;8.0,<br class="title-page-name"/>"Moana (2016)"=&gt;8.0, "La La Land (2016)"=&gt;8.0,"Captain America: Civil War (2016)"=&gt;10.0,"Finding Dory (2016)"=&gt;7.0,"Zootopia (2016)"=&gt;9.0)<br class="title-page-name"/>Recommendations for Musk: Dict{String,Float64}()<br class="title-page-name"/>Recommendations for Sam: Dict("Hell or High Water (2016)"=&gt;10.0,<br class="title-page-name"/>"La La Land (2016)"=&gt;9.0,"Moonlight (2016)"=&gt;10.0,"Zootopia (2016)"=&gt;7.0,"Manchester by the Sea (2016)"=&gt;8.0)</pre>
<p class="calibre2">The data looks quite good, considering that this is a toy example. A production quality recommender system should be based on millions of such ratings.</p>
<p class="calibre2">However, if you look closely, you might notice that something's not quite right—the <kbd class="calibre12">Recommendations for Kit</kbd>. Kit likes kids movies—light-hearted animated comedies. Our system recommends him, with quite a lot of weight, a lot of dramas! What gives? If we look at the similarity data for Kit, we'll see that he's very well correlated with Dean and Dean likes drama. That might sound weird, but it's actually correct if we check the data:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; movies[:, [Symbol("Movie title"), :Dean, :Kit]]</strong> </pre>
<p class="calibre2">The output is as follows:</p>
<p class="CDPAlignCenter"><img src="assets/55770a5d-31dc-478e-8e62-acd08f85223d.png" class="calibre81"/></p>
<p class="calibre2">Notice how the only movies they both watched are <kbd class="calibre12">The Jungle Book (2016)</kbd> and <kbd class="calibre12">Finding Dory (2016)</kbd>, and how the ratings are correlated since both give higher ratings to <kbd class="calibre12">Finding Dory (2016)</kbd>. Therefore, there is a strong positive correlation between Dean and Kit. But what our algorithm doesn't take into account is that even if Dean likes <kbd class="calibre12">Finding Dory (2016)</kbd> more than <kbd class="calibre12">The Jungle Book (2016)</kbd>, he still doesn't really like either, as indicated by the low ratings of <kbd class="calibre12">4</kbd> and <kbd class="calibre12">2</kbd>, respectively.</p>
<p class="calibre2">The solution is quite simple, though—we just need to remove ratings that don't indicate a strong positive preference. Again, we can use a rating equal to or larger than <kbd class="calibre12">7</kbd> to count as a like. So, in the <kbd class="calibre12"><span>user_similarity</span></kbd> function, please look for the following line:</p>
<pre class="calibre17">common_movies = ratings[(ratings[user] .&gt; 0) .&amp; (ratings[target_user] .&gt; 0), :]</pre>
<p class="calibre2">Replace <kbd class="calibre12">ratings[user] .&gt; 0</kbd> with <kbd class="calibre12">ratings[user] .&gt; 7</kbd> so that the whole line now reads as follows:</p>
<pre class="calibre17">common_movies = ratings[Array(ratings[user] .&gt; 7) .&amp; Array(ratings[target_user] .&gt; 0), :]</pre>
<p class="calibre2">What this does is now compute similarity only based on favorites. As a result, <kbd class="calibre12">Kit</kbd> is no longer similar to <kbd class="calibre12">Dean</kbd> (the correlation coefficient is <kbd class="calibre12">0</kbd>).</p>
<p class="calibre2">Another consequence of the fact that our recommendations are more targeted is that we no longer have recommendations for all the users—but this is, again, caused by the fact that we're working with a very small example dataset. Here are the final recommendations:</p>
<pre class="calibre17">Recommendations for Acton: Dict("Moonlight (2016)"=&gt;9.0) 
Recommendations for Annie: Dict{String,Float64}() 
Recommendations for Comey: Dict( 
"Moana (2016)"=&gt;9.0, 
"Moonlight (2016)"=&gt;9.0) 
Recommendations for Dean: Dict( 
"Moana (2016)"=&gt;8.0, 
"Zootopia (2016)"=&gt;9.0) 
Recommendations for Kit: Dict{String,Float64}() 
Recommendations for Missie: Dict{String,Float64}() 
Recommendations for Musk: Dict{String,Float64}() 
Recommendations for Sam: Dict{String,Float64}() </pre>
<p class="calibre2">We only have suggestions for Acton, Comey, and Dean, but they are now much more accurate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Item-item CF</h1>
                </header>
            
            <article>
                
<p class="calibre2">User-based CF works quite well and is widely used in production in the wild, but it does have a few considerable downsides. First, it's difficult to get enough preference information from users, leaving many of them without a solid base for relevant recommendations. Second, as the platform and the underlying business grows, the number of users will grow much faster than the number of items. Netflix, for example, to keep the discussion in the familiar area of movies, grows its user base massively by expanding into new countries, while the production of movies stays pretty much the same on a yearly basis. Finally, the user's data does change quite a lot, so the rating matrix would have to be updated often, which is a resource-intensive and time-consuming process.</p>
<p class="calibre2">These problems became painfully obvious at Amazon, some 10 years ago. They realized that since the number of products grows at a much slower rate than the number of users, instead of computing user similarity, they could compute item similarity and make recommendations stemming from the list of related items.</p>
<p class="calibre2">The following diagram should help you understand the difference between item-based (or item-item) and user-based (or user-item) CF:</p>
<p class="CDPAlignCenter"><img src="assets/cfd88d93-1653-4b45-aa1e-324b48177322.png" class="calibre82"/></p>
<p class="calibre2">The preceding diagram shows that <strong class="calibre4">Annie</strong> purchased <strong class="calibre4">A</strong>, <strong class="calibre4">B</strong>, and <strong class="calibre4">E</strong>, <strong class="calibre4">Bob</strong> purchased <strong class="calibre4">A</strong>, <strong class="calibre4">B</strong>, and <strong class="calibre4">D</strong>, and <strong class="calibre4">Charley</strong> purchased <strong class="calibre4">A</strong> and <strong class="calibre4">C</strong>. The purchasing behavior of <strong class="calibre4">Annie</strong> and <strong class="calibre4">Bob</strong> will indicate a correlation between <strong class="calibre4">A</strong> and <strong class="calibre4">B</strong>, and since <strong class="calibre4">Charley</strong> already purchased <strong class="calibre4">A</strong> but not <strong class="calibre4">B</strong>, we can recommend <strong class="calibre4">Charley</strong> to take a look at <strong class="calibre4">B</strong>.</p>
<p class="calibre2">From an implementation perspective, there are similarities to user-item CF, but it is more involved as it includes an extra layer of analysis. Let's try this out with our imaginary movie rankings. Let's create a new file called <kbd class="calibre12">item_based_recommendations.jl</kbd> to host our code.</p>
<p class="calibre2">Here is the complete implementation:</p>
<pre class="calibre17">using CSV, DataFrames, DelimitedFiles, Statistics<br class="title-page-name"/><br class="title-page-name"/>const minimum_similarity = 0.8<br class="title-page-name"/><br class="title-page-name"/>function setup_data()<br class="title-page-name"/>    movies = readdlm("top_10_movies_user_rankings.tsv", '\t')<br class="title-page-name"/>    movies = permutedims(movies, (2,1))<br class="title-page-name"/>    movies = convert(DataFrame, movies)<br class="title-page-name"/>    <br class="title-page-name"/>    names = convert(Array, movies[1, :])[1,:]<br class="title-page-name"/>    names!(movies, [Symbol(name) for name in names])<br class="title-page-name"/>    deleterows!(movies, 1)<br class="title-page-name"/>    rename!(movies, [Symbol("Movie title") =&gt; :User])<br class="title-page-name"/>end<br class="title-page-name"/><br class="title-page-name"/>function movie_similarity(target_movie)<br class="title-page-name"/>    similarity = Dict{Symbol,Float64}()<br class="title-page-name"/>    for movie in names(movies[:, 2:end])<br class="title-page-name"/>        movie == target_movie &amp;&amp; continue<br class="title-page-name"/>        ratings = movies[:, [movie, target_movie]]<br class="title-page-name"/>        common_users = ratings[(ratings[movie] .&gt;= 0) .&amp; (ratings[target_movie] .&gt; 0), :]<br class="title-page-name"/>    <br class="title-page-name"/>        correlation = try<br class="title-page-name"/>            cor(common_users[movie], common_users[target_movie])<br class="title-page-name"/>        catch<br class="title-page-name"/>            0.0<br class="title-page-name"/>        end<br class="title-page-name"/><br class="title-page-name"/>        similarity[movie] = correlation<br class="title-page-name"/>    end<br class="title-page-name"/><br class="title-page-name"/>    # println("The movie $target_movie is similar to $similarity")<br class="title-page-name"/>    similarity<br class="title-page-name"/>end<br class="title-page-name"/><br class="title-page-name"/>function recommendations(target_movie)<br class="title-page-name"/>    recommended = Dict{String,Vector{Tuple{String,Float64}}}()<br class="title-page-name"/>    # @show target_movie<br class="title-page-name"/>    # @show movie_similarity(target_movie)<br class="title-page-name"/><br class="title-page-name"/>    for (movie, similarity) in movie_similarity(target_movie)<br class="title-page-name"/>        movie == target_movie &amp;&amp; continue<br class="title-page-name"/>        similarity &gt; minimum_similarity || continue<br class="title-page-name"/>        # println("Checking to which users we can recommend $movie")<br class="title-page-name"/>        recommended["$movie"] = Vector{Tuple{String,Float64}}()<br class="title-page-name"/><br class="title-page-name"/>        for user_row in eachrow(movies)<br class="title-page-name"/>            if user_row[target_movie] &gt;= 5<br class="title-page-name"/>                # println("$(user_row[:User]) has watched $target_movie so we can recommend similar movies")<br class="title-page-name"/>                if user_row[movie] == 0<br class="title-page-name"/>                    # println("$(user_row[:User]) has not watched $movie so we can recommend it")<br class="title-page-name"/>                    # println("Recommending $(user_row[:User]) the movie $movie")<br class="title-page-name"/>                    push!(recommended["$movie"], (user_row[:User], user_row[target_movie] * similarity))<br class="title-page-name"/>                end<br class="title-page-name"/>            end<br class="title-page-name"/>        end<br class="title-page-name"/>    end<br class="title-page-name"/><br class="title-page-name"/>    recommended<br class="title-page-name"/>end<br class="title-page-name"/><br class="title-page-name"/>const movies = setup_data()<br class="title-page-name"/>println("Recommendations for users that watched Finding Dory (2016): $(recommendations(Symbol("Finding Dory (2016)")))")</pre>
<p class="calibre2">To keep the code simpler, we're only generating recommendations for a single movie—but it should be relatively simple to extend it to come up with recommendations for each movie in our list (you can try this as an exercise). We'll only suggest similar movies to the users that have watched <kbd class="calibre12">Finding Dory (2016)</kbd>.</p>
<p class="calibre2">Let's take it apart and see how the script works.</p>
<div class="packttip">As you can see, I've added some <kbd class="calibre24"><span>println</span></kbd> and <kbd class="calibre24">@show</kbd> calls that output extra debug information—they're commented out, but feel free to uncomment them when running the file to help you better understand what each section does and what the workflow of the code is.</div>
<p class="calibre2">Setting up our data matrix is more difficult now. We need to transpose our initial dataset, that is, rotate it. The <kbd class="calibre12"><span>setup_data</span></kbd> function is dedicated to this task alone—loading the data file, transposing the matrix, and setting up the data into a <kbd class="calibre12"><span>DataFrame</span></kbd>. It's a proper <strong class="calibre4">extract, transform, load</strong> (<strong class="calibre4">ETL</strong>) process in just a few lines of code, which is pretty cool! Let's look at this closely—it's quite a common day-to-day data science task.</p>
<p class="calibre2">In the first line of the function, we load the data into a Julia matrix. The <kbd class="calibre12"><span>readdlm</span></kbd> function is not as powerful as <kbd class="calibre12"><span>DataFrames</span></kbd>, so it has no knowledge of headers, gobbling everything into an <kbd class="calibre12">Array</kbd>:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; movies = readdlm("top_10_movies_user_rankings.tsv", '\t')</strong> </pre>
<p class="calibre2">We'll end up with the following matrix:</p>
<p class="CDPAlignCenter"><img src="assets/21550340-67d6-4f77-afaf-7dfd21684eb9.png" class="calibre83"/></p>
<p class="calibre2">As we can see, the headings are mixed with the actual data.</p>
<p class="calibre2">Now, we need to transpose the matrix. Unfortunately, transposing doesn't work smoothly for all kinds of matrices in Julia yet, and the recommended way is to do this via <kbd class="calibre12"><span>permutedims</span></kbd>:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; movies = permutedims(movies, (2,1))</strong> </pre>
<p class="calibre2">The output is as follows:</p>
<p class="CDPAlignCenter"><img src="assets/98126d1d-de90-4aed-a074-3d811562d948.png" class="calibre84"/></p>
<p class="calibre2">We're getting closer!</p>
<p class="calibre2">Next, we convert it into a <kbd class="calibre12"><span>DataFrame</span></kbd>:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; movies = convert(DataFrame, movies)</strong> </pre>
<p class="calibre2"><span class="calibre5">The output is as </span><span class="calibre5">follows:</span></p>
<p class="CDPAlignCenter"><img src="assets/d83bebec-1537-4045-af08-2594af140382.png" class="calibre85"/></p>
<div class="packttip">If you run the previous code yourself, you might notice that the REPL will omit some of the <kbd class="calibre24">DataFrame</kbd> columns, since the output is too wide. To get Julia to display all the columns, like in this snippet, you can use the <kbd class="calibre24">showall</kbd> function, as in <kbd class="calibre24">showall(movies)</kbd>.</div>
<p class="calibre2">It looks good, but we need to give the columns proper names, using the data that is now on the first row. Let's extract all the columns names into a <kbd class="calibre12">Vector</kbd>:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; movie_names = convert(Array, movies[1, :])[1,:] 
11-element Array{Any,1}: 
 "Movie title" 
 "Moonlight (2016)" 
 "Zootopia (2016)" 
 "Arrival (2016)" 
 "Hell or High Water (2016)" 
 "La La Land (2016)" 
 "The Jungle Book (2016)" 
 "Manchester by the Sea (2016)" 
 "Finding Dory (2016)" 
 "Captain America: Civil War (2016)" 
 "Moana (2016)"</strong> </pre>
<p class="calibre2">Now, we can use it to name the columns:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; names!(movies, [Symbol(name) for name in movie_names])</strong> </pre>
<p class="calibre2"><span class="calibre5">The output is as follows:</span></p>
<p class="CDPAlignCenter"><img src="assets/4f684580-a0ec-4773-9c5c-f1b37e44829c.png" class="calibre86"/></p>
<p class="calibre2">Our <kbd class="calibre12"><span>DataFrame</span></kbd> looks better already. The only things left to do are to remove the extra row with the headers and change the <kbd class="calibre12">Movie title</kbd> header to <kbd class="calibre12">User</kbd>:</p>
<pre class="calibre17"><strong class="calibre1">julia&gt; deleterows!(movies, 1) julia&gt; rename!(movies, Symbol("Movie title") =&gt; :User)</strong> </pre>
<p class="calibre2"><span class="calibre5">The output is as follows:</span></p>
<p class="CDPAlignCenter"><img src="assets/2db2e1c8-55e2-46f4-aa4d-a354f54c782a.png" class="calibre87"/></p>
<p class="calibre2">All done—our ETL process is complete!</p>
<p class="calibre2">We start our recommender by invoking the <kbd class="calibre12"><span>recommendations</span></kbd> function, passing in the name of the movie, <kbd class="calibre12">Finding Dory (2016)</kbd>, as a <kbd class="calibre12"><span>Symbol</span></kbd>. The first thing this function does is invoke the <kbd class="calibre12"><span>movie_similarity</span></kbd> function, which computes which other movies are similar to <kbd class="calibre12">Finding Dory (2016)</kbd> based on the users' ratings. For our target movie, we'll get the following results:</p>
<pre class="calibre17">Dict( 
Symbol("La La Land (2016)")=&gt;-0.927374, 
Symbol("Captain America: Civil War (2016)")=&gt;-0.584176, 
Symbol("The Jungle Book (2016)")=&gt;0.877386, 
Symbol("Manchester by the Sea (2016)")=&gt;-0.785933, 
Symbol("Arrival (2016)")=&gt;-0.927809, 
Symbol("Zootopia (2016)")=&gt;0.826331, 
Symbol("Moonlight (2016)")=&gt;-0.589269, 
Symbol("Hell or High Water (2016)")=&gt;-0.840462, 
Symbol("Moana (2016)")=&gt;0.933598<br class="title-page-name"/>) </pre>
<p class="calibre2">We can see here that there's an almost perfect negative correlation with <kbd class="calibre12">La La Land (2016)</kbd> (so users that like <kbd class="calibre12">La La Land (2016)</kbd> do not like <kbd class="calibre12">Finding Dory (2016)</kbd>). There is also a very strong positive correlation with <kbd class="calibre12">The Jungle Book (2016)</kbd>, <kbd class="calibre12">Zootopia (2016)</kbd>, and <kbd class="calibre12">Moana (2016)</kbd>, which makes sense, since they're all animations.</p>
<p class="calibre2">Here is where the logic gets a bit more complicated. Now, we have a list of movies that are similar to <kbd class="calibre12">Finding Dory (2016)</kbd>. To make recommendations, we want to look at all the users that have watched <kbd class="calibre12">Finding Dory (2016)</kbd> (and gave it a good enough rating), and suggest similar movies that they haven't watched yet (movies that have a rating of 0). This time, we'll be using a minimum rating of 5 instead of the previous 7, since given our very limited dataset, 7 would be too restrictive and would yield no recommendations. We'll compute the weight of the suggestions as the product between the user's rating of <kbd class="calibre12">Finding Dory (2016)</kbd> and the correlation coefficient between <kbd class="calibre12">Finding Dory (2016)</kbd> and the recommended movie. Makes sense? Let's see it in action!</p>
<p class="calibre2">If we run the script, we get the following output:</p>
<pre class="calibre17">Recommendations for users that watched Finding Dory (2016): <br class="title-page-name"/>Dict( 
    "The Jungle Book (2016)"=&gt; Tuple{String,Float64}[("Comey", 4.38693)], 
    "Moana (2016)"=&gt; Tuple{String,Float64}[("Comey", 4.66799)], 
    "Zootopia (2016)"=&gt; Tuple{String,Float64}[]<br class="title-page-name"/>)</pre>
<p class="calibre2">The only user that would be (kind of) interested in watching movies similar to <kbd class="calibre12">Finding Dory (2016)</kbd> in our small dataset is <kbd class="calibre12">Comey</kbd>—but the recommendations won't be great. The algorithm estimates a weight (and thus, a rating) of <kbd class="calibre12">4.38693</kbd> for <kbd class="calibre12">The Jungle Book (2016)</kbd> and <kbd class="calibre12">4.66799</kbd> for <kbd class="calibre12">Moana (2016)</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">This concludes the first part of our journey into recommender systems. They are an extremely important part of today's online business models and their usefulness is ever-growing, in direct relation to the exponential growth of data generated by our connected software and hardware. Recommender systems are a very efficient solution to the information overload problem—or rather, an information filter problem. Recommenders provide a level of filtering that's appropriate for each user, turning information, yet again, into a vector of customer empowerment.</p>
<p class="calibre2">Although it's critical to understand how the various types of recommender systems work, in order to be able to choose the right algorithm for the types of problems you'll solve in your work as a data scientist, implementing production-grade systems by hand is not something most people do. As with almost everything in the realm of software development, it's best to use stable, powerful, and mature existing libraries when they're available.</p>
<p class="calibre2">In the next chapter, we'll learn how to build a more powerful recommender system using existing Julia libraries. We'll generate recommendations for a dating site, taking advantage of publicly available and anonymized dating data. In the process, we'll learn about yet another type of recommender system, called model-based (as a side note, all of the algorithms that were discussed in this chapter were memory-based, but don't worry—I'll explain everything in a minute).</p>


            </article>

            
        </section>
    </body></html>