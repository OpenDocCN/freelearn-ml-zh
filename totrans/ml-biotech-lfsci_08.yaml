- en: 'Chapter 6: Unsupervised Machine Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章：无监督机器学习
- en: Oftentimes, many data science tutorials that you will encounter in courses and
    training revolve around the field of **Supervised Machine Learning** (**SML**)
    in which data and its corresponding labels are used to develop predictive models
    to automate tasks. However, in real-world data, the availability of pre-labeled
    or categorized data is seldom the case, and most datasets you will encounter will
    be in their raw and unlabeled form. For cases such as these, or whose primary
    objectives are more exploratory or not necessarily of automatable fashion, the
    field of unsupervised ML will be of great value.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你将在课程和培训中遇到的许多数据科学教程都围绕**监督机器学习**（SML）领域展开，在该领域中，数据和相应的标签被用来开发预测模型以自动化任务。然而，在现实世界的数据中，预先标记或分类数据的可用性很少，你将遇到的多数数据集将以原始和无标签的形式存在。对于这些情况，或者主要目标是更探索性的或不是必然可自动化的，无监督机器学习的领域将非常有价值。
- en: 'Over the course of this chapter, we will explore many methods relating to the
    areas of clustering and **Dimensionality Reduction** (**DR**). The main topics
    we will explore are listed here:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的整个过程中，我们将探讨与聚类和**降维**（DR）领域相关的许多方法。我们将探讨的主要主题如下：
- en: Introduction to **Unsupervised Learning** (**UL**)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**（UL）简介'
- en: Understanding clustering algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解聚类算法
- en: Tutorial – breast cancer prediction via clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教程 – 通过聚类进行乳腺癌预测
- en: Understanding DR
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 DR
- en: Tutorial – exploring DR models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教程 – 探索 DR 模型
- en: With these topics in mind, let's now go ahead and get started!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在心中牢记这些主题后，我们现在就可以开始着手了！
- en: Introduction to UL
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UL 简介
- en: We will define UL as a subset of ML in which models are trained without the
    existence of categories or labels. Unlike its supervised counterpart, UL relies
    on the development of models to capture patterns in the form of features to extract
    insights from the data. Let's now take a closer look at the two main categories
    of UL.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 UL 定义为机器学习的一个子集，其中模型是在没有类别或标签存在的情况下进行训练的。与它的监督学习对应物不同，UL 依赖于模型的发展来捕捉以特征形式存在的模式，从而从数据中提取洞察。现在让我们更详细地看看
    UL 的两个主要类别。
- en: 'There exist many different methods and techniques that fall within the scope
    of UL. We can group these methods into two main categories: those with **discrete**
    data (**clustering**) and those with **continuous** data (**DR**). We can see
    a graphical representation of this here:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 UL 的范围内存在许多不同的方法和技术。我们可以将这些方法分为两大类：具有**离散**数据（聚类）的方法和具有**连续**数据（DR）的方法。我们在这里可以看到这种图形表示：
- en: '![Figure 6.1 – The two types of UL ](img/B17761_06_001.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – UL 的两种类型](img/B17761_06_001.jpg)'
- en: Figure 6.1 – The two types of UL
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – UL 的两种类型
- en: In each of these techniques, data is either grouped or transformed in order
    to determine labels or extract insights and representations without knowing the
    labels or categories of the data ahead of time. Take, for example, the breast
    cancer dataset we worked with in [*Chapter 5*](B17761_05_Final_JM_ePub.xhtml#_idTextAnchor082),
    *Understanding Machine Learning*, in which we developed a classification model.
    We trained the model by explicitly telling it which observations within the data
    were malignant and which were benign, thus allowing it to learn the differences
    through the features. Similar to our supervised model, we can train an unsupervised
    **clustering** model to make similar predictions by clustering our data into groups
    (malignant and benign) without knowing the labels or classes ahead of time. There
    are many different types of clustering models we can use, and we will explore
    a few of these in the following section, and others further along in this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些技术中，数据要么被分组，要么被转换，以便在不知道数据事先的标签或类别的情况下确定标签或提取洞察和表示。以我们在 [*第 5 章*](B17761_05_Final_JM_ePub.xhtml#_idTextAnchor082)
    中使用的乳腺癌数据集为例，*理解机器学习*，我们开发了一个分类模型。我们通过明确告诉模型数据中哪些观察结果是恶性的，哪些是良性的来训练模型，从而允许它通过特征学习差异。与我们的监督模型类似，我们可以训练一个无监督的**聚类**模型，通过将我们的数据聚类成组（恶性和良性）来做出类似的预测，而不需要事先知道标签或类别。我们可以使用许多不同类型的聚类模型，我们将在下一节中探索其中的一些，并在本章的后面部分探索其他模型。
- en: In addition to clustering our data, we can also explore and transform our data
    through a method known as **DR**, which we will define as the transformation of
    high-dimensional data into a lower-dimensional space in which the meaningful properties
    of the features are retained. Data transformations can either be used to reduce
    the number of features down to a few or to engineer new and useful features for
    a given dataset. One of the most popular methods that fall within this category
    is a process known as **Principal Component Analysis** (**PCA**)—we will explore
    this specific model in detail further along in this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除了聚类我们的数据外，我们还可以通过一种称为**DR**的方法来探索和转换我们的数据，我们将将其定义为将高维数据转换到低维空间的过程，在这个空间中保留了特征的有意义属性。数据转换可以用来将特征数量减少到几个，或者为给定数据集设计新的和有用的特征。属于这一类最受欢迎的方法之一是称为**主成分分析**（**PCA**）的过程——我们将在本章后面详细探讨这个特定模型。
- en: Within the scope of both of these categories falls a niche field that is not
    quite yet a third category given its broad application—this is known as **anomaly
    detection**. Anomaly detection within the scope of UL, as the name suggests, is
    a method for the detection of anomalies within an unlabeled dataset. Note that,
    unlike clustering methods in which there is generally a balance within the different
    labels of a dataset (for example, 50:50), anomalies tend to be rare in the sense
    that the number of observations is usually anything but balanced. The most popular
    methods today when it comes to anomaly detection from an unsupervised perspective
    tend to not only include **clustering** and **DR**, but also **neural networks**
    and **isolation forests**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个范畴的范围内，有一个应用广泛的领域，它还不完全是一个第三类，因为它具有广泛的应用——这被称为**异常检测**。在UL的范畴内，异常检测正如其名，是一种在未标记数据集中检测异常的方法。请注意，与聚类方法不同，其中数据集的不同标签之间通常有一个平衡（例如，50:50），但异常通常很少见，因为观察的数量通常不是平衡的。从无监督的角度来看，目前最流行的异常检测方法不仅包括**聚类**和**DR**，还包括**神经网络**和**隔离森林**。
- en: Now that we've gained a sense of some of the high-level concepts relating to
    UL and know our objectives, let's now go ahead and get started with some details
    and examples for each.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对UL的一些高级概念有了感觉，并且知道了我们的目标，那么现在让我们开始一些细节和每个例子。
- en: Understanding clustering algorithms
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解聚类算法
- en: One of the most common methods that fall within the category of UL is **clustering
    analysis**. The main idea behind clustering analysis is the grouping of data into
    two or more categories of a similar nature to form groups or **clusters**. Within
    this section, we will explore these different clustering models, and subsequently
    apply our knowledge in a real-world scenario concerning the development of predictive
    models for the detection of breast cancer. Let's go ahead and explore some of
    the most common clustering algorithms.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在UL的范畴内，最常见的属于该类的方法是**聚类分析**。聚类分析背后的主要思想是将数据分组到两个或更多具有相似性质的类别中，形成组或**簇**。在本节中，我们将探讨这些不同的聚类模型，并随后将我们的知识应用于一个现实世界的场景，即开发用于检测乳腺癌的预测模型。让我们继续探索一些最常见的聚类算法。
- en: Exploring the different clustering algorithms
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索不同的聚类算法
- en: 'There exists not one, but a broad spectrum of clustering algorithms, each with
    its own approach to how to best cluster data depending on the dataset at hand.
    We can divide these clustering algorithms into two general categories: **hierarchical**
    and **partitional** clustering. We can see a graphical representation of this
    here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 存在的不仅仅是单一的一种，而是一系列广泛的聚类算法，每种算法都有其独特的处理数据聚类的最佳方法，这取决于手头的数据集。我们可以将这些聚类算法分为两大类：**层次聚类**和**划分聚类**。我们可以在下面看到这种分类的图形表示：
- en: '![Figure 6.2 – The two types of clustering algorithms ](img/B17761_06_002.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 两种聚类算法](img/B17761_06_002.jpg)'
- en: Figure 6.2 – The two types of clustering algorithms
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 两种聚类算法
- en: With these different areas of clustering in mind, let's now go ahead and explore
    these in more detail, beginning with hierarchical clustering.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑了这些不同的聚类领域之后，现在让我们更详细地探讨这些内容，从层次聚类开始。
- en: Hierarchical clustering
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层次聚类
- en: '**Hierarchical clustering**, as the name suggests, is a method that attempts
    to cluster data based on a given hierarchy using two types of approaches: **agglomerative**
    or **divisive**. Agglomerative clustering is known as a *bottom-up* approach in
    which each observation in a dataset is assigned its own cluster and is subsequently
    merged with other clusters to form a hierarchy. Alternatively, **divisive clustering**
    is a *top-down* approach in which all observations for a given dataset begin in
    a single cluster and are then split up. We can see a graphical representation
    of this here:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**层次聚类**，正如其名所示，是一种尝试根据给定的层次结构使用两种类型的方法（**聚合**或**分解**）对数据进行聚类的算法。聚合聚类是一种**自下而上**的方法，其中数据集中的每个观测值都被分配给其自己的聚类，随后与其他聚类合并以形成一个层次结构。另一方面，**分解聚类**是一种**自上而下**的方法，其中给定数据集的所有观测值最初在一个单独的聚类中，然后被分割。我们可以在这里看到这种图形表示：'
- en: '![Figure 6.3 – The difference between agglomerative and divisive clustering  ](img/B17761_06_003.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 聚类和分解聚类的区别](img/B17761_06_003.jpg)'
- en: Figure 6.3 – The difference between agglomerative and divisive clustering
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 聚类和分解聚类的区别
- en: With the concept of hierarchical clustering in mind, we can imagine a number
    of useful applications this can help us with when it comes to phylogenetic trees
    and other areas of biology. On the other hand, there also exist other methods
    of clustering in which hierarchy is not accounted for, such as when using **Euclidean**
    distance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到层次聚类的概念，我们可以想象出许多有用的应用，这可以帮助我们在系统发育树和其他生物学领域。另一方面，也存在其他聚类方法，其中没有考虑层次结构，例如使用**欧几里得**距离时。
- en: Euclidean distance
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: In addition to hierarchical clustering, we also have a set of models that fall
    under the idea of **partition-based clustering**. The main idea here is separating
    or partitioning your dataset to form clusters using a given method. Two of the
    most common types of partition-based clustering are **distance-based clustering**
    and **probability-based clustering**. When it comes to distance-based clustering,
    the main idea here is determining whether a given data point belongs to a cluster
    based solely on distance such as **Euclidean distance**. An example of this is
    the **K-Means** clustering algorithm—one of the most common clustering algorithms,
    given its simplicity.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了层次聚类之外，我们还有一套属于**基于划分的聚类**思想的模型。这里的核心理念是使用给定方法将数据集分割或划分成聚类。两种最常见的基于划分的聚类类型是**基于距离的聚类**和**基于概率的聚类**。在基于距离的聚类中，这里的核心理念是仅根据距离（如**欧几里得距离**）来确定给定数据点是否属于一个聚类。一个例子是**K-Means**聚类算法——这是一种最常用的聚类算法，鉴于其简单性。
- en: 'Note that **Euclidean** distance, sometimes referred to as **Pythagorean**
    distance, from a mathematical perspective, is defined as the distance between
    two points on a Cartesian coordinate system. For example, for two points, *p*
    (*p1*, *p2*) and *q* (*q1*, *q2*), the Euclidean distance can be calculated as
    follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，从数学角度来看，**欧几里得**距离，有时也称为**毕达哥拉斯**距离，定义为笛卡尔坐标系上两点之间的距离。例如，对于两个点，*p* (*p1*,
    *p2*) 和 *q* (*q1*, *q2*)，欧几里得距离可以计算如下：
- en: '![](img/Formula_B17761_06_001.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761_06_001.jpg)'
- en: 'Within the context of two dimensions, this model is fairly simple and easy
    to calculate. However, the complexity of this model can increase when given more
    dimensions, simply represented as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维的上下文中，这个模型相当简单且易于计算。然而，当给定更多维度时，该模型的复杂性会增加，简单表示如下：
- en: '![](img/Formula_B17761_06_002.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761_06_002.jpg)'
- en: Now that we have gained a better sense of the concept of Euclidean distance,
    let's now take a look at an actual application known as K-Means.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经更好地理解了欧几里得距离的概念，让我们现在看看一个实际的应用，即K-Means。
- en: K-Means clustering
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-Means聚类
- en: With the concept of Euclidean distance in mind, let's now take a close look
    at how this can be applied within the context of K-Means. The K-Means algorithm
    attempts to cluster data by separating samples into *k* groups consisting of equal
    variance and minimizing a **criterion** (inertia). The algorithm's objective is
    to select *k* **centroids** that minimize the inertia.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到欧几里得距离的概念，我们现在来仔细看看它如何在K-Means的上下文中应用。K-Means算法试图通过将样本分离成具有相等方差的*k*组来聚类数据，并最小化一个**标准**（惯性）。该算法的目标是选择*k*个**质心**以最小化惯性。
- en: 'The K-Means model is quite simple in the sense that it operates in three simple
    steps, represented as stars in the following diagram:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，K-Means模型非常简单，因为它只通过三个简单的步骤进行操作，如下面的图中用星号表示：
- en: '![Figure 6.4 – K-Means clustering steps ](img/B17761_06_004.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – K-Means聚类步骤](img/B17761_06_004.jpg)'
- en: Figure 6.4 – K-Means clustering steps
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – K-Means聚类步骤
- en: First, a specified number of *k* **centroids** are randomly initialized. Second,
    each of the observations, represented by the circles, is then clustered based
    on distance. The mean of all observations in a given cluster is then calculated,
    and the centroid is moved to that mean. The process repeats over and over until
    convergence is reached based on a predetermined threshold.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，随机初始化指定数量的*k*个**质心**。其次，每个观测值（用圆圈表示）根据距离进行聚类。然后计算给定簇中所有观测值的平均值，并将质心移动到该平均值。这个过程会不断重复，直到达到预定的阈值为止，从而实现收敛。
- en: '**K-Means** is one of the most commonly used clustering algorithms out there,
    given its simplicity and relatively acceptable computation. It works well with
    high-dimensional data and is relatively easy to implement. However, it does have
    its limitations in the sense that it does make the assumption that the clusters
    are of a spherical nature, which often leads to the misgrouping of data with clusters
    of non-spherical shapes. Take, for example, another dataset in which the clusters
    are not of a spherical nature but are more ovular. The application of the **K-Means**
    model, which operates on the notion of **distance**, would not yield the most
    accurate results, as shown in the following screenshot:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-Means** 是最常用的聚类算法之一，鉴于其简单性和相对可接受的计算能力。它适用于高维数据，并且相对容易实现。然而，它确实有其局限性，即它假设簇是球形的，这往往会导致非球形簇的数据被错误分组。以另一个簇不是球形而是更像椭圆形的数据集为例。基于**距离**概念的**K-Means**模型的应用不会产生最准确的结果，如下面的截图所示：'
- en: '![Figure 6.5 – K-Means clustering with non-spherical clusters ](img/B17761_06_005.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 非球形簇的K-Means聚类](img/B17761_06_005.jpg)'
- en: Figure 6.5 – K-Means clustering with non-spherical clusters
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 非球形簇的K-Means聚类
- en: When operating with non-spherical clusters, a good alternative to a **distance**-based
    model would be a statistical-based approach such as a **Gaussian Mixture Model**
    (**GMM**).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当与非球形簇一起操作时，一个基于**距离**的模型的好替代方案是统计方法，如**高斯混合模型**（**GMM**）。
- en: GMMs
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GMMs
- en: GMMs, within the context of clustering, are algorithms that consist of a particular
    number of **Gaussian distributions**. Each of these distributions represents a
    particular cluster. So far within the confines of this book, we have not yet discussed
    Gaussian distributions—a concept you will often hear about and come across throughout
    your career as a data scientist. Let's go ahead and define this.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类的背景下，GMMs是一组由特定数量的**高斯分布**组成的算法。每个分布代表一个特定的簇。到目前为止，在这本书的范围内，我们还没有讨论高斯分布——这是一个你作为数据科学家在整个职业生涯中经常会听到并遇到的概念。让我们继续定义这个概念。
- en: 'A **Gaussian distribution** can be thought of as a statistical equation representing
    data points that are symmetrically distributed around their mean value. You will
    often hear this distribution referred to as a bell curve. We can represent the
    **probability density function** of a Gaussian distribution as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**高斯分布**可以被视为一个统计方程，表示围绕其均值对称分布的数据点。你经常会听到这种分布被称为钟形曲线。我们可以将高斯分布的**概率密度函数**表示如下：'
- en: '![](img/Formula_B17761_06_003.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761_06_003.jpg)'
- en: Here, ![](img/Formula_B17761_06_004.png) represents the mean and ![](img/Formula_B17761_06_005.png)
    represents the variance. Note that this function represents a single variable.
    Upon the addition of other variables, we would begin to venture into the space
    of multivariate Gaussian models, in which *x* and ![](img/Formula_B17761_06_006.png)
    represent vectors of length ![](img/Formula_B17761_06_007.png). In a dataset consisting
    of *k* clusters, we would need a mixture of *k* Gaussian distributions, in which
    each distribution has a mean and variance. These two values are determined through
    a technique known as **Expectation-Maximization** (**EM**).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![公式B17761_06_004](img/Formula_B17761_06_004.png)代表均值，![公式B17761_06_005](img/Formula_B17761_06_005.png)代表方差。请注意，此函数代表一个单一变量。当添加其他变量时，我们将开始进入多元高斯模型的空间，其中*x*和![公式B17761_06_006](img/Formula_B17761_06_006.png)代表长度为![公式B17761_06_007](img/Formula_B17761_06_007.png)的向量。在一个包含*k*个聚类的数据集中，我们需要*k*个高斯分布的混合，其中每个分布都有一个均值和方差。这两个值通过称为**期望最大化**（**EM**）的技术来确定。
- en: We will define **EM** as an algorithm that determines the proper parameters
    for a given model when some data is considered missing or incomplete. These missing
    or incomplete items are known as **latent variables**, and within the confines
    of UL, we can consider the actual clusters to be unknown. Note that if the clusters
    were known, we would be able to determine the mean and variance; however, we need
    to know the mean and variance to determine the cluster (think of the classic chicken-or-egg
    situation). We can use EM within the scope of the data to determine the proper
    values of these two variables to best fit the model parameters. With all this
    in mind, we are now in a position to discuss GMMs more intelligently.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将**EM**定义为一种算法，当某些数据被认为缺失或不完整时，它确定给定模型的适当参数。这些缺失或不完整的项被称为**潜在变量**，在UL的范围内，我们可以认为实际的聚类是未知的。请注意，如果聚类是已知的，我们将能够确定均值和方差；然而，我们需要知道均值和方差来确定聚类（想想经典的先有鸡还是先有蛋的情况）。我们可以在数据的范围内使用EM来确定这两个变量的适当值，以最佳地拟合模型参数。考虑到所有这些，我们现在可以更智能地讨论GMMs。
- en: 'We previously defined a GMM as a model consisting of multiple Gaussian distributions.
    We will now elaborate on this definition by including the fact that it is a probabilistic
    model consisting of multiple Gaussian distributions and utilizes a **soft clustering**
    approach by determining the membership of a data point to a given cluster based
    on probability rather than a distance. Notice that this is in contrast to K-Means,
    which utilizes a **hard clustering** approach. Using the previous example dataset
    shown in *Figure 6.5* in the previous section, the application of a GMM would
    likely lead to improved results, as depicted here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前将GMM定义为由多个高斯分布组成的模型。现在，我们将通过包括它是一个由多个高斯分布组成的概率模型，并利用**软聚类**方法来详细阐述这个定义，该方法通过基于概率而不是距离来确定数据点到给定聚类的隶属关系。请注意，这与K-Means形成对比，K-Means使用**硬聚类**方法。使用上一节中*图6.5*所示的前一个示例数据集，应用GMM可能会带来改进的结果，如图所示：
- en: '![Figure 6.6 – K-Means clustering versus GMMs ](img/B17761_06_006.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – K-Means聚类与GMMs](img/B17761_06_006.jpg)'
- en: Figure 6.6 – K-Means clustering versus GMMs
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – K-Means聚类与GMMs
- en: Within this section, we discussed a few of the most common clustering algorithms
    commonly used in many applications within the field of biotechnology. We see clustering
    being applied in areas such as bio-molecular data, scientific literature, manufacturing,
    and even oncology, as we will experience in the following tutorial.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了在生物技术领域许多应用中常用的几种最常见的聚类算法。我们看到聚类被应用于生物分子数据、科学文献、制造甚至肿瘤学等领域，正如我们将在下面的教程中所体验到的。
- en: Tutorial – breast cancer prediction via clustering
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教程 – 通过聚类预测乳腺癌
- en: Over the course of this tutorial, we will explore the application of commonly
    used clustering algorithms for the analysis and prediction of cancer using the
    `Wisconsin Breast Cancer` dataset we applied in [*Chapter 5*](B17761_05_Final_JM_ePub.xhtml#_idTextAnchor082),
    *Understanding Machine Learning*. When we last visited this dataset, we approached
    the development of a model from the perspective of a supervised classifier in
    which we knew the labels of our observations ahead of time. However, in most real-world
    scenarios, knowledge of the labels ahead of time is rare. **Clustering analysis**,
    as we will soon see, can be highly valuable in these situations, and can even
    be used to label data to use within the context of a classifier later on. Over
    the course of this tutorial, we will develop our models using the data but pretend
    that we do not know the labels ahead of time. We will only use known labels to
    compare the results of our models. With this in mind, let's go ahead and get started!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程的整个过程中，我们将探讨使用`Wisconsin乳腺癌`数据集来分析和预测癌症的常用聚类算法的应用，该数据集我们在[*第五章*](B17761_05_Final_JM_ePub.xhtml#_idTextAnchor082)《理解机器学习》中应用过。当我们上次访问这个数据集时，我们从监督分类器的角度来开发模型，我们事先知道观察的标签。然而，在大多数实际场景中，事先知道标签的情况很少见。正如我们很快就会看到的，**聚类分析**在这些情况下可以非常有价值，甚至可以用来为数据打标签，以便在分类器的上下文中使用。在本教程的整个过程中，我们将使用数据来开发我们的模型，但假装我们不知道标签。我们只会使用已知的标签来比较我们模型的结果。考虑到这一点，让我们开始吧！
- en: 'We will begin by importing our dataset as we have previously done and check
    the shape, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入我们的数据集，就像我们之前做的那样，并检查其形状，如下所示：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We notice that there are 569 rows of data in this dataset. In our previous
    application, we had cleaned up the data to address missing and corrupt values.
    Let''s go ahead and clean those up, as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到这个数据集中有569行数据。在我们之前的应用中，我们已经清理了数据以解决缺失和损坏的值。让我们继续清理这些数据，如下所示：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With the current shape of the data consisting of 569 rows with 32 columns, this
    now matches our previous dataset, and we are now ready to proceed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当前数据的形状由569行和32列组成，这与我们之前的数据集相匹配，我们现在可以继续进行了。
- en: 'Although we will not be using these labels to develop any models, let''s take
    a quick look at them, as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会使用这些标签来开发任何模型，但让我们快速看一下它们，如下所示：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can see in the following screenshot that there are two classes—`M` for malignant
    and `B` for benign. The two classes are not perfectly balanced but will do for
    the purposes of our clustering model:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的屏幕截图中，我们可以看到有两个类别——`M`代表恶性，`B`代表良性。这两个类别并不完全平衡，但对我们聚类模型来说足够了：
- en: '![Figure 6.7 – The distribution of the two classes ](img/B17761_06_007.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – 两个类别的分布](img/B17761_06_007.jpg)'
- en: Figure 6.7 – The distribution of the two classes
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 两个类别的分布
- en: 'To make our comparison to these labels easier during the following steps of
    our clustering analysis, let''s go ahead and encode these labels as numerical
    values in which we will convert `M` to `1` and `B` to `0`, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在接下来的聚类分析步骤中更容易地比较这些标签，让我们继续将这些标签编码为数值，其中我们将`M`转换为`1`，将`B`转换为`0`，如下所示：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can use the `df.head()` function to see the first few rows of our dataset
    and confirm that the `diagnosis` column did in fact get encoded properly. Next,
    we will prepare a quick pairplot of a few select features, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`df.head()`函数查看数据集的前几行，并确认`diagnosis`列确实被正确编码。接下来，我们将准备一个快速的对数图来展示几个选定的特征，如下所示：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can use the `markers` argument to specify two distinct shapes to plot the
    two classes, yielding the following pairplot showing scatter plots of our features:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`markers`参数来指定两个不同的形状来绘制两个类别，得到以下配对图，显示了我们的特征的散点图：
- en: '![Figure 6.8 – Pairplot of select features ](img/B17761_06_008.png.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – 选择特征的配对图](img/B17761_06_008.png.jpg)'
- en: Figure 6.8 – Pairplot of select features
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 选择特征的配对图
- en: 'Our first objective is to look over the many features and get a sense of which
    two features show the least amount of overlap or the best degree of separation.
    We can see that the `smoothness_mean` and `texture_mean` columns have a high degree
    of overlap; however, `radius_mean` and `texture_mean` seem less so. We can take
    a closer look at these by plotting a scatter plot using the `seaborn` library,
    as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要目标是浏览许多特征，并了解哪两个特征重叠最少或分离度最高。我们可以看到 `smoothness_mean` 和 `texture_mean`
    列有很高的重叠度；然而，`radius_mean` 和 `texture_mean` 的重叠似乎较少。我们可以通过使用 `seaborn` 库绘制散点图来更仔细地观察这些特征，如下所示：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Notice that once again, we can use the `style` and `markers` arguments to shape
    the data points, thus yielding the following diagram as output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们再次可以使用 `style` 和 `markers` 参数来塑造数据点，从而得到以下图表作为输出：
- en: '![Figure 6.9 – Scatter plot of the two features that showed good separation
    ](img/B17761_06_009.png.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – 两个表现出良好分离的特征的散点图](img/B17761_06_009.png.jpg)'
- en: Figure 6.9 – Scatter plot of the two features that showed good separation
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 两个表现出良好分离的特征的散点图
- en: 'Next, we will normalize our data. In statistics, normalization or standardization
    can have a wide variety of meanings and are sometimes used interchangeably. We
    will define normalization to mean the rescaling of values into a range of [*0*,*1*].
    On the other hand, we will define standardization to mean the rescaling of data
    to have a mean value of 0, and a standard deviation value of 1\. For the purposes
    of our current objectives, we will want to standardize our data as we have previously
    done using the `StandardScaler` class. Recall that this class standardizes features
    within the dataset by removing the mean and scaling to variance, which can be
    represented as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对数据进行归一化。在统计学中，归一化或标准化可能有多种含义，有时可以互换使用。我们将定义归一化是指将值重新缩放到 [*0*，*1*] 的范围内。另一方面，我们将定义标准化是指将数据重新缩放到平均值和标准差均为1。为了实现我们的当前目标，我们希望使用
    `StandardScaler` 类对数据进行标准化，就像我们之前做的那样。回想一下，这个类通过移除均值并缩放到方差来在数据集内标准化特征，如下所示：
- en: '![](img/Formula_B17761_06_008.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761_06_008.jpg)'
- en: 'Here, ![](img/Formula_B17761_06_009.png) is the standard score of a sample,
    ![](img/Formula_B17761_06_010.png) is the mean, and ![](img/Formula_B17761_06_011.png)
    is the standard deviation. We can apply this in Python with the following code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B17761_06_009.png) 是样本的标准分数，![](img/Formula_B17761_06_010.png)
    是平均值，而 ![](img/Formula_B17761_06_011.png) 是标准差。我们可以在Python中使用以下代码应用此方法：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: With our dataset scaled, we are now ready to start applying a few models. We
    will begin with the agglomerative clustering model from the `sklearn` library.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集缩放后，我们现在可以开始应用一些模型了。我们将从 `sklearn` 库中的凝聚聚类模型开始。
- en: Agglomerative clustering
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 凝聚聚类
- en: 'Recall that **agglomerative** clustering is a method in which clusters are
    formed by recursively merging clusters together. Let''s go ahead and implement
    the agglomerative clustering algorithm with our dataset, as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，**凝聚聚类**是一种通过递归合并聚类来形成聚类的算法。让我们继续使用我们的数据集实现凝聚聚类算法，如下所示：
- en: 'First, we will import the specific class of interest from the `sklearn` library,
    and then create an instance of our model by specifying the number of classes we
    want and setting the linkage as `ward`—one of the most common agglomerative clustering
    methods used. The code is illustrated in the following snippet:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将从 `sklearn` 库中导入感兴趣的特定类，然后通过指定我们想要的类别数量并将连接设置为 `ward`（这是最常用的凝聚聚类方法之一）来创建我们模型的实例。代码如下所示：
- en: '[PRE7]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we will fit our model to our dataset, and predict the clusters to which
    they belong. Notice in the following code snippet that we used the `fit_predict()`
    function, using the first two features, `radius_mean` and `texture_mean`, and
    not the whole dataset:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将我们的模型拟合到数据集上，并预测它们所属的聚类。注意在下面的代码片段中，我们使用了 `fit_predict()` 函数，使用前两个特征
    `radius_mean` 和 `texture_mean`，而不是整个数据集：
- en: '[PRE8]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can then use `matplotlib` and `seaborn` to generate a diagram showing the
    actual (`true`) results on the left and predicted agglomerative clustering results
    on the right, as follows:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 `matplotlib` 和 `seaborn` 生成一个图表，显示左侧的实际（真实）结果和右侧预测的凝聚聚类结果，如下所示：
- en: '[PRE9]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Notice in the preceding code snippet the use of the `subplot()` functionality
    in which the value `122` was used to represent `1` as the total number of rows,
    `2` as the total number of columns, and `2` as the specific index location of
    the plot. You can view the output here:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意在前面的代码片段中使用了`subplot()`功能，其中`122`的值被用来表示`1`为总行数，`2`为总列数，`2`为特定索引位置的绘图。您可以在以下位置查看输出：
- en: '![Figure 6.10 – Results of the agglomerative clustering model relative to the
    actual results ](img/B17761_06_010.png.jpg)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图6.10 – 聚类模型相对于实际结果的比较结果](img/B17761_06_010.png.jpg)'
- en: Figure 6.10 – Results of the agglomerative clustering model relative to the
    actual results
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.10 – 聚类模型相对于实际结果的比较结果
- en: 'From an initial estimation, we see that the model did a fairly reasonable job
    in distinguishing between the two clusters, having known very little about the
    actual true outcome. We can get a quick measure of its performance using the `accuracy_score`
    method from `sklearn`. Although getting a sense of the recall and f-1 scores is
    also important, we will stick to accuracy for simplicity for now. The code is
    illustrated in the following snippet:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从初始估计来看，模型在区分两个聚类方面做得相当合理，尽管对实际真实结果了解甚少。我们可以使用`sklearn`中的`accuracy_score`方法来快速衡量其性能。虽然了解召回率和f-1分数也很重要，但为了简单起见，我们现在将坚持使用准确率。以下代码片段展示了这一过程：
- en: '[PRE10]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In summary, the agglomerative clustering model using only the first two features
    of the dataset yielded an accuracy of roughly 83%—not a bad first attempt! If
    you are following along using the provided code, I would encourage you to try
    adding yet another feature and fitting the model with three or four or five features
    instead of just two and see whether you are able to improve the performance. Better
    yet, explore the other features provided in this dataset, and see whether you
    can find others that offer better separation and beat our 83% metric. Let's now
    investigate the performance of K-Means instead.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，仅使用数据集的前两个特征进行的层次聚类模型得到了大约83%的准确率——这不是一个糟糕的初次尝试！如果您正在使用提供的代码进行跟随，我鼓励您尝试添加另一个特征，并用三个、四个或五个特征而不是两个特征来拟合模型，看看您是否能够提高性能。更好的是，探索这个数据集中提供的其他特征，看看您是否可以找到其他提供更好分离的特征，并击败我们的83%指标。现在，让我们研究K-Means的性能。
- en: K-Means clustering
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-Means聚类
- en: 'Let''s now investigate the application of **K-Means** clustering using the
    dataset. Recall that the K-Means algorithm attempts to cluster data by partitioning
    the data into *k* clusters based on the location of their centroids. We can apply
    the K-Means algorithm using the following steps:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们研究使用数据集应用**K-Means**聚类的应用。回想一下，K-Means算法试图通过根据数据点的质心位置将数据划分为*k*个聚类来聚类数据。我们可以按照以下步骤应用K-Means算法：
- en: 'We will begin by importing the `KMeans` class from the `sklearn` library, as
    follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先从`sklearn`库中导入`KMeans`类，如下所示：
- en: '[PRE11]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we can initialize an instance of the K-Means model and specify the number
    of clusters being `2`, the number of iterations being `10`, and the initialization
    method being `k-means++`. This initialization setting simply selects the initial
    cluster centers using an algorithm, with the aim of speeding up convergence. We
    can adjust the parameters in a process known as tuning in order to maximize the
    performance of the model. The code is illustrated in the following snippet:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以初始化K-Means模型的实例，并指定聚类数量为`2`，迭代次数为`10`，初始化方法为`k-means++`。这种初始化设置简单地使用算法选择初始聚类中心，目的是加快收敛速度。我们可以通过一个称为调整的过程来调整参数，以最大化模型性能。以下代码片段展示了这一过程：
- en: '[PRE12]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can then use the `fit_predict()` method to fit our data and predict the
    clusters for each of the observations. Notice in the following code snippet that
    the model is only fitting and predicting the outcomes based on the first two features
    alone:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`fit_predict()`方法来拟合我们的数据，并预测每个观察值的聚类。注意在以下代码片段中，模型仅基于前两个特征进行拟合和预测结果：
- en: '[PRE13]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we can go ahead and plot the results of our predictions in comparison
    to the true values of the known classes using the `seaborn` library, as follows:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`seaborn`库来绘制预测结果与已知类别的真实值之间的比较结果，如下所示：
- en: '[PRE14]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Upon executing this code, we get a scatter plot showing our results, as follows:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行此代码后，我们得到一个散点图，显示了我们的结果，如下所示：
- en: '![Figure 6.11 – Results of the K-Means clustering model relative to the actual
    results ](img/B17761_06_011.png.jpg)'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图6.11 – K-Means聚类模型相对于实际结果的结果](img/B17761_06_011.png.jpg)'
- en: '[PRE15]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, using the `subplot()` methodology, we can generate four plots to illustrate
    the changes in which each individual subplot represents one of the plots depicted.
    Here''s the code we''ll need:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`subplot()`方法，我们可以生成四个图表来展示变化，其中每个子图代表一个展示的图表。以下是所需的代码：
- en: '[PRE16]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'With the code executed, we yield the following diagram showing the results:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码执行后，我们得到以下展示结果的图：
- en: '![Figure 6.12 – Results of the K-Means clustering model with increasing features
    ](img/B17761_06_012.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12 – 随着特征增加的K-Means聚类模型的结果](img/B17761_06_012.jpg)'
- en: Figure 6.12 – Results of the K-Means clustering model with increasing features
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – 随着特征增加的K-Means聚类模型的结果
- en: We can calculate the accuracy using only two features to be ~86%, whereas three
    features yielded 89%. We will notice, however, that the numbers not only begin
    to plateau with more features included but also decrease when all features were
    included, yielding a lower accuracy of 82%. Note that as we begin to add more
    features to the model, we are adding more dimensions. For example, with three
    features, we are now using a **three-dimensional** (**3D**) model, as shown by
    the blended border between the two datasets. In some cases, the more features
    we have, the bigger the strain it will have on a given model. This borders a concept
    known as the **Curse of Dimensionality** (**COD**) in the sense that the volume
    of the space begins to increase at an incredible rate given more dimensions, which
    can impact the performance of the model. We will touch on some of the ways we
    can remedy this in the future, particularly in the following tutorial, as we begin
    to discuss **DR**.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用仅两个特征来计算准确率，大约为86%，而三个特征产生了89%。然而，我们会注意到，随着更多特征的加入，准确率不仅开始趋于平稳，而且在所有特征都包含时还会下降，产生了较低的准确率82%。请注意，当我们开始向模型添加更多特征时，我们实际上是在增加更多的维度。例如，有三个特征时，我们现在使用的是一个**三维**（**3D**）模型，正如两个数据集之间的混合边界所示。在某些情况下，我们拥有的特征越多，对给定模型的压力就越大。这涉及到一个被称为**维度诅咒**（**COD**）的概念，即在更多维度的情况下，空间的体积开始以惊人的速度增加，这可能会影响模型的性能。我们将在未来的教程中讨论一些可以补救的方法，特别是当我们开始讨论**DR**时。
- en: In summary, we were able to apply the K-Means model on our dataset and were
    able to yield a considerable accuracy of 89% using the first three features. Let's
    now go ahead and explore the application of a statistical method such as GMM.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们能够在我们的数据集上应用K-Means模型，并且使用前三个特征实现了相当高的准确率，达到了89%。现在让我们继续探索统计方法如GMM的应用。
- en: GMMs
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GMMs
- en: 'Let''s now explore the application of GMM**s** on our dataset. Recall that
    these models represent a mixture of probability distributions, and the membership
    of an observation to a cluster is calculated based on that probability and not
    on Euclidean distance. With that in mind, let''s go ahead and get started, as
    follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索GMM**s**在我们数据集上的应用。回想一下，这些模型代表概率分布的混合，一个观察值属于哪个聚类的计算是基于该概率而不是欧几里得距离。考虑到这一点，让我们继续开始，如下所示：
- en: 'We can begin by importing the `GaussianMixture` class from the `sklearn` library,
    like this:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过从`sklearn`库中导入`GaussianMixture`类来开始，如下所示：
- en: '[PRE17]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we will create an instance of the model and specify the number of components
    as `2`, and set the covariance type as `full` such that each component has its
    own covariance matrix, as follows:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建模型的一个实例，并将组件数量指定为`2`，将协方差类型设置为`full`，这样每个组件都有自己的协方差矩阵，如下所示：
- en: '[PRE18]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We will then fit the data model with our data, once again using only the first
    two features, and predict the clusters for each of the observations, as follows:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用前两个特征将数据模型拟合到我们的数据上，并预测每个观察值的聚类，如下所示：
- en: '[PRE19]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we can go ahead and plot the results using the `seaborn` library,
    as follows:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`seaborn`库来绘制结果，如下所示：
- en: '[PRE20]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Upon executing our code, we yield the following output, showing the actual
    results of the dataset relative to our predicted ones:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行我们的代码后，我们得到以下输出，显示了数据集的实际结果与我们的预测结果相对比：
- en: '![Figure 6.13 – Results of the GMM relative to the actual results ](img/B17761_06_013.png.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13 – GMM相对于实际结果的结果](img/B17761_06_013.png.jpg)'
- en: Figure 6.13 – Results of the GMM relative to the actual results
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – GMM相对于实际结果的结果
- en: Once again, we can see that the boundary between the two classes is very defined
    within the Gaussian model, in which there is little to no blending, as the actual
    results show, thus yielding an accuracy of ~85%. Notice, however, that relative
    to the K-Means model, the GMM predicted a dense circular distribution in blue,
    with some members of the orange class wrapping around it in a very non-circular
    fashion.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以看到在高斯模型中，两个类别的边界非常明确，几乎没有混合，正如实际结果所示，从而实现了大约85%的准确率。然而，请注意，与K-Means模型相比，GMM预测了一个密集的蓝色圆形分布，橙色类别的某些成员以一种非常非圆形的方式围绕它：
- en: 'Similar to the previous model, we can once again add some more features to
    this model in an attempt to further improve the performance. However, we see in
    the following screenshot that despite the addition of more features from left
    to right, the model does not improve, and the predictive capabilities begin to
    suffer:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的模型类似，我们再次尝试向这个模型添加一些更多特性，以期进一步提高其性能。然而，从下面的截图我们可以看到，尽管从左到右添加了更多特性，但模型并没有得到改善，其预测能力开始下降：
- en: '![Figure 6.14 – Results of the GMM with increasing features ](img/B17761_06_014.png.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图6.14 – 增加特征后的GMM结果](img/B17761_06_014.png.jpg)'
- en: Figure 6.14 – Results of the GMM with increasing features
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – 增加特征后的GMM结果
- en: 'In summary, over the course of this tutorial, we investigated the use of clustering
    analysis to develop various predictive models for a dataset while assuming the
    absence of labels. Throughout the tutorial, we investigated the use of three of
    the most common clustering models: `Wisconsin Breast Cancer` dataset. We determined
    that the K-Means model using three features showed optimal performance relative
    to other models, some of which utilized the dataset as a whole. We can speculate
    that all of the features contribute some level of significance when it comes to
    predictive power; however, the inclusion of all features within the models showed
    degraded performance. We will investigate some ways to mitigate this in the following
    section, pertaining to **DR**.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本教程中，我们探讨了在假设没有标签的情况下，使用聚类分析来开发各种预测模型的方法。在整个教程中，我们研究了三种最常见的聚类模型：`Wisconsin乳腺癌`数据集。我们确定，相对于其他模型，使用三个特征的K-Means模型在性能上表现最佳，其中一些模型使用了整个数据集。我们可以推测，所有特征在预测能力方面都具有一定的意义；然而，在模型中包含所有特征会导致性能下降。我们将在下一节中探讨一些减轻这种情况的方法，这涉及到**DR**。
- en: Understanding DR
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解DR
- en: 'The second category of **UL** that we will discuss is known as **DR**. As the
    full name states, these are simply methods used to reduce the number of dimensions
    in a given dataset. Take, for example, a highly featured dataset with 100 or so
    columns—DR algorithms can be used to help reduce the number of columns down to
    perhaps 5 while preserving the value that each of those original 100 columns contains.
    You can think of DR as the process of condensing a dataset in a horizontal fashion.
    The resulting columns can generally be divided into two types: new features, in
    the sense that a new column with new numerical values was generated in a process
    known as **Feature Engineering** (**FE**), or old features, in the sense that
    only the most useful columns were preserved in a process known as **feature selection**.
    Over the course of the following section and within the confines of UL, we will
    be focusing more on the aspect of FE as we create new features representing reduced
    versions of many others. We can see a graphical illustration of this concept here:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要讨论的**UL**的第二类被称为**DR**。正如全名所表明的，这些方法只是用来减少给定数据集中维度的数量。以一个具有大约100个列的高特征数据集为例——DR算法可以用来帮助将列数减少到大约5个，同时保留每个原始100个列所包含的价值。你可以把DR看作是按水平方式压缩数据集的过程。结果列通常可以分为两种类型：新特征，即在称为**特征工程**（**FE**）的过程中生成了具有新数值的新列，或者旧特征，即在称为**特征选择**的过程中只保留了最有用的列。在接下来的章节中，在UL的范围内，我们将更多地关注**FE**的方面，因为我们创建了许多其他数据集的简化版本的新特征。我们可以在这里看到这个概念的图形说明：
- en: '![Figure 6.15 – Graphical representation of DR ](img/B17761_06_015.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图6.15 – DR的图形表示](img/B17761_06_015.jpg)'
- en: Figure 6.15 – Graphical representation of DR
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 – DR的图形表示
- en: 'There are many different methods that we can use to implement DR, each with
    its own process and underlying theory; however, before we begin implementing these,
    there is a very important concept we need to address. You are now probably wondering
    why DR matters. Why would any data scientist eliminate features after another
    data scientist or data engineer went through all of the trouble to put together
    a comprehensive and rich dataset to begin with? There are three answers to this
    question, as outlined here:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用许多不同的方法来实现数据降维（DR），每种方法都有自己的流程和理论基础；然而，在我们开始实施这些方法之前，有一个非常重要的概念我们需要解决。你现在可能想知道为什么DR很重要。为什么任何数据科学家会在另一个数据科学家或数据工程师费尽周折地构建一个全面且丰富的数据集之后，还要删除特征呢？对此问题有三个答案，如下所述：
- en: We are not necessarily eliminating any data from our given dataset but are exploring
    our data from a different window, which may provide some new insights that we
    would not have seen using the original dataset.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们并不一定从我们的数据集中删除任何数据，而是在不同的视角下探索我们的数据，这可能会提供一些我们使用原始数据集看不到的新见解。
- en: Developing models with many features is a computationally expensive process,
    therefore the ability to train our model using fewer features will always be faster,
    less computationally intensive, and more favorable.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发具有许多特征的模型是一个计算成本高昂的过程，因此使用较少特征来训练我们的模型将始终更快、计算量更少、更有利。
- en: The use of DR can help reduce noise within the dataset to further improve clustering
    models and data visualizations.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DR可以帮助减少数据集中的噪声，从而进一步提高聚类模型和数据可视化。
- en: With these answers in mind, let's now go ahead and talk about a concept that
    you will hear in many meetings, discussions, and interviews—the COD.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑到这些答案，现在让我们来谈谈你将在许多会议、讨论和面试中听到的概念——COD。
- en: Avoiding the COD
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免COD
- en: 'The COD is regarded as a general phenomenon that arises when handling highly
    dimensional datasets—a term that was originally coined by Richard E. Bellman.
    In essence, the COD refers to issues that arise with highly dimensional datasets
    that do not occur in lower-dimensional datasets of similar size. As the number
    of features in a given dataset increases, the total number of samples will also
    increase proportionally. Take, for example, some dataset consisting of one dimension.
    Within this dataset, let''s assume that we would need to examine a total of 10
    regions. If we added a second dimension, we would now need to examine a total
    of 100 regions. Finally, if we added a third dimension, we would now need to examine
    a total of 1,000 regions. Think back for a moment to some of the datasets we have
    been working with so far that extend well beyond 1,000 rows and have at least
    10 columns—the complexity of datasets such as these can grow quite rapidly. The
    main takeaway point here is that feature growth has a large impact on the development
    of a model. We can see a graphical illustration of this here:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: COD被视为在处理高维数据集时出现的一种普遍现象——这个术语最初是由理查德·E·贝尔曼提出的。本质上，COD指的是在高维数据集中出现的问题，而在类似大小但低维的数据集中不会出现。随着给定数据集中特征数量的增加，样本总数也会成比例增加。以一个一维数据集为例。在这个数据集中，假设我们需要检查总共10个区域。如果我们添加第二个维度，我们现在需要检查总共100个区域。最后，如果我们添加第三个维度，我们现在需要检查总共1,000个区域。回想一下我们迄今为止一直在处理的一些数据集，这些数据集的行数远远超过1,000，至少有10列——这些数据集的复杂性可以迅速增长。这里的主要启示是特征增长对模型的发展有重大影响。我们可以在这里看到这一点的图形说明：
- en: '![Figure 6.16 – Graphical representation of the COD ](img/B17761_06_016.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图6.16 – COD的图形表示](img/B17761_06_016.jpg)'
- en: Figure 6.16 – Graphical representation of the COD
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – COD的图形表示
- en: 'As the number of **features** begins to increase, so does the overall **complexity**
    of an ML model, which can have a number of negative impacts such as overfitting,
    thus resulting in poor **performance**. One of the main motivations to reduce
    the dimensionality of a dataset is to ensure that overfitting is avoided, thus
    resulting in a more robust model. We can see a graphical illustration of this
    here:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 随着特征数量的增加，机器学习（ML）模型的整体复杂性也会增加，这可能会产生许多负面影响，如过拟合，从而导致性能不佳。减少数据集维度的主要动机之一是确保避免过拟合，从而产生更稳健的模型。我们可以在这里看到这一点的图形说明：
- en: '![Figure 6.17 – The effect of higher dimensions on model performance ](img/B17761_06_017.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17 – 高维对模型性能的影响](img/B17761_06_017.jpg)'
- en: Figure 6.17 – The effect of higher dimensions on model performance
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17 – 高维对模型性能的影响
- en: The necessity to reduce datasets from being highly dimensional to a low-dimensional
    form is especially true in the life science and biotechnology sectors. Throughout
    the many processes that scientists and engineers face within this field, there
    are generally hundreds of features relating to any given process. Whether we are
    looking for datasets relating to protein structures, monoclonal antibody titer,
    small molecule docking site selection, **Bispecific T-cell Engager** (**BiTE**)
    drug design, or even datasets relating to **Natural Language Processing** (**NLP**),
    the reduction of features will always be useful and in many cases necessary for
    the development of a good ML model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集从高度维数形式降低到低维形式的需求在生命科学和生物技术领域尤为明显。在这个领域中，科学家和工程师面临的各种过程中，通常与任何给定过程相关的特征有数百个。无论我们是在寻找与蛋白质结构、单克隆抗体滴度、小分子对接位点选择、**双特异性T细胞连接器**（**BiTE**）药物设计，甚至是与**自然语言处理**（**NLP**）相关的数据集，特征的减少总是有用的，并且在许多情况下对于开发一个好的机器学习模型是必要的。
- en: Now that we have gained a better understanding of DR as it relates to the concept
    of the COD and the many benefits that can arise from these methods, let's now
    go ahead and look at a few of the most common models we should know about in this
    field.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对 DR 及其与 COD 概念的关系以及这些方法可能带来的众多好处有了更好的理解，让我们继续看看在这个领域中我们应该了解的一些最常见模型。
- en: Tutorial – exploring DR models
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教程 – 探索 DR 模型
- en: There are many different ways we can classify the numerous dimensionality algorithms
    out there, based on type, function, or outcome, and so on. However, for the purposes
    of getting a strong overview of DR in just a few pages within this chapter, we
    will classify our models as being either of a linear or non-linear fashion. Linear
    and non-linear models are two different types of data transformations. We can
    think of data transformations as methods in which data is altered or reshaped
    in one way or another. We can loosely define linear methods as transformations
    in which the output of a model is proportional to its input. Take, for example,
    *p* and *q* being two mathematical vectors.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据类型、功能、结果等对众多维度算法进行多种不同的分类。然而，为了在本章的几页内对 DR 有一个强有力的概述，我们将把我们的模型分类为线性或非线性。线性模型和非线性模型是两种不同的数据转换类型。我们可以将数据转换视为数据以某种方式改变或重塑的方法。我们可以粗略地将线性方法定义为模型输出与其输入成比例的转换。以
    *p* 和 *q* 作为两个数学向量为例。
- en: 'We can consider a transformation to be linear when the following apply:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当以下条件成立时，我们可以认为一个转换是线性的：
- en: The transformation of *p* is multiplied by a scalar and its result is the same
    as multiplying *p* by the scalar and then applying the transformation.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p* 的转换乘以一个标量，其结果与将 *p* 乘以标量然后应用转换相同。'
- en: The transformation of *p* + *q* is the same as the transformation of *p* + the
    transformation of *q*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p* + *q* 的转换与 *p* + *q* 的转换相同。'
- en: 'If a model does not satisfy these two properties, it is considered a non-linear
    model. Many different models fall within the scope of these two classes; however,
    for the purposes of this chapter, we will take a look at four main models that
    have gained quite a bit of popularity within the data science community over recent
    years. We can see a graphical illustration of this here:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型不满足这两个特性，则被视为非线性模型。许多不同的模型都包含在这两个类别中；然而，为了本章的目的，我们将探讨近年来在数据科学社区中相当受欢迎的四个主要模型。这里我们可以看到这一点的图形说明：
- en: '![Figure 6.18 – Two examples of models for each of the fields of DR ](img/B17761_06_018.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.18 – DR 各个领域的模型示例](img/B17761_06_018.jpg)'
- en: Figure 6.18 – Two examples of models for each of the fields of DR
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18 – DR 各个领域的模型示例
- en: Within the scope of linear methods, we will take a close look at **PCA** and
    **Singular Value Decomposition** (**SVD**). In addition, within the scope of non-linear
    methods, we will take a close look at **t-distributed Stochastic Neighbor Embedding**
    (**t-SNE**) and **Uniform Manifold Approximation and Projection** (**UMAP**).
    With these four models in mind and how they fit into the grand scheme of DR, let's
    go ahead and get started.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性方法的范围内，我们将仔细研究**PCA**和**奇异值分解**（**SVD**）。此外，在非线性方法的范围内，我们将仔细研究**t分布随机邻域嵌入**（**t-SNE**）和**均匀流形近似与投影**（**UMAP**）。考虑到这四种模型以及它们在DR宏大方案中的位置，让我们开始吧。
- en: PCA
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PCA
- en: 'One of the most common and widely discussed forms of UL is **PCA**. PCA is
    a linear form of DR, allowing users to transform a large dataset of correlated
    features into a smaller number of uncorrelated features known as principal components.
    These **principal components**, although numerically fewer than their original
    features, can still retain as much of the variation or *richness* as the original
    dataset. We can see a graphical illustration of this here:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见且广泛讨论的UL形式之一是**PCA**。PCA是一种线性形式的DR，允许用户将大量相关特征的大数据集转换成更少的无关特征，这些特征被称为主成分。这些**主成分**，尽管在数量上少于原始特征，但仍能保留与原始数据集一样多的变化或**丰富性**。我们可以在下面看到这一点的图形说明：
- en: '![Figure 6.19 – Graphical representation of PCA and its principal components
    ](img/B17761_06_019.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图6.19 – PCA及其主成分的图形表示](img/B17761_06_019.jpg)'
- en: Figure 6.19 – Graphical representation of PCA and its principal components
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 – PCA及其主成分的图形表示
- en: 'There are a few things that need to happen in order to effectively implement
    PCA on any given dataset. Let''s take a high-level overview of what these steps
    are and how they can impact the final outcome. We must first normalize or standardize
    our data to ensure that the mean is 0 and the standard deviation is 1\. Next,
    we calculate what is known as the **covariance matrix**, which is a square matrix
    containing the covariance between each of the pairs of elements. In a **two-dimensional**
    (**2D**) dataset, we can represent a covariance matrix as such:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地在任何给定数据集上实施PCA，需要发生几件事情。让我们从高层次概述这些步骤以及它们如何影响最终结果。我们首先必须对数据进行归一化或标准化，以确保均值为0，标准差为1。接下来，我们计算所谓的**协方差矩阵**，它是一个包含每对元素之间协方差的方阵。在一个**二维**（**2D**）数据集中，我们可以将协方差矩阵表示如下：
- en: '![](img/Formula_B17761_06_012.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761_06_012.jpg)'
- en: 'Next, we can calculate the **eigenvalues** and **eigenvectors** for the covariance
    matrix, as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以计算协方差矩阵的**特征值**和**特征向量**，如下所示：
- en: '![](img/Formula_B17761_06_013.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761_06_013.jpg)'
- en: 'Here, *![](img/Formula_Symbol.png)* is an eigenvalue for a given matrix *A*,
    and *I* is the identity matrix. Using the eigenvector, we can determine the eigenvalue
    *v* using the following equation:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*![](img/Formula_Symbol.png)*是给定矩阵*A*的特征值，而*I*是单位矩阵。使用特征向量，我们可以使用以下方程确定特征值*v*：
- en: '![](img/Formula_B17761_06_014.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761_06_014.jpg)'
- en: Next, the eigenvalues are ordered from the largest to the smallest, which represent
    the components in order of significance. A dataset with *n* variables or features
    will have *n* eigenvalues and eigenvectors. We can then limit the number of eigenvalues
    or vectors to a predetermined number, thus reducing the dimensions of our dataset.
    We can then form what we call a feature vector, using the eigenvectors of interest.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将特征值从大到小排序，这代表了按重要性顺序的成分。具有*n*个变量或特征的数据集将具有*n*个特征值和特征向量。然后我们可以将特征值或向量的数量限制在预定的数量，从而降低数据集的维度。然后我们可以使用感兴趣的特征向量形成我们所说的特征向量。
- en: 'Finally, we can form the **principal components** using the **transpose** of
    the feature vector, as well as the transpose of the scaled data of the original
    dataset, and multiplying the two together, as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过特征向量的**转置**以及原始数据集缩放数据的**转置**，将它们相乘，从而形成**主成分**，如下所示：
- en: '![](img/Formula_B17761_06_015.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761_06_015.jpg)'
- en: Here, *PrincipalComponents* is returned as a matrix. Easy, right?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*PrincipalComponents*以矩阵的形式返回。简单，对吧？
- en: 'Let''s now go ahead and implement PCA using Python, as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用Python实现PCA，如下所示：
- en: 'First, we import PCA from the `sklearn` library and instantiate a new PCA model.
    We can set the number of components as `2`, representing the fact that we only
    want two components returned to us, and use `full` for `svd_solver`. We can then
    fit the data on our scaled dataset, as follows:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从 `sklearn` 库中导入 PCA 并实例化一个新的 PCA 模型。我们可以将组件数量设置为 `2`，表示我们只想返回两个组件，并将 `svd_solver`
    设置为 `full`。然后，我们可以将数据拟合到我们的缩放数据集上，如下所示：
- en: '[PRE21]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we can transform our data and assign our output matrix to the `data_pca_2d`
    variable, as follows:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以转换我们的数据并将输出矩阵分配给 `data_pca_2d` 变量，如下所示：
- en: '[PRE23]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we can go ahead and plot the results using `seaborn`, as follows:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用 `seaborn` 绘制结果，如下所示：
- en: '[PRE24]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Upon executing this code, this will yield a scatter plot showing our principal
    components with our points colored using `y`, as shown here:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行此代码后，这将生成一个散点图，显示我们的主成分，我们的点使用 `y` 着色，如图所示：
- en: '![Figure 6.20 – Scatter plot of the PCA results ](img/B17761_06_020.png.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.20 – PCA 结果的散点图](img/B17761_06_020.png.jpg)'
- en: Figure 6.20 – Scatter plot of the PCA results
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20 – PCA 结果的散点图
- en: PCA is a fast and efficient method best used as a precursor to the development
    of ML models when the number of dimensions has become too complex. Think back
    for a moment to the dataset we used in our clustering analysis relating to breast
    cancer predictions. Instead of running our models on the raw or scaled data, we
    could implement a DR algorithm such as PCA to reduce our dimensions down only
    two principal components before applying the subsequent clustering model. Remember
    that PCA is only one of many linear models. Let's now go ahead and explore another
    popular linear model known as SVD.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 是一种快速且高效的方法，最好在维度变得过于复杂时作为开发 ML 模型的先导使用。回想一下我们在乳腺癌预测相关的聚类分析中使用的数据集。我们可以在原始或缩放数据上运行我们的模型之前，实现一个
    DR 算法，如 PCA，将维度减少到仅两个主成分，然后再应用后续的聚类模型。记住，PCA 只是许多线性模型之一。现在，让我们继续探索另一个流行的线性模型，称为
    SVD。
- en: SVD
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SVD
- en: '**SVD** is a popular **matrix decomposition** method commonly used to reduce
    a dataset to a simpler form. In this section, we will focus specifically on the
    application of truncated SVD. This model is quite similar to that of PCA; however,
    the main difference is that the estimator does not center prior to its computation.
    Essentially, this difference allows the model to be used with sparse matrices
    quite efficiently.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**SVD** 是一种流行的 **矩阵分解** 方法，通常用于将数据集简化为更简单的形式。在本节中，我们将专门讨论截断 SVD 的应用。此模型与 PCA
    非常相似；然而，主要区别在于估计量在计算之前不进行中心化。本质上，这种差异使得模型能够非常有效地用于稀疏矩阵。'
- en: 'Let''s now introduce and take a look at a new dataset that we can use to apply
    SVD: *single-cell RNA* (where **RNA** stands for **ribonucleic acid**). The dataset
    can be found at [http://blood.stemcells.cam.ac.uk/data/nestorowa_corrected_log2_transformed_counts.txt](http://blood.stemcells.cam.ac.uk/data/nestorowa_corrected_log2_transformed_counts.txt).
    This dataset pertains to the topic of single-cell sequencing—a process that examines
    sequences of individual cells to better understand their properties and functions.
    Datasets such as these tend to have many columns of data, making them prime candidates
    for DR models. Let''s go ahead and import this dataset, as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们介绍并查看一个我们可以用来应用 SVD 的新数据集：*单细胞 RNA*（其中 **RNA** 代表 **核糖核酸**）。该数据集可在 [http://blood.stemcells.cam.ac.uk/data/nestorowa_corrected_log2_transformed_counts.txt](http://blood.stemcells.cam.ac.uk/data/nestorowa_corrected_log2_transformed_counts.txt)
    找到。此类数据集涉及单细胞测序的主题——这是一个检查单个细胞序列的过程，以更好地了解它们的属性和功能。此类数据集通常具有许多数据列，使它们成为 DR 模型的理想候选。让我们继续导入此数据集，如下所示：
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Taking a look at the shape, we can see that there are 3,991 rows of data and
    1,645 columns. Relative to the many other datasets we have used, this number is
    quite large. Within the field of biotechnology, DR is very commonly used to help
    reduce such datasets into more manageable entities. Notice that the index contains
    some information about the type of cell we are looking at. To make our visuals
    more interesting, let''s capture this annotation data by executing the following
    code:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 观察形状，我们可以看到有 3,991 行数据和 1,645 列。相对于我们使用的许多其他数据集，这个数字相当大。在生物技术领域，DR 非常常用以帮助将此类数据集减少到更易于管理的实体。注意，索引包含有关我们正在查看的细胞类型的一些信息。为了使我们的视觉效果更有趣，让我们通过执行以下代码来捕获此注释数据：
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'With the data all set, let''s go ahead and implement truncated SVD on this
    dataset. We can once again begin by instantiating a truncated SVD model and setting
    the components to `2` with `7` iterations, as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 数据都准备好了，让我们继续在这个数据集上实现截断SVD。我们可以再次通过实例化一个截断SVD模型并将组件设置为`2`，迭代次数为`7`来开始，如下所示：
- en: '[PRE27]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we can go ahead and use the `fit_transform()` method to both fit our
    data and transform the DataFrame to a two-column dataset, as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以继续使用`fit_transform()`方法来拟合我们的数据和将DataFrame转换为一个两列的数据集，如下所示：
- en: '[PRE28]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, we can finish things up by plotting our dataset using a scatter plot,
    and color by annotation. The code is illustrated in the following snippet:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过散点图和颜色标注来绘制我们的数据集。代码在以下片段中展示：
- en: '[PRE29]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can see the results of executing this code in the following screenshot:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下屏幕截图中看到执行此代码的结果：
- en: '![Figure 6.21 – Scatter plot of the results of the SVD model ](img/B17761_06_021.png.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图6.21 – SVD模型结果的散点图](img/B17761_06_021.png.jpg)'
- en: Figure 6.21 – Scatter plot of the results of the SVD model
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21 – SVD模型结果的散点图
- en: In the preceding screenshot, we can see the almost 1,400 columns worth of data
    being reduced to a simple 2D representation—quite fascinating, isn't it? One of
    the biggest advantages of being able to reduce data in this fashion is that it
    assists with model development. Let's assume, for the sake of example, that we
    wish to implement any of our previous clustering algorithms on this extensive
    dataset. It would take considerably longer to train any given model on a dataset
    of nearly 1,400 columns compared to a dataset with 2 columns. In fact, if we implemented
    a GMM on this dataset, the total training time would be **12.4 s ± 158 ms** using
    the original dataset, relative to **4.06 ms ± 26.6 ms** using the reduced dataset.
    Although linear models can be very useful when it comes to DR, non-linear models
    can also be similarly impressive. Next, let's take a look at a popular model known
    as t-SNE.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，我们可以看到近1400列的数据被简化为简单的2D表示——是不是很令人着迷？能够以这种方式减少数据的一个最大的优点是它有助于模型开发。让我们假设，为了举例，我们希望将我们之前提到的任何聚类算法应用于这个庞大的数据集。在近1400列的数据集上训练任何给定模型所需的时间将比在2列的数据集上长得多。实际上，如果我们在这个数据集上实现GMM，使用原始数据集的总训练时间将是**12.4秒
    ± 158毫秒**，而使用减少后的数据集则是**4.06毫秒 ± 26.6毫秒**。虽然线性模型在处理DR时非常有用，但非线性模型也可以有类似惊人的效果。接下来，让我们看看一个流行的模型，称为t-SNE。
- en: t-SNE
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: t-SNE
- en: On the side of non-linear DR, one of the most popular models commonly seen in
    action is **t-SNE**. One of the unique features of the t-SNE model relative to
    the other dimensionality models we have talked about is the fact that it uses
    probability distribution to represent similarities between neighbors. Simply stated,
    t-SNE is a statistical method allowing for the DR and visualization of high-dimensional
    data in which similar points are close together and dissimilar ones are further
    apart.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在非线性DR方面，最常见的一个流行模型是**t-SNE**。与我们所讨论的其他维度模型相比，t-SNE模型的一个独特特征是它使用概率分布来表示邻居之间的相似性。简单来说，t-SNE是一种统计方法，允许对高维数据进行DR和可视化，其中相似点靠近，不相似点远离。
- en: t-SNE is a type of **manifold** model, which from a mathematical perspective
    is a topological space resembling **Euclidean** space. The concept of a manifold
    is complex, extensive, and well beyond the scope of this book. For the purposes
    of simplicity, we will state that manifolds describe a large number of geometric
    surfaces such as a sphere, torus, or cross surface. Within the confines of the
    t-SNE model, the main objective is to use geometric shapes to give users a feel
    or intuition of how the high-dimensional data is arranged or organized.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE是一种**流形**模型，从数学角度来看，它是一个类似于**欧几里得**空间的拓扑空间。流形的概念复杂、广泛，超出了本书的范围。为了简化，我们将说明流形描述了大量的几何表面，如球体、环面或十字形表面。在t-SNE模型的范围内，主要目标是使用几何形状来使用户对高维数据的排列或组织有一个感觉或直觉。
- en: Let's now take a close look at the application of t-SNE using Python. Once again,
    we can apply this model on our single-cell RNA dataset and get a sense of what
    the high-dimensional organization of this data looks like from a geometric perspective.
    Many parameters within t-SNE can be changed and tuned to fit given purposes; however,
    there is one in particular worth mentioning briefly—perplexity. `scikit-learn`
    library recommends considering values between 5 and 50\. Let's go ahead and take
    a look at a few examples.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们仔细看看 t-SNE 在 Python 中的应用。再次，我们可以在我们的单细胞 RNA 数据集上应用这个模型，并从几何角度了解这个数据的高维组织结构。t-SNE
    中的许多参数都可以更改和调整以适应特定目的；然而，有一个参数特别值得简要提及——困惑度。`scikit-learn` 库建议考虑介于 5 和 50 之间的值。让我们继续看看几个示例。
- en: 'Implementing this model is quite simple, thanks to the high-level `scikit-learn`.
    We can begin by importing the `TSNE` class from `scikit-learn` and setting the
    number of components to `2` and the perplexity to `10`. We can then chain the
    `fit_transform()` method using our dataset, as illustrated in the following code
    snippet:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `scikit-learn` 的高层支持，实现这个模型相当简单。我们可以从导入 `scikit-learn` 中的 `TSNE` 类开始，将组件数设置为
    `2`，将困惑度设置为 `10`。然后我们可以使用我们的数据集链式调用 `fit_transform()` 方法，如下面的代码片段所示：
- en: '[PRE30]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can then go ahead and plot our data to visualize the results using `seaborn`,
    as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以继续使用 `seaborn` 来绘制我们的数据，以可视化结果，如下所示：
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can see the output of this in the following screenshot:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的屏幕截图中看到这个结果：
- en: '![Figure 6.22 – Scatter plot of the results of the t-SNE model ](img/B17761_06_022.png.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.22 – t-SNE 模型的结果散点图](img/B17761_06_022.png.jpg)'
- en: Figure 6.22 – Scatter plot of the results of the t-SNE model
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.22 – t-SNE 模型的结果散点图
- en: 'Quite the result! We can see in the preceding screenshot that the model, without
    any knowledge of the labels, made a 2D projection of the relationship between
    the data points using the huge dataset it was given. The geometric shape produced
    gives us a sense of the *look* and *feel* of the data. We can see based on this
    depiction that a few points seem to be considered outliers as they are depicted
    much further away, like islands relative to the main continent. Recall that we
    used a perplexity value of `10` for this particular diagram. Let''s go ahead and
    explore this parameter using a few different values, as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 真是令人印象深刻！从前面的屏幕截图中我们可以看到，该模型在没有标签知识的情况下，使用它所给出的巨大数据集，对数据点之间的关系进行了二维投影。产生的几何形状给我们一种关于数据的*外观*和*感觉*。我们可以从这个描述中看出，一些点似乎被认为是异常值，因为它们被描绘得离得很远，就像岛屿相对于大陆一样。回想一下，我们为这个特定的图表使用了`10`的困惑度值。让我们继续探索这个参数，使用几个不同的值，如下所示：
- en: '[PRE32]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Using these calculated values, we can visualize them next to each other using
    the `seaborn` library, as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些计算出的值，我们可以使用 `seaborn` 库将它们并排放置，如下所示：
- en: '![Figure 6.23 – Scatter plots of the t-SNE model with increasing perplexities
    ](img/B17761_06_023.png.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.23 – 随着困惑度增加的 t-SNE 模型的散点图](img/B17761_06_023.png.jpg)'
- en: Figure 6.23 – Scatter plots of the t-SNE model with increasing perplexities
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.23 – 随着困惑度增加的 t-SNE 模型的散点图
- en: When it comes to high-dimensional data, t-SNE is one of the most commonly used
    models to not only reduce your dimensions but also explore your data by getting
    a unique feel for its features and their relationships. Although t-SNE can be
    useful and effective, it does have a few negative aspects. First, it does not
    scale well for large sample sizes such as those you would see in some cases of
    RNA sequencing data. Second, it also does not preserve global data structures
    in the sense that similarities across different clusters are not well maintained.
    Another popular model that attempts to address some of these concerns and utilizes
    a similar approach to t-SNE is known as UMAP. Let's explore this model in the
    following section.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到高维数据时，t-SNE 是最常用的模型之一，它不仅能降低你的维度，还能通过获取其特征及其关系的独特感觉来探索你的数据。尽管 t-SNE 可能很有用且有效，但它确实有几个负面方面。首先，它对于大样本量（如你在某些
    RNA 测序数据案例中看到的那样）的扩展性不好。其次，它也没有保留全局数据结构，也就是说，不同簇之间的相似性没有得到很好的维护。另一个试图解决这些担忧并采用与
    t-SNE 类似方法的流行模型被称为 UMAP。让我们在下一节中探索这个模型。
- en: UMAP
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: UMAP
- en: 'The **UMAP** model is a popular algorithm used for the reduction of dimensions
    and visualizing of data based on manifold learning techniques, similar to that
    of t-SNE. There are three main assumptions that the algorithm is founded on, as
    described on their main website ([https://umap-learn.readthedocs.io/en/latest](https://umap-learn.readthedocs.io/en/latest))
    and outlined here:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**UMAP**模型是一个流行的算法，用于基于流形学习技术的降维和数据可视化，类似于t-SNE。该算法基于三个主要假设，如他们在主要网站([https://umap-learn.readthedocs.io/en/latest](https://umap-learn.readthedocs.io/en/latest))上所述，并在以下内容中概述：'
- en: The dataset is uniformly distributed on a Riemannian manifold.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集在黎曼流形上均匀分布。
- en: The Riemannian metric is locally constant.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黎曼度量在局部是常数。
- en: The manifold is locally connected.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流形在局部是连通的。
- en: 'Although UMAP and t-SNE are quite similar, there are a few key differences
    between them. One of the most important differences relates to the idea of similarity
    preservation. The UMAP model claims to preserve both local and global data in
    the sense that both local and global—or inter-cluster and intra-cluster—information
    is maintained. We can see a graphical representation of this concept here:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管UMAP和t-SNE非常相似，但它们之间有一些关键的区别。其中最重要的区别之一与相似性保留的概念有关。UMAP模型声称在局部和全局数据中保持相似性，即局部和全局信息，或簇间和簇内信息得到维护。我们可以在这里看到这个概念的图形表示：
- en: '![Figure 6.24 – Graphical representation of local and global similarities ](img/B17761_06_024.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图6.24 – 局部和全局相似性的图形表示](img/B17761_06_024.jpg)'
- en: Figure 6.24 – Graphical representation of local and global similarities
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24 – 局部和全局相似性的图形表示
- en: 'Let''s now go ahead and apply UMAP on our single-cell RNA dataset. We can begin
    by importing the `umap` library and instantiating a new instance of the UMAP model
    in which we specify the number of components as `2` and the number of neighbors
    as `5`. This second parameter represents the size of a local **neighborhood**
    used for manifold approximation. The code is illustrated in the following snippet:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来应用UMAP到我们的单细胞RNA数据集。我们可以从导入`umap`库并实例化一个新的UMAP模型开始，我们指定组件数量为`2`，邻居数量为`5`。这个第二个参数表示用于流形近似的局部**邻域**的大小。代码如下所示：
- en: '[PRE33]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can then go ahead and plot the data using `seaborn`, as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`seaborn`来绘制数据，如下所示：
- en: '[PRE34]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Upon executing the code, we yield the following output:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，我们得到以下输出：
- en: '![Figure 6.25 – Scatter plot of the UMAP results ](img/B17761_06_025.png.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图6.25 – UMAP结果的散点图](img/B17761_06_025.png.jpg)'
- en: Figure 6.25 – Scatter plot of the UMAP results
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25 – UMAP结果的散点图
- en: Once again, quite the visual! We can see in this depiction relative to t-SNE
    that some clusters have moved around. If you recall in t-SNE, the majority of
    the data was pulled together with no regard as to how similar clusters were to
    one another. Using UMAP, this information is preserved, and we are able to get
    a better sense of how these clusters relate to one another. Notice the spread
    of the data relative to its depiction in t-SNE. Similar to t-SNE, we can see that
    some *groups* of points are clustered together in different neighborhoods.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，非常直观的视觉展示！我们可以看到，与t-SNE相比，一些簇已经移动了位置。如果你还记得在t-SNE中，大部分数据被拉在一起，而不管簇之间的相似性如何。使用UMAP，这个信息被保留下来，我们能够更好地了解这些簇之间的关系。注意数据相对于其在t-SNE中的表示的分布。类似于t-SNE，我们可以看到一些*组*的点在不同的邻域中聚集在一起。
- en: In summary, UMAP is a powerful model similar to t-SNE in which both local and
    global information is preserved when it comes to neighborhoods or clusters. Most
    commonly used for visualizations, this model is an excellent way to gain a sense
    of the *look* and *feel* of any high-dimensional dataset in just a few lines of
    code.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，UMAP是一个功能强大的模型，类似于t-SNE，在处理邻域或簇时，它保留了局部和全局信息。这个模型最常用于可视化，只需几行代码就能很好地了解任何高维数据集的“外观”和“感觉”。
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Over the course of this chapter, we gained a strong and high-level understanding
    of the field of UL, its uses, and its applications. We then explored a few of
    the most popular ML methods as they relate to clustering and DR. Within the field
    of clustering, we looked over some of the most commonly used models such as hierarchical
    clustering, K-Means clustering, and GMMs. We learned about the differences between
    Euclidean distances and probabilities and how they relate to model predictions.
    In addition, we also applied these models to the `Wisconsin Breast Cancer` dataset
    and managed to achieve relatively high accuracy in a few of them. Within the field
    of DR, we gained a strong understanding of the significance of the field as it
    relates to the COD. We then implemented a number of models such as PCA, SVD, t-SNE,
    and UMAP using the *single-cell RNA* dataset in which we managed to reduce more
    than 1,400 columns down to 2\. We then visualized our results using `seaborn`
    and examined the differences between the models. Over the course of this chapter,
    we managed to develop our models without the use of labels, which we used only
    for comparison after the development process.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的整个过程中，我们对该领域（UL）有了强烈和高级的理解，包括其用途和应用。然后，我们探讨了与聚类和DR相关的几种最流行的ML方法。在聚类的领域中，我们回顾了一些最常用的模型，如层次聚类、K-Means聚类和GMMs。我们学习了欧几里得距离和概率之间的差异以及它们如何与模型预测相关。此外，我们还将这些模型应用于`Wisconsin乳腺癌`数据集，并在其中几个模型中实现了相对较高的准确率。在DR领域中，我们对该领域与COD相关的意义有了深入的理解。然后，我们使用*单细胞RNA*数据集实现了PCA、SVD、t-SNE和UMAP等模型，将超过1400列减少到2列。然后，我们使用`seaborn`可视化我们的结果，并检查了模型之间的差异。在本章的整个过程中，我们设法在不使用标签的情况下开发我们的模型，我们只在开发过程之后使用标签进行比较。
- en: Over the course of the next chapter, we will explore the field of SML, in which
    we use data in addition to its labels to develop powerful predictive models.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨SML领域，在该领域中，我们除了使用数据标签外，还使用数据本身来开发强大的预测模型。
