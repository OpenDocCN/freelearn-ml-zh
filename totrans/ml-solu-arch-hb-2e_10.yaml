- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced ML Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on making it so far! By now, you should have developed a good
    understanding of the core fundamental skills that an ML solutions architect needs
    in order to operate effectively across the ML lifecycle. In this chapter, we will
    delve into advanced ML concepts. Our focus will be on exploring a range of options
    for distributed model training for large models and datasets. Understanding the
    concept and techniques for distributed training is becoming increasingly important
    as all large-scale model training such as GPT will require distributed training
    architecture. Furthermore, we’ll delve into diverse technical approaches aimed
    at optimizing model inference latency. As model sizes grow larger, having a good
    grasp on how to optimize models for low-latency inference is becoming an essential
    skill in ML engineering. Lastly, we will close this chapter with a hands-on lab
    on distributed model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Training large-scale models with distributed training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving low-latency model inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on lab – running distributed model training with PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need access to your AWS environment for the hands-on portion of this
    chapter. All the code samples are located at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: Training large-scale models with distributed training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As ML algorithms grow more complex and the volumes of available training data
    expand exponentially, model training times have become a major bottleneck. Single-device
    training on massive datasets or gigantic models like large language models is
    increasingly impractical given memory, time, and latency constraints. For example,
    state-of-the-art language models have rapidly scaled from millions of parameters
    a decade ago to hundreds of billions today. The following graph illustrates how
    language models have evolved in recent years:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – The growth of language models  ](img/B20836_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: The growth of language models'
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome computational challenges, distributed training techniques have
    become critical to accelerate model development by parallelizing computation across
    clusters of GPUs or TPUs in the cloud. By sharding data and models across devices
    and nodes, distributed training enables the scaling out of computation to train
    modern massive models and data volumes in reasonable timeframes. There are two
    main types of distributed training: data parallelism and model parallelism. Before
    we get into the details of distributed training, let’s quickly review how a neural
    network trains again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Deep neural network training ](img/B20836_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Deep neural network training'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows how an **artificial neural network** (**ANN**) trains.
    The training data is fed to the ANN in a forward pass. The loss (the difference
    between the predicted value and the true value) is calculated at the end of the
    forward pass, and the backward pass calculates the gradients for all the parameters.
    These parameters are updated with new values for the next step until the loss
    is minimized. In the following sections, we’ll look at distributed model training
    using data parallelism and model parallelism, two methods for scaling model training
    for large training datasets and large model sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed model training using data parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data-parallel distributed training approach partitions a large training
    dataset into smaller subsets and trains each subset on different devices concurrently.
    This parallelization allows multiple training processes to run simultaneously
    on available compute resources, accelerating the overall training time. To leverage
    data-parallel training, the ML frameworks and algorithms used need to have support
    for distributed training. Frameworks like TensorFlow and PyTorch both provide
    modules and libraries for data parallelism training.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed earlier, one key task in training **deep learning** (**DL**)
    models is to calculate the gradients concerning the loss function for every batch
    of the data, and then update the model parameters with gradient information to
    minimize the loss gradually. Instead of running the gradient calculations and
    parameter updates on a single device, the basic concept behind data-parallel distributed
    training is to run multiple training processes using the same algorithm in parallel,
    with each process using a different subset of the training dataset. The following
    diagram shows the main concept behind data parallelism in training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Data parallelism concept ](img/B20836_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Data parallelism concept'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, there are three nodes in a cluster participating in a distributed
    data-parallel training job, with each node having two devices. The partial gradients
    that are calculated by each device are represented by w0 ~ w5 for each of the
    devices on the nodes, while W is the value for a global parameter for the model.
    Specifically, data-parallel distributed training has the following main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Each device (CPU or GPU) on every node loads a copy of the same algorithm and
    a subset of the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each device runs a training loop to calculate the gradients (w0~w5) to optimize
    its loss function and exchange the gradients with other devices in the cluster
    at each training step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gradients from all the devices are aggregated and the common model parameters
    (W) are calculated using these aggregated gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each device pulls down the newly calculated common model parameters (W) and
    continues with the next step of model training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Steps 2* to *4* are repeated until the model training is completed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In a distributed training setting, efficiently exchanging gradients and parameters
    across processes is one of the most important aspects of ML system engineering
    design. Several distributed training topologies have been developed over the years
    to optimize communications across different training processes. In this chapter,
    we will discuss two of the most widely adopted topologies for data-parallel distributed
    training: **parameter server** and **AllReduce**.'
  prefs: []
  type: TYPE_NORMAL
- en: Parameter server overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **parameter server** (**PS**) is a topology built on the concept of server
    nodes and worker nodes. The worker nodes are responsible for running the training
    loops and calculating the gradients, while the server nodes are responsible for
    aggregating the gradients and calculating the globally shared parameters. The
    following diagram shows the architecture of a PS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Parameter server architecture ](img/B20836_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Parameter server architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the server node is called the PS, and is usually implemented as a key-value
    or vector store for storing gradients and parameters. As the number of model parameters
    to manage can become very large, there can also be multiple server nodes for managing
    the global parameters and gradient aggregations. In a multi-parameter server configuration,
    there is also a server manager that manages and coordinates all the server nodes
    to ensure consistency.
  prefs: []
  type: TYPE_NORMAL
- en: In this architecture, the worker nodes only communicate with the PS nodes to
    exchange gradients and parameters, and not with each other. In a multi-server
    node environment, each server node also communicates with every other server node
    to replicate the parameters for reliability and scalability. The gradients and
    parameters are exchanged so that updates can be implemented synchronously and
    asynchronously. The synchronous gradient update strategy blocks the devices from
    processing the next mini-batch of data until the gradients from all the devices
    have been synchronized. This means that each update has to wait for the slowest
    device to complete. This can slow down training and make the training process
    less robust in terms of device failure.
  prefs: []
  type: TYPE_NORMAL
- en: On the positive side, synchronous updates do not have to worry about stale gradients,
    which can lead to higher model accuracy. Asynchronous updates do not need to wait
    for all the devices to be synchronized before processing the next mini-batch of
    data, though this might lead to reduced accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The main limitation of this approach is that the PS can become a communication
    bottleneck, particularly for large models with billions or trillions of parameters.
    As the model size grows, the amount of data that needs to be transmitted between
    the workers and the PS increases significantly, leading to potential communication
    overhead and bandwidth constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, as the number of worker nodes increases, the PS needs to handle
    an increasing number of gradient updates and parameter distributions, which can
    become a scalability challenge. This centralized architecture can limit the overall
    throughput and efficiency of the distributed training process, especially when
    the number of workers becomes very large. Furthermore, slower or underperforming
    worker nodes can slow down the entire training process, as the PS must wait for
    all gradients before updating the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the PS in frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'PS distributed training is natively supported by several DL frameworks, including
    TensorFlow. Specifically, TensorFlow supports PS-based distributed training natively
    with its `ParameterServerStrategy` API. The following code sample shows how to
    instantiate the `ParameterServerStrategy` API for TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this code sample, the `cluster_resolver` parameter helps discover and resolve
    the IP addresses of workers.
  prefs: []
  type: TYPE_NORMAL
- en: '`ParameterServerStrategy` can be used directly with the `model.fit()` function
    of Keras or a custom training loop by wrapping the model with the `strategy.scope()`
    syntax. See the following sample syntax on how to use `scope()` to wrap a model
    for distributed training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In addition to a PS implementation, which is natively supported within DL libraries,
    there are also general-purpose PS training frameworks, such as BytePS from ByteDance
    and Herring from Amazon, which work with different DL frameworks. SageMaker uses
    Herring under the hood for data-parallel distributed training through its SageMaker
    distributed training library.
  prefs: []
  type: TYPE_NORMAL
- en: One of the shortcomings of the PS strategy is the inefficient use of network
    bandwidth. The Herring library addresses this shortcoming by combining AWS **Elastic
    Fabric Adapter** (**EFA**) and the parameter sharding technique, which makes use
    of network bandwidth to achieve faster distributed training. EFA takes advantage
    of cloud resources and their characteristics, such as multi-path backbones, to
    improve network communication efficiency. You can find out more about Herring
    at [https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud](https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud).
  prefs: []
  type: TYPE_NORMAL
- en: AllReduce overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the PS architecture is easy to understand and set up, it does come with
    several challenges. For example, the PS architecture requires additional nodes
    for the PSs, and it is also hard to determine the right ratio between server nodes
    and worker nodes to ensure the server nodes do not become bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AllReduce topology tries to improve some of the limitations of PSs by eliminating
    the server nodes and distributing all the gradient aggregation and global parameter
    updates to all workers, hence it’s called **AllReduce**. The following diagram
    shows the topology of AllReduce:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – AllReduce architecture ](img/B20836_10_05.png)Figure 10.5:
    AllReduce architecture'
  prefs: []
  type: TYPE_NORMAL
- en: In an AllReduce topology, each node sends gradients of parameters to all the
    other nodes at each training step. Then, each node aggregates the gradients and
    performs a reduce function (such as `average`, `sum`, or `max`) locally before
    calculating the new parameters using the next training step. Since every node
    needs to communicate with every other node, this results in a large number of
    networks of communication between the nodes, and duplicate compute and storage
    are required as every node has a copy of all the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more efficient AllReduce architecture is Ring AllReduce. In this architecture,
    each node only sends some gradients to its next neighboring node, and each node
    is responsible for aggregating the gradients for the global parameters that it
    is assigned to calculate. This architecture greatly reduces the amount of network
    communication in a cluster and compute overhead, so it is more efficient for model
    training. The following diagram shows the Ring AllReduce architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Ring AllReduce ](img/B20836_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Ring AllReduce'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the PS approach, the Ring AllReduce approach has better scalability
    as the number of workers increases. Since there is no centralized PS, the communication
    load is distributed among the workers, reducing the potential bottleneck. The
    Ring AllReduce approach also has a lower communication overhead, especially for
    large models with billions or trillions of parameters. Instead of sending individual
    gradients to a central server, the gradients are summed and passed along a ring
    topology, reducing the overall amount of data that needs to be transmitted.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the Ring AllReduce approach offers better scalability and efficiency
    for large-scale distributed training. It distributes the communication load among
    workers, reducing potential bottlenecks and synchronization overhead. However,
    the PS approach may still be suitable for smaller-scale distributed training scenarios
    or when fault tolerance is less of a concern.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing AllReduce and Ring AllReduce in frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The AllReduce and Ring AllReduce architectures are natively supported within
    multiple DL frameworks, including TensorFlow and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow supports AllReduce distributed training across multiple GPUs on
    one machine with its `tf.distribute.MirroredStrategy` API. With this strategy,
    each GPU has a copy of the model, and all the model parameters are mirrored across
    different devices. An efficient AllReduce mechanism is used to keep these parameters
    in sync. The following code sample shows how to instantiate the `MirroredStrategy`
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For multi-machine distributed training, TensorFlow uses the `tf.distribute.MultiWorkerMirroredStrategy`
    API. Similar to `MirroredStrategy`, `MultiWorkerMirroredStrategy` creates copies
    of all the parameters across all the devices on all the machines and synchronizes
    them with the AllReduce mechanism. The following code sample shows how to instantiate
    the `MultiWorkerMirroredStrategy` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Similar to `ParameterServerStrategy`, `MirroredStrategy` and `MultiWorkerMirroredStrategy`
    can work with the `keras model.fit()` function or a custom training loop. To associate
    a model with a training strategy, you can use the same `strategy.scope()` syntax.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch also provides native support for AllReduce-based distributed training
    via its `torch.nn.DataParallel` and `torch.nn.parallel.DistributedDataParallel`
    APIs. The `torch.nn.DataParallel` API supports single-process multi-threading
    across GPUs on the same machine, while `torch.nn.parallel.DistributedDataParallel`
    supports multi-processing across GPUs and machines. The following code sample
    shows how to initiate a distributed training cluster and wrap a model for distributed
    training using the `DistributedDataParallel` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Another popular implementation of the general-purpose Ring AllReduce architecture
    is **Horovod**, which was created by the engineers at Uber. Horovod works with
    multiple DL frameworks, including TensorFlow and PyTorch. You can find out more
    about Horovod at [https://github.com/horovod/horovod](https://github.com/horovod/horovod).
  prefs: []
  type: TYPE_NORMAL
- en: Distributed model training using model parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compared to data parallelism, model parallelism is still relatively low in its
    adoption since most of the distributed training that happens today involves data
    parallelism that deals with large datasets. However, the applications of state-of-the-art
    big DL algorithms such as BERT, GPT, and T5 are driving the increasing adoption
    of model parallelism. The qualities of these models are known to increase with
    the model’s size, and these large NLP models require a large amount of memory
    to store the model’s states (which include the model’s parameters, optimizer states,
    and gradients) and memory for other overheads.
  prefs: []
  type: TYPE_NORMAL
- en: 'As such, these models can no longer fit into the memory of a single GPU. While
    data parallelism helps solve the large dataset challenge, it cannot help with
    training large models due to its large memory size requirements. Model parallelism
    allows you to split a single large model across multiple devices so that the total
    memory across multiple devices is enough to hold a copy of the model. Model parallelism
    also allows for a larger batch size for model training as a result of the larger
    collective memory across multiple devices. There are two main approaches to splitting
    the model for parallel distributed training: splitting by layers and splitting
    by tensors. Next, let’s explore these two approaches in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Naïve model parallelism overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As an ANN consists of many layers, one way to split the model is to distribute
    the layers across multiple devices. For example, if you have an 8-layer **multi-layer
    perceptron** (**MLP**)network and two GPUs (GPU0 and GPU1), you can simply place
    the first four layers in GPU0 and the last four layers in GPU1\. During training,
    the first four layers of the model are trained as you would normally train a model
    in a single device. When the first four layers are complete, the output from the
    fourth layer will be copied from GPU0 to GPU1, incurring a communication overhead.
    After getting the output from GPU0, GPU1 continues training layers five to eight.
    The following diagram illustrates splitting a model by layers across multiple
    devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Naïve model parallelism ](img/B20836_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Naïve model parallelism'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing model parallelism by splitting requires knowledge of the training
    task. It is not a trivial task to design an efficient model parallelism strategy.
    Here are a few heuristics that could be helpful for the split-layer design:'
  prefs: []
  type: TYPE_NORMAL
- en: Place neighboring layers on the same devices to minimize communication overhead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balance the workload between devices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different layers have different compute and memory utilization properties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an ANN model is inherently a sequential process, which means that the
    network layers are processed sequentially, while the backward process will only
    start when the forward process is completed. When you’re splitting layers across
    multiple devices, only the device currently processing the layers on it will be
    busy; the other devices will be idle, wasting compute resources, which results
    in a waste of hardware resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the processing of sequences for the forward
    and backward passes for one batch of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Naïve model parallelism  ](img/B20836_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Naïve model parallelism'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, **F0**, **F1**, and **F2** are the forward passes
    on the different neural network layers on each device. **B2**, **B1**, and **B0**
    are the backward passes for the layers on each device. As you can see, when one
    of the devices is busy with either a forward pass or a backward pass, the other
    devices are idle.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve model parallelism has the benefit of implementation simplicity, and it
    is suitable for models with a large number of layers. However, it has scalability
    challenges due to the sequential nature of layer execution. In addition, it may
    run into potential load imbalance issues if layers have varying computational
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at an approach (pipeline model parallelism) that can help increase
    resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline model parallelism overview
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To resolve the resource-idling issue, pipeline model parallelism can be implemented.
    This improves on naïve model parallelism so that different devices can work in
    parallel on the different stages of the training pipeline on a smaller chunk of
    data batch, commonly known as a micro-batch. The following diagram shows how pipeline
    model parallelism works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Pipeline model parallelism ](img/B20836_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Pipeline model parallelism'
  prefs: []
  type: TYPE_NORMAL
- en: With pipeline model parallelism, instead of processing one batch of data through
    each full forward and backward pass, that one batch of data is broken down into
    smaller mini-batches. In the preceding diagram, after **Device 0** completes the
    forward pass for the first mini-batch, **Device 1** can start its forward pass
    on the output of the **Device 1** forward pass. Instead of waiting for **Device
    1** and **Device 2** to complete their forward passes and backward passes, **Device
    0** starts to process the next mini-batch of data. This scheduled pipeline allows
    for higher utilization of the hardware resources, resulting in faster model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other variations of pipeline parallelism. One example is interleaved
    parallelism, where a backward execution is prioritized whenever possible. This
    improves the utilization of devices for end-to-end model training. The following
    diagram shows how an interleaved pipeline works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Interleaved pipeline ](img/B20836_10_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Interleaved pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline model parallelism has been implemented in various frameworks and products
    such as the SageMaker distributed training library and DeepSpeed distributed training
    framework, which we will cover in greater detail in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at an overview of tensor parallelism, also known as tensor
    slicing.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor parallelism/tensor slicing overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we mentioned earlier, tensor parallelism is another approach to split a large
    model to make it fit into memory. Before we dive into this, let’s quickly review
    what a tensor is and how it is processed by an ANN.
  prefs: []
  type: TYPE_NORMAL
- en: A **tensor** is a multi-dimensional matrix of a single data type such as a 32-bit
    floating-point or 8-bit integer. In the forward pass of neural network training,
    a dot product is used on the input tensor and weight matrix tensors (the connections
    between the input tensors and the neurons in the hidden layer). You can find out
    more about dot products at [https://en.wikipedia.org/wiki/Dot_product](https://en.wikipedia.org/wiki/Dot_product).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates a dot product between the input vector and
    the weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Matrix calculation  ](img/B20836_10_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Matrix calculation'
  prefs: []
  type: TYPE_NORMAL
- en: In this matrix calculation, you get an output vector of **[5,11,17]**. If there
    is a single device for dot product calculation, three separate calculations will
    be performed sequentially to get the output vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if we break up the single weights matrix into three vectors and use
    a dot product separately? This can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Splitting the matrix calculation ](img/B20836_10_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12: Splitting the matrix calculation'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, you would get three separate values that are the same as the
    individual values in the output vector in the preceding diagram. If there are
    three separate devices for performing dot product calculations, we can perform
    these three dot product calculations in parallel and combine the values into a
    single vector at the end if needed. This is the basic concept of how tensor parallelism
    works. With tensor parallelism, each device works independently without the need
    for any communication until the end, which is when the results need to be synchronized.
    This strategy allows for faster tensor processing as multiple devices can work
    in parallel to reduce the training time and increase the utilization of computing
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing model-parallel training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To implement model parallelism, you can manually design the parallelism strategy
    by deciding how to split the layers and tensors, as well as their placements,
    across different devices and nodes. However, it is not trivial to do this efficiently,
    especially for large clusters. To make the model parallelism implementation easier,
    several model parallelism library packages have been developed. In this section,
    we’ll take a closer look at some of these libraries. Note that the frameworks
    we will discuss can support both data parallelism and model parallelism and that
    both techniques are often used together to train large models with large training
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Megatron-LM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Megatron-LM is an open-source distributed training framework developed by Nvidia.
    It supports data parallelism, tensor parallelism, and pipeline model parallelism,
    as well as a combination of all three for extreme-scale model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Megatron-LM implements micro-batch-based pipeline model parallelism to improve
    device utilization. It also implements periodic pipeline flushes to ensure that
    the optimizer steps are synchronized across devices. Two different pipeline schedules
    are supported by Megatron-LM, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The default schedule works by completing the forward pass for all micro-batches
    first, before starting the backward pass for all the batches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The interleaved stage schedule works by running multiple different subsets of
    layers on a single device, instead of running just a single continuous set of
    layers. This can further improve the utilization of devices and reduce idle time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Megatron-LM implements a specific tensor parallelism strategy for transformer-based
    models. A transformer consists mainly of self-attention blocks, followed by a
    two-layer MLP. For the MLP portion, Megatron-LM splits the weight matrix by columns.
    The matrices for the self-attention heads are also partitioned by columns. The
    following diagram shows how the different parts of the transformers can be split:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – Tensor parallelism for transformers ](img/B20836_10_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.13: Tensor parallelism for transformers'
  prefs: []
  type: TYPE_NORMAL
- en: Using data parallelism, pipeline model parallelism, and tensor parallelism together,
    Megatron-LM can be used to train extremely large transformer-based models (with
    a trillion parameters) scaled across thousands of GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training using Megatron-LM involves the following key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the Megatron library using the `initialize_megatron()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting up the Megatron model optimizer using the `setup_model_and_optimizer()`
    function by wrapping the original model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the model using the `train()` function, which takes the Megatron model
    and optimizer as input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Megatron-LM has been used for many large-model training projects, such as BERT,
    GPT, and the Biomedical domain language model. Its scalable architecture can be
    used to train models with trillions of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DeepSpeed is an open-source distributed training framework developed by Microsoft.
    Similar to Megatron-LM, DeepSpeed also supports tensor-slicing (another name for
    splitting tensors) parallelism, pipeline parallelism, and data parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed implements micro-batch-based pipeline model parallelism, where a batch
    is broken into micro-batches to be processed by different devices in parallel.
    Specifically, DeepSpeed implements interleaved pipeline parallelism to optimize
    resource efficiency and utilization. Similar to Megatron-LM, DeepSpeed can use
    data parallelism, pipeline model parallelism, and tensor parallelism together
    to train extremely large deep neural networks. This is also known as DeepSpeed
    3D parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: One core capability of the DeepSpeed framework is its **Zero Redundancy Optimizer**
    (**ZeRO**). ZeRO is capable of managing memory efficiently by partitioning parameters,
    optimizer states, and gradients across devices instead of keeping a copy on all
    devices. The partitions are brought together at runtime when needed. This allows
    ZeRO to reduce the memory footprint by eight times compared to regular data parallelism
    techniques. ZeRO is also capable of using CPU and GPU memory together to train
    large models.
  prefs: []
  type: TYPE_NORMAL
- en: The attention-based mechanism is widely adopted in DL models, such as the transformer
    model, to address text and image inputs. However, its ability to address long
    input sequences is limited due to its large memory and compute requirements. DeepSpeed
    helps alleviate this issue with its implementation of a sparse attention kernel
    – a technology that reduces the compute and memory requirements of attention computation
    via block-sparse computation.
  prefs: []
  type: TYPE_NORMAL
- en: One major bottleneck in large-scale distributed training is the communication
    overhead due to gradient sharing and updates. Communication compression, such
    as 1-bit compression, has been adopted as an effective mechanism to reduce the
    communication overhead. DeepSpeed has an implementation of a 1-bit Adam optimizer,
    which can reduce the communication overhead by up to five times to improve the
    training speed. 1-bit compression works by representing each number using 1 bit,
    combined with error compensation, which remembers the error during gradient compression
    and adds the error back to the next step to compensate for the error.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use DeepSpeed, you need to modify your training script. The following steps
    explain the main changes you need to make to a training script to run distributed
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `deepspeed.initialize()` function to wrap the model and return a DeepSpeed
    model engine. This model engine will be used to run a forward pass and a backward
    pass.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the returned DeepSpeed model engine to run the forward pass, backward pass,
    and step function to update the model parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DeepSpeed primarily supports the PyTorch framework and requires minor code changes
    to adopt model training using PyTorch. DeepSpeed has been used for training models
    with hundreds of billions of parameters and has delivered some of the fastest
    model training times. You can find out more about DeepSpeed at [https://www.deepspeed.ai](https://www.deepspeed.ai).
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker distributed training library
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Amazon’s **SageMaker distributed training** (**SMD**) library is part of the
    Amazon SageMaker service offering. SMD supports data parallelism (by using Herring
    under the hood) and interleaved pipeline model parallelism. Unlike DeepSpeed and
    Megatron-LM, where you need to manually decide on your model partitions, **SageMaker
    Model Parallel** (**SMP**) has a feature for automated model splitting support.
  prefs: []
  type: TYPE_NORMAL
- en: This automated model-splitting feature of SMP balances memory and communication
    constraints between devices to optimize performance. Automated model splitting
    takes place during the first training step, where a version of a model is constructed
    in CPU memory. The graph is analyzed, a partition decision is made, and different
    model partitions are loaded into different GPUs. The partition software performs
    framework-specific analysis for TensorFlow and PyTorch to determine the partition
    decision. It considers graph structures such as variable/parameter sharing, parameter
    sizes, and constraints to balance the number of variables and the number of operations
    for each device to come up with split decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the SMD library, you need to make some changes to your existing training
    scripts and create SageMaker training jobs. There are different instructions for
    TensorFlow and PyTorch. The following are examples for the PyTorch framework:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify the PyTorch training script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call `smp.init()` to initialize the library.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrap the model with `smp.DistributedModel()`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrap the optimizer with `smp.DistributedOptimizer()`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Restrict each process to its own device through `torch.cuda.set_device(smp.local_rank())`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the wrapped model to perform a forward pass and a backward pass.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the distributed optimizer to update the parameters.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a SageMaker training job using SageMaker PyTorch Estimator and enable
    SMP distributed training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: FairScale
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: FairScale is a distributed training framework developed by Facebook AI Research
    (FAIR). It is built on top of the popular PyTorch deep learning library and provides
    a set of utilities and APIs for efficient distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: FairScale supports various distributed training paradigms, including data parallelism,
    model parallelism, and a combination of both. FairScale provides efficient implementations
    of these techniques, along with optimizations and techniques to reduce communication
    overhead and improve scalability.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key features of FairScale is its support for various model parallel
    strategies, such as tensor parallelism, pipeline parallelism, and hybrid parallelism.
    These strategies allow users to distribute large models across multiple accelerators
    in different ways, enabling efficient utilization of available hardware resources
    and better scaling for massive models.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to distributed training capabilities, FairScale also offers tools
    for optimizing memory usage, such as activation checkpointing, gradient checkpointing,
    and mixed precision training. These techniques help reduce the memory footprint
    of large models, allowing users to train models that would otherwise exceed the
    available memory on a single device.
  prefs: []
  type: TYPE_NORMAL
- en: FairScale is designed to be user-friendly and easy to integrate into existing
    PyTorch codebases. It provides a high-level API that abstracts away many of the
    complexities of distributed training, allowing users to focus on model development
    and experimentation rather than low-level implementation details. To use FairScale,
    you simply install the package using `pip install fairscale` and import the library
    into your training script using `import fairscale`. You can then use its various
    supported features for distributed data and model-parallel training.
  prefs: []
  type: TYPE_NORMAL
- en: While distributed model training allows us to train extremely large models,
    running inferences on these large models can result in high latency due to the
    size of the models and other technological constraints. Next, let’s explore the
    various techniques we can use to achieve low-latency inference.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving low-latency model inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As ML models continue to grow and get deployed to different hardware devices,
    latency can become an issue for certain inference use cases that require low-latency
    and high-throughput inferences, such as real-time fraud detection.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the overall model inference latency for a real-time application, there
    are different optimization considerations and techniques we can use, including
    model optimization, graph optimization, hardware acceleration, and inference engine
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will focus on model optimization, graph optimization, and
    hardware optimization. Before we get into these various topics, let’s first understand
    how model inference works, specifically for DL models, since that’s what most
    of the inference optimization processes focus on.
  prefs: []
  type: TYPE_NORMAL
- en: How model inference works and opportunities for optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed earlier in this book, DL models are constructed as computational
    graphs with nodes and edges, where the nodes represent the different operations
    and the edges represent the data flow. Examples of such operations include addition,
    matrix multiplication, activation (for example, Sigmoid and ReLU), and pooling.
    These operations perform computations on tensors as inputs and produce tensors
    as outputs. For example, the *c=matmul(a,b)* operation takes *a* and *b* as input
    tensors and produces *c* as the output tensor. Deep learning frameworks, such
    as TensorFlow and PyTorch, have built-in operators to support different operations.
    The implementation of an operator is also called a kernel.
  prefs: []
  type: TYPE_NORMAL
- en: During inference time for a trained model, the DL framework’s runtime will walk
    through the computational graph and invoke the appropriate kernels (such as add
    or Sigmoid) for each of the nodes in the graph. The kernel will take various inputs,
    such as the inference data samples, learned model parameters, and intermediate
    outputs, from the preceding operators and perform specific computations according
    to the data flow defined by the computational graph to produce the final predictions.
    The size of a trained model is mainly determined by the number of nodes in a graph,
    as well as the number of model parameters and their numerical precisions (for
    example, floating-point 32, floating-point 16, or integer 8).
  prefs: []
  type: TYPE_NORMAL
- en: Different hardware providers such as Nvidia and Intel also provide hardware-specific
    implementations of kernels for common computational graph operations. cuDNN is
    the library from Nvidia for optimized kernel implementations for their GPU devices,
    while MKL-DNN is the library from Intel for optimized kernel implementations for
    Intel chips. These hardware-specific implementations take advantage of the unique
    capabilities of the underlying hardware architecture. They can perform better
    than the kernels that are implemented by the DL framework implementation since
    the framework implementations are hardware agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: Now we understand how inference works, let’s explore some common optimization
    techniques we can use to improve model latency.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware acceleration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Different hardware produces varying inference latency performance for different
    ML models. The list of common hardware for model inference includes the CPU, GPU,
    **application-specific integrated circuit** (**ASIC**), **field-programmable gate
    array** (**FPGA**), and edge hardware (such as Nvidia Jetson Nano). In this section,
    we will review the core architecture characteristics for some of these pieces
    of hardware and how their designs help with model inference acceleration. It is
    worth noting that while this section focuses on inference, some of the hardware
    is also suitable for training acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: Central processing units (CPUs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A CPU is a general-purpose chip for running computer programs. It consists
    of four main building blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: The control unit is the brain of the CPU that directs the operations of the
    CPU; that is, it instructs other components such as memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **arithmetic logic unit** (**ALU**) is the basic unit that performs arithmetic
    and logical operations, such as addition and subtraction, on the input data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The address generation unit is used for calculating an address to access memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory management, which is used for all memory components such as the main
    memory and the local cache. A CPU can also be made up of multiple cores, with
    each core having a control unit and ALUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The degree of parallel executions in a CPU mainly depends on how many cores
    it has. Each core normally runs a single thread at a time, except for hyper-threading
    (a proprietary simultaneous multi-threading implementation from Intel). The more
    cores it has, the higher the degree of parallel executions. A CPU is designed
    to handle a large set of instructions and manage the operations of many other
    components; it usually has high performance and a complex core, but there aren’t
    many of them. For example, the Intel Xeon processor can have up to 56 cores.
  prefs: []
  type: TYPE_NORMAL
- en: CPUs are usually not suited for neural network-based model inference if low
    latency is the main requirement. Neural network inference mainly involves operations
    that can be parallelized at a large scale (for example, matrix multiplication).
    Since the total number of cores for a CPU is usually small, it cannot be parallelized
    at scale to meet the needs of neural network inference. On the positive side,
    CPUs are more cost-effective and usually have good memory capacities for hosting
    larger models.
  prefs: []
  type: TYPE_NORMAL
- en: Graphics processing units (GPUs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The design of a GPU is the opposite of the design of a CPU. Instead of having
    a few powerful cores, it has thousands of less powerful cores that are designed
    to perform a small set of instructions highly efficiently. The basic design of
    a GPU core is like that of a CPU. It also contains a control unit, ALU, and a
    local memory cache. However, the GPU control unit handles a much simpler instruction
    set, and the local memory is much smaller.
  prefs: []
  type: TYPE_NORMAL
- en: When the GPU processes instructions, it schedules blocks of threads, and within
    each block of threads, all the threads perform the same operations but on different
    pieces of data – a parallelization scheme called **Single Instruction Multiple
    Data** (**SIMD**). This architecture fits nicely with how a DL model works, where
    many neurons perform the same operation (mainly matrix multiplication) on different
    pieces of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Nvidia GPU architecture contains two main components:'
  prefs: []
  type: TYPE_NORMAL
- en: The global memory component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **streaming multiprocessor** (**SM**) component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An SM is analogous to a CPU and each SM has many **Computer Unified Device Architecture**
    (**CUDA**) cores, special functional units that perform different arithmetic operations.
    It also has a small, shared memory and cache, and many registers. A CUDA core
    is responsible for functions such as floating-point/integer operations, logic
    calculation, and branching. The thread block mentioned previously is executed
    by the SM. The global memory is located on the same GPU board. When you’re training
    an ML model, both the model and the data need to be loaded into the global memory.
  prefs: []
  type: TYPE_NORMAL
- en: In a multi-GPU configuration, low-latency and high-throughput communication
    channels are available, such as the Nvidia NVLink. GPUs are well suited for low-latency
    and high-throughput neural network model inferences due to their massive number
    of CUDA cores for large-scale parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the latest Nvidia GPU generation is the Blackwell B200
    GPU. It is built with 208 billion transistors, and its NVLink switch system allows
    multi-GPU communication across multiple servers at 1.8TB/s. For large-scale distributed
    training, a cluster of 576 B200 GPUs can work together. Another noteworthy feature
    of the B200 is its 2^(nd) generation Transformer Engine designed to accelerate
    the training of transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Application-specific integrated circuit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An **application-specific integrated circuit** (**ASIC**) is a primary alternative
    to a GPU. ASIC chips are purpose-designed for particular DL architectures for
    computation and data flow, so are faster and require less power than GPUs. For
    example, Google’s **Tensor Processing Unit** (**TPU**) has dedicated **Matrix
    Units** (**MXUs**) designed for efficient matrix computations, and AWS offers
    the Inferentia chip, an ASIC designed for model inference. To speed up model inference,
    the Amazon Inferentia chip and Google’s TPU chip both use the systolic array mechanism
    to speed up arithmetic calculations for deep neural networks. While general-purpose
    chips such as CPUs and GPUs use local registers between different ALU computations
    to transfer data and results, a systolic array allows you to chain multiple ALUs
    to reduce register access to speed up processing. The following diagram shows
    how data flows within a systolic array architecture versus a regular architecture
    that’s used in CPUs and GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Systolic array processing versus CPU/GPU processing ](img/B20836_10_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.14: Systolic array processing versus CPU/GPU processing'
  prefs: []
  type: TYPE_NORMAL
- en: The Amazon Inferentia chip can be used directly with Amazon SageMaker for inference
    with improved latency. You can do this by selecting one of the supported Inferentia
    chips for model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'While not directly applicable to inference, it is worth mentioning that AWS
    the also provides AWS Trainium accelerator, a purpose-built chip for training
    large deep learning models. Each Trainium accelerator consists of 2 NeuronCore
    cores. Each core is an independent compute engine with 4 main engines: the TensorEngine,
    VectorEngine, ScalarEngine, and GPSIMD-Engine. It also has on-chip SRAM memory.'
  prefs: []
  type: TYPE_NORMAL
- en: Each engine of the NeuronCore is optimized for unique computations. The ScalarEngine
    is optimized for scalar computation and can be highly parallelized with support
    for various data types such as FP32, FP16, BF16, INT8, INT16, and Int32\. It can
    perform 1,600 floating-point operations per cycle. The VectorEngine is optimized
    for vector computation. The VectorEngine is also highly parallelized and can perform
    2,500 floating-point operations per cycle. The TensorEngine is based on a power-optimized
    systolic array, which is highly optimized for tensor computations. Each TensorEngine
    can deliver over 100 TFLOPS of FP16/BF16 tensor computations. GPSIMD-Engine consists
    of 8 fully programmable 512-bit wide general-purpose processors, which can execute
    straight-line C-code, and have direct access to the other NeuronCore-v2 engines,
    as well as the SRAM memory.
  prefs: []
  type: TYPE_NORMAL
- en: Model optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you’re processing computational graphs for DL model inference, the size
    of the neural network (such as its number of layers, neurons, and so on), the
    number of model parameters, and the numerical precision of the model parameters
    directly impact the performance of model inference. The model optimization approach
    focuses on reducing the size of the neural network, the number of model parameters,
    and the numerical precisions to reduce inference latency. In general, there are
    two main approaches to model optimization: quantization and pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditionally, deep neural networks are trained with **floating-point 32 bit**
    (**FP32**). However, for many neural networks, FP32 is not needed for the required
    precision.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization for DL is a network compression approach that uses lower precision
    numbers, such as **floating-point 16 bit** (**FP16**) or **integer 8 bit** (**INT8**)
    instead of FP32, to represent static model parameters and perform numerical computation
    with dynamic data inputs/activation, all while having minimal or no impact on
    model performance. For example, an INT8 representation takes up four times less
    space than the FP32 representation, which significantly reduces the memory requirements
    and computational costs for neural networks, which means it can improve the overall
    latency for model inference.
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of quantization algorithms, including uniform and
    non-uniform quantization algorithms. Both approaches map real values in a continuous
    domain to discrete lower-precision values in the quantized domain. In the uniform
    case, the quantized values in the quantized domains are evenly spaced, whereas
    the non-uniform case has varying quantized values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the difference between a uniform and a non-uniform
    quantization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 – Uniform and non-uniform quantization ](img/B20836_10_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.15: Uniform and non-uniform quantization'
  prefs: []
  type: TYPE_NORMAL
- en: Quantization can be performed both post-training and during training (quantization-aware
    training). Post-training quantization takes a trained model, quantizes the weights,
    and regenerates a quantized model. Quantization-aware training involves fine-tuning
    a full precision model. During training, the higher-precision real numbers are
    reduced to lower-precision numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Other techniques include mixed-precision training, a method that utilizes lower
    precision, such as 16-bit floating-point or 8-bit integers, for computations during
    the training of ML models, while retaining higher precision (32-bit) for weight
    updates. This approach aims to achieve notable speedups and memory savings, particularly
    on hardware optimized for low-precision operations. Another technique is quantization
    with knowledge distillation, which involves transferring knowledge from a larger,
    high-precision teacher model to a smaller, quantized student model. The student
    model is trained to replicate the outputs of the teacher model, enabling it to
    acquire a more accurate quantized representation. While this technique can achieve
    higher accuracy compared to direct quantization, it necessitates an additional
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization support is natively available in DL frameworks such as PyTorch
    and TensorFlow. For example, PyTorch supports both forms of quantization via its
    `torch.quantization` package. TensorFlow supports quantization through the `tf.lite`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning (also known as sparsity)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Pruning** is another network compression technique that eliminates some of
    the model weights and neurons that don’t impact model performance to reduce the
    size of the model to make inference faster. For example, weights that are close
    to zero or redundant can usually be removed.'
  prefs: []
  type: TYPE_NORMAL
- en: Pruning techniques can be classified into static and dynamic pruning. Static
    pruning takes place offline before the model is deployed, while dynamic pruning
    is performed during runtime. Here, we will discuss some of the key concepts and
    approaches for static pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Static pruning mainly consists of three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Parameter selection for pruning targeting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pruning the neurons.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tuning or retraining if needed. Retraining may improve the model performance
    of the pruned neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are several approaches for selecting the parameters for static pruning,
    including the magnitude-based approach, the penalty-based approach, and dropout
    removal:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Magnitude-based approach**: It is widely accepted that large model weights
    are more important than small model weights. So, one intuitive way to select weights
    for pruning is to look at zero-value weights or those weights within a defined
    absolute threshold. The magnitude of the neural network activation layer can also
    be used to determine whether the associated neurons can be removed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Penalty-based approach**: In the penalty-based approach, the goal is to modify
    the loss function or add additional constraints so that some weights are forced
    to become zeros or near-zeros. The weights that are zeros or close to zeros can
    then be pruned. An example of the penalty-based approach is using LASSO to shrink
    the weights of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout removal**: Dropout layers are used in deep neural network training
    as regularizers to avoid overfitting data. While dropout layers are useful in
    training, they are not useful for inference and can be removed to reduce the number
    of parameters without impacting the model’s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization-based pruning**: This technique involves adding regularization
    terms to the loss function during training, which encourages the model to learn
    sparse representations. Examples include L1 regularization (LASSO), which promotes
    sparsity by driving some weights to exactly zero, and Group Lasso, which prunes
    entire filters or channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement-based pruning**: This method adopts RL to compress models automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL frameworks, such as TensorFlow and PyTorch, provide APIs for pruning models.
    For example, you can use the `tensorflow_model_optimization` package and its `prune_low_magnitude`
    API for magnitude-based pruning. PyTorch provides model-pruning support via its
    `torch.nn.utils.prune` API.
  prefs: []
  type: TYPE_NORMAL
- en: The primary trade-off with both quantization and pruning is the potential loss
    of model accuracy or performance in exchange for efficiency gains. More aggressive
    quantization or pruning can lead to higher compression rates but may also result
    in a larger drop in accuracy. Finding the right balance between compression and
    accuracy is crucial. Quantized or pruned models may also have limited portability
    across different hardware platforms or deep learning frameworks, as the optimized
    formats and sparse structures may not be universally supported.
  prefs: []
  type: TYPE_NORMAL
- en: Graph and operator optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to hardware acceleration and model optimization, there are additional
    optimization techniques that focus on the execution optimization of the computational
    graph, as well as hardware-specific operator and tensor optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Graph optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graph optimization focuses on reducing the number of operations that are performed
    in computational graphs to speed up inference. Multiple techniques are used for
    graph optimization, including operator fusion, dead code elimination, and constant
    folding.
  prefs: []
  type: TYPE_NORMAL
- en: Operator fusion combines multiple operations in a subgraph into a single operation
    to improve latency. In a typical execution of a subgraph with multiple operations,
    system memory is accessed for read/write to transfer data between operations,
    which is an expensive task. Operator fusion reduces the number of memory accesses,
    as well as optimizing the overall computations since the computations are now
    happening in a single kernel without the intermediate results being saved to memory.
    This approach also reduces the memory footprint due to a smaller number of operations
    being performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the concept of operator fusion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – Graph operator fusion ](img/B20836_10_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.16: Graph operator fusion'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the **matrix multiplication**, **add**, and **ReLU**
    operators are being fused into a single operator for execution in a single kernel
    to reduce memory access and the time needed to start multiple kernels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Constant folding is the process of evaluating constants at compile time instead
    of runtime to speed up processing during runtime. For example, for the following
    expression, *A* can be assigned to a value of 300 at compile time instead of being
    dynamically calculated at runtime, which requires a more computational cycle:
    *A = 100 + 200*. Dead code elimination removes the code that does not affect the
    program’s results. This ensures that the program doesn’t waste computation on
    useless operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Operator optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Operator optimization (also known as tensor optimization) focuses on hardware-specific
    optimization for a specific model. Different hardware devices have different memory
    layouts and computational units and, as such, hardware-specific optimization is
    often required to take full advantage of the hardware architecture. Multiple techniques
    have been developed for operator optimization on different hardware devices, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Nested parallelism, which takes advantage of the GPU memory hierarchy and enables
    data reuse across threads through shared memory regions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory latency hiding, which overlaps the memory operation with computation
    to maximize memory and compute resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While graph optimization, operator optimization, and model optimization address
    different areas of optimization, they are often combined to provide end-to-end
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Model compilers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Manually optimizing end-to-end model performance is non-trivial. Adding the
    dimensions of multiple ML frameworks and a wide range of target hardware devices
    for optimization makes this a very challenging problem. To simplify the optimization
    process for different ML frameworks and different devices, several open-source
    and commercial products have been developed. We will briefly talk about a few
    such packages in this section.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow XLA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TensorFlow **Accelerated Linear Algebra** (**XLA**) is a DL compiler for TensorFlow.
    It compiles a TensorFlow graph into a sequence of execution kernels specifically
    optimized for the model. XLA transforms the original TensorFlow graph into an
    **intermediate representation** (**IR**) before performing several optimizations
    on the IR, such as operator fusion for faster computation. The output from the
    optimization step is then used for generating hardware-specific code that optimizes
    the performance of the different target hardware devices, such as CPUs and GPUs.
    XLA is used at Google in production for many accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Glow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch Glow is a DL compiler for multiple DL frameworks. Similar to XLA, it
    also uses an IR to represent the original computational graph to perform optimizations.
    Unlike XLA, PyTorch Glow uses two layers of IRs. The first layer is used for performing
    domain-specific optimizations such as quantization, while the second IR layer
    is used for memory-related optimization such as memory latency hiding. After the
    second layer of IR optimization, the target device-dependent code is generated
    for running the models on different devices.
  prefs: []
  type: TYPE_NORMAL
- en: Apache TVM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache **Tensor Virtual Machine** (**TVM**) is an open-source compiler framework
    for model optimization. It optimizes and compiles models built with different
    frameworks, such as PyTorch and TensorFlow, for different target CPUs, GPUs, and
    specialized hardware devices for accelerated performance. TVM supports optimization
    at different levels, including graph optimization and operator optimization targeting
    specific hardware. It also comes with a runtime for efficiently executing the
    compiled models.
  prefs: []
  type: TYPE_NORMAL
- en: One key feature of TVM is AutoTVM, which uses ML to search for the optimal sequences
    of code execution for different hardware devices. This ML-based search algorithm
    can significantly outperform baseline benchmarks by using vendor-provided optimization
    libraries such as cuDNN. This ML-based approach also enables efficient compilation
    scaling for a large number of hardware devices.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker Neo
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon SageMaker Neo is the model-compiling feature in SageMaker. It mainly
    uses Apache TVM as its underlying compiler library. With SageMaker Neo, you take
    a model that’s been trained on different ML/DL frameworks such as TensorFlow and
    PyTorch, choose the target processors such as Intel, Apple, ARM, or Nvidia, and
    then SageMaker Neo compiles an optimized model for the target hardware. Neo also
    provides a runtime library for each target platform to load and execute the compiled
    model. SageMaker Neo is a managed offering, so you don’t need to manage the underlying
    infrastructure and processes for model compilation and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Inference engine optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One common model deployment pattern is to use open-source inference engines
    or commercial hosting platforms for model serving. So, inference engine optimization
    is another approach that helps reduce model latency and inference throughput.
    In this section, we will talk about a few considerations. Note that there are
    no universal rules for inference engine optimization as it is sometimes engine-
    and model-specific. It is important to test and validate different configurations
    for the final deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Inference batching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have a large number of inference requests and there is no strict latency
    requirement on a single prediction request, then inference batching is a technique
    that can help reduce the total inference time for the requests. With inference
    batching, instead of running predictions one at a time for each request, multiple
    requests are batched together and sent to the inference engine. This technique
    reduces the total number of requests round-trips, thus reducing the total inference
    time. Inference engines such as TensorFlow Serving and TorchServe provide built-in
    support for batch inference. You can find the configuration details for TorchServe
    and TensorFlow Serving batch inference at [https://pytorch.org/serve/batch_inference_with_ts.html](https://pytorch.org/serve/batch_inference_with_ts.html)
    and [https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration](https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration),
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling parallel serving sessions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If your model hosting server has multiple compute cores, you can configure the
    number of parallel serving sessions to maximize the utilization of the available
    cores. For example, you can configure the `TENSORFLOW_INTRA_OP_PARALLELISM` setting
    in TensorFlow Serving based on the number of cores that can run multiple serving
    sessions in parallel to optimize throughput. TorchServe has settings for the number
    of workers per model and the number of threads for parallelization optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Picking a communication protocol
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inference engines such as TensorFlow and TorchServe provide support for the
    gRPC protocol, which is a faster serialization format than the REST protocol.
    The gPRC protocol provides better overall performance but does have performance
    benchmarks as different models could behave differently. The REST protocol may
    be your preferred option depending on your specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: With that, you have learned about the technical approaches to large-scale training
    and low-latency model inference.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have discussed some of the considerations for inference optimization,
    we will next explore some of the more recent advancements in inference technology
    for large language models.
  prefs: []
  type: TYPE_NORMAL
- en: Inference in large language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As language models continue to grow in size, the task of deploying and running
    them becomes increasingly challenging. Even with model optimization efforts, these
    expansive models often exceed the memory capacity of a single GPU. To address
    this hurdle and enable the deployment of these large language models, numerous
    ML inference frameworks have emerged. Let’s delve into a few of these frameworks
    to gain insight into how they facilitate the inference process for these language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation Inference (TGI)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TGI is an optimized serving solution by HuggingFace for deploying open-source
    large language models like Falcon and FLAN-T5\. TGI has the following key capabilities
    for large language model inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tensor parallelism**: This feature allows a large language model to be deployed
    across multiple GPUs so it can fit into the combined GPU memory as well as faster
    inference across multiple GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantization**: TGI can perform model quantization with the bitsandbytes
    and GPT-Q quantization library packages to reduce the model size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous batching**: This feature increases the throughput of the inference
    by running multiple input sequences by using the same loaded model parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepSpeed-Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to being a framework for large-scale distributed training, DeepSpeed
    can also help optimize the inference of large language models. It has the following
    key features for inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model parallelism**: This feature fits a large model that would otherwise
    not fit into GPU memory by splitting a model across multiple GPU devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference-optimized kernel**: DeepSpeed can fuse element-wise operations,
    matrix multiplications, transpositions, and reductions all into a single kernel,
    significantly reducing the number of kernel invocations as well as main memory
    access to reduce main memory access latency. DeepSpeed-Inference kernels are also
    fine-tuned to maximize the memory bandwidth utilization for loading the parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantization**: A novel approach to quantizing models, called Mixture of
    Quantization, involves both shrinking the model and reducing the inference cost
    during production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FastTransformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'FasterTransformer is a high-performance library developed by Nvidia, designed
    to accelerate the inference of transformer-based neural networks, particularly
    for large models that span multiple GPUs and nodes in a distributed setup. This
    open-source framework is focused on optimizing transformer blocks, encompassing
    both encoder and decoder components. It has the following key features supporting
    inference of transformer-based models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model parallelism**: With FastTransformer, a transformer model can be split
    across multiple GPUs for faster inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization support:** FasterTransformer optimizes transformer-based models
    with techniques like layer fusion, multi-head attention acceleration using data
    caching, **General Matrix Multiply** (**GEMM**) kernel autotuning for efficient
    matrix multiplication, and support for lower-precision data types like FP16, BF16,
    and INT8\. Operations on these data types can be accelerated by Tensor Cores on
    the recent NVIDIA GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on lab – running distributed model training with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an ML solutions architect, you need to explore and design different model-training
    paradigms to meet different model-training requirements. In this hands-on lab,
    you will use the SageMaker Training service to run data-parallel distributed training.
    We will use PyTorch’s `torch.nn.parallel.DistributedDataParallel` API as the distributed
    training framework and run the training job on a small cluster. We will reuse
    the dataset and training scripts from the hands-on lab in *Chapter 8*, *Building
    a Data Science Environment Using AWS ML Services*.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Chapter 8*, we trained a financial sentiment model using the data science
    environment you created using SageMaker. The model was trained using a single
    GPU in the Studio Notebook and SageMaker training service. Anticipating the future
    needs of model training with large datasets, we need to design an ML training
    process using multiple GPUs to scale out training horizontally.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset description
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the financial phrase dataset for this lab: [https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news).'
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the training script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we need to add distributed training support to the training script.
    To start, create a `code` directory in your Studio notebook environment, create
    a copy of the `train.py` file and save it to the `code` directory, and then rename
    the file `train-dis.py`, and open the `train-dis.py` file. You will need to make
    changes to the following three main functions. The following steps are meant to
    highlight the key changes needed. To run the lab, you can download the modified
    `train-dis.py` file from [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10).
    The following are the key changes to be made in the `train-dis.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Modifying the** `train()` **function**: You need to make some changes to
    the `train()` function to enable distributed training. The following are the key
    changes that are required:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Process group initialization**: To enable distributed training, we need to
    initialize and register each training process on each device to be included in
    the training group. This can be achieved by calling the `torch.distributed.init_process_group()`
    function. This function will block until all the processes have been registered.
    There are a few concepts that we need to be familiar with during this initialization
    step:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**World size**: This is the total number of processes in a distributed training
    group. Since we will run one process on each device (CPU or GPU), the world size
    is also the same as the total number of devices in a training cluster. For example,
    if you have two servers and each server has two GPUs, then the world size is four
    for this training group. The `torch.distributed.init_process_group()` function
    uses this information to understand how many processes to include in the distributed
    training job.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rank**: This is the unique index that’s assigned to each process in the training
    group. For example, the ranks for all the processes in a training group with a
    world size of four would be [0,1,2,3]. This unique index helps uniquely identify
    each process within a training group for communication.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local rank**: This uniquely identifies a device in a server node. For example,
    if there are two devices in a server node, the local rank for two devices would
    be [0,1]. Local rank allows you to select a specific device to load the model
    and data for model training.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backend**: This is the low-level communication library for exchanging and
    aggregating data among the different processes. PyTorch distributed training supports
    several communication backends, including NCCL, MPI, and Gloo. You choose a different
    backend based on the device and networking configuration. It uses these backends
    to send, receive, broadcast, or reduce data during distributed training. We are
    not going to get into the technical details of these backends in this book. If
    you are interested in how these backends work, you can easily find internet sources
    that cover these topics.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wrap the training algorithm with PyTorch distributed library**: To use the
    PyTorch distributed library support for training, you need to wrap the algorithm
    with the PyTorch distributed training library. You can achieve this with the `torch.nn.parallel.DistributedDataParallel()`
    API. This allows the algorithm to participate in distributed training to exchange
    gradients and update global parameters.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saving model using a single device**: In a multi-device server node, you
    only want one device to save the final model to avoid I/O conflicts. You can achieve
    this by selecting a device with a specific local rank ID.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modifying the** `get_data_loader()` **function**: To ensure a different subset
    of training data is loaded onto different devices on the server nodes, we need
    to configure the PyTorch DataLoader API to load data based on the rank of the
    training process. This can be done using the `torch.utils.data.distributed.DistributedSampler`
    API.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adding multi-processing launch support for multi-device server nodes**: For
    server nodes with multiple devices, we need to spawn several parallel processes
    based on the number of devices available. To enable this, we can use `torch.multiprocessing`
    to kick off multiple running processes on each node.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the training script modified, we will update the laucher notebook next.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying and running the launcher notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are now ready to modify the launcher notebook to kick off the model training
    job:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, copy the `bert-financial-sentiment-Launcher.ipynb` file from `chapter
    8` and save it as `bert-financial-sentiment-dis-Launcher.ipynb`. Open the new
    notebook and replace the second cell’s content with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The main code changes are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Point the entry point to the new training script (`entry_point="train-dis.py"`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase the number of compute instance from 1 to 2 for multi-node training
    (`instance_count=2`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the instance type for multi-GPU support (`instance_type= "ml.g4dn.12xlarge"`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Increase the batch size because there are now more GPUs to handle a larger
    batch size (`"batch_size" : 64`)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Download [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/code/requirements.txt](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/code/requirements.txt)
    and upload it to the `code` directory in your Studio Notebook. This `requirements.txt`
    contains the library packages to be installed in the trainer container. In this
    case, we want to install the transformer package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you don’t want to manually revise the launcher notebook, you can download
    the revised launcher notebook at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/bert-financial-sentiment-dis-launcher.ipynb](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/bert-financial-sentiment-dis-launcher.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Now, just execute each cell in the new notebook to kick off the distributed
    training. You can track the training status directly inside the notebook, and
    the detail status in CloudWatch Logs. You should see a total of eight processes
    running in parallel. Take note of the total training time and accuracy and see
    how they compare with the results you got from *Chapter 8*, *Building a Data Science
    Environment Using AWS ML Services*.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully trained a BERT model using the PyTorch
    distributed training library.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved into the advanced topics of ML engineering. We covered
    distributed training for handling extensive datasets and large-scale models, along
    with strategies for achieving low-latency inference. Hopefully, you now have a
    solid understanding of data parallelism and model parallelism, as well as the
    diverse technology choices available, such as the PyTorch distributed library
    and SageMaker distributed training library, for implementing distributed training
    using these approaches. Additionally, you should be well equipped to discuss various
    techniques for optimizing models to minimize inference latency, including the
    utilization of model compiler tools designed for automated model optimization.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have focused on training ML models from scratch and designing ML
    platforms for the training and deployment of ML models to support the development
    of intelligent applications. However, we don’t always need to build models from
    scratch. In the next chapter, we will explore ready-to-use AI services and see
    how AI services can be used to build intelligent applications quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Leave a review!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enjoying this book? Help readers like you by leaving an Amazon review. Scan
    the QR code below to get a free eBook of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Review_Copy.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Limited Offer*'
  prefs: []
  type: TYPE_NORMAL
