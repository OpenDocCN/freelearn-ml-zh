- en: Chapter 4. Machine Learning Tools, Libraries, and Frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered the Machine learning solution architecture
    and the implementation aspects of a technology platform—Hadoop. In this chapter,
    we will look at some of the highly adopted and upcoming Machine learning tools,
    libraries, and frameworks. This chapter is a primer for the following chapters
    as it covers how to implement a specific Machine learning algorithm using out-of-box
    functions of an identified Machine learning framework.
  prefs: []
  type: TYPE_NORMAL
- en: We will first cover the landscape of open source and commercial Machine learning
    libraries or tools that are available in the market, and pick the top five open
    source options. For each of the identified options, starting from installation
    steps, learning the syntax, implementing a complex Machine learning algorithm,
    to plotting graphs, we will cover it all. This chapter is mandatory for the readers
    in the order of occurrence as it is a foundation for all the example implementations
    in the chapters that follow.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the identified frameworks can operate as standalone libraries and can
    run on Hadoop as well. In addition to learning how to program and implement a
    Machine learning algorithm, we will also cover how each of the identified frameworks
    integrate and run on Hadoop; this is what differentiates these tutorials from
    the mainstream ones found on the web.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics listed here are covered in depth in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief list of commercial and open source Machine learning libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top libraries or frameworks covered are R, Mahout, Julia, Python (Machine learning
    libraries, in particular), and Spark.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Mahout is a framework used for running Machine learning algorithms built
    over Hadoop and is a Java-based open source Machine learning option. This framework
    can also work standalone. It is known for running Machine learning algorithms
    to heavy volumes of data. This framework is a part of Hadoop ecosystem components
    and has its distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R is an open source Machine learning and a data mining tool that is adopted
    very widely in the Machine learning community. This framework library can either
    work standalone or can be run on Hadoop using the Hadoop runtime R extensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia is an open source high-performance programming language that supports
    running numeric and statistical computing functions in a distributed and parallel
    way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python is an interpreted, high-level programming language that is designed to
    try out different things and it is something that does not fall into the traditional
    waterfall way of development. We will explore the basic Python libraries—**NumPy**
    and **SciPy** and use scikit-learn to execute our first Machine learning program.
    Also, we will explore how to write a Hadoop MapReduce program in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache Spark and its Machine learning core libraries: Spark is a cluster computing
    system with API for Java, Python, and Scala. We will explore the **MLlib API**
    for Machine learning and use a version for Apache Hadoop. The focus will be to
    explore the Spark Java APIs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief introduction to Spring XD and the related Machine learning libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each of the identified Machine learning frameworks, integration with Hadoop
    will be a primary focus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning tools – A landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several open source and commercial Machine learning frameworks and
    tools in the market that have evolved over the last few decades. While the field
    of Machine learning itself is evolving in building powerful algorithms for diverse
    requirements across domains, we now see a surge of open source options for large-scale
    Machine learning that have reached a significant level of maturity and are being
    widely adopted by the data science and Machine learning communities.
  prefs: []
  type: TYPE_NORMAL
- en: The model has changed significantly in the recent past, and researchers are
    encouraged to publish their software under an open source model. Since there are
    problems that authors face while publishing their work in using algorithmic implementations
    for Machine learning, any work that is reviewed and improvised through usage by
    the data science community is considered to be of more value.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure shows a concept model of some important commercial and
    open source Machine learning frameworks and tools in the market. The highlighted
    ones will be covered in depth in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning tools – A landscape](img/B03980_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Some of these libraries are around specific programming languages such as Java,
    Python, C++, Scala, and so on. Some of these libraries, like Julia, Spark, and
    Mahout already support distributed, and parallel processing and others such as
    R and Python can run as MapReduce functions on Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sections, for each of the highlighted Machine learning libraries,
    the following will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the library or tool with the details of out-of-box Machine learning
    functions supported
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation, setup, and configuration guide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to syntax and basic data processing functions, and then the Advanced
    Machine learning functions example implementations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samples for visualizations and plotting (wherever applicable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration and execution on the Hadoop platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Mahout is a Machine learning library that comes packaged with Apache
    Hadoop and forms an important part of the Hadoop ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Mahout came into existence in 2008 as a subproject of Apache Lucene (an open
    source search engine). Lucene is an API that has an implementation of search,
    text mining, and information-retrieval techniques. Most of these search and text
    analytics internally apply Machine learning techniques. The recommendation engines
    that were built for the search engines started off under a new subproject called
    Mahout. Mahout means the *rider of an elephant*, signifying the running of Machine
    learning algorithms over Hadoop. It is a scalable Machine learning implementation
    that can run in a standalone mode (does not tightly integrate with Hadoop) as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: '![Apache Mahout](img/B03980_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Mahout is a set of some basic Machine learning Java libraries used for classification,
    clustering, pattern mining, and so on. Though Mahout today provides support for
    a subset of Machine learning algorithms, it still ranks among the most adopted
    frameworks as it inherently supports analytics on large datasets to the degree
    of hundreds of millions of rows, which can be unstructured in nature as well.
  prefs: []
  type: TYPE_NORMAL
- en: How does Mahout work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mahout implements Hadoop MapReduce, and the most important aspect is that it
    works on top of Hadoop and applies a distributed computing paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: '![How does Mahout work?](img/B03980_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following are some of the specific Machine learning tasks that Mahout currently
    implements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collaborative Filtering / Recommendation**: This takes a user input and finds
    items that the users might like'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: This takes a bunch of documents as input and groups them based
    on the topics they refer/belong to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**: This takes a bunch of documents and, based on the existing
    categorization of the documents, learns what category a given document might belong
    to, and maps the document to that category'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frequent itemset mining**: This takes a bunch of items as input and, based
    on the learning from the real occurrences, identifies which items occur or appear
    together'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are certain algorithms, for example, logistic regression and SVM (more
    about these algorithms will be covered in the chapters to follow), which cannot
    be parallelized and run in a standalone mode.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and setting up Apache Mahout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we will look at how to run Mahout in a standalone mode and
    on Hadoop. Though there was the new 1.0 version of Apache Mahout available at
    the time of writing this book, we will use version 0.9 (the latest stable version)
    in all the examples. The operating system used is Ubuntu 12.04 desktop 32-bit
    version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are the dependencies and key requirements for installing Apache Mahout:'
  prefs: []
  type: TYPE_NORMAL
- en: JDK (1.6 or above; we will use 1.7 u9 version for the examples throughout this
    book)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maven (2.2 or above; we will use 3.0.4 for the examples throughout this book)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Hadoop (2.0; not mandatory as Mahout can be run locally)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Mahout (0.9 distribution)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Development environment—Eclipse IDE (Luna)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html "Chapter 3. An Introduction to Hadoop's Architecture
    and Ecosystem"), *An Introduction to Hadoop's Architecture and Ecosystem*, we
    have seen how Apache Hadoop 2.0 single node installation is done along with the
    required prerequisites like Java.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover the setting up of Maven, Eclipse for the development
    environment, and configuring Apache Mahout to run on and off Hadoop. As the considered
    platform and related frameworks are open sources, we will use the VirtualBox machine
    emulator hosted by the Windows 7 Professional edition.
  prefs: []
  type: TYPE_NORMAL
- en: As you may recollect, Hadoop cannot run as a root user, and hence we have a
    user created for this purpose—`practical-ml` to install and run everything.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Maven
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is recommended that Maven is used to get the required Mahout jars, and it
    gets easy to switch to any newer versions easily with Mahout. In the absence of
    Maven, downloading the dependencies will get more complicated. For more details
    on specific features of Maven and its utility in application development, refer
    to [https://www.packtpub.com/application-development/apache-maven-3-cookbook](https://www.packtpub.com/application-development/apache-maven-3-cookbook).
  prefs: []
  type: TYPE_NORMAL
- en: 'Maven version 3.0.4 can be downloaded from one of the mirrors of the Apache
    website. The following command can be used for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To manually install Maven, perform the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract the distribution archive that is, `apache-maven-3.0.4-bin.tar.gz` to
    the directory you wish to install Maven 3.0.4.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these instructions, the `/usr/local/apache-maven` path will be chosen.
    An `apache-maven-3.0.4` subdirectory will be created from the archive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following lines need to be appended to the `.bashrc` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`JAVA_HOME` should point to a location where the JDK is installed. For example,
    export `JAVA_HOME=/usr/java/jdk1.7\. $JAVA_HOME/bin` is in your `PATH` environment
    variable. The `PATH` variable is set during the Java installation. This should
    be verified.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now check for the successful installation of Maven by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In case there are any proxy settings, we will have to explicitly update the
    proxy settings in the `settings.xml` file, which is in the `conf` folder of Maven
    installation.
  prefs: []
  type: TYPE_NORMAL
- en: Setting-up Apache Mahout using Eclipse IDE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The procedure detailed next covers the steps to set up the Mahout environment,
    code base, accessing of examples, running, debugging, and testing them using Eclipse
    IDE. This is the recommended way to set up and is the simplest way to set up Apache
    Mahout for the development teams.
  prefs: []
  type: TYPE_NORMAL
- en: Execute the following steps to get the Apache Mahout tar, untar it and navigate
    to the installation.
  prefs: []
  type: TYPE_NORMAL
- en: Set up Eclipse IDE.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The latest version of Eclipse can be downloaded from the following link:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://www.eclipse.org/downloads/](https://www.eclipse.org/downloads/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Download Mahout Distribution from the direct link using the command here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the archive from it using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the project into an Eclipse project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The earlier command builds the Eclipse project.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set the `M2_REPO` classpath variable to point to the local repository path.
    The following command adds all the Maven jars to the Eclipse classpath:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let's import the Eclipse Mahout projects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate from the menu, **File** | **Import** | **General** | **Existing Projects**
    into **Workspace**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Setting-up Apache Mahout using Eclipse IDE](img/B03980_04_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Setting up Apache Mahout without Eclipse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Download Mahout Distribution from the direct link using the command here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the Mahout distribution to the `/usr/local` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Set the Java, Maven, and Mahout paths in the `.bashrc` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the `.bashrc` file using the command here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following content to the file:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To run Mahout in the local mode (this means in the standalone mode where there
    is no need for Hadoop, and the algorithms will not run in parallel or MapReduce
    mode).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set the local mode to true using the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will force Mahout to not look for the Hadoop configurations in `$HADOOP_CONF_DIR`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`MAHOUT_LOCAL` is set, so we don''t add `HADOOP_CONF_DIR` to the classpath.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There is an alternative to run Mahout on Hadoop. Firstly, ensure Hadoop 2.x
    is installed and configured successfully. Then, follow these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: Set `$HADOOP_HOME`, `$HADOOP_CONF_DIR` are set and added to `$PATH`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The above sets the mode in which Hadoop is run (for example, in `core-site.xml`,
    `hdfs-site.xml`, `mapred-site.xml`, and so on.)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, launch the Hadoop instance using the command here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Check `http://localhost:50030`, and `http://localhost:50070` URLs to confirm
    whether Hadoop is up and running.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Build Apache Mahout using Maven by running the following Maven command from
    the Mahout directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following output is seen on a successful install:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up Apache Mahout without Eclipse](img/B03980_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Mahout Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following figure depicts different packages in Mahout that provide some
    out-of-box support for several Machine learning algorithms. At the core, the modules
    are the utilities, math vectors, collections, and Hadoop with MapReduce for the
    parallel processing and the file system for distributed storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, over the core modules are the Machine learning packages as listed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolutionary Algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommenders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FPM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimension Reduction![Mahout Packages](img/B03980_04_06.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More details are covered on the previous packages in detail in the chapters
    to follow, with example implementations using each of the packages for an identified
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing vectors in Mahout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we understand, to demonstrate most of the Machine learning algorithm implementations
    in Mahout, we need the data in classic Mahout dataset format. At the core, the
    code for this is primary to use some Mahout ready-to-use scripts with some minor
    changes in the settings. Given below is the standard process:'
  prefs: []
  type: TYPE_NORMAL
- en: Create sequence files from the raw text files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sequence files** are predominantly a binary encoding of the key/value pair
    representation of data. The attributes given next are the key header elements
    that represent metadata details:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Version
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Key name
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Value name
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compression
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate vectors from the sequence files. More on the actual commands to generate
    sequence files is covered in the following chapters while demonstrating the implementation
    for each of the identified Machine learning algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running functions on these working vectors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are different types of vector implementations in Mahout, and the definitions
    hold good in general as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing vectors in Mahout](img/B03980_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Dense Vectors**: These vectors are usually an array of doubles, and the size
    of this vector is the same as the number of features in the dataset. Since all
    the entries are preallocated irrespective of a zero value, these vectors are called
    dense vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sparse Vectors**: These vectors are arrays of vectors and are represented
    only with non-zero or null values. With sparse vectors, there are two subcategories:
    the random-access and sequential-access sparse vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random Access Sparse Vectors**: Random access sparse vectors are the HashMap
    representations where the key is an integer value, and the value is a double value.
    At any given point in time, a value can be accessed by passing in the given key.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential Access Sparse Vectors**: These vectors are nothing but a set of
    two arrays where the first array is the array of keys (the integers), and the
    second array is an array of values (the doubles). These vectors are optimized
    for linear reads, unlike the random access sparse vectors. Again, the storage
    is done for only non-zero values.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: For a detailed understanding of working with Apache Mahout, refer to the Packt
    Publication for Apache Mahout titled *Apache Mahout Cookbook*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: While this section covers a framework that is built to work with Hadoop with
    small configuration changes, in the next section, we cover the powerful and highly
    adopted option in the market—R. Hadoop provides explicit adapters to have the
    R programs work in the MapReduce model, which is covered next.
  prefs: []
  type: TYPE_NORMAL
- en: R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R is a language for data analysis and is used as an environment that is a primary
    driver in the field of Machine learning, statistical computing, and data mining
    and provides a comprehensive platform for basic and advanced visualizations or
    graphics. Today, R is a basic skill that almost all data scientists or would-be
    data scientists have or *must* learn.
  prefs: []
  type: TYPE_NORMAL
- en: R is primarily a GNU project known to be similar to the S language that was
    initially developed at Bell Laboratories (formerly known as AT&T and now, Lucent
    Technologies) by John Chambers and team. The initial goal for S was to support
    all statistical functions and was widely used by hard-core statisticians.
  prefs: []
  type: TYPE_NORMAL
- en: R comes with a wide range of open source packages that can be downloaded and
    configured free of cost, and are installed or loaded on a need basis into the
    R environment. These packages provide out-of-box support for a wide variety of
    statistical techniques that include linear and non-linear modeling, time-series
    analysis, classification, clustering, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Along with these, the highly extensible graphical functions are available. The
    support for these advanced graphical functions has been a primary differentiator
    for R as the output is known for its publication quality plots. In addition to
    these, R also supports many open source graphical libraries and visualization
    tools that are both open source and commercial in nature.
  prefs: []
  type: TYPE_NORMAL
- en: Though, at the core, R is not meant to work in a distributed environment or
    run the algorithms in a parallel mode, there are several extensions available
    (both open source and commercial) that make R more scalable and support large
    dataset. In this chapter, we will cover how R can be integrated with Apache Hadoop,
    and thus can run and leverage the MapReduce capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, R is free software that is widely adopted and has many committers
    and support groups constantly working on retaining its high relevance in the field
    of data science.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the key capabilities that R supports today are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to effectively manage and store data that the models operate on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facilitating some core suite of functions for calculations on arrays, vectors,
    and matrices among others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several out-of-box Machine learning functions that can be loaded on demand and
    help implement data science projects with ease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced and sophisticated graphical functions that can be used with ease and
    help to produce valuable dashboards for business owners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A wide and active community of adopters and committers that has developed rapidly
    with extensions via a large collection of packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R is considered as a platform that supports newly developing methods of interactive
    data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and setting up R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For all the examples in this book, we will use the stable version 2.15.1 of
    R and the CRAN references for all the latest R packages.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the [https://cran.r-project.org/bin/windows/base/old/2.15.1/](https://cran.r-project.org/bin/windows/base/old/2.15.1/)
    link to download R for Windows.
  prefs: []
  type: TYPE_NORMAL
- en: A detailed installation process is covered at [https://cran.r-project.org/doc/manuals/R-admin.html#Top](https://cran.r-project.org/doc/manuals/R-admin.html#Top).
  prefs: []
  type: TYPE_NORMAL
- en: We can use R with the R GUI or the IDE RStudio. Following are the screenshots
    of the R interface that the users can see post a successful installation of the
    R GUI and R IDE, and the RStudio.
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing and setting up R](img/B03980_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will need to set the CRAN mirror path to be able to access and load the required
    R packages by navigating from the menu path **Packages** | **Set CRAN mirror**
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing and setting up R](img/B03980_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows a list of mirror sites from which the developer
    can choose the most appropriate one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing and setting up R](img/B03980_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The R Editor can be used to write any advanced operations, and the results
    can be seen on the console as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing and setting up R](img/B03980_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following is a screenshot of a graphical plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing and setting up R](img/B03980_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Integrating R with Apache Hadoop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have seen Apache Hadoop and its core components, HDFS and YARN (MapReduce
    2.0), and R. There are three different ways in which we can look at integrating
    R with Hadoop, and hence the support for large-scale Machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Approach 1 – Using R and Streaming APIs in Hadoop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To integrate an R function with Hadoop and see it running in a MapReduce mode,
    Hadoop supports Streaming APIs for R. These Streaming APIs primarily help in running
    any script that can access and operate with standard I/O in a MapReduce mode.
    So in the case of R, there wouldn''t be any explicit client side integration done
    with R. The following is an example of R and streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Approach 2 – Using the Rhipe package of R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is a package in R called Rhipe that allows running a MapReduce job within
    R. To use this way of implementing R on Hadoop; there are some prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: R needs to be installed on each DataNode in the Hadoop Cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protocol Buffers will be installed and available on each DataNode (for more
    information on Protocol Buffers refer to [http://wiki.apache.org/hadoop/ProtocolBuffers](http://wiki.apache.org/hadoop/ProtocolBuffers))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rhipe should be available on each data node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a sample format for using the `Rhipe` library in R to implement
    MapReduce:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Approach 3 – Using RHadoop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RHadoop, very similar to Rhipe, facilitates running R functions in a MapReduce
    mode. It is an open source library built by Revolution Analytics. Following are
    some packages, which are a part of the RHadoop library:'
  prefs: []
  type: TYPE_NORMAL
- en: '**plyrmr**: This is a package that provides functions for common data manipulation
    requirements for large datasets running on Hadoop'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rmr**: This is a package that has a collection of functions that integrate
    R and Hadoop'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rdfs**: This is a package with functions that help interface R and HDFS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rhbase**: This is a package with functions that help interface R and HBase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example that uses the rmr package and demonstrates steps
    to integrate R and Hadoop using the functions from this package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Summary of R/Hadoop integration approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In summary, all of the previous three approaches yield results and facilitate
    integrating R and Hadoop. They help in scaling R to operate on the large-scale
    data that will help with HDFS. Each of these approaches has pros and cons. Here
    is a summary of conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop Streaming API is the simplest of all the approaches as there are no complications
    regarding installation and setup requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both Rhipe and RHadoop require some effort to setup R and related packages on
    the Hadoop cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regarding implementation approach, Streaming API is more of a command line map,
    and reduce functions are inputs to the function, whereas both Rhipe and RHadoop
    allow developers to define and call custom MapReduce functions within R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case of Hadoop Streaming API, there is no client side integration required,
    whereas both Rhipe and RHadoop require the client side integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The alternatives to scaling Machine learning are Apache Mahout, Apache Hive,
    and some commercial versions of R from Revolution Analytics, Segue framework,
    and others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing in R (using examples)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will briefly cover some implementation aspects of R, and
    focus on learning the syntax and understanding some core functions and its usage.
  prefs: []
  type: TYPE_NORMAL
- en: R Expressions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'R can be used as a simple math calculator; here are some basic ways of using
    it. Here is a trace of what is seen on the R console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Assignments
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This is used to assign value to a variable and apply some operations to this
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 1: Assigning a numeric value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Case 2: Assigning a string literal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Case 3: Assigning a logical value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Functions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are many out-of-box functions and to invoke a function in R, we should
    provide the function name and pass required arguments. Here are some examples
    of functions and the results, as seen in the R Console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the command to get help for a function in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: R Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A vector is a simple list of values by definition that forms the core of R data
    types. Many of the Machine learning functions leverage these.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key functions with their usage context:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Function/Syntax | Purpose | Example | Output on R Console |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `m:n` | Outputs numbers from `m` to `n` increment by 1 | `> 5:9` | `[1] 5
    6 7 8 9` |'
  prefs: []
  type: TYPE_TB
- en: '| `seq(m,n)` | Outputs numbers from `m` to `n` increment by 1 | `> seq(5,9)`
    | `[1] 5 6 7 8 9` |'
  prefs: []
  type: TYPE_TB
- en: '| `seq(m,n, i)` | Outputs numbers from `m` to `n` increment by `i` | `> seq(1,3,0.5)`
    | `[1] 1 1.5 2 2.5 3` |'
  prefs: []
  type: TYPE_TB
- en: Assigning, accessing, and manipulating vectors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The following table has examples for creating, accessing, and manipulating
    matrices in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Purpose | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Creating a vector of literals | `> sentence <- c(''practical'', ''machine'',
    ''learning'')` |'
  prefs: []
  type: TYPE_TB
- en: '| Accessing the third value of the vectors | `> sentence[3]``[1] "learning."`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Updating a value in the vector | `> sentence[1] <- "implementing"` |'
  prefs: []
  type: TYPE_TB
- en: '| Adding a new value to the vector | `> sentence[4] <- "algorithms"` |'
  prefs: []
  type: TYPE_TB
- en: '| Getting values for the given indices | `> sentence[c(1,3)]``[1] "implementing"
    "learning"` |'
  prefs: []
  type: TYPE_TB
- en: '| Getting values for range of indices | `> sentence[2:4]``[1] "machine" "learning"
    "algorithms"` |'
  prefs: []
  type: TYPE_TB
- en: '| Adding a range of new values | `> sentence[5:7] <- c(''for'',''large'',''datasets'')`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Incrementing vector values by 1 | `> a <- c(1, 2, 3)``> a + 1``[1] 2 3 4`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dividing each value in vector by a value | `> a / 2``[1] 0.5 1.0 1.5` |'
  prefs: []
  type: TYPE_TB
- en: '| Multiplying each value of the vector by a value | `> a*2``[1] 2 4 6` |'
  prefs: []
  type: TYPE_TB
- en: '| Adding two vectors | `> b <- c(4, 5, 6)``> a + b``[1] 5 7 9` |'
  prefs: []
  type: TYPE_TB
- en: '| Comparing two vectors | `> a == c(1, 99, 3)``[1] TRUE FALSE TRUE` |'
  prefs: []
  type: TYPE_TB
- en: '| Applying a function on each value of the vector | `> sqrt(a)``[1] 1.000000
    1.414214 1.732051` |'
  prefs: []
  type: TYPE_TB
- en: R Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Matrices are two-dimensional vectors that have rows and columns. The following
    table has examples for creating, accessing, and manipulating matrices in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Purpose | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Creating a 3 X 4 matrix with values defaulted to zero | `> matrix(0, 3, 4)``[,1]
    [,2] [,3] [,4]``[1,] 0 0 0 0``[2,] 0 0 0 0``[3,] 0 0 0 0` |'
  prefs: []
  type: TYPE_TB
- en: '| Initializing a matrix with a range of values | `> a <- 1:12``> m <- matrix(a,
    3, 4)``[,1] [,2] [,3] [,4]``[1,] 1 4 7 10``[2,] 2 5 8 11``[3,] 3 6 9 12` |'
  prefs: []
  type: TYPE_TB
- en: '| Accessing a value from the matrix | `> m[2, 3]``[1] 8` |'
  prefs: []
  type: TYPE_TB
- en: '| Assigning a value to a position of choice in a matrix | `> m[1, 4] <- 0`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieving an array of the entire row or a column of choice | `> m[2,]``[1]
    2 5 8 11``> m[3,]``[1] 7 8 9` |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieving a subset of the bigger matrix | `> m[, 2:4]``[,1] [,2] [,3]``[1,]
    4 7 10``[2,] 5 8 11` |'
  prefs: []
  type: TYPE_TB
- en: R Factors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In data analytics and Machine learning, it is common to group or categorize
    data. For example, a good or a bad customer. R's `factor` data type is used to
    track the categorized data. All that needs to be done is defining a vector of
    categories and passing it as a parameter to the `factor` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates creation and assignment of categories using
    `factors`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of the defined categories usually has an integer value associated with
    the literal. Passing the `factor` to the `as.integer` function will give the integer
    equivalents, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: R Data Frames
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data frames relate to the concept of database tables. This data type is very
    powerful in R, and it helps tie different related attributes of a dataset together.
    For example, the number of items purchased has a relationship with the total bill
    value and the overall applicable discount. There should be a way to link these
    attributes, and data frames help to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Purpose | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Creating a data frame and checking the values | `> purchase <- data.frame(totalbill,
    noitems, discount``> print(purchase)``totalbill noitems discount``1 300 5 10``2
    200 3 7.5``3 100 1 5``)` |'
  prefs: []
  type: TYPE_TB
- en: '| Accessing the data of the data frame using indexes or labels | `> purchase[[2]]``[1]
    5 3 1``> purchase[["totalbill"]]``[1] 300 200 100``> purchase$discount``[1] 10
    7.5 5` |'
  prefs: []
  type: TYPE_TB
- en: '| Loading data frames with the data from CSV files | `> list.files()``[1] "monthlypurchases.csv"``>
    read.csv("monthlypurchases.csv")``Amount Items Discount``1 2500 35 15``2 5464
    42 25``3 1245 8 6` |'
  prefs: []
  type: TYPE_TB
- en: R Statistical frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'R supports a bunch of statistical out-of-box functions that help statisticians
    explain the data. Some of the functions with examples are shown in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | `limbs <- c(4, 3, 4, 3, 2, 4, 4, 4)``names(limbs) <- c(''One-Eye'',
    ''Peg-Leg'', ''Smitty'', ''Hook'', ''Scooter'', ''Dan'', ''Mikey'', ''Blackbeard'')``>
    mean(limbs)``[1] 3.5` |'
  prefs: []
  type: TYPE_TB
- en: '| Median | `> median(limbs)``[1] 4` |'
  prefs: []
  type: TYPE_TB
- en: '| Standard deviation | `> pounds <- c(45000, 50000, 35000, 40000, 35000, 45000,
    10000, 15000)``> deviation <- sd(pounds)` |'
  prefs: []
  type: TYPE_TB
- en: Each piece of the contained R code is saved for a run in a file with the `.R`
    extension.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have seen how R can be set up and how some basic functions
    and data types can be used. There are many Machine learning specific packages
    that we will be exploring in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a detailed understanding of working with R for Machine learning, refer to
    the Packt Publication for R titled *Machine learning with R*.
  prefs: []
  type: TYPE_NORMAL
- en: Julia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Julia, in recent times, has gained much popularity and adoption in the Machine
    learning and data science fields as a high-performance alternative to Python.
    Julia is a dynamic programming language that is built to support distributed and
    parallel computing, thus known to be convenient and fast.
  prefs: []
  type: TYPE_NORMAL
- en: Performance in Julia is a result of the JIT compiler and type interfacing feature.
    Also, unlike other numeric programming languages, Julia does not enforce vectorization
    of values. Similar to R, MATLAB, and Python, Julia provides ease and expressiveness
    for high-level numerical computing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are some key characteristics of Julia:'
  prefs: []
  type: TYPE_NORMAL
- en: The core APIs and mathematical primitive operations are written in Julia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It consists rich types for constructing and describing objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia supports for multiple dispatch that enable using functions across many
    combinations of arguments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It facilitates the automation of specialized code generation for different argument
    types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proven performance is on par with statically compiled languages like C
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a free and open source programming language (MIT licensed)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User-defined types are as fast and compact as built-ins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not enforce or require vectorization code for performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is designed for distributed and parallel computation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia comes with co-routines and lightweight threading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia supports the ability to invoke the C functions directly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shell-like capabilities for managing processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides Lisp-like macros
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and setting up Julia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using Julia's latest version that was available at the time of writing
    this book—v 0.3.4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Julia programs can be built and executed by:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Julia command line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Juno—an IDE for Julia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a ready-to-use environment at [https://juliabox.org/](https://juliabox.org/),
    where the Julia environment can be accessed using a browser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading and using the command line version of Julia
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use the link [http://julialang.org/downloads/](http://julialang.org/downloads/)
    to download the required Julia version.
  prefs: []
  type: TYPE_NORMAL
- en: Download the appropriate executable and run it.![Downloading and using the command
    line version of Julia](img/B03980_04_13.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the successful installation, open the Julia console and Julia is ready
    to use.![Downloading and using the command line version of Julia](img/B03980_04_14.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using Juno IDE for running Julia
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Juno IDE makes developing Julia code easy. Download the latest Juno IDE version
    from [http://junolab.org/docs/install.html](http://junolab.org/docs/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Juno has Julia''s core APIs and functions that help in simplifying the development
    process. Following is a screenshot of how Juno can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using Juno IDE for running Julia](img/B03980_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using Julia via the browser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using this option does not require any installation of Julia. Follow these
    steps to access the Julia environment online:'
  prefs: []
  type: TYPE_NORMAL
- en: Access [https://juliabox.org/](https://juliabox.org/) from the browser![Using
    Julia via the browser](img/B03980_04_16.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in using the Google account. This will create a unique instance of Julia
    for the logged-in user. This will give access to the Julia console and the IJulia
    instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With one of the three approaches that we have seen previously, we have access
    to the Julia console from where the Julia code can be executed. Each piece of
    the contained Julia code is built in a file with a `.jl` extension.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Julia code from the command line
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Julia compiles the code at runtime and translates each method into a machine
    code using **just-in-time** (**JIT**) compilers. Internally, it utilizes **Low-Level
    Virtual Machine** (**LLVM**) for optimization and code generation. LLVM is a full-fledged
    project that is a collection of standard compiler technologies. This is used as
    a part of iOS.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the shell of choice, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, open the Julia console from the Julia command line installation
    and run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Implementing in Julia (with examples)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will cover some basic topics under coding Julia and understanding
    the syntax. At the end of this section, readers should be able to easily write
    their Julia script and run the same. Regarding syntax, Julia programming language
    is very similar to MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: Using variables and assignments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Variables in Julia, like any other programming language, are used for storing
    and manipulating data. Following is an example of defining, assigning, and manipulating
    variables and values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Julia, being a mathematical programming language, provides several fundamental
    constants. Here is an example that can be directly used in the code. Additionally,
    we can define our constants and reassign values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Numeric primitives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For any mathematical programming language that supports numeric-based computing,
    Integers and floating-point values form the basic building blocks and are called
    numeric primitives.
  prefs: []
  type: TYPE_NORMAL
- en: Julia comes with a support for large set numeric primitives that are extensive
    and very well-complimented mathematical functions.
  prefs: []
  type: TYPE_NORMAL
- en: Data structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Julia supports several data structures in addition to all the primitive data
    types such as Vectors, Matrices, Tuples, Dictionaries, Sets and so on. Following
    are some example representations with the usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Working with Strings and String manipulations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are some examples of operating with Strings in Julia:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Packages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Julia comes with several packages that have inbuilt functions and support many
    out-of-box features for implementing Machine learning algorithms as well. Following
    is the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Images.jl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Graphs.jl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataFrames.jl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DimensionalityReduction.jl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Distributions.jl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NLOpt.jl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ArgParse.jl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Logging.jl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FactCheck.jl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`METADATA.jl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More details on Julia packages can be accessed at [https://github.com/JuliaLang/](https://github.com/JuliaLang/).
  prefs: []
  type: TYPE_NORMAL
- en: Interoperability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This following section covers the integration aspects of Julia with various
    other programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating with C
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Julia is flexible and without any wrappers, supports invoking C functions directly.
    Following is an example that demonstrates how this is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Integrating with Python
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to the C function calls, Julia supports invoking Python functions directly.
    It is important that we have the `PyCall` package installed to be able to do so.
    `PyCall.jl` offers automatic type conversion between Julia and Python. For example,
    Julia arrays are converted to NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is an example that demonstrates invoking Python functions from the
    Julia code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Integrating with MATLAB
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Following example demonstrates integrating Julia to invoke MATLAB functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Graphics and plotting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Julia has several packages that help produce graphs and plots. Some of them
    are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Gadfly.jl`: This is very similar to ggplot2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Winston.jl`: This is very similar to Matplotlib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Gaston.jl`: This interfaces with gnuplot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The example here demonstrates using `PyPlot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![Graphics and plotting](img/B03980_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Benefits of adopting Julia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some of the direct benefits that one can look forward to for adopting
    Julia in the Machine learning implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: Julia facilitates fast prototyping without compromising on performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It inherently supports the parallelization of code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides a simpler way of expressing algorithms with special Julia types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia can easily invoke or integrate with C, Python, MATLAB, and C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia is facilitated by an enthusiastic, friendly, and supportive community
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It works with Hadoop and leverages Hive-based querying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating Julia and Hadoop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Integrating any programming language with Hadoop typically means the data stored
    in Hadoop should be accessible, and the program should be able to execute a specific
    logic on the data. This can happen either by retrieving the data from Hadoop and
    bringing it closer to the program or by moving the program to the data and to
    execute in a MapReduce or parallel processing mode. Obviously, in the first case
    where the data is fetched from Hadoop and brought to the code for executing the
    logic, there needs to be sufficient RAM to be able to hold and process this data
    in the memory, and this could restrict the ability to run on really large volumes.
    In the second case, where the code is taken to the data that is distributed across
    the data nodes, the logic should be parallelizable, and the Map and Reduce logics
    should be built.
  prefs: []
  type: TYPE_NORMAL
- en: The Julia integration with the Hadoop platform is slightly in its initial stages,
    and the current approach that is detailed is the first approach described previously
    where the connection to Hadoop/HDFS is made from the Julia code using a standard
    ODBC connectivity. The data is fetched into the RAM for further processing. Now,
    this code can run directly on the DataNode and can update the HDFS data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use `ODBC.jl` that can be obtained from GitHub using the following
    link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/quinnj/ODBC.jl](https://github.com/quinnj/ODBC.jl)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a simple low-level ODBC interface for Julia. It can be installed through
    the Julia package manager using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: Following command creates a Julia package repository (only runs once for all
    packages)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Following command creates the `ODBC repo` folder and downloads the `ODBC` package
    and dependency (if needed)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Following command loads the ODBC module for use (needs to be run with each new
    Julia instance)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Following are some important functions that can be used to work with Hadoop/HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: To connect using an ODBC datasource, user and password use—`co = ODBC.connect("mydatasource",usr="johndoe",pwd="12345")`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To disconnect use `disconnect(connection::Connection=conn)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To connect using a connection string use `advancedconnect(conn_string::String)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To ask a query and fetch a subset of data on the datasource, this query string
    is a Hive query that will be run on HDFS—`query(connecti on Connection=conn, querystring;
    fi le=: DataFrame,delim=''\t'')`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example implementation is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use following command to load ODBC module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'To connect to Hadoop cluster via Hive use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'To write a Hive query and store it as a Julia string, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'To run a query, save results directly to file use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The Julia program can now access the data from this file to execute Machine
    learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python is one of the highly adopted programming or scripting languages in the
    field of Machine learning and data science. Python is always known for its ease
    of learning, implementation, and maintenance. Python is highly portable and can
    run on the Unix-based, Windows and Mac platforms. With the availability of libraries
    such as Pydoop and SciPy, its relevance in the world of big data analytics has
    tremendously increased.     Some of the key reasons for the popularity of Python in solving Machine learning
    problems are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Python is known to be well suited for data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a versatile scripting language that can be used for writing some basic
    quick and dirty scripts for testing some basic functions, or it can be used in
    real-time applications leveraging its full-featured toolkits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python comes with complete Machine learning packages (refer to [http://mloss.org/software/](http://mloss.org/software/))
    and can be used in a plug-and-play manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toolkit options in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we go deeper into what toolkit options we have in Python, let's first
    understand the toolkit options trade-offs that should be considered before choosing
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the questions that we should evaluate for the appropriate toolkit can
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: What are my performance priorities? Do I need offline or real-time processing
    implementations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How transparent are the toolkits? Can I customize the library myself?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the community status? How fast are bugs fixed and how is the community
    support and expert communication availability?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are three options in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Use Python external bindings. These are the interfaces to popular packages in
    markets such as Matlab, R, Octave, and so on. This option will work well, in case
    we already have some implementations existing in the previously mentioned frameworks
    that we are looking at seamlessly migrating into Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Python-based toolkits. There are some toolkits written in Python that come
    with a bunch algorithms. Some of the Python toolkits will be covered in the next
    section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write your logic/toolkit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of Python (using examples)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Python has two core toolkits, which are more of building blocks and almost
    all the specialized toolkits that are listed here use these core toolkits. These
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NumPy**: NumPy is about fast and efficient arrays built in Python'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SciPy**: This is a bunch of algorithms for standard operations built in NumPy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a bunch of C/C++ based implementations such as LIBLINEAR, LIBSVM,
    OpenCV, and others
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see some of the popular Python toolkits and also those that have
    been updated within a span of a year of writing this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NLTK**: This stands for natural language toolkit. This focuses on the **Natural
    language processing** (**NLP**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mlpy**: This is Machine learning algorithms toolkit that comes with support
    for some key Machine learning algorithms such as classifications, regression,
    and clustering among others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PyML**: This toolkit focuses on **Support Vector Machine** (**SVM**). We
    will cover more on this in the coming chapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PyBrain**: This toolkit focuses on Neural networks and related functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mdp-toolkit**: The focus of this toolkit is data processing and it supports
    scheduling and parallelizing the processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scikit-learn**: This is one of the most popular toolkits and is being highly
    adopted by data scientists in the recent past. It has support for supervised,
    and unsupervised learning, some special support for feature selection, and visualizations
    as well. There is a large team that is actively building this toolkit and is known
    for its excellent documentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pydoop**: This is the Python integration with the Hadoop platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pydoop** and **SciPy** are heavily deployed in big data analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore the scikit-learn toolkit, and demonstrate all
    our examples in the upcoming chapters using this toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: For a Python programmer, using scikit-learn can help bring Machine learning
    into a production system very easily.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Python and setting up scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Following are the core Python toolkit versions and dependencies for installing
    Python and scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: Python (>= 2.6 or >= 3.3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy (>= 1.6.1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SciPy (>= 0.9).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A working C++ compiler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be using the wheel packages (`.whl` files) for scikit-learn from PyPI,
    and install it using the pip utility.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install in your home directory, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'For using the git repo directly from the GitHub to install scikit-learn on
    the local disk, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Loading data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Scikit-learn comes with a few standard datasets, for instance, the `iris` and
    `digits` datasets that can be used for building and running Machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some steps to follow to load the standard datasets shipped with scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is an open-source framework for fast, big data or large-scale processing
    with the support for streaming, SQL, Machine learning, and graph processing. This
    framework is implemented in Scala and supports programming languages such as Java,
    Scala, and Python. The magnitude of performance is up to 10X to 20X is the traditional
    Hadoop stack. Spark is a general purpose framework and allows interactive programming
    along with the support for streaming. Spark can work with Hadoop supporting Hadoop
    formats like SequenceFiles or InputFormats in a standalone mode. It includes local
    file systems, Hive, HBase, Cassandra, and Amazon S3 among others.
  prefs: []
  type: TYPE_NORMAL
- en: We will use Spark 1.2.0 for all the examples throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts the core modules of Apache Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Apache Spark](img/B03980_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Some of the basic functions of Spark framework include task scheduling, interaction
    with storage systems, fault tolerance, and memory management. Spark follows a
    programming paradigm called **Resilient Distributed Dataset** (**RDD**). This
    is primarily related to managing distributed data storage and parallel computing.
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark SQL** is Spark''s package for querying and processing structured and
    unstructured data. The core functions of this package are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To facilitate loading the data from varied structured sources such as Hive,
    JSON, and others
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To provide integration between SQL and regular Python or Java or Scala code,
    and provide the capability to build custom functions that can execute on distributed
    data and in parallel
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To support the SQL-based querying from external tools through standard database
    connections (JDBC/ODBC) including **Tableau**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark Streaming** module is used for processing real-time, large-scale streams
    of data. This API is different from the Streaming I/O API of Hadoop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLib** module provides out-of-box Machine learning algorithm functions that
    are scalable and can run on a cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GraphX** module provides functions for graph manipulations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to use Spark in conjunction with the Scala
    programming language. Let's now have a quick overview of Scala and learn how to
    code in Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scala is a strongly typed programming language that requires **JVM** (**Java
    Virtual Machine**) to run. It is an independent platform and can leverage Java
    APIs. We will use interpretive prompt to run Scala with Spark. The command prompt
    here shows how Scala can be run with Spark using the interpretive prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scala](img/B03980_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's look at some Scala examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code can be pasted directly into the command prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Programming with Resilient Distributed Datasets (RDD)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RDDs are Spark's core abstraction for working with data. They are immutable
    distributed collections of elements. All functions in Spark only work on RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark automatically distributes the data contained in RDDs across the nodes
    within a cluster as partitions and supports parallel processing to be performed
    on them. RDDs can be created by importing from external datasets or distributing
    collections in the driver program. The following command demonstrates this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The `collect()` method will write the output to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the results is usually saved to the external storage system.
    The `count()` function gives the number of output lines. The following will print
    out the lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The `take()` function will fetch *n* records from the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: RDDs process in a lazy manner by Spark to bring in the efficiency while handling
    large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: To reuse RDD in multiple actions, you can ask Spark to persist it using `RDD.persist()`.
  prefs: []
  type: TYPE_NORMAL
- en: We can ask Spark to persist our data in some different places. After computing
    it the first time, Spark will store the RDD contents in the memory (partitioned
    across the machines in your cluster) and reuse them for future actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, following are the basic steps to process RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: Create input RDDs from external data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transforming them to define new RDDs using transformations, for example `filter()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Storing intermediate RDDs for reuse using `persist()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Invoking any required function (for example, `count()`) to start a parallel
    computation process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Following is an example of RDD using Pi Estimation with Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Spring XD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though this book does not include Spring XD framework to demonstrate the Machine
    learning algorithm, a small introduction is given here as this is found to be
    fast emerging for adoption in the Machine learning world.
  prefs: []
  type: TYPE_NORMAL
- en: XD stands for eXtreme Data. This open source framework is built by the Pivotal
    team (earlier the SpringSource) as the one-stop-shop for developing and deploying
    big data applications.
  prefs: []
  type: TYPE_NORMAL
- en: Spring XD is a distributed and extensible framework that unifies data ingestion,
    analytics functions in real-time, batch, and supports data export. Spring XD is
    built on Spring Integration and Spring Batch frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are some key features:'
  prefs: []
  type: TYPE_NORMAL
- en: Spring XD is a unified platform for batch and stream workloads. It is an open
    and extensible runtime.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalable and high-performance, it is a distributed data ingestion framework
    that can ingest data from a variety of sources that include HDFS, NOSQL, or Splunk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports for real-time analytics at ingestion time, for example, gathering
    metrics and counting values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has workflow management through batch jobs that include interactions with
    standard RDBMS and Hadoop systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a scalable and high-performance data export, for example, from HDFS to
    an RDBMS or NoSQL database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spring XD is known to implement Lambda Architecture that in theory is defined
    to support both batch and real-time processing. More information on evolutionary
    architectures such as Lambda Architecture is covered in [Chapter 14](ch14.html
    "Chapter 14. New generation data architectures for Machine learning"), *New generation
    data architectures for Machine learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spring XD architecture primarily has three architecture layers to help facilitate
    the previous features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speed Layer**: This is about accessing and processing data in real time.
    This process keeps the system more up-to-date.![Spring XD](img/B03980_04_19.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Batch Layer**: The Batch layer has access to the complete master dataset
    also called the data lake meaning *source of truth*.![Spring XD](img/B03980_04_20.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Serving Layer**: The Service layer is more of a query layer that is responsible
    for exposing the data post processing to an unsubscribed consumer. This layer
    makes batch data queryable and is usually known for high throughput driven responses.![Spring
    XD](img/B03980_04_23.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Spring XD Runtime architecture is shown here (source Pivotal):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spring XD](img/B03980_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the open source options for implementing Machine
    learning, and covered installation, implementation, and execution of libraries,
    tools, and frameworks such as Apache Mahout, Python, R, Julia, and Apache Spark's
    MLib. Importantly, we covered the integration of these frameworks with the big
    data platform—Apache Hadoop. This chapter is more of a foundation for the coming
    chapters where we will learn how to use these frameworks in implementing specific
    Machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
