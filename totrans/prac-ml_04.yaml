- en: Chapter 4. Machine Learning Tools, Libraries, and Frameworks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章：机器学习工具、库和框架
- en: In the previous chapter, we covered the Machine learning solution architecture
    and the implementation aspects of a technology platform—Hadoop. In this chapter,
    we will look at some of the highly adopted and upcoming Machine learning tools,
    libraries, and frameworks. This chapter is a primer for the following chapters
    as it covers how to implement a specific Machine learning algorithm using out-of-box
    functions of an identified Machine learning framework.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了机器学习解决方案架构和技术平台——Hadoop的实施方面。在本章中，我们将探讨一些高度采用和即将推出的机器学习工具、库和框架。本章是后续章节的入门，因为它涵盖了如何使用特定机器学习框架的现成函数实现特定的机器学习算法。
- en: We will first cover the landscape of open source and commercial Machine learning
    libraries or tools that are available in the market, and pick the top five open
    source options. For each of the identified options, starting from installation
    steps, learning the syntax, implementing a complex Machine learning algorithm,
    to plotting graphs, we will cover it all. This chapter is mandatory for the readers
    in the order of occurrence as it is a foundation for all the example implementations
    in the chapters that follow.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍市场上可用的开源和商业机器学习库或工具的概况，并挑选出前五个开源选项。对于每个已确定的选项，从安装步骤开始，学习语法，实现复杂的机器学习算法，到绘制图表，我们将全面介绍。对于读者来说，本章是按顺序出现的，因为它构成了后续章节中所有示例实现的基础。
- en: Each of the identified frameworks can operate as standalone libraries and can
    run on Hadoop as well. In addition to learning how to program and implement a
    Machine learning algorithm, we will also cover how each of the identified frameworks
    integrate and run on Hadoop; this is what differentiates these tutorials from
    the mainstream ones found on the web.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 每个已确定的框架都可以作为独立的库运行，也可以在Hadoop上运行。除了学习如何编程和实现机器学习算法外，我们还将介绍每个已确定的框架如何集成和运行在Hadoop上；这正是这些教程与网上主流教程的不同之处。
- en: 'The topics listed here are covered in depth in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细介绍了以下主题：
- en: A brief list of commercial and open source Machine learning libraries.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商业和开源机器学习库的简要列表。
- en: Top libraries or frameworks covered are R, Mahout, Julia, Python (Machine learning
    libraries, in particular), and Spark.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涵盖的顶级库或框架是R、Mahout、Julia、Python（特别是机器学习库）和Spark。
- en: Apache Mahout is a framework used for running Machine learning algorithms built
    over Hadoop and is a Java-based open source Machine learning option. This framework
    can also work standalone. It is known for running Machine learning algorithms
    to heavy volumes of data. This framework is a part of Hadoop ecosystem components
    and has its distribution.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Mahout是一个用于在Hadoop上运行机器学习算法的框架，是一个基于Java的开源机器学习选项。这个框架也可以独立运行。它以运行大量数据的机器学习算法而闻名。这个框架是Hadoop生态系统组件的一部分，并有自己的发行版。
- en: R is an open source Machine learning and a data mining tool that is adopted
    very widely in the Machine learning community. This framework library can either
    work standalone or can be run on Hadoop using the Hadoop runtime R extensions.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R是一种在机器学习社区中广泛采用的机器学习和数据挖掘工具。这个框架库既可以独立运行，也可以使用Hadoop运行时R扩展在Hadoop上运行。
- en: Julia is an open source high-performance programming language that supports
    running numeric and statistical computing functions in a distributed and parallel
    way.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia是一种开源的高性能编程语言，支持以分布式和并行的方式运行数值和统计计算函数。
- en: Python is an interpreted, high-level programming language that is designed to
    try out different things and it is something that does not fall into the traditional
    waterfall way of development. We will explore the basic Python libraries—**NumPy**
    and **SciPy** and use scikit-learn to execute our first Machine learning program.
    Also, we will explore how to write a Hadoop MapReduce program in Python.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python是一种解释型、高级编程语言，旨在尝试不同的事物，并且它不属于传统的瀑布式开发方式。我们将探索基本的Python库——**NumPy**和**SciPy**，并使用scikit-learn来执行我们的第一个机器学习程序。此外，我们还将探讨如何在Python中编写Hadoop
    MapReduce程序。
- en: 'Apache Spark and its Machine learning core libraries: Spark is a cluster computing
    system with API for Java, Python, and Scala. We will explore the **MLlib API**
    for Machine learning and use a version for Apache Hadoop. The focus will be to
    explore the Spark Java APIs.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark及其机器学习核心库：Spark是一个具有Java、Python和Scala API的集群计算系统。我们将探索**MLlib API**用于机器学习，并使用Apache
    Hadoop的一个版本。重点将放在探索Spark Java API上。
- en: A brief introduction to Spring XD and the related Machine learning libraries.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对Spring XD及其相关机器学习库的简要介绍。
- en: For each of the identified Machine learning frameworks, integration with Hadoop
    will be a primary focus.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个已识别的机器学习框架，与Hadoop的集成将是一个主要关注点。
- en: Machine learning tools – A landscape
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工具 – 一览图
- en: There are several open source and commercial Machine learning frameworks and
    tools in the market that have evolved over the last few decades. While the field
    of Machine learning itself is evolving in building powerful algorithms for diverse
    requirements across domains, we now see a surge of open source options for large-scale
    Machine learning that have reached a significant level of maturity and are being
    widely adopted by the data science and Machine learning communities.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上存在许多开源和商业机器学习框架和工具，在过去几十年中不断发展。虽然机器学习本身在构建满足不同领域多样化需求的强大算法方面也在不断发展，但我们现在看到开源的大规模机器学习选项激增，这些选项已经达到显著成熟水平，并且被数据科学和机器学习社区广泛采用。
- en: The model has changed significantly in the recent past, and researchers are
    encouraged to publish their software under an open source model. Since there are
    problems that authors face while publishing their work in using algorithmic implementations
    for Machine learning, any work that is reviewed and improvised through usage by
    the data science community is considered to be of more value.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在最近几年发生了显著变化，研究人员被鼓励在开源模式下发布他们的软件。由于作者在发布他们的工作时使用算法实现机器学习会遇到问题，因此任何经过数据科学社区审查和改进的工作都被认为更有价值。
- en: The following figure shows a concept model of some important commercial and
    open source Machine learning frameworks and tools in the market. The highlighted
    ones will be covered in depth in this chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了市场上一些重要的商业和开源机器学习框架和工具的概念模型。本章将深入探讨其中突出显示的部分。
- en: '![Machine learning tools – A landscape](img/B03980_04_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习工具 – 一览图](img/B03980_04_01.jpg)'
- en: Some of these libraries are around specific programming languages such as Java,
    Python, C++, Scala, and so on. Some of these libraries, like Julia, Spark, and
    Mahout already support distributed, and parallel processing and others such as
    R and Python can run as MapReduce functions on Hadoop.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些库针对特定的编程语言，如Java、Python、C++、Scala等。其中一些库，如Julia、Spark和Mahout已经支持分布式和并行处理，而其他如R和Python则可以在Hadoop上作为MapReduce函数运行。
- en: 'In the following sections, for each of the highlighted Machine learning libraries,
    the following will be covered:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下各节中，对于每个突出显示的机器学习库，以下内容将被涵盖：
- en: An overview of the library or tool with the details of out-of-box Machine learning
    functions supported
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对库或工具的概述，包括支持的即用型机器学习函数的详细信息
- en: Installation, setup, and configuration guide
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装、设置和配置指南
- en: Introduction to syntax and basic data processing functions, and then the Advanced
    Machine learning functions example implementations
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法介绍和基本数据处理函数，然后是高级机器学习函数示例实现
- en: Samples for visualizations and plotting (wherever applicable)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化和绘图示例（ wherever applicable）
- en: Integration and execution on the Hadoop platform
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Hadoop平台上的集成和执行
- en: Apache Mahout
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Mahout
- en: Apache Mahout is a Machine learning library that comes packaged with Apache
    Hadoop and forms an important part of the Hadoop ecosystem.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Mahout是一个与Apache Hadoop打包的机器学习库，构成了Hadoop生态系统的重要组成部分。
- en: Mahout came into existence in 2008 as a subproject of Apache Lucene (an open
    source search engine). Lucene is an API that has an implementation of search,
    text mining, and information-retrieval techniques. Most of these search and text
    analytics internally apply Machine learning techniques. The recommendation engines
    that were built for the search engines started off under a new subproject called
    Mahout. Mahout means the *rider of an elephant*, signifying the running of Machine
    learning algorithms over Hadoop. It is a scalable Machine learning implementation
    that can run in a standalone mode (does not tightly integrate with Hadoop) as
    well.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Mahout于2008年作为Apache Lucene（一个开源搜索引擎）的子项目诞生。Lucene是一个具有搜索、文本挖掘和信息检索技术实现的API。大多数这些搜索和文本分析在内部应用机器学习技术。为搜索引擎构建的推荐引擎最初是在一个新的子项目Mahout下开始的。Mahout意味着“大象的骑手”，象征着机器学习算法在Hadoop上的运行。它是一个可扩展的机器学习实现，可以以独立模式运行（不紧密集成到Hadoop中）。
- en: '![Apache Mahout](img/B03980_04_02.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Mahout](img/B03980_04_02.jpg)'
- en: Mahout is a set of some basic Machine learning Java libraries used for classification,
    clustering, pattern mining, and so on. Though Mahout today provides support for
    a subset of Machine learning algorithms, it still ranks among the most adopted
    frameworks as it inherently supports analytics on large datasets to the degree
    of hundreds of millions of rows, which can be unstructured in nature as well.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Mahout是一组基本的机器学习Java库，用于分类、聚类、模式挖掘等。尽管今天的Mahout提供了对机器学习算法子集的支持，但它仍然是最受欢迎的框架之一，因为它本质上支持对数亿行的大型数据集进行数据分析，这些数据集在本质上可能是非结构化的。
- en: How does Mahout work?
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作？
- en: Mahout implements Hadoop MapReduce, and the most important aspect is that it
    works on top of Hadoop and applies a distributed computing paradigm.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Mahout实现了Hadoop MapReduce，最重要的方面是它在Hadoop之上运行并应用分布式计算范式。
- en: '![How does Mahout work?](img/B03980_04_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B03980_04_03.jpg)'
- en: 'Following are some of the specific Machine learning tasks that Mahout currently
    implements:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Mahout目前实现的一些特定机器学习任务：
- en: '**Collaborative Filtering / Recommendation**: This takes a user input and finds
    items that the users might like'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤/推荐**：这接受用户输入并找到用户可能喜欢的项目'
- en: '**Clustering**: This takes a bunch of documents as input and groups them based
    on the topics they refer/belong to'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：这接受一些文档作为输入，并根据它们所涉及或属于的主题将它们分组'
- en: '**Classification**: This takes a bunch of documents and, based on the existing
    categorization of the documents, learns what category a given document might belong
    to, and maps the document to that category'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：这接受一些文档，并根据文档的现有分类，学习给定文档可能属于哪个类别，并将文档映射到该类别'
- en: '**Frequent itemset mining**: This takes a bunch of items as input and, based
    on the learning from the real occurrences, identifies which items occur or appear
    together'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频繁项集挖掘**：这接受一些项目作为输入，并根据从实际发生的学习中，确定哪些项目发生或一起出现'
- en: There are certain algorithms, for example, logistic regression and SVM (more
    about these algorithms will be covered in the chapters to follow), which cannot
    be parallelized and run in a standalone mode.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些算法，例如逻辑回归和SVM（关于这些算法的更多内容将在后续章节中介绍），不能并行化并在独立模式下运行。
- en: Installing and setting up Apache Mahout
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装和设置Apache Mahout
- en: In this chapter, we will look at how to run Mahout in a standalone mode and
    on Hadoop. Though there was the new 1.0 version of Apache Mahout available at
    the time of writing this book, we will use version 0.9 (the latest stable version)
    in all the examples. The operating system used is Ubuntu 12.04 desktop 32-bit
    version.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何在独立模式和Hadoop上运行Mahout。尽管在撰写本书时Apache Mahout有新的1.0版本可用，但我们将在所有示例中使用0.9版本（最新的稳定版本）。使用的操作系统是Ubuntu
    12.04桌面32位版本。
- en: 'Following are the dependencies and key requirements for installing Apache Mahout:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是安装Apache Mahout的依赖项和关键要求：
- en: JDK (1.6 or above; we will use 1.7 u9 version for the examples throughout this
    book)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDK (1.6或更高版本；本书中的示例我们将使用1.7 u9版本)
- en: Maven (2.2 or above; we will use 3.0.4 for the examples throughout this book)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maven (2.2或更高版本；本书中的示例我们将使用3.0.4版本)
- en: Apache Hadoop (2.0; not mandatory as Mahout can be run locally)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Hadoop (2.0；不是强制性的，因为Mahout可以在本地运行)
- en: Apache Mahout (0.9 distribution)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Mahout (0.9版本)
- en: Development environment—Eclipse IDE (Luna)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发环境——Eclipse IDE（Luna）
- en: In [Chapter 3](ch03.html "Chapter 3. An Introduction to Hadoop's Architecture
    and Ecosystem"), *An Introduction to Hadoop's Architecture and Ecosystem*, we
    have seen how Apache Hadoop 2.0 single node installation is done along with the
    required prerequisites like Java.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 3 章](ch03.html "第 3 章。Hadoop 架构和生态系统的介绍") 中，我们了解到如何进行 Apache Hadoop 2.0
    单节点安装，以及所需的先决条件，如 Java。
- en: In this chapter, we will cover the setting up of Maven, Eclipse for the development
    environment, and configuring Apache Mahout to run on and off Hadoop. As the considered
    platform and related frameworks are open sources, we will use the VirtualBox machine
    emulator hosted by the Windows 7 Professional edition.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍 Maven、Eclipse 开发环境的设置，以及配置 Apache Mahout 在 Hadoop 上和离线运行。由于所考虑的平台和相关框架是开源的，我们将使用
    Windows 7 专业版提供的 VirtualBox 虚拟机仿真器。
- en: As you may recollect, Hadoop cannot run as a root user, and hence we have a
    user created for this purpose—`practical-ml` to install and run everything.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所回忆的，Hadoop 不能以 root 用户身份运行，因此我们为此创建了用户 `practical-ml` 来安装和运行所有内容。
- en: Setting up Maven
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 Maven
- en: It is recommended that Maven is used to get the required Mahout jars, and it
    gets easy to switch to any newer versions easily with Mahout. In the absence of
    Maven, downloading the dependencies will get more complicated. For more details
    on specific features of Maven and its utility in application development, refer
    to [https://www.packtpub.com/application-development/apache-maven-3-cookbook](https://www.packtpub.com/application-development/apache-maven-3-cookbook).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 建议使用 Maven 来获取所需的 Mahout jar 包，这样就可以轻松地切换到任何更新的 Mahout 版本。如果没有 Maven，下载依赖项将会变得更加复杂。有关
    Maven 的具体功能和其在应用程序开发中的实用性的更多详细信息，请参阅[https://www.packtpub.com/application-development/apache-maven-3-cookbook](https://www.packtpub.com/application-development/apache-maven-3-cookbook)。
- en: 'Maven version 3.0.4 can be downloaded from one of the mirrors of the Apache
    website. The following command can be used for this purpose:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Maven 版本 3.0.4 可以从 Apache 网站的镜像之一下载。以下命令可用于此目的：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To manually install Maven, perform the following instructions:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要手动安装 Maven，请按照以下说明操作：
- en: Extract the distribution archive that is, `apache-maven-3.0.4-bin.tar.gz` to
    the directory you wish to install Maven 3.0.4.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分发存档文件，即 `apache-maven-3.0.4-bin.tar.gz`，提取到您希望安装 Maven 3.0.4 的目录中。
- en: With these instructions, the `/usr/local/apache-maven` path will be chosen.
    An `apache-maven-3.0.4` subdirectory will be created from the archive.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些说明，将选择 `/usr/local/apache-maven` 路径。从存档中创建一个 `apache-maven-3.0.4` 子目录。
- en: 'The following lines need to be appended to the `.bashrc` file:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下行需要追加到 `.bashrc` 文件中：
- en: '[PRE1]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`JAVA_HOME` should point to a location where the JDK is installed. For example,
    export `JAVA_HOME=/usr/java/jdk1.7\. $JAVA_HOME/bin` is in your `PATH` environment
    variable. The `PATH` variable is set during the Java installation. This should
    be verified.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`JAVA_HOME` 应指向 JDK 安装的位置。例如，导出 `JAVA_HOME=/usr/java/jdk1.7`，`JAVA_HOME/bin`
    应包含在您的 `PATH` 环境变量中。`PATH` 变量是在 Java 安装期间设置的。这应该得到验证。'
- en: 'We can now check for the successful installation of Maven by running the following
    command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过运行以下命令来检查 Maven 的安装是否成功：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In case there are any proxy settings, we will have to explicitly update the
    proxy settings in the `settings.xml` file, which is in the `conf` folder of Maven
    installation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有任何代理设置，我们必须明确更新 Maven 安装目录中 `conf` 文件夹下的 `settings.xml` 文件中的代理设置。
- en: Setting-up Apache Mahout using Eclipse IDE
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Eclipse IDE 设置 Apache Mahout
- en: The procedure detailed next covers the steps to set up the Mahout environment,
    code base, accessing of examples, running, debugging, and testing them using Eclipse
    IDE. This is the recommended way to set up and is the simplest way to set up Apache
    Mahout for the development teams.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步详细说明了设置 Mahout 环境、代码库、访问示例、运行、调试和测试的步骤，使用 Eclipse IDE 进行这些操作是推荐的，也是最简单的方法来为开发团队设置
    Apache Mahout。
- en: Execute the following steps to get the Apache Mahout tar, untar it and navigate
    to the installation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以获取 Apache Mahout 的 tar 文件，解压它并导航到安装目录。
- en: Set up Eclipse IDE.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 Eclipse IDE。
- en: 'The latest version of Eclipse can be downloaded from the following link:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以从以下链接下载最新版本的 Eclipse：
- en: '[https://www.eclipse.org/downloads/](https://www.eclipse.org/downloads/)'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://www.eclipse.org/downloads/](https://www.eclipse.org/downloads/)'
- en: 'Download Mahout Distribution from the direct link using the command here:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从直接链接下载 Mahout 分发版：
- en: '[PRE3]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Extract the archive from it using the following command:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从存档中提取：
- en: '[PRE4]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Convert the project into an Eclipse project:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将项目转换为 Eclipse 项目：
- en: '[PRE5]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The earlier command builds the Eclipse project.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前的命令构建了 Eclipse 项目。
- en: 'Set the `M2_REPO` classpath variable to point to the local repository path.
    The following command adds all the Maven jars to the Eclipse classpath:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `M2_REPO` 类路径变量设置为指向本地仓库路径。以下命令将所有 Maven jar 添加到 Eclipse 类路径：
- en: '[PRE6]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, let's import the Eclipse Mahout projects.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们导入 Eclipse Mahout 项目。
- en: Navigate from the menu, **File** | **Import** | **General** | **Existing Projects**
    into **Workspace**.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从菜单中导航，**文件** | **导入** | **通用** | **现有项目**到**工作空间**。
- en: '![Setting-up Apache Mahout using Eclipse IDE](img/B03980_04_04.jpg)'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用 Eclipse IDE 设置 Apache Mahout](img/B03980_04_04.jpg)'
- en: Setting up Apache Mahout without Eclipse
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 Apache Mahout 而不使用 Eclipse
- en: 'Download Mahout Distribution from the direct link using the command here:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从直接链接下载 Mahout 发行版：
- en: '[PRE7]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Extract the Mahout distribution to the `/usr/local` folder:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Mahout 发行版提取到 `/usr/local` 文件夹：
- en: '[PRE8]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Set the Java, Maven, and Mahout paths in the `.bashrc` file.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `.bashrc` 文件中设置 Java、Maven 和 Mahout 路径。
- en: 'Open the `.bashrc` file using the command here:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用以下命令打开 `.bashrc` 文件：
- en: '[PRE9]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Add the following content to the file:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将以下内容添加到文件中：
- en: '[PRE10]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To run Mahout in the local mode (this means in the standalone mode where there
    is no need for Hadoop, and the algorithms will not run in parallel or MapReduce
    mode).
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在本地模式（这意味着在独立模式中，不需要 Hadoop，算法将不会以并行或 MapReduce 模式运行）下运行 Mahout。
- en: 'Set the local mode to true using the following command:'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用以下命令将本地模式设置为 true：
- en: '[PRE11]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This will force Mahout to not look for the Hadoop configurations in `$HADOOP_CONF_DIR`.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将强制 Mahout 不在 `$HADOOP_CONF_DIR` 中查找 Hadoop 配置。
- en: '`MAHOUT_LOCAL` is set, so we don''t add `HADOOP_CONF_DIR` to the classpath.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`MAHOUT_LOCAL` 已设置，因此我们不需要将 `HADOOP_CONF_DIR` 添加到类路径中。'
- en: 'There is an alternative to run Mahout on Hadoop. Firstly, ensure Hadoop 2.x
    is installed and configured successfully. Then, follow these instructions:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种在 Hadoop 上运行 Mahout 的替代方法。首先，确保已成功安装和配置 Hadoop 2.x。然后，按照以下说明操作：
- en: Set `$HADOOP_HOME`, `$HADOOP_CONF_DIR` are set and added to `$PATH`.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 `$HADOOP_HOME`、`$HADOOP_CONF_DIR` 并将其添加到 `$PATH`。
- en: '[PRE12]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The above sets the mode in which Hadoop is run (for example, in `core-site.xml`,
    `hdfs-site.xml`, `mapred-site.xml`, and so on.)
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的设置设置了 Hadoop 运行的模式（例如，在 `core-site.xml`、`hdfs-site.xml`、`mapred-site.xml`
    等等中）。
- en: 'Now, launch the Hadoop instance using the command here:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令启动 Hadoop 实例：
- en: '[PRE13]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Check `http://localhost:50030`, and `http://localhost:50070` URLs to confirm
    whether Hadoop is up and running.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 `http://localhost:50030` 和 `http://localhost:50070` 网址以确认 Hadoop 是否正在运行。
- en: 'Build Apache Mahout using Maven by running the following Maven command from
    the Mahout directory:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Maven 从 Mahout 目录运行以下 Maven 命令来构建 Apache Mahout：
- en: '[PRE14]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following output is seen on a successful install:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功安装后，将看到以下输出：
- en: '![Setting up Apache Mahout without Eclipse](img/B03980_04_05.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![设置 Apache Mahout 而不使用 Eclipse](img/B03980_04_05.jpg)'
- en: Mahout Packages
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Mahout 包
- en: The following figure depicts different packages in Mahout that provide some
    out-of-box support for several Machine learning algorithms. At the core, the modules
    are the utilities, math vectors, collections, and Hadoop with MapReduce for the
    parallel processing and the file system for distributed storage.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了 Mahout 中提供对多个机器学习算法一些即用型支持的不同的包。在核心，模块包括实用工具、数学向量、集合、以及用于并行处理和分布式存储的
    Hadoop 和 MapReduce。
- en: 'Moreover, over the core modules are the Machine learning packages as listed
    here:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在核心模块之上还有以下列出的机器学习包：
- en: Classification
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Clustering
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Evolutionary Algorithms
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化算法
- en: Recommenders
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统
- en: Regression
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归
- en: FPM
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FPM
- en: Dimension Reduction![Mahout Packages](img/B03980_04_06.jpg)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度缩减![Mahout 包](img/B03980_04_06.jpg)
- en: More details are covered on the previous packages in detail in the chapters
    to follow, with example implementations using each of the packages for an identified
    problem.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，将详细介绍之前提到的包，并使用每个包针对一个已识别的问题提供示例实现。
- en: Implementing vectors in Mahout
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Mahout 中实现向量
- en: 'As we understand, to demonstrate most of the Machine learning algorithm implementations
    in Mahout, we need the data in classic Mahout dataset format. At the core, the
    code for this is primary to use some Mahout ready-to-use scripts with some minor
    changes in the settings. Given below is the standard process:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，为了演示 Mahout 中大多数机器学习算法的实现，我们需要经典 Mahout 数据集格式的数据。在核心，此代码主要使用一些 Mahout
    可用脚本，并在设置中进行一些小的更改。以下为标准流程：
- en: Create sequence files from the raw text files.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从原始文本文件创建序列文件。
- en: '**Sequence files** are predominantly a binary encoding of the key/value pair
    representation of data. The attributes given next are the key header elements
    that represent metadata details:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**序列文件**主要是数据的关键/值对表示的二进制编码。以下给出的属性是键头元素，代表元数据细节：'
- en: Version
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本
- en: Key name
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键名
- en: Value name
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值名
- en: Compression
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩
- en: Generate vectors from the sequence files. More on the actual commands to generate
    sequence files is covered in the following chapters while demonstrating the implementation
    for each of the identified Machine learning algorithms.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从序列文件生成向量。关于生成序列文件的实际命令的更多内容将在以下章节中介绍，同时展示每个已识别的机器学习算法的实现。
- en: Running functions on these working vectors
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这些工作向量上运行函数
- en: There are different types of vector implementations in Mahout, and the definitions
    hold good in general as well.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Mahout中有不同类型的向量实现，这些定义在一般情况下也是适用的。
- en: '![Implementing vectors in Mahout](img/B03980_04_07.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![在Mahout中实现向量](img/B03980_04_07.jpg)'
- en: '**Dense Vectors**: These vectors are usually an array of doubles, and the size
    of this vector is the same as the number of features in the dataset. Since all
    the entries are preallocated irrespective of a zero value, these vectors are called
    dense vectors.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密集向量**：这些向量通常是一个双精度浮点数数组，这个向量的大小与数据集中特征的数量相同。由于所有条目都是预先分配的，无论是否为零值，因此这些向量被称为密集向量。'
- en: '**Sparse Vectors**: These vectors are arrays of vectors and are represented
    only with non-zero or null values. With sparse vectors, there are two subcategories:
    the random-access and sequential-access sparse vectors.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏向量**：这些向量是向量的数组，并且只表示非零或空值。对于稀疏向量，有两种子类别：随机访问和顺序访问稀疏向量。'
- en: '**Random Access Sparse Vectors**: Random access sparse vectors are the HashMap
    representations where the key is an integer value, and the value is a double value.
    At any given point in time, a value can be accessed by passing in the given key.'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机访问稀疏向量**：随机访问稀疏向量是HashMap表示，其中键是一个整数值，值是一个双精度浮点数。在任何给定的时间点，可以通过传递给定的键来访问一个值。'
- en: '**Sequential Access Sparse Vectors**: These vectors are nothing but a set of
    two arrays where the first array is the array of keys (the integers), and the
    second array is an array of values (the doubles). These vectors are optimized
    for linear reads, unlike the random access sparse vectors. Again, the storage
    is done for only non-zero values.'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序访问稀疏向量**：这些向量实际上是一组两个数组，第一个数组是键（整数）数组，第二个数组是值（双精度浮点数）数组。这些向量针对线性读取进行了优化，与随机访问稀疏向量不同。同样，存储仅针对非零值。'
- en: Note
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For a detailed understanding of working with Apache Mahout, refer to the Packt
    Publication for Apache Mahout titled *Apache Mahout Cookbook*.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了详细了解如何使用Apache Mahout，请参考Packt出版社出版的关于Apache Mahout的书籍《Apache Mahout Cookbook》。
- en: While this section covers a framework that is built to work with Hadoop with
    small configuration changes, in the next section, we cover the powerful and highly
    adopted option in the market—R. Hadoop provides explicit adapters to have the
    R programs work in the MapReduce model, which is covered next.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本节涵盖了只需进行少量配置更改即可与Hadoop一起工作的框架，但在下一节中，我们将介绍市场上广泛采用的强大选项——R。Hadoop提供了明确的适配器，以便R程序能够在MapReduce模型中运行，这一点将在下一节中介绍。
- en: R
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R
- en: R is a language for data analysis and is used as an environment that is a primary
    driver in the field of Machine learning, statistical computing, and data mining
    and provides a comprehensive platform for basic and advanced visualizations or
    graphics. Today, R is a basic skill that almost all data scientists or would-be
    data scientists have or *must* learn.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: R是一种数据分析语言，它被用作机器学习、统计计算和数据挖掘领域的主要驱动环境，并为基本和高级可视化或图形提供了一个全面的平台。如今，R是几乎所有数据科学家或即将成为数据科学家的人必须学习的基本技能。
- en: R is primarily a GNU project known to be similar to the S language that was
    initially developed at Bell Laboratories (formerly known as AT&T and now, Lucent
    Technologies) by John Chambers and team. The initial goal for S was to support
    all statistical functions and was widely used by hard-core statisticians.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: R主要是一个GNU项目，类似于最初在贝尔实验室（以前称为AT&T，现在是朗讯科技）由约翰·查默斯及其团队开发的S语言。S的初始目标是支持所有统计函数，并被硬核统计学家广泛使用。
- en: R comes with a wide range of open source packages that can be downloaded and
    configured free of cost, and are installed or loaded on a need basis into the
    R environment. These packages provide out-of-box support for a wide variety of
    statistical techniques that include linear and non-linear modeling, time-series
    analysis, classification, clustering, and so on.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: R 随带一系列开源包，可以免费下载和配置，并根据需要安装或加载到 R 环境中。这些包为各种统计技术提供即插即用的支持，包括线性和非线性建模、时间序列分析、分类、聚类等。
- en: Along with these, the highly extensible graphical functions are available. The
    support for these advanced graphical functions has been a primary differentiator
    for R as the output is known for its publication quality plots. In addition to
    these, R also supports many open source graphical libraries and visualization
    tools that are both open source and commercial in nature.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还提供了高度可扩展的图形功能。这些高级图形功能的支持是R的主要差异化因素，因为其输出以其出版物质量的图表而闻名。除此之外，R还支持许多开源图形库和可视化工具，这些工具既开源又商业。
- en: Though, at the core, R is not meant to work in a distributed environment or
    run the algorithms in a parallel mode, there are several extensions available
    (both open source and commercial) that make R more scalable and support large
    dataset. In this chapter, we will cover how R can be integrated with Apache Hadoop,
    and thus can run and leverage the MapReduce capabilities.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管R在核心上并不旨在在分布式环境中工作或以并行模式运行算法，但有一些扩展（包括开源和商业）可以使R更具可扩展性并支持大数据集。在本章中，我们将介绍如何将R与Apache
    Hadoop集成，从而可以运行并利用MapReduce功能。
- en: Most importantly, R is free software that is widely adopted and has many committers
    and support groups constantly working on retaining its high relevance in the field
    of data science.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，R 是一种广泛采用的免费软件，拥有许多提交者和支持小组不断努力保持其在数据科学领域的极高相关性。
- en: 'Some of the key capabilities that R supports today are listed here:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: R 支持的一些关键功能如下列出：
- en: The ability to effectively manage and store data that the models operate on
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够有效管理和存储模型操作的数据的能力
- en: Facilitating some core suite of functions for calculations on arrays, vectors,
    and matrices among others
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方便进行数组、向量和其他核心函数的计算
- en: Several out-of-box Machine learning functions that can be loaded on demand and
    help implement data science projects with ease
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几种即插即用的机器学习函数，可以根据需要加载，并有助于轻松实现数据科学项目。
- en: Advanced and sophisticated graphical functions that can be used with ease and
    help to produce valuable dashboards for business owners
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以轻松使用并帮助生成对商业主有价值的仪表板的先进和复杂的图形功能
- en: A wide and active community of adopters and committers that has developed rapidly
    with extensions via a large collection of packages
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个广泛且活跃的采用者和提交者社区，通过大量包的扩展迅速发展
- en: R is considered as a platform that supports newly developing methods of interactive
    data analysis
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 被视为一个支持新兴的交互式数据分析方法的平台
- en: Installing and setting up R
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装和设置 R
- en: For all the examples in this book, we will use the stable version 2.15.1 of
    R and the CRAN references for all the latest R packages.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的所有示例中，我们将使用 R 的稳定版本 2.15.1 和 CRAN 引用，以获取所有最新的 R 包。
- en: Refer to the [https://cran.r-project.org/bin/windows/base/old/2.15.1/](https://cran.r-project.org/bin/windows/base/old/2.15.1/)
    link to download R for Windows.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[https://cran.r-project.org/bin/windows/base/old/2.15.1/](https://cran.r-project.org/bin/windows/base/old/2.15.1/)链接下载
    R for Windows。
- en: A detailed installation process is covered at [https://cran.r-project.org/doc/manuals/R-admin.html#Top](https://cran.r-project.org/doc/manuals/R-admin.html#Top).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 详细安装过程请参阅[https://cran.r-project.org/doc/manuals/R-admin.html#Top](https://cran.r-project.org/doc/manuals/R-admin.html#Top)。
- en: We can use R with the R GUI or the IDE RStudio. Following are the screenshots
    of the R interface that the users can see post a successful installation of the
    R GUI and R IDE, and the RStudio.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 R GUI 或 RStudio 来使用 R。以下是用户在成功安装 R GUI、R IDE 和 RStudio 后可以看到的 R 界面截图。
- en: '![Installing and setting up R](img/B03980_04_08.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![安装和设置 R](img/B03980_04_08.jpg)'
- en: We will need to set the CRAN mirror path to be able to access and load the required
    R packages by navigating from the menu path **Packages** | **Set CRAN mirror**
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置 CRAN 镜像路径，以便通过导航到菜单路径 **包** | **设置 CRAN 镜像** 来访问和加载所需的 R 包。
- en: '![Installing and setting up R](img/B03980_04_09.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![安装和设置 R](img/B03980_04_09.jpg)'
- en: 'The following screenshot shows a list of mirror sites from which the developer
    can choose the most appropriate one:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了开发者可以选择的最合适的镜像站点列表：
- en: '![Installing and setting up R](img/B03980_04_10.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![安装和设置 R](img/B03980_04_10.jpg)'
- en: 'The R Editor can be used to write any advanced operations, and the results
    can be seen on the console as shown here:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: R 编辑器可以用来编写任何高级操作，结果可以在控制台看到，如下所示：
- en: '![Installing and setting up R](img/B03980_04_11.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![安装和设置 R](img/B03980_04_11.jpg)'
- en: 'Following is a screenshot of a graphical plot:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个图形图表的截图：
- en: '![Installing and setting up R](img/B03980_04_12.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![安装和设置 R](img/B03980_04_12.jpg)'
- en: Integrating R with Apache Hadoop
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 R 与 Apache Hadoop 集成
- en: So far, we have seen Apache Hadoop and its core components, HDFS and YARN (MapReduce
    2.0), and R. There are three different ways in which we can look at integrating
    R with Hadoop, and hence the support for large-scale Machine learning.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了 Apache Hadoop 及其核心组件，HDFS 和 YARN（MapReduce 2.0），以及 R。我们可以以三种不同的方式来看待将
    R 与 Hadoop 集成，因此支持大规模机器学习。
- en: Approach 1 – Using R and Streaming APIs in Hadoop
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方法 1 – 在 Hadoop 中使用 R 和 Streaming API
- en: 'To integrate an R function with Hadoop and see it running in a MapReduce mode,
    Hadoop supports Streaming APIs for R. These Streaming APIs primarily help in running
    any script that can access and operate with standard I/O in a MapReduce mode.
    So in the case of R, there wouldn''t be any explicit client side integration done
    with R. The following is an example of R and streaming:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 R 函数与 Hadoop 集成并看到它在 MapReduce 模式下运行，Hadoop 支持 R 的 Streaming API。这些 Streaming
    API 主要帮助在 MapReduce 模式下运行任何可以访问和操作标准 I/O 的脚本。因此，在 R 的情况下，不会与 R 进行任何显式的客户端集成。以下是一个
    R 和流处理的示例：
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Approach 2 – Using the Rhipe package of R
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方法 2 – 使用 R 的 Rhipe 包
- en: 'There is a package in R called Rhipe that allows running a MapReduce job within
    R. To use this way of implementing R on Hadoop; there are some prerequisites:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: R 中有一个名为 Rhipe 的包，允许在 R 中运行 MapReduce 作业。要使用这种方式在 Hadoop 上实现 R，有一些先决条件：
- en: R needs to be installed on each DataNode in the Hadoop Cluster
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要在 Hadoop 集群中的每个 DataNode 上安装 R
- en: Protocol Buffers will be installed and available on each DataNode (for more
    information on Protocol Buffers refer to [http://wiki.apache.org/hadoop/ProtocolBuffers](http://wiki.apache.org/hadoop/ProtocolBuffers))
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Protocol Buffers 将在每个 DataNode 上安装和可用（有关 Protocol Buffers 的更多信息，请参阅 [http://wiki.apache.org/hadoop/ProtocolBuffers](http://wiki.apache.org/hadoop/ProtocolBuffers)）
- en: Rhipe should be available on each data node
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rhipe 应该在每个数据节点上可用
- en: 'The following is a sample format for using the `Rhipe` library in R to implement
    MapReduce:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在 R 中使用 `Rhipe` 库实现 MapReduce 的示例格式：
- en: '[PRE16]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Approach 3 – Using RHadoop
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方法 3 – 使用 RHadoop
- en: 'RHadoop, very similar to Rhipe, facilitates running R functions in a MapReduce
    mode. It is an open source library built by Revolution Analytics. Following are
    some packages, which are a part of the RHadoop library:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Rhipe 非常相似的 RHadoop，便于在 MapReduce 模式下运行 R 函数。它是由 Revolution Analytics 开发的开源库。以下是一些
    RHadoop 库的组成部分：
- en: '**plyrmr**: This is a package that provides functions for common data manipulation
    requirements for large datasets running on Hadoop'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**plyrmr**：这是一个提供用于在 Hadoop 上运行大型数据集的常见数据操作函数的包'
- en: '**rmr**: This is a package that has a collection of functions that integrate
    R and Hadoop'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**rmr**：这是一个包含将 R 和 Hadoop 集成的函数集合的包'
- en: '**rdfs**: This is a package with functions that help interface R and HDFS'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**rdfs**：这是一个提供帮助 R 和 HDFS 交互的函数的包'
- en: '**rhbase**: This is a package with functions that help interface R and HBase'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**rhbase**：这是一个包含帮助 R 和 HBase 交互的函数的包'
- en: 'The following is an example that uses the rmr package and demonstrates steps
    to integrate R and Hadoop using the functions from this package:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个使用 rmr 包的示例，展示了使用该包中的函数集成 R 和 Hadoop 的步骤：
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Summary of R/Hadoop integration approaches
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: R/Hadoop 集成方法总结
- en: 'In summary, all of the previous three approaches yield results and facilitate
    integrating R and Hadoop. They help in scaling R to operate on the large-scale
    data that will help with HDFS. Each of these approaches has pros and cons. Here
    is a summary of conclusions:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，所有前面三种方法都能产生结果并促进 R 和 Hadoop 的集成。它们帮助将 R 规模化以在 HDFS 上操作，这将有助于处理大规模数据。每种方法都有其优缺点。以下是一些结论的总结：
- en: Hadoop Streaming API is the simplest of all the approaches as there are no complications
    regarding installation and setup requirements
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop Streaming API 是所有方法中最简单的一个，因为没有关于安装和设置要求的复杂性
- en: Both Rhipe and RHadoop require some effort to setup R and related packages on
    the Hadoop cluster
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rhipe 和 RHadoop 都需要在 Hadoop 集群上设置 R 和相关包，这需要一些努力
- en: Regarding implementation approach, Streaming API is more of a command line map,
    and reduce functions are inputs to the function, whereas both Rhipe and RHadoop
    allow developers to define and call custom MapReduce functions within R
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于实现方法，Streaming API 更像是一个命令行映射，reduce 函数是函数的输入，而 Rhipe 和 RHadoop 允许开发者在 R 中定义和调用自定义的
    MapReduce 函数
- en: In case of Hadoop Streaming API, there is no client side integration required,
    whereas both Rhipe and RHadoop require the client side integration
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Hadoop Streaming API 的情况下，不需要客户端集成，而 Rhipe 和 RHadoop 都需要客户端集成
- en: The alternatives to scaling Machine learning are Apache Mahout, Apache Hive,
    and some commercial versions of R from Revolution Analytics, Segue framework,
    and others
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的替代方案包括 Apache Mahout、Apache Hive 以及来自 Revolution Analytics、Segue 框架和其他一些商业版本的
    R
- en: Implementing in R (using examples)
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 R 中实现（使用示例）
- en: In this section, we will briefly cover some implementation aspects of R, and
    focus on learning the syntax and understanding some core functions and its usage.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍 R 的某些实现方面，并专注于学习语法以及理解一些核心函数及其用法。
- en: R Expressions
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: R 表达式
- en: 'R can be used as a simple math calculator; here are some basic ways of using
    it. Here is a trace of what is seen on the R console:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: R 可以用作简单的数学计算器；以下是使用它的基本方法。以下是 R 控制台上的输出跟踪：
- en: '[PRE18]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Assignments
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分配
- en: 'This is used to assign value to a variable and apply some operations to this
    variable:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这用于将值分配给变量并对该变量执行一些操作：
- en: 'Case 1: Assigning a numeric value:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 情况 1：分配一个数值：
- en: '[PRE19]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Case 2: Assigning a string literal:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 情况 2：分配一个字符串字面量：
- en: '[PRE20]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Case 3: Assigning a logical value:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 情况 3：分配一个逻辑值：
- en: '[PRE21]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Functions
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 函数
- en: 'There are many out-of-box functions and to invoke a function in R, we should
    provide the function name and pass required arguments. Here are some examples
    of functions and the results, as seen in the R Console:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多现成的函数，要在 R 中调用一个函数，我们应该提供函数名并传递所需的参数。以下是一些函数及其结果的示例，如 R 控制台所示：
- en: '[PRE22]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is the command to get help for a function in R:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是获取 R 中函数帮助的命令：
- en: '[PRE23]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: R Vectors
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: R 向量
- en: A vector is a simple list of values by definition that forms the core of R data
    types. Many of the Machine learning functions leverage these.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，向量是一个简单的值列表，它是 R 数据类型的核心。许多机器学习函数都利用了这些。
- en: 'Here are some key functions with their usage context:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关键函数及其使用上下文：
- en: '| Function/Syntax | Purpose | Example | Output on R Console |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 函数/语法 | 目的 | 示例 | R 控制台上的输出 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `m:n` | Outputs numbers from `m` to `n` increment by 1 | `> 5:9` | `[1] 5
    6 7 8 9` |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| `m:n` | 输出从 `m` 到 `n` 的数字，每次增加 1 | `> 5:9` | `[1] 5 6 7 8 9` |'
- en: '| `seq(m,n)` | Outputs numbers from `m` to `n` increment by 1 | `> seq(5,9)`
    | `[1] 5 6 7 8 9` |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| `seq(m,n)` | 输出从 `m` 到 `n` 的数字，每次增加 1 | `> seq(5,9)` | `[1] 5 6 7 8 9` |'
- en: '| `seq(m,n, i)` | Outputs numbers from `m` to `n` increment by `i` | `> seq(1,3,0.5)`
    | `[1] 1 1.5 2 2.5 3` |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| `seq(m,n, i)` | 输出从 `m` 到 `n` 的数字，每次增加 `i` | `> seq(1,3,0.5)` | `[1] 1 1.5
    2 2.5 3` |'
- en: Assigning, accessing, and manipulating vectors
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分配、访问和操作向量
- en: 'The following table has examples for creating, accessing, and manipulating
    matrices in R:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 下表提供了在 R 中创建、访问和操作矩阵的示例：
- en: '| Purpose | Example |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 目的 | 示例 |'
- en: '| --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Creating a vector of literals | `> sentence <- c(''practical'', ''machine'',
    ''learning'')` |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 创建一个字面量向量 | `> sentence <- c(''practical'', ''machine'', ''learning'')` |'
- en: '| Accessing the third value of the vectors | `> sentence[3]``[1] "learning."`
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 访问向量的第三个值 | `> sentence[3]``[1] "learning."` |'
- en: '| Updating a value in the vector | `> sentence[1] <- "implementing"` |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 更新向量中的值 | `> sentence[1] <- "implementing"` |'
- en: '| Adding a new value to the vector | `> sentence[4] <- "algorithms"` |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 向向量中添加新值 | `> sentence[4] <- "algorithms"` |'
- en: '| Getting values for the given indices | `> sentence[c(1,3)]``[1] "implementing"
    "learning"` |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 获取给定索引的值 | `> sentence[c(1,3)]``[1] "implementing" "learning"` |'
- en: '| Getting values for range of indices | `> sentence[2:4]``[1] "machine" "learning"
    "algorithms"` |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 获取指定索引范围的值 | `> sentence[2:4]``[1] "machine" "learning" "algorithms"` |'
- en: '| Adding a range of new values | `> sentence[5:7] <- c(''for'',''large'',''datasets'')`
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 添加一系列新值 | `> sentence[5:7] <- c(''for'',''large'',''datasets'')` |'
- en: '| Incrementing vector values by 1 | `> a <- c(1, 2, 3)``> a + 1``[1] 2 3 4`
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 向量值加 1 | `> a <- c(1, 2, 3)```> a + 1``[1] 2 3 4` |'
- en: '| Dividing each value in vector by a value | `> a / 2``[1] 0.5 1.0 1.5` |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 将向量中的每个值除以一个值 | `> a / 2``[1] 0.5 1.0 1.5` |'
- en: '| Multiplying each value of the vector by a value | `> a*2``[1] 2 4 6` |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 将向量的每个值乘以一个值 | `> a*2``[1] 2 4 6` |'
- en: '| Adding two vectors | `> b <- c(4, 5, 6)``> a + b``[1] 5 7 9` |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 添加两个向量 | `> b <- c(4, 5, 6)``> a + b``[1] 5 7 9` |'
- en: '| Comparing two vectors | `> a == c(1, 99, 3)``[1] TRUE FALSE TRUE` |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 比较两个向量 | `> a == c(1, 99, 3)``[1] TRUE FALSE TRUE` |'
- en: '| Applying a function on each value of the vector | `> sqrt(a)``[1] 1.000000
    1.414214 1.732051` |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 对向量的每个值应用一个函数 | `> sqrt(a)``[1] 1.000000 1.414214 1.732051` |'
- en: R Matrices
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: R 矩阵
- en: 'Matrices are two-dimensional vectors that have rows and columns. The following
    table has examples for creating, accessing, and manipulating matrices in R:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵是具有行和列的二维向量。以下表格展示了在 R 中创建、访问和操作矩阵的示例：
- en: '| Purpose | Example |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 目的 | 示例 |'
- en: '| --- | --- |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Creating a 3 X 4 matrix with values defaulted to zero | `> matrix(0, 3, 4)``[,1]
    [,2] [,3] [,4]``[1,] 0 0 0 0``[2,] 0 0 0 0``[3,] 0 0 0 0` |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 创建一个默认值为零的 3 X 4 矩阵 | `> matrix(0, 3, 4)``[,1] [,2] [,3] [,4]``[1,] 0 0 0
    0``[2,] 0 0 0 0``[3,] 0 0 0 0` |'
- en: '| Initializing a matrix with a range of values | `> a <- 1:12``> m <- matrix(a,
    3, 4)``[,1] [,2] [,3] [,4]``[1,] 1 4 7 10``[2,] 2 5 8 11``[3,] 3 6 9 12` |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 使用一系列值初始化矩阵 | `> a <- 1:12``> m <- matrix(a, 3, 4)``[,1] [,2] [,3] [,4]``[1,]
    1 4 7 10``[2,] 2 5 8 11``[3,] 3 6 9 12` |'
- en: '| Accessing a value from the matrix | `> m[2, 3]``[1] 8` |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 从矩阵中访问一个值 | `> m[2, 3]``[1] 8` |'
- en: '| Assigning a value to a position of choice in a matrix | `> m[1, 4] <- 0`
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 在矩阵中选择的位置赋值 | `> m[1, 4] <- 0` |'
- en: '| Retrieving an array of the entire row or a column of choice | `> m[2,]``[1]
    2 5 8 11``> m[3,]``[1] 7 8 9` |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 获取整个行或所选列的数组 | `> m[2,]``[1] 2 5 8 11``> m[3,]``[1] 7 8 9` |'
- en: '| Retrieving a subset of the bigger matrix | `> m[, 2:4]``[,1] [,2] [,3]``[1,]
    4 7 10``[2,] 5 8 11` |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 从较大的矩阵中检索子集 | `> m[, 2:4]``[,1] [,2] [,3]``[1,] 4 7 10``[2,] 5 8 11` |'
- en: R Factors
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: R 因子
- en: In data analytics and Machine learning, it is common to group or categorize
    data. For example, a good or a bad customer. R's `factor` data type is used to
    track the categorized data. All that needs to be done is defining a vector of
    categories and passing it as a parameter to the `factor` function.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析与机器学习中，分组或分类数据是很常见的。例如，好客户或坏客户。R 的 `factor` 数据类型用于跟踪分类数据。需要做的只是定义一个类别向量，并将其作为参数传递给
    `factor` 函数。
- en: 'The following example demonstrates creation and assignment of categories using
    `factors`:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用 `factors` 创建和赋值类别的操作：
- en: '[PRE24]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Each of the defined categories usually has an integer value associated with
    the literal. Passing the `factor` to the `as.integer` function will give the integer
    equivalents, as shown here:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 每个定义的类别通常与一个整数值相关联。将 `factor` 传递给 `as.integer` 函数将给出整数等效值，如下所示：
- en: '[PRE25]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: R Data Frames
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: R 数据框
- en: 'Data frames relate to the concept of database tables. This data type is very
    powerful in R, and it helps tie different related attributes of a dataset together.
    For example, the number of items purchased has a relationship with the total bill
    value and the overall applicable discount. There should be a way to link these
    attributes, and data frames help to do so:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框与数据库表的概念相关。在 R 中，这种数据类型非常强大，它有助于将数据集的不同相关属性联系起来。例如，购买的商品数量与总账单价值以及整体适用折扣有关。应该有一种方法将这些属性联系起来，数据框有助于做到这一点：
- en: '| Purpose | Example |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 目的 | 示例 |'
- en: '| --- | --- |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Creating a data frame and checking the values | `> purchase <- data.frame(totalbill,
    noitems, discount``> print(purchase)``totalbill noitems discount``1 300 5 10``2
    200 3 7.5``3 100 1 5``)` |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 创建数据框并检查值 | `> purchase <- data.frame(totalbill, noitems, discount``> print(purchase)``totalbill
    noitems discount``1 300 5 10``2 200 3 7.5``3 100 1 5``)` |'
- en: '| Accessing the data of the data frame using indexes or labels | `> purchase[[2]]``[1]
    5 3 1``> purchase[["totalbill"]]``[1] 300 200 100``> purchase$discount``[1] 10
    7.5 5` |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 使用索引或标签访问数据框的数据 | `> purchase[[2]]``[1] 5 3 1``> purchase[["totalbill"]]``[1]
    300 200 100``> purchase$discount``[1] 10 7.5 5` |'
- en: '| Loading data frames with the data from CSV files | `> list.files()``[1] "monthlypurchases.csv"``>
    read.csv("monthlypurchases.csv")``Amount Items Discount``1 2500 35 15``2 5464
    42 25``3 1245 8 6` |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 使用从 CSV 文件加载数据的数据框 | `> list.files()``[1] "monthlypurchases.csv"``> read.csv("monthlypurchases.csv")``Amount
    Items Discount``1 2500 35 15``2 5464 42 25``3 1245 8 6` |'
- en: R Statistical frameworks
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: R 统计框架
- en: 'R supports a bunch of statistical out-of-box functions that help statisticians
    explain the data. Some of the functions with examples are shown in the following
    table:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: R 支持大量统计内置函数，帮助统计学家解释数据。以下表格展示了其中一些带有示例的函数：
- en: '| Function | Example |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 示例 |'
- en: '| --- | --- |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Mean | `limbs <- c(4, 3, 4, 3, 2, 4, 4, 4)``names(limbs) <- c(''One-Eye'',
    ''Peg-Leg'', ''Smitty'', ''Hook'', ''Scooter'', ''Dan'', ''Mikey'', ''Blackbeard'')``>
    mean(limbs)``[1] 3.5` |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | `limbs <- c(4, 3, 4, 3, 2, 4, 4, 4)` `names(limbs) <- c(''One-Eye'',
    ''Peg-Leg'', ''Smitty'', ''Hook'', ''Scooter'', ''Dan'', ''Mikey'', ''Blackbeard'')`
    `> mean(limbs)` `[1] 3.5` |'
- en: '| Median | `> median(limbs)``[1] 4` |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 中位数 | `> median(limbs)` `[1] 4` |'
- en: '| Standard deviation | `> pounds <- c(45000, 50000, 35000, 40000, 35000, 45000,
    10000, 15000)``> deviation <- sd(pounds)` |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | `pounds <- c(45000, 50000, 35000, 40000, 35000, 45000, 10000, 15000)`
    `> deviation <- sd(pounds)` |'
- en: Each piece of the contained R code is saved for a run in a file with the `.R`
    extension.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 每段包含的R代码都保存在一个以`.R`扩展名命名的文件中，以便于运行。
- en: In this section, we have seen how R can be set up and how some basic functions
    and data types can be used. There are many Machine learning specific packages
    that we will be exploring in the following chapters.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了如何设置R以及如何使用一些基本函数和数据类型。在接下来的章节中，我们将探索许多特定于机器学习的包。
- en: Note
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For a detailed understanding of working with R for Machine learning, refer to
    the Packt Publication for R titled *Machine learning with R*.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用R进行机器学习的详细理解，请参阅Packt出版的《Machine learning with R》。
- en: Julia
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Julia
- en: Julia, in recent times, has gained much popularity and adoption in the Machine
    learning and data science fields as a high-performance alternative to Python.
    Julia is a dynamic programming language that is built to support distributed and
    parallel computing, thus known to be convenient and fast.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，Julia在机器学习和数据科学领域获得了广泛的流行和采用，作为Python的高性能替代品。Julia是一种动态编程语言，旨在支持分布式和并行计算，因此被认为是方便且快速的。
- en: Performance in Julia is a result of the JIT compiler and type interfacing feature.
    Also, unlike other numeric programming languages, Julia does not enforce vectorization
    of values. Similar to R, MATLAB, and Python, Julia provides ease and expressiveness
    for high-level numerical computing.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Julia的性能是JIT编译器和类型接口功能的结果。此外，与其它数值编程语言不同，Julia不强制执行值的向量化。类似于R、MATLAB和Python，Julia为高级数值计算提供了便利和表达性。
- en: 'Following are some key characteristics of Julia:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Julia的一些关键特性：
- en: The core APIs and mathematical primitive operations are written in Julia
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心API和数学原始操作是用Julia编写的
- en: It consists rich types for constructing and describing objects
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含丰富的类型，用于构建和描述对象
- en: Julia supports for multiple dispatch that enable using functions across many
    combinations of arguments
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia支持多派发，允许在许多参数组合中使用函数
- en: It facilitates the automation of specialized code generation for different argument
    types
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它简化了针对不同参数类型的专用代码生成的自动化
- en: Proven performance is on par with statically compiled languages like C
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经证实的性能与静态编译语言如C相当
- en: It is a free and open source programming language (MIT licensed)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一种免费且开源的编程语言（MIT授权）
- en: User-defined types are as fast and compact as built-ins
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户自定义类型与内置类型一样快速且紧凑
- en: It does not enforce or require vectorization code for performance
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不强制或要求为性能编写向量化代码
- en: It is designed for distributed and parallel computation
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是为分布式和并行计算设计的
- en: Julia comes with co-routines and lightweight threading
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia自带协程和轻量级线程
- en: Julia supports the ability to invoke the C functions directly
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia支持直接调用C函数的能力
- en: Shell-like capabilities for managing processes
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于shell的能力来管理进程
- en: It provides Lisp-like macros
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了Lisp样式的宏
- en: Installing and setting up Julia
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装和设置Julia
- en: We will be using Julia's latest version that was available at the time of writing
    this book—v 0.3.4.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用本书编写时可用的最新版本的Julia——v 0.3.4。
- en: 'Julia programs can be built and executed by:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Julia程序可以通过以下方式构建和执行：
- en: Using Julia command line
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Julia命令行
- en: Using Juno—an IDE for Julia
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Juno——Julia的IDE
- en: Using a ready-to-use environment at [https://juliabox.org/](https://juliabox.org/),
    where the Julia environment can be accessed using a browser
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[https://juliabox.org/](https://juliabox.org/)使用现成的环境，可以通过浏览器访问Julia环境
- en: Downloading and using the command line version of Julia
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载和使用Julia的命令行版本
- en: Use the link [http://julialang.org/downloads/](http://julialang.org/downloads/)
    to download the required Julia version.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[http://julialang.org/downloads/](http://julialang.org/downloads/)链接下载所需的Julia版本。
- en: Download the appropriate executable and run it.![Downloading and using the command
    line version of Julia](img/B03980_04_13.jpg)
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载适当的可执行文件并运行它。![下载和使用Julia的命令行版本](img/B03980_04_13.jpg)
- en: After the successful installation, open the Julia console and Julia is ready
    to use.![Downloading and using the command line version of Julia](img/B03980_04_14.jpg)
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装成功后，打开 Julia 控制台，Julia 即可使用。![下载和使用 Julia 的命令行版本](img/B03980_04_14.jpg)
- en: Using Juno IDE for running Julia
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Juno IDE 运行 Julia
- en: Juno IDE makes developing Julia code easy. Download the latest Juno IDE version
    from [http://junolab.org/docs/install.html](http://junolab.org/docs/install.html).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Juno IDE 使开发 Julia 代码变得简单。从 [http://junolab.org/docs/install.html](http://junolab.org/docs/install.html)
    下载最新的 Juno IDE 版本。
- en: 'Juno has Julia''s core APIs and functions that help in simplifying the development
    process. Following is a screenshot of how Juno can be used:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: Juno 拥有 Julia 的核心 API 和函数，有助于简化开发过程。以下是如何使用 Juno 的截图：
- en: '![Using Juno IDE for running Julia](img/B03980_04_15.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Juno IDE 运行 Julia](img/B03980_04_15.jpg)'
- en: Using Julia via the browser
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过浏览器使用 Julia
- en: 'Using this option does not require any installation of Julia. Follow these
    steps to access the Julia environment online:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此选项不需要安装 Julia。按照以下步骤在线访问 Julia 环境：
- en: Access [https://juliabox.org/](https://juliabox.org/) from the browser![Using
    Julia via the browser](img/B03980_04_16.jpg)
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过浏览器访问 [https://juliabox.org/](https://juliabox.org/)![通过浏览器使用 Julia](img/B03980_04_16.jpg)
- en: Log in using the Google account. This will create a unique instance of Julia
    for the logged-in user. This will give access to the Julia console and the IJulia
    instances.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Google 账户登录。这将为登录用户创建一个独特的 Julia 实例。这将提供访问 Julia 控制台和 IJulia 实例的权限。
- en: With one of the three approaches that we have seen previously, we have access
    to the Julia console from where the Julia code can be executed. Each piece of
    the contained Julia code is built in a file with a `.jl` extension.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们之前看到的三种方法之一，我们可以从 Julia 控制台执行 Julia 代码。包含的每段 Julia 代码都构建在一个以 `.jl` 扩展名结尾的文件中。
- en: Running the Julia code from the command line
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从命令行运行 Julia 代码
- en: Julia compiles the code at runtime and translates each method into a machine
    code using **just-in-time** (**JIT**) compilers. Internally, it utilizes **Low-Level
    Virtual Machine** (**LLVM**) for optimization and code generation. LLVM is a full-fledged
    project that is a collection of standard compiler technologies. This is used as
    a part of iOS.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 在运行时编译代码，并使用**即时编译器**（**JIT**）将每个方法转换为机器代码。内部，它使用**低级虚拟机**（**LLVM**）进行优化和代码生成。LLVM
    是一个完整的项目，它是一系列标准编译技术的集合。这被用作 iOS 的一部分。
- en: 'From the shell of choice, run the following:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 从所选的壳中运行以下命令：
- en: '[PRE26]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Alternatively, open the Julia console from the Julia command line installation
    and run the following command:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，从 Julia 命令行安装中打开 Julia 控制台并运行以下命令：
- en: '[PRE27]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Implementing in Julia (with examples)
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Julia 中实现（示例）
- en: In this section, we will cover some basic topics under coding Julia and understanding
    the syntax. At the end of this section, readers should be able to easily write
    their Julia script and run the same. Regarding syntax, Julia programming language
    is very similar to MATLAB.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些关于 Julia 编程和语法理解的基本主题。在本节结束时，读者应该能够轻松地编写他们的 Julia 脚本并运行它。关于语法，Julia
    编程语言与 MATLAB 非常相似。
- en: Using variables and assignments
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用变量和赋值
- en: 'Variables in Julia, like any other programming language, are used for storing
    and manipulating data. Following is an example of defining, assigning, and manipulating
    variables and values:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Julia 中，变量像任何其他编程语言一样，用于存储和操作数据。以下是一个定义、赋值和操作变量及值的示例：
- en: '[PRE28]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Julia, being a mathematical programming language, provides several fundamental
    constants. Here is an example that can be directly used in the code. Additionally,
    we can define our constants and reassign values:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 作为一种数学编程语言，提供了几个基本常数。以下是一个可以直接在代码中使用的示例。此外，我们可以定义自己的常数并重新赋值：
- en: '[PRE29]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Numeric primitives
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数值原语
- en: For any mathematical programming language that supports numeric-based computing,
    Integers and floating-point values form the basic building blocks and are called
    numeric primitives.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何支持基于数值计算的数学编程语言，整数和浮点值构成了基本构建块，被称为数值原语。
- en: Julia comes with a support for large set numeric primitives that are extensive
    and very well-complimented mathematical functions.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 随带支持大量数值原语，这些原语广泛且与数学函数非常互补。
- en: Data structures
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据结构
- en: 'Julia supports several data structures in addition to all the primitive data
    types such as Vectors, Matrices, Tuples, Dictionaries, Sets and so on. Following
    are some example representations with the usage:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 支持多种数据结构，除了所有原始数据类型（如 Vectors、Matrices、Tuples、Dictionaries、Sets 等）之外。以下是一些示例表示及其用法：
- en: '[PRE30]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Working with Strings and String manipulations
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与字符串和字符串操作一起工作
- en: 'Here are some examples of operating with Strings in Julia:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些在 Julia 中操作字符串的示例：
- en: '[PRE31]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Packages
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 包
- en: 'Julia comes with several packages that have inbuilt functions and support many
    out-of-box features for implementing Machine learning algorithms as well. Following
    is the list:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 随带了一些内置函数和许多开箱即用的功能，支持实现机器学习算法。以下是列表：
- en: '`Images.jl`'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Images.jl`'
- en: '`Graphs.jl`'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Graphs.jl`'
- en: '`DataFrames.jl`'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataFrames.jl`'
- en: '`DimensionalityReduction.jl`'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DimensionalityReduction.jl`'
- en: '`Distributions.jl`'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Distributions.jl`'
- en: '`NLOpt.jl`'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NLOpt.jl`'
- en: '`ArgParse.jl`'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ArgParse.jl`'
- en: '`Logging.jl`'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Logging.jl`'
- en: '`FactCheck.jl`'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FactCheck.jl`'
- en: '`METADATA.jl`'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`METADATA.jl`'
- en: More details on Julia packages can be accessed at [https://github.com/JuliaLang/](https://github.com/JuliaLang/).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于 Julia 包的详细信息可以在 [https://github.com/JuliaLang/](https://github.com/JuliaLang/)
    查找。
- en: Interoperability
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 互操作性
- en: This following section covers the integration aspects of Julia with various
    other programming languages.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分涵盖了 Julia 与各种其他编程语言的集成方面。
- en: Integrating with C
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与 C 集成
- en: 'Julia is flexible and without any wrappers, supports invoking C functions directly.
    Following is an example that demonstrates how this is done:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 灵活，无需任何包装，可以直接调用 C 函数。以下是一个示例，演示了如何做到这一点：
- en: '[PRE32]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Integrating with Python
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与 Python 集成
- en: Similar to the C function calls, Julia supports invoking Python functions directly.
    It is important that we have the `PyCall` package installed to be able to do so.
    `PyCall.jl` offers automatic type conversion between Julia and Python. For example,
    Julia arrays are converted to NumPy arrays.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 与 C 函数调用类似，Julia 支持直接调用 Python 函数。重要的是我们必须安装 `PyCall` 包才能这样做。`PyCall.jl` 提供了
    Julia 和 Python 之间的自动类型转换。例如，Julia 数组被转换为 NumPy 数组。
- en: 'Following is an example that demonstrates invoking Python functions from the
    Julia code:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，演示了从 Julia 代码中调用 Python 函数：
- en: '[PRE33]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Integrating with MATLAB
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与 MATLAB 集成
- en: 'Following example demonstrates integrating Julia to invoke MATLAB functions:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了将 Julia 与 MATLAB 函数集成：
- en: '[PRE34]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Graphics and plotting
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图形和绘图
- en: 'Julia has several packages that help produce graphs and plots. Some of them
    are listed here:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 有几个包可以帮助生成图表和图形。其中一些列在这里：
- en: '`Gadfly.jl`: This is very similar to ggplot2'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Gadfly.jl`: 这与 ggplot2 非常相似'
- en: '`Winston.jl`: This is very similar to Matplotlib'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Winston.jl`: 这与 Matplotlib 非常相似'
- en: '`Gaston.jl`: This interfaces with gnuplot'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Gaston.jl`: 这与 gnuplot 接口'
- en: 'The example here demonstrates using `PyPlot`:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例演示了使用 `PyPlot`：
- en: '[PRE35]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Graphics and plotting](img/B03980_04_17.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![图形和绘图](img/B03980_04_17.jpg)'
- en: Benefits of adopting Julia
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采用 Julia 的好处
- en: 'Here are some of the direct benefits that one can look forward to for adopting
    Julia in the Machine learning implementations:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是采用 Julia 在机器学习实现中可以期待的一些直接好处：
- en: Julia facilitates fast prototyping without compromising on performance
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia 促进了快速原型设计，同时不牺牲性能
- en: It inherently supports the parallelization of code
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它固有地支持代码的并行化
- en: It provides a simpler way of expressing algorithms with special Julia types
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了一种更简单的方式来使用特殊的 Julia 类型表达算法
- en: Julia can easily invoke or integrate with C, Python, MATLAB, and C++
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia 可以轻松调用或与 C、Python、MATLAB 和 C++ 集成
- en: Julia is facilitated by an enthusiastic, friendly, and supportive community
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia 由一个热情、友好和支持性的社区所促进
- en: It works with Hadoop and leverages Hive-based querying
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与 Hadoop 一起工作，并利用基于 Hive 的查询
- en: Integrating Julia and Hadoop
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Julia 与 Hadoop 集成
- en: Integrating any programming language with Hadoop typically means the data stored
    in Hadoop should be accessible, and the program should be able to execute a specific
    logic on the data. This can happen either by retrieving the data from Hadoop and
    bringing it closer to the program or by moving the program to the data and to
    execute in a MapReduce or parallel processing mode. Obviously, in the first case
    where the data is fetched from Hadoop and brought to the code for executing the
    logic, there needs to be sufficient RAM to be able to hold and process this data
    in the memory, and this could restrict the ability to run on really large volumes.
    In the second case, where the code is taken to the data that is distributed across
    the data nodes, the logic should be parallelizable, and the Map and Reduce logics
    should be built.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 将任何编程语言与 Hadoop 集成通常意味着存储在 Hadoop 中的数据应该是可访问的，并且程序应该能够对数据进行特定的逻辑处理。这可以通过从 Hadoop
    中检索数据并将其移至程序附近，或者将程序移动到数据所在位置并执行 MapReduce 或并行处理模式来实现。显然，在第一种情况下，即从 Hadoop 中检索数据并将其带到代码中以执行逻辑时，需要足够的
    RAM 来在内存中保存和处理这些数据，这可能会限制在真正大型数据量上运行的能力。在第二种情况下，即将代码带到分布在不同数据节点上的数据时，逻辑应该是可并行化的，并且应该构建
    Map 和 Reduce 逻辑。
- en: The Julia integration with the Hadoop platform is slightly in its initial stages,
    and the current approach that is detailed is the first approach described previously
    where the connection to Hadoop/HDFS is made from the Julia code using a standard
    ODBC connectivity. The data is fetched into the RAM for further processing. Now,
    this code can run directly on the DataNode and can update the HDFS data.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 与 Hadoop 平台的集成目前还处于初期阶段，详细说明的当前方法是之前描述的第一个方法，即使用标准 ODBC 连接从 Julia 代码连接到
    Hadoop/HDFS。数据被检索到 RAM 中以进行进一步处理。现在，此代码可以直接在 DataNode 上运行并更新 HDFS 数据。
- en: 'We will use `ODBC.jl` that can be obtained from GitHub using the following
    link:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用可以从 GitHub 获取的 `ODBC.jl`——`[ODBC.jl](https://github.com/quinnj/ODBC.jl)`。
- en: '[https://github.com/quinnj/ODBC.jl](https://github.com/quinnj/ODBC.jl)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[ODBC.jl](https://github.com/quinnj/ODBC.jl)'
- en: 'This is a simple low-level ODBC interface for Julia. It can be installed through
    the Julia package manager using the following commands:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个为 Julia 设计的简单低级 ODBC 接口。您可以使用以下命令通过 Julia 软件包管理器进行安装：
- en: Following command creates a Julia package repository (only runs once for all
    packages)
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令创建 Julia 软件包仓库（对于所有软件包只运行一次）
- en: '[PRE36]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Following command creates the `ODBC repo` folder and downloads the `ODBC` package
    and dependency (if needed)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令创建 `ODBC repo` 文件夹并下载 `ODBC` 软件包及其依赖项（如果需要）
- en: '[PRE37]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Following command loads the ODBC module for use (needs to be run with each new
    Julia instance)
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令用于加载 ODBC 模块以供使用（需要与每个新的 Julia 实例一起运行）
- en: '[PRE38]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Following are some important functions that can be used to work with Hadoop/HDFS:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可以用于与 Hadoop/HDFS 一起使用的重要函数：
- en: To connect using an ODBC datasource, user and password use—`co = ODBC.connect("mydatasource",usr="johndoe",pwd="12345")`.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用 ODBC 数据源、用户名和密码进行连接，请使用——`co = ODBC.connect("mydatasource",usr="johndoe",pwd="12345")`。
- en: To disconnect use `disconnect(connection::Connection=conn)`.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要断开连接，请使用 `disconnect(connection::Connection=conn)`。
- en: To connect using a connection string use `advancedconnect(conn_string::String)`.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用连接字符串进行连接，请使用 `advancedconnect(conn_string::String)`。
- en: 'To ask a query and fetch a subset of data on the datasource, this query string
    is a Hive query that will be run on HDFS—`query(connecti on Connection=conn, querystring;
    fi le=: DataFrame,delim=''\t'')`.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '要对数据源进行查询并检索数据子集，此查询字符串是一个将在 HDFS 上运行的 Hive 查询——`query(connecti on Connection=conn,
    querystring; fi le=: DataFrame,delim=''\t'')`。'
- en: 'An example implementation is given here:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例实现：
- en: 'Use following command to load ODBC module:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令来加载 ODBC 模块：
- en: '[PRE39]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To connect to Hadoop cluster via Hive use this:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过 Hive 连接到 Hadoop 集群，请使用以下命令：
- en: '[PRE40]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To write a Hive query and store it as a Julia string, use the following command:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 要编写 Hive 查询并将其存储为 Julia 字符串，请使用以下命令：
- en: '[PRE41]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To run a query, save results directly to file use the following command:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行查询并将结果直接保存到文件，请使用以下命令：
- en: '[PRE42]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The Julia program can now access the data from this file to execute Machine
    learning algorithms.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Julia 程序可以访问此文件中的数据以执行机器学习算法。
- en: Python
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python
- en: 'Python is one of the highly adopted programming or scripting languages in the
    field of Machine learning and data science. Python is always known for its ease
    of learning, implementation, and maintenance. Python is highly portable and can
    run on the Unix-based, Windows and Mac platforms. With the availability of libraries
    such as Pydoop and SciPy, its relevance in the world of big data analytics has
    tremendously increased.     Some of the key reasons for the popularity of Python in solving Machine learning
    problems are listed here:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: Python是机器学习和数据科学领域中高度采用的编程或脚本语言之一。Python总是以其易于学习、实现和维护而闻名。Python高度可移植，可以在基于Unix的、Windows和Mac平台上运行。随着Pydoop和SciPy等库的可用性，其在大数据分析领域的重要性大大增加。以下是一些Python在解决机器学习问题中流行的关键原因：
- en: Python is known to be well suited for data analysis
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python被认为非常适合数据分析。
- en: It is a versatile scripting language that can be used for writing some basic
    quick and dirty scripts for testing some basic functions, or it can be used in
    real-time applications leveraging its full-featured toolkits
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一种多用途的脚本语言，可以用来编写一些基本的快速脚本，用于测试一些基本功能，或者可以用于实时应用，利用其功能齐全的工具包。
- en: Python comes with complete Machine learning packages (refer to [http://mloss.org/software/](http://mloss.org/software/))
    and can be used in a plug-and-play manner
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python附带完整的机器学习包（请参阅[http://mloss.org/software/](http://mloss.org/software/)），并且可以以即插即用的方式使用。
- en: Toolkit options in Python
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python中的工具包选项
- en: Before we go deeper into what toolkit options we have in Python, let's first
    understand the toolkit options trade-offs that should be considered before choosing
    one.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨Python中可用的工具包选项之前，让我们首先了解在选择工具包之前应考虑的工具包选项权衡。
- en: 'Some of the questions that we should evaluate for the appropriate toolkit can
    be as follows:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该评估的一些关于适当工具包的问题可能如下：
- en: What are my performance priorities? Do I need offline or real-time processing
    implementations?
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的性能优先级是什么？我需要离线或实时处理实现吗？
- en: How transparent are the toolkits? Can I customize the library myself?
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具包的透明度如何？我能自己定制库吗？
- en: What is the community status? How fast are bugs fixed and how is the community
    support and expert communication availability?
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社区状况如何？错误修复的速度如何？社区支持和专家沟通的可用性如何？
- en: 'There are three options in Python:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: Python中有三种选项：
- en: Use Python external bindings. These are the interfaces to popular packages in
    markets such as Matlab, R, Octave, and so on. This option will work well, in case
    we already have some implementations existing in the previously mentioned frameworks
    that we are looking at seamlessly migrating into Python.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python外部绑定。这些是Matlab、R、Octave等市场中的流行包的接口。如果我们已经有一些在之前提到的框架中存在的实现，并且希望无缝迁移到Python，这个选项将工作得很好。
- en: Use Python-based toolkits. There are some toolkits written in Python that come
    with a bunch algorithms. Some of the Python toolkits will be covered in the next
    section.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于Python的工具包。有一些用Python编写的工具包附带了一系列算法。下一节将介绍一些Python工具包。
- en: Write your logic/toolkit.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写你的逻辑/工具包。
- en: Implementation of Python (using examples)
  id: totrans-411
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python的实现（使用示例）
- en: 'Python has two core toolkits, which are more of building blocks and almost
    all the specialized toolkits that are listed here use these core toolkits. These
    are as follows:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: Python有两个核心工具包，它们更像是构建块，并且这里列出的几乎所有专用工具包都使用这些核心工具包。以下是这些工具包：
- en: '**NumPy**: NumPy is about fast and efficient arrays built in Python'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NumPy**：NumPy是关于在Python中构建快速高效的数组。'
- en: '**SciPy**: This is a bunch of algorithms for standard operations built in NumPy'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SciPy**：这是一系列在NumPy中构建的标准操作算法。'
- en: There are a bunch of C/C++ based implementations such as LIBLINEAR, LIBSVM,
    OpenCV, and others
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多基于C/C++的实现，例如LIBLINEAR、LIBSVM、OpenCV等。
- en: 'Let''s now see some of the popular Python toolkits and also those that have
    been updated within a span of a year of writing this book:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看一些流行的Python工具包，以及那些在本书编写一年内更新的工具包：
- en: '**NLTK**: This stands for natural language toolkit. This focuses on the **Natural
    language processing** (**NLP**).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NLTK**：这代表自然语言工具包。它专注于**自然语言处理**（**NLP**）。'
- en: '**mlpy**: This is Machine learning algorithms toolkit that comes with support
    for some key Machine learning algorithms such as classifications, regression,
    and clustering among others.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mlpy**：这是一个包含对一些关键机器学习算法（如分类、回归和聚类等）支持的机器学习算法工具包。'
- en: '**PyML**: This toolkit focuses on **Support Vector Machine** (**SVM**). We
    will cover more on this in the coming chapters.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PyML**：这个工具包专注于**支持向量机**（**SVM**）。我们将在接下来的章节中详细介绍。'
- en: '**PyBrain**: This toolkit focuses on Neural networks and related functions.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PyBrain**：这个工具包专注于神经网络和相关功能。'
- en: '**mdp-toolkit**: The focus of this toolkit is data processing and it supports
    scheduling and parallelizing the processing.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mdp-toolkit**：这个工具包的焦点是数据处理，它支持调度和并行化处理。'
- en: '**scikit-learn**: This is one of the most popular toolkits and is being highly
    adopted by data scientists in the recent past. It has support for supervised,
    and unsupervised learning, some special support for feature selection, and visualizations
    as well. There is a large team that is actively building this toolkit and is known
    for its excellent documentation.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scikit-learn**：这是最受欢迎的工具包之一，在最近几年被数据科学家广泛采用。它支持监督学习和无监督学习，一些特殊支持用于特征选择和可视化。有一个大型团队正在积极构建这个工具包，以其出色的文档而闻名。'
- en: '**Pydoop**: This is the Python integration with the Hadoop platform.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pydoop**：这是与Hadoop平台集成的Python。'
- en: '**Pydoop** and **SciPy** are heavily deployed in big data analytics.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pydoop**和**SciPy**在大数据分析中被广泛部署。'
- en: In this chapter, we will explore the scikit-learn toolkit, and demonstrate all
    our examples in the upcoming chapters using this toolkit.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨scikit-learn工具包，并在接下来的章节中使用这个工具包演示所有示例。
- en: For a Python programmer, using scikit-learn can help bring Machine learning
    into a production system very easily.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Python程序员来说，使用scikit-learn可以帮助非常容易地将机器学习引入生产系统。
- en: Installing Python and setting up scikit-learn
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装Python和设置scikit-learn
- en: 'Following are the core Python toolkit versions and dependencies for installing
    Python and scikit-learn:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是安装Python和scikit-learn的核心Python工具包版本和依赖项：
- en: Python (>= 2.6 or >= 3.3)
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python（>= 2.6或>= 3.3）
- en: NumPy (>= 1.6.1)
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy（>= 1.6.1）
- en: SciPy (>= 0.9).
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SciPy（>= 0.9）。
- en: A working C++ compiler
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个有效的C++编译器
- en: We will be using the wheel packages (`.whl` files) for scikit-learn from PyPI,
    and install it using the pip utility.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自PyPI的scikit-learn的wheel包（`.whl`文件），并使用pip实用程序进行安装。
- en: 'To install in your home directory, use the following:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的家目录中安装，请使用以下命令：
- en: '[PRE43]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'For using the git repo directly from the GitHub to install scikit-learn on
    the local disk, use the following command:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 要直接从GitHub的git仓库安装scikit-learn到本地磁盘，请使用以下命令：
- en: '[PRE44]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Loading data
  id: totrans-438
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加载数据
- en: Scikit-learn comes with a few standard datasets, for instance, the `iris` and
    `digits` datasets that can be used for building and running Machine learning algorithms.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn附带了一些标准数据集，例如`iris`和`digits`数据集，可用于构建和运行机器学习算法。
- en: 'Here are some steps to follow to load the standard datasets shipped with scikit-learn:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载scikit-learn附带的标准数据集的步骤：
- en: '[PRE45]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Apache Spark
  id: totrans-442
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark
- en: Apache Spark is an open-source framework for fast, big data or large-scale processing
    with the support for streaming, SQL, Machine learning, and graph processing. This
    framework is implemented in Scala and supports programming languages such as Java,
    Scala, and Python. The magnitude of performance is up to 10X to 20X is the traditional
    Hadoop stack. Spark is a general purpose framework and allows interactive programming
    along with the support for streaming. Spark can work with Hadoop supporting Hadoop
    formats like SequenceFiles or InputFormats in a standalone mode. It includes local
    file systems, Hive, HBase, Cassandra, and Amazon S3 among others.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个开源框架，用于快速处理大数据或大规模数据，支持流处理、SQL、机器学习和图处理。这个框架是用Scala实现的，并支持Java、Scala和Python等编程语言。性能比传统的Hadoop堆栈高10倍到20倍。Spark是一个通用框架，允许交互式编程，并支持流处理。Spark可以以独立模式与支持Hadoop格式的Hadoop一起工作，如SequenceFiles或InputFormats。它包括本地文件系统、Hive、HBase、Cassandra和Amazon
    S3等。
- en: We will use Spark 1.2.0 for all the examples throughout this book.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在整本书中使用Spark 1.2.0。
- en: 'The following figure depicts the core modules of Apache Spark:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了Apache Spark的核心模块：
- en: '![Apache Spark](img/B03980_04_24.jpg)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark](img/B03980_04_24.jpg)'
- en: Some of the basic functions of Spark framework include task scheduling, interaction
    with storage systems, fault tolerance, and memory management. Spark follows a
    programming paradigm called **Resilient Distributed Dataset** (**RDD**). This
    is primarily related to managing distributed data storage and parallel computing.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: Spark框架的一些基本功能包括任务调度、与存储系统的交互、容错性和内存管理。Spark遵循名为**弹性分布式数据集**（**RDD**）的编程范式。这主要与分布式数据存储和并行计算管理相关。
- en: '**Spark SQL** is Spark''s package for querying and processing structured and
    unstructured data. The core functions of this package are:'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**是Spark用于查询和处理结构化和非结构化数据的包。此包的核心功能包括：'
- en: To facilitate loading the data from varied structured sources such as Hive,
    JSON, and others
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了便于从各种结构化数据源（如Hive、JSON等）加载数据
- en: To provide integration between SQL and regular Python or Java or Scala code,
    and provide the capability to build custom functions that can execute on distributed
    data and in parallel
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了在SQL和常规Python或Java或Scala代码之间提供集成，并提供构建可以在分布式数据上并行执行的自定义函数的能力
- en: To support the SQL-based querying from external tools through standard database
    connections (JDBC/ODBC) including **Tableau**
  id: totrans-451
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持通过标准数据库连接（JDBC/ODBC）从外部工具进行基于SQL的查询，包括**Tableau**
- en: '**Spark Streaming** module is used for processing real-time, large-scale streams
    of data. This API is different from the Streaming I/O API of Hadoop.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming**模块用于处理实时、大规模的数据流。此API与Hadoop的Streaming I/O API不同。'
- en: '**MLib** module provides out-of-box Machine learning algorithm functions that
    are scalable and can run on a cluster.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLib**模块提供开箱即用的可扩展机器学习算法函数，可以在集群上运行。'
- en: '**GraphX** module provides functions for graph manipulations.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX**模块提供图形操作的功能。'
- en: In this chapter, we will learn how to use Spark in conjunction with the Scala
    programming language. Let's now have a quick overview of Scala and learn how to
    code in Scala.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何结合Scala编程语言使用Spark。现在让我们快速概述Scala并学习如何在Scala中编码。
- en: Scala
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scala
- en: Scala is a strongly typed programming language that requires **JVM** (**Java
    Virtual Machine**) to run. It is an independent platform and can leverage Java
    APIs. We will use interpretive prompt to run Scala with Spark. The command prompt
    here shows how Scala can be run with Spark using the interpretive prompt.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: Scala是一种强类型编程语言，需要**JVM**（**Java虚拟机**）来运行。它是一个独立平台，可以利用Java API。我们将使用解释器提示符来运行带有Spark的Scala。这里的命令提示符显示了如何使用解释器提示符运行Spark中的Scala。
- en: '![Scala](img/B03980_04_18.jpg)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![Scala](img/B03980_04_18.jpg)'
- en: Let's look at some Scala examples.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些Scala的示例。
- en: 'The following code can be pasted directly into the command prompt:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可以直接粘贴到命令提示符中：
- en: '[PRE46]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Programming with Resilient Distributed Datasets (RDD)
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用弹性分布式数据集（RDD）进行编程
- en: RDDs are Spark's core abstraction for working with data. They are immutable
    distributed collections of elements. All functions in Spark only work on RDDs.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: RDD是Spark处理数据的核心抽象。它们是不可变的分布式元素集合。Spark中的所有函数都仅在RDD上操作。
- en: 'Spark automatically distributes the data contained in RDDs across the nodes
    within a cluster as partitions and supports parallel processing to be performed
    on them. RDDs can be created by importing from external datasets or distributing
    collections in the driver program. The following command demonstrates this function:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: Spark自动将RDD中包含的数据分布到集群中的节点上作为分区，并支持对这些分区进行并行处理。可以通过从外部数据集导入或分发驱动程序程序中的集合来创建RDD。以下命令演示了此功能：
- en: '[PRE47]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The `collect()` method will write the output to the console:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '`collect()`方法将输出写入控制台：'
- en: '[PRE48]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output of the results is usually saved to the external storage system.
    The `count()` function gives the number of output lines. The following will print
    out the lines:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的输出通常保存到外部存储系统。`count()`函数给出输出行的数量。以下将打印出这些行：
- en: '[PRE49]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The `take()` function will fetch *n* records from the result:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '`take()`函数将获取结果中的*n*条记录：'
- en: '[PRE50]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: RDDs process in a lazy manner by Spark to bring in the efficiency while handling
    large datasets.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs通过Spark以懒加载的方式处理，从而在处理大数据集时提高效率。
- en: To reuse RDD in multiple actions, you can ask Spark to persist it using `RDD.persist()`.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 要在多个操作中重用RDD，你可以要求Spark使用`RDD.persist()`来持久化它。
- en: We can ask Spark to persist our data in some different places. After computing
    it the first time, Spark will store the RDD contents in the memory (partitioned
    across the machines in your cluster) and reuse them for future actions.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以要求Spark将我们的数据持久化到不同的位置。第一次计算后，Spark将RDD内容存储在内存中（跨集群中的机器分区）并用于未来的操作。
- en: 'Hence, following are the basic steps to process RDDs:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以下处理RDD的基本步骤：
- en: Create input RDDs from external data.
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从外部数据创建输入RDD。
- en: Transforming them to define new RDDs using transformations, for example `filter()`.
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用转换（例如`filter()`）将它们转换为定义新的RDD。
- en: Storing intermediate RDDs for reuse using `persist()`.
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`persist()`存储中间RDD以供重用。
- en: Invoking any required function (for example, `count()`) to start a parallel
    computation process.
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用任何所需的函数（例如，`count()`）以启动并行计算过程。
- en: 'Following is an example of RDD using Pi Estimation with Scala:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个使用Scala的Pi Estimation的RDD示例：
- en: '[PRE51]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Spring XD
  id: totrans-482
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spring XD
- en: Though this book does not include Spring XD framework to demonstrate the Machine
    learning algorithm, a small introduction is given here as this is found to be
    fast emerging for adoption in the Machine learning world.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这本书没有包括Spring XD框架来演示机器学习算法，但在这里给出一个小介绍，因为发现在机器学习世界中快速被采用。
- en: XD stands for eXtreme Data. This open source framework is built by the Pivotal
    team (earlier the SpringSource) as the one-stop-shop for developing and deploying
    big data applications.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: XD代表极端数据。这个开源框架由Pivotal团队（之前称为SpringSource）构建，作为开发和大数据应用程序部署的一站式商店。
- en: Spring XD is a distributed and extensible framework that unifies data ingestion,
    analytics functions in real-time, batch, and supports data export. Spring XD is
    built on Spring Integration and Spring Batch frameworks.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: Spring XD是一个分布式且可扩展的框架，它统一了数据摄取、实时、批处理中的分析功能，并支持数据导出。Spring XD建立在Spring Integration和Spring
    Batch框架之上。
- en: 'Following are some key features:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关键特性：
- en: Spring XD is a unified platform for batch and stream workloads. It is an open
    and extensible runtime.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spring XD是一个批处理和流工作负载的统一平台。它是一个开放且可扩展的运行时。
- en: Scalable and high-performance, it is a distributed data ingestion framework
    that can ingest data from a variety of sources that include HDFS, NOSQL, or Splunk.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展且高性能，它是一个分布式数据摄取框架，可以从包括HDFS、NOSQL或Splunk在内的各种来源摄取数据。
- en: It supports for real-time analytics at ingestion time, for example, gathering
    metrics and counting values.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持在摄取时进行实时分析，例如收集指标和计数值。
- en: It has workflow management through batch jobs that include interactions with
    standard RDBMS and Hadoop systems.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过批处理作业进行工作流管理，包括与标准RDBMS和Hadoop系统的交互。
- en: It is a scalable and high-performance data export, for example, from HDFS to
    an RDBMS or NoSQL database.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一个可扩展且高性能的数据导出，例如从HDFS到RDBMS或NoSQL数据库。
- en: Spring XD is known to implement Lambda Architecture that in theory is defined
    to support both batch and real-time processing. More information on evolutionary
    architectures such as Lambda Architecture is covered in [Chapter 14](ch14.html
    "Chapter 14. New generation data architectures for Machine learning"), *New generation
    data architectures for Machine learning*.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: Spring XD已知实现了Lambda架构，理论上定义了支持批处理和实时处理。有关Lambda架构等进化架构的更多信息，请参阅[第14章](ch14.html
    "第14章。机器学习的新一代数据架构")，*机器学习的新一代数据架构*。
- en: 'Spring XD architecture primarily has three architecture layers to help facilitate
    the previous features:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: Spring XD架构主要包含三个架构层，以帮助实现上述功能：
- en: '**Speed Layer**: This is about accessing and processing data in real time.
    This process keeps the system more up-to-date.![Spring XD](img/B03980_04_19.jpg)'
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**速度层**：这是关于实时访问和处理数据。这个过程使系统保持更及时。![Spring XD](img/B03980_04_19.jpg)'
- en: '**Batch Layer**: The Batch layer has access to the complete master dataset
    also called the data lake meaning *source of truth*.![Spring XD](img/B03980_04_20.jpg)'
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**批处理层**：批处理层可以访问完整的master数据集，也称为数据湖，意味着*真相之源*。![Spring XD](img/B03980_04_20.jpg)'
- en: '**Serving Layer**: The Service layer is more of a query layer that is responsible
    for exposing the data post processing to an unsubscribed consumer. This layer
    makes batch data queryable and is usually known for high throughput driven responses.![Spring
    XD](img/B03980_04_23.jpg)'
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**服务层**：服务层更像是查询层，负责将后处理数据暴露给未订阅的消费者。这一层使得批数据可查询，并且通常以其高吞吐量驱动的响应而闻名。![Spring
    XD](img/B03980_04_23.jpg)'
- en: 'Spring XD Runtime architecture is shown here (source Pivotal):'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了Spring XD运行时架构（来源：Pivotal）：
- en: '![Spring XD](img/B03980_04_22.jpg)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![Spring XD](img/B03980_04_22.jpg)'
- en: Summary
  id: totrans-499
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about the open source options for implementing Machine
    learning, and covered installation, implementation, and execution of libraries,
    tools, and frameworks such as Apache Mahout, Python, R, Julia, and Apache Spark's
    MLib. Importantly, we covered the integration of these frameworks with the big
    data platform—Apache Hadoop. This chapter is more of a foundation for the coming
    chapters where we will learn how to use these frameworks in implementing specific
    Machine learning algorithms.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了实现机器学习的开源选项，并涵盖了Apache Mahout、Python、R、Julia和Apache Spark的MLib等库、工具和框架的安装、实现和执行。重要的是，我们还介绍了这些框架与大数据平台Apache
    Hadoop的集成。本章更多的是为后续章节打基础，在后续章节中我们将学习如何使用这些框架来实现特定的机器学习算法。
