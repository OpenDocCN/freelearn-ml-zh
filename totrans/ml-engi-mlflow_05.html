<html><head></head><body>
		<div id="_idContainer039">
			<h1 id="_idParaDest-56"><a id="_idTextAnchor066"/><em class="italic">Chapter 3</em>: Your Data Science Workbench</h1>
			<p>In this chapter, you will learn about MLflow in the context of creating a local environment so that you can develop your machine learning project locally with the different features provided by MLflow. This chapter is focused on machine learning engineering, and one of the most important roles of a machine learning engineer is to build up an environment where model developers and practitioners can be efficient. We will also demonstrate a hands-on example of how we can use workbenches to accomplish specific tasks.</p>
			<p>Specifically, we will look at the following topics in this chapter: </p>
			<ul>
				<li>Understanding the value of a data science workbench</li>
				<li>Creating your own data science workbench</li>
				<li>Using the workbench for stock prediction</li>
			</ul>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor067"/>Technical requirements </h1>
			<p>For this chapter, you will need the following prerequisites: </p>
			<ul>
				<li>The latest version of Docker installed on your machine. If you don’t already have it installed, please follow the instructions at <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.<p>The latest version of Docker Compose installed. If you don’t already have it installed, please follow the instructions at https://docs.docker.com/compose/install/.</p></li>
				<li>Access to Git in the command line, and installed as described in this <strong class="bold">Uniform Resource Locator</strong> (<strong class="bold">URL</strong>): <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</a>.</li>
				<li>Access to a <strong class="source-inline">bash</strong> terminal (Linux or Windows). </li>
				<li>Access to a browser.</li>
				<li>Python 3.5+ installed.</li>
				<li>MLflow installed locally, as described in <a href="B16783_01_Final_SB_epub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introducing MLflow</em>.</li>
			</ul>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor068"/><a id="_idTextAnchor069"/>Understanding the value of a data science workbench</h1>
			<p>A <a id="_idIndexMarker104"/>data science workbench is an environment to standardize the machine learning tools and practices of an organization, allowing for rapid onboarding and development of models and analytics. One critical machine learning engineering function is to support data science practitioners with tools that empower and accelerate their day-to-day activities.</p>
			<p>In a data science team, the ability to rapidly test multiple approaches and techniques is paramount. Every day, new libraries and open source tools are created. It is common for a project to need more than a dozen libraries in order to test a new type of model. These multitudes of libraries, if not collated correctly, might cause bugs or incompatibilities in the model.</p>
			<p>Data is at the <a id="_idIndexMarker105"/>center of a data science workflow. Having clean datasets available for developing and evaluating models is critical. With an abundance of huge datasets, specialized big data tooling is necessary to process the data. Data can appear in multiple formats and velocities for analysis or experimentation, and can be available in multiple formats and <a id="_idIndexMarker106"/>mediums. It can be available through files, the cloud, or <strong class="bold">REpresentational State Transfer</strong> (<strong class="bold">REST</strong>) <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>). </p>
			<p>Data science is mostly a collaborative craft; it’s part of a workflow to share models and processes among team members. Invariably, one pain point that emerges from that activity is the cross-reproducibility of model development jobs among practitioners. Data scientist A shares a training script of a model that assumes version 2.6 of a library, but data scientist B is using version 2.8 environment. Tracing and fixing the issue can take hours in some cases. If this problem occurs in a production environment, it can become extremely costly to the company.</p>
			<p>When iterating—for instance—over a model, each run contains multiple parameters that can be tweaked to improve it. Maintaining traceability of which parameter yielded a specific performance metric—such as accuracy, for instance—can be problematic if we don’t store details of the experiment in a structured manner. Going back to a specific batch of settings that produced a better model may be impossible if we only keep the latest settings during the model development phase.</p>
			<p>The need to <a id="_idIndexMarker107"/>iterate quickly can cause many frustrations when translating prototype code to a production environment, where it can be executed in a reliable manner. For instance, if you are developing a new trading model in a Windows machine with easy access to <strong class="bold">graphics processing units</strong> (<strong class="bold">GPUs</strong>) for inference, your <a id="_idIndexMarker108"/>engineering team member may decide to reuse the existing Linux infrastructure without GPU access. This leads to a situation where your production algorithm ends up taking 5 hours and locally runs in 30 seconds, impacting the final outcome of the project.</p>
			<p>It is clear that a data science department risks systemic technical pain if issues related to the environment and tools are not addressed upfront. To summarize, we can list the following main points as described in this section:</p>
			<ul>
				<li>Reproducibility friction</li>
				<li>The complexity of handling large and varied datasets</li>
				<li>Poor management of experiment settings</li>
				<li>Drift between local and production environments</li>
			</ul>
			<p>A data science workbench addresses the pain points described in this section by creating a structured environment where a machine learning practitioner can be empowered to develop and deploy their models reliably, with reduced friction. A no-friction environment will allow highly costly model development hours to be focused on developing and iterating models, rather than on solving tooling and data technical issues.</p>
			<p>After having delved into the motivation for building a data science workbench for a machine learning team, we will next start designing the data science workbench based on known pain points.</p>
			<h1 id="_idParaDest-59">Creating your own data science workb<a id="_idTextAnchor070"/><a id="_idTextAnchor071"/>ench</h1>
			<p>In order to address <a id="_idIndexMarker109"/>common frictions for developing models in data science, as described in the previous section, we need to provide data scientists and practitioners with a standardized environment in which they can develop and manage their work. A data science workbench should allow you to quick-start a project, and the availability of an environment with a set of starting tools and frameworks allows data scientists to rapidly jump-start a project.</p>
			<p>The data scientist and machine learning practitioner are at the center of the workbench: they should have a reliable platform that allows them to develop and add value to the organization, with their models at their fingertips. </p>
			<p>The following diagram depicts the core features of a data science workbench:</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B16783_03_001.jpg" alt="Figure 3.1 – Core features of a data science workbench &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Core features of a data science workbench </p>
			<p>In order to think about the<a id="_idIndexMarker110"/> design of our data science workbench and based on the diagram in <em class="italic">Figure 3.1</em>, we need the following core features in our data science workbench:</p>
			<ul>
				<li><strong class="bold">Dependency Management</strong>: Having dependency management built into your local environment helps in handling reproducibility issues and preventing library conflicts between different environments. This is generally achieved by using environment managers such as Docker or having environment management frameworks available in your programming language. MLflow provides this through the support of Docker- or Conda-based environments.</li>
				<li><strong class="bold">Data Management</strong>: Managing data in a local environment can be complex and daunting if you have to handle huge datasets. Having a standardized definition of how you handle data in your local projects allows others to freely collaborate on your projects and understand the structures available.</li>
				<li><strong class="bold">Model Management</strong>: Having<a id="_idIndexMarker111"/> the different models organized and properly stored provides an easy structure to be able to work through many ideas at the same time and persist the ones that have potential. MLflow helps support this through the model format abstraction and <strong class="bold">Model Registry</strong> component to manage models.</li>
				<li><strong class="bold">Deployment</strong>: Having a development environment aligned with the production environment where the model will be serviced requires deliberation in the local environment. The production environment needs to be ready to receive a model from a model developer, with the least possible friction. This smooth deployment workflow is only possible if the local environment is engineered correctly.</li>
				<li><strong class="bold">Experimentation Management</strong>: Tweaking parameters is the most common thing that a machine learning practitioner does. Being able to keep abreast of the different versions and specific parameters can quickly become cumbersome for the model developer.<p class="callout-heading">Important note</p><p class="callout">In this section, we will implement the foundations of a data science workbench from scratch with MLflow, with support <a id="_idIndexMarker112"/>primarily for local development. There are a <a id="_idIndexMarker113"/>couple of very opinionated and feature-rich options provided by cloud providers such as <strong class="bold">Amazon Web Services </strong>(<strong class="bold">AWS</strong>) Sagemaker, Google AI, and <strong class="bold">Azure Machine Learning</strong> (<strong class="bold">Azure ML</strong>).</p></li>
			</ul>
			<p>Machine learning engineering teams have freedom in terms of the use cases and technologies that the team they are serving will use. </p>
			<p>The following steps demonstrate a good workflow<a id="_idIndexMarker114"/> for development with a data science workbench:</p>
			<ul>
				<li>The model developer installs the company workbench package through an installer or by cloning the repository.</li>
				<li>The model developer runs a command to start a project.</li>
				<li>The model developer chooses a set of options based on configuration or a prompt.</li>
				<li>The basic scaffolding is produced with specific folders for the following items:<p>a) <strong class="source-inline">Data</strong>: This will contain all the data assets of your current project</p><p>b) <strong class="source-inline">Notebooks</strong>: To hold all the iterative development notebooks with all the steps required to produce the model</p><p>c) <strong class="source-inline">Model</strong>: A folder that contains the binary model or a reference to models, potentially in binary format</p><p>d) <strong class="source-inline">Source Code</strong>: A folder to store the structured code component of the code and reusable libraries</p><p>e) <strong class="source-inline">Output</strong>: A folder for any specific outputs of the project—for instance, visualizations, reports, or predictions</p></li>
				<li>A project folder is created with the standards for the organization around packages, dependency management, and tools.</li>
				<li>The model developer is free to iterate and create models using supported tooling at an organizational level.</li>
			</ul>
			<p>Establishing a data science workbench provides a tool for acceleration and democratization of machine learning in the organization, due to standardization and efficient adoption of machine learning best practices.</p>
			<p>We will start our workbench implementation in our chapter with sensible components used industrywide. </p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor072"/>Building our workbench</h2>
			<p>We will have the<a id="_idIndexMarker115"/> following components in the architecture of our development environment: </p>
			<ul>
				<li><strong class="bold">Docker/Docker Compose</strong>: Docker<a id="_idIndexMarker116"/> will be used to handle each of the main component dependencies of the architecture, and Docker Compose will be used as a<a id="_idIndexMarker117"/> coordinator between different containers of software pieces. The advantage of having each component of the workbench architecture in Docker is that neither element’s libraries will conflict with the other.</li>
				<li><strong class="bold">JupyterLab</strong>: The<a id="_idIndexMarker118"/> de facto environment to develop data science code and analytics in the context of machine learning.</li>
				<li><strong class="bold">MLflow</strong>: MLflow is at<a id="_idIndexMarker119"/> the cornerstone of the workbench, providing facilities for experiment tracking, model management, registry, and deployment interface.</li>
				<li><strong class="bold">PostgreSQL database</strong>: The<a id="_idIndexMarker120"/> PostgreSQL database is part of the architecture at this stage, as the storage layer for MLflow for backend metadata. Other relational databases could be used as the MLflow backend for metadata, but we will use PostgreSQL. </li>
			</ul>
			<p>Our data science workbench<a id="_idIndexMarker121"/> design can be seen in the following diagram:</p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/image0021.jpg" alt="Figure 3.2 – Our data science workbench design &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Our data science workbench design </p>
			<p><em class="italic">Figure 3.2</em> illustrates the layout of the proposed components that will underpin our data science workbench. </p>
			<p>The usual <a id="_idIndexMarker122"/>workflow of the practitioner, once the environment is up and running, is to develop their code in Jupyter and run their experiments with MLflow support. The environment will automatically route to the right MLflow installation configured to the correct backend, as shown in <em class="italic">Figure 3.2</em>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Our data science workbench, as defined in this chapter, is a complete local environment. As the book progresses, we will introduce cloud-based environments and link our workbench to shared resources.</p>
			<p>A sample layout of the project is available in the following GitHub folder: </p>
			<p>https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter03/gradflow</p>
			<p>You can see a representation of the general layout of the workbench in terms of files here:</p>
			<p class="source-code">├── Makefile</p>
			<p class="source-code">├── README.md</p>
			<p class="source-code">├── data</p>
			<p class="source-code">├── docker</p>
			<p class="source-code">├── docker-compose.yml</p>
			<p class="source-code">├── docs</p>
			<p class="source-code">├── notebooks</p>
			<p class="source-code">├── requirements.txt</p>
			<p class="source-code">├── setup.py</p>
			<p class="source-code">├── src</p>
			<p class="source-code">├── tests</p>
			<p class="source-code">└── tox.ini</p>
			<p>The main <a id="_idIndexMarker123"/>elements of this folder structure are outlined here:</p>
			<ul>
				<li><strong class="source-inline">Makefile</strong>: This allows control of your workbench. By issuing commands, you can ask your workbench to set up a new environment notebook to start MLflow in different formats.</li>
				<li><strong class="source-inline">README.md</strong>: A file that contains a sample description of your project and how to run it. </li>
				<li><strong class="source-inline">data</strong> folder: A folder where we store the datasets used during development and mount the data directories of the database when running locally.</li>
				<li><strong class="source-inline">docker</strong>: A folder that encloses the Docker images of the different subsystems that our environment consists of.</li>
				<li><strong class="source-inline">docker-compose.yml</strong>: A file that contains the orchestration of different services in our workbench environment—namely: Jupyter Notebooks, MLflow, and PostgreSQL to back MLflow. </li>
				<li><strong class="source-inline">docs</strong>: Contains relevant project documentation that we want persisted for the project.</li>
				<li><strong class="source-inline">notebooks</strong>: A folder that contains the notebook information.</li>
				<li><strong class="source-inline">requirements.txt</strong>: A requirements file to add libraries to the project. </li>
				<li><strong class="source-inline">src</strong>: A folder that encloses the source code of the project, to be updated in further phases of the project.</li>
				<li><strong class="source-inline">tests</strong>: A folder that contains end-to-end testing for the code of the project.</li>
				<li><strong class="source-inline">tox.ini</strong>: A templated file that controls the execution of unit tests.</li>
			</ul>
			<p>We will now move on to using our own development environment for a stock-prediction problem, based on the framework we have just built.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor073"/>Using the workbench for stock prediction </h1>
			<p>In this section, we will use the<a id="_idIndexMarker124"/> workbench step by step to set up a new project. Follow the instructions step by step to start up your environment and use the workbench for the stock-prediction project.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">It is critical that all packages/libraries listed in the <em class="italic">Technical requirements</em> section are correctly installed on your local machine to enable you to follow along.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor074"/>Starting up your environment</h2>
			<p>We will move on<a id="_idIndexMarker125"/> next to exploring your own development environment, based on the development environment shown in this section. Please execute the following steps:</p>
			<ol>
				<li>Copy the contents of the project available in https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter03/gradflow.</li>
				<li>Start your local environment by running the following command:<p class="source-code">make</p></li>
				<li>Inspect the created environments, like this: <p class="source-code">$ docker ps </p><p>The following screenshot presents three Docker images: the first for Jupyter, the second for MLflow, and the third for the PostgreSQL database. The status should show <strong class="source-inline">Up x minutes</strong>:</p></li>
			</ol>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/image0031.jpg" alt="Figure 3.3 – Running Docker images &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – Running Docker images </p>
			<p>The<a id="_idIndexMarker126"/> usual ports used by your workbench are listed as follows: Jupyter serves in port <strong class="source-inline">8888</strong>, MLflow serves in port <strong class="source-inline">5000</strong>, and PostgreSQL serves in port <strong class="source-inline">5432</strong>.</p>
			<p>In case any of the containers fail, you might want to check if the ports are used by different services. If this is the case, you will need to turn off all of the other services.</p>
			<p>Check your Jupyter Notebooks environment at <a href="http://localhost:8888">http://localhost:8888</a>, as illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/image0041.jpg" alt="Figure 3.4 – Running Jupyter environment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – Running Jupyter environment</p>
			<p>You<a id="_idIndexMarker127"/> should have a usable environment, allowing you to create new <strong class="source-inline">notebooks</strong> file in the specified folder.</p>
			<p>Check your MLflow environment at http://localhost:5000, as illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/image0051.jpg" alt="Figure 3.5 – Running MLflow environment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – Running MLflow environment</p>
			<p><em class="italic">Figure 3.5</em> shows<a id="_idIndexMarker128"/> your experiment tracker environment in MLflow that you will use to visualize your experiments running in MLflow.</p>
			<p>Run a sample experiment in MLflow by running the <strong class="source-inline">notebook</strong> file available in <strong class="source-inline">/notebooks/mlflow_sample.ipynb</strong>, as illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/image0061.jpg" alt="Figure 3.6 – Excerpt of mlflow_sample code&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – Excerpt of mlflow_sample code</p>
			<p>The code in <em class="italic">Figure 3.6</em> imports MLflow and creates a dummy experiment manually, on the second line, using <strong class="source-inline">mlflow.set_experiment(‘mlflow_experiment’)</strong>.</p>
			<p>The <strong class="source-inline">with mlflow.start_run()</strong> line is responsible for starting and tearing down the experiment in MLflow.</p>
			<p>In the three following lines, we log a couple of string-type test parameters, using the <strong class="source-inline">mlflow.log_param</strong> function. To log numeric values, we will use the <strong class="source-inline">mlflow.log_metric</strong> function. </p>
			<p>Finally, we <a id="_idIndexMarker129"/>also log the entire file that executed the function to ensure traceability of the model and code that originated it, using the <strong class="source-inline">mlflow.log_artifact(“mlflow_example.ipynb”)</strong> function.</p>
			<p>Check the sample runs, to confirm that the environment is working correctly. You should go back to the MLflow <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>) available at http://localhost:5000 and check if the new experiment was created, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/image0071.jpg" alt="Figure 3.7 – MLflow test experiment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – MLflow test experiment</p>
			<p><em class="italic">Figure 3.7</em> displays the additional parameters that we used on our specific experiment and the specific metric named <strong class="source-inline">i</strong> that is visible in the <strong class="bold">Metrics</strong> column.</p>
			<p>Next, you <a id="_idIndexMarker130"/>should click on the experiment created to have access to the details of the run we have executed so far. This is illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/image0081.jpg" alt="Figure 3.8 – MLflow experiment details&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – MLflow experiment details</p>
			<p>Apart from details of the metrics, you also have access to the <strong class="source-inline">mlflow_example</strong> notebook file at a specific point in time. </p>
			<p>At this stage, you have your environment running and working as expected. Next, we will update it with our own algorithm; we’ll use the one we created in <a href="B16783_02_Final_SB_epub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Your Machine Learning Project</em>.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor075"/>Updating with your own algorithms</h2>
			<p>Let’s update the<a id="_idIndexMarker131"/> notebook file that we created in <a href="B16783_02_Final_SB_epub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">ML Problem Framing</em>, and add it to the notebook folder on your local workbench. The code excerpt is presented here:</p>
			<p class="source-code">import mlflow</p>
			<p class="source-code">class RandomPredictor(mlflow.pyfunc.PythonModel):</p>
			<p class="source-code">  def __init__(self):</p>
			<p class="source-code">    pass</p>
			<p class="source-code">  def predict(self, context, model_input):</p>
			<p class="source-code">    return model_input.apply(lambda column: random.randint(0,1))</p>
			<p>Under the <strong class="source-inline">notebook</strong> folder in the <strong class="source-inline">notebooks/stockpred_randomizer.ipynb</strong> file, you can follow along with the integration of the preceding code excerpt in our recently created data science workbench. We will proceed as follows:</p>
			<ol>
				<li value="1">We will first import all the dependencies needed and run the first cell of the notebook, as follows:<div id="_idContainer031" class="IMG---Figure"><img src="image/image0091.jpg" alt="Figure 3.9 – MLflow experiment details&#13;&#10;"/></div><p class="figure-caption">Figure 3.9 – MLflow experiment details</p></li>
				<li>Let’s declare and execute the class outlined in <em class="italic">Figure 3.9</em>, represented in the second cell of the notebook, as follows: <div id="_idContainer032" class="IMG---Figure"><img src="image/image010.jpg" alt="Figure 3.10 – Notebook cell with the RandomPredictor class declaration&#13;&#10;"/></div><p class="figure-caption">Figure 3.10 – Notebook cell with the RandomPredictor class declaration</p></li>
				<li>We can <a id="_idIndexMarker132"/>now save our model in the MLflow infrastructure so that we can test the loading of the model. <strong class="source-inline">model_path</strong> holds the folder name where the model will be saved. You need to instantiate the model in an <strong class="source-inline">r</strong> variable and use <strong class="source-inline">mlflow.pyfunc.save_model</strong> to save the model locally, as illustrated in the following code snippet:<div id="_idContainer033" class="IMG---Figure"><img src="image/image011.jpg" alt="Figure 3.11 – Notebook demonstrating saving the model&#13;&#10;"/></div><p class="figure-caption">Figure 3.11 – Notebook demonstrating saving the model</p><p>You can see on the left pane of your notebook environment that a new folder was created alongside your files to store your models. This folder will store the Conda environment and the pickled/binarized Python function of your model, as illustrated in the following screenshot:</p><div id="_idContainer034" class="IMG---Figure"><img src="image/image012.jpg" alt="Figure 3.12 – Notebook demonstrating the saved model folder&#13;&#10;"/></div><p class="figure-caption">Figure 3.12 – Notebook demonstrating the saved model folder</p></li>
				<li>Next, we<a id="_idIndexMarker133"/> can load and use the model to check that the saved model is usable, as follows:</li>
			</ol>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/image013.jpg" alt="Figure 3.13 – Notebook demonstrating the saved model folder&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13 – Notebook demonstrating the saved model folder</p>
			<p><em class="italic">Figure 3.14</em> demonstrates the creation of a random input <strong class="bold">pandas DataFrame</strong> and the use of <strong class="source-inline">loaded_model</strong> to predict over the input vector. We will run the experiment with the name <strong class="source-inline">stockpred_experiment_days_up</strong>, logging as a metric the number of days on which the market was up on each of the models, as follows:</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/image014.jpg" alt="Figure 3.14 – Notebook cell demonstrating use of the loaded model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14 – Notebook cell demonstrating use of the loaded model</p>
			<p>To check the<a id="_idIndexMarker134"/> last runs of the experiment, you can look at http://localhost:5000 and check that the new experiment was created, as illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/image015.jpg" alt="Figure 3.15 – Initial UI of MLflow for our stockpred experiment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15 – Initial UI of MLflow for our stockpred experiment</p>
			<p>You can now compare multiple runs of our algorithm and see differences in the <strong class="bold">Days Up</strong> metric, as illustrated in the following screenshot. You can choose accordingly to delve <a id="_idIndexMarker135"/>deeper on a run that you would like to have more details about:</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/image016.jpg" alt="Figure 3.16 – Logged details of the artifacts saved&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.16 – Logged details of the artifacts saved</p>
			<p>In <em class="italic">Figure 3.16</em>, you can clearly see the logged details of our run—namely, the artifact model and the <strong class="bold">Days Up</strong> metric.</p>
			<p>In order to tear down the environment properly, you must run the following command in the same fo<a id="_idTextAnchor076"/>lder:</p>
			<p class="source-code">make down</p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor077"/>Summary</h1>
			<p>In this chapter, we introduced the concept of a data science workbench and explored some of the motivation behind adopting this tool as a way to accelerate our machine learning engineering practice.</p>
			<p>We designed a data science workbench, using MLflow and adjacent technologies based on our requirements. We detailed the steps to set up your development environment with MLflow and illustrated how to use it with existing code. In later sections, we explored the workbench and added to it our stock-trading algorithm developed in the last chapter.</p>
			<p>In the next chapter, we will focus on experimentation to improve our models with MLflow, using the workbench developed in this chapter.</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor078"/>Further reading</h1>
			<p>In order to further your knowledge, you can consult the documentation in the fo<a id="_idTextAnchor079"/><a id="_idTextAnchor080"/>llowing links: </p>
			<ul>
				<li>Cookiecutter documentation page: <a href="https://cookiecutter.readthedocs.io/en/1.7.2/">https://cookiecutter.readthedocs.io/en/1.7.2/</a><span class="hidden"> </span></li>
				<li>Reference information about cookie cutters: <a href="https://drivendata.github.io/cookiecutter-data-science/">https://drivendata.github.io/cookiecutter-data-science/</a></li>
				<li>The motivation behind data science workbenches: <a href="https://dzone.com/articles/what-is-a-data-science-workbench-and-why-do-data-s#">https://dzone.com/articles/what-is-a-data-science-workbench-and-why-do-data-s#</a></li>
			</ul>
		</div>
	</body></html>