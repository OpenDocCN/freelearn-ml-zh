["```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = np.array([[3,1], [2,5], [1,8], [6,4], [5,2], [3,5], [4,7], [4,-1]])\n```", "```py\ny = [0, 1, 1, 0, 0, 1, 1, 0]\n```", "```py\nclass_0 = np.array([X[i] for i in range(len(X)) if y[i]==0])\nclass_1 = np.array([X[i] for i in range(len(X)) if y[i]==1])\n```", "```py\nplt.figure()\nplt.scatter(class_0[:,0], class_0[:,1], color='black', marker='s')\nplt.scatter(class_1[:,0], class_1[:,1], color='black', marker='x')\nplt.show()\n```", "```py\nline_x = range(10)\nline_y = line_x\n```", "```py\nplt.figure()\nplt.scatter(class_0[:,0], class_0[:,1], color='black', marker='s')\nplt.scatter(class_1[:,0], class_1[:,1], color='black', marker='x')\nplt.plot(line_x, line_y, color='black', linewidth=3)\nplt.show()\n```", "```py\nimport numpy as np\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\nX = np.array([[4, 7], [3.5, 8], [3.1, 6.2], [0.5, 1], [1, 2], [1.2, 1.9], [6, 2], [5.7, 1.5], [5.4, 2.2]])\ny = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n```", "```py\nclassifier = linear_model.LogisticRegression(solver='lbfgs', C=100)\n```", "```py\nclassifier.fit(X, y)\n```", "```py\nx_min, x_max = min(X[:, 0]) - 1.0, max(X[:, 0]) + 1.0\ny_min, y_max = min(X[:, 1]) - 1.0, max(X[:, 1]) + 1.0\n```", "```py\n# denotes the step size that will be used in the mesh grid\nstep_size = 0.01\n\n# define the mesh grid\nx_values, y_values = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))\n```", "```py\n# compute the classifier output\nmesh_output = classifier.predict(np.c_[x_values.ravel(), y_values.ravel()])\n\n# reshape the array\nmesh_output = mesh_output.reshape(x_values.shape)\n```", "```py\n# Plot the output using a colored plot \nplt.figure()\n\n# choose a color scheme you can find all the options \n# here: http://matplotlib.org/examples/color/colormaps_reference.html\nplt.pcolormesh(x_values, y_values, mesh_output, cmap=plt.cm.gray)\n```", "```py\n# Overlay the training points on the plot \nplt.scatter(X[:, 0], X[:, 1], c=y, s=80, edgecolors='black', linewidth=1, cmap=plt.cm.Paired)\n\n# specify the boundaries of the figure\nplt.xlim(x_values.min(), x_values.max())\nplt.ylim(y_values.min(), y_values.max())\n\n# specify the ticks on the X and Y axes\nplt.xticks((np.arange(int(min(X[:, 0])-1), int(max(X[:, 0])+1), 1.0)))\nplt.yticks((np.arange(int(min(X[:, 1])-1), int(max(X[:, 1])+1), 1.0)))\n\nplt.show()\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\n```", "```py\ninput_file = 'data_multivar.txt'\nX = []\ny = []\nwith open(input_file, 'r') as f:\n    for line in f.readlines():\n        data = [float(x) for x in line.split(',')]\n        X.append(data[:-1])\n        y.append(data[-1])\nX = np.array(X)\ny = np.array(y)\n```", "```py\nclassifier_gaussiannb = GaussianNB()\nclassifier_gaussiannb.fit(X, y)\ny_pred = classifier_gaussiannb.predict(X)\n```", "```py\naccuracy = 100.0 * (y == y_pred).sum() / X.shape[0]\nprint(\"Accuracy of the classifier =\", round(accuracy, 2), \"%\")\n```", "```py\nAccuracy of the classifier = 99.5 %\n```", "```py\nx_min, x_max = min(X[:, 0]) - 1.0, max(X[:, 0]) + 1.0\ny_min, y_max = min(X[:, 1]) - 1.0, max(X[:, 1]) + 1.0\n\n# denotes the step size that will be used in the mesh grid\nstep_size = 0.01\n\n# define the mesh grid\nx_values, y_values = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))\n\n# compute the classifier output\nmesh_output = classifier_gaussiannb.predict(np.c_[x_values.ravel(), y_values.ravel()])\n\n# reshape the array\nmesh_output = mesh_output.reshape(x_values.shape)\n\n# Plot the output using a colored plot \nplt.figure()\n\n# choose a color scheme \nplt.pcolormesh(x_values, y_values, mesh_output, cmap=plt.cm.gray)\n\n# Overlay the training points on the plot \nplt.scatter(X[:, 0], X[:, 1], c=y, s=80, edgecolors='black', linewidth=1, cmap=plt.cm.Paired)\n\n# specify the boundaries of the figure\nplt.xlim(x_values.min(), x_values.max())\nplt.ylim(y_values.min(), y_values.max())\n\n# specify the ticks on the X and Y axes\nplt.xticks((np.arange(int(min(X[:, 0])-1), int(max(X[:, 0])+1), 1.0)))\nplt.yticks((np.arange(int(min(X[:, 1])-1), int(max(X[:, 1])+1), 1.0)))\n\nplt.show()\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB \n\ninput_file = 'data_multivar.txt'\n\nX = []\ny = []\nwith open(input_file, 'r') as f:\n    for line in f.readlines():\n        data = [float(x) for x in line.split(',')]\n        X.append(data[:-1])\n        y.append(data[-1]) \n\nX = np.array(X)\ny = np.array(y)\n\n#Splitting the dataset for training and testing\nfrom sklearn import model_selection\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=5)\n\n#Building the classifier\nclassifier_gaussiannb_new = GaussianNB()\nclassifier_gaussiannb_new.fit(X_train, y_train)\n```", "```py\ny_test_pred = classifier_gaussiannb_new.predict(X_test)\n```", "```py\naccuracy = 100.0 * (y_test == y_test_pred).sum() / X_test.shape[0]\nprint(\"Accuracy of the classifier =\", round(accuracy, 2), \"%\")\n```", "```py\nAccuracy of the classifier = 98.0 %\n```", "```py\n#Plot a classifier\n#Define the data\nX= X_test\ny=y_test\n\n# define ranges to plot the figure \nx_min, x_max = min(X[:, 0]) - 1.0, max(X[:, 0]) + 1.0\ny_min, y_max = min(X[:, 1]) - 1.0, max(X[:, 1]) + 1.0\n\n# denotes the step size that will be used in the mesh grid\nstep_size = 0.01\n\n# define the mesh grid\nx_values, y_values = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))\n\n# compute the classifier output\nmesh_output = classifier_gaussiannb_new.predict(np.c_[x_values.ravel(), y_values.ravel()])\n\n# reshape the array\nmesh_output = mesh_output.reshape(x_values.shape)\n\n# Plot the output using a colored plot \nplt.figure()\n\n# choose a color scheme\nplt.pcolormesh(x_values, y_values, mesh_output, cmap=plt.cm.gray)\n\n# Overlay the training points on the plot \nplt.scatter(X[:, 0], X[:, 1], c=y, s=80, edgecolors='black', linewidth=1, cmap=plt.cm.Paired)\n\n# specify the boundaries of the figure\nplt.xlim(x_values.min(), x_values.max())\nplt.ylim(y_values.min(), y_values.max())\n\n# specify the ticks on the X and Y axes\nplt.xticks((np.arange(int(min(X[:, 0])-1), int(max(X[:, 0])+1), 1.0)))\nplt.yticks((np.arange(int(min(X[:, 1])-1), int(max(X[:, 1])+1), 1.0)))\n\nplt.show()\n```", "```py\nfrom sklearn import model_selection\nnum_validations = 5\naccuracy = model_selection.cross_val_score(classifier_gaussiannb,\n        X, y, scoring='accuracy', cv=num_validations)\nprint \"Accuracy: \" + str(round(100*accuracy.mean(), 2)) + \"%\"\n```", "```py\nf1 = model_selection.cross_val_score(classifier_gaussiannb,\n X, y, scoring='f1_weighted', cv=num_validations)\nprint \"F1: \" + str(round(100*f1.mean(), 2)) + \"%\"\nprecision = model_selection.cross_val_score(classifier_gaussiannb,\n X, y, scoring='precision_weighted', cv=num_validations)\nprint \"Precision: \" + str(round(100*precision.mean(), 2)) + \"%\"\nrecall = model_selection.cross_val_score(classifier_gaussiannb,\n X, y, scoring='recall_weighted', cv=num_validations)\nprint \"Recall: \" + str(round(100*recall.mean(), 2)) + \"%\"\n\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n```", "```py\n# Show confusion matrix\ndef plot_confusion_matrix(confusion_mat):\n    plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Paired)\n    plt.title('Confusion matrix')\n    plt.colorbar()\n    tick_marks = np.arange(4)\n    plt.xticks(tick_marks, tick_marks)\n    plt.yticks(tick_marks, tick_marks)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n```", "```py\ny_true = [1, 0, 0, 2, 1, 0, 3, 3, 3]\ny_pred = [1, 1, 0, 2, 1, 0, 1, 3, 3]\nconfusion_mat = confusion_matrix(y_true, y_pred)\nplot_confusion_matrix(confusion_mat)\n```", "```py\nfrom sklearn.metrics import classification_report\ny_true = [1, 0, 0, 2, 1, 0, 3, 3, 3]\ny_pred = [1, 1, 0, 2, 1, 0, 1, 3, 3]\ntarget_names = ['Class-0', 'Class-1', 'Class-2', 'Class-3']\nprint(classification_report(y_true, y_pred, target_names=target_names))\n```", "```py\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\n```", "```py\ninput_file = 'car.data.txt'\n# Reading the data\nX = []\ncount = 0\nwith open(input_file, 'r') as f:\n    for line in f.readlines():\n        data = line[:-1].split(',')\n        X.append(data)\nX = np.array(X)\n```", "```py\n# Convert string data to numerical data\nlabel_encoder = []\nX_encoded = np.empty(X.shape)\nfor i,item in enumerate(X[0]):\n    label_encoder.append(preprocessing.LabelEncoder())\n    X_encoded[:, i] = label_encoder[-1].fit_transform(X[:, i])\nX = X_encoded[:, :-1].astype(int)\ny = X_encoded[:, -1].astype(int)\n```", "```py\n# Build a Random Forest classifier\nparams = {'n_estimators': 200, 'max_depth': 8, 'random_state': 7}\nclassifier = RandomForestClassifier(**params)\nclassifier.fit(X, y)\n```", "```py\n# Cross validation\nfrom sklearn import model_selection\n\naccuracy = model_selection.cross_val_score(classifier, \n        X, y, scoring='accuracy', cv=3)\nprint(\"Accuracy of the classifier: \" + str(round(100*accuracy.mean(), 2)) + \"%\")\n```", "```py\nAccuracy of the classifier: 78.19%\n```", "```py\n# Testing encoding on single data instance\ninput_data = ['high', 'low', '2', 'more', 'med', 'high']\ninput_data_encoded = [-1] * len(input_data)\nfor i,item in enumerate(input_data):\n    input_data_encoded[i] = int(label_encoder[i].transform([input_data[i]]))\ninput_data_encoded = np.array(input_data_encoded)\n```", "```py\n# Predict and print output for a particular datapoint\noutput_class = classifier.predict([input_data_encoded])\nprint(\"Output class:\", label_encoder[-1].inverse_transform(output_class)[0])\n```", "```py\nOutput class: acc\n```", "```py\n# Validation curves\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import validation_curve\n\nclassifier = RandomForestClassifier(max_depth=4, random_state=7)\n\nparameter_grid = np.linspace(25, 200, 8).astype(int)\ntrain_scores, validation_scores = validation_curve(classifier, X, y, \"n_estimators\", parameter_grid, cv=5)\nprint(\"##### VALIDATION CURVES #####\")\nprint(\"\\nParam: n_estimators\\nTraining scores:\\n\", train_scores)\nprint(\"\\nParam: n_estimators\\nValidation scores:\\n\", validation_scores)\n```", "```py\n# Plot the curve\nplt.figure()\nplt.plot(parameter_grid, 100*np.average(train_scores, axis=1), color='black')\nplt.title('Training curve')\nplt.xlabel('Number of estimators')\nplt.ylabel('Accuracy')\nplt.show()\n```", "```py\nclassifier = RandomForestClassifier(n_estimators=20, random_state=7)\nparameter_grid = np.linspace(2, 10, 5).astype(int)\ntrain_scores, valid_scores = validation_curve(classifier, X, y, \n        \"max_depth\", parameter_grid, cv=5)\nprint(\"\\nParam: max_depth\\nTraining scores:\\n\", train_scores)\nprint(\"\\nParam: max_depth\\nValidation scores:\\n\", validation_scores)\n```", "```py\n# Plot the curve\nplt.figure()\nplt.plot(parameter_grid, 100*np.average(train_scores, axis=1), color='black')\nplt.title('Validation curve')\nplt.xlabel('Maximum depth of the tree')\nplt.ylabel('Accuracy')\nplt.show()\n```", "```py\nfrom sklearn.model_selection import validation_curve\n\nclassifier = RandomForestClassifier(random_state=7)\n\nparameter_grid = np.array([200, 500, 800, 1100])\ntrain_scores, validation_scores = validation_curve(classifier, X, y, \"n_estimators\", parameter_grid, cv=5)\nprint(\"\\n##### LEARNING CURVES #####\")\nprint(\"\\nTraining scores:\\n\", train_scores)\nprint(\"\\nValidation scores:\\n\", validation_scores)\n\n```", "```py\n# Plot the curve\nplt.figure()\nplt.plot(parameter_grid, 100*np.average(train_scores, axis=1), color='black')\nplt.title('Learning curve')\nplt.xlabel('Number of training samples')\nplt.ylabel('Accuracy')\nplt.show()\n```", "```py\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.naive_bayes import GaussianNB\n```", "```py\ninput_file = 'adult.data.txt'\n# Reading the data\nX = []\ny = []\ncount_lessthan50k = 0\ncount_morethan50k = 0\nnum_images_threshold = 10000\n```", "```py\nwith open(input_file, 'r') as f:\n    for line in f.readlines():\n        if '?' in line:\n            continue\n        data = line[:-1].split(', ')\n        if data[-1] == '<=50K' and count_lessthan50k < num_images_threshold:\n            X.append(data)\n            count_lessthan50k = count_lessthan50k + 1\n        elif data[-1] == '>50K' and count_morethan50k < num_images_threshold:\n            X.append(data)\n            count_morethan50k = count_morethan50k + 1\n        if count_lessthan50k >= num_images_threshold and count_morethan50k >= num_images_threshold:\n            break\nX = np.array(X)\n```", "```py\n# Convert string data to numerical data\nlabel_encoder = []\nX_encoded = np.empty(X.shape)\nfor i,item in enumerate(X[0]):\n    if item.isdigit():\n        X_encoded[:, i] = X[:, i]\n    else:\n        label_encoder.append(preprocessing.LabelEncoder())\n        X_encoded[:, i] = label_encoder[-1].fit_transform(X[:, i])\nX = X_encoded[:, :-1].astype(int)\ny = X_encoded[:, -1].astype(int)\n```", "```py\n# Build a classifier\nclassifier_gaussiannb = GaussianNB()\nclassifier_gaussiannb.fit(X, y)\n```", "```py\n# Cross validation\nfrom sklearn import model_selection\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=5)\nclassifier_gaussiannb = GaussianNB()\nclassifier_gaussiannb.fit(X_train, y_train)\ny_test_pred = classifier_gaussiannb.predict(X_test)\n```", "```py\n# compute F1 score of the classifier\nf1 = model_selection.cross_val_score(classifier_gaussiannb,\n        X, y, scoring='f1_weighted', cv=5)\nprint(\"F1 score: \" + str(round(100*f1.mean(), 2)) + \"%\")\n```", "```py\nF1 score: 75.9%\n```", "```py\n# Testing encoding on single data instance\ninput_data = ['39', 'State-gov', '77516', 'Bachelors', '13', 'Never-married', 'Adm-clerical', 'Not-in-family', 'White', 'Male', '2174', '0', '40', 'United-States']\ncount = 0\ninput_data_encoded = [-1] * len(input_data)\nfor i,item in enumerate(input_data):\n    if item.isdigit():\n        input_data_encoded[i] = int([input_data[i]])\n    else:\n        input_data_encoded[i] = int(label_encoder[count].transform([input_data[i]]))\n        count = count + 1 \ninput_data_encoded = np.array(input_data_encoded)\n```", "```py\n# Predict and print output for a particular datapoint\noutput_class = classifier_gaussiannb.predict([input_data_encoded])\nprint(label_encoder[-1].inverse_transform(output_class)[0])\n```", "```py\n<=50K\n```", "```py\nimport numpy as np\ninput_file = 'wine.txt'\nX = []\ny = []\nwith open(input_file, 'r') as f:\n  for line in f.readlines():\n     data = [float(x) for x in line.split(',')]\n     X.append(data[1:])\n     y.append(data[0])\nX = np.array(X)\ny = np.array(y)\n```", "```py\nfrom sklearn import model_selection\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=5)\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier_DecisionTree = DecisionTreeClassifier()\nclassifier_DecisionTree.fit(X_train, y_train)\n```", "```py\ny_test_pred = classifier_DecisionTree.predict(X_test)\n\naccuracy = 100.0 * (y_test == y_test_pred).sum() / X_test.shape[0]\nprint(\"Accuracy of the classifier =\", round(accuracy, 2), \"%\")\n```", "```py\nAccuracy of the classifier = 91.11 %\n```", "```py\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_mat = confusion_matrix(y_test, y_test_pred)\nprint(confusion_mat)\n```", "```py\n[[17  2  0]\n [ 1 12  1]\n [ 0  0 12]]\n```", "```py\nfrom sklearn.datasets import fetch_20newsgroups\n```", "```py\nNewsClass = ['rec.sport.baseball', 'rec.sport.hockey']\n```", "```py\nDataTrain = fetch_20newsgroups(subset='train',categories=NewsClass, shuffle=True, random_state=42)\n```", "```py\nprint(DataTrain.target_names)\n```", "```py\n['rec.sport.baseball', 'rec.sport.hockey']\n```", "```py\nprint(len(DataTrain.data))\nprint(len(DataTrain.target))\n```", "```py\n1197\n1197\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nCountVect = CountVectorizer()\nXTrainCounts = CountVect.fit_transform(DataTrain.data)\nprint(XTrainCounts.shape)\n```", "```py\n(1197, 18571)\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nTfTransformer = TfidfTransformer(use_idf=False).fit(XTrainCounts)\nXTrainNew = TfTransformer.transform(XTrainCounts)\nTfidfTransformer = TfidfTransformer()\nXTrainNewidf = TfidfTransformer.fit_transform(XTrainCounts)\n```", "```py\nfrom sklearn.naive_bayes import MultinomialNB\n\nNBMultiClassifier = MultinomialNB().fit(XTrainNewidf, DataTrain.target)\n```", "```py\nNewsClassPred = NBMultiClassifier.predict(XTrainNewidf)\n\naccuracy = 100.0 * (DataTrain.target == NewsClassPred).sum() / XTrainNewidf.shape[0]\nprint(\"Accuracy of the classifier =\", round(accuracy, 2), \"%\")\n```", "```py\nAccuracy of the classifier = 99.67 %\n```"]