- en: Profiling and Accelerating Your Apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you have a problem with a slow app, first of all, you need to find which
    exact parts of your code are taking quite a lot of processing time. A good way
    of finding such parts of the code, which are also called **bottlenecks**, is to
    profile the app. One of the good profilers available that allow an app to be profiled
    without modifications being introduced to the app is called `pyinstrument` ([https://github.com/joerick/pyinstrument](https://github.com/joerick/pyinstrument)).
    Here, we profile the app of [Chapter 10](7eaac815-5888-4352-aa83-7a3b50d0d275.xhtml), *Learning
    to Detect and Track Objects*, using `pyinstrument`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We have passed an output `.html` file where we want the profiling report information
    to be saved with a `-o` option.
  prefs: []
  type: TYPE_NORMAL
- en: We have also specified how the report should be rendered with a `-r` option,
    to state that we want an HTML output. Once the app is terminated, the profiling
    report will be generated, and it can be viewed in a browser.
  prefs: []
  type: TYPE_NORMAL
- en: You can omit both options.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the latter case, the report will be shown in the console. Once we terminate
    the app, we can open the generated `.html` file in the browser, which will show
    a report similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae15457c-82a9-4805-a06c-09b66ee7469c.png)'
  prefs: []
  type: TYPE_IMG
- en: First of all, we can note that quite a lot of time is spent on the script itself.
    This should be expected, as an object detection model is making an inference on
    each frame, and that it is quite a heavy operation. We can also note that tracking
    also takes quite a lot of time, especially in the `iou` function.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, depending on a particular application of the app, in order to accelerate
    tracking, it can be enough to replace the `iou` function with a different one
    that is more efficient. In this app, the `iou` function was used to compute `iou_matrix`,
    which stores the **Intersection Over Union** (**IOU**) metric for each possible
    pair of detection and tracking boxes. When you work on accelerating your code,
    in order to save time, it might be a good idea to change the code with an accelerated
    version in place and profile it again, in order to check whether it meets your
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: But let's take the appropriate relevant code out of the app and analyze the
    possibilities of accelerating it using **Numba**, which we will cover in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerating with Numba
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Numba is a compiler that optimizes code written in pure Python using the **Low-Level
    Virtual Machine** (**LLVM**) compiler infrastructure. It efficiently compiles
    math-heavy Python code to reach performance similar to **C**, **C++**, and **Fortran**.
    It understands a range of `numpy` functions, Python `construct` libraries, and
    operators, as well as a range of math functions from the standard library, and
    generates corresponding native code for **Graphical Processing Units** (**GPUs**)
    and **Central Processing Units** (**CPUs**), with simple annotations.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use the **IPython** interactive interpreter to work
    with the code. It is an enhanced interactive Python shell that particularly supports
    so-called **magic commands**, which—in our case—we will use for timing functions.
    One of the options is to use the interpreter directly in the console. A couple
    of other options are to use **Jupyter Notebook** or **JupyterLab**. If you are
    using the **Atom **editor, you might want to consider the **Hydrogen** plugin,
    which implements an interactive coding environment right in the editor.
  prefs: []
  type: TYPE_NORMAL
- en: 'To import NumPy and Numba, run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We are using **Numba version 0.49**, which is the most recent version at the
    time of writing. Throughout this section, you will note that we will have to change
    the code in such a way that it could be compiled using this version of Numba.
  prefs: []
  type: TYPE_NORMAL
- en: Supposedly, in future versions, Numba will support more functions, and some—or
    all—modifications might be not required. When you work on the code of your app,
    please refer to the **Numba** documentation for the supported features, available
    at [https://numba.pydata.org/numba-doc/latest/index.html](https://numba.pydata.org/numba-doc/latest/index.html) at
    the time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we cover some important possibilities of Numba and illustrate results
    on a certain example, so that you will have your vision on how Numba can help
    you with accelerating the code of your own apps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now isolate the code that we want to accelerate, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, this is the function that computes the `iou` of two boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For now, we have left it as it is from [Chapter 10](7eaac815-5888-4352-aa83-7a3b50d0d275.xhtml), *Learning
    to Detect and Track Objects*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next is the part of the code that calculates `iou_matrix` using the previous
    function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We have wrapped up the corresponding loops and matrix definition in a single
    new function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to test performance, let''s define two sets of `random` bounding boxes,
    like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We have defined two sets of `100` bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can estimate how much time it takes to compose `iou_matrix` of these
    bounding boxes by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `%timeit` magic command executes the function multiple times, computes
    the average execution time, as well as the deviation from the average, and outputs
    the result, which looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can note that it takes about one-third of 1 second to compute the matrix.
    Hence, if we have 100 objects in the scene and we want to process multiple frames
    in 1 second, there will be a huge bottleneck in the app. Let's now accelerate
    this code on a CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerating with the CPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Numba has several code-generation utilities that generate machine code out
    of Python code. One of its central features is the `@numba.jit` decorator. This
    decorator allows you to mark a function for optimization by Numba''s compiler.
    For example, the following function calculates the product of all the elements
    in an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It can be viewed as a `np.product`. custom implementation. The decorator tells
    Numba to compile the function into machine code, which results in much faster
    execution time compared to the Python version. Numba always tries to compile the
    specified function. In the case of operations in the function that cannot be fully
    compiled, Numba falls back to the so-called **object mode**, which uses the **Python/C
    API** and handles all values as Python objects, to perform operations on them.
  prefs: []
  type: TYPE_NORMAL
- en: The latter is much slower than the former. When we pass `nopython=True`, we
    explicitly tell it to raise an exception when the function cannot be compiled
    to full machine code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the same decorator with the `iou` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can note that this function differs slightly from the Python function. First
    of all, we have used our custom implementation of `np.product`. If we try to use
    the native implementation with the current version of Numba, we will end up with
    an exception, as the native `np.product` is not currently supported by the Numba
    compiler. It's a similar story with the first two lines of the function, where
    Numba fails to interpret the automatic unpacking of the array into a tuple.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are ready to time our function, as we did previously, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The latter produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can note that we already have a huge acceleration (about 20 times), but
    let''s proceed further. We can note that `calc_iou_matrix` is still in pure Python
    and it has nested loops, which might take quite a lot of time. Let''s create a
    compiled version of it, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, this function differs from the original one, as Numba could not interpret `enumerate`.
    Timing this implementation will produce an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We again have an acceleration. This version is twice as fast compared with the
    previous one. Let's continue with the acceleration and get it as fast as possible,
    but before doing that, let's first familiarize ourselves with the `vectorize` decorator.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `vectorize` decorator allows functions to be created that can be used as
    NumPy `ufuncs` class out of functions that work on scalar arguments, as in the
    following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The function performs some specific operation when given a pair of scalars,
    and the `vectorize` decorator makes it possible to do the same operation on NumPy
    arrays, for example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'NumPy casting rules also work—for example, you can replace one of the arrays
    with a scalar or an array with shape `(1,4)`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Another decorator that we will use to accelerate our `iou_matrix` computation
    is `guvectorize`. This decorator takes the concept of `vectorize` one step further.
    It allows `ufuncs` to be written that return arrays with different dimensionality.
    We can note that, when calculating the IOU matrix, the output array has a shape
    composed of the numbers of bounding boxes in each passed array. We use the decorator
    as follows to compute the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter tells Numba to compile a function that works on 8-byte floats
    (`float64`). It also specifies the dimensionalities of the input and output arrays
    with semicolons. The second parameter is the signature, which specifies how the
    dimensions of the input and output arrays are matched with each other. Once we
    execute the function with the input, the `z` output is waiting there with the
    correct shape and just needs to be filled in the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we time this implementation as we did previously, we obtain an output similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Again, we are about 30 times faster compared to the previous case. In comparison
    with the initial pure Python implementation, we are about 1,000 times faster,
    which is quite impressive.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Numba, CUDA, and GPU acceleration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have seen how simple it is to create CPU-accelerated code using Numba. Numba
    also provides a similar interface to make a computation on a GPU using **Compute
    Unified Device Architecture** (**CUDA**). Let's port our IOU matrix calculation
    function to be computed on a GPU using Numba.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can instruct Numba to make the computation on a GPU by slightly modifying
    the decorator parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have instructed Numba to make the computation on a GPU by passing `target="cuda"`.
    We also have work to do on the `iou` function. The new function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: First of all, we have changed the decorator, which now uses `numba.cuda.jit`
    instead of `numba.jit`. The latter instructs Numba to create a function that is
    executed on a GPU. This function itself is called from a function that is running
    on a GPU device. For that purpose, we have passed `device=True`, which explicitly
    states that this function is intended to be used from functions that are calculated
    on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: You can also note that we made quite a few modifications so that we have eliminated
    all the NumPy function calls. As with CPU acceleration, this is due to the fact
    that `numba.cuda` cannot currently perform all operations that were available
    in the function, and we replaced them with the ones that `numba.cuda` supports.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, in computer vision, your app will require GPU acceleration only when
    you are working with **deep neural network**s (**DNNs**). Most of the modern deep
    learning frameworks, such as **TensorFlow**, **PyTorch**, and **MXNet**, support
    GPU acceleration out of the box, allowing you to be away from low-level GPU programming
    and to concentrate on your models instead. After analyzing the frameworks, if
    you find yourself with a specific algorithm that you think should be necessarily
    implemented with CUDA directly, you might want to analyze the `numba.cuda` API,
    which supports most of the CUDA features.
  prefs: []
  type: TYPE_NORMAL
