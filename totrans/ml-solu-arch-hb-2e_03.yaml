- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring ML Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While ML algorithm design may not be the primary role of ML solutions architects,
    it is still essential for them to possess a comprehensive understanding of common
    real-world ML algorithms and their applications in solving business problems.
    This knowledge empowers ML solutions architects to identify suitable data science
    solutions and design the necessary technology infrastructure for deploying these
    algorithms effectively.
  prefs: []
  type: TYPE_NORMAL
- en: By familiarizing themselves with a range of ML algorithms, ML solutions architects
    can grasp the strengths, limitations, and specific use cases of each algorithm.
    This enables them to evaluate business requirements accurately and select the
    most appropriate algorithmic approach to address a given problem. Whether it’s
    classification, regression, clustering, or recommendation systems, understanding
    the underlying algorithms equips architects with the knowledge required to make
    informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore the fundamentals of ML and delve into common
    ML and deep learning algorithms. We’ll cover tasks such as classification, regression,
    object detection, recommendation, forecasting, and natural language generation.
    By understanding the core principles and applications of these algorithms, you’ll
    gain the knowledge to identify suitable ML solutions for real-world problems.
    This chapter aims to equip you with the expertise to make informed decisions and
    design effective ML solutions across various domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: How machines learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considerations for choosing ML algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms for classification and regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms for clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms for time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms for recommendation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms for computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms for natural language processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on exercise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this chapter provides an introduction to ML algorithms for readers
    who are new to applying these algorithms. If you already have experience as a
    data scientist or ML engineer, you may want to skip this chapter and go directly
    to *Chapter 4*, where we discuss data management for ML.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You need a personal computer (**Mac** or **Windows**) to complete the hands-on
    exercise portion of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You also need to download the dataset from [https://www.kaggle.com/mathchi/churn-for-bank-customers](https://www.kaggle.com/mathchi/churn-for-bank-customers).
    Additional instructions will be provided in the *Hands-on exercise* section.
  prefs: []
  type: TYPE_NORMAL
- en: How machines learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 1*, *Navigating the ML Lifecycle with ML Solutions Architecture*,
    we discussed the self-improvement capability of ML algorithms through data processing
    and parameter updates, leading to the generation of models akin to compiled binaries
    in computer source code. But how does an algorithm actually learn? In essence,
    ML algorithms learn by optimizing an objective function, also known as a loss
    function, which involves minimizing or maximizing it. An objective function can
    be seen as a business metric, such as the disparity between projected and actual
    product sales. The aim of optimization is to reduce this disparity. To achieve
    this, an ML algorithm iterates and processes extensive historical sales data (training
    data), adjusting its internal model parameters until the gaps between projected
    and actual values are minimized. This process of finding the optimal model parameters
    is referred to as optimization, with mathematical routines specifically designed
    for this purpose known as optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the concept of optimization, let’s consider a simple example
    of training an ML model to predict product sales based on its price. In this case,
    we can use a linear function as the ML algorithm, represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: sales = W * price + B
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, our objective is to minimize the disparity between the predicted
    and actual sales values. To achieve this, we employ the **mean square error**
    (**MSE**) as the loss function for optimization. The specific task is to determine
    the optimal values for the model parameters *W* and *B*, commonly referred to
    as weight and bias. The weight assigns a relative significance to each input variable,
    while the bias represents the average output value. Our aim is to identify the
    *W* and *B* values that yield the lowest MSE in order to enhance the accuracy
    of the sales predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are multiple techniques available for solving ML optimization problems.
    Among them, gradient descent and its variations are widely used for optimizing
    neural networks and various other ML algorithms. Gradient descent is an iterative
    approach that involves calculating the rate of error change (gradient) associated
    with each input variable. Based on this gradient, the model parameters (*W* and
    *B* in this example) are updated step by step to gradually reduce the error. The
    learning rate, a hyperparameter of the ML algorithm, controls the magnitude of
    parameter updates at each iteration. This allows for fine-tuning the optimization
    process. The following figure illustrates the optimization of the W value using
    gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Gradient descent ](img/B20836_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Gradient descent'
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient descent optimization process involves several key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the value of *W* randomly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the error (loss) using the assigned value of *W*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient (rate of change) of the error with respect to the loss
    function. The gradient can be positive, zero, or negative.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the gradient is positive or negative, update the value of *W* in a direction
    that reduces the error in the next iteration. In this example, we move *W* to
    the right to increase its value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *4* until the gradient becomes zero, indicating that the
    optimal value of *W* has been reached and convergence has been achieved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In addition to gradient descent, alternative optimization techniques like the
    normal equation can be used to find optimal parameters for ML algorithms such
    as linear regression. Unlike the iterative approach of gradient descent, the normal
    equation offers a one-step analytical solution for calculating the coefficients
    of linear regression models. Other ML algorithms may also have algorithm-specific
    optimization methods for model training, which will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of ML algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With that brief overview of the fundamental concepts behind how machines learn,
    let’s now explore various ML algorithms in more depth. The field of ML has seen
    the development of numerous algorithms, with ongoing research and innovation from
    both academia and industry. In this section, we will explore several well-known
    traditional and deep learning algorithms, examining their applications across
    various types of ML problems such as forecasting, recommendation, and natural
    language processing. Additionally, we will look at the strengths and weaknesses
    of different algorithms and discuss which situations each one is best suited for.
    This will help you build an understanding of the different capabilities of each
    algorithm and the types of problems they can be used to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Before we delve into these algorithms, it’s important to discuss the factors
    to consider when selecting an appropriate algorithm for a given task.
  prefs: []
  type: TYPE_NORMAL
- en: Consideration for choosing ML algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When choosing a ML algorithm, there are several key considerations to keep
    in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem type**: Different algorithms are better suited for different types
    of problems. For example, classification algorithms are suitable for tasks where
    the goal is to categorize data into distinct classes, while regression algorithms
    are used for predicting continuous numerical values. Understanding the problem
    type is crucial in selecting the most appropriate algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset size**: The size of your dataset can impact the choice of algorithm.
    Some algorithms perform well with small datasets, while others require large amounts
    of data to generalize effectively. If you have limited data, simpler algorithms
    with fewer parameters may be preferable to prevent overfitting. Overfitting is
    when a trained model that learns the training data too well but fails to generalize
    to new, unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature space**: Consider the number and nature of features in your dataset.
    Some algorithms can handle high-dimensional feature spaces, while others are more
    suitable for datasets with fewer features. Feature engineering and dimensionality
    reduction techniques can also be applied to enhance algorithm performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational efficiency**: The computational requirements of an algorithm
    should be taken into account, especially if you have large datasets or limited
    computational resources. Some algorithms are computationally expensive and may
    not be feasible for certain environments. Time complexity and space complexity
    are quantitative measures used to assess the efficiency of ML algorithms. Big
    *O* notation represents the upper bound estimation for time and space requirements.
    For example, linear search has a time complexity of *O(N)*, while binary search
    has *O(log N)*. Understanding these complexities helps evaluate algorithm efficiency
    and scalability, aiding in algorithm selection for specific tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretability**: Depending on your application, the interpretability of
    the algorithm’s results may be important. Some algorithms, such as decision trees
    or linear models, offer easily interpretable outcomes, while others, like deep
    neural networks, provide more complex and abstract representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithm complexity and assumptions**: Different algorithms make different
    assumptions about the underlying data distribution. Consider whether these assumptions
    are valid for your dataset. Additionally, the complexity of the algorithm can
    impact its ease of implementation, training time, and ability to handle noisy
    or incomplete data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By considering these factors, you can make an informed decision when selecting
    a ML algorithm that best suits your specific problem and available resources.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms for classification and regression problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The vast majority of ML problems today primarily involve classification and
    regression. Classification is a ML task that assigns categories or classes to
    data points, such as labeling a credit card transaction as fraudulent or not fraudulent.
    Regression, on the other hand, is a ML technique used to predict continuous numeric
    values, such as predicting the price of a house.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we’ll explore common algorithms used for classification
    and regression tasks. We will explain how each algorithm works, the types of problems
    each algorithm is suited for, and their limitations. This will help build intuition
    on when to select different algorithms for the different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Linear regression** algorithms are developed to solve regression problems
    by predicting continuous values based on independent inputs. They find wide applications
    in various practical scenarios, such as estimating product sales based on price
    or determining crop yield based on rainfall and fertilizer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear regression utilizes a linear function of a set of coefficients and input
    variables to predict a scalar output. The formula for the linear regression is
    expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_03_002.png)'
  prefs: []
  type: TYPE_IMG
- en: In the linear regression equation, the *X*[s] represent the input variables,
    *W*[s] denote the coefficients, and *![](img/B20836_03_new.png)* represents the
    error term. Linear regression aims to estimate the output value by calculating
    the weighted sum of the inputs, assuming a linear relationship between the output
    and inputs. The intuition behind linear regression is to find a line or hyperplane
    that can estimate the value for a set of input values. Linear regression can work
    efficiently with small datasets, offering interpretability through the coefficients’
    assessment of input and output variables. However, it may not perform well with
    complex, nonlinear datasets. Additionally, linear regression assumes independence
    among input features and struggles when there is co-linearity (the value of one
    feature influences the value of another feature), as it becomes challenging to
    assess the significance of correlated features.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Logistic regression** is commonly employed for binary and multi-class classification
    tasks. It can predict the probability of an event occurring, such as whether a
    person will click on an advertisement or qualify for a loan. Logistic regression
    is a valuable tool in real-world scenarios where the outcome is binary and requires
    estimating the likelihood of a particular class. By utilizing a logistic function,
    this algorithm maps the input variables to a probability score, enabling effective
    classification decision-making.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression is a statistical model used to estimate the probability
    of an event or outcome, such as transaction fraud or passing an exam. It is a
    linear model similar to linear regression, but with a different output transformation.
    The goal of logistic regression is to find a decision boundary, represented by
    a line or hyperplane, that effectively separates the two classes of data points.
    By applying a logistic function to the linear combination of input variables,
    logistic regression ensures that the predicted output falls within the range of
    0 and 1, representing the probability of belonging to a particular class. The
    following formula is the function for the logistic regression, where *X* is a
    linear combination of input variables (*b+wx*). Here, the w is the regression
    coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_03_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Like linear regression, logistic regression offers fast training speed and interpretability
    as its advantages. However, due to its linear nature, logistic regression is not
    suitable for solving problems with complex non-linear relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Decision trees** find extensive application in various real-world ML scenarios,
    including heart disease prediction, target marketing, and loan default prediction.
    They are versatile and can be used for both classification and regression problems.'
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree is motivated by the idea that data can be divided hierarchically
    based on rules, leading to similar data points following the same decision path.
    It achieves this by splitting the input data using different features at different
    branches of the tree. For example, if age is a feature used for splitting at a
    branch, a conditional check like age > 50 would be used to divide the data. The
    decision of which feature to use for splitting and where to split is made using
    algorithms such as the Gini purity index and information gain. The Gini index
    measures the probability of misclassification, while information gain quantifies
    the reduction in entropy resulting from the split.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we won’t delve into specific algorithm details. However, the general
    concept of decision tree involves experimenting with various split options and
    conditions, calculating metric values (e.g., information gain) for each split
    option, and selecting the option that yields the highest value. During prediction,
    input data traverses the tree based on the learned branching logic, and the final
    prediction is determined by the terminal node (leaf node). Refer to *Figure 3.2*
    for an example structure of a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Decision tree ](img/B20836_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Decision tree'
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of decision trees over linear regression and logistic regression
    is their ability to capture non-linear relationships and interactions between
    features. Decision trees can handle complex data patterns and are not limited
    to linear relationships between input variables and the output. They can represent
    decision boundaries that are more flexible and can handle both numerical and categorical
    features.
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree is advantageous in that it can handle data with minimal preprocessing,
    accommodate both categorical and numerical features, and handle missing values
    and varying feature scales. It is also highly interpretable, allowing for easy
    visualization and analysis of decision paths. Furthermore, decision trees are
    computationally efficient. However, they can be sensitive to outliers and prone
    to **overfitting**, particularly when dealing with a large number of features
    and noisy data. Overfitting occurs when the model memorizes the training data
    but performs poorly on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: A notable limitation of decision trees and tree-based algorithms is their inability
    to extrapolate beyond the range of training inputs. For instance, if a housing
    price model is trained on square footage data ranging from 500 to 3,000 sq ft,
    a decision tree would be unable to make predictions beyond 3,000 sq ft. In contrast,
    a linear model would be capable of capturing the trend and making predictions
    beyond the observed range.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Random forest** algorithm is widely employed in various real-world applications
    across e-commerce, healthcare, and finance sectors. They are particularly valuable
    for classification and regression tasks. Real-world examples of these tasks include
    insurance underwriting decisions, disease prediction, loan payment default prediction,
    and targeted marketing efforts. The versatility of random forest algorithms allows
    them to be applied in a wide range of industries to address diverse business challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the preceding decision tree section, a decision tree uses a
    single tree to make its decisions, and the root node of the tree (the first feature
    to split the tree) has the most influence on the final decision. The motivation
    behind this random forest is that combining the decisions of multiple trees can
    lead to improved overall performance. The way that a random forest works is to
    create multiple smaller **subtrees**, also called **weak learner trees**, where
    each subtree uses a random subset of all the features to come to a decision, and
    the final decision is made by either majority voting (for classification) or averaging
    (for regression). This process of combining the decision from multiple models
    is also referred to as **ensemble learning**. Random forest algorithms also allow
    you to introduce different degrees of randomness, such as **bootstrap sampling**,
    which involves using the same sample multiple times in a single tree. This helps
    make the model more generalized and less prone to overfitting. The following figure
    illustrates how the random forest algorithm processes input data instances using
    multiple subtrees and combines their outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Random forest ](img/B20836_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Random forest'
  prefs: []
  type: TYPE_NORMAL
- en: Random forests have several advantages over decision trees. They provide improved
    accuracy by combining the predictions of multiple trees through majority voting
    or averaging. They also reduce overfitting by introducing randomness in the model
    and using diverse subsets of features. Random forests handle large feature sets
    better by focusing on different aspects of the data. They are robust to outliers
    and provide feature importance estimation. Additionally, random forests support
    parallel processing for training large datasets across multiple machines. The
    limitations of random forests include reduced interpretability compared to decision
    trees, longer training and prediction times, increased memory usage, and the need
    for hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting machine and XGBoost algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gradient boosting and XGBoost are also popular multi-tree-based ML algorithms
    used in various domains like credit scoring, fraud detection, and insurance claim
    prediction. Unlike random forests that combine results from weak learner trees
    at the end, gradient boosting sequentially aggregates results from different trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Random forests utilize parallel independent weak learners, while gradient boosting
    employs a sequential approach where each weak learner tree corrects the errors
    of the previous tree. Gradient boosting offers more hyperparameters to fine-tune
    and can achieve superior performance with proper tuning. It also allows for custom
    loss functions, providing flexibility in modeling real-world scenarios. Refer
    to the following figure for an illustration of how gradient boosting trees operate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Gradient boosting ](img/B20836_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Gradient boosting'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting offers several key advantages. Firstly, it excels in handling
    imbalanced datasets, making it highly suitable for tasks such as fraud detection
    and risk management. Secondly, it has the potential to achieve higher performance
    than other algorithms when properly tuned. Additionally, gradient boosting supports
    custom loss functions, providing flexibility in modeling real-world applications.
    Lastly, it can effectively capture complex relationships in the data and produce
    accurate predictions. Gradient boosting, despite its advantages, also has some
    limitations to consider. Firstly, due to its sequential nature, it lacks parallelization
    capabilities, making it slower in training compared to algorithms that can be
    parallelized. Secondly, gradient boosting is sensitive to noisy data, including
    outliers, which can lead to overfitting and reduced generalization performance.
    Lastly, the complexity of gradient boosting models can make them less interpretable
    compared to simpler algorithms like decision trees, making it challenging to understand
    the underlying relationships in the data.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost, a widely-used implementation of gradient boosting, has gained popularity
    for its success in Kaggle competitions. While it shares the same underlying concept
    as gradient boosting, XGBoost offers several improvements. It enables training
    a single tree across multiple cores and CPUs, leading to faster training times.
    XGBoost incorporates powerful regularization techniques to mitigate overfitting
    and reduce model complexity. It also excels in handling sparse datasets. In addition
    to XGBoost, other popular variations of gradient boosting trees include LightGBM
    and CatBoost.
  prefs: []
  type: TYPE_NORMAL
- en: K-nearest neighbor algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**K-nearest neighbor** (**K-NN**) is a versatile algorithm used for both classification
    and regression tasks. It is also employed in search systems and recommendation
    systems. The underlying assumption of K-NN is that similar items tend to have
    close proximity to each other in the feature space. To determine this proximity,
    distances between different data points are measured, often using metrics like
    Euclidean distance.'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of classification, K-NN starts by loading the training data along
    with their respective class labels. When a new data point needs to be classified,
    its distances to the existing data points are calculated, typically using Euclidean
    distance. The K nearest neighbors to the new data point are identified, and their
    class labels are retrieved. The class label for the new data point is then determined
    through majority voting, where the most frequent class among the K nearest neighbors
    is assigned to the new data point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration of using K-NN for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram of a diagram  Description automatically generated](img/B20836_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: K-NN for classification'
  prefs: []
  type: TYPE_NORMAL
- en: For regression tasks, K-NN follows a similar approach. The distances between
    the new data point and the existing data points are computed, and the K nearest
    neighbors are selected. The predicted scalar value for the new data point is obtained
    by averaging the values of the K closest data points.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of K-NN is its simplicity and lack of the need for training or
    tuning with hyperparameters, apart from selecting the value of K. The dataset
    is loaded directly into the model without the need to train a model. It is worth
    noting that the choice of K significantly impacts the performance of the K-NN
    mode. The optimal K is often found through an iterative trial-and-error process
    by evaluation of hold-out dataset. The results of K-NN are also easily explainable,
    as each prediction can be understood by examining the properties of the nearest
    neighbors. However, K-NN has some limitations.
  prefs: []
  type: TYPE_NORMAL
- en: As the number of data points increases, the complexity of the model grows, and
    predictions can become slower, especially with large datasets. K-NN is not suitable
    for high-dimensional datasets, as the concept of proximity becomes less meaningful
    in higher-dimensional spaces. The algorithm is also sensitive to noisy data and
    missing data, requiring outlier removal and data imputation techniques to handle
    such cases effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer perceptron (MLP) networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned earlier, an **artificial neural network** (**ANN**) emulates the
    learning process of the human brain. The brain comprises numerous interconnected
    neurons that process information. Each neuron in a network processes inputs (electrical
    impulses) from another neuron, processes and transforms the inputs, and sends
    the output to neurons in the network. Here is an illustration depicting a human
    neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Human brain neuron ](img/B20836_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Human brain neuron'
  prefs: []
  type: TYPE_NORMAL
- en: An artificial neuron operates in a similar manner. The following diagram illustrates
    an artificial neuron, which consists of a linear function combined with an activation
    function. The activation function modifies the output of the linear function,
    such as compressing it within a specific range, such as 0 to 1 (sigmoid activation),
    -1 to 1 (tanh activation), or maintaining values above 0 (ReLU). The activation
    function is employed to capture non-linear relationships between inputs and outputs.
    Alternatively, each neuron can be viewed as a linear classifier, akin to logistic
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Artificial neuron ](img/B20836_03_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Artificial neuron'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you stack a large number of neurons into different layers (*input layer*,
    *hidden layers*, and *output layer*) and connect all of the neurons together between
    two adjacent layers, we have an ANN called **multi-layer perceptron** (**MLP**).
    Here, the term *perceptron* means *artificial neuron*, and it was originally invented
    by Frank Rosenblatt in 1957\. The idea behind MLP is that each hidden layer will
    learn some higher-level representation (features) of the previous layer, and those
    higher-level features capture the more important information in the previous layer.
    When the output from the final hidden layer is used for prediction, the network
    has extracted the most important information from the raw inputs for training
    a classifier or regressor. The following figure shows the architecture of an MLP
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Multi-layer perceptron ](img/B20836_03_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Multi-layer perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: During model training, the weights (*W*) of each neuron in every layer are adjusted
    using gradient descent to optimize the training objective. This adjustment process
    is known as backpropagation. It involves propagating the total error back through
    the network, attributing a portion of the error to each neuron based on its contribution.
    This allows the fine-tuning of the weights in each neuron, ensuring that every
    neuron in every layer influences the final output to improve overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: MLP is a versatile neural network suitable for both classification and regression
    tasks, similar to random forest and XGBoost. While commonly applied to tabular
    data, it can also handle diverse data formats like images and text. MLP excels
    in capturing intricate nonlinear patterns within the dataset and exhibits efficient
    computational processing, thanks to its parallelization capabilities. However,
    MLP typically demands a larger training dataset to achieve optimal performance
    compared to traditional ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms for clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering is a data mining method that involves grouping items together based
    on their shared attributes. One practical application of clustering is to create
    customer segments by analyzing demographics, transaction history, or behavior
    data. Other examples include social network analysis, document grouping, and anomaly
    detection. Various clustering algorithms exist, and we will focus on the K-means
    clustering algorithm in this section, which is one of the most widely used clustering
    algorithms due to its simplicity. Some other popular clustering algorithms are
    hierarchical clustering and DBSCAN.
  prefs: []
  type: TYPE_NORMAL
- en: K-means algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The K-means algorithm is widely employed in real-world applications, including
    customer segmentation analysis, document classification based on document attributes,
    and insurance fraud detection. It is a versatile algorithm that can effectively
    group data points in various domains for different purposes.
  prefs: []
  type: TYPE_NORMAL
- en: K-means aims to group similar data points together in clusters, and it is an
    unsupervised algorithm, meaning it doesn’t rely on labeled data. The algorithm
    begins by randomly assigning K centroids, which represent the centers of the clusters.
    It then iteratively adjusts the assignment of data points to the nearest centroid
    and updates the centroids to the mean of the data points in each cluster. This
    process continues until convergence, resulting in well-defined clusters based
    on similarity.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering offers several advantages, including its simplicity and ease
    of understanding, making it accessible to beginners. It is computationally efficient
    and can handle large datasets effectively. The resulting clusters are interpretable,
    providing valuable insights into the underlying patterns in the data. K-means
    is versatile and applicable to various types of data, including numerical, categorical,
    and mixed attribute datasets. However, there are some drawbacks to consider. Selecting
    the optimal number of clusters (*K*) can be subjective and challenging. The algorithm
    is sensitive to the initial placement of centroids, which can lead to different
    cluster formations. K-means assumes spherical clusters with equal variance, which
    may not hold true in all cases. It is also sensitive to outliers and struggles
    with non-linear data relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms for time series analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A time series consists of a sequence of data points recorded at successive time
    intervals. It is commonly used to analyze and predict trends in various domains,
    such as finance, retail, and sales. Time series analysis allows us to understand
    past patterns and make future predictions based on the relationship between current
    and past values. Forecasting in time series relies on the assumption that future
    values are influenced by previous observations at different time points.
  prefs: []
  type: TYPE_NORMAL
- en: Time series data exhibits several important characteristics, including trend,
    seasonality, and stationarity. **Trend** refers to the long-term direction of
    the data, whether it shows an overall increase or decrease over time. It helps
    to identify the underlying pattern and understand the general behavior of the
    time series. **Seasonality**, on the other hand, captures repeating patterns within
    a fixed interval, often occurring in cycles or seasons. It helps to identify regular
    fluctuations that repeat over specific time periods, such as daily, weekly, or
    yearly patterns. **Stationarity** refers to the property of a time series where
    statistical properties, such as mean and variance, remain constant over time.
    Stationarity is crucial because many forecasting techniques assume that the underlying
    data is stationary. Non-stationary time series can lead to inaccurate or unreliable
    forecasts. Therefore, it is important to assess and address the stationarity of
    a time series before applying forecasting techniques.
  prefs: []
  type: TYPE_NORMAL
- en: ARIMA algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **autoregressive integrated moving average** (**ARIMA**) algorithm finds
    practical applications in various real-world scenarios, including budget forecasting,
    sales forecasting, patient visit forecasting, and customer support call volume
    forecasting. ARIMA is a powerful tool for analyzing and predicting time series
    data, allowing organizations to make informed decisions and optimize their operations
    in these areas. By leveraging historical patterns and trends in the data, ARIMA
    enables accurate forecasts and assists businesses in effectively managing their
    resources and planning for the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'ARIMA operates on the premise that the value of a variable in a given period
    is influenced by its own previous values (autoregressive), the deviations from
    the mean follow a pattern based on previous deviations (moving average), and trend
    and seasonality can be eliminated by differencing (calculating the differences
    between consecutive data points). This differencing process aims to transform
    the time series into a stationary state, where statistical properties like mean
    and variance remain constant over time. These three components of ARIMA can be
    mathematically represented using the following formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_03_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where the **autoregressive** (**AR**) component is expressed as a regression
    of previous values (also known as lags):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_03_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The constant *C* represents a drift:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_03_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **moving average** (**MA**) component is expressed as a weighted average
    of forecasting errors for the previous time periods, where it represents a constant:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_03_007.png)'
  prefs: []
  type: TYPE_IMG
- en: The **integrated component** (time series differencing) of a time series can
    be expressed as the difference between the values in one period from the previous
    period.
  prefs: []
  type: TYPE_NORMAL
- en: ARIMA is a suitable choice for forecasting single time series (univariate) data
    as it doesn’t rely on additional variables. It outperforms simpler forecasting
    techniques like simple moving average, exponential smoothing, or linear regression.
    Additionally, ARIMA provides interpretability, allowing for a clear understanding
    of the underlying patterns. However, due to its backward-looking nature, ARIMA
    may struggle to accurately forecast unexpected events. Furthermore, being a linear-based
    model, ARIMA may not effectively capture complex non-linear relationships in time
    series data.
  prefs: []
  type: TYPE_NORMAL
- en: DeepAR algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning-based forecasting algorithms offer solutions to the limitations
    of traditional models like ARIMA. They excel at capturing complex non-linear relationships
    and can effectively utilize multivariate datasets. These models enable the training
    of a global model, allowing for a single model to handle multiple similar target
    time series. This eliminates the need for creating separate models for each individual
    time series, providing a more efficient and scalable approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep Autoregressive** (**DeepAR**) is a state-of-the-art forecasting algorithm
    based on neural networks, designed to handle large datasets with multiple similar
    target time series. It has the capability to incorporate related time series,
    such as product prices or holiday schedules, to enhance the accuracy of its forecasting
    models. This feature proves particularly valuable when dealing with spiky events
    triggered by external variables, allowing for more precise and reliable predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: DeepAR utilizes a **recurrent neural network** (**RNN**) as its underlying model
    to capture patterns in the target time series. It goes beyond single-variable
    forecasting by incorporating multiple target time series and additional external
    supporting time series. Instead of considering individual values, the RNN takes
    input vectors representing the values of various variables at each time period.
    By jointly learning the patterns of these combined vectors over time, DeepAR can
    effectively capture the intrinsic non-linear relationships and shared patterns
    among the different time series. This approach enables DeepAR to train a single
    global model that can be used for forecasting across multiple similar target time
    series.
  prefs: []
  type: TYPE_NORMAL
- en: DeepAR excels in handling complex multivariate datasets; however, it performs
    best when trained with large amounts of data. It is particularly useful in real-world
    scenarios involving large-scale retail forecasting for numerous items, where external
    factors like marketing campaigns and holiday schedules need to be taken into account.
    By leveraging its capability to model multiple variables simultaneously, DeepAR
    can provide accurate predictions and insights in such practical use cases.
  prefs: []
  type: TYPE_NORMAL
- en: A significant drawback of DeepAR is the black-box nature of the deep learning
    model, which lacks interpretability and transparency. This makes the forecasts
    more difficult to explain and justify than simpler statistical methods. Another
    major disadvantage is the data-hungry nature of DeepAR, whereby it performs poorly
    when the dataset is small.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms for recommendation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recommender system is an essential ML technology that predicts a user’s
    preference for items, primarily relying on user or item attribute similarities
    or user-item interactions. It has gained widespread adoption in various industries,
    including retail, media and entertainment, finance, and healthcare. Over the years,
    the field of recommendation algorithms has evolved significantly, from making
    recommendations based on preferences and behaviors of similar users, to a reinforcement-learning-based
    approach where algorithms learn to make sequential decisions over time, taking
    into account user feedback and interaction. In the following section, we will
    explore some commonly used algorithms in the realm of recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Collaborative filtering** is a popular recommendation algorithm that leverages
    the notion that individuals with similar interests or preferences in one set of
    items are likely to have similar interests in other items as well. By analyzing
    the collective experiences and behaviors of different users, collaborative filtering
    can effectively recommend items to individual users based on the preferences of
    similar users. This approach taps into the experiences of the crowd to provide
    personalized and relevant recommendations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates an item-user interaction matrix in the context
    of movie ratings. As you can see, it is a **sparse matrix**. This means that there
    are many empty entries in the matrix, which is expected as it is unlikely for
    any individual to have watched every movie:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – User-item interaction matrix for collaborative filtering ](img/B20836_03_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: User-item interaction matrix for collaborative filtering'
  prefs: []
  type: TYPE_NORMAL
- en: One of the major benefits of collaborative filtering is that it can provide
    highly personalized recommendations matched to each user’s unique interests. Unlike
    content-based systems, collaborative filtering models do not need to analyze and
    understand item features and content. Instead, they rely solely on behavioral
    patterns like ratings, purchases, clicks, and preferences across users to uncover
    correlations. This allows collaborative systems to get a nuanced profile of a
    user’s likes and dislikes based on crowd wisdom. The algorithms can then generate
    recommendations tailored to that specific user, going beyond obvious suggestions.
    This level of personalization and ability to capture user preferences makes collaborative
    filtering a powerful approach, especially for large catalogs where analyzing content
    is infeasible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Collaborative filtering also comes with some notable downsides. A major issue
    is the cold-start problem: collaborative models struggle when new users or items
    with no ratings are introduced. The algorithms rely heavily on crowd ratings,
    so they cannot effectively recommend to users new items that lack this historical
    data. Collaborative systems can also lead to limited diversity, creating filter
    bubbles and obvious recommendations rather than novel ones. They commonly face
    sparsity issues as the user-item matrix is often sparse, especially for large
    catalogs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Matrix factorization** is a technique commonly used in collaborative filtering
    for recommendation systems. It involves learning vector representations, or embeddings,
    for both users and items in the user-item interaction matrix. The goal is to approximate
    the original matrix by taking the product of the learned user and item embedding
    matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: This allows us to predict the missing entries in the matrix, which represent
    the likely ratings that a user would give to unseen items. To make predictions,
    we simply compute the dot product between the user embedding and the item embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding is a fundamental concept in ML that plays a crucial role in various
    domains. It involves creating numerical representations for entities, such as
    words or objects, in a manner that captures their semantic similarity. These representations
    are organized in a multi-dimensional space, where similar entities are positioned
    closer to each other. By using embeddings, we can uncover the underlying latent
    semantics of the objects, enabling more effective analysis and modeling. In the
    upcoming sections, we will delve deeper into embedding techniques and their applications
    in NLP algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix factorization provides major scalability benefits so collaborative filtering
    can be applied to extremely large catalogs. However, the algorithm loses some
    transparency due to the latent factor modeling. Overall, matrix factorization
    extends collaborative filtering to much bigger datasets but sacrifices some interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-armed bandit/contextual bandit algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Collaborative filtering-based recommender systems heavily rely on prior interaction
    data between identified users and items to make accurate recommendations. However,
    these systems face challenges when there is a lack of prior interactions or when
    the user is anonymous, resulting in a cold-start problem. To address this issue,
    one approach is to utilize a **multi-armed bandit** (**MAB**) based recommendation
    system. This approach draws inspiration from the concept of trial and error, similar
    to a gambler simultaneously playing multiple slot machines and observing which
    machine yields the best overall return. By employing reinforcement learning techniques,
    MAB-based recommendation systems dynamically explore and exploit different recommendations
    to optimize the user experience, even in the absence of substantial prior interaction
    data.
  prefs: []
  type: TYPE_NORMAL
- en: MAB algorithms operate under the paradigm of online learning, where there is
    no pre-existing training data to train a model prior to deployment. Instead, the
    model incrementally learns and adapts as data becomes available. In the initial
    stages of MAB learning, the model recommends all available options (such as products
    on an e-commerce site) with equal probabilities to users. As users begin to interact
    with a subset of the items and provide feedback (rewards), the MAB model adjusts
    its strategy. It starts to offer items that have yielded higher rewards (e.g.,
    more user interactions) more frequently, exploiting the knowledge of their positive
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: However, the model also continues to allocate a smaller percentage of recommendations
    to new items, aiming to explore their potential for receiving interactions. This
    balance between exploration (offering new items) and exploitation (offering items
    with known rewards) is a fundamental tradeoff in MAB algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: MAB algorithms face several limitations. Striking the right balance between
    exploration and exploitation can be challenging, leading to suboptimal solutions
    in certain environments. Handling high-dimensional contextual information poses
    a challenge as well, and the algorithms may be sensitive to noisy rewards. Additionally,
    the cold-start problem arises when there is limited historical data for new items
    or users.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms for computer vision problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computer vision refers to the ability of computers to interpret and understand
    visual representations, such as images and videos, in order to perform tasks like
    object identification, image classification, text detection, face recognition,
    and activity detection. These tasks rely on pattern recognition, where images
    are labeled with object names and bounding boxes, and computer vision models are
    trained to recognize these patterns and make predictions on new images. Computer
    vision technology finds numerous applications in practical domains such as content
    management, security, augmented reality, self-driving cars, medical diagnosis,
    sports analytics, and quality inspection in manufacturing. In the following section,
    we will delve deeper into a few neural network architectures specifically designed
    for computer vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Although the upcoming sections involve deep learning architectures, embeddings,
    and other techniques—elements that may not strictly conform to the traditional
    definition of algorithms—we will be referring to them as “algorithms” for the
    sake of semantic consistency throughout this chapter. With this, we hope to facilitate
    a smoother understanding of the nuanced concepts we’ll be exploring.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **convolutional neural network** (**CNN**) is a deep learning architecture
    specifically designed for processing and analyzing image data. It takes inspiration
    from the functioning of the animal visual cortex. In the visual cortex, individual
    neurons respond to visual stimuli within specific subregions of the visual field.
    These subregions, covered by different neurons, partially overlap to cover the
    entire visual field. Similarly, in a CNN, different filters are applied to interact
    with subregions of an image, capturing and responding to the information within
    that region. This allows the CNN to extract meaningful features and patterns from
    the image data.
  prefs: []
  type: TYPE_NORMAL
- en: A CNN architecture consists of multiple layers that repeat in a pattern. Each
    layer has different sublayers with specific functions. The convolutional layer
    plays a crucial role in feature extraction from input images. It utilizes convolutional
    filters, which are matrices defined by height and width, to extract relevant features.
    These convolutional layers process the input images by convolving them with the
    filters, producing feature maps that are passed to the next layer in the network.
  prefs: []
  type: TYPE_NORMAL
- en: The pooling layer, found after one or multiple convolutional layers, reduces
    the dimensionality of the extracted features. It combines multiple outputs into
    a single output, resulting in a more compact representation. Two commonly used
    pooling techniques are max pooling, which selects the maximum value from the outputs,
    and average pooling, which calculates the average value.
  prefs: []
  type: TYPE_NORMAL
- en: Following the convolutional and pooling layers, a fully connected layer is employed
    to combine and flatten the outputs from the previous layer. This layer aggregates
    the extracted features and feeds them into an output layer, typically used for
    tasks like image classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of a CNN is illustrated in the following figure, showcasing
    the flow of information through the various layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – CNN architecture ](img/B20836_03_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: CNN architecture'
  prefs: []
  type: TYPE_NORMAL
- en: CNN-based models offer efficient training due to their high degree of parallelism.
    This is particularly advantageous for tasks involving large-scale image data,
    where parallel processing can significantly accelerate training time. While CNNs
    are primarily used for computer vision tasks, their success has led to their application
    in other domains as well, including natural language processing. By adapting the
    principles of convolution and hierarchical feature extraction, CNNs have shown
    promise in tasks such as text classification and sentiment analysis. This demonstrates
    the versatility and effectiveness of CNN-based models beyond their traditional
    application in computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs have their limitations. CNNs lack interpretability due to their complex
    architecture, behaving like black boxes. This makes them unsuitable when model
    explainability is critical. In addition, CNNs require large training datasets
    to properly learn features and avoid overfitting. Their performance suffers significantly
    on smaller datasets.
  prefs: []
  type: TYPE_NORMAL
- en: ResNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As computer vision tasks grow in complexity, the addition of more layers in
    CNNs enhances their capability for image classification by enabling the learning
    of increasingly intricate features. However, as the number of layers increases
    in a CNN architecture, performance may deteriorate. This is commonly referred
    to as the **vanishing gradient** problem, where signals originating from the initial
    inputs, including crucial information, gradually diminish as they traverse through
    multiple layers of the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: '**Residual networks** (**ResNet**) address the vanishing gradient problem by
    implementing a layer-skipping technique. Rather than processing signals sequentially
    through each layer, ResNet introduces skip connections that allow signals to bypass
    certain layers. This can be visualized as a highway with fewer exits, enabling
    the signals from earlier layers to be preserved and carried forward without loss.
    The ResNet architecture is depicted in the following figure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – ResNet architecture ](img/B20836_03_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: ResNet architecture'
  prefs: []
  type: TYPE_NORMAL
- en: ResNet can be used for different computer vision tasks such as *image classification*,
    *object detection* (detecting all objects in a picture), and producing models
    with much higher accuracy than a vanilla CNN network. However, a potential disadvantage
    of ResNet is increased computational complexity due to the introduction of skip
    connections. The additional connections require more memory and computational
    resources, making training and inference more computationally expensive compared
    to shallower architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms for natural language processing (NLP) problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLP focuses on the relationship between computers and human language. It involves
    the processing and analysis of extensive amounts of natural language data with
    the objective of enabling computers to comprehend the meaning behind human language
    and extract valuable information from it. NLP encompasses a wide range of tasks
    within the field of data science. Some of these tasks include document classification,
    topic modeling, converting speech to text, generating speech from text, extracting
    entities from text, language translation, understanding and answering questions,
    reading comprehension, and language generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'ML algorithms cannot process raw text data directly. To train NLP models effectively,
    it is necessary to convert the words within an input text into numerical representations
    within the context of other words, sentences, or documents. Before the advancement
    of embedding, there were two widely used methods for representing the relevance
    of words in a text: **bag-of-words** (**BOW**) and term **frequency–inverse document
    frequency** (**TF-IDF**).'
  prefs: []
  type: TYPE_NORMAL
- en: BOW is simply the count of a word appearing in a text (document). For example,
    if the input documents are `I need to go to the bank to make a deposit` and `I
    am taking a walk along the river bank`, and you count the number of appearances
    for each unique word in each input document, you will get *1* for the word *I*,
    and *3* for the word *to* in the first document, as an example. If we have a vocabulary
    for all the unique words in the two documents, the vector representation for the
    first document can be `[1 1 3 1 1 1 1 1 1 0 0 0 0 0]`, where each position represents
    a unique word in the vocabulary (for example, the first position represents the
    word *I*, and the third position represents the word *to*). Now, this vector can
    be fed into an ML algorithm to train a model such as text classification. The
    main idea behind BOW is that a word that appears more frequently has stronger
    weights in a text.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF has two components. The first component, *TF*, is the ratio of the number
    of times a vocabulary word appears in a document over the total number of words
    in the document. Using the preceding first document, the word *I* would have a
    TF value of *1/11* for the first sentence, and the word *walk* would have a TF
    value of *0/11*, since *walk* does not appear in the first sentence. While TF
    measures the importance of a word in the context of one text, the IDF component
    measures the importance of a word across all the documents. Mathematically, it
    is the log of the ratio of the number of documents over the number of documents
    where a word appears. The final value of TF-IDF for a word would be the *TF* term
    multiplied by the *IDF* term. In general, TF-IDF works better than BOW.
  prefs: []
  type: TYPE_NORMAL
- en: Although BOW and TF-IDF are useful for NLP tasks, they lack the ability to capture
    the semantic meaning of words and often result in large and sparse input vectors.
    This is where the concept of embedding plays a crucial role.
  prefs: []
  type: TYPE_NORMAL
- en: 'Embedding is a technique used to generate low-dimensional representations (mathematical
    vectors) for words or sentences, which capture the semantic meaning of the text.
    The underlying idea is that words or sentences with similar semantic meanings
    tend to occur in similar contexts. In a multi-dimensional space, the mathematical
    representations of semantically similar entities are closer to each other than
    those with different meanings. For instance, if we consider sports-related words
    like soccer, tennis, and bike, their embeddings would be close to each other in
    the high-dimensional embedding space, measured by metrics like cosine similarity,
    which measures how similar two vectors are by calculating the cosine of the angle
    between them. The embedding vector represents the intrinsic meaning of the word,
    with each dimension representing a specific attribute associated with the word.
    Visualizing embeddings in a multidimensional space shows the proximity of related
    entities. The following diagram provides a visual depiction of the closeness in
    this multidimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Embedding representation ](img/B20836_03_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: Embedding representation'
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, embeddings have become a crucial component for achieving good results
    in most NLP tasks. Compared to other techniques like simple word counts, embeddings
    offer more meaningful representations of the underlying text. This has led to
    their widespread adoption in various ML algorithms designed for NLP. In this section,
    we will delve into several of these algorithms such as BERT and GPT, exploring
    their specific applications and benefits in the context of NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Thomas Mikolov created **Word2Vec** in 2013\. It supports two different techniques
    for learning embedding: **continuous bag-of-words** (**CBOW**) and **continuous-skip-gram**.
    CBOW tries to predict a word for a given window of surrounding words, and continuous-skip-gram
    tries to predict surrounding words for a given word. The training dataset for
    Word2Vec could be any running text available, such as **Wikipedia**. The process
    of generating a training dataset for CBOW is to run a sliding window across running
    text (for example, a window of five words) and choose one of the words as the
    target and the rest as inputs (the order of words is not considered). In the case
    of continuous-skip-gram, the target and inputs are reversed. With the training
    dataset, the problem can be turned into a multi-class classification problem,
    where the model will learn to predict the classes (for example, words in the vocabulary)
    for the target word and assign each predicted word with a probability distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec embeddings can be trained using a straightforward one-hidden-layer
    MLP network. In this approach, the input to the MLP network is a matrix that represents
    the neighboring words, while the output is a probability distribution for the
    target words. During training, the weights of the hidden layer are optimized,
    and once the training process is complete, these weights serve as the actual embeddings
    for the words. The resulting embeddings capture the semantic relationships and
    contextual meanings of the words, enabling them to be effectively utilized in
    various natural language processing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: As large-scale word embedding training can be expensive and time-consuming,
    Word2Vec embeddings are usually trained as a pre-training task so that they can
    be readily used for downstream tasks such as text classification or entity extraction.
    This approach of using embeddings as features for downstream tasks is called a
    **feature-based application**. There are pre-trained embeddings (for example,
    Tomas Mikolov’ *Word2Vec* and Stanford’s *GloVe*) in the public domain that can
    be used directly. The embeddings are a *1:1* mapping between each word and its
    vector representation.
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Word2Vec produces a fixed embedding representation for each word in the vocabulary,
    disregarding the contextual variations in meaning. However, words can have different
    meanings depending on the specific context in which they are used. For instance,
    the word “bank” can refer to a financial institution or the land alongside a body
    of water. To address this issue, contextualized word embeddings have been developed.
    These embeddings take into account the surrounding words or the overall context
    in which a word appears, allowing for a more nuanced and context-aware representation.
    By considering context, these embeddings capture the diverse meanings a word can
    have, enabling more accurate and context-specific analyses in downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**BERT**, which stands for **Bidirectional Encoder Representations from Transformers**,
    is a language model that takes context into consideration by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting randomly masked words in sentences (the context) and taking the order
    of words into consideration. This is also known as **language modeling**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the next sentence from a given sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Released in 2018, this context-aware embedding approach provides better representation
    for words and can significantly improve language tasks such as *reading comprehension*,
    *sentiment analysis*, and *named entity recognition*. Additionally, BERT generates
    embeddings at subword levels (a segment between a word and a character, for example,
    the word *embeddings* is broken up into *em*, *bed*, *ding*, and *s*). This allows
    it to handle the **out-of-vocabulary** (**OOV**) issue, another limitation of
    Word2Vec, which only generates embeddings on known words and will treat OOV words
    simply as unknown.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain word embeddings with BERT, the process differs from the straightforward
    word-to-vector mapping used in Word2Vec. Instead, sentences are inputted into
    a pre-trained BERT model, and embeddings are extracted dynamically. This approach
    generates embeddings that are contextualized within the context of the given sentences.
    Besides word-level embeddings, BERT is also capable of producing embeddings for
    entire sentences. **Pre-training** is the term used to describe the process of
    learning embeddings using input tokens, and the following diagram illustrates
    the components involved in a BERT model for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – BERT model pre-training ](img/B20836_03_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: BERT model pre-training'
  prefs: []
  type: TYPE_NORMAL
- en: 'Architecturally, BERT mainly uses a building block called a **transformer**.
    A transformer has a stack of encoders and a stack of decoders inside it, and it
    transforms one sequence of inputs into another sequence. Each encoder has two
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: A self-attention layer mainly calculates the strength of the connection between
    one token (represented as a vector) and all other tokens in the input sentence,
    and this connection helps with the encoding of each token. One way to think about
    self-attention is which words in a sentence are more connected than other words
    in a sentence. For example, if the input sentence is *The dog crossed a busy street*,
    then we would say the words *dog* and *crossed* have stronger connections with
    the word *The* than the word *a* and *busy*, which would have strong connections
    with the word *street*. The output of the self-attention layer is a sequence of
    vectors; each vector represents the original input token as well as the importance
    it has with other words in the inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A feed-forward network layer (single hidden layer MLP) extracts higher-level
    representation from the output of the self-attention layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inside the decoder, there is also a self-attention layer and feed-forward layer,
    plus an extra encoder-decoder layer that helps the decoder to focus on the right
    places in the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of BERT, only the encoder part of the transformer is used. BERT
    can be used for a number of NLP tasks, including *question answering*, *text classification*,
    *named entity extraction*, and *text summarization*. It achieved state-of-the-art
    performance in many of the tasks when it was released. BERT pre-training has also
    been adopted for different domains, such as scientific text and biomedical text,
    to understand domain-specific languages. The following figure showcases how a
    pre-trained BERT model is used to train a model for a question-answering task
    using the fine-tuning technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – BERT fine-tuning ](img/B20836_03_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: BERT fine-tuning'
  prefs: []
  type: TYPE_NORMAL
- en: While BERT’s pre-trained embeddings can be extracted for downstream tasks such
    as text classification and question answering, a more straightforward way to use
    its pre-trained embeddings is through a technique called **fine-tuning**. With
    fine-tuning, an additional output layer is added to the BERT network to perform
    a specific task, such as question answering or entity extraction. During fine-tuning,
    the pre-trained model is loaded, and you plug in the task-specific input (for
    example, question/passage pairs in question answering) and output (start/end and
    span for the answers in the passage) to fine-tune a task-specific model. With
    fine-tuning, the pre-trained model weights are updated.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although technologies like ChatGPT have popularized Generative AI, the concept
    of generative models is not new. **Generative Adversarial Networks** (**GANs**),
    a prominent example of generative AI technology, have been around for years and
    have been successfully applied in various real-world domains, with image synthesis
    being a notable application. Generative AI has become one of the most transformative
    AI technologies, and I have devoted the entire *Chapter 15 and 16* to delve deeper
    into real-world generative AI use cases, practical technology solutions, and ethical
    considerations. In this chapter, we will familiarize ourselves with several generative
    AI algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The GAN is a type of generative model designed to generate realistic data instances,
    such as images. It employs a two-part network consisting of a Generator and a
    Discriminator. The Generator network is responsible for generating instances,
    while the Discriminator network learns to distinguish between real and fake instances
    generated by the Generator. This adversarial setup encourages the Generator to
    continually improve its ability to produce increasingly realistic data instances.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Generative adversarial network  ](img/B20836_03_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15: GAN'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the training process, the Discriminator network in a GAN is exposed
    to two different data sources: one from a real dataset, which serves as positive
    examples, and one from the Generator network, which generates synthetic or fake
    samples. The Discriminator is trained to classify and distinguish between the
    real and fake samples, optimizing its loss to accurately predict the source of
    each sample. Conversely, the Generator network is trained to produce synthetic
    data that appears indistinguishable from real data, aiming to deceive the Discriminator.
    The Generator is penalized when the Discriminator correctly identifies its generated
    data as fake.'
  prefs: []
  type: TYPE_NORMAL
- en: Both networks learn and update their parameters using backpropagation, enabling
    them to improve iteratively. During the generation phase, the Generator is provided
    with random inputs to produce new synthetic samples. Throughout training, the
    Generator and Discriminator networks are alternately trained in a connected manner,
    allowing them to learn and optimize their performance as a unified system.
  prefs: []
  type: TYPE_NORMAL
- en: GANs have had a lot of success in generating realistic images that can fool
    humans. They can be applied to many applications, such as translating sketches
    to realistic-looking images, converting text inputs and generating images that
    correspond to the text, and generating realistic human faces. However, getting
    GANs to converge and stabilize during training can be difficult, leading to issues
    like failure to learn. Additionally, newer technologies have emerged in realistic
    image generation, which are a lot more capable than GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Generative pre-trained transformer (GPT)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike BERT, which requires fine-tuning using a large domain-specific dataset
    for the different downstream NLP tasks, the **Generative Pre-trained Transformer**
    (**GPT**), developed by **OpenAI**, can learn how to perform a task with just
    seeing a few examples (or no example). This learning process is called **few-shot
    learning** or **zero-shot learning**. In a few-shot scenario, the GPT model is
    provided with a few examples, a task description, and a prompt, and the model
    will use these inputs and start to generate output tokens one by one. For instance,
    when using GPT-3 for translation, the task description could be “translate English
    to Chinese,” and the training data would consist of a few examples of Chinese
    sentences translated from English sentences. To translate a new English sentence
    using the trained model, you provide the English sentence as a prompt, and the
    model generates the corresponding translated text in Chinese. It’s important to
    note that few-shot or zero-shot learning does not involve updating the model’s
    parameter weights, unlike the fine-tuning technique.
  prefs: []
  type: TYPE_NORMAL
- en: GPT, like BERT, utilizes the Transformer architecture as its primary component
    and employs a training approach called next word prediction. This entails predicting
    the word that should follow a given sequence of input words. However, GPT diverges
    from BERT in that it exclusively employs the Transformer decoder block, whereas
    BERT employs the Transformer encoder block. Like BERT, GPT incorporates masked
    words to learn embeddings. However, unlike BERT, which randomly masks words and
    predicts the missing ones, GPT restricts the self-attention calculation to exclude
    words to the right of the target word. This approach is referred to as masked
    self-attention.
  prefs: []
  type: TYPE_NORMAL
- en: GPT and its chatbot interface, ChatGPT, have showcased exceptional capabilities
    in numerous traditional NLP tasks like language modeling, language translation,
    and question answering. Additionally, they have proven their efficacy in pioneering
    domains such as generating programming code or ML code, composing website content,
    and answering questions. Consequently, GPT has paved the way for a novel AI paradigm
    known as Generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Large Language Models** (**LLMs**) are a class of generative AI model that
    is capable of generating text, translating languages, producing creative content,
    and providing informative answers to questions. LLMs are trained on extensive
    datasets comprising text and code, with billions of model parameters, allowing
    them to understand and learn the statistical patterns and relationships between
    words and phrases. For example, GPT-3, an LLM, has 175 billion parameters after
    training. This training enables LLMs to effectively process and generate human-like
    text across a wide range of applications. While GPTs serve as a notable example
    of LLMs, the open-source community and other companies have developed additional
    LLMs in recent years, mainly using similar Transformer-based architecture. LLMs
    are also called foundation models. Unlike traditional ML models, which are trained
    for specific tasks, foundation models are pre-trained on massive datasets and
    can handle multiple tasks. In addition, foundation models can be fine-tuned and
    adapted for additional tasks. The remarkable capabilities and adaptability of
    foundation models have found many exciting AI use cases that were not easily solvable
    before. Now, let’s briefly review a few other popular foundation models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google’s Pathways Language Model** (**PaLM**) is a 540-billion-parameter,
    decoder-only Transformer model. It offers similar capabilities as GPT, including
    text generation, translation, code generation, question answering, summarization,
    and support for creating chatbots. PaLM is trained using a new architecture called
    Pathways. Pathways is a modular architecture, meaning that it is composed of modules,
    each is responsible for a specific task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Meta’s Large Language Model Meta AI** (**LLaMA**) is an LLM available in
    multiple sizes from 7 billion parameters to 65 billion parameters. While it is
    a smaller model compared to GPT and PaLM, it offers several advantages such as
    requiring fewer computational resources. LLaMA offers similar capabilities as
    other LLMs such as generating creative text, answering questions, and solving
    mathematical problems. Meta has issued a noncommercial license for LLaMA, emphasizing
    its usage in research contexts. LLaMA has also been found to perform extremely
    well when it is fine-tuned with additional training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Big Science BLOOM** is a 176-billion-parameter LLM that can generate text
    in 46 different languages and 13 programming languages. The development of BLOOM
    involved the collaborative efforts of more than 1,000 researchers from over 70
    countries and 250 institutions. As part of the Responsible AI License, individuals
    and institutions who agree to its terms can use and build upon the model on their
    local machines or cloud platforms. The model is easily accessible within the Hugging
    Face ecosystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bloomberg BloombergGPT** – While general-purpose LLMs like GPT and LLaMA
    can perform well in a range of tasks across different domain domains, domains
    such as financial services and life science require domain-specific LLMs to solve
    tough domain-focused problems. BloombergGPT is an example of domain-focused LLMs
    that are specifically trained for industries. BloombergGPT represents a significant
    advancement in applying this technology to finance. The model will enhance existing
    financial NLP tasks such as sentiment analysis, named entity recognition, news
    classification, and question answering, among others. Drawing from its extensive
    collection and curation resources, Bloomberg utilized its four-decade archive
    of financial language documents, resulting in a comprehensive 363-billion-token
    dataset comprising English financial documents. This dataset was augmented with
    a 345-billion-token public dataset, yielding a training corpus with over 700 billion
    tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While these LLM models have exhibited remarkable capabilities, they also come
    with significant limitations, including generating misinformation (hallucinations)
    and toxic content, and displaying potential bias. LLMs also consume significantly
    more resources to train and run. It is worth noting that while LLMs can help solve
    some new problems, many of the common problems (e.g., name entity extraction,
    document classification, sentiment analysis) have been solved with existing NLP
    techniques, which remain highly viable options for those tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, AI’s remarkable ability to generate high-resolution, photorealistic
    images and never-seen-before generative art or manipulate images with precision
    has garnered significant attention. Behind all these amazing capabilities is a
    new type of deep learning model called the diffusion model. Building upon the
    foundations of deep learning and GANs, the diffusion model introduces a novel
    approach to generating high-quality, realistic data instances.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike GANs, which try to generate realistic fake images by fooling a discriminator
    network, the diffusion model works by first adding noises to the input data (e.g.,
    images) incrementally over many steps until the input data is unrecognizable,
    a process called diffusion steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is then trained to reverse the diffusion steps from noise to the
    original data. In more technical terms, the training process of a diffusion model
    involves optimizing a set of learnable parameters through backpropagation. The
    model learns to generate realistic samples by maximizing the likelihood of the
    training data given a sequence of diffusion steps. This iterative process allows
    the model to capture complex dependencies, intricate patterns, and structures,
    and generate highly realistic and diverse data instances. The following figure
    illustrates this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_03_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.16: How a diffusion model works'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the diffusion model offers flexibility and controllability in the
    generation process. By adjusting the diffusion steps, one can control the trade-off
    between sample quality and diversity. This allows users to fine-tune the model
    to suit their specific needs, whether it’s emphasizing fidelity to the training
    data or encouraging more creative and novel outputs. Compared to GANs, diffusion
    models can generate more realistic images and are more stable than GANs.
  prefs: []
  type: TYPE_NORMAL
- en: The diffusion model has shown great promise in various domains, including computer
    vision, natural language processing, and audio synthesis. Its ability to generate
    high-quality data with fine-grained details has opened up exciting possibilities
    for applications such as image generation, video prediction, text generation,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'The open-source community and private companies have developed many models
    based on this diffusion approach. Two of the more popular models worth mentioning
    are Stable Diffusion and DALL-E 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DALL-E 2 by OpenAI**: DALL-E 2 is a text-to-image model developed by OpenAI.
    DALL-E 2 was trained using a dataset of images and text descriptions. First released
    in January 2022, DALL-E 2 has shown remarkable capabilities in generating and
    manipulating images from text descriptions. It has also been applied for inpainting
    (modifying regions in images), outpainting (extending an image), and image-to-image
    translation. The images generated by DALL-E 2 are often indistinguishable from
    real images, and they can be used for a variety of purposes, such as creating
    art and generating marketing materials. From a model-training perspective, DALL-E
    2 training comprises two key steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linking textual semantics and visual representations**: This step involves
    learning how a piece of text, such as “a man wearing a hat” is semantically linked
    to an actual image of “a man wearing a hat”. To do this, DALL-E 2 uses a model
    called **Contrastive Language-Image Pre-training** (**CLIP**). CLIP is trained
    with hundreds of millions of images and their associated descriptions. After it
    is trained, it can output a text-conditioned visual encoding given a piece of
    text description. You can learn more about CLIP at [https://openai.com/research/clip](https://openai.com/research/clip).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generating images from visual embeddings**: This step learns to reverse the
    image from the visual embeddings generated by the CLIP. For this step, DALL-E
    2 uses a model called GLIDE, which is based on the diffusion model. You can learn
    more about GLIDE at [https://arxiv.org/abs/2112.10741](https://arxiv.org/abs/2112.10741).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: After the model is trained, DALL-E 2 can generate new images closely related
    to an input text description.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Stable Diffusion by Stability AI**: Stable Diffusion is an algorithm developed
    by Compvis (the Computer Vision research group at Ludwig Maximilian University
    of Munich) and sponsored primarily by Stability AI. The model is also a text-to-image
    model trained using a dataset of real images and text descriptions, which allows
    the model to generate realistic images using text descriptions. First released
    in August 2022, Stable Diffusion has been shown to be effective at generating
    high-quality images from text descriptions. Architecturally, it employs a CLIP
    encoder to condition the model on text descriptions, and it uses UNET as the denoising
    neural network to generate images from visual encodings. It is an open-source
    model with the code and model weights released to the public. You can get more
    details on Stable Diffusion at [https://github.com/CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As powerful as the diffusion models are, they do come with some concerns, including
    copyright infringement and the creation of harmful images.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this hands-on exercise, we will build a **Jupyter** **Notebook** environment
    on your local machine and build and train an ML model in your local environment.
    The goal of the exercise is to get some familiarity with the installation process
    of setting up a local data science environment, and then learn how to analyze
    the data, prepare the data, and train an ML model using one of the algorithms
    we covered in the preceding sections. First, let’s take a look at the problem
    statement. The following diagram illustrates the flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_03_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.17: ML problem-solving flow'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we start, let’s first review the business problem that we need to solve.
    A retail bank has been experiencing a high customer churn rate for its retail
    banking business. To proactively implement preventive measures to reduce potential
    churn, the bank needs to know who the potential churners are, so the bank can
    target those customers with incentives directly to prevent them from leaving.
    From a business perspective, it is far more expensive to acquire a new customer
    than offering incentives to keep an existing customer.
  prefs: []
  type: TYPE_NORMAL
- en: As an ML solutions architect, you have been tasked to run some quick experiments
    to validate the ML approach for this problem. There is no ML tooling available,
    so you have decided to set up a Jupyter environment on your local machine for
    this task.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset description
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will use a dataset from the Kaggle site for bank customers’ churn for modeling.
    You can access the dataset at [https://www.kaggle.com/mathchi/churn-for-bank-customers](https://www.kaggle.com/mathchi/churn-for-bank-customers).
    Note that you will need Kaggle account to download the file. The dataset contains
    14 columns for features such as credit score, gender, and balance, and a target
    variable column, `Exited`, to indicate whether a customer churned or not. We will
    review those features in more detail in later sections.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Jupyter Notebook environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s set up a local data science environment for data analysis and experimentation.
    We will be using the popular Jupyter Notebook on your local computer. Setting
    up a Jupyter Notebook environment on a local machine consists of the following
    key components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**: Python is a general-purpose programming language, and it is one
    of the most popular programming languages for data science work. The installation
    instructions can be found at [https://www.python.org/downloads](https://www.python.org/downloads).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PIP**: PIP is a Python package installer used for installing different Python
    library packages, such as ML algorithms, data manipulation libraries, or visualization.
    The installation instructions can be found at [https://pip.pypa.io/en/stable/installation/](https://pip.pypa.io/en/stable/installation/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jupyter Notebook**: Jupyter Notebook is a web application designed for authoring
    documents (called notebooks) that contain code, description, and/or visualizations.
    It is one of the most popular tools used by data scientists for experimentation
    and modeling. The installation instructions can be found at [https://jupyter.org/install](https://jupyter.org/install).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the exercise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow along with these steps to run the lab:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With your environment configured, let’s get started with the actual data science
    work. First, download the data files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s create a folder called `MLSALab` on your local machine to store all the
    files. You can create the folder anywhere on your local machine as long as you
    can get to it. I have a Mac, so I created one directly inside the default user’s
    `Documents` folder.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create another subfolder called `Lab1-bankchurn` under the `MLSALab` folder.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Visit the [https://www.kaggle.com/mathchi/churn-for-bank-customers](https://www.kaggle.com/mathchi/churn-for-bank-customers)
    site and download the data file (an archive file) and save it in the `MSSALab/Lab1-bankchurn`
    folder. Create a Kaggle account if you do not already have one. Extract the archive
    file inside the folder, and you will see a file called `churn.csv`. You can now
    delete the archive file.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Launch Jupyter Notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inside the Terminal window (or the Command Prompt window for Windows systems),
    navigate to the `MLSALab` folder and run the following command to start the Jupyter
    Notebook server on your machine:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'A browser window will open up and display the Jupyter Notebook environment
    (see the following screenshot). Detailed instructions on how Jupyter Notebook
    works are out of scope for this lab. If you are not familiar with how Jupyter
    Notebook works, you can easily find information on the internet:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Jupyter Notebook ](img/B20836_03_18.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.18: Jupyter Notebook'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the `Lab1-bankchurn` folder and you will see the `churn.csv` file.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let’s create a new data science notebook inside the Jupyter Notebook environment.
    To do this, click on the **New** dropdown and select **Python 3** (see the following
    screenshot):![Figure 3.18 – Creating a new Jupyter notebook ](img/B20836_03_19.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 3.19: Creating a new Jupyter notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will see a screen similar to the following screenshot. This is an empty
    notebook that we will use to explore data and build models. The section next to
    **In [ ]:** is called a **cell**, and we will enter our code into the cell. To
    run the code in the cell, you click on the **Run** button on the toolbar. To add
    a new cell, you click on the **+** button on the toolbar:![Figure 3.19 – Empty
    Jupyter notebook ](img/B20836_03_20.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 3.20: Empty Jupyter notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add a new cell by clicking on the **+** button in the toolbar, enter the following
    code block inside the first empty cell, and run the cell by clicking on the **Run**
    button in the toolbar. This code block downloads a number of Python packages for
    data manipulation (`pandas`), visualization (`matplotlib`), and model training
    and evaluation (`scikit-learn`). We will cover scikit-learn in greater detail
    in *Chapter 5*, *Exploring Open-Source ML Libraries*. We will use these packages
    in the following sections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can load and explore the data. Add the following code block in a new
    cell to load the Python library packages and load the data from the `churn.csv`
    file. You will see a table with 14 columns, where the `Exited` column is the target
    column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can explore the dataset using a number of tools to understand information
    with the commands that follow, such as *dataset statistics*, the *pairwise correlation
    between different features*, and *data distributions*. The `describe()` function
    returns basic statistics about the data such as mean, standard deviation, min,
    and max, for each numerical column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `hist()` function plots the histogram for the selected columns, and `corr()`
    calculates the correlation matrix between the different features in the data.
    Try them out one at a time in a new cell to understand the data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The dataset needs transformations in order to be used for model training. The
    following code block will convert the `Geography` and `Gender` values from categorical
    strings to ordinal numbers so they can be taken by the ML algorithm later. Please
    note that model accuracy is not the main purpose of this exercise, and we are
    performing ordinal transformation for demonstration purposes. We will be using
    a popular Python ML library called sklearn for this exercise. Sklearn is also
    one of the easiest libraries to use and understand, especially for beginners.
    We will also discuss this library in more detail in *Chapter 5, Exploring Open-Source
    ML Libraries*. Copy and run the following code block in a new cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Often, there could be some columns not needed for model training, as they do
    not contribute to model predictive power or could cause bias from an inclusion
    perspective. We can drop them using the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, the dataset has only the features we care about. Next, we need to split
    the data for training and validation. We also prepare each dataset by splitting
    the target variable, `Exited`, from the rest of the input features. Enter and
    run the following code block in a new cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are ready to train the model. Enter and run the following code block in
    a new cell. Here, we will use the random forest algorithm to train the model,
    and the `fit()` function kicks off the model training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will test the accuracy of the model using the `test` dataset. Here,
    we get the predictions returned by the model using the `predict()` function, and
    then use the `accuracy_score()` function to calculate the model accuracy using
    the predicted values (`churn_prediction_y`) and the true values (`churn_test_y`)
    for the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You have successfully installed the Jupyter data science environment
    on your local machine and trained a model using the random forest algorithm. You
    have validated that an ML approach could potentially solve this business problem.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored various ML algorithms that can be applied
    to solve different types of ML problems. By now, you should have a good understanding
    of which algorithms are suitable for which specific tasks. Additionally, you have
    set up a basic data science environment on your local machine, utilized the scikit-learn
    ML libraries to analyze and preprocess data, and successfully trained an ML model.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapter, our focus will shift to the intersection of data management
    and the ML lifecycle. We will delve into the significance of effective data management
    and discuss how to build a comprehensive data management platform on **Amazon
    Web Services** (**AWS**) to support downstream ML tasks. This platform will provide
    the necessary infrastructure and tools to streamline data processing, storage,
    and retrieval, ultimately enhancing the overall ML workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code7020572834663656.png)'
  prefs: []
  type: TYPE_IMG
