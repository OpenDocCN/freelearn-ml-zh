- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Exploring ML Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索机器学习算法
- en: While ML algorithm design may not be the primary role of ML solutions architects,
    it is still essential for them to possess a comprehensive understanding of common
    real-world ML algorithms and their applications in solving business problems.
    This knowledge empowers ML solutions architects to identify suitable data science
    solutions and design the necessary technology infrastructure for deploying these
    algorithms effectively.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然机器学习算法设计可能不是机器学习解决方案架构师的主要角色，但对他们来说，仍然需要具备对常见现实世界机器学习算法及其在解决商业问题中的应用的全面理解。这种知识使机器学习解决方案架构师能够识别合适的数据科学解决方案，并设计部署这些算法所需的技术基础设施。
- en: By familiarizing themselves with a range of ML algorithms, ML solutions architects
    can grasp the strengths, limitations, and specific use cases of each algorithm.
    This enables them to evaluate business requirements accurately and select the
    most appropriate algorithmic approach to address a given problem. Whether it’s
    classification, regression, clustering, or recommendation systems, understanding
    the underlying algorithms equips architects with the knowledge required to make
    informed decisions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通过熟悉各种机器学习算法，机器学习解决方案架构师可以掌握每个算法的优势、局限性和特定用例。这使得他们能够准确评估业务需求，并选择最合适的算法方法来解决给定的问题。无论是分类、回归、聚类还是推荐系统，理解底层算法为架构师提供了做出明智决策所需的知识。
- en: In this chapter, we will explore the fundamentals of ML and delve into common
    ML and deep learning algorithms. We’ll cover tasks such as classification, regression,
    object detection, recommendation, forecasting, and natural language generation.
    By understanding the core principles and applications of these algorithms, you’ll
    gain the knowledge to identify suitable ML solutions for real-world problems.
    This chapter aims to equip you with the expertise to make informed decisions and
    design effective ML solutions across various domains.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨机器学习的基础知识，并深入研究常见的机器学习和深度学习算法。我们将涵盖分类、回归、目标检测、推荐、预测和自然语言生成等任务。通过理解这些算法的核心原理和应用，您将获得识别适合现实世界问题的机器学习解决方案的知识。本章旨在为您提供跨各种领域的有见地的决策和设计有效的机器学习解决方案的专长。
- en: 'Specifically, we will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章将涵盖以下主题：
- en: How machines learn
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习是如何工作的
- en: Considerations for choosing ML algorithms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择机器学习算法的考虑因素
- en: Algorithms for classification and regression
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类和回归算法
- en: Algorithms for clustering
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类算法
- en: Algorithms for time series
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列算法
- en: Algorithms for recommendation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐算法
- en: Algorithms for computer vision
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉算法
- en: Algorithms for natural language processing
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理算法
- en: Generative AI algorithms
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式人工智能算法
- en: Hands-on exercise
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动手实践练习
- en: Note that this chapter provides an introduction to ML algorithms for readers
    who are new to applying these algorithms. If you already have experience as a
    data scientist or ML engineer, you may want to skip this chapter and go directly
    to *Chapter 4*, where we discuss data management for ML.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，本章为对应用这些算法的新读者提供了机器学习算法的介绍。如果您已经拥有数据科学家或机器学习工程师的经验，您可能希望跳过本章，直接进入*第4章*，在那里我们讨论机器学习的数据管理。
- en: Technical requirements
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You need a personal computer (**Mac** or **Windows**) to complete the hands-on
    exercise portion of this chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成本章的动手实践练习部分，您需要一个个人电脑（**Mac**或**Windows**）。
- en: You also need to download the dataset from [https://www.kaggle.com/mathchi/churn-for-bank-customers](https://www.kaggle.com/mathchi/churn-for-bank-customers).
    Additional instructions will be provided in the *Hands-on exercise* section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要从[https://www.kaggle.com/mathchi/churn-for-bank-customers](https://www.kaggle.com/mathchi/churn-for-bank-customers)下载数据集。在*动手实践练习*部分将提供额外的说明。
- en: How machines learn
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习是如何工作的
- en: In *Chapter 1*, *Navigating the ML Lifecycle with ML Solutions Architecture*,
    we discussed the self-improvement capability of ML algorithms through data processing
    and parameter updates, leading to the generation of models akin to compiled binaries
    in computer source code. But how does an algorithm actually learn? In essence,
    ML algorithms learn by optimizing an objective function, also known as a loss
    function, which involves minimizing or maximizing it. An objective function can
    be seen as a business metric, such as the disparity between projected and actual
    product sales. The aim of optimization is to reduce this disparity. To achieve
    this, an ML algorithm iterates and processes extensive historical sales data (training
    data), adjusting its internal model parameters until the gaps between projected
    and actual values are minimized. This process of finding the optimal model parameters
    is referred to as optimization, with mathematical routines specifically designed
    for this purpose known as optimizers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第一章*，*使用机器学习解决方案架构导航机器学习生命周期* 中，我们讨论了机器学习算法通过数据处理和参数更新实现自我改进的能力，从而生成类似于计算机源代码中编译二进制的模型。但算法实际上是如何学习的呢？本质上，机器学习算法通过优化目标函数（也称为损失函数）来学习，这涉及到最小化或最大化它。目标函数可以被视为一个业务指标，例如预测和实际产品销售之间的差异。优化的目的是减少这种差异。为了实现这一点，机器学习算法迭代并处理大量的历史销售数据（训练数据），调整其内部模型参数，直到预测值和实际值之间的差距最小化。这个过程被称为优化，专门为此目的设计的数学程序称为优化器。
- en: 'To illustrate the concept of optimization, let’s consider a simple example
    of training an ML model to predict product sales based on its price. In this case,
    we can use a linear function as the ML algorithm, represented as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明优化的概念，让我们考虑一个简单的例子：训练一个机器学习模型来根据其价格预测产品销售。在这种情况下，我们可以使用线性函数作为机器学习算法，如下所示：
- en: sales = W * price + B
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: sales = W * price + B
- en: 'In this example, our objective is to minimize the disparity between the predicted
    and actual sales values. To achieve this, we employ the **mean square error**
    (**MSE**) as the loss function for optimization. The specific task is to determine
    the optimal values for the model parameters *W* and *B*, commonly referred to
    as weight and bias. The weight assigns a relative significance to each input variable,
    while the bias represents the average output value. Our aim is to identify the
    *W* and *B* values that yield the lowest MSE in order to enhance the accuracy
    of the sales predictions:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们的目标是使预测销售值和实际销售值之间的差异最小化。为了实现这一点，我们采用 **均方误差**（**MSE**）作为优化的损失函数。具体任务是确定模型参数
    *W* 和 *B* 的最佳值，通常称为权重和偏差。权重为每个输入变量分配相对重要性，而偏差表示平均输出值。我们的目标是找到产生最低 MSE 的 *W* 和
    *B* 值，以提高销售预测的准确性：
- en: '![](img/B20836_03_001.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20836_03_001.png)'
- en: 'There are multiple techniques available for solving ML optimization problems.
    Among them, gradient descent and its variations are widely used for optimizing
    neural networks and various other ML algorithms. Gradient descent is an iterative
    approach that involves calculating the rate of error change (gradient) associated
    with each input variable. Based on this gradient, the model parameters (*W* and
    *B* in this example) are updated step by step to gradually reduce the error. The
    learning rate, a hyperparameter of the ML algorithm, controls the magnitude of
    parameter updates at each iteration. This allows for fine-tuning the optimization
    process. The following figure illustrates the optimization of the W value using
    gradient descent:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 解决机器学习优化问题有多种技术可用。其中，梯度下降及其变体被广泛用于优化神经网络和各种其他机器学习算法。梯度下降是一种迭代方法，它涉及计算与每个输入变量相关的误差变化率（梯度）。根据这个梯度，模型参数（在本例中的
    *W* 和 *B*）逐步更新，以逐步减少误差。学习率是机器学习算法的一个超参数，它控制每次迭代的参数更新幅度。这允许对优化过程进行微调。以下图展示了使用梯度下降优化
    W 值的过程：
- en: '![Figure 3.1 – Gradient descent ](img/B20836_03_01.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 梯度下降](img/B20836_03_01.png)'
- en: 'Figure 3.1: Gradient descent'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：梯度下降
- en: 'The gradient descent optimization process involves several key steps:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降优化过程涉及几个关键步骤：
- en: Initialize the value of *W* randomly.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化 *W* 的值。
- en: Calculate the error (loss) using the assigned value of *W*.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分配的 *W* 值来计算误差（损失）。
- en: Compute the gradient (rate of change) of the error with respect to the loss
    function. The gradient can be positive, zero, or negative.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算误差相对于损失函数的梯度（变化率）。梯度可以是正的、零或负的。
- en: If the gradient is positive or negative, update the value of *W* in a direction
    that reduces the error in the next iteration. In this example, we move *W* to
    the right to increase its value.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果梯度是正的或负的，更新*W*的值，使其在减少下一次迭代中误差的方向上移动。在这个例子中，我们将*W*向右移动以增加其值。
- en: Repeat *steps 2* to *4* until the gradient becomes zero, indicating that the
    optimal value of *W* has been reached and convergence has been achieved.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 2*到*4*，直到梯度变为零，这表明*W*的最佳值已经达到，并且收敛已经实现。
- en: In addition to gradient descent, alternative optimization techniques like the
    normal equation can be used to find optimal parameters for ML algorithms such
    as linear regression. Unlike the iterative approach of gradient descent, the normal
    equation offers a one-step analytical solution for calculating the coefficients
    of linear regression models. Other ML algorithms may also have algorithm-specific
    optimization methods for model training, which will be discussed in the next section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了梯度下降之外，还可以使用如正规方程这样的替代优化技术来寻找机器学习算法（如线性回归）的最佳参数。与梯度下降的迭代方法不同，正规方程为计算线性回归模型的系数提供了一个一步的解析解。其他机器学习算法也可能有针对模型训练的特定算法优化方法，这些将在下一节中讨论。
- en: Overview of ML algorithms
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法概述
- en: With that brief overview of the fundamental concepts behind how machines learn,
    let’s now explore various ML algorithms in more depth. The field of ML has seen
    the development of numerous algorithms, with ongoing research and innovation from
    both academia and industry. In this section, we will explore several well-known
    traditional and deep learning algorithms, examining their applications across
    various types of ML problems such as forecasting, recommendation, and natural
    language processing. Additionally, we will look at the strengths and weaknesses
    of different algorithms and discuss which situations each one is best suited for.
    This will help you build an understanding of the different capabilities of each
    algorithm and the types of problems they can be used to solve.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在简要概述了机器学习背后的基本概念之后，现在让我们更深入地探讨各种机器学习算法。机器学习领域已经发展了许多算法，学术界和工业界都在持续地进行研究和创新。在本节中，我们将探讨几个著名的传统和深度学习算法，检查它们在预测、推荐和自然语言处理等各种类型的机器学习问题中的应用。此外，我们还将探讨不同算法的优缺点，并讨论每种算法最适合的情况。这将帮助你建立对每个算法不同能力和它们可以解决的各类问题的理解。
- en: Before we delve into these algorithms, it’s important to discuss the factors
    to consider when selecting an appropriate algorithm for a given task.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨这些算法之前，讨论在选择适合特定任务的算法时需要考虑的因素是很重要的。
- en: Consideration for choosing ML algorithms
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择机器学习算法的考虑因素
- en: 'When choosing a ML algorithm, there are several key considerations to keep
    in mind:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择机器学习算法时，有几个关键因素需要考虑：
- en: '**Problem type**: Different algorithms are better suited for different types
    of problems. For example, classification algorithms are suitable for tasks where
    the goal is to categorize data into distinct classes, while regression algorithms
    are used for predicting continuous numerical values. Understanding the problem
    type is crucial in selecting the most appropriate algorithm.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题类型**：不同的算法更适合不同类型的问题。例如，分类算法适用于目标是将数据分类到不同类别的任务，而回归算法用于预测连续的数值。理解问题类型对于选择最合适的算法至关重要。'
- en: '**Dataset size**: The size of your dataset can impact the choice of algorithm.
    Some algorithms perform well with small datasets, while others require large amounts
    of data to generalize effectively. If you have limited data, simpler algorithms
    with fewer parameters may be preferable to prevent overfitting. Overfitting is
    when a trained model that learns the training data too well but fails to generalize
    to new, unseen data.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集大小**：你的数据集大小可能会影响算法的选择。有些算法在小数据集上表现良好，而其他算法则需要大量的数据才能有效地泛化。如果你数据有限，具有较少参数的简单算法可能更可取，以防止过拟合。过拟合是指训练模型对训练数据学习得太好，但无法泛化到新的、未见过的数据。'
- en: '**Feature space**: Consider the number and nature of features in your dataset.
    Some algorithms can handle high-dimensional feature spaces, while others are more
    suitable for datasets with fewer features. Feature engineering and dimensionality
    reduction techniques can also be applied to enhance algorithm performance.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征空间**：考虑你的数据集中特征的数量和性质。一些算法可以处理高维特征空间，而其他算法则更适合具有较少特征的集合。特征工程和降维技术也可以应用于提高算法性能。'
- en: '**Computational efficiency**: The computational requirements of an algorithm
    should be taken into account, especially if you have large datasets or limited
    computational resources. Some algorithms are computationally expensive and may
    not be feasible for certain environments. Time complexity and space complexity
    are quantitative measures used to assess the efficiency of ML algorithms. Big
    *O* notation represents the upper bound estimation for time and space requirements.
    For example, linear search has a time complexity of *O(N)*, while binary search
    has *O(log N)*. Understanding these complexities helps evaluate algorithm efficiency
    and scalability, aiding in algorithm selection for specific tasks.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算效率**：算法的计算需求应予以考虑，特别是如果你有大量数据集或有限的计算资源。一些算法计算成本高昂，可能不适合某些环境。时间复杂度和空间复杂度是用于评估机器学习算法效率的定量指标。大O符号表示时间和空间需求的上限估计。例如，线性搜索的时间复杂度为*O(N)*，而二分搜索的时间复杂度为*O(log
    N)*。理解这些复杂性有助于评估算法效率和可扩展性，有助于为特定任务选择算法。'
- en: '**Interpretability**: Depending on your application, the interpretability of
    the algorithm’s results may be important. Some algorithms, such as decision trees
    or linear models, offer easily interpretable outcomes, while others, like deep
    neural networks, provide more complex and abstract representations.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：根据你的应用，算法结果的解释性可能很重要。一些算法，如决策树或线性模型，提供易于解释的结果，而其他算法，如深度神经网络，则提供更复杂和抽象的表示。'
- en: '**Algorithm complexity and assumptions**: Different algorithms make different
    assumptions about the underlying data distribution. Consider whether these assumptions
    are valid for your dataset. Additionally, the complexity of the algorithm can
    impact its ease of implementation, training time, and ability to handle noisy
    or incomplete data.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法复杂度和假设**：不同的算法对潜在数据分布有不同的假设。考虑这些假设是否适用于你的数据集。此外，算法的复杂性可能会影响其实施的简便性、训练时间和处理噪声或不完整数据的能力。'
- en: By considering these factors, you can make an informed decision when selecting
    a ML algorithm that best suits your specific problem and available resources.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过考虑这些因素，你可以在选择最适合你特定问题和可用资源的机器学习算法时做出明智的决定。
- en: Algorithms for classification and regression problems
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于分类和回归问题的算法
- en: The vast majority of ML problems today primarily involve classification and
    regression. Classification is a ML task that assigns categories or classes to
    data points, such as labeling a credit card transaction as fraudulent or not fraudulent.
    Regression, on the other hand, is a ML technique used to predict continuous numeric
    values, such as predicting the price of a house.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的大多数机器学习问题主要涉及分类和回归。分类是机器学习任务，它将类别或类分配给数据点，例如将信用卡交易标记为欺诈或不欺诈。另一方面，回归是一种机器学习技术，用于预测连续的数值，例如预测房屋价格。
- en: In the upcoming section, we’ll explore common algorithms used for classification
    and regression tasks. We will explain how each algorithm works, the types of problems
    each algorithm is suited for, and their limitations. This will help build intuition
    on when to select different algorithms for the different tasks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨用于分类和回归任务的常见算法。我们将解释每个算法的工作原理，每种算法适合解决的问题类型，以及它们的局限性。这将有助于建立对不同任务选择不同算法的直觉。
- en: Linear regression algorithms
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归算法
- en: '**Linear regression** algorithms are developed to solve regression problems
    by predicting continuous values based on independent inputs. They find wide applications
    in various practical scenarios, such as estimating product sales based on price
    or determining crop yield based on rainfall and fertilizer.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归**算法是通过基于独立输入预测连续值来解决回归问题的。它们在各种实际场景中得到了广泛应用，例如根据价格估计产品销售或根据降雨量和肥料确定作物产量。'
- en: 'Linear regression utilizes a linear function of a set of coefficients and input
    variables to predict a scalar output. The formula for the linear regression is
    expressed as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归利用一组系数和输入变量的线性函数来预测标量输出。线性回归的公式如下所示：
- en: '![](img/B20836_03_002.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20836_03_002.png)'
- en: In the linear regression equation, the *X*[s] represent the input variables,
    *W*[s] denote the coefficients, and *![](img/B20836_03_new.png)* represents the
    error term. Linear regression aims to estimate the output value by calculating
    the weighted sum of the inputs, assuming a linear relationship between the output
    and inputs. The intuition behind linear regression is to find a line or hyperplane
    that can estimate the value for a set of input values. Linear regression can work
    efficiently with small datasets, offering interpretability through the coefficients’
    assessment of input and output variables. However, it may not perform well with
    complex, nonlinear datasets. Additionally, linear regression assumes independence
    among input features and struggles when there is co-linearity (the value of one
    feature influences the value of another feature), as it becomes challenging to
    assess the significance of correlated features.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归方程中，*X*[s]代表输入变量，*W*[s]表示系数，![图片](img/B20836_03_new.png)表示误差项。线性回归旨在通过计算输入的加权总和来估计输出值，假设输出和输入之间存在线性关系。线性回归背后的直觉是找到一个线或超平面，可以估计一组输入值的值。线性回归可以有效地处理小数据集，通过系数评估输入和输出变量提供可解释性。然而，它可能无法很好地处理复杂、非线性数据集。此外，线性回归假设输入特征之间相互独立，当存在共线性（一个特征的价值影响另一个特征的价值）时，它可能会遇到困难，因为评估相关特征的重要性变得具有挑战性。
- en: Logistic regression algorithms
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归算法
- en: '**Logistic regression** is commonly employed for binary and multi-class classification
    tasks. It can predict the probability of an event occurring, such as whether a
    person will click on an advertisement or qualify for a loan. Logistic regression
    is a valuable tool in real-world scenarios where the outcome is binary and requires
    estimating the likelihood of a particular class. By utilizing a logistic function,
    this algorithm maps the input variables to a probability score, enabling effective
    classification decision-making.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归**通常用于二元和多类分类任务。它可以预测事件发生的概率，例如，一个人是否会点击广告或是否有资格获得贷款。逻辑回归是现实场景中非常有价值的一种工具，在这些场景中，结果为二元，需要估计特定类别的可能性。通过利用逻辑函数，该算法将输入变量映射到概率分数，从而实现有效的分类决策。'
- en: 'Logistic regression is a statistical model used to estimate the probability
    of an event or outcome, such as transaction fraud or passing an exam. It is a
    linear model similar to linear regression, but with a different output transformation.
    The goal of logistic regression is to find a decision boundary, represented by
    a line or hyperplane, that effectively separates the two classes of data points.
    By applying a logistic function to the linear combination of input variables,
    logistic regression ensures that the predicted output falls within the range of
    0 and 1, representing the probability of belonging to a particular class. The
    following formula is the function for the logistic regression, where *X* is a
    linear combination of input variables (*b+wx*). Here, the w is the regression
    coefficient:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种用于估计事件或结果发生概率的统计模型，例如交易欺诈或通过考试。它与线性回归类似，但输出转换不同。逻辑回归的目标是找到一个决策边界，由一条线或超平面表示，能够有效地将数据点的两个类别分开。通过将输入变量的线性组合应用逻辑函数，逻辑回归确保预测输出落在0到1的范围内，表示属于特定类别的概率。以下公式是逻辑回归的函数，其中*X*是输入变量的线性组合(*b+wx*)。在这里，w是回归系数：
- en: '![](img/B20836_03_003.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20836_03_003.png)'
- en: Like linear regression, logistic regression offers fast training speed and interpretability
    as its advantages. However, due to its linear nature, logistic regression is not
    suitable for solving problems with complex non-linear relationships.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归一样，逻辑回归具有快速训练速度和可解释性作为其优点。然而，由于其线性特性，逻辑回归不适用于解决具有复杂非线性关系的问题。
- en: Decision tree algorithms
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树算法
- en: '**Decision trees** find extensive application in various real-world ML scenarios,
    including heart disease prediction, target marketing, and loan default prediction.
    They are versatile and can be used for both classification and regression problems.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**在各种现实世界的机器学习场景中得到了广泛的应用，包括心脏病预测、目标营销和贷款违约预测。它们用途广泛，可以用于分类和回归问题。'
- en: A decision tree is motivated by the idea that data can be divided hierarchically
    based on rules, leading to similar data points following the same decision path.
    It achieves this by splitting the input data using different features at different
    branches of the tree. For example, if age is a feature used for splitting at a
    branch, a conditional check like age > 50 would be used to divide the data. The
    decision of which feature to use for splitting and where to split is made using
    algorithms such as the Gini purity index and information gain. The Gini index
    measures the probability of misclassification, while information gain quantifies
    the reduction in entropy resulting from the split.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的概念源于数据可以根据规则分层划分的想法，导致相似的数据点遵循相同的决策路径。它通过在树的各个分支使用不同的特征来分割输入数据来实现这一点。例如，如果年龄是用于分支分割的特征，则可以使用条件检查如年龄
    > 50 来划分数据。选择用于分割的特征以及分割位置是通过诸如基尼纯度指数和信息增益等算法来决定的。基尼指数衡量误分类的概率，而信息增益量化了分割导致的熵减少。
- en: In this book, we won’t delve into specific algorithm details. However, the general
    concept of decision tree involves experimenting with various split options and
    conditions, calculating metric values (e.g., information gain) for each split
    option, and selecting the option that yields the highest value. During prediction,
    input data traverses the tree based on the learned branching logic, and the final
    prediction is determined by the terminal node (leaf node). Refer to *Figure 3.2*
    for an example structure of a decision tree.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们不会深入探讨具体的算法细节。然而，决策树的一般概念涉及尝试不同的分割选项和条件，计算每个分割选项的度量值（例如，信息增益），并选择产生最高值的选项。在预测过程中，输入数据根据学习到的分支逻辑遍历树，最终预测由终端节点（叶节点）确定。请参阅*图3.2*以了解决策树的一个示例结构。
- en: '![Figure 3.2 – Decision tree ](img/B20836_03_02.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2 – 决策树](img/B20836_03_02.png)'
- en: 'Figure 3.2: Decision tree'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：决策树
- en: The main advantage of decision trees over linear regression and logistic regression
    is their ability to capture non-linear relationships and interactions between
    features. Decision trees can handle complex data patterns and are not limited
    to linear relationships between input variables and the output. They can represent
    decision boundaries that are more flexible and can handle both numerical and categorical
    features.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归和逻辑回归相比，决策树的主要优势在于它们能够捕捉特征之间的非线性关系和交互。决策树可以处理复杂的数据模式，并且不仅限于输入变量和输出之间的线性关系。它们可以表示更灵活的决策边界，并可以处理数值和分类特征。
- en: A decision tree is advantageous in that it can handle data with minimal preprocessing,
    accommodate both categorical and numerical features, and handle missing values
    and varying feature scales. It is also highly interpretable, allowing for easy
    visualization and analysis of decision paths. Furthermore, decision trees are
    computationally efficient. However, they can be sensitive to outliers and prone
    to **overfitting**, particularly when dealing with a large number of features
    and noisy data. Overfitting occurs when the model memorizes the training data
    but performs poorly on unseen data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的优势在于它可以处理最少预处理的数据，适应分类和数值特征，并处理缺失值和变化的特征尺度。它也非常易于解释，允许轻松可视化和分析决策路径。此外，决策树在计算上效率很高。然而，它们可能对异常值敏感，并且容易过拟合，尤其是在处理大量特征和噪声数据时。过拟合发生在模型记住训练数据但在未见数据上表现不佳的情况下。
- en: A notable limitation of decision trees and tree-based algorithms is their inability
    to extrapolate beyond the range of training inputs. For instance, if a housing
    price model is trained on square footage data ranging from 500 to 3,000 sq ft,
    a decision tree would be unable to make predictions beyond 3,000 sq ft. In contrast,
    a linear model would be capable of capturing the trend and making predictions
    beyond the observed range.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树和基于树的算法的一个显著局限性是它们无法在训练输入范围之外进行外推。例如，如果一个房价模型是在500到3,000平方英尺的平方英尺数据上训练的，那么决策树将无法对超过3,000平方英尺的数据进行预测。相比之下，线性模型能够捕捉趋势并在观察范围之外进行预测。
- en: Random forest algorithm
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林算法
- en: '**Random forest** algorithm is widely employed in various real-world applications
    across e-commerce, healthcare, and finance sectors. They are particularly valuable
    for classification and regression tasks. Real-world examples of these tasks include
    insurance underwriting decisions, disease prediction, loan payment default prediction,
    and targeted marketing efforts. The versatility of random forest algorithms allows
    them to be applied in a wide range of industries to address diverse business challenges.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**算法在电子商务、医疗保健和金融等各个领域的实际应用中得到了广泛应用。它们在分类和回归任务中尤其有价值。这些任务的现实世界例子包括保险承保决策、疾病预测、贷款支付违约预测和定向营销活动。随机森林算法的通用性允许它们在广泛的行业中应用，以解决各种商业挑战。'
- en: As discussed in the preceding decision tree section, a decision tree uses a
    single tree to make its decisions, and the root node of the tree (the first feature
    to split the tree) has the most influence on the final decision. The motivation
    behind this random forest is that combining the decisions of multiple trees can
    lead to improved overall performance. The way that a random forest works is to
    create multiple smaller **subtrees**, also called **weak learner trees**, where
    each subtree uses a random subset of all the features to come to a decision, and
    the final decision is made by either majority voting (for classification) or averaging
    (for regression). This process of combining the decision from multiple models
    is also referred to as **ensemble learning**. Random forest algorithms also allow
    you to introduce different degrees of randomness, such as **bootstrap sampling**,
    which involves using the same sample multiple times in a single tree. This helps
    make the model more generalized and less prone to overfitting. The following figure
    illustrates how the random forest algorithm processes input data instances using
    multiple subtrees and combines their outputs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述决策树章节所述，决策树使用单一树来做出决策，树的根节点（第一个分割树的特性）对最终决策影响最大。随机森林背后的动机在于，结合多个树的决策可以导致整体性能的提升。随机森林的工作方式是创建多个较小的**子树**，也称为**弱学习树**，其中每个子树使用所有特征的一个随机子集来做出决策，最终决策通过多数投票（用于分类）或平均（用于回归）来做出。将多个模型的决策结合起来的这个过程也被称为**集成学习**。随机森林算法还允许你引入不同程度的随机性，例如**自助采样**，这涉及到在单个树中使用相同的样本多次。这有助于使模型更加通用，并减少过拟合的风险。以下图示说明了随机森林算法如何使用多个子树处理输入数据实例，并组合它们的输出。
- en: '![Figure 3.3 – Random forest ](img/B20836_03_03.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – 随机森林](img/B20836_03_03.png)'
- en: 'Figure 3.3: Random forest'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：随机森林
- en: Random forests have several advantages over decision trees. They provide improved
    accuracy by combining the predictions of multiple trees through majority voting
    or averaging. They also reduce overfitting by introducing randomness in the model
    and using diverse subsets of features. Random forests handle large feature sets
    better by focusing on different aspects of the data. They are robust to outliers
    and provide feature importance estimation. Additionally, random forests support
    parallel processing for training large datasets across multiple machines. The
    limitations of random forests include reduced interpretability compared to decision
    trees, longer training and prediction times, increased memory usage, and the need
    for hyperparameter tuning.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 与决策树相比，随机森林具有几个优势。通过多数投票或平均结合多个树的预测，它们提高了准确性。它们通过在模型中引入随机性和使用特征的不同子集来减少过拟合。随机森林通过关注数据的不同方面更好地处理大型特征集。它们对异常值具有鲁棒性，并提供特征重要性估计。此外，随机森林支持跨多台机器的并行处理，以训练大型数据集。随机森林的局限性包括与决策树相比的可解释性降低、较长的训练和预测时间、增加的内存使用以及需要调整超参数。
- en: Gradient boosting machine and XGBoost algorithms
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升机与XGBoost算法
- en: Gradient boosting and XGBoost are also popular multi-tree-based ML algorithms
    used in various domains like credit scoring, fraud detection, and insurance claim
    prediction. Unlike random forests that combine results from weak learner trees
    at the end, gradient boosting sequentially aggregates results from different trees.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升和XGBoost也是流行的基于多树的机器学习算法，广泛应用于信用评分、欺诈检测和保险索赔预测等各个领域。与随机森林在最后结合弱学习树的结果不同，梯度提升按顺序聚合来自不同树的结果。
- en: 'Random forests utilize parallel independent weak learners, while gradient boosting
    employs a sequential approach where each weak learner tree corrects the errors
    of the previous tree. Gradient boosting offers more hyperparameters to fine-tune
    and can achieve superior performance with proper tuning. It also allows for custom
    loss functions, providing flexibility in modeling real-world scenarios. Refer
    to the following figure for an illustration of how gradient boosting trees operate:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林利用并行独立弱学习器，而梯度提升采用一种顺序方法，其中每个弱学习器树纠正前一个树的错误。梯度提升提供了更多的超参数来微调，并且通过适当的调整可以实现卓越的性能。它还允许自定义损失函数，为建模现实世界场景提供了灵活性。参考以下图示了解梯度提升树的工作原理：
- en: '![Figure 3.4 – Gradient boosting ](img/B20836_03_04.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 – 梯度提升](img/B20836_03_04.png)'
- en: 'Figure 3.4: Gradient boosting'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：梯度提升
- en: Gradient boosting offers several key advantages. Firstly, it excels in handling
    imbalanced datasets, making it highly suitable for tasks such as fraud detection
    and risk management. Secondly, it has the potential to achieve higher performance
    than other algorithms when properly tuned. Additionally, gradient boosting supports
    custom loss functions, providing flexibility in modeling real-world applications.
    Lastly, it can effectively capture complex relationships in the data and produce
    accurate predictions. Gradient boosting, despite its advantages, also has some
    limitations to consider. Firstly, due to its sequential nature, it lacks parallelization
    capabilities, making it slower in training compared to algorithms that can be
    parallelized. Secondly, gradient boosting is sensitive to noisy data, including
    outliers, which can lead to overfitting and reduced generalization performance.
    Lastly, the complexity of gradient boosting models can make them less interpretable
    compared to simpler algorithms like decision trees, making it challenging to understand
    the underlying relationships in the data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升提供了几个关键优势。首先，它在处理不平衡数据集方面表现出色，这使得它非常适合欺诈检测和风险管理等任务。其次，当适当调整时，它有可能比其他算法实现更高的性能。此外，梯度提升支持自定义损失函数，为建模现实世界应用提供了灵活性。最后，它能够有效地捕捉数据中的复杂关系并产生准确的预测。尽管梯度提升具有优势，但也存在一些需要考虑的局限性。首先，由于其顺序性，它缺乏并行化能力，因此在训练速度上比可以并行化的算法慢。其次，梯度提升对噪声数据敏感，包括异常值，这可能导致过拟合和降低泛化性能。最后，梯度提升模型的复杂性使其比决策树等简单算法更难以解释，这使得理解数据中的潜在关系变得具有挑战性。
- en: XGBoost, a widely-used implementation of gradient boosting, has gained popularity
    for its success in Kaggle competitions. While it shares the same underlying concept
    as gradient boosting, XGBoost offers several improvements. It enables training
    a single tree across multiple cores and CPUs, leading to faster training times.
    XGBoost incorporates powerful regularization techniques to mitigate overfitting
    and reduce model complexity. It also excels in handling sparse datasets. In addition
    to XGBoost, other popular variations of gradient boosting trees include LightGBM
    and CatBoost.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost，梯度提升的一个广泛使用的实现，因其在大规模Kaggle竞赛中的成功而受到欢迎。虽然它与梯度提升具有相同的基本概念，但XGBoost提供了一些改进。它允许在多个核心和CPU上训练单个树，从而缩短了训练时间。XGBoost结合了强大的正则化技术来减轻过拟合并降低模型复杂性。它还在处理稀疏数据集方面表现出色。除了XGBoost之外，其他流行的梯度提升树变体还包括LightGBM和CatBoost。
- en: K-nearest neighbor algorithm
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K近邻算法
- en: '**K-nearest neighbor** (**K-NN**) is a versatile algorithm used for both classification
    and regression tasks. It is also employed in search systems and recommendation
    systems. The underlying assumption of K-NN is that similar items tend to have
    close proximity to each other in the feature space. To determine this proximity,
    distances between different data points are measured, often using metrics like
    Euclidean distance.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**K近邻**（**K-NN**）是一种多用途算法，用于分类和回归任务。它也被用于搜索系统和推荐系统。K-NN的基本假设是，在特征空间中，相似的项目往往彼此靠近。为了确定这种接近性，测量不同数据点之间的距离，通常使用欧几里得距离等度量。'
- en: In the case of classification, K-NN starts by loading the training data along
    with their respective class labels. When a new data point needs to be classified,
    its distances to the existing data points are calculated, typically using Euclidean
    distance. The K nearest neighbors to the new data point are identified, and their
    class labels are retrieved. The class label for the new data point is then determined
    through majority voting, where the most frequent class among the K nearest neighbors
    is assigned to the new data point.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类的情况下，K-NN算法首先加载训练数据及其相应的类别标签。当需要对新数据点进行分类时，会计算其与现有数据点的距离，通常使用欧几里得距离。确定新数据点最近的K个邻居，并检索它们的类别标签。然后通过多数投票确定新数据点的类别标签，即将K个最近邻居中最频繁的类别分配给新数据点。
- en: 'The following diagram is an illustration of using K-NN for classification:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示展示了如何使用K-NN进行分类：
- en: '![A diagram of a diagram of a diagram  Description automatically generated](img/B20836_03_05.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![一个图示的图示描述自动生成](img/B20836_03_05.png)'
- en: 'Figure 3.5: K-NN for classification'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：K-NN用于分类
- en: For regression tasks, K-NN follows a similar approach. The distances between
    the new data point and the existing data points are computed, and the K nearest
    neighbors are selected. The predicted scalar value for the new data point is obtained
    by averaging the values of the K closest data points.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归任务，K-NN遵循类似的方法。计算新数据点与现有数据点之间的距离，并选择K个最近邻居。通过平均K个最近数据点的值来获得新数据点的预测标量值。
- en: One advantage of K-NN is its simplicity and lack of the need for training or
    tuning with hyperparameters, apart from selecting the value of K. The dataset
    is loaded directly into the model without the need to train a model. It is worth
    noting that the choice of K significantly impacts the performance of the K-NN
    mode. The optimal K is often found through an iterative trial-and-error process
    by evaluation of hold-out dataset. The results of K-NN are also easily explainable,
    as each prediction can be understood by examining the properties of the nearest
    neighbors. However, K-NN has some limitations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN的一个优点是其简单性以及不需要训练或调整超参数（除了选择K的值）。数据集直接加载到模型中，无需训练模型。值得注意的是，K的选择对K-NN模型的表现有显著影响。最优的K值通常通过迭代试错过程，通过评估保留的数据集来找到。K-NN的结果也易于解释，因为每个预测都可以通过检查最近邻居的性质来理解。然而，K-NN也有一些局限性。
- en: As the number of data points increases, the complexity of the model grows, and
    predictions can become slower, especially with large datasets. K-NN is not suitable
    for high-dimensional datasets, as the concept of proximity becomes less meaningful
    in higher-dimensional spaces. The algorithm is also sensitive to noisy data and
    missing data, requiring outlier removal and data imputation techniques to handle
    such cases effectively.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据点的数量增加，模型的复杂性增加，预测可能会变慢，尤其是在大型数据集上。K-NN不适合高维数据集，因为在更高维空间中，邻近的概念变得不那么有意义。该算法对噪声数据和缺失数据也很敏感，需要移除异常值和数据插补技术来有效处理这些情况。
- en: Multi-layer perceptron (MLP) networks
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多层感知器（MLP）网络
- en: 'As mentioned earlier, an **artificial neural network** (**ANN**) emulates the
    learning process of the human brain. The brain comprises numerous interconnected
    neurons that process information. Each neuron in a network processes inputs (electrical
    impulses) from another neuron, processes and transforms the inputs, and sends
    the output to neurons in the network. Here is an illustration depicting a human
    neuron:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，**人工神经网络**（**ANN**）模拟了人脑的学习过程。大脑由众多相互连接的神经元组成，它们处理信息。网络中的每个神经元都从另一个神经元接收输入（电脉冲），处理并转换输入，然后将输出发送到网络中的其他神经元。以下是人脑神经元的示意图：
- en: '![Figure 3.5 – Human brain neuron ](img/B20836_03_06.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5 – 人脑神经元](img/B20836_03_06.png)'
- en: 'Figure 3.6: Human brain neuron'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：人脑神经元
- en: An artificial neuron operates in a similar manner. The following diagram illustrates
    an artificial neuron, which consists of a linear function combined with an activation
    function. The activation function modifies the output of the linear function,
    such as compressing it within a specific range, such as 0 to 1 (sigmoid activation),
    -1 to 1 (tanh activation), or maintaining values above 0 (ReLU). The activation
    function is employed to capture non-linear relationships between inputs and outputs.
    Alternatively, each neuron can be viewed as a linear classifier, akin to logistic
    regression.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元以类似的方式运作。以下图示展示了一个人工神经元，它由一个线性函数与一个激活函数组合而成。激活函数会修改线性函数的输出，例如将其压缩在特定范围内，如0到1（Sigmoid激活），-1到1（Tanh激活），或保持值在0以上（ReLU）。激活函数被用来捕捉输入和输出之间的非线性关系。或者，每个神经元也可以被视为一个线性分类器，类似于逻辑回归。
- en: '![Figure 3.6 – Artificial neuron ](img/B20836_03_07.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图3.6 – 人工神经元](img/B20836_03_07.png)'
- en: 'Figure 3.7: Artificial neuron'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7：人工神经元
- en: 'When you stack a large number of neurons into different layers (*input layer*,
    *hidden layers*, and *output layer*) and connect all of the neurons together between
    two adjacent layers, we have an ANN called **multi-layer perceptron** (**MLP**).
    Here, the term *perceptron* means *artificial neuron*, and it was originally invented
    by Frank Rosenblatt in 1957\. The idea behind MLP is that each hidden layer will
    learn some higher-level representation (features) of the previous layer, and those
    higher-level features capture the more important information in the previous layer.
    When the output from the final hidden layer is used for prediction, the network
    has extracted the most important information from the raw inputs for training
    a classifier or regressor. The following figure shows the architecture of an MLP
    network:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当将大量神经元堆叠成不同的层（*输入层*、*隐藏层*和*输出层*）并将所有神经元在相邻层之间连接起来时，我们就得到了一个称为**多层感知器**（**MLP**）的ANN。在这里，“感知器”一词意味着“人工神经元”，它最初由Frank
    Rosenblatt于1957年发明。MLP背后的理念是每个隐藏层将学习前一层的一些高级表示（特征），这些高级特征捕捉了前一层的更重要的信息。当使用最终隐藏层的输出进行预测时，网络已经从原始输入中提取了最重要的信息，用于训练分类器或回归器。以下图示展示了MLP网络的架构：
- en: '![Figure 3.7 – Multi-layer perceptron ](img/B20836_03_08.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7 – 多层感知器](img/B20836_03_08.png)'
- en: 'Figure 3.8: Multi-layer perceptron'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8：多层感知器
- en: During model training, the weights (*W*) of each neuron in every layer are adjusted
    using gradient descent to optimize the training objective. This adjustment process
    is known as backpropagation. It involves propagating the total error back through
    the network, attributing a portion of the error to each neuron based on its contribution.
    This allows the fine-tuning of the weights in each neuron, ensuring that every
    neuron in every layer influences the final output to improve overall performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练过程中，通过梯度下降调整每一层中每个神经元的权重（*W*），以优化训练目标。这种调整过程被称为反向传播。它涉及将总误差反向传播通过网络，根据每个神经元的贡献将一部分误差分配给每个神经元。这允许对每个神经元中的权重进行微调，确保每一层的每个神经元都对最终输出产生影响，从而提高整体性能。
- en: MLP is a versatile neural network suitable for both classification and regression
    tasks, similar to random forest and XGBoost. While commonly applied to tabular
    data, it can also handle diverse data formats like images and text. MLP excels
    in capturing intricate nonlinear patterns within the dataset and exhibits efficient
    computational processing, thanks to its parallelization capabilities. However,
    MLP typically demands a larger training dataset to achieve optimal performance
    compared to traditional ML algorithms.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: MLP是一种多才多艺的神经网络，适用于分类和回归任务，类似于随机森林和XGBoost。虽然通常应用于表格数据，但它也可以处理各种数据格式，如图像和文本。MLP在捕捉数据集中的复杂非线性模式方面表现出色，并因其并行化能力而具有高效的计算处理能力。然而，与传统的机器学习算法相比，MLP通常需要更大的训练数据集才能达到最佳性能。
- en: Algorithms for clustering
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类算法
- en: Clustering is a data mining method that involves grouping items together based
    on their shared attributes. One practical application of clustering is to create
    customer segments by analyzing demographics, transaction history, or behavior
    data. Other examples include social network analysis, document grouping, and anomaly
    detection. Various clustering algorithms exist, and we will focus on the K-means
    clustering algorithm in this section, which is one of the most widely used clustering
    algorithms due to its simplicity. Some other popular clustering algorithms are
    hierarchical clustering and DBSCAN.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种数据挖掘方法，它涉及根据共享属性将项目分组在一起。聚类的一个实际应用是通过分析人口统计、交易历史或行为数据来创建客户细分。其他例子包括社交网络分析、文档分组和异常检测。存在各种聚类算法，在本节中我们将重点关注K-means聚类算法，这是最广泛使用的聚类算法之一，因为它简单易行。其他一些流行的聚类算法包括层次聚类和DBSCAN。
- en: K-means algorithm
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: K-means算法
- en: The K-means algorithm is widely employed in real-world applications, including
    customer segmentation analysis, document classification based on document attributes,
    and insurance fraud detection. It is a versatile algorithm that can effectively
    group data points in various domains for different purposes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: K-means算法在现实世界的应用中得到了广泛的应用，包括客户细分分析、基于文档属性的文档分类和保险欺诈检测。它是一种多用途算法，可以有效地将数据点分组到各种领域，用于不同的目的。
- en: K-means aims to group similar data points together in clusters, and it is an
    unsupervised algorithm, meaning it doesn’t rely on labeled data. The algorithm
    begins by randomly assigning K centroids, which represent the centers of the clusters.
    It then iteratively adjusts the assignment of data points to the nearest centroid
    and updates the centroids to the mean of the data points in each cluster. This
    process continues until convergence, resulting in well-defined clusters based
    on similarity.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: K-means算法旨在将相似的数据点分组到簇中，它是一种无监督算法，这意味着它不依赖于标记数据。算法首先随机分配K个质心，这些质心代表簇的中心。然后，它迭代地调整数据点到最近质心的分配，并将质心更新为每个簇中数据点的平均值。这个过程一直持续到收敛，从而根据相似性形成定义良好的簇。
- en: K-means clustering offers several advantages, including its simplicity and ease
    of understanding, making it accessible to beginners. It is computationally efficient
    and can handle large datasets effectively. The resulting clusters are interpretable,
    providing valuable insights into the underlying patterns in the data. K-means
    is versatile and applicable to various types of data, including numerical, categorical,
    and mixed attribute datasets. However, there are some drawbacks to consider. Selecting
    the optimal number of clusters (*K*) can be subjective and challenging. The algorithm
    is sensitive to the initial placement of centroids, which can lead to different
    cluster formations. K-means assumes spherical clusters with equal variance, which
    may not hold true in all cases. It is also sensitive to outliers and struggles
    with non-linear data relationships.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类具有几个优点，包括其简单性和易于理解，这使得它对初学者来说易于访问。它是计算高效的，可以有效地处理大型数据集。生成的簇是可解释的，为数据中的潜在模式提供了有价值的见解。K-means算法灵活多样，适用于各种类型的数据，包括数值、分类和混合属性数据集。然而，也有一些缺点需要考虑。选择最佳簇数（*K*）可能是主观的且具有挑战性。该算法对质心的初始位置敏感，可能导致不同的簇形成。K-means假设簇是球形的且具有相等的方差，这在所有情况下可能并不成立。它对异常值也很敏感，并且在与非线性数据关系作斗争时存在困难。
- en: Algorithms for time series analysis
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列分析算法
- en: A time series consists of a sequence of data points recorded at successive time
    intervals. It is commonly used to analyze and predict trends in various domains,
    such as finance, retail, and sales. Time series analysis allows us to understand
    past patterns and make future predictions based on the relationship between current
    and past values. Forecasting in time series relies on the assumption that future
    values are influenced by previous observations at different time points.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列是由在连续时间间隔内记录的一系列数据点组成的。它通常用于分析预测各个领域的趋势，如金融、零售和销售。时间序列分析使我们能够理解过去的模式，并根据当前值与过去值之间的关系进行未来预测。时间序列预测依赖于未来值受不同时间点先前观察影响的假设。
- en: Time series data exhibits several important characteristics, including trend,
    seasonality, and stationarity. **Trend** refers to the long-term direction of
    the data, whether it shows an overall increase or decrease over time. It helps
    to identify the underlying pattern and understand the general behavior of the
    time series. **Seasonality**, on the other hand, captures repeating patterns within
    a fixed interval, often occurring in cycles or seasons. It helps to identify regular
    fluctuations that repeat over specific time periods, such as daily, weekly, or
    yearly patterns. **Stationarity** refers to the property of a time series where
    statistical properties, such as mean and variance, remain constant over time.
    Stationarity is crucial because many forecasting techniques assume that the underlying
    data is stationary. Non-stationary time series can lead to inaccurate or unreliable
    forecasts. Therefore, it is important to assess and address the stationarity of
    a time series before applying forecasting techniques.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据表现出几个重要的特征，包括趋势、季节性和平稳性。**趋势**指的是数据的长期方向，它表示数据随时间是否呈现整体增加或减少。它有助于识别潜在的模式并理解时间序列的一般行为。另一方面，**季节性**捕捉固定间隔内的重复模式，通常在周期或季节中发生。它有助于识别在特定时间周期内重复的规律性波动，如每日、每周或年度模式。**平稳性**指的是时间序列的属性，其中统计属性，如均值和方差，随时间保持恒定。平稳性至关重要，因为许多预测技术假设基础数据是平稳的。非平稳时间序列可能导致预测不准确或不可靠。因此，在应用预测技术之前，评估和解决时间序列的平稳性是很重要的。
- en: ARIMA algorithm
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ARIMA算法
- en: The **autoregressive integrated moving average** (**ARIMA**) algorithm finds
    practical applications in various real-world scenarios, including budget forecasting,
    sales forecasting, patient visit forecasting, and customer support call volume
    forecasting. ARIMA is a powerful tool for analyzing and predicting time series
    data, allowing organizations to make informed decisions and optimize their operations
    in these areas. By leveraging historical patterns and trends in the data, ARIMA
    enables accurate forecasts and assists businesses in effectively managing their
    resources and planning for the future.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**自回归积分移动平均**（**ARIMA**）算法在各种实际场景中有实际应用，包括预算预测、销售预测、患者就诊预测和客户支持呼叫量预测。ARIMA是分析预测时间序列数据的有力工具，允许组织在这些领域做出明智的决策并优化其运营。通过利用数据中的历史模式和趋势，ARIMA可以实现准确的预测，并帮助企业在有效管理资源和规划未来方面发挥作用。'
- en: 'ARIMA operates on the premise that the value of a variable in a given period
    is influenced by its own previous values (autoregressive), the deviations from
    the mean follow a pattern based on previous deviations (moving average), and trend
    and seasonality can be eliminated by differencing (calculating the differences
    between consecutive data points). This differencing process aims to transform
    the time series into a stationary state, where statistical properties like mean
    and variance remain constant over time. These three components of ARIMA can be
    mathematically represented using the following formulas:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ARIMA基于以下前提运作：给定时期变量的值受其自身先前值（自回归）的影响，与平均值之间的偏差遵循基于先前偏差的模式（移动平均），通过差分（计算连续数据点之间的差异）可以消除趋势和季节性。这个差分过程旨在将时间序列转换为平稳状态，其中统计属性如均值和方差随时间保持恒定。ARIMA的这三个组件可以用以下公式进行数学表示：
- en: '![](img/B20836_03_004.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20836_03_004.png)'
- en: 'Where the **autoregressive** (**AR**) component is expressed as a regression
    of previous values (also known as lags):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，**自回归**（**AR**）组件表示为先前值的回归（也称为滞后）：
- en: '![](img/B20836_03_005.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20836_03_005.png)'
- en: 'The constant *C* represents a drift:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 常数*C*代表漂移：
- en: '![](img/B20836_03_006.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20836_03_006.png)'
- en: 'The **moving average** (**MA**) component is expressed as a weighted average
    of forecasting errors for the previous time periods, where it represents a constant:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**移动平均**（**MA**）组件表示为先前时间周期的预测误差的加权平均值，其中它代表一个常数：'
- en: '![](img/B20836_03_007.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20836_03_007.png)'
- en: The **integrated component** (time series differencing) of a time series can
    be expressed as the difference between the values in one period from the previous
    period.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列的**集成组件**（时间序列差分）可以表示为前一个周期内值与当前周期内值的差。
- en: ARIMA is a suitable choice for forecasting single time series (univariate) data
    as it doesn’t rely on additional variables. It outperforms simpler forecasting
    techniques like simple moving average, exponential smoothing, or linear regression.
    Additionally, ARIMA provides interpretability, allowing for a clear understanding
    of the underlying patterns. However, due to its backward-looking nature, ARIMA
    may struggle to accurately forecast unexpected events. Furthermore, being a linear-based
    model, ARIMA may not effectively capture complex non-linear relationships in time
    series data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ARIMA是预测单时间序列（单变量）数据的合适选择，因为它不依赖于其他变量。它优于简单的预测技术，如简单移动平均、指数平滑或线性回归。此外，ARIMA提供了可解释性，使得对潜在模式有清晰的理解。然而，由于其向后看的本质，ARIMA可能难以准确预测意外事件。此外，作为一个基于线性模型的模型，ARIMA可能无法有效地捕捉时间序列数据中的复杂非线性关系。
- en: DeepAR algorithm
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepAR算法
- en: Deep learning-based forecasting algorithms offer solutions to the limitations
    of traditional models like ARIMA. They excel at capturing complex non-linear relationships
    and can effectively utilize multivariate datasets. These models enable the training
    of a global model, allowing for a single model to handle multiple similar target
    time series. This eliminates the need for creating separate models for each individual
    time series, providing a more efficient and scalable approach.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的预测算法为传统模型如ARIMA的局限性提供了解决方案。它们擅长捕捉复杂的非线性关系，并能有效地利用多变量数据集。这些模型能够训练一个全局模型，使得一个模型可以处理多个相似的目标时间序列。这消除了为每个单独的时间序列创建单独模型的需求，提供了一种更高效和可扩展的方法。
- en: '**Deep Autoregressive** (**DeepAR**) is a state-of-the-art forecasting algorithm
    based on neural networks, designed to handle large datasets with multiple similar
    target time series. It has the capability to incorporate related time series,
    such as product prices or holiday schedules, to enhance the accuracy of its forecasting
    models. This feature proves particularly valuable when dealing with spiky events
    triggered by external variables, allowing for more precise and reliable predictions.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度自回归**（**DeepAR**）是一种基于神经网络的先进预测算法，旨在处理具有多个相似目标时间序列的大数据集。它具有结合相关时间序列的能力，例如产品价格或假日安排，以增强其预测模型的准确性。当处理由外部变量触发的峰值事件时，这一特性尤其有价值，它允许进行更精确和可靠的预测。'
- en: DeepAR utilizes a **recurrent neural network** (**RNN**) as its underlying model
    to capture patterns in the target time series. It goes beyond single-variable
    forecasting by incorporating multiple target time series and additional external
    supporting time series. Instead of considering individual values, the RNN takes
    input vectors representing the values of various variables at each time period.
    By jointly learning the patterns of these combined vectors over time, DeepAR can
    effectively capture the intrinsic non-linear relationships and shared patterns
    among the different time series. This approach enables DeepAR to train a single
    global model that can be used for forecasting across multiple similar target time
    series.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAR利用**循环神经网络**（**RNN**）作为其底层模型来捕捉目标时间序列中的模式。它通过结合多个目标时间序列和额外的外部支持时间序列超越了单变量预测。RNN不是考虑单个值，而是接受代表每个时间周期各种变量值的输入向量。通过联合学习这些组合向量随时间变化的模式，DeepAR可以有效地捕捉不同时间序列之间的内在非线性关系和共享模式。这种方法使得DeepAR能够训练一个单一的全局模型，该模型可用于跨多个相似的目标时间序列进行预测。
- en: DeepAR excels in handling complex multivariate datasets; however, it performs
    best when trained with large amounts of data. It is particularly useful in real-world
    scenarios involving large-scale retail forecasting for numerous items, where external
    factors like marketing campaigns and holiday schedules need to be taken into account.
    By leveraging its capability to model multiple variables simultaneously, DeepAR
    can provide accurate predictions and insights in such practical use cases.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAR在处理复杂的多变量数据集方面表现出色；然而，当使用大量数据进行训练时，它的表现最佳。它在涉及大规模零售预测的实际情况中特别有用，例如对众多商品的预测，需要考虑外部因素如营销活动和假日安排。通过利用其同时建模多个变量的能力，DeepAR可以在这种实际应用场景中提供准确的预测和见解。
- en: A significant drawback of DeepAR is the black-box nature of the deep learning
    model, which lacks interpretability and transparency. This makes the forecasts
    more difficult to explain and justify than simpler statistical methods. Another
    major disadvantage is the data-hungry nature of DeepAR, whereby it performs poorly
    when the dataset is small.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAR的一个显著缺点是深度学习模型的黑盒性质，它缺乏可解释性和透明度。这使得预测比简单的统计方法更难以解释和证明。另一个主要缺点是DeepAR对数据的贪婪性，当数据集较小时，其表现不佳。
- en: Algorithms for recommendation
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐算法
- en: The recommender system is an essential ML technology that predicts a user’s
    preference for items, primarily relying on user or item attribute similarities
    or user-item interactions. It has gained widespread adoption in various industries,
    including retail, media and entertainment, finance, and healthcare. Over the years,
    the field of recommendation algorithms has evolved significantly, from making
    recommendations based on preferences and behaviors of similar users, to a reinforcement-learning-based
    approach where algorithms learn to make sequential decisions over time, taking
    into account user feedback and interaction. In the following section, we will
    explore some commonly used algorithms in the realm of recommender systems.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是一种关键的机器学习技术，它预测用户对物品的偏好，主要依赖于用户或物品属性的相似性或用户-物品交互。它在零售、媒体和娱乐、金融和医疗保健等各个行业中得到了广泛的应用。多年来，推荐算法领域已经发生了显著的变化，从基于相似用户的偏好和行为进行推荐，发展到基于强化学习的方法，其中算法随着时间的推移学习做出连续的决策，同时考虑用户反馈和交互。在下一节中，我们将探讨推荐系统领域的一些常用算法。
- en: Collaborative filtering algorithm
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 协同过滤算法
- en: '**Collaborative filtering** is a popular recommendation algorithm that leverages
    the notion that individuals with similar interests or preferences in one set of
    items are likely to have similar interests in other items as well. By analyzing
    the collective experiences and behaviors of different users, collaborative filtering
    can effectively recommend items to individual users based on the preferences of
    similar users. This approach taps into the experiences of the crowd to provide
    personalized and relevant recommendations.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**协同过滤**是一种流行的推荐算法，它利用了这样一个观点：在一组物品中具有相似兴趣或偏好的个人，在其他物品中也可能具有相似的兴趣。通过分析不同用户的集体经验和行为，协同过滤可以有效地根据相似用户的偏好为单个用户推荐物品。这种方法利用了群体的经验，提供个性化的相关推荐。'
- en: 'The following figure illustrates an item-user interaction matrix in the context
    of movie ratings. As you can see, it is a **sparse matrix**. This means that there
    are many empty entries in the matrix, which is expected as it is unlikely for
    any individual to have watched every movie:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了电影评分背景下的物品-用户交互矩阵。如图所示，它是一个**稀疏矩阵**。这意味着矩阵中有许多空项，这是预期的，因为任何个人都不太可能观看每一部电影：
- en: '![Figure 3.8 – User-item interaction matrix for collaborative filtering ](img/B20836_03_09.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图3.8 – 协同过滤的用户-物品交互矩阵](img/B20836_03_09.png)'
- en: 'Figure 3.9: User-item interaction matrix for collaborative filtering'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9：协同过滤的用户-物品交互矩阵
- en: One of the major benefits of collaborative filtering is that it can provide
    highly personalized recommendations matched to each user’s unique interests. Unlike
    content-based systems, collaborative filtering models do not need to analyze and
    understand item features and content. Instead, they rely solely on behavioral
    patterns like ratings, purchases, clicks, and preferences across users to uncover
    correlations. This allows collaborative systems to get a nuanced profile of a
    user’s likes and dislikes based on crowd wisdom. The algorithms can then generate
    recommendations tailored to that specific user, going beyond obvious suggestions.
    This level of personalization and ability to capture user preferences makes collaborative
    filtering a powerful approach, especially for large catalogs where analyzing content
    is infeasible.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤的主要优点之一是它可以提供高度个性化的推荐，与每个用户的独特兴趣相匹配。与基于内容的系统不同，协同过滤模型不需要分析和理解物品特征和内容。相反，它们完全依赖于行为模式，如评分、购买、点击和用户之间的偏好，以揭示相关性。这使得协同系统能够根据群体智慧获得用户喜好的细致轮廓。然后，算法可以生成针对特定用户的定制推荐，超越明显的建议。这种个性化的程度和捕捉用户偏好的能力使协同过滤成为一种强大的方法，尤其是在分析内容不可行的庞大目录中。
- en: 'Collaborative filtering also comes with some notable downsides. A major issue
    is the cold-start problem: collaborative models struggle when new users or items
    with no ratings are introduced. The algorithms rely heavily on crowd ratings,
    so they cannot effectively recommend to users new items that lack this historical
    data. Collaborative systems can also lead to limited diversity, creating filter
    bubbles and obvious recommendations rather than novel ones. They commonly face
    sparsity issues as the user-item matrix is often sparse, especially for large
    catalogs.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤也有一些明显的缺点。一个主要问题是冷启动问题：当引入新用户或没有评分的项目时，协同模型会面临挑战。算法高度依赖于群体评分，因此它们无法有效地向缺乏历史数据的用户推荐新项目。协同系统也可能导致多样性有限，形成过滤气泡和明显的推荐而不是新颖的推荐。它们通常面临稀疏性问题，因为用户-项目矩阵通常是稀疏的，尤其是在大型目录中。
- en: '**Matrix factorization** is a technique commonly used in collaborative filtering
    for recommendation systems. It involves learning vector representations, or embeddings,
    for both users and items in the user-item interaction matrix. The goal is to approximate
    the original matrix by taking the product of the learned user and item embedding
    matrices.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵分解**是协同过滤推荐系统中常用的一种技术。它涉及在用户-项目交互矩阵中学习用户和项目的向量表示或嵌入。目标是通过对学习的用户和项目嵌入矩阵进行乘积来近似原始矩阵。'
- en: This allows us to predict the missing entries in the matrix, which represent
    the likely ratings that a user would give to unseen items. To make predictions,
    we simply compute the dot product between the user embedding and the item embedding.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够预测矩阵中的缺失条目，这些条目代表用户对未见项目可能给出的评分。为了进行预测，我们只需计算用户嵌入和项目嵌入之间的点积。
- en: Embedding is a fundamental concept in ML that plays a crucial role in various
    domains. It involves creating numerical representations for entities, such as
    words or objects, in a manner that captures their semantic similarity. These representations
    are organized in a multi-dimensional space, where similar entities are positioned
    closer to each other. By using embeddings, we can uncover the underlying latent
    semantics of the objects, enabling more effective analysis and modeling. In the
    upcoming sections, we will delve deeper into embedding techniques and their applications
    in NLP algorithms.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是机器学习中的一个基本概念，在各个领域发挥着至关重要的作用。它涉及以捕捉实体（如单词或对象）的语义相似性的方式创建实体的数值表示。这些表示组织在一个多维空间中，其中相似的实体彼此靠近。通过使用嵌入，我们可以揭示对象的潜在语义，从而实现更有效的分析和建模。在接下来的章节中，我们将深入了解嵌入技术及其在自然语言处理算法中的应用。
- en: Matrix factorization provides major scalability benefits so collaborative filtering
    can be applied to extremely large catalogs. However, the algorithm loses some
    transparency due to the latent factor modeling. Overall, matrix factorization
    extends collaborative filtering to much bigger datasets but sacrifices some interpretability.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解提供了主要的可扩展性优势，因此协同过滤可以应用于极其庞大的目录。然而，由于潜在因子建模，该算法失去了一些透明度。总的来说，矩阵分解将协同过滤扩展到更大的数据集，但牺牲了一些可解释性。
- en: Multi-armed bandit/contextual bandit algorithm
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多臂老虎机/上下文老虎机算法
- en: Collaborative filtering-based recommender systems heavily rely on prior interaction
    data between identified users and items to make accurate recommendations. However,
    these systems face challenges when there is a lack of prior interactions or when
    the user is anonymous, resulting in a cold-start problem. To address this issue,
    one approach is to utilize a **multi-armed bandit** (**MAB**) based recommendation
    system. This approach draws inspiration from the concept of trial and error, similar
    to a gambler simultaneously playing multiple slot machines and observing which
    machine yields the best overall return. By employing reinforcement learning techniques,
    MAB-based recommendation systems dynamically explore and exploit different recommendations
    to optimize the user experience, even in the absence of substantial prior interaction
    data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于协同过滤的推荐系统严重依赖于已识别用户和项目之间的先前交互数据来做出准确的推荐。然而，当缺乏先前交互或用户匿名时，这些系统面临挑战，导致冷启动问题。为了解决这个问题，一种方法是基于**多臂老虎机**（**MAB**）的推荐系统。这种方法从试错的概念中汲取灵感，类似于一个同时玩多个老虎机和观察哪个机器产生最佳整体回报的赌徒。通过采用强化学习技术，基于MAB的推荐系统可以动态探索和利用不同的推荐来优化用户体验，即使在缺乏大量先前交互数据的情况下。
- en: MAB algorithms operate under the paradigm of online learning, where there is
    no pre-existing training data to train a model prior to deployment. Instead, the
    model incrementally learns and adapts as data becomes available. In the initial
    stages of MAB learning, the model recommends all available options (such as products
    on an e-commerce site) with equal probabilities to users. As users begin to interact
    with a subset of the items and provide feedback (rewards), the MAB model adjusts
    its strategy. It starts to offer items that have yielded higher rewards (e.g.,
    more user interactions) more frequently, exploiting the knowledge of their positive
    performance.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: MAB算法在在线学习的范式下运行，其中在部署模型之前没有预先存在的训练数据。相反，模型随着数据的可用性逐渐学习和适应。在MAB学习的初始阶段，模型以相等的概率向用户推荐所有可用的选项（例如电子商务网站上的产品）。随着用户开始与项目的一部分进行互动并提供反馈（奖励），MAB模型调整其策略。它开始更频繁地提供产生更高奖励的项目（例如，更多用户互动），利用它们积极表现的已知知识。
- en: However, the model also continues to allocate a smaller percentage of recommendations
    to new items, aiming to explore their potential for receiving interactions. This
    balance between exploration (offering new items) and exploitation (offering items
    with known rewards) is a fundamental tradeoff in MAB algorithms.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，该模型仍然继续将较小比例的推荐分配给新项目，旨在探索它们获得互动的潜力。这种在探索（提供新项目）和利用（提供已知奖励的项目）之间的平衡是MAB算法中的一个基本权衡。
- en: MAB algorithms face several limitations. Striking the right balance between
    exploration and exploitation can be challenging, leading to suboptimal solutions
    in certain environments. Handling high-dimensional contextual information poses
    a challenge as well, and the algorithms may be sensitive to noisy rewards. Additionally,
    the cold-start problem arises when there is limited historical data for new items
    or users.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: MAB算法面临几个限制。在探索和利用之间取得正确的平衡可能具有挑战性，导致在某些环境中出现次优解。处理高维上下文信息也是一个挑战，算法可能对噪声奖励敏感。此外，当新项目或用户的历史数据有限时，会出现冷启动问题。
- en: Algorithms for computer vision problems
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉问题的算法
- en: Computer vision refers to the ability of computers to interpret and understand
    visual representations, such as images and videos, in order to perform tasks like
    object identification, image classification, text detection, face recognition,
    and activity detection. These tasks rely on pattern recognition, where images
    are labeled with object names and bounding boxes, and computer vision models are
    trained to recognize these patterns and make predictions on new images. Computer
    vision technology finds numerous applications in practical domains such as content
    management, security, augmented reality, self-driving cars, medical diagnosis,
    sports analytics, and quality inspection in manufacturing. In the following section,
    we will delve deeper into a few neural network architectures specifically designed
    for computer vision tasks.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是指计算机解释和理解视觉表示的能力，例如图像和视频，以便执行诸如对象识别、图像分类、文本检测、人脸识别和活动检测等任务。这些任务依赖于模式识别，其中图像被标注为对象名称和边界框，计算机视觉模型被训练以识别这些模式并在新图像上做出预测。计算机视觉技术在内容管理、安全、增强现实、自动驾驶汽车、医疗诊断、体育分析和制造业的质量检验等实际领域有众多应用。在下一节中，我们将深入探讨一些专门为计算机视觉任务设计的神经网络架构。
- en: Although the upcoming sections involve deep learning architectures, embeddings,
    and other techniques—elements that may not strictly conform to the traditional
    definition of algorithms—we will be referring to them as “algorithms” for the
    sake of semantic consistency throughout this chapter. With this, we hope to facilitate
    a smoother understanding of the nuanced concepts we’ll be exploring.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管即将到来的部分涉及深度学习架构、嵌入和其他技术——这些元素可能并不严格符合传统算法的定义——但为了在整个章节中保持语义一致性，我们将它们称为“算法”。通过这种方式，我们希望促进对我们将要探讨的细微概念的更流畅理解。
- en: Convolutional neural networks
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: A **convolutional neural network** (**CNN**) is a deep learning architecture
    specifically designed for processing and analyzing image data. It takes inspiration
    from the functioning of the animal visual cortex. In the visual cortex, individual
    neurons respond to visual stimuli within specific subregions of the visual field.
    These subregions, covered by different neurons, partially overlap to cover the
    entire visual field. Similarly, in a CNN, different filters are applied to interact
    with subregions of an image, capturing and responding to the information within
    that region. This allows the CNN to extract meaningful features and patterns from
    the image data.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNN**）是一种专门设计用于处理和分析图像数据的深度学习架构。它从动物视觉皮层的功能中汲取灵感。在视觉皮层中，单个神经元对视觉场中特定子区域的视觉刺激做出反应。这些由不同神经元覆盖的子区域部分重叠，以覆盖整个视觉场。同样，在CNN中，不同的滤波器被应用于与图像的子区域交互，捕捉并响应该区域内的信息。这使得CNN能够从图像数据中提取有意义的特征和模式。'
- en: A CNN architecture consists of multiple layers that repeat in a pattern. Each
    layer has different sublayers with specific functions. The convolutional layer
    plays a crucial role in feature extraction from input images. It utilizes convolutional
    filters, which are matrices defined by height and width, to extract relevant features.
    These convolutional layers process the input images by convolving them with the
    filters, producing feature maps that are passed to the next layer in the network.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: CNN架构由多个层组成，这些层按照一定模式重复。每一层都有不同的子层，具有特定的功能。卷积层在从输入图像中提取特征方面起着至关重要的作用。它利用卷积滤波器，这些滤波器是由高度和宽度定义的矩阵，以提取相关特征。这些卷积层通过将滤波器与输入图像进行卷积处理输入图像，产生特征图，并将其传递到网络中的下一层。
- en: The pooling layer, found after one or multiple convolutional layers, reduces
    the dimensionality of the extracted features. It combines multiple outputs into
    a single output, resulting in a more compact representation. Two commonly used
    pooling techniques are max pooling, which selects the maximum value from the outputs,
    and average pooling, which calculates the average value.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个或多个卷积层之后，发现了一个池化层，它减少了提取的特征的维度。它将多个输出组合成一个输出，从而得到一个更紧凑的表示。两种常用的池化技术是最大池化，它从输出中选择最大值，以及平均池化，它计算平均值。
- en: Following the convolutional and pooling layers, a fully connected layer is employed
    to combine and flatten the outputs from the previous layer. This layer aggregates
    the extracted features and feeds them into an output layer, typically used for
    tasks like image classification.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积和池化层之后，使用一个全连接层来组合和展平前一层输出的结果。该层聚合提取的特征，并将它们输入到输出层，通常用于图像分类等任务。
- en: 'The architecture of a CNN is illustrated in the following figure, showcasing
    the flow of information through the various layers:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的架构在以下图中展示，展示了信息通过各个层的流动：
- en: '![Figure 3.9 – CNN architecture ](img/B20836_03_10.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9 – CNN架构](img/B20836_03_10.png)'
- en: 'Figure 3.10: CNN architecture'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10：CNN架构
- en: CNN-based models offer efficient training due to their high degree of parallelism.
    This is particularly advantageous for tasks involving large-scale image data,
    where parallel processing can significantly accelerate training time. While CNNs
    are primarily used for computer vision tasks, their success has led to their application
    in other domains as well, including natural language processing. By adapting the
    principles of convolution and hierarchical feature extraction, CNNs have shown
    promise in tasks such as text classification and sentiment analysis. This demonstrates
    the versatility and effectiveness of CNN-based models beyond their traditional
    application in computer vision.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于CNN具有高度的并行性，因此基于CNN的模型提供了高效的训练。这对于涉及大规模图像数据的任务特别有利，因为并行处理可以显著加快训练时间。虽然CNN主要用于计算机视觉任务，但它们的成功已经导致它们在其他领域也得到了应用，包括自然语言处理。通过适应卷积和层次特征提取的原则，CNN在文本分类和情感分析等任务中显示出希望。这证明了基于CNN的模型在计算机视觉传统应用之外的灵活性和有效性。
- en: CNNs have their limitations. CNNs lack interpretability due to their complex
    architecture, behaving like black boxes. This makes them unsuitable when model
    explainability is critical. In addition, CNNs require large training datasets
    to properly learn features and avoid overfitting. Their performance suffers significantly
    on smaller datasets.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs有其局限性。由于复杂的架构，CNNs缺乏可解释性，表现得像黑盒。这使得当模型可解释性至关重要时，它们不适用。此外，CNNs需要大量的训练数据集来正确学习特征并避免过拟合。它们在较小数据集上的性能显著下降。
- en: ResNet
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ResNet
- en: As computer vision tasks grow in complexity, the addition of more layers in
    CNNs enhances their capability for image classification by enabling the learning
    of increasingly intricate features. However, as the number of layers increases
    in a CNN architecture, performance may deteriorate. This is commonly referred
    to as the **vanishing gradient** problem, where signals originating from the initial
    inputs, including crucial information, gradually diminish as they traverse through
    multiple layers of the CNN.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算机视觉任务复杂性的增加，在卷积神经网络（CNNs）中增加更多层可以增强其图像分类能力，通过学习越来越复杂的特征。然而，随着CNN架构中层数的增加，性能可能会下降。这通常被称为**梯度消失**问题，其中来自初始输入的信号，包括关键信息，在穿过CNN的多个层时逐渐减弱。
- en: '**Residual networks** (**ResNet**) address the vanishing gradient problem by
    implementing a layer-skipping technique. Rather than processing signals sequentially
    through each layer, ResNet introduces skip connections that allow signals to bypass
    certain layers. This can be visualized as a highway with fewer exits, enabling
    the signals from earlier layers to be preserved and carried forward without loss.
    The ResNet architecture is depicted in the following figure.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**残差网络**（**ResNet**）通过实现层跳过技术来解决梯度消失问题。ResNet不是按顺序通过每一层处理信号，而是引入了跳过连接，允许信号绕过某些层。这可以想象成一条出口较少的高速公路，使得早期层的信号得以保留并向前传递而不会损失。ResNet架构在以下图中展示。'
- en: '![Figure 3.10 – ResNet architecture ](img/B20836_03_11.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图3.10 – ResNet架构](img/B20836_03_11.png)'
- en: 'Figure 3.11: ResNet architecture'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11：ResNet架构
- en: ResNet can be used for different computer vision tasks such as *image classification*,
    *object detection* (detecting all objects in a picture), and producing models
    with much higher accuracy than a vanilla CNN network. However, a potential disadvantage
    of ResNet is increased computational complexity due to the introduction of skip
    connections. The additional connections require more memory and computational
    resources, making training and inference more computationally expensive compared
    to shallower architectures.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet可用于不同的计算机视觉任务，如**图像分类**、**目标检测**（检测图片中的所有对象）以及产生比传统CNN网络精度更高的模型。然而，ResNet的一个潜在缺点是由于引入了跳过连接，计算复杂度增加。额外的连接需要更多的内存和计算资源，使得训练和推理比浅层架构更昂贵。
- en: Algorithms for natural language processing (NLP) problems
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）问题的算法
- en: NLP focuses on the relationship between computers and human language. It involves
    the processing and analysis of extensive amounts of natural language data with
    the objective of enabling computers to comprehend the meaning behind human language
    and extract valuable information from it. NLP encompasses a wide range of tasks
    within the field of data science. Some of these tasks include document classification,
    topic modeling, converting speech to text, generating speech from text, extracting
    entities from text, language translation, understanding and answering questions,
    reading comprehension, and language generation.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）关注计算机与人类语言之间的关系。它涉及处理和分析大量自然语言数据，目的是使计算机能够理解人类语言背后的含义并从中提取有价值的信息。NLP涵盖了数据科学领域内的广泛任务。其中一些任务包括文档分类、主题建模、语音转文本、文本生成语音、从文本中提取实体、语言翻译、理解和回答问题、阅读理解以及语言生成。
- en: 'ML algorithms cannot process raw text data directly. To train NLP models effectively,
    it is necessary to convert the words within an input text into numerical representations
    within the context of other words, sentences, or documents. Before the advancement
    of embedding, there were two widely used methods for representing the relevance
    of words in a text: **bag-of-words** (**BOW**) and term **frequency–inverse document
    frequency** (**TF-IDF**).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法不能直接处理原始文本数据。为了有效地训练自然语言处理模型，有必要将输入文本中的单词转换成其他单词、句子或文档上下文中的数值表示。在嵌入技术发展之前，有两种广泛用于表示文本中单词相关性的方法：**词袋模型**（**BOW**）和**词频-逆文档频率**（**TF-IDF**）。
- en: BOW is simply the count of a word appearing in a text (document). For example,
    if the input documents are `I need to go to the bank to make a deposit` and `I
    am taking a walk along the river bank`, and you count the number of appearances
    for each unique word in each input document, you will get *1* for the word *I*,
    and *3* for the word *to* in the first document, as an example. If we have a vocabulary
    for all the unique words in the two documents, the vector representation for the
    first document can be `[1 1 3 1 1 1 1 1 1 0 0 0 0 0]`, where each position represents
    a unique word in the vocabulary (for example, the first position represents the
    word *I*, and the third position represents the word *to*). Now, this vector can
    be fed into an ML algorithm to train a model such as text classification. The
    main idea behind BOW is that a word that appears more frequently has stronger
    weights in a text.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: BOW（词袋模型）简单来说就是文本（文档）中单词出现的计数。例如，如果输入的文档是“我需要去银行存款”和“我沿着河岸散步”，并且你计算每个输入文档中每个独特单词的出现次数，你会得到单词“I”的计数为*1*，在第一个文档中单词“to”的计数为*3*，仅作为一个例子。如果我们有两个文档中所有独特单词的词汇表，第一个文档的向量表示可以是
    `[1 1 3 1 1 1 1 1 1 0 0 0 0 0]`，其中每个位置代表词汇表中的一个独特单词（例如，第一个位置代表单词“I”，第三个位置代表单词“to”）。现在，这个向量可以被输入到机器学习算法中，以训练文本分类等模型。BOW背后的主要思想是，在文本中，出现频率更高的单词具有更强的权重。
- en: TF-IDF has two components. The first component, *TF*, is the ratio of the number
    of times a vocabulary word appears in a document over the total number of words
    in the document. Using the preceding first document, the word *I* would have a
    TF value of *1/11* for the first sentence, and the word *walk* would have a TF
    value of *0/11*, since *walk* does not appear in the first sentence. While TF
    measures the importance of a word in the context of one text, the IDF component
    measures the importance of a word across all the documents. Mathematically, it
    is the log of the ratio of the number of documents over the number of documents
    where a word appears. The final value of TF-IDF for a word would be the *TF* term
    multiplied by the *IDF* term. In general, TF-IDF works better than BOW.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF有两个组成部分。第一个组成部分，*TF*，是词汇表中的单词在文档中出现的次数与文档中总单词数的比率。使用前面的第一个文档，单词“I”在第一句话中的TF值为*1/11*，而单词“walk”的TF值为*0/11*，因为“walk”没有出现在第一句话中。虽然TF衡量一个单词在文本上下文中的重要性，但IDF组件衡量一个单词在整个文档集中的重要性。从数学上讲，它是文档数量与包含该单词的文档数量的比率的对数。一个单词的TF-IDF最终值将是*TF*项乘以*IDF*项。一般来说，TF-IDF比BOW效果更好。
- en: Although BOW and TF-IDF are useful for NLP tasks, they lack the ability to capture
    the semantic meaning of words and often result in large and sparse input vectors.
    This is where the concept of embedding plays a crucial role.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然BOW和TF-IDF对于自然语言处理任务很有用，但它们缺乏捕捉单词语义意义的能力，并且经常导致输入向量大且稀疏。这就是嵌入概念发挥关键作用的地方。
- en: 'Embedding is a technique used to generate low-dimensional representations (mathematical
    vectors) for words or sentences, which capture the semantic meaning of the text.
    The underlying idea is that words or sentences with similar semantic meanings
    tend to occur in similar contexts. In a multi-dimensional space, the mathematical
    representations of semantically similar entities are closer to each other than
    those with different meanings. For instance, if we consider sports-related words
    like soccer, tennis, and bike, their embeddings would be close to each other in
    the high-dimensional embedding space, measured by metrics like cosine similarity,
    which measures how similar two vectors are by calculating the cosine of the angle
    between them. The embedding vector represents the intrinsic meaning of the word,
    with each dimension representing a specific attribute associated with the word.
    Visualizing embeddings in a multidimensional space shows the proximity of related
    entities. The following diagram provides a visual depiction of the closeness in
    this multidimensional space:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是一种用于生成单词或句子低维表示（数学向量）的技术，它能够捕捉文本的语义意义。其基本思想是，具有相似语义意义的单词或句子往往出现在相似的环境中。在多维空间中，语义相似实体的数学表示彼此更接近，而意义不同的实体则更远。例如，如果我们考虑与运动相关的单词，如足球、网球和自行车，它们在高度嵌入空间中的嵌入会彼此靠近，这是通过诸如余弦相似度等度量来衡量的，余弦相似度通过计算它们之间角度的余弦值来衡量两个向量之间的相似度。嵌入向量代表了单词的内在意义，每个维度代表与单词相关联的特定属性。在多维空间中可视化嵌入显示了相关实体的邻近性。以下图表提供了这个多维空间中邻近性的视觉描述：
- en: '![Figure 3.11 – Embedding representation ](img/B20836_03_12.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11 – 嵌入表示](img/B20836_03_12.png)'
- en: 'Figure 3.12: Embedding representation'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12：嵌入表示
- en: Nowadays, embeddings have become a crucial component for achieving good results
    in most NLP tasks. Compared to other techniques like simple word counts, embeddings
    offer more meaningful representations of the underlying text. This has led to
    their widespread adoption in various ML algorithms designed for NLP. In this section,
    we will delve into several of these algorithms such as BERT and GPT, exploring
    their specific applications and benefits in the context of NLP tasks.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，嵌入已经成为在大多数NLP任务中取得良好结果的关键组成部分。与简单的词频统计等其他技术相比，嵌入提供了更有意义的底层文本表示。这导致了它们在各种针对NLP设计的ML算法中的广泛应用。在本节中，我们将深入研究这些算法，如BERT和GPT，探讨它们在NLP任务背景下的具体应用和优势。
- en: Word2Vec
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2Vec
- en: 'Thomas Mikolov created **Word2Vec** in 2013\. It supports two different techniques
    for learning embedding: **continuous bag-of-words** (**CBOW**) and **continuous-skip-gram**.
    CBOW tries to predict a word for a given window of surrounding words, and continuous-skip-gram
    tries to predict surrounding words for a given word. The training dataset for
    Word2Vec could be any running text available, such as **Wikipedia**. The process
    of generating a training dataset for CBOW is to run a sliding window across running
    text (for example, a window of five words) and choose one of the words as the
    target and the rest as inputs (the order of words is not considered). In the case
    of continuous-skip-gram, the target and inputs are reversed. With the training
    dataset, the problem can be turned into a multi-class classification problem,
    where the model will learn to predict the classes (for example, words in the vocabulary)
    for the target word and assign each predicted word with a probability distribution.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 托马斯·米科尔洛夫于2013年创建了**Word2Vec**。它支持两种不同的学习嵌入的技术：**连续词袋模型**（**CBOW**）和**连续跳字模型**。CBOW试图预测给定窗口周围单词的一个单词，而连续跳字模型则试图预测给定单词的周围单词。Word2Vec的训练数据集可以是任何可用的运行文本，例如**维基百科**。CBOW生成训练数据集的过程是在运行文本上运行一个滑动窗口（例如，五个单词的窗口）并选择其中一个单词作为目标，其余作为输入（不考虑单词的顺序）。在连续跳字模型的情况下，目标和输入是相反的。有了训练数据集，问题可以转化为一个多类分类问题，其中模型将学习预测目标单词的类别（例如，词汇表中的单词）并为每个预测的单词分配一个概率分布。
- en: Word2Vec embeddings can be trained using a straightforward one-hidden-layer
    MLP network. In this approach, the input to the MLP network is a matrix that represents
    the neighboring words, while the output is a probability distribution for the
    target words. During training, the weights of the hidden layer are optimized,
    and once the training process is complete, these weights serve as the actual embeddings
    for the words. The resulting embeddings capture the semantic relationships and
    contextual meanings of the words, enabling them to be effectively utilized in
    various natural language processing tasks.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 嵌入可以使用简单的单隐藏层 MLP 网络进行训练。在这种方法中，MLP 网络的输入是一个表示邻近单词的矩阵，而输出是目标单词的概率分布。在训练过程中，隐藏层的权重被优化，一旦训练完成，这些权重就作为单词的实际嵌入。生成的嵌入捕捉了单词的语义关系和上下文意义，使得它们可以在各种自然语言处理任务中有效利用。
- en: As large-scale word embedding training can be expensive and time-consuming,
    Word2Vec embeddings are usually trained as a pre-training task so that they can
    be readily used for downstream tasks such as text classification or entity extraction.
    This approach of using embeddings as features for downstream tasks is called a
    **feature-based application**. There are pre-trained embeddings (for example,
    Tomas Mikolov’ *Word2Vec* and Stanford’s *GloVe*) in the public domain that can
    be used directly. The embeddings are a *1:1* mapping between each word and its
    vector representation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大规模词嵌入训练可能成本高昂且耗时，Word2Vec 嵌入通常作为预训练任务进行训练，以便它们可以方便地用于下游任务，如文本分类或实体提取。这种将嵌入用作下游任务特征的方法称为**基于特征的应用**。公共领域中有预训练的嵌入（例如，Tomas
    Mikolov 的 *Word2Vec* 和斯坦福大学的 *GloVe*），可以直接使用。这些嵌入是每个单词与其向量表示之间的**1:1**映射。
- en: BERT
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT
- en: Word2Vec produces a fixed embedding representation for each word in the vocabulary,
    disregarding the contextual variations in meaning. However, words can have different
    meanings depending on the specific context in which they are used. For instance,
    the word “bank” can refer to a financial institution or the land alongside a body
    of water. To address this issue, contextualized word embeddings have been developed.
    These embeddings take into account the surrounding words or the overall context
    in which a word appears, allowing for a more nuanced and context-aware representation.
    By considering context, these embeddings capture the diverse meanings a word can
    have, enabling more accurate and context-specific analyses in downstream tasks.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 为词汇表中的每个单词生成一个固定的嵌入表示，忽略了意义在上下文中的变化。然而，单词的意义可能因它们被使用的具体上下文而有所不同。例如，“bank”一词可以指代金融机构或水体旁边的土地。为了解决这个问题，已经开发出了上下文化的词嵌入。这些嵌入考虑了单词周围的其他单词或单词出现的整体上下文，从而允许更细腻和上下文感知的表示。通过考虑上下文，这些嵌入捕捉了单词可能具有的多种意义，使得在下游任务中可以进行更准确和上下文特定的分析。
- en: '**BERT**, which stands for **Bidirectional Encoder Representations from Transformers**,
    is a language model that takes context into consideration by the following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**，即**来自 Transformer 的双向编码器表示**，是一种通过以下方式考虑上下文的语言模型：'
- en: Predicting randomly masked words in sentences (the context) and taking the order
    of words into consideration. This is also known as **language modeling**.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测句子（上下文）中随机遮蔽的单词并考虑单词的顺序。这也被称为**语言模型**。
- en: Predicting the next sentence from a given sentence.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从给定的句子中预测下一个句子。
- en: Released in 2018, this context-aware embedding approach provides better representation
    for words and can significantly improve language tasks such as *reading comprehension*,
    *sentiment analysis*, and *named entity recognition*. Additionally, BERT generates
    embeddings at subword levels (a segment between a word and a character, for example,
    the word *embeddings* is broken up into *em*, *bed*, *ding*, and *s*). This allows
    it to handle the **out-of-vocabulary** (**OOV**) issue, another limitation of
    Word2Vec, which only generates embeddings on known words and will treat OOV words
    simply as unknown.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 2018 年发布，这种上下文感知的嵌入方法为单词提供了更好的表示，可以显著提高诸如**阅读理解**、**情感分析**和**命名实体识别**等语言任务。此外，BERT
    在子词级别（例如，单词和字符之间的一个片段，例如，单词 *embeddings* 被分解为 *em*、*bed*、*ding* 和 *s*）生成嵌入。这使得它可以处理
    Word2Vec 的另一个限制——**词汇表外**（**OOV**）问题，Word2Vec 只生成已知单词的嵌入，并将 OOV 单词简单地视为未知。
- en: To obtain word embeddings with BERT, the process differs from the straightforward
    word-to-vector mapping used in Word2Vec. Instead, sentences are inputted into
    a pre-trained BERT model, and embeddings are extracted dynamically. This approach
    generates embeddings that are contextualized within the context of the given sentences.
    Besides word-level embeddings, BERT is also capable of producing embeddings for
    entire sentences. **Pre-training** is the term used to describe the process of
    learning embeddings using input tokens, and the following diagram illustrates
    the components involved in a BERT model for this purpose.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用BERT获得词嵌入，其过程与Word2Vec中使用的直接词到向量的映射不同。相反，句子被输入到一个预训练的BERT模型中，并动态地提取嵌入。这种方法生成的嵌入是在给定句子的上下文中上下文化的。除了词级嵌入外，BERT还能够生成整个句子的嵌入。**预训练**是指使用输入标记学习嵌入的过程，以下图展示了BERT模型为此目的所涉及到的组件。
- en: '![Figure 3.13 – BERT model pre-training ](img/B20836_03_13.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图3.13 – BERT模型预训练](img/B20836_03_13.png)'
- en: 'Figure 3.13: BERT model pre-training'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13：BERT模型预训练
- en: 'Architecturally, BERT mainly uses a building block called a **transformer**.
    A transformer has a stack of encoders and a stack of decoders inside it, and it
    transforms one sequence of inputs into another sequence. Each encoder has two
    components:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在架构上，BERT主要使用一个称为**transformer**的构建块。transformer内部有一堆编码器和一堆解码器，它将一个输入序列转换成另一个序列。每个编码器有两个组成部分：
- en: A self-attention layer mainly calculates the strength of the connection between
    one token (represented as a vector) and all other tokens in the input sentence,
    and this connection helps with the encoding of each token. One way to think about
    self-attention is which words in a sentence are more connected than other words
    in a sentence. For example, if the input sentence is *The dog crossed a busy street*,
    then we would say the words *dog* and *crossed* have stronger connections with
    the word *The* than the word *a* and *busy*, which would have strong connections
    with the word *street*. The output of the self-attention layer is a sequence of
    vectors; each vector represents the original input token as well as the importance
    it has with other words in the inputs.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力层主要计算一个标记（表示为一个向量）与输入句子中所有其他标记之间的连接强度，这种连接有助于每个标记的编码。关于自注意力的一种思考方式是句子中哪些词比句子中的其他词更紧密地连接。例如，如果输入句子是*The
    dog crossed a busy street*，那么我们会说单词*dog*和*crossed*与单词*The*的连接比单词*a*和*busy*的连接更强，而后者将与单词*street*有较强的连接。自注意力层的输出是一系列向量；每个向量代表原始输入标记以及它在输入中与其他单词的重要性。
- en: A feed-forward network layer (single hidden layer MLP) extracts higher-level
    representation from the output of the self-attention layer.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络层（单隐藏层MLP）从自注意力层的输出中提取高级表示。
- en: Inside the decoder, there is also a self-attention layer and feed-forward layer,
    plus an extra encoder-decoder layer that helps the decoder to focus on the right
    places in the inputs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码器内部，也存在一个自注意力层和前馈层，以及一个额外的编码器-解码器层，这有助于解码器关注输入中的正确位置。
- en: 'In the case of BERT, only the encoder part of the transformer is used. BERT
    can be used for a number of NLP tasks, including *question answering*, *text classification*,
    *named entity extraction*, and *text summarization*. It achieved state-of-the-art
    performance in many of the tasks when it was released. BERT pre-training has also
    been adopted for different domains, such as scientific text and biomedical text,
    to understand domain-specific languages. The following figure showcases how a
    pre-trained BERT model is used to train a model for a question-answering task
    using the fine-tuning technique:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在BERT的情况下，只使用了transformer的编码器部分。BERT可以用于多种NLP任务，包括*问答*、*文本分类*、*命名实体提取*和*文本摘要*。当它发布时，在许多任务中实现了最先进的性能。BERT预训练也被应用于不同的领域，如科学文本和生物医学文本，以理解特定领域的语言。以下图展示了如何使用微调技术使用预训练的BERT模型训练一个问答任务的模型：
- en: '![Figure 3.14 – BERT fine-tuning ](img/B20836_03_14.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图3.14 – BERT微调](img/B20836_03_14.png)'
- en: 'Figure 3.14: BERT fine-tuning'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14：BERT微调
- en: While BERT’s pre-trained embeddings can be extracted for downstream tasks such
    as text classification and question answering, a more straightforward way to use
    its pre-trained embeddings is through a technique called **fine-tuning**. With
    fine-tuning, an additional output layer is added to the BERT network to perform
    a specific task, such as question answering or entity extraction. During fine-tuning,
    the pre-trained model is loaded, and you plug in the task-specific input (for
    example, question/passage pairs in question answering) and output (start/end and
    span for the answers in the passage) to fine-tune a task-specific model. With
    fine-tuning, the pre-trained model weights are updated.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然BERT的预训练嵌入可以提取用于下游任务，如文本分类和问答，但使用其预训练嵌入的一种更直接的方法是通过一种称为**微调**的技术。通过微调，BERT网络中添加了一个额外的输出层以执行特定任务，例如问答或实体提取。在微调过程中，加载预训练模型，并插入特定任务的输入（例如，问答中的问题/段落对）和输出（段落中答案的起始/结束和跨度）以微调特定任务的模型。通过微调，预训练模型的权重得到更新。
- en: Generative AI algorithms
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成式AI算法
- en: Although technologies like ChatGPT have popularized Generative AI, the concept
    of generative models is not new. **Generative Adversarial Networks** (**GANs**),
    a prominent example of generative AI technology, have been around for years and
    have been successfully applied in various real-world domains, with image synthesis
    being a notable application. Generative AI has become one of the most transformative
    AI technologies, and I have devoted the entire *Chapter 15 and 16* to delve deeper
    into real-world generative AI use cases, practical technology solutions, and ethical
    considerations. In this chapter, we will familiarize ourselves with several generative
    AI algorithms.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像ChatGPT这样的技术已经普及了生成式AI，但生成模型的概念并不新鲜。**生成对抗网络**（GANs），作为生成式AI技术的突出例子，已经存在多年，并在各种实际应用领域取得了成功，其中图像合成是一个显著的应用。生成式AI已成为最具变革性的AI技术之一，我专门在*第15章和第16章*中深入探讨了实际生成式AI用例、实用技术解决方案和伦理考量。在本章中，我们将熟悉几种生成式AI算法。
- en: Generative adversarial network
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: The GAN is a type of generative model designed to generate realistic data instances,
    such as images. It employs a two-part network consisting of a Generator and a
    Discriminator. The Generator network is responsible for generating instances,
    while the Discriminator network learns to distinguish between real and fake instances
    generated by the Generator. This adversarial setup encourages the Generator to
    continually improve its ability to produce increasingly realistic data instances.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是一种旨在生成真实数据实例的生成模型，例如图像。它采用由生成器和判别器组成的两部分网络。生成器网络负责生成实例，而判别器网络学习区分生成器生成的真实和假实例。这种对抗性设置鼓励生成器不断改进其生成越来越真实数据实例的能力。
- en: '![Figure 3.16 – Generative adversarial network  ](img/B20836_03_15.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图3.16 – 生成对抗网络](img/B20836_03_15.png)'
- en: 'Figure 3.15: GAN'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15：GAN
- en: 'During the training process, the Discriminator network in a GAN is exposed
    to two different data sources: one from a real dataset, which serves as positive
    examples, and one from the Generator network, which generates synthetic or fake
    samples. The Discriminator is trained to classify and distinguish between the
    real and fake samples, optimizing its loss to accurately predict the source of
    each sample. Conversely, the Generator network is trained to produce synthetic
    data that appears indistinguishable from real data, aiming to deceive the Discriminator.
    The Generator is penalized when the Discriminator correctly identifies its generated
    data as fake.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，GAN中的判别器网络暴露于两种不同的数据源：一种来自真实数据集，作为正例，另一种来自生成器网络，生成合成或假样本。判别器被训练以分类和区分真实和假样本，优化其损失以准确预测每个样本的来源。相反，生成器网络被训练以生成看起来与真实数据不可区分的合成数据，目的是欺骗判别器。当判别器正确识别其生成的数据为假时，生成器会受到惩罚。
- en: Both networks learn and update their parameters using backpropagation, enabling
    them to improve iteratively. During the generation phase, the Generator is provided
    with random inputs to produce new synthetic samples. Throughout training, the
    Generator and Discriminator networks are alternately trained in a connected manner,
    allowing them to learn and optimize their performance as a unified system.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 两个网络都使用反向传播来学习和更新它们的参数，使它们能够迭代地改进。在生成阶段，生成器被提供随机输入以产生新的合成样本。在整个训练过程中，生成器和判别器网络交替以连接方式训练，使它们能够作为一个统一系统学习和优化它们的性能。
- en: GANs have had a lot of success in generating realistic images that can fool
    humans. They can be applied to many applications, such as translating sketches
    to realistic-looking images, converting text inputs and generating images that
    correspond to the text, and generating realistic human faces. However, getting
    GANs to converge and stabilize during training can be difficult, leading to issues
    like failure to learn. Additionally, newer technologies have emerged in realistic
    image generation, which are a lot more capable than GANs.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: GANs在生成能够欺骗人类的逼真图像方面取得了很大的成功。它们可以应用于许多应用，例如将草图转换为逼真的图像，将文本输入转换为与文本对应的图像，以及生成逼真的人类面孔。然而，在训练期间使GANs收敛和稳定可能很困难，导致学习失败等问题。此外，在逼真图像生成方面出现了新的技术，它们比GANs的能力要强得多。
- en: Generative pre-trained transformer (GPT)
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成预训练转换器（GPT）
- en: Unlike BERT, which requires fine-tuning using a large domain-specific dataset
    for the different downstream NLP tasks, the **Generative Pre-trained Transformer**
    (**GPT**), developed by **OpenAI**, can learn how to perform a task with just
    seeing a few examples (or no example). This learning process is called **few-shot
    learning** or **zero-shot learning**. In a few-shot scenario, the GPT model is
    provided with a few examples, a task description, and a prompt, and the model
    will use these inputs and start to generate output tokens one by one. For instance,
    when using GPT-3 for translation, the task description could be “translate English
    to Chinese,” and the training data would consist of a few examples of Chinese
    sentences translated from English sentences. To translate a new English sentence
    using the trained model, you provide the English sentence as a prompt, and the
    model generates the corresponding translated text in Chinese. It’s important to
    note that few-shot or zero-shot learning does not involve updating the model’s
    parameter weights, unlike the fine-tuning technique.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 与BERT不同，BERT需要使用大型特定领域数据集进行微调，以针对不同的下游NLP任务，而**生成预训练转换器**（**GPT**），由**OpenAI**开发，只需看到几个示例（或没有示例）就能学习如何执行任务。这个过程被称为**少样本学习**或**零样本学习**。在少样本场景中，GPT模型被提供几个示例、任务描述和提示，然后模型将使用这些输入并开始逐个生成输出标记。例如，当使用GPT-3进行翻译时，任务描述可以是“将英语翻译成中文”，训练数据将包括一些从英语句子翻译成中文句子的示例。要使用训练好的模型翻译新的英语句子，你提供英语句子作为提示，然后模型生成相应的中文翻译文本。需要注意的是，少样本或零样本学习不涉及更新模型的参数权重，这与微调技术不同。
- en: GPT, like BERT, utilizes the Transformer architecture as its primary component
    and employs a training approach called next word prediction. This entails predicting
    the word that should follow a given sequence of input words. However, GPT diverges
    from BERT in that it exclusively employs the Transformer decoder block, whereas
    BERT employs the Transformer encoder block. Like BERT, GPT incorporates masked
    words to learn embeddings. However, unlike BERT, which randomly masks words and
    predicts the missing ones, GPT restricts the self-attention calculation to exclude
    words to the right of the target word. This approach is referred to as masked
    self-attention.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: GPT，就像BERT一样，以Transformer架构作为其主要组件，并采用了一种称为下一个词预测的训练方法。这包括预测给定输入词序列后应跟的单词。然而，GPT与BERT不同，它仅使用Transformer解码器块，而BERT使用Transformer编码器块。与BERT一样，GPT通过包含掩码词来学习嵌入。然而，与BERT随机掩码单词并预测缺失的单词不同，GPT将自注意力计算限制为排除目标词右侧的单词。这种方法被称为掩码自注意力。
- en: GPT and its chatbot interface, ChatGPT, have showcased exceptional capabilities
    in numerous traditional NLP tasks like language modeling, language translation,
    and question answering. Additionally, they have proven their efficacy in pioneering
    domains such as generating programming code or ML code, composing website content,
    and answering questions. Consequently, GPT has paved the way for a novel AI paradigm
    known as Generative AI.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: GPT及其聊天机器人界面ChatGPT在众多传统的自然语言处理任务中展示了卓越的能力，如语言建模、语言翻译和问答。此外，它们在开创性领域如生成编程代码或机器学习代码、编写网站内容以及回答问题方面也证明了其有效性。因此，GPT为一种新的AI范式——生成式AI铺平了道路。
- en: Large Language Model
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: '**Large Language Models** (**LLMs**) are a class of generative AI model that
    is capable of generating text, translating languages, producing creative content,
    and providing informative answers to questions. LLMs are trained on extensive
    datasets comprising text and code, with billions of model parameters, allowing
    them to understand and learn the statistical patterns and relationships between
    words and phrases. For example, GPT-3, an LLM, has 175 billion parameters after
    training. This training enables LLMs to effectively process and generate human-like
    text across a wide range of applications. While GPTs serve as a notable example
    of LLMs, the open-source community and other companies have developed additional
    LLMs in recent years, mainly using similar Transformer-based architecture. LLMs
    are also called foundation models. Unlike traditional ML models, which are trained
    for specific tasks, foundation models are pre-trained on massive datasets and
    can handle multiple tasks. In addition, foundation models can be fine-tuned and
    adapted for additional tasks. The remarkable capabilities and adaptability of
    foundation models have found many exciting AI use cases that were not easily solvable
    before. Now, let’s briefly review a few other popular foundation models:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型**（**LLMs**）是一类能够生成文本、翻译语言、创作内容以及为问题提供信息性答案的生成式人工智能模型。LLMs 在包含文本和代码的广泛数据集上训练，拥有数十亿个模型参数，这使得它们能够理解和学习词语和短语之间的统计模式和关系。例如，经过训练后，LLM
    GPT-3 拥有1750亿个参数。这种训练使得LLMs能够有效地处理和生成跨越广泛应用的类似人类的文本。虽然GPTs是LLMs的一个显著例子，但近年来开源社区和其他公司也开发了额外的LLMs，主要使用类似的基于Transformer的架构。LLMs也被称为基础模型。与为特定任务训练的传统机器学习模型不同，基础模型是在大量数据集上预训练的，可以处理多个任务。此外，基础模型还可以进行微调和适应额外任务。基础模型的卓越能力和适应性发现了许多以前难以解决的令人兴奋的AI应用。现在，让我们简要回顾一下其他一些流行的基础模型：'
- en: '**Google’s Pathways Language Model** (**PaLM**) is a 540-billion-parameter,
    decoder-only Transformer model. It offers similar capabilities as GPT, including
    text generation, translation, code generation, question answering, summarization,
    and support for creating chatbots. PaLM is trained using a new architecture called
    Pathways. Pathways is a modular architecture, meaning that it is composed of modules,
    each is responsible for a specific task.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google的Pathways语言模型**（**PaLM**）是一个仅具有解码器功能的5400亿参数Transformer模型。它提供了与GPT类似的特性，包括文本生成、翻译、代码生成、问答、摘要以及支持创建聊天机器人。PaLM使用一种名为Pathways的新架构进行训练。Pathways是一种模块化架构，意味着它由模块组成，每个模块负责特定任务。'
- en: '**Meta’s Large Language Model Meta AI** (**LLaMA**) is an LLM available in
    multiple sizes from 7 billion parameters to 65 billion parameters. While it is
    a smaller model compared to GPT and PaLM, it offers several advantages such as
    requiring fewer computational resources. LLaMA offers similar capabilities as
    other LLMs such as generating creative text, answering questions, and solving
    mathematical problems. Meta has issued a noncommercial license for LLaMA, emphasizing
    its usage in research contexts. LLaMA has also been found to perform extremely
    well when it is fine-tuned with additional training data.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Meta的大型语言模型Meta AI**（**LLaMA**）是一个从70亿参数到650亿参数的多种尺寸的LLM。与GPT和PaLM相比，它是一个较小的模型，但提供了几个优势，例如需要较少的计算资源。LLaMA提供了与其他LLM类似的特性，如生成创意文本、回答问题和解决数学问题。Meta为LLaMA发布了非商业许可证，强调其在研究环境中的使用。当LLaMA与额外的训练数据进行微调时，它被发现表现极为出色。'
- en: '**Big Science BLOOM** is a 176-billion-parameter LLM that can generate text
    in 46 different languages and 13 programming languages. The development of BLOOM
    involved the collaborative efforts of more than 1,000 researchers from over 70
    countries and 250 institutions. As part of the Responsible AI License, individuals
    and institutions who agree to its terms can use and build upon the model on their
    local machines or cloud platforms. The model is easily accessible within the Hugging
    Face ecosystem.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大科学BLOOM**是一个拥有1760亿参数的LLM，能够用46种不同的语言和13种编程语言生成文本。BLOOM的开发涉及来自70多个国家和250多个机构的1000多名研究者的协作努力。作为负责任的AI许可证的一部分，同意其条款的个人和机构可以在他们的本地机器或云平台上使用和构建该模型。该模型在Hugging
    Face生态系统中易于访问。'
- en: '**Bloomberg BloombergGPT** – While general-purpose LLMs like GPT and LLaMA
    can perform well in a range of tasks across different domain domains, domains
    such as financial services and life science require domain-specific LLMs to solve
    tough domain-focused problems. BloombergGPT is an example of domain-focused LLMs
    that are specifically trained for industries. BloombergGPT represents a significant
    advancement in applying this technology to finance. The model will enhance existing
    financial NLP tasks such as sentiment analysis, named entity recognition, news
    classification, and question answering, among others. Drawing from its extensive
    collection and curation resources, Bloomberg utilized its four-decade archive
    of financial language documents, resulting in a comprehensive 363-billion-token
    dataset comprising English financial documents. This dataset was augmented with
    a 345-billion-token public dataset, yielding a training corpus with over 700 billion
    tokens.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**彭博BloombergGPT** – 虽然像GPT和LLaMA这样的通用LLM在不同的领域域中可以很好地执行各种任务，但像金融服务和生命科学这样的领域需要特定领域的LLM来解决针对特定领域的难题。BloombergGPT是专门为行业训练的领域特定LLM的例子。BloombergGPT在将这项技术应用于金融方面取得了重大进步。该模型将增强现有的金融NLP任务，如情感分析、命名实体识别、新闻分类和问答等。利用其广泛的收集和策划资源，彭博利用其40年的金融语言文档档案，形成了一个包含3630亿个标记的全面数据集，包括英语金融文档。该数据集通过一个3450亿个标记的公共数据集进行了增强，从而产生了超过7000亿个标记的训练语料库。'
- en: While these LLM models have exhibited remarkable capabilities, they also come
    with significant limitations, including generating misinformation (hallucinations)
    and toxic content, and displaying potential bias. LLMs also consume significantly
    more resources to train and run. It is worth noting that while LLMs can help solve
    some new problems, many of the common problems (e.g., name entity extraction,
    document classification, sentiment analysis) have been solved with existing NLP
    techniques, which remain highly viable options for those tasks.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些LLM模型已经展示了非凡的能力，但它们也伴随着重大的局限性，包括生成错误信息（幻觉）和有害内容，以及显示潜在的偏见。LLM在训练和运行时也消耗了大量的资源。值得注意的是，虽然LLM可以帮助解决一些新问题，但许多常见问题（例如，命名实体提取、文档分类、情感分析）已经通过现有的NLP技术得到解决，这些技术仍然是这些任务的可行选项。
- en: Diffusion model
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩散模型
- en: Recently, AI’s remarkable ability to generate high-resolution, photorealistic
    images and never-seen-before generative art or manipulate images with precision
    has garnered significant attention. Behind all these amazing capabilities is a
    new type of deep learning model called the diffusion model. Building upon the
    foundations of deep learning and GANs, the diffusion model introduces a novel
    approach to generating high-quality, realistic data instances.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，AI在生成高分辨率、逼真图像以及以前未见过的生成艺术或精确操纵图像方面的非凡能力引起了广泛关注。所有这些令人惊叹的能力背后是一个新的深度学习模型，称为扩散模型。在深度学习和GANs的基础上，扩散模型引入了一种生成高质量、逼真数据实例的新方法。
- en: Unlike GANs, which try to generate realistic fake images by fooling a discriminator
    network, the diffusion model works by first adding noises to the input data (e.g.,
    images) incrementally over many steps until the input data is unrecognizable,
    a process called diffusion steps.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 与试图通过欺骗判别网络生成逼真假图像的GANs不同，扩散模型通过首先在输入数据（例如，图像）上逐步添加噪声，经过许多步骤，直到输入数据无法识别，这个过程称为扩散步骤。
- en: 'The model is then trained to reverse the diffusion steps from noise to the
    original data. In more technical terms, the training process of a diffusion model
    involves optimizing a set of learnable parameters through backpropagation. The
    model learns to generate realistic samples by maximizing the likelihood of the
    training data given a sequence of diffusion steps. This iterative process allows
    the model to capture complex dependencies, intricate patterns, and structures,
    and generate highly realistic and diverse data instances. The following figure
    illustrates this process:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，该模型被训练以从噪声到原始数据的扩散步骤进行逆向操作。从更技术性的角度来说，扩散模型的训练过程涉及通过反向传播优化一组可学习的参数。模型通过最大化给定一系列扩散步骤的训练数据的似然性来学习生成逼真的样本。这个迭代过程允许模型捕捉复杂的依赖关系、复杂的模式和结构，并生成高度逼真和多样化的数据实例。以下图示说明了这个过程：
- en: '![](img/B20836_03_16.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20836_03_16.png)'
- en: 'Figure 3.16: How a diffusion model works'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16：扩散模型的工作原理
- en: Furthermore, the diffusion model offers flexibility and controllability in the
    generation process. By adjusting the diffusion steps, one can control the trade-off
    between sample quality and diversity. This allows users to fine-tune the model
    to suit their specific needs, whether it’s emphasizing fidelity to the training
    data or encouraging more creative and novel outputs. Compared to GANs, diffusion
    models can generate more realistic images and are more stable than GANs.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，扩散模型在生成过程中提供了灵活性和可控性。通过调整扩散步骤，可以控制样本质量和多样性之间的权衡。这使用户能够微调模型以满足他们的特定需求，无论是强调对训练数据的忠实度，还是鼓励更多创造性和新颖的输出。与GANs相比，扩散模型可以生成更逼真的图像，并且比GANs更稳定。
- en: The diffusion model has shown great promise in various domains, including computer
    vision, natural language processing, and audio synthesis. Its ability to generate
    high-quality data with fine-grained details has opened up exciting possibilities
    for applications such as image generation, video prediction, text generation,
    and more.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型在计算机视觉、自然语言处理和音频合成等各个领域都显示出巨大的潜力。它生成具有精细细节的高质量数据的能力为图像生成、视频预测、文本生成等应用开辟了令人兴奋的可能性。
- en: 'The open-source community and private companies have developed many models
    based on this diffusion approach. Two of the more popular models worth mentioning
    are Stable Diffusion and DALL-E 2:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 开源社区和私营公司已经基于这种扩散方法开发了众多模型。其中两个值得提到的流行模型是Stable Diffusion和DALL-E 2：
- en: '**DALL-E 2 by OpenAI**: DALL-E 2 is a text-to-image model developed by OpenAI.
    DALL-E 2 was trained using a dataset of images and text descriptions. First released
    in January 2022, DALL-E 2 has shown remarkable capabilities in generating and
    manipulating images from text descriptions. It has also been applied for inpainting
    (modifying regions in images), outpainting (extending an image), and image-to-image
    translation. The images generated by DALL-E 2 are often indistinguishable from
    real images, and they can be used for a variety of purposes, such as creating
    art and generating marketing materials. From a model-training perspective, DALL-E
    2 training comprises two key steps:'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI的DALL-E 2**：DALL-E 2是由OpenAI开发的一种文本到图像模型。DALL-E 2使用图像和文本描述的数据集进行训练。首次发布于2022年1月，DALL-E
    2在从文本描述生成和操作图像方面显示出非凡的能力。它还应用于图像修复（修改图像中的区域）、图像扩展（扩展图像）和图像到图像的翻译。DALL-E 2生成的图像通常与真实图像难以区分，可用于各种目的，如创作艺术和生成营销材料。从模型训练的角度来看，DALL-E
    2的训练包括两个关键步骤：'
- en: '**Linking textual semantics and visual representations**: This step involves
    learning how a piece of text, such as “a man wearing a hat” is semantically linked
    to an actual image of “a man wearing a hat”. To do this, DALL-E 2 uses a model
    called **Contrastive Language-Image Pre-training** (**CLIP**). CLIP is trained
    with hundreds of millions of images and their associated descriptions. After it
    is trained, it can output a text-conditioned visual encoding given a piece of
    text description. You can learn more about CLIP at [https://openai.com/research/clip](https://openai.com/research/clip).'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链接文本语义和视觉表示**：这一步涉及学习如何将一段文本，例如“一个戴帽子的男人”，在语义上与实际的“一个戴帽子的男人”图像相联系。为此，DALL-E
    2使用一种称为**对比语言-图像预训练**（**CLIP**）的模型。CLIP使用数亿张图像及其相关描述进行训练。训练完成后，它可以根据一段文本描述输出一个文本条件下的视觉编码。您可以在[https://openai.com/research/clip](https://openai.com/research/clip)了解更多关于CLIP的信息。'
- en: '**Generating images from visual embeddings**: This step learns to reverse the
    image from the visual embeddings generated by the CLIP. For this step, DALL-E
    2 uses a model called GLIDE, which is based on the diffusion model. You can learn
    more about GLIDE at [https://arxiv.org/abs/2112.10741](https://arxiv.org/abs/2112.10741).'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从视觉嵌入生成图像**：这一步学习从CLIP生成的视觉嵌入中逆向生成图像。为此步骤，DALL-E 2使用了一个名为GLIDE的模型，该模型基于扩散模型。您可以在[https://arxiv.org/abs/2112.10741](https://arxiv.org/abs/2112.10741)了解更多关于GLIDE的信息。'
- en: After the model is trained, DALL-E 2 can generate new images closely related
    to an input text description.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型训练完成后，DALL-E 2可以生成与输入文本描述密切相关的新图像。
- en: '**Stable Diffusion by Stability AI**: Stable Diffusion is an algorithm developed
    by Compvis (the Computer Vision research group at Ludwig Maximilian University
    of Munich) and sponsored primarily by Stability AI. The model is also a text-to-image
    model trained using a dataset of real images and text descriptions, which allows
    the model to generate realistic images using text descriptions. First released
    in August 2022, Stable Diffusion has been shown to be effective at generating
    high-quality images from text descriptions. Architecturally, it employs a CLIP
    encoder to condition the model on text descriptions, and it uses UNET as the denoising
    neural network to generate images from visual encodings. It is an open-source
    model with the code and model weights released to the public. You can get more
    details on Stable Diffusion at [https://github.com/CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Stable Diffusion by Stability AI**：Stable Diffusion是由慕尼黑路德维希-马克西米利安大学的计算机视觉研究小组Compvis开发并由Stability
    AI主要赞助的算法。该模型也是一个使用真实图像和文本描述的数据集进行训练的文本到图像模型，这使得模型能够使用文本描述生成逼真的图像。首次发布于2022年8月，Stable
    Diffusion已被证明在从文本描述生成高质量图像方面非常有效。在架构上，它使用CLIP编码器对模型进行文本描述的条件化，并使用UNET作为去噪神经网络从视觉编码中生成图像。它是一个开源模型，代码和模型权重已公开发布。您可以在[https://github.com/CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)上获取更多关于Stable
    Diffusion的详细信息。'
- en: As powerful as the diffusion models are, they do come with some concerns, including
    copyright infringement and the creation of harmful images.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管扩散模型非常强大，但它们确实带来了一些担忧，包括版权侵权和有害图像的创建。
- en: Hands-on exercise
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动手练习
- en: 'In this hands-on exercise, we will build a **Jupyter** **Notebook** environment
    on your local machine and build and train an ML model in your local environment.
    The goal of the exercise is to get some familiarity with the installation process
    of setting up a local data science environment, and then learn how to analyze
    the data, prepare the data, and train an ML model using one of the algorithms
    we covered in the preceding sections. First, let’s take a look at the problem
    statement. The following diagram illustrates the flow:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个动手练习中，我们将在您的本地机器上构建一个**Jupyter** **Notebook**环境，并在您的本地环境中构建和训练一个ML模型。练习的目标是熟悉设置本地数据科学环境的安装过程，然后学习如何使用我们在前面章节中介绍的一种算法来分析数据、准备数据和训练ML模型。首先，让我们看一下问题陈述。以下图表说明了流程：
- en: '![](img/B20836_03_17.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20836_03_17.png)'
- en: 'Figure 3.17: ML problem-solving flow'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17：ML问题解决流程
- en: Let’s get started.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Problem statement
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: Before we start, let’s first review the business problem that we need to solve.
    A retail bank has been experiencing a high customer churn rate for its retail
    banking business. To proactively implement preventive measures to reduce potential
    churn, the bank needs to know who the potential churners are, so the bank can
    target those customers with incentives directly to prevent them from leaving.
    From a business perspective, it is far more expensive to acquire a new customer
    than offering incentives to keep an existing customer.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们首先回顾一下我们需要解决的商业问题。一家零售银行一直在经历其零售银行业务的高客户流失率。为了主动实施预防措施以减少潜在的流失，银行需要知道潜在的流失者是谁，这样银行就可以直接针对这些客户提供激励措施，以防止他们离开。从商业角度来看，吸引新客户比提供激励措施以保留现有客户要昂贵得多。
- en: As an ML solutions architect, you have been tasked to run some quick experiments
    to validate the ML approach for this problem. There is no ML tooling available,
    so you have decided to set up a Jupyter environment on your local machine for
    this task.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名ML解决方案架构师，您被要求运行一些快速实验以验证该问题的ML方法。目前没有可用的ML工具，因此您决定在您的本地机器上设置一个Jupyter环境来完成这项任务。
- en: Dataset description
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集描述
- en: You will use a dataset from the Kaggle site for bank customers’ churn for modeling.
    You can access the dataset at [https://www.kaggle.com/mathchi/churn-for-bank-customers](https://www.kaggle.com/mathchi/churn-for-bank-customers).
    Note that you will need Kaggle account to download the file. The dataset contains
    14 columns for features such as credit score, gender, and balance, and a target
    variable column, `Exited`, to indicate whether a customer churned or not. We will
    review those features in more detail in later sections.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用 Kaggle 网站上的银行客户流失数据集进行建模。您可以通过 [https://www.kaggle.com/mathchi/churn-for-bank-customers](https://www.kaggle.com/mathchi/churn-for-bank-customers)
    访问数据集。请注意，您需要 Kaggle 账户才能下载文件。该数据集包含 14 个特征列，如信用评分、性别和余额，以及一个目标变量列 `Exited`，用于指示客户是否流失。我们将在后面的章节中更详细地审查这些特征。
- en: Setting up a Jupyter Notebook environment
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Jupyter Notebook 环境
- en: 'Now, let’s set up a local data science environment for data analysis and experimentation.
    We will be using the popular Jupyter Notebook on your local computer. Setting
    up a Jupyter Notebook environment on a local machine consists of the following
    key components:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们设置一个本地数据科学环境，用于数据分析和实验。我们将使用您本地计算机上的流行 Jupyter Notebook。在本地机器上设置 Jupyter
    Notebook 环境包括以下关键组件：
- en: '**Python**: Python is a general-purpose programming language, and it is one
    of the most popular programming languages for data science work. The installation
    instructions can be found at [https://www.python.org/downloads](https://www.python.org/downloads).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python**：Python 是一种通用编程语言，也是数据科学工作中最受欢迎的编程语言之一。安装说明可以在 [https://www.python.org/downloads](https://www.python.org/downloads)
    找到。'
- en: '**PIP**: PIP is a Python package installer used for installing different Python
    library packages, such as ML algorithms, data manipulation libraries, or visualization.
    The installation instructions can be found at [https://pip.pypa.io/en/stable/installation/](https://pip.pypa.io/en/stable/installation/).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PIP**：PIP 是一个 Python 包安装程序，用于安装不同的 Python 库包，如机器学习算法、数据处理库或可视化库。安装说明可以在 [https://pip.pypa.io/en/stable/installation/](https://pip.pypa.io/en/stable/installation/)
    找到。'
- en: '**Jupyter Notebook**: Jupyter Notebook is a web application designed for authoring
    documents (called notebooks) that contain code, description, and/or visualizations.
    It is one of the most popular tools used by data scientists for experimentation
    and modeling. The installation instructions can be found at [https://jupyter.org/install](https://jupyter.org/install).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jupyter Notebook**：Jupyter Notebook 是一个用于编写包含代码、描述和/或可视化的文档（称为笔记本）的 Web 应用程序。它是数据科学家用于实验和建模最受欢迎的工具之一。安装说明可以在
    [https://jupyter.org/install](https://jupyter.org/install) 找到。'
- en: Running the exercise
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行练习
- en: 'Follow along with these steps to run the lab:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤运行实验室：
- en: 'With your environment configured, let’s get started with the actual data science
    work. First, download the data files:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境配置完成后，让我们开始实际的数据科学工作。首先，下载数据文件：
- en: Let’s create a folder called `MLSALab` on your local machine to store all the
    files. You can create the folder anywhere on your local machine as long as you
    can get to it. I have a Mac, so I created one directly inside the default user’s
    `Documents` folder.
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在本地机器上创建一个名为 `MLSALab` 的文件夹来存储所有文件。您可以在本地机器上的任何位置创建该文件夹，只要您能访问它。我有一个 Mac，所以我直接在默认用户的
    `Documents` 文件夹中创建了一个。
- en: Create another subfolder called `Lab1-bankchurn` under the `MLSALab` folder.
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `MLSALab` 文件夹下创建另一个名为 `Lab1-bankchurn` 的子文件夹。
- en: Visit the [https://www.kaggle.com/mathchi/churn-for-bank-customers](https://www.kaggle.com/mathchi/churn-for-bank-customers)
    site and download the data file (an archive file) and save it in the `MSSALab/Lab1-bankchurn`
    folder. Create a Kaggle account if you do not already have one. Extract the archive
    file inside the folder, and you will see a file called `churn.csv`. You can now
    delete the archive file.
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [https://www.kaggle.com/mathchi/churn-for-bank-customers](https://www.kaggle.com/mathchi/churn-for-bank-customers)
    网站，下载数据文件（一个存档文件），并将其保存到 `MSSALab/Lab1-bankchurn` 文件夹中。如果您还没有账户，请创建一个 Kaggle 账户。在文件夹内解压存档文件，您将看到一个名为
    `churn.csv` 的文件。现在您可以删除存档文件。
- en: 'Launch Jupyter Notebook:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Jupyter Notebook：
- en: 'Inside the Terminal window (or the Command Prompt window for Windows systems),
    navigate to the `MLSALab` folder and run the following command to start the Jupyter
    Notebook server on your machine:'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端窗口（或 Windows 系统的命令提示符窗口）中，导航到 `MLSALab` 文件夹，并运行以下命令以在您的机器上启动 Jupyter Notebook
    服务器：
- en: '[PRE0]'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A browser window will open up and display the Jupyter Notebook environment
    (see the following screenshot). Detailed instructions on how Jupyter Notebook
    works are out of scope for this lab. If you are not familiar with how Jupyter
    Notebook works, you can easily find information on the internet:'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将打开一个浏览器窗口并显示Jupyter Notebook环境（见以下截图）。有关Jupyter Notebook如何工作的详细说明超出了本实验的范围。如果您不熟悉Jupyter
    Notebook的工作方式，您可以在互联网上轻松找到相关信息：
- en: '![Figure 3.17 – Jupyter Notebook ](img/B20836_03_18.png)'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![图3.17 – Jupyter Notebook](img/B20836_03_18.png)'
- en: 'Figure 3.18: Jupyter Notebook'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.18：Jupyter Notebook
- en: Click on the `Lab1-bankchurn` folder and you will see the `churn.csv` file.
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 `Lab1-bankchurn` 文件夹，您将看到 `churn.csv` 文件。
- en: Now, let’s create a new data science notebook inside the Jupyter Notebook environment.
    To do this, click on the **New** dropdown and select **Python 3** (see the following
    screenshot):![Figure 3.18 – Creating a new Jupyter notebook ](img/B20836_03_19.png)
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们在Jupyter Notebook环境中创建一个新的数据科学笔记本。为此，点击 **新建** 下拉菜单并选择 **Python 3**（见以下截图）：![图3.18
    – 创建新的Jupyter笔记本](img/B20836_03_19.png)
- en: 'Figure 3.19: Creating a new Jupyter notebook'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.19：创建新的Jupyter笔记本
- en: You will see a screen similar to the following screenshot. This is an empty
    notebook that we will use to explore data and build models. The section next to
    **In [ ]:** is called a **cell**, and we will enter our code into the cell. To
    run the code in the cell, you click on the **Run** button on the toolbar. To add
    a new cell, you click on the **+** button on the toolbar:![Figure 3.19 – Empty
    Jupyter notebook ](img/B20836_03_20.png)
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将看到一个类似于以下截图的屏幕。这是一个空的笔记本，我们将使用它来探索数据和构建模型。**In [ ]:** 旁边的部分称为 **单元格**，我们将在此单元格中输入代码。要运行单元格中的代码，您点击工具栏上的
    **运行** 按钮。要添加新的单元格，您点击工具栏上的 **+** 按钮：![图3.19 – 空的Jupyter笔记本](img/B20836_03_20.png)
- en: 'Figure 3.20: Empty Jupyter notebook'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.20：空的Jupyter笔记本
- en: 'Add a new cell by clicking on the **+** button in the toolbar, enter the following
    code block inside the first empty cell, and run the cell by clicking on the **Run**
    button in the toolbar. This code block downloads a number of Python packages for
    data manipulation (`pandas`), visualization (`matplotlib`), and model training
    and evaluation (`scikit-learn`). We will cover scikit-learn in greater detail
    in *Chapter 5*, *Exploring Open-Source ML Libraries*. We will use these packages
    in the following sections:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击工具栏上的 **+** 按钮添加新的单元格，在第一个空单元格中输入以下代码块，并通过点击工具栏上的 **运行** 按钮来运行单元格。此代码块下载了用于数据处理（`pandas`）、可视化（`matplotlib`）以及模型训练和评估（`scikit-learn`）的多个Python包。我们将在第5章
    *探索开源机器学习库* 中更详细地介绍scikit-learn。我们将在以下章节中使用这些包：
- en: '[PRE1]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can load and explore the data. Add the following code block in a new
    cell to load the Python library packages and load the data from the `churn.csv`
    file. You will see a table with 14 columns, where the `Exited` column is the target
    column:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以加载数据并探索。在新的单元格中添加以下代码块以加载Python库包并从 `churn.csv` 文件加载数据。您将看到一个包含14列的表格，其中
    `Exited` 列是目标列：
- en: '[PRE2]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can explore the dataset using a number of tools to understand information
    with the commands that follow, such as *dataset statistics*, the *pairwise correlation
    between different features*, and *data distributions*. The `describe()` function
    returns basic statistics about the data such as mean, standard deviation, min,
    and max, for each numerical column.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用以下命令使用多种工具探索数据集，以理解信息，例如 *数据集统计*、*不同特征之间的成对相关性* 和 *数据分布*。`describe()` 函数返回有关数据的基本统计信息，例如均值、标准差、最小值和最大值，针对每个数值列。
- en: 'The `hist()` function plots the histogram for the selected columns, and `corr()`
    calculates the correlation matrix between the different features in the data.
    Try them out one at a time in a new cell to understand the data:'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`hist()` 函数绘制所选列的直方图，而 `corr()` 计算数据中不同特征之间的相关矩阵。请在新的单元格中逐个尝试它们，以了解数据：'
- en: '[PRE3]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The dataset needs transformations in order to be used for model training. The
    following code block will convert the `Geography` and `Gender` values from categorical
    strings to ordinal numbers so they can be taken by the ML algorithm later. Please
    note that model accuracy is not the main purpose of this exercise, and we are
    performing ordinal transformation for demonstration purposes. We will be using
    a popular Python ML library called sklearn for this exercise. Sklearn is also
    one of the easiest libraries to use and understand, especially for beginners.
    We will also discuss this library in more detail in *Chapter 5, Exploring Open-Source
    ML Libraries*. Copy and run the following code block in a new cell:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了用于模型训练，数据集需要进行转换。以下代码块将`Geography`和`Gender`值从分类字符串转换为序数，以便它们可以被ML算法后续使用。请注意，模型准确性不是这个练习的主要目的，我们进行序数转换是为了演示。我们将在这个练习中使用一个流行的Python
    ML库，即sklearn。Sklearn也是最容易使用和理解的库之一，特别是对于初学者。我们还会在*第5章，探索开源ML库*中更详细地讨论这个库。请在新的单元格中复制并运行以下代码块：
- en: '[PRE4]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Often, there could be some columns not needed for model training, as they do
    not contribute to model predictive power or could cause bias from an inclusion
    perspective. We can drop them using the following code block:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常，可能有一些列对于模型训练不是必需的，因为它们不会对模型的预测能力做出贡献，或者从包含的角度来看可能会引起偏差。我们可以使用以下代码块来删除它们：
- en: '[PRE5]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, the dataset has only the features we care about. Next, we need to split
    the data for training and validation. We also prepare each dataset by splitting
    the target variable, `Exited`, from the rest of the input features. Enter and
    run the following code block in a new cell:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，数据集只包含我们关心的特征。接下来，我们需要将数据分为训练集和验证集。我们还通过从其余输入特征中分割目标变量`Exited`来准备每个数据集。请在新的单元格中输入并运行以下代码块：
- en: '[PRE6]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We are ready to train the model. Enter and run the following code block in
    a new cell. Here, we will use the random forest algorithm to train the model,
    and the `fit()` function kicks off the model training:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经准备好训练模型。请在新的单元格中输入并运行以下代码块。在这里，我们将使用随机森林算法来训练模型，`fit()`函数启动模型训练：
- en: '[PRE7]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we will test the accuracy of the model using the `test` dataset. Here,
    we get the predictions returned by the model using the `predict()` function, and
    then use the `accuracy_score()` function to calculate the model accuracy using
    the predicted values (`churn_prediction_y`) and the true values (`churn_test_y`)
    for the test dataset:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用`test`数据集来测试模型的准确性。在这里，我们使用`predict()`函数获取模型返回的预测值，然后使用`accuracy_score()`函数通过预测值（`churn_prediction_y`）和测试数据集的真实值（`churn_test_y`）来计算模型的准确性：
- en: '[PRE8]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Congratulations! You have successfully installed the Jupyter data science environment
    on your local machine and trained a model using the random forest algorithm. You
    have validated that an ML approach could potentially solve this business problem.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经在本地机器上成功安装了Jupyter数据科学环境，并使用随机森林算法训练了一个模型。你已经验证了ML方法有可能解决这个商业问题。
- en: Summary
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have explored various ML algorithms that can be applied
    to solve different types of ML problems. By now, you should have a good understanding
    of which algorithms are suitable for which specific tasks. Additionally, you have
    set up a basic data science environment on your local machine, utilized the scikit-learn
    ML libraries to analyze and preprocess data, and successfully trained an ML model.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了各种可以应用于解决不同类型机器学习（ML）问题的ML算法。到目前为止，你应该对哪些算法适合哪些特定任务有了很好的理解。此外，你已经在本地机器上设置了一个基本的数据科学环境，利用scikit-learn
    ML库来分析和预处理数据，并成功训练了一个ML模型。
- en: In the upcoming chapter, our focus will shift to the intersection of data management
    and the ML lifecycle. We will delve into the significance of effective data management
    and discuss how to build a comprehensive data management platform on **Amazon
    Web Services** (**AWS**) to support downstream ML tasks. This platform will provide
    the necessary infrastructure and tools to streamline data processing, storage,
    and retrieval, ultimately enhancing the overall ML workflow.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们的重点将转向数据管理和ML生命周期的交集。我们将深入研究有效数据管理的重要性，并讨论如何构建一个综合性的数据管理平台在**亚马逊网络服务**（**AWS**）上以支持下游的ML任务。这个平台将提供必要的基础设施和工具来简化数据处理、存储和检索，从而最终提高整体的ML工作流程。
- en: Join our community on Discord
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mlsah](https://packt.link/mlsah)'
- en: '![](img/QR_Code7020572834663656.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code7020572834663656.png)'
