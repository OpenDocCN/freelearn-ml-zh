- en: '*Chapter 8*: Support Vector Regression'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Support vector regression** (**SVR**) can be an excellent option when the
    assumptions of linear regression models do not hold, such as when the relationship
    between our features and our target is too complicated to be described by a linear
    combination of weights. Even better, SVR allows us to model that complexity without
    having to expand the feature space.'
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines identify the hyperplane that maximizes the margin between
    two classes. The support vectors are the data points closest to the margin that
    *support* it, if you will. This turns out to be as useful for regression modeling
    as it is for classification. SVR finds the hyperplane containing the greatest
    number of data points. We will discuss how that works in the first section of
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than minimizing the sum of the squared residuals, as ordinary least squares
    regression does, SVR minimizes the coefficients within an acceptable error range.
    Like ridge and lasso regression, this can reduce model variance and the risk of
    overfitting. SVR works best when we are working with a small- to medium-sized
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is also quite flexible, allowing us to specify the acceptable
    error range, use kernels to model nonlinear relationships, and adjust hyperparameters
    to get the best bias-variance tradeoff possible. We will demonstrate that in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts of SVR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVR with a linear model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using kernels for nonlinear SVR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be working with the scikit-learn and `matplotlib` libraries.
    You can use `pip` to install these packages.
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts of SVR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start this section by discussing how support vector machines are used
    for classification. We will not go into much detail here, leaving a detailed discussion
    of support vector classification to [*Chapter 13*](B17978_13_ePub.xhtml#_idTextAnchor152),
    *Support Vector Machine Classification*. But starting with support vector machines
    for classification will lead nicely to an explanation of SVR.
  prefs: []
  type: TYPE_NORMAL
- en: 'As I discussed at the beginning of this chapter, support vector machines find
    the hyperplane that maximizes the margin between classes. When there are only
    two features present, that hyperplane is just a line. Consider the following example
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Support vector machine classification based on two features
    ](img/B17978_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Support vector machine classification based on two features
  prefs: []
  type: TYPE_NORMAL
- en: The two classes in this diagram, represented by red circles and blue squares,
    are **linearly separable** using the two features, x1 and x2\. The bold line is
    the decision boundary. It is the line that is furthest away from border data points
    for each class, or the maximum margin. These points are known as the support vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Since the data in the preceding plot is linearly separable, we can use what
    is known as **hard margin classification** without problems; that is, we can be
    strict about all the observations for each class being on the correct side of
    the decision boundary. But what if our data points look like what’s shown in the
    following plot?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Support vector machine classification with soft margins ](img/B17978_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Support vector machine classification with soft margins
  prefs: []
  type: TYPE_NORMAL
- en: These data points are not linearly separable. In this case, we can choose **soft
    margin classification** and ignore the outlier red circles.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss support vector classification in much greater detail in [*Chapter
    13*](B17978_13_ePub.xhtml#_idTextAnchor152), *Support Vector Machine Classification*,
    but this illustrates some of the key support vector machine concepts. These concepts
    can be applied well to models involving a continuous target. This is called **support
    vector regression** or **SVR**.
  prefs: []
  type: TYPE_NORMAL
- en: 'When building an SVR model, we decide on the acceptable amount of prediction
    error, ɛ. Errors within ɛ of our prediction, ![](img/B17978_08_001.png), in a
    one-feature model are not penalized. This is sometimes referred to as the epsilon-insensitive
    tube. SVR minimizes the coefficients consistent with all data points falling within
    that range. This is illustrated in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – SVR with an acceptable error range ](img/B17978_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – SVR with an acceptable error range
  prefs: []
  type: TYPE_NORMAL
- en: Stated more precisely, SVR minimizes the square of the coefficients, subject
    to the constraint that the error, ε, does not exceed a given amount.
  prefs: []
  type: TYPE_NORMAL
- en: It minimizes ![](img/B17978_08_002.png) with the constraint that ![](img/B17978_08_003.png),
    where ![](img/B17978_08_004.png) is a vector of weights (or coefficients), ![](img/B17978_08_005.png)
    is the actual target value minus the predicted value, and ![](img/B17978_08_006.png)
    is the acceptable amount of error.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, it is not reasonable to expect all the data points to fall within
    the desired range. But we can still seek to minimize that deviation. Let’s denote
    the distance of the wayward points from the margin as ξ. This gives us a new objective
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We minimize ![](img/B17978_08_007.png) with the constraint that ![](img/B17978_08_008.png),
    where *C* is a hyperparameter indicating how tolerant the model should be of errors
    outside the margin. A value of 0 for *C* means that it is not at all tolerant
    of those large errors. This is equivalent to the original objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – SVR with data points outside the acceptable range ](img/B17978_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – SVR with data points outside the acceptable range
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see several advantages of SVR. It is sometimes more important that
    our errors will not exceed a certain amount, than picking a model with the lowest
    absolute error. It may matter more if we are often off by a little but rarely
    by a lot than if we are often spot on but occasionally way off. Since this approach
    also minimizes our weights, it has the same advantages as regularization, and
    we reduce the likelihood of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear SVR and the kernel trick
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have not yet fully addressed the issue of linear separability with SVR.
    For simplicity, we will return to a classification problem involving two features.
    Let’s look at a plot of two features against a categorical target. The target
    has two possible values, represented by the dots and squares. x1 and x2 are numeric
    and have negative values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Class labels not linearly separable with two features ](img/B17978_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Class labels not linearly separable with two features
  prefs: []
  type: TYPE_NORMAL
- en: 'What can we do in a case like this to identify a margin between the classes?
    It is often the case that a margin can be identified at a higher dimension. In
    this example, we can use a polynomial transformation, as illustrated in the following
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Using polynomial transformation to establish the margin ](img/B17978_08_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Using polynomial transformation to establish the margin
  prefs: []
  type: TYPE_NORMAL
- en: There is now a third dimension, which is the sum of the squares of x1 and x2\.
    The dots are all higher than the squares. This is similar to how we used polynomial
    transformation in the previous chapter to specify a nonlinear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: One drawback of this approach is that we can quickly end up with too many features
    for our model to perform well. This is where the **kernel trick** comes in very
    handy. SVR can use a kernel function to expand the feature space implicitly without
    actually creating more features. This is done by creating a vector of values that
    can be used to fit a nonlinear margin.
  prefs: []
  type: TYPE_NORMAL
- en: While this allows us to fit a polynomial transformation such as the hypothetical
    one illustrated in the preceding plot, the most frequently used kernel function
    with SVR is the **radial basis function** (**RBF**). RBF is popular because it
    is faster than the other common kernel functions and because its gamma parameter
    makes it very flexible. We will explore how to use it in the last section of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: But for now, let’s start with a relatively straightforward linear model to see
    SVR in action.
  prefs: []
  type: TYPE_NORMAL
- en: SVR with a linear model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We often have enough domain knowledge to take an approach that is more nuanced
    than simply minimizing prediction errors in our training data. Using this knowledge
    may allow us to accept more bias in our model, when small amounts of bias do not
    matter much substantively, to reduce variance. With SVR, we can adjust hyperparameters
    such as epsilon (the acceptable error range) and *C* (which adjusts the tolerance
    for errors outside of that range) to improve our model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a linear model can perform well on your data, linear SVR might be a good
    choice. We can build a linear SVR model with scikit-learn’s `LinearSVR` class.
    Let’s try creating a linear SVR model with the gasoline tax data that we used
    in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need many of the same libraries that we used in the previous chapter to
    create the training and testing DataFrames and to preprocess the data. We also
    need to import the `LinearSVR` and `uniform` modules from scikit-learn and scipy,
    respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to import the `OutlierTrans` class, which we first discussed in
    [*Chapter 7*](B17978_07_ePub.xhtml#_idTextAnchor091), *Linear Regression Models*,
    to handle outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we load the gasoline tax data and create training and testing DataFrames.
    We create lists for numerical and binary features, as well as a separate list
    for `motorization_rate`. As we saw when we looked at the data in the previous
    chapter, we need to do a little more preprocessing with `motorization_rate`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This dataset contains gasoline tax data for each country in 2014, as well as
    fuel income dependence and measures of the strength of the democratic institutions:
    `polity`, `democracy_polity`, and `autocracy_polity`. `democracy_polity` is a
    binarized `polity` variable, taking on a value of 1 for countries with high `polity`
    scores. `autocracy_polity` has a value of 1 for countries with low `polity` scores.
    The `polity` feature is a measure of how democratic a country is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at summary statistics for the training data. We will need to standardize
    the data since there are dramatically different ranges and SVR performs much better
    on standardized data. Also, notice that `motorization_rate` has a lot of missing
    values. We may want to do better than simple imputation with that feature. We
    have decent non-missing counts for the dummy columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to build a column transformer to handle different data types. We can
    use `SimpleImputer` for the categorical features and numerical features, except
    for `motorization_rate`. We will use KNN imputation for the `motorization_rate`
    feature later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we are ready to fit our linear SVR model. We will choose a value of `0.2`
    for `epsilon`. This means that we are fine with any error within 0.2 standard
    deviations of the actual value (we use `TransformedTargetRegressor` to standardize
    the target). We will leave *C* – the hyperparameter determining our model’s tolerance
    for values outside of epsilon – at its default value of 1.0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before we fit our model, we still need to handle missing values for `motorization_rate`.
    We will add the KNN imputer to a pipeline after the column transformations. Since
    `motorization_rate` will be the only feature with missing values after the column
    transformations, the KNN imputer only changes values for that feature.
  prefs: []
  type: TYPE_NORMAL
- en: We need to use the target transformer because the column transformer will only
    change the features, not the target. We will pass the pipeline we just created
    to the target transformer’s `regressor` parameter to do the feature transformations,
    and indicate that we just want to do standard scaling for the target.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the default loss function for linear SVR is L1, but we could have
    chosen L2 instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `ttr.regressor_` to access all the elements of the pipeline, including
    the `linearsvr` object. This is how we get to the `coef_` attribute. The coefficients
    that are substantially different from 0 are `VAT_Rate` and the autocracy and national
    oil company dummies. Our model estimates a positive relationship between value-added
    tax rates and gasoline taxes, all else being equal. It estimates a negative relationship
    between having an autocracy or having a national oil company, and gasoline taxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we have not done any feature selection here. Instead, we are relying
    on the L1 regularization to push feature coefficients to near 0\. If we had many
    more features, or we were more concerned about computation time, it would be important
    to think about our feature selection strategy more carefully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do some cross-validation on this model. The mean absolute error and r-squared
    are not great, though that is certainly impacted by the small sample size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have not done any hyperparameter tuning yet. We do not know if our values
    for `epsilon` and *C* are the best ones for our model. Therefore, we need to do
    a grid search to experiment with different hyperparameter values. We will start
    with an exhaustive grid search, which often is not practical ( I recommend not
    running the next few steps on your machine unless you have a fairly high-performing
    one). After the exhaustive grid search, we will do a randomized grid search, which
    is usually substantially easier on system resources.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by creating a `LinearSVR` object without the `epsilon` hyperparameter
    specified, and we will recreate the pipeline. Then, we will create a dictionary,
    `svr_params`, with values to check for `epsilon` and *C*, called `regressor_linearsvr_epsilon`
    and `regressor_linearsvr_C`, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remember from our grid search from the previous chapter that the names of the
    keys must correspond with our pipeline steps. Our pipeline, which in this case
    can be accessed as the transformed target’s `regressor` object, has a `linearsvr`
    object with attributes for `epsilon` and *C*.
  prefs: []
  type: TYPE_NORMAL
- en: We will pass the `svr_params` dictionary to a `GridSearchCV` object and indicate
    that we want the scoring to be based on r-squared (if we wanted to base scoring
    on the mean absolute error, we could have also done that).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will run the `fit` method of the grid search object. I should repeat
    the warning I mentioned previously that you may not want to run an exhaustive
    grid search unless you are using a high-performing machine, or you do not mind
    letting it run while you go get a cup of coffee. Note that it takes about 26 seconds
    to run each time on my machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use the `best_params_` attribute of the grid search to get the
    hyperparameters associated with the highest score. We can see the score with those
    parameters with the `best_scores_` attribute. This tells us that we get the highest
    r-squared, which is 0.6, with a *C* of 0.1 and an `epsilon` value of 0.2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is good to know which values to choose for our hyperparameters. However,
    the exhaustive grid search was quite expensive computationally. Let’s try a randomized
    search instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will indicate that the random values for both `epsilon` and *C* should come
    from a uniform distribution with values between 0 and 1.5\. Then, we will pass
    that dictionary to a `RandomizedSearchCV` object. This runs substantially faster
    than the exhaustive grid search – a little over 1 second per iteration. This gives
    us higher `epsilon` and *C* values than the exhaustive grid search – that is,
    0.23 and 0.7, respectively. The r-squared value is a little lower, however:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at the predictions based on the best model from the randomized grid
    search. The randomized grid search object’s `predict` method can generate those
    predictions for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s look at the distribution of our residuals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Residual distribution for the gasoline tax linear SVR model
    ](img/B17978_08_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Residual distribution for the gasoline tax linear SVR model
  prefs: []
  type: TYPE_NORMAL
- en: Here, there is a little bit of bias (some overpredicting overall) and some positive
    skew.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also view a scatterplot of the predicted values against the residuals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Scatterplot of predictions and residuals for the gasoline tax
    linear SVR model ](img/B17978_08_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Scatterplot of predictions and residuals for the gasoline tax linear
    SVR model
  prefs: []
  type: TYPE_NORMAL
- en: These residuals are problematic. We are always overpredicting (predicted values
    are higher than actual values) at the lower and upper range of the predicted values.
    This is not what we want and is perhaps warning us of an unaccounted-for nonlinear
    relationship.
  prefs: []
  type: TYPE_NORMAL
- en: When our data is linearly separable, linear SVR can be an efficient choice.
    It can be used in many of the same situations where we would have used linear
    regression or linear regression with regularization. Its relative efficiency means
    we are not as concerned about using it with datasets that contain more than 10,000
    observations as we are with nonlinear SVR. However, when linear separability is
    not possible, we should explore nonlinear models.
  prefs: []
  type: TYPE_NORMAL
- en: Using kernels for nonlinear SVR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall from our discussion at the beginning of this chapter that we can use
    a kernel function to fit a nonlinear epsilon-insensitive tube. In this section,
    we will run a nonlinear SVR with the land temperatures data that we worked with
    in the previous chapter. But first, we will construct a linear SVR with the same
    data for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will model the average temperature for weather stations as a function of
    latitude and elevation. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by loading the familiar libraries. The only new class is `SVR`
    from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will load the land temperatures data and create training and testing
    DataFrames. We will also take a look at some descriptive statistics. There are
    several missing values for elevation and the ranges of the two features are very
    different. There are also some exceedingly low average temperatures:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s start with a linear SVR model of average temperatures. We can be fairly
    conservative with how we handle the outliers, only setting them to missing when
    the interquartile range is more than three times above or below the interquartile
    range. (We created the `OutlierTrans` class in [*Chapter 7*](B17978_07_ePub.xhtml#_idTextAnchor091),
    *Linear Regression Models*.) We will use KNN imputation for the missing elevation
    values and scale the data. Remember that we need to use the target transformer
    to scale the target variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Just as we did in the previous section, we will use a dictionary, `svr_params`,
    to indicate that we want to sample values from a uniform distribution for our
    hyperparameters – that is, `epsilon` and *C*. We will pass this dictionary to
    the `RandomizedSearchCV` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running `fit`, we can get the best parameters for `epsilon` and *C*,
    and the mean absolute error for the best model. The mean absolute error is fairly
    decent at about 2.8 degrees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Scatterplot of predictions and residuals for the land temperatures
    linear SVR model ](img/B17978_08_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Scatterplot of predictions and residuals for the land temperatures
    linear SVR model
  prefs: []
  type: TYPE_NORMAL
- en: There is a good amount of overpredicting at the upper range of the predicted
    values. We typically underpredict values just below that, between predicted gas
    tax values from 15 to 25 degrees. Perhaps we can improve the fit with a nonlinear
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do not have to change much to run a nonlinear SVR. We just need to create
    an `SVR` object and choose a kernel function. `rbf` is typically selected. (You
    may not want to fit this model on your machine unless you are using good hardware,
    or do not mind doing something else for a while and coming back for your results.)
    Take a look at the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There is a noticeable improvement in terms of the mean absolute error. Here,
    we can see that the `gamma` and C hyperparameters are doing a fair bit of work
    for us. If we are okay being about 2 degrees off on average, this model gets us
    there.
  prefs: []
  type: TYPE_NORMAL
- en: We go into much more detail regarding the gamma and C hyperparameters in [*Chapter
    13*](B17978_13_ePub.xhtml#_idTextAnchor152)*, Support Vector Machine Classification*.
    We also explore other kernels besides the rbf kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look again at the residuals to see if there is something problematic
    in how our errors are distributed, as was the case with our linear model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Scatterplot of predictions and residuals for the land temperatures
    nonlinear SVR model ](img/B17978_08_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Scatterplot of predictions and residuals for the land temperatures
    nonlinear SVR model
  prefs: []
  type: TYPE_NORMAL
- en: These residuals look substantially better than those for the linear model.
  prefs: []
  type: TYPE_NORMAL
- en: This illustrates how using a kernel function can increase the complexity of
    our model without us having to increase the feature space. By using the `rbf`
    kernel and adjusting the C and `gamma` hyperparameters, we address some of the
    underfitting we saw with the linear model. This is one of the great advantages
    of nonlinear SVR. The disadvantage, as we also saw, was that it was quite taxing
    on system resources. A dataset that contains 12,000 observations is at the upper
    limit of what can be handled easily with nonlinear SVR, particularly with a grid
    search for the best hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The examples in this chapter illustrated some of the advantages of SVR. The
    algorithm allows us to adjust hyperparameters to address underfitting or overfitting.
    This can be done without increasing the number of features. SVR is also less sensitive
    to outliers than methods such as linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: When we can build a good model with linear SVR, it is a perfectly reasonable
    choice. It can be trained much faster than a nonlinear model. However, we can
    often improve performance with a nonlinear SVR, as we saw in the last section
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'This discussion leads us to what we will explore in the next chapter, where
    we will look at two popular non-parametric regression algorithms: k-nearest neighbors
    and decision tree regression. These two algorithms make almost no assumptions
    about the distribution of our features and targets. Similar to SVR, they can capture
    complicated relationships in the data without increasing the feature space.'
  prefs: []
  type: TYPE_NORMAL
