- en: Chapter 5. Generic Object Detection for Industrial Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will introduce you to the world of generic object detection, with
    a closer look at the advantages that industrial applications yield compared to
    the standard academic research cases. As many of you will know, OpenCV 3 contains
    the well-known **Viola and Jones algorithm** (embedded as the CascadeClassifier
    class), which was specifically designed for robust face detection. However, the
    same interface can efficiently be used to detect any desired object class that
    suits your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More information on the Viola and Jones algorithm can be found in the following
    publication:'
  prefs: []
  type: TYPE_NORMAL
- en: Rapid object detection using a boosted cascade of simple features, Viola P.
    and Jones M., (2001). In Computer Vision and Pattern Recognition, 2001 (CVPR 2001).
    Proceedings of the 2001 IEEE Computer Society Conference on (Vol. 1, pp. I-511).
    IEEE.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter assumes that you have a basic knowledge of the cascade classification
    interface of OpenCV 3\. If not, here are some great starting points for understanding
    this interface and the basic usage of the supplied parameters and software:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.opencv.org/master/modules/objdetect/doc/cascade_classification.html](http://docs.opencv.org/master/modules/objdetect/doc/cascade_classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://docs.opencv.org/master/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html](http://docs.opencv.org/master/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://docs.opencv.org/master/doc/user_guide/ug_traincascade.html](http://docs.opencv.org/master/doc/user_guide/ug_traincascade.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Or you can simply read one of the PacktPub books that discuss this topic in
    more detail such as [Chapter 3](part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69
    "Chapter 3. Recognizing Facial Expressions with Machine Learning"), *Training
    a Smart Alarm to Recognize the Villain and His Cat*, of the *OpenCV for Secret
    Agents* book by Joseph Howse.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I will take you on a tour through specific elements that are
    important when using the Viola and Jones face detection framework for generic
    object detection. You will learn how to adapt your training data to the specific
    situation of your setup, how to make your object detection model rotation invariant,
    and you will find guidelines on how to improve the accuracy of your detector by
    smartly using environment parameters and situational knowledge. We will dive deeper
    into the actual object class model and explain what happens, combined with some
    smart tools for visualizing the actual process of object detection. Finally, we
    will look at GPU possibilities, which will lead to faster processing times. All
    of this will be combined with code samples and example use cases of general object
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: Difference between recognition, detection, and categorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For completely understanding this chapter, it is important that you understand
    that the Viola and Jones detection framework based on cascade classification is
    actually an object categorization technique and that it differs a lot from the
    concept of object recognition. This leads to a common mistake in computer vision
    projects, where people do not analyze the problem well enough beforehand and thus
    wrongfully decide to use this technique for their problems. Take into consideration
    the setup described in the following figure, which consists of a computer with
    a camera attached to it. The computer has an internal description of four objects
    (plane, cup, car, and snake). Now, we consider the case where three new images
    are supplied to the camera of the system.
  prefs: []
  type: TYPE_NORMAL
- en: '![Difference between recognition, detection, and categorization](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A simple computer vision setup
  prefs: []
  type: TYPE_NORMAL
- en: In the case that image A is presented to the system, the system creates a description
    of the given input image and tries to match it with the descriptions of the images
    in the computer memory database. Since that specific cup is in a slightly rotated
    position, the descriptor of the cup's memory image will have a closer match than
    the other object images in memory and thus this system is able to successfully
    recognize the known cup. This process is called **object recognition**, and is
    applied in cases where we know exactly which object we want to find in our input
    image.
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *"The goal of object recognition is to match (recognize) a specific object
    or scene. Examples include recognizing a specific building, such as the Pisa Tower,
    or a specific painting, such as the Mona Lisa. The object is recognized despite
    changes in scale, camera viewpoint, illumination conditions and partial occlusion."*
    |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Andrea Vedaldi and Andrew Zisserman* |'
  prefs: []
  type: TYPE_TB
- en: However, this technique has some downsides. If an object is presented to the
    system that doesn't have a description in the image database, the system will
    still return the closest match and thus the result could be very misleading. To
    avoid this we tend to put a threshold on the matching quality. If the threshold
    is not reached, we simply do not provide a match.
  prefs: []
  type: TYPE_NORMAL
- en: When image B is presented to the same system, we experience a new problem. The
    difference between the the given input image and the cup image in memory is so
    large (different size, different shape, different print, and so on) that the descriptor
    of image B will not be matched to the description of the cup in memory, again
    a large downside of object recognition. The problems even rise further, when image
    C is presented to the system. There, the known car from computer memory is presented
    to the camera system, but it is presented in a completely different setup and
    background than the one in memory. This could lead to the background influencing
    the object descriptor so much that the object is not recognized anymore.
  prefs: []
  type: TYPE_NORMAL
- en: '**Object detection** goes a bit further; it tries to find a given object in
    varying setups by learning a more object specific description instead of just
    a description of the image itself. In a situation where the detectable object
    class becomes more complex, and the variation of an object is large over several
    input images—we are no longer talking about single object detection, but rather
    about detecting a class of objects—this is where **object categorization** comes
    into play.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With object categorization, we try to learn a generic model for the object
    class that can handle a lot of variation inside the object class, as shown in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Difference between recognition, detection, and categorization](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'An example of object classes with lots of variation: cars and chairs/sofas'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside such a single object class, we try to cope with different forms of variation,
    as seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Difference between recognition, detection, and categorization](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Variation within a single object class: illumination changes, object pose,
    clutter, occlusions, intra-class appearance, and viewpoint'
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to make sure that your application actually is of the third
    and latter case if you plan to use the Viola and Jones object detection framework.
    In that case, the object instances you want to detect are not known beforehand
    and they have a large intra-class variance. Each object instance can have differences
    in shape, color, size, orientation, and so on. The Viola and Jones algorithm will
    model all that variance into a single object model that will be able to detect
    any given instance of the class, even if the object instance has never been seen
    before. And this is the large power of object categorization techniques, where
    they generalize well over a set of given object samples to learn specifics for
    the complete object class.
  prefs: []
  type: TYPE_NORMAL
- en: These techniques allow us to train object detectors for more complex classes
    and thus make object categorization techniques ideal to use in industrial applications
    such as object inspection, object picking, and so on, where typically used threshold-based
    segmentation techniques seem to fail due this large variation in the setup.
  prefs: []
  type: TYPE_NORMAL
- en: If your application does not handle objects in these difficult situations, then
    consider using other techniques such as object recognition if it suits your needs!
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start with the real work, let me take the time to introduce to you
    the basic steps that are common in object detection applications. It is important
    to pay equal attention to all the steps and definitely not to try and skip some
    of them for gaining time. These would all influence the end result of the object
    detector interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data collection**: This step includes collecting the necessary data for building
    and testing your object detector. The data can be acquired from a range of sources
    going from video sequences to images captured by a webcam. This step will also
    make sure that the data is formatted correctly to be ready to be passed to the
    training stage.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The actual model training**: In this step, you will use the data gathered
    in the first step to train an object model that will be able to detect that model
    class. Here, we will investigate the different training parameters and focus on
    defining the correct settings for your application.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Object detection**: Once you have a trained object model, you can use it
    to try and detect object instances in the given test images.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Validation**: Finally, it is important to validate the detection result of
    the third step, by comparing each detection with a manually defined ground truth
    of the test data. Various options for efficiency and accuracy validation will
    be discussed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's continue by explaining the first step, the data collection in more detail,
    which is also the first subtopic of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Smartly selecting and preparing application specific training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss how much training samples are needed according
    to the situational context and highlight some important aspects when preparing
    your annotations on the positive training samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by defining the principle of object categorization and its relation
    to training data, which can be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Smartly selecting and preparing application specific training data](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of positive and negative training data for an object model
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that the algorithm takes a set of positive object instances, which
    contain the different presentations of the object you want to detect (this means
    object instances under different lighting conditions, different scales, different
    orientations, small shape changes, and so on) and a set of negative object instances,
    which contains everything that you do not want to detect with your model. Those
    are then smartly combined into an object model and used to detect new object instances
    in any given input image as seen in the figure above.
  prefs: []
  type: TYPE_NORMAL
- en: The amount of training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many object detection algorithms depend heavily on large quantities of training
    data, or at least that is what is expected. This paradigm came to existence due
    to the academic research cases, mainly focusing on very challenging cases such
    as pedestrian and car detection. These are both object classes where a huge amount
    of intra-class variance exists, resulting in:'
  prefs: []
  type: TYPE_NORMAL
- en: A very large positive and negative training sample set, leading up to thousands
    and even millions of samples for each set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The removal of all information that pollutes the training set, rather than helping
    it, such as color information, and simply using feature information that is more
    robust to all this intra-class variation such as edge information and pixel intensity
    differences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, models were trained that successfully detect pedestrians and cars
    in about every possible situation, with the downside that training them required
    several weeks of processing.. However, when you look at more industrial specific
    cases, such as the picking of fruit from bins or the grabbing of objects from
    a conveyor belt, you can see that the amount of variance in objects and background
    is rather limited compared to these very challenging academic research cases.
    And this is a fact that we can use to our own advantage.
  prefs: []
  type: TYPE_NORMAL
- en: We know that the accuracy of the resulting object model is highly dependent
    on the training data used. In cases where your detector needs to work in all possible
    situations, supplying huge amounts of data seems reasonable. The complex learning
    algorithms will then decide which information is useful and which is not. However,
    in more confined cases, we could build object models by considering what our object
    model actually needs to do.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the Facebook DeepFace application, used for detecting faces in
    every possible situation using the neural networks approach uses 4.4 million labeled
    faces.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More information on the DeepFace algorithm can be found in:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deepface: Closing the gap to human-level performance in face verification,
    Taigman Y., Yang M., Ranzato M. A., and Wolf L. (2014, June). In Computer Vision
    and Pattern Recognition (CVPR), 2014, IEEE Conference on (pp. 1701-1708).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We therefore suggest using only meaningful positive and negative training samples
    for your object model by following a set of simple rules:'
  prefs: []
  type: TYPE_NORMAL
- en: For the positive samples, only use **natural occurring samples**. There are
    many tools out there that create artificially rotated, translated, and skewed
    images to turn a small training set into a large training set. However, research
    has proven that the resulting detector is less performant than simply collecting
    positive object samples that cover the actual situation of your application. Better
    use a small set of decent **high quality object samples**, rather than using a
    large set of low quality non-representative samples for your case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the negative samples, there are two possible approaches, but both start
    from the principle that you collect negative samples in the situation where your
    detector will be used, which is very different from the normal way of training
    object detects, where just a large set of random samples not containing the object
    are being used as negatives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Either point a camera at your scene and start grabbing random frames to sample
    negative windows from.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Or use your positive images to your advantage. Cut out the actual object regions
    and make the pixels black. Use those masked images as negative training data.
    Keep in mind that in this case the ratio between background information and actual
    object occurring in the window needs to be large enough. If your images are filled
    with object instances, cutting them will result in a complete loss of relevant
    background information and thus reduce the discriminative power of your negative
    training set.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to use a very small set of negative windows. If in your case only 4 or 5
    background situations can occur, then there is no need to use 100 negative images.
    Just take those five specific cases and sample negative windows from them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficiently collecting data in this way ensures that you will end up with a
    very robust model for your specific application! However, keep in mind that this
    also has some consequences. The resulting model will not be robust towards different
    situations than the ones trained for. However, the benefit in training time and
    the reduced need of training samples completely outweighs this downside.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Software for negative sample generation based on OpenCV 3 can be found at [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/generate_negatives/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/generate_negatives/).
  prefs: []
  type: TYPE_NORMAL
- en: You can use the negative sample generation software to generate samples like
    you can see in the following figure, where object annotations of strawberries
    are removed and replaced by black pixels.
  prefs: []
  type: TYPE_NORMAL
- en: '![The amount of training data](img/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of the output of the negative image generation tool, where annotations
    are cut out and replaced by black pixels
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the ratio between the object pixels and the background pixels
    is still large enough in order to ensure that the model will not train his background
    purely based on those black pixel regions. Keep in mind that avoiding the approach
    of using these black pixelated images, by simply collecting negative images, is
    always better. However, many companies forget this important part of data collection
    and just end up without a negative data set meaningful for the application. Several
    tests I performed proved that using a negative dataset from random frames from
    your application have a more discriminative negative power than black pixels cutout
    based images.
  prefs: []
  type: TYPE_NORMAL
- en: Creating object annotation files for the positive samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When preparing your positive data samples, it is important to put some time
    in your annotations, which are the actual locations of your object instances inside
    the larger images. Without decent annotations, you will never be able to create
    decent object detectors. There are many tools out there for annotation, but I
    have made one for you based on OpenCV 3, which allows you to quickly loop over
    images and put annotations on top of them.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Software for object annotation based on OpenCV 3 can be found at [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/object_annotation/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/object_annotation/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The OpenCV team was kind enough to also integrate this tool into the main repository
    under the apps section. This means that if you build and install the OpenCV apps
    during installation, that the tool is also accessible by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the software is quite straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by running the CMAKE script inside the GitHub folder of the specific
    project. After running CMAKE, the software will be accessible through an executable.
    The same approach applies for every piece of software in this chapter. Running
    the CMAKE interface is quite straightforward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will result in an executable that needs some input parameters, being the
    location of the positive image files and the output detection file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Keep in mind to always assign the absolute path of all files!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, parse the content of your positive image folder to a file (by using
    the supplied `folder_listing` software inside the object annotation folder), and
    then follow this by executing the annotation command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The folder listing tool should generate a file, which looks exactly like this:![Creating
    object annotation files for the positive samples](img/00076.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A sample positive samples file generated by the folder listing tool
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, fire up the annotation tool with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will fire up the software and give you the first image in a window, ready
    to apply annotations, as shown in the following figure:![Creating object annotation
    files for the positive samples](img/00077.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A sample of the object annotation tool
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can start by selecting the top-left corner of the object, then moving the
    mouse until you reach the bottom right corner of the object, which can be seen
    in the left part of the preceding figure. However, the software allows you to
    start your annotation from each possible corner. If you are unhappy with the selection,
    then reapply this step, until the annotation suits your needs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you agree on the selected bounding box, press the button that confirms
    a selection, which is key *C* by default. This will confirm the annotation, change
    its color from red to green, and add it to the annotations file. Be sure only
    to accept an annotation if you are 100% sure of the selection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the preceding two steps for the same image until you have annotated every
    single object instance in the image, as seen in the right part of the preceding
    example image. Then press the button that saves the result and loads in the following
    image, which is the *N* key by default.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, you will end up with a file called `annotations.txt`, which combines
    the location of the image files together with the ground truth locations of all
    object instances that occur inside the training images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to adapt the buttons that need to be pressed for all the separate
    actions, then open up the `object_annotation.cpp` file and browse to line 100
    and line 103\. There you can adapt the ASCII values assigned to the button you
    want to use for the operation.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of all ASCII codes assigned to your keyboard keys can be found at
    [http://www.asciitable.com/](http://www.asciitable.com/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The output from the software is a list of object detections in a `*.txt` file
    for each folder of positive image samples, which has a specific structure as seen
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating object annotation files for the positive samples](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of an object annotation tool
  prefs: []
  type: TYPE_NORMAL
- en: It starts with the absolute file location of each image in the folder. There
    was a choice of not using relative paths since the file will then be fully dependent
    on the location where it is stored. However, if you know what you are doing, then
    using relative file locations in relation to the executable should work just fine.
    Using the absolute path makes it more universal and more failsafe. The file location
    is followed by the number of detections for that specific image, which allows
    us to know beforehand how many ground truth objects we can expect. For each of
    the objects, the (x, y) coordinates are stored to the top-left corner combined
    with the width and the height of the bounding box. This is continued for each
    image, which is each time a new line appears in the detection output file.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is important for further model training that each set of ground truth values
    captured from other annotation systems is first converted to this format in order
    to ensure the decent working of the cascade classifier software embedded in OpenCV
    3.
  prefs: []
  type: TYPE_NORMAL
- en: 'A second point of attention when processing positive training images containing
    object instances, is that you need to pay attention to the way you perform the
    actual placement of the bounding box of an object instance. A good and accurately
    annotated ground truth set will always give you a more reliable object model and
    will yield better test and accuracy results. Therefore, I suggest using the following
    points of attention when performing object annotation for your application:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that the bounding box contains the complete object, but at the same
    time avoid as much background information as possible. The ratio of object information
    compared to background information should always be larger than 80%. Otherwise,
    the background could yield enough features to train your model on and the end
    result will be your detector model focusing on the wrong image information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viola and Jones suggests using squared annotations, based on a 24x24 pixel model,
    because it fits the shape of a face. However, this is not mandatory! If your object
    class is more rectangular like, then do annotate rectangular bounding boxes instead
    of squares. It is observed that people tend to push rectangular shaped objects
    in a square model size, and then wonder why it is not working correctly. Take,
    for example, the case of a pencil detector, where the model dimensions will be
    more like 10x70 pixels, which is in relation to the actual pencil dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try doing concise batches of images. It is better to restart the application
    10 times, than to have a system crash when you are about to finish a set of 1,000
    images with corresponding annotations. If somehow, the software or your computer
    fails it ensures that you only need to redo a small set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing your positive dataset into the OpenCV data vector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before the OpenCV 3 software allows you to train a cascade classifier object
    model, you will need to push your data into an OpenCV specific data vector format.
    This can be done by using the provided sample creation tool of OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The sample creation tool can be found at [https://github.com/Itseez/opencv/tree/master/apps/createsamples/](https://github.com/Itseez/opencv/tree/master/apps/createsamples/)
    and should be built automatically if OpenCV was installed correctly, which makes
    it usable through the `opencv_createsamples` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating the sample vector is quite easy and straightforward by applying the
    following instruction from the command line interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You have now prepared your positive training set. The last thing you should
    do is create a folder with the negative images, from which you will sample the
    negative windows randomly, and apply the folder listing functionality to it. This
    will result in a negative data referral file that will be used by the training
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter selection when training an object model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have built a decent training samples dataset, which is ready to process,
    the time has arrived to fire up the cascade classifier training software of OpenCV
    3, which uses the Viola and Jones cascade classifier framework to train your object
    detection model. The training itself is based on applying the boosting algorithm
    on either Haar wavelet features or Local Binary Pattern features. Several types
    of boosting are supported by the OpenCV interface, but for convenience, we use
    the frequently used AdaBoost interface.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you are interested in knowing all the technical details of the feature calculation,
    then have a look at the following papers which describe them in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**HAAR**: Papageorgiou, Oren and Poggio, "A general framework for object detection",
    International Conference on Computer Vision, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LBP**: T. Ojala, M. Pietikäinen, and D. Harwood (1994), "Performance evaluation
    of texture measures with classification based on Kullback discrimination of distributions",
    Proceedings of the 12th IAPR International Conference on Pattern Recognition (ICPR
    1994), vol. 1, pp. 582 - 585.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This section will discuss several parts of the training process in more detail.
    It will first elaborate on how OpenCV runs its cascade classification process.
    Then, we will take a deeper look at all the training parameters provided and how
    they can influence the training process and accuracy of the resulting model. Finally,
    we will open up the model file and look in more detail at what we can find there.
  prefs: []
  type: TYPE_NORMAL
- en: Training parameters involved in training an object model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The parameters discussed earlier are the most important ones to dig into when
    trying to train a successful classifier. Once this works, you can increase the
    performance of your classifier even more, by looking at the way boosting forms
    its weak classifiers. This can be adapted by the `-maxDepth` and `-maxWeakCount`
    parameters. However, for most cases, using **stump weak classifiers** (single
    layer decision trees) on single features is the best way to start, ensuring that
    single stage evaluation is not too complex and thus fast at detection time.
  prefs: []
  type: TYPE_NORMAL
- en: The cascade classification process in detail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you select the correct training parameters, you can start the cascade classifier
    training process, which will build your cascade classifier object detection model.
    In order to fully understand the cascade classification process that builds up
    your object model, it is important to know how OpenCV does its training of the
    object model, based on the boosting process.
  prefs: []
  type: TYPE_NORMAL
- en: Before we do this, we will have a quick look at the outline of the boosting
    principle in general.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More information on the boosting principle can be found in Freund Y., Schapire
    R., and Abe N (1999). A short introduction to boosting. Journal-Japanese Society
    For Artificial Intelligence, 14(771-780), 1612
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind boosting is that you have a very large pool of features that
    can be shaped into classifiers. Using all those features for a single classifier
    would mean that every single window in your test image will need to be processed
    for all these features, which will take a very long time and make your detection
    slow, especially if you consider how many negative windows are available in a
    test image. To avoid this, and to reject as many negative windows as fast as possible,
    boosting selects the features that are best at separating the positive and negative
    data and combines them into classifiers, until the classifier does a bit better
    than random guessing on the negative samples. This first step is called a weak
    classifier. Boosting repeats this process until the combination of all these weak
    classifiers reach the desired accuracy of the algorithm. The combination is called
    the strong classifier. The main advantage of this process is that tons of negative
    samples will already be discarded by the few early stages, with only evaluating
    a small set of features, thus decreasing detection time a lot.
  prefs: []
  type: TYPE_NORMAL
- en: We will now try to explain the complete process using the output generated by
    the cascade training software embedded in OpenCV 3\. The following figure illustrates
    how a strong cascade classifier is built from a set of stages of weak classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: '![The cascade classification process in detail](img/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A combination of weak classifier stages and early rejection of misclassified
    windows resulting in the famous cascade structure
  prefs: []
  type: TYPE_NORMAL
- en: 'The cascade classifier training process follows an iterative process to train
    subsequent stages of weak classifiers (1…N). Each stage consists of a set of weak
    classifiers, until the criteria for that specific stage have been reached. The
    following steps are an overview of what is happening at training each stage in
    OpenCV 3, according to the input parameters given and the training data provided.
    If you are interested in more specific details of each subsequent step, then do
    read the research paper of Viola and Jones (you can have a look at the citation
    on the first page of this chapter) on cascade classifiers. All steps described
    here are subsequently repeated for each stage until the desired accuracy for the
    strong classifier is reached. The following figure shows how such a stage output
    looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The cascade classification process in detail](img/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example output of a classifier stage training
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – grabbing positive and negative samples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You will notice that the first thing the training does is grabbing training
    samples for the current stage—first the positive samples from the data vector
    you supplied, and then the random negative window samples from the negative images
    that you supplied. This will be outputted for both steps as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If no positive samples can be found anymore, an error will be generated and
    training will be stopped. The total number of samples needed will increase once
    you start discarding positives that are no longer useful. The grabbing of the
    negatives for the current stage can take much longer than the positive sample
    grabbing since all windows that are correctly classified by the previous stages
    are discarded and new ones are searched. The deeper you go into the amount of
    stages, the harder this gets. As long as the number of samples grabbed keeps increasing
    (and yes, this can be very slow, so be patient), your application is still running.
    If no more negatives are found, the application will end training and you will
    need to lower the amount of negatives for each stage or add extra negative images.
  prefs: []
  type: TYPE_NORMAL
- en: The acceptance ratio that is achieved by the previous stage is reported after
    the grabbing of the negative windows. This value indicates whether the model trained
    until now is strong enough for your detection purposes or not!
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – precalculation of integral image and all possible features from the
    training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we have both positive and negative window-sized samples, the precalculation
    will calculate every single feature that is possible within the window size and
    apply it for each training sample. This can take some time according to the size
    of your model and according to the amount of training samples, especially when
    knowing that a model of 24x24 pixels can yield more than 16,000 features. As suggested
    earlier, assigning more memory can help out here or you could decide on selecting
    LBP features, of which the calculation is rather fast compared to HAAR features.
  prefs: []
  type: TYPE_NORMAL
- en: All features are calculated on the integral image representation of the original
    input window. This is done in order to speed up the calculation of the features.
    The paper by Viola and Jones explains in detail why this integral image representation
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: The features calculated are dumped into a large feature pool from which the
    boosting process can select the features needed to train the weak classifiers
    that will be used within each stage.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – firing up the boosting process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, the cascade classifier training is ready for the actual boosting process.
    This happens in several small steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Every possible weak classifier inside the feature pool is being calculated.
    Since we use stumps, which are basically weak classifiers based on single feature
    to create a decision tree, there are as many weak classifiers as features. If
    you prefer, you can decide to train actual decision trees with a predefined maximum
    depth, but this goes outside of the scope of this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each weak classifier is trained in order to minimize the misclassification rate
    on the training samples. For example, when using Real AdaBoost as a boosting technique,
    the Gini index is minimized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More information on the Gini index, used for the misclassification rate on
    the training samples can be found in:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Gastwirth, J. L. (1972). The estimation of the Lorenz curve and Gini index.
    The Review of Economics and Statistics, 306-316.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The weak classifier with the lowest misclassification rate is added as the next
    weak classifier to the current stage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the weak classifiers that are already added to the stage, the algorithm
    calculates the overall stage threshold, which is set so that the desired hit rate
    is guaranteed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, the weights of the samples are adapted based on their classification in
    the last iteration, which will yield a new set of weak classifiers in the next
    iteration, and thus the whole process can start again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'During the combination of the weak classifiers inside a single stage, which
    is visualized in the training output, the boosting process makes sure that:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall stage threshold does not drop below the minimum hit rate that was
    selected by the training parameters.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The false alarm rate on the negative samples decreases compared to the previous
    stage.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This process continues until:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The false acceptance ratio on the negative samples is lower than the maximum
    false alarm rate set. The process then simply starts training a new stage of weak
    classifiers for the detection model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The required stage false alarm rate is reached, which is `maxFalseAlarmRate^#stages`.
    This will yield an end to the model training since the model satisfies our requirements
    and better results cannot be achieved anymore. This will not happen often, since
    this value drops rather quickly, and after several stages, this would mean that
    you correctly classify more than 99% of your positive and negative samples all
    together.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The hit rate drops below the stage specific minimal hit rate, which is the `minHitRate^#stages`.
    At this stage, too many positives get wrongly classified and the maximum performance
    for your model is reached.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 4 – saving the temporary result to a stage file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After training each stage, the stage specific details about the weak classifiers
    and the thresholds are stored in the data folder, in a separate XML file. If the
    desired number of stages has been reached, then these subfiles are combined into
    a single cascade XML file.
  prefs: []
  type: TYPE_NORMAL
- en: However, the fact that every stage is stored separately means that you can stop
    the training at any time and create an in-between object detection model, by simply
    restarting the training command, but changing the `-numStages` parameter to the
    stage value on which you want to check the model's performance. This is ideal
    when you want to perform an evaluation on a validation set to ensure that your
    model does not start overfitting on the training data!
  prefs: []
  type: TYPE_NORMAL
- en: The resulting object model explained in detail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It has been observed that many users of the cascade classifier algorithm embedded
    in OpenCV 3 do not know the meaning of the inner construction of the object model
    which is stored in the XML files, which sometimes leads to wrong perceptions of
    the algorithm. This subsection will explain each internal part of the trained
    object models. We will discuss a model based on stump-typed weak classifiers,
    but the idea is practically the same for any other type of weak classifiers inside
    a stage, such as decision trees. The biggest difference is that the weight calculation
    inside the model gets more complex as compared to when using stump features. As
    to the weak classifiers structure inside each stage, this will be discussed for
    both HAAR- and LBP-based features since these are the two most used features inside
    OpenCV for training cascade classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The two models that will be used for explaining everything can be found at
  prefs: []
  type: TYPE_NORMAL
- en: '[OpenCVsource/data/haarcascades/haarcascade_frontalface_default.xml](http://OpenCVsource/data/haarcascades/haarcascade_frontalface_default.xml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenCVsource/data/lbpcascades/lbpcascade_frontalface.xml](http://OpenCVsource/data/lbpcascades/lbpcascade_frontalface.xml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first part of each XML stored model describes the parameters that specify
    the characteristics of the model itself and some of the important training parameters.
    Subsequently, we can find the type of training that is used, which is limited
    to boosting for now, and the type of features used for building the weak classifiers.
    We also have the width and height of the object model that will be trained, the
    parameters of the boosting process, which include the type of boosting used, the
    selected minimum hit ratio, and the selected maximum false acceptance rate. It
    also contains information about how the weak classifier stages are built, in our
    case as a combination of one feature deep trees, called stumps, with a maximum
    of 100 weak classifiers on a single stage. For the HAAR wavelet based model, we
    can then see which features are used, being only the basic upright features or
    the combined rotated 45-degree set.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the training-specific parameters, it starts to get interesting. Here,
    we find more information about the actual structure of the cascade classifier
    object model. The amount of stages is described, and then iteratively the model
    sums up the training results and thresholds for each separate stage which were
    generated by the boosting process. The basic structure of an object model can
    be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We start with an empty iteration tag for each stage. At each stage the number
    of weak classifiers that were used are defined, which in our case shows how many
    single layer decision trees (stumps) were used inside the stage. The stage threshold
    defines the threshold on the final stage score for a window. This is generated
    by scoring the window with each weak classifier and then summing and weighing
    the results for the complete stage. For each single weak classifier, we collect
    the internal structure, based on the decision nodes and layers used. The values
    present are the boosting values used for creating the decision tree and the leaf
    values, which are used to score a window that is evaluated by the weak classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The specifics for the internal node structure are different for HAAR wavelets
    and -based features. The storage of the leaf scores is equal. The values of the
    internal nodes, however, specify the relation to the bottom part of the code,
    which contains the actual features area, and which are also different for both
    the HAAR and the LBP approach. The difference between both techniques can be seen
    in the following sections, grabbing for both models the first tree of the first
    stage and a part of the feature set.
  prefs: []
  type: TYPE_NORMAL
- en: HAAR-like wavelet feature models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are two code snippets from the HAAR wavelet feature-based model,
    containing the internal node structure and the features structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'For the internal nodes, there are four values present at each node:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node left and node right**: These values indicate that we have a stump with
    two leafs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The node feature index**: This points the index of the feature used at this
    node according to its position inside the features list of that model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The node threshold**: This is the threshold that is set on the feature value
    for this weak classifier, which is learned from all the positive and negative
    samples in this stage of training. Since we are looking at models with stump based
    weak classifiers, this is also the stage threshold, which is set in the boosting
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The features inside the HAAR-based model are described by a set of rectangles,
    which can be up to three rectangles, so as to calculate every possible feature
    from a window. Then, there is a value indicating if the feature itself is tilted
    over 45 degrees or not. For each rectangle, which is a partial feature value,
    we have:'
  prefs: []
  type: TYPE_NORMAL
- en: The location of the rectangle, which is defined by upper-left corner x and y
    coordinates and the width and height of the rectangle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The weight for that specific partial feature. These weights are used to combine
    both partial feature rectangles into a predefined feature. These weights allow
    us to represent each feature with less rectangles than is actually necessary.
    An example of this can be seen in the following figure:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![HAAR-like wavelet feature models](img/00082.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A three rectangle feature can be represented by a two rectangle weighted combination
    reducing the need of an extra area calculation
  prefs: []
  type: TYPE_NORMAL
- en: The feature sum is finally calculated by first summing all values of the pixels
    inside the rectangle and then multiplying it with the weight factor. Finally,
    those weighted sums are combined together to yield as a final feature value. Keep
    in mind that all the coordinates retrieved for a single feature are in relation
    to the window/model size and not the complete image which is processed.
  prefs: []
  type: TYPE_NORMAL
- en: Local binary pattern models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are two code snippets from the LBP feature-based model, containing
    the internal node structure and the features structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The software takes in several input arguments, such as the model location,
    the image where the visualization needs to happen, and the output folder where
    the results need to be stored. However, in order to use the software correctly,
    there are some points of attention:'
  prefs: []
  type: TYPE_NORMAL
- en: The model needs to be HAAR wavelet or LBP feature based. Deleted because this
    functionality is no longer supported in OpenCV 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to supply an image that is an actual model detection for visualization
    purposes and resize it to the model scale or a positive training sample from the
    training data. This is to ensure that a feature of your model is placed at the
    correct location.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inside the code, you can adapt the visualization scales, one being for the video
    output of your model and one for the images that represent the stages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following two figures illustrate the visualization result of the Haar wavelet
    and LBP feature based frontal face model respectively, both incorporated into
    the OpenCV 3 repository under the data folder. The reason for the low image resolution
    of the visualization is quite obvious. The training process happens on a model
    scale; therefore, I wanted to start from an image of that size to illustrate that
    specific details of an object get removed, while general specifics of the object
    class still occur to be able to differentiate classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization tool for object models](img/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A set of frames from the video visualization of the frontal face model for both
    Haar wavelet and Local Binary Pattern features
  prefs: []
  type: TYPE_NORMAL
- en: The visualizations for example also clearly show that an LBP model needs less
    features and thus less weak classifiers to separate the training data successfully,
    which yields a faster model at detection time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization tool for object models](img/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A visualization of the first stage of the frontal face model for both Haar wavelet
    and Local Binary Pattern features
  prefs: []
  type: TYPE_NORMAL
- en: Using cross-validation to achieve the best model possible
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Making sure that you get the absolute best model given your training, testing
    the data can be done by applying a cross validation approach, such as the leave-one-out
    approach. The idea behind this is that you combine both training and test set
    and vary the test set that you use from the larger set. With each random test
    set and training set, you build a separate model and you perform the evaluation
    using precision-recall, which is discussed further in this chapter. Finally, the
    model that provides the best result could be adopted as a final solution. Thus,
    it could mitigate the impact of an error due to a new instance that is not represented
    in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More information on the topic of cross validation can be found in Kohavi R.
    (1995, August), a study of cross-validation and bootstrap for accuracy estimation
    and model selection in Ijcai (Vol. 14, No. 2, pp. 1137-1145).
  prefs: []
  type: TYPE_NORMAL
- en: Using scene specific knowledge and constraints to optimize the detection result
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once your cascade classifier object model is trained, you can use it to detect
    instances of the same object class in new input images, which are supplied to
    the system. However, once you apply your object model, you will notice that there
    are still false positive detections and objects that are not found. This section
    will cover techniques to improve your detection results, by removing, for example,
    most of the false positive detections with scene-specific knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Using the parameters of the detection command to influence your detection result
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you apply an object model to a given input image, you must consider several
    things. Let's first take a look at the detection function and some of the parameters
    that can be used to filter out your detection output. OpenCV 3 supplies three
    possible interfaces. We will discuss the benefits of using each one of them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Interface 1:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The first interface is the most basic one. It allows you to fast evaluate your
    trained model on a given test image. There are several elements on this basic
    interface that will allow you to manipulate the detection output. We will discuss
    these parameters in some more detail and highlight some points of attention when
    selecting the correct value.
  prefs: []
  type: TYPE_NORMAL
- en: scaleFactor is the scale step used to downscale the original image in order
    to create the image pyramid, which allows us to perform multiscale detections
    using only a single scale model. One downside is that this doesn't allow you to
    detect objects that are smaller than the object size. Using a value of 1.1 means
    that in each step the dimensions are reduced by 10% compared to the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing this value will make your detector run faster since it has less scale
    levels to evaluate, but it will yield the risk of losing detections that are in
    between scale steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decreasing the value will make your detector run slower, since more scale levels
    need to be evaluated, but it will increase the chance of detecting objects that
    were missed before. Also, it will yield more detections on an actual object, resulting
    in a higher certainty.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that adding scale levels also gives rise to more false positive
    detections, since those are bound to each layer of the image pyramid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A second interesting parameter to adapt for your needs is the `minNeighbors`
    parameter. It describes how many overlapping detections occur due to the sliding
    window approach. Each detection overlapping by more than 50% with another will
    be merged together as a sort of nonmaxima suppression.
  prefs: []
  type: TYPE_NORMAL
- en: Putting this value on 0 means that you will get all detections generated by
    the windows that get through the complete cascade. However, due to the sliding
    window approach (with steps of 8 pixels) many detections will happen for a single
    window, due to the nature of cascade classifiers, which train in some variance
    on object parameters in order to better generalize over an object class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a value means that you want to count how many windows there should be,
    at least those combined by the nonmaxima suppression in order to keep the detection.
    This is interesting since an actual object should yield far more detections than
    a false positive. So, increasing this value will reduce the number of false positive
    detections (which have a low amount of overlapping detections) and keep the true
    detections (which have a large amount of overlapping detections).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A downside is that on a certain point, actual objects with a lower certainty
    of detections and thus less overlapping windows will disappear while some false
    positive detections might still stand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `minSize` and `maxSize` parameters to effectively reduce the scale space
    pyramid. In an industrial setup with, for example, a fixed camera position, such
    as a conveyor belt setup, you can in most cases guarantee that objects will have
    certain dimensions. Adding scale values in this case and thus defining a scale
    range will decrease processing time for a single image a lot, by removing undesired
    scale levels. As an extra advantage, all false positive detections on those undesired
    scales will also disappear. If you leave these values blank, the algorithm will
    start building the image pyramid at input image dimensions, in a bottom-up manner,
    downscale in steps equaling the scale percentage, until one of the dimensions
    is smaller than the largest object dimension. This will be the top of the image
    pyramid, which is also the place where later, at the detection time, the detection
    algorithm will start running its object detector.
  prefs: []
  type: TYPE_NORMAL
- en: '**Interface 2:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The second interface brings a small addition, by adding the `numDetections`
    parameter. This allows you to put the `minNeighbors` value on 1, applying the
    merging of overlapping windows as nonmaxima suppression, but at the same time
    returning you a value of the overlapping windows, which were merged. This value
    can be seen as a certainty score of your detection. The higher the value, the
    better or the more certain the detection.
  prefs: []
  type: TYPE_NORMAL
- en: '**Interface 3:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: A downside of this interface is that 100 windows with a very small certainty
    of detection on an individual basis can simply out rule a single detection with
    a very high individual certainty of detection. This is where the third interface
    can bring us the solution. It allows us to look at the individual scores of each
    detection window (described by the threshold value of the last stage of the classifier).
    You can then grab all those values and threshold the certainty score of those
    individual windows. When applying nonmaxima suppression in this case, the threshold
    values of all overlapping windows are combined.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keep in mind that if you want to try out the third interface in OpenCV 3.0,
    you have to put the parameter `outputRejectLevels` on `true`. If you do not do
    this, then the level weights matrix, which has the threshold scores, will not
    be filled.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Software illustrating the two most used interfaces for object detection can
    be found at [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_simple](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_simple)
    and [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_score](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_score).
    OpenCV detection interfaces change frequently and that it is possible that new
    interfaces are already available which are not discussed here.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing object instance detection and reducing false positive detections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have chosen the most appropriate way of retrieving the object detections
    for your application, you can evaluate the proper output of your algorithm. Two
    of the most common problems found after training an object detector are:'
  prefs: []
  type: TYPE_NORMAL
- en: Object instances that are not being detected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too much false positive detections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason for the first problem can be explained by looking at the generic
    object model that we trained for the object class based on positive training samples
    of that object class. This lets us conclude that the training either:'
  prefs: []
  type: TYPE_NORMAL
- en: Did not contain enough positive training samples, making it impossible to generalize
    well over new object samples. In this case, it is important to add those false
    negative detections as positive samples to the training set and retrain your model
    with the extra data. This principle is called "reinforced learning".
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We overtrained our model to the training set, again reducing the generalization
    of the model. To avoid this, reduce the model in stages and thus in complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second problem is quite normal and happens more than often. It is impossible
    to supply enough negative samples and at the same time ensure that there will
    not be a single negative window that could still yield a positive detection at
    a first run. This is mainly due to the fact that it is very hard for us humans
    to understand how the computer sees an object based on features. On the other
    hand, it is impossible to grasp every possible scenario (lighting conditions,
    interactions during the production process, filth on the camera, and so on) at
    the very start when training an object detector. You should see the creation of
    a good and stable model as an iterative process.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An approach to avoid the influence of lighting conditions can be to triplicate
    the training set by generating artificial dark and artificial bright images for
    each sample. However, keep in mind the disadvantages of artificial data as discussed
    in the beginning of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to reduce the amount of false positive detections, we generally need
    to add more negative samples. However, it is important not to add randomly generated
    negative windows, since the extra knowledge that they would bring to the model
    would, in most cases, simply be minimal. It is better to add meaningful negative
    windows that can increase the quality of the detector. This is known as **hard
    negative mining** using a **bootstrapping** process. The principle is rather simple:'
  prefs: []
  type: TYPE_NORMAL
- en: Start by training a first object model based on your initial training set of
    positive and negative window samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, collect a set of negative images, which are either specific to your application
    (if you want to train an object detector specific to your setup) or which are
    more general (if you want your object detector to work in versatile conditions).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run your detector on that set of negative images, with a low certainty threshold
    and save all found detections. Cut them out of the supplied negative images and
    rescale them towards the object model size dimensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, retrain your object model, but add all the found windows to your negative
    training set in order to ensure that your model will now be trained with this
    extra knowledge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will ensure that the accuracy of your model goes up by a fair and decent
    amount depending on the quality of your negative images.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When adding the found extra and useful negative samples, add them to the top
    of your `background.txt` file! This forces the OpenCV training interface to first
    grab these more important negative samples before sampling all the standard negative
    training images provided! Be sure that they have exactly the required model size
    so that they can only be used once as a negative training sample.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining rotation invariance object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The biggest advantage of this approach is that you only need to train a single
    orientation model and can put your time in updating and tweaking that single model
    in order to make it as efficient as possible. Another advantage is that you can
    combine all the detections in different rotations, by providing some overlap,
    and then increase the certainty of a detection, by smartly removing false positives
    that do not get detections over multiple orientations. So basically, it is kind
    of a trade-off between benefits and downsides of the approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are still some downsides to this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: You will need to apply a multiscale detector to each layer of your 3D representation
    matrix. This will definitely increase the search time for object instances compared
    to single orientation object detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will create false positive detections on each orientation, which will also
    be warped back, thus increasing the total number of false positive detections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take a deeper look at parts of the source code used for performing this
    rotation invariance and explain what is actually happening. The first interesting
    part can be found in the creation of the 3D matrix of rotated images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Basically, what we do is read the original image, create a vector of Mat objects
    that can contain each rotated input image, and apply the rotation function on
    top of it. As you will notice, we immediately apply all preprocessing which is
    needed for efficient object detection using the cascade classifier interface such
    as rendering the image to grayscale values and applying a histogram equalization
    in order to cope a bit with illumination changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rotate function can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This code first calculates a rotation matrix based on the angle, which is expressed
    in degrees, that we want to be rotated and then applies an affine transformation
    based on this rotation matrix. Keep in mind that rotating an image like this can
    lead to an information loss of objects at the borders. This code example assumes
    your objects will occur at the center of the image and thus this does not influence
    the result. You can avoid this by enlarging the original image by adding black
    borders to that. The width and height of the image are equal so that the image
    information loss is minimal. This can be done by adding the following code right
    behind the reading of the original input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This code will simply expand the original image to match a square region depending
    on the largest dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, on each level of the 3D image representation, a detection is performed
    and the found detections are warped back to the original image using a similar
    approach as warping the original image:'
  prefs: []
  type: TYPE_NORMAL
- en: Take the four corner points of the found detection in the rotated image and
    add them into a matrix for rotation warping (code line 95-103).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the inverse transformation matrix based on the angle of the current rotated
    image (code line 106-108).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, draw a rotated rectangle on the information of the rotated four matrix
    points (code line 111-128).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following figure shows the exact result of applying a rotation invariant
    face detection to an image with faces in multiple orientations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Obtaining rotation invariance object detection](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Rotation invariant face detection starting with the following angle steps [1
    degree, 10 degrees, 25 degrees, 45 degrees]
  prefs: []
  type: TYPE_NORMAL
- en: We see that four times the suggested technique is applied to the same input
    image. We played around with the parameters in order to see the influence on detection
    time and the detections returned. In all cases, we applied a search from 0 to
    360 degrees, but changed the angle step in between each stage of the 3D rotation
    matrix from 0 to 45 degrees.
  prefs: []
  type: TYPE_NORMAL
- en: '| Applied angle step | Total time for executing all detections |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 degree | 220 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 10 degrees | 22.5 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 25 degrees | 8.6 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| 45 degrees | 5.1 seconds |'
  prefs: []
  type: TYPE_TB
- en: As we can see, the detection time is reduced drastically when increasing the
    step of the angle. Knowing that an object model on itself could cover at least
    20 degrees in total, we can easily reduce the step in order to significantly decrease
    the processing time.
  prefs: []
  type: TYPE_NORMAL
