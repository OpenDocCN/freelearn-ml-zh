- en: Chapter 5. Generic Object Detection for Industrial Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 工业应用中的通用目标检测
- en: This chapter will introduce you to the world of generic object detection, with
    a closer look at the advantages that industrial applications yield compared to
    the standard academic research cases. As many of you will know, OpenCV 3 contains
    the well-known **Viola and Jones algorithm** (embedded as the CascadeClassifier
    class), which was specifically designed for robust face detection. However, the
    same interface can efficiently be used to detect any desired object class that
    suits your needs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向您介绍通用目标检测的世界，并更详细地探讨工业应用与标准学术研究案例相比的优势。正如许多人所知，OpenCV 3包含著名的**Viola和Jones算法**（作为CascadeClassifier类嵌入），该算法专门设计用于鲁棒的人脸检测。然而，相同的接口可以有效地用于检测任何满足您需求的物体类别。
- en: Note
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'More information on the Viola and Jones algorithm can be found in the following
    publication:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Viola和Jones算法的更多信息可以在以下出版物中找到：
- en: Rapid object detection using a boosted cascade of simple features, Viola P.
    and Jones M., (2001). In Computer Vision and Pattern Recognition, 2001 (CVPR 2001).
    Proceedings of the 2001 IEEE Computer Society Conference on (Vol. 1, pp. I-511).
    IEEE.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单特征的提升级联进行快速目标检测，Viola P. 和 Jones M.，(2001)。在计算机视觉和模式识别，2001 (CVPR 2001)。IEEE计算机学会会议论文集，第1卷，第I-511页。IEEE。
- en: 'This chapter assumes that you have a basic knowledge of the cascade classification
    interface of OpenCV 3\. If not, here are some great starting points for understanding
    this interface and the basic usage of the supplied parameters and software:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设您对OpenCV 3的级联分类接口有基本了解。如果没有，以下是一些理解此接口和提供的参数及软件的基本用法的好起点：
- en: '[http://docs.opencv.org/master/modules/objdetect/doc/cascade_classification.html](http://docs.opencv.org/master/modules/objdetect/doc/cascade_classification.html)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://docs.opencv.org/master/modules/objdetect/doc/cascade_classification.html](http://docs.opencv.org/master/modules/objdetect/doc/cascade_classification.html)'
- en: '[http://docs.opencv.org/master/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html](http://docs.opencv.org/master/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://docs.opencv.org/master/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html](http://docs.opencv.org/master/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html)'
- en: '[http://docs.opencv.org/master/doc/user_guide/ug_traincascade.html](http://docs.opencv.org/master/doc/user_guide/ug_traincascade.html)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://docs.opencv.org/master/doc/user_guide/ug_traincascade.html](http://docs.opencv.org/master/doc/user_guide/ug_traincascade.html)'
- en: Note
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Or you can simply read one of the PacktPub books that discuss this topic in
    more detail such as [Chapter 3](part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69
    "Chapter 3. Recognizing Facial Expressions with Machine Learning"), *Training
    a Smart Alarm to Recognize the Villain and His Cat*, of the *OpenCV for Secret
    Agents* book by Joseph Howse.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以简单地阅读PacktPub出版的关于此主题更详细讨论的书籍之一，例如[第3章](part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69
    "第3章 使用机器学习识别面部表情")，*训练一个智能警报器来识别恶棍和他的猫*，来自Joseph Howse的*OpenCV for Secret Agents*一书。
- en: In this chapter, I will take you on a tour through specific elements that are
    important when using the Viola and Jones face detection framework for generic
    object detection. You will learn how to adapt your training data to the specific
    situation of your setup, how to make your object detection model rotation invariant,
    and you will find guidelines on how to improve the accuracy of your detector by
    smartly using environment parameters and situational knowledge. We will dive deeper
    into the actual object class model and explain what happens, combined with some
    smart tools for visualizing the actual process of object detection. Finally, we
    will look at GPU possibilities, which will lead to faster processing times. All
    of this will be combined with code samples and example use cases of general object
    detection.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将带您了解使用Viola和Jones人脸检测框架进行通用目标检测时的重要元素。您将学习如何调整您的训练数据以适应您的特定设置，如何使您的目标检测模型具有旋转不变性，并且您将找到关于如何通过智能地使用环境参数和情境知识来提高检测器精度的指南。我们将更深入地探讨实际的物体类别模型，并解释发生了什么，结合一些用于可视化目标检测实际过程的智能工具。最后，我们将探讨GPU的可能性，这将导致更快的处理时间。所有这些都将结合代码示例和通用目标检测的示例用例。
- en: Difference between recognition, detection, and categorization
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别、检测和分类之间的区别
- en: For completely understanding this chapter, it is important that you understand
    that the Viola and Jones detection framework based on cascade classification is
    actually an object categorization technique and that it differs a lot from the
    concept of object recognition. This leads to a common mistake in computer vision
    projects, where people do not analyze the problem well enough beforehand and thus
    wrongfully decide to use this technique for their problems. Take into consideration
    the setup described in the following figure, which consists of a computer with
    a camera attached to it. The computer has an internal description of four objects
    (plane, cup, car, and snake). Now, we consider the case where three new images
    are supplied to the camera of the system.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完全理解这一章，重要的是你要理解基于级联分类的Viola和Jones检测框架实际上是一种物体分类技术，并且它与物体识别的概念有很大不同。这导致计算机视觉项目中常见的错误，即人们在事先没有充分分析问题的情况下，错误地决定使用这项技术来解决问题。考虑以下图所示的设置，它由一个连接相机的计算机组成。计算机内部有四个物体的描述（飞机、杯子、汽车和蛇）。现在，我们考虑向系统的相机提供三个新图像的情况。
- en: '![Difference between recognition, detection, and categorization](img/00071.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![识别、检测和分类之间的差异](img/00071.jpeg)'
- en: A simple computer vision setup
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的计算机视觉设置
- en: In the case that image A is presented to the system, the system creates a description
    of the given input image and tries to match it with the descriptions of the images
    in the computer memory database. Since that specific cup is in a slightly rotated
    position, the descriptor of the cup's memory image will have a closer match than
    the other object images in memory and thus this system is able to successfully
    recognize the known cup. This process is called **object recognition**, and is
    applied in cases where we know exactly which object we want to find in our input
    image.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图像A呈现给系统，系统会创建给定输入图像的描述，并尝试将其与计算机内存数据库中图像的描述相匹配。由于特定的杯子略微旋转，杯子内存图像的描述符将与内存中其他物体图像的匹配更接近，因此这个系统能够成功识别出已知的杯子。这个过程被称为**物体识别**，并应用于我们知道我们想要在输入图像中找到的确切物体的情况。
- en: '|   | *"The goal of object recognition is to match (recognize) a specific object
    or scene. Examples include recognizing a specific building, such as the Pisa Tower,
    or a specific painting, such as the Mona Lisa. The object is recognized despite
    changes in scale, camera viewpoint, illumination conditions and partial occlusion."*
    |   |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|   | *"物体识别的目标是匹配（识别）特定的物体或场景。例如，识别特定的建筑，如比萨斜塔，或特定的画作，如蒙娜丽莎。即使物体在比例、相机视角、光照条件和部分遮挡等方面发生变化，也能识别出该物体。"*
    |   |'
- en: '|   | --*Andrea Vedaldi and Andrew Zisserman* |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|   | --*安德烈亚·韦达利和安德鲁·齐斯泽曼* |'
- en: However, this technique has some downsides. If an object is presented to the
    system that doesn't have a description in the image database, the system will
    still return the closest match and thus the result could be very misleading. To
    avoid this we tend to put a threshold on the matching quality. If the threshold
    is not reached, we simply do not provide a match.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这项技术有一些缺点。如果一个物体呈现给系统，而图像数据库中没有该物体的描述，系统仍然会返回最接近的匹配，因此结果可能会非常误导。为了避免这种情况，我们倾向于在匹配质量上设置一个阈值。如果没有达到阈值，我们就简单地不提供匹配。
- en: When image B is presented to the same system, we experience a new problem. The
    difference between the the given input image and the cup image in memory is so
    large (different size, different shape, different print, and so on) that the descriptor
    of image B will not be matched to the description of the cup in memory, again
    a large downside of object recognition. The problems even rise further, when image
    C is presented to the system. There, the known car from computer memory is presented
    to the camera system, but it is presented in a completely different setup and
    background than the one in memory. This could lead to the background influencing
    the object descriptor so much that the object is not recognized anymore.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当图像B呈现给相同的系统时，我们会遇到一个新的问题。给定输入图像与内存中杯子图像的差异如此之大（不同大小、不同形状、不同图案等），以至于图像B的描述符将不会与内存中杯子的描述相匹配，这是物体识别的一个大缺点。当图像C呈现给系统时，问题甚至进一步加剧。在那里，计算机内存中的已知汽车被呈现给相机系统，但它呈现的设置和背景与内存中的完全不同。这可能导致背景对物体描述符的影响如此之大，以至于物体不再被识别。
- en: '**Object detection** goes a bit further; it tries to find a given object in
    varying setups by learning a more object specific description instead of just
    a description of the image itself. In a situation where the detectable object
    class becomes more complex, and the variation of an object is large over several
    input images—we are no longer talking about single object detection, but rather
    about detecting a class of objects—this is where **object categorization** comes
    into play.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**物体检测**更进一步；它试图通过学习更具体的物体描述而不是仅仅学习图像本身的描述来在变化的设置中找到给定的物体。在可检测的物体类别变得更加复杂，并且物体在多个输入图像中的变化很大——我们不再谈论单个物体检测，而是关于检测一个物体类别——这就是**物体分类**发挥作用的地方。'
- en: 'With object categorization, we try to learn a generic model for the object
    class that can handle a lot of variation inside the object class, as shown in
    the following figure:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在物体分类中，我们试图学习一个通用的模型来处理物体类别中的大量变化，如下面的图所示：
- en: '![Difference between recognition, detection, and categorization](img/00072.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![识别、检测和分类之间的区别](img/00072.jpeg)'
- en: 'An example of object classes with lots of variation: cars and chairs/sofas'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有大量变化的物体类别示例：汽车和椅子/沙发
- en: 'Inside such a single object class, we try to cope with different forms of variation,
    as seen in the following figure:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样一个单一物体类别中，我们试图应对不同的变化形式，如下面的图所示：
- en: '![Difference between recognition, detection, and categorization](img/00073.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![识别、检测和分类之间的区别](img/00073.jpeg)'
- en: 'Variation within a single object class: illumination changes, object pose,
    clutter, occlusions, intra-class appearance, and viewpoint'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 单个物体类别内的变化：光照变化、物体姿态、杂乱、遮挡、类内外观和视角
- en: It is very important to make sure that your application actually is of the third
    and latter case if you plan to use the Viola and Jones object detection framework.
    In that case, the object instances you want to detect are not known beforehand
    and they have a large intra-class variance. Each object instance can have differences
    in shape, color, size, orientation, and so on. The Viola and Jones algorithm will
    model all that variance into a single object model that will be able to detect
    any given instance of the class, even if the object instance has never been seen
    before. And this is the large power of object categorization techniques, where
    they generalize well over a set of given object samples to learn specifics for
    the complete object class.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划使用Viola和Jones物体检测框架，确保您的应用程序实际上属于第三种和后续情况非常重要。在这种情况下，您想要检测的物体实例事先是未知的，并且它们具有很大的类内方差。每个物体实例可能在形状、颜色、大小、方向等方面有所不同。Viola和Jones算法将所有这些方差建模成一个单一的对象模型，该模型能够检测到该类别的任何给定实例，即使该物体实例之前从未见过。这正是物体分类技术强大的地方，它们在给定的一组物体样本上很好地泛化，以学习整个物体类别的具体特征。
- en: These techniques allow us to train object detectors for more complex classes
    and thus make object categorization techniques ideal to use in industrial applications
    such as object inspection, object picking, and so on, where typically used threshold-based
    segmentation techniques seem to fail due this large variation in the setup.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术使我们能够为更复杂的类别训练物体检测器，因此物体分类技术在工业应用中非常理想，例如物体检查、物体拣选等，在这些应用中，通常使用的基于阈值的分割技术似乎由于设置中的这种大范围变化而失败。
- en: If your application does not handle objects in these difficult situations, then
    consider using other techniques such as object recognition if it suits your needs!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的应用程序不处理这些困难情况中的物体，那么考虑使用其他技术，如物体识别，如果它符合您的需求的话！
- en: 'Before we start with the real work, let me take the time to introduce to you
    the basic steps that are common in object detection applications. It is important
    to pay equal attention to all the steps and definitely not to try and skip some
    of them for gaining time. These would all influence the end result of the object
    detector interface:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始实际工作之前，让我花点时间向您介绍在物体检测应用中常见的几个基本步骤。注意所有步骤并确保不要试图跳过其中的一些步骤以节省时间是非常重要的。这些都会影响物体检测接口的最终结果：
- en: '**Data collection**: This step includes collecting the necessary data for building
    and testing your object detector. The data can be acquired from a range of sources
    going from video sequences to images captured by a webcam. This step will also
    make sure that the data is formatted correctly to be ready to be passed to the
    training stage.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据收集**：这一步包括收集构建和测试您的对象检测器所需的数据。数据可以从视频序列到由网络摄像头捕获的图像等多种来源获取。这一步还将确保数据格式正确，以便准备好传递到训练阶段。'
- en: '**The actual model training**: In this step, you will use the data gathered
    in the first step to train an object model that will be able to detect that model
    class. Here, we will investigate the different training parameters and focus on
    defining the correct settings for your application.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实际模型训练**：在这一步，您将使用第一步收集到的数据来训练一个能够检测该模型类的对象模型。在这里，我们将研究不同的训练参数，并专注于定义适合您应用的正确设置。'
- en: '**Object detection**: Once you have a trained object model, you can use it
    to try and detect object instances in the given test images.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**对象检测**：一旦您有一个训练好的对象模型，您就可以使用它来尝试在给定的测试图像中检测对象实例。'
- en: '**Validation**: Finally, it is important to validate the detection result of
    the third step, by comparing each detection with a manually defined ground truth
    of the test data. Various options for efficiency and accuracy validation will
    be discussed.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**验证**：最后，通过将每个检测与测试数据的手动定义的地面真实值进行比较，验证第三步的检测结果非常重要。我们将讨论各种用于效率和精度验证的选项。'
- en: Let's continue by explaining the first step, the data collection in more detail,
    which is also the first subtopic of this chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续详细解释第一步，即数据收集，这也是本章的第一个子主题。
- en: Smartly selecting and preparing application specific training data
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能选择和准备特定于应用的训练数据
- en: In this section, we will discuss how much training samples are needed according
    to the situational context and highlight some important aspects when preparing
    your annotations on the positive training samples.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论根据情境背景需要多少训练样本，并强调在准备正训练样本的注释时的一些重要方面。
- en: 'Let''s start by defining the principle of object categorization and its relation
    to training data, which can be seen in the following figure:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义对象分类的原则及其与训练数据的关系，如图所示：
- en: '![Smartly selecting and preparing application specific training data](img/00074.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![智能选择和准备特定于应用的训练数据](img/00074.jpeg)'
- en: An example of positive and negative training data for an object model
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个对象模型的正负训练数据的示例
- en: The idea is that the algorithm takes a set of positive object instances, which
    contain the different presentations of the object you want to detect (this means
    object instances under different lighting conditions, different scales, different
    orientations, small shape changes, and so on) and a set of negative object instances,
    which contains everything that you do not want to detect with your model. Those
    are then smartly combined into an object model and used to detect new object instances
    in any given input image as seen in the figure above.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的想法是，它接受一组正对象实例，这些实例包含您想要检测的对象的不同表现形式（这意味着在不同光照条件下、不同尺度、不同方向、小的形状变化等对象实例），以及一组负对象实例，这些实例包含您不希望模型检测到的所有内容。然后，这些实例被巧妙地组合成一个对象模型，并用于检测如图所示输入图像中的新对象实例。
- en: The amount of training data
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据量
- en: 'Many object detection algorithms depend heavily on large quantities of training
    data, or at least that is what is expected. This paradigm came to existence due
    to the academic research cases, mainly focusing on very challenging cases such
    as pedestrian and car detection. These are both object classes where a huge amount
    of intra-class variance exists, resulting in:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 许多对象检测算法高度依赖于大量的训练数据，或者至少这是预期的。这种范式是由于学术研究案例的出现，主要关注非常具有挑战性的案例，如行人和汽车检测。这些都是存在大量类内差异的对象类别，导致：
- en: A very large positive and negative training sample set, leading up to thousands
    and even millions of samples for each set.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个非常大的正负训练样本集，每个集合可能包含数千甚至数百万个样本。
- en: The removal of all information that pollutes the training set, rather than helping
    it, such as color information, and simply using feature information that is more
    robust to all this intra-class variation such as edge information and pixel intensity
    differences.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除所有污染训练集而不是帮助它的信息，例如颜色信息，而仅仅使用对这类类内变异性更鲁棒的特征信息，如边缘信息和像素强度差异。
- en: As a result, models were trained that successfully detect pedestrians and cars
    in about every possible situation, with the downside that training them required
    several weeks of processing.. However, when you look at more industrial specific
    cases, such as the picking of fruit from bins or the grabbing of objects from
    a conveyor belt, you can see that the amount of variance in objects and background
    is rather limited compared to these very challenging academic research cases.
    And this is a fact that we can use to our own advantage.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练出的模型能够在几乎所有可能的情况下成功检测行人和汽车，但缺点是训练它们需要几周的处理时间。然而，当你观察更具有工业特定性的案例，例如从箱子中挑选水果或从传送带上抓取物体时，你会发现与这些极具挑战性的学术研究案例相比，物体和背景的变异性相当有限。这是一个我们可以利用的事实。
- en: We know that the accuracy of the resulting object model is highly dependent
    on the training data used. In cases where your detector needs to work in all possible
    situations, supplying huge amounts of data seems reasonable. The complex learning
    algorithms will then decide which information is useful and which is not. However,
    in more confined cases, we could build object models by considering what our object
    model actually needs to do.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，最终对象模型的准确性高度依赖于所使用的训练数据。在需要检测器在所有可能情况下工作的案例中，提供大量数据似乎是合理的。复杂的学习算法将决定哪些信息是有用的，哪些不是。然而，在更受限的案例中，我们可以通过考虑我们的对象模型实际上需要做什么来构建对象模型。
- en: For example, the Facebook DeepFace application, used for detecting faces in
    every possible situation using the neural networks approach uses 4.4 million labeled
    faces.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Facebook DeepFace应用程序，使用神经网络方法在所有可能的情况下检测人脸，使用了440万张标记人脸。
- en: Note
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'More information on the DeepFace algorithm can be found in:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 关于DeepFace算法的更多信息可以在以下找到：
- en: 'Deepface: Closing the gap to human-level performance in face verification,
    Taigman Y., Yang M., Ranzato M. A., and Wolf L. (2014, June). In Computer Vision
    and Pattern Recognition (CVPR), 2014, IEEE Conference on (pp. 1701-1708).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'Deepface: Closing the gap to human-level performance in face verification,
    Taigman Y., Yang M., Ranzato M. A., and Wolf L. (2014, June). In Computer Vision
    and Pattern Recognition (CVPR), 2014, IEEE Conference on (pp. 1701-1708).'
- en: 'We therefore suggest using only meaningful positive and negative training samples
    for your object model by following a set of simple rules:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们建议通过遵循一系列简单规则，只为你的对象模型使用有意义的正负样本训练样本：
- en: For the positive samples, only use **natural occurring samples**. There are
    many tools out there that create artificially rotated, translated, and skewed
    images to turn a small training set into a large training set. However, research
    has proven that the resulting detector is less performant than simply collecting
    positive object samples that cover the actual situation of your application. Better
    use a small set of decent **high quality object samples**, rather than using a
    large set of low quality non-representative samples for your case.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于正样本，仅使用**自然发生样本**。市面上有许多工具可以创建人工旋转、平移和倾斜的图像，将小型的训练集变成大型的训练集。然而，研究表明，这样得到的检测器性能不如简单地收集覆盖你应用实际情况的正样本。更好的做法是使用一组质量上乘的**高质量对象样本**，而不是使用大量低质量且不具有代表性的样本。
- en: For the negative samples, there are two possible approaches, but both start
    from the principle that you collect negative samples in the situation where your
    detector will be used, which is very different from the normal way of training
    object detects, where just a large set of random samples not containing the object
    are being used as negatives.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于负样本，有两种可能的方法，但两者都始于这样一个原则：你收集负样本的情况是你检测器将要使用的情况，这与通常训练对象检测的方法非常不同，后者只是使用一大组不包含对象的随机样本作为负样本。
- en: Either point a camera at your scene and start grabbing random frames to sample
    negative windows from.
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要么将摄像头对准你的场景，开始随机抓取帧以从负窗口中采样。
- en: Or use your positive images to your advantage. Cut out the actual object regions
    and make the pixels black. Use those masked images as negative training data.
    Keep in mind that in this case the ratio between background information and actual
    object occurring in the window needs to be large enough. If your images are filled
    with object instances, cutting them will result in a complete loss of relevant
    background information and thus reduce the discriminative power of your negative
    training set.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者利用您的正图像的优势。裁剪实际的物体区域并将像素变为黑色。使用这些掩码图像作为负训练数据。请记住，在这种情况下，背景信息和窗口中实际发生的物体的比例需要足够大。如果您的图像充满了物体实例，裁剪它们将导致相关背景信息的完全丢失，从而降低负训练集的区分力。
- en: Try to use a very small set of negative windows. If in your case only 4 or 5
    background situations can occur, then there is no need to use 100 negative images.
    Just take those five specific cases and sample negative windows from them.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽量使用一个非常小的负样本集合。如果你的情况下只有4或5种背景情况可能发生，那么就没有必要使用100个负图像。只需从这五个具体案例中采样负窗口。
- en: Efficiently collecting data in this way ensures that you will end up with a
    very robust model for your specific application! However, keep in mind that this
    also has some consequences. The resulting model will not be robust towards different
    situations than the ones trained for. However, the benefit in training time and
    the reduced need of training samples completely outweighs this downside.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式高效地收集数据确保您最终会得到一个针对您特定应用的非常健壮的模型！然而，请记住，这也带来了一些后果。生成的模型将不会对训练时的情况之外的情景具有鲁棒性。然而，在训练时间和减少训练样本需求方面的好处完全超过了这一缺点。
- en: Note
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: Software for negative sample generation based on OpenCV 3 can be found at [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/generate_negatives/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/generate_negatives/).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于OpenCV 3的负样本生成软件可以在[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/generate_negatives/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/generate_negatives/)找到。
- en: You can use the negative sample generation software to generate samples like
    you can see in the following figure, where object annotations of strawberries
    are removed and replaced by black pixels.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用负样本生成软件生成如图所示的样本，其中草莓的物体注释被移除并用黑色像素替换。
- en: '![The amount of training data](img/00075.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![训练数据量](img/00075.jpeg)'
- en: An example of the output of the negative image generation tool, where annotations
    are cut out and replaced by black pixels
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 负图像生成工具输出示例，其中注释被裁剪并用黑色像素替换
- en: As you can see, the ratio between the object pixels and the background pixels
    is still large enough in order to ensure that the model will not train his background
    purely based on those black pixel regions. Keep in mind that avoiding the approach
    of using these black pixelated images, by simply collecting negative images, is
    always better. However, many companies forget this important part of data collection
    and just end up without a negative data set meaningful for the application. Several
    tests I performed proved that using a negative dataset from random frames from
    your application have a more discriminative negative power than black pixels cutout
    based images.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，物体像素与背景像素之间的比例仍然足够大，以确保模型不会仅基于这些黑色像素区域训练其背景。请记住，通过简单地收集负图像来避免使用这些黑色像素化图像的方法，始终是更好的。然而，许多公司忘记了数据收集的这个重要部分，最终导致没有对应用有意义的负数据集。我进行的几个测试证明，使用来自应用随机帧的负数据集比基于黑色像素裁剪的图像具有更强的负区分力。
- en: Creating object annotation files for the positive samples
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为正样本创建物体注释文件
- en: When preparing your positive data samples, it is important to put some time
    in your annotations, which are the actual locations of your object instances inside
    the larger images. Without decent annotations, you will never be able to create
    decent object detectors. There are many tools out there for annotation, but I
    have made one for you based on OpenCV 3, which allows you to quickly loop over
    images and put annotations on top of them.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备您的正数据样本时，花些时间在注释上是很重要的，这些注释是您物体实例在较大图像中的实际位置。没有适当的注释，您将永远无法创建出优秀的物体检测器。市面上有许多注释工具，但我基于OpenCV
    3为您制作了一个，它允许您快速遍历图像并在其上添加注释。
- en: Note
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: Software for object annotation based on OpenCV 3 can be found at [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/object_annotation/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/object_annotation/).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基于OpenCV 3的对象标注软件可以在[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/object_annotation/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/object_annotation/)找到。
- en: 'The OpenCV team was kind enough to also integrate this tool into the main repository
    under the apps section. This means that if you build and install the OpenCV apps
    during installation, that the tool is also accessible by using the following command:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV团队非常友好，将此工具集成到主仓库的“apps”部分。这意味着，如果您在安装过程中构建并安装了OpenCV应用，则可以使用以下命令访问该工具：
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using the software is quite straightforward:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该软件相当简单：
- en: 'Start by running the CMAKE script inside the GitHub folder of the specific
    project. After running CMAKE, the software will be accessible through an executable.
    The same approach applies for every piece of software in this chapter. Running
    the CMAKE interface is quite straightforward:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在GitHub文件夹中的特定项目内运行CMAKE脚本。运行CMAKE后，软件将通过可执行文件提供访问。这种方法适用于本章中的每个软件组件。运行CMAKE界面相当简单：
- en: '[PRE1]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will result in an executable that needs some input parameters, being the
    location of the positive image files and the output detection file.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将生成一个需要一些输入参数的可执行文件，包括正图像文件的位置和输出检测文件。
- en: Note
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Keep in mind to always assign the absolute path of all files!
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，始终分配所有文件的绝对路径！
- en: 'First, parse the content of your positive image folder to a file (by using
    the supplied `folder_listing` software inside the object annotation folder), and
    then follow this by executing the annotation command:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，将您的正图像文件夹的内容解析到文件中（通过在对象标注文件夹内使用的提供的`folder_listing`软件），然后执行标注命令：
- en: '[PRE2]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The folder listing tool should generate a file, which looks exactly like this:![Creating
    object annotation files for the positive samples](img/00076.jpeg)
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件夹列表工具应该生成一个文件，其外观与以下所示完全相同：![为正样本创建对象标注文件](img/00076.jpeg)
- en: A sample positive samples file generated by the folder listing tool
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文件夹列表工具生成的正样本文件示例
- en: 'Now, fire up the annotation tool with the following command:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令启动标注工具：
- en: '[PRE3]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will fire up the software and give you the first image in a window, ready
    to apply annotations, as shown in the following figure:![Creating object annotation
    files for the positive samples](img/00077.jpeg)
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将启动软件，并在窗口中显示第一张图像，准备应用标注，如图所示：![为正样本创建对象标注文件](img/00077.jpeg)
- en: A sample of the object annotation tool
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对象标注工具的示例
- en: You can start by selecting the top-left corner of the object, then moving the
    mouse until you reach the bottom right corner of the object, which can be seen
    in the left part of the preceding figure. However, the software allows you to
    start your annotation from each possible corner. If you are unhappy with the selection,
    then reapply this step, until the annotation suits your needs.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以从选择对象的左上角开始，然后移动鼠标直到达到对象的右下角，这可以在前图的左侧部分看到。然而，该软件允许您从每个可能的角落开始标注。如果您对选择不满意，则重新应用此步骤，直到标注符合您的需求。
- en: Once you agree on the selected bounding box, press the button that confirms
    a selection, which is key *C* by default. This will confirm the annotation, change
    its color from red to green, and add it to the annotations file. Be sure only
    to accept an annotation if you are 100% sure of the selection.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您同意所选的边界框，请按确认选择的按钮，默认为键 *C*。这将确认标注，将其颜色从红色变为绿色，并将其添加到标注文件中。请确保只有当您100%确信选择时才接受标注。
- en: Repeat the preceding two steps for the same image until you have annotated every
    single object instance in the image, as seen in the right part of the preceding
    example image. Then press the button that saves the result and loads in the following
    image, which is the *N* key by default.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对同一图像重复前面的两个步骤，直到您已标注图像中的每个对象实例，如前例图像的右侧部分所示。然后按保存结果的按钮，默认为 *N* 键。
- en: Finally, you will end up with a file called `annotations.txt`, which combines
    the location of the image files together with the ground truth locations of all
    object instances that occur inside the training images.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您将得到一个名为`annotations.txt`的文件，它结合了图像文件的存储位置以及训练图像中出现的所有对象实例的地面真实位置。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you want to adapt the buttons that need to be pressed for all the separate
    actions, then open up the `object_annotation.cpp` file and browse to line 100
    and line 103\. There you can adapt the ASCII values assigned to the button you
    want to use for the operation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想调整所有单独操作所需的按钮，那么请打开`object_annotation.cpp`文件，浏览到第100行和第103行。在那里，您可以调整分配给要用于操作的按钮的ASCII值。
- en: An overview of all ASCII codes assigned to your keyboard keys can be found at
    [http://www.asciitable.com/](http://www.asciitable.com/).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[http://www.asciitable.com/](http://www.asciitable.com/)找到分配给键盘键的所有ASCII码的概述。
- en: 'The output from the software is a list of object detections in a `*.txt` file
    for each folder of positive image samples, which has a specific structure as seen
    in the following figure:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 软件输出的结果是在每个正样本文件夹中，一个`*.txt`文件的对象检测列表，其结构如下所示（以下图所示）：
- en: '![Creating object annotation files for the positive samples](img/00078.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![为正样本创建物体标注文件](img/00078.jpeg)'
- en: An example of an object annotation tool
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 物体标注工具的示例
- en: It starts with the absolute file location of each image in the folder. There
    was a choice of not using relative paths since the file will then be fully dependent
    on the location where it is stored. However, if you know what you are doing, then
    using relative file locations in relation to the executable should work just fine.
    Using the absolute path makes it more universal and more failsafe. The file location
    is followed by the number of detections for that specific image, which allows
    us to know beforehand how many ground truth objects we can expect. For each of
    the objects, the (x, y) coordinates are stored to the top-left corner combined
    with the width and the height of the bounding box. This is continued for each
    image, which is each time a new line appears in the detection output file.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 它从文件夹中每个图像的绝对文件位置开始。有选择不使用相对路径，因为这样文件将完全依赖于其存储的位置。然而，如果您知道自己在做什么，那么相对于可执行文件使用相对文件位置应该可以正常工作。使用绝对路径使其更通用且更安全。文件位置后面跟着该特定图像的检测数量，这使我们事先知道可以期待多少个地面真实对象。对于每个对象，存储到顶左角的(x,
    y)坐标与边界框的宽度和高度相结合。这为每个图像继续进行，每次检测输出文件中出现新行。
- en: Tip
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: It is important for further model training that each set of ground truth values
    captured from other annotation systems is first converted to this format in order
    to ensure the decent working of the cascade classifier software embedded in OpenCV
    3.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于进一步模型训练来说，将来自其他标注系统的每一组地面真实值首先转换为这种格式非常重要，以确保OpenCV 3中嵌入的级联分类软件能够良好工作。
- en: 'A second point of attention when processing positive training images containing
    object instances, is that you need to pay attention to the way you perform the
    actual placement of the bounding box of an object instance. A good and accurately
    annotated ground truth set will always give you a more reliable object model and
    will yield better test and accuracy results. Therefore, I suggest using the following
    points of attention when performing object annotation for your application:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理包含对象实例的正训练图像时，第二个需要注意的点是在实际放置对象实例边界框的方式上。一个良好且准确标注的地面真实集将始终为您提供更可靠的对象模型，并将产生更好的测试和准确度结果。因此，我建议在为您的应用程序进行物体标注时注意以下要点：
- en: Make sure that the bounding box contains the complete object, but at the same
    time avoid as much background information as possible. The ratio of object information
    compared to background information should always be larger than 80%. Otherwise,
    the background could yield enough features to train your model on and the end
    result will be your detector model focusing on the wrong image information.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保边界框包含整个对象，同时尽可能避免尽可能多的背景信息。对象信息与背景信息的比率应始终大于80%。否则，背景可能会提供足够多的特征来训练您的模型，最终结果将是您的检测器模型专注于错误图像信息。
- en: Viola and Jones suggests using squared annotations, based on a 24x24 pixel model,
    because it fits the shape of a face. However, this is not mandatory! If your object
    class is more rectangular like, then do annotate rectangular bounding boxes instead
    of squares. It is observed that people tend to push rectangular shaped objects
    in a square model size, and then wonder why it is not working correctly. Take,
    for example, the case of a pencil detector, where the model dimensions will be
    more like 10x70 pixels, which is in relation to the actual pencil dimensions.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Viola 和 Jones 建议使用基于24x24像素模型的平方标注，因为它适合人脸的形状。然而，这并不是强制性的！如果你的目标类别更像是矩形，那么你应该标注矩形边界框而不是正方形。观察到人们倾向于将矩形形状的对象推入平方模型尺寸，然后
    wonder 为什么它没有正确工作。以铅笔检测器为例，模型尺寸将更像是10x70像素，这与实际的铅笔尺寸相关。
- en: Try doing concise batches of images. It is better to restart the application
    10 times, than to have a system crash when you are about to finish a set of 1,000
    images with corresponding annotations. If somehow, the software or your computer
    fails it ensures that you only need to redo a small set.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试做简洁的图像批次。最好是重启应用程序10次，而不是在即将完成一组1000张带有相应标注的图像时系统崩溃。如果软件或你的计算机失败了，它确保你只需要重新做一小部分。
- en: Parsing your positive dataset into the OpenCV data vector
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将你的正样本数据集解析到OpenCV数据向量
- en: Before the OpenCV 3 software allows you to train a cascade classifier object
    model, you will need to push your data into an OpenCV specific data vector format.
    This can be done by using the provided sample creation tool of OpenCV.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV 3软件允许你训练级联分类器目标模型之前，你需要将你的数据推送到一个OpenCV特定的数据向量格式。这可以通过使用提供的OpenCV样本创建工具来完成。
- en: Note
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The sample creation tool can be found at [https://github.com/Itseez/opencv/tree/master/apps/createsamples/](https://github.com/Itseez/opencv/tree/master/apps/createsamples/)
    and should be built automatically if OpenCV was installed correctly, which makes
    it usable through the `opencv_createsamples` command.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 样本创建工具可以在[https://github.com/Itseez/opencv/tree/master/apps/createsamples/](https://github.com/Itseez/opencv/tree/master/apps/createsamples/)找到，并且如果OpenCV安装正确，它应该会自动构建，这使得可以通过`opencv_createsamples`命令使用。
- en: 'Creating the sample vector is quite easy and straightforward by applying the
    following instruction from the command line interface:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用以下命令行界面指令创建样本向量非常简单直接：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You have now prepared your positive training set. The last thing you should
    do is create a folder with the negative images, from which you will sample the
    negative windows randomly, and apply the folder listing functionality to it. This
    will result in a negative data referral file that will be used by the training
    interface.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经准备好了你的正样本训练集。你最后应该做的事情是创建一个包含负图像的文件夹，从这些图像中你将随机采样负窗口，并对其应用文件夹列出功能。这将生成一个负数据引用文件，该文件将由训练界面使用。
- en: Parameter selection when training an object model
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练目标模型时的参数选择
- en: Once you have built a decent training samples dataset, which is ready to process,
    the time has arrived to fire up the cascade classifier training software of OpenCV
    3, which uses the Viola and Jones cascade classifier framework to train your object
    detection model. The training itself is based on applying the boosting algorithm
    on either Haar wavelet features or Local Binary Pattern features. Several types
    of boosting are supported by the OpenCV interface, but for convenience, we use
    the frequently used AdaBoost interface.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你构建了一个不错的训练样本数据集，它已经准备好处理，那么是时候启动OpenCV 3的级联分类器训练软件了，它使用Viola和Jones级联分类器框架来训练你的目标检测模型。训练本身是基于应用提升算法在Haar小波特征或局部二值模式特征上。OpenCV界面支持多种提升类型，但为了方便，我们使用常用的AdaBoost界面。
- en: Note
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you are interested in knowing all the technical details of the feature calculation,
    then have a look at the following papers which describe them in detail:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解特征计算的详细技术细节，请查看以下详细描述它们的论文：
- en: '**HAAR**: Papageorgiou, Oren and Poggio, "A general framework for object detection",
    International Conference on Computer Vision, 1998.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HAAR**: Papageorgiou, Oren 和 Poggio, "一个用于目标检测的通用框架", 国际计算机视觉会议, 1998年。'
- en: '**LBP**: T. Ojala, M. Pietikäinen, and D. Harwood (1994), "Performance evaluation
    of texture measures with classification based on Kullback discrimination of distributions",
    Proceedings of the 12th IAPR International Conference on Pattern Recognition (ICPR
    1994), vol. 1, pp. 582 - 585.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LBP**: T. Ojala, M. Pietikäinen, and D. Harwood (1994), "基于Kullback分布判别的纹理度量性能评估"，第12届IAPR国际模式识别会议（ICPR
    1994）论文集，第1卷，第582 - 585页。'
- en: This section will discuss several parts of the training process in more detail.
    It will first elaborate on how OpenCV runs its cascade classification process.
    Then, we will take a deeper look at all the training parameters provided and how
    they can influence the training process and accuracy of the resulting model. Finally,
    we will open up the model file and look in more detail at what we can find there.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将更详细地讨论训练过程的几个部分。它将首先阐述OpenCV如何运行其级联分类过程。然后，我们将深入探讨所有提供的训练参数以及它们如何影响训练过程和结果的准确性。最后，我们将打开模型文件，更详细地查看其中可以找到的内容。
- en: Training parameters involved in training an object model
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练对象模型时涉及的训练参数
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The parameters discussed earlier are the most important ones to dig into when
    trying to train a successful classifier. Once this works, you can increase the
    performance of your classifier even more, by looking at the way boosting forms
    its weak classifiers. This can be adapted by the `-maxDepth` and `-maxWeakCount`
    parameters. However, for most cases, using **stump weak classifiers** (single
    layer decision trees) on single features is the best way to start, ensuring that
    single stage evaluation is not too complex and thus fast at detection time.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试训练一个成功的分类器时，前面讨论的参数是最重要的几个需要深入挖掘的。一旦这个方法有效，你可以通过查看提升方法形成其弱分类器的方式，进一步提高你分类器的性能。这可以通过`-maxDepth`和`-maxWeakCount`参数来实现。然而，对于大多数情况，使用**树桩弱分类器**（单层决策树）对单个特征进行操作是开始的最佳方式，确保单阶段评估不会过于复杂，因此在检测时间上更快。
- en: The cascade classification process in detail
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 级联分类过程的详细说明
- en: Once you select the correct training parameters, you can start the cascade classifier
    training process, which will build your cascade classifier object detection model.
    In order to fully understand the cascade classification process that builds up
    your object model, it is important to know how OpenCV does its training of the
    object model, based on the boosting process.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你选择了正确的训练参数，你就可以开始级联分类器的训练过程，这将构建你的级联分类器对象检测模型。为了完全理解构建你的对象模型所涉及的级联分类过程，了解OpenCV如何基于提升过程进行对象模型的训练是非常重要的。
- en: Before we do this, we will have a quick look at the outline of the boosting
    principle in general.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这样做之前，我们将快速浏览一下提升原理的一般概述。
- en: Note
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: More information on the boosting principle can be found in Freund Y., Schapire
    R., and Abe N (1999). A short introduction to boosting. Journal-Japanese Society
    For Artificial Intelligence, 14(771-780), 1612
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 关于提升原理的更多信息可以在Freund Y., Schapire R., and Abe N (1999)的《提升简明介绍》中找到。Journal-Japanese
    Society For Artificial Intelligence, 14(771-780), 1612
- en: The idea behind boosting is that you have a very large pool of features that
    can be shaped into classifiers. Using all those features for a single classifier
    would mean that every single window in your test image will need to be processed
    for all these features, which will take a very long time and make your detection
    slow, especially if you consider how many negative windows are available in a
    test image. To avoid this, and to reject as many negative windows as fast as possible,
    boosting selects the features that are best at separating the positive and negative
    data and combines them into classifiers, until the classifier does a bit better
    than random guessing on the negative samples. This first step is called a weak
    classifier. Boosting repeats this process until the combination of all these weak
    classifiers reach the desired accuracy of the algorithm. The combination is called
    the strong classifier. The main advantage of this process is that tons of negative
    samples will already be discarded by the few early stages, with only evaluating
    a small set of features, thus decreasing detection time a lot.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 提升背后的思想是，你有一个非常大的特征池，可以将其塑造成分类器。使用所有这些特征来构建单个分类器意味着你的测试图像中的每一个窗口都需要处理所有这些特征，这将花费非常长的时间，并使检测变慢，尤其是当你考虑到测试图像中可用的负窗口数量时。为了避免这种情况，并尽可能快地拒绝尽可能多的负窗口，提升选择那些最能区分正负数据的特征，并将它们组合成分类器，直到分类器在负样本上的表现略好于随机猜测。这一步被称为弱分类器。提升重复此过程，直到所有这些弱分类器的组合达到算法所需的确切精度。这种组合被称为强分类器。这个过程的主要优势是，大量的负样本将在少数早期阶段被丢弃，只需评估一小组特征，从而大大减少检测时间。
- en: We will now try to explain the complete process using the output generated by
    the cascade training software embedded in OpenCV 3\. The following figure illustrates
    how a strong cascade classifier is built from a set of stages of weak classifiers.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将尝试使用OpenCV 3中嵌入的级联训练软件生成的输出，来解释整个过程。以下图示说明了如何从一组弱分类器阶段构建一个强大的级联分类器。
- en: '![The cascade classification process in detail](img/00080.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![级联分类过程细节](img/00080.jpeg)'
- en: A combination of weak classifier stages and early rejection of misclassified
    windows resulting in the famous cascade structure
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 结合弱分类器阶段和早期拒绝错误分类窗口，形成了著名的级联结构
- en: 'The cascade classifier training process follows an iterative process to train
    subsequent stages of weak classifiers (1…N). Each stage consists of a set of weak
    classifiers, until the criteria for that specific stage have been reached. The
    following steps are an overview of what is happening at training each stage in
    OpenCV 3, according to the input parameters given and the training data provided.
    If you are interested in more specific details of each subsequent step, then do
    read the research paper of Viola and Jones (you can have a look at the citation
    on the first page of this chapter) on cascade classifiers. All steps described
    here are subsequently repeated for each stage until the desired accuracy for the
    strong classifier is reached. The following figure shows how such a stage output
    looks like:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 级联分类器训练过程遵循迭代过程来训练后续阶段的弱分类器（1…N）。每个阶段由一组弱分类器组成，直到达到该特定阶段的特定标准。以下步骤概述了在OpenCV
    3中根据输入参数和提供的数据在训练每个阶段时发生的情况。如果你对每个后续步骤的更具体细节感兴趣，那么请阅读Viola和Jones的研究论文（你可以在本章的第一页查看引用）。这里描述的所有步骤都会在达到强分类器所需的确切精度之前，对每个阶段重复进行。以下图示显示了这样一个阶段输出的样子：
- en: '![The cascade classification process in detail](img/00081.jpeg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![级联分类过程细节](img/00081.jpeg)'
- en: An example output of a classifier stage training
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器阶段训练的一个示例输出
- en: Step 1 – grabbing positive and negative samples
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 1 – 抓取正面和负样本
- en: 'You will notice that the first thing the training does is grabbing training
    samples for the current stage—first the positive samples from the data vector
    you supplied, and then the random negative window samples from the negative images
    that you supplied. This will be outputted for both steps as:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到训练的第一步是抓取当前阶段的训练样本——首先是从你提供的数据向量中获取的正面样本，然后是从你提供的负图像中随机获取的负样本窗口。这两个步骤的输出如下：
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If no positive samples can be found anymore, an error will be generated and
    training will be stopped. The total number of samples needed will increase once
    you start discarding positives that are no longer useful. The grabbing of the
    negatives for the current stage can take much longer than the positive sample
    grabbing since all windows that are correctly classified by the previous stages
    are discarded and new ones are searched. The deeper you go into the amount of
    stages, the harder this gets. As long as the number of samples grabbed keeps increasing
    (and yes, this can be very slow, so be patient), your application is still running.
    If no more negatives are found, the application will end training and you will
    need to lower the amount of negatives for each stage or add extra negative images.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果找不到更多的正样本，将生成错误并停止训练。当你开始丢弃不再有用的正样本时，所需的样本总数会增加。当前阶段的负样本抓取可能比正样本抓取花费更长的时间，因为所有被前阶段正确分类的窗口都被丢弃，并搜索新的窗口。随着阶段数量的增加，这会变得更加困难。只要抓取的样本数量持续增加（是的，这可能会非常慢，所以请耐心等待），你的应用程序仍在运行。如果找不到更多的负样本，应用程序将结束训练，你需要降低每个阶段的负样本数量或添加额外的负图像。
- en: The acceptance ratio that is achieved by the previous stage is reported after
    the grabbing of the negative windows. This value indicates whether the model trained
    until now is strong enough for your detection purposes or not!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在抓取负窗口后，报告了前一个阶段实现的接受率。这个值表明，到目前为止训练的模型是否足够强大，可以用于你的检测目的！
- en: Step 2 – precalculation of integral image and all possible features from the
    training data
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第2步 - 训练数据的积分图像和所有可能特征的预计算
- en: Once we have both positive and negative window-sized samples, the precalculation
    will calculate every single feature that is possible within the window size and
    apply it for each training sample. This can take some time according to the size
    of your model and according to the amount of training samples, especially when
    knowing that a model of 24x24 pixels can yield more than 16,000 features. As suggested
    earlier, assigning more memory can help out here or you could decide on selecting
    LBP features, of which the calculation is rather fast compared to HAAR features.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了正负样本窗口大小的样本，预计算将计算窗口大小内所有可能的单个特征，并将其应用于每个训练样本。这可能会花费一些时间，具体取决于你的模型大小和训练样本的数量，特别是当你知道一个24x24像素的模型可以产生超过16,000个特征时。如前所述，分配更多的内存可以有所帮助，或者你可以选择选择LBP特征，其计算速度相对于HAAR特征要快得多。
- en: All features are calculated on the integral image representation of the original
    input window. This is done in order to speed up the calculation of the features.
    The paper by Viola and Jones explains in detail why this integral image representation
    is used.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所有特征都是在原始输入窗口的积分图像表示上计算的。这样做是为了加快特征的计算速度。Viola和Jones的论文详细解释了为什么使用这种积分图像表示。
- en: The features calculated are dumped into a large feature pool from which the
    boosting process can select the features needed to train the weak classifiers
    that will be used within each stage.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出的特征被倒入一个大的特征池中，提升过程可以从这个池中选择训练弱分类器所需的特征。这些弱分类器将在每个阶段中使用。
- en: Step 3 – firing up the boosting process
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第3步 - 启动提升过程
- en: 'Now, the cascade classifier training is ready for the actual boosting process.
    This happens in several small steps:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，级联分类器训练已准备好进行实际的提升过程。这发生在几个小步骤中：
- en: Every possible weak classifier inside the feature pool is being calculated.
    Since we use stumps, which are basically weak classifiers based on single feature
    to create a decision tree, there are as many weak classifiers as features. If
    you prefer, you can decide to train actual decision trees with a predefined maximum
    depth, but this goes outside of the scope of this chapter.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征池中所有的可能弱分类器都在被计算。由于我们使用的是基于单个特征的stumps（基本弱分类器）来构建决策树，因此弱分类器的数量与特征的数量相同。如果你愿意，你可以选择使用预定义的最大深度来训练实际的决策树，但这超出了本章的范围。
- en: Each weak classifier is trained in order to minimize the misclassification rate
    on the training samples. For example, when using Real AdaBoost as a boosting technique,
    the Gini index is minimized.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个弱分类器都会被训练，以最小化训练样本上的误分类率。例如，当使用Real AdaBoost作为提升技术时，Gini指数会被最小化。
- en: Note
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'More information on the Gini index, used for the misclassification rate on
    the training samples can be found in:'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于用于训练样本误分类率的Gini指数的更多信息，可以在以下内容中找到：
- en: Gastwirth, J. L. (1972). The estimation of the Lorenz curve and Gini index.
    The Review of Economics and Statistics, 306-316.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Gastwirth, J. L. (1972). The estimation of the Lorenz curve and Gini index.
    The Review of Economics and Statistics, 306-316.
- en: The weak classifier with the lowest misclassification rate is added as the next
    weak classifier to the current stage.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有最低误分类率的弱分类器被添加到当前阶段的下一个弱分类器。
- en: Based on the weak classifiers that are already added to the stage, the algorithm
    calculates the overall stage threshold, which is set so that the desired hit rate
    is guaranteed.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于已经添加到阶段的弱分类器，算法计算整体阶段阈值，该阈值被设置为保证所需的命中率。
- en: Now, the weights of the samples are adapted based on their classification in
    the last iteration, which will yield a new set of weak classifiers in the next
    iteration, and thus the whole process can start again.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，样本的权重根据它们在上一次迭代中的分类进行调整，这将产生下一轮迭代中的一组新的弱分类器，因此整个过程可以再次开始。
- en: 'During the combination of the weak classifiers inside a single stage, which
    is visualized in the training output, the boosting process makes sure that:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单个阶段内组合弱分类器（这在训练输出中可视化）时，提升过程确保：
- en: The overall stage threshold does not drop below the minimum hit rate that was
    selected by the training parameters.
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整体阶段阈值不会低于由训练参数选择的最低命中率。
- en: The false alarm rate on the negative samples decreases compared to the previous
    stage.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与前一个阶段相比，负样本上的误报率降低。
- en: 'This process continues until:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此过程持续进行，直到：
- en: The false acceptance ratio on the negative samples is lower than the maximum
    false alarm rate set. The process then simply starts training a new stage of weak
    classifiers for the detection model.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在负样本上的误接受率低于设定的最大误报率。然后，过程简单地开始为检测模型训练新的阶段弱分类器。
- en: The required stage false alarm rate is reached, which is `maxFalseAlarmRate^#stages`.
    This will yield an end to the model training since the model satisfies our requirements
    and better results cannot be achieved anymore. This will not happen often, since
    this value drops rather quickly, and after several stages, this would mean that
    you correctly classify more than 99% of your positive and negative samples all
    together.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 达到了所需的阶段误报率，即`maxFalseAlarmRate^#stages`。这将导致模型训练结束，因为模型满足我们的要求，并且无法再获得更好的结果。这种情况不会经常发生，因为这个值下降得相当快，经过几个阶段后，这意味着你正确分类了超过99%的正负样本。
- en: The hit rate drops below the stage specific minimal hit rate, which is the `minHitRate^#stages`.
    At this stage, too many positives get wrongly classified and the maximum performance
    for your model is reached.
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命中率下降到阶段特定的最小命中率，即`minHitRate^#stages`。在这个阶段，太多的正样本被错误分类，并且你的模型的最大性能已经达到。
- en: Step 4 – saving the temporary result to a stage file
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第4步 – 将临时结果保存到阶段文件
- en: After training each stage, the stage specific details about the weak classifiers
    and the thresholds are stored in the data folder, in a separate XML file. If the
    desired number of stages has been reached, then these subfiles are combined into
    a single cascade XML file.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练每个阶段后，关于弱分类器和阈值的特定阶段细节被存储在数据文件夹中，在一个单独的XML文件中。如果达到了所需的阶段数，则将这些子文件合并成一个单一的级联XML文件。
- en: However, the fact that every stage is stored separately means that you can stop
    the training at any time and create an in-between object detection model, by simply
    restarting the training command, but changing the `-numStages` parameter to the
    stage value on which you want to check the model's performance. This is ideal
    when you want to perform an evaluation on a validation set to ensure that your
    model does not start overfitting on the training data!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每个阶段都单独存储的事实意味着你可以在任何时候停止训练，并通过简单地重新启动训练命令来创建一个中间的对象检测模型，只需将`-numStages`参数更改为你想要检查模型性能的阶段值。当你想要在一个验证集上执行评估以确保你的模型不会开始过度拟合训练数据时，这是理想的！
- en: The resulting object model explained in detail
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果对象模型被详细解释
- en: It has been observed that many users of the cascade classifier algorithm embedded
    in OpenCV 3 do not know the meaning of the inner construction of the object model
    which is stored in the XML files, which sometimes leads to wrong perceptions of
    the algorithm. This subsection will explain each internal part of the trained
    object models. We will discuss a model based on stump-typed weak classifiers,
    but the idea is practically the same for any other type of weak classifiers inside
    a stage, such as decision trees. The biggest difference is that the weight calculation
    inside the model gets more complex as compared to when using stump features. As
    to the weak classifiers structure inside each stage, this will be discussed for
    both HAAR- and LBP-based features since these are the two most used features inside
    OpenCV for training cascade classifiers.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到许多使用OpenCV 3中嵌入的级联分类器算法的用户不知道存储在XML文件中的对象模型内部结构的含义，这有时会导致对算法的错误理解。本节将解释训练对象模型的每个内部部分。我们将讨论基于树桩型弱分类器的模型，但这个想法对于任何其他类型的弱分类器在阶段内部都是相同的，例如决策树。最大的不同是，与使用树桩特征相比，模型内部的权重计算变得更加复杂。至于每个阶段内部的弱分类器结构，我们将讨论基于HAAR和LBP特征的情况，因为这两个是OpenCV中用于训练级联分类器的最常用的特征。
- en: Note
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The two models that will be used for explaining everything can be found at
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 将用于解释所有内容的两个模型可以在以下位置找到
- en: '[OpenCVsource/data/haarcascades/haarcascade_frontalface_default.xml](http://OpenCVsource/data/haarcascades/haarcascade_frontalface_default.xml)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenCV源代码/data/haarcascades/haarcascade_frontalface_default.xml](http://OpenCVsource/data/haarcascades/haarcascade_frontalface_default.xml)'
- en: '[OpenCVsource/data/lbpcascades/lbpcascade_frontalface.xml](http://OpenCVsource/data/lbpcascades/lbpcascade_frontalface.xml)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenCV源代码/data/lbpcascades/lbpcascade_frontalface.xml](http://OpenCVsource/data/lbpcascades/lbpcascade_frontalface.xml)'
- en: The first part of each XML stored model describes the parameters that specify
    the characteristics of the model itself and some of the important training parameters.
    Subsequently, we can find the type of training that is used, which is limited
    to boosting for now, and the type of features used for building the weak classifiers.
    We also have the width and height of the object model that will be trained, the
    parameters of the boosting process, which include the type of boosting used, the
    selected minimum hit ratio, and the selected maximum false acceptance rate. It
    also contains information about how the weak classifier stages are built, in our
    case as a combination of one feature deep trees, called stumps, with a maximum
    of 100 weak classifiers on a single stage. For the HAAR wavelet based model, we
    can then see which features are used, being only the basic upright features or
    the combined rotated 45-degree set.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 存储的每个XML模型的第一个部分描述了指定模型自身特征和一些重要训练参数的参数。随后，我们可以找到所使用的训练类型，目前仅限于提升，以及用于构建弱分类器的特征类型。我们还有将要训练的对象模型的宽度和高度，提升过程的参数，包括使用的提升类型、选定的最小命中比率和选定的最大误接受率。它还包含有关如何构建弱分类器阶段的信息，在我们的案例中，作为称为树桩的单个特征深度树的组合，每个阶段最多有100个弱分类器。对于基于HAAR小波模型，我们可以看到使用了哪些特征，仅限于基本的垂直特征或组合旋转45度的集合。
- en: 'After the training-specific parameters, it starts to get interesting. Here,
    we find more information about the actual structure of the cascade classifier
    object model. The amount of stages is described, and then iteratively the model
    sums up the training results and thresholds for each separate stage which were
    generated by the boosting process. The basic structure of an object model can
    be seen here:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练特定参数之后，事情开始变得有趣。在这里，我们可以找到更多关于级联分类器对象模型实际结构的信息。描述了阶段的数量，然后通过迭代，模型总结了由提升过程生成的每个单独阶段的训练结果和阈值。对象模型的基本结构如下所示：
- en: '[PRE9]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We start with an empty iteration tag for each stage. At each stage the number
    of weak classifiers that were used are defined, which in our case shows how many
    single layer decision trees (stumps) were used inside the stage. The stage threshold
    defines the threshold on the final stage score for a window. This is generated
    by scoring the window with each weak classifier and then summing and weighing
    the results for the complete stage. For each single weak classifier, we collect
    the internal structure, based on the decision nodes and layers used. The values
    present are the boosting values used for creating the decision tree and the leaf
    values, which are used to score a window that is evaluated by the weak classifier.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个阶段开始时使用一个空的迭代标签。在每一个阶段，定义了所使用的弱分类器的数量，在我们的情况下，这显示了在阶段内部使用了多少个单层决策树（树桩）。阶段阈值定义了窗口最终阶段得分的阈值。这是通过使用每个弱分类器对窗口进行评分，然后对整个阶段的评分结果进行求和和加权生成的。对于每个单个弱分类器，我们收集基于决策节点和所使用的层的内部结构。现有的值是用于创建决策树和叶值的提升值，这些叶值用于对由弱分类器评估的窗口进行评分。
- en: The specifics for the internal node structure are different for HAAR wavelets
    and -based features. The storage of the leaf scores is equal. The values of the
    internal nodes, however, specify the relation to the bottom part of the code,
    which contains the actual features area, and which are also different for both
    the HAAR and the LBP approach. The difference between both techniques can be seen
    in the following sections, grabbing for both models the first tree of the first
    stage and a part of the feature set.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 内部节点结构的具体细节对于HAAR小波和基于特征的模型是不同的。叶评分的存储是相同的。然而，内部节点的值指定了与代码底部部分的关系，该部分包含实际的特征区域，并且对于HAAR和LBP方法都是不同的。这两种技术之间的差异可以在以下部分看到，为两种模型抓取第一阶段的第一个树和特征集的一部分。
- en: HAAR-like wavelet feature models
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HAAR-like wavelet feature models
- en: 'The following are two code snippets from the HAAR wavelet feature-based model,
    containing the internal node structure and the features structure:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从基于HAAR小波特征的模型中提取的两个代码片段，包含内部节点结构和特征结构：
- en: '[PRE10]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'For the internal nodes, there are four values present at each node:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内部节点，每个节点有四个值：
- en: '**Node left and node right**: These values indicate that we have a stump with
    two leafs.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点左和节点右**：这些值表示我们有一个有两个叶子的树桩。'
- en: '**The node feature index**: This points the index of the feature used at this
    node according to its position inside the features list of that model.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点特征索引**：这指向该节点在模型特征列表中的位置所使用的特征索引。'
- en: '**The node threshold**: This is the threshold that is set on the feature value
    for this weak classifier, which is learned from all the positive and negative
    samples in this stage of training. Since we are looking at models with stump based
    weak classifiers, this is also the stage threshold, which is set in the boosting
    process.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点阈值**：这是设置在该弱分类器特征值上的阈值，该阈值是从训练阶段的全部正负样本中学习的。由于我们正在查看基于树桩的弱分类器的模型，这也是阶段阈值，它在提升过程中设置。'
- en: 'The features inside the HAAR-based model are described by a set of rectangles,
    which can be up to three rectangles, so as to calculate every possible feature
    from a window. Then, there is a value indicating if the feature itself is tilted
    over 45 degrees or not. For each rectangle, which is a partial feature value,
    we have:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 基于HAAR的模型中的特征由一组矩形描述，这些矩形最多可以是三个，以便从窗口中计算每个可能的特征。然后，有一个值表示特征本身是否倾斜超过45度。对于每个矩形，即部分特征值，我们有：
- en: The location of the rectangle, which is defined by upper-left corner x and y
    coordinates and the width and height of the rectangle.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩形的定位，由矩形的左上角x和y坐标以及矩形的宽度和高度定义。
- en: 'The weight for that specific partial feature. These weights are used to combine
    both partial feature rectangles into a predefined feature. These weights allow
    us to represent each feature with less rectangles than is actually necessary.
    An example of this can be seen in the following figure:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该特定部分特征的权重。这些权重用于将两个部分特征矩形组合成一个预定义的特征。这些权重使我们能够用比实际必要的更少的矩形来表示每个特征。以下图示展示了这一例子：
- en: '![HAAR-like wavelet feature models](img/00082.jpeg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![HAAR-like wavelet feature models](img/00082.jpeg)'
- en: A three rectangle feature can be represented by a two rectangle weighted combination
    reducing the need of an extra area calculation
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一个三矩形特征可以通过两个矩形加权组合来表示，从而减少了额外面积计算的需求。
- en: The feature sum is finally calculated by first summing all values of the pixels
    inside the rectangle and then multiplying it with the weight factor. Finally,
    those weighted sums are combined together to yield as a final feature value. Keep
    in mind that all the coordinates retrieved for a single feature are in relation
    to the window/model size and not the complete image which is processed.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 特征和最终是通过首先将矩形内所有像素的值相加，然后乘以权重因子来计算的。最后，将这些加权和组合在一起，得到最终的特征值。请记住，为单个特征检索到的所有坐标都与窗口/模型大小相关，而不是整个处理过的图像。
- en: Local binary pattern models
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局部二进制模式模型
- en: 'The following are two code snippets from the LBP feature-based model, containing
    the internal node structure and the features structure:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从基于LBP特征模型的两个代码片段，包含内部节点结构和特征结构：
- en: '[PRE11]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The software takes in several input arguments, such as the model location,
    the image where the visualization needs to happen, and the output folder where
    the results need to be stored. However, in order to use the software correctly,
    there are some points of attention:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 该软件接受多个输入参数，例如模型位置、需要可视化的图像以及需要存储结果的输出文件夹。然而，为了正确使用该软件，有一些需要注意的点：
- en: The model needs to be HAAR wavelet or LBP feature based. Deleted because this
    functionality is no longer supported in OpenCV 3.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型需要基于HAAR小波或LBP特征。已删除，因为此功能不再支持OpenCV 3。
- en: You need to supply an image that is an actual model detection for visualization
    purposes and resize it to the model scale or a positive training sample from the
    training data. This is to ensure that a feature of your model is placed at the
    correct location.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要提供一个用于可视化的实际模型检测图像，并将其调整到模型尺度或训练数据中的正训练样本。这是为了确保您的模型特征放置在正确的位置。
- en: Inside the code, you can adapt the visualization scales, one being for the video
    output of your model and one for the images that represent the stages.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在代码中，您可以调整可视化尺度，一个用于您模型的视频输出，另一个用于表示阶段的图像。
- en: The following two figures illustrate the visualization result of the Haar wavelet
    and LBP feature based frontal face model respectively, both incorporated into
    the OpenCV 3 repository under the data folder. The reason for the low image resolution
    of the visualization is quite obvious. The training process happens on a model
    scale; therefore, I wanted to start from an image of that size to illustrate that
    specific details of an object get removed, while general specifics of the object
    class still occur to be able to differentiate classes.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两个图分别展示了Haar小波和LBP特征基于的前脸模型的可视化结果，两者都包含在OpenCV 3仓库下的数据文件夹中。可视化图像分辨率低的原因非常明显。训练过程是在模型尺度上进行的；因此，我想从一个相同大小的图像开始，以说明物体的具体细节被移除，而物体类别的普遍特性仍然存在，以便能够区分类别。
- en: '![Visualization tool for object models](img/00084.jpeg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![物体模型的可视化工具](img/00084.jpeg)'
- en: A set of frames from the video visualization of the frontal face model for both
    Haar wavelet and Local Binary Pattern features
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Haar小波和局部二进制模式特征的前脸模型视频可视化的一组帧
- en: The visualizations for example also clearly show that an LBP model needs less
    features and thus less weak classifiers to separate the training data successfully,
    which yields a faster model at detection time.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，可视化也清楚地表明，LBP模型需要更少的特征和因此更少的弱分类器来成功分离训练数据，这使得检测时间更快。
- en: '![Visualization tool for object models](img/00085.jpeg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![物体模型的可视化工具](img/00085.jpeg)'
- en: A visualization of the first stage of the frontal face model for both Haar wavelet
    and Local Binary Pattern features
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Haar小波和局部二进制模式特征的前脸模型第一阶段的可视化
- en: Using cross-validation to achieve the best model possible
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用交叉验证以实现最佳模型
- en: Making sure that you get the absolute best model given your training, testing
    the data can be done by applying a cross validation approach, such as the leave-one-out
    approach. The idea behind this is that you combine both training and test set
    and vary the test set that you use from the larger set. With each random test
    set and training set, you build a separate model and you perform the evaluation
    using precision-recall, which is discussed further in this chapter. Finally, the
    model that provides the best result could be adopted as a final solution. Thus,
    it could mitigate the impact of an error due to a new instance that is not represented
    in the training set.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在您的训练数据中获取最佳模型，可以通过应用交叉验证方法，如留一法来完成测试数据。其背后的思想是将训练集和测试集结合起来，并从更大的集中改变所使用的测试集。对于每个随机测试集和训练集，您将构建一个单独的模型，并使用本章进一步讨论的精确度-召回率进行评估。最后，提供最佳结果的模型可以采用作为最终解决方案。因此，它可以减轻由于训练集中未表示的新实例而导致的错误的影响。
- en: Note
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: More information on the topic of cross validation can be found in Kohavi R.
    (1995, August), a study of cross-validation and bootstrap for accuracy estimation
    and model selection in Ijcai (Vol. 14, No. 2, pp. 1137-1145).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 关于交叉验证主题的更多信息，可以在 Kohavi R. (1995, 八月) 的研究中找到，该研究探讨了在 Ijcai (第 14 卷，第 2 期，第
    1137-1145 页) 中使用交叉验证和自助法进行准确度估计和模型选择。
- en: Using scene specific knowledge and constraints to optimize the detection result
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用场景特定知识和约束来优化检测结果
- en: Once your cascade classifier object model is trained, you can use it to detect
    instances of the same object class in new input images, which are supplied to
    the system. However, once you apply your object model, you will notice that there
    are still false positive detections and objects that are not found. This section
    will cover techniques to improve your detection results, by removing, for example,
    most of the false positive detections with scene-specific knowledge.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的级联分类器对象模型训练完成，您就可以使用它来检测新输入图像中相同对象类的实例，这些图像被提供给系统。然而，一旦应用了您的对象模型，您会发现仍然存在误报检测和未检测到的对象。本节将介绍一些技术，通过例如使用场景特定知识来移除大多数误报检测，以改善您的检测结果。
- en: Using the parameters of the detection command to influence your detection result
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用检测命令的参数来影响您的检测结果
- en: If you apply an object model to a given input image, you must consider several
    things. Let's first take a look at the detection function and some of the parameters
    that can be used to filter out your detection output. OpenCV 3 supplies three
    possible interfaces. We will discuss the benefits of using each one of them.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将对象模型应用于给定的输入图像，必须考虑几个因素。让我们首先看看检测函数以及可以用来过滤检测输出的某些参数。OpenCV 3 提供了三个可能的接口。我们将讨论使用每个接口的优点。
- en: '**Interface 1:**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**接口 1：**'
- en: '[PRE13]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first interface is the most basic one. It allows you to fast evaluate your
    trained model on a given test image. There are several elements on this basic
    interface that will allow you to manipulate the detection output. We will discuss
    these parameters in some more detail and highlight some points of attention when
    selecting the correct value.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个接口是最基本的。它允许您快速评估在给定测试图像上的训练模型。在这个基本界面上有几个元素，可以让您操作检测输出。我们将更详细地讨论这些参数，并强调在选择正确值时需要注意的一些要点。
- en: scaleFactor is the scale step used to downscale the original image in order
    to create the image pyramid, which allows us to perform multiscale detections
    using only a single scale model. One downside is that this doesn't allow you to
    detect objects that are smaller than the object size. Using a value of 1.1 means
    that in each step the dimensions are reduced by 10% compared to the previous step.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: scaleFactor 是用于将原始图像降级以创建图像金字塔的尺度步长，这使我们能够仅使用单个尺度模型执行多尺度检测。一个缺点是这不允许检测比对象尺寸更小的对象。使用
    1.1 的值意味着在每一步中，尺寸相对于前一步减少了 10%。
- en: Increasing this value will make your detector run faster since it has less scale
    levels to evaluate, but it will yield the risk of losing detections that are in
    between scale steps.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加此值将使您的检测器运行更快，因为它需要评估的尺度级别更少，但会带来丢失位于尺度步骤之间的检测的风险。
- en: Decreasing the value will make your detector run slower, since more scale levels
    need to be evaluated, but it will increase the chance of detecting objects that
    were missed before. Also, it will yield more detections on an actual object, resulting
    in a higher certainty.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少值会使您的探测器运行得更慢，因为需要评估更多的尺度级别，但会增加检测之前遗漏的对象的机会。此外，它会在实际对象上产生更多的检测，从而提高确定性。
- en: Keep in mind that adding scale levels also gives rise to more false positive
    detections, since those are bound to each layer of the image pyramid.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请记住，添加尺度级别也会导致更多的假阳性检测，因为这些与图像金字塔的每一层都有关。
- en: A second interesting parameter to adapt for your needs is the `minNeighbors`
    parameter. It describes how many overlapping detections occur due to the sliding
    window approach. Each detection overlapping by more than 50% with another will
    be merged together as a sort of nonmaxima suppression.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的参数是`minNeighbors`参数。它描述了由于滑动窗口方法而发生的重叠检测的数量。任何与其他检测重叠超过50%的检测将被合并为一个非极大值抑制。
- en: Putting this value on 0 means that you will get all detections generated by
    the windows that get through the complete cascade. However, due to the sliding
    window approach (with steps of 8 pixels) many detections will happen for a single
    window, due to the nature of cascade classifiers, which train in some variance
    on object parameters in order to better generalize over an object class.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将此值设为0意味着您将获得通过完整级联的所有检测生成的检测。然而，由于滑动窗口方法（以8像素的步长）以及级联分类器的性质（它们在对象参数上训练以更好地泛化对象类别），对于单个窗口，许多检测将发生。
- en: Adding a value means that you want to count how many windows there should be,
    at least those combined by the nonmaxima suppression in order to keep the detection.
    This is interesting since an actual object should yield far more detections than
    a false positive. So, increasing this value will reduce the number of false positive
    detections (which have a low amount of overlapping detections) and keep the true
    detections (which have a large amount of overlapping detections).
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个值意味着您想要计算应该有多少个窗口，至少那些通过非极大值抑制组合在一起的窗口，以保持检测。这很有趣，因为实际对象应该产生比假阳性更多的检测。因此，增加这个值将减少假阳性检测的数量（它们重叠检测的数量很少）并保持真实检测（它们有大量的重叠检测）。
- en: A downside is that on a certain point, actual objects with a lower certainty
    of detections and thus less overlapping windows will disappear while some false
    positive detections might still stand.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个缺点是，在某个点上，实际对象由于检测确定性较低和重叠窗口较少而消失，而一些假阳性检测可能仍然存在。
- en: Use the `minSize` and `maxSize` parameters to effectively reduce the scale space
    pyramid. In an industrial setup with, for example, a fixed camera position, such
    as a conveyor belt setup, you can in most cases guarantee that objects will have
    certain dimensions. Adding scale values in this case and thus defining a scale
    range will decrease processing time for a single image a lot, by removing undesired
    scale levels. As an extra advantage, all false positive detections on those undesired
    scales will also disappear. If you leave these values blank, the algorithm will
    start building the image pyramid at input image dimensions, in a bottom-up manner,
    downscale in steps equaling the scale percentage, until one of the dimensions
    is smaller than the largest object dimension. This will be the top of the image
    pyramid, which is also the place where later, at the detection time, the detection
    algorithm will start running its object detector.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`minSize`和`maxSize`参数有效地减少尺度空间金字塔。在一个工业设置中，例如，固定相机位置，如传送带设置，在大多数情况下可以保证对象将具有特定的尺寸。在这种情况下添加尺度值并定义尺度范围将大大减少单个图像的处理时间，通过去除不需要的尺度级别。作为额外的好处，所有那些不需要的尺度上的假阳性检测也将消失。如果您留这些值为空，算法将从头开始构建图像金字塔，以输入图像尺寸为基础，以等于尺度百分比的步长进行下采样，直到其中一个维度小于最大对象维度。这将成为图像金字塔的顶部，也是检测算法在检测时间开始运行对象检测器的位置。
- en: '**Interface 2:**'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**界面 2：**'
- en: '[PRE14]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The second interface brings a small addition, by adding the `numDetections`
    parameter. This allows you to put the `minNeighbors` value on 1, applying the
    merging of overlapping windows as nonmaxima suppression, but at the same time
    returning you a value of the overlapping windows, which were merged. This value
    can be seen as a certainty score of your detection. The higher the value, the
    better or the more certain the detection.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个接口通过添加`numDetections`参数进行了一些小的改进。这允许你将`minNeighbors`的值设置为1，将重叠窗口的合并视为非极大值抑制，同时返回合并的重叠窗口的值。这个值可以看作是你检测的置信度分数。值越高，检测越好或越确定。
- en: '**Interface 3:**'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**接口 3：**'
- en: '[PRE15]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: A downside of this interface is that 100 windows with a very small certainty
    of detection on an individual basis can simply out rule a single detection with
    a very high individual certainty of detection. This is where the third interface
    can bring us the solution. It allows us to look at the individual scores of each
    detection window (described by the threshold value of the last stage of the classifier).
    You can then grab all those values and threshold the certainty score of those
    individual windows. When applying nonmaxima suppression in this case, the threshold
    values of all overlapping windows are combined.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这个接口的一个缺点是，100个单个检测置信度非常低的窗口可以简单地否定一个单个检测置信度非常高的检测。这就是第三个接口可以为我们提供解决方案的地方。它允许我们查看每个检测窗口的个体分数（由分类器最后阶段的阈值值描述）。然后你可以抓取所有这些值，并设置这些个体窗口的置信度分数阈值。在这种情况下应用非极大值抑制时，所有重叠窗口的阈值值会合并。
- en: Tip
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Keep in mind that if you want to try out the third interface in OpenCV 3.0,
    you have to put the parameter `outputRejectLevels` on `true`. If you do not do
    this, then the level weights matrix, which has the threshold scores, will not
    be filled.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，如果你想在OpenCV 3.0中尝试第三个接口，你必须将参数`outputRejectLevels`设置为`true`。如果不这样做，那么包含阈值分数的水平权重矩阵将不会被填充。
- en: Note
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Software illustrating the two most used interfaces for object detection can
    be found at [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_simple](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_simple)
    and [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_score](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_score).
    OpenCV detection interfaces change frequently and that it is possible that new
    interfaces are already available which are not discussed here.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在以下链接找到展示对象检测两种最常用接口的软件：[https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_simple](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_simple)
    和 [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_score](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_score)。OpenCV的检测接口经常变化，因此可能已经存在这里未讨论的新接口。
- en: Increasing object instance detection and reducing false positive detections
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高对象实例检测并减少误报检测
- en: 'Once you have chosen the most appropriate way of retrieving the object detections
    for your application, you can evaluate the proper output of your algorithm. Two
    of the most common problems found after training an object detector are:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你为你的应用选择了最合适的方法来检索对象检测，你就可以评估你算法的正确输出。在训练对象检测器后，最常见的两个问题是：
- en: Object instances that are not being detected.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未检测到的对象实例。
- en: Too much false positive detections.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过多的误报检测。
- en: 'The reason for the first problem can be explained by looking at the generic
    object model that we trained for the object class based on positive training samples
    of that object class. This lets us conclude that the training either:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题的原因可以通过查看我们基于该对象类的正样本训练数据训练的通用对象模型来解释。这让我们得出结论，训练要么：
- en: Did not contain enough positive training samples, making it impossible to generalize
    well over new object samples. In this case, it is important to add those false
    negative detections as positive samples to the training set and retrain your model
    with the extra data. This principle is called "reinforced learning".
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有足够的正样本训练数据，这使得无法很好地泛化到新的对象样本。在这种情况下，重要的是要将那些误检作为正样本添加到训练集中，并使用额外数据重新训练你的模型。这个原则被称为“强化学习”。
- en: We overtrained our model to the training set, again reducing the generalization
    of the model. To avoid this, reduce the model in stages and thus in complexity.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们过度训练了模型以适应训练集，这再次减少了模型的泛化能力。为了避免这种情况，逐步减少模型的大小和复杂性。
- en: The second problem is quite normal and happens more than often. It is impossible
    to supply enough negative samples and at the same time ensure that there will
    not be a single negative window that could still yield a positive detection at
    a first run. This is mainly due to the fact that it is very hard for us humans
    to understand how the computer sees an object based on features. On the other
    hand, it is impossible to grasp every possible scenario (lighting conditions,
    interactions during the production process, filth on the camera, and so on) at
    the very start when training an object detector. You should see the creation of
    a good and stable model as an iterative process.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题相当普遍，并且经常发生。不可能提供足够的负样本，同时确保在第一次运行时不会有任何负窗口仍然可能产生正检测。这主要是因为我们人类很难理解计算机如何根据特征来识别对象。另一方面，在训练对象检测器时，不可能一开始就掌握所有可能的场景（光照条件、生产过程中的交互、相机上的污垢等）。您应该将创建一个良好且稳定的模型视为一个迭代过程。
- en: Note
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: An approach to avoid the influence of lighting conditions can be to triplicate
    the training set by generating artificial dark and artificial bright images for
    each sample. However, keep in mind the disadvantages of artificial data as discussed
    in the beginning of this chapter.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 避免光照条件影响的处理方法可以是，通过为每个样本生成人工暗淡和人工明亮图像来使训练集翻倍。然而，请记住本章开头讨论的人工数据的缺点。
- en: 'In order to reduce the amount of false positive detections, we generally need
    to add more negative samples. However, it is important not to add randomly generated
    negative windows, since the extra knowledge that they would bring to the model
    would, in most cases, simply be minimal. It is better to add meaningful negative
    windows that can increase the quality of the detector. This is known as **hard
    negative mining** using a **bootstrapping** process. The principle is rather simple:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少误报检测的数量，我们通常需要添加更多的负样本。然而，重要的是不要添加随机生成的负窗口，因为这些窗口为模型带来的额外知识在大多数情况下只是微小的。添加有意义的负窗口，以提高检测器的质量会更好。这被称为使用**自举**过程的**硬负样本挖掘**。原理相当简单：
- en: Start by training a first object model based on your initial training set of
    positive and negative window samples.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，根据您的初始正负窗口样本训练集训练第一个对象模型。
- en: Now, collect a set of negative images, which are either specific to your application
    (if you want to train an object detector specific to your setup) or which are
    more general (if you want your object detector to work in versatile conditions).
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，收集一组负图像，这些图像要么针对您的应用特定（如果您想训练针对您设置的特定对象检测器）或者更通用（如果您希望您的对象检测器能在多种条件下工作）。
- en: Run your detector on that set of negative images, with a low certainty threshold
    and save all found detections. Cut them out of the supplied negative images and
    rescale them towards the object model size dimensions.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这组负图像上运行您的检测器，使用低置信度阈值并保存所有找到的检测。从提供的负图像中裁剪它们，并重新调整大小以适应对象模型尺寸。
- en: Now, retrain your object model, but add all the found windows to your negative
    training set in order to ensure that your model will now be trained with this
    extra knowledge.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，重新训练您的对象模型，但将所有找到的窗口添加到您的负训练集中，以确保您的模型现在将使用这些额外知识进行训练。
- en: This will ensure that the accuracy of your model goes up by a fair and decent
    amount depending on the quality of your negative images.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这将确保您的模型精度根据负图像的质量以公平和合理的方式提高。
- en: Tip
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: When adding the found extra and useful negative samples, add them to the top
    of your `background.txt` file! This forces the OpenCV training interface to first
    grab these more important negative samples before sampling all the standard negative
    training images provided! Be sure that they have exactly the required model size
    so that they can only be used once as a negative training sample.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当添加找到的额外且有用的负样本时，请将它们添加到`background.txt`文件的顶部！这迫使OpenCV训练界面首先获取这些更重要的负样本，然后再采样所有标准负训练图像！确保它们具有精确的模型大小，这样它们就只能作为一次负训练样本使用。
- en: Obtaining rotation invariance object detection
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获得旋转不变性对象检测
- en: '[PRE16]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The biggest advantage of this approach is that you only need to train a single
    orientation model and can put your time in updating and tweaking that single model
    in order to make it as efficient as possible. Another advantage is that you can
    combine all the detections in different rotations, by providing some overlap,
    and then increase the certainty of a detection, by smartly removing false positives
    that do not get detections over multiple orientations. So basically, it is kind
    of a trade-off between benefits and downsides of the approach.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的最大优点是，你只需要训练一个单方向模型，可以将你的时间投入到更新和调整这个单一模型，使其尽可能高效。另一个优点是，你可以通过提供一些重叠来结合不同旋转的所有检测，然后通过智能地移除在多个方向上没有检测到的假阳性，来增加检测的确定性。所以基本上，这是一种在方法的好处和缺点之间进行权衡。
- en: 'However, there are still some downsides to this approach:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法仍然存在一些缺点：
- en: You will need to apply a multiscale detector to each layer of your 3D representation
    matrix. This will definitely increase the search time for object instances compared
    to single orientation object detection.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要将多尺度检测器应用于你的3D表示矩阵的每一层。这肯定会增加与单方向对象检测相比的对象实例搜索时间。
- en: You will create false positive detections on each orientation, which will also
    be warped back, thus increasing the total number of false positive detections.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将在每个方向上创建假阳性检测，这些检测也将被扭曲回，从而增加假阳性检测的总数。
- en: 'Let''s take a deeper look at parts of the source code used for performing this
    rotation invariance and explain what is actually happening. The first interesting
    part can be found in the creation of the 3D matrix of rotated images:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地看看用于执行此旋转不变性的源代码部分，并解释实际上发生了什么。第一个有趣的部分可以在创建旋转图像的3D矩阵时找到：
- en: '[PRE17]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Basically, what we do is read the original image, create a vector of Mat objects
    that can contain each rotated input image, and apply the rotation function on
    top of it. As you will notice, we immediately apply all preprocessing which is
    needed for efficient object detection using the cascade classifier interface such
    as rendering the image to grayscale values and applying a histogram equalization
    in order to cope a bit with illumination changes.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们做的是读取原始图像，创建一个包含每个旋转输入图像的Mat对象向量的数组，并在其上应用旋转函数。正如你将注意到的，我们立即应用所有需要的预处理步骤，以便使用级联分类器接口进行高效的对象检测，例如将图像渲染为灰度值并应用直方图均衡化，以应对光照变化。
- en: 'The rotate function can be seen here:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转函数在这里可以看到：
- en: '[PRE18]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This code first calculates a rotation matrix based on the angle, which is expressed
    in degrees, that we want to be rotated and then applies an affine transformation
    based on this rotation matrix. Keep in mind that rotating an image like this can
    lead to an information loss of objects at the borders. This code example assumes
    your objects will occur at the center of the image and thus this does not influence
    the result. You can avoid this by enlarging the original image by adding black
    borders to that. The width and height of the image are equal so that the image
    information loss is minimal. This can be done by adding the following code right
    behind the reading of the original input image:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码首先根据我们想要旋转的角度（以度为单位）计算一个旋转矩阵，然后根据这个旋转矩阵应用仿射变换。记住，以这种方式旋转图像可能会导致边缘对象的信息丢失。此代码示例假设你的对象将出现在图像的中心，因此这不会影响结果。你可以通过在原始图像周围添加黑色边框来避免这种情况。图像的宽度和高度相等，这样图像信息损失最小。这可以通过在读取原始输入图像后立即添加以下代码来完成：
- en: '[PRE19]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This code will simply expand the original image to match a square region depending
    on the largest dimension.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将简单地根据最大尺寸将原始图像扩展到匹配一个正方形区域。
- en: 'Finally, on each level of the 3D image representation, a detection is performed
    and the found detections are warped back to the original image using a similar
    approach as warping the original image:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在3D图像表示的每一层上，都会执行检测，并使用与扭曲原始图像类似的方法将找到的检测扭曲回原始图像：
- en: Take the four corner points of the found detection in the rotated image and
    add them into a matrix for rotation warping (code line 95-103).
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将旋转图像中找到的四个检测到的角落点添加到一个用于旋转扭曲的矩阵中（代码行95-103）。
- en: Apply the inverse transformation matrix based on the angle of the current rotated
    image (code line 106-108).
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据当前旋转图像的角度应用逆变换矩阵（代码行106-108）。
- en: Finally, draw a rotated rectangle on the information of the rotated four matrix
    points (code line 111-128).
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在旋转的四矩阵点信息上绘制一个旋转矩形（代码行111-128）。
- en: The following figure shows the exact result of applying a rotation invariant
    face detection to an image with faces in multiple orientations.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了将旋转不变性人脸检测应用于具有多个方向的人脸图像的确切结果。
- en: '![Obtaining rotation invariance object detection](img/00086.jpeg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![获得旋转不变性目标检测](img/00086.jpeg)'
- en: Rotation invariant face detection starting with the following angle steps [1
    degree, 10 degrees, 25 degrees, 45 degrees]
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以以下角度步长开始进行旋转不变性人脸检测：[1度，10度，25度，45度]
- en: We see that four times the suggested technique is applied to the same input
    image. We played around with the parameters in order to see the influence on detection
    time and the detections returned. In all cases, we applied a search from 0 to
    360 degrees, but changed the angle step in between each stage of the 3D rotation
    matrix from 0 to 45 degrees.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到四次建议的技术被应用于相同的输入图像。我们调整了参数以观察对检测时间和返回的检测的影响。在所有情况下，我们都从0到360度进行了搜索，但在3D旋转矩阵的每个阶段之间改变了角度步长，从0到45度。
- en: '| Applied angle step | Total time for executing all detections |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 应用角度步长 | 执行所有检测的总时间 |'
- en: '| --- | --- |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 degree | 220 seconds |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 1度 | 220秒 |'
- en: '| 10 degrees | 22.5 seconds |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 10度 | 22.5秒 |'
- en: '| 25 degrees | 8.6 seconds |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 25度 | 8.6秒 |'
- en: '| 45 degrees | 5.1 seconds |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 45度 | 5.1秒 |'
- en: As we can see, the detection time is reduced drastically when increasing the
    step of the angle. Knowing that an object model on itself could cover at least
    20 degrees in total, we can easily reduce the step in order to significantly decrease
    the processing time.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，当增加角度步长时，检测时间会大幅减少。知道一个物体模型本身至少可以覆盖总共20度，我们可以轻松地减小步长以显著减少处理时间。
