- en: Assisted Drawing with RNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNNs进行辅助绘图
- en: In the previous chapter, we walked through building a simple drawing application
    that would try to infer what the user was drawing and present them with alternatives
    based on the most likely predicted categories; the intention of this application
    was to improve the efficiency of sketching tasks by giving the user completed
    sketches, obtained through Microsoft's Bing image search, rather than having to
    spend time fussing over the details.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们介绍了构建一个简单的绘图应用程序的过程，该应用程序会尝试推断用户正在绘制的内容，并根据最可能的预测类别向用户提供替代方案；这个应用程序的目的是通过提供由微软的Bing图像搜索获得的完成草图来提高绘图任务的效率，而不是花费时间纠结于细节。
- en: In this chapter, we'll revisit this application but look at an alternative for
    inferring what the user is drawing, and, in doing so, we will be exposing ourselves
    to new types of data and machine learning models. Following the familiar format,
    we will first revise the task, explore the data and model, and then walk through
    building up the required functionality in a playground, before migrating it across
    to our application. Let's get started.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重新审视这个应用程序，但将探讨一种推断用户正在绘制的内容的替代方法。在这个过程中，我们将接触新的数据类型和机器学习模型。遵循熟悉的格式，我们首先将回顾任务，探索数据和模型，然后在一个游乐场中逐步构建所需的功能，最后将其迁移到我们的应用程序中。让我们开始吧。
- en: Assisted drawing
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 辅助绘图
- en: In this section, we will briefly describe this chapter's project and what we
    aim to achieve. Recall from the previous chapter that we described an application
    capable of predicting what the user was trying to sketch, and fetched similar
    images based on the predicted categories, such as a sailboat. Based on this prediction,
    the application would search and download images of that category. After downloading,
    it would sort them based on their similarity with regards to the user's sketch.
    Then it would present the ordered alternatives to the user, which they could swap
    their sketch with.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍本章的项目以及我们的目标。回顾前一章，我们描述了一个能够预测用户试图绘制的图像的应用程序，并根据预测的分类（如帆船）检索相似图像。基于这个预测，应用程序会搜索并下载该类别的图像。下载后，它会根据与用户草图相似度进行排序。然后，它会向用户展示排序后的替代方案，用户可以用草图进行交换。
- en: 'The finished project is shown as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 完成的项目如下所示：
- en: '![](img/c711add4-cacb-4d48-b65a-f9a351ecf11f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c711add4-cacb-4d48-b65a-f9a351ecf11f.png)'
- en: The model used for performing this classification was based on a **Convolutional
    Neural Network** (**CNN**), a type of neural network well suited for understanding
    images owing to its ability to find local patterns and build on top of these lower
    patterns to find more complex and interesting patterns. We took advantage of these
    higher order patterns by using them as a basis to sort our downloaded images,
    such that those that were more similar in style to the user's sketch would be
    shown first. We reasoned how this worked by comparing it with measurements of
    similarities between sentences using words as features—words being analogous to
    our higher order patterns—and distance formulas to calculate the similarities.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 用于执行此分类的模型基于**卷积神经网络**（**CNN**），这是一种非常适合理解图像的神经网络类型，因为它能够找到局部模式并在这些较低层次模式的基础上构建更复杂和有趣的模式。我们利用这些高级模式，将它们作为排序下载图像的基础，使得那些在风格上与用户草图更相似的图像会首先显示出来。我们通过比较使用单词作为特征（单词类似于我们的高级模式）和距离公式来计算相似度，来推理这种工作的原理。
- en: 'But our approach suffered from a bit of overhead; to perform accurate classification,
    we needed a significant amount of the sketch completed, as well as needing to
    use memory and CPU cycles to rasterize the image before we could feed it into
    our model. In this chapter, we will be using an alternative that doesn''t rely
    on pixels as its features but rather the **sequences of strokes** used to draw
    it. There are numerous reasons you may want to do this, including:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们的方法存在一些开销；为了进行准确的分类，我们需要完成大量的草图，还需要使用内存和CPU周期将图像光栅化，然后才能将其输入到我们的模型中。在本章中，我们将使用一种不依赖于像素作为特征，而是使用绘制它所用的**笔触序列**作为特征的替代方法。你可能有无数个理由想要这样做，包括：
- en: Accessibility to the data or larger dataset
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据或更大数据集的可访问性
- en: Potential improvements to the accuracy of the predictions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测准确性的潜在改进
- en: Generative capabilities, that is, being able to predict and generate the next
    set of strokes
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成能力，即能够预测和生成下一组笔触
- en: But here, it gives us the opportunity to explore a type of data that encodes
    essentially the same thing—a sketch. Let's explore this further in the next section,
    where we introduce the dataset and model that will be used in this project.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但在这里，它为我们提供了探索一种本质上编码相同内容的数据类型——草图的机会。让我们在下一节中进一步探讨，我们将介绍在这个项目中将使用的数据集和模型。
- en: Recurrent Neural Networks for drawing classification
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘图分类的循环神经网络
- en: The model used in this chapter was trained on the dataset used in Google's AI
    experiment *Quick**, Draw!*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的模型是在谷歌的 AI 实验 *Quick**, Draw!* 中使用的数据集上训练的。
- en: '*Quick, Draw!* is a game where players are challenged to draw a given object
    to see whether the computer can recognize it; an extract of the data is shown
    as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*Quick, Draw!* 是一款游戏，玩家被挑战绘制一个给定的物体，看看计算机是否能识别它；以下展示了数据的一个摘录：'
- en: '![](img/68103d5c-1a1f-404a-96e5-d9638b275429.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/68103d5c-1a1f-404a-96e5-d9638b275429.jpg)'
- en: 'The technique was inspired from the work done on handwritten recognition (Google
    Translate), where, rather than looking at the image as a whole, the team worked
    with data features describing how the characters were drawn. This is illustrated
    in the following image:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术是从手写识别（谷歌翻译）的工作中受到启发的，在那里，团队不是将整个图像作为一个整体来看待，而是与描述字符如何绘制的特征数据一起工作。这在下图中得到了说明：
- en: '![](img/607c6e39-9ebf-4990-883f-b0d2e905b949.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/607c6e39-9ebf-4990-883f-b0d2e905b949.png)'
- en: Source: https://experiments.withgoogle.com/ai/quick-draw
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://experiments.withgoogle.com/ai/quick-draw](https://experiments.withgoogle.com/ai/quick-draw)
- en: The hypothesis here is that there exists some consistent pattern of how people
    draw certain types of objects; but to discover those patterns, we would need a
    lot of data, which we do have. The dataset consists of over 50 million drawings
    across 345 categories obtained cleverly, from the players of the *Quick, Draw!*
    game. Each sample is described with timestamped vectors and associated metadata
    describing the country the player was based in and the category asked of the user.
    You can learn more about the dataset from the official website: [https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的假设是，人们绘制某些类型物体时存在某种一致的规律；但要发现这些规律，我们需要大量的数据，而这些数据我们确实拥有。数据集包括从 *Quick, Draw!*
    游戏玩家那里巧妙获得的超过 5000 万个绘图，涵盖了 345 个类别。每个样本都由带时间戳的向量和关联元数据描述，这些元数据说明了玩家所在的国家和用户请求的类别。您可以从官方网站了解更多关于数据集的信息：[https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset)。
- en: 'To make the dataset and training manageable, our model was only trained on
    172 of the 345 categories, but the accompanying Notebook used to create and train
    the model is available for those wanting to delve into the details. To get a better
    understanding of the data, let''s have a peek at a single sample, as shown here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使数据集和训练变得可管理，我们的模型只在 345 个类别中的 172 个类别上进行了训练，但用于创建和训练模型的 Notebook 供那些想要深入了解细节的人使用。为了更好地理解数据，让我们看一下单个样本，如下所示：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The details of the sketch are broken down into an array of strokes, each described
    by a three-dimensional array containing the `x`, `y` positions and `timestamp`
    that make up the path of the stroke:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 草图的细节被分解成一系列的笔触，每个笔触由一个包含 `x`、`y` 位置和 `timestamp` 的三维数组描述，这些构成了笔触的路径：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As mentioned previously, this being an example from the **raw dataset**, the
    team behind *Quick, Draw!* has released many variants of the data, from raw samples
    to preprocessed and compressed versions. We are mostly interested in exploring
    the raw and simplified versions: the former because it''s the closest representation
    we have that will represent the data we obtain from the user, and the latter because
    it was used to train the model.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这是一个来自 **原始数据集** 的示例，*Quick, Draw!* 背后的团队发布了许多数据变体，从原始样本到预处理和压缩版本。我们主要对探索原始和简化版本感兴趣：前者因为它是我们拥有的最接近用户获取数据的表示，后者因为它被用来训练模型。
- en: '**Spoiler**: Most of this chapter deals with preprocessing the user input.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**剧透**：本章的大部分内容都涉及用户输入的预处理。'
- en: Both raw and simplified versions have stored each category in an individual
    file in the NDJSON  file format.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 原始和简化版本都将每个类别存储在单独的文件中，文件格式为 NDJSON。
- en: The NDJSON file format, short for newline delimited JSON, is a convenient format
    for storing and streaming structured data that may be processed one record at
    a time. As the name suggests, it stores multiple JSON-formatted objects in single
    lines. In our case, this means each sample is stored as a separate object delimited
    by a new line; you can learn more about the format at [http://ndjson.org](http://ndjson.org).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: NDJSON文件格式，即换行分隔的JSON，是一种方便的格式，用于存储和流式传输可能逐条记录处理的结构化数据。正如其名所示，它将多个JSON格式的对象存储在单行中。在我们的情况下，这意味着每个样本都存储为一个单独的对象，由换行符分隔；你可以在[http://ndjson.org](http://ndjson.org)了解更多关于该格式的信息。
- en: You may be wondering what the difference is between the raw and simplified versions.
    We will go into the details when we build the preprocessing functionality required
    for this application, but as the name implies, the simplified version reduces
    the complexity of each stroke by removing any unnecessary points, along with applying
    some level of standardization—a typical requirement when dealing with any data
    to make the samples more comparable.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道原始版本和简化版本之间的区别是什么。当我们构建此应用程序所需的预处理功能时，我们将详细介绍这些细节，但正如其名所示，简化版本通过删除任何不必要的点以及应用某种程度的标准化来降低每个笔触的复杂性——这是处理任何数据时的典型要求，以便使样本更具可比性。
- en: Now that we have a better understanding of the data we are dealing with, let's
    turn our attention to building up some level of intuition of how we can learn
    from these sequences, by briefly discussing the details of the model used in this
    chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对我们处理的数据有了更好的理解，让我们转向如何从这些序列中学习，通过简要讨论本章使用的模型细节来建立一些直觉。
- en: In previous chapters, we saw many examples of how CNNs can learn useful patterns
    from local 2D patches, which themselves can be built upon to further abstract
    from raw pixels into something with more descriptive power. This is fairly intuitive
    given our understanding of images is not made up of independent pixels but rather
    a collection of pixels related to their neighbors, which in turn describe parts
    of an object. In [Chapter 1](7d4f641f-7137-4a8a-ae6e-2bb0e2a6db5c.xhtml), *Introduction
    to Machine Learning*, we introduced a **Recurrent Neural Network** (**RNN**),
    a major component of building the **Sequence to Sequence** (**Seq2Seq**) model
    used for language translation, and we saw how its ability to remember made it
    well suited for data made up of sequences where order matters. As highlighted
    previously, our given samples are made up of sequences of strokes; the RNN is
    a likely candidate for learning to classify sketches.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了许多例子，说明了卷积神经网络（CNNs）如何从局部的二维块中学习有用的模式，这些模式本身可以进一步从原始像素抽象出更具描述力的东西。鉴于我们对图像的理解并非由独立的像素组成，而是一系列与相邻像素相关的像素集合，这些像素集合反过来描述了物体的各个部分，这一点相当直观。在[第1章](7d4f641f-7137-4a8a-ae6e-2bb0e2a6db5c.xhtml)《机器学习简介》中，我们介绍了一种**循环神经网络**（**RNN**），它是构建用于语言翻译的**序列到序列**（**Seq2Seq**）模型的主要组成部分，并看到了其记忆能力使其非常适合顺序数据，其中顺序很重要。正如之前所强调的，我们的给定样本由笔触序列组成；循环神经网络是学习对草图进行分类的合适候选者。
- en: 'As a quick recap, RNNs implement a type of **selective memory** using a feedback
    loop, which itself is adjusted during training; diagrammatically this is shown
    as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作为快速回顾，循环神经网络（RNNs）通过一个反馈循环实现了一种**选择性记忆**，这个反馈循环在训练过程中进行调整；从图解上看，如下所示：
- en: '![](img/9ce9ab70-0538-49dd-8a85-49ab5503d2d0.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9ce9ab70-0538-49dd-8a85-49ab5503d2d0.png)'
- en: On the left is the actual network, and on the right we have the same network
    unrolled across four time steps. As the points of the sketch's strokes are fed
    in, they are multiplied by the layer's weight along with the current state before
    being fed back in and/or outputted. During training, this feedback allows the
    network to learn patterns of an ordered sequence. We can stack these recurrent
    layers on top of each other to learn more complex and abstract patterns as we
    did with CNN.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 左边是实际的网络，右边是在四个时间步上展开的相同网络。随着草图笔触的点被输入，它们被层权重与当前状态相乘，然后被反馈回网络并/或输出。在训练过程中，这种反馈允许网络学习有序序列的模式。我们可以将这些循环层堆叠起来，就像我们在CNN中做的那样，以学习更复杂和抽象的模式。
- en: 'But recurrent layers are not the only way to learn patterns from sequential
    data. If you generalize the concept of CNNs as something being able to learn local
    patterns across any dimension (as opposed to just two dimensions), then you can
    see how we could use 1D convolutional layers to achieve a similar effect as our recurrent
    layers. Therein, similar to 2D convolutional layers, we learn 1D kernels across
    sequences (treating time as a spatial dimension) to find local patterns to represent
    our data. Using a convolutional layer has the advantage of being considerably
    computationally cheaper than its counterpart, making it ideal for processor- and
    power-constrained devices, such as mobile phones. It is also advantageous for
    its ability to learn patterns independent of order, similar to how 2D kernels
    are invariant of position. In this figure, we illustrate how the 1D convolutional
    layer operates on input data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 但递归层并不是从序列数据中学习模式的唯一方法。如果你将CNN的概念推广为能够在任何维度上学习局部模式（而不仅仅是两个维度），那么你就可以看到我们如何可以使用1D卷积层来实现与递归层类似的效果。在那里，类似于2D卷积层，我们在序列中学习1D核（将时间视为空间维度）以找到局部模式来表示我们的数据。使用卷积层的优势是它的计算成本远低于其对应层，这使得它非常适合处理器和功率受限的设备，如手机。它还有其能够学习独立于顺序的模式的能力，类似于2D核对位置的不变性。在此图中，我们说明了1D卷积层如何作用于输入数据：
- en: '![](img/7ebfa401-7372-4c04-bfc3-91ef33e6983b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ebfa401-7372-4c04-bfc3-91ef33e6983b.png)'
- en: 'In this context, strokes (local to the window size) will be learned, independent
    of where they are in the sequence, and a compact representation will be outputted,
    which we can then feed into an RNN to learn ordered sequences from these strokes
    (rather than from raw points). Intuitively you can think of our model as initially
    learning strokes such as vertical and horizontal strokes (independent of time),
    and then learning (in our subsequent layers made up of RNNs) higher-order patterns
    such as shapes from the ordered sequence of these strokes. The following figure
    illustrates this concept:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，笔触（局部于窗口大小）将独立于它们在序列中的位置而被学习，并将输出紧凑的表示，然后我们可以将其输入到RNN中，从这些笔触中学习有序序列（而不是从原始点中学习）。直观地，你可以将我们的模型视为最初学习垂直和水平等笔触（独立于时间），然后在后续的由RNN组成的层中学习从这些笔触的有序序列中更高阶的模式，如形状。以下图说明了这个概念：
- en: '![](img/41eb49b5-30d3-446d-9269-458d3679581c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41eb49b5-30d3-446d-9269-458d3679581c.png)'
- en: On the left, we have the raw points inputted into the model. The middle part
    shows how a 1D convolutional layer can learn local patterns from these points
    in a form of strokes. And finally, at the far right, we have the subsequent RNNs
    learning order-sensitive patterns from the sequence of these strokes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，我们有输入到模型中的原始点。中间部分显示了1D卷积层如何从这些点中学习局部模式，以笔触的形式。最后，在右侧，我们有后续的RNN从这些笔触的序列中学习顺序敏感的模式。
- en: One more concept to introduce before introducing the model, but, before doing
    so, I want you to quickly think of how you draw a square. Do you draw it in a
    clockwise direction or anti-clockwise direction?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍模型之前，还有一个概念需要介绍，但在这样做之前，我想让你快速思考一下你如何画一个正方形。你是按顺时针方向画还是按逆时针方向画？
- en: The last concept I want to briefly introduce in this section is bidirectional
    layers; bidirectional layers attempt to make our network invariant to the previous
    question. We discussed earlier how RNNs are sensitive to order, which is precisely
    why they are useful here, but as I hope  has been highlighted, our sketch may
    be drawn in the reverse order. To account for this, we can use a bidirectional
    layer, which, as the name implies, processes the input sequence in two directions
    (chronologically and anti-chronologically) and then merges their representations.
    By processing a sequence in both directions, our model can become somewhat invariant
    to the direction in which we draw.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我想简要介绍的最后一种概念是双向层；双向层试图使我们的网络对先前的问题不变。我们之前讨论了RNN对顺序的敏感性，这正是它们在这里有用的原因，但正如我希望已经强调的，我们的草图可能是按相反的顺序绘制的。为了解决这个问题，我们可以使用双向层，正如其名称所暗示的，它以两个方向（按时间顺序和逆时间顺序）处理输入序列，然后合并它们的表示。通过从两个方向处理序列，我们的模型可以变得对绘制方向有一定的不变性。
- en: 'We have now introduced all the building blocks used for this model; the following
    figure shows the model in its entirety:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经介绍了用于此模型的所有构建块；以下图显示了整个模型：
- en: '![](img/2f02a2a5-9683-40fc-9c5f-c61caf9e715b.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f02a2a5-9683-40fc-9c5f-c61caf9e715b.png)'
- en: As a reminder, this book is focused on the application of machine learning related
    to Core ML. Therefore we won't be going into the details of this (or any) model,
    but cover just enough to have an intuitive understanding of how the model works
    for you to use and explore further.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，这本书主要关注与Core ML相关的机器学习应用。因此，我们不会深入探讨这个（或任何）模型的细节，但会涵盖足够的内容，以便您能够直观地理解模型的工作原理，以便您使用和进一步探索。
- en: 'As shown previously, our model is comprised of a stack of 1D convolutional
    layers that feed into a stack of **Long Short-Term Memory** (**LSTM**), an implementation
    of an RNN, before being fed into a fully connected layer where our prediction
    is made. This model was trained on 172 categories, each using 10,000 training
    samples and 1,000 validation samples. After 16 epochs, the model achieved approximately
    78% accuracy on both the training and validation data, as shown here:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的模型由一系列一维卷积层组成，这些层输入到一个**长短期记忆**（**LSTM**）堆栈中，这是一个循环神经网络（RNN）的实现，然后输入到一个全连接层，在那里我们做出预测。这个模型在172个类别上进行了训练，每个类别使用10,000个训练样本和1,000个验证样本。经过16个epoch后，模型在训练和验证数据上达到了大约78%的准确率，如下所示：
- en: '![](img/7040b3d5-bb2c-4031-8c3d-4a9ed07dd5f1.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7040b3d5-bb2c-4031-8c3d-4a9ed07dd5f1.png)'
- en: We now have our model but have skimmed across what we are actually feeding into
    our model. In the next section, we will discuss what our model was trained with
    (and therefore expecting) and implement the required functionality to prepare
    it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了我们的模型，但只是匆匆浏览了我们实际上输入到模型中的内容。在下一节中，我们将讨论我们的模型是用什么进行训练的（因此期望什么）并实现所需的函数来准备它。
- en: Input data and preprocessing
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入数据和预处理
- en: In this section, we will implement the preprocessing functionality required
    to transform our raw user input into something the model is expecting. We will
    build up this functionality in a playground project before migrating it across
    to our project in the next section.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现预处理功能，以将我们的原始用户输入转换为模型所期望的格式。我们将在操场项目中构建此功能，然后在下一节中将其迁移到我们的项目中。
- en: 'If you haven''t done so, pull down the latest code from the accompanying repository
    [https://github.com/PacktPublishing/Machine-Learning-with-Core-ML](https://github.com/PacktPublishing/Machine-Learning-with-Core-ML).
    Once downloaded, navigate to the directory `Chapter8/Start/` and open the playground
    project `ExploringQuickDrawData.playground`. Once loaded, you will see the playground
    for this chapter, as shown:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，请从配套的仓库中拉取最新的代码[https://github.com/PacktPublishing/Machine-Learning-with-Core-ML](https://github.com/PacktPublishing/Machine-Learning-with-Core-ML)。下载后，导航到目录`Chapter8/Start/`并打开操场项目`ExploringQuickDrawData.playground`。一旦加载，您将看到本章的操场，如下所示：
- en: '![](img/b0eb7b69-a4e1-4d6d-86b9-e2fb3e3b6ab6.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0eb7b69-a4e1-4d6d-86b9-e2fb3e3b6ab6.png)'
- en: The playground includes a few samples of the raw *Quick, Draw!* dataset, a single
    simplified extract, as well as the complied model and supporting classes we created
    in the previous chapter to represent a sketch (`Stroke.swift`, `Sketch.swift`)
    and render it (`SketchView.swift`). Our goal for this section will be to better
    understand the data and the preprocessing required before feeding our model; in
    doing so, we will be extending our existing classes to encapsulate this functionality.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 操场包括一些原始*Quick, Draw!*数据集的样本，一个简化的提取，以及我们在上一章中创建的编译模型和支持类，用于表示草图（`Stroke.swift`，`Sketch.swift`）和渲染它（`SketchView.swift`）。本节的目标将是更好地理解在将数据输入模型之前所需的预处理；这样做时，我们将扩展我们现有的类以封装此功能。
- en: 'Let''s start reviewing what code exists before we move forward; if you scroll
    down the opened source file, you will see the methods `createFromJSON` and `drawSketch`.
    The former takes in a JSON object (the format our samples are saved in) and returns
    a strongly typed object: `StrokeSketch`. As a reminder, each sample is made up
    of:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续前进之前，让我们先回顾一下现有的代码；如果您向下滚动打开的源文件，您将看到`createFromJSON`和`drawSketch`方法。前者接受一个JSON对象（我们的样本保存的格式）并返回一个强类型对象：`StrokeSketch`。提醒一下，每个样本由以下内容组成：
- en: '`key_id`: Unique identifier'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`key_id`: 唯一标识符'
- en: '`word`: Category label'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word`: 类别标签'
- en: '`countrycode`: Country code where the sample was drawn'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`countrycode`: 样本抽取的国家代码'
- en: '`timestamp`: Timestamp when the sample was created'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timestamp`: 样本创建的时间戳'
- en: '`recognized`: A flag indicating whether the sketch was currently recognized'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`recognized`: 一个标志，表示草图是否当前被识别'
- en: '`drawing`: A multi-dimensional array consisting of arrays of *x*, *y* coordinates
    along with the elapsed time since the point was created'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drawing`：一个多维数组，由*x*，*y*坐标的数组组成，以及自点创建以来经过的时间'
- en: The `StrokeSketch` maps the word to the label property and *x*, *y* coordinates
    to the stroke points. We discard everything else as it is not deemed useful in
    classification and not used by our model. The `drawSketch` method is a utility
    method that handles scaling and centering the sketch before creating an instance
    of a `SketchView` to render the scaled and centered sketch.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`StrokeSketch`将单词映射到标签属性，并将*x*，*y*坐标映射到笔触点。我们丢弃其他所有内容，因为在分类中认为它没有用，并且我们的模型也没有使用。`drawSketch`方法是一个实用方法，它在创建`SketchView`实例以渲染缩放和居中的草图之前处理缩放和居中。'
- en: The last block of code preloads the JSON files and makes them available through
    the dictionary `loadedJSON`, where the key is the associated filename and value
    is the loaded JSON object.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一块代码预加载JSON文件，并通过字典`loadedJSON`使它们可用，其中键是相关文件名，值是加载的JSON对象。
- en: 'Let''s start by taking a peek at the data, comparing the raw samples to the
    simplified samples; add the following code to your playground:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看数据，比较原始样本和简化样本；将以下代码添加到您的游乐场中：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the previous code snippet, we are simply getting a reference to our loaded
    JSON files and passing the samples at index 0 and 1 to our `createFromJSON` file,
    which will return their `StrokeSketch` representation. We then proceed to pass
    this into our `drawSketch` method to create the view to render them. After running,
    you can preview each of the sketches by clicking on the eye icon located to the
    right-hand panel on the same line as the call to the method `drawSketch`. The
    following image presents both outputs side by side for comparison:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们只是获取我们加载的JSON文件的引用，并将索引0和1的样本传递给我们的`createFromJSON`文件，它将返回它们的`StrokeSketch`表示。然后我们继续将其传递给`drawSketch`方法以创建渲染视图。运行后，您可以通过单击位于方法`drawSketch`同一行的右侧面板上的眼睛图标来预览每个草图。以下图像展示了两个输出并排比较：
- en: '![](img/f25840e0-da15-4335-9fa2-3db291acfba2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f25840e0-da15-4335-9fa2-3db291acfba2.png)'
- en: The major differences between the samples from the raw dataset and simplified
    dataset can be seen in the preceding figure. The raw sample is much larger and
    smoother. What is not obvious from the previous image is that the simplified sample
    is positioned to the top left while the raw one consists of points in their original
    and absolute positions (recalling that our `drawSketch` method rescales, if required,
    and centers the sketch).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始数据集和简化数据集的样本中可以看到的主要差异在前面的图中。原始样本要大得多，也更平滑。从之前的图像中不明显的是，简化样本位于左上角，而原始样本由原始和绝对位置上的点组成（回想一下，我们的`drawSketch`方法在需要时重新缩放并居中草图）。
- en: 'As a reminder, the raw samples resemble the input we are expecting to receive
    from the user, while on the other hand our model was trained on the samples from
    the simplified dataset. Therefore, we need to perform the same preprocessing steps
    that have been used to transform the raw data into its simplified counterparts
    before feeding our model. These steps, described in the repository for the data
    at [https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset),
    are listed as follows, and this is what we will now implement in our playground:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，原始样本类似于我们期望从用户那里接收到的输入，而另一方面，我们的模型是在简化数据集的样本上训练的。因此，我们需要执行与将原始数据转换为简化对应物相同的预处理步骤，在将数据输入我们的模型之前。这些步骤在数据存储库[https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset)中描述如下，这是我们将在我们的游乐场中实现的内容：
- en: Align the drawing to the top-left corner to have minimum values of zero
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将绘图对齐到左上角，使其最小值为零
- en: Uniformly scale the drawing to have a maximum value of 255
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将绘图均匀缩放，使其最大值为255
- en: Resample all strokes with a one pixel spacing
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一像素间距对所有笔触进行重采样
- en: Simplify all strokes using the Ramer-Douglas-Peucker algorithm with an epsilon
    value of 2.0
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用2.0的epsilon值，使用Ramer-Douglas-Peucker算法简化所有笔触
- en: The Ramer–Douglas–Peucker algorithm takes a curve composed of line segments
    (strokes) and finds a simpler curve with fewer points. You can learn more about
    the algorithm here: [https://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm](https://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Ramer-Douglas-Peucker算法接受由线段（笔画）组成的曲线，并找到具有更少点的更简单曲线。您可以在以下链接中了解更多关于该算法的信息：[https://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm](https://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm).
- en: The rationale behind these steps should be fairly self-explanatory and is highlighted
    from the figure showing the two sketches of an airplane. That is, the airplane
    should be invariant to its actual position on the screen and invariant to the
    scale. And simplifying the stroke makes it easier for our model to learn as it
    helps ensure that we only capture salient features.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤背后的原理应该是相当直观的，并且从显示飞机两个草图的图中可以突出显示。也就是说，飞机应该对其在屏幕上的实际位置不变，并且对比例不变。简化笔画使得我们的模型更容易学习，因为它有助于确保我们只捕获显著特征。
- en: 'Start off by creating an extension of your `StrokeSketch` class and stubbing
    out the method `simplify`, as shown:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建您的`StrokeSketch`类的扩展，并像下面所示地创建`simplify`方法的占位符：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will be mutating a clone of the instance of itself, which is why we first
    create a copy. We next want to calculate the scale factor required to scale the
    sketch to have a maximum height and/or width of 255 while respecting its aspect
    ratio; add the following code to your `simplify` method, which does just this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将修改实例的副本，这就是为什么我们首先创建一个副本。接下来，我们想要计算将草图缩放到最大高度和/或宽度为255，同时尊重其宽高比所需的比例因子；将以下代码添加到您的`simplify`方法中，它正是这样做的：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For each dimension (`width` and `height`), we have calculated the scale required
    to ensure that our sketch is either scaled up or down to a dimension of `255`.
    We now need to apply this to each of the points associated with each of the strokes
    held by the `StrokeSketch` class; as we''re iterating through each point, it also
    makes sense to align our sketch to the top-left corner (`x= 0`, `y = 0`) as a
    required preprocessing step. We can do this simply by subtracting the minimum
    value of each of the dimensions. Append the following code to your `simplify`
    method to do this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个维度（`宽度`和`高度`），我们已经计算了确保我们的草图被缩放到`255`维度的所需比例。现在我们需要将此应用于`StrokeSketch`类持有的每个笔画相关的每个点；当我们遍历每个点时，将草图对齐到左上角（`x=
    0`，`y = 0`）作为所需的预处理步骤也是有意义的。我们可以通过减去每个维度的最小值来实现这一点。将以下代码附加到您的`simplify`方法中来完成此操作：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our final step is to simplify the curve using the Ramer-Douglas-Peucker algorithm;
    to do this, we will make the `Stroke` responsible for implementing the details
    and just delegate the task there. Add the final piece of code to your `simplify`
    method within your `StrokeSketch` extension:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一步是使用Ramer-Douglas-Peucker算法简化曲线；为此，我们将使`Stroke`负责实现细节，并将任务委托给那里。将以下代码添加到您的`StrokeSketch`扩展中的`simplify`方法中：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The Ramer-Douglas-Peucker algorithm recursively traverses the curve, initially
    starting with the first and last point and finding the point that is furthest
    from this line segment. If the point is closer than a given threshold, then any
    points currently marked to be kept can be discarded, but if the point is greater
    than our threshold then that point must be kept. The algorithm then recursively
    calls itself with the first point and furthest point as well as furthest point
    and last point. After traversing the whole curve, the result is a simplified curve
    that only consists of the points marked as being kept, as described previously.
    The process is summarized in the following figure:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Ramer-Douglas-Peucker算法递归遍历曲线，最初从第一个和最后一个点开始，找到离这个线段最远的点。如果这个点比给定的阈值近，那么可以丢弃当前标记为保留的任何点，但如果这个点比我们的阈值大，那么这个点必须保留。然后算法递归地调用自身，使用第一个点和最远点以及最远点和最后一个点。遍历整个曲线后，结果是只包含之前描述的标记为保留的点的简化曲线。这个过程在以下图中总结：
- en: '![](img/d6c502b8-0c3e-4d6d-8ad9-80cce1158c4d.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d6c502b8-0c3e-4d6d-8ad9-80cce1158c4d.png)'
- en: 'Let''s start by extending the `CGPoint` structure to include a method for calculating
    the distance of a point given a line; add this code to your playground:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从扩展`CGPoint`结构以包括一个计算给定直线上的点距离的方法开始；将此代码添加到您的游乐场中：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here, we have added a static method to the `CGPoint` structure; it calculates
    the perpendicular distance of a point given a line (which is the value we compare
    with our threshold to simplify our line, as previously described). Next, we will
    implement the recursive method as described, which will be used to build up the
    curve by testing and discarding any points under our threshold. As mentioned,
    we will encapsulate this functionality within the `Stroke` class itself, so we
    start off by stubbing out the extension:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们向`CGPoint`结构中添加了一个静态方法；它计算给定一条线（这是我们之前描述的比较阈值以简化线条的值）的点的垂直距离。接下来，我们将实现描述中的递归方法，它将用于通过测试和丢弃低于阈值的任何点来构建曲线。如前所述，我们将此功能封装在`Stroke`类本身中，所以我们首先通过创建扩展的占位符开始：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, within the extension, add the recursive method:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在扩展中添加递归方法：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Most of this should make sense as it's a direct implementation of the algorithm
    described. We start off by finding the furthest distance, which must be greater
    than our threshold; otherwise, the point is ignored. We add the point to the array
    of points to keep and then pass each end of the segment to our recursive method
    until we have traversed the whole curve.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这大部分应该很容易理解，因为它是对描述的算法的直接实现。我们首先找到最远的距离，它必须大于我们的阈值；否则，该点将被忽略。我们将该点添加到要保留的点数组中，然后将线段的每个端点传递给我们的递归方法，直到我们遍历整个曲线。
- en: 'The last method we need to implement is the method responsible for initiating
    this process, which we will also encapsulate within our `Stroke` extension; so
    go ahead and add the following method to your extension:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要实现的最后一个方法是负责启动此过程的那个方法，我们也将将其封装在我们的`Stroke`扩展中；所以请继续并添加以下方法到你的扩展中：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `simplify` method simply (excuse the pun) creates an array of points of
    our simplified curve, adding the first point, before kicking off the recursive
    method we had just implemented. Then, when the curve has been traversed, it finally
    adds the last point before returning the `Stroke` with the simplified points.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`simplify`方法简单地（请原谅这个双关语）创建了一个包含简化曲线点的数组，添加第一个点，然后启动我们刚刚实现的递归方法。然后，当曲线被遍历后，它最终添加最后一个点，并在返回简化点的`Stroke`之前完成。'
- en: 'At this point, we have implemented the functionality required to transform
    raw input into its simplified form, as specified in the *Quick, Draw!* repository.
    Let''s verify our work by comparing our simplified version of a raw sketch with
    an existing simplified version of the same sketch. Add the following code to your
    playground:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经实现了将原始输入转换为简化形式所需的功能，正如在*Quick, Draw!*仓库中指定的那样。让我们通过比较我们的简化原始草图与现有简化草图来验证我们的工作。将以下代码添加到你的游乐场中：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As we did before, you can click on the eye icon within the right-hand-side
    panel for each of the `drawSketch` calls to preview each of the sketches. The
    first is the sketch from the raw dataset, the second is from the simplified dataset,
    and third is by using our simplified implementation, using the sample from the
    raw dataset. If everything goes as per the plan, then you should see something
    that resembles the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，你可以点击右侧面板中每个`drawSketch`调用内的眼睛图标来预览每个草图。第一个是从原始数据集中的草图，第二个是从简化数据集中，第三个是使用我们的简化实现，使用原始数据集的样本。如果一切按计划进行，你应该会看到以下类似的内容：
- en: '![](img/7670a6d8-78ec-475a-be3c-787882db8ebd.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7670a6d8-78ec-475a-be3c-787882db8ebd.png)'
- en: At close inspection, our simplified version looks as though it is more aggressive
    than the sample from the simplified dataset, but we can easily tweak this by adjusting
    our threshold. However, for all intents and purposes, this will suffice for now.
    At this point, we have the required functionality to simplify our dataset, transforming
    it to something that resembles the training dataset. But before feeding our data
    into the model, we have more preprocessing to do; let's do that now, starting
    with a quick discussion of what our model is expecting.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察，我们的简化版本看起来比简化数据集的样本更具侵略性，但我们可以通过调整阈值轻松地调整这一点。然而，从所有目的来看，目前这已经足够了。到目前为止，我们已经拥有了简化数据集所需的功能，将其转换成类似于训练数据集的形式。但在将数据输入模型之前，我们还有更多的预处理要做；现在就让我们开始，先快速讨论一下我们的模型期望的是什么。
- en: Our model is expecting each sample to have three dimensions; point position
    *(x, y)* and a flag indicating whether the point is the last point for its associated
    stroke. The reason for having this flag is that we are passing in a fixed-length sequence
    of size 75\. That is, each sketch will be either truncated to squeeze into this
    sequence or padded out with leading zeros to fill it. And using a flag is a way
    to add context indicating whether it is the end of the stroke or not (keeping
    in mind that our sequence represents our sketch and our sketch is made up of many
    strokes).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型期望每个样本具有三个维度；点位置*(x, y)*和一个标志位，表示该点是否为其相关笔画的最后一个点。拥有这个标志位的原因是我们传递了一个固定长度的序列，大小为75。也就是说，每个草图要么被截断以挤入这个序列，要么用前导零填充以填满它。使用标志位是一种添加上下文的方式，指示它是否是笔画的结束（记住我们的序列代表我们的草图，我们的草图由许多笔画组成）。
- en: 'Next, as usual, we normalize the inputs to a range of *0.0 - 1.0* to avoid
    having our model fluctuate while training due to large weights. The last adjustment
    is converting our absolute values into deltas, which makes a lot of sense when
    you think about it. The first reason is that we want our model to be invariant
    to the actual position of each point; that is, we could draw the same sketch side
    by side, and ideally we want these to be classified as the same class. In the
    previous chapter, we achieved this by using a CNN operating on pixel data range
    and positions as we are doing here. The second reason for using deltas rather
    than absolute values is that the delta carries more useful information than the
    absolute position, that is, direction. After implementing this, we will be ready
    to test out our model, so let''s get going; start by adding the following extension
    and method that will be responsible for this preprocessing step:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，像往常一样，我们将输入归一化到*0.0 - 1.0*的范围内，以避免在训练过程中由于权重过大而导致模型波动。最后的调整是将我们的绝对值转换为增量，这当你这么想的时候很有道理。第一个原因是，我们希望我们的模型对每个点的实际位置不变；也就是说，我们可以并排绘制相同的草图，理想情况下，我们希望这些被分类为同一类。在前一章中，我们通过使用在像素数据范围和位置上操作的CNN实现了这一点，就像我们现在所做的那样。使用增量而不是绝对值的第二个原因是，增量比绝对位置携带更多的有用信息，即方向。在实现这一点后，我们将准备好测试我们的模型，让我们开始吧；首先添加以下扩展和方法，它将负责这个预处理步骤：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here we have added the static method `preprocess` to our `StrokeSketch` class
    via an extension; within this method, we begin by setting up the buffer that will
    be passed to our model. The size of this buffer needs to fit a full sequence,
    which is calculated simply by multiplying the sequence length (`75`) with the
    number of dimensions (`3`). We then call `simplify` on the `StrokeSketch` instance
    to obtain the simplified sketch, ensuring that it closely resembles the data we
    had trained our model on.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过扩展将静态方法`preprocess`添加到`StrokeSketch`类中；在这个方法中，我们首先设置将传递给我们的模型的缓冲区。这个缓冲区的大小需要适合完整的序列，这可以通过简单地乘以序列长度（`75`）和维度数（`3`）来计算。然后我们调用`StrokeSketch`实例上的`simplify`以获得简化的草图，确保它与我们训练模型的数据非常相似。
- en: 'Next, we will iterate through each point for every stroke, normalizing the
    point and determining the value of the flag (one indicating the end of the stroke;
    otherwise it''s zero). Append the following code to your `preprocess` method:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对每一笔的每个点进行迭代，对点进行归一化，并确定标志位的值（一个表示笔画的结束；否则为0）。将以下代码添加到您的`preprocess`方法中：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We start by obtaining the minimum and maximum values, which we will use when
    normalizing each point (using the equation *x^i−min(x)/max(x)−min(x)*, where *x[i]*
    is a single point and *x* represents all points within that stroke). Then we create
    a temporary place to store the data before iterating through all our points, normalizing
    each one, and determining the value of the flag as described previously.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先获取最小值和最大值，我们将使用这些值来归一化每个点（使用方程*x^i−min(x)/max(x)−min(x)*，其中*x[i]*是一个单独的点，*x*代表该笔画内的所有点）。然后我们创建一个临时位置来存储数据，在迭代所有我们的点、归一化每个点并确定标志位的值，如之前所述之前。
- en: 'We now want to calculate the deltas of each point and finally remove the last
    point as we are unable to calculate its delta; append the following to your `preprocess`
    method:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要计算每个点的增量，并最终移除最后一个点，因为我们无法计算它的增量；将以下内容添加到您的`preprocess`方法中：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The previous code should be self-explanatory; the only notable point worth highlighting
    is that we are now dealing with a flattened array, and therefore we need to use
    a stride of `3` when traversing the data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码应该是自解释的；唯一值得强调的显著点是，我们现在处理的是一个展平的数组，因此我们在遍历数据时需要使用`3`的步长。
- en: 'One last chunk of code to add! We need to ensure that our array is equal to
    75 samples (our sequence length, that is, an array of length 225). We do this
    by either truncating the array if too large or padding it out if too small. We
    can easily do this while copying the data from our temporary array, `data`, across
    to the buffer that we will be passing to our model, `array`. Here we first calculate
    the starting index and then proceed to iterate through the whole sequence, copying
    the data across if the current index has passed our starting index, or else padding
    it with zeros. Add the following snippet to finish off your `preprocess` method:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 需要添加的最后一段代码！我们需要确保我们的数组等于75个样本（即我们的序列长度，也就是长度为225的数组）。我们可以通过截断数组（如果太大）或填充它（如果太小）来实现这一点。我们可以在从我们的临时数组`data`复制数据到我们将传递给模型的缓冲区`array`时轻松完成此操作。在这里，我们首先计算起始索引，然后遍历整个序列，如果当前索引已经超过起始索引，则复制数据，否则用零填充。将以下片段添加到完成您的`preprocess`方法：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With our `preprocess` method now complete, we are ready to test out our model.
    We will start by instantiating our model (contained within the playground) and
    then feeding in a airplane sample we have used previously, before testing with
    the other categories. Append the following code to your playground:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`preprocess`方法现在已经完成，我们准备测试我们的模型。我们将首先实例化我们的模型（包含在游乐场中），然后输入我们之前使用过的飞机样本，在测试其他类别之前进行测试。将以下代码添加到您的游乐场中：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If all goes well, your playground will output the following to the console:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，您的游乐场将在控制台输出以下内容：
- en: '![](img/a8f8d488-654f-42e6-9bff-09105f9155e2.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8f8d488-654f-42e6-9bff-09105f9155e2.png)'
- en: 'It has predicted the category of airplane and done so fairly confidently (with
    a probability of approximately 77%). Before we migrate our code into our application,
    let''s test with some other categories; we will start by implementing a method
    to handle all the leg work and then proceed to pass some samples to perform inference.
    Add the following method to your playground, which will be responsible for obtaining
    and preprocessing the sample before passing it to your model for prediction and
    then returning the results as a formatted string containing the most likely category
    and probability:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 它已经预测了飞机类别，并且相当自信地预测了（概率约为77%）。在我们将代码迁移到我们的应用程序之前，让我们测试一些其他类别；我们将首先实现一个处理所有前期工作的方法，然后传递一些样本以执行推理。将以下方法添加到您的游乐场中，它将负责在将样本传递给模型进行预测之前获取和预处理样本，然后以包含最可能类别和概率的格式化字符串返回结果：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'With most of the work now done, we are just left with the nail-biting task
    of testing that our preprocessing implementation and model are sufficiently able
    to predict the samples we pass. Let''s test with each category; add the following
    code to your playground:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在大部分工作已经完成，我们只剩下令人紧张的测试任务，即测试我们的预处理实现和模型是否足够能够预测我们传递的样本。让我们对每个类别进行测试；将以下代码添加到您的游乐场中：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output for each of these can be seen in this screenshot:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这些输出的截图如下所示：
- en: '![](img/2a2de26c-17a5-4434-b88a-f2779824fb32.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a2de26c-17a5-4434-b88a-f2779824fb32.png)'
- en: Not bad! We managed to predict all the categories correctly, albeit the truck
    was only given the probability of 41%. And interestingly, our simplified airplane
    sample was given a higher probability (84%) than its counterpart from the raw
    dataset (77%).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！我们成功预测了所有类别，尽管卡车只给出了41%的概率。而且有趣的是，我们的简化飞机样本被赋予了更高的概率（84%），而其来自原始数据集的对应样本的概率为77%。
- en: 'Out of curiosity, let''s peek at the truck sample we asked our model to predict:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，让我们看看我们要求模型预测的卡车样本：
- en: '![](img/8d845293-d905-407a-a5c3-597757130114.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d845293-d905-407a-a5c3-597757130114.png)'
- en: All due respect to the artist, but I would be pushed to predict a truck from
    this sketch, so full credit to our model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对艺术家表示敬意，但我很难从这个草图预测出一辆卡车，所以完全归功于我们的模型。
- en: 'We have now exposed our model to a variety of categories, and each one we are
    able to predict correctly, which implies that our preprocessing code has been
    satisfactorily implemented. We are now ready to migrate our code across to our
    application, but before doing so, one very last experiment. Let''s think about
    how our model has been trained and how it will be used in the context of the application.
    The model was trained on sequences that are essentially strokes, the user made
    while drawing their sketch. This is precisely how users will be interacting with
    our application; they will be sketching something with a series (or sequence)
    of strokes; each time they finish a stroke, we want to try and predict what it
    is they are trying to draw. Let''s mimic that behavior by building up a sample
    stroke by stroke, predicting after each subsequent stroke is added to evaluate
    how well the model performs in a more realistic setting. Add the following code
    to your playground:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经将我们的模型暴露于各种类别中，并且我们能够正确预测每一个类别，这意味着我们的预处理代码已经满意地实现了。我们现在准备将我们的代码迁移到我们的应用程序中，但在这样做之前，还有一个最后的实验。让我们思考一下我们的模型是如何被训练的，以及它将在应用程序的上下文中如何被使用。模型是在用户绘制草图时产生的序列（即笔触）上训练的。这正是用户将如何与我们的应用程序互动；他们将通过一系列（或序列）的笔触来绘制某物；每次他们完成一笔，我们都想尝试预测他们试图绘制的内容。让我们通过逐步构建一个样本笔触，并在每次添加后续笔触后进行预测，来模拟这种行为，以评估模型在更真实的环境中的表现。将以下代码添加到您的游乐场中：
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Nothing new has being introduced here; we are just loading in a sketch, slowly
    building it up stroke by stroke as discussed, and passing up the partial sketch
    to our model to perform inference. Here are the results, with their corresponding
    sketches to give the results more context:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有引入任何新内容；我们只是在加载一个草图，像之前讨论的那样，逐笔慢慢构建，并将部分草图传递给我们的模型进行推理。以下是结果，以及它们对应的草图，以提供更多上下文：
- en: '![](img/63f5743c-3516-4b1e-9869-0381e9ee2c7b.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63f5743c-3516-4b1e-9869-0381e9ee2c7b.png)'
- en: All reasonable predictions, possibly uncovering how a lot of people draw a **hockey
    puck**, **mouth**, and **bee**. Now, satisfied with our implementation, let's
    move on to the next section, where we will migrate this code and look at how we
    can obtain and compile a model at runtime.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的合理预测，可能揭示了许多人如何绘制**冰球**、**嘴巴**和**蜜蜂**。现在，对我们实现的结果感到满意后，让我们继续到下一部分，我们将迁移这段代码，并看看我们如何在运行时获取和编译模型。
- en: Bringing it all together
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有这些整合在一起
- en: If you haven't done already, pull down the latest code from the accompanying
    repository: [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the directory `Chapter8/Start/QuickDrawRNN` and open
    the project `QuickDrawRNN.xcodeproj`. Once loaded, you will see a project that
    should look familiar to you as it is almost a replica of the project we built
    in the previous chapter. For this reason, I won't be going over the details here,
    but feel free to refresh your memory by skimming through the previous chapter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有做，请从配套的仓库中拉取最新的代码：[https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml)。下载后，导航到目录`Chapter8/Start/QuickDrawRNN`并打开项目`QuickDrawRNN.xcodeproj`。一旦加载，您将看到一个您应该很熟悉的项目，因为它几乎是我们之前章节中构建的项目的一个复制品。因此，我这里不会详细说明，但您可以自由地通过快速浏览上一章来刷新您的记忆。
- en: Rather I want to spend some time highlighting what I consider one of the most
    important aspects of designing and building the interface between people and machine
    learning systems. Let's start with this and then move on to migrating our code
    across from our playground project.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我更想花些时间强调我认为设计和构建人与机器学习系统之间界面最重要的一个方面。让我们从这里开始，然后继续将我们的代码从游乐场项目迁移过来。
- en: I consider Quick, Draw! a great example that highlights a major responsibility
    of the designer of any interface of a machine learning system. What makes it stand
    out is not the clever preprocessing that makes it invariant to scale and translation.
    Nor is it the sophisticated architecture that can effectively learn complex sequences,
    but rather the mechanism used to capture the training data. One major obstacle
    we have in creating intelligent systems is obtaining (enough) clean and labeled
    data that we can use to train our models. *Quick, Draw!* tackled this by, I assume,
    intentionally being a tool for capturing and labeling data through the façade
    of a compelling game—compelling enough to motivate a large number of users to
    generate sufficient amounts of labeled data. Although some of the sketches are
    questionable, the sheer number of sketches dilutes these outliers.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为Quick, Draw!是一个很好的例子，它突出了任何机器学习系统界面设计师的主要责任。让它脱颖而出的不是使它对缩放和转换不变的巧妙预处理。也不是它能够有效学习复杂序列的复杂架构，而是用来捕获训练数据的机制。我们在创建智能系统时面临的一个主要障碍是获取（足够的）干净和标记的数据，我们可以用这些数据来训练我们的模型。"Quick,
    Draw!"通过，我假设，故意成为一个通过引人入胜的游戏界面来捕获和标记数据的工具——足够吸引人，足以激励大量用户生成足够数量的标记数据。尽管一些草图可能值得怀疑，但草图的数量本身稀释了这些异常值。
- en: The point is that machine learning systems are not static, and we should design
    opportunities to allow the user to correct the system, where applicable, and capture
    new data, either implicitly (with the user's consent) and/or explicitly. Allowing
    a level of transparency between the user and system and allowing the user to correct
    the model when wrong not only provides us with new data to improve our model,
    but also—just as important—assists the user in building a useful mental model
    of the system. Thus it builds some intuition around the affordances of our system,
    which help them use it correctly.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的关键在于机器学习系统不是静态的，我们应该设计机会让用户在适用的情况下纠正系统，并捕获新的数据，无论是隐式地（在用户同意的情况下）还是显式地。在用户和系统之间允许一定程度的透明度，并允许用户在模型错误时进行纠正，这不仅为我们提供了改进模型的新数据，而且——同样重要的是——帮助用户构建一个有用的系统心理模型。因此，它围绕我们系统的可用性建立了一些直觉，这有助于他们正确使用它。
- en: 'In our example project, we can easily expose the predictions and provide the
    means for the user to correct the model. But to ensure that this chapter is concise,
    we will just look at how we obtain an updated model that typically (remembering
    that Core ML is suited for inference as opposed to training) we would train off
    the device. In such a case, you would upload the data to a central server and
    fetch an updated model when available. As mentioned before, here we will look
    at the latter: how we obtain the updated model. Let''s see how.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例项目中，我们可以轻松地展示预测并提供用户纠正模型的方法。但为了确保本章简洁，我们只看看我们如何获得一个更新的模型，通常（记住Core ML适合推理而不是训练），我们会在设备上对其进行训练。在这种情况下，你会上传数据到中央服务器，并在可用时获取更新的模型。如前所述，这里我们将探讨后者：我们如何获得更新的模型。让我们看看。
- en: 'Previously I mentioned, and implied, that you would typically upload new training
    data and train your model off the device. This, of course, is not the only option
    and it''s reasonable to perform training on the device using the user''s personal
    data to tune a model. The advantage of training locally is privacy and lower latency,
    but it has the disadvantage of diminishing collective intelligence, that is, improvement
    of the model from collective behavior. Google proposed a clever solution that
    ensured privacy and allowed for collaboration. In a post titled *Federated Learning:
    Collaborative Machine Learning without Centralized Training Data*, they described
    a technique of training locally on the device using personalized data and then
    uploading only the tuned model to the server, where it would average the weights
    from the crowd before updating a central model. I encourage you to read the post
    at [https://research.googleblog.com/2017/04/federated-learning-collaborative.html](https://research.googleblog.com/2017/04/federated-learning-collaborative.html).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我提到过，并暗示过，你通常会上传新的训练数据并在设备上训练你的模型。当然，这并不是唯一的选择，使用用户的个人数据在设备上调整模型进行训练也是合理的。在本地训练的优势是隐私和低延迟，但它的缺点是会削弱集体智慧，即从集体行为中改进模型。谷歌提出了一种巧妙的解决方案，确保了隐私并允许协作。在一篇题为“*联邦学习：无需集中训练数据的协同机器学习*”的帖子中，他们描述了一种在设备上使用个性化数据本地训练的技术，然后将调整后的模型仅上传到服务器，服务器会平均来自众人的权重，在更新中央模型之前。我鼓励你阅读这篇帖子，[https://research.googleblog.com/2017/04/federated-learning-collaborative.html](https://research.googleblog.com/2017/04/federated-learning-collaborative.html)。
- en: 'As you may have come to expect when using Core ML, the bulk of the work is
    not interfacing with the framework but rather the activities before and after
    it. Compiling and instantiating a model can be done in just two lines of code,
    as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在使用Core ML时可能已经预料到的，大部分工作不是与框架接口，而是它之前和之后的活动。编译和实例化一个模型只需两行代码，如下所示：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Where `modelUrl` is a URL of a locally stored `.mlmodel` file. Passing it to
    `compileModel` will return the `.mlmodelc` file. This can be used to initialize
    an instance of `MLModel`, which provides the same capabilities as a model bundled
    with your application.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`modelUrl`是本地存储的`.mlmodel`文件的URL。将其传递给`compileModel`将返回`.mlmodelc`文件。这可以用来初始化`MLModel`的一个实例，它提供了与你的应用程序捆绑的模型相同的功能。
- en: Downloading and compilation are time consuming. So you not only want to do this
    off the main thread but also want to avoid having to perform the task unnecessary;
    that is, cache locally and only update when required. Let's implement this functionality
    now; click on the `QueryFacade.swift` file on the left-hand-side panel to bring
    it to focus in the main editor window. Then add a new extension to the `QueryFacade`
    class, which is where we will add our code responsible for downloading and compiling
    the model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 下载和编译是耗时的。因此，你不仅想在主线程之外做这件事，还想避免执行不必要的任务；也就是说，本地缓存，并在需要时才更新。现在让我们实现这个功能；点击左侧面板上的`QueryFacade.swift`文件，将其带到主编辑窗口的焦点。然后向`QueryFacade`类添加一个新的扩展，我们将在这里添加负责下载和编译模型的代码。
- en: 'Our first task is to test whether we need to download the model. We do this
    by simply checking whether we have the model and our model is considered recent.
    We will use `NSUserDefaults` to keep track of the location of the compiled model
    as well as a timestamp of when it was last updated. Add the following code to
    your extension of `QueryFacade`, which is be responsible for checking whether we
    need to download the model:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要任务是测试我们是否需要下载模型。我们通过简单地检查我们是否有模型以及我们的模型是否被认为是最近的来做到这一点。我们将使用`NSUserDefaults`来跟踪编译模型的存储位置以及最后一次更新的时间戳。将以下代码添加到负责检查我们是否需要下载模型的`QueryFacade`扩展中：
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As mentioned, we first check whether the model exists, and if so, then test
    how many days have elapsed since the model was last updated, testing this against
    some arbitrary threshold for which we consider the model to be stale.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们首先检查模型是否存在，如果存在，则测试自模型上次更新以来过去了多少天，并将这个测试与一些任意阈值进行比较，我们认为模型已经过时。
- en: 'The next method we implement will be responsible for downloading the model
    (the `.mlmodel` file); this should look familiar to most iOS developers, with
    the only notable piece of code being the use of a semaphore to make the task synchronous,
    as the calling method will be running this off the main thread. Append the following
    code to your `QueryFacade` extension:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要实现的方法将负责下载模型（`.mlmodel` 文件）；这对大多数 iOS 开发者来说应该很熟悉，唯一值得注意的是代码中使用了一个信号量来使任务同步，因为调用此方法的方法将在主线程上运行。将以下代码附加到你的
    `QueryFacade` 扩展中：
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'I have highlighted the statements related to making this task synchronous;
    essentially, calling `semaphore.wait(timeout: .distantFuture)` will hold the current
    thread until it is signaled to move on, via `semaphore.signal()`. If successful,
    this method returns the local URL of the downloaded file.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '我已经突出显示了与使此任务同步相关的语句；本质上，调用 `semaphore.wait(timeout: .distantFuture)` 将保持当前线程，直到通过
    `semaphore.signal()` 信号通知它继续。如果成功，此方法将返回已下载文件的本地 URL。'
- en: 'Our last task is to tie this all together; the next method we implement will
    be called when `QueryFacade` is instantiated (which we will add just after this).
    It will be responsible for checking whether we need to download the model, proceeding
    to download and compile if necessary, and instantiating an instance variable `model`,
    which we can use to perform inference. Append the final snippet of code to your
    `QueryFacade` extension:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后一个任务是把这些所有东西结合起来；我们接下来要实现的方法将在 `QueryFacade` 实例化时被调用（我们将在之后添加）。它将负责检查是否需要下载模型，如果需要，则继续下载和编译，并实例化一个实例变量
    `model`，我们可以使用它来进行推理。将最后的代码片段附加到你的 `QueryFacade` 扩展中：
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We start by checking whether we need to download the model, and if so, proceed
    to download and compile it:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先检查是否需要下载模型，如果是，则继续下载和编译它：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To avoid having to perform this step unnecessarily, we then save the details
    somewhere permanently, setting the model''s location and the current timestamp
    in `NSUserDefaults`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免不必要地执行此步骤，我们随后将详细信息永久保存到某个地方，设置模型的位置和当前时间戳到 `NSUserDefaults`：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, we instantiate and assign an instance of `MLModel` to our instance
    variable `model`. The last task is to update the constructor of the `QueryFacade` class
    to kick off this process when instantiated; update the `QueryFacade` `init` method
    with the following code:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将创建并分配一个 `MLModel` 实例给我们的实例变量 `model`。最后一个任务是更新 `QueryFacade` 类的构造函数，以便在实例化时启动此过程；使用以下代码更新
    `QueryFacade` 的 `init` 方法：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: At this stage, we have our model ready for performing inference; our next task
    is to migrate the code we developed in our playground to our project and then
    hook it all up. Given that we have spent the first part of this chapter discussing
    the details, I will skip the specifics here but rather include the additions for
    convenience and completeness.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们的模型已经准备好进行推理；我们的下一个任务是迁移我们在游乐场中开发的代码到我们的项目中，并将它们全部连接起来。鉴于我们已经在本章的第一部分讨论了细节，我将在这里跳过具体内容，而是为了方便和完整性包括添加的内容。
- en: 'Let''s start with our extensions to the `CGPoint` structure; add a new swift
    file to your project called `CGPointRNNExtension.swift` and add the following
    code in it:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从对 `CGPoint` 结构的扩展开始；在你的项目中添加一个新的 Swift 文件，命名为 `CGPointRNNExtension.swift`，并在其中添加以下代码：
- en: '[PRE27]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, add another new swift file to your project called `StrokeRNNExtension.swift`
    and add the following code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在你的项目中添加另一个新的 Swift 文件，命名为 `StrokeRNNExtension.swift`，并添加以下代码：
- en: '[PRE28]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, we will add a couple of methods that we implemented in the playground
    to our `StrokeSketch` class to handle the required preprocessing; start by adding
    a new `.swift` file called `StrokeSketchExtension.swift` and block out the extension
    as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将添加几个在游乐场中实现的方法到我们的 `StrokeSketch` 类中，以处理所需的预处理；首先添加一个新的 `.swift` 文件，命名为
    `StrokeSketchExtension.swift`，并按照以下方式定义扩展：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we copy and paste in the `simplify` method, which we implement in the
    playground as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将复制并粘贴 `simplify` 方法，我们将在游乐场中按如下方式实现它：
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As a reminder, this method is responsible for the preprocessing of a sequence
    of strokes, as described previously. Next, we add our static method `preprocess` to
    the `StrokeSketch` extension, which takes an instance of `StrokeSketch` and is
    responsible for putting its simplified state into a data structure that we can
    pass to our model for inference:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，此方法负责对一系列笔迹进行预处理，如前所述。接下来，我们在 `StrokeSketch` 扩展中添加我们的静态方法 `preprocess`，它接受一个
    `StrokeSketch` 实例，并负责将其简化状态放入我们可以传递给模型进行推理的数据结构中：
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: If anything looks unfamiliar, then I encourage you to revisit the previous section,
    where we delve into the details of what these methods do (and why).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有任何内容看起来不熟悉，我鼓励你重新阅读前面的章节，在那里我们深入探讨了这些方法的作用细节（以及原因）。
- en: We now have our model and functionality for preprocessing the input; our last
    task is to tie this all together. Head back to the `QueryFacade` class and locate
    the method `classifySketch`. As a reminder, this method is called via `queryCurrentSketch`,
    which in turn is triggered anytime the user completes a stroke. The method is
    expected to return a dictionary of category and probability pairs, which is then
    used to search and download related drawings of most likely categories. At this
    point, it's simply a matter of using the work we have previously done, with one
    little caveat. If you recall from previous chapters, when we imported our model
    into the project, Xcode would conveniently generate a strongly typed wrapper for
    our model and its associated inputs and outputs. A disadvantage of downloading
    and importing at runtime is that we forgo these generated wrappers and are left
    to do it manually.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了模型和预处理输入的功能；我们的最后一项任务是把这些都整合起来。回到 `QueryFacade` 类，找到 `classifySketch`
    方法。作为一个提醒，这个方法是通过 `queryCurrentSketch` 调用的，而 `queryCurrentSketch` 又会在用户完成一笔画时被触发。该方法预期返回一个包含类别和概率对的字典，然后用于搜索和下载最可能类别的相关绘画。在这个阶段，这只是一个使用我们之前完成的工作的问题，只有一个小的注意事项。如果你还记得前面的章节，当我们把模型导入到项目中时，Xcode
    会方便地为我们模型的输入和输出生成一个强类型包装器。在运行时下载和导入的一个缺点是我们放弃了这些生成的包装器，不得不手动完成。
- en: Starting backwards, after making the prediction, we are expecting an instance
    of `MLFeatureProvider` to be returned, which in turn has a method called `featureValue`.
    This returns an instance of `MLFeatureValue` for a given output key (`classLabelProbs`).
    The returned instance of `MLFeatureValue` exposes properties set by the model
    during inference; here we are interested in the `dictionaryValue` property of
    type `[String:Double]` (category and its associated probability).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 从后往前，在做出预测之后，我们期望返回一个 `MLFeatureProvider` 的实例，它有一个名为 `featureValue` 的方法。这个方法为给定的输出键（`classLabelProbs`）返回一个
    `MLFeatureValue` 的实例。返回的 `MLFeatureValue` 实例暴露了模型在推理期间设置的属性；这里我们感兴趣的是类型为 `[String:Double]`
    的 `dictionaryValue` 属性（类别及其相关的概率）。
- en: Obviously, to obtain this output, we need to call `predict` on our model, which
    is expecting an instance adhering to the `MLFeatureProvider` protocol that was
    generated for us, as mentioned previously. Given that in most instances you will
    have access and knowledge of the model, the easiest way to generate this wrapper
    is to import the model and extract the generated input, which is exactly what
    we will do.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，为了获得这个输出，我们需要在我们的模型上调用 `predict` 方法，该模型期望一个符合我们之前提到的 `MLFeatureProvider`
    协议的实例。鉴于在大多数情况下，你将能够访问和了解模型，生成这个包装器最简单的方法是导入模型并提取生成的输入，这正是我们将要做的。
- en: 'Locate the file `CoreMLModels/Chapter8/quickdraw.mlmodel` in the accompanying
    repository [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml),
    and drag the file into your project as we have done in previous chapters. Once
    imported, select it from the left-hand-side panel and click on the arrow button
    within the Model Class section, as shown in the following screenshot:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在配套的仓库 [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml)
    中找到文件 `CoreMLModels/Chapter8/quickdraw.mlmodel`，并将其拖入你的项目中，就像我们在前面的章节中所做的那样。一旦导入，从左侧面板中选择它，然后在模型类部分点击箭头按钮，如图所示：
- en: '![](img/53a57d2c-ab0c-45af-9142-23417cbc2556.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/53a57d2c-ab0c-45af-9142-23417cbc2556.png)'
- en: 'This will open up the generated classes; locate the class `quickdrawInput`
    and copy and paste it to your `QueryFacade.swift`, ensuring that it''s outside
    the `QueryFacade` class (or extensions). Because we are only concerned with the
    `strokeSeq` input, we can strip all other variables; clean it up such that you
    are left with something like the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开生成的类；找到类 `quickdrawInput` 并将其复制粘贴到你的 `QueryFacade.swift` 文件中，确保它位于 `QueryFacade`
    类（或扩展）之外。因为我们只关心 `strokeSeq` 输入，我们可以移除所有其他变量；清理后，你将得到如下所示的内容：
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We are finally ready to perform inference; return to the `classifySketch` method
    within the `QueryFacade` class and add the following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好进行推理了；回到 `QueryFacade` 类中的 `classifySketch` 方法，并添加以下代码：
- en: '[PRE33]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: No doubt most of this will look familiar to you; we start by extracting the
    features via the `preprocess` method we implemented at the start of this chapter.
    Once we have obtained these features, we wrap them in an instance of `quickdrawInput`,
    before passing them to our model's `prediction` method to perform inference. If
    successful, we are returned the output, with which we proceed to extract the appropriate
    output, as discussed previously. Finally we sort the results before returning
    them to the caller.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，其中大部分对你来说都很熟悉；我们首先通过本章开头实现的`preprocess`方法提取特征。一旦我们获得了这些特征，我们就将它们包装在`quickdrawInput`的一个实例中，然后再将它们传递给模型的`prediction`方法进行推理。如果成功，我们将返回输出，然后我们继续提取适当输出，如前所述。最后，我们在返回给调用者之前对结果进行排序。
- en: 'With that complete, you are now in a good position to test. Build and deploy
    to the simulator or device, and if everything goes as planned, you should be able
    to test the accuracy of your mode (or drawing, depending on how you look at it):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，你现在处于一个很好的测试位置。构建并部署到模拟器或设备上，如果一切按计划进行，你应该能够测试你模式（或绘制，取决于你如何看待它）的准确性：
- en: '![](img/2cdd1326-a576-4740-aead-489a0e951658.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2cdd1326-a576-4740-aead-489a0e951658.png)'
- en: Let's wrap up this chapter by reviewing what we have covered.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回顾我们已经涵盖的内容来结束本章。
- en: Summary
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we revisited a previous problem (sketch recognition) but used
    a different dataset and different approach. Previously, we tackled the problem
    using CNN, but in this chapter, we identified nuances of how the data was collected,
    which in turn allowed us to take a different approach using an RNN. As usual,
    most of the effort was spent in preparing the data for the model. This, in doing
    so, highlighted some techniques we can use to make our data invariant to scale
    and translation, as well as the usefulness of reducing details of the inputs (through
    simplification) to assist our model in more easily finding patterns.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了一个先前的问题（草图识别），但使用了不同的数据集和不同的方法。之前，我们使用CNN来解决这个问题，但在这章中，我们识别了数据收集的细微差别，这反过来又使我们能够采用不同的方法使用RNN。像往常一样，大部分努力都花在为模型准备数据上。这样做突出了我们可以用来使数据对缩放和转换不变的技术，以及减少输入细节（通过简化）以帮助我们的模型更容易找到模式的有用性。
- en: Finally, we highlighted an important aspect of designing interfaces for machine
    learning systems, that is, adding a layer of transparency and control for the
    user to help them build a useful mental model of the system and improve the model
    through explicit user feedback, such as corrections.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们强调了为机器学习系统设计界面的重要方面，即添加一层透明度和控制层，以帮助用户构建一个有用的心理模型，并通过显式的用户反馈（如更正）来改进模型。
- en: 'Let''s continue our journey into the world of machine learning applications
    and dive into the next chapter, where we will look at our final visual application:
    image segmentation.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续我们的机器学习应用之旅，深入下一章，我们将探讨我们的最终视觉应用：图像分割。
