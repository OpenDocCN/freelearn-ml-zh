- en: Chapter 4. Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, another supervised method is introduced: classification. We
    will introduce the simplest classifier, the Logistic Regressor, which shares the
    same foundations as the Linear Regressor, but it targets classification problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following chapter, you''ll find:'
  prefs: []
  type: TYPE_NORMAL
- en: A formal and mathematical definition of the classification problem, for both
    binary and multiclass problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to evaluate classifier performances—that is, their metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The math behind Logistic Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A revisited formula for SGD, specifically built for Logistic Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The multiclass case, with Multiclass Logistic Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a classification problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the name Logistic Regression suggests a regression operation, the
    goal of Logistic Regression is classification. In a very rigorous world such as
    statistics, why is this technique ambiguously named? Simple, the name is not wrong
    at all, and it makes perfect sense: it just requires a bit of an introduction
    and investigation. After that you''ll fully understand why it''s named Logistic
    Regression, and you''ll no longer think that it''s a wrong name.'
  prefs: []
  type: TYPE_NORMAL
- en: First, let's introduce what a classification problem is, what a classifier is,
    how it operates, and what its output is.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we presented regression as the operation of estimating
    a continuous value in a target variable; mathematically speaking, the predicted
    variable is a real number in the range (*−∞*, *+∞*). Classification, instead,
    predicts a class, that is, an index in a finite set of classes. The simplest case
    is named binary classification, and the output is typically a Boolean value (`true`/`false`).
    If the class is `true` the sample is typically called a *positive sample*; otherwise
    it's a *negative sample*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To state some examples, here are some questions that refer to a binary classification
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Is this email spam?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is my house worth at least $200,000?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the banner/email clicked/opened by the user?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the current document about finance?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a person in the image? Is it a man or a woman?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Putting a threshold on the output of a regression problem, to determine whether
    the value is greater or lower than a fixed threshold, is actually a binary classification
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: When the output can have multiple values (that is, the predicted label is a
    categorical variable), the classification is named a multiclass one. Usually,
    the possible labels are named levels or classes, and the list of them should be
    finite and known in advance (or else it will be an unsupervised problem, not a
    supervised one).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of multiclass classification problems are:'
  prefs: []
  type: TYPE_NORMAL
- en: Which kind of flower is this?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the primary topic of this webpage?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which kind of network attack am I experiencing?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which digit/letter is drawn in the image?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Formalization of the problem: binary classification'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start now with the simplest type of classification: the **binary classification**.
    Don''t worry; in a few pages things are going to be more complex when we focus
    on the multiclass classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, the generic observation is an *n*-dimensional feature vector (*x[i]*)
    paired with its label: the generic *i*-th can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Formalization of the problem: binary classification](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The model underneath the classifier is a function and is called a **classification
    function**, which can be either linear or non linear. The form of the function
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Formalization of the problem: binary classification](img/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: During the prediction task, the classification function is applied to a new
    feature vector, and the output of the classifier represents the class to which
    the input sample is classified, that is, the predicted label. A perfect classifier
    predicts, for every possible input, the correct class `y`.
  prefs: []
  type: TYPE_NORMAL
- en: The feature vector *x* should comprise numbers. If you're dealing with categorical
    features (such as gender, membership, and words), you should be able to take that
    variable to one or more numeric variables (usually binary). We'll see more about
    this point later on in the book, in [Chapter 5](part0035_split_000.html#11C3M2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 5. Data Preparation"), *Data Preparation*, which is devoted to data preparation
    of variables into the most suitable form for regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'To have a visual understanding of what''s going on, let''s consider now a binary
    classification problem, where every feature has two dimensions (a 2-D problem).
    Let''s first define the input dataset; here the `make_classifier` method of the
    Scikit-learn library comes in very handy. It creates a dummy dataset for classification,
    providing the number of classes, the dimensionality of the problem, and the number
    of observations as parameters. Additionally, you should specify that each feature
    is informative (and there are no redundancies) and each class is composed of a
    single cluster of points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Formalization of the problem: binary classification](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Assessing the classifier's performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand if a classifier is a good one, or equivalently, to identify the
    classifier with the best performance in the classification task, we need to define
    some metrics. There is no single metric since the classification goal can be different—for
    example, the correctness or completeness of a defined label, minimization of the
    number of misclassifications, correct ordering in respect of the likelihood of
    having a certain label, and quite a few others. All the measures can be derived
    from the classification matrix after having applied a cost matrix: the outcome
    highlights which errors are more expensive and which are not so expensive in terms
    of results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics exposed here can be used for both binary and multiclass classification.
    Although it is not a measure of performance, let''s start from the confusion matrix,
    the simplest metric that gives us a visual impact of the correct classifications
    and the misclassification errors for each class. On the rows there are the true
    labels, on the column the predicted one. Let''s also create a dummy label set
    and a predicted set for the following experiments. In our example the original
    labels are six `0` and four `1`; the classifier misclassified entries are two
    `0` and one `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now create the confusion matrix for this experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'From this matrix we can extract some evidence:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of samples is `10` (the sum of the whole matrix).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of samples labeled `0` in the original is `6`; `1`s are `4` (the
    sum for the lines). These numbers are named support.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of samples labeled `0` in the predicted dataset is `5`; `1`s are
    `5` (the sum as columns).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correct classifications are `7` (the sum of the diagonal).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Misclassifications are `3` (the sum of all numbers not on the diagonal).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A perfect classification example would have had all the numbers on the diagonal,
    and `0` elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'This matrix can also be represented graphically, using a heatmap. This is a
    very impactful representation, especially when dealing with multiclass problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Assessing the classifier''s performance](img/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The first measure we're going to explore to evaluate the classifier's performance
    is accuracy. Accuracy is the percentage of correct classifications, over the total
    number of samples. You can derive this error measure directly from the confusion
    matrix by dividing the sum over the diagonal by the sum of the elements in the
    matrix. The best possible accuracy is `1.0` and the worst one is `0.0`. In the
    preceding example, accuracy amounts to *7/10 = 0.7*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Python, this becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Another very popular measure is precision. It considers only one label and counts
    the percentage of correct classifications on that label. While considering our
    label "1", the precision is the number in the bottom right of the confusion matrix,
    divided by the sum of the elements in the second column—that is, *3/5=0.6*. Values
    are bounded between 0 and 1, where 1 is the best possible result and 0 the worst.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this function in Scikit-learn expects a binary input, where only
    the class under examination is marked as `true` (this is sometime named a *class
    indicator*). To extract a precision score for each label, you should then make
    each class a binary vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Paired with precision you''ll frequently find another error measure, recall.
    If precision is about the quality of what you got (that is, the quality of the
    results marked with the label `1`), recall is about the quality of what you could
    have gotten—that is, how many instances of `1` you''ve been able to extract properly.
    Also, here, this measure is class-based, and to compute the recall score for class
    `1` you should divide the bottom right number in the confusion matrix by the sum
    of the second line, that is, *3/4=0.75*. Recall is bounded `0` and `1`; the best
    score is `1` and means that all the instances of "1" in the original dataset have
    been correctly classified as "1"; a score equal to `0` means that no "1"s have
    been classified properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Precision and recall are two metrics that indicate how well the classifier performed
    on a class. Merge their score, using a harmonic average, and you'll get the comprehensive
    f1-score, helping you to figure out at a glance the performance on both error
    measures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Assessing the classifier''s performance](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In Python this is easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In conclusion, if there are so many error scores, which is the best to use?
    The solution is not very easy, and often it is better to have and evaluate the
    classifier on all of them. How can we do that? Is it a long function to write?
    No, Scikit-learn here comes to help us here, providing a method to compute all
    these scores for each class (this is really handy). Here is how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Assessing the classifier''s performance](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Defining a probability-based approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's gradually introduce how logistic regression works. We said that it's a
    classifier, but its name recalls a regressor. The element we need to join the
    pieces is the probabilistic interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a binary classification problem, the output can be either "0" or "1". What
    if we check the probability of the label belonging to class "1"? More specifically,
    a classification problem can be seen as: given the feature vector, find the class
    (either 0 or 1) that maximizes the conditional probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining a probability-based approach](img/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here''s the connection: if we compute a probability, the classification problem
    *looks like* a regression problem. Moreover, in a binary classification problem,
    we just need to compute the probability of membership of class "1", and therefore
    it looks like a well-defined regression problem. In the regression problem, classes
    are no longer "1" or "0" (as strings), but 1.0 and 0.0 (as the probability of
    belonging to class "1").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now try fitting a multiple linear regressor on a dummy classification
    problem, using a probabilistic interpretation. We reuse the same dataset we created
    earlier in this chapter, but first we split the dataset into train and test sets,
    and we convert the `y` vector to floating point values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Defining a probability-based approach](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, with these few methods, we split the datasets into two folds, (train and
    test) and we converted all the numbers in the *y* array to floating point. In
    the last cell, we effectively check the operation. Now, if *y = 1.0*, it means
    that the relative observation is 100% class "1"; *y = 0.0* implies that the observation
    is 0% class "1". Since it's a binary classification task, it implies that it's
    also 100% class "0" (note that the percentages here refer to probability).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now proceed with the regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![Defining a probability-based approach](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The output—that is, the prediction of the regressor—should be the probability
    of belonging to class 1\. As you can see in the last cell output, that''s not
    a proper probability, since it contains values below 0 and greater than 1\. The
    simplest idea here is clipping results between 0 and 1, and putting a threshold
    at `0.5`: if the value is *>0.5*, then the predicted class is "1"; otherwise the
    predicted class is "0".'
  prefs: []
  type: TYPE_NORMAL
- en: This procedure works, but we can do better. We've seen how easy it is to transit
    from a classification problem to a regression one, and then go back with predicted
    values to predicted classes. With this process in mind, let's again start the
    analysis, digging further in its core algorithm while introducing some changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our dummy problem, we applied the linear regression model to estimate the
    probability of the observation belonging to class "1". The regression model was
    (as we''ve seen in the previous chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining a probability-based approach](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we''ve seen that the output is not a proper probability. To be a probability,
    we need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Bound the output between 0.0 and 1.0 (clipping).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the prediction is equal to the threshold (we chose 0.5 previously), the probability
    should be 0.5 (symmetry).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To have both conditions `true`, the best we could do is to send the output of
    the regressor through a sigmoid curve, or an S-shaped curve. A sigmoid generically
    maps values in R (the field of real numbers) to values in the range `[0,1]`, and
    its value when mapping `0` is `0.5`.
  prefs: []
  type: TYPE_NORMAL
- en: On the basis of such a hypothesis, we can now write (for the first time) the
    formula underneath the logistic regression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining a probability-based approach](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note also that the weight `W[0]` (the bias weight) will take care of the misalignment
    of the central point of the sigmoid (it's in 0, whereas the threshold is in 0.5).
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s all. That''s the logistic regression algorithm. There is just one thing
    missing: why logistic? What''s the *σ* function?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, the answer to both questions is trivial: the standard choice of sigma
    is the logistic function, also named the inverse-logit function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining a probability-based approach](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Although there are infinite functions that satisfy the sigmoid constraints,
    the logistic has been chosen because it's continuous, easily differentiable, and
    quick to compute. If the results are not satisfactory, always consider that, by
    introducing a couple of parameters, you can change the steepness and the center
    of the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sigmoid function is quickly drawn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Defining a probability-based approach](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can immediately see that, for a very low **t**, the function tends to the
    value **0**; for a very high **t**, the function tends to be **1**, and, in the
    center, where **t** is **0**, the function is **0.5**. Exactly the sigmoid function
    we were looking for.
  prefs: []
  type: TYPE_NORMAL
- en: More on the logistic and logit functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, why did we use the inverse of the logit function? Isn''t there anything
    better than that? The answer to this question comes from statistics: we''re dealing
    with probabilities, and the logit function is a great fit. In statistics, the
    logit function applied to a probability, returns the log-odds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![More on the logistic and logit functions](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This function transforms numbers from range `[0,1]` to numbers in (*−∞*, *+∞*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see if you can intuitively understand the logic behind the selection
    of the inverse-logit function as the sigmoid function for the logistic regression.
    Let''s first write down the probabilities for both classes, according to this
    logistic regression equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![More on the logistic and logit functions](img/00070.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now compute the log-odds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![More on the logistic and logit functions](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, not surprisingly, that''s also the **logit** function, applied to
    the probability of getting a "1":'
  prefs: []
  type: TYPE_NORMAL
- en: '![More on the logistic and logit functions](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The chain of our reasoning is finally closed, and here''s why logistic regression
    is based on, as the definition implies, the logistic function. Actually, logistic
    regression is a model of the big category of the GLM: the generalized linear model.
    Each model has a different function, a different formulation, a different operative
    hypothesis, and not, surprisingly, a different goal.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see some code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we start with the dummy dataset we created at the beginning of the chapter.
    Creating and fitting a logistic regressor classifier is really easy: thanks to
    Scikit-learn, it just requires a couple of lines of Python code. As for regressors,
    to train the model you need to call the `fit` method, whereas for predicting the
    class you just need to call the `predict` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Let''s see some code](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that here we''re not making a regression operation; that''s why the label
    vector must comprise integers (or class indexes). The report shown at the bottom
    shows a very accurate prediction: all the scores are close to 1 for all classes.
    Since we have `33` samples in the test set, `0.97` means just one case misclassified.
    That''s almost perfect in this dummy example!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try to dig under the hood even more. First, we would like to check
    the decision boundary of the classifier: which part of the bidimensional space
    has points being classified as "1"; and where are the "0"s? Let''s see how you
    can visually see the decision boundary here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Let''s see some code](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The separation is almost vertical. "1"s are on the left (yellow) side; "0"s
    on the right (red). From the earlier screenshot, you can immediately perceive
    the misclassification: it''s pretty close to the boundary. Therefore, its probability
    of belonging to class "1" will be very close to 0.5.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see the bare probabilities and the weight vector. To compute the
    probability, you need to use the `predict_proba` method of the classifier. It
    returns two values for each observation: the first is the probability of being
    of class "0"; the second the probability for class "1". Since we''re interested
    in class "1", here we just select the second value for all the observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Let''s see some code](img/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the screenshot, pure yellow and pure red are where the predicted probability
    is very close to 1 and 0 respectively. The black dot is the origin *(0,0)* of
    the Cartesian bidimensional space, and the arrow is the representation of the
    weight vector of the classifier. As you can see, it''s orthogonal to the decision
    boundary, and it''s *pointing* toward the "1" class. The weight vector is actually
    the model itself: if you need to store it in a file, consider that it''s just
    a couple of floating point numbers and nothing more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, I''d want to focus on speed. Let''s now see how much time the classifier
    takes to train and to predict the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Although timings are computer-specific (here we're training it and predicting
    using the full 100-point dataset), you can see that Logistic Regression is a very
    fast technique both during training and when predicting the class and the probability
    for all classes.
  prefs: []
  type: TYPE_NORMAL
- en: Pros and cons of logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression is a very popular algorithm because of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s linear: it''s the equivalent of the linear regression for classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's very simple to understand, and the output can be the most likely class,
    or the probability of membership.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It''s simple to train: it has very few coefficients (one coefficient for each
    feature, plus one bias). This makes the model very small to store (you just need
    to store a vector of weights).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It''s computationally efficient: using some special tricks (see later in the
    chapter), it can be trained very quickly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has an extension for multiclass classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unfortunately, it''s not a perfect classifier and has some drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s often not very performant, compared to most advanced algorithms, because
    it tends to underfit (no flexibility: the boundary has to be a line or a hyperplane)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It''s linear: if the problem is non-linear, there is no way to properly fit
    this classifier onto the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisiting gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we introduced the gradient descent technique to speed
    up processing. As we''ve seen with Linear Regression, the fitting of the model
    can be made in two ways: closed form or iterative form. Closed form gives the
    best possible solution in one step (but it''s a very complex and time-demanding
    step); iterative algorithms, instead, reach the minima step by step with few calculations
    for each update and can be stopped at any time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient descent is a very popular choice for fitting the Logistic Regression
    model; however, it shares its popularity with Newton''s methods. Since Logistic
    Regression is the base of the iterative optimization, and we''ve already introduced
    it, we will focus on it in this section. Don''t worry, there is no winner or any
    best algorithm: all of them can reach the very same model eventually, following
    different paths in the coefficients'' space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we should compute the derivate of the loss function. Let''s make it
    a bit longer, and let''s start deriving the logistic function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting gradient descent](img/00076.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Its first-order derivative is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting gradient descent](img/00077.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is another reason why logistic regression used the logistic function:
    its derivate is computationally light. Now, let''s assume that the training observations
    are independent. Computing the likelihood, with respect to the set of weights,
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting gradient descent](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in the last row, we used a trick based on the fact that *y[i]* can
    be either 0 or 1\. If *y[i]=1*, only the first factor of the multiplication is
    computed; otherwise it's the second factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now compute the log-likelihood: it will make things easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting gradient descent](img/00079.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we have two considerations to make. First: the SGD works with one point
    at a time; therefore, the log-likelihood, step-by-step, is just a function of
    one point. Hence, we can remove the sum over all the points, and name *(x,y)*
    the point under observation. Second, we need to maximize the likelihood: to do
    so, we need to extract its partial derivative with respect to the generic *k*-th
    coefficient of *W*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The math here becomes a bit complex; therefore we will just write the last
    result (this is the thinking we will use in our model). Deriving and understanding
    the equations in the middle is given to the reader as homework:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting gradient descent](img/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we''re trying to maximize the likelihood (and its log version), the right
    formula for updating the weights is the Stochastic Gradient Ascent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting gradient descent](img/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'That''s the generic formula. In our case, the update step for each coefficient
    composing *W* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting gradient descent](img/00082.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *(x,y)* is the (stochastic) random observation chosen for the update step,
    and the learning step.
  prefs: []
  type: TYPE_NORMAL
- en: To see a real example of what the SGD produces, check the last section of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The extension to Logistic Regression, for classifying more than two classes,
    is Multiclass Logistic Regression. Its foundation is actually a generic approach:
    it doesn''t just work for Logistic Regressors, it also works with other binary
    classifiers. The base algorithm is named **One-vs-rest**, or **One-vs-all**, and
    it''s simple to grasp and apply.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s describe it with an example: we have to classify three kinds of flowers
    and, given some features, the possible outputs are three classes: `f1`, `f2`,
    and `f3`. That''s not what we''ve seen so far; in fact, this is not a binary classification
    problem. Instead, it seems very easy to break down this problem into three simpler
    problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem #1**: Positive examples (that is, the ones that get the label "1")
    are `f1`; negative examples are all the others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Problem #2**: Positive examples are `f2`; negative examples are `f1` and
    `f3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Problem #3**: Positive examples are `f3`; negative examples are `f1` and
    `f2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all three problems, we can use a binary classifier, as Logistic Regressor,
    and, unsurprisingly, the first classifier will output *P(y = f1|x)*; the second
    and the third will output respectively *P(y = f2|x) and P(y = f3|x)*.
  prefs: []
  type: TYPE_NORMAL
- en: To make the final prediction, we just need to select the classifier that emitted
    the highest probability. Having trained three classifiers, the feature space is
    not divided in two subplanes, but according to the decision boundary of the three
    classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach of One-vs-all is very convenient, in fact:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of classifiers to fit is exactly the same as the number of classes.
    Therefore, the model will be composed by *N* (where *N* is the number of classes)
    weight vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, this operation is embarrassingly parallel and the training of the
    *N* classifiers can be made simultaneously, using multiple threads (up to *N*
    threads).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the classes are balanced, the training time for each classifier is similar,
    and the predicting time is the same (even for unbalanced classes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a better understanding, let''s make a multiclass classification example,
    creating a dummy three-class dataset, splitting it as training and test sets,
    training a Multiclass Logistic Regressor, applying it on the training set, and
    finally visualizing the boundaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Multiclass Logistic Regression](img/00083.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Multiclass Logistic Regression](img/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Multiclass Logistic Regression](img/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: On this dummy dataset, the classifier has achieved a perfect classification
    (precision, recall, and f1-score are all 1.0). In the last picture, you can see
    that the decision boundaries define three areas, and create a non-linear division.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s observe the first feature vector, its original label, and its
    predicted label (both reporting class "0"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: To get its probabilities to belong to each of the three classes, you can simply
    apply the `predict_proba` method (exactly as in the binary case), and the classifier
    will output the three probabilities. Of course, their sum is 1.0, and the highest
    value is, naturally, one for class "0".
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: An example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now look at a practical example, containing what we've seen so far in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Our dataset is an artificially created one, composed of 10,000 observations
    and 10 features, all of them informative (that is, no redundant ones) and labels
    "0" and "1" (binary classification). Having all the informative features is not
    an unrealistic hypothesis in machine learning, since usually the feature selection
    or feature reduction operation selects non-related features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, we'll show you how to use different libraries, and different modules, to
    perform the classification task, using logistic regression. We won't focus here
    on how to measure the performance, but on how the coefficients can compose the
    model (what we've named in the previous chapters).
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we will use Statsmodel. After having loaded the right modules,
    we need to add an additional feature to the input set in order to have the bias
    weight `W[0]`. After that, training the model is really simple: we just need to
    instantiate a `logit` object and use its `fit` method. Statsmodel will train the
    model and will show whether it was able to train a model (*Optimization terminated
    successfully*) or it failed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a detailed insight into the model, use the method summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![An example](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Two tables are returned: the first one is about the dataset and model performances;
    the second is about the weights of the model. Statsmodel provides a lot of information
    on the model; some of it has been shown in [Chapter 2](part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 2. Approaching Simple Linear Regression"), *Approaching Simple Linear
    Regression*, about a trained regressor. Here, instead, we have a brief description
    of the information shown for the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Converged**: This tells whether the classification model has reached convergence
    while being trained. Use the model only if this parameter is `true`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log-Likelihood**: This is the logarithm of the likelihood. It''s what we
    previously named.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LL-Null**: This is the Log-Likelihood when only the intercept is used as
    a predictor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLR p-value**: This is the chi-squared probability of getting a log-likelihood
    ratio statistically greater than LLR. Basically, it shows how the model is better
    than guessing with a constant value. LLR is the log-likelihood ratio, that is,
    the logarithm of the likelihood of the null model (intercept only), divided by
    the likelihood of the alternate model (full model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pseudo R-squared**: This can be seen as the proportion of the total variability
    unexplained by the model. It''s computed as *1-Log-likelihood/LL-Null*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As for the coefficient table, there is one line for each coefficient: `const`
    is the weight associated to the intercept term (that is, the bias weight); `x1`,
    `x2`, … `x10` are the weights associated to the 10 features composing the model.
    For each of them there are a few values:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coef**: This is the weight in the model associated to that feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Std err**: This is the standard error of the coefficient, that is its (predicted)
    standard deviation (across all observations) divided by the square root of the
    sample size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Z**: This is the ratio between the standard error and the coefficient (it''s
    the stat t-value).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P>|z|**: This is the probability of obtaining a t-value greater than z, while
    sampling from the same population.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[95.0% Conf. Int.]**: This is the interval where, with 95% confidence, the
    real value of the coefficient is. It is computed as *coefficient +/- 1.96 * std
    err*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An alternate method to obtain the same result (often used when the model contains
    a small number of features) is to write down the formula involved in the regression.
    This is possible thanks to the Statsmodel formula API, which makes the fitting
    operation similar to what you would use in R. We first need to name the features,
    then we write down the formula (using the names we set), and lastly we fit the
    model. With this method, the intercept term is automatically added to the model.
    Its output, then, is the same as the preceding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s change our approach, and let''s now fully implement the stochastic gradient
    descent formula. Each piece of the formula has a function, and the `main` function
    is optimization. With respect to the linear regression, here the big difference
    is the `loss` function, which is the logistic (that is, the sigmoid):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The coefficients produced with the stochastic gradient descent approach are
    the same as the ones Statsmodels derived previously. The code implementation,
    as seen before, is not best optimized; though reasonably efficient at working
    out the solution, it''s just an instructive way to understand how SGD works under
    the hood in the logistic regression task. Try to play around, checking the relation
    between the number of iterations, alpha, eta, and the final outcome: you''ll understand
    how these parameters are connected, as well as how to select the best settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we switch to the Scikit-learn library, and its implementation of Logistic
    Regression. Scikit-learn has two implementations: one based on the *classic* solution
    of the logistic regression optimization, and the other one based on a quick SGD
    implementation. We''ll explore them both.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we start with the classic Logistic Regression implementation. The training
    is really simple, and just requires a couple of parameters. We will set its parameters
    to the extreme, so the solution is not regularized (C is very high) and the stopping
    criterion on tolerance is very low. We do that in this example to get the same
    weights in the model; in a real experiment, these parameters will guide hyperparameter
    optimization. For more information about regularization, please refer to [Chapter
    6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6 "Chapter 6. Achieving
    Generalization"), *Achieving Generalization*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![An example](img/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As the last model, we try the Scikit-learn implementation of the SGD. Getting
    the same weights is really tricky, since the model is really complex, and the
    parameters should be optimized for performance, not for obtaining the same result
    as for the closed form approach. So, use this example to understand the coefficients
    in the model, but not for training a real-world model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![An example](img/00088.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've seen in this chapter how to build a binary classifier based on Linear
    Regression and the logistic function. It's fast, small, and very effective, and
    can be trained using an incremental technique based on SGD. Moreover, with very
    little effort (the One-vs-Rest approach), the Binary Logistic Regressor can become
    multiclass.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will focus on how to prepare data: to obtain the maximum
    from the supervised algorithm, the input dataset must be carefully cleaned and
    normalized. In fact, real world datasets can have missing data, errors, and outliers,
    and variables can be categorical and with different ranges of values. Fortunately,
    some popular algorithms deal with these problems, transforming the dataset in
    the best way possible for the machine learning algorithm.'
  prefs: []
  type: TYPE_NORMAL
