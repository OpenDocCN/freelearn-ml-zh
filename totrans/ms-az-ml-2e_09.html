<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer197">
			<h1 id="_idParaDest-113"><em class="italic"><a id="_idTextAnchor112"/>Chapter 7</em>: Advanced Feature Extraction with NLP</h1>
			<p>In the previous chapters, we learned about many standard transformation and preprocessing approaches within the Azure Machine Learning service as well as typical labeling techniques using the Azure Machine Learning Data Labeling service. In this chapter, we want to go one step further to extract semantic features from textual and categorical data—a problem that users often face when training ML models. This chapter will describe the foundations of feature extraction with <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>). This will help you to practically implement semantic embeddings using NLP for your ML pipelines.</p>
			<p>First, we will take a look at the differences between <em class="italic">textual</em>, <em class="italic">categorical</em>, <em class="italic">nominal</em>, and <em class="italic">ordinal</em> data. This classification will help you to decide the best feature extraction and transformation technique per feature type. Later, we will look at the most common transformations for categorical values, namely <strong class="bold">label encoding</strong> and <strong class="bold">one-hot encoding</strong>. Both techniques will be compared and tested to understand the different use cases and applications for both techniques.</p>
			<p>Next, we will tackle the numerical embedding of textual data. To achieve this, we will build a simple <strong class="bold">bag-of-words</strong> model, using a <strong class="bold">count vectorizer</strong>. To sanitize the input, we will build an NLP pipeline consisting of a <strong class="bold">tokenizer</strong>, stop word removal, <strong class="bold">stemming</strong>, and <strong class="bold">lemmatization</strong>. We will learn how these different techniques affect a sample dataset step by step.</p>
			<p>Following this, we will replace the word count method with a much better word frequency weighting approach—the <strong class="bold">Term Frequency-Inverse Document Frequency</strong> (<strong class="bold">TF-IDF</strong>) algorithm. This will help you to compute the importance of words when given a whole corpus of documents by weighting the occurrence of a term in one document over the frequency in the corpus. Additionally, we will look at <strong class="bold">Singular Value Decomposition</strong> (<strong class="bold">SVD</strong>) for reducing the size of the term dictionary. As a next step, we will improve the term embedding quality by leveraging word semantics, and we will look under the hood of semantic embeddings such as <strong class="bold">Global Vectors</strong> (<strong class="bold">GloVe</strong>) and <strong class="bold">Word2Vec</strong>.</p>
			<p>In the last section, we will take a look at current state-of-the-art language models that are based on sequence-to-sequence deep neural networks with over 100 million parameters. We will train a small end-to-end model using <strong class="bold">Long Short-Term Memory </strong>(<strong class="bold">LSTM</strong>), perform word embedding and sentiment analysis using <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>), and compare both custom solutions to Azure's text analytics capabilities in Cognitive Services.</p>
			<p>In this chapter, the following topics will be covered:</p>
			<ul>
				<li>Understanding categorical data</li>
				<li>Building a simple bag-of-words model</li>
				<li>Leveraging term importance and semantics</li>
				<li>Implementing end-to-end language models</li>
			</ul>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor113"/>Technical requirements</h1>
			<p>In this chapter, we will use the following Python libraries and versions to create categorical encodings, create semantic embeddings, train an end-to-end model, and perform classic NLP preprocessing steps:</p>
			<ul>
				<li><strong class="source-inline">azureml-sdk 1.34.0 </strong></li>
				<li><strong class="source-inline">azureml-widgets 1.34.0 </strong></li>
				<li><strong class="source-inline">tensorflow 2.6.0 </strong></li>
				<li><strong class="source-inline">numpy 1.19.5 </strong></li>
				<li><strong class="source-inline">pandas 1.3.2 </strong></li>
				<li><strong class="source-inline">scikit-learn 0.24.2 </strong></li>
				<li><strong class="source-inline">nltk 3.6.2 </strong></li>
				<li><strong class="source-inline">genism 3.8.3 </strong></li>
			</ul>
			<p>Similar to previous chapters, you can execute this code using either a local Python interpreter or a notebook environment hosted in Azure Machine Learning.</p>
			<p>All code examples in this chapter can be found in the GitHub repository for this book: <a href="https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07">https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07</a>.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor114"/>Understanding categorical data</h1>
			<p><strong class="bold">Categorical data</strong> comes in many forms, shapes, and meanings. It is extremely important to understand <a id="_idIndexMarker942"/>what type of data you are dealing with—is it a string, text, or numeric value disguised as a categorical value? This information is essential for data preprocessing, feature extraction, and model selection.</p>
			<p>In this section, first, we will <a id="_idIndexMarker943"/>take a look at the different types <a id="_idIndexMarker944"/>of categorical data—namely <em class="italic">ordinal</em>, <em class="italic">nominal</em>, and <em class="italic">text</em>. Depending <a id="_idIndexMarker945"/>on the type, you can use different methods to extract information or other valuable data from it. Please bear in mind that categorical data is ubiquitous, whether it is in an ID column, a nominal category, an ordinal category, or a free-text field. It's worth mentioning that the more information you have on the data, the easier the preprocessing is.</p>
			<p>Next, we will actually preprocess the ordinal and nominal categorical data by transforming it into numerical values. This is a required step when you want to use an ML algorithm later on that can't interpret categorical data, which is true for most algorithms except, for example, decision tree-based approaches. Most other algorithms can only operate (for example, compute a loss function) on a numeric value and so a transformation is required.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor115"/>Comparing textual, categorical, and ordinal data</h2>
			<p>Many ML <a id="_idIndexMarker946"/>algorithms, such as support vector machines, neural <a id="_idIndexMarker947"/>networks, linear regression, and <a id="_idIndexMarker948"/>more, can only be applied to numeric <a id="_idIndexMarker949"/>data. However, in real-world datasets, we often <a id="_idIndexMarker950"/>find non-numeric columns, such as columns <a id="_idIndexMarker951"/>that contain textual data. The goal of this chapter is to transform textual data into numeric data as an advanced feature extraction step, which allows us to plug the processed data into any ML algorithm.</p>
			<p>When working with real-world data, you will be confronted with many different types of textual and/or categorical data. To optimize ML algorithms, you need to understand the differences in order to apply different preprocessing techniques to the different types. But first, let's <a id="_idIndexMarker952"/>define the three different textual data types:</p>
			<ul>
				<li><em class="italic">Textual data</em>: Free text</li>
				<li><em class="italic">Categorical nominal data</em>: Non-orderable categories</li>
				<li><em class="italic">Categorical ordinal data</em>: Orderable categories</li>
			</ul>
			<p>The difference <a id="_idIndexMarker953"/>between textual data and categorical <a id="_idIndexMarker954"/>data is that, in textual data, we want to capture semantic similarities (that is, the similarity in the meaning of the words), whereas, in categorical data, we want to differentiate between a small number of variables.</p>
			<p>The <a id="_idIndexMarker955"/>difference between categorical nominal data and <a id="_idIndexMarker956"/>categorical ordinal data is that nominal data cannot be ordered (all categories have the same weight), whereas ordinal categories can be logically ordered on an ordinal scale.</p>
			<p><em class="italic">Figure 7.1</em> shows an <a id="_idIndexMarker957"/>example dataset of comments <a id="_idIndexMarker958"/>on news articles, where the first column, named <strong class="source-inline">statement</strong>, is a textual field, the column named <strong class="source-inline">topic</strong> is a nominal category, and <strong class="source-inline">rating</strong> is an ordinal category:</p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="image/B17928_07_001.jpg" alt="Figure 7.1 – Comparing different textual data types " width="1171" height="330"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Comparing different textual data types</p>
			<p>Understanding the differences between these data representations is essential to find the proper embedding technique afterward. It seems quite natural to replace ordinal categories with an ordinal numeric scale and to embed nominal categories in an orthogonal space. On the contrary, it's not obvious how to embed textual data into a numerical space where the semantics are preserved—this will be covered in the later sections of this chapter that deal with NLP.</p>
			<p>Please <a id="_idIndexMarker959"/>note that instead of categorical values, you will <a id="_idIndexMarker960"/>also see continuous numeric variables <a id="_idIndexMarker961"/>representing categorical information, for <a id="_idIndexMarker962"/>example, IDs from a dimension <a id="_idIndexMarker963"/>or lookup table. Although these are <a id="_idIndexMarker964"/>numeric values, you should consider treating them as categorical nominal values, if possible. Here is an example dataset:</p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/B17928_07_002.jpg" alt="Figure 7.2 – Comparing numerical categorical values " width="999" height="278"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Comparing numerical categorical values</p>
			<p>In this example, we can see that the <strong class="source-inline">sensorId</strong> value is a numeric value that should be interpreted as a categorical nominal value instead of a numeric value by default because it doesn't have a numeric meaning. What do you get when you subtract <strong class="source-inline">sensorId</strong> <strong class="source-inline">2</strong> from <strong class="source-inline">sensorId</strong> <strong class="source-inline">1</strong>? Is <strong class="source-inline">sensorId</strong> <strong class="source-inline">10</strong> 10 times larger than <strong class="source-inline">sensorId</strong> <strong class="source-inline">1</strong>? These are the typical questions to ask to discover and encode these categorical values. We will discover, in <a href="B17928_09_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 9</em></a>, <em class="italic">Building ML Models Using Azure Machine Learning</em>, that by specifying that these values are categorical, a gradient-boosted tree model can optimize these features instead of treating them as continuous variables.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor116"/>Transforming categories into numeric values</h2>
			<p>Let's start <a id="_idIndexMarker965"/>by converting categorical <a id="_idIndexMarker966"/>variables (both ordinal and nominal) into numeric values. In this section, we will look at two common techniques <a id="_idIndexMarker967"/>for categorical encoding: <strong class="bold">label encoding</strong> and <strong class="bold">one-hot encoding</strong> (also called <em class="italic">dummy coding</em>). While <strong class="bold">label encoding</strong> replaces <a id="_idIndexMarker968"/>a categorical <a id="_idIndexMarker969"/>feature column with a numerical feature column, <strong class="bold">one-hot encoding</strong> uses multiple <a id="_idIndexMarker970"/>columns (where the number of columns equals the number of unique values) to encode a single feature.</p>
			<p>Both techniques are applied in the same way. During the training iteration, these techniques find all of the unique values in a feature column and assign them a specific numeric value (multidimensional value for one-hot encoding). As a result, a lookup dictionary defining this replacement is stored in the encoder. When the encoder is applied, the values in the applied column are transformed (replaced) using the lookup dictionary. If the list of possible values is known beforehand, most implementations allow the encoder to initialize the lookup dictionary directly from the list of known values, rather than finding the unique values in the training set. This has the benefit of specifying the order of the values in the dictionary, so orders the encoded values.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Please note that it's often possible that certain categorical feature values in the test set don't appear in the training set and, hence, are not stored in the lookup dictionary. So, you should add a default category to your encoder that can also transform unseen values into numeric values.</p>
			<p>Now, we will use two different categorical data columns, one ordinal and one nominal category, to showcase the different encodings. <em class="italic">Figure 7.3</em> shows a nominal feature, <strong class="source-inline">topic</strong>, which could represent a list of articles by a news agency:</p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="image/B17928_07_003.jpg" alt="Figure 7.3 – Nominal categorical data " width="1171" height="344"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Nominal categorical data</p>
			<p><em class="italic">Figure 7.4</em> contains <a id="_idIndexMarker971"/>the ordinal <a id="_idIndexMarker972"/>category of <strong class="source-inline">rating</strong>; it could represent a feedback form for purchased articles on a website:</p>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="image/B17928_07_004.jpg" alt="Figure 7.4 – Ordinal categorical data " width="1177" height="352"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Ordinal categorical data</p>
			<p>To preserve the meaning of the categories, we require different preprocessing techniques for the different categorical data types. First, we take a look at the <em class="italic">label encoder</em>. The label encoder assigns an incrementing value to each unique categorical value in a feature column. So, it transforms categories into a numeric value between <strong class="source-inline">0</strong> and <strong class="source-inline">N-1</strong>, where <strong class="source-inline">N</strong> represents the number of unique values.</p>
			<p>Let's test the label encoder in the <strong class="source-inline">topic</strong> column within the first table. We train the encoder on the data and replace the <strong class="source-inline">topic</strong> column with a numeric topic ID. Here is an example snippet to train the label encoder and transform the dataset:</p>
			<p class="source-code">from sklearn import preprocessing</p>
			<p class="source-code">data = load_articles()</p>
			<p class="source-code">enc = <strong class="bold">preprocessing.LabelEncoder()</strong></p>
			<p class="source-code">enc.fit(data)</p>
			<p class="source-code">enc.transform(data)</p>
			<p><em class="italic">Figure 7.5</em> shows <a id="_idIndexMarker973"/>the results of the <a id="_idIndexMarker974"/>preceding transformation. Each topic was encoded as a numerical increment, <strong class="source-inline">topicId</strong>:</p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/B17928_07_005.jpg" alt="Figure 7.5 – Label-encoded topics " width="1271" height="339"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Label-encoded topics</p>
			<p>The generated lookup table for <strong class="source-inline">topicId</strong> is shown in <em class="italic">Figure 7.6</em>. This lookup dictionary was learned by the encoder during the <strong class="source-inline">fit()</strong> method and can be applied to categorical data using the <strong class="source-inline">transform()</strong> method:</p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/B17928_07_006.jpg" alt="Figure 7.6 – A lookup dictionary for topics " width="1197" height="245"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – A lookup dictionary for topics</p>
			<p>As you can see in the previous screenshots, encoding nominal data with labels is easy and straightforward. However, the resulting numerical data has different mathematical properties from the distinct nominal categories. So, let's find out how this method works for ordinal data.</p>
			<p>In the next example, we naïvely apply the label encoder to the ratings dataset. The encoder is trained by iterating the training data in order to create the lookup dictionary:</p>
			<p class="source-code">from sklearn import preprocessing</p>
			<p class="source-code">data = load_ratings()</p>
			<p class="source-code">enc = <strong class="bold">preprocessing.LabelEncoder()</strong></p>
			<p class="source-code">enc.fit(data)</p>
			<p class="source-code">enc.transform(data)</p>
			<p><em class="italic">Figure 7.7</em> shows <a id="_idIndexMarker975"/>the result of the <a id="_idIndexMarker976"/>encoded ratings as <strong class="source-inline">ratingId</strong>, which is very similar to the previous example. However, in the case of ratings, the numerical properties of the ratings data are similar to the ordinal properties of the categorical ratings:</p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/B17928_07_007.jpg" alt="Figure 7.7 – Label-encoded ratings " width="1196" height="340"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Label-encoded ratings</p>
			<p>Additionally, let's look at the lookup dictionary, in <em class="italic">Figure 7.8</em>, that the encoder learned from the input data:</p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/B17928_07_008.jpg" alt="Figure 7.8 – The lookup dictionary for ratings " width="1339" height="339"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – The lookup dictionary for ratings</p>
			<p>Do you <a id="_idIndexMarker977"/>see something odd in the <a id="_idIndexMarker978"/>autogenerated lookup dictionary? Due to the order of the categorical values in the training data, we created a numeric list with the following order:</p>
			<p class="source-code">good &lt; very good &lt; bad &lt; average</p>
			<p>This is probably not what we anticipated when applying a label encoder to an ordinal categorical value. The ordering we would be looking for is similar to the following:</p>
			<p class="source-code">very bad &lt; bad &lt; average &lt; good &lt; very good</p>
			<p>In order to create a label encoder with the right order, we can pass the ordered list of categorical values to the encoder. This would create a more meaningful encoding, as shown in <em class="italic">Figure 7.9</em>:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/B17928_07_009.jpg" alt="Figure 7.9 – Label-encoded ratings with custom order " width="1180" height="357"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – Label-encoded ratings with custom order</p>
			<p>To achieve this in Python, we have to use pandas' categorical ordinal variable, which is a special kind of label encoder that requires a list of ordered categories as input:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">data = load_ratings()</p>
			<p class="source-code">categories = [</p>
			<p class="source-code">    'very bad', 'bad', 'average', 'good', 'very good']</p>
			<p class="source-code">data = <strong class="bold">pd.Categorical(data,</strong></p>
			<p class="source-code"><strong class="bold">                      categories=categories,</strong></p>
			<p class="source-code"><strong class="bold">                      ordered=True)</strong></p>
			<p class="source-code">print(data.codes)</p>
			<p>Under <a id="_idIndexMarker979"/>the hood, we implicitly created <a id="_idIndexMarker980"/>the following lookup dictionary for the encoder by passing the categories directly to it in order:</p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/B17928_07_010.jpg" alt="Figure 7.10 – A lookup dictionary for ratings with custom orders " width="1108" height="328"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10 – A lookup dictionary for ratings with custom orders</p>
			<p>As you can see in the preceding example, a label encoder can be quickly applied to any categorical data without much afterthought. The result of the label encoder is a single numerical feature and a categorical lookup table. Additionally, we can see, in the examples with topics and ratings, that label encoding is more suitable for ordinal data. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The key takeaway is that the label encoder is great for encoding ordinal categorical data. You also learned that the order of elements matters, and so it is good practice to manually pass the categories to the encoder in the correct order.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor117"/>Orthogonal embedding using one-hot encoding</h2>
			<p>In the second part of this section, we will take a look at the <strong class="bold">one-hot encoder</strong>. This will help us <a id="_idIndexMarker981"/>to create an equal-length encoding <a id="_idIndexMarker982"/>for nominal categorical values. The one-hot encoder replaces each unique categorical value in a feature column with a vector of size <strong class="source-inline">N</strong>, where <strong class="source-inline">N</strong> represents the number of unique values. This vector contains only zeros, except for one column that contains <strong class="source-inline">1</strong> and represents the column for this specific value. Here is a code snippet showing you how to apply the one-hot encoder to the <strong class="source-inline">articles</strong> dataset:</p>
			<p class="source-code">from sklearn import preprocessing</p>
			<p class="source-code">data = [load_articles()]</p>
			<p class="source-code">enc = <strong class="bold">preprocessing.OneHotEncoder()</strong></p>
			<p class="source-code">enc.fit(data)</p>
			<p class="source-code">enc.transform(data)</p>
			<p>The output of the preceding code is shown in <em class="italic">Figure 7.11</em>:</p>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/B17928_07_011.jpg" alt="Figure 7.11 – One-hot-encoded articles " width="1081" height="314"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.11 – One-hot-encoded articles</p>
			<p>The lookup <a id="_idIndexMarker983"/>dictionary for one-hot encoding has <strong class="source-inline">N+1</strong> columns, where <strong class="source-inline">N</strong> is the number of unique values in the encoded column. As <a id="_idIndexMarker984"/>we can see in the lookup dictionary in <em class="italic">Figure 7.12</em>, all N-dimensional vectors in the dictionary are orthogonal and of an equal length, <strong class="source-inline">1</strong>:</p>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/B17928_07_012.jpg" alt="Figure 7.12 – The lookup dictionary for articles " width="1186" height="247"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.12 – The lookup dictionary for articles</p>
			<p>Now, let's compare this technique with ordinal data and apply one-hot encoding to the ratings table. The result is shown in <em class="italic">Figure 7.13</em>:</p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/B17928_07_013.jpg" alt="Figure 7.13 – One-hot-encoded ratings " width="1332" height="385"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.13 – One-hot-encoded ratings</p>
			<p>In the preceding figure, we can see that even if the original category values are ordinal, the encoded values can no longer be sorted, and so, this property is lost after the numeric encoding. Therefore, we can conclude that one-hot encoding is great for nominal categorical values where the number of unique values is small.</p>
			<p>So far, we've <a id="_idIndexMarker985"/>learned how to embed nominal <a id="_idIndexMarker986"/>and ordinal categorical values into numeric values by using a lookup dictionary and one-dimensional or N-dimensional numeric embedding. However, we discovered that it is somewhat limited in many aspects, such as the number of unique categories and capabilities to embed free text. In the following sections, we will learn how to extract words using a simple NLP pipeline.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor118"/>Semantics and textual values</h2>
			<p>It's worth taking the time to understand that a categorical value and a textual value are not the same. Although <a id="_idIndexMarker987"/>they might both be stored as a string and could have the same data type in your dataset, usually, a categorical value represents a finite set of categories, whereas a text value can hold any textual information.</p>
			<p>So, why is this distinction important? Once you preprocess your categorical data and embed it into a numerical space, nominal categories will often be implemented as orthogonal vectors. You will not automatically be able to compute a distance from category A to category B or create a semantic meaning between the categories.</p>
			<p>However, with <a id="_idIndexMarker988"/>textual data, usually, you start feature extraction with a different approach that assumes that you will find similar terms in the same text feature of your dataset samples. You can use this information to compute meaningful similarity scores between two textual columns; for example, to measure the number of words that are in common.</p>
			<p>Therefore, we recommend that you thoroughly check what kind of categorical values you have and how you are aiming to preprocess them. Also, a great exercise is to compute the similarity between two rows and see whether it matches your prediction. Let's take a look at a simple textual preprocessing approach using a dictionary-based bag-of-words embedding.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor119"/>Building a simple bag-of-words model</h1>
			<p>In this section, we will <a id="_idIndexMarker989"/>look at a surprisingly simple concept to tackle the shortcomings of label encoding for textual data using a technique called bag-of-words, which will build a foundation for a simple NLP pipeline. Don't worry if these techniques look too simple when you read through them; we will gradually build on top of them with tweaks, optimizations, and improvements to build a modern NLP pipeline.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor120"/>A naïve bag-of-words model using counting</h2>
			<p>In this section, the main concept that we will build is the bag-of-words model. It is a very simple <a id="_idIndexMarker990"/>concept; that is, it involves modeling any document as a collection of words that appear in a given document with the frequency of each word. Hence, we throw away sentence structure, word order, punctuation marks, and more and reduce the documents to a raw count of words. Following this, we can vectorize this word count into a numeric vector representation, which can then be used for ML, analysis, document comparisons, and much more. While this word count model sounds very simple, we will encounter quite a few language-specific obstacles along the way that we will need to resolve.</p>
			<p>Let's get started and define a sample document that we will transform throughout this section:</p>
			<p class="source-code">Almost before we knew it, we had left the ground. The unknown holds its grounds.</p>
			<p>Applying a naïve word count to the document gives us our first (too simple) bag-of-words model:</p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/B17928_07_014.jpg" alt="Figure 7.14 – A naïve bag-of-words model " width="1650" height="736"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14 – A naïve bag-of-words model</p>
			<p>However, there <a id="_idIndexMarker991"/>are many problems with a naïve approach such as the preceding one. We have mixed different punctuation marks, notations, nouns, verbs, adverbs, and adjectives in different declinations, conjugations, tenses, and cases. Therefore, we have to build a pipeline to clean and normalize the data using NLP. In this section, we will build a pipeline with the following cleaning steps before feeding <a id="_idIndexMarker992"/>the data into a <strong class="bold">count vectorizer</strong> that, ultimately, counts the word occurrences and collects them in a feature vector.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor121"/>Tokenization – turning a string into a list of words</h2>
			<p>The first <a id="_idIndexMarker993"/>step in building the pipeline is to separate a corpus into documents and a document into words. This process is called <strong class="bold">tokenization</strong> because <a id="_idIndexMarker994"/>the resulting tokens contain words and punctuation marks. While splitting a corpus into documents, documents into sentences, and sentences into words sounds trivial, with a bit <a id="_idIndexMarker995"/>of <strong class="bold">Regular Expression</strong> (<strong class="bold">RegEx</strong>), there are many non-trivial language-specific issues. Think about the different uses of periods, commas, and quotes, and <a id="_idIndexMarker996"/>think about whether you would have thought about the following words in English: <em class="italic">don't</em>, <em class="italic">Mr. Smith</em>, <em class="italic">Johann S. Bach</em>, and more. The <strong class="bold">Natural Language Toolkit</strong> (<strong class="bold">NLTK</strong>) Python package provides implementations and pretrained transformers for many NLP algorithms, as well as for word tokenization. Let's split our document into tokens using <strong class="source-inline">nltk</strong>:</p>
			<p class="source-code">from nltk.tokenize import word_tokenize</p>
			<p class="source-code">nltk.download('punkt')</p>
			<p class="source-code">tokens = <strong class="bold">word_tokenize(document)</strong></p>
			<p class="source-code">print(tokens)</p>
			<p>The preceding <a id="_idIndexMarker997"/>code will output a list of tokens that contains words and punctuation marks:</p>
			<p class="source-code">['Almost', 'before', 'we', 'knew', 'it', ',', 'we', 'had', 'left', 'the', 'ground', '.', 'The', 'unknown', 'holds', 'its', 'grounds', '.']</p>
			<p>When you <a id="_idIndexMarker998"/>execute the preceding code snippet, <strong class="source-inline">nltk</strong> will download the pretrained punctuation model in order to run the word tokenizer. The output of the tokenizer is the words and punctuation marks.</p>
			<p>In the next step, we will remove the punctuation marks as they are not relevant for the subsequent <em class="italic">stemming</em> process. However, we will bring them back for <em class="italic">lemmatization</em> later in this section:</p>
			<p class="source-code">words = [word.lower() for word in tokens if <strong class="bold">word.isalnum()</strong>]</p>
			<p class="source-code">print(words)</p>
			<p>The result will only contain the words of the original document without any punctuation marks:</p>
			<p class="source-code">['almost', 'before', 'we', 'knew', 'it', 'we', 'had', 'left', 'the', 'ground', 'the', 'unknown', 'holds', 'its', 'grounds']</p>
			<p>In the preceding code, we used the <strong class="source-inline">word.islanum()</strong> function to only extract alphanumeric tokens and make them all lowercase. The preceding list of words already looks much better than the initial naïve model. However, it still contains a lot of unnecessary words, such as <em class="italic">the</em>, <em class="italic">we</em>, <em class="italic">had</em>, and more, which don't convey any information.</p>
			<p>In order to filter out the noise for a specific language, it makes sense to remove these words that often appear in texts and don't add any semantic meaning to the text. It is common practice to remove these so-called <strong class="bold">stop words</strong> using a pretrained lookup dictionary. You can load and use such a dictionary by using the pretrained <strong class="source-inline">nltk</strong> library in Python:</p>
			<p class="source-code">from nltk.corpus import stopwords</p>
			<p class="source-code">stopword_set = set(<strong class="bold">stopwords.words('english')</strong>)</p>
			<p class="source-code">words = [word for word in words if word not in stopword_set]</p>
			<p class="source-code">print(words)</p>
			<p>Now the <a id="_idIndexMarker999"/>resulting list only contains words that are not stop words:</p>
			<p class="source-code">['almost', 'knew', 'left', 'ground', 'unknown', 'holds', 'grounds']</p>
			<p>The preceding <a id="_idIndexMarker1000"/>code gives us a nice pipeline where we end up with only the semantically meaningful words. We can take this list of words to the next step and apply a more sophisticated transformation/normalization to each word. If we applied the count vectorizer at this stage, we would end up with the simple bag-of-words model, as shown in <em class="italic">Figure 7.15</em>:</p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="image/B17928_07_015.jpg" alt="Figure 7.15 – A simple bag-of-words model " width="1197" height="455"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.15 – A simple bag-of-words model</p>
			<p>As you can see in the previous figure, the list of terms that are included in the bag-of-words model is already far cleaner than the naïve example. This is because it doesn't contain any punctuation marks or stop words.</p>
			<p>You might ask what qualifies a word as a stop word other than it occurring relatively often in a piece of text? Well, that's an excellent question! We can measure the importance <a id="_idIndexMarker1001"/>of each word in the <a id="_idIndexMarker1002"/>current context compared to its occurrences across the text using the <strong class="bold">TF-IDF</strong> method, which will be discussed in the <em class="italic">Measuring the importance of words using TF-IDF</em> section.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor122"/>Stemming – the rule-based removal of affixes</h2>
			<p>In the next step, we want to normalize affixes—word endings to create plurals and conjugations. You <a id="_idIndexMarker1003"/>can see that with each step, we are diving deeper into the concept of a single language—in this case, English. However, when applying these steps to a different language, it's likely that completely different transformations will need to be used. This is what makes NLP such a difficult field.</p>
			<p>Removing the affixes <a id="_idIndexMarker1004"/>of words to obtain the stem of a word is also called <strong class="bold">stemming</strong>. Stemming refers to a rule-based (heuristic) approach to transform each occurrence of a word into its word stem. Here is a simple example of some expected transformations:</p>
			<p class="source-code">cars   -&gt; car</p>
			<p class="source-code">saying -&gt; say</p>
			<p class="source-code">flies  -&gt; fli</p>
			<p>As you can see in the preceding example, such a heuristic approach for stemming has to be built specifically for each language. This is generally true for all other NLP algorithms as well. For the sake of brevity, in this book, we will only discuss English examples.</p>
			<p>A popular algorithm for stemming in English is Porter's algorithm, which defines five sequential reduction rules, such as removing <em class="italic">-ed</em>, <em class="italic">-ing</em>, -<em class="italic">ate</em>, <em class="italic">-tion</em>, <em class="italic">-ence</em>, <em class="italic">-ance</em>, and more, from the end of words. The <strong class="source-inline">nltk</strong> library comes with an implementation of Porter's stemming algorithm:</p>
			<p class="source-code">from nltk.stem import PorterStemmer</p>
			<p class="source-code">stemmer = PorterStemmer()</p>
			<p class="source-code">words = [<strong class="bold">stemmer.stem(word)</strong> for word in words]</p>
			<p class="source-code">print(words)</p>
			<p>The resulting list of words after stemming looks like this:</p>
			<p class="source-code">['almost', 'knew', 'left', 'ground', 'unknown', 'hold', 'ground']</p>
			<p>In the <a id="_idIndexMarker1005"/>preceding code, we simply apply <strong class="source-inline">stemmer</strong> to each word in the tokenized <a id="_idIndexMarker1006"/>document. The bag-of-words model after this step is shown in <em class="italic">Figure 7.16</em>:</p>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<img src="image/B17928_07_016.jpg" alt="Figure 7.16 – The bag-of-words model after stemming " width="1134" height="399"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.16 – The bag-of-words model after stemming</p>
			<p>While this algorithm works well with affixes, it can't avoid normalizing conjugations and tenses. This will be our next problem to tackle using lemmatization.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor123"/>Lemmatization – dictionary-based word normalization</h2>
			<p>When looking <a id="_idIndexMarker1007"/>at the stemming examples, we can already see the limitations <a id="_idIndexMarker1008"/>of that approach. For example, what would happen with irregular verb conjugations—such as <em class="italic">are</em>, <em class="italic">am</em>, or <em class="italic">is</em>—that should all be normalized to the same word, <em class="italic">be</em>? This is exactly what lemmatization tries to solve using <a id="_idIndexMarker1009"/>a pretrained set of vocabulary and conversion rules, called lemmas. The <strong class="bold">lemmas</strong> are stored in a lookup dictionary and look similar to the following transformations:</p>
			<p class="source-code">are    -&gt; be</p>
			<p class="source-code">is     -&gt; be</p>
			<p class="source-code">taught -&gt; teach</p>
			<p class="source-code">better -&gt; good</p>
			<p>There is one very important point to make when discussing lemmatization. Each lemma needs to be applied to the correct word type, hence a lemma for nouns, verbs, adjectives, and more. The reason for this is that a word can be either a noun or a verb in the past tense. In <a id="_idIndexMarker1010"/>our example, <strong class="source-inline">ground</strong> could come <a id="_idIndexMarker1011"/>from the noun <em class="italic">ground</em> or the verb <em class="italic">grind</em>; <strong class="source-inline">left</strong> could be an adjective or the past tense of <em class="italic">leave</em>. So, we also need to extract the word type from the word in a sentence—this <a id="_idIndexMarker1012"/>process is called <strong class="bold">Point of Speech</strong> (<strong class="bold">POS</strong>) tagging. Luckily, the <strong class="source-inline">nltk</strong> library has us covered once again. To estimate the correct POS tag, we also need to provide the punctuation mark:</p>
			<p class="source-code">import nltk</p>
			<p class="source-code">nltk.download('averaged_perceptron_tagger')</p>
			<p class="source-code">tags = <strong class="bold">nltk.pos_tag(tokens)</strong></p>
			<p class="source-code">print(tags)</p>
			<p>Here are the resulting POS tags:</p>
			<p class="source-code">[('Almost', 'RB'), ('before', 'IN'), ('we', 'PRP'), ('knew', 'VBD'), ('it', 'PRP'), (',', ','), ('we', 'PRP'), ('had', 'VBD'), ('left', 'VBN'), ('the', 'DT'), ('ground', 'NN'), ('.', '.'), ('The', 'DT'), ('unknown', 'JJ'), ('holds', 'VBZ'), ('its', 'PRP$'), ('grounds', 'NNS'), ('.', '.')]</p>
			<p>The POS tags describe the word type of each token in the document. You can find a complete list of tags using the <strong class="source-inline">nltk.help.upenn_tagset()</strong> command. Here is an example of how to do so from the command line:</p>
			<p class="source-code">import nltk</p>
			<p class="source-code">nltk.download('tagsets')</p>
			<p class="source-code"><strong class="bold">nltk.help.upenn_tagset()</strong></p>
			<p>The preceding command will print the list of POS tags:</p>
			<p class="source-code">CC: conjunction, coordinating</p>
			<p class="source-code">    &amp; 'n and both but either et for less minus neither nor or </p>
			<p class="source-code">    plus so therefore times v. versus vs. whether yet</p>
			<p class="source-code">CD: numeral, cardinal</p>
			<p class="source-code">    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 </p>
			<p class="source-code">    one forty- seven 1987 twenty '79 zero two 78-degrees </p>
			<p class="source-code">    eighty-four IX '60s .025 fifteen 271,124 dozen quintillion </p>
			<p class="source-code">    DM2,000 ...</p>
			<p class="source-code">DT: determiner</p>
			<p class="source-code">    all an another any both del each either every half la many </p>
			<p class="source-code">    much nary neither no some such that the them these this </p>
			<p class="source-code">    those</p>
			<p class="source-code">EX: existential there</p>
			<p class="source-code">    there</p>
			<p class="source-code">FW: foreign word</p>
			<p class="source-code">    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si </p>
			<p class="source-code">    vous lutihaw alai je jour objets salutaris fille quibusdam </p>
			<p class="source-code">    pas </p>
			<p class="source-code">...</p>
			<p>The POS tags <a id="_idIndexMarker1013"/>also include tenses for verbs and other <a id="_idIndexMarker1014"/>very useful information. However, for the lemmatization in this section, we only need to know the word type—<em class="italic">noun</em>, <em class="italic">verb</em>, <em class="italic">adjective</em>, or <em class="italic">adverb</em>. One possible choice of lemmatizer is the WordNet lemmatizer in <strong class="source-inline">nltk</strong>. WordNet is a lexical database of English words that groups them into groups of concepts and word types.</p>
			<p>To apply the lemmatizer to the output of the stemming, we need to filter the POS tags by punctuation marks and stop words, similar to the previous preprocessing step. Then, we can use <a id="_idIndexMarker1015"/>the word tags for the resulting words. Let's apply the lemmatizer using <strong class="source-inline">nltk</strong>:</p>
			<p class="source-code">from nltk.corpus import wordnet</p>
			<p class="source-code">from nltk.stem import WordNetLemmatizer</p>
			<p class="source-code">nltk.download('wordnet')       </p>
			<p class="source-code">lemmatizer = WordNetLemmatizer()</p>
			<p class="source-code">tag_dict = {</p>
			<p class="source-code">    "J": wordnet.ADJ,</p>
			<p class="source-code">    "N": wordnet.NOUN,</p>
			<p class="source-code">    "V": wordnet.VERB,</p>
			<p class="source-code">    "R": wordnet.ADV</p>
			<p class="source-code">}</p>
			<p class="source-code">pos = [tag_dict.get(t[0].upper(), wordnet.NOUN) \</p>
			<p class="source-code">        for t in zip(*tags)[1]]</p>
			<p class="source-code">words = [lemmatizer.lemmatize(w, pos=p) \</p>
			<p class="source-code">        for w, p in zip(words, pos)]</p>
			<p class="source-code">print(words)</p>
			<p>The code <a id="_idIndexMarker1016"/>outputs the lemmatized words:</p>
			<p class="source-code">['almost', 'know', 'leave', 'ground', 'unknown', 'hold', 'ground']</p>
			<p>The preceding list of words looks a lot cleaner than what we found in previous models. This is because we normalized the tenses of the verbs and transformed them into their infinitive form. The resulting bag-of-words model is shown in <em class="italic">Figure 7.17</em>:</p>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="image/B17928_07_017.jpg" alt="Figure 7.17 – The bag-of-words model after lemmatization " width="1199" height="402"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.17 – The bag-of-words model after lemmatization</p>
			<p>This technique <a id="_idIndexMarker1017"/>is extremely helpful for cleaning <a id="_idIndexMarker1018"/>up irregular forms of words in your dataset. However, it works based on rules—called lemmas—and, hence, it can only be used for languages and words where such lemmas are available.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor124"/>A bag-of-words model in scikit-learn</h2>
			<p>Finally, we can put all our previous steps together to create a state-of-the-art NLP preprocessing <a id="_idIndexMarker1019"/>pipeline to normalize the input documents <a id="_idIndexMarker1020"/>and run them through a count vectorizer so that we can transform them into a numeric feature vector. Doing so for multiple documents allows us to easily compare the semantics of the document in a numerical space. We could compute cosine similarities on the document's feature vectors to compute their similarity, plug them into a supervised classification method, or perform clustering on the resulting document concepts.</p>
			<p>To recap, let's take a look at the final pipeline for the simple bag-of-words model. I want to emphasize that this model is only the start of our journey in feature extraction using NLP. We performed the following steps for normalization:</p>
			<ol>
				<li>Tokenization</li>
				<li>Removing punctuation marks</li>
				<li>Removing stop words</li>
				<li>Stemming</li>
				<li>Lemmatization with POS tagging</li>
			</ol>
			<p>In the last step, we applied <strong class="source-inline">CountVectorizer</strong> in scikit-learn. This will count the occurrences of each word, create a global corpus of words, and output a sparse feature vector of word frequencies. Here is the sample code to pass the preprocessed data from <strong class="source-inline">nltk</strong> to <strong class="source-inline">CountVectorizer</strong>:</p>
			<p class="source-code">from sklearn.feature_extraction.text import CountVectorizer</p>
			<p class="source-code">count_vect = <strong class="bold">CountVectorizer()</strong></p>
			<p class="source-code">data = [" ".join(words)]</p>
			<p class="source-code">X_train_counts = count_vect.fit_transform(data)</p>
			<p class="source-code">print(X_train_counts)</p>
			<p>The <a id="_idIndexMarker1021"/>transformed bag-of-words model contains <a id="_idIndexMarker1022"/>coordinates and counts:</p>
			<p class="source-code">  (0, 0)        1</p>
			<p class="source-code">  (0, 3)        1</p>
			<p class="source-code">  (0, 4)        1</p>
			<p class="source-code">  (0, 1)        2</p>
			<p class="source-code">  (0, 5)        1</p>
			<p class="source-code">  (0, 2)        1</p>
			<p>The coordinates refer to the <strong class="source-inline">(document id, term id)</strong> pair, whereas the count refers to the term frequency. To better understand this output, we can also look at the internal vocabulary of the model. The <strong class="source-inline">vocabulary_</strong> parameter contains a lookup dictionary for the term ids:</p>
			<p class="source-code">print(count_vect.vocabulary_)</p>
			<p>The code outputs the model's word dictionary:</p>
			<p class="source-code">{'almost': 0, 'know': 3, 'leave': 4, 'ground': 1, 'unknown': 5, 'hold': 2}</p>
			<p>In the preceding example, we transform the preprocessed document back into a string before passing it to <strong class="source-inline">CountVectorizer</strong>. The reason for this is that <strong class="source-inline">CountVectorizer</strong> comes with some configurable preprocessing techniques out of the box, such <a id="_idIndexMarker1023"/>as tokenization, stop word removal, and <a id="_idIndexMarker1024"/>more. For this demonstration, we want to apply it to the preprocessed data. The output of the transformation is a sparse feature vector containing the term frequencies.</p>
			<p>Let's find out how we can combine multiple terms with semantic concepts.</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor125"/>Leveraging term importance and semantics</h1>
			<p>Everything we have <a id="_idIndexMarker1025"/>done up to now has been relatively simple and based on word stems or <a id="_idIndexMarker1026"/>so-called tokens. The bag-of-words model was nothing but a dictionary of tokens counting the occurrence of tokens per field. In this section, we will take a look at a common technique to further improve matching between documents using n-gram and skip-gram combinations of terms.</p>
			<p>Combining terms in multiple ways will explode your dictionary. This will turn into a problem if you have a large corpus; for instance, 10 million words. Hence, we will look at a common preprocessing technique to reduce the dimensionality of a large dictionary through SVD.</p>
			<p>While, now, this approach is a lot more complicated, it is still based on a bag-of-words model that already works well on a large corpus, in practice. However, of course, we can do better and try to understand the importance of words. Therefore, we will tackle another popular technique in NLP to compute the importance of terms.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor126"/>Generalizing words using n-grams and skip-grams</h2>
			<p>In the <a id="_idIndexMarker1027"/>previous pipeline, we considered each word on <a id="_idIndexMarker1028"/>its own without any context. However, as we <a id="_idIndexMarker1029"/>all know, context matters <a id="_idIndexMarker1030"/>a lot in language. Sometimes, words belong together and only make sense in context rather than on their own. To introduce this context into the same type of algorithm, we will introduce <strong class="bold">n-grams</strong> and <strong class="bold">skip-grams</strong>. Both techniques are heavily used in NLP for preprocessing datasets and extracting relevant features from text data.</p>
			<p>Let's start <a id="_idIndexMarker1031"/>with n-grams. An <strong class="bold">n-gram</strong> is a concatenation for <strong class="source-inline">N</strong> consecutive entities (that is, characters, words, or tokens) of an input dataset. Here are some examples for computing the n-grams in a list of characters:</p>
			<p class="source-code">A, B, C, D -&gt; 1-Gram: A, B, C, D</p>
			<p class="source-code">A, B, C, D -&gt; 2-Gram: AB, BC, CD</p>
			<p class="source-code">A, B, C, D -&gt; 3-Gram: ABC, BCD</p>
			<p>Here is <a id="_idIndexMarker1032"/>an example using the built-in <strong class="source-inline">ngram_range</strong> parameter <a id="_idIndexMarker1033"/>in scikit-learn's <strong class="source-inline">CountVectorizer</strong> to generate multiple n-grams for the input data:</p>
			<p class="source-code">from sklearn.feature_extraction.text import CountVectorizer</p>
			<p class="source-code">count_vect = CountVectorizer(<strong class="bold">ngram_range=(1,2)</strong>)</p>
			<p class="source-code">X_train_counts = count_vect.fit_transform(data)</p>
			<p class="source-code">print(count_vect.vocabulary_)</p>
			<p>As you <a id="_idIndexMarker1034"/>can see, the vocabulary now contains <a id="_idIndexMarker1035"/>both the 1-gram and 2-gram representations of each term:</p>
			<p class="source-code">{'almost': 0, 'before': 2, 'we': 24, 'knew': 15, 'it': 11, 'had': 7, 'left': 17, 'the': 19, 'ground': 4, 'unknown': 22, 'holds': 9, 'its': 13, 'grounds': 6, 'almost before': 1, 'before we': 3, 'we knew': 26, 'knew it': 16, 'it we': 12, 'we had': 25, 'had left': 8, 'left the': 18, 'the ground': 20, 'ground the': 5, 'the unknown': 21, 'unknown holds': 23, 'holds its': 10, 'its grounds': 14}</p>
			<p>In the preceding code, we can see that instead of the original words, we now have a combination of two consecutive words in our trained vocabulary.</p>
			<p>We can extend the concept of n-grams to also allow the model to skip words. This a great option, if we for example want to perform a 2-gram, but in one of our samples there is an adjective in-between two words and in the other those words are directly next to each other. To achieve this, we need a method that allows us to define how many words it is allowed to skip to find matching words. Here is an example using the same characters as before:</p>
			<p class="source-code">A, B, C, D -&gt; 2-Gram (1 skip): AB, AC, BC, BD, CD</p>
			<p class="source-code">A, B, C, D -&gt; 2-Gram (2 skip): AB, AC, AD, BC, BD, CD</p>
			<p>Luckily, we find the generalized version of n-grams implemented in <strong class="source-inline">nltk</strong> as the <strong class="source-inline">nltk.skipgrams</strong> method. Setting the skip distance to <strong class="source-inline">0</strong> results in the traditional n-gram algorithm. We can apply it to our original dataset:</p>
			<p class="source-code">terms = list(<strong class="bold">nltk.skipgrams(document.split(' '), 2, 1)</strong>)</p>
			<p class="source-code">print(terms)</p>
			<p>Similar <a id="_idIndexMarker1036"/>to the 2-gram example, the method <a id="_idIndexMarker1037"/>produces a list of combinations <a id="_idIndexMarker1038"/>of paired terms. However, in this <a id="_idIndexMarker1039"/>case, we allowed one skip word to be present between those pairs:</p>
			<p class="source-code">[('Almost', 'before'), ('Almost', 'we'), ('before', 'we'), ('before', 'knew'), ('we', 'knew'), ('we', 'it,'), ('knew', 'it,'), ('knew', 'we'), ('it,', 'we'), ('it,', 'had'), ('we', 'had'), ('we', 'left'), ('had', 'left'), ('had', 'the'), ('left', 'the'), ('left', 'ground.'), ('the', 'ground.'), ('the', 'The'), ('ground.', 'The'), ('ground.', 'unknown'), ('The', 'unknown'), ('The', 'holds'), ('unknown', 'holds'), ('unknown', 'its'), ('holds', 'its'), ('holds', 'grounds.'), ('its', 'grounds.')]</p>
			<p>In the preceding code, we can observe that skip-grams can generate a lot of additional useful feature dimensions for the NLP model. In real-world scenarios, both techniques are often used because the individual word order plays a big role in the semantics.</p>
			<p>However, the explosion of new feature dimensions could be devastating if the input documents are, for example, all websites from the web or large documents. Therefore, we also need a way to avoid an explosion of the dimensions while capturing all of the semantics from the input data. We will tackle this challenge in the next section.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor127"/>Reducing word dictionary size using SVD</h2>
			<p>A common <a id="_idIndexMarker1040"/>problem with NLP is the vast number <a id="_idIndexMarker1041"/>of words in a corpus and, hence, exploding dictionary sizes. In the previous example, we saw that the size of the dictionary defines the size of the orthogonal term vector. Therefore, a dictionary size of 20,000 terms would result in 20,000-dimensional feature vectors. Even without any n-gram enrichment, this feature vector dimension is too large to be processed on standard PCs.</p>
			<p>Therefore, we need an algorithm to reduce the dimensions of the generated <strong class="source-inline">CountVectorizer</strong> while preserving the present information. Optimally, we would only remove <a id="_idIndexMarker1042"/>redundant information from the <a id="_idIndexMarker1043"/>input data and project it onto a lower-dimensional space while preserving all of the original information.</p>
			<p>The PCA transformation would be a great fit for our solution <a id="_idIndexMarker1044"/>and help us to transform the input data into lower linearly unrelated dimensions. However, computing the eigenvalues requires a symmetric matrix (the same number of rows and columns), which, in our case, we don't have. Hence, we can use the SVD algorithm, which generalizes the eigenvector computation to non-symmetric matrices. Due to its numeric stability, it is often used in NLP and information retrieval systems.</p>
			<p>The usage <a id="_idIndexMarker1045"/>of SVD in NLP applications is also called <strong class="bold">Latent Semantic Analysis</strong> (<strong class="bold">LSA</strong>), as the principal components can be interpreted as concepts in a latent feature space. The SVD embedding transforms the high-dimensional feature vector into a lower-dimensional concept space. Each dimension in the concept space is constructed by a linear combination of term vectors. By dropping the concepts with the smallest variance, we also reduce the dimensions of the resulting concept space to something that is a lot smaller and easier to handle. Typical concept spaces have 10s to 100s of dimensions, while word dictionaries usually have over 100,000.</p>
			<p>Let's look at an example using the <strong class="source-inline">TruncatedSVD</strong> implementation from <strong class="source-inline">sklearn</strong>. The SVD is implemented as a transformer class, and so, we need to call <strong class="source-inline">fit_transform()</strong> to fit a dictionary and transform it using  the same step. The SVD is configured to only keep the components with the highest variance using the <strong class="source-inline">n_components</strong> argument:</p>
			<p class="source-code">from sklearn.decomposition import TruncatedSVD</p>
			<p class="source-code">svd = <strong class="bold">TruncatedSVD(n_components=5)</strong></p>
			<p class="source-code">X_lsa = svd.fit_transform(X_train_counts) </p>
			<p>In the preceding code, we perform the LSA on the <strong class="source-inline">X_train_counts</strong> data and the output of <strong class="source-inline">CountVectorizer</strong> using SVD. We configure the SVD to only keep the first five components with the highest variance.</p>
			<p>By reducing the dimensionality of your dataset, you lose information. Thankfully, we can compute the amount of variance in the remaining dataset using the trained SVD object, as shown in the following example:</p>
			<p class="source-code">Print(<strong class="bold">svd.explained_variance_ratio_.</strong>sum())</p>
			<p>The <a id="_idIndexMarker1046"/>preceding command outputs the variance as a number between 0 and 1, where 1 means <a id="_idIndexMarker1047"/>that the SVD transformation is an exact lossless mapping of the original data into the latent space:</p>
			<p class="source-code">0.19693920498587408</p>
			<p>In this case, with only five components, the SVD retained 20% of the variance of the original dataset.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Depending on the task, we usually aim to preserve more than 80–90% of the original variance after the latent transformation.</p>
			<p>In the previous code example, we computed the variance of the data that is preserved after the transformation to the configured number of components. Hence, we can now increase or reduce the number of components in order to keep a specific percentage of the information in the transformed data. This is a very helpful operation and is used in many practical NLP implementations.</p>
			<p>Note that we are still using the original word dictionary from the bag-of-words model. One particular downside of this model is that the more often a term occurs, the higher its count (and, therefore, weight) will get. This is a problem because, now, any term that is not a stop word and appears often in the text will receive a high weight—independent of the importance of the term within a certain document. Therefore, we introduce another extremely popular preprocessing technique—<strong class="bold">TF-IDF</strong>.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor128"/>Measuring the importance of words using TF-IDF</h2>
			<p>One particular downside of the bag-of-words approach is that we simply count the absolute <a id="_idIndexMarker1048"/>number of words in a context without checking whether the word generally appears frequently across all documents. A term that appears in every document might not <a id="_idIndexMarker1049"/>be relevant for our model, as it contains less information and more often it appears across other documents. Hence, an important technique in text mining is to compute the importance of a certain word in a given context.</p>
			<p>Therefore, instead of an absolute count of terms in a context, we want to compute the number of terms in the context relative to a corpus of documents. By doing so, we will give higher weight to terms that appear only in a certain context, and reduce the amount of weight given to terms that appear in many different documents. This is exactly what the TF-IDF algorithm does. It is easy to compute a weight (<em class="italic">w</em>) for a term (<em class="italic">t</em>) in a document (<em class="italic">d</em>) according to the following equation:</p>
			<p><img src="image/Formula_07_001.png" alt="" width="556" height="113"/></p>
			<p>While the term frequency (<em class="italic">f</em><span class="subscript">t</span>) counts all of the terms in a document, the inverse document frequency is computed by dividing the total number of documents (<em class="italic">N</em>) by the counts of a term in all documents (<em class="italic">f</em><span class="subscript">d</span>). The <em class="italic">IDF</em> term is usually log-transformed, as the total count of a term across all documents can get quite large.</p>
			<p>In the following example, we will not use the TF-IDF function directly. Instead, we will use <strong class="source-inline">TfidfVectorizer</strong>, which does the counting and then applies the TF-IDF function to the result in one step. Again, the function is implemented as a <strong class="source-inline">sklearn</strong> transformer, and hence, we call <strong class="source-inline">fit_transform()</strong> to train and transform the dataset:</p>
			<p class="source-code">from sklearn.feature_extraction.text import TfidfVectorizer</p>
			<p class="source-code">vect = TfidfVectorizer()</p>
			<p class="source-code">data = [" ".join(words)]</p>
			<p class="source-code">X_train_counts = vect.fit_transform(data)</p>
			<p class="source-code">print(X_train_counts)</p>
			<p>The result is formatted in a similar manner to the earlier example containing <strong class="source-inline">(document id, term id)</strong> pairs and their TF-IDF values:</p>
			<p class="source-code">(0, 2)        0.3333333333333333</p>
			<p class="source-code">(0, 5)        0.3333333333333333</p>
			<p class="source-code">(0, 1)        0.6666666666666666</p>
			<p class="source-code">(0, 4)        0.3333333333333333</p>
			<p class="source-code">(0, 3)        0.3333333333333333</p>
			<p class="source-code">(0, 0)        0.3333333333333333</p>
			<p>In the <a id="_idIndexMarker1050"/>preceding code, we apply <strong class="source-inline">TfidfVectorizer</strong> directly, which returns the same <a id="_idIndexMarker1051"/>result as using <strong class="source-inline">CountVectorizer</strong> and <strong class="source-inline">TfidfTransformer</strong> combined. We transform a dataset containing the words of the bag-of-words model and return the TF-IDF values. We can also return the terms for each TF-IDF value:</p>
			<p class="source-code">print(vect.get_feature_names())</p>
			<p>The preceding code returns the vocabulary of the model:</p>
			<p class="source-code">['almost', 'ground', 'hold', 'know', 'leave', 'unknown']</p>
			<p>In this example, we can see that <strong class="source-inline">ground</strong> gets a TF-IDF value of <strong class="source-inline">0.667</strong>, whereas all the other terms receive a value of <strong class="source-inline">0.333</strong>. This count will now scale relatively when more documents are added to the corpus—hence, if the word <strong class="source-inline">hold</strong> were to be included again, the TF-IDF value would decrease.</p>
			<p>In any real-world pipeline, we would always use all the techniques presented in this chapter—tokenization, stop word removal, stemming, lemmatization, n-grams/skip-grams, TF-IDF, and SVD—combined in a single pipeline. The result would be a numeric representation of n-grams/skip-grams of tokens weighted by importance and transformed into a latent semantic space. Using these techniques for your first NLP pipeline will get you quite far, as you can now capture a lot of information from your textual data.</p>
			<p>So far, we have learned how to numerically encode many kinds of categorical and textual values by using either one-dimensional or N-dimensional labels or counting and weighting <a id="_idIndexMarker1052"/>word stems and character combinations. While many of these methods <a id="_idIndexMarker1053"/>work well in many situations where you require simple numeric embedding, they all have a serious limitation—they don't encode semantics. Let's take a look at how we can extract the semantic meaning of text in the same pipeline.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor129"/>Extracting semantics using word embeddings</h2>
			<p>When computing the similarity of news, you would imagine that topics such as tennis, <em class="italic">Formula 1</em>, or <em class="italic">soccer</em> <a id="_idIndexMarker1054"/>would be semantically <a id="_idIndexMarker1055"/>more similar to each other than topics such as politics, economics, or science. Yet, in terms of the previously discussed techniques, all encoded categories are seen as semantically the same. In this <a id="_idIndexMarker1056"/>section, we will discuss a simple method of semantic embedding, which is also called <strong class="bold">word embedding</strong>.</p>
			<p>The previously discussed pipeline using LSA transforms multiple documents into terms and then transforms those terms into semantic concepts that can be compared with other documents. However, the semantic meaning is based on the term occurrences and importance—there is no measurement of semantics between individual terms.</p>
			<p>Hence, what we are looking for is an embedding of terms into numerical multidimensional space such that each word represents one point in this space. This allows us to compute a numerical distance between multiple words in this space in order to compare the semantic meaning of two words. The most interesting benefit of word embeddings is that algebra on the word embeddings is not only numerically possible but also makes sense. Consider the following example:</p>
			<p class="source-code">King – Man + Woman = Queen</p>
			<p>We can create such an embedding by mapping a corpus of words on an N-dimensional numeric space and optimizing the numeric distance based on the word semantics—for example, based on the distance between words in a corpus. The resulting optimization outputs a dictionary of words in the corpus and their numeric N-dimensional representation. In this numeric space, words have the same, or at least similar, properties as in the semantic space. A great benefit is that these embeddings can be trained unsupervised, so no training data has to be labeled.</p>
			<p>One of the first embeddings <a id="_idIndexMarker1057"/>is called <strong class="bold">Word2Vec</strong> and is based on a continuous bag-of-words model or a continuous skip-gram model to count and measure the words in a window. Let's try this functionality and perform a semantic word embedding using Word2Vec:</p>
			<ol>
				<li value="1">The <a id="_idIndexMarker1058"/>best Python implementation <a id="_idIndexMarker1059"/>for word embeddings is <strong class="bold">Gensim</strong>, which we will also use here. We need to <a id="_idIndexMarker1060"/>feed our tokens into the model in order to train it:<p class="source-code">from gensim.models import Word2Vec</p><p class="source-code">model = Word2Vec(words, size=100, window=5)</p><p class="source-code">vector = model.wv['ground']</p></li>
			</ol>
			<p>In the preceding code, we load the <strong class="source-inline">Word2Vec</strong> model and initialize it with the list of tokens from the previous sections, which is stored in the <strong class="source-inline">words</strong> variable. The <strong class="source-inline">size</strong> attribute defines the dimension of the resulting vectors, and the <strong class="source-inline">window</strong> parameter decides how many words we should consider per window. Once the model has been trained, we can simply look up the word embedding in the model's dictionary.</p>
			<p>The code will automatically train the embedding on the set of tokens we provided. The resulting model stores the word-to-vector mapping in the <strong class="source-inline">wv</strong> property. Optimally, we also use a large corpus or pretrained model that is either provided by <strong class="source-inline">gensim</strong> or another NLP library, such as <strong class="source-inline">NLTK</strong>, to train the embedding and fine-tune it with a smaller dataset.</p>
			<ol>
				<li value="2">Next, we can use the trained model to embed all the terms from our document using the Word2Vec embedding. However, this will result in multiple vectors as each word returns its own embedding. Therefore, you need to combine all the vectors into a single vector using the mathematical mean of all the embeddings. This procedure is quite similar to the one used to generate a concept in LSA. Also, other reduction techniques are possible; for example, weighing the individual embedding vectors using their TF-IDF values:<p class="source-code">dim = len(model.wv.vectors[0])</p><p class="source-code">X = np.mean([model.wv[w] for w in words if w in model.wv] \</p><p class="source-code">        or [np.zeros(dim)], axis=0)</p></li>
			</ol>
			<p>In the <a id="_idIndexMarker1061"/>preceding function, we <a id="_idIndexMarker1062"/>compute the mean from all the word embedding vectors of the terms—this is called a <strong class="bold">mean embedding</strong>, and <a id="_idIndexMarker1063"/>it represents the concept of this document in the embedding space. If a word is not found in the embedding, we need to replace it with zeros in the computation.</p>
			<p>You can use such a semantic embedding for your application by downloading a pretrained embedding, for example, on the Wikipedia corpus. Then, you can loop through your sanitized input tokens and look up the words in the dictionary of the numeric embedding.</p>
			<p>GloVe is another popular technique for encoding words as numerical vectors, developed by Stanford University. In contrast to the continuous window-based approach, it uses global word-to-word co-occurrence statistics to determine the linear relationships between words:</p>
			<ol>
				<li value="1">Let's take a look at the pretrained 6 B tokens embedding trained on Wikipedia and the Gigaword news archive:<p class="source-code"># download pre-trained dictionary from </p><p class="source-code"># http://nlp.stanford.edu/data/glove.6B.zip</p><p class="source-code">glove = {}</p><p class="source-code">with open('glove.6B.100d.txt') as f:</p><p class="source-code">  for line in f:</p><p class="source-code">    word, coefs = line.split(maxsplit=1)</p><p class="source-code">    coefs = np.fromstring(coefs, 'f', sep=' ')</p><p class="source-code">    glove[word] = coefs</p></li>
			</ol>
			<p>In the preceding code, we only open and parse the pretrained word embedding in order to store the word and vectors in a lookup dictionary.</p>
			<ol>
				<li value="2">Then, we <a id="_idIndexMarker1064"/>use the dictionary <a id="_idIndexMarker1065"/>to look up tokens in our training data and merge them by computing the mean of all GloVe vectors:<p class="source-code">X = np.mean([glove[w] for w in words if w in glove] \</p><p class="source-code">      or [np.zeros(dim)], axis=0)</p></li>
			</ol>
			<p>The preceding code works very similar to before and returns one vector per word, which is aggregated by taking their mean at the end. Again, this corresponds with a semantic concept using all the tokens of the training data.</p>
			<p>Gensim provides <a id="_idIndexMarker1066"/>other popular models for semantic embeddings, such as <em class="italic">doc2word</em>, <em class="italic">fastText</em>, and <em class="italic">GloVe</em>. The <strong class="source-inline">gensim</strong> Python library is a great place for utilizing these pretrained embeddings or for training your own models. Now you can replace your bag-of-words model with a mean embedding of the word vectors to also capture word semantics. However, your pipeline will still be built out of many tunable components.</p>
			<p>In the next section, we will take a look at building end-to-end state-of-the-art language models and reusing some of the language features from Azure Cognitive Services.</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor130"/>Implementing end-to-end language models</h1>
			<p>In the <a id="_idIndexMarker1067"/>previous sections, we trained and concatenated multiple pieces to implement a final algorithm where most of the individual steps need to be trained as well. Lemmatization contains a dictionary of conversion rules. Stop words are stored in the dictionary. Stemming needs rules for each language and word that the embedding needs to train—TF-IDF and SVD are only computed on your training data but are independent of each other.</p>
			<p>This is a similar problem to the traditional computer vision approach, which we will discuss in more depth in <a href="B17928_10_ePub.xhtml#_idTextAnchor165"><em class="italic">Chapter 10</em></a>, <em class="italic">Training Deep Neural Networks on Azure</em>, where many classic algorithms are combined into a pipeline of feature extractors and classifiers. Similar to breakthroughs of end-to-end models trained via gradient descent and backpropagation in computer vision, deep neural networks—especially sequence-to-sequence models—have replaced the classical approach of performing each step of the transformation and training process manually.</p>
			<p>In this section, first, we will take a look at improving our previous model using custom embedding <a id="_idIndexMarker1068"/>and an LSTM implementation to model a token sequence. This will give you a good understanding of how we are moving from an individual preprocessor-based pipeline to a full end-to-end approach using deep learning.</p>
			<p>Sequence-to-sequence <a id="_idIndexMarker1069"/>models are models based on encoders and decoders that are trained on a variable set of inputs. This encoder/decoder architecture is used for a variety of tasks, such as machine translation, image captioning, and summarization. A nice benefit of these models is that you can reuse the encoder part of this network to convert a set of inputs into a fixed-set numerical representation of the encoder.</p>
			<p>Next, we will look at the state-of-the-art language representation models and discuss how they can be used for feature engineering and the preprocessing of your text data. We will use BERT to perform sentiment analysis and numeric embedding.</p>
			<p>Finally, we will also look at reusing the Azure Cognitive Services APIs for text analytics to carry out advanced modeling and feature extraction, such as text or sentence sentiment, keywords, or entity recognition. This is a nice approach because you can leverage the know-how and amount of training data from Microsoft to perform complex text analytics using a simple HTTP request.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor131"/>The end-to-end learning of token sequences</h2>
			<p>Instead of concatenating different pieces of algorithms into a single pipeline, we want to build and <a id="_idIndexMarker1070"/>train an end-to-end model that can train the word embedding, pre-form latent semantic transformation, and capture sequential information in the text in a single model. </p>
			<p>The benefit of such a model is that each processing step can be fine-tuned for the user's prediction task in a single combined optimization process:</p>
			<ol>
				<li value="1">The first part of the pipeline will look extremely similar to the previous sections. We will build a tokenizer that converts documents into sequences of tokens that are then <a id="_idIndexMarker1071"/>transformed into a numerical model based on the token sequence. Then, we will use <strong class="source-inline">pad_sequences</strong> to align all of the documents to the same length:<p class="source-code">from tensorflow.keras.preprocessing.text import Tokenizer</p><p class="source-code">from tensorflow.keras.preprocessing.sequence import \</p><p class="source-code">       pad_sequences</p><p class="source-code">num_words = 1000</p><p class="source-code">tokenizer = <strong class="bold">Tokenizer(num_words=num_words)</strong></p><p class="source-code">tokenizer.fit_on_texts(X_words)</p><p class="source-code">X = tokenizer.texts_to_sequences(X_words)</p><p class="source-code">X = pad_sequences(X, maxlen=2000)</p></li>
				<li>In the next step, we will build a simple model using Keras, an embedding layer, and an LSTM layer to capture token sequences. The embedding layer will perform a similar operation to GloVe, where the words will be embedded into a semantic space. The LSTM cell will ensure that we are comparing sequences of words instead of single words at a time. Then, we will use a dense layer with a <em class="italic">softmax</em> activation to implement a classifier head:<p class="source-code">from tensorflow.keras.layers import Embedding, LSTM, Dense</p><p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">embed_dim = 128</p><p class="source-code">lstm_out = 196</p><p class="source-code">model = Sequential()</p><p class="source-code">model.add(<strong class="bold">Embedding</strong>(</p><p class="source-code">    num_words, embed_dim, input_length=X.shape[1]))</p><p class="source-code">model.add(<strong class="bold">LSTM</strong>(</p><p class="source-code">    lstm_out, recurrent_dropout=0.2, dropout=0.2))</p><p class="source-code">model.add(<strong class="bold">Dense</strong>(</p><p class="source-code">    len(labels), activation='softmax'))</p><p class="source-code">model.compile(loss='categorical_crossentropy', </p><p class="source-code">              optimizer='adam',</p><p class="source-code">              metrics=['categorical_crossentropy'])</p></li>
			</ol>
			<p>As you <a id="_idIndexMarker1072"/>can see in the preceding function, we build a simple neural network using three layers (that is, <strong class="source-inline">Embedding</strong>, <strong class="source-inline">LSTM</strong>, and <strong class="source-inline">Dense</strong>) and a <strong class="source-inline">softmax</strong> activation for classification. This means that in order to train this model, we would also need a classification problem to be solved at the same time. Hence, we do need labeled training data to perform analysis using this approach. In the next section, we will examine how sequence-to-sequence models are used in input-output text sequences to learn an implicit text representation.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor132"/>State-of-the-art sequence-to-sequence models</h2>
			<p>In recent years, another <a id="_idIndexMarker1073"/>type of model has replaced the traditional NLP pipelines—transformer-based models. These types of models are fully end-to-end and use sequence-to-sequence mapping, positional encoding, and multi-head attention layers. This allows the models to look forward and backward in a text, pay attention to specific patterns, and learn tasks fully end to end. As you might be able to tell, these models have complex architectures and usually have well over 100 million or over 1 billion parameters.</p>
			<p>Sequence-to-sequence models are now state of the art for many complex end-to-end NLP problems such as classification (for example, sentiment or text analysis), language understanding (for example, entity recognition), translation, text generation, summarization, and more.</p>
			<p>One popular <a id="_idIndexMarker1074"/>sequence-to-sequence model is BERT, which, today, exists in <a id="_idIndexMarker1075"/>many different variations and configurations. Models based on the BERT architecture seem to perform particularly well but have already been outperformed by newer updated architectures, tuned parameters, or models with more training data.</p>
			<p>The easiest way to get started using these new NLP models is with the <em class="italic">Hugging Face</em> <strong class="source-inline">transformers</strong> library, which provides end-to-end models (or pipelines) along with pretrained tokenizers and models. The <strong class="source-inline">transformers</strong> library implements all model architectures for both <em class="italic">TensorFlow</em> and <em class="italic">PyTorch</em>. The models can be easily consumed and used in an application, trained from scratch, or fine-tuned using domain-specific custom training data.</p>
			<p>The following example shows how to implement sentiment analysis using the default <strong class="source-inline">sentiment-analysis</strong> pipeline, which, at the time of writing, uses the <strong class="source-inline">TFDistilBertForSequenceClassification</strong> model:</p>
			<p class="source-code">from transformers import pipeline</p>
			<p class="source-code">classifier = <strong class="bold">pipeline("sentiment-analysis")</strong></p>
			<p class="source-code">result = classifier("Azure ML is quite good.")[0]</p>
			<p class="source-code">print("Label: %s, with score: %.2f" %</p>
			<p class="source-code">         (result['label'], result['score']))</p>
			<p>As you can see in the previous example, it's very simple to use a pretrained model for an end-to-end prediction task. These three lines of code can easily be integrated into your feature extraction pipeline to enrich your training data with sentiments.</p>
			<p>Besides end-to-end models, another popular application of NLP is to provide semantic embeddings for textual data during preprocessing. This can also be implemented using the <strong class="source-inline">transformers</strong> library and any of the many supported models.</p>
			<p>To do this, first, we initialize a pretrained tokenizer for BERT. This will help us to split the input data into the correct format for the BERT model:</p>
			<p class="source-code">from transformers import BertTokenizer</p>
			<p class="source-code">tokenizer = BertTokenizer.from_pretrained('bert-base-cased')</p>
			<p class="source-code">inputs = tokenizer("Azure ML is quite good.",   </p>
			<p class="source-code">                   return_tensors="tf")</p>
			<p>Once we <a id="_idIndexMarker1076"/>have transformed the input into a token sequence, we can evaluate the BERT model. To retrieve the numerical embedding, we need to understand the latent state of the encoder, which we can retrieve using the <strong class="source-inline">last_hidden_state</strong> property:</p>
			<p class="source-code">from transformers import TFBertModel</p>
			<p class="source-code">model = TFBertModel.from_pretrained('bert-base-uncased')</p>
			<p class="source-code">outputs = model(**inputs)</p>
			<p class="source-code">print(outputs.last_hidden_state)</p>
			<p>The last hidden layer contains the latent representation of the model, which we can now use as a semantic numerical representation in our model:</p>
			<p class="source-code">&lt;tf.Tensor: shape=(1, 10, 768), dtype=float32, numpy=</p>
			<p class="source-code">array([[[-0.30760652,  0.19552925,  0.1440584 , ...,  0.08283961,</p>
			<p class="source-code">          0.16151786,  0.23049755],…</p>
			<p>The key takeaway from these models is that they use an encoder/decoder-based architecture, which allows us to simply borrow the encoder to embed text into a semantic numerical feature space. Hence, a common approach is to download the pretrained model and perform a forward pass through the encoder part of the network. The fixed-sized numerical output can now be used as a feature vector for any other model. This is a common preprocessing step and a good trade-off for using a state-of-the-art language model for numerical embedding.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor133"/>Text analytics using Azure Cognitive Services</h2>
			<p>A good approach in many engineering disciplines is to not reinvent the wheel when many <a id="_idIndexMarker1077"/>other companies have already solved the same problem far better than you will ever be able to solve it. This might be the case <a id="_idIndexMarker1078"/>for basic text analytics and text understanding tasks that Microsoft has developed, implemented, and trained and now offers as a service.</p>
			<p>What if I told you that when working with Azure, text understanding features such as sentiment analysis, key phrase extraction, language detection, named entity recognition, and <a id="_idIndexMarker1079"/>the extraction of <strong class="bold">Personally Identifiable Information</strong> (<strong class="bold">PII</strong>) is just one request away? Azure provides the Text Analytics API as part of Cognitive Services, which will solve all of these problems for you.</p>
			<p>This won't solve the need to transform a piece of text into numerical values, but it will make it easier to extract semantics from your text. One example would be to perform a key phrase extraction or sentiment analysis using Cognitive Services as an additional feature engineering step, instead of implementing your own NLP pipeline.</p>
			<p>Let's implement a function that returns the sentiment for a given document using the Text Analytics API of Cognitive Services. This is great when you want to enrich your data with additional attributes, such as overall sentiment, in the text. Let's start by setting up all the parameters we will need to call the Cognitive Services API:</p>
			<p class="source-code">import requests</p>
			<p class="source-code">region='westeurope'</p>
			<p class="source-code">language='en'</p>
			<p class="source-code">version='v3.1'</p>
			<p class="source-code">key = '&lt;insert access key&gt;'</p>
			<p class="source-code">url = "https://{region}.api.cognitive.microsoft.com" + \</p>
			<p class="source-code">    + "/text/analytics/{version}/sentiment".format(</p>
			<p class="source-code">           region=region, version=version)</p>
			<p>Next, we <a id="_idIndexMarker1080"/>define the content and metadata <a id="_idIndexMarker1081"/>of the request. We create a <strong class="source-inline">payload</strong> object that contains a single document and the text we want to analyze:</p>
			<p class="source-code">params = {</p>
			<p class="source-code">    'showStats': False</p>
			<p class="source-code">}</p>
			<p class="source-code">headers = {</p>
			<p class="source-code">    'Content-Type': 'application/json',</p>
			<p class="source-code">    'Ocp-Apim-Subscription-Key': key</p>
			<p class="source-code">}</p>
			<p class="source-code">payload = {</p>
			<p class="source-code">    'documents': [{</p>
			<p class="source-code">        'id': '1',</p>
			<p class="source-code">        <strong class="bold">'text': 'This is some input text that I love.',</strong></p>
			<p class="source-code">        'language': language   </p>
			<p class="source-code">    }]</p>
			<p class="source-code">}</p>
			<p>Finally, we need to send the payload, heads, and parameters to the Cognitive Services API:</p>
			<p class="source-code">response = requests.post(url,</p>
			<p class="source-code">                         json=payload,</p>
			<p class="source-code">                         params=params,</p>
			<p class="source-code">                         headers=headers)</p>
			<p class="source-code">result = response.json()</p>
			<p class="source-code">print(result)</p>
			<p>The preceding code looks very similar to the computer vision example that we saw in <a href="B17928_02_ePub.xhtml#_idTextAnchor034"><em class="italic">Chapter 2</em></a>, <em class="italic">Choosing the Right Machine Learning Service in Azure</em>. In fact, it uses the same <a id="_idIndexMarker1082"/>API but just a different endpoint <a id="_idIndexMarker1083"/>for Text Analytics and, in this case, sentiment analysis functionality. Let's run this code and look at the output, which looks very similar to the following snippet:</p>
			<p class="source-code">{</p>
			<p class="source-code">  'documents': [{</p>
			<p class="source-code">    'id': '1',</p>
			<p class="source-code">    'sentiment': 'positive',</p>
			<p class="source-code">    'confidenceScores': {</p>
			<p class="source-code">      'positive': 1.0,</p>
			<p class="source-code">      'neutral': 0.0,</p>
			<p class="source-code">      'negative': 0.0},</p>
			<p class="source-code">  ...}],</p>
			<p class="source-code">  ...</p>
			<p class="source-code">}</p>
			<p>We can observe that the JSON response contains a sentiment classification for each document (<strong class="source-inline">positive</strong>, <strong class="source-inline">neutral</strong>, and <strong class="source-inline">negative</strong>) as well as numeric confidence scores for each class. Also, you can see that the resulting documents are stored in an array and marked with an <strong class="source-inline">id</strong> value. Hence, you can send multiple documents to this API using an ID to identify each document.</p>
			<p>Using custom pretrained language models is great, but for standardized text analytics, we can simply reuse Cognitive Services. Microsoft has invested tons of resources into the research and production of these language models, which you can use for your own data pipelines for a relatively small amount of money. Therefore, if you prefer using a managed <a id="_idIndexMarker1084"/>service instead of running your <a id="_idIndexMarker1085"/>customer transformer model, you should try this Text Analytics API.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor134"/>Summary</h1>
			<p>In this chapter, you learned how to preprocess textual and categorical nominal and ordinal data using state-of-the-art NLP techniques.</p>
			<p>You can now build a classical NLP pipeline with stop word removal, <em class="italic">lemmatization</em> and <em class="italic">stemming</em>, <em class="italic">n-grams</em>, and count term occurrences using a <em class="italic">bag-of-words</em> model. We used <em class="italic">SVD</em> to reduce the dimensionality of the resulting feature vector and to generate lower-dimensional topic encoding. One important tweak to the count-based bag-of-words model is to compare the relative term frequencies of a document. You learned about the <em class="italic">TF-IDF</em> function and can use it to compute the importance of a word in a document compared to the corpus.</p>
			<p>In the following section, we looked at <em class="italic">Word2Vec</em> and <em class="italic">GloVe</em>, which are pretrained dictionaries of numeric word embeddings. Now you can easily reuse a pretrained word embedding for commercial NLP applications with great improvements and accuracy due to the semantic embedding of words.</p>
			<p>Finally, we finished the chapter by looking at a state-of-the-art approach to language modeling, using end-to-end language representations, such as <em class="italic">BERT</em> and BERT-based architectures, which are trained as sequence-to-sequence models. The benefit of these models is that you can reuse the encoder to transform a sequence of text into a numerical representation, which is a very common task during feature extraction.</p>
			<p>In the next chapter, we will look at how to train an ML model using Azure Machine Learning, applying everything we have learned so far.</p>
		</div>
	</div>
</div>
</body></html>