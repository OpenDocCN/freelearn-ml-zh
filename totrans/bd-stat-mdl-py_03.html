<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-51"><a id="_idTextAnchor055"/>3</h1>
<h1 id="_idParaDest-52"><a id="_idTextAnchor056"/>Hypothesis Testing</h1>
<p>In this chapter, we will begin discussing drawing statistical conclusions from data, putting together sampling and experiment design from <a href="B18945_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a><em class="italic">, Sampling and Generalization</em> and distributions from <a href="B18945_02.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a><em class="italic">, Distributions of Data</em>. Our primary use of statistical modeling is to answer questions of interest from data. Hypothesis testing provides a formal framework for answering questions of interest with measures of uncertainty. First, we will cover the goals and structure of hypothesis testing. Then, we will talk about the errors that can occur from hypothesis tests and define the expected error rate. Then, we will walk through the hypothesis test process utilizing the z-test. Finally, we will discuss statistical power analysis.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>The goal of hypothesis testing</li>
<li>Type I and type II errors</li>
<li>Basics of the z-test – the z-score, z-statistic, critical values, and p-values</li>
<li>One-sample and two-sample z-tests for means and proportions</li>
<li>Selecting error rate and power analysis</li>
<li>Applying the power analysis to the z-test</li>
</ul>
<h1 id="_idParaDest-53"><a id="_idTextAnchor057"/>The goal of hypothesis testing</h1>
<p>Put simply, the goal of<a id="_idIndexMarker219"/> hypothesis testing is to decide whether the data we have is sufficient to support a particular hypothesis. The hypothesis test provides a <em class="italic">formal</em> framework for testing a hypothesis based on our data rather than attempting to decide based on visual inspection. In this section, we will discuss the process of hypothesis testing. In the next section, <em class="italic">Basics of the z-test – the z-score, z-statistic, critical values, and p-values</em>, we will put the<a id="_idIndexMarker220"/> process to work by walking through an example in detail with the z-test.</p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor058"/>Overview of a hypothesis test for the mean</h2>
<p>To<a id="_idIndexMarker221"/> understand the hypothesis testing process, let’s start with a simple example. Suppose we have a factory with machines that produce widgets, and we expect our machines to produce widgets at a<a id="_idIndexMarker222"/> certain rate (30 widgets per hour). We start by <a id="_idIndexMarker223"/>constructing two hypotheses, the <strong class="bold">null hypothesis</strong> and the <strong class="bold">alternative hypothesis</strong>. The null hypothesis and alternative hypothesis are given the following symbols, respectively: H 0 and H a. To create the null hypothesis, we will start by assuming what we want to test is true. In our example, the null hypothesis would be that <em class="italic">the machines output a mean of 30 widgets per hour</em>. Once we have determined the null hypothesis, we then create the alternative hypothesis, which is just the contradiction of the null hypothesis. In our example, the alternative hypothesis is that <em class="italic">the machines do not output a mean of 30 widgets per hour</em>. Notice that our hypotheses do not indicate any directionality, that is, the alternative hypothesis contains values both less than and greater than the expected value. This is called a <strong class="bold">two-sided test</strong>, meaning <a id="_idIndexMarker224"/>there are two alternatives to the null hypothesis. We also <a id="_idIndexMarker225"/>have <strong class="bold">one-sided tests</strong>. For example, if we had said the null hypothesis was that <em class="italic">the machines output more than a mean of 30 widgets per hour</em>, the alternative would be that <em class="italic">the machines output less than a mean of 30 widgets per hour</em>. This set of hypotheses would be a one-sided test.</p>
<p>The two-sided null hypothesis and alternative hypothesis from our example can be stated mathematically as follows:</p>
<p>H 0 :  _ x  = 30</p>
<p>H a :  _ x  ≠ 30</p>
<p>Once we have the null and alternative hypotheses and the data, we will run the test with software. Let’s set the implementation details of the test aside for now (these will be covered in detail in the next section). There are two possible outcomes of a statistical test: reject the null hypothesis or fail to reject the null hypothesis. If the mean output of our machines is statistically different from the stated value in the null hypothesis, then we will <em class="italic">reject the null hypothesis</em>. This means that, given the data, the value stated in the null hypothesis is not a <em class="italic">plausible</em> value for the mean. However, if the mean output of our machines is not statistically different from the value listed in the null hypothesis, we <em class="italic">fail to reject the null hypothesis</em>. This means that, given the data, the value stated in the null hypothesis is a <em class="italic">plausible</em> value for the mean. After running the test for the null hypothesis and determining the conclusion, we will provide a confidence interval (discussed in the next section) and determine the scope of inference.</p>
<h2 id="_idParaDest-55"><a id="_idTextAnchor059"/>Scope of inference</h2>
<p>The scope of inference<a id="_idIndexMarker226"/> is determined by the sampling design discussed in <a href="B18945_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>. There are two questions to consider –<em class="italic"> what is the population, and how was the sample selected from the population?</em> In this example, let’s assume that we are testing the mean output of machines from a large factory (possibly hundreds of machines). The population is then the machines in the factory. If we take a random sample of machines, then our conclusion can be extrapolated to the entire population.</p>
<p>While our current example is realistic, it is rather simple. In other scenarios, there may be additional considerations. For example, the machines in the factory may have different models and different ages, which may impact output. In that case, we could use stratified random sampling and make inferences for each stratum.</p>
<h2 id="_idParaDest-56"><a id="_idTextAnchor060"/>Hypothesis test steps</h2>
<p>This <a id="_idIndexMarker227"/>section provides an overview of hypothesis testing from creating a hypothesis to drawing a conclusion. As we continue through this chapter, keep the following key hypothesis test steps in mind:</p>
<ul>
<li>State the null hypothesis and alternative hypothesis</li>
<li>Perform the statistical test</li>
<li>Determine the conclusion: reject or fail to reject the null hypothesis</li>
<li>Provide a statistical conclusion, a confidence interval, and a scope of inference</li>
</ul>
<p>These steps are applicable to any statistical test, and we will continue to follow this series of steps for hypothesis tests. In the next section, we will discuss the types of errors that can result from hypothesis tests.</p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor061"/>Type I and Type II errors</h1>
<p>While data can give us a good idea of the characteristics of a distribution, it is possible for a hypothesis test to result in an error. Errors can occur because we are taking a random sample from a population. While randomization makes it less likely that a sample contains sampling bias, <em class="italic">there is no guarantee that a random sample will be representative of the population</em>. There are two possible errors that could occur as a result of a hypothesis test:</p>
<ul>
<li><strong class="bold">Type I error</strong>: Rejecting the<a id="_idIndexMarker228"/> null hypothesis when it is actually true</li>
<li><strong class="bold">Type II error</strong>: Failure to<a id="_idIndexMarker229"/> reject the null hypothesis when it is actually false</li>
</ul>
<h2 id="_idParaDest-58"><a id="_idTextAnchor062"/>Type I errors</h2>
<p>A type I error occurs <a id="_idIndexMarker230"/>when a hypothesis test results in <em class="italic">rejecting the null hypothesis, but the null hypothesis is actually true</em>. For example, say we have a distribution of data with a population mean of 30. We state our null hypothesis as H 0 :  _ x  = 30. We take a random sample for our test, but the random values in the sample happen to be on the higher side of the distribution. Thus, the test result suggests that we should reject the null hypothesis. In this case, we have made a type I error. This type of error is also called<a id="_idIndexMarker231"/> a <strong class="bold">false positive</strong>.</p>
<p>When we make statistical tests, it is always possible that we will come to an incorrect conclusion due to the data sampled from the target population. The <em class="italic">probability of making a type one error is specified by</em> α. Said another way, α represents how often we expect to make an error (the expected error rate). This is a free parameter we can select for our test (α is also called the <em class="italic">level of significance</em>). It is common to use 0.05 for α, but there is no evidential basis for using 0.05; different values may be appropriate in other contexts. Later in the chapter, we will discuss selecting the type I error rate.</p>
<h2 id="_idParaDest-59"><a id="_idTextAnchor063"/>Type II errors</h2>
<p>The other type of error we can make is<a id="_idIndexMarker232"/> called a type II error. In this case, <em class="italic">we fail to reject the null hypothesis when it is actually false</em>. Let’s consider another example. Say we have a distribution of data and we want to test whether the mean of the distribution is 30 or not. We take a random sample for the test and the test suggests that we should not reject the null hypothesis. However, the true population mean is 35. In this case, we<a id="_idIndexMarker233"/> have made a type II error. This type of error is also called a <strong class="bold">false negative</strong>.</p>
<p>As mentioned previously, it is <a id="_idIndexMarker234"/>always possible that a statistical test could lead to an erroneous conclusion. Thus, we want to control the probability of making an error. However, unlike α, the Type II error rate β is not simply a free parameter that we can select. To understand the likeliness of making a type II error, we generally will conduct a power analysis, which will show how various factors, such as sample size, will impact the type II error rate. In the next section, we will discuss selecting the error rate and power analysis.</p>
<p>We can summarize the possible results of a hypothesis test with the help of the table in <em class="italic">Figure 3</em><em class="italic">.1</em>.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style"/>
<td class="No-Table-Style" colspan="2">
<p>Null Hypothesis is:</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p>True</p>
</td>
<td class="No-Table-Style">
<p>False</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style" rowspan="2">
<p>Decision about Null Hypothesis</p>
</td>
<td class="No-Table-Style">
<p>Don’t Reject</p>
</td>
<td class="No-Table-Style">
<p>Correct Inference</p>
<p>(1- α)</p>
</td>
<td class="No-Table-Style">
<p>Type II Error</p>
<p>(β)</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Reject</p>
</td>
<td class="No-Table-Style">
<p>Type I Error</p>
<p>(α)</p>
</td>
<td class="No-Table-Style">
<p>Correct Inference</p>
<p>(1-β)</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Results of the hypothesis test</p>
<p>In this section, we have discussed the types of errors that can occur when drawing conclusions from statistical tests. In the next section, we will walk through an example of hypothesis testing with the z-test and later in the chapter, we will discuss how to select an error rate and how to analyze statistical power and related factors.</p>
<h1 id="_idParaDest-60"><a id="_idTextAnchor064"/>Basics of the z-test – the z-score, z-statistic, critical values, and p-values</h1>
<p>In this section, we will discuss a type of hypothesis test called <a id="_idIndexMarker235"/>the z-test. It is a statistical procedure using sample data assumed to be normally distributed to determine whether a statistical statement related to the value of a population parameter should be rejected or not. The test can be performed on the following:</p>
<ul>
<li>One sample (a left-tailed z-test, right-tailed z-test, or two-tailed z-test)</li>
<li>Two samples (a two-sample z-test)</li>
<li>Proportions (a one-proportion z-test or two-proportion z-test)</li>
</ul>
<p>The test assumes that the standard deviation is known and the sample size is large enough. In practice, a sample size that is larger than 30 should be considered.</p>
<p>Before going into different types of z-tests, we will discuss the z-score and z-statistic.</p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor065"/>The z-score and z-statistic</h2>
<p>To<a id="_idIndexMarker236"/> measure how far a particular value from a mean is, we could use the z-score or the z-statistic as statistical techniques together with the mean and the standard deviation to determine the relative location.</p>
<p>A z-score<a id="_idIndexMarker237"/> is computed with the following formula:</p>
<p>z i =  x i −  _ x  _ σ </p>
<p>Here, z i is the z-score for x i,  _ x  is the sample mean, and σ is the sample standard deviation. The z-score is also known as the z-value, standardized value, or standard score. Let’s consider a few examples. The standard deviation tells us how far a sample is from the mean of the distribution. If z i = 1.8, that point is 1.8 standard deviations away from the mean. Similarly if  z i = − 1.5, then that point is 1.5 standard deviations away from the mean. The sign of the determines whether it is greater or less than the sample mean. A z i of -1.5 is less than the mean and a z i of 1.8 is greater than the mean. Now let us go through an example. In a high school in Dallas (in the US), we ask students to take anonymous IQ tests for some statistical research. The data collected from that school is normally distributed with an IQ score population mean μ = 98 and a population standard deviation σ = 12. A particular student took an IQ test and his score is 110. He has an IQ score greater than the score mean but he wants to know whether he is in the top 5%. First, we will use the<a id="_idIndexMarker238"/> z-score formula to calculate it:</p>
<p>z student = 110 − 98 _ 12  = 12 _ 12 = 1.</p>
<p>The student can check a <a id="_idIndexMarker239"/>z-table (<em class="italic">Figure 3</em><em class="italic">.2</em>), for example, from the website <a href="http://www.z-table.com">http://www.z-table.com</a>, and get the value of 0.8413. He is in the top 1-0.8413 = 0.1587 or 15.87% of his school IQ scores.</p>
<div><div><img alt="Figure 3.2 – z-table" height="542" src="img/B18945_03_002.jpg" width="682"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – z-table</p>
<p>In Python, we can use<a id="_idIndexMarker240"/> the <strong class="bold">cumulative distribution function</strong> (<strong class="bold">CDF</strong>) to calculate it:</p>
<pre class="source-code">
import scipy
round(scipy.stats.norm.cdf(1),4)
# 0.8413</pre>
<p>We also get the same value in Python as in the z-table check; it is 0.8413 for the z-score = 1 in this example.</p>
<p>Another example here is a simple random sample of 10 scores taken from the IQ survey:</p>
<p>90, 78, 110, 110, 99, 115, 130, 100, 95, 93</p>
<p>To compute the <a id="_idIndexMarker241"/>z-score for each IQ score in the sample, we need to calculate the mean and the standard deviation of this sample and then apply the z-score formula. Fortunately, we can use the <code>scipy</code> library again as follows:</p>
<pre class="source-code">
import pandas as pd
import numpy as np
import scipy.stats as stats
IQ = np.array([90, 78,110, 110, 99, 115,130, 100, 95, 93])
z_score = stats.zscore(IQ)
# Create dataframe
data_zscore = {
  "IQ score": IQ,
  "z-score": z_score
}
IQ_zscore = pd.DataFrame(data_zscore)
IQ_zscore</pre>
<p>We created an array of IQ scores called <code>IQ</code> and used <code>z-score</code> from <code>scipy.stats</code> to compute <code>z_score</code>. Finally, we created the following output DataFrame.</p>
<div><div><img alt="Figure 3.3 – Output DataFrame" height="339" src="img/B18945_03_003.jpg" width="197"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Output DataFrame</p>
<p>Before discussing the <a id="_idIndexMarker242"/>z-statistic, we will introduce the notion of sampling distributions. Going back to the last example, as a rule of thumb, we perform the process of selecting a simple random sample of 35 IQ scores from the pool of IQ scores of the high school repeatedly, as many times as needed for the study. We then compute the mean score of each sample, called  _ x . Because we have various samples selected, we also have various possible values of  _ x . The expected value of   _ x  is as follows:</p>
<p>E( _ x ) = μ</p>
<p>Here, μ is the population mean. σ  _ x  denotes the standard distribution of  _ x . Practically, in many sampling situations, a population is relatively large compared to small sample sizes. Then, the standard deviation of  _ x  can be given as follows:</p>
<p>σ  _ x  =  σ _ √ _ n  </p>
<p>Here, σ is the population standard deviation and n is the sample size for finite or infinite populations such that the population is large and the sample size is small, relatively. Note that σ  _ x  is also called the standard error of the mean to help us determine how far the sample mean is from the population mean. Note that E( _ x ) = μ, independent of the sample size. Sample size and standard error are inversely correlated: when the sample size is increased, the standard error decreases. Since the sampling distribution of  _ x  is assumed to be normally distributed, the sample distribution is given by the following formula:</p>
<p>z  _ x  =  ‾ x  − μ _ σ  ‾ x  ￼</p>
<p>In hypothesis tests about a population mean, we use test statistics where its formula is given as follows:</p>
<p>z = z  _ x  =   _ x  − μ _ σ/ √ _ n  </p>
<p>Here,  _ x  is the sample mean, μ is the population mean, σ is the population standard deviation, and n is the sample size.</p>
<p>Consider the IQ test example again. The IQ score data has a mean μ = 98 and a standard deviation σ = 12. Suppose the data is normally distributed. Let x be the score taken randomly from the IQ data. What is the probability that x is between 95 and 104? We will compute the z-scores when x = 95 and x = 104 as follows:</p>
<p>z 95 =  95 − 98 _ 12  = − 0.25,</p>
<p>z 100 =  104 − 98 _ 12  = 0.5.</p>
<p>Therefore, the<a id="_idIndexMarker243"/> probability that the taken score is between 95 and 104 is:</p>
<p>P(95 &lt; x &lt; 104) = P(− 0.25 &lt; z &lt; 0.5) = 0.6915 − 0.4013 = 0.2902.</p>
<p>Then, the probability is about 29.02% that an IQ score taken at random from the data is between 95 and 104. We also can get the values from a z-table as follows.</p>
<div><div><img alt="Figure 3.4 – z-table" height="717" src="img/B18945_03_004.jpg" width="683"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – z-table</p>
<div><div><img alt="Figure 3.5 – z-table" height="723" src="img/B18945_03_005.jpg" width="693"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – z-table</p>
<p>In Python, we can<a id="_idIndexMarker244"/> implement the code as follows:</p>
<pre class="source-code">
#calculate z scores at x=95 and 104
zscore_95 = round((95-98)/12,2)
zscore_104 = round((104-98)/12,2)
#calculate cdf and probability
cdf_95 = stats.norm.cdf(zscore_95)
cdf_104 = stats.norm.cdf(zscore_104)
prob = abs(cdf_95-cdf_104)
#print the probability
print(f"The probability that the taken score between 95 and 104 is {round(prob*100,2)}%!")</pre>
<p>Another question raised is what is the probability that the mean score  _ x  of four scores taken randomly is between 95 and 104? To solve this question, we use the notion of the z-statistic. We <a id="_idIndexMarker245"/>assume the mean of  _ x  is also 98 and  _ x  has a normal distribution. Then, the standard error of  _ x  is:</p>
<p>σ  _ x  =  σ _ √ _ n   =  12 _ √ _ 4   = 6,</p>
<p>then:</p>
<p>z =   _ x  − μ  _ x  _ σ  _ x   =   _ x  − 98 _ 6 .</p>
<p>It is easy to calculate z 95 = − 0.5 and z 104 = 1. By using the z-table, we can get the probability that the mean score  _ x  of six scores taken randomly is between 95 and 104 as follows:</p>
<p>0.8413 – 0.3085 = 0.5328</p>
<p>Or, about 53.28%. To use this idea, we can implement the code in Python as follows:</p>
<pre class="source-code">
import pandas as pd
import numpy as np
import scipy.stats as stats
import math
# standard error
n= 4
sigma = 12
se = sigma/math.sqrt(n)
#calculate z scores at x=95 and 104
zscore_95 = round((95-98)/se,2)
zscore_104 = round((104-98)/se,2)
#calculate cdf and probability
cdf_95 = stats.norm.cdf(zscore_95)
cdf_104 = stats.norm.cdf(zscore_104)
prob = abs(cdf_95-cdf_104)
#print the probability
print(f"The probability that the taken score between 95 and 104 is {round(prob*100,2)}%!")</pre>
<p>Note that in the preceding code, we also need the library<a id="_idIndexMarker246"/> called <code>math</code> for calculating the square root function, <code>math.sqrt()</code>.</p>
<p>In the following section, we will discuss a z-test for means.</p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor066"/>A z-test for means</h2>
<p>In this part, one-sample and<a id="_idIndexMarker247"/> two-sample z-tests related to a population mean or means of two populations respectively are considered.</p>
<h3>A one-sample z-test</h3>
<p>A selected <a id="_idIndexMarker248"/>sample for research from a population should be normally <a id="_idIndexMarker249"/>distributed. The population standard deviation is supposed to be known, at least for practical purposes.</p>
<p>This test is still applicable in cases where the population cannot be assumed to be normally distributed but the sample size needs to be considered large enough by a rule of thumb, based on the experiences of researchers involved in the study. To perform the hypothesis testing, we need to develop the null and alternative hypotheses. The following figure illustrates three null and alternative hypotheses corresponding to left-tailed, right-tailed, and two-tailed z-tests.</p>
<div><div><img alt="Figure 3.6 – Left-tailed, right-tailed, and two-tailed hypothesis tests" height="309" src="img/B18945_03_006.jpg" width="1098"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Left-tailed, right-tailed, and two-tailed hypothesis tests</p>
<p>H 0 : μ ≥ μ 0 H 0 : μ ≤ μ 0  H 0 : μ = μ 0</p>
<p>H a : μ &lt; μ 0 H a : μ &gt; μ 0  H a : μ ≠ μ 0</p>
<p>Next, we need to specify the level of significance, α, the probability of rejecting the null hypothesis when it is true. In other words, it is the probability of a type I error, aswe discussed in the last section. Then, we calculate the value of the test statistic. There are two approaches using a p-value or a critical value for the hypothesis testing.</p>
<p>In the p-value approach, we use the value of the test statistic to calculate a probability, denoted by the p-value, which takes on values as extreme as or more extreme than the test statistic derived from the sample. The smaller the p-value is, the more it indicates evidence against the null hypothesis, or in other words, the probability used to determine whether H 0 should be rejected. The rejection rule (reject H 0) is the p-value being less than or equal to the specified level of significance α in the research. In order to find the p-value based on the value of the test statistic in Python, we use the following syntax:</p>
<pre class="source-code">
 scipy.stats.norm.sf(abs(x))</pre>
<p>Here, <code>x</code> is the z-score. For example, we want to find the p-value associated with a z-score of -2.67 in a left-tailed test. The Python implementation is as follows:</p>
<pre class="source-code">
import scipy.stats
#find p-value
round(scipy.stats.norm.sf(abs(-2.67)),4)</pre>
<p>The output will be 0.0038. Similar Python code is used in the case of a right-tailed test. For a two-tailed test, we need to multiply the value by 2:</p>
<pre class="source-code">
#find p-value for two-tailed test
scipy.stats.norm.sf(abs(2.67))*2</pre>
<p>The following<a id="_idIndexMarker250"/> figure illustrates the idea of how the p-value is computed in each <a id="_idIndexMarker251"/>type of test.</p>
<div><div><img alt="Figure 3.7 – p-values in hypothesis testing" height="396" src="img/B18945_03_007.jpg" width="1183"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – p-values in hypothesis testing</p>
<p>The last step is to interpret the statistical conclusion.</p>
<p>On the other hand, in the critical value approach, we will need to compute a critical value for the test statistic by using the level of significance. Critical values are the boundaries of the critical region where we can reject the null hypothesis.</p>
<div><div><img alt="Figure 3.8 – Critical regions in hypothesis testing" height="300" src="img/B18945_03_008.jpg" width="1083"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Critical regions in hypothesis testing</p>
<p>To<a id="_idIndexMarker252"/> compute the <a id="_idIndexMarker253"/>critical value in Python, we use the following syntax:</p>
<pre class="source-code">
scipy.stats.norm.ppf(alpha)</pre>
<p>Here, <code>alpha</code> is the level of significance to be used. The following is the implementation of the code in Python for left-tailed, right-tailed, and two-tailed tests:</p>
<pre class="source-code">
import scipy.stats
alpha = 0.05 # level of significance
#find Z critical value for left-tailed test
print(f" The critical value is {scipy.stats.norm.ppf(alpha)}")
#find Z critical value for left-tailed test
print(f" The critical value is {scipy.stats.norm.ppf(1-alpha)}")
##find Z critical value for two-tailed test
print(f" The critical values are {-scipy.stats.norm.ppf(1-alpha/2)} and {scipy.stats.norm.ppf(1-alpha/2)}")</pre>
<p>Here’s the output of the preceding code:</p>
<p><code>The critical value </code><code>is -1.6448536269514729</code></p>
<p><code>The critical value </code><code>is 1.6448536269514722</code></p>
<p><code>The critical values are -1.959963984540054 </code><code>and 1.959963984540054</code></p>
<p>At the level of significance α = 0.05 for the left-tailed test, the critical value is about -1.64485. Since this is a left-tailed test, if the test statistic is less than or equal to this critical value, we reject the null hypothesis. Similarly, for the right-tailed test, if the test statistic is greater than or equal to 1.64485, we reject the null hypothesis. For the two-tailed test, we reject the null hypothesis if the test statistic is greater than or equal to 1.95996 or less than or equal to -1.95996.</p>
<p>After determining whether to reject the null hypothesis, we interpret the statistical conclusion.</p>
<p>Let us discuss the <a id="_idIndexMarker254"/>IQ test scores again in the high school in Dallas. The IQ score <a id="_idIndexMarker255"/>data has a mean of μ = 98 and a standard deviation of σ = 12. A researcher wants to know whether IQ scores will be affected by some IQ training. He recruits 30 students and trains them to answer IQ questions 2 hours per day for 30 days and records their IQ levels after finishing the training period. The IQ scores of 30 students after the training section are 95, 110, 105, 120, 125, 110, 98, 90, 99, 100,110, 112, 106, 92, 108, 97, 95, 99, 100, 100,103, 125, 122, 110, 112, 102, 92, 97, 89, and 102. Their average score is 104.17. We can easily implement the calculation in Python as follows:</p>
<pre class="source-code">
IQscores = [95,110, 105, 120, 125, 110, 98, 90, 99, 100,
            110, 112, 106, 92, 108, 97, 95, 99, 100, 100,
            103, 125, 122, 110, 112, 102, 92, 97, 89, 102]
IQmean = np.array(IQscores).mean()</pre>
<p>We define the null hypothesis and the alternative hypothesis:</p>
<p>H 0 : μ after = μ = 98</p>
<p>H a : μ after &gt; μ = 98</p>
<p>We choose the level of significance α=0.05. Previously, the critical value for the right-tailed test was 1.64485. We now calculate the test statistic on the problem:</p>
<p>z =   _ x  − μ _ σ/ √ _ n   =  104.17 − 98  _ 12/ √ _ 30   = 2.8162</p>
<p>It is implemented in Python as follows:</p>
<pre class="source-code">
n=30 #number of students
sigma =12 #population standard deviation
IQmean = 104.17 # IQ mean of 30 students after the training
mu = 98 # population mean
z = (IQmean-mu)/(sigma/math.sqrt(n))</pre>
<p>Since the test statistic value is 2.8162 &gt; 1.64485, we reject the null hypothesis. This means that the training does affect the IQ levels of these students and helps them improve their IQ scores.</p>
<p>We also can<a id="_idIndexMarker256"/> use<a id="_idIndexMarker257"/> the <code>ztest()</code> function from the <code>statsmodels</code> package (<em class="italic">Seabold, Skipper, and Josef Perktold, “statsmodels: Econometric and statistical modeling with python.” Proceedings of the 9th Python in Science Conference. 2010</em>) to perform one- or two-sample z-tests (which we will discuss in the next part). The syntax is as follows:</p>
<pre class="source-code">
statsmodels.stats.weightstats.ztest(x1, x2=None,
    value=0, alternative='two-sided')</pre>
<p>Here, we see the following:</p>
<ul>
<li><code>x1</code>: The first of the two independent samples</li>
<li><code>x2</code>: The second of the two independent samples (if performing a two-sample z-test)</li>
<li><code>value</code>: In the one -sample case, <code>value</code> is the mean of <code>x1</code> under the null hypothesis. In the two-sample case, <code>value</code> is the difference between the mean of <code>x1</code> and the mean of <code>x2</code> under the null hypothesis. The test statistic is <code>x1_mean - x2_mean - </code><code>value</code>.</li>
<li><code>alternative</code>: The alternative hypothesis:</li>
</ul>
<p><code>'two-sided'</code>: Two-sided test</p>
<p><code>'larger'</code>: Right-tailed test</p>
<p><code>'smaller'</code> : Left-tailed test</p>
<p>The following<a id="_idIndexMarker258"/> Python <a id="_idIndexMarker259"/>code shows how we perform a one-sample z-test:</p>
<pre class="source-code">
from statsmodels.stats.weightstats import ztest as ztest
#IQ scores after training sections
IQscores = [95,110, 105, 120, 125, 110, 98, 90, 99, 100,
            110, 112, 106, 92, 108, 97, 95, 99, 100, 100,
            103, 125, 122, 110, 112, 102, 92, 97, 89, 102]
#perform one sample z-test
z_statistic, p_value = ztest(IQscores, value=98, alternative = 'larger')
print(f"The test statistic is {z_statistic} and the
    corresponding p-value is {p_value}.")</pre>
<p>The test<a id="_idIndexMarker260"/> statistic is 3.3975 and the p-value = 0.00034 &lt; 0.05, where 0.05 is <a id="_idIndexMarker261"/>the level of significance. Therefore, we have enough evidence to reject the null hypothesis. This means that the training does affect the IQ levels of these students.</p>
<h3>Two-sample z-test</h3>
<p>We <a id="_idIndexMarker262"/>consider <a id="_idIndexMarker263"/>two normally distributed and independent populations. Let Ω be the hypothesized difference between two population means, μ 1 and μ 2. Similarly, as in the case of the one-sample z-test, we have three forms for the null and alternative hypotheses:</p>
<p>H 0 : μ 1 − μ 2 ≥ Ω  H 0 : μ 1 − μ 2 ≤ Ω  H 0 : μ 1 − μ 2 = Ω</p>
<p>H a : μ 1 − μ 2 &lt; Ω  H a : μ 1 − μ 2 &gt; Ω  H a : μ 1 − μ 2 ≠ Ω</p>
<p>In many problems, Ω = 0. That means that for the case of the two-tailed test, the null hypothesis is zero, or in other words, μ 1 and μ 2 are equal. The test statistic for hypothesis tests is computed as follows:</p>
<p>z =  (‾ x 1 − ‾ x 2) − Ω  ___________  √ _ σ 1 2 _ n 1  + σ 2 2 _ n 2   </p>
<p>Here, _ x 1 and _ x 2 are the sample means with the sample sizes n 1and n 2randomly taken from the two populations with the means μ 1 and μ 2, respectively. σ 1 and σ 2 are the standard deviations for these two populations. With two independent simple random samples, the point estimator _ x 1 − _ x 2 has a standard error given as follows:</p>
<p>σ ‾ x 1−‾ x 2 = √ _  σ 1 2 _ n 1  +  σ 2 2 _ n 2  </p>
<p>The <a id="_idIndexMarker264"/>distribution of _ x 1 − _ x 2 can be considered a normal distribution when the sample sizes are large enough. The step-by-step approach to the two-sample z-test hypothesis test is similar to that<a id="_idIndexMarker265"/> of the one-sample z-test.</p>
<p>Let us now consider an example. We study the IQ scores of students from two schools, named A and B, in Dallas, and we want to know whether the mean IQ levels for these two schools are different. A simple random sample of 30 students from each school is recorded.</p>
<p>A= [95,110, 105, 120, 125, 110, 98, 90, 99, 100,110, 112, 106, 92, 108, 97, 95, 99, 100, 100, 103, 125, 122, 110, 112, 102, 92, 97, 89, 102]</p>
<p>B = [98, 90, 100, 93, 91, 79, 90, 100, 121, 89, 101, 98, 75, 90, 95, 99, 100, 120, 121, 95,</p>
<p> 96, 89, 115, 99, 95, 121, 122, 98, 97, 97]</p>
<p>The null and alternative hypotheses are:</p>
<p>H 0 : μ 1 − μ 2 = 0</p>
<p>H a : μ 1 − μ 2 ≠ 0</p>
<p>We<a id="_idIndexMarker266"/> choose the level of significance α=0.05. In Python, by using the <code>ztest()</code> function of the <code>statsmodels</code> package, we <a id="_idIndexMarker267"/>perform the following calculation:</p>
<pre class="source-code">
from statsmodels.stats.weightstats import ztest as ztest
#IQ score
A= [95,110, 105, 120, 125, 110, 98, 90, 99, 100,
    110, 112, 106, 92, 108, 97, 95, 99, 100, 100,
    103, 125, 122, 110, 112, 102, 92, 97, 89,102] #school A
B = [98, 90, 100, 93, 91, 79, 90, 100, 121, 89,
     101, 98, 75, 90, 95, 99, 100, 120, 121, 95,
     96, 89, 115, 99, 95, 121, 122, 98, 97, 97]  # school B
#perform two- sample z-test
z_statistic, p_value = ztest(A, B, value=0, alternative = 'two-sided')
print(f"The test statistic is {z_statistic} and the corresponding p-value is {p_value}.")</pre>
<p>In the <a id="_idIndexMarker268"/>preceding code, we chose <code>alternative = 'two-sided'</code> related to the null and alternative hypotheses for the study. The test statistic and the p-value produced by the Python code are 1.757 and 0.079, respectively. Using a level of significance of 0.05, since the p-value &gt; 0.05, we fail to reject the null hypothesis. In other words, we do not have enough evidence to show that the IQ mean scores between the students from the two<a id="_idIndexMarker269"/> schools are different.</p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor067"/>z-test for proportions</h2>
<p>We can also test for differences in<a id="_idIndexMarker270"/> proportions. Let’s take a look at how to perform the z-test for proportions.</p>
<h3>A one-proportion z-test</h3>
<p>One-proportion z-tests <a id="_idIndexMarker271"/>are used to compare the difference between a sample<a id="_idIndexMarker272"/> proportion  _ p  and a hypothesized proportion p0. Similarly, as in a z-test for means, we have three forms for the null and alternative hypotheses – left-tailed, right-tailed, and two-tailed tests:</p>
<p>H 0 :  _ p  ≥ p 0  H 0 :  _ p  ≤ p 0 H 0 :  _ p  = p 0</p>
<p>H a :  _ p  &lt; p 0  H a :  _ p  &gt; p 0 H a :  _ p  ≠ p 0</p>
<p>The test statistic is computed as follows:</p>
<p>z =   _ p  − p 0 _____________  √ ___________ p 0(1 − p 0) _ n   </p>
<p>Here, <em class="italic">n</em> is the sample size. Let us consider an example. In a community college in Houston, a researcher wants to know whether students support some changes equal to 80%. He will use a one-proportion z-test at the level of significance α = 0.05. To implement the code in Python, he can use <code>proportions_ztest</code> from the <code>statsmodel</code> library. The syntax is as follows:</p>
<pre class="source-code">
statsmodels.stats.proportion.proportions_ztest(count,
    nobs, value=None, alternative='two-sided')</pre>
<ul>
<li><code>count</code>: The number of successes</li>
<li><code>nobs</code>: The number of trials or observations</li>
<li><code>value</code>: This is the value of the null hypothesis, equal to the proportion in the case of a one-sample test. In the case of a two-sample test, the null hypothesis is <code>prop[0] - prop[1] = value</code>, where <code>prop</code> is the proportion in the two samples. If not provided, <code>value</code> = 0, and the null hypothesis is <code>prop[0] = prop[1]</code>.<code>alternative</code>: The alternative hypothesis:<ul><li><code>'two-sided'</code>: Two-sided test</li><li><code>'larger'</code>: Right-tailed test</li><li><code>'smaller'</code>: Left-tailed test</li></ul></li>
</ul>
<p>The researcher gathers a sample of data with an observed sample proportion p = 0.84, the hypothesized population proportion p 0 = 0.8, and the sample size n = 500. The null and alternative hypotheses are as follows:</p>
<p>H 0 :  _ p  = p 0,</p>
<p>H a :  _ p  ≠ p 0.</p>
<p>We will<a id="_idIndexMarker273"/> implement <a id="_idIndexMarker274"/>a one-proportion two-tailed z-test in Python to calculate the test statistic and p-value:</p>
<pre class="source-code">
#import proportions_ztest function
from statsmodels.stats.proportion import proportions_ztest
count = 0.8*500
nobs = 500
value = 0.84
#perform one proportion two-tailed z-test
z_statistic, p_value = proportions_ztest(count, nobs,
    value, alternative = 'two-sided')
print(f"The test statistic is {z_statistic} and the
    corresponding p-value is {p_value}.")</pre>
<p>The test statistic is -2.236067977499786 and the corresponding p-value is 0.0253473186774685. Since the p-value &lt; 0.05 (the level of significance), we reject the null hypothesis. There<a id="_idIndexMarker275"/> is enough evidence to suggest the proportion of students who<a id="_idIndexMarker276"/> support the changes is different from 0.8.</p>
<h3>A two-proportion z-test</h3>
<p>This test is used <a id="_idIndexMarker277"/>to test the difference between two population proportions. There<a id="_idIndexMarker278"/> are also three forms of the null and alternative hypotheses. The test statistic is computed as follows:</p>
<p>z =  ‾ p 1 − ‾ p 2  ____________  √ _____________   _ p (1 −  _ p )( 1 _ n 1 +  1 _ n 2)  </p>
<p>Here, _ p 1 is a sample proportion for a simple random sample from population 1 and _ p 2 is a sample proportion for a simple random sample from population 2, n 1 and n 2 are the sample sizes, and  _ p  is the total pooled proportion, calculated as follows:</p>
<p> _ p  =  n 1‾ p 1 + n 2‾ p 2 _ n 1 + n 2 .</p>
<p>We consider a similar example if there is a difference in the proportion of students from school A who support the changes compared to the proportion of students from school B. Here, n 1 = 100, n 2 = 100,  _ p 1 = 0.8, and _ p 2 = 0.7. You can use <code>proportions_ztest</code> from the <code>statsmodels</code> library (<em class="italic">Seabold, Skipper, and Josef Perktold, “statsmodels: Econometric and statistical modeling with python.” Proceedings of the 9th Python in Science Conference. 2010</em>) to perform the hypothesis test. Here, we will compute the p-value directly using the test statistic. The null and alternative hypotheses are as follows:</p>
<p>H 0 : _ p 1 = _ p 2 (The two population proportions are equal)</p>
<p>H a : _ p 1 ≠ _ p 2 (The two population proportions are different)</p>
<p>Next, we specify that the level of significance for the two-tailed test is α = 0.05. We then calculate the test statistic and p-value as follows:</p>
<pre class="source-code">
scipy.stats.norm.sf(abs(z))*2</pre>
<p>The test statistic is 1.633 and the p-value for a two-tailed test is 0.10. Because the p-value is greater than the specified level of significance, 0.05, we fail to reject the null hypothesis. There is enough evidence to say that the proportion of students who support the changes is different between school A and school B.</p>
<p>Finally, the<a id="_idIndexMarker279"/> implemented <a id="_idIndexMarker280"/>Python code for the preceding calculation is as follows:</p>
<pre class="source-code">
import math
import scipy
p_1bar = 0.8
p_2bar = 0.7
n1 = 100.0
n2 = 100.0
p= (p_1bar*n1 + p_2bar*n2)/(n1+n2)   # the total pooled proportion
z = (p_1bar-p_2bar)/math.sqrt(p*(1-p)*(1/n1+1/n2))
pval = scipy.stats.norm.sf(abs(z))*2
print(f"The test statistic is {z} and the p-value for two tailed test is {pval}.")</pre>
<p>In this section, we went through different important statistical notions such as the z-score, z-statistics, critical values, p-value, and z-test for means and proportions. We will discuss selecting the error rate and power analysis in the next section.</p>
<p>Selecting the error rate and power analysis</p>
<p>Statistics generalizes approximations around which to form acceptable conclusions from behaviors in data. Therefore, errors in statistics are unavoidable. The significance of findings from statistical models is essentially determined by the <a id="_idIndexMarker281"/>error rate. One method that can be used to minimize errors in modeling, especially with lower sample volumes, is the power test. Statistical power is the probability of correctly rejecting a null hypothesis, thus minimizing type II errors. Where alpha (<em class="italic">α</em>) is a type I error and beta (<em class="italic">β</em>) is a type II error, power’s formulation is <em class="italic">1 – β</em>. To refresh, a type I error is the probability of incorrectly rejecting the null hypothesis.</p>
<p>As noted, power<a id="_idIndexMarker282"/> is more important with smaller samples because the law of large numbers typically helps minimize errors as the sample size increases if an appropriate sampling method is chosen. Power is also important when the differences being compared are relatively small. One scenario in which power analysis may be particularly useful is when sampling is expensive, for example, with human studies. Power analyses can be used to find an appropriate minimum sample size given the desired power, type I error rate, and effect size. The relationship between these parameters can be explored as needed. Effect size is the difference or similarity of the data being compared in the hypothesis test, such as a standardized difference of means or correlation. As a common level of significance (type I error) in hypothesis testing is anywhere between 0.01 and 0.1, a common level of power is anywhere between 0.8 and 0.9, although these measures are case-dependent (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7745163/#:~:text=The%20ideal%20power%20of%20a,high%20as%200.8%20or%200.9).</p>
<p>A few notable properties<a id="_idIndexMarker283"/> of power are as follows:</p>
<ul>
<li>There must be sufficient power to detect a meaningful difference</li>
<li>Power increases with sample size</li>
<li>Power increases with the effect size</li>
<li>Standard deviation (as well as variance and standard error) decreases as power increases</li>
</ul>
<p>In the following plot, we can see that type I error (α) is where we may falsely assume the null hypothesis and conclude<a id="_idIndexMarker284"/> there is no statistically significant difference when, in fact, the data points don’t belong to data source 2 (the null hypothesis), but to data source 1. Type II error (β) is where we might make the mistake of assuming the data in that region belongs to data source 1 (the alternate hypothesis) when, in fact, it belongs to data source 2.</p>
<div><div><img alt="Figure 3.9 – Visualizing error in a left-tailed two-population z-test" height="617" src="img/B18945_03_009.jpg" width="1100"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – Visualizing error in a left-tailed two-population z-test</p>
<p>As illustrated in the preceding plot, we see a left-tailed t-test comparing the null distribution on the right to the alternate distribution on the left. In this plot, we can see two prominent concepts at hand:</p>
<ul>
<li>As <em class="italic">α</em> becomes smaller, the critical value slides left, moving more toward distributional outliers, <em class="italic">β</em> becomes larger, and vice versa</li>
<li>Statistical power<a id="_idIndexMarker285"/> is the area of data source 2 to the right of <em class="italic">β</em>, as <em class="italic">power = (</em><em class="italic">1-β)</em></li>
</ul>
<p>A type I error is determined based on a pre-selected threshold; a researcher may feel most comfortable with a 90, 95, or 99% level of confidence. However, a type II error is based on parameterization (standard deviation, effect size, sample size, and so on). Therefore, we can use a<a id="_idIndexMarker286"/> power analysis to identify the sample size, and vice versa. The implementation of power analysis for various hypothesis tests will be implemented in the following two chapters.</p>
<h2 id="_idParaDest-64"><a id="_idTextAnchor068"/>Power analysis for a two-population pooled z-test</h2>
<p>Let us look at a dataset <a id="_idIndexMarker287"/>of salaries for professors of two<a id="_idIndexMarker288"/> different disciplines, discipline A and discipline B. We want to know whether there is a statistically significant difference between them based on the data we have. First, we need to perform a power analysis to know whether we have enough samples to trust the results of the z-test we may perform to test this hypothesis. The components of a power analysis we need for this include effect size, the type I error rate, a desired type II error rate, the direction of the alternate hypothesis (group 2 is expected to be larger than, smaller than, or could be either larger or smaller than group 1), and the ratio of observations in the larger sample relative to that of the smaller. Discipline A has 181 salaries and discipline B has 216. Therefore, 216 will be our numerator corresponding to what we will consider as <em class="italic">group 1</em> (discipline A will be <em class="italic">group 2</em>).</p>
<p>Let us suppose we are not sure whether one group will be larger or smaller than the other; we will consider this a two-sided hypothesis test. The effect size for this z-test is the difference between the two groups. We will use <strong class="bold">Cohen’s d</strong>. To calculate that, we need to divide the difference between the means of the two groups by the pooled standard deviation. Calculated here, the pooled standard deviation is the number of samples in group 1 multiplied by the variance of group 1, plus the same for group 2, all divided by the combined sample size for the two groups. The following is the equation for the pooled standard deviation:</p>
<p>√ _____________   n 1 σ 1 2 + n 2 σ 2 2  _ n 1 + n 2  </p>
<p>For effect size, we need to divide the difference of means by the effect size as follows:</p>
<p> |μ 1 − μ 2| ______________  √ ___________  n 1 σ 1 2 + n 2 σ 2 2  _ n 1 + n 2   </p>
<p>Note that in the <a id="_idIndexMarker289"/>equation for effect size, we take the <a id="_idIndexMarker290"/>absolute value of the difference of means. That is because, for Cohen’s d, we always need a positive difference of means for this test.</p>
<p>As calculated here, if we want a power of 80% (the type II error rate is 20%) and a type I error rate (a level of significance) of 0.05, we will need 172 samples in group 1 and 145 samples in group 2. However, if we wanted a power of 90% and a level of significance of 0.01 (99% confidence), we would need 325 samples in group 1 and 274 samples in group 2:</p>
<pre class="source-code">
import statsmodels.api as sm
import math
df_prof = sm.datasets.get_rdataset("Salaries", "carData").data
df_prof_A = df_prof.loc[df_prof['discipline'] == 'A']
df_prof_B = df_prof.loc[df_prof['discipline'] == 'B']
def pooled_standard_deviation(dataset1, dataset2, column) -&gt; float:
    pooledSD = math.sqrt(((len(dataset1) - 1)*(dataset1[column].std()**2)+(len(dataset2) - 1)*(dataset2[column].std()**2))/(len(dataset1) + len(dataset2) - 2))
    return pooledSD;
stdDeviation = pooled_standard_deviation(
    dataset1 = df_prof_A, dataset2=df_prof_B,
    column='salary')
from statsmodels.stats.power import NormalIndPower
effect = abs(df_prof_B['salary'].mean() –
    df_prof_A['salary'].mean() ) / stdDeviation
# The difference between two means divided by std if pooled 2-sample
alpha = 0.05
power = 0.8
ratio=1.19 # # of obs in sample 2 relative to sample 1
analysis = NormalIndPower()
result = analysis.solve_power(effect, power=power, nobs1=None, ratio=ratio, alpha=alpha, alternative='two-sided')
print('Sample Size Required in Sample 1: {:.3f}'.format(
    result*ratio)) # nobs1 is the sample size.
print('Sample Size Required in Sample 2: {:.3f}'.format(
    result)) # nobs2 is the sample size.</pre>
<p>The <a id="_idIndexMarker291"/>output from this<a id="_idIndexMarker292"/> code is as follows:</p>
<pre class="source-code">
Sample Size Required in Sample 1: 171.620
Sample Size Required in Sample 2: 144.218
effect = abs(df_prof_B['salary'].mean() –
    df_prof_A['salary'].mean() ) / stdDeviation
alpha = 0.01
power = 0.9
ratio=1.19 # # of obs in sample 2 relative to sample 1
analysis = NormalIndPower()
result = analysis.solve_power(effect, power=power,
    nobs1=None, ratio=ratio, alpha=alpha,
    alternative='two-sided')
print('Sample Size Required in Sample 1: {:.3f}'.format(
    result*ratio)) # nobs1 is the sample size.
print('Sample Size Required in Sample 2: {:.3f}'.format(
    result)) # nobs2 is the sample size.</pre>
<p>The <a id="_idIndexMarker293"/>output of <a id="_idIndexMarker294"/>this code is as follows:</p>
<p><code>Sample Size Required in Sample </code><code>1: 325.346</code></p>
<p><code>Sample Size Required in Sample </code><code>2: 273.400</code></p>
<h1 id="_idParaDest-65"><a id="_idTextAnchor069"/>Summary</h1>
<p>In this chapter, we introduced the concept of a hypothesis test. We started with a basic outline of a hypothesis test with the four key steps:</p>
<ul>
<li>State the hypothesis</li>
<li>Perform the test</li>
<li>Determine whether to reject or fail to reject the null hypothesis</li>
<li>Draw a statistical conclusion with a scope of inference</li>
</ul>
<p>Then we talked about potential errors that can occur and false positives and false negatives and defined the expected error rate (alpha) of a test and the power (beta) of a test.</p>
<p>We also discussed the statistical procedure called the z-test. This is a type of hypothesis test using sample data assumed to be normally distributed. The z-score and z-statistic were also introduced in the section on different types of z-tests, such as one-sample or two-sample z-tests for means or proportions.</p>
<p>Finally, we discussed the concept and motivation behind the power analysis, which can be used to identify the probability of incorrectly rejecting the null hypothesis and selecting the sample size. We also explored the parameters of the analysis for a two-population pooled z-test. Here, we briefly examined effect size, which is the value of impact (the effect of a treatment) we search for when performing the hypothesis test. We will discuss power analysis in the next two chapters as we iterate over different applications of hypothesis testing.</p>
<p>In the next chapter, we will discuss more parametric hypothesis tests. While these tests will still require distribution assumptions, the assumptions will be less strict than the assumptions of the z-test.</p>
</div>
</div></body></html>