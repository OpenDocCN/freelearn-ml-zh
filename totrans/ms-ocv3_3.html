<html><head></head><body>
        

            
                <h1 class="header-title">Number Plate Recognition using SVM and Neural Network</h1>
            

            
                
<p>This chapter introduces us to the steps needed to create an application for <strong>Automatic Number Plate Recognition</strong> (<strong>ANPR</strong>). There are different approaches and techniques based on different situations, for example, IR camera, fixed car position, light conditions, and so on. We can proceed to construct an ANPR application to detect automobile license plates in a photograph taken between 2 or 3 meters from a car, in ambiguous light condition and with non-parallel ground with minor perspective distortions in the automobile's plate.</p>
<p>The main purpose of this chapter is to introduce us to image segmentation and feature extraction, pattern recognition basics, and two important pattern recognition algorithms: that are <strong>Support Vector Machine</strong> (<strong>SVM</strong>) and <strong>Artificial Neural Network</strong> (<strong>ANN</strong>). In this chapter, we will cover the following topics:</p>
<ul>
<li>ANPR</li>
<li>Plate detection</li>
<li>Plate recognition</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Introduction to ANPR</h1>
            

            
                
<p>Automatic Number Plate Recognition, or known by other terms such as <strong>Automatic License-Plate Recognition</strong> (<strong>ALPR</strong>), <strong>Automatic Vehicle Identification</strong> (<strong>AVI</strong>), or <strong>Car Plate Recognition</strong> (<strong>CPR</strong>), is a surveillance method that uses <strong>Optical Character Recognition</strong> (<strong>OCR</strong>) and other methods such as segmentations and detection to read vehicle registration plates.</p>
<p>The best results in an ANPR system can be obtained with an <strong>Infrared</strong> (<strong>IR</strong>) camera, because the segmentation steps for detection and OCR segmentation are easy, and clean, and they minimize errors. This is due to the laws of light, the basic one being that the angle of incidence equals the angle of reflection. We can see this basic reflection when we see a smooth surface such as a plane mirror. Reflection off of rough surfaces such as paper, leads to a type of reflection known as diffuse or scatter reflection. However, the majority of country plates have special characteristics named retro-reflection, that is, the surface of the plate is made with a material that is covered with thousands of tiny hemispheres that cause light to be reflected back to the source, as we can see in the following figure:</p>
<div><img class=" image-border" src="img/image_04_001.png"/></div>
<p>If we use a camera with filter-coupled, structured infrared light projector, we can retrieve just the Infrared light, and then, we have a very high quality image to segment, with which we can subsequently detect and recognize the plate number that is independent of any light environment, as shown in the following image:</p>
<div><img class=" image-border" src="img/image_04_002.png"/></div>
<p>We will not use IR photographs in this chapter; we will use regular photographs so that we do not obtain the best results, and we get a higher level of detection errors and higher false recognition rate, as opposed to if we used an IR camera. However, the steps for both are the same.</p>
<p>Each country has different license plate sizes and specifications. It is useful to know these specifications in order to get the best results and reduce errors. Algorithms used in every chapter are designed for explaining the basics of ANPR and concrete for license plates used in Spain, but we can extend it to any country or specification.</p>
<p>In this chapter, we will work with license plates from Spain. In Spain, there are three different sizes and shapes of license plates, but we will only use the most common (large) license plate, which has a 520 mm width by a 110 mm height. Two groups of characters are separated by a 41 mm space, and a 14 mm width separates each individual character. The first group of characters have four numeric digits, and the second group has three letters without the vowels A, E, I, O, U, or the letters N or Q. All characters have dimensions of 45 mm by 77 mm.</p>
<p>This data is important for character segmentation since we can check both the character and blank spaces to verify that we get a character and no other image segment:</p>
<div><img class=" image-border" src="img/image_04_003.png"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">ANPR algorithm</h1>
            

            
                
<p>Before explaining the ANPR code, we need to define the main steps and tasks in the ANPR algorithm. ANPR is divided in two main steps: plate detection and plate recognition. Plate detection has the purpose of detecting the location of the plate in the whole camera frame. When a plate is detected in an image, the plate segment is passed to the second step (plate recognition), which uses an OCR algorithm to determine the alphanumeric characters on the plate.</p>
<p>In the following diagram, we can see the two main algorithm steps: plate detection and plate recognition. After these steps, the program draws over the camera frame the plate's characters that have been detected. The algorithms can return bad results or may not return any result:</p>
<div><img height="413" width="377" class=" image-border" src="img/image_04_004.png"/></div>
<p>In each step shown in the previous figure, we will define three additional steps that are commonly used in pattern recognition algorithms. These steps are as follows:</p>
<ol>
<li><strong>Segmentation</strong>: This step detects and removes each patch/region of interest in the image.</li>
<li><strong>Feature extraction</strong>: This step extracts from each patch a set of characteristics.</li>
<li><strong>Classification</strong>: This step extracts each character from the plate recognition-step or classifies each image patch into <em>plate</em> or <em>no plate</em> in the plate-detection step.</li>
</ol>
<p>In the following diagram, we can see these pattern recognition steps in the whole algorithm application:</p>
<div><img height="443" width="485" class=" image-border" src="img/image_04_005.png"/></div>
<p>Aside from the main application, whose purpose is to detect and recognize a car plate number, we will briefly explain two more tasks that are usually not explained:</p>
<ul>
<li>How to train a pattern recognition system</li>
<li>How to evaluate it</li>
</ul>
<p>These tasks, however, can be more important than the main application, because if we do not train the pattern recognition system correctly, our system can fail and not work correctly; different patterns need different training's and evaluation. We need to evaluate our system in different environments, conditions, and features to get the best results. These two tasks are sometimes used together, since different features can produce different results that we can see in the evaluation section.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Plate detection</h1>
            

            
                
<p>In this step, we have to detect all the plates in a current camera frame. To do this task, we divide it in two main steps: segmentation and segment classification. The feature step is not explained because we use the image patch as a vector feature.</p>
<p>In the first step (segmentation), we will apply different filters, morphological operations, contour algorithms, and validations to retrieve parts of the image that could have a plate.</p>
<p>In the second step (classification), we will apply a <strong>Support Vector Machine</strong> (<strong>SVM</strong>) classifier to each image patch, our feature. Before creating our main application, we will train with two different classes: <em>plate</em> and <em>non-plate</em>. We will work with parallel frontal view color images having 800 pixels of width and taken between 2 and 4 meters from a car. These requirements are important for correct segmentations. We can get perform detection if we create a multi-scale image algorithm.</p>
<p>In the next image, we will shown all process involved in plate detection:</p>
<ul>
<li>Sobel filter</li>
<li>Threshold operation</li>
<li>Close morphologic operation</li>
<li>Mask of one of filled area</li>
<li>In red possible detected plates (features images)</li>
<li>Detected plates after SVM classifier</li>
</ul>
<div><img class=" image-border" src="img/image_04_006.png"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Segmentation</h1>
            

            
                
<p>Segmentation is the process of dividing an image into multiple segments. This process is to simplify the image for analysis and make feature extraction easier.</p>
<p>One important feature of plate segmentation is the high number of vertical edges in a license plate, assuming that the image was taken frontally and the plate is not rotated and without perspective distortion. This feature can be exploited during the first segmentation step to eliminate regions that don't have any vertical edges.</p>
<p>Before finding vertical edges, we need to convert the color image to a grayscale image (because color can't help us in this task) and remove possible noise generated from the camera or other ambient noise. We will apply a 5x5 Gaussian blur and remove noise. If we don't apply a noise removal method, we can get a lot of vertical edges that produce fail detection:</p>
<pre>
      //convert image to gray 
      Mat img_gray; 
      cvtColor(input, img_gray, CV_BGR2GRAY); 
      blur(img_gray, img_gray, Size(5,5));    
</pre>
<p>To find the vertical edges, we will use a Sobel filter and find the first horizontal derivate. The derivate is a mathematic function that allows us to find vertical edges on an image. The definition of Sobel function in OpenCV is as follows:</p>
<pre>
      void Sobel(InputArray src, OutputArray dst, int ddepth, int       
      xorder, int yorder, int ksize=3, double scale=1, double delta=0,        int borderType=BORDER_DEFAULT ) 
</pre>
<p>Here, <kbd>ddepth</kbd> is the destination image depth; <kbd>xorder</kbd> is the order of the derivate by x; <kbd>yorder</kbd> is the order of the derivate by y; <kbd>ksize</kbd> is the kernel size of 1, 3, 5, or 7; <kbd>scale</kbd> is an optional factor for computed derivative values; <kbd>delta</kbd> is an optional value added to the result; and <kbd>borderType</kbd> is the pixel interpolation method.</p>
<p>Then, for our case, we can use <kbd>xorder=1</kbd>, <kbd>yorder=0,</kbd> and <kbd>ksize=3</kbd>:</p>
<pre>
      //Find vertical lines. Car plates have high density of vertical        
      lines 
      Mat img_sobel; 
      Sobel(img_gray, img_sobel, CV_8U, 1, 0, 3, 1, 0); 
</pre>
<p>After applying a Sobel filter, we will apply a threshold filter to obtain a binary image with a threshold value obtained through Otsu's method. Otsu's algorithm needs an 8-bit input image, and Otsu's method automatically determines the optimal threshold value:</p>
<pre>
      //threshold image 
      Mat img_threshold; 
      threshold(img_sobel, img_threshold, 0, 255,       
      CV_THRESH_OTSU+CV_THRESH_BINARY); 
</pre>
<p>To define Otsu's method in threshold function, we will combine the type parameter with the <kbd>CV_THRESH_OTSU</kbd> value and the threshold value parameter is ignored.</p>
<p>When the <kbd>CV_THRESH_OTSU</kbd> value is defined, the threshold function returns the optimal threshold value obtained by Otsu's algorithm.</p>
<p>By applying a close morphological operation, we can remove blank spaces between each vertical edge line and connect all regions that have a high number of edges. In this step, we have possible regions that can contain plates.</p>
<p>First, we will define our structural element to use in our morphological operation. We will use the <kbd>getStructuringElement</kbd> function to define a structural rectangular element with a 17x3 dimension size in our case; this may be different in other image sizes:</p>
<pre>
      Mat element = getStructuringElement(MORPH_RECT, Size(17, 3)); 
</pre>
<p>Then, we will use this structural element in a close morphological operation using the <kbd>morphologyEx</kbd> function:</p>
<pre>
      morphologyEx(img_threshold, img_threshold, CV_MOP_CLOSE, 
      element); 
</pre>
<p>After applying these functions, we have regions in the image that could contain a plate; however, most of the regions do not contain license plates. These regions can be split with a connected component analysis or using the <kbd>findContours</kbd> function. This last function retrieves the contours of a binary image with different methods and results. We only need to get the external contours with any hierarchical relationship and any polygonal approximation results:</p>
<pre>
      //Find contours of possibles plates 
      vector&lt; vector&lt; Point&gt;&gt; contours; 
      findContours(img_threshold, 
                contours,            // a vector of contours 
                CV_RETR_EXTERNAL,    // retrieve the external contours 
                CV_CHAIN_APPROX_NONE); // all pixels of each contours 
</pre>
<p>For each contour detected, extract the bounding rectangle of minimal area. OpenCV brings up the <kbd>minAreaRect</kbd> function for this task. This function returns a rotated <kbd>RotatedRect</kbd> rectangle class. Then, using a vector iterator over each contour, we can get the rotated rectangle and make some preliminary validations before we classify each region:</p>
<pre>
      //Start to iterate to each contour founded 
      vector&lt;vector&lt;Point&gt;&gt;::iterator itc= contours.begin(); 
      vector&lt;RotatedRect&gt; rects; 

      //Remove patch that has no inside limits of aspect ratio and      
      area.    
      while (itc!=contours.end()) { 
        //Create bounding rect of object 
          RotatedRect mr= minAreaRect(Mat(*itc)); 
          if(!verifySizes(mr)){ 
          itc= contours.erase(itc); 
          }else{ 
          ++itc; 
          rects.push_back(mr); 
        } 
      } 
</pre>
<p>We make basic validations about the regions detected based on their area and aspect ratio. We will consider that a region can be a plate if the aspect ratio is approximately <em>520/110 = 4.727272</em> (plate width divided by plate height) with an error margin of 40 percent and an area based on a minimum of 15 pixels and maximum of 125 pixels for the height of plate. These values are calculated depending on the image size and camera position:</p>
<pre>
      bool DetectRegions::verifySizes(RotatedRect candidate ){ 

      float error=0.4; 
        //Spain car plate size: 52x11 aspect 4,7272 
      const float aspect=4.7272; 
        //Set a min and max area. All other patchs are discarded 
      int min= 15*aspect*15; // minimum area 
      int max= 125*aspect*125; // maximum area 
        //Get only patches that match to a respect ratio. 
      float rmin= aspect-aspect*error; 
      float rmax= aspect+aspect*error; 

      int area= candidate.size.height * candidate.size.width; 
      float r= (float)candidate.size.width /       
      (float)candidate.size.height; 
      if(r&lt;1) 
          r= 1/r; 

      if(( area &lt; min || area &gt; max ) || ( r &lt; rmin || r &gt; rmax )){ 
          return false; 
      }else{ 
          return true; 
        } 
      } 
</pre>
<p>We can make even more improvements using the license plate's white background property. All plates have the same background color, and we can use a flood fill algorithm to retrieve the rotated rectangle for precise cropping.</p>
<p>The first step to crop the license plate is to get several seeds near the last rotated rect center. Then, we will get the minimum size of plate between the width and height, and use it to generate random seeds near the patch center.</p>
<p>We want to select the white region, and we need several seeds to touch at least one white pixel. Then, for each seed, we use a <kbd>floodFill</kbd> function to draw a new mask image to store the new closest cropping region:</p>
<pre>
      for(int i=0; i&lt; rects.size(); i++){ 
      //For better rect cropping for each possible box 
      //Make floodfill algorithm because the plate has white background 
      //And then we can retrieve more clearly the contour box 
        ircle(result, rects[i].center, 3, Scalar(0,255,0), -1); 
      //get the min size between width and height 
        float minSize=(rects[i].size.width &lt; rects[i].size.height)?      
      rects[i].size.width:rects[i].size.height; 
      minSize=minSize-minSize*0.5; 
      //initialize rand and get 5 points around center for floodfill      
      algorithm 
      srand ( time(NULL) ); 
      //Initialize floodfill parameters and variables 
      Mat mask; 
      mask.create(input.rows + 2, input.cols + 2, CV_8UC1); 
      mask= Scalar::all(0); 
      int loDiff = 30; 
      int upDiff = 30; 
      int connectivity = 4; 
      int newMaskVal = 255; 
      int NumSeeds = 10; 
      Rect ccomp; 
      int flags = connectivity + (newMaskVal &lt;&lt; 8 ) +             
      CV_FLOODFILL_FIXED_RANGE + CV_FLOODFILL_MASK_ONLY; 
      for(int j=0; j&lt;NumSeeds; j++){ 
      Point seed; 
      seed.x=rects[i].center.x+rand()%(int)minSize-(minSize/2); 
      seed.y=rects[i].center.y+rand()%(int)minSize-(minSize/2); 
      circle(result, seed, 1, Scalar(0,255,255), -1); 
      int area = floodFill(input, mask, seed, Scalar(255,0,0), &amp;ccomp,        Scalar(loDiff, loDiff, loDiff), Scalar(upDiff, upDiff, upDiff),  
      flags); 
</pre>
<p>The <kbd>floodfill</kbd> function fills a connected component with a color into a mask image starting from a point seed, and sets maximal lower and upper brightness/color difference between the pixel to fill and the pixel neighbors or pixel seed:</p>
<pre>
     intfloodFill(InputOutputArray image, InputOutputArray mask, Point       seed, Scalar newVal, Rect* rect=0, Scalar loDiff=Scalar(), Scalar       upDiff=Scalar(), int flags=4 ) 
</pre>
<p>The <kbd>newval</kbd> parameter is the new color we want to put into the image when filling. Parameters <kbd>loDiff</kbd> and <kbd>upDiff</kbd> are the maximal lower and maximal upper brightness/color difference between the pixel to fill and the pixel neighbors or pixel seed.</p>
<p>The parameter <kbd>flag</kbd> is a combination of the following bits:</p>
<ul>
<li><strong>Lower bits</strong>: These bits contain connectivity value, 4 (by default) or 8, used within the function. Connectivity determines which neighbors of a pixel are considered</li>
<li><strong>Upper bits</strong>: These can be 0 or a combination of the following values-<kbd>CV_FLOODFILL_FIXED_RANGE</kbd> and <kbd>CV_FLOODFILL_MASK_ONLY</kbd>.</li>
</ul>
<p><kbd>CV_FLOODFILL_FIXED_RANGE</kbd> sets the difference between the current pixel and the seed pixel. <kbd>CV_FLOODFILL_MASK_ONLY</kbd> will only fill the image mask and not change the image itself.</p>
<p>Once we have a crop mask, we will get a minimal area rectangle from the image mask points and check the validity size again. For each mask, a white pixel gets the position and uses the <kbd>minAreaRect</kbd> function for retrieving the closest crop region:</p>
<pre>
      //Check new floodfill mask match for a correct patch. 
      //Get all points detected for get Minimal rotated Rect 
      vector&lt;Point&gt; pointsInterest; 
      Mat_&lt;uchar&gt;::iterator itMask= mask.begin&lt;uchar&gt;(); 
      Mat_&lt;uchar&gt;::iterator end= mask.end&lt;uchar&gt;(); 
      for( ; itMask!=end; ++itMask) 
        if(*itMask==255) 
          pointsInterest.push_back(itMask.pos()); 
      RotatedRect minRect = minAreaRect(pointsInterest); 
      if(verifySizes(minRect)){ 
</pre>
<p>The segmentation process is finished, and we have valid regions. Now, we can crop each detected region, remove possible rotation, crop the image region, resize the image, and equalize the light of the cropped image regions.</p>
<p>First, we need to generate the transform matrix with <kbd>getRotationMatrix2D</kbd> to remove possible rotations in the detected region. We need to pay attention to height, because <kbd>RotatedRect</kbd> can be returned and rotated at 90 degrees. So, we have to check the rectangle aspect, and if it is less than <kbd>1</kbd>, we need to rotate it by 90 degrees:</p>
<pre>
      //Get rotation matrix 
      float r= (float)minRect.size.width / (float)minRect.size.height; 
      float angle=minRect.angle;     
      if(r&lt;1) 
      angle=90+angle; 
      Mat rotmat= getRotationMatrix2D(minRect.center, angle,1); 
</pre>
<p>With the transform matrix, we now can rotate the input image by an affine transformation (affine transformation in geometry is a transformation that takes parallel lines to parallel lines) with the <kbd>warpAffine</kbd> function where we set the input and destination images, the transform matrix, the output size (same as input in our case), and the interpolation method to use. We can define the border method and border value if needed:</p>
<pre>
      //Create and rotate image 
      Mat img_rotated; 
      warpAffine(input, img_rotated, rotmat, input.size(), 
      CV_INTER_CUBIC); 
</pre>
<p>After we rotate the image, we will crop the image with <kbd>getRectSubPix</kbd> which crops and copies an image portion of width and height centered in a point. If the image is rotated, we need to change the width and height sizes with the C++ <kbd>swap</kbd> function:</p>
<pre>
      //Crop image 
      Size rect_size=minRect.size; 
      if(r &lt; 1) 
      swap(rect_size.width, rect_size.height); 
      Mat img_crop; 
      getRectSubPix(img_rotated, rect_size, minRect.center, 
      img_crop); 
</pre>
<p>Cropped images are not good for use in training and classification since they do not have the same size. Also, each image contains different light conditions, making them more different. To resolve this, we resize all the images to same width and height, and apply a light histogram equalization:</p>
<pre>
      Mat resultResized; 
      resultResized.create(33,144, CV_8UC3); 
<strong>      resize(img_crop, resultResized, resultResized.size(), 0, 0,       
      INTER_CUBIC);</strong> 
      //Equalize croped image 
      Mat grayResult; 
      cvtColor(resultResized, grayResult, CV_BGR2GRAY); 
      blur(grayResult, grayResult, Size(3,3)); 
<strong>      equalizeHist(grayResult, grayResult);</strong>
</pre>
<p>For each detected region, we store the cropped image and its position in a vector:</p>
<pre>
      output.push_back(Plate(grayResult,minRect.boundingRect())); 
</pre>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Classification</h1>
            

            
                
<p>After we preprocess and segment all possible parts of an image, we now need to decide whether each segment is (or is not) a license plate. To do this, we will use a <strong>Support Vector Machine</strong> (<strong>SVM</strong>) algorithm.</p>
<p>A Support Vector Machine is a pattern recognition algorithm included in a family of supervised learning algorithms that was originally created for binary classification. Supervised learning is the machine learning algorithm technique that is trained with labeled data. We need to train the algorithm with an amount of data that is labeled; each data set needs to have a class.</p>
<p>The SVM creates one or more hyperplanes, which is used to discriminate each class of data.</p>
<p>A classic example is a 2D point set that defines two classes; the SVM searches the optimal line that differentiates each class:</p>
<div><img height="362" width="326" class=" image-border" src="img/image_04_007.png"/></div>
<p>The first task before any classification is to train our classifier; this is a job before the main application and it's named "offline training". This is not an easy job because it requires a sufficient amount of data to train the system, but a bigger dataset does not always imply the best results. In our case, we do not have enough data due to the fact that there are no public license plate databases. Because of this, we need to take hundreds of car photos, and then preprocess and segment all of it.</p>
<p>We trained our system with 75 license plate images and 35 images without license plates, containing a 144x33 pixel resolution. We can see a sample of this data in the following image. This is not a large dataset, but sufficient enough to get decent results for our chapter. In a real application, we would need to train with more data:</p>
<div><img class=" image-border" src="img/image_04_008.png"/></div>
<p>To easily understand how machine learning works, we will proceed to use image pixel features of the classifier algorithm (keep in mind that there are better methods and features to train an SVM, such as <strong>Principal Components Analysis</strong> (<strong>PCA</strong>), Fourier transform, texture analysis, and so on).</p>
<p>We need to create the images to train our system using the <kbd>DetectRegions</kbd> class and set the <kbd>savingRegions</kbd> variable to "true" in order to save the images. We can use the <kbd>segmentAllFiles.sh</kbd> bash script to repeat the process on all image files under a folder. This can be taken from the source code of the book:</p>
<p>To make this easier, we will store all image training data that is processed and prepared into an XML file for use directly with the SVM function. The <kbd>trainSVM.cpp</kbd> application creates this file using the folders and number of image files.</p>
<p>Training data for a machine learning OpenCV algorithm is stored in an <em>N</em>x<em>M</em> matrix, with <em>N</em> samples and <em>M</em> features. Each dataset is saved as a row in the training matrix. <br/>
<br/>
The classes are stored in another matrix with <em>n</em>x1 size, where each class is identified by a float number.</p>
<p>OpenCV has an easy way to manage a data file in the XML or YAML format with the <kbd>FileStorage</kbd> class. This class lets us store and read OpenCV variables and structures or our custom variables. With this function, we can read the training data matrix and training classes and save it in <kbd>SVM_TrainingData</kbd> and <kbd>SVM_Classes</kbd>:</p>
<pre>
      FileStorage fs; 
      fs.open("SVM.xml", FileStorage::READ); 
      Mat SVM_TrainingData; 
      Mat SVM_Classes; 
      fs["TrainingData"] &gt;&gt;SVM_TrainingData; 
      fs["classes"] &gt;&gt;SVM_Classes; 
</pre>
<p>Now, we have the training data in the <kbd>SVM_TrainingData</kbd> variable and labels in <kbd>SVM_Classes</kbd>. Then, we only have to create the training data object that connects data and labels to use in our machine learning algorithm. To do this, we will use the <kbd>TrainData</kbd> class as a OpenCV pointer <kbd>Ptr</kbd> class as follows:</p>
<pre>
      Ptr&lt;TrainData&gt; trainData = TrainData::create(SVM_TrainingData,         
      ROW_SAMPLE, SVM_Classes); 
</pre>
<p>We will create the classifier object using the <kbd>SVM</kbd> class using the <kbd>Ptr</kbd>OpenCV class:</p>
<pre>
      Ptr&lt;SVM&gt; svmClassifier = SVM::create() 
</pre>
<p>Now, we need to set the SVM parameters that define the basic parameters to use in an SVM algorithm. To do this, we only have to change some object variables. After different experiments, we will choose the next parameter's setup:</p>
<pre>
      svmClassifier-
      &gt;setTermCriteria(TermCriteria(TermCriteria::MAX_ITER, 1000,       
      0.01)); 
      svmClassifier-&gt;setC(0.1); 
      svmClassifier-&gt;setKernel(SVM::LINEAR); 
</pre>
<p>We chose a 1000 iterations for training, a C param variable optimization of 0.1, and finally, a kernel function.</p>
<p>We only need train our classifier with the<kbd>train</kbd> function and the train data:</p>
<pre>
      svmClassifier-&gt;train(trainData); 
</pre>
<p>Our classifier is ready to predict a possible cropped image using the <kbd>predict</kbd> function of our SVM class; this function returns the class identifier <kbd>i</kbd>. In our case, we will label a plate class with 1 and no plate class with 0. Then, for each detected region that can be a plate, we will use SVM to classify it as plate or no plate, and save only the correct responses. The following code is a part of a main application called online processing:</p>
<pre>
      vector&lt;Plate&gt; plates; 
      for(int i=0; i&lt; posible_regions.size(); i++) 
      { 
      Mat img=posible_regions[i].plateImg; 
      Mat p= img.reshape(1, 1);//convert img to 1 row m features 
      p.convertTo(p, CV_32FC1); 
<strong>      int response = (int)svmClassifier.predict( p );</strong> 
      if(response==1) 
      plates.push_back(posible_regions[i]); 
      } 
</pre>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Plate recognition</h1>
            

            
                
<p>The second step in License Plate Recognition aims to retrieve the characters of the license plate with Optical Character Recognition. For each detected plate, we proceed to segment the plate for each character and use an Artificial Neural Network machine learning algorithm to recognize the character. Also, in this section, you will learn how to evaluate a classification algorithm.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">OCR segmentation</h1>
            

            
                
<p>First, we will obtain a plate image patch as an input to the OCR segmentation function with an equalized histogram. We then need to apply only a threshold filter and use this threshold image as the input of a Find Contours algorithm. We can observe this process in the following image:</p>
<div><img height="186" width="133" class=" image-border" src="img/image_04_009.png"/></div>
<p>This segmentation process is coded as follows:</p>
<pre>
      Mat img_threshold; 
      threshold(input, img_threshold, 60, 255, CV_THRESH_BINARY_INV); 
      if(DEBUG) 
      imshow("Threshold plate", img_threshold); 
      Mat img_contours; 
      img_threshold.copyTo(img_contours); 
      //Find contours of possibles characters 
      vector&lt; vector&lt; Point&gt;&gt; contours; 
      findContours(img_contours, 
          contours, // a vector of contours 
          CV_RETR_EXTERNAL, // retrieve the external contours 
          CV_CHAIN_APPROX_NONE); // all pixels of each contours 
</pre>
<p>We used the <kbd>CV_THRESH_BINARY_INV</kbd> parameter to invert the threshold output by turning the white input values black and the black input values white. This is needed to get the contours of each character, because the contours algorithm looks for white pixels.</p>
<p>For each detected contour, we can make a size verification and remove all regions where the size is smaller or the aspect is not correct. In our case, the characters have a 45/77 aspect, and we can accept a 35 percent error of aspect for rotated or distorted characters. If an area is higher than 80 percent, we will consider that region to be a black block and not a character. For counting the area, we can use the <kbd>countNonZero</kbd> function that counts the number of pixels with a value higher than 0:</p>
<pre>
      bool OCR::verifySizes(Mat r){ 
        //Char sizes 45x77 
      float aspect=45.0f/77.0f; 
      float charAspect= (float)r.cols/(float)r.rows; 
      float error=0.35; 
      float minHeight=15; 
      float maxHeight=28; 
        //We have a different aspect ratio for number 1, and it can be 
       ~0.2 
       float minAspect=0.2; 
      float maxAspect=aspect+aspect*error; 
        //area of pixels 
      float area=countNonZero(r); 
        //bb area 
      float bbArea=r.cols*r.rows; 
        //% of pixel in area 
      float percPixels=area/bbArea; 
      if(percPixels &lt; 0.8 &amp;&amp; charAspect &gt; minAspect &amp;&amp; charAspect &lt;  
      maxAspect &amp;&amp; r.rows &gt;= minHeight &amp;&amp; r.rows &lt; maxHeight) 
        return true; 
       else 
        return false; 
      } 
</pre>
<p>If a segmented character is verified, we have to preprocess it to set the same size and position for all characters, and save it in a vector with the auxiliary <kbd>CharSegment</kbd> class. This class saves the segmented character image and the position that we need to order the characters, because the Find Contour algorithm does not return the contours in the correct and needed order.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Feature extraction</h1>
            

            
                
<p>The next step for each segmented character is to extract the features for training and classify the Artificial Neural Network algorithm.</p>
<p>Unlike plate detection, the feature extraction step used in SVM doesn't use all of the image pixels. We will apply more common features used in OCR that contain horizontal and vertical accumulation histograms and low-resolution image samples. We can see this feature more graphically in the next image, as each image has a low resolution 5x5 image and the histogram accumulations:</p>
<div><img class=" image-border" src="img/image_04_010.png"/></div>
<p>For each character, we will count the number of pixels in a row or column with a nonzero value using the <kbd>countNonZero</kbd> function and store it in a new data matrix called <kbd>mhist</kbd>. We will normalize it by looking for the maximum value in the data matrix using the <kbd>minMaxLoc</kbd> function and divide all elements of <kbd>mhist</kbd> by the maximum value with the <kbd>convertTo</kbd> function. We will create the <kbd>ProjectedHistogram</kbd> function to create the accumulation histograms that have a binary image and a type of histogram that we need, horizontal or vertical, as input:</p>
<pre>
      Mat OCR::ProjectedHistogram(Mat img, int t) 
      { 
      int sz=(t)?img.rows:img.cols; 
      Mat mhist=Mat::zeros(1,sz,CV_32F); 

      for(int j=0; j&lt;sz; j++){ 
      Mat data=(t)?img.row(j):img.col(j); 
      mhist.at&lt;float&gt;(j)=countNonZero(data); 
      } 

      //Normalize histogram 
      double min, max; 
      minMaxLoc(mhist, &amp;min, &amp;max); 

      if(max&gt;0) 
      mhist.convertTo(mhist,-1 , 1.0f/max, 0); 

      return mhist; 
      } 
</pre>
<p>Other features use a low-resolution sample image. Instead of using the whole character image, we will create a low-resolution character, for example, a character of 5x5. We will train the system with 5x5, 10x10, 15x15, and 20x20 characters and then evaluate which one returns the best result to use it in our system. Once we have all features, we will create a matrix of <em>M</em> columns by one row where the columns are the features:</p>
<pre>
      Mat OCR::features(Mat in, int sizeData){ 
        //Histogram features 
<strong>      Mat vhist=ProjectedHistogram(in,VERTICAL);</strong><strong>Mat       
      hhist=ProjectedHistogram(in,HORIZONTAL);</strong> 
        //Low data feature 
<strong>      Mat lowData;</strong><strong>resize(in, lowData, Size(sizeData, sizeData) );</strong> 
      int numCols=vhist.cols + hhist.cols + lowData.cols * 
      lowData.cols; 
      Mat out=Mat::zeros(1,numCols,CV_32F); 
        //Asign values to feature 
      int j=0; 
      for(int i=0; i&lt;vhist.cols; i++){ 
        out.at&lt;float&gt;(j)=vhist.at&lt;float&gt;(i); j++;} 
      for(int i=0; i&lt;hhist.cols; i++){ 
        out.at&lt;float&gt;(j)=hhist.at&lt;float&gt;(i); 
        j++;} 
      for(int x=0; x&lt;lowData.cols; x++){ 
        for(int y=0; y&lt;lowData.rows; y++){ 
          out.at&lt;float&gt;(j)=(float)lowData.at&lt;unsigned char&gt;(x,y); 
          j++; 
          } 
        } 
      return out; 
      } 
</pre>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">OCR classification</h1>
            

            
                
<p>In the classification step, we used an Artificial Neural Network machine learning algorithm, more specifically, a <strong>Multi-Layer Perceptron</strong> (<strong>MLP</strong>) which is the most commonly used ANN algorithm.</p>
<p>MLP consists of a network of neurons with an input layer, output layer, and one or more hidden layers. Each layer has one or more neurons connected with the previous and next layers.</p>
<p>The following example represents a three-layer perceptron (is a binary classifier that maps a real-valued vector input to a single binary value output) with three inputs, two outputs, and the hidden layer including five neurons:</p>
<div><img height="223" width="250" class=" image-border" src="img/image_04_011.png"/></div>
<p>All neurons in an MLP are similar, and each one has several inputs (the previous linked neurons) and several output links with the same value (the next linked neurons). Each neuron calculates the output value as a sum of the weighted inputs plus a bias term and is transformed by a selected activation function:</p>
<div><img height="214" width="311" class=" image-border" src="img/image_04_012.png"/></div>
<p>There are three widely used activation functions: Identity, Sigmoid, and Gaussian. The most common and default activation function is the Sigmoid function; it has an alpha and beta value set to 1:</p>
<div><img height="167" width="314" class=" image-border" src="img/image_04_013.png"/></div>
<p>An ANN-trained network has a vector of input with features; it passes the values to the hidden layer and computes the results with the weights and activation function. It passes outputs further downstream until it gets the output layer that has the number of neurons classes.</p>
<p>The weight of each layer, synapses, and neuron is computed and learned by training the ANN algorithm. To train our classifier, we will create two matrices of data, as we did in the SVM training, but the training labels are a bit different. Instead of an <em>N</em>x1 matrix, where <em>N</em> stands for training data rows and 1 is the column, we will use the label number identifier. We have to create an <em>N</em>x<em>M</em> matrix, where <em>N</em> is the training/samples data and <em>M</em> are the classes (10 digits + 20 letters in our case), and set 1 in a position <em>i</em>, <em>j</em> if the data row <em>i</em> is classified with class <em>j</em>:</p>
<div><img height="212" width="202" class=" image-border" src="img/image_04_014.png"/></div>
<p>We will create an <kbd>OCR::train</kbd> function to create all needed matrix and train our system, with the training data matrix, classes matrix, and the number of hidden neurons in the hidden layers. The training data is loaded from an XML file, just as we did in SVM training.</p>
<p>We have to define the number of neurons in each layer to initialize the ANN class. For our sample, we will use only one hidden layer. Then, we will define a matrix of one row and three columns. The first column position is the number of features, the second column position is the number of hidden neurons on the hidden layer, and the third column position is the number of classes.</p>
<p>OpenCV defines an <kbd>ANN_MLP</kbd> class for ANN. With the <kbd>create</kbd> function, we can initiate the class pointer and later define the number of layers and neurons and the activation function. We can thencreate the training data like SVM, and <kbd>alpha</kbd> and <kbd>beta</kbd> parameters of training method:</p>
<pre>
      void OCR::train(Mat TrainData, Mat classes, int nlayers){ 
      Mat_&lt;int&gt; layerSizes(1, 3); 
      layerSizes(0, 0) = data.cols; 
      layerSizes(0, 1) = nlayers; 
      layerSizes(0, 2) = numCharacters; 
      ann= ANN_MLP::create(); 
      ann-&gt;setLayerSizes(layerSizes); 
      ann-&gt;setActivationFunction(ANN_MLP::SIGMOID_SYM, 0, 0); 
      ann-&gt;setTrainMethod(ANN_MLP::BACKPROP, 0.0001, 0.0001); 

      //Prepare trainClases 
      //Create a mat with n trained data by m classes 
      Mat trainClasses; 
      trainClasses.create( TrainData.rows, numCharacters, CV_32FC1 ); 
      for( int i = 0; i &lt;trainClasses.rows; i++ ) 
      { 
          for( int k = 0; k &lt; trainClasses.cols; k++ ) 
          { 
            //If class of data i is same than a k class 
         if( k == classes.at&lt;int&gt;(i) ) 
             trainClasses.at&lt;float&gt;(i,k) = 1; 
               else 
                   trainClasses.at&lt;float&gt;(i,k) = 0; 
           } 
          } 

       Ptr&lt;TrainData&gt; trainData = TrainData::create(data, ROW_SAMPLE,           trainClasses); 
       //Learn classifier 
        ann-&gt;train( trainData ); 

      } 
</pre>
<p>After training, we can classify any segmented plate features using the <kbd>OCR::classify</kbd> function:</p>
<pre>
      int OCR::classify(Mat f){ 
      int result=-1; 
      Mat output; 
<strong>      ann.predict(f, output);</strong> 
      Point maxLoc; 
      double maxVal; 
<strong>      minMaxLoc(output, 0, &amp;maxVal, 0, &amp;maxLoc);</strong> 
      //We need know where in output is the max val, the x (cols) is        
      the class. 
      return maxLoc.x; 
      } 
</pre>
<p>The <kbd>ANN_MLP</kbd> class uses the <kbd>predict</kbd> function for classifying a feature vector in a class. Unlike the SVM <kbd>classify</kbd> function, the ANN predict function returns a row with the size of equal to the number of classes, with the probability of belonging the input feature to each class.</p>
<p>To get the best result, we can use the <kbd>minMaxLoc</kbd> function to get the max and min response, and the position in the matrix. The class of our character is specified by the <em>x</em> position of higher value:</p>
<div><img height="215" width="371" class=" image-border" src="img/image_04_015.png"/></div>
<p>To finish each plate detected, we order its characters and return a string with the <kbd>str()</kbd> function of the <kbd>Plate</kbd> class, and we can draw it on the original image:</p>
<pre>
      string licensePlate=plate.str(); 
      rectangle(input_image, plate.position, Scalar(0,0,200)); 
      putText(input_image, licensePlate, Point(plate.position.x,              plate.position.y), CV_FONT_HERSHEY_SIMPLEX, 1, 
      Scalar(0,0,200),2); 
</pre>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Evaluation</h1>
            

            
                
<p>Our project is finished. However, when we train a machine learning algorithm like OCR, for example, we need to know the best features and parameters to use and how to correct the classification, recognition, and detection errors in our system.</p>
<p>We need to evaluate our system with different situations and parameters and evaluate the errors produced in order to get the best parameters that minimize those errors.</p>
<p>In this chapter, we evaluated the OCR task with variables: size of low-level resolution image feature and the number of hidden neurons in the hidden layer.</p>
<p>We created the <kbd>evalOCR.cpp</kbd> application where we uses the XML training data file generated by the <kbd>trainOCR.cpp</kbd> application. The <kbd>OCR.xml</kbd> file contains the training data matrix for 5x5, 10x10, 15x15, and 20x20 downsampled image features:</p>
<pre>
      Mat classes; 
      Mat trainingData; 
      //Read file storage. 
      FileStorage fs; 
      fs.open("OCR.xml", FileStorage::READ); 
      fs[data] &gt;&gt; trainingData; 
      fs["classes"] &gt;&gt; classes; 
</pre>
<p>The evaluation application gets each downsampled matrix feature and gets 100 random rows for traning, as well as other rows for testing the ANN algorithm and checking the error.</p>
<p>Before training the system, we will test each random sample and check whether the response is correct. If the response is not correct, we increment the error counter variable and then divide by the number of samples to evaluate. This indicates the error ratio between 0 and 1 for training with random data:</p>
<pre>
      float test(Mat samples, Mat classes){ 
      float errors=0; 
      for(int i=0; i&lt;samples.rows; i++){ 
        int result= ocr.classify(samples.row(i)); 
        if(result!= classes.at&lt;int&gt;(i)) 
        errors++; 
      } 
      return errors/samples.rows; 
      } 
</pre>
<p>The application returns output command-line error ratio for each sample size. For a good evaluation, we need to train the application with different random training rows. This produces different test error values. Then, we can add up all the errors and obtain an average. To do this task, we will create the bash UNIX script to automate it:</p>
<pre>
      #!/bin/bash 
      echo "#ITS t 5 t 10 t 15 t 20"&gt;data.txt 
      folder=$(pwd) 

      for numNeurons in 10 20 30 40 50 60 70 80 90 100 120 150 200 500 
      do 
      s5=0; 
      s10=0; 
      s15=0; 
      s20=0; 
      for j in {1..100} 
      do 
      echo $numNeurons $j 
      a=$($folder/build/evalOCR $numNeurons TrainingDataF5) 
      s5=$(echo "scale=4; $s5+$a" | bc -q 2&gt;/dev/null) 

      a=$($folder/build/evalOCR $numNeurons TrainingDataF10) 
      s10=$(echo "scale=4; $s10+$a" | bc -q 2&gt;/dev/null) 

      a=$($folder/build/evalOCR $numNeurons TrainingDataF15) 
      s15=$(echo "scale=4; $s15+$a" | bc -q 2&gt;/dev/null) 

      a=$($folder/build/evalOCR $numNeurons TrainingDataF20) 
      s20=$(echo "scale=4; $s20+$a" | bc -q 2&gt;/dev/null) 
      done 

      echo "$i t $s5 t $s10 t $s15 t $s20" 
      echo "$i t $s5 t $s10 t $s15 t $s20"&gt;&gt;data.txt 
      done 
</pre>
<p>This script saves a <kbd>data.txt</kbd> file that contains all results for each size and neuron hidden layer number. This file can be used for plotting with <em>gnuplot</em>. We can see the result in the following image:</p>
<div><img class=" image-border" src="img/image_04_016.png"/></div>
<p>We can see that the lowest error is over 8 percent and is using 20 neurons in hidden layer and character's features extracted from a downscaled to 10x10 image patch.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Summary</h1>
            

            
                
<p>In this chapter, you learned how an Automatic Plate License Recognition program works and its two important steps: plate localization and plate recognition.</p>
<p>In the first step, you learned how to segment an image looking for patches where we can have a plate, and use a simple heuristics and SVM algorithm to make a binary classification for patches with <em>plates</em> and <em>no plates</em>.</p>
<p>In the second step, you learned how to segment with the Find Contours algorithm, extract feature vector from each character, and use an ANN to classify each feature in a character class.</p>
<p>You also learned how to evaluate a machine algorithm with training with random samples, and using different parameters and features.</p>
<p>In the next chapter, you will learn how to create a face-recognition application using eigenfaces.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    </body></html>