["```py\nval ratigsFile = \"data/BX-Book-Ratings.csv\"\nvar ratingDF = spark.read.format(\"com.databricks.spark.csv\")\n      .option(\"delimiter\", \";\")\n      .option(\"header\", true)\n      .load(ratigsFile)\n```", "```py\n/* Explore and query on books         */\nval booksFile = \"data/BX-Books.csv\"\nvar bookDF = spark.read.format(\"com.databricks.spark.csv\")\n            .option(\"header\", \"true\")\n            .option(\"delimiter\", \";\")\n            .load(booksFile)    \nbookDF = bookDF.select(bookDF.col(\"ISBN\"), \n                       bookDF.col(\"Book-Title\"), \n                       bookDF.col(\"Book-Author\"), \n                       bookDF.col(\"Year-Of-Publication\"))\n\nbookDF = bookDF.withColumnRenamed(\"Book-Title\", \"Title\")\n                .withColumnRenamed(\"Book-Author\", \"Author\")\n                .withColumnRenamed(\"Year-Of-Publication\", \"Year\")\n\nbookDF.show(10)\n```", "```py\nval numDistinctBook = bookDF.select(bookDF.col(\"ISBN\")).distinct().count()\nprintln(\"Got \" + numDistinctBook + \" books\") \n```", "```py\nGot 271,379 books\n```", "```py\nratingsDF.createOrReplaceTempView(\"ratings\")\nmoviesDF.createOrReplaceTempView(\"books\")\n```", "```py\n/* Explore and query ratings for books         */\nval numRatings = ratingDF.count()\nval numUsers = ratingDF.select(ratingDF.col(\"UserID\")).distinct().count()\nval numBooks = ratingDF.select(ratingDF.col(\"ISBN\")).distinct().count()\nprintln(\"Got \" + numRatings + \" ratings from \" + numUsers + \" users on \" + numBooks + \" books\")\n```", "```py\n// Get the max, min ratings along with the count of users who have rated a book.    \nval statDF = spark.sql(\"select books.Title, bookrates.maxRating, bookrates.minRating, bookrates.readerID \"\n      + \"from(SELECT ratings.ISBN,max(ratings.Rating) as maxRating,\"\n      + \"min(ratings.Rating) as minRating,count(distinct UserID) as readerID \"\n      + \"FROM ratings group by ratings.ISBN) bookrates \"\n      + \"join books on bookrates.ISBN=books.ISBN \" + \"order by bookrates.readerID desc\")\n\n    statDF.show(10)\n```", "```py\n// Show the top 10 most-active users and how many times they rated a book\nval mostActiveReaders = spark.sql(\"SELECT ratings.UserID, count(*) as CT from ratings \"\n      + \"group by ratings.UserID order by CT desc limit 10\")\nmostActiveReaders.show()\n```", "```py\n// Find the movies that user 130554 rated higher than 5\nval ratingBySpecificReader = spark.sql(\n      \"SELECT ratings.UserID, ratings.ISBN,\"\n        + \"ratings.Rating, books.Title FROM ratings JOIN books \"\n        + \"ON books.ISBN=ratings.ISBN \"\n        + \"WHERE ratings.UserID=130554 and ratings.Rating > 5\")\n\nratingBySpecificReader.show(false)\n```", "```py\nval splits = ratingDF.randomSplit(Array(0.60, 0.40), 1357L)\nval (trainingData, testData) = (splits(0), splits(1))\n\ntrainingData.cache\ntestData.cache\n\nval numTrainingSample = trainingData.count()\nval numTestSample = testData.count()\nprintln(\"Training: \" + numTrainingSample + \" test: \" + numTestSample) \n```", "```py\nval trainRatingsRDD = trainingData.rdd.map(row => {\n      val userID = row.getString(0)\n      val ISBN = row.getInt(1)\n      val ratings = row.getString(2)\n      Rating(userID.toInt, ISBN, ratings.toDouble)\n    })\n```", "```py\nval testRatingsRDD = testData.rdd.map(row => {\n      val userID = row.getString(0)\n      val ISBN = row.getInt(1)\n      val ratings = row.getString(2)\n      Rating(userID.toInt, ISBN, ratings.toDouble)\n    })\n```", "```py\nval model : MatrixFactorizationModel = new ALS()\n      .setIterations(10)\n      .setBlocks(-1)\n      .setAlpha(1.0)\n      .setLambda(0.01)\n      .setRank(25)\n      .setSeed(1234579L)\n      .setImplicitPrefs(false) // We want explicit feedback\n      .run(trainRatingsRDD)\n```", "```py\nvar rmseTest = computeRmse(model, testRatingsRDD, true)\nprintln(\"Test RMSE: = \" + rmseTest) //Less is better\n```", "```py\nTest RMSE: = 1.6867585251053991 \n```", "```py\n//Compute the RMSE to evaluate the model. Less the RMSE better the model and it's prediction capability. \ndef computeRmse(model: MatrixFactorizationModel, ratingRDD: RDD[Rating], implicitPrefs: Boolean): Double =         {\n    val predRatingRDD: RDD[Rating] = model.predict(ratingRDD.map(entry => (entry.user, entry.product)))\n    val predictionsAndRatings = predRatingRDD.map {entry => ((entry.user, entry.product), entry.rating)}\n                                .join(ratingRDD\n                                .map(entry => ((entry.user, entry.product), entry.rating)))\n                                .values    \n    math.sqrt(predictionsAndRatings.map(x => (x._1 - x._2) * (x._1 - x._2)).mean()) // return MSE\n          }\n```", "```py\nprintln(\"Recommendations: (ISBN, Rating)\")\nprintln(\"----------------------------------\")\nval recommendationsUser = model.recommendProducts(276747, 10)\nrecommendationsUser.map(rating => (rating.product, rating.rating)).foreach(println)\nprintln(\"----------------------------------\")\n```", "```py\nRecommendations: (ISBN => Rating)\n (1051401851,15.127044702142243)\n (2056910662,15.11531283195148)\n (1013412890,14.75898119158678)\n (603241602,14.53024153450836)\n (1868529062,14.180262929540024)\n (746990712,14.121654522195225)\n (1630827789,13.741728003481194)\n (1179316963,13.571754513473993)\n (505970947,13.506755847456258)\n (632523982,13.46591014905454)\n ----------------------------------\n```", "```py\nval new_user_ID = 300000 // new user ID randomly chosen\n\n//The format of each line is (UserID, ISBN, Rating)\nval new_user_ratings = Seq(\n      (new_user_ID, 817930596, 15.127044702142243),\n      (new_user_ID, 1149373895, 15.11531283195148),\n      (new_user_ID, 1885291767, 14.75898119158678),\n      (new_user_ID, 459716613, 14.53024153450836),\n      (new_user_ID, 3362860, 14.180262929540024),\n      (new_user_ID, 1178102612, 14.121654522195225),\n      (new_user_ID, 158895996, 13.741728003481194),\n      (new_user_ID, 1007741925, 13.571754513473993),\n      (new_user_ID, 1033268461, 13.506755847456258),\n      (new_user_ID, 651677816, 13.46591014905454))\n\nval new_user_ratings_RDD = spark.sparkContext.parallelize(new_user_ratings)\nval new_user_ratings_DF = spark.createDataFrame(new_user_ratings_RDD).toDF(\"UserID\", \"ISBN\", \"Rating\")\n\nval newRatingsRDD = new_user_ratings_DF.rdd.map(row => {\n      val userId = row.getInt(0)\n      val movieId = row.getInt(1)\n      val ratings = row.getDouble(2)\n      Rating(userId, movieId, ratings)\n    }) \n```", "```py\nval complete_data_with_new_ratings_RDD = trainRatingsRDD.union(newRatingsRDD)\n```", "```py\nval newModel : MatrixFactorizationModel = new ALS()\n      .setIterations(10)\n      .setBlocks(-1)\n      .setAlpha(1.0)\n      .setLambda(0.01)\n      .setRank(25)\n      .setSeed(123457L)\n      .setImplicitPrefs(false)\n      .run(complete_data_with_new_ratings_RDD)\n```", "```py\n// Making Predictions. Get the top 10 book predictions for user 276724\n//Book recommendation for a specific user. Get the top 10 book predictions for reader 276747\nprintln(\"Recommendations: (ISBN, Rating)\")\nprintln(\"----------------------------------\")\nval newPredictions = newModel.recommendProducts(276747, 10)\nnewPredictions.map(rating => (rating.product, rating.rating)).foreach(println)\nprintln(\"----------------------------------\")\n```", "```py\nRecommendations: (ISBN, Rating)\n ----------------------------------\n (1901261462,15.48152758068679)\n (1992983531,14.306018295431224)\n (1438448913,14.05457411015043)\n (2022242154,13.516608439192192)\n (817930596,13.487733919030019)\n (1079754533,12.991618591680165)\n (611897245,12.716161072778828)\n (11041460,12.44511878072316)\n (651596038,12.13345082904184)\n (1955775932,11.7254312955358)\n ----------------------------------\n```", "```py\nvar newrmseTest = computeRmse(newModel, testRDD, true)\nprintln(\"Test RMSE: = \" + newrmseTest) //Less is better\n```", "```py\nTest RMSE: = 4.892434600794704\n```"]