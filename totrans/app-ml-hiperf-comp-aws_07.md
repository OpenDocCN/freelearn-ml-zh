# 7

# 大规模部署机器学习模型

在前面的章节中，我们学习了如何存储数据、执行数据处理以及为机器学习应用进行模型训练。在训练机器学习模型并使用测试数据集进行验证后，接下来的任务通常是对新数据和未见数据进行推理。对于任何机器学习应用来说，训练好的模型应该能够很好地泛化未见数据，以避免过拟合。此外，对于实时应用，模型应该能够在访问所有相关数据（包括新数据和存储数据）的同时，以最小的延迟执行推理。同时，与模型相关的计算资源应该能够根据推理请求数量进行扩展或缩减，以便在优化成本的同时，不牺牲性能和实时机器学习应用的需求。

对于不需要实时推理的使用案例，训练好的模型应该能够在合理的时间内对包含数千个变量的非常大的数据集进行推理。此外，在几种情况下，我们可能不想费力去管理推理所需的服务器和软件包，而是将精力集中在开发和改进我们的机器学习模型上。

考虑到上述所有因素，AWS提供了多种选项来部署机器学习模型，以便对新数据和未见数据执行推理。这些选项包括实时推理、批量推理和异步推理。在本章中，我们将讨论使用Amazon SageMaker进行机器学习模型的托管部署选项，以及各种功能，如模型的高可用性、自动扩展和蓝/绿部署。

本章将涵盖以下主题：

+   AWS上的托管部署

+   选择正确的部署选项

+   批量推理

+   实时推理

+   异步推理

+   模型端点的高可用性

+   蓝绿部署

# AWS上的托管部署

数据科学家和机器学习从业者通常非常专注于开发机器学习模型以解决业务问题。问题表述和开发优雅的解决方案、选择合适的算法以及训练模型以确保其提供可靠和准确的结果是我们希望我们的数据科学家和数据工程师关注的机器学习问题解决周期的主要组成部分。

然而，一旦我们有一个好的模型，我们希望在新的数据上运行实时或批量推理。部署模型并管理它通常是需要专门工程师和计算资源的任务。这是因为，我们首先需要确保我们有所有正确的包和库，以便模型能够正确工作。然后，我们还需要决定模型运行所需的计算资源类型和数量。在实时应用中，我们通常是为了满足峰值性能需求而进行设计，就像为 IT 项目配置服务器一样。

模型部署并运行后，我们还需要确保一切按预期工作。此外，在现实世界场景中，数据科学家通常需要定期手动进行分析，以检测模型或数据漂移。如果在这些漂移中检测到任何一个，数据科学家将再次经历整个探索性分析、特征工程、模型开发、模型训练、超参数优化、模型评估和模型部署的周期。所有这些任务都需要大量的努力和资源，因此，许多组织已经转向使用 **机器学习操作**（**MLOps**）工作流程和管理模型部署选项来自动化这些流程，这些选项能够很好地适应不断变化的工作负载。

Amazon SageMaker 提供了多种完全管理的模型部署选项。在本节中，我们将概述这些管理部署选项及其优势，然后在接下来的几节中详细讨论其中的一些部署选项。

## Amazon SageMaker 管理模型部署选项

Amazon SageMaker 提供以下管理部署模型选项：

+   **批量转换**：SageMaker 批量转换用于对大型数据集进行推理。在这种情况下没有持久端点。此方法通常用于在需要离线对大型数据集进行推理的非实时机器学习用例中执行推理。

+   **实时端点**：SageMaker 实时端点用于需要持久机器学习模型端点用例，该端点对少量数据样本进行实时推理。

+   **异步推理**：Amazon SageMaker 异步推理部署了一个异步端点，用于对大型负载（高达 1 GB）进行推理，具有长处理时间和低延迟。

+   **无服务器推理**：在所有之前的方法中，用户都需要选择用于推理的计算实例类型。Amazon SageMaker 无服务器推理会自动选择服务器类型，并根据端点的负载进行扩展和缩减。这对于具有不可预测流量模式的应用程序通常很有用。

让我们接下来探索可用的计算资源种类。

## 可用的计算资源种类

为了进行推理，有各种各样的计算实例可供选择。在撰写本文时，大约有70多个实例可用于执行机器学习推理。这些实例具有不同的计算能力和内存，可以服务于不同的用例。还有使用**图形处理单元**（**GPUs**）进行推理的选项。此外，SageMaker还支持Inf1实例，用于高性能和低成本推理。这些选项使SageMaker模型部署和推理非常灵活，适用于各种机器学习用例。

## 经济高效的模型部署

Amazon SageMaker有多种优化模型部署成本的选择。有多个模型可以共享一个容器的多模型端点。这有助于降低托管成本，因为端点利用率由于多个模型共享相同的端点而提高。此外，它还允许不同模型之间共享内存资源。SageMaker还有构建和部署多容器端点的选项。此外，我们可以将扩展策略附加到我们的端点，以便在流量增加时分配更多计算资源，在流量减少时关闭实例以节省成本。

另一种经济高效的模型部署选项是SageMaker无服务器推理。无服务器推理利用AWS Lambda根据流量增加或减少来扩展和缩减计算资源。它对于流量模式不可预测的场景特别有用。

## 蓝绿部署

每当我们更新SageMaker模型端点时，SageMaker都会自动使用蓝绿部署。在蓝绿部署中，SageMaker使用一组新实例来部署更新的端点，然后将流量从旧实例群切换到新实例群。Amazon SageMaker为蓝绿部署提供以下流量切换策略：

+   同时流量切换

+   金丝雀流量切换

+   线性流量切换

我们将在本章后面更详细地讨论这些流量模式。

## 推理推荐器

使用Amazon SageMaker推理推荐器，我们可以自动获取用于部署我们的模型端点的计算实例类型的推荐。它通过负载测试各种实例并提供测试实例类型的推理成本、吞吐量和延迟来给出实例推荐。这有助于我们决定用于部署模型端点的实例类型。

## MLOps集成

使用Amazon SageMaker，我们可以轻松构建与**持续集成和持续交付**（**CI/CD**）管道集成的机器学习工作流程。这些工作流程可用于自动化整个机器学习生命周期，包括数据标注、数据处理和特征工程、模型训练和注册、后处理、端点部署以及数据漂移和模型漂移监控的模型监控。对于模型部署，这些工作流程可用于自动化批量推理的过程，以及将模型端点从开发环境推送到预生产和生产环境。

## 模型注册

Amazon SageMaker提供了使用SageMaker模型注册功能注册和编目机器学习模型的能力。使用模型注册，我们可以将训练模型的多个版本包含在模型包组中。这样，每次我们训练和注册模型时，它都会作为新版本添加到模型包组中。此外，使用模型注册，我们还可以将元数据和训练指标关联到机器学习模型，批准或拒绝模型，如果批准，则将模型移至生产环境。模型注册的这些功能有助于构建自动化机器学习工作流程所需的CI/CD管道。

## 弹性推理

对于需要非常高的吞吐量和低延迟的机器学习用例，我们通常最终会使用GPU机器，从而显著增加推理的成本。使用Amazon SageMaker，我们可以将弹性推理添加到我们的端点。弹性推理通过仅附加适量的GPU推理加速器到任何SageMaker实例类型，为我们端点提供推理加速。这有助于显著降低延迟和提高吞吐量，同时与使用GPU实例进行推理相比，成本要低得多。

## 在边缘设备上部署

许多机器学习用例需要模型在边缘设备上运行，例如移动设备、摄像头和专用设备。这些设备通常具有较低的计算资源、内存和存储。此外，由于设备硬件和操作系统的可变性，部署、管理和监控设备上的机器学习模型是一项困难的任务。使用Amazon SageMaker Edge Manager，可以在具有不同硬件和软件配置的设备群上部署、监控和管理机器学习模型。SageMaker Edge Manager使用SageMaker Neo编译机器学习模型，并将这些编译后的模型打包以部署到边缘设备。此外，我们还可以收集边缘设备上模型使用的数据样本，并将它们发送到云端进行数据分析，以确定数据和质量问题，如数据漂移和模型漂移。

现在，让我们在下一节中讨论AWS上各种模型部署选项。

# 选择合适的部署选项

如前所述，AWS提供了多种模型部署和推理选项。有时决定正确的模型部署选项可能会让人困惑和不知所措。选择正确的模型部署选项的决定实际上取决于用例参数和需求。在决定部署选项时，以下是一些重要的考虑因素：

+   我们是否有需要实时、持久端点以实时和非常快速地按需对新数据进行推理的应用程序？

+   我们的应用程序是否可以等待一分钟或两分钟，直到计算资源上线后再获取推理结果？

+   我们是否有不需要近乎实时结果的使用场景？我们能否每天/每周或按需对一批数据进行推理？

+   我们是否有不可预测且不均匀的流量模式需要推理？我们是否需要根据流量调整计算资源的大小？

+   我们试图进行推理的数据有多大（数据点/行数）？

+   我们是否需要一直使用专用资源来执行推理，或者我们可以采用无服务器的方法？

+   我们能否在一个端点中打包多个模型以节省成本？

+   我们是否需要一个由多个模型、预处理步骤和后处理步骤组成的推理管道？

在以下小节中，我们将讨论何时选择亚马逊SageMaker提供的不同类型的模型部署和推理选项，同时解答之前提到的问题。我们还将提供每个模型部署选项的典型用例示例。

## 使用批量推理

**亚马逊SageMaker批量转换**用于不需要持久实时端点且可以在大量数据上执行推理的情况。以下示例说明了SageMaker批量转换的使用：

+   **预测性维护**：在制造工厂中，各种组件的传感器和机器数据可能全天收集。对于此类用例，不需要实时或异步端点。夜间，可以使用机器学习模型预测某个组件是否即将失效，或者机械的一部分是否需要维护。这些模型将在大量数据上运行，并使用SageMaker批量转换进行推理。这些模型的结果可以用来制定和执行维护计划。

+   **房价预测**：房地产公司在新房价和市场方向预测出台前收集几天（有时是几周）的数据。这些模型不需要实时或异步端点，因为它们只需要在几天或几周后以及在大量的数据上运行。对于此类用例，SageMaker 批量转换是推理的理想选择。SageMaker 批量转换作业可以在机器学习管道中定期运行，处理新数据和历史数据，通过地区预测房价调整和市场方向。这些结果反过来又可以用来确定是否需要重新训练机器学习模型。

我们将在本章后面的部分详细介绍 Amazon SageMaker 中的批量、实时和异步推理选项。

## 使用实时端点

亚马逊 SageMaker 的实时端点应在需要实时持久化模型端点、随着新数据的到来进行低延迟预测时作为模型部署的选择。实时端点由 Amazon SageMaker 完全管理，并可部署为多模型和多容器端点。以下是一些实时端点的示例用例：

+   **欺诈交易**：客户使用信用卡在线购买商品或在实体零售店购买。这种金融交易可以是信用卡的实际持卡人进行的，也可以是盗窃的信用卡进行的。金融机构需要实时决定是否批准这笔交易。在这种情况下，可以将机器学习模型部署为实时端点。该模型可以使用客户的人口统计数据和历史购买记录（来自历史数据表），同时使用一些来自当前交易的数据，例如 IP 地址和网页浏览器参数（如果是在线交易），或实时使用来自摄像头的商店位置和图像以及/或视频（如果是实体交易），以判断交易是否为欺诈。金融机构可以使用这个决定来批准或拒绝交易，或联系客户进行通知、手动验证和批准。

+   **实时情感分析**：客户在与客户服务代表进行聊天或电话交谈。机器学习模型实时分析聊天或语音对话的转录文本，以确定客户所表现出的情感。根据情感，如果客户不满意，代表可以提供各种促销活动或升级案件至主管，以防止事情失控。这个机器学习模型应该使用实时端点部署，以便正确确定情感而没有任何延迟或延迟。

+   **质量保证**：在制造工厂中，产品正在装配线上组装，需要进行严格的质量控制，以便尽快去除有缺陷的产品。这同样是一个应用，其中来自机器学习模型的实时推理，该模型使用实时图像或视频流将对象分类为有缺陷或正常，将非常有用。

与实时端点类似，我们还有使用异步端点的选项，我们将在下一节中了解。

## 使用异步推理

Amazon SageMaker 异步推理端点与实时端点非常相似。异步端点可以排队推理请求，并且在需要接近实时延迟的同时处理大量工作负载时是首选的部署选项。以下示例说明了潜在的异步端点用例。

**轨道检查**：几列火车在其日常路线上运行，并配备了摄像头，这些摄像头可以捕捉轨道和开关的图像和视频以进行缺陷检测。这些火车没有足够的带宽来实时传输这些数据以进行推理。当这些火车停靠在车站时，大量图像和视频可以发送到 SageMaker 异步推理端点进行推理，以确定一切是否正常，或者轨道或开关上是否存在任何缺陷。与异步端点关联的计算实例将在接收到来自任何火车的数据后立即启动，对数据进行推理，然后一旦所有数据都已被处理，就关闭。这将有助于降低与实时端点相比的成本。

# 批量推理

对于在数据集上执行批量推理，我们可以使用 SageMaker 批量转换。当不需要实时持久部署的机器学习模型时，应使用推理。批量转换在推理数据集很大或我们需要对数据集进行大量预处理时也非常有用。例如，从数据中去除偏差或噪声，将语音数据转换为文本，以及过滤和归一化图像和视频数据。

我们可以将输入数据传递给 SageMaker 批量转换，要么是一个文件，要么是多个文件。对于单个文件中的表格数据，文件中的每一行都被解释为一个数据记录。如果我们选择了多个实例来执行批量转换作业，SageMaker 将将输入文件分发到不同的实例以进行批量转换作业。单个数据文件也可以分成多个小批量，并且可以在不同的实例上并行对这些小批量进行批量转换。*图 7.1* 展示了 SageMaker 批量转换的简化典型架构示例：

![图 7.1 – Amazon SageMaker 批量转换的示例架构](img/B18493_07_001.jpg)

图 7.1 – Amazon SageMaker 批量转换的示例架构

如图中所示，数据是从**Amazon S3**读取的。使用**Amazon SageMaker Processing**对数据进行预处理和特征工程，以便将数据转换为机器学习模型期望的正确格式。然后，使用**Amazon SageMaker Batch Transform**对训练好的机器学习模型进行批量推理。结果随后写回到**Amazon S3**。批量转换中使用的机器学习模型可以是使用Amazon SageMaker训练的，也可以是在Amazon SageMaker之外训练的模型。批量转换的两个主要步骤如下：

1.  创建一个转换器对象。

1.  创建一个批量转换作业以进行推理。

这些步骤在以下小节中描述。

## 创建一个转换器对象

我们首先需要创建`Transformer`类的对象，以便运行SageMaker批量转换作业。在创建`Transformer`对象时，我们可以指定以下参数：

+   `model_name`：这是我们打算用于推理的机器学习模型的名称。这也可以是一个内置的SageMaker模型，对于这种模型，我们可以直接使用内置估计器的`transformer`方法。

+   `instance_count`：我们将用于运行批量转换作业的EC2实例数量。

+   `instance_type`：我们可以使用的EC2实例类型。有多种实例可供批量转换作业使用。实例的选择取决于我们用例的计算和内存需求，以及数据类型和大小。

此外，我们还可以指定其他几个参数，例如批量策略和输出路径。完整的参数列表可以在*参考文献*部分的文档页面上找到。在以下示例中，我们使用了SageMaker的内置XGBoost容器。对于运行自己的容器/模型或框架（如PyTorch和TensorFlow）的批量转换器，容器镜像可能会有所不同。

*图7.2* – *7.8*展示了SageMaker的XGBoost模型在训练数据上拟合的示例，然后为这个训练模型创建了一个转换器对象。我们指定转换作业应在`ml.m4.xlarge`类型的一个实例上运行，并期望`text/csv`数据：

1.  如*图7.2*所示，我们可以指定运行示例中代码所需的各个包和SageMaker参数。

![图7.2 – 在SageMaker中设置包和存储桶，并下载用于模型训练和推理的数据集](img/B18493_07_002.jpg)

图7.2 – 在SageMaker中设置包和存储桶，并下载用于模型训练和推理的数据集

1.  如*图7.3*所示，我们可以读取数据并对分类变量进行独热编码。

![图7.3 – 对分类变量进行独热编码并显示结果](img/B18493_07_003.jpg)

图7.3 – 对分类变量进行独热编码并显示结果

1.  *图7.4*展示了数据被分割成训练、验证和测试分区，这些分区将在模型训练过程中使用。

![图7.4 – 将数据分割成训练、验证和测试集以用于模型训练和测试](img/B18493_07_004.jpg)

图7.4 – 将数据分割成训练、验证和测试集以用于模型训练和测试

1.  如*图7.5*所示，我们可以按照机器学习模型（XGBoost）期望的顺序重新排列数据表中的列。此外，我们还可以将训练和验证数据文件上传到S3桶中，以便进行模型训练步骤。

![图7.5 – 重新组织数据并将其上传到S3桶以进行训练](img/B18493_07_005.jpg)

图7.5 – 重新组织数据并将其上传到S3桶以进行训练

1.  *图7.6*显示我们正在使用SageMaker的XGBoost容器来训练我们的模型。它还指定了用于训练和验证模型的 数据通道。

![图7.6 – 在S3中指定模型训练的容器以及训练和验证数据路径](img/B18493_07_006.jpg)

[在S3中指定模型训练的容器以及训练和验证数据路径](img/B18493_07_006.jpg)

图7.6 – 在S3中指定模型训练的容器以及训练和验证数据路径

如*图7.7*所示，我们需要定义估计器、实例类型和实例数量，并设置XGBoost模型所需的各个超参数。我们还将通过调用`fit`方法启动训练作业。

![图7.7 – 定义SageMaker估计器进行训练并设置超参数](img/B18493_07_007.jpg)

图7.7 – 定义SageMaker估计器进行训练并设置超参数

接下来，让我们创建一个批量转换作业。

## 创建用于推理的批量转换作业

在创建transformer对象之后，我们需要创建一个批量转换作业。*图7.8*展示了使用批量transformer对象的`transform`方法调用启动批量转换作业的示例。在这个转换调用中，我们将指定我们想要在上面进行批量推理的Amazon S3中数据的存储位置。此外，我们还将指定数据的内容类型（在这种情况下为`text/csv`），以及数据文件中记录的分割方式（在这种情况下，每行包含一个记录）。

![图7.8 – 创建transformer对象，准备批量推理数据，并启动批量转换作业](img/B18493_07_008.jpg)

图7.8 – 创建transformer对象，准备批量推理数据，并启动批量转换作业

*图7.9*展示了从S3读取结果并然后绘制结果（实际值与预测值）的示例：

![图7.9 – 创建一个用于读取批量转换结果并绘制结果（实际值与预测值）的辅助函数](img/B18493_07_009.jpg)

图7.9 – 创建一个辅助函数来读取批处理转换的结果，并绘制结果（实际值与预测值对比）

*图7.10* 展示了生成的对比图：

![图7.10 – 实际值与预测环值对比图](img/B18493_07_010.jpg)

图7.10 – 实际值与预测环值对比图

本节中讨论的代码和步骤概述了训练机器学习模型的过程，然后使用该模型对数据进行批推理。

## 优化批处理转换作业

SageMaker批处理转换还提供了使用一些超参数优化转换作业的选项，如下所述：

+   `max_concurrent_transforms`：在任何给定时间可以向每个批处理转换容器发出的最大HTTP请求数。为了获得最佳性能，此值应等于我们用于运行批处理转换作业的计算工作线程数。

+   `max_payload`：此值指定了发送到批处理转换进行推理的单个HTTP请求中有效负载的最大大小（以MB为单位）。

+   `strategy`：此指定了策略，即我们是否希望在批处理中只有一个记录或多个记录。

SageMaker批处理转换是一个非常实用的选项，用于对大型数据集进行推理，这些用例不需要实时延迟和高吞吐量。SageMaker批处理转换可以通过使用MLOps工作流程来执行，该工作流程可以在有新数据需要进行推理时触发。然后可以使用批处理转换作业结果生成自动报告。

接下来，我们将学习如何使用Amazon SageMaker的实时推理选项部署实时端点以对数据进行预测。

# 实时推理

如本章前面所述，当我们需要非常低延迟的结果时，就会出现实时推理的需求。一些日常用例是使用机器学习模型进行实时推理的例子，例如人脸检测、欺诈检测、缺陷和异常检测以及实时聊天中的情感分析。在Amazon SageMaker中，可以通过将我们的模型部署到SageMaker托管服务作为实时端点来执行实时推理。*图7.11* 展示了使用实时端点的典型SageMaker机器学习工作流程。

![图7.11 – SageMaker实时端点的示例架构](img/B18493_07_011.jpg)

图7.11 – SageMaker实时端点的示例架构

在此图中，我们首先从Amazon S3存储桶中读取我们的数据。使用Amazon SageMaker Processing在此数据上执行数据预处理和特征工程。然后，在处理后的数据上训练机器学习模型，接着进行结果评估和后处理（如果有）。之后，该模型被部署为实时端点，以低延迟实时执行新数据的推理。图中还显示了SageMaker Model Monitor，它连接到端点，以检测发送到端点进行推理的新数据中的数据和概念漂移。SageMaker还提供了将我们的机器学习模型注册到SageMaker Model Registry的选项，用于各种目的，如编目、版本控制和自动化部署。

## 将机器学习模型托管为实时端点

Amazon SageMaker为我们提供了多种选项来托管模型或多个模型作为实时端点。我们可以使用SageMaker Python SDK、AWS SDK for Python（Boto3）、SageMaker控制台或AWS **命令行界面**（**CLI**）来托管我们的模型。此外，这些端点还可以托管单个模型、单个端点中的一个容器中的多个模型，以及单个端点中使用不同容器托管的多个模型。此外，单个端点还可以托管包含预处理逻辑的序列推理管道的模型。我们将在以下小节中介绍这些多个选项。

### 单个模型

如前所述，我们可以使用多种选项托管模型端点。在此，我们将向您展示如何使用Amazon SageMaker SDK托管包含单个模型的端点。使用Amazon SageMaker SDK创建单个模型端点涉及两个步骤，如下所述：

1.  `Model` 类用于创建可以部署为HTTPS端点的模型对象。我们还可以使用在SageMaker中训练的模型。例如，图 *图7**.7* 中所示的XGBoost模型。

1.  `deploy()` 方法用于创建HTTPS端点。`deploy` 方法需要实例类型以及部署模型所需的初始实例数量。这如图 *图7**.12* 所示：

![图7.12 – 调用我们之前训练的XGBoost估计器的部署方法，将我们的模型部署在单个ml.m4.xlarge类型的实例上](img/B18493_07_012.jpg)

图7.12 – 调用我们之前训练的XGBoost估计器的部署方法，将我们的模型部署在单个ml.m4.xlarge类型的实例上

*图7**.13* 展示了一个自定义推理函数，用于序列化测试数据，并将其发送到实时端点进行推理：

![图7.13 – 将数据序列化以发送到实时端点。此外，创建一个辅助函数以使用端点进行推理](img/B18493_07_013.jpg)

图7.13 – 将数据序列化以发送到实时端点。此外，创建一个辅助函数以使用端点进行推理

*图7**.14* 展示了几条记录的推理结果以及我们目标变量——`Rings`的实际值：

![图7.14 – 显示一些预测结果与实际值（Rings）的比较](img/B18493_07_014.jpg)

图7.14 – 显示一些预测结果与实际值（Rings）的比较

只要端点没有被删除，它就会继续产生费用。因此，我们只有在有需要实时推理结果的使用场景时才应使用实时端点。对于可以在批量中进行推理的使用场景，我们应该使用 SageMaker Batch Transform。

### 多个模型

对于在单个端点中托管多个模型，我们可以使用 SageMaker 多模型端点。这些端点也可以用来托管同一模型的多个变体。多模型端点是节省实时端点推理成本的一种非常经济的方法，因为当我们使用多模型端点时，端点的利用率通常更高。我们可以使用业务逻辑来决定用于推理的模型。

在多模型端点中，模型间的内存资源也是共享的。当我们的模型在大小和延迟上相当时，这非常有用。如果有模型的延迟需求或每秒事务量有显著差异，则建议为这些模型使用单模型端点。我们可以使用 SageMaker 控制台或 AWS SDK for Python（Boto3）创建多模型端点。我们可以遵循与创建单模型端点类似的步骤来创建多模型端点，但有一些不同。Boto3 的步骤如下：

1.  首先，我们需要一个支持多模型端点部署的容器。

1.  然后，我们需要使用 Boto3 SageMaker 客户端创建一个使用此容器的模型。

1.  对于多模型端点，我们还需要创建一个端点配置，指定实例类型和初始计数。

1.  最后，我们需要使用 Boto3 SageMaker 客户端的 `create_endpoint()` API 调用来创建端点。

在调用多模型端点时，我们还需要传递一个目标模型参数来指定我们想要使用请求中的数据进行推理的模型。

### 多个容器

我们还可以部署使用不同容器（如不同框架）的模型作为多容器端点。这些容器可以单独运行，也可以作为推理管道按顺序运行。多容器端点也有助于提高端点利用率效率，从而降低与实时端点相关的成本。可以使用 Boto3 创建多容器端点。创建多容器端点的过程与创建多模型和单模型端点非常相似。首先，我们需要创建一个具有多个容器的模型作为参数，然后创建端点配置，最后创建端点。

### 推理管道

我们也可以将包含两个到五个容器的实时端点作为单个端点后面的推理管道托管。这些容器中的每一个都可以是预训练的 SageMaker 内置算法、我们的自定义算法、预处理代码、预测或后处理代码。推理管道中的所有容器以顺序方式运行。第一个容器处理初始 HTTP 请求。来自第一个容器的响应作为请求发送到第二个容器，依此类推。来自最终容器的响应由 SageMaker 发送到客户端。推理管道可以被视为一个可以托管在单个端点后面或用于运行批量转换作业的单个模型。由于推理管道中的所有容器都在同一个 EC2 实例上运行，因此容器之间的通信延迟非常低。

### 监控已部署的模型

在将模型部署到生产环境中后，数据科学家和机器学习工程师需要持续检查模型的质量。这是因为随着时间的推移，模型的质量可能会漂移，并可能开始做出错误的预测。这可能是由几个原因造成的，例如数据集中一个或多个变量的分布发生变化，随着时间的推移在数据集中引入偏差，或者某些其他未知的过程或参数发生变化。

传统上，数据科学家通常每隔几周运行一次分析，以检查数据或模型质量是否发生变化，如果有变化，他们需要再次经历特征工程、模型训练和部署的整个过程。使用 SageMaker Model Monitor，这些步骤可以自动化。我们可以设置警报来检测数据质量、模型质量、模型中的偏差漂移和特征分布漂移是否有任何偏移，然后采取纠正措施，例如修复质量问题并重新训练模型。

*图 7.15* 展示了使用 SageMaker Model Monitor 与端点（实时或异步）的示例：

![图 7.15 – 部署端点的 SageMaker Model Monitor 工作流程（实时或异步）](img/B18493_07_015.jpg)

图 7.15 – 部署端点的 SageMaker Model Monitor 工作流程（实时或异步）

首先，我们需要在我们的端点上启用 Model Monitor，无论是在创建时还是之后。此外，我们还需要运行一个基线处理作业。这个处理作业分析数据并为基线数据（通常是模型训练或验证所用的相同数据集）创建统计和约束。我们还需要在我们的 SageMaker 端点上启用数据捕获，以便能够捕获新的数据以及推理结果。然后，在固定的时间间隔运行另一个处理作业，以创建新的统计和约束，将它们与基线统计和约束进行比较，并在分析指标中存在任何偏移的情况下，使用 Amazon CloudWatch 指标配置警报。

如果任何指标发生违规，将生成警报，并且可以将这些警报发送给数据科学家或机器学习工程师进行进一步分析，并在需要时重新训练模型。或者，我们也可以使用这些警报来触发一个工作流程（MLOps），以重新训练我们的机器学习模型。模型监控器是数据科学家的一个非常有价值的工具。它可以减少在模型部署后随着时间的推移检测数据集和模型偏差和漂移的繁琐手动过程。

# 异步推理

SageMaker实时端点适用于具有非常低延迟推理要求（最多60秒）的机器学习用例，以及推理数据的大小不是很大（最大6 MB）。另一方面，批量转换适用于对非常大的数据集进行离线推理。异步推理是SageMaker中另一种相对较新的推理选项，可以处理高达1 GB的数据，并且处理推理请求可能需要长达15分钟。因此，它们适用于没有非常低延迟推理要求的用例。

异步端点与实时端点有多个相似之处。要创建异步端点，就像创建实时端点一样，我们需要执行以下步骤：

1.  创建一个模型。

1.  为异步端点创建端点配置。异步端点有一些额外的参数。

1.  创建异步端点。

与实时端点相比，异步端点也有一些不同之处，如下所述：

+   与实时端点的一个主要不同之处在于，当没有推理请求时，我们可以将端点实例的规模降至零。这可以减少与端点推理相关的成本。

+   与实时端点相比的另一个关键区别是，我们不是在推理请求中按顺序传递有效负载，而是将数据上传到Amazon S3位置，并将S3 URI与请求一起传递。内部，SageMaker保留这些请求的队列，并按接收顺序处理它们。就像实时端点一样，我们也可以对异步端点进行监控，以检测模型和数据漂移，以及新的偏差。

展示SageMaker异步端点的代码在*图7.16*中，使用我们在批量转换示例中创建的相同模型。

![图7.16 – 创建异步端点配置，随后创建异步端点](img/B18493_07_016.jpg)

图7.16 – 创建异步端点配置，随后创建异步端点

*图7.17*显示了在之前章节中用于批量转换和实时端点示例的Abalone数据上执行异步推理的样本结果。

![图 7.17 – 序列化推理请求并调用异步端点对数据进行推理](img/B18493_07_017.jpg)

图 7.17 – 序列化推理请求并调用异步端点对数据进行推理。

*图 7.18* 展示了几个数据点的实际和预测结果。

![图 7.18 – 使用异步端点显示一些预测结果与实际值（环）的对比](img/B18493_07_018.jpg)

图 7.18 – 使用异步端点显示一些预测结果与实际值（环）的对比

在下一节中，我们将探讨 SageMaker 端点的高可用性和容错能力。

# 模型端点的高可用性

Amazon SageMaker 提供了已部署端点的容错性和高可用性。在本节中，我们将讨论 AWS 云基础设施和 Amazon SageMaker 的各种功能和选项，我们可以使用这些功能和选项来确保我们的端点是容错的、有弹性的和高度可用的。

## 在多个实例上部署

SageMaker 给我们提供了在多个实例上部署端点的选项。这可以防止实例故障。如果一个实例宕机，其他实例仍然可以处理推理请求。此外，如果我们的端点部署在多个实例上，并且发生可用区故障或实例故障，SageMaker 会自动尝试将我们的实例分布到不同的可用区，从而提高端点的弹性。将端点部署使用不同可用区的小型实例类型也是一种良好的实践。

## 端点自动扩展

通常，我们为峰值负载和流量设计我们的在线应用程序。对于基于机器学习的应用程序也是如此，我们需要托管端点以实时或近实时地进行推理。在这种情况下，我们通常使用最大数量的实例来部署模型，以便服务于峰值工作负载和流量。

如果我们不这样做，那么当推理请求多于实例可以处理的总量时，我们的应用程序可能会开始超时。这种方法会导致计算资源的浪费或我们机器学习应用程序服务的中断。

为了避免这种场景，Amazon SageMaker 允许我们使用自动扩展策略来配置我们的端点。使用自动扩展策略，随着流量的增加，我们的端点部署的实例数量也会增加，随着流量的减少而减少。这不仅有助于提高我们的推理端点的高可用性，还有助于显著降低成本。我们可以通过 SageMaker 控制台、AWS CLI 或应用程序自动扩展 API 来为模型启用自动扩展。

为了应用自动扩展，我们需要一个自动扩展策略。自动扩展策略使用 Amazon CloudWatch 指标和由我们指定的目标值来决定何时向上或向下扩展实例。我们还需要定义端点可以部署的最小和最大实例数。自动扩展策略的其他组件包括执行自动扩展所需的权限、服务关联的 IAM 角色以及缩放活动后的冷却期，在开始下一次缩放活动之前等待一段时间。

*图 7.19* – *7.22* 显示使用 Amazon SageMaker 控制台在异步端点上配置自动扩展：

1.  *图 7**.19* 显示，最初端点仅部署在单个实例上：

![图 7.19 – 端点运行时间设置显示端点正在实例上部署且未使用自动扩展](img/B18493_07_019.jpg)

图 7.19 – 端点运行时间设置显示端点正在实例上部署且未使用自动扩展

点击**配置自动扩展**按钮后，您的屏幕应类似于*图 7**.20*和*图 7**.21*中所示。

1.  如*图 7**.20*所示，我们需要将最小和最大实例数分别更新为`2`和`10`。

![图 7.20 – 在端点上配置自动扩展以使用 2 – 10 个实例，根据流量（使用 SageMaker 控制台）](img/B18493_07_020.jpg)

图 7.20 – 在端点上配置自动扩展以使用 2 – 10 个实例，根据流量（使用 SageMaker 控制台）。

1.  如*图 7**.21*所示，我们需要更新`200`的值。

![图 7.21 – 设置自动扩展的缩放时间和目标指标阈值以触发自动扩展](img/B18493_07_021.jpg)

图 7.21 – 设置自动扩展的缩放时间和目标指标阈值以触发自动扩展

一旦达到此目标值并且我们走出冷却期，无论我们是在目标值之上还是之下，SageMaker 都会自动启动新实例或关闭实例。

1.  *图 7**.22* 显示端点更新后应用我们的自动扩展策略的结果。

![图 7.22 – 应用自动扩展策略后运行的端点](img/B18493_07_022.jpg)

图 7.22 – 应用自动扩展策略后运行的端点

应用自动扩展策略后，也可以更新或删除该策略。

## 无中断的端点修改

我们还可以修改已部署的端点，而不会影响在生产中部署的模型的可用性。除了在上一节中讨论的应用自动扩展策略外，我们还可以更新端点的计算实例配置。我们还可以添加新的模型变体并更改不同模型变体之间的流量模式。所有这些任务都可以在不影响生产中部署的端点的情况下完成。

现在我们已经讨论了如何确保我们的端点具有高可用性，接下来让我们讨论蓝/绿部署。

# 蓝绿部署

在一个生产环境中，我们的模型正在实时或接近实时地运行以进行推理，当我们需要更新我们的端点时，确保这一过程不会造成任何中断或问题非常重要。Amazon SageMaker在更新我们的端点时自动使用蓝/绿部署方法。在这种场景下，一个新的集群，称为绿色集群，被配置了我们的更新端点。然后，工作负载从旧的集群（称为蓝色集群）转移到绿色集群。经过一段评估期以确保一切运行正常，没有出现任何问题后，蓝色集群被终止。SageMaker还提供了以下三种不同的流量转移模式，用于蓝/绿部署，使我们能够对流量转移模式有更多的控制。

## 同时全部更新

在这种流量转移模式下，所有流量一次性从蓝色集群转移到绿色集群。蓝色（旧）集群在一段时间内（称为烘烤期）保持运行，以确保一切正常，性能和功能符合预期。烘烤期结束后，一次性终止蓝色集群。这种类型的蓝绿部署最小化了整体更新时间，同时也最小化了成本。一次性方法的缺点是，回归性更新会影响所有流量，因为整个流量一次性转移到绿色集群。

## 金丝雀

在金丝雀蓝/绿部署中，首先将一小部分流量从蓝色集群转移到绿色集群。这部分开始服务部分流量的绿色集群被称为金丝雀，其容量应小于新集群容量的50%。如果在烘烤期间一切正常，没有触发CloudWatch警报，则其余流量也将转移到绿色集群，之后终止蓝色集群。如果在烘烤期间触发任何警报，SageMaker将所有流量回滚到蓝色集群。

使用金丝雀蓝/绿部署的优势在于，它将回归性更新的影响范围仅限于金丝雀集群，而不是整个集群，这与一次性蓝/绿部署不同。使用金丝雀部署的缺点是，在整个部署期间，蓝色和绿色集群都处于运行状态，从而增加了成本。

## 线性

在线性蓝/绿部署中，流量以预指定的固定步骤从蓝色舰队转移到绿色舰队。一开始，流量的一部分转移到绿色舰队，同时相同比例的蓝色舰队被停用。如果在烘烤期间没有警报响起，SageMaker将启动第二部分的转移，依此类推。如果在任何时刻警报响起，SageMaker将所有流量回滚到蓝色舰队。由于流量是分步骤转移到绿色舰队的，线性蓝/绿部署显著降低了回归更新的风险。线性部署方法的成本与配置用于将流量从蓝色舰队转移到绿色舰队的步骤数量成正比。

如本节所述，SageMaker具有这些蓝/绿部署方法，以确保我们在更新端点时有安全防护措施。这些蓝/绿部署方法确保我们能够在生产环境中以无干扰或最小干扰的方式更新我们的推理端点。

现在让我们总结一下本章所涵盖的内容。

# 摘要

在本章中，我们讨论了使用Amazon SageMaker时可以使用的各种托管部署方法。我们讨论了不同部署/推理方法对不同用例类型的适用性。我们展示了如何进行批量推理和部署实时和异步端点。我们还讨论了如何配置SageMaker以自动扩展和缩减，以及SageMaker如何确保在发生故障时，我们的端点被部署到多个可用区。我们还简要介绍了Amazon SageMaker提供的各种蓝/绿部署方法，以便在生产环境中更新我们的端点。

在许多实际场景中，我们没有高性能的实例集群可用于实时处理新数据和未见数据。对于此类应用，我们需要使用边缘计算设备。这些设备通常在计算能力、内存、连接性和带宽方面有限制，并且需要模型被优化以便能够在这些边缘设备上使用。

在下一章中，我们将扩展这一讨论，了解如何在边缘设备上使用机器学习模型。

# 参考文献

您可以参考以下资源获取更多信息：

+   在单个端点上托管多个模型：[https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html)

+   Amazon EC2 Inf1实例：[https://aws.amazon.com/ec2/instance-types/inf1/](https://aws.amazon.com/ec2/instance-types/inf1/)

+   使用SageMaker批量转换自己的推理代码：[https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-batch-code.html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-batch-code.html)

+   单个端点后多个不同容器模型：[https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html)

+   Amazon SageMaker 上的无服务器推理：[https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html)

+   使用 Amazon SageMaker 的蓝绿部署：[https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html](https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html)

+   金丝雀流量切换：[https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green-canary.html](https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green-canary.html)

+   SageMaker 的 Transformer 类：[https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html?highlight=transformer](https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html?highlight=transformer)

+   Amazon SageMaker 实时推理：[https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html)

+   使用 Amazon SageMaker 运行模型的最佳实践：[https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-best-practices.html](https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-best-practices.html)
