<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Semi-Supervised and Active Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Semi-Supervised and Active Learning</h1></div></div></div><p>In <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span> and <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>, we discussed two major groups of machine learning techniques which apply to opposite situations when it comes to the availability of labeled data—one where all target values are known and the other where none are. In contrast, the techniques in this chapter address the situation when we must analyze and learn from data that is a mix of a small portion with labels and a large number of unlabeled instances. </p><p>In speech and image recognition, a vast quantity of data is available, and in various forms. However, the cost of labeling or classifying all that data is costly and therefore, in practice, the proportion of speech or images that are classified to those that are not classified is very small. Similarly, in web text or document classification, there are an enormous number of documents on the World Wide Web but classifying them based on either topics or contexts requires domain experts—this makes the process complex and expensive. In this chapter, we will discuss two broad topics that cover the area of "learning from unlabeled data", namely <span class="strong"><strong>Semi-Supervised Learning</strong></span> (<span class="strong"><strong>SSL</strong></span>) and Active Learning. We <a id="id776" class="indexterm"/>will introduce each of the topics and discuss the taxonomy and algorithms associated with each as we did in previous chapters. Since the  book emphasizes the practical approach, we will discuss tools and libraries available for each type of learning. We will then consider real-world case studies and demonstrate the techniques that are useful when applying the tools in practical situations.</p><p>Here is the list of topics that are covered in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Semi-Supervised Learning:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Representation, notation, and assumptions</li><li class="listitem" style="list-style-type: disc">Semi-Supervised Learning techniques:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Self-training SSL</li><li class="listitem" style="list-style-type: disc">Co-training SSL</li><li class="listitem" style="list-style-type: disc">Cluster and label SSL</li><li class="listitem" style="list-style-type: disc">Transductive graph label propagation</li><li class="listitem" style="list-style-type: disc">Transductive SVM</li></ul></div></li><li class="listitem" style="list-style-type: disc">Case study in Semi-Supervised Learning</li></ul></div></li><li class="listitem" style="list-style-type: disc">Active Learning:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Representation and notation</li><li class="listitem" style="list-style-type: disc">Active Learning scenarios</li><li class="listitem" style="list-style-type: disc">Active Learning approaches:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Uncertainty sampling<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Least confident sampling</li><li class="listitem" style="list-style-type: disc">Smallest margin sampling</li><li class="listitem" style="list-style-type: disc">Label entropy sampling</li></ul></div></li><li class="listitem" style="list-style-type: disc">Version space sampling:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Query by disagreement</li><li class="listitem" style="list-style-type: disc">Query by committee</li></ul></div></li><li class="listitem" style="list-style-type: disc">Data distribution sampling:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Expected model change</li><li class="listitem" style="list-style-type: disc">Expected error reduction</li><li class="listitem" style="list-style-type: disc">Variance reduction</li><li class="listitem" style="list-style-type: disc">Density weighted methods</li></ul></div></li></ul></div></li><li class="listitem" style="list-style-type: disc">Case study in Active Learning</li></ul></div></li></ul></div><div class="section" title="Semi-supervised learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec34"/>Semi-supervised learning</h1></div></div></div><p>The idea <a id="id777" class="indexterm"/>behind semi-supervised learning is to learn from labeled and unlabeled data to improve the predictive power of the models.  The notion is explained with a simple illustration, <span class="emphasis"><em>Figure 1</em></span>, which shows that when a large amount of unlabeled data is available, for example, HTML documents on the web, the expert can classify a few of them into known categories such as sports, news, entertainment, and so on. This small set of labeled data together with the large unlabeled dataset can then be used by semi-supervised learning techniques to learn models. Thus, using the knowledge of both labeled and unlabeled data, the model can classify unseen documents in the future. In contrast, supervised learning uses labeled data only:</p><div class="mediaobject"><img src="graphics/B05137_04_003.jpg" alt="Semi-supervised learning"/></div><div class="mediaobject"><img src="graphics/B05137_04_005.jpg" alt="Semi-supervised learning"/><div class="caption"><p>Figure 1. Semi-Supervised Learning process (bottom) contrasted with Supervised Learning (top) using classification of web documents as an example. The main difference is the amount of labeled data available for learning, highlighted by the qualifier "small" in the semi-supervised case.</p></div></div><div class="section" title="Representation, notation, and assumptions"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec48"/>Representation, notation, and assumptions</h2></div></div></div><p>As before, we <a id="id778" class="indexterm"/>will introduce the notation <a id="id779" class="indexterm"/>we use in this chapter. The dataset <span class="emphasis"><em>D</em></span> consists of individual data instances represented as <span class="strong"><strong>x</strong></span>, which is also represented as a set {<span class="strong"><strong>x</strong></span><sub>1</sub>, <span class="strong"><strong>x</strong></span><sub>2</sub>,…<span class="strong"><strong>x</strong></span><sub>n</sub>}, the set of data instances without labels. The labels associated with <a id="id780" class="indexterm"/>these data instances are {<span class="emphasis"><em>y</em></span><sub>1</sub>, <span class="emphasis"><em>y</em></span><sub>2</sub>, … <span class="emphasis"><em>y</em></span><sub>n</sub>}. The entire labeled dataset can be represented as paired elements in a set, as given by <span class="emphasis"><em>D</em></span> = {(<span class="strong"><strong>x</strong></span><sub>1</sub>, <span class="emphasis"><em>y</em></span><sub>1</sub>), (<span class="strong"><strong>x</strong></span>2,<span class="emphasis"><em>y</em></span><sub>2</sub>), … (<span class="strong"><strong>x</strong></span><sub>n</sub>, <span class="emphasis"><em>y</em></span><sub>n</sub>)} where <span class="strong"><strong>x</strong></span><sub>i</sub> ∈ ℝ<sup>d</sup>. In semi-supervised learning, we divide the dataset <span class="emphasis"><em>D</em></span> further into two sets <span class="emphasis"><em>U</em></span> and <span class="emphasis"><em>L</em></span> for unlabeled and labeled data respectively. </p><p>The labeled data <span class="inlinemediaobject"><img src="graphics/B05137_04_015.jpg" alt="Representation, notation, and assumptions"/></span> consists of all labeled data with known outcomes {y<sub>1</sub>, y<sub>2</sub>, .. y<sub>l</sub>}. The unlabeled data <span class="inlinemediaobject"><img src="graphics/B05137_04_017.jpg" alt="Representation, notation, and assumptions"/></span> is the dataset where the outcomes are not known. |<span class="emphasis"><em>U</em></span>| &gt; |<span class="emphasis"><em>L</em></span>| .</p><p>Inductive semi-supervised learning consists of a set of techniques, which, given the training set <span class="emphasis"><em>D</em></span> with labeled data <span class="inlinemediaobject"><img src="graphics/B05137_04_015.jpg" alt="Representation, notation, and assumptions"/></span> and unlabeled data <span class="inlinemediaobject"><img src="graphics/B05137_04_017.jpg" alt="Representation, notation, and assumptions"/></span>, learns a model represented as <span class="inlinemediaobject"><img src="graphics/B05137_04_021.jpg" alt="Representation, notation, and assumptions"/></span> so that the model <span class="emphasis"><em>f</em></span> can be a good predictor on unseen data beyond the training unlabeled data <span class="emphasis"><em>U</em></span>. It "induces" a model that can be used just like supervised learning algorithms to predict on unseen instances.</p><p>Transductive semi-supervised learning consists of a set of techniques, which, given the training set <span class="emphasis"><em>D</em></span>, learns a model <span class="inlinemediaobject"><img src="graphics/B05137_04_024.jpg" alt="Representation, notation, and assumptions"/></span> that makes predictions on unlabeled data alone. It is not required to perform on unseen future instances and hence is a simpler form of SSL than inductive based learning.</p><p>Some of the assumptions made in the semi-supervised learning algorithms that should hold true for these types of learning to be successful are noted in the following list. For SSL to work, one <a id="id781" class="indexterm"/>or more of these assumptions must be true:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Semi-supervised smoothness</strong></span>: In simple terms, if two points are "close" in terms of density or distance, then their labels agree. Conversely, if two points are separated and in different density regions, then their labels need not agree. </li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Cluster togetherness</strong></span>: If the data instances of classes tend to form a cluster, then the unlabeled data can aid the clustering algorithm to find better clusters. </li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Manifold togetherness</strong></span>: In many real-world datasets, the high-dimensional data lies in a low-dimensional manifold, enabling learning algorithms to overcome the curse of dimensionality. If this is true in the given dataset, the unlabeled data also maps to the manifold and can improve the learning.</li></ul></div></div><div class="section" title="Semi-supervised learning techniques"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec49"/>Semi-supervised learning techniques</h2></div></div></div><p>In <a id="id782" class="indexterm"/>this section, we will describe different SSL techniques, and some accompanying algorithms. We will use the same structure as in previous chapters and describe each method in three subsections: <span class="emphasis"><em>Inputs and outputs</em></span>, <span class="emphasis"><em>How does it work?</em></span>, and <span class="emphasis"><em>Advantages and limitations</em></span>.</p><div class="section" title="Self-training SSL"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec55"/>Self-training SSL</h3></div></div></div><p>Self-training <a id="id783" class="indexterm"/>is the simplest form <a id="id784" class="indexterm"/>of SSL, where we perform a simple iterative process of imputing the data from the unlabeled set by applying the model learned from the labeled set (<span class="emphasis"><em>References</em></span> [1]):</p><div class="mediaobject"><img src="graphics/B05137_04_025.jpg" alt="Self-training SSL"/><div class="caption"><p>Figure 2.  Self-training SSL in binary classification with some labeled data shown with blue rectangles and yellow circles. After various iterations, the unlabeled data gets mapped to the respective classes.</p></div></div><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec109"/>Inputs and outputs</h4></div></div></div><p>The input <a id="id785" class="indexterm"/>is training data with a small amount of labeled and a large amount <a id="id786" class="indexterm"/>of unlabeled data. A base classifier, either linear or non-linear, such as Naïve Bayes, KNN, Decision Tree, or other, is provided along with the hyper-parameters needed for each of the algorithms. The constraints on data types will be similar to the base learner. Stopping conditions such as <span class="emphasis"><em>maximum iterations reached</em></span> or <span class="emphasis"><em>unlabeled data exhausted</em></span> are choices that must be made as well. Often, we use base learners which give probabilities or ranks to the outputs. As output, this technique generates models that can be used for performing predictions on unseen datasets other than the unlabeled data provided.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec110"/>How does it work? </h4></div></div></div><p>The <a id="id787" class="indexterm"/>entire algorithm can be summarized as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">While stopping criteria not reached:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Train the classifier model <span class="inlinemediaobject"><img src="graphics/B05137_04_021.jpg" alt="How does it work?"/></span> with labeled data <span class="emphasis"><em>L</em></span></li><li class="listitem">Apply the classifier model <span class="emphasis"><em>f</em></span> on unlabeled data <span class="emphasis"><em>U</em></span></li><li class="listitem">Choose <span class="emphasis"><em>k</em></span> most confident predictions from <span class="emphasis"><em>U</em></span> as set <span class="emphasis"><em>L</em></span><sub>u</sub></li><li class="listitem">Augment the labeled data with the k data points <span class="emphasis"><em>L = L </em></span><span class="emphasis"><em>∪</em></span> <span class="emphasis"><em>L</em></span><sub>u</sub></li></ol></div></li><li class="listitem">Repeat all the steps under 2.</li></ol></div><p>In the abstract, self-training can be seen as an expectation maximization process applied to a semi-supervised setting. The process of training the classifier model is finding the parameter θ using MLE or MAP. Computing the labels using the learned model is similar to the <span class="emphasis"><em>EXPECTATION</em></span> step where <span class="inlinemediaobject"><img src="graphics/B05137_04_031.jpg" alt="How does it work?"/></span> is estimating the label from <span class="emphasis"><em>U</em></span> given the parameter θ. The iterative next step of learning the model with augmented labels is akin to the <span class="emphasis"><em>MAXIMIZATION</em></span> step where the new parameter is tuned to <span class="emphasis"><em>θ'</em></span>.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec111"/>Advantages and limitations</h4></div></div></div><p>The <a id="id788" class="indexterm"/>advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Simple, works <a id="id789" class="indexterm"/>with most supervised learning techniques. </li><li class="listitem" style="list-style-type: disc">Outliers and noise can cause mistakes in predictions to be reinforced and the technique to degrade.</li></ul></div></div></div><div class="section" title="Co-training SSL or multi-view SSL"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec56"/>Co-training SSL or multi-view SSL</h3></div></div></div><p>Co-training <a id="id790" class="indexterm"/>based SSL involves learning from two different "views" of the same data. It is a special case of multi-view SS (References [2]). Each view <a id="id791" class="indexterm"/>can be considered as a feature set of the point capturing <a id="id792" class="indexterm"/>some domain knowledge <a id="id793" class="indexterm"/>and is orthogonal to the other view. For example, a web documents dataset can be considered to have two views: one view is features representing the text and the other view is features representing hyperlinks to other documents. The assumption is that there is enough data for each view and learning from each view improves the overall labeling process. In datasets where such partitions of features are not possible, splitting features randomly into disjoint sets forms the views.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec112"/>Inputs and outputs</h4></div></div></div><p>Input <a id="id794" class="indexterm"/>is training data with a few labeled and a large number <a id="id795" class="indexterm"/>of unlabeled data. In addition to providing the data points, there are feature sets corresponding to each view and the assumption is that these feature sets are not overlapping and solve different classification problems. A base classifier, linear or non-linear, such as Naïve Bayes, KNN, Decision Tree, or any other, is selected along with the hyper-parameters needed for each of the algorithms. As output, this method generates models that can be used for performing predictions on unseen datasets other than the unlabeled data provided.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec113"/>How does it work?</h4></div></div></div><p>We will <a id="id796" class="indexterm"/>demonstrate the algorithm using two views of the data:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Initialize the data as <span class="inlinemediaobject"><img src="graphics/B05137_04_015.jpg" alt="How does it work?"/></span> labeled and <span class="inlinemediaobject"><img src="graphics/B05137_04_017.jpg" alt="How does it work?"/></span> unlabeled. Each data point has two views <span class="strong"><strong>x = [x</strong></span><span class="strong"><strong><sup>1</sup></strong></span><span class="strong"><strong>,x</strong></span><span class="strong"><strong><sup>2</sup></strong></span><span class="strong"><strong>]</strong></span> and <span class="emphasis"><em>L = [L</em></span><span class="emphasis"><em><sup>1</sup></em></span><span class="emphasis"><em>,L</em></span><span class="emphasis"><em>2</em></span><span class="emphasis"><em>]</em></span>.
</li><li class="listitem">While stopping criteria not reached:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Train the classifier models <span class="inlinemediaobject"><img src="graphics/B05137_04_035.jpg" alt="How does it work?"/></span> and <span class="inlinemediaobject"><img src="graphics/B05137_04_036.jpg" alt="How does it work?"/></span> with labeled data <span class="emphasis"><em>L</em></span>1 and <span class="emphasis"><em>L</em></span>2  respectively.
</li><li class="listitem">Apply the classifier models <span class="emphasis"><em>f</em></span><sup>1</sup> and <span class="emphasis"><em>f</em></span><sup>2</sup> on unlabeled data <span class="emphasis"><em>U</em></span> using their own features.</li><li class="listitem">Choose <span class="emphasis"><em>k</em></span> the most confident predictions from <span class="emphasis"><em>U</em></span>, applying <span class="emphasis"><em>f</em></span><sup>1</sup> and <span class="emphasis"><em>f</em></span><sup>2</sup> as set <span class="emphasis"><em>L</em></span><sub>u</sub><sup>1</sup> and <span class="emphasis"><em>L</em></span><sub>u</sub><sup>2</sup> respectively.</li><li class="listitem">Augment the labeled data with the <span class="emphasis"><em>k</em></span> data points <span class="emphasis"><em>L</em></span><sup>1</sup> = <span class="emphasis"><em>L</em></span><sup>1</sup> ∪ <span class="emphasis"><em>L</em></span><sub>u</sub><sup>1</sup> and <span class="emphasis"><em>L</em></span><sup>2</sup> = <span class="emphasis"><em>L</em></span><sup>2</sup> ∪ <span class="emphasis"><em>L</em></span><sub>u</sub><sup>2</sup></li></ol></div></li><li class="listitem">Repeat all the steps under 2.</li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec114"/>Advantages and limitations</h4></div></div></div><p>The advantages and limitations are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">When the features have different aspects or a mix of different domains, co-training <a id="id797" class="indexterm"/>becomes more beneficial than simple <a id="id798" class="indexterm"/>self-training</li><li class="listitem" style="list-style-type: disc">The necessary and sufficient condition of having orthogonal views and ability to learn from them poses challenges for the generality of the technique</li></ul></div></div></div><div class="section" title="Cluster and label SSL"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec57"/>Cluster and label SSL</h3></div></div></div><p>This <a id="id799" class="indexterm"/>technique, like <a id="id800" class="indexterm"/>self-training, is quiet generic and applicable to <a id="id801" class="indexterm"/>domains and datasets <a id="id802" class="indexterm"/>where the clustering supposition mentioned in the assumptions section holds true (References [3]).</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec115"/>Inputs and outputs</h4></div></div></div><p>Input <a id="id803" class="indexterm"/>is training <a id="id804" class="indexterm"/>data with a few labeled <a id="id805" class="indexterm"/>and a large number of unlabeled instances. A clustering algorithm <a id="id806" class="indexterm"/>and its parameters along with a classification algorithm with its parameters constitute additional inputs. The technique generates a classification model that can help predict the classes of unseen data.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec116"/>How does it work?</h4></div></div></div><p>The <a id="id807" class="indexterm"/>abstract algorithm <a id="id808" class="indexterm"/>can be given as: </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Initialize data as <span class="inlinemediaobject"><img src="graphics/B05137_04_015.jpg" alt="How does it work?"/></span> labeled and <span class="inlinemediaobject"><img src="graphics/B05137_04_017.jpg" alt="How does it work?"/></span> unlabeled.
</li><li class="listitem">Cluster the entire data, both labeled and unlabeled using the clustering algorithm.</li><li class="listitem">For each cluster let <span class="emphasis"><em>S</em></span> be the set of labeled instances drawn from set <span class="emphasis"><em>L</em></span>.<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Learn a supervised model from <span class="emphasis"><em>S</em></span>, <span class="emphasis"><em>f</em></span><sub>s</sub> = <span class="emphasis"><em>L</em></span><sub>s</sub>.</li><li class="listitem">Apply the model <span class="emphasis"><em>f</em></span><sub>s</sub> and classify unlabeled instances for each cluster using the preceding model.</li></ol></div></li><li class="listitem">
Since all the unlabeled instances <span class="inlinemediaobject"><img src="graphics/B05137_04_017.jpg" alt="How does it work?"/></span> get assigned a label by the preceding process, a supervised classification model is run on the entire set.
<div class="mediaobject"><img src="graphics/B05137_04_048.jpg" alt="How does it work?"/><div class="caption"><p>Figure 3.  Clustering and label SSL – clustering followed by classification</p></div></div></li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec117"/>Advantages and limitations</h4></div></div></div><p>The <a id="id809" class="indexterm"/>advantages and <a id="id810" class="indexterm"/>limitations are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Works <a id="id811" class="indexterm"/>very well when the cluster assumption holds true <a id="id812" class="indexterm"/>and the choice of clustering algorithm and parameters are correct</li><li class="listitem" style="list-style-type: disc">Large number of parameters and choices make this an unwieldy technique in many real-world problems</li></ul></div></div></div><div class="section" title="Transductive graph label propagation"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec58"/>Transductive graph label propagation</h3></div></div></div><p>The <a id="id813" class="indexterm"/>key idea <a id="id814" class="indexterm"/>behind graph-based methods is to represent every instance in the dataset, labeled and unlabeled, as a node and compute the edges as some form of "similarity" between them. Known labels are used to propagate labels in the unlabeled data using the basic concepts of label smoothness as discussed in the assumptions section, that is, similar data points will lie "close" o each other graphically (<span class="emphasis"><em>References</em></span> [4]).</p><p>Figure 4 shows how the similarity indicated by the thickness of the arrow from the first data point to the last varies when the handwritten digit pattern changes. Knowing the first label, the label propagation can effectively label the next three digits due to the similarity in features while the last digit, though labeled the same, has a lower similarity as compared to the first three.</p><div class="mediaobject"><img src="graphics/B05137_04_050.jpg" alt="Transductive graph label propagation"/><div class="caption"><p>Figure 4.  Transductive graph label propagation – classification of hand-written digits. Leftmost and rightmost images are labeled, others are unlabeled. Arrow thickness is a visual measure of similarity to labeled digit "2" on the left.</p></div></div><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec118"/>Inputs and outputs</h4></div></div></div><p>Input <a id="id815" class="indexterm"/>is training data with a few labeled <a id="id816" class="indexterm"/>and a large number of unlabeled data. The graph weighting or similarity computing method, such as k-nearest weighting, Gaussian decaying distance, or ϵ-radius method is chosen. Output is the labeled set for the entire data; it generally doesn't build inductive models like the algorithms seen previously. </p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec119"/>How does it work? </h4></div></div></div><p>The <a id="id817" class="indexterm"/>general label propagation method follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Build a graph <span class="emphasis"><em>g = (V,E)</em></span> where:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Vertices <span class="emphasis"><em>V = {1, 2…n}</em></span> correspond to data belonging to both labeled set <span class="emphasis"><em>L</em></span> and unlabeled set <span class="emphasis"><em>U</em></span>.</li><li class="listitem" style="list-style-type: disc">Edges <span class="emphasis"><em>E</em></span> are weight matrices <span class="strong"><strong>W</strong></span>, such that <span class="strong"><strong>W</strong></span><sub>i,j</sub> represents similarity in some form between two data points <span class="strong"><strong>x</strong></span><sub>i</sub>, <span class="strong"><strong>x</strong></span><sub>j</sub>.</li></ul></div></li><li class="listitem">
Compute the diagonal degree matrix <span class="strong"><strong>D</strong></span> by <span class="inlinemediaobject"><img src="graphics/B05137_04_059.jpg" alt="How does it work?"/></span>.
</li><li class="listitem">
Assume the labeled set is binary and has <span class="inlinemediaobject"><img src="graphics/B05137_04_060.jpg" alt="How does it work?"/></span>. Initialize the labels of all unlabeled data to be 0. <span class="inlinemediaobject"><img src="graphics/B05137_04_061.jpg" alt="How does it work?"/></span></li><li class="listitem">Iterate at <span class="emphasis"><em>t</em></span> = 0:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B05137_04_063.jpg" alt="How does it work?"/></span></li><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B05137_04_064.jpg" alt="How does it work?"/></span> (reset the labels of labeled instances back to the original)</li><li class="listitem">
Go back to step 4, until convergence <span class="inlinemediaobject"><img src="graphics/B05137_04_065.jpg" alt="How does it work?"/></span></li></ol></div></li><li class="listitem">
Label the unlabeled points <span class="inlinemediaobject"><img src="graphics/B05137_04_017.jpg" alt="How does it work?"/></span> using the convergence labels <span class="inlinemediaobject"><img src="graphics/B05137_04_065.jpg" alt="How does it work?"/></span>.
</li></ol></div><p>There are many variations based on similarity, optimization selected in iterations, and so on.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec120"/>Advantages and limitations</h4></div></div></div><p>The <a id="id818" class="indexterm"/>advantages and limitations are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <a id="id819" class="indexterm"/>graph-based semi-supervised learning methods are costly in terms of computations—generally O(n<sup>3</sup>) where <span class="emphasis"><em>n</em></span> is the number of instances. Though speeding and caching techniques help, the computational cost over large data makes it infeasible in many real-world data situations. </li><li class="listitem" style="list-style-type: disc">The transductive nature makes it difficult for practical purposes where models need to be induced for unseen data. There are extensions such as Harmonic Mixtures, and so on, which address these concerns.</li></ul></div></div></div><div class="section" title="Transductive SVM (TSVM)"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec59"/>Transductive SVM (TSVM)</h3></div></div></div><p>Transductive <a id="id820" class="indexterm"/>SVM is one <a id="id821" class="indexterm"/>of the oldest and most popular transductive semi-supervised learnig methods, introduced by Vapnik (<span class="emphasis"><em>References</em></span> [5]). The key principle is that unlabeled data along with labeled data can help find the decision boundary using concepts of large margins. The underlying principle is that the decision boundaries normally don't lie in high density regions!</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec121"/>Inputs and outputs</h4></div></div></div><p>Input <a id="id822" class="indexterm"/>is training data with few labeled and a large <a id="id823" class="indexterm"/>number of unlabeled data. Input has to be numeric features for TSVM computations. The choice of kernels, kernel parameters, and cost factors, which are all SVM-based parameters, are also input variables. The output is labels for the unlabeled dataset.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec122"/>How does it work?</h4></div></div></div><p>Generally, SVM <a id="id824" class="indexterm"/>works as an optimization problem in the labeled hard boundary SVM formulated in terms of weight vector <span class="strong"><strong>w</strong></span> and the bias <span class="emphasis"><em>b</em></span> <span class="inlinemediaobject"><img src="graphics/B05137_04_070.jpg" alt="How does it work?"/></span> subject to <span class="inlinemediaobject"><img src="graphics/B05137_04_072.jpg" alt="How does it work?"/></span>
</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Initialize the data as <span class="inlinemediaobject"><img src="graphics/B05137_04_015.jpg" alt="How does it work?"/></span> labeled and <span class="inlinemediaobject"><img src="graphics/B05137_04_017.jpg" alt="How does it work?"/></span> unlabeled.
</li><li class="listitem">In TSVM, the equation is modified as follows:<div class="mediaobject"><img src="graphics/B05137_04_073.jpg" alt="How does it work?"/></div></li></ol></div><p>This is subject to the following condition:</p><div class="mediaobject"><img src="graphics/B05137_04_074.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_04_075.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_04_076.jpg" alt="How does it work?"/></div><p>This is exactly like inductive SVM but using only labeled data. When we constrain the unlabeled <a id="id825" class="indexterm"/>data to conform to the side of the hyperplane of labeled data in order to maximize the margin, it results in unlabeled data being labeled with maximum margin separation! By adding the penalty factor to the constraints or replacing the dot product in the input space with kernels as in inductive SVM, complex non-linear noisy datasets can be labeled from unlabeled data. </p><p>
<span class="emphasis"><em>Figure 5</em></span> illustrates the concept of TSVM in comparison with inductive SVM run on labeled data only and why TSVM can find better decision boundaries using the unlabeled datasets. The unlabeled datasets on either side of hyperplane are closer to their respective classes, thus helping find better margin separators.</p><div class="mediaobject"><img src="graphics/B05137_04_077.jpg" alt="How does it work?"/><div class="caption"><p>Figure 5. Transductive SVM</p></div></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec123"/>Advantages and limitations</h4></div></div></div><p>The <a id="id826" class="indexterm"/>advantages and limitations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">TSVMs can <a id="id827" class="indexterm"/>work very well in linear or non-linear datasets given noiseless labeled data.</li><li class="listitem" style="list-style-type: disc">TSVMs have the same issues in finding hyper-parameters and tuning them to get the best results as inductive SVMs.</li></ul></div></div></div></div><div class="section" title="Case study in semi-supervised learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec50"/>Case study in semi-supervised learning</h2></div></div></div><p>For <a id="id828" class="indexterm"/>this case study, we use another well-studied dataset from the UCI Repository, the Wisconsin Breast Cancer dataset. In the first part of the experiment, we demonstrate how to apply the Transductive SVM technique of semi-supervised learning using the open-source library called <code class="literal">JKernelMachines</code>. We choose the SVMLight algorithm and a Gaussian kernel for this technique. </p><p>In the second part, we use KEEL, a GUI-based framework and compare results from several evolutionary learning based algorithms using the UCI Breast Cancer dataset. The tools, methodology, and evaluation measures are described in the following subsections.</p><div class="section" title="Tools and software"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec60"/>Tools and software</h3></div></div></div><p>The <a id="id829" class="indexterm"/>two open source Java tools <a id="id830" class="indexterm"/>used in the semi-supervised learning case study are <code class="literal">JKernelMachines</code>, a Transductive SVM, and KEEL, a GUI-based tool that uses evolutionary algorithms for learning.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note08"/>Note</h3><p>
<span class="strong"><strong>JKernelMachines (Transductive SVM)</strong></span>
</p><p>
<code class="literal">JKernelMachines</code> is a pure Java library that provides an efficient framework for using and rapidly developing specialized kernels. Kernels are similarity functions <a id="id831" class="indexterm"/>used in SVMs. <code class="literal">JKernelMachines</code> provides kernel implementations defined on structured data in addition to standard kernels on vector data such as Linear and Gaussian. In particular, it offers a combination of kernels, kernels defined over lists, and kernels with various caching strategies.  The library also contains SVM optimization algorithm implementations including LaSVM and One-Class SVM using SMO.  The creators of the library report that the results of JKernelMachines on some common UCI repository datasets are comparable to or better than the Weka library.</p></div></div><p>The example of loading data and running Transductive SVM using <code class="literal">JKernelMachines</code> is given here:</p><div class="informalexample"><pre class="programlisting">try {
//load the labeled training data
List&lt;TrainingSample&lt;double[]&gt;&gt; labeledTraining = ArffImporter.importFromFile("resources/breast-labeled.arff");
//load the unlabeled data
List&lt;TrainingSample&lt;double[]&gt;&gt; unlabeledData =ArffImporter.importFromFile("resources/breast-unlabeled.arff");
//create a kernel with Gaussian and gamma set to 1.0
DoubleGaussL2 k = new DoubleGaussL2(1.0);
//create transductive SVM with SVM light
S3VMLight&lt;double[]&gt; svm = new S3VMLight&lt;double[]&gt;(k);
//send the training labeled and unlabeled data
svm.train(labeledTraining, unlabeledData);
} catch (IOException e) {
	e.printStackTrace();
}</pre></div><p>In the second approach, we use KEEL with the same dataset.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>
<span class="strong"><strong>KEEL</strong></span>
</p><p>
<span class="strong"><strong>KEEL</strong></span> (<span class="strong"><strong>Knowledge Extraction based on Evolutionary Learning</strong></span>) is a non-commercial (GPLv3) Java tool with GUI which enables users to analyze the behavior <a id="id832" class="indexterm"/>of evolutionary learning for a variety of data mining problems, including regression, classification, and unsupervised learning. It relieves users from the burden of programming sophisticated evolutionary algorithms and allows them to focus on new learning models created using the toolkit. KEEL is intended to meet the needs of researchers as well as students.</p><p>KEEL contains algorithms for data preprocessing and post-processing as well as statistical <a id="id833" class="indexterm"/>libraries, and a Knowledge Extraction Algorithms Library which incorporates multiple <a id="id834" class="indexterm"/>evolutionary learning algorithms with classical learning techniques.</p><p>The GUI wizard included in the tool offers different functional components for each stage of the pipeline, including:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data management: Import, export of data, data transformation, visualization, and so on</li><li class="listitem" style="list-style-type: disc">Experiment design: Selection of classifier, estimator, unsupervised techniques, validation method, and so on </li><li class="listitem" style="list-style-type: disc">SSL experiments: Transductive and inductive classification (see image of off-line method for SSL experiment design in this section)</li><li class="listitem" style="list-style-type: disc">Statistical <a id="id835" class="indexterm"/>analysis: This provides tests for pair-wise and multiple comparisons, parametric, and non-parametric procedures.</li></ul></div><p>For more info, visit <a class="ulink" href="http://sci2s.ugr.es/keel/">http://sci2s.ugr.es/keel/</a> and <a class="ulink" href="http://sci2s.ugr.es/keel/pdf/keel/articulo/Alcalaetal-SoftComputing-Keel1.0.pdf">http://sci2s.ugr.es/keel/pdf/keel/articulo/Alcalaetal-SoftComputing-Keel1.0.pdf</a>.</p></div></div><div class="mediaobject"><img src="graphics/B05137_04_078.jpg" alt="Tools and software"/><div class="caption"><p>Figure 6: KEEL – wizard-based graphical interface </p></div></div></div><div class="section" title="Business problem"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec61"/>Business problem</h3></div></div></div><p>Breast <a id="id836" class="indexterm"/>cancer is the top cancer in women worldwide and is increasing particularly in developing countries where the majority of cases are diagnosed in late stages.  Examination of tumor mass using a non-surgical procedure is an inexpensive and preventative measure for early detection of the disease. </p><p>In this case-study, a marked dataset from such a procedure is used and the goal is to classify the breast cancer data into Malignant and Benign using multiple SSL techniques.</p></div><div class="section" title="Machine learning mapping"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec62"/>Machine learning mapping</h3></div></div></div><p>To <a id="id837" class="indexterm"/>illustrate the techniques learned in this chapter so far, we will use SSL to classify. Whereas the dataset contains labels for all the examples, in order to treat this as a problem where we can apply SSL, we will consider a fraction of the data to be unlabeled. In fact, we <a id="id838" class="indexterm"/>run multiple experiments using different fractions of unlabeled data for comparison. The different base learners used are classification algorithms familiar to us from previous chapters. </p></div><div class="section" title="Data collection"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec63"/>Data collection</h3></div></div></div><p>This <a id="id839" class="indexterm"/>dataset was collected by the University of Wisconsin Hospitals, Madison. The dataset is available in Weka AARF format. The data is not partitioned into training, validation and test.</p><div class="section" title="Data quality analysis"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec64"/>Data quality analysis</h4></div></div></div><p>The <a id="id840" class="indexterm"/>examples in the data contain no unique identifier. There are 16 examples for which the Bare Nuclei attribute has missing values. The target Class is the only categorical attribute and has two values. All other attributes are continuous and in the range [1, 10].</p></div></div><div class="section" title="Data sampling and transformation"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec65"/>Data sampling and transformation</h3></div></div></div><p>In the <a id="id841" class="indexterm"/>experiments, we <a id="id842" class="indexterm"/>present results for 10-fold cross-validation. For comparison, four runs were performed each using a different fraction of labeled data—10%, 20%, 30%, and 40%.</p><p>A numeric sample code number was added to each example as a unique identifier.  The categorical values Malignant and Benign, for the class attribute, were replaced by the numeric values 4 and 2 respectively.</p></div><div class="section" title="Datasets and analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec66"/>Datasets and analysis</h3></div></div></div><p>The Breast <a id="id843" class="indexterm"/>Cancer Dataset Wisconsin (Original) is available from the UCI Machine Learning Repository at: <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)">https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)</a>.</p><p>This <a id="id844" class="indexterm"/>database was originally obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg.  The dataset was created by Dr. Wolberg for the diagnosis and prognosis of breast tumors.  The data is based exclusively on measurements involving the <span class="strong"><strong>Fine Needle Aspirate</strong></span> (<span class="strong"><strong>FNA</strong></span>) test. In this test, fluid from a breast mass is extracted using a small-gauge <a id="id845" class="indexterm"/>needle and then visually examined under a microscope.</p><p>A total of 699 instances <a id="id846" class="indexterm"/>with nine numeric attributes and a binary class (malignant/benign) constitute the dataset. The percentage <a id="id847" class="indexterm"/>of missing values is 0.2%. There are 65.5% malignant and 34.5% benign cases in the dataset. The feature names and range of valid values are listed in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Num.</p>
</th><th style="text-align: left" valign="bottom">
<p>Feature Name</p>
</th><th style="text-align: left" valign="bottom">
<p>Domain</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>Sample code number</p>
</td><td style="text-align: left" valign="top">
<p>id number</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>Clump Thickness </p>
</td><td style="text-align: left" valign="top">
<p>1 - 10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>Uniformity of Cell Size</p>
</td><td style="text-align: left" valign="top">
<p>1 - 10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>Uniformity of Cell Shape</p>
</td><td style="text-align: left" valign="top">
<p>1 - 10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>Marginal Adhesion</p>
</td><td style="text-align: left" valign="top">
<p>1 - 10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>Single Epithelial Cell Size</p>
</td><td style="text-align: left" valign="top">
<p>1 - 10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>Bare Nuclei</p>
</td><td style="text-align: left" valign="top">
<p>1 - 10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>Bland Chromatin </p>
</td><td style="text-align: left" valign="top">
<p>1 - 10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>Normal Nucleoli </p>
</td><td style="text-align: left" valign="top">
<p>1 - 10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>Mitoses </p>
</td><td style="text-align: left" valign="top">
<p>1 - 10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>11</p>
</td><td style="text-align: left" valign="top">
<p>Class</p>
</td><td style="text-align: left" valign="top">
<p>2 for benign, 4 for malignant</p>
</td></tr></tbody></table></div><div class="section" title="Feature analysis results"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec124"/>Feature analysis results</h4></div></div></div><p>Summary <a id="id848" class="indexterm"/>statistics by feature appear in Table 1.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>Clump Thickness</p>
</th><th style="text-align: left" valign="bottom">
<p>Cell Size Unifor-mity</p>
</th><th style="text-align: left" valign="bottom">
<p>Cell Shape Unifor-mity</p>
</th><th style="text-align: left" valign="bottom">
<p>Marginal Adhesion</p>
</th><th style="text-align: left" valign="bottom">
<p>Single Epi Cell Size</p>
</th><th style="text-align: left" valign="bottom">
<p>Bare Nuclei</p>
</th><th style="text-align: left" valign="bottom">
<p>Bland Chromatin</p>
</th><th style="text-align: left" valign="bottom">
<p>Normal Nucleoli</p>
</th><th style="text-align: left" valign="bottom">
<p>Mitoses</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>mean</p>
</td><td style="text-align: left" valign="top">
<p>4.418</p>
</td><td style="text-align: left" valign="top">
<p>3.134</p>
</td><td style="text-align: left" valign="top">
<p>3.207</p>
</td><td style="text-align: left" valign="top">
<p>2.807</p>
</td><td style="text-align: left" valign="top">
<p>3.216</p>
</td><td style="text-align: left" valign="top">
<p>3.545</p>
</td><td style="text-align: left" valign="top">
<p>3.438</p>
</td><td style="text-align: left" valign="top">
<p>2.867</p>
</td><td style="text-align: left" valign="top">
<p>1.589</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>std</p>
</td><td style="text-align: left" valign="top">
<p>2.816</p>
</td><td style="text-align: left" valign="top">
<p>3.051</p>
</td><td style="text-align: left" valign="top">
<p>2.972</p>
</td><td style="text-align: left" valign="top">
<p>2.855</p>
</td><td style="text-align: left" valign="top">
<p>2.214</p>
</td><td style="text-align: left" valign="top">
<p>3.644</p>
</td><td style="text-align: left" valign="top">
<p>2.438</p>
</td><td style="text-align: left" valign="top">
<p>3.054</p>
</td><td style="text-align: left" valign="top">
<p>1.715</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>min</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>25%</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>50%</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>75%</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>max</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 1. Features summary </em></span></p></blockquote></div></div></div><div class="section" title="Experiments and results"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec67"/>Experiments and results</h3></div></div></div><p>Two SSL <a id="id849" class="indexterm"/>algorithms were <a id="id850" class="indexterm"/>selected for the experiments—self-training and co-training. In addition, four classification methods were chosen as base learners—Naïve Bayes, C4.5, K-NN, and SMO. Further, each experiment was run using four different partitions of labeled and unlabeled data (10%, 20%, 30%, and 40% labeled).</p><p>The hyper-parameters for the algorithms and base classifiers are given in Table 2.  You can see the accuracy across the different runs corresponding to four partitions of labeled and unlabeled data for the two SSL algorithms. </p><p>Finally, we give the performance results for each experiment for the case of 40% labeled.  Performance metrics provided are Accuracy and the Kappa statistic with standard deviations.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Method</p>
</th><th style="text-align: left" valign="bottom">
<p>Parameters</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Self-Training</p>
</td><td style="text-align: left" valign="top">
<p>MAX_ITER = 40</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Co-Training</p>
</td><td style="text-align: left" valign="top">
<p>MAX_ITER = 40, Initial Unlabeled Pool=75</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>KNN</p>
</td><td style="text-align: left" valign="top">
<p>K = 3, Euclidean distance</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>C4.5</p>
</td><td style="text-align: left" valign="top">
<p>pruned tree, confidence = 0.25, 2 examples per leaf</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>NB</p>
</td><td style="text-align: left" valign="top">
<p>No parameters specified</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>SMO</p>
</td><td style="text-align: left" valign="top">
<p>C = 1.0, Tolerance Parameter = 0.001, Epsilon= 1.0E-12, Kernel Type = Polynomial, Polynomial degree = 1, Fit logistic models = true</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 2. Base classifier hyper-parameters for self-training and co-training </em></span></p></blockquote></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>SSL Algorithm</p>
</td><td style="text-align: left" valign="top">
<p>10%</p>
</td><td style="text-align: left" valign="top">
<p>20%</p>
</td><td style="text-align: left" valign="top">
<p>30%</p>
</td><td style="text-align: left" valign="top">
<p>40%</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Self-Training C 4.5</p>
</td><td style="text-align: left" valign="top">
<p>0.9</p>
</td><td style="text-align: left" valign="top">
<p>0.93</p>
</td><td style="text-align: left" valign="top">
<p>0.94</p>
</td><td style="text-align: left" valign="top">
<p>0.947</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Co-Training SMO</p>
</td><td style="text-align: left" valign="top">
<p>0.959</p>
</td><td style="text-align: left" valign="top">
<p>0.949</p>
</td><td style="text-align: left" valign="top">
<p>0.962</p>
</td><td style="text-align: left" valign="top">
<p>0.959</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 3. Model accuracy for samples with varying fraction of labeled examples </em></span></p></blockquote></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>Accuracy (no unlabeled)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>C4.5 10-fold CV</p>
</td><td style="text-align: left" valign="top">
<p>0.947</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>SMO 10 fold CV</p>
</td><td style="text-align: left" valign="top">
<p>0.967</p>
</td></tr></tbody></table></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td colspan="2" style="text-align: center" valign="top">
<p>10 fold CV Wisconsin 40% Labeled Data</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>Self-Training (kNN)</p>
</td><td style="text-align: left" valign="top">
<p>Accuracy</p>
</td><td style="text-align: left" valign="top">
<p>0.9623 (1)</p>
</td><td style="text-align: left" valign="top">
<p>Kappa</p>
</td><td style="text-align: left" valign="top">
<p>0.9170 (2)</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0329</p>
</td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0714</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Self-Training (C45)</p>
</td><td style="text-align: left" valign="top">
<p>Accuracy</p>
</td><td style="text-align: left" valign="top">
<p>0.9606 (3)</p>
</td><td style="text-align: left" valign="top">
<p>Kappa</p>
</td><td style="text-align: left" valign="top">
<p>0.9144</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0241</p>
</td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0511</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Self-Training (NB)</p>
</td><td style="text-align: left" valign="top">
<p>Accuracy</p>
</td><td style="text-align: left" valign="top">
<p>0.9547</p>
</td><td style="text-align: left" valign="top">
<p>Kappa</p>
</td><td style="text-align: left" valign="top">
<p>0.9036</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0252</p>
</td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0533</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Self-Training (SMO)</p>
</td><td style="text-align: left" valign="top">
<p>Accuracy</p>
</td><td style="text-align: left" valign="top">
<p>0.9547</p>
</td><td style="text-align: left" valign="top">
<p>Kappa</p>
</td><td style="text-align: left" valign="top">
<p>0.9035</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0208</p>
</td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0435</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Co-Training (NN)</p>
</td><td style="text-align: left" valign="top">
<p>Accuracy</p>
</td><td style="text-align: left" valign="top">
<p>0.9492</p>
</td><td style="text-align: left" valign="top">
<p>Kappa</p>
</td><td style="text-align: left" valign="top">
<p>0.8869</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0403</p>
</td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0893</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Co-Training (C45)</p>
</td><td style="text-align: left" valign="top">
<p>Accuracy</p>
</td><td style="text-align: left" valign="top">
<p>0.9417</p>
</td><td style="text-align: left" valign="top">
<p>Kappa</p>
</td><td style="text-align: left" valign="top">
<p>0.8733</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0230</p>
</td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0480</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Co-Training (NB)</p>
</td><td style="text-align: left" valign="top">
<p>Accuracy</p>
</td><td style="text-align: left" valign="top">
<p>0.9622 (2)</p>
</td><td style="text-align: left" valign="top">
<p>Kappa</p>
</td><td style="text-align: left" valign="top">
<p>0.9193 (1)</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0290</p>
</td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0614</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Co-Training (SMO)</p>
</td><td style="text-align: left" valign="top">
<p>Accuracy</p>
</td><td style="text-align: left" valign="top">
<p>0.9592</p>
</td><td style="text-align: left" valign="top">
<p>Kappa</p>
</td><td style="text-align: left" valign="top">
<p>0.9128 (3)</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0274</p>
</td><td style="text-align: left" valign="top">
<p>Std Dev</p>
</td><td style="text-align: left" valign="top">
<p>0.0580</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 4. Model performance comparison using 40% labeled examples. The top ranking performers in each category are shown in parentheses. </em></span></p></blockquote></div><div class="section" title="Analysis of semi-supervised learning"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec125"/>Analysis of semi-supervised learning</h4></div></div></div><p>With 40% of <a id="id851" class="indexterm"/>labeled data, semi-supervised self-training with C4.5 achieves the same result as 100% of labeled data with just C4.5. This shows the strength of semi-supervised learning when the data is sparsely labeled.</p><p>SMO with polynomial kernel, with 30-40% data comes close to the 100% data but not as good as C4.5.</p><p>Self-training and co-training with four classifiers on 40% labeled training data shows</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">KNN as base classifier and self-training has the highest accuracy (0.9623) which indicates the non-linear boundary of the data. Co-training with Naïve Bayes comes very close.</li><li class="listitem" style="list-style-type: disc">Self-training with classifiers such as linear Naïve Bayes, non-linear C4.5 and highly non-linear <a id="id852" class="indexterm"/>KNN shows steady improvements in accuracy: 0.9547, 0.9606, 0.9623, which again shows that using self-training but choosing the right underlying classifier for the problem is very important.</li><li class="listitem" style="list-style-type: disc">Co-training with Naive Bayes has highest Kappa statistic (0.9193) and almost similar accuracy as KNN with self-training. The independence relationship between features—hence breaking the feature sets into orthogonal feature sets and using them for classifiers—improves the learning.</li></ul></div></div></div></div></div></div>
<div class="section" title="Active learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec36"/>Active learning</h1></div></div></div><p>Although <a id="id853" class="indexterm"/>active learning has many similarities with semi-supervised learning, it has its own distinctive approach to modeling with datasets containing labeled and unlabeled data. It has roots in the basic human psychology that asking more questions often tends to solve problems.</p><p>The main idea behind active learning is that if the learner gets to pick the instances to learn from rather than being handed labeled data, it can learn more effectively with less data (<span class="emphasis"><em>Reference</em></span> [6]). With very small amount of labeled data, it can carefully pick instances from unlabeled data to get label information and use that to iteratively improve learning. This basic approach of querying for unlabeled data to get labels from a so-called oracle—an expert in the domain—distinguishes active learning from semi-supervised or passive learning. The following figure illustrates the difference and the iterative process involved:</p><div class="mediaobject"><img src="graphics/B05137_04_079.jpg" alt="Active learning"/><div class="caption"><p>Figure 7. Active Machine Learning process contrasted with Supervised and Semi-Supervised Learning process.</p></div></div><div class="section" title="Representation and notation"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec52"/>Representation and notation</h2></div></div></div><p>The dataset <span class="emphasis"><em>D</em></span>, which <a id="id854" class="indexterm"/>represents all the data instances and their labels, is given by <span class="emphasis"><em>D</em></span> = {(<span class="strong"><strong>x</strong></span><sub>1</sub>, <span class="emphasis"><em>y</em></span><sub>2</sub>), (<span class="strong"><strong>x</strong></span><sub>2</sub>, <span class="emphasis"><em>y</em></span><sub>2</sub>), … (<span class="strong"><strong>x</strong></span><sub>n</sub>, <span class="emphasis"><em>y</em></span><sub>n</sub>)} where <span class="inlinemediaobject"><img src="graphics/B05137_04_012.jpg" alt="Representation and notation"/></span> are the <a id="id855" class="indexterm"/>individual instances of data and {<span class="emphasis"><em>y</em></span><sub>1</sub>, <span class="emphasis"><em>y</em></span><sub>2</sub>, … <span class="emphasis"><em>y</em></span><sub>n</sub>} is the set of associated labels. <span class="emphasis"><em>D</em></span> is composed of two sets <span class="emphasis"><em>U</em></span>, labeled data and <span class="emphasis"><em>L</em></span>, unlabeled data. <span class="strong"><strong>x</strong></span> is the set {<span class="strong"><strong>x</strong></span><sub>1</sub>, <span class="strong"><strong>x</strong></span><sub>2</sub>,… <span class="strong"><strong>x</strong></span><sub>n</sub>} of data instances without labels.</p><p>The dataset <span class="inlinemediaobject"><img src="graphics/B05137_04_015.jpg" alt="Representation and notation"/></span> consists of all labeled data with known outcomes {<span class="emphasis"><em>y</em></span><sub>1</sub>, <span class="emphasis"><em>y</em></span><sub>2</sub>, … <span class="emphasis"><em>y</em></span><sub>l</sub>} whereas <span class="inlinemediaobject"><img src="graphics/B05137_04_017.jpg" alt="Representation and notation"/></span> is the dataset where the outcomes are not known. As before, |<span class="emphasis"><em>U</em></span>|&gt;&gt; |<span class="emphasis"><em>L</em></span>|.</p></div><div class="section" title="Active learning scenarios"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec53"/>Active learning scenarios</h2></div></div></div><p>Active <a id="id856" class="indexterm"/>learning scenarios can be broadly classified as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Stream-based active learning</strong></span>: In this approach, an instance or example is picked only from the unlabeled dataset and a decision is made whether to ignore the data or pass it on to the oracle to get its label (<span class="emphasis"><em>Referee</em></span>[10,11]).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Pool-based active learning</strong></span>: In this approach, the instances are queried from the unlabeled dataset and then ranked on the basis of informativeness and a set from these is sent to the Oracle to get the labels (<span class="emphasis"><em>Referees</em></span> [12]). </li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Query synthesis</strong></span>: In this method, the learner has only information about input space (features) and synthesizes queries from the unlabeled set for the membership. This <a id="id857" class="indexterm"/>is not used in practical applications, as most often it doesn't consider the data generating distribution and hence often the queries are arbitrary or meaningless.</li></ul></div></div><div class="section" title="Active learning approaches"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec54"/>Active learning approaches</h2></div></div></div><p>Regardless <a id="id858" class="indexterm"/>of the scenario involved, every active learning approach includes selecting a query strategy or sampling method which establishes the mechanism for picking the queries in each iteration.  Each method reveals a distinct way of seeking out unlabeled examples with the best information content for improving the learning process.  In the following subsections, we describe the major query strategy frameworks, how they work, their merits and limitations, and the different strategies within each framework.</p><div class="section" title="Uncertainty sampling"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec69"/>Uncertainty sampling</h3></div></div></div><p>The <a id="id859" class="indexterm"/>key idea behind this form of sampling is to select <a id="id860" class="indexterm"/>instances from the unlabeled pool that the current model is most uncertain about. The learner can then avoid the instances the model is more certain or confident in classifying (<span class="emphasis"><em>Reerences</em></span> [8]). </p><p>Probabilistic based models (Naïve Bayes, Logistic Regression, and so on) are the most natural choices for such methods as they give confidence measures on the given model, say <span class="emphasis"><em>θ</em></span> for the data <span class="strong"><strong>x</strong></span>, for a class <span class="emphasis"><em>y</em></span><sub>i</sub><span class="emphasis"><em> i</em></span> ϵ classes, and the probability <span class="inlinemediaobject"><img src="graphics/B05137_04_086.jpg" alt="Uncertainty sampling"/></span> as the posterior probability.</p><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec126"/>How does it work?</h4></div></div></div><p>The general <a id="id861" class="indexterm"/>process for all uncertainty-based algorithms is outlined as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Initialize the data as labeled, <span class="inlinemediaobject"><img src="graphics/B05137_04_015.jpg" alt="How does it work?"/></span> and unlabeled, <span class="inlinemediaobject"><img src="graphics/B05137_04_017.jpg" alt="How does it work?"/></span>.
</li><li class="listitem">While there is still unlabeled data:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Train the classifier model <span class="inlinemediaobject"><img src="graphics/B05137_04_021.jpg" alt="How does it work?"/></span> with the labeled data <span class="emphasis"><em>L</em></span>.
</li><li class="listitem">Apply the classifier model <span class="emphasis"><em>f</em></span> on the unlabeled data <span class="emphasis"><em>U</em></span> to assess informativeness <span class="emphasis"><em>J</em></span> using one of the sampling mechanisms (see next section)</li><li class="listitem">Choose <span class="emphasis"><em>k</em></span> most informative data from <span class="emphasis"><em>U</em></span> as set <span class="emphasis"><em>L</em></span><sub>u</sub> to get labels from the oracle.</li><li class="listitem">Augment the labeled data with the <span class="emphasis"><em>k</em></span> new labeled data points obtained in the previous step: <span class="emphasis"><em>L</em></span> = <span class="emphasis"><em>L </em></span>∪ <span class="emphasis"><em>L</em></span><sub>u</sub>.</li></ol></div></li><li class="listitem">Repeat all the steps under 2.</li></ol></div><p>Some of the <a id="id862" class="indexterm"/>most common query synthesis algorithms to sample the informative instances from the data are given next.</p><div class="section" title="Least confident sampling"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl5sec10"/>Least confident sampling</h5></div></div></div><p>In <a id="id863" class="indexterm"/>this technique, the data instances are sorted based on their confidence in reverse, and the instances most likely to be queried or selected are the ones the model is least confident about. The idea behind this is the least confident ones are the ones near the margin or separating hyperplane and getting their labels will be the best way to learn the boundaries effectively.</p><p>This can be formulated as <span class="inlinemediaobject"><img src="graphics/B05137_04_088.jpg" alt="Least confident sampling"/></span>. </p><p>The disadvantage of this method is that it effectively considers information of the best; information on the rest of the posterior distribution is not used.</p></div><div class="section" title="Smallest margin sampling"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl5sec11"/>Smallest margin sampling</h5></div></div></div><p>This <a id="id864" class="indexterm"/>is margin-based sampling, where the instances with smaller margins have more ambiguity than instances with larger margins. </p><p>This can be formulated as <span class="inlinemediaobject"><img src="graphics/B05137_04_089.jpg" alt="Smallest margin sampling"/></span> where <span class="inlinemediaobject"><img src="graphics/B05137_04_090.jpg" alt="Smallest margin sampling"/></span> and <span class="inlinemediaobject"><img src="graphics/B05137_04_091.jpg" alt="Smallest margin sampling"/></span> are two labels for the instance <span class="strong"><strong>x</strong></span>.</p></div><div class="section" title="Label entropy sampling"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl5sec12"/>Label entropy sampling</h5></div></div></div><p>Entropy, which <a id="id865" class="indexterm"/>is the measure of average information content in the data and is the impurity measure, can be used to sample the instances. This can be formulated as:</p><div class="mediaobject"><img src="graphics/B05137_04_092.jpg" alt="Label entropy sampling"/></div></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec127"/>Advantages and limitations</h4></div></div></div><p>The <a id="id866" class="indexterm"/>advantages and limitations are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Label <a id="id867" class="indexterm"/>entropy sampling is the simplest approach and can work with any probabilistic classifiers—this is the biggest advantage</li><li class="listitem" style="list-style-type: disc">Presence of outliers or wrong feedback can go unnoticed and models can degrade</li></ul></div></div></div></div><div class="section" title="Version space sampling"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec55"/>Version space sampling</h2></div></div></div><p>Hypothesis <span class="emphasis"><em>H</em></span> is <a id="id868" class="indexterm"/>the set of all the particular <a id="id869" class="indexterm"/>models that generalize or explain the training data; for example, all possible sets of weights that separate two linearly separable classes. Version spaces <span class="emphasis"><em>V</em></span> are subsets of hypothesis <span class="emphasis"><em>H</em></span>, which are consistent with the training data as defined by Tom Mitchell (<span class="emphasis"><em>References</em></span> [15]) such that <span class="inlinemediaobject"><img src="graphics/B05137_04_095.jpg" alt="Version space sampling"/></span>. </p><p>The idea behind this sampling is to query instances from the unlabeled dataset that reduce the size of the version space or minimize |<span class="emphasis"><em>V</em></span>|. </p><div class="section" title="Query by disagreement (QBD)"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec70"/>Query by disagreement (QBD)</h3></div></div></div><p>QBD is <a id="id870" class="indexterm"/>one of the earliest <a id="id871" class="indexterm"/>algorithms which works on maintaining a version space <span class="emphasis"><em>V</em></span>—when two hypotheses disagree on the label of new incoming data, that instance is selected for getting labels from the oracle or expert.</p><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec128"/>How does it work? </h4></div></div></div><p>The entire algorithm can be summarized as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Initialize <span class="inlinemediaobject"><img src="graphics/B05137_04_095.jpg" alt="How does it work?"/></span> as the set of all legal hypotheses.
</li><li class="listitem">
Initialize the data as <span class="inlinemediaobject"><img src="graphics/B05137_04_015.jpg" alt="How does it work?"/></span> labeled and <span class="inlinemediaobject"><img src="graphics/B05137_04_017.jpg" alt="How does it work?"/></span> unlabeled.
</li><li class="listitem">While data <span class="strong"><strong>x</strong></span><span class="emphasis"><em><sub>'</sub></em></span> is in <span class="emphasis"><em>U</em></span>:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
If <span class="inlinemediaobject"><img src="graphics/B05137_04_099.jpg" alt="How does it work?"/></span> for any  <span class="emphasis"><em>h</em></span><sub>2</sub> ∈ <span class="emphasis"><em>V</em></span>:


<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Query the label of <span class="strong"><strong>x</strong></span><span class="emphasis"><em><sub>'</sub></em></span> and get <span class="emphasis"><em>y<sub>'</sub></em></span>.</li><li class="listitem"><span class="emphasis"><em>V</em></span> = {h: h(<span class="strong"><strong>x</strong></span><sub>'</sub>) = <span class="emphasis"><em>y<sub>'</sub></em></span> for all points.</li></ol></div></li><li class="listitem">Otherwise:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Ignore <span class="strong"><strong>x</strong></span><span class="emphasis"><em><sub>'</sub></em></span>. </li></ol></div></li></ol></div></li></ol></div><div class="section" title="Query by Committee (QBC)"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl5sec13"/>Query by Committee (QBC)</h5></div></div></div><p>Query <a id="id872" class="indexterm"/>by Committee overcomes the limitation of <a id="id873" class="indexterm"/>Query by Disagreement related to maintaining all possible version spaces by creating a committee of classifiers and using their votes as the mechanism to capture the disagreement (<span class="emphasis"><em>References</em></span> [7]). </p></div></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec129"/>How does it work?</h4></div></div></div><p>For this algorithm: </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Initialize the data as <span class="inlinemediaobject"><img src="graphics/B05137_04_015.jpg" alt="How does it work?"/></span> labeled and <span class="inlinemediaobject"><img src="graphics/B05137_04_017.jpg" alt="How does it work?"/></span> unlabeled.
</li><li class="listitem">Train the committee of models <span class="emphasis"><em>C</em></span> = {<span class="emphasis"><em>θ</em></span><sup>1</sup><span class="emphasis"><em>θ</em></span><sup>2</sup>, ... <span class="emphasis"><em>θ</em></span><sup>c</sup>} on the labeled data <span class="emphasis"><em>w</em></span> (see the following).</li><li class="listitem">For all data <span class="strong"><strong>x</strong></span><span class="emphasis"><em><sup>'</sup></em></span> in the <span class="emphasis"><em>U</em></span>:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Vote for predictions on <span class="strong"><strong>x</strong></span><span class="emphasis"><em>'</em></span> as {<span class="inlinemediaobject"><img src="graphics/B05137_04_107.jpg" alt="How does it work?"/></span>.
</li><li class="listitem">Rank the instances based on maximum disagreement (see the following).</li><li class="listitem">Choose <span class="emphasis"><em>k</em></span> most informative data from <span class="emphasis"><em>U</em></span> as set <span class="emphasis"><em>L</em></span><sub>u</sub> to get labels from the oracle.</li><li class="listitem">Augment the labeled data with the <span class="emphasis"><em>k</em></span> new labeled data points <span class="emphasis"><em>L</em></span> = <span class="emphasis"><em>L</em></span> ∪ <span class="emphasis"><em>L</em></span><sub>u</sub>.</li><li class="listitem">Retrain the models {<span class="emphasis"><em>θ</em></span><sub>1</sub>, <span class="emphasis"><em>θ</em></span><sub>2</sub>, ... <span class="emphasis"><em>θ</em></span><sub>c</sub>} with new <span class="emphasis"><em>L</em></span>.</li></ol></div></li></ol></div><p>With the two tasks of training the committee of learners and choosing the disagreement methods, each has various choices.</p><p>Training different models can be either done using different samples from <span class="emphasis"><em>L</em></span> or they can be trained using ensemble methods such as boosting and bagging. </p><p>Vote entropy is <a id="id874" class="indexterm"/>one of the methods chosen as the disagreement metric to rank. The mathematical way of representing it is:</p><div class="mediaobject"><img src="graphics/B05137_04_109.jpg" alt="How does it work?"/></div><p>Here, <span class="emphasis"><em>V(y<sub>i</sub>)</em></span> is number of votes given to the label <span class="emphasis"><em>y</em></span><sub>i</sub> from all possible labels and |<span class="emphasis"><em>C</em></span>| is size of the committee.</p><p>
<span class="strong"><strong>Kullback-Leibler</strong></span> (<span class="strong"><strong>KL</strong></span>) divergence <a id="id875" class="indexterm"/>is an information theoretic measure of divergence between two probability distributions. The disagreement is quantified as the average divergence of each committee's prediction from that of the consensus in the committee <span class="emphasis"><em>C</em></span>:</p><div class="mediaobject"><img src="graphics/B05137_04_114.jpg" alt="How does it work?"/></div></div></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec56"/>Advantages and limitations</h2></div></div></div><p>The <a id="id876" class="indexterm"/>advantages and limitations are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Simplicity <a id="id877" class="indexterm"/>and the fact that it can work with any supervised algorithm gives it a great advantage.</li><li class="listitem" style="list-style-type: disc">There are theoretical guarantees of minimizing errors and generalizing in some conditions. </li><li class="listitem" style="list-style-type: disc">Query by Disagreement suffers from maintaining a large number of valid hypotheses.</li><li class="listitem" style="list-style-type: disc">These methods still suffer from the issue of wrong feedback going unnoticed and models potentially degrading.</li></ul></div></div><div class="section" title="Data distribution sampling"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec57"/>Data distribution sampling</h2></div></div></div><p>The <a id="id878" class="indexterm"/>previous methods selected the best instances from the unlabeled set based either on the uncertainty posed by <a id="id879" class="indexterm"/>the samples on the models or by reducing the hypothesis space size. Neither of these methods worked on what is best for the model itself. The idea behind data distribution sampling is that adding samples that help reduce the errors to the model serves to improve predictions on unseen instances using expected values (<span class="emphasis"><em>References</em></span> [13 and 14]).</p><div class="section" title="How does it work?"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec71"/>How does it work?</h3></div></div></div><p>There <a id="id880" class="indexterm"/>are different ways to find what is the best sample for the given model and here we will describe each one in some detail.</p><div class="section" title="Expected model change"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec130"/>Expected model change</h4></div></div></div><p>The idea behind this is to select examples from the unlabeled set that that will bring maximum change in the model: </p><div class="mediaobject"><img src="graphics/B05137_04_115.jpg" alt="Expected model change"/></div><p>Here, P<sub>θ</sub> (<span class="emphasis"><em>y</em></span>|<span class="strong"><strong>x</strong></span>) = expectation over labels of <span class="strong"><strong>x</strong></span>, <span class="inlinemediaobject"><img src="graphics/B05137_04_117.jpg" alt="Expected model change"/></span> is the sum over unlabeled instances <a id="id881" class="indexterm"/>of the entropy of including <span class="strong"><strong>x</strong></span>
<span class="emphasis"><em>'</em></span> after retraining with <span class="strong"><strong>x</strong></span>.</p></div><div class="section" title="Expected error reduction"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec131"/>Expected error reduction</h4></div></div></div><p>Here, the <a id="id882" class="indexterm"/>approach is to select examples from the unlabeled set that reduce the model's generalized error the most. The generalized error is measured using the unlabeled set with expected labels:</p><div class="mediaobject"><img src="graphics/B05137_04_119.jpg" alt="Expected error reduction"/></div><p>Here, Pθ (<span class="emphasis"><em>y</em></span>|<span class="strong"><strong>x</strong></span>) = expectation over labels of <span class="strong"><strong>x</strong></span>, <span class="inlinemediaobject"><img src="graphics/B05137_04_120.jpg" alt="Expected error reduction"/></span> is the sum over unlabeled instances of the entropy of including x<span class="emphasis"><em><sup>'</sup></em></span> after retraining with <span class="strong"><strong>x</strong></span>.</p><div class="section" title="Variance reduction"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl5sec14"/>Variance reduction</h5></div></div></div><p>The <a id="id883" class="indexterm"/>general equation of estimation on an out-of-sample error in terms of noise-bias-variance is given by the following:</p><div class="mediaobject"><img src="graphics/B05137_04_121.jpg" alt="Variance reduction"/></div><p>Here, <span class="strong"><strong>G</strong></span>(<span class="strong"><strong>x</strong></span>) is the model's prediction given the label <span class="emphasis"><em>y</em></span>. In variance reduction, we select examples from the unlabeled set that most reduce the variance in the model:</p><div class="mediaobject"><img src="graphics/B05137_04_124.jpg" alt="Variance reduction"/></div><p>Here, <span class="emphasis"><em>θ</em></span> + represents the model after it has been retrained with the new point <span class="strong"><strong>x</strong></span>
<span class="emphasis"><em><sup>'</sup></em></span> and its label <span class="emphasis"><em>y<sup>'</sup></em></span>.</p></div><div class="section" title="Density weighted methods"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl5sec15"/>Density weighted methods</h5></div></div></div><p>In <a id="id884" class="indexterm"/>this approach, we select examples from the unlabeled set that have average similarity to the labeled set. </p><p>This can be represented as follows:</p><div class="mediaobject"><img src="graphics/B05137_04_127.jpg" alt="Density weighted methods"/></div><p>Here, <span class="emphasis"><em>sim</em></span>(<span class="strong"><strong>x</strong></span>, <span class="strong"><strong>x</strong></span>
<span class="emphasis"><em><sup>'</sup></em></span>) is the density term or the similarity term where H<sub>θ</sub>(<span class="emphasis"><em>y</em></span>|<span class="strong"><strong>x</strong></span>) is the base utility measure.</p></div></div></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec58"/>Advantages and limitations</h2></div></div></div><p>The <a id="id885" class="indexterm"/>advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The biggest <a id="id886" class="indexterm"/>advantage is that they work directly on the model as an optimization objective rather than implicit or indirect methods described before</li><li class="listitem" style="list-style-type: disc">These methods can work on pool- or stream-based scenarios</li><li class="listitem" style="list-style-type: disc">These methods have some theoretical guarantees on the bounds and generalizations</li><li class="listitem" style="list-style-type: disc">The biggest disadvantage of these methods is computational cost and difficulty in implementation</li></ul></div></div></div>
<div class="section" title="Case study in active learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec37"/>Case study in active learning</h1></div></div></div><p>This <a id="id887" class="indexterm"/>case study uses another well-known publicly available dataset to demonstrate active learning techniques using open source Java libraries. As before, we begin with defining the business problem, what tools and frameworks are used, how the principles of machine learning are realized in the solution, and what the data analysis steps reveal.  Next, we describe the experiments that were conducted, evaluate the performance of the various models, and provide an analysis of the results.</p><div class="section" title="Tools and software"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec59"/>Tools and software</h2></div></div></div><p>For the <a id="id888" class="indexterm"/>experiments in Active Learning, JCLAL was <a id="id889" class="indexterm"/>the tool used. JCLAL is a Java framework for Active Learning, supporting single-label and multi-label learning.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>JCLAL is <a id="id890" class="indexterm"/>open source and is distributed under the GNU general public license: <a class="ulink" href="https://sourceforge.net/p/jclal/git/ci/master/tree/">https://sourceforge.net/p/jclal/git/ci/master/tree/</a>.</p></div></div></div><div class="section" title="Business problem"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec60"/>Business problem</h2></div></div></div><p>The <a id="id891" class="indexterm"/>abalone dataset, which is used in these experiments, contains data on various physical and anatomical characteristics of abalone—commonly known as sea snails. The goal is to predict the number of rings in the shell, which is indicative of the age of the specimen.</p></div><div class="section" title="Machine learning mapping"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec61"/>Machine learning mapping</h2></div></div></div><p>As <a id="id892" class="indexterm"/>we have seen, active learning is characterized by starting with a small set of labeled data accompanied by techniques of querying the unlabeled data such that we incrementally add instances to the labeled set.  This is performed over multiple iterations, a batch at a time.  The number of iterations and batch size are hyper-parameters for these techniques.  The querying strategy and the choice of supervised learning method used to train on the growing number of labeled instances are additional inputs.</p></div><div class="section" title="Data Collection"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec62"/>Data Collection</h2></div></div></div><p>As before, we <a id="id893" class="indexterm"/>will use an existing dataset available from the UCI repository (<a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Abalone">https://archive.ics.uci.edu/ml/datasets/Abalone</a>). The original owners of the database <a id="id894" class="indexterm"/>are the Department of Primary Industry and Fisheries in Tasmania, Australia.</p><p>The data types and descriptions of the attributes accompany the data and are reproduced in <span class="emphasis"><em>Table 5</em></span>. The class attribute, Rings, has 29 distinct classes:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Name</p>
</th><th style="text-align: left" valign="bottom">
<p>Data type</p>
</th><th style="text-align: left" valign="bottom">
<p>Measurement units</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Sex</p>
</td><td style="text-align: left" valign="top">
<p>nominal</p>
</td><td style="text-align: left" valign="top">
<p>M, F, and I (infant)</p>
</td><td style="text-align: left" valign="top">
<p>sex of specimen</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Length</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top">
<p>mm</p>
</td><td style="text-align: left" valign="top">
<p>longest shell measurement</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Diameter</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top">
<p>mm</p>
</td><td style="text-align: left" valign="top">
<p>perpendicular to length</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Height</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top">
<p>mm</p>
</td><td style="text-align: left" valign="top">
<p>with meat in shell</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Whole weight</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top">
<p>grams</p>
</td><td style="text-align: left" valign="top">
<p>whole abalone</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Shucked weight</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top">
<p>grams</p>
</td><td style="text-align: left" valign="top">
<p>weight of meat</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Viscera weight</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top">
<p>grams</p>
</td><td style="text-align: left" valign="top">
<p>gut weight (after bleeding)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Shell weight</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top">
<p>grams</p>
</td><td style="text-align: left" valign="top">
<p>after being dried</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Rings</p>
</td><td style="text-align: left" valign="top">
<p>integer</p>
</td><td style="text-align: left" valign="top">
<p>count</p>
</td><td style="text-align: left" valign="top">
<p>+1.5 gives the age in years</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 5. Abalone dataset features </em></span></p></blockquote></div></div><div class="section" title="Data sampling and transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec63"/>Data sampling and transformation</h2></div></div></div><p>For <a id="id895" class="indexterm"/>this experiment, we treated a randomly <a id="id896" class="indexterm"/>selected 4,155 records as unlabeled and kept the remaining 17 as labeled. There is no transformation of the data. </p></div><div class="section" title="Feature analysis and dimensionality reduction"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec64"/>Feature analysis and dimensionality reduction</h2></div></div></div><p>With <a id="id897" class="indexterm"/>only eight features, there is no need <a id="id898" class="indexterm"/>for dimensionality reduction. The dataset comes with some statistics on the features, reproduced in <span class="emphasis"><em>Table 6</em></span>:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="bottom">
<p><span class="strong"><strong>Length</strong></span></p>
</td><td style="text-align: left" valign="bottom">
<p><span class="strong"><strong>Diameter</strong></span></p>
</td><td style="text-align: left" valign="bottom">
<p><span class="strong"><strong>Height</strong></span></p>
</td><td style="text-align: left" valign="bottom">
<p><span class="strong"><strong>Whole</strong></span></p>
</td><td style="text-align: left" valign="bottom">
<p><span class="strong"><strong>Shucked</strong></span></p>
</td><td style="text-align: left" valign="bottom">
<p><span class="strong"><strong>Viscera</strong></span></p>
</td><td style="text-align: left" valign="bottom">
<p><span class="strong"><strong>Shell</strong></span></p>
</td><td style="text-align: left" valign="bottom">
<p><span class="strong"><strong>Rings</strong></span></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Min</p>
</td><td style="text-align: left" valign="top">
<p>0.075</p>
</td><td style="text-align: left" valign="top">
<p>0.055</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.002</p>
</td><td style="text-align: left" valign="top">
<p>0.001</p>
</td><td style="text-align: left" valign="top">
<p>0.001</p>
</td><td style="text-align: left" valign="top">
<p>0.002</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Max</p>
</td><td style="text-align: left" valign="top">
<p>0.815</p>
</td><td style="text-align: left" valign="top">
<p>0.65</p>
</td><td style="text-align: left" valign="top">
<p>1.13</p>
</td><td style="text-align: left" valign="top">
<p>2.826</p>
</td><td style="text-align: left" valign="top">
<p>1.488</p>
</td><td style="text-align: left" valign="top">
<p>0.76</p>
</td><td style="text-align: left" valign="top">
<p>1.005</p>
</td><td style="text-align: left" valign="top">
<p>29</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Mean</p>
</td><td style="text-align: left" valign="top">
<p>0.524</p>
</td><td style="text-align: left" valign="top">
<p>0.408</p>
</td><td style="text-align: left" valign="top">
<p>0.14</p>
</td><td style="text-align: left" valign="top">
<p>0.829</p>
</td><td style="text-align: left" valign="top">
<p>0.359</p>
</td><td style="text-align: left" valign="top">
<p>0.181</p>
</td><td style="text-align: left" valign="top">
<p>0.239</p>
</td><td style="text-align: left" valign="top">
<p>9.934</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>SD</p>
</td><td style="text-align: left" valign="top">
<p>0.12</p>
</td><td style="text-align: left" valign="top">
<p>0.099</p>
</td><td style="text-align: left" valign="top">
<p>0.042</p>
</td><td style="text-align: left" valign="top">
<p>0.49</p>
</td><td style="text-align: left" valign="top">
<p>0.222</p>
</td><td style="text-align: left" valign="top">
<p>0.11</p>
</td><td style="text-align: left" valign="top">
<p>0.139</p>
</td><td style="text-align: left" valign="top">
<p>3.224</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Correl</p>
</td><td style="text-align: left" valign="top">
<p>0.557</p>
</td><td style="text-align: left" valign="top">
<p>0.575</p>
</td><td style="text-align: left" valign="top">
<p>0.557</p>
</td><td style="text-align: left" valign="top">
<p>0.54</p>
</td><td style="text-align: left" valign="top">
<p>0.421</p>
</td><td style="text-align: left" valign="top">
<p>0.504</p>
</td><td style="text-align: left" valign="top">
<p>0.628</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 6. Summary statistics by feature </em></span></p></blockquote></div></div><div class="section" title="Models, results, and evaluation"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec65"/>Models, results, and evaluation</h2></div></div></div><p>We <a id="id899" class="indexterm"/>conducted two sets of experiments.  The first <a id="id900" class="indexterm"/>used pool-based scenarios and the second, stream-based.  In each set, we used entropy sampling, least confident sampling, margin sampling, and <a id="id901" class="indexterm"/>vote entropy sampling.  The classifiers used were Naïve Bayes, Logistic Regression, and J48 (implementation of C4.5). For every experiment, 100 iterations were run, with batch sizes of 1 and 10. In <span class="emphasis"><em>Table 7</em></span>, we present a subset of these results, specifically, pool-based and stream-based scenarios for each sampling method using Naïve Bayes, Simple Logistic, and C4.5 classifiers with a batch size of 10.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>The full set of results can be seen at <a class="ulink" href="https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter4">https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter4</a>.</p></div></div><p>The JCLAL library requires an XML configuration file to specify which scenario to use, the query strategy selected, batch size, max iterations, and base classifier.  The following is an example configuration:</p><div class="informalexample"><pre class="programlisting">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;
&lt;experiment&gt;
    &lt;process evaluation-method-type="net.sf.jclal.evaluation.method.RealScenario"&gt;
        &lt;file-labeled&gt;datasets/abalone-labeled.arff&lt;/file-labeled&gt;
        &lt;file-unlabeled&gt;datasets/abalone-unlabeled.arff&lt;/file-unlabeled&gt;    
        &lt;algorithm type="net.sf.jclal.activelearning.algorithm.ClassicalALAlgorithm"&gt;
      &lt;stop-criterion type="net.sf.jclal.activelearning.stopcriteria.MaxIteration"&gt;
              &lt;max-iteration&gt;10&lt;/max-iteration&gt;	
      &lt;/stop-criterion&gt;
      &lt;stop-criterion type="net.sf.jclal.activelearning.stopcriteria.UnlabeledSetEmpty"/&gt;
            &lt;listener type="net.sf.jclal.listener.RealScenarioListener"&gt;
                &lt;informative-instances&gt;reports/real-scenario-informative-data.txt&lt;/informative-instances&gt;
            &lt;/listener&gt;
            &lt;scenario type="net.sf.jclal.activelearning.scenario.PoolBasedSamplingScenario"&gt;
                &lt;batch-mode type="net.sf.jclal.activelearning.batchmode.QBestBatchMode"&gt;
                    &lt;batch-size&gt;1&lt;/batch-size&gt;
                &lt;/batch-mode&gt;
                &lt;oracle type="net.sf.jclal.activelearning.oracle.ConsoleHumanOracle"/&gt;
               &lt;query-strategy type="net.sf.jclal.activelearning.singlelabel.querystrategy.EntropySamplingQueryStrategy"&gt;
                    &lt;wrapper-classifier type="net.sf.jclal.classifier.WekaClassifier"&gt;
                        &lt;classifier type="weka.classifiers.bayes.NaiveBayes"/&gt;
                    &lt;/wrapper-classifier&gt;
                &lt;/query-strategy&gt;
            &lt;/scenario&gt;
        &lt;/algorithm&gt;
    &lt;/process&gt;
&lt;/experiment&gt;</pre></div><p>The tools itself is invoked via the following: </p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -jar jclal-&lt;version&gt;.jar -cfg &lt;config-file&gt;</strong></span>
</pre></div><div class="section" title="Pool-based scenarios"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec72"/>Pool-based scenarios</h3></div></div></div><p>In <a id="id902" class="indexterm"/>the following three tables, we compare results using pool-based scenarios when using Naïve Bayes, Simple Logistic, and C4.5 classifiers.</p><p>
<span class="strong"><strong>Naïve Bayes:</strong></span>
</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Experiment</p>
</th><th style="text-align: left" valign="bottom">
<p>Area Under ROC</p>
</th><th style="text-align: left" valign="bottom">
<p>F Measure</p>
</th><th style="text-align: left" valign="bottom">
<p>False Positive Rate</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Recall</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>PoolBased-EntropySampling-NaiveBayes-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6021</p>
</td><td style="text-align: left" valign="top">
<p>0.1032</p>
</td><td style="text-align: left" valign="top">
<p>0.0556(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1805</p>
</td><td style="text-align: left" valign="top">
<p>0.1304</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PoolBased-KLDivergence-NaiveBayes-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6639(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1441(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.0563</p>
</td><td style="text-align: left" valign="top">
<p>0.1765</p>
</td><td style="text-align: left" valign="top">
<p>0.1504</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PoolBased-LeastConfidentSampling-NaiveBayes-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6406</p>
</td><td style="text-align: left" valign="top">
<p>0.1300</p>
</td><td style="text-align: left" valign="top">
<p>0.0827</p>
</td><td style="text-align: left" valign="top">
<p>0.1835(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1810(1)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PoolBased-VoteEntropy-NaiveBayes-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6639(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1441(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.0563</p>
</td><td style="text-align: left" valign="top">
<p>0.1765</p>
</td><td style="text-align: left" valign="top">
<p>0.1504</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 7. Performance of pool-based scenario using Naïve Bayes classifier </em></span></p></blockquote></div><p>
<span class="strong"><strong>Logistic Regression</strong></span>:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Experiment</p>
</th><th style="text-align: left" valign="bottom">
<p>Area Under ROC</p>
</th><th style="text-align: left" valign="bottom">
<p>F Measure</p>
</th><th style="text-align: left" valign="bottom">
<p>False Positive Rate</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Recall</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>PoolBased-EntropySampling-SimpleLogistic-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6831</p>
</td><td style="text-align: left" valign="top">
<p>0.1571</p>
</td><td style="text-align: left" valign="top">
<p>0.1157</p>
</td><td style="text-align: left" valign="top">
<p>0.1651</p>
</td><td style="text-align: left" valign="top">
<p>0.2185(1)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PoolBased-KLDivergence-SimpleLogistic-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.7175(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1616</p>
</td><td style="text-align: left" valign="top">
<p>0.1049</p>
</td><td style="text-align: left" valign="top">
<p>0.2117(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.2065</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PoolBased-LeastConfidentSampling-SimpleLogistic-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6629</p>
</td><td style="text-align: left" valign="top">
<p>0.1392</p>
</td><td style="text-align: left" valign="top">
<p>0.1181(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1751</p>
</td><td style="text-align: left" valign="top">
<p>0.1961</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PoolBased-VoteEntropy-SimpleLogistic-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6959</p>
</td><td style="text-align: left" valign="top">
<p>0.1634(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.0895</p>
</td><td style="text-align: left" valign="top">
<p>0.2307</p>
</td><td style="text-align: left" valign="top">
<p>0.1880</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 8. Performance of pool-based scenario using Logistic Regression classifier </em></span></p></blockquote></div><p>
<span class="strong"><strong>C4.5</strong></span>:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Experiment</p>
</th><th style="text-align: left" valign="bottom">
<p>Area Under ROC</p>
</th><th style="text-align: left" valign="bottom">
<p>F Measure</p>
</th><th style="text-align: left" valign="bottom">
<p>False Positive Rate</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Recall</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>PoolBased-EntropySampling-J48-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6730(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.3286(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.0737</p>
</td><td style="text-align: left" valign="top">
<p>0.3432(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.32780(1)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PoolBased-KLDivergence-J48-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6686</p>
</td><td style="text-align: left" valign="top">
<p>0.2979</p>
</td><td style="text-align: left" valign="top">
<p>0.0705(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.3153</p>
</td><td style="text-align: left" valign="top">
<p>0.2955</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PoolBased-LeastConfidentSampling-J48-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6591</p>
</td><td style="text-align: left" valign="top">
<p>0.3094</p>
</td><td style="text-align: left" valign="top">
<p>0.0843</p>
</td><td style="text-align: left" valign="top">
<p>0.3124</p>
</td><td style="text-align: left" valign="top">
<p>0.3227</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PoolBased-VoteEntropy-J48-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6686</p>
</td><td style="text-align: left" valign="top">
<p>0.2979</p>
</td><td style="text-align: left" valign="top">
<p>0.0706</p>
</td><td style="text-align: left" valign="top">
<p>0.3153</p>
</td><td style="text-align: left" valign="top">
<p>0.2955</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 9. Performance of pool-based scenario using C4.5 classifier </em></span></p></blockquote></div></div><div class="section" title="Stream-based scenarios"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec73"/>Stream-based scenarios</h3></div></div></div><p>In the <a id="id903" class="indexterm"/>following three tables, we have results for experiments on stream-based scenarios using Naïve Bayes, Logistic Regression, and C4.5 classifiers with four different sampling methods.</p><p>
<span class="strong"><strong>Naïve Bayes</strong></span>:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Experiment</p>
</th><th style="text-align: left" valign="bottom">
<p>Area Under ROC</p>
</th><th style="text-align: left" valign="bottom">
<p>F Measure</p>
</th><th style="text-align: left" valign="bottom">
<p>False Positive Rate</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Recall</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>StreamBased-EntropySampling-NaiveBayes-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6673(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1432(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.0563</p>
</td><td style="text-align: left" valign="top">
<p>0.1842(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1480</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>StreamBased-LeastConfidentSampling-NaiveBayes-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.5585</p>
</td><td style="text-align: left" valign="top">
<p>0.0923</p>
</td><td style="text-align: left" valign="top">
<p>0.1415</p>
</td><td style="text-align: left" valign="top">
<p>0.1610</p>
</td><td style="text-align: left" valign="top">
<p>0.1807(1)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>StreamBased-MarginSampling-NaiveBayes-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6736(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1282</p>
</td><td style="text-align: left" valign="top">
<p>0.0548(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1806</p>
</td><td style="text-align: left" valign="top">
<p>0.1475</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>StreamBased-VoteEntropyQuery-NaiveBayes-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.5585</p>
</td><td style="text-align: left" valign="top">
<p>0.0923</p>
</td><td style="text-align: left" valign="top">
<p>0.1415</p>
</td><td style="text-align: left" valign="top">
<p>0.1610</p>
</td><td style="text-align: left" valign="top">
<p>0.1807(1)</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 10. Performance of stream-based scenario using Naïve Bayes classifier </em></span></p></blockquote></div><p>
<span class="strong"><strong>Logistic Regression</strong></span>:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Experiment</p>
</th><th style="text-align: left" valign="bottom">
<p>Area Under ROC</p>
</th><th style="text-align: left" valign="bottom">
<p>F Measure</p>
</th><th style="text-align: left" valign="bottom">
<p>False Positive Rate</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Recall</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>StreamBased-EntropySampling-SimpleLogistic-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.7343(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1994(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.0871</p>
</td><td style="text-align: left" valign="top">
<p>0.2154</p>
</td><td style="text-align: left" valign="top">
<p>0.2185(1)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>StreamBased-LeastConfidentSampling-SimpleLogistic-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.7068</p>
</td><td style="text-align: left" valign="top">
<p>0.1750</p>
</td><td style="text-align: left" valign="top">
<p>0.0906</p>
</td><td style="text-align: left" valign="top">
<p>0.2324(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.2019</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>StreamBased-MarginSampling-SimpleLogistic-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.7311</p>
</td><td style="text-align: left" valign="top">
<p>0.1994(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.0861</p>
</td><td style="text-align: left" valign="top">
<p>0.2177</p>
</td><td style="text-align: left" valign="top">
<p>0.214</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>StreamBased-VoteEntropy-SimpleLogistic-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.5506</p>
</td><td style="text-align: left" valign="top">
<p>0.0963</p>
</td><td style="text-align: left" valign="top">
<p>0.0667(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1093</p>
</td><td style="text-align: left" valign="top">
<p>0.1117</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 11. Performance of stream-based scenario using Logistic Regression classifier </em></span></p></blockquote></div><p>
<span class="strong"><strong>C4.5</strong></span>:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Experiment</p>
</th><th style="text-align: left" valign="bottom">
<p>Area Under ROC</p>
</th><th style="text-align: left" valign="bottom">
<p>F Measure</p>
</th><th style="text-align: left" valign="bottom">
<p>False Positive Rate</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Recall</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>StreamBased-EntropySampling-J48-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6648</p>
</td><td style="text-align: left" valign="top">
<p>0.3053</p>
</td><td style="text-align: left" valign="top">
<p>0.0756</p>
</td><td style="text-align: left" valign="top">
<p>0.3189(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.3032</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>StreamBased-LeastConfidentSampling-J48-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6748(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.3064(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.0832</p>
</td><td style="text-align: left" valign="top">
<p>0.3128</p>
</td><td style="text-align: left" valign="top">
<p>0.3189(1)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>StreamBased-MarginSampling-J48-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.6660</p>
</td><td style="text-align: left" valign="top">
<p>0.2998</p>
</td><td style="text-align: left" valign="top">
<p>0.0728(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.3163</p>
</td><td style="text-align: left" valign="top">
<p>0.2967</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>StreamBased-VoteEntropy-J48-b10</p>
</td><td style="text-align: left" valign="top">
<p>0.4966</p>
</td><td style="text-align: left" valign="top">
<p>0.0627</p>
</td><td style="text-align: left" valign="top">
<p>0.0742</p>
</td><td style="text-align: left" valign="top">
<p>0.1096</p>
</td><td style="text-align: left" valign="top">
<p>0.0758</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 12. Performance of stream-based scenario using C4.5 classifier </em></span></p></blockquote></div></div></div><div class="section" title="Analysis of active learning results"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec67"/>Analysis of active learning results</h2></div></div></div><p>It is quite interesting to see that pool-based, Query By Committee—an ensemble method—using KL-Divergence sampling does really well across most classifiers. As discussed <a id="id904" class="indexterm"/>in the section, these methods have been proven to have a theoretical guarantee on reducing the errors by keeping a large hypothesis space, and this experimental result supports that empirically.</p><p>Pool-based, entropy-based sampling using C4.5 as a classifier has the highest Precision, Recall, FPR and F-Measure. Also with stream-based, entropy sampling with C4.5, the metrics are similarly high. With different sampling techniques and C4.5 using pool-based as in KL-Divergence, LeastConfident or vote entropy, the metrics are significantly higher. Thus, this can be attributed more strongly to the underlying classifier C4.5 in finding non-linear patterns.</p><p>The Logistic Regression algorithm performs very well in both stream-based and pool-based when considering AUC. This may be completely due to the fact that LR has a good probabilistic approach in confidence mapping, which is an important factor for giving good AUC scores.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec40"/>Summary</h1></div></div></div><p>After a tour of supervised and unsupervised machine learning techniques and their application to real-world datasets in the previous chapters, this chapter introduces the concepts, techniques, and tools of <span class="strong"><strong>Semi-Supervised Learning</strong></span> (<span class="strong"><strong>SSL</strong></span>) and <span class="strong"><strong>Active Learning</strong></span> (<span class="strong"><strong>AL</strong></span>).</p><p>In SSL, we are given a few labeled examples and many unlabeled ones—the goal is either to simply train on the labeled ones in order to classify the unlabeled ones (transductive SSL), or use the unlabeled and labeled examples to train models to correctly classify new, unseen data (inductive SSL). All techniques in SSL are based on one or more of the assumptions related to semi-supervised smoothness, cluster togetherness, and manifold togetherness.</p><p>Different SSL techniques are applicable to different situations. The simple self-training SSL is straightforward and works with most supervised learning algorithms; when the data is from more than just one domain, the co-training SSL is a suitable method.  When the cluster togetherness assumption holds, the cluster and label SSL technique can be used; a "closeness" measure is exploited by transductive graph label propagation, which can be computationally expensive.  Transductive SVM performs well with linear or non-linear data and we see an example of training a TSVM with a Gaussian kernel on the UCI Breast Cancer dataset using the <code class="literal">JKernelMachines</code> library.  We present experiments comparing SSL models using the graphical Java tool KEEL in the concluding part of the SSL portion of the chapter.</p><p>We introduced active learning (AL) in the second half of the chapter. In this type of learning, various strategies are used to query the unlabeled portion of the dataset in order to present the expert with examples that will prove most effective in learning from the entire dataset. As the expert, or oracle, provides the labels to the selected instances, the learner steadily improves its ability to generalize.  The techniques of AL are characterized by the choice of classifier, or committee of classifiers, and importantly, on the querying strategy chosen.  These strategies include uncertainty sampling, where the instances with the least confidence are queries, version sampling, where a subset of the hypotheses that explain the training data are selected, and data distribution sampling, which involves improving the model by selections that would decrease the generalization error. We presented a case study using the UCI abalone dataset to demonstrate active learning in practice. The tool used here is the JCLAL Java framework for active learning.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec41"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Yarowsky, D (1995). <span class="emphasis"><em>Unsupervised word sense disambiguation rivaling supervised methods</em></span>. Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (pp. 189–196)</li><li class="listitem">Blum, A., and Mitchell, T (1998). <span class="emphasis"><em>Combining labeled and unlabeled data with co-training</em></span>. COLT: Proceedings of the Workshop on Computational Learning Theory.</li><li class="listitem">Demiriz, A., Bennett, K., and Embrechts, M (1999). <span class="emphasis"><em>Semi-supervised clustering using genetic algorithms</em></span>. Proceedings of Artificial Neural Networks in Engineering.</li><li class="listitem">Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux (2006). <span class="emphasis"><em>Label Propagation and Quadratic Criterion</em></span>. In Semi-Supervised Learning, pp. 193-216</li><li class="listitem">T. Joachims (1998). <span class="emphasis"><em>Transductive Inference for Text Classiﬁcation using Support Vector Machines</em></span>, ICML.</li><li class="listitem">B. Settles (2008). <span class="emphasis"><em>Curious Machines: Active Learning with Structured Instances</em></span>. PhD thesis, University of Wisconsin–Madison.</li><li class="listitem">D. Angluin (1988). <span class="emphasis"><em>Queries and concept learning</em></span>. Machine Learning, 2:319–342.</li><li class="listitem">D. Lewis and W. Gale (1994). <span class="emphasis"><em>A sequential algorithm for training text classifiers</em></span>. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3–12. ACM/Springer.</li><li class="listitem">H.S. Seung, M. Opper, and H. Sompolinsky (1992). <span class="emphasis"><em>Query by committee</em></span>. In Proceedings of the ACM Workshop on Computational Learning Theory, pages 287–294.</li><li class="listitem">D. Cohn, L. Atlas, R. Ladner, M. El-Sharkawi, R. Marks II, M. Aggoune, and D. Park (1992). <span class="emphasis"><em>Training connectionist networks with queries and selective sampling</em></span>. In Advances in Neural Information Processing Systems (NIPS). Morgan Kaufmann.</li><li class="listitem">D. Cohn, L. Atlas, and R. Ladner (1994). <span class="emphasis"><em>Improving generalization with active learning</em></span>. Machine Learning, 15(2):201–221.</li><li class="listitem">D. Lewis and J. Catlett (1994). <span class="emphasis"><em>Heterogeneous uncertainty sampling for supervised learning</em></span>. In Proceedings of the International Conference on Machine Learning (ICML), pages 148–156. Morgan Kaufmann.</li><li class="listitem">S. Dasgupta, A. Kalai, and C. Monteleoni (2005). <span class="emphasis"><em>Analysis of perceptron-based active learning</em></span>. In Proceedings of the Conference on Learning Theory (COLT), pages 249–263. Springer. </li><li class="listitem">S. Dasgupta, D. Hsu, and C. Monteleoni (2008). <span class="emphasis"><em>A general agnostic active learning algorithm</em></span>. In Advances in Neural Information Processing Systems (NIPS), volume 20, pages 353–360. MIT Press. </li><li class="listitem">T. Mitchell (1982). <span class="emphasis"><em>Generalization as search</em></span>. Artificial Intelligence, 18:203–226.</li></ol></div></div></body></html>