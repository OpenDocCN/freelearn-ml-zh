<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">The ML Kit SDK</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will discuss ML Kit, which was announced by Firebase at the Google I/O 2018. This SDK packages Google's mobile machine learning offerings under a single umbrella.</p>
<p>Mobile application developers may want to implement features in their mobile apps that require machine learning capabilities. However, they may not have knowledge of machine learning concepts and which algorithms to use for which scenarios, how to build the model, train the model, and so on.</p>
<p>ML Kit tries to address this problem by identifying all the potential use cases for machine learning in the context of mobile devices, and providing ready-made APIs. If the correct inputs are passed to these, the required output is received, with no further coding required.</p>
<p>Additionally, this kit enables the inputs to be passed either to on-device APIs that work offline, or to online APIs that are hosted in the cloud.</p>
<p>To top it all, ML Kit also provides options for developers with expertise in machine learning, allowing them to build their own models using TensorFlow/TensorFlow Lite, and them import them into the application and invoke them using ML Kit APIs.</p>
<p>ML Kit also offers further useful features, such as model upgrade and monitoring capabilities (if hosted with Firebase).</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>ML Kit and its features</li>
<li>Creating an image-labeling sample using ML Kit on-device APIs</li>
<li>Creating the same<span> sample</span> using ML Kit cloud APIs</li>
<li>Creating Face Detection application </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding ML Kit</h1>
                </header>
            
            <article>
                
<p class="graf graf--p graf-after--figure">ML Kit encompasses all the existing Google offerings for machine learning on mobile. It bundles the Google Cloud Vision API<span>,</span><span> </span>TensorFlow Lite<span>, and the</span><span> </span>Android Neural Networks API<span> </span><span>together in a single SDK, as shown:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-963 image-border" src="assets/e34862b1-59df-45ab-acca-1aebfdb354b5.png" style="width:13.92em;height:13.75em;"/></p>
<p class="graf graf--p graf-after--figure"><span>ML Kit enables developers to utilize machine learning in their mobile applications for both Android and iOS apps, in a very easy way. Inference</span> can be carried out by invoking APIs that are either on-device or on-cloud.</p>
<p>The advantages of on-device APIs are that they work completely offline, and are more secure as no data is sent to the cloud. By contrast, on-cloud APIs do require network connectivity, and do send data off-device, but allow for greater accuracy.</p>
<p>ML Kit offers APIs covering the following machine learning scenarios that may be required by mobile application developers:</p>
<ul>
<li>Image labeling</li>
<li>Text recognition</li>
<li>Landmark detection</li>
<li>Face detection</li>
<li>Barcode scanning </li>
</ul>
<p><span>All these APIs are implemented using complex machine learning algorithms. However, those details are wrapped. The mobile developer need not get into the details of which algorithms are used for implementing these APIs; all that needs to be done is to </span><span>pass the desired data to the SDK, and in return the correct output will be received back, depending on which part of ML Kit is being used. </span></p>
<p>If the provided APIs don't cover a specific use case, you can build your own TensorFlow Lite model. ML Kit will help to host that model, and serve it to your mobile application.</p>
<p><span>Since Firebase ML Kit provides both on-device and on-cloud capabilities, developers can come up with innovative solutions to leverage either or both, based on the specific problem at hand. All they need to know is that on-device APIs are fast and work offline, while Cloud APIs utilize the Google Cloud platform to provide predictions with increased levels of accuracy.</span></p>
<p><span>The following diagram describes the issues to consider when deciding between on-device or on-cloud APIs:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-914 image-border" src="assets/7ad6b4d2-8c3b-420a-96ae-4d80d493b816.png" style="width:35.83em;height:22.83em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ML Kit APIs</h1>
                </header>
            
            <article>
                
<p><span>Not all APIs provided by ML Kit are supported in both on-device and on-cloud modes. The following table shows which APIs are supported in each mode:</span></p>
<p class="aspectRatioPlaceholder-fill CDPAlignCenter CDPAlign"><img src="assets/d35b8fc6-71dc-4e0d-ae3c-e28c568a0796.png" style="width:26.00em;height:13.58em;"/></p>
<p>Let's look at the details of each API.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text recognition</h1>
                </header>
            
            <article>
                
<p>ML Kit's text recognition APIs help with the recognition of text in any Latin-based language, using the mobile device camera. They are available both on-device and on-cloud.</p>
<p>The on-device API allows for recognition of sparse text, or text present in images. The cloud API does the same, but also allows for recognition of bulk text, such as in documents. The cloud API also supports recognition of more languages than device APIs are capable of.</p>
<p>Possible use cases for these APIs would be to recognize text in images, to scan for characters that may be embedded in images, or to automate tedious data entry.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Face detection</h1>
                </header>
            
            <article>
                
<p>The ML Kit's face detection API allows for the detection of faces in an image or video. Once the face is detected, we can apply the following refinements:</p>
<ul>
<li><strong>Landmark detection</strong>: Determining specific points of interest (landmarks) within the face, such as the eyes</li>
<li><strong>Classification</strong>: Classifying the face based on certain characteristics, such as whether the eyes are open or closed</li>
<li><strong>Face tracking</strong>: Recognizing and tracking the same face (in various positions) across different frames of video</li>
</ul>
<p>Face detection can be done only on-device and in real time. There may be many use cases for mobile device applications, in which the camera captures an image and manipulates it based on landmarks or classifications, to produce selfies, avatars, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Barcode scanning</h1>
                </header>
            
            <article>
                
<p>ML Kit's barcode-scanning API helps read data encoded using most standard barcode formats. It supports <span>linear formats such as Codabar, Code 39, Code 93, Code 128, EAN-8, EAN-13, ITF, UPC-A, or UPC-E, as well as </span><span>2-D formats such as Aztec, Data Matrix, PDF417, or QR codes. </span></p>
<p><span>The API can recognize and scan barcodes regardless of their orientation. Any structured data that is stored as a barcode can be recognized.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image labeling</h1>
                </header>
            
            <article>
                
<p>ML Kit's image-labeling APIs help recognize entities in an image. There is no need for any other metadata information to be provided for this entity recognition. Image labeling gives insight into the content of images. The ML Kit API provides the entities in the images, along with a confidence score for each one.</p>
<p>Image labeling is available both on-device and on-cloud, with the difference being the number of labels supported. The on-device API supports around 400 labels, while the cloud-based API supports up to 10,000.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Landmark recognition</h1>
                </header>
            
            <article>
                
<p>The ML Kit's landmark recognition API helps recognize well-known landmarks in an image.</p>
<p>This API, when given an image as input, will provide the landmarks found in the image along with geographical coordinates and region information. The k<span>nowledge graph entity ID is also returned for the landmark. This ID is a string that uniquely identifies the landmark that was recognized.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Custom model inference</h1>
                </header>
            
            <article>
                
<div class="feature-intro block">
<p>If the APIs provided out-of-the-box are not sufficient for your use case, ML Kit also provides the option to create your own custom model and deploy it through ML Kit.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a text recognition app using Firebase on-device APIs</h1>
                </header>
            
            <article>
                
<p>To get started in ML Kit, you need to sign in to your Google account, activate your Firebase account, and create a Firebase project. Follow these steps:</p>
<ul>
<li>Go to <a href="https://firebase.google.com/">https://firebase.google.com/.</a></li>
<li>Sign in to your Google account, if you are not already signed in.</li>
<li>Click <span class="packt_screen"><span class="packt_screen">Go</span> to console</span> in the menu bar.</li>
<li>Click <span class="packt_screen">Add project</span> to create a project and open it.</li>
</ul>
<p>Now open Android Studio, and create a project with an empty activity. Note down the app package name that you have given while creating the project<span>—for example,  </span><kbd>com.packt.mlkit.textrecognizationondevice</kbd>.</p>
<p>Next, go to the Firebase console. In the <span class="packt_screen">Project overview</span> menu, click <span class="packt_screen">Add app</span> and give the required information. It will give you a JSON file to download. Add to the app folder of your project in project view in Android Studio, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7e38470b-fdb7-42a7-a5ff-2880d1e01259.png" style="width:23.33em;height:28.92em;"/></p>
<p>Next, add the following lines of code to the manifest file:</p>
<pre><span>&lt;uses-feature </span><span>android</span><span>:name=</span><span>"android.hardware.camera2.full" </span><span>/&lt;<br/></span><span>&lt;uses-permission </span><span>android</span><span>:name=</span><span>"android.permission.CAMERA" </span><span>/&lt;<br/></span><span>&lt;uses-permission </span><span>android</span><span>:name=</span><span>"android.permission.INTERNET" </span><span>/&lt;<br/></span><span>&lt;uses-permission </span><span>android</span><span>:name=</span><span>"android.permission.WRITE_EXTERNAL_STORAGE" </span><span>/&lt;<br/></span><span>&lt;uses-permission </span><span>android</span><span>:name=</span><span>"android.permission.READ_EXTERNAL_STORAGE" </span><span>/&lt;</span></pre>
<p>We need these permissions for our app to work. <span>The next line tells the Firebase dependencies to download the </span><span><strong>text recognition</strong> (</span><strong>OCR</strong><span>) model from the Google server, and keep it in the device for inference:</span></p>
<pre><span>&lt;meta-data<br/></span><span>   </span><span>android</span><span>:name=</span><span>"com.google.firebase.ml.vision.DEPENDENCIES"<br/></span><span>    </span><span>android</span><span>:value=</span><span>"ocr" </span><span>/&lt;</span></pre>
<p>The whole manifest file will look as follows:</p>
<pre><span>&lt;?</span><span>xml version=</span><span>"1.0" </span><span>encoding=</span><span>"utf-8"</span><span>?&lt;<br/></span><span>&lt;manifest </span><span>xmlns:</span><span>android</span><span>=</span><span>"http://schemas.android.com/apk/res/android"<br/></span><span>    </span><span>package=</span><span>"com.packt.mlkit.testrecognizationondevice"</span><span>&lt;<br/></span><span><br/></span><span>    &lt;uses-feature </span><span>android</span><span>:name=</span><span>"android.hardware.camera2.full" </span><span>/&lt;<br/></span><span>    &lt;uses-permission </span><span>android</span><span>:name=</span><span>"android.permission.CAMERA" </span><span>/&lt;<br/></span><span>    &lt;uses-permission </span><span>android</span><span>:name=</span><span>"android.permission.INTERNET" </span><span>/&lt;<br/></span><span>    &lt;uses-permission </span><span>android</span><span>:name=</span><span>"android.permission.WRITE_EXTERNAL_STORAGE" </span><span>/&lt;<br/></span><span>    &lt;uses-permission </span><span>android</span><span>:name=</span><span>"android.permission.READ_EXTERNAL_STORAGE" </span><span>/&lt;<br/></span><span>    &lt;application<br/></span><span>        </span><span>android</span><span>:allowBackup=</span><span>"true"<br/></span><span>        </span><span>android</span><span>:icon=</span><span>"@mipmap/ic_launcher"<br/></span><span>        </span><span>android</span><span>:label=</span><span>"@string/app_name"<br/></span><span>        </span><span>android</span><span>:roundIcon=</span><span>"@mipmap/ic_launcher_round"<br/></span><span>        </span><span>android</span><span>:supportsRtl=</span><span>"true"<br/></span><span>        </span><span>android</span><span>:theme=</span><span>"@style/AppTheme"</span><span>&lt;<br/></span><span><br/></span><span>        &lt;meta-data<br/></span><span>            </span><span>android</span><span>:name=</span><span>"com.google.firebase.ml.vision.DEPENDENCIES"<br/></span><span>            </span><span>android</span><span>:value=</span><span>"ocr" </span><span>/&lt;<br/></span><span><br/></span><span><br/></span><span>        &lt;activity </span><span>android</span><span>:name=</span><span>".MainActivity"</span><span>&lt;<br/></span><span>            &lt;intent-filter&lt;<br/></span><span>                &lt;action </span><span>android</span><span>:name=</span><span>"android.intent.action.MAIN" </span><span>/&lt;<br/></span><span><br/></span><span>                &lt;category </span><span>android</span><span>:name=</span><span>"android.intent.category.LAUNCHER" </span><span>/&lt;<br/></span><span>            &lt;/intent-filter&lt;<br/></span><span>        &lt;/activity&lt;<br/></span><span>    &lt;/application&lt;<br/></span><span><br/></span><span>&lt;/manifest&lt;</span></pre>
<p>Now, we need to add the Firebase dependencies to the project. To do so, we need to add the following lines to the project <kbd>build.gradle</kbd> file:</p>
<pre>buildscript {<br/>    <br/>    repositories {<br/>        google()<br/>        jcenter()<br/>    }<br/>    dependencies {<br/>        classpath <span>'com.android.tools.build:gradle:3.1.4' //this version will defer dependeds on your environment.<br/></span><span>        </span>classpath <span>'com.google.gms:google-services:4.0.1'<br/></span><span><br/></span><span>        </span><span>// NOTE: Do not place your application dependencies here; they belong<br/></span><span>        // in the individual module build.gradle files<br/></span><span>    </span>}<br/>}</pre>
<p>Then open the module app <kbd>build.gradle</kbd> file, and add the following dependencies:</p>
<pre>implementation <span>'com.google.firebase:firebase-ml-vision:17.0.0'<br/></span>implementation <span>'com.google.firebase:firebase-core:16.0.3'</span></pre>
<p>Also add the following line to the bottom of that file:</p>
<pre>apply <span>plugin</span>: <span>'com.google.gms.google-services'</span></pre>
<p>Now, in your layout file, write the following <kbd>.xml</kbd> code to define the elements:</p>
<pre>&lt;?xml version="1.0" encoding="utf-8"?&lt;<br/>&lt;RelativeLayout <br/>    <br/>    android:layout_width="match_parent"<br/>    android:layout_height="match_parent"<br/>    tools:context="(main activity)"&lt; &lt;!-- Here your fully qualified main activity class name will come. --&lt;<br/> <br/>    &lt;TextureView<br/>        android:id="@+id/preview"<br/>        android:layout_width="match_parent"<br/>        android:layout_height="wrap_content"<br/>        android:layout_above="@id/btn_takepic"<br/>        android:layout_alignParentTop="true"/&lt;<br/><br/>    &lt;Button<br/>        android:id="@+id/btn_takepic"<br/>        android:layout_width="wrap_content"<br/>        android:layout_height="wrap_content"<br/>        android:layout_alignParentBottom="true"<br/>        android:layout_centerHorizontal="true"<br/>        android:layout_marginBottom="16dp"<br/>        android:layout_marginTop="16dp"<br/>        android:text="Start Labeling"<br/>        /&lt;<br/>&lt;/RelativeLayout&lt;</pre>
<p>Now, it's time to code your application's main activity class.</p>
<p>Please download the application code from Packt Github repository at <a href="https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/mlkit">https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/mlkit</a>.</p>
<p>We are assuming you are already familiar with Android<span>—</span>so, we are discussing the code using Firebase functionalities:</p>
<pre><span>import </span>com.google.firebase.FirebaseApp<span>;<br/></span><span>import </span>com.google.firebase.ml.vision.FirebaseVision<span>;<br/></span><span>import </span>com.google.firebase.ml.vision.common.FirebaseVisionImage<span>;<br/></span><span>import </span>com.google.firebase.ml.vision.text.FirebaseVisionTextRecognizer<span>;<br/></span><span>import </span>com.google.firebase.ml.vision.text.*<span>;</span></pre>
<p><span>The preceding code will import the firebase libraries.</span></p>
<pre><span>private </span>FirebaseVisionTextRecognizer <span>textRecognizer</span><span>;</span></pre>
<p><span><br/>
The preceding line will declare the firebase text recognizer.<br/></span></p>
<pre>FirebaseApp fapp= FirebaseApp.<span>initializeApp</span>(getBaseContext())<span>;</span></pre>
<p><span>The preceding line will i</span>nitialize the Firebase application context.</p>
<pre><span>        </span><span>textRecognizer </span>= FirebaseVision.<span>getInstance</span>().getOnDeviceTextRecognizer()<span>;</span></pre>
<p><span>The preceding line will g</span>et the on-device text recognizer.</p>
<pre><span>   </span><span>    </span><span>takePictureButton</span>.setOnClickListener(<span>new </span>View.OnClickListener() {<br/>            <span>@Override<br/></span><span>            </span><span>public void </span><span>onClick</span>(View v) {<br/>                takePicture()<span>;<br/>                <strong>//In this function we are having the code to decode the characters in the picture</strong><br/></span><span>            </span>}<br/>        })<span>;<br/></span><span>    </span>}</pre>
<p>The preceding code snippet registers the on-click-event listener for the take-picture button.</p>
<p> </p>
<pre>Bitmap bmp = BitmapFactory.<span>decodeByteArray</span>(bytes<span>,</span><span>0</span><span>,</span>bytes.<span>length</span>)<span>;</span></pre>
<p>Creating a bitmap from the byte array.</p>
<pre>FirebaseVisionImage firebase_image = FirebaseVisionImage.<span>fromBitmap</span>(bmp)<span>;</span></pre>
<p>The preceding line creates a firebase image object to pass through the recognizer.</p>
<pre><span> </span><span>textRecognizer</span>.processImage(firebase_image)</pre>
<p><span>The preceding line p</span>asses the created image object to the recognizer for processing.</p>
<pre>.addOnSuccessListener(<span>new </span>OnSuccessListener&lt;FirebaseVisionText&lt;() {<br/>                                    <span>@Override<br/></span><span>                                    </span><span>public void </span><span>onSuccess</span>(FirebaseVisionText result) {<br/><strong>//On receiving the results displaying to the user.</strong>                                       Toast.<span>makeText</span>(getApplicationContext()<span>,</span>result.getText()<span>,</span>Toast.<span>LENGTH_LONG</span>).show()<span>;<br/></span><span>                                    </span>}<br/>                                })</pre>
<p>The preceding code block will add the on-success listener. It will receive a firebase vision text object, which it in turn displays to the user in the form of a <kbd>Toast</kbd> message.</p>
<pre>.addOnFailureListener(<br/><span>            new </span>OnFailureListener() {<br/><span>                @Override<br/></span><span>                </span><span>public void </span><span>onFailure</span>(<span>@NonNull </span>Exception e) <br/>                    {<br/>                        Toast.<span>makeText</span>(getApplicationContext()<span>,</span><span>"Unable to read the text"</span><span>,</span>Toast.<span>LENGTH_LONG</span>).show()<span>;<br/></span><span>                    </span>}<br/>                  })<span>;</span></pre>
<p><span>The preceding code block will add the <kbd>on-failure</kbd> listener. It will receive an exception object, which is in turn</span><span> a display error message to the user in the form of a <kbd>Toast</kbd> message.</span></p>
<p>When you run the preceding code, you will have the following output in your device:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/70a9a836-c149-4fc8-88af-fc888459126f.png" style="width:24.08em;height:42.75em;"/></p>
<p><span>Note that you must be connected to the internet while installing this app, as Firebase needs to download the model to your device.</span></p>
<p> </p>
<p> </p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a text recognition app using Firebase on-cloud APIs</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to convert the on-device app to a cloud app. The difference is that on-device apps download the model and store it on the device. This allows for a lower inference time, allowing the app to make quick predictions.</p>
<p>By contrast, cloud-based apps upload the image to the Google server, meaning inference will happen there. It won't work if you are not connected to the internet.</p>
<p>In this case, why use a cloud-based model? Because on-device, the model has limited space and processing hardware, whereas Google's servers are scalable. The Google on-cloud text recognizer model is also able to decode multiple languages.</p>
<p>To get started, you need a Google Cloud subscription. Follow these steps:</p>
<ul>
<li class="CDPAlignLeft CDPAlign">Go to your Firebase project console</li>
<li>In the menu on the left, you will see that you are currently on the Spark Plan (the free tier)</li>
<li>Click <span class="packt_screen">Upgrade</span>, and follow the instructions to upgrade to the Blaze Plan, which is pay-as-you-go</li>
<li>You need to provide credit card or payment details for verification purposes<span>—these</span> will not be charged automatically</li>
<li>Once you subscribe, you will receive 1,000 Cloud Vision API requests free each month</li>
</ul>
<div class="packt_infobox">This program can be tried only if you have a upgraded Blaze Plan and not a free tier account. The steps are given to create a upgraded account and please follow steps to get the account to try the program given.</div>
<p>By default, Cloud Vision is not enabled for your project. To do so, you need to go to the following link: <a href="https://console.cloud.google.com/apis/library/vision.googleapis.com/?authuser=0">https://console.cloud.google.com/apis/library/vision.googleapis.com/?authuser=0</a>. In the top menu dropdown, select the Firebase project containing the<span> Android app you added in the previous section.</span></p>
<p>Click <span class="packt_screen">Enable</span> to enable this feature for your app. The page will look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aa14a5d0-5a6b-4135-bb4a-bd85d8a04f02.png"/></p>
<p>Now return to your code, and make the following changes.</p>
<p>You can find the application code in our Packt Github repository at:<a href="https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/Testrecognizationoncloud"> https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/Testrecognizationoncloud</a>.</p>
<p>All the other files, except the main activity, have no changes.</p>
<p>The changes are as follows:</p>
<pre><span>import </span>com.google.firebase.FirebaseApp<span>;<br/></span><span>import </span>com.google.firebase.ml.vision.FirebaseVision<span>;<br/></span><span>import </span>com.google.firebase.ml.vision.common.FirebaseVisionImage<span>;<br/></span><strong><span>import </span>com.google.firebase.ml.vision.document.FirebaseVisionDocumentText<span>;<br/></span><span>import </span>com.google.firebase.ml.vision.document.FirebaseVisionDocumentTextRecognizer<span>;<br/></span></strong></pre>
<p>Now, we need to import the preceding packages as dependencies.</p>
<pre><span> </span><span>private </span>FirebaseVisionDocumentTextRecognizer <span>textRecognizer</span><span>;</span></pre>
<p>The preceding code will declare the document text recognizer.</p>
<pre><span>textRecognizer </span>= FirebaseVision.<span>getInstance</span>().getCloudDocumentTextRecognizer()<span>;</span></pre>
<p><span>The preceding code i</span>nstantiates and assigns the cloud text recognizer.</p>
<pre><span>   </span><span>    </span><span>takePictureButton</span>.setOnClickListener(<span>new </span>View.OnClickListener() {<br/>            <span>@Override<br/></span><span>            </span><span>public void </span><span>onClick</span>(View v) {<br/>                takePicture()<span>;<br/>                <strong>//In this function we are having the code to decode the characters in the picture</strong><br/></span><span>            </span>}<br/>        })<span>;<br/></span><span>    </span>}</pre>
<p><span>The preceding code r</span>egisters the on-click-event listener for the take-picture button.</p>
<pre>Bitmap bmp = BitmapFactory.<span>decodeByteArray</span>(bytes<span>,</span><span>0</span><span>,</span>bytes.<span>length</span>)<span>;</span></pre>
<p><span>The preceding line c</span>reates a bitmap from the byte array.</p>
<pre>FirebaseVisionImage firebase_image = FirebaseVisionImage.<span>fromBitmap</span>(bmp)<span>;</span></pre>
<p><span>The preceding line c</span>reates a firebase image object to pass through the recognizer.</p>
<pre><span> </span><span>textRecognizer</span>.processImage(firebase_image)</pre>
<p><span>The preceding line p</span>asses the created image object to the recognizer for processing.</p>
<pre>.addOnSuccessListener(<span>new </span>OnSuccessListener&lt;FirebaseVisionDocumentText&lt;() {<br/>                                    <span>@Override<br/></span><span>                                    </span><span>public void </span><span>onSuccess</span>(FirebaseVisionDocumentText result) {<br/>                                        Toast.<span>makeText</span>(getApplicationContext()<span>,</span>result.getText()<span>,</span>Toast.<span>LENGTH_LONG</span>).show()<span>;<br/></span><span>                                    </span>}<br/>                                })</pre>
<p>The preceding code block will add the on-success listener. It will receive a FirebaseVision document text object, which is in turn displayed to the user in the form of a <kbd>Toast</kbd> message.</p>
<pre>.addOnFailureListener(<br/><span>            new </span>OnFailureListener() {<br/><span>                @Override<br/></span><span>                </span><span>public void </span><span>onFailure</span>(<span>@NonNull </span>Exception e) <br/>                    {<br/>                        Toast.<span>makeText</span>(getApplicationContext()<span>,</span><span>"Unable to read the text"</span><span>,</span>Toast.<span>LENGTH_LONG</span>).show()<span>;<br/></span><span>                    </span>}<br/>                  })<span>;</span></pre>
<p><span>The preceding code block will add the on-failure listener. It will receive an exception object, which is in turn</span><span> a display error message to the user in the form of a <kbd>Toast</kbd> message.</span></p>
<pre>Once you run the code with the internet-connected device , you will get the same output as before, but from the cloud.</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Face detection using ML Kit</h1>
                </header>
            
            <article>
                
<p>Now we will try to understand how face detection works with ML Kit. Face detection, which was previously part of the Mobile Vision API, has now been moved to ML Kit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Face detection concepts</h1>
                </header>
            
            <article>
                
<p>The Google Developers page defines face detection as the process of automatically locating and detecting human faces in visual media (digital images or video). The detected face is reported at a position with an associated size and orientation. After the face is detected, we can search for landmarks present in the face such as the eyes and nose.</p>
<p>Here are some important terms to understand before we can move on to programming face detection with ML Kit:</p>
<ul>
<li><strong>Face Orientation</strong>: Detects faces at a range of different angles.</li>
<li><strong>Face Recognition</strong>: Determines whether two faces can belong to the same person.</li>
<li><strong>Face Tracking</strong>: Refers to detecting faces in videos.</li>
<li><strong>Landmark</strong>: Refers to a point of interest within a face. This corresponds to the notable features on a face, such as the right eye, left eye, and nose base.</li>
<li><strong>Classification</strong>: Determines the presence of facial characteristics, such as open or closed eye or a smiling or serious face.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sample solution for face detection using ML Kit</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now open Android Studio, and create a project with an empty activity. Note down the app package name that you have given while creating the project—for example, <kbd>com.packt.mlkit.facerecognization</kbd>.</p>
<p class="mce-root">Here we are going to modify the text recognization code to predict faces. So, we are not changing the package names and other things. Just the code changes. The project structure is the same as shown previously:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-905 image-border" src="assets/bfd38de1-3b33-45d5-b438-bc647837d680.png" style="width:27.08em;height:33.17em;"/></p>
<p><span>It's time to code our application's main activity class. First we need to download the application code from the Packt GitHub repository at </span><a href="https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/facerecognization">https://github.com/PacktPublishing/Machine-Learning-for-Mobile/tree/master/facerecognization</a><span>. and open the project in Android Studio. </span></p>
<p class="mce-root">Then we will add the following lines of code to the Gradle dependencies. Open the <kbd>build.gradle</kbd> file of the module app and add the following dependencies:</p>
<pre class="mce-root">implementation 'com.google.android.gms:play-services-vision:11.4.0'<br/>implementation 'com.android.support.constraint:constraint-layout:1.0.2'</pre>
<p class="mce-root">Now we will  add the import statements to work with face detection:</p>
<pre class="mce-root">import com.google.android.gms.vision.Frame;<br/>import com.google.android.gms.vision.face.Face;<br/>import com.google.android.gms.vision.face.FaceDetector;</pre>
<p><span>The following statement will declare the <kbd>FaceDetector</kbd> object:</span></p>
<pre class="mce-root"><span>private FaceDetector detector;</span></pre>
<p>Now we will create an object and assign it to the declared detector:</p>
<pre class="mce-root">detector = new FaceDetector.Builder(getApplicationContext())<br/> .setTrackingEnabled(false)<br/> .setLandmarkType(FaceDetector.ALL_LANDMARKS)<br/> .setClassificationType(FaceDetector.ALL_CLASSIFICATIONS)<br/> .build();</pre>
<p><span>We declared a string object to save the prediction messages to the user:</span></p>
<pre class="mce-root">String scanResults = "";</pre>
<p class="mce-root"><span>Here we will check whether the detector is operational; we also have a bitmap object that was obtained from the camera:</span></p>
<pre class="mce-root">if (detector.isOperational() &amp;&amp; bmp != null) {</pre>
<p class="mce-root">Then we create a frame object, which <kbd>FaceDetector</kbd> class detect method needs to predict the face information:</p>
<pre class="mce-root">Frame frame = new Frame.Builder().setBitmap(bmp).build();<span>SparseArray&lt;Face&gt; faces = detector.detect(frame);</span></pre>
<p class="mce-root">Once it successfully detects, it will return the face object array. <span>The following code appends the information that each <kbd>nface</kbd> object has to our results string:</span></p>
<pre>for (int index = 0; index &lt; faces.size(); ++index) {<br/>    Face face = faces.valueAt(index);<br/>    scanResults += "Face " + (index + 1) + "\n";<br/>    scanResults += "Smile probability:" + "\n" ;<br/>    scanResults += String.valueOf(face.getIsSmilingProbability()) + "\n";          scanResults += "Left Eye Open Probability: " + "\n";<br/>    scanResults += String.valueOf(face.getIsLeftEyeOpenProbability()) + "\n";<br/>    scanResults += "Right Eye Open Probability: " + "\n";<br/>    scanResults += String.valueOf(face.getIsRightEyeOpenProbability()) + "\n";<br/>}</pre>
<p>If no faces are returned, then the following error message will be shown:</p>
<pre class="mce-root">if (faces.size() == 0) {<br/>    scanResults += "Scan Failed: Found nothing to scan";<br/> } </pre>
<p class="mce-root"><span>If the face size is not <kbd>0</kbd>, that means it already went through the <kbd>for</kbd> loop, which appended the faces information to our results text. Now we will add the total number of faces and end the result string:</span></p>
<pre class="mce-root">else {<br/>    scanResults += "No of Faces Detected: " + "\n";<br/>    scanResults += String.valueOf(faces.size()) + <br/>   \n";<br/>    scanResults += "---------" + "\n";<br/>}</pre>
<p class="mce-root"><span>If the detector is not operational then the error message will be shown to the user as follows:</span></p>
<pre class="mce-root">else {<br/>    scanResults += "Could not set up the detector!";<br/>}</pre>
<p class="mce-root"><span>Finally, the following code will show the results to the reader:</span></p>
<pre class="mce-root">Toast.makeText(getApplicationContext(),scanResults,Toast.LENGTH_LONG).show();</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the app</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now it's time to run the app. For that, you will have to connect your mobile to your desktop through the USB debugging option in your mobile and install the app:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-906 image-border" src="assets/024b29f4-2d08-4cb6-9efd-7533311a3949.png" style="width:22.58em;height:40.17em;"/></p>
<p class="mce-root">On running the app, you will have the following as the output:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/766c66a2-276a-4889-8b0d-b89b66259545.jpg" style="width:18.08em;height:32.08em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we discussed </span><span>ML Kit SDK, which was announced by Firebase at Google I/O 2018. We covered different </span><span>APIs provided by ML Kit, such as i</span><span>mage labeling, </span><span>text recognition, </span><span>landmark detection, and more. </span><span>We then created a text recognition app using on-device APIs, and then using on-cloud APIs. We also create an Face detection application by making minor changes in text recognition application.  </span><span>In the next chapter, we will learn about a spam messages classifier and build a sample implementation of such a classifier for iOS.</span></p>


            </article>

            
        </section>
    </body></html>