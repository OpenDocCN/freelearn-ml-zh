<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Learning Object Classification</h1>
                </header>
            
            <article>
                
<p>In <a href="b788527c-5892-4547-8add-0864ccbd3f95.xhtml"/><a href="b788527c-5892-4547-8add-0864ccbd3f95.xhtml" target="_blank">Chapter 5</a>, <em>Automated Optical Inspection, Object Segmentation, and Detection</em>, we introduced the basic concepts of object segmentation and detection. This refers to isolating the objects that appear in an image for future processing and analysis. This chapter explains how to classify each of these isolated objects. To allow us to classify each object, we have to train our system to be capable of learning the required parameters so that it decide which specific label will be assigned to the detected object (depending on the different categories taken into account during the training phase).</p>
<p>This chapter introduces the basics concepts of machine learning to classify images with different labels. To do this, we are going to create a basic application based on the segmentation algorithm of <a href="b788527c-5892-4547-8add-0864ccbd3f95.xhtml">Chapter 5</a>, <em>Automated Optical Inspection, Object Segmentation, and Detection</em>. This segmentation algorithm extracts parts of images that contain unknown objects. For each detected <span>object</span><span>, we are going to extract different features that are going to be classified using a machine learning algorithm.</span> Finally, we are going <span>to show </span><span>the </span><span>obtained</span><span> </span><span>results</span> <span>using our user interface,</span><span> together with the labels of each object detected in the input image.</span></p>
<p>This chapter involves different topics and algorithms, including the following:</p>
<ul>
<li>Introduction to machine learning concepts</li>
<li>Common machine learning algorithms and processes</li>
<li>Feature extraction</li>
<li>support vector machines (SVM)</li>
<li>Training and prediction</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This chapter requires familiarity with the basic C++ programming language. All of the code that's used in this chapter can be downloaded from the following GitHub link: <a href="https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_06">https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_06</a>. This code can be executed on any operating system, though it is only tested on Ubuntu.</p>
<p><span>Check out the following video to see the Code in Action:</span><br/>
<a href="http://bit.ly/2KGD4CO">http://bit.ly/2KGD4CO</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing machine learning concepts</h1>
                </header>
            
            <article>
                
<p><span>Machine learning is a concept that was defined by <em>Arthur Samuel</em> </span><span>in 1959 </span><span>as a </span><span>field of study that gives computers the ability to learn without being explicitly programmed. <em>Tom M. Mitchel</em> provided a more formal definition for machine learning, in which he links the concept of samples with experience data, labels, and a performance measurement of algorithms.</span></p>
<div class="packt_infobox"><span>The <strong>machine learning</strong> definition by <em>Arthur Samuel</em> is referenced in <em>Some Studies in Machine Learning Using the Game of Checkers</em> in <em>IBM Journal of Research and Development</em> (<em>Volume</em>: <em>3</em>, <em>Issue</em>: <em>3</em>), <em>p</em>. <em>210</em>. It was also referenced in <em>The New Yorker</em> and <em>Office Management</em> in the same year. <br/></span> <span>The more formal definition from <em>Tom M. Mitchel</em> is referenced in <em>Machine Learning Book, McGray Hill 1997:</em> (</span><a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html"><span class="URLPACKT">http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html</span></a><span>).</span></div>
<p>Machine learning involves pattern recognition and learning theory in artificial intelligence, and is related with computational statistics. It<span> is used in hundreds of applications, such as <strong>optical character recognition</strong> (<strong>OCR</strong>), spam filtering, search engines, and thousands of computer vision applications, such as the example that we will develop in this chapter, where a machine learning algorithm tries to classify objects that appear in the input image.</span></p>
<p>Depending on how machine learning algorithms learn from the input data, we can divide them into three categories:</p>
<ul>
<li><strong>Supervised learning</strong>: The computer learns from a set of labeled data. The goal here is to learn the parameters of the model and rules that allow computers to map the relationship between data and output label results.</li>
</ul>
<ul>
<li><strong>Unsupervised learning</strong>: No labels are given and the computer tries to discover the input structure of the given data.</li>
<li><strong>Reinforcement learning</strong>: The computer interacts with a dynamic environment, reaching their goal and learning from their mistakes.</li>
</ul>
<p>Depending on the results we wish to gain from our machine learning algorithm, we can categorize the results as follows:</p>
<ul>
<li><strong>Classification</strong>: The space of the inputs can be divided into <strong>N</strong> classes, and the prediction results for a given sample are one of these training classes. This is one of the <span>most used categories. A typical example can be email spam filtering, where there are only two classes: <span class="ScreenTextPACKT">spam</span> and non-spam. Alternatively, we can use OCR, where only N characters are available and each character is one class.</span></li>
<li><strong>Regression</strong>: The output is a continuous value instead of a discrete value like a classification result. One example of regression could be the prediction of a house price given the house's size, number of years since it was built, and location.</li>
<li><strong>Clustering</strong>: The input is to be divided into N groups, which is typically done using unsupervised training.</li>
<li><strong>Density estimation</strong>: Finds the (probability) distribution of inputs.</li>
</ul>
<p>In our example, we are going to use a supervised learning and classification algorithm where a training dataset with labels is used to train the model and the result of the model's prediction is one of the possible labels. In machine learning, there are several approaches and methods for this. Some of the more popular ones include the following: <strong>support vector machines</strong> (<strong>SVM</strong>), <strong>artificial neural networks</strong> (<strong>ANN</strong>), clustering, k-nearest neighbors, decision trees, and deep learning. Almost all of these methods and approaches are supported, implemented, and well documented in OpenCV. In this chapter, we are going to explain support vector machines.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OpenCV machine learning algorithms</h1>
                </header>
            
            <article>
                
<p>OpenCV implements eight of these machine learning algorithms. All of them are inherited from the <kbd><span class="CodeInTextPACKT">StatModel</span></kbd> class:</p>
<ul>
<li>Artificial neural networks</li>
<li>Random trees</li>
<li>Expectation maximization</li>
<li>k-nearest neighbors</li>
<li>Logistic regression</li>
<li>Normal Bayes classifiers</li>
</ul>
<ul>
<li>support vector machine</li>
<li>Stochastic gradient descent SVMs</li>
</ul>
<p>Version 3 supports deep learning at a basic level, but version 4 is stable and more supported. We will delve into deep learning in detail in further chapters.</p>
<div class="packt_infobox"><span>To get more information about each algorithm, read the OpenCV document page for machine learning at </span><a href="http://docs.opencv.org/trunk/dc/dd6/ml_intro.html"><span class="URLPACKT">http://docs.opencv.org/trunk/dc/dd6/ml_intro.html</span></a>.</div>
<p class="CDPAlignLeft CDPAlign">The following diagram shows the machine learning class hierarchy:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-963 image-border" src="assets/969f81a8-ad37-401b-9b90-f3c4dab87b48.png" style="width:162.50em;height:27.00em;"/></p>
<p>The <kbd><span class="CodeInTextPACKT">StatModel</span></kbd> class is the base class for all machine learning algorithms. This provides the prediction and all the read and write functions that are very important for saving and reading our machine learning parameters and training data.</p>
<p>In machine learning, the most time-consuming and computing resource-consuming part is the training method. Training can take from seconds to weeks or months for large datasets and complex machine learning structures. For example, in deep learning, big neural network structures with more than 100,000 image datasets can take a long time to train. With deep learning algorithms, it is common to use parallel hardware processing such as GPUs with CUDA technology to decrease the computing time during training, or most new chip devices such as Intel Movidius. This means that we cannot train our algorithm each time we run our application, and therefore it's recommended to save our trained model with all of the parameters that have been learned. In future executions, we only have to load/read from our saved model without training, except if we need to update our model with more sample data.</p>
<p><kbd><span class="CodeInTextPACKT">StatModel</span></kbd> is the base class of all machine learning classes, such as SVM or ANN, except deep learning methods. <kbd>StatModel</kbd> is basically a virtual class that defines the two most important functions—<kbd><span class="CodeInTextPACKT">train</span></kbd> and <kbd><span class="CodeInTextPACKT">predict</span></kbd>. The <kbd>train</kbd> method is the main method that's responsible for learning model parameters using a training dataset. This has the following three possible calls:</p>
<pre>bool train(const Ptr&lt;TrainData&gt;&amp; trainData, int flags=0 ); 
bool train(InputArray samples, int layout, InputArray responses); 
Ptr&lt;_Tp&gt; train(const Ptr&lt;TrainData&gt;&amp; data, int flags=0 ); </pre>
<p>The train function has the following parameters:</p>
<ul>
<li><kbd><span class="CodeInTextPACKT">TrainData</span></kbd>: Training data that can be loaded or created from the <kbd><span class="CodeInTextPACKT">TrainData</span></kbd> class. This class is new in OpenCV 3 and helps developers create training data and abstract from the machine learning algorithm. This is done because different algorithms require different types of structures of arrays for training and prediction, such as the ANN algorithm.</li>
<li><kbd><span class="CodeInTextPACKT">samples</span></kbd>: An array of training array samples such as training data in the format required by the machine learning algorithm.</li>
<li><span class="CodeInTextPACKT"><kbd>layout</kbd></span>: <kbd><span class="CodeInTextPACKT">ROW_SAMPLE</span></kbd> (training samples are the matrix rows) or <kbd><span class="CodeInTextPACKT">COL_SAMPLE</span></kbd> (training samples are the matrix columns).</li>
<li><kbd><span class="CodeInTextPACKT">responses</span></kbd>: Vector of responses associated with the sample data.</li>
<li><kbd>flags</kbd><span>: Optional flags defined by each method.</span></li>
</ul>
<p>The last train method creates and trains a model of the <kbd>_TP</kbd> class type. The only classes accepted are the classes that implement a static create method with no parameters or with all default parameter values.</p>
<p>The <kbd>predict</kbd> method is much simpler and has only one possible call:</p>
<pre>float StatModel::predict(InputArray samples, OutputArray results=noArray(), int flags=0) </pre>
<p>The predict function has the following parameters:</p>
<ul>
<li><kbd><span class="CodeInTextPACKT">samples</span></kbd>: The input samples to predict results from the model can consist of any amount of data, whether single or multiple.</li>
<li><kbd><span class="CodeInTextPACKT">results</span></kbd>: The results of each input row sample (computed by the algorithm from the previously trained model).</li>
<li><kbd><span class="CodeInTextPACKT">flags</span></kbd>: These optional flags are model-dependent. Some models, such as Boost, are recognized by the SVM <kbd><span class="CodeInTextPACKT">StatModel::RAW_OUTPUT</span></kbd> flag, which makes the method return the raw results (the sum), and not the class label.</li>
</ul>
<p class="mce-root"/>
<p>The <kbd>StatModel</kbd> class provides an interface for other very useful methods:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><kbd><span class="CodeInTextPACKT">isTrained()</span></kbd> returns true if the model is trained</li>
<li><kbd><span class="CodeInTextPACKT">isClassifier()</span></kbd> returns true if the model is a classifier, or false in the case of regression</li>
<li><kbd><span class="CodeInTextPACKT">getVarCount()</span></kbd> returns the number of variables in training samples</li>
<li><kbd><span class="CodeInTextPACKT">save(const string&amp; filename)</span></kbd> saves the model in the filename</li>
<li><kbd><span class="CodeInTextPACKT">Ptr&lt;_Tp&gt; load(const string&amp; filename)</span></kbd> loads the <kbd>&lt;indexentry content="StatModel class:Ptr load(const string&amp; filename)"&gt;</kbd> model from a filename, for example—<span class="CodeInTextPACKT"><kbd>Ptr&lt;SVM&gt; svm = StatModel::load&lt;SVM&gt;("my_svm_model.xml")</kbd></span></li>
<li><kbd><span class="CodeInTextPACKT">calcError(const Ptr&lt;TrainData&gt;&amp; data, bool test, OutputArray resp)</span></kbd> calculates the error from test data, where the <span class="CodeInTextPACKT">data</span> is the training data. If the test parameter is true, the method calculates the error from a test subset of data; if its false, the method calculates the error from all training data. <kbd>resp</kbd> is the optional output result.</li>
</ul>
</li>
</ul>
<p>Now, we are going to introduce how a basic application that uses machine learning in a computer vision application <span>is constructed</span><span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computer vision and the machine learning workflow </h1>
                </header>
            
            <article>
                
<p>Computer vision applications with machine learning have a common basic structure. This structure is divided into different steps:</p>
<ol>
<li><strong>Pre-process</strong></li>
<li><strong>Segmentation</strong></li>
<li><strong>Feature extraction</strong></li>
<li><strong>Classification result</strong></li>
<li><strong>P</strong><strong>ost-process</strong></li>
</ol>
<p>These are<span> common</span> in almost all computer vision applications<span>, while others are omitted. In the following diagram, you can see the different steps that are involved:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-964 image-border" src="assets/521ce802-ce25-4153-bd62-596f5840e17a.png" style="width:25.58em;height:26.25em;"/></div>
<p>Almost all computer vision applications start with a <strong>Pre-process</strong> applied to the input image, which consists of the removal of light and noise, filtering, blurring, and so on. After applying all pre-processing required to the input image, the second step is <strong>Segmentation</strong>. In this step, we have to extract the regions of interest in the image and isolate each one as a unique object of interest. For example, in a face detection system, we have to separate the faces from the rest of the parts in the scene. After detecting the objects inside the image, we continue to the next step. Here, we have to extract the features of each one; the features are normally <span>a vector of characteristics of objects. A characteristic describes our objects and can be the area of an object, contour, texture pattern, pixels, and so on.</span></p>
<p>Now, we have the descriptor, also known as a feature vector or feature set, of our object. Descriptors are the features that describe an object, and we use these to train or predict a model. To do this, we have to create a large dataset of features where thousands of images are pre-processed. We then use the extracted features (image/object characteristics) such as area, size, and aspect ration, in the <strong>Train</strong> model function we choose. In the following diagram, we can see how a dataset is fed into a <strong>Machine Learning Algorithm</strong> to train and <strong>generate</strong> a <strong>Model</strong>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-965 image-border" src="assets/ff9a0fe6-976b-48b7-aaf7-bed6afbe594a.png" style="width:36.92em;height:19.50em;"/></div>
<p>When we <strong>Train</strong> with a dataset, the <strong>Model</strong> learns all the parameters required to be able to predict when a new vector of features with an unknown label is given as input to our algorithm. In the following diagram, we can see how an unknown vector of features is used to <strong>Predict</strong> using the generated <strong>Model</strong>, thus returning the <strong>Classification<span> result</span></strong><span> </span>or regression:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-966 image-border" src="assets/ddc7b6bc-9708-454e-9738-f97959badc3f.png" style="width:37.00em;height:25.25em;"/></div>
<p>After predicting the result, the post-processing of output data <span>is sometimes required, f</span><span>or example, merging multiple classifications to decrease the prediction error or merging multiple labels. A sample case in Optical Character recognition is where the <strong>Classification result</strong> is according to each predicted character, and by combining the results of character recognition, we construct a word. This means that we can create a post-processing method to correct errors in detected words. With this small introduction to machine learning for computer vision, we are going to implement our own application that uses machine learning to classify objects in a slide tape. We are going to use support vector machines as our classification method and explain how to use them. The other machine learning algorithms are used in a very similar way. The OpenCV documentation has detailed information about all of the machine learning algorithms at the following link: <a href="https://docs.opencv.org/master/dd/ded/group__ml.html">https://docs.opencv.org/master/dd/ded/group__ml.html</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automatic object inspection classification example</h1>
                </header>
            
            <article>
                
<p>In <a href="b788527c-5892-4547-8add-0864ccbd3f95.xhtml">Chapter 5</a>, <em>Automated Optical Inspection, Object Segmentation, and Detection</em>, we looked at an example of automatic object inspection segmentation where a carrier tape contained three different types of object: nuts, screws, and rings. With computer vision, we will be able to recognize each one of these so that we can send notifications to a robot or put each one in a different box. The following is a basic diagram of the carrier tape:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8e9e15e5-03f9-44fa-a97e-16d176e4c291.png" style="width:45.00em;height:36.25em;"/></div>
<p>In <a href="b788527c-5892-4547-8add-0864ccbd3f95.xhtml"><span class="ChapterrefPACKT">Chapter 5</span></a><span class="ChapterrefPACKT">, <span><em>Automated Optical Inspection</em>, <em>Object Segmentation</em>, <em>and Detection</em>,</span></span> we pre-processed <span>the input images and extracted the regions of interest, isolating each object using different techniques. Now, we are going to apply all the concepts we explained in the previous sections in this example to extract features and classify each object, allowing the robot to put each one in a different box. In our application, we are only going to show the labels of each image, but we could send </span><span>the positions in the image and the label </span><span>to other devices, such as a robot. At this point, our goal is to give an input image with different objects, allowing the computer to detect the objects and show the objects' names over each image, as demonstrated in the following images. However, to learn the steps of the whole process, we are going to train our system by</span><span> creating a plot to show</span> the feature distribution that <span>we are going to use, and visualize it with</span><span> different colors. We will also show </span><span>the pre-processed input image, and the output classification result obtained. The final result looks as follows:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-967 image-border" src="assets/58162061-407f-44aa-98c5-941e9a0a5ee7.png" style="width:100.00em;height:58.33em;"/></div>
<p>We are going to follow these steps for our example application:</p>
<ol>
<li>
<p>For each input image:</p>
<ul>
<li>Preprocess the image</li>
<li>Segment the image</li>
</ul>
</li>
<li>For each object in an image:
<ul>
<li><span>Extract the features</span></li>
<li><span>Add the features to the training feature vector with a corresponding label (nut, screw, ring)</span></li>
</ul>
</li>
<li>Create an SVM model.</li>
<li>Train our SVM model with the training feature vector.</li>
<li>Preprocess the input image to classify each segmented object.</li>
<li>Segment the input image.</li>
<li>For each object detected:
<ul>
<li>Extract the features</li>
<li>Predict it with the SVM</li>
<li>model</li>
<li>Paint the result in the output image </li>
</ul>
</li>
</ol>
<p>For pre-processing and segmentation, we are going to use the code found in <a href="b788527c-5892-4547-8add-0864ccbd3f95.xhtml">Chapter 5</a>, <span><em>Automated Optical Inspection</em>, <em>Object Segmentation</em>, <em>and Detection.</em> W</span>e are then going to explain how to extract the features and create the vectors required to <strong>train</strong> and <strong>predict</strong> our model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature extraction</h1>
                </header>
            
            <article>
                
<p>The next thing we need to do is extract the features for each object. To understand the feature vector concept, we are going to extract very simple features in our example, as this is enough to get good results. In other solutions, we can get more complex features such as texture descriptors, contour descriptors, and so on. In our example, we only have nuts, rings, and screws in different positions and orientations in the image. The same object can be in any position of image and orientation, for example, the screw or the nut. We can see different orientations in the following image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span><img src="assets/79ebe946-48c9-4941-8750-df622f1880ae.png" style="width:18.00em;height:13.50em;"/></span></div>
<p>We are going to explore some features or characteristics that could improve the accuracy of our machine learning algorithm. These possible characteristics of our different objects (nuts, screws, and rings) are as follows:</p>
<ul>
<li>The area of the object</li>
<li>The aspect ratio, that is, the width divided by the height of the bounding rectangle</li>
<li>The number of holes</li>
<li>The number of contour sides</li>
</ul>
<p>These characteristics can describe our objects <span>very well</span><span>, and if we use all of them, the classification error will be very small. However, in our implemented example, we are only going to use the first </span><span>two</span><span> </span><span>characteristics, area and aspect ratio, for learning purposes, because we can plot these characteristics in a 2D graphic and show that these values </span><span>correctly</span><span> </span><span>describe our objects. We can also show that we can </span><span>visually</span><span> </span><span>differentiate between one kind of object and another in the graphic plot. To extract these features, we are going to use the black/white ROI image </span><span>as input, </span><span>where only one object appears in white with a black background. This input</span> is the segmentation result of <a href="b788527c-5892-4547-8add-0864ccbd3f95.xhtml">Chapter 5</a><span>,</span> <span><em>Automated Optical Inspection</em>, <em>Object Segmentation</em>, <em>and Detection</em></span><span>. W</span><span>e are going to use the</span> <kbd><span class="CodeInTextPACKT">findCountours</span></kbd> <span>algorithm for segmenting objects and create the</span> <kbd><span class="CodeInTextPACKT">ExtractFeatures</span></kbd> <span>function for this purpose, as we can see in the following code:</span></p>
<pre>vector&lt; vector&lt;float&gt; &gt; ExtractFeatures(Mat img, vector&lt;int&gt;* left=NULL, vector&lt;int&gt;* top=NULL) 
{ 
  vector&lt; vector&lt;float&gt; &gt; output; 
  vector&lt;vector&lt;Point&gt; &gt; contours; 
  Mat input= img.clone(); 
   
  vector&lt;Vec4i&gt; hierarchy; 
  findContours(input, contours, hierarchy, RETR_CCOMP, CHAIN_APPROX_SIMPLE); 
  // Check the number of objects detected 
  if(contours.size() == 0){ 
    return output; 
  } 
  RNG rng(0xFFFFFFFF); 
  for(auto i=0; i&lt;contours.size(); i++){ 
     
    Mat mask= Mat::zeros(img.rows, img.cols, CV_8UC1); 
    drawContours(mask, contours, i, Scalar(1), FILLED, LINE_8, hierarchy, 1); 
    Scalar area_s= sum(mask); 
    float area= area_s[0]; 
 
    if(area&gt;500){ //if the area is greater than min. 
       
      RotatedRect r= minAreaRect(contours[i]); 
      float width= r.size.width; 
      float height= r.size.height; 
      float ar=(width&lt;height)?height/width:width/height; 
 
      vector&lt;float&gt; row; 
      row.push_back(area); 
      row.push_back(ar); 
      output.push_back(row); 
      if(left!=NULL){ 
          left-&gt;push_back((int)r.center.x); 
      } 
      if(top!=NULL){ 
          top-&gt;push_back((int)r.center.y); 
      } 
      <br/>      // Add image to the multiple image window class, See the class on full github code   <br/>      miw-&gt;addImage("Extract Features", mask*255); 
      miw-&gt;render(); 
      waitKey(10); 
    } 
  } 
  return output; 
} </pre>
<p>Let's explain the code that we use to extract features. We are going to create a function that has one image <span>as input a</span><span>nd return</span><span> two vectors of the left and top position for each object detected in the image as a parameter. This data will be used for drawing </span><span>the corresponding label </span><span>over each object</span><span>. The output of a function is a vector of vectors of floats. In other words, it is a matrix where each row contains the features of each object that's detected.</span></p>
<p>First, we have to create the output vector variable and the contours variable that are going to be used in our find contours algorithm segmentation. We also have to create a copy of our input image, because the <kbd><span class="CodeInTextPACKT">findCoutours</span></kbd> OpenCV functions modify the input image:</p>
<pre>  vector&lt; vector&lt;float&gt; &gt; output; 
  vector&lt;vector&lt;Point&gt; &gt; contours; 
  Mat input= img.clone(); 
  vector&lt;Vec4i&gt; hierarchy; 
  findContours(input, contours, hierarchy, RETR_CCOMP, CHAIN_APPROX_SIMPLE); </pre>
<p>Now, we can use the <kbd><span class="CodeInTextPACKT">findContours</span></kbd> function to retrieve each object in an image. If we don't detect any contour, we return an empty output matrix, as we can see in the following snippet:</p>
<pre>if(contours.size() == 0){ 
    return output; 
  } </pre>
<p>If <span>objects</span><span> </span><span>are detected, for each contour we are going to draw the object in white on a black image (zero values). This will be done using</span> <kbd>1</kbd> <span>values, like a mask image. The following piece of code generates the mask image:</span></p>
<pre>for(auto i=0; i&lt;contours.size(); i++){ 
    Mat mask= Mat::zeros(img.rows, img.cols, CV_8UC1); 
    drawContours(mask, contours, i, Scalar(1), FILLED, LINE_8, hierarchy, 1); </pre>
<p>It's important to use the value of <kbd>1</kbd> to draw inside the shape because we can calculate the area by summing all of the values inside the contour, as shown in the following code:</p>
<pre>    Scalar area_s= sum(mask); 
    float area= area_s[0]; </pre>
<p>This area is our first feature. We are going to use this value as a filter to remove all possible small objects that we have to avoid. All objects with an area less than the minimum threshold area that we considered will be discarded. After passing the filter, we create the second feature and the aspect ratio of the object. This refers to the maximum of the width or height, divided by the minimum of the width or height. This feature can tell the difference between the screw and other objects easily. The following code describes how to calculate the aspect ratio:</p>
<pre>if(area&gt;MIN_AREA){ //if the area is greater than min. 
      RotatedRect r= minAreaRect(contours[i]); 
      float width= r.size.width; 
      float height= r.size.height; 
      float ar=(width&lt;height)?height/width:width/height; </pre>
<p>Now we have the features, we only have to add them to the output vector. To do this, we will create a row vector of floats and add the values, followed by adding this row to the output vector, as shown in the following code:</p>
<pre>vector&lt;float&gt; row; 
row.push_back(area); 
row.push_back(ar); 
output.push_back(row);</pre>
<p class="mce-root"/>
<p>If the left and top parameters are passed, then add the top-left values to output the parameters:</p>
<pre>  if(left!=NULL){ 
      left-&gt;push_back((int)r.center.x); 
  }<br/>  if(top!=NULL){ 
      top-&gt;push_back((int)r.center.y); 
  } </pre>
<p>Finally, we are going to show the detected objects in a window for user feedback. When we finish processing all of the objects in the image, we are going to return the output feature vector, as described in the following code snippet:</p>
<pre>      miw-&gt;addImage("Extract Features", mask*255); 
      miw-&gt;render(); 
      waitKey(10); 
    } 
  } 
  return output; </pre>
<p>Now that we have extracted the features of each input image, we can continue with the next step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training an SVM model</h1>
                </header>
            
            <article>
                
<p>We are now going to use supervised learning and then obtain a set of images for each object and its corresponding label. There is no minimum number of images in the dataset; if we provide more images for the training process, we will get a better classification model (in most cases). However, for simple classifiers, it could be enough to train simple models. To do this, we created three folders (<kbd>screw</kbd>, <kbd>nut</kbd>, and <kbd>ring</kbd>), where all of the images of each type are placed together. For each image in the folder, we have to extract the features, add them to the <kbd>train</kbd> feature matrix and, at the same time, create a new vector with the labels for each row corresponding to each training matrix. To evaluate our system, we will split each folder into a number of images according to testing and training. We will leave around 20 images for testing and the others for training. We are then going to create two vectors of labels and two matrices for training and testing.</p>
<p>Let's go inside of our code. First, we have to create our model. We are going to declare the model out of all functions to be able to gain access to it as a global variable. OpenCV uses the <kbd><span class="CodeInTextPACKT">Ptr</span></kbd> template class for pointer management:</p>
<pre>Ptr&lt;SVM&gt; svm;</pre>
<p>After declaring the pointer to the new SVM model, we are going to create it and train it. We created the <kbd><span class="CodeInTextPACKT">trainAndTest</span></kbd> function for this purpose. The complete function code is as follows:</p>
<pre>void trainAndTest() 
{ 
  vector&lt; float &gt; trainingData; 
  vector&lt; int &gt; responsesData; 
  vector&lt; float &gt; testData; 
  vector&lt; float &gt; testResponsesData; 
 
  int num_for_test= 20; 
 
  // Get the nut images 
  readFolderAndExtractFeatures("../data/nut/nut_%04d.pgm", 0, num_for_test, trainingData, responsesData, testData, testResponsesData); 
  // Get and process the ring images 
  readFolderAndExtractFeatures("../data/ring/ring_%04d.pgm", 1, num_for_test, trainingData, responsesData, testData, testResponsesData); 
  // get and process the screw images 
  readFolderAndExtractFeatures("../data/screw/screw_%04d.pgm", 2, num_for_test, trainingData, responsesData, testData, testResponsesData); 
   
  cout &lt;&lt; "Num of train samples: " &lt;&lt; responsesData.size() &lt;&lt; endl; 
 
  cout &lt;&lt; "Num of test samples: " &lt;&lt; testResponsesData.size() &lt;&lt; endl; 
   
  // Merge all data  
  Mat trainingDataMat(trainingData.size()/2, 2, CV_32FC1, &amp;trainingData[0]); 
  Mat responses(responsesData.size(), 1, CV_32SC1, &amp;responsesData[0]); 
 
  Mat testDataMat(testData.size()/2, 2, CV_32FC1, &amp;testData[0]); 
  Mat testResponses(testResponsesData.size(), 1, CV_32FC1, &amp;testResponsesData[0]); <br/>  <br/><span class="pl-c1">  Ptr</span><span>&lt;TrainData&gt; tdata= </span><span class="pl-c1">TrainData::create</span><span>(trainingDataMat, ROW_SAMPLE, responses);</span><br/><br/>  svm = cv::ml::SVM::create();<br/>  svm-&gt;setType(cv::ml::SVM::C_SVC);<br/>  svm-&gt;setNu(0.05); <br/>  svm-&gt;setKernel(cv::ml::SVM::CHI2);<br/>  svm-&gt;setDegree(1.0);<br/>  svm-&gt;setGamma(2.0);<br/>  svm-&gt;setTermCriteria(TermCriteria(TermCriteria::MAX_ITER, 100, 1e-6));<br/>  svm-&gt;train(tdata); 
 
  if(testResponsesData.size()&gt;0){ 
    cout &lt;&lt; "Evaluation" &lt;&lt; endl; 
    cout &lt;&lt; "==========" &lt;&lt; endl; 
    // Test the ML Model 
    Mat testPredict; 
    svm-&gt;predict(testDataMat, testPredict); 
    cout &lt;&lt; "Prediction Done" &lt;&lt; endl; 
    // Error calculation 
    Mat errorMat= testPredict!=testResponses; 
    float error= 100.0f * countNonZero(errorMat) / testResponsesData.size(); 
    cout &lt;&lt; "Error: " &lt;&lt; error &lt;&lt; "%" &lt;&lt; endl; 
    // Plot training data with error label 
    plotTrainData(trainingDataMat, responses, &amp;error); 
 
  }else{ 
    plotTrainData(trainingDataMat, responses); 
  } 
} </pre>
<p>Now, let's explain the code. First of all, we have to create the required variables to store the training and testing data:</p>
<pre>  vector&lt; float &gt; trainingData; 
  vector&lt; int &gt; responsesData; 
  vector&lt; float &gt; testData; 
  vector&lt; float &gt; testResponsesData; </pre>
<p>As we mentioned previously, we have to read all of the images from each folder, extract the features, and save them in our training and testing data. To do this, we are going to use the <kbd><span class="CodeInTextPACKT">readFolderAndExtractFeatures</span></kbd> function, as follows:</p>
<pre>  int num_for_test= 20; 
  // Get the nut images 
  readFolderAndExtractFeatures("../data/nut/tuerca_%04d.pgm", 0, num_for_test, trainingData, responsesData, testData, testResponsesData); 
  // Get and process the ring images 
  readFolderAndExtractFeatures("../data/ring/arandela_%04d.pgm", 1, num_for_test, trainingData, responsesData, testData, testResponsesData); 
  // get and process the screw images 
  readFolderAndExtractFeatures("../data/screw/tornillo_%04d.pgm", 2, num_for_test, trainingData, responsesData, testData, testResponsesData); </pre>
<p>The <kbd><span class="CodeInTextPACKT">readFolderAndExtractFeatures</span></kbd> function uses the <kbd><span class="CodeInTextPACKT">VideoCapture</span></kbd> OpenCV function to read all of the images in a folder, including videos and camera frames. For each image that's read, we extract the features and add them to the corresponding output vector:</p>
<pre>bool readFolderAndExtractFeatures(string folder, int label, int num_for_test,  
  vector&lt;float&gt; &amp;trainingData, vector&lt;int&gt; &amp;responsesData,   
  vector&lt;float&gt; &amp;testData, vector&lt;float&gt; &amp;testResponsesData) 
{ 
  VideoCapture images; 
  if(images.open(folder)==false){ 
    cout &lt;&lt; "Can not open the folder images" &lt;&lt; endl; 
    return false; 
  } 
  Mat frame; 
  int img_index=0; 
  while(images.read(frame)){ 
    //// Preprocess image 
    Mat pre= preprocessImage(frame); 
    // Extract features 
    vector&lt; vector&lt;float&gt; &gt; features= ExtractFeatures(pre); 
    for(int i=0; i&lt; features.size(); i++){ 
      if(img_index &gt;= num_for_test){ 
        trainingData.push_back(features[i][0]); 
        trainingData.push_back(features[i][1]); 
        responsesData.push_back(label);     
      }else{ 
        testData.push_back(features[i][0]); 
        testData.push_back(features[i][1]); 
        testResponsesData.push_back((float)label);     
      } 
    } 
    img_index++; 
  } 
  return true;   
} </pre>
<p>After filling all of the vectors with features and labels, we have to convert from vectors to an OpenCV <kbd><span class="CodeInTextPACKT">Mat</span></kbd> format so that we can send it to the training function:</p>
<pre>// Merge all data  
Mat trainingDataMat(trainingData.size()/2, 2, CV_32FC1, &amp;trainingData[0]); 
Mat responses(responsesData.size(), 1, CV_32SC1, &amp;responsesData[0]); 
Mat testDataMat(testData.size()/2, 2, CV_32FC1, &amp;testData[0]); 
Mat testResponses(testResponsesData.size(), 1, CV_32FC1, &amp;testResponsesData[0]); </pre>
<p>Now, we are ready to create and train our machine learning model. As we stated previously, we are going to use the support vector machine for this. First, we are going to set up the basic model parameters, as follows:</p>
<pre>// Set up SVM's parameters 
svm = cv::ml::SVM::create();<br/>svm-&gt;setType(cv::ml::SVM::C_SVC);<br/>svm-&gt;setNu(0.05);<br/>svm-&gt;setKernel(cv::ml::SVM::CHI2);<br/>svm-&gt;setDegree(1.0);<br/>svm-&gt;setGamma(2.0);<br/>svm-&gt;setTermCriteria(TermCriteria(TermCriteria::MAX_ITER, 100, 1e-6));</pre>
<p>We are now going to define the SVM type and kernel to use, as well as the criteria to stop the learning process. In our case, we are going to use a number of maximum iterations, stopping at 100 iterations. For more information about each parameter and what it does, check the OpenCV documentation at the following link:<a href="https://docs.opencv.org/master/d1/d2d/classcv_1_1ml_1_1SVM.html"> https://docs.opencv.org/master/d1/d2d/classcv_1_1ml_1_1SVM.html</a>. After creating the <span>setup </span><span>parameters, we are going to create the model by calling the</span> <kbd>train</kbd> <span>method and using</span> <kbd><span class="CodeInTextPACKT">trainingDataMat</span></kbd> <span>and</span> <span class="CodeInTextPACKT">response</span><span> matrices as a <kbd>TrainData</kbd> object:</span></p>
<pre>  // Train the SVM 
  svm-&gt;train(tdata); </pre>
<p>We use the test vector (setting the <kbd><span class="CodeInTextPACKT">num_for_test</span></kbd> variable to greater than <kbd>0</kbd>) to obtain an approximation error of our model. To get the error estimation, we are going to predict all test vector features to obtain the SVM prediction results and compare these results to the original labels:</p>
<pre>if(testResponsesData.size()&gt;0){ 
    cout &lt;&lt; "Evaluation" &lt;&lt; endl; 
    cout &lt;&lt; "==========" &lt;&lt; endl; 
    // Test the ML Model 
    Mat testPredict; 
    svm-&gt;predict(testDataMat, testPredict); 
    cout &lt;&lt; "Prediction Done" &lt;&lt; endl; 
    // Error calculation 
    Mat errorMat= testPredict!=testResponses; 
    float error= 100.0f * countNonZero(errorMat) / testResponsesData.size(); 
    cout &lt;&lt; "Error: " &lt;&lt; error &lt;&lt; "%" &lt;&lt; endl; 
    // Plot training data with error label 
    plotTrainData(trainingDataMat, responses, &amp;error); 
 
  }else{ 
    plotTrainData(trainingDataMat, responses); 
  } </pre>
<p>We use the <kbd><span class="CodeInTextPACKT">predict</span></kbd> function by using the <kbd><span class="CodeInTextPACKT">testDataMat</span></kbd> features and a new <kbd><span class="CodeInTextPACKT">Mat</span></kbd> for prediction results. The <kbd>predict</kbd> function makes it possible to make multiple predictions at the same time, giving a matrix as the result instead of only one row or vector. After prediction, we only have to compute the differences of <kbd><span class="CodeInTextPACKT">testPredict</span></kbd> with our <kbd><span class="CodeInTextPACKT">testResponses</span></kbd> (the original labels). If there are differences, we only have to count how many there are and divide this by the total number of tests in order to calculate the error.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="packt_infobox">We can use the new <kbd><span class="CodeInTextPACKT">TrainData</span></kbd> class to generate the feature vectors, samples, and split our train data between test and train vectors.</div>
<p>Finally, we are going to show the training data in a 2D plot, where the <em>y</em>-axis is the aspect ratio feature and the <em>x</em>-axis is the area of objects. Each point has different colors and shapes (cross, square, and circle) that show each different kind of object, and we can <span>clearly</span><span> see the groups of objects in the following image:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span><img src="assets/c0d5aeff-d753-4785-a611-f29df5d40c3f.png" style="width:26.08em;height:26.08em;"/></span></div>
<p>We are now very close to finishing our application sample. At this point, we have trained the SVM model; we can now use it for classification to detect the type of a new incoming and unknown feature vector. The next step is to predict an input image with unknown objects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Input image prediction</h1>
                </header>
            
            <article>
                
<p>We are now ready to explain the main function, which loads the input image and predicts the objects that appear inside it. We are going to use something like the following picture <span>as the input image.</span> <span>Here,</span><span> multiple different </span><span>objects </span><span>appear</span><span> i</span><span>n the image. We did not have the labels or names of these, but the computer must be able to identify them:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span><img src="assets/4fda7040-c34d-4631-a829-c029286088f7.png" style="width:19.92em;height:14.92em;"/></span></div>
<p>As with all training images, we have to load and pre-process the input image, as follows:</p>
<ol>
<li>First, we load and convert the image into gray color values.</li>
<li>Then, we apply the pre-processing tasks (as we learned in <a href="b788527c-5892-4547-8add-0864ccbd3f95.xhtml">Chapter 5</a>, <span><em>Automated Optical Inspect</em>ion, <em>Object Segmentation</em>, <em>and Detection)</em> </span>using the <kbd><span class="CodeInTextPACKT">preprocessImage</span></kbd> function:</li>
</ol>
<pre>    Mat pre= preprocessImage(img); </pre>
<ol start="3">
<li>Now, we are going to extract the feature of vectors for all objects that appear in the image and the top-left positions of each one by using the <kbd><span class="CodeInTextPACKT">ExtractFeatures</span></kbd> that we previously described:</li>
</ol>
<pre>    // Extract features <br/>    vector&lt;int&gt; pos_top, pos_left; <br/>    vector&lt; vector&lt;float&gt; &gt;<br/>    features=ExtractFeatures(pre, &amp;pos_left,     &amp;pos_top); </pre>
<ol start="4">
<li>We store each object we detect as a feature row and then convert each row as a <kbd>Mat</kbd> of one row and two features:</li>
</ol>
<pre>     for(int i=0; i&lt; features.size(); i++){ 
         Mat trainingDataMat(1, 2, CV_32FC1, &amp;features[i][0]);</pre>
<ol start="5">
<li>After this, we can predict the single object using the <kbd>predict</kbd> function of our <kbd><span class="CodeInTextPACKT">StatModel</span></kbd> SVM. <span>The float result of the prediction is the label of the object detected. Then, to finish the application, we have to draw the label of each object that's detected and classified over the output image:</span></li>
</ol>
<pre>     float result= svm-&gt;predict(trainingDataMat); </pre>
<ol start="6">
<li>We are going to use a <kbd><span class="CodeInTextPACKT">stringstream</span></kbd> to store the text and a <kbd><span class="CodeInTextPACKT">Scalar</span></kbd> to store the color for each different label:</li>
</ol>
<pre>     stringstream ss; 
     Scalar color; 
     if(result==0){ 
       color= green; // NUT 
       ss &lt;&lt; "NUT"; 
     }else if(result==1){ 
       color= blue; // RING 
       ss &lt;&lt; "RING" ; 
     }else if(result==2){ 
       color= red; // SCREW 
       ss &lt;&lt; "SCREW"; 
     } </pre>
<ol start="7">
<li>We are also going to draw the label text over each object using its detected position in the <kbd><span class="CodeInTextPACKT">ExtractFeatures</span></kbd> function:</li>
</ol>
<pre>     putText(img_output,  
           ss.str(),  
           Point2d(pos_left[i], pos_top[i]),  
           FONT_HERSHEY_SIMPLEX,  
           0.4,  
           color); </pre>
<ol start="8">
<li>Finally, we are going to draw our results in the output window:</li>
</ol>
<pre>       miw-&gt;addImage("Binary image", pre); 
       miw-&gt;addImage("Result", img_output); 
       miw-&gt;render(); 
       waitKey(0); </pre>
<p>The final result of our application shows a window tiled with four screens. Here, the top-left image is the input training image, the top-right is the plot training image, the bottom left is the input image to analyze pre-processed images, and the bottom-right is the final result of the prediction:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-968 image-border" src="assets/162298b6-6d4e-42e0-8019-c7467243b7fc.png" style="width:100.00em;height:58.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about the basics of machine learning and applied them to a small sample application. This allowed us to understand the basic techniques that we can use to create our own machine learning application. Machine learning is complex and involves different techniques for each use case (supervised learning, unsupervised, clustering, and so on). We also learned how to create the most typical machine learning application, the supervised learning application, with SVM. The most important concepts in supervised machine learning are as follows: you must have an appropriate number of samples or a dataset, you must accurately choose the features that describe our objects (for more information on image features, go to <a href="58a72603-be5a-465f-aa7b-fc8ab1aae596.xhtml">Chapter 8</a>, <em>Video Surveillance</em>, <em>Background Modeling</em>, <em>and Morphological Operations</em>)<em>,</em> and you must choose a model that gives the best predictions.</p>
<p>If we don't get the correct predictions, we have to check each one of these concepts to find the issue.</p>
<p>In the next chapter, we are going to introduce background subtraction methods, which are very useful for video surveillance applications where the background doesn't give us any interesting information and must be discarded so that we can segment the image to detect and analyze the image objects.</p>


            </article>

            
        </section>
    </body></html>