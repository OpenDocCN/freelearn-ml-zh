["```py\n    def all_moves_from_board(board, sign):\n        move_list = []\n        for i, v in enumerate(board):\n            if v == EMPTY_SIGN:\n                move_list.append(board[:i] + sign + board[i+1:])\n        return move_list\n    ```", "```py\n    all_moves_from_board_list( [ EMPTY_SIGN * 9 ], AI_SIGN )\n    ```", "```py\n     ['X........',\n     '.X.......',\n     '..X......',\n     '...X.....',\n     '....X....',\n     '.....X...',\n     '......X..',\n     '.......X.',\n     '........X']\n     ['XO.......',\n     'X.O......',\n     'X..O.....',\n     'X...O....',\n     'X....O...',\n     'X.....O..',\n     'X......O.',\n    .\n    .\n    .\n    .\n    '......OX.',\n     '.......XO',\n     'O.......X',\n     '.O......X',\n     '..O.....X',\n     '...O....X',\n     '....O...X',\n     '.....O..X',\n     '......O.X',\n     '.......OX']\n    ```", "```py\n    def filter_wins(move_list, ai_wins, opponent_wins):\n        for board in move_list:\n            won_by = game_won_by(board)\n            if won_by == AI_SIGN:\n                ai_wins.append(board)\n                move_list.remove(board)\n            elif won_by == OPPONENT_SIGN:\n                opponent_wins.append(board)\n                move_list.remove(board)\n    ```", "```py\n    def count_possibilities():\n        board = EMPTY_SIGN * 9\n        move_list = [board]\n        ai_wins = []\n        opponent_wins = []\n        for i in range(9):\n            print('step ' + str(i) + '. Moves: ' + \\        str(len(move_list)))\n            sign = AI_SIGN if i % 2 == 0 else OPPONENT_SIGN\n            move_list = all_moves_from_board_list(move_list, sign)\n            filter_wins(move_list, ai_wins, opponent_wins)\n        print('First player wins: ' + str(len(ai_wins)))\n        print('Second player wins: ' + str(len(opponent_wins)))\n        print('Draw', str(len(move_list)))\n        print('Total', str(len(ai_wins) + len(opponent_wins) + \\    len(move_list)))\n    ```", "```py\n    count_possibilities()\n    ```", "```py\n    step 0\\. Moves: 1\n    step 1\\. Moves: 9\n    step 2\\. Moves: 72\n    step 3\\. Moves: 504\n    step 4\\. Moves: 3024\n    step 5\\. Moves: 13680\n    step 6\\. Moves: 49402\n    step 7\\. Moves: 111109\n    step 8\\. Moves: 156775\n    First player wins: 106279\n    Second player wins: 68644\n    Draw 91150\n    Total 266073\n    ```", "```py\n    def player_can_win(board, sign):\n        next_moves = all_moves_from_board(board, sign)\n        for next_move in next_moves:\n            if game_won_by(next_move) == sign:\n                return True\n        return False\n    ```", "```py\n    def ai_move(board):\n        new_boards = all_moves_from_board(board, AI_SIGN)\n        for new_board in new_boards:\n            if game_won_by(new_board) == AI_SIGN:\n                return new_board\n        safe_moves = []\n        for new_board in new_boards:\n            if not player_can_win(new_board, OPPONENT_SIGN):\n                safe_moves.append(new_board)\n        return choice(safe_moves) if len(safe_moves) > 0 else \\        new_boards[0]\n    ```", "```py\n    def all_moves_from_board( board, sign ):\n    ```", "```py\n    def all_moves_from_board(board, sign):\n        move_list = []\n        for i, v in enumerate(board):\n            if v == EMPTY_SIGN:\n                new_board = board[:i] + sign + board[i+1:]\n                move_list.append(new_board)\n                if game_won_by(new_board) == AI_SIGN:\n                    return [new_board]\n        if sign == AI_SIGN:\n            safe_moves = []\n            for move in move_list:\n                if not player_can_win(move, OPPONENT_SIGN):\n                    safe_moves.append(move)\n            return safe_moves if len(safe_moves) > 0 else \\            move_list[0:1]\n        else:\n            return move_list\n    ```", "```py\n    count_possibilities()\n    ```", "```py\n    step 0\\. Moves: 1\n    step 1\\. Moves: 9\n    step 2\\. Moves: 72\n    step 3\\. Moves: 504\n    step 4\\. Moves: 3024\n    step 5\\. Moves: 5197\n    step 6\\. Moves: 18606\n    step 7\\. Moves: 19592\n    step 8\\. Moves: 30936\n    ```", "```py\n    First player wins: 20843\n    Second player wins: 962\n    Draw 20243\n    Total 42048\n    ```", "```py\n    def all_moves_from_board(board, sign):\n        if sign == AI_SIGN:\n            empty_field_count = board.count(EMPTY_SIGN)\n            if empty_field_count == 9:\n                return [sign + EMPTY_SIGN * 8]\n            elif empty_field_count == 7:\n                return [\n                    board[:8] + sign if board[8] == \\                    EMPTY_SIGN else\n                    board[:4] + sign + board[5:]\n                ]\n        move_list = []\n        for i, v in enumerate(board):\n            if v == EMPTY_SIGN:\n                new_board = board[:i] + sign + board[i+1:]\n                move_list.append(new_board)\n                if game_won_by(new_board) == AI_SIGN:\n                    return [new_board]\n        if sign == AI_SIGN:\n    ```", "```py\n            safe_moves = []\n            for move in move_list:\n                if not player_can_win(move, OPPONENT_SIGN):\n                    safe_moves.append(move)\n            return safe_moves if len(safe_moves) > 0 else \\            move_list[0:1]\n        else:\n            return move_list\n    ```", "```py\n    countPossibilities()\n    ```", "```py\n    step 0\\. Moves: 1\n    step 1\\. Moves: 1\n    step 2\\. Moves: 8\n    step 3\\. Moves: 8\n    step 4\\. Moves: 48\n    step 5\\. Moves: 38\n    step 6\\. Moves: 108\n    step 7\\. Moves: 76\n    step 8\\. Moves: 90\n    First player wins: 128\n    Second player wins: 0\n    Draw 60\n    ```", "```py\n    from easyAI import TwoPlayersGame\n    from easyAI.Player import Human_Player\n    class ConnectFour(TwoPlayersGame):\n        def __init__(self, players):\n            self.players = players\n        def possible_moves(self):\n            return []\n        def make_move(self, move):\n            return\n        def unmake_move(self, move):\n    # optional method (speeds up the AI)\n            return\n        def lose(self):\n            return False\n        def is_over(self):\n            return (self.possible_moves() == []) or self.lose()\n        def show(self):\n            print ('board')\n        def scoring(self):\n            return -100 if self.lose() else 0\n\n    if __name__ == \"__main__\":\n        from easyAI import AI_Player, Negamax\n        ai_algo = Negamax(6)\n    ```", "```py\n    __init__\n    possible_moves\n    make_move\n    unmake_move (optional)\n    lose\n    show\n    ```", "```py\n    def __init__(self, players):\n            self.players = players\n            # 0 1 2 3 4 5 6\n            # 7 8 9 10 11 12 13\n            # ...\n            # 35 36 37 38 39 40 41\n            self.board = [0 for i in range(42)]\n            self.nplayer = 1 # player 1 starts.\n            def generate_winning_tuples():\n                tuples = []\n                # horizontal\n                tuples += [\n                    list(range(row*7+column, row*7+column+4, 1))\n                    for row in range(6)\n                    for column in range(4)]\n                # vertical\n                tuples += [\n                    list(range(row*7+column, row*7+column+28, 7))\n                    for row in range(3)\n                    for column in range(7)\n                ]\n    ```", "```py\n                # diagonal forward\n                tuples += [\n                    list(range(row*7+column, row*7+column+32, 8))\n                    for row in range(3)\n                    for column in range(4)\n                ]\n                # diagonal backward\n                tuples += [\n                    list(range(row*7+column, row*7+column+24, 6))\n                    for row in range(3)\n                    for column in range(3, 7, 1)\n                ]\n                return tuples\n    self.tuples=generate_winning_tuples()\n    ```", "```py\n    def possible_moves(self):\n            return [column+1\n                    for column in range(7)\n                    if any([\n                        self.board[column+row*7] == 0\n                        for row in range(6)\n                    ])\n                    ]\n    ```", "```py\n    def make_move(self, move):\n            column = int(move) - 1\n            for row in range(5, -1, -1):\n                index = column + row*7\n                if self.board[index] == 0:\n                    self.board[index] = self.nplayer\n                    return\n        def unmake_move(self, move):\n    # optional method (speeds up the AI)\n            column = int(move) - 1\n            for row in range(6):\n                index = column + row*7\n                if self.board[index] != 0:\n                    self.board[index] = 0\n                    return\n    ```", "```py\n    def lose(self):\n            return any([all([(self.board[c] == self.nopponent)\n                             for c in line])\n                        for line in self.tuples])\n        def is_over(self):\n            return (self.possible_moves() == []) or self.lose()\n    ```", "```py\n    def show(self):\n            print('\\n'+'\\n'.join([\n                ' '.join([['.', 'O', 'X'][self.board[7*row+column]]\n                         for column in range(7)]\n                         )\n                for row in range(6)])\n            )\n    ```", "```py\nx = np.array(range(1, 19))\ny = np.array([\n    147026,\n    144272,\n    140020,\n    143801,\n    146233,\n    144539,\n    141273,\n    135389,\n    142500,\n    139452,\n    139722,\n    135300,\n    137289,\n    136511,\n    132884,\n    125683,\n    127255,\n    124275\n])\n```", "```py\n[a, b] = np.polyfit(x, y, 1)\n[-1142.0557275541753, 148817.5294117646]\n```", "```py\nimport matplotlib.pyplot as plot\nplot.scatter( x, y )\nplot.plot( [0, 30], [b, 30*a+b] )\nplot.show()\n```", "```py\nimport quandl\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom matplotlib import pyplot as plot\nfrom sklearn import svm\ndata_frame = quandl.get(\"YALE/SPCOMP\")\ndata_frame[['Long Interest Rate', 'Real Price',\n           'Real Dividend', 'Cyclically Adjusted PE Ratio']]\ndata_frame.fillna(-100, inplace=True)\n# We shift the price data to be predicted 20 years forward\ndata_frame['Real Price Label'] = data_frame['RealPrice'].shift(-240)\n# Then exclude the label column from the features\nfeatures = np.array(data_frame.drop('Real Price Label', 1))\n# We scale before dropping the last 240 rows from the features\nscaled_features = preprocessing.scale(features)\n# Save the last 240 rows before dropping them\nscaled_features_latest240 = scaled_features[-240:]\n# Exclude the last 240 rows from the data used for # # modelbuilding\nscaled_features = scaled_features[:-240]\n# Now we can drop the last 240 rows from the data frame\ndata_frame.dropna(inplace=True)\n# Then build the labels from the remaining data\nlabel = np.array(data_frame['Real Price Label'])\n# The rest of the model building stays\n(features_train,\n    features_test,\n    label_train,\n    label_test\n) = model_selection.train_test_split(\n    scaled_features,\n    label,\n   test_size=0.1\n)\n```", "```py\nmodel = linear_model.LinearRegression()\nmodel.fit(features_train, label_train)\nmodel.score(features_test, label_test)\n```", "```py\n     0.8978136465083912\n    ```", "```py\n    label_predicted = model.predict(features_test)\n    plot.plot(\n        label_test, label_predicted, 'o',\n        [0, 3000], [0, 3000]\n    )\n    ```", "```py\nIt is now time to perform a linear multiple regression with quadratic polynomials. The only change is in the Linear Regression model\npoly_regressor = PolynomialFeatures(degree=3)\npoly_scaled_features = poly_regressor.fit_transform(scaled_features)\n(poly_features_train,\n poly_features_test,\n poly_label_train,\n poly_label_test) = model_selection.train_test_split(\n    poly_scaled_features,\n    label,\n    test_size=0.1)\nmodel = linear_model.LinearRegression()\nmodel.fit(poly_features_train, poly_label_train)\nprint('Polynomial model score: ', model.score(\n    poly_features_test, poly_label_test))\nprint('\\n')\npoly_label_predicted = model.predict(poly_features_test)\nplot.plot(\n    poly_label_test, poly_label_predicted, 'o',\n    [0, 3000], [0, 3000]\n)\n```", "```py\nmodel = svm.SVR(kernel='poly')\nmodel.fit(features_train, label_train)\nlabel_predicted = model.predict(features_test)\nplot.plot(\n    label_test, label_predicted, 'o',\n    [0,3000], [0,3000]\n)\n```", "```py\nmodel.score(features_test, label_test)\n```", "```py\n    CheckingAccountStatus DurationMonths CreditHistory CreditPurpose CreditAmount SavingsAccount EmploymentSince DisposableIncomePercent PersonalStatusSex OtherDebtors PresentResidenceMonths Property Age OtherInstallmentPlans Housing NumberOfExistingCreditsInBank Job LiabilityNumberOfPeople Phone ForeignWorker CreditScore\n    ```", "```py\n    import pandas\n    data_frame = pandas.read_csv('german.data', sep=' ')\n    data_frame.replace('NA', -1000000, inplace=True)\n    ```", "```py\n    labels = {\n     'CheckingAccountStatus': ['A11', 'A12', 'A13', 'A14'],\n     'CreditHistory': ['A30', 'A31', 'A32', 'A33', 'A34'],\n     'CreditPurpose': ['A40', 'A41', 'A42', 'A43', 'A44', 'A45', 'A46', 'A47', 'A48', 'A49', 'A410'],\n     'SavingsAccount': ['A61', 'A62', 'A63', 'A64', 'A65'],\n     'EmploymentSince': ['A71', 'A72', 'A73', 'A74', 'A75'],\n     'PersonalStatusSex': ['A91', 'A92', 'A93', 'A94', 'A95'],\n     'OtherDebtors': ['A101', 'A102', 'A103'],\n     'Property': ['A121', 'A122', 'A123', 'A124'],\n     'OtherInstallmentPlans': ['A141', 'A142', 'A143'],\n     'Housing': ['A151', 'A152', 'A153'],\n     'Job': ['A171', 'A172', 'A173', 'A174'],\n     'Phone': ['A191', 'A192'],\n     'ForeignWorker': ['A201', 'A202']\n    }\n    ```", "```py\n    from sklearn import preprocessing\n    label_encoders = {}\n    data_frame_encoded = pandas.DataFrame()\n    for column in data_frame:\n        if column in labels:\n            label_encoders[column] = preprocessing.LabelEncoder()\n            label_encoders[column].fit(labels[column])\n            data_frame_encoded[column] = label_encoders[\n                column].transform(data_frame[column])\n        else:\n            data_frame_encoded[column] = data_frame[column]\n    ```", "```py\ndata_frame_encoded.head()\nCheckingAccountStatus DurationMonths CreditHistory CreditPurpose \\\n0                     0             6             4             4\n1                     1             48             2             4\n2                     3             12             4             7\n3                     0             42             2             3\n4                     0             24             3             0\n   CreditAmount SavingsAccount EmploymentSince DisposableIncomePercent \\\n0         1169             4                4                        4\n1         5951             0                2                        2\n2         2096             0                3                        2\n3         7882             0                3                        2\n4         4870             0                2                        3\n   PersonalStatusSex OtherDebtors     ...     Property Age \\\n0                 2             0     ...             0 67\n1                 1             0     ...             0 22\n2                 2             0     ...             0 49\n3                 2             2     ...             1 45\n4                 2             0     ...             3 53\n   OtherInstallmentPlans Housing NumberOfExistingCreditsInBank Job \\\n0                     2        1                             2    2\n1                     2        1                             1    2\n2                     2        1                             1    1\n3                     2        2                             1    2\n4                     2        2                             2    2\n   LiabilityNumberOfPeople Phone ForeignWorker CreditScore\n0                        1     1             0            1\n1                        1     0             0            2\n2                        2     0             0            1\n3                        2     0             0            1\n4                        2     0             0            2\n[5 rows x 21 columns]\nlabel_encoders\n{'CheckingAccountStatus': LabelEncoder(),\n 'CreditHistory': LabelEncoder(),\n 'CreditPurpose': LabelEncoder(),\n 'EmploymentSince': LabelEncoder(),\n 'ForeignWorker': LabelEncoder(),\n 'Housing': LabelEncoder(),\n 'Job': LabelEncoder(),\n 'OtherDebtors': LabelEncoder(),\n 'OtherInstallmentPlans': LabelEncoder(),\n 'PersonalStatusSex': LabelEncoder(),\n 'Phone': LabelEncoder(),\n 'Property': LabelEncoder(),\n 'SavingsAccount': LabelEncoder()}\n```", "```py\n    import numpy as np\n    features = np.array(\n        data_frame_encoded.drop(['CreditScore'], 1)\n    )\n    label = np.array(data_frame_encoded['CreditScore'])\n    ```", "```py\n    scaled_features = preprocessing.MinMaxScaler(\n    feature_range=(0,1)).fit_transform(features)\n    ```", "```py\n    from sklearn import model_selection\n    features_train, features_test, label_train,\n    label_test = model_selection.train_test_split(\n        scaled_features,\n        label,\n        test_size = 0.2\n    )\n    ```", "```py\n    You must have completed Exercise 13, to be able to complete this activity\n    classifier = neighbors.KNeighborsClassifier(n_neighbors=10)\n    classifier.fit(\n        features_train,label_train\n        )\n    classifier.score(features_test, label_test)\n    ```", "```py\n    K=10: accuracy is 71.5%\n    K=15: accuracy is 70.5%\n    K=25: accuracy is 72%\n    K=50: accuracy is 74%\n    ```", "```py\n    classifier = svm.SVC(kernel=\"linear\")\n    classifier.fit(features_train, label_train)\n    classifier.score(features_test, label_test)\n    ```", "```py\n    classifier = svm.SVC(kernel=\"poly\", C=2, degree=4, gamma=0.05)\n    classifier.fit(features_train, label_train)\n    classifier.score(features_test, label_test)\n    ```", "```py\n    classifier = svm.SVC(kernel=\"poly\", C=2, degree=4, gamma=0.25)\n    classifier.fit(features_train, label_train)\n    classifier.score(features_test, label_test)\n    ```", "```py\n    classifier = svm.SVC(kernel=\"poly\", C=2, degree=4, gamma=0.5)\n    classifier.fit(features_train, label_train)\n    classifier.score(features_test, label_test)\n    ```", "```py\n    classifier = svm.SVC(kernel=\"sigmoid\")\n    classifier.fit(features_train, label_train)\n    classifier.score(features_test, label_test)\n    ```", "```py\n    classifier = svm.SVC(kernel=\"rbf\", gamma=0.15)\n    classifier.fit(features_train, label_train)\n    classifier.score(features_test, label_test)\n    ```", "```py\n    import pandas\n    data_frame = pandas.read_csv('car.data')\n    ```", "```py\n    data_frame.head()\n    Buying Maintenance Doors Persons LuggageBoot Safety Class\n    0 vhigh     vhigh     2     2     small    low unacc\n    1 vhigh     vhigh     2     2     small    med unacc\n    2 vhigh     vhigh     2     2     small high unacc\n    3 vhigh     vhigh     2     2         med    low unacc\n    4 vhigh     vhigh     2     2         med    med unacc\n    ```", "```py\n    labels = {\n        'Buying': ['vhigh', 'high', 'med', 'low'],\n        'Maintenance': ['vhigh', 'high', 'med', 'low'],\n        'Doors': ['2', '3', '4', '5more'],\n        'Persons': ['2', '4', 'more'],\n        'LuggageBoot': ['small', 'med', 'big'],\n        'Safety': ['low', 'med', 'high'],\n        'Class': ['unacc', 'acc', 'good', 'vgood']\n    }\n    from sklearn import preprocessing\n    label_encoders = {}\n    data_frame_encoded = pandas.DataFrame()\n    for column in data_frame:\n        if column in labels:\n            label_encoders[column] = preprocessing.LabelEncoder()\n            label_encoders[column].fit(labels[column])\n            data_frame_encoded[column] = label_encoders[column].transform(data_frame[column])\n        else:\n    data_frame_encoded[column] = data_frame[column]\n    ```", "```py\n    import numpy as np\n    features = np.array(data_frame_encoded.drop(['Class'], 1))\n    label = np.array( data_frame_encoded['Class'] )\n    ```", "```py\n    from sklearn import model_selection\n    features_train, features_test, label_train, label_test = model_selection.train_test_split(\n        features,\n        label,\n        test_size=0.1\n    )\n    ```", "```py\n    from sklearn.tree import DecisionTreeClassifier\n    decision_tree = DecisionTreeClassifier()\n    decision_tree.fit(features_train, label_train)\n    ```", "```py\n    DecisionTreeClassifier(\n        class_weight=None,\n        criterion='gini',\n        max_depth=None,\n        max_features=None,\n        max_leaf_nodes=None,\n        min_impurity_decrease=0.0,\n        min_impurity_split=None,\n        min_samples_leaf=1,\n        min_samples_split=2,\n        min_weight_fraction_leaf=0.0,\n        presort=False,\n        random_state=None,\n        splitter='best'\n    )\n    ```", "```py\n    decision_tree.score( features_test, label_test )\n    ```", "```py\n     0.9884393063583815\n    ```", "```py\n    from sklearn.metrics import classification_report\n    print(\n        classification_report(\n            label_test,\n            decision_tree.predict(features_test)\n        )\n    )\n    ```", "```py\n                 precision    recall f1-score support\n             0     0.97     0.97     0.97        36\n             1     1.00     1.00     1.00         5\n             2     1.00     0.99     1.00     127\n             3     0.83     1.00     0.91         5\n    avg / total     0.99     0.99     0.99     173\n    ```", "```py\n    from sklearn import model_selection\n    features_train, features_test, label_train, label_test = model_selection.train_test_split(\n        features,\n        label,\n        test_size=0.1\n    )\n    ```", "```py\n    from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n    random_forest_classifier = RandomForestClassifier(n_estimators=100, max_depth=6)\n    random_forest_classifier.fit(features_train, label_train)\n    extra_trees_classifier =ExtraTreesClassifier(\n        n_estimators=100, max_depth=6\n    )\n    extra_trees_classifier.fit(features_train, label_train)\n    ```", "```py\n    from sklearn.metrics import classification_report\n    print(\n        classification_report(\n            label_test,\n            random_forest_classifier.predict(features_test)\n        )\n    )\n    ```", "```py\n                     precision    recall f1-score support\n             0     0.78     0.78     0.78        36\n             1     0.00     0.00     0.00         5\n             2     0.94     0.98     0.96     127\n             3     0.75     0.60     0.67         5\n    avg / total     0.87     0.90     0.89     173\n    ```", "```py\n    print(\n        classification_report(\n            label_test,\n            extra_trees_classifier.predict(features_test)\n        )\n    )\n    ```", "```py\n                 precision    recall f1-score support\n              0     0.72     0.72     0.72        36\n              1     0.00     0.00     0.00         5\n              2     0.93     1.00     0.96     127\n              3     0.00     0.00     0.00         5\n    avg / total     0.83     0.88     0.86     173\n    ```", "```py\n    random_forest_classifier.score(features_test, label_test)\n    ```", "```py\n     0.9017341040462428\n    ```", "```py\n    extra_trees_classifier.score(features_test, label_test)\n    ```", "```py\n     0.884393063583815\n    ```", "```py\n    random_forest_classifier.feature_importances_\n    ```", "```py\n    array([0.12656512, 0.09934031, 0.02073233, 0.35550329, 0.05411809, 0.34374086])\n    ```", "```py\n    extra_trees_classifier.feature_importances_\n    ```", "```py\n    array([0.08699494, 0.07557066, 0.01221275, 0.38035005, 0.05879822, 0.38607338])\n    ```", "```py\n    data_frame_encoded.head()\n    ```", "```py\n    Buying Maintenance Doors Persons LuggageBoot Safety Class\n    0     3            3     0        0            2     1    \n    1     3            3     0        0            2     2    \n    2     3            3     0        0            2     0    \n    3     3            3     0        0            1     1    \n    4     3            3     0        0            1     2    \n    ```", "```py\n    features2 = np.array(data_frame_encoded.drop(['Class', 'Doors'], 1))\n    label2 = np.array(data_frame_encoded['Class'])\n    features_train2,\n    features_test2,\n    label_train2,\n    label_test2 = model_selection.train_test_split(\n        features2,\n        label2,\n        test_size=0.1\n    )\n    random_forest_classifier2 = RandomForestClassifier(\n        n_estimators=100, max_depth=6\n    )\n    random_forest_classifier2.fit(features_train2, label_train2)\n    extra_trees_classifier2 = ExtraTreesClassifier(\n        n_estimators=100, max_depth=6\n    )\n    extra_trees_classifier2.fit(features_train2, label_train2)\n    ```", "```py\n    print(\n        classification_report(\n            label_test2,\n            random_forest_classifier2.predict(features_test2)\n        )\n    )\n    ```", "```py\n                precision    recall f1-score support\n             0     0.89     0.85     0.87        40\n             1     0.00     0.00     0.00         3\n             2     0.95     0.98     0.96     125\n             3     1.00     1.00     1.00         5\n    avg / total     0.92     0.93     0.93     173\n    ```", "```py\n    print(\n        classification_report(\n            label_test2,\n            extra_trees_classifier2.predict(features_test2)\n        )\n    )\n    ```", "```py\n                precision    recall f1-score support\n             0     0.78     0.78     0.78        40\n             1     0.00     0.00     0.00         3\n             2     0.93     0.98     0.95     125\n             3     1.00     0.40     0.57         5\n    avg / total     0.88     0.90     0.88     173\n    ```", "```py\n    random_forest_classifier2 = RandomForestClassifier(\n        n_estimators=150,\n        max_ depth=8,\n        criterion='entropy',\n        max_features=5\n    )\n    random_forest_classifier2.fit(features_train2, label_train2)\n    print(\n        classification_report(\n            label_test2,\n            random_forest_classifier2.predict(features_test2)\n        )\n    )\n    ```", "```py\n               precision    recall f1-score support\n              0     0.95     0.95     0.95        40\n              1     0.50     1.00     0.67         3\n              2     1.00     0.97     0.98     125\n              3     0.83     1.00     0.91         5\n    avg / total     0.97     0.97     0.97     173\n    ```", "```py\n    extra_trees_classifier2 = ExtraTreesClassifier(\n        n_estimators=150,\n        max_depth=8,\n        criterion='entropy',\n        max_features=5\n    )\n    extra_trees_classifier2.fit(features_train2, label_train2)\n    print(\n        classification_report(\n            label_test2,\n            extra_trees_classifier2.predict(features_test2)\n        )\n    )\n    ```", "```py\n                precision    recall f1-score support\n              0     0.92     0.88     0.90        40\n              1     0.40     0.67     0.50         3\n              2     0.98     0.97     0.97     125\n              3     0.83     1.00     0.91         5\n    avg / total     0.95     0.94     0.94     173\n    ```", "```py\n    import pandas\n    pandas.read_csv('Sales_Transactions_Dataset_Weekly.csv')\n    ```", "```py\n    import numpy as np\n    drop_columns = ['Product_Code']\n    for w in range(0, 52):\n        drop_columns.append('W' + str(w))\n    features = data_frame.drop(dropColumns, 1)\n    ```", "```py\n    from sklearn.preprocessing import MinMaxScaler\n    scaler = MinMaxScaler()\n    scaled_features = scaler.fit_transform(features)\n    ```", "```py\n    from sklearn.cluster import KMeans\n    k_means_model = KMeans()\n    k_means_model.fit(scaled_features)\n    ```", "```py\n    k_means_model.labels_\n    ```", "```py\n    k_means_model.cluster_centers_\n    ```", "```py\n     array([5, 5, 4, 5, 5, 3, 4, 5, 5, 5, 5, 5, 4, 5, 0, 0, 0, 0, 0, 4, 4, 4,\n           4, 0, 0, 5, 0, 0, 5, 0, 4, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 4, 0, 0, 5, 0, 0, 5, 0,\n           ...\n           1, 7, 3, 2, 6, 7, 6, 2, 2, 6, 2, 7, 2, 7, 2, 6, 1, 3, 2, 2, 6, 6,\n           7, 7, 7, 1, 1, 2, 1, 2, 7, 7, 6, 2, 7, 6, 6, 6, 1, 6, 1, 6, 7, 7,\n           1, 1, 3, 5, 3, 3, 3, 5, 7, 2, 2, 2, 3, 2, 2, 7, 7, 3, 3, 3, 3, 2,\n           2, 6, 3, 3, 5, 3, 2, 2, 6, 7, 5, 2, 2, 2, 6, 2, 7, 6, 1])\n    ```", "```py\n    image = Image.open('destructuring.jpg')\n    pixels = image.load()\n    ```", "```py\n    import pandas\n    data_frame = pandas.DataFrame(\n        [[x,y,pixels[x,y][0], pixels[x,y][1], pixels[x,y][2]]\n            for x in range(image.size[0])\n            for y in range(image.size[1])\n        ],\n        columns=['x', 'y', 'r', 'g', 'b']\n    )\n    ```", "```py\n    from sklearn.cluster import MeanShift\n    mean_shift_model = MeanShift()\n    mean_shift_model.fit(data_frame)\n    for i in range(len(mean_shift_model.cluster_centers_)):\n        image = Image.open('destructuring.jpg')\n        pixels = image.load()\n        for j in range(len(data_frame)):\n            if (mean_shift_model.labels_[j] != i ):\n               pixels[ int(data_frame['x'][j]),\n           int(data_frame['y'][j]) ] = (255, 255, 255)\n        image.save( 'cluster' + str(i) + '.jpg' )\n    ```", "```py\n    k_means_model = KMeans(n_clusters=8)\n    k_means_model.fit(data_frame)\n    for i in range(len(k_means_model.cluster_centers_)):\n        image = Image.open('destructuring.jpg')\n        pixels = image.load()\n        for j in range(len(data_frame)):\n            if (k_means_model.labels_[j] != i):\n                pixels[int(data_frame['x'][j]), int(data_frame['y'][j])] = (255, 255, 255)\n        image.save('kmeanscluster' + str(i) + '.jpg')\n    ```", "```py\n    import tensorflow.keras.datasets.mnist as mnist\n    (features_train, label_train),\n    (features_test, label_test) = mnist.load_data()\n    features_train = features_train / 255.0\n    features_test = features_test / 255.0\n    def flatten(matrix):\n        return [elem for row in matrix for elem in row]\n    features_train_vector = [\n        flatten(image) for image in features_train\n    ]\n    features_test_vector = [\n        flatten(image) for image in features_test\n    ]\n    import numpy as np\n    label_train_vector = np.zeros((label_train.size, 10))\n    for i, label in enumerate(label_train_vector):\n        label[label_train[i]] = 1\n    label_test_vector = np.zeros((label_test.size, 10))\n    for i, label in enumerate(label_test_vector):\n        label[label_test[i]] = 1\n    ```", "```py\n    import tensorflow as tf\n    f = tf.nn.softmax\n    x = tf.placeholder(tf.float32, [None, 28 * 28 ])\n    W = tf.Variable( tf.random_normal([784, 10]))\n    b = tf.Variable( tf.random_normal([10]))\n    y = f(tf.add(tf.matmul( x, W ), b ))\n    ```", "```py\n    import random\n    y_true = tf.placeholder(tf.float32, [None, 10])\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n        logits=y,\n        labels=y_true\n    )\n    cost = tf.reduce_mean(cross_entropy)\n    optimizer = tf.train.GradientDescentOptimizer(\n        learning_rate = 0.5\n    ).minimize(cost)\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    iterations = 600\n    batch_size = 200\n    sample_size = len(features_train_vector)\n    for _ in range(iterations):\n        indices = random.sample(range(sample_size), batchSize)\n        batch_features = [\n            features_train_vector[i] for i in indices\n        ]\n        batch_labels = [\n            label_train_vector[i] for i in indices\n        ]\n        min = i * batch_size\n        max = (i+1) * batch_size\n        dictionary = {\n            x: batch_features,\n            y_true: batch_labels\n        }\n        session.run(optimizer, feed_dict=dictionary)\n    ```", "```py\n    label_predicted = session.run(classify( x ), feed_dict={\n        x: features_test_vector\n    })\n    label_predicted = [\n        np.argmax(label) for label in label_predicted\n    ]\n    confusion_matrix(label_test, label_predicted)\n    ```", "```py\n    array([[ 0, 0, 223, 80, 29, 275, 372, 0, 0, 1],\n       [ 0, 915, 4, 10, 1, 13, 192, 0, 0, 0],\n       [ 0, 39, 789, 75, 63, 30, 35, 0, 1, 0],\n       [ 0, 6, 82, 750, 13, 128, 29, 0, 0, 2],\n       [ 0, 43, 16, 16, 793, 63, 49, 0, 2, 0],\n       [ 0, 22, 34, 121, 40, 593, 76, 5, 0, 1],\n       [ 0, 29, 34, 6, 44, 56, 788, 0, 0, 1],\n       [ 1, 54, 44, 123, 715, 66, 24, 1, 0, 0],\n       [ 0, 99, 167, 143, 80, 419, 61, 0, 4, 1],\n       [ 0, 30, 13, 29, 637, 238, 58, 3, 1, 0]], dtype=int64)\n    ```", "```py\n    accuracy_score(label_test, label_predicted)\n    ```", "```py\n     0.4633\n    ```", "```py\n    for _ in range(iterations):\n        indices = random.sample(range(sample_size), batch_size)\n        batch_features = [\n            features_train_vector[i] for i in indices\n        ]\n        batch_labels = [\n            label_train_vector[i] for i in indices\n        ]\n        min = i * batch_size\n        max = (i+1) * batch_size\n        dictionary = {\n            x: batch_features,\n            y_true: batch_labels\n        }\n        session.run(optimizer, feed_dict=dictionary)\n    ```", "```py\n    array([\n     [946, 0,    6,    3,    0,    1, 15,    2,    7,    0],\n     [ 0,1097,    3,    7,    1,    0,    4,    0, 23,    0],\n     [11, 3, 918, 11, 18,    0, 13,    8, 50,    0],\n     [3,    0, 23, 925,    2, 10,    4,    9, 34,    0],\n     [2,    2,    6,    1, 929,    0, 14,    2, 26,    0],\n     [16, 4,    7, 62,    8, 673, 22,    3, 97,    0],\n     [8,    2,    4,    3,    8,    8, 912,    2, 11,    0],\n     [5,    9, 33,    6,    9,    1,    0, 949, 16,    0],\n     [3,    4,    5, 12,    7,    4, 12,    3, 924,    0],\n     [8,    5,    7, 40, 470, 11,    5, 212, 251,    0]\n    ],\n         dtype=int64)\n    ```", "```py\n    x = tf.placeholder(tf.float32, [None, 28 * 28 ])\n    f1 = tf.nn.relu\n    W1 = tf.Variable(tf.random_normal([784, 200]))\n    b1 = tf.Variable(tf.random_normal([200]))\n    layer1_out = f1(tf.add(tf.matmul(x, W1), b1))\n    f2 = tf.nn.softmax\n    W2 = tf.Variable(tf.random_normal([200, 100]))\n    b2 = tf.Variable(tf.random_normal([100]))\n    layer2_out = f2(tf.add(tf.matmul(layer1_out, W2), b2))\n    f3 = tf.nn.softmax\n    W3 = tf.Variable(tf.random_normal([100, 10]))\n    b3 = tf.Variable( tf.random_normal([10]))\n    y = f3(tf.add(tf.matmul(layer2_out, W3), b3))\n    ```", "```py\n    y_true = tf.placeholder(tf.float32, [None, 10])\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n        logits=y,\n        labels=y_true\n    )\n    cost = tf.reduce_mean(cross_entropy)\n    optimizer = tf.train.GradientDescentOptimizer(\n    learning_rate=0.5).minimize(cost)\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    iterations = 600\n    batch_size = 200\n    sample_size = len(features_train_vector)\n    for _ in range(iterations):\n        indices = random.sample(range(sample_size), batchSize)\n        batch_features = [\n            features_train_vector[i] for i in indices\n        ]\n        batch_labels = [\n            label_train_vector[i] for i in indices\n        ]\n        min = i * batch_size\n        max = (i+1) * batch_size\n        dictionary = {\n            x: batch_features,\n            y_true: batch_labels\n        }\n        session.run(optimizer, feed_dict=dictionary)\n    ```", "```py\n    label_predicted = session.run(y, feed_dict={\n        x: features_test_vector\n    })\n    label_predicted = [\n        np.argmax(label) for label in label_predicted\n    ]\n    confusion_matrix(label_test, label_predicted)\n    ```", "```py\n    array([[ 801, 11,    0, 14,    0,    0, 56,    0, 61, 37],\n         [ 2, 1069,    0, 22,    0,    0, 18,    0,    9, 15],\n         [ 276, 138,    0, 225,    0,    2, 233,    0, 105, 53],\n         [ 32, 32,    0, 794,    0,    0, 57,    0, 28, 67],\n         [ 52, 31,    0, 24,    0,    3, 301,    0, 90, 481],\n         [ 82, 50,    0, 228,    0,    3, 165,    0, 179, 185],\n         [ 71, 23,    0, 14,    0,    0, 712,    0, 67, 71],\n         [ 43, 85,    0, 32,    0,    3, 31,    0, 432, 402],\n         [ 48, 59,    0, 192,    0,    2, 45,    0, 425, 203],\n         [ 45, 15,    0, 34,    0,    2, 39,    0, 162, 712]],\n         dtype=int64)\n    ```", "```py\n    accuracy_score(label_test, label_predicted)\n    ```", "```py\narray([[ 954,    0,    2,    1,    0,    6,    8,    0,    5,    4],\n     [ 0, 1092,    5,    3,    0,    0,    6,    0, 27,    2],\n     [ 8,    3, 941, 16,    0,    2, 13,    0, 35, 14],\n     [ 1,    1, 15, 953,    0, 14,    2,    0, 13, 11],\n     [ 4,    3,    8,    0,    0,    1, 52,    0, 28, 886],\n     [ 8,    1,    5, 36,    0, 777, 16,    0, 31, 18],\n     [ 8,    1,    6,    1,    0,    6, 924,    0,    9,    3],\n     [ 3, 10, 126, 80,    0,    4,    0,    0, 35, 770],\n     [ 4,    0,    6, 10,    0,    6,    4,    0, 926, 18],\n     [ 4,    5,    1,    8,    0,    2,    2,    0, 18, 969]],\n     dtype=int64)\n```"]