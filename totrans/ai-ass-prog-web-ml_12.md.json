["```py\n# Importing necessary libraries\nimport pandas as pd\nfrom google.colab import drive\n# Mount Google Drive\ndrive.mount('/content/drive')\n# Path to the dataset\ndata_path = '/content/drive/My Drive/Chapter05/amazon_product_review_data.csv'\n# Load the dataset into a DataFrame\ndf = pd.read_csv(data_path) \n```", "```py\n# Importing necessary libraries\nimport nltk\nimport string\n# Download NLTK resources (only need to do this once)\nnltk.download('punkt')\nnltk.download('stopwords')\n# Get the list of stopwords\nstopwords = nltk.corpus.stopwords.words('english')\n# Function to preprocess text\ndef preprocess_text(text):\n    # Tokenization\n    tokens = nltk.word_tokenize(text)\n    # Lowercasing\n    tokens = [token.lower() for token in tokens]\n    # Removing stopwords and punctuation\n    tokens = [token for token in tokens if token not in stopwords and token not in string.punctuation]\n    # Join tokens back into text\n    preprocessed_text = ' '.join(tokens)\n    return preprocessed_text\n# Preprocess the review body\ndf['Cleaned_Review'] = df['review_body'].apply(preprocess_text)\n# Display the preprocessed data\ndf.head() \n```", "```py\nmarket_place customer_id review_id product_id product_parent product_title product_category star_rating helpful_votes total_votes vine verified_purchase review_headline review_body review_date sentiments Cleaned_Review 0 \"US\" \"42521656\" \"R26MV8D0KG6QI6\" \"B000SAQCWC\" \"159713740\" \"The Cravings Place Chocolate Chunk Cookie Mix... \"Grocery\" 1 0 0 0 \\t(N) 1 \\t(Y) \"Using these for years - love them.\" \"As a family allergic to wheat, dairy, eggs, n... 2015-08-31 positive `` family allergic wheat dairy eggs nuts sever... 1 \"US\" \"12049833\" \"R1OF8GP57AQ1A0\" \"B00509LVIQ\" \"138680402\" \"Mauna Loa Macadamias, 11 Ounce Packages\" \"Grocery\" 1 0 0 0 \\t(N) 1 \\t(Y) \"Wonderful\" \"My favorite nut. Creamy, crunchy, salty, and ... 2015-08-31 positive `` favorite nut creamy crunchy salty slightly ... 2 \"US\" \"107642\" \"R3VDC1QB6MC4ZZ\" \"B00KHXESLC\" \"252021703\" \"Organic Matcha Green Tea Powder - 100% Pure M... \"Grocery\" 1 0 0 0 \\t(N) 0 \\t(N) \"Five Stars\" \"This green tea tastes so good! My girlfriend ... 2015-08-31 positive `` green tea tastes good girlfriend loves '' 3 \"US\" \"6042304\" \"R12FA3DCF8F9ER\" \"B000F8JIIC\" \"752728342\" \"15oz Raspberry Lyons Designer Dessert Syrup S... \"Grocery\" 1 0 0 0 \\t(N) 1 \\t(Y) \"Five Stars\" \"I love Melissa's brand but this is a great se... 2015-08-31 positive `` love melissa 's brand great second ca n't g... 4 \"US\" \"18123821\" \"RTWHVNV6X4CNJ\" \"B004ZWR9RQ\" \"552138758\" \"Stride Spark Kinetic Fruit Sugar Free Gum, 14... \"Grocery\" 1 0 0 0 \\t(N) 1 \\t(Y) \"Five Stars\" \"good\" 2015-08-31 positive `` good '' \n```", "```py\n# Importing necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n# Splitting the data into training and testing sets\nX = df['Cleaned_Review']\ny = df['sentiments']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# TF-IDF vectorization\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n# Initialize and train the logistic regression model\nlr_model = LogisticRegression()\nlr_model.fit(X_train_tfidf, y_train)\n# Predictions\ny_pred = lr_model.predict(X_test_tfidf)\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred)) \n```", "```py\n Accuracy: 0.86\nClassification Report:\n              precision    recall  f1-score   support\n    negative       0.00      0.00      0.00        14\n    positive       0.86      1.00      0.92        86\n    accuracy                           0.86       100\n   macro avg       0.43      0.50      0.46       100\nweighted avg       0.74      0.86      0.80       100 \n```", "```py\n!pip install shap \n```", "```py\n# Importing necessary libraries\nimport shap\n# Initialize the SHAP explainer with the logistic regression model and training data\nexplainer = shap.Explainer(lr_model, X_train_tfidf)\n# Explain the model's predictions for a specific instance (e.g., the first instance in the test set)\ninstance_index = 0  # You can choose any instance index from the test set\nshap_values = explainer.shap_values(X_test_tfidf[instance_index])\n# Visualize the SHAP values\nshap.summary_plot(shap_values, features=X_test_tfidf[instance_index], feature_names=tfidf_vectorizer.get_feature_names()) \n```", "```py\nAttributeError: 'TfidfVectorizer' object has no attribute 'get_feature_names' \n```", "```py\n# Get feature names from the TfidfVectorizer vocabulary\nfeature_names = tfidf_vectorizer.get_feature_names_out()\n# Visualize the SHAP values\nshap.summary_plot(shap_values, features=X_test_tfidf[instance_index], feature_names=feature_names) \n```", "```py\n!pip install eli5 \n```", "```py\n# Importing necessary libraries\nimport eli5\n# Explain the logistic regression model using ELI5\neli5.show_weights(lr_model, vec=tfidf_vectorizer, top=20)  # Show top 20 most important features \n```", "```py\n# Explain the logistic regression model using ELI5 with feature names\neli5.show_weights(lr_model, vec=tfidf_vectorizer, top=20, feature_names=tfidf_vectorizer.get_feature_names_out()) \n```", "```py\nAccuracy: 0.86\nClassification Report:\n              precision    recall  f1-score   support\n    negative       0.00      0.00      0.00        14\n    positive       0.86      1.00      0.92        86. \n```", "```py\nimport numpy as np\n# Calculate class weights\nfrom sklearn.utils.class_weight import compute_class_weight\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n# Initialize and train the logistic regression model with class weights\nlr_model_balanced = LogisticRegression(class_weight=dict(zip(np.unique(y_train), class_weights)))\nlr_model_balanced.fit(X_train_tfidf, y_train)\n# Predictions\ny_pred_balanced = lr_model_balanced.predict(X_test_tfidf)\n# Evaluate the model with balanced class weights\naccuracy_balanced = accuracy_score(y_test, y_pred_balanced)\nprint(\"Accuracy with balanced class weights:\", accuracy_balanced)\nprint(\"\\nClassification Report with balanced class weights:\")\nprint(classification_report(y_test, y_pred_balanced)) \n```", "```py\n precision    recall  f1-score   support\n    negative       0.27      0.21      0.24        14\n    positive       0.88      0.91      0.89        86\n    accuracy                           0.81       100\n   macro avg       0.57      0.56      0.57       100\nweighted avg       0.79      0.81      0.80       100 \n```", "```py\n# Importing necessary libraries\nfrom sklearn.model_selection import GridSearchCV\n# Define hyperparameters grid\nparam_grid = {\n    'C': [0.01, 0.1, 1.0, 10.0],  # Regularization strength (smaller values indicate stronger regularization)\n    'solver': ['liblinear', 'lbfgs'],  # Optimization algorithm\n}\n# Initialize logistic regression model\nlr_model_tuned = LogisticRegression()\n# Initialize GridSearchCV with logistic regression model and hyperparameters grid\ngrid_search = GridSearchCV(lr_model_tuned, param_grid, cv=5, scoring='accuracy')\n# Perform grid search\ngrid_search.fit(X_train_tfidf, y_train)\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n# Use the best model from grid search\nbest_lr_model = grid_search.best_estimator_\n# Predictions\ny_pred_tuned = best_lr_model.predict(X_test_tfidf)\n# Evaluate the tuned model\naccuracy_tuned = accuracy_score(y_test, y_pred_tuned)\nprint(\"Accuracy with tuned model:\", accuracy_tuned)\nprint(\"\\nClassification Report with tuned model:\")\nprint(classification_report(y_test, y_pred_tuned)) \n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Initialize CountVectorizer\nbow_vectorizer = CountVectorizer(max_features=5000)  # You can adjust max_features as needed\n# Transform text data into bag-of-words representation\nX_train_bow = bow_vectorizer.fit_transform(X_train)\nX_test_bow = bow_vectorizer.transform(X_test)\n# Train logistic regression model with bag-of-words representation\nlr_model_bow = LogisticRegression()\nlr_model_bow.fit(X_train_bow, y_train)\n# Evaluate model performance\ny_pred_bow = lr_model_bow.predict(X_test_bow)\naccuracy_bow = accuracy_score(y_test, y_pred_bow)\nprint(\"Accuracy with bag-of-words representation:\", accuracy_bow)\n# Add other evaluation metrics if needed \n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Initialize TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed\n# Transform text data into TF-IDF representation\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n# Train logistic regression model with TF-IDF representation\nlr_model_tfidf = LogisticRegression()\nlr_model_tfidf.fit(X_train_tfidf, y_train)\n# Evaluate model performance\ny_pred_tfidf = lr_model_tfidf.predict(X_test_tfidf)\naccuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\nprint(\"Accuracy with TF-IDF representation:\", accuracy_tfidf)\n# Add other evaluation metrics if needed \n```", "```py\n# Assuming you have pre-trained word embeddings loaded (e.g., GloVe)\n# Convert text data into word embeddings representation\n# You may need to tokenize and pad sequences before applying word embeddings\n# Train logistic regression model with word embeddings representation\nlr_model_word_embeddings = LogisticRegression()\nlr_model_word_embeddings.fit(X_train_word_embeddings, y_train)\n# Evaluate model performance\ny_pred_word_embeddings = lr_model_word_embeddings.predict(X_test_word_embeddings)\naccuracy_word_embeddings = accuracy_score(y_test, y_pred_word_embeddings)\nprint(\"Accuracy with word embeddings representation:\", accuracy_word_embeddings)\n# Add other evaluation metrics if needed \n```", "```py\nNameError Traceback (most recent call last) <ipython-input-20-8a483831a6f9> in <cell line: 7>() 5 # Train logistic regression model with word embeddings representation 6 lr_model_word_embeddings = LogisticRegression() ----> 7 lr_model_word_embeddings.fit(X_train_word_embeddings, y_train) 8 9 # Evaluate model performance NameError: name 'X_train_word_embeddings' is not defined. We haven't pretrained word embeddings but have tokenized it earlier. We tokenized the text using nltk.word_tokenize(), converted tokens to lowercase, removed stopwords and punctuation using list comprehensions, and then joined the tokens back into text. Finally, we applied the preprocess_text() function to the 'Review_body' column of the DataFrame and stored the preprocessed text in a new column called 'Cleaned_Review'. \n```", "```py\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n# Define a custom transformer to tokenize the text data\nclass TokenizerTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return [text.split() for text in X]\n# Initialize and fit Word2Vec model on tokenized text data\nw2v_model = Word2Vec(sentences=TokenizerTransformer().fit_transform(X_train), min_count=1)\n# Define a function to average word vectors for each document\ndef average_word_vectors(words, model, vocabulary, num_features):\n    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n    nwords = 0.\n    for word in words:\n        if word in vocabulary:\n            nwords = nwords + 1.\n            feature_vector = np.add(feature_vector, model.wv[word])\n    if nwords:\n        feature_vector = np.divide(feature_vector, nwords)\n    return feature_vector\n# Define a custom transformer to convert tokenized text data into word embeddings representation\nclass WordEmbeddingsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, model):\n        self.model = model\n        self.num_features = model.vector_size\n        self.vocabulary = set(model.wv.index_to_key)\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return np.array([average_word_vectors(words, self.model, self.vocabulary, self.num_features) for words in X])\n# Convert tokenized text data into word embeddings representation\nX_train_word_embeddings = WordEmbeddingsTransformer(w2v_model).fit_transform(TokenizerTransformer().fit_transform(X_train))\nX_test_word_embeddings = WordEmbeddingsTransformer(w2v_model).transform(TokenizerTransformer().transform(X_test))\n# Train logistic regression model with word embeddings representation\nlr_model_word_embeddings = LogisticRegression()\nlr_model_word_embeddings.fit(X_train_word_embeddings, y_train)\n# Evaluate model performance\ny_pred_word_embeddings = lr_model_word_embeddings.predict(X_test_word_embeddings)\naccuracy_word_embeddings = accuracy_score(y_test, y_pred_word_embeddings)\nprint(\"Accuracy with word embeddings representation (Word2Vec):\", accuracy_word_embeddings)\n# Add other evaluation metrics if needed \n```"]