<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting User Behavior with Tree-Based Methods</h1>
                </header>
            
            <article>
                
<p style="padding-left: 30px"><span>This chapter will introduce decision trees, random forests, and gradient-boosted trees. Decision trees methodology is a popular technique used in data science that provides a visual representation of how the information in the training set can be represented as a hierarchy. Traversing the hierarchy based on an observation helps you to predict the probability of that event.  We will explore how to use these algorithms can be used to predict when a user may click on online advertisement based on existing advertising click records. </span>Additionally<span>, we will show how to use AWS <strong>Elastic MapReduce</strong> (<strong>EMR</strong>) with Apache Spark and the SageMaker XGBoost service to engineer models in the context of big data. </span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Understanding decision trees</li>
<li><span>Understanding </span>random forests algorithms</li>
<li><span>Understanding </span>gradient boosting algorithms</li>
<li>Predicting clicks on log streams</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding decision trees</h1>
                </header>
            
            <article>
                
<p>Decision trees graphically show the decisions to be made, the observed events that may occur, and the probabilities of the outcomes given a specific set of observable events occurring together. Decision trees are used as a popular machine learning algorithm, where, based on a dataset of observable events and the known outcomes, we can construct a decision tree that can represent the probability of an event occurring.</p>
<p>The following table shows a very simple example of how decision trees can be generated:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 174px"><strong>Car Make</strong></td>
<td style="width: 10px"><strong>Year</strong></td>
<td style="width: 182px"><strong>Price</strong></td>
</tr>
<tr>
<td style="width: 174px">BMW</td>
<td style="width: 10px">2015</td>
<td style="width: 182px">&gt;$40K</td>
</tr>
<tr>
<td style="width: 174px">BMW</td>
<td style="width: 10px">2018</td>
<td style="width: 182px">&gt;$40K</td>
</tr>
<tr>
<td style="width: 174px">Honda </td>
<td style="width: 10px">2015</td>
<td style="width: 182px">&lt;$40K</td>
</tr>
<tr>
<td style="width: 174px">Honda </td>
<td style="width: 10px">2018</td>
<td style="width: 182px">&gt;$40K</td>
</tr>
<tr>
<td style="width: 174px">Nissan</td>
<td style="width: 10px">2015</td>
<td style="width: 182px">&lt;$40K</td>
</tr>
<tr>
<td style="width: 174px">Nissan</td>
<td style="width: 10px">2018</td>
<td style="width: 182px">
<p class="mce-root">&gt;$40K</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>This is a very simple dataset that is represented by the following decision tree:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-961 image-border" src="assets/d6743663-fd91-403f-9a57-775f6fe90608.png" style="width:24.17em;height:19.92em;"/></p>
<p>The aim of the machine learning algorithm is to generate decision trees that best represent the observations in the dataset. For a new observation, if we traverse the decision tree, the leaf nodes represent the class variable or event that is most likely to occur. In the preceding example, we have a dataset that has information regarding the make and the year of a used car. The class variable (also called the <strong>feature label</strong>) is the price of the car. We can observe in the dataset that, irrespective of the year variable value, the price of a BMW car is greater than $40,000. However, if the make of the car is not BMW, the cost of the car is determined by the year the car was produced. The example is based on a very small amount of data. However, the decision tree represents the information in the dataset, and if we have to determine the cost of a new car where<span> </span><span>the make is BMW and year is 2015, then we can predict that the cost is greater than $40,000. For more complex decision trees, the leaf nodes also have a probability associated with them that represents the probability of the class value occurring. In this chapter, we will study algorithms that can be used to generate such decision trees. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recursive splitting</h1>
                </header>
            
            <article>
                
<p>Decision trees can be built by recursively splitting the dataset into subsets. During each split, we evaluate splits based on all the input attributes and use a cost function to determine which split has the lowest cost. The cost functions generally evaluate the loss of information when we split the dataset into two branches. This partitioning of the dataset into smaller subsets is also referred to as recursive partitioning. The cost of splitting the datasets into subsets is generally determined by how records with similar class variables are grouped together in each dataset. Hence, the most optimal split would be when observations in each subset will have the same class variable values. </p>
<p>Such recursive splitting of decision trees is a top-down approach in generating decision trees. This is also a greedy algorithm since we made the decision at each point on how to divide the dataset, without considering how it may affect the later splits. </p>
<p>In the preceding example, we made the first split based on the make of the car. This is because one of our subsets, where the make is BMW, has a 100% probability of the price of the car being greater than $40,000. Similarly, if we had made a split based on the year, we would also get a subset of the year equal to 2018 that also has a 100% probability of the cost of the car is greater than $40,000. Hence, for the same dataset, we can generate multiple decision trees that represent the dataset. There are various cost functions that we will look at that generate different decision trees based on the same dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of decision trees</h1>
                </header>
            
            <article>
                
<p>There are following two main types of decision trees that most data scientists have to work with based on the class variables in the dataset:</p>
<ul>
<li><strong>Classification trees: </strong>Classification trees are decision trees that are used to predict discrete values. This means that the class variable of the dataset used to generate classification trees is a discrete value. The preceding <span><span>example regarding</span></span> car prices at the start of this section is a classification tree as it only has two values of the class variable. </li>
<li><strong>Regression trees: </strong><span>Regression trees are decision trees that are used to</span> predict <span>real numbers, such as the</span> example <span>in</span> <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml"/>C<em>hapter 3</em><em>, Predicting House Value with Regression Algorithms</em><span>, where we were predicting the price of the house.</span></li>
</ul>
<p>The term <strong>Classification and Regression Trees</strong> (<strong>CART</strong>) is used to describe the algorithm for generating decision trees. CART is a popular algorithm for decision trees. Other popular decision tree algorithms include ID3 and C4.5. These algorithms are different from each other in terms of the cost functions they use for splitting the dataset and the criteria used to determine when to stop splitting. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cost functions </h1>
                </header>
            
            <article>
                
<p>As discussed in the section on <em>Recursive splitting</em>, we need cost functions to determine whether splitting on a given input variable is better than other variables. The effectiveness of these cost functions is crucial for the quality of the decision trees being built. In this section, we'll discuss two popular cost functions for generating a decision tree. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gini Impurity</h1>
                </header>
            
            <article>
                
<p>Gini Impurity is defined as the measurement of the likelihood of incorrect classification of a random observation, given the random observation is classified based on the distribution of the class variables in the dataset. Consider a dataset with  <img class="fm-editor-equation" src="assets/1cb79df3-f4d0-4635-a7eb-a36ca0e0d4e7.png" style="width:0.67em;height:0.92em;"/> class variables, and  <img class="fm-editor-equation" src="assets/2cfcf87b-3030-4cc6-ba1c-c60a62ddbc18.png" style="width:1.33em;height:1.17em;"/> is the fraction of observations in the dataset labeled as <img class="fm-editor-equation" src="assets/f7eea272-393c-47d6-9b01-81233bc5a30d.png" style="width:0.50em;height:1.33em;"/>. <em>Gini Impurity</em> can be calculated using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/55adff7c-2b06-42b2-8fb0-ad42229aa761.png" style="width:23.33em;height:3.75em;"/>      .. 4.1</p>
<div class="packt_infobox">Gini Impurity tells us the amount of noise present in the dataset, based on the distributions of various class variables.</div>
<p>For example, in the car price dataset presented at the start of <em>Understanding Decision Trees</em> section, we have two class variables: greater than 40,000 and less than 40,000. If we had to calculate the Gini Impurity of the dataset, it could be calculated as shown in the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/31bfe354-0fab-4186-adb4-18ee95eaf77f.png" style="width:32.83em;height:1.50em;"/></p>
<p>Hence, there is a lot of noise in the base dataset since each class variable has 50% of the observations. </p>
<p>However, when we create a branch where the make of the car, the Gini Impurity of that subset of the dataset is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a3bab794-0e25-46aa-a736-3190545ed1fa.png" style="width:34.33em;height:1.50em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/658abb2a-deee-4d28-ab16-80ec3f2aba2c.png" style="width:35.50em;height:1.42em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0ce42a6c-fe7d-4249-9883-a8d70c2ec65f.png" style="width:35.17em;height:1.42em;"/></p>
<p>Since the branch of for BMW only contains class values of <em>&gt;40K</em>, there is no noise in the branch and the value of Gini Impurity is <em>0</em>. Note that when the subset of the data only has one class value, the Gini Impurity value is always <em>0</em>.</p>
<p>Gini Impurity is used to calculate the Gini Index for each attribute. The Gini Index is a weighted sum of all the values of an attribute on which we create branches. For an attribute, <img class="fm-editor-equation" src="assets/f6e07e55-9112-4986-8e76-7b75afbfd099.png" style="width:0.50em;height:0.58em;"/>, that has <img class="fm-editor-equation" src="assets/ff667824-2211-4440-a6bd-fba1eea27034.png" style="width:0.92em;height:0.83em;"/> unique values, Gini Gain is calculated using formula below. <img class="fm-editor-equation" src="assets/5e9ca5cf-4d11-4848-990b-c1b3ac2e3961.png" style="width:1.33em;height:1.17em;"/> is<span> the fraction of observations in the dataset where the value of the attribute, <img class="fm-editor-equation" src="assets/ea9db92c-195e-4477-be7f-68149a481142.png" style="width:0.83em;height:0.92em;"/>, is <img class="fm-editor-equation" src="assets/98d43c54-cf8a-464d-9376-503ccb926ee0.png" style="width:0.50em;height:1.33em;"/></span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/43d86f3b-a08a-4d4e-b190-a83c7424690d.png" style="width:17.58em;height:3.08em;"/>     </p>
<p>Hence, in our preceding example, the Gini Index for the <em>Make</em> attribute that has three distinct values is calculated as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/54b85ded-6b89-4176-b5ee-b331e651c0e6.png" style="width:99.17em;height:4.83em;"/></p>
<p>Similarly, we calculate the Gini Index for other attributes. In our example, the Gini Index for the <em>Year</em> attribute is 0.4422. We encourage you to calculate this value on your own. Our aim is to pick the attribute that generates the lowest Gini Index score. For a perfect classification, where all the class values in each branch are the same, the Gini Index score will be 0. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Information gain</h1>
                </header>
            
            <article>
                
<p>Information gain is based on the concept of entropy, which is commonly used in physics to represent the unpredictability in a random variable. For example, if we have an unbiased coin, the entropy of the coin is represented as <em>1</em>, as it has the highest unpredictability. However, if a coin is biased, and has a 100% chance of heads, the entropy of the coin is 0.</p>
<p>This concept of entropy can also be used to determine the unpredictability of the class variable in a given branch. The entropy, denoted as <em>H</em>, of a branch is calculated using the formula below. <img class="fm-editor-equation" src="assets/faf67a04-8f90-4c08-b99d-a1ae95804c69.png" style="width:2.33em;height:1.17em;"/> represents the entropy of the attribute. <img class="fm-editor-equation" src="assets/a03e73dd-2003-4c47-a55a-8b355a5ded39.png" style="width:0.83em;height:1.08em;"/> is the number of class variables in the dataset. <img class="fm-editor-equation" src="assets/5a31aed3-e450-45d4-9618-e8721da55b32.png" style="width:1.33em;height:1.17em;"/> is the fraction of observations in the dataset that belong to the class, <img class="fm-editor-equation" src="assets/fbd47654-3821-45aa-89dd-d73f69e93690.png" style="width:0.50em;height:1.33em;"/>:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5d65e3d7-daba-4b80-b152-dade6bf59135.png" style="width:9.75em;height:3.17em;"/>           </p>
<p> </p>
<p><strong>Step 1</strong>: In our example, for the entire dataset, we can calculate the entropy of the dataset as follows.</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/01cf8bda-1f09-470f-aa69-9846c0257f32.png" style="width:44.67em;height:3.50em;"/></p>
<p><strong>Step 2</strong>: In our decision tree, we split the tree based on the make of the car. Hence, we also calculate the entropy of each branch of the tree, as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/db694e43-90fd-41a6-b5c3-1843aa950295.png" style="width:35.17em;height:1.25em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dbc40d20-a5fb-47a0-bf2b-68bde5a06fbd.png" style="width:51.58em;height:3.83em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b9e63287-3d3e-41bc-be71-2384ef33fdc2.png" style="width:52.58em;height:3.83em;"/></p>
<p><strong>Step 3</strong>: Based on the entropy of the parent and the branches, we can evaluate the branch using a measure called <strong>information gain</strong>. For a parent branch, <img class="fm-editor-equation" src="assets/16b7bb2a-eac9-4d30-8422-29ecf3315a9c.png" style="width:0.83em;height:1.00em;"/>, and attribute, <img class="fm-editor-equation" src="assets/88e885c5-21d3-47e0-a455-15404f53baf9.png" style="width:0.58em;height:0.67em;"/>, information gain is represented as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bdea57aa-19c4-4d55-b646-4844a3415ba5.png" style="width:12.50em;height:1.25em;"/></p>
<p><img class="fm-editor-equation" src="assets/873963f4-165f-49bf-9406-1ab0817f9ce8.png" style="width:3.42em;height:1.25em;"/> is the weighted sum of the entropy of the children. In our example, <img class="fm-editor-equation" src="assets/b1162a12-fc43-4cda-ad99-31d329d8df34.png" style="width:2.67em;height:1.00em;"/> of the <em>Make</em> attribute is calculated as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ef8027cc-d825-411c-8cef-ccdf02baba9e.png" style="width:29.75em;height:1.08em;"/></p>
<p>Hence, the information gain for the <em>Make</em> attribute is calculated as follows: </p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/85f98981-1681-43e7-b486-1da57abb3f24.png" style="width:22.33em;height:1.33em;"/></p>
<p>Similarly, we can calculate the information gain score for other attributes. The attribute with the highest information gain should be used to split the dataset for the highest quality of a decision tree. Information gain is used in the ID3 and C4.5 algorithms. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Criteria to stop splitting trees</h1>
                </header>
            
            <article>
                
<p>As decision tree generation algorithms are recursive, we need a criterion that indicates when to stop splitting the trees. There are various criteria we can set to stop splitting the trees. <span>Let us now look at the list of commonly used criteria</span>:</p>
<ul>
<li><strong>Number of observations in the node</strong>:<strong> </strong>We can set criteria to stop the recursion in a branch if the number of observations is less than a pre-specified amount. A good rule of thumb is to stop the recursion when there is fewer than 5% of the total training data in a branch. If we over split the data, such that each node only has one data point, it leads to overfitting the decision tree to the training data. Any new observation that has not been previously seen will not be accurately classified in such trees. </li>
<li><strong>Purity of the node</strong>:<strong> </strong>In the <span><em>Gini Impurity</em> section</span>, we learned to calculate the likelihood of error in classifying a random observation. We can also use the same methodology to calculate the purity of the dataset. If the purity of the subset in a branch is greater than a pre-specified threshold, we can stop splitting based on that branch.</li>
<li><strong>The depth of the tree</strong>:<strong> </strong>We can also pre-specify the limit on the depth of the tree. If the depth of any branch exceeds the limit, we can stop splitting the branch further. </li>
<li><strong>Pruning trees</strong>:<strong> </strong>Another strategy is to let the trees grow fully. This avoids the branch splitting being terminated prematurely, without looking ahead. However, after the full tree is built, it is likely that the tree is large and there may be overfitting in some branches. Hence, pruning strategies are applied to evaluate each branch of the tree; any branch that introduces less than the pre-specified amount of impurity in the parent branch is eliminated. There are various techniques to prune decision trees. We encourage our readers to explore this topic further in the libraries that they implement their decision trees in. </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding random forest algorithms</h1>
                </header>
            
            <article>
                
<p><span> There are two main disadvantages to using decision trees. First, the decision trees use algorithms that make a choice to split on an attribute based on a cost function. The decision tree algorithm is a greedy algorithm that optimizes toward a local optimum when making every decision regarding splitting the dataset into two subsets. However, it does not explore whether making a suboptimal decision while splitting over an attribute, would lead to a more optimal decision tree in the future. Hence, we do not get a globally optimum tree when running this algorithm. Second, decision trees tend to overfit to the training data. For example, a small sample of observations available in the dataset may lead to a branch that provides a very high probability of a certain class event occurring. This leads to the decision trees being really good at generating correct predictions for the dataset that was used for training. However, for observations that they have never seen before, decision trees may not be accurate due to overfitting to the training data. </span></p>
<p>To tackle these issues, the random forest algorithm can be used to improve the accuracy of the existing decision tree algorithms. In this approach, we divide the training data into random subsets and create a collection of decision trees, each based on a subset. This tackles the issue of overfitting, as we no longer rely on one tree to make the decision that has overfit to the entire training set. Secondly, this also helps with the issue of splitting on only one attribute based on a cost function. Different decision trees in random forests may make decisions on splitting based on different attributes, based on the random sample they are training on. </p>
<p>During the prediction phase, the random forest algorithm gets a probability of an event from each branch and uses a voting methodology to generate a prediction. This helps us suppress predictions from trees that may have overfitted or made sub-optimal decisions when generating the trees. Such an approach to divide the training set into random subsets and train multiple machine learning models is known as <strong>Bagging</strong>. The Bagging approach can also be applied to other machine learning algorithms. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding gradient boosting algorithms</h1>
                </header>
            
            <article>
                
<p><span>Gradient boosting algorithms are also used to address the disadvantages of the decision tree algorithm. However, unlike the random forests algorithm, which trains multiple trees based on random subsets of training data, gradient-boosting algorithms train multiple trees sequentially by reducing the errors in the decision trees. Gradient boosting decision trees are based on a popular machine learning technique called <strong>Adaptive Boosting</strong>, where we learn why a machine learning model is making errors, and then train a new machine learning model that reduces the errors from the previous models.</span></p>
<p><span>Gradient boosting algorithms discover patterns in the data that are difficult to represent in the decision trees, and add a greater weight to the training examples, which can lead to correct predictions. Thus, similar to random forests, we generate multiple decision trees from subsets of the training data. However, during each step, the subset of training data is not selected randomly. Instead, we create a subset of training data, where the examples that would lead to fewer errors in decision trees are prioritized. We stop this process when we cannot observe patterns in errors that may lead to more optimizations. </span></p>
<p>Examples of how random forest algorithms and gradient-boosting algorithms are implemented are provided in the next section. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting clicks on log streams</h1>
                </header>
            
            <article>
                
<p>In this section, we will show you how to use tree-based methods to predict who will click on a mobile advertisement given a set of conditions, such as region, where the ad is shown, time of day, location of the banner, and the application delivering the advertisement.  </p>
<p>The dataset we will use throughout the rest of the chapter is obtained from <em>Shioji, Enno, 2017, Adform click prediction dataset,</em> <a href="https://doi.org/10.7910/DVN/TADBY7">https://doi.org/10.7910/DVN/TADBY7</a><em>, Harvard Dataverse, V2</em>.<a href="https://www.kaggle.com/c/avazu-ctr-prediction/data"/></p>
<p>The main task is to build a classifier capable of predicting whether a user will click on an advertisement given the conditions. Having such a model is very useful for ad-tech platforms that select which ads to show to users and when. These platforms can use these models to only show ads to users who are likely to click on the ad being delivered. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The dataset is large enough (5 GB) to justify the use of technologies that span multiple machines to perform the training. We will first look at how to use AWS EMR to carry out this task with Apache Spark. We will also show how to do this with SageMaker services. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Elastic MapReduce (EMR)</h1>
                </header>
            
            <article>
                
<p>EMR is an AWS service that allows us to run and scale Apache Spark, Hadoop, HBase, Presto, Hive, and other big data frameworks. We will cover more EMR details in <a href="691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml">Chapter 15</a>, <em>Tuning Clusters for Machine Learning</em>. However, for now, let's think of EMR as a service that allows us to launch several interconnected machines with running software, such as Apache Spark, that coordinates distributed processing. EMR clusters have a master and several slaves. The master typically orchestrates the jobs, whereas the slaves process and combine the data to provide the master with a result. This result can range from a simple number (for example, a count of rows) to a machine learning model capable of making predictions. The Apache Spark Driver is the machine that coordinates the jobs necessary to complete the operation. The driver typically runs on the master node but it can also be configured to run on a slave node. The Spark executors (the demons that Spark uses to crunch the data) typically run on the EMR slaves. </p>
<p>EMR can also host notebook servers that connect to the cluster. This way, we can run our notebook paragraphs and this will trigger any distributed processing through Apache Spark. There are two ways to host notebooks on Apache Spark: EMR notebooks and JupyterHub EMR Application. We will use the first method in this chapter, and will cover <span>JupyterHub in</span> <a href="691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml">Chapter 15</a>, <span><em>Tuning Clusters for Machine Learning</em></span>.</p>
<p>Through EMR notebooks, you can launch the cluster and the notebook at the same time through the <strong>EMR notebooks</strong> link on the console (<a href="https://console.aws.amazon.com/elasticmapreduce/home">https://console.aws.amazon.com/elasticmapreduce/home</a>).</p>
<p class="mce-root"/>
<p>You can create the cluster and notebook simultaneously by clicking on the <span class="packt_screen">Create Notebook</span> button, as seen in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-962 image-border" src="assets/de2ef949-ffb7-4b85-b029-e104dab52790.png" style="width:124.33em;height:97.25em;"/></p>
<p>Once you create the notebook, it will click on the <span class="packt_screen">Open</span> button, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-963 image-border" src="assets/804659f6-2720-4e02-bb01-6f62a27b27e6.png" style="width:59.33em;height:32.50em;"/></p>
<p>Clicking on the <span class="packt_screen">Open</span> button opens the notebook for us to start coding. The notebook is a standard Jupyter Notebook as it can be seen in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-964 image-border" src="assets/f9a106eb-25fa-4d0f-9af5-3bad1df69518.png" style="width:85.08em;height:20.92em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Alternatively, you can create the cluster separately and attach the notebook to the cluster. The advantage of doing so is that you have access to additional advanced options.</p>
<div class="packt_tip">
<p><span>We recommend at least 10 machines (for instance, 10 m5.xlarge nodes) to run the code from this chapter in a timely fashion.  Additionally, w</span>e suggest you increase the Livy session timeout if your jobs take longer than an hour to complete. For such jobs, the notebook may get disconnected from the cluster. Livy is the software responsible for the communication between the notebook and the cluster. The following screenshot shows the create cluster options including a way to extend the Livy session timeout:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/92fd675d-eee4-4138-ae7d-4179f17419a4.png"/></p>
</div>
<p><span>On </span><a href="691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml">Chapter 15</a>, <em>Tuning Clusters for Machine Learning, </em><span>we will cover more details regarding cluster configuration. </span></p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training with Apache Spark on EMR</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's now explore the training with Apache Spark on EMR.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the data</h1>
                </header>
            
            <article>
                
<p>The first step is to upload the data to EMR. You can do this straight from the notebook or download the dataset locally and then uploaded it to S3 using the command-line tools from AWS (awscli). In order to use the command-line tools from AWS, you need to create AWS access keys on the IAM console. Details on how to do that can be found here: <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html</a>.</p>
<p>Once you have your AWS access and secret keys, you can configure them by executing <kbd>aws configure</kbd> on the command line. </p>
<p>First, we will get a portion of the dataset through the following <kbd>wget</kbd> command:</p>
<pre>wget -O /tmp/adform.click.2017.01.json.gz https://dataverse.harvard.edu/api/access/datafile/:persistentId/?persistentId=doi:10.7910/DVN/TADBY7/JCI3VG</pre>
<p>Next, we will unzip and upload the CSV dataset onto a <kbd>s3</kbd> bucket called <kbd>mastering-ml-aws</kbd> as shown by the following command:</p>
<pre>gunzip /tmp/adform.click.2017.01.json.gz<br/><br/>aws s3 cp /tmp/adform.click.2017.01.json s3://mastering-ml-aws/chapter4/training-data/adform.click.2017.01.json</pre>
<p>Once the data is in S3, we can come back to our notebook and start coding to train the classifier. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>The EMR notebooks, as opposed to the examples we ran locally in previous chapters <em>(</em><a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a>,<em> Classifying Twitter Feeds with Naive Bayes and</em> <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>, <em>Predicting House Value with Regression Algorithms)</em> have implicit variables to access the Spark context. In particular, the Spark session is named <kbd>spark</kbd>. The first paragraph run will always initialize the context and trigger the Spark driver.</p>
<p class="mce-root"/>
<p>In the following screenshot, we can see the spark application starting and a link to the Spark UI:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-966 image-border" src="assets/02ff36ff-e8d8-4321-9ae9-36c3f28ffe4e.png" style="width:39.33em;height:11.58em;"/></p>
<p>The next step is to load our dataset and explore the different the first few rows by running the following snippet:</p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-python2">
<pre>ctr_df = spark.read.json(s3_train_path)
ctr_df.show(5)</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<p class="prompt">The output of the above show command is:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-967 image-border" src="assets/5e2044ea-ac04-4c4d-8a48-9b1fb485b0b0.png" style="width:206.50em;height:25.25em;"/></div>
</div>
</div>
</div>
<p>The <kbd>spark.read.json</kbd>  method, the first command from the preceding code block, reads the JSON data into a dataset similar to what we've done before with CSV using <kbd>spark.read.csv</kbd>. We can observe our dataset has 10 features and an <kbd>l</kbd> column indicating the label which we're trying to predict, that is, if the user clicked (1) or didn't click (0) in the advertisement. You might realize that some features are multivalued (more than one value in a cell) and some are null. To simplify the code examples in this chapter we will just pick the first five features by constructing a new dataset and name these features <kbd>f0</kbd> through <kbd>f4</kbd> while also replacing null features with the value <kbd>0</kbd> and only taking the first value in the case of multivalued features:</p>
<pre>df = ctr_df.selectExpr("coalesce(c0[0],0) as f0",<br/>                       "coalesce(c1[0],0) as f1",<br/>                       "coalesce(c2[0],0) as f2",<br/>                       "coalesce(c3[0],0) as f3",<br/>                       "coalesce(c4[0],0) as f4",<br/>                       "l as click")</pre>
<p>The <kbd>selectExpr</kbd> command above allows us to use SQL-like operations. In this particular case we will use coalesce operation which transforms any null expressions into the value <kbd>0</kbd>. Also note that we're always just taking the first value for multivalued features.</p>
<div class="packt_infobox">Generally, it's a bad idea to discard features as they might carry important predictive value. Likewise, replacing nulls for a fixed value can also be sub-optimal. We should consider common imputation techniques for missing values such as replacing with a point estimate (medians, modes, and means are commonly used). Alternatively, a model can be trained to fill in the missing value from the remaining features. In order to keep our focus on using trees in this chapter, we won't go deeper on the issue of missing values.</div>
<p>Our <kbd>df</kbd> dataset now looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-968 image-border" src="assets/1172103f-359d-456d-84fb-10caa8390080.png" style="width:33.75em;height:9.33em;"/></p>
<p>Now we do something quite Spark specific, which is to reshuffle the different portions of the CSV into different machines and cache them in memory. The command to do such thing is as follows:</p>
<pre>df = df.repartition(100).cache()</pre>
<p>Since we will repeatedly iterate on processing the same dataset, by loading it in memory, it will significantly speed up any future operation made for <kbd>df</kbd> . The repartitioning helps to make sure the data is better distributed throughout the cluster, hence increasing the parallelization.</p>
<p>The <kbd>describe()</kbd> method builds a dataframe with some basic stats (<kbd>min</kbd>, <kbd>max</kbd>, <kbd>mean</kbd>,  <kbd>count</kbd>) of the different fields in our dataset, as seen in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-969 image-border" src="assets/a789e41c-f793-4e40-b93e-490782298ade.png" style="width:189.50em;height:27.50em;"/></p>
<p>We can observe that most features range from low negative values to very large integers, suggesting these are anonymized feature values for which a hash function was applied.  The field we're trying to predict is <kbd>click</kbd>, which is <kbd>1</kbd> when the user clicked on the advertisement and 0 when the user didn't click.  The mean value for the click column informs us that there is certain degree of label imbalance (as about 18% of the instances are clicks). Additionally, the <kbd>count</kbd> row tell us that there is a total of 12,000,000 rows on our dataset. </p>
<p>Another useful inspection is to understand the cardinality of the categorical values.   The following screenshot from our notebooks shows the different number of unique values each feature gets:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5b320793-9f7f-49df-bf65-c31acbf00164.png" style="width:18.67em;height:19.25em;"/></p>
<p>As you can see, the f4 feature is an example of a category that has many distinct values. These kinds of features often require special attention, as we will see later in this section. </p>
<p>Decision trees and most of Spark ML libraries require our features to be numerical only. It happens by chance that our features are already in numerical form, but these really represent categories which were hashed into numbers. In <a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a>, <em>Classifying Twitter Feeds with Naive Bayes,</em> we learned that in order to train a classifier, we need to provide a vector of numbers. For this reason, we need to transform our categories into numbers in our dataset to include them in our vectors. This transformation is often called <strong>feature encoding</strong>. There are two popular ways to do this: through one-hot encoding or categorical encoding (also called <strong>string indexing</strong>).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the following generic examples, we assume that the <kbd>site_id</kbd> feature could only take up to three distinct values: <kbd>siteA</kbd>, <kbd>siteB</kbd>, and <kbd>siteC</kbd>.  These examples will also illustrate the case in which we have string features to encode into numbers (not integer hashes as in our dataset). </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Categorical encoding </h1>
                </header>
            
            <article>
                
<p>Categorical encoding (or string indexing) is the simplest kind of encoding, in which we assign a number to each site value. Let's look at an example in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>site_id</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>site_id_indexed</kbd></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>siteA</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>1</kbd></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>siteB</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>2</kbd></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>siteC</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>3</kbd></td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">One-hot encoding</h1>
                </header>
            
            <article>
                
<p><span><span>In t</span></span>his kind of encoding, we create new binary columns for each possible site value and set the value as <kbd>1</kbd> when the value is present, as shown in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>site_id</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>siteA</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>siteB</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>siteC</kbd></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>siteA</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>1</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>0</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>0</kbd></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>siteB</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>0</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>1</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>0</kbd></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>siteC</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>0</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>0</kbd></td>
<td class="CDPAlignCenter CDPAlign"><kbd>1</kbd></td>
</tr>
</tbody>
</table>
<p> </p>
<p>Categorical encoding is simple; however, it may create an artificial ordering of the features, and some ML algorithms are sensitive to that. One-hot encoding has the additional benefit of supporting multi-valued features (for example, if a row has two sites, we can set a <kbd>1</kbd> in both columns). However, one-hot encoding adds more features to our dataset, which increases the dimensionality. Adding more dimensions to our dataset makes the training more complex and may reduce its predictive ability. This is known as the <strong>curse of dimensionality</strong>. </p>
<p>Let's see how we would use categorical encoding on a sample of our dataset to transform the C1 feature (a categorical feature) into numerical values:</p>
<pre>from pyspark.ml.feature import StringIndexer<br/><br/>string_indexer = StringIndexer(inputCol="f0", outputCol="f0_index")<br/>string_indexer_model = string_indexer.fit(df)<br/>ctr_df_indexed = string_indexer_model.transform(df).select('f0','f0_index')<br/>ctr_df_indexed.show(5)</pre>
<p> The preceding code first instantiates a <kbd>StringIndexer</kbd> that will encode column <kbd>f0</kbd> into a new column <kbd>f0_index</kbd> <span>upon fitting, goes through the dataset and finds distinct feature values that assign an index based on the popularity of such values. Then we can use the </span><kbd>transform()</kbd><span> method to get indices for each value. </span>The output of the preceding final <kbd>show()</kbd> command is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-970 image-border" src="assets/1d2c0b3a-c31c-4f11-bc7f-e7f75243b79e.png" style="width:12.42em;height:10.42em;"/></p>
<p>In the above screenshot we can see the numerical value that each  raw (hashed) categorical value was assigned to.</p>
<p>To perform one-hot encoding on the values, we use the <kbd>OneHotEncoder</kbd> transformer:</p>
<pre>from pyspark.ml.feature import OneHotEncoder<br/><br/>encoder = OneHotEncoder(inputCol="f0_index", outputCol="f0_encoded")<br/>encoder.transform(ctr_df_indexed).distinct().show(5)</pre>
<p><span>The preceding commands generates the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-971 image-border" src="assets/0303965c-ce6c-422a-bbbb-91c92eb20b7a.png" style="width:20.83em;height:9.83em;"/></p>
<p>Note how the different <kbd>f0</kbd> values get mapped to the corresponding boolean vector. We did the encoding for just one feature; however, for training, we need to go through the same process for several features. For this reason, we built a function that builds all the indexing and encoding stages necessary for our pipeline:</p>
<pre>def categorical_one_hot_encoding_stages(columns):<br/>    indexers = [StringIndexer(inputCol=column, <br/>                              outputCol=column + "_index", <br/>                              handleInvalid='keep') <br/>                for column in columns]<br/>    encoders = [OneHotEncoder(inputCol=column + "_index", <br/>                              outputCol=column + "_encoded") <br/>                for column in columns]<br/>    return indexers + encoders</pre>
<p>The following code builds a training pipeline, including the <kbd>DecisionTree</kbd> estimator:</p>
<pre>from pyspark.ml.feature import OneHotEncoder<br/>from pyspark.ml.feature import StringIndexer<br/>from pyspark.ml.feature import ChiSqSelector<br/>from pyspark.ml import Pipeline<br/>from pyspark.ml.feature import VectorAssembler<br/>from pyspark.ml.classification import DecisionTreeClassifier<br/><br/>categorical_columns = ['f0','f1','f2','f3','f4']<br/>encoded_columns = [column + '_encoded' for column in categorical_columns] <br/><br/>categorical_stages = categorical_one_hot_encoding_stages(categorical_columns) vector_assembler = VectorAssembler(inputCols=encoded_columns,<br/>                                   outputCol="features")<br/>selector = ChiSqSelector(numTopFeatures=100, featuresCol="features",<br/>                         outputCol="selected_features", labelCol="click")<br/>decision_tree = DecisionTreeClassifier(labelCol="click",                                       <br/>                                       featuresCol="selected_features")<br/><br/>pipeline = Pipeline(stages=categorical_stages + [vector_assembler, selector, <br/>                                                 decision_tree])</pre>
<p>In the preceding code,<kbd>VectorAssembler</kbd> constructs a vector with all features that require encoding as well as the numerical features ( <kbd>VectorAssembler</kbd> can take as input columns that can be vectors or scalars so you can use numerical features directly if existent in your dataset). Given the high number of one-hot-encoded values, the feature vector can be huge and make the trainer very slow or require massive amounts of memory. One way to mitigate that is to use a <strong>chi-squared</strong> feature selector. In our pipeline, we have selected the best 100 features. By best, we mean the features that have more predictive power—note how the chi-squared estimator takes both the features and the label to decide on the best features. Finally, we include the decision engine estimator stage, which is the one that will actually create the classifier. </p>
<p class="mce-root"/>
<div class="packt_tip"><span> If we attempt to string index features with very large cardinality, the driver will collect all possible values (in order to keep a value-to-index dictionary for transformation). In such an attempt, the driver will most likely run out of memory as we're looking at millions of distinct values to keep. For these cases, you need other strategies, such as keeping only the features with the most predictive ability or considering only the most popular values. Check out our article, which includes a solution to this problem at </span><a href="https://medium.com/dataxutech/how-to-write-a-custom-spark-classifier-categorical-naive-bayes-60f27843b4ad">https://medium.com/dataxutech/how-to-write-a-custom-spark-classifier-categorical-naive-bayes-60f27843b4ad</a><span>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a model</h1>
                </header>
            
            <article>
                
<p>Our pipeline is now constructed, so we can proceed to split our dataset for testing and training and then we fit the model:</p>
<pre>train_df, test_df = df.randomSplit([0.8, 0.2], seed=17)<br/>pipeline_model = pipeline.fit(train_df)</pre>
<p>Once this is executed, the Spark Driver will figure out the best plan for distributing the processing necessary to train the model across many machines.</p>
<p>By following the Spark UI link shown at the beginning of this section, we can see the status of the different jobs running on EMR:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-972 image-border" src="assets/1ded0e39-d9eb-4dd5-9e98-a0a93532ff6a.png" style="width:178.92em;height:74.75em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Once the model is trained, we can explore the decision tree behind it. We can do this by inspecting the last stage of the pipeline (that is, the decision tree model). </p>
<p>The following code snippet shows the result of outputting the decision tree in text format:</p>
<pre>print(pipeline_model.stages[-1].toDebugString)<br/><br/>DecisionTreeClassificationModel (uid=DecisionTreeClassifier_3cc3252e8007) of depth 5 with 11 nodes<br/>  If (feature 3 in {1.0})<br/>   Predict: 1.0<br/>  Else (feature 3 not in {1.0})<br/>   If (feature 21 in {1.0})<br/>    Predict: 1.0<br/>   Else (feature 21 not in {1.0})<br/>    If (feature 91 in {1.0})<br/>     Predict: 1.0<br/>    Else (feature 91 not in {1.0})<br/>     If (feature 27 in {1.0})<br/>      Predict: 1.0<br/>     Else (feature 27 not in {1.0})<br/>      If (feature 29 in {1.0})<br/>       Predict: 1.0<br/>      Else (feature 29 not in {1.0})<br/>       Predict: 0.0 </pre>
<p>Note how each decision is based on a feature that takes a value of <kbd>0</kbd> or <kbd>1</kbd>. This is because we have used one-hot encoding on our pipeline. If we had used the categorical encoding (string indexing), we would have seen a condition that involves several indexed values, such as the following example:</p>
<pre> If (feature 1 in {3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,17.0,27.0})
       Predict: 0.0
 Else (feature 1 not in {3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,17.0,27.0})
       Predict: 1.0</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating our model</h1>
                </header>
            
            <article>
                
<p class="mce-root">Contrary to our Twitter classification problem in <a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a>, <em>Classifying Twitter Feeds with Naive Baye</em>s, the label in this dataset is very skewed. This is because there are only a few occasions where users decide to click on ads. The accuracy measurement we used in <a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a>, <em>Classifying Twitter Feeds with Naive Bayes</em>, would not be suitable, as a model that never predicts a click would still have very high accuracy (all non-clicks would result in correct predictions). Two possible alternatives for this case could be to use metrics derived from the ROC or <span><strong>precision-recall curves</strong></span>, which can be seen in the following section.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Area Under ROC Curve</h1>
                </header>
            
            <article>
                
<p><span>The</span> <strong><span>Receiver Operating Characteristic</span></strong> (<strong>ROC</strong>) is a representation of a trade-off between true-positive rates and false-positive rates. True-positive rates describe how good a model is at predicting a positive class when the actual class is positive. True-positive rates are calculated as the ratio of true positives predicted by a model, to the sum of true positives and false negatives. False-positive rates describe how often the model predicts the positive class, when the actual class is negative. False-positive rates are calculated as the ratio of false positives, to the sum of false positives and true negatives. ROC is a plot where the <em>x</em> axis is represented by the false-positive rate with a range of 0-1, while the <em>y</em> axis is represented as the true-positive rate. <strong>Area Under Curve</strong> (<strong>AUC</strong>) is the measure of the area under the ROC curve. AUC is a measure of predictiveness of a classification model. </p>
<p>Three examples of receiver operator curves are seen in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fc95ba2b-6336-4f9b-96e7-dec2cbbcb0d4.png" style="width:42.25em;height:29.58em;"/></p>
<p class="mce-root"/>
<p>In the preceding plot, the dotted line represents an example of when <kbd>AUC</kbd> is <kbd>1</kbd>. Such AUCs occur when all the positive outcomes are classified correctly. The solid line represents the <kbd>AUC</kbd> that is <kbd>0.5</kbd>. For a binary classifier, the <kbd>AUC</kbd> is <kbd>0.5</kbd> when the predictions coming from the machine learning model are similar to randomly generating an outcome. This indicates that the machine learning model is no better than a random-number generator in predicting outcomes. The dashed line represents the <kbd>AUC</kbd> that is <kbd>0.66</kbd>. This happens when a machine learning model predicts some examples correctly, but not all. However, if the <kbd>AUC</kbd> is higher than <kbd>0.5</kbd> for the binary classifier, the model is better than just randomly guessing the outcome. However, if it is below 0.5, this means that the machine learning model is worse than a random-outcome generator. Thus, AUC is a good measure of comparing machine learning models and evaluating their effectiveness.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Area under the precision-recall curve</h1>
                </header>
            
            <article>
                
<p class="mce-root">The precision-recall<span> </span>curve represents a tradeoff between precision and<span> </span>recall<span> </span>in a prediction model. <strong>Precision</strong> is defined as the ratio of true positives to the total number of positive predictions<span> </span>made<span> </span>by the model.<span> </span><strong>Recall</strong><span> </span>is defined as the ratio of positive predictions to the total number of actual positive predictions. </p>
<div class="packt_infobox">Note that the precision-recall curve does not model true negative values. This is useful in cases of the unbalanced dataset. ROC curves may provide a very optimistic view of a model if the model is good at classifying true negatives and generates a smaller number of false positives. </div>
<p>The following plot shows an example of a precision-recall curve:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-823 image-border" src="assets/d6012ad5-b4ee-4bdd-965c-db7d4adb5d20.png" style="width:39.50em;height:27.00em;"/></p>
<p>In the preceding screenshot, the dashed line shows when the area under precision-recall curve is <kbd>0.5</kbd>. This indicates that the precision is always 0.5, which is similar to a random-number generator. The solid line represents the precision-recall curve that is better than random. The precision recall curve also can be used to evaluate a machine learning model, similar to the ROC area. However, the precision-recall curve should be used when the dataset is unbalanced, and the ROC should be used when the dataset is balanced.</p>
<p>So, going back to our example, we can use Spark's <kbd>BinaryClassificationEvaluator</kbd> to calculate the scores by providing the actual and predicted labels on our test dataset. First we will apply the model on our test dataset to get the predictions and scores:</p>
<pre>test_transformed = pipeline_model.transform(test_df)</pre>
<p>By applying the previous transformation <kbd>test_transformed</kbd> will have all columns included in <kbd>test_df</kbd> plus an additional one called <kbd>rawPrediction</kbd> which will have a score which can be used for evaluation:</p>
<pre><br/>from pyspark.ml.evaluation import BinaryClassificationEvaluator<br/><br/>evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction", <br/>                                          labelCol="click")<br/>evaluator.evaluate(test_transformed, <br/>                   {evaluator.metricName: "areaUnderROC"})</pre>
<p class="mce-root"/>
<p>The output of the preceding command is 0.43. The fact that we got an ROC metric lower than 0.5 means that our classifier is even worse than random classifier and hence it is not a good model for predicting clicks! In the next section, we will show how to use ensemble models to improve our predictive ability. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training tree ensembles on EMR</h1>
                </header>
            
            <article>
                
<p>Decision trees can be useful for understanding the decisions made by our classifier, especially when decision trees are small and readable. However, decision trees tend to overfit the data (by learning the details of the training dataset and not being able to generalize on new data). For this reason, ML practitioners tend to use tree ensembles, such as random forests and gradient-boosted trees, which are explained in the previous sections in this chapter under <em>Understanding gradient boosting algorithms</em> and <em>Understanding random forest algorithms</em>.</p>
<p>In our code examples, to use random forests or gradient boosted trees, we just need to replace the last stage of our pipeline with the corresponding constructor:</p>
<pre>from pyspark.ml.classification import RandomForestClassifier<br/><br/>random_forest = RandomForestClassifier(labelCol="click",                                                                                        <br/>                                       featuresCol="features")<br/><br/>pipeline_rf = Pipeline(stages=categorical_stages + \<br/>                              [vector_assembler, random_forest])</pre>
<p>Note how we get a better ROC value with random forests on our sampled dataset:</p>
<pre>rf_pipeline_model = pipeline_rf.fit(train_df)<br/><br/>evaluator.evaluate(rf_pipeline_model.transform(test_df), <br/>                   {evaluator.metricName: "areaUnderROC"})<br/><br/>&gt;&gt; 0.62</pre>
<p>We can see that now we get a ROC greater than 0.5 which means that our model has improved an is now better than random guessing. Similarly, you can train a gradient boosted tree with the <kbd>pyspark.mllib.tree.GradientBoostedTrees</kbd> class. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training gradient-boosted trees with the SageMaker services</h1>
                </header>
            
            <article>
                
<p>In the <em>Training a model</em> and <em>Evaluating our model</em> sections, we learned how to build and evaluate a random forest classifier using Spark on EMR. In this section, we will see how to train a gradient boosted tree using the SageMaker services through the SageMaker notebooks. The XGBoost SageMaker service allows us to train gradient-boosted trees in a distributed fashion. Given that our clickthrough data is relatively large, it will be convenient to use such a service. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>In order to use the SageMaker services, we will need to place our training and testing data in S3. The documentation at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html</a> requires us to drop the data as CSV files where the first column indicates the training label (target feature) and the rest of the columns represent the training features (other formats are supported but we will use CSV in our example). For splitting and preparing the data in this way, EMR is still the best option as we want our data preparation to be distributed as well. Given our testing and training Spark datasets from the last <em>Preparing the data</em> section, we can apply the pipeline model, not for getting predictions in this case, but instead, for obtaining the selected encoded features for each row.  </p>
<p>In the following snippet, for both <kbd>test_df</kbd> and <kbd>train_df</kbd> we apply the model transformation:</p>
<pre>test_transformed = model.transform(test_df)<br/>train_transformed = model.transform(train_df)</pre>
<p>The following screenshot shows the last three columns of the <kbd>test_transformed</kbd> dataframe:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-973 image-border" src="assets/04932beb-ce2f-4b4b-af48-02cff4bab6f9.png" style="width:39.33em;height:12.33em;"/></p>
<p>The transformed datasets includes the feature vector column (named <kbd>selected_features</kbd> with a size of 100). We need to transform these two columns into a CSV with 101 columns (the <kbd>click</kbd> and the <kbd>selected_features</kbd> vectors flattened out). A simple transformation in Spark allows us to do this. We define a <kbd>deconstruct_vector</kbd> function, which we will use to obtain a Spark dataframe with the label and each vector component as a distinct column. We then save that to S3 both for training and testing as a CSV without headers, as SageMaker requires.</p>
<p>In the following code snippet, we provide the <kbd>deconstruct_vector</kbd> function as well as the series of transformations needed to save the dataframe:</p>
<pre>def deconstruct_vector(row):<br/>    arr = row['selected_features'].toArray()<br/>    return tuple([row['click']] + arr.tolist())<br/><br/>df_for_csv = train_transformed.select("click", "selected_features") \<br/>                .rdd.map(deconstruct_vector).toDF() <br/><br/>df_for_csv.write.csv('s3://mastering-ml-aws/chapter4/train-trans-vec-csv-1/', <br/>                     header=False)</pre>
<p>In a similar fashion, we will save an additional CSV file that will not include the label (just the features) under the <kbd>s3://mastering-ml-aws/chapter4/test-trans-vec-csv-no-label</kbd> path. We will use this dataset to score the testing dataset through the SageMaker batch transform job in the next section, <em>Training with SageMaker XGBoost</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training with SageMaker XGBoost   </h1>
                </header>
            
            <article>
                
<p>Now that our datasets for training and testing are in S3 in the right format, we can launch our SageMaker notebook instance and start coding our trainer. Let's perform the following steps:</p>
<ol>
<li>Instantiate the SageMaker session, container, and variables with the location of our datasets:</li>
</ol>
<pre style="padding-left: 60px">import sagemaker<br/>from sagemaker import get_execution_role<br/>import boto3<br/><br/>sess = sagemaker.Session()<br/>role = get_execution_role()<br/>container = sagemaker.amazon.amazon_estimator.get_image_uri('us-east-1', <br/>                                                            'xgboost', <br/>                                                            'latest')<br/><br/>s3_validation_data = \<br/>    's3://mastering-ml-aws/chapter4/test-trans-vec-csv-1/'<br/>s3_train_data = \<br/>    's3://mastering-ml-aws/chapter4/train-trans-vec-csv-1/'<br/>s3_test_data = \<br/>    's3://mastering-ml-aws/chapter4/test-trans-vec-csv-no-label/'<br/>s3_output_location = \<br/>    's3://mastering-ml-aws/chapter4/sagemaker/output/xgboost/'</pre>
<ol start="2">
<li>Create a classifier by instantiating a SageMaker estimator and providing the basic parameters, such as the number and type of machines to use (details can be found in the AWS documentation at <a href="https://sagemaker.readthedocs.io/en/stable/estimators.html">https://sagemaker.readthedocs.io/en/stable/estimators.html</a> ):</li>
</ol>
<pre style="padding-left: 60px"><span class="n">sagemaker_model</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">Estimator</span><span class="p">(</span><span class="n">container</span><span class="p">,<br/></span>    role,<br/>    train_instance_count=1,<br/>    train_instance_type='ml.c4.4xlarge',<br/>    train_volume_size=30,<br/>    train_max_run=360000,<br/>    input_mode='File',<br/>    output_path=s3_output_location,<br/>    sagemaker_session=sess)</pre>
<ol start="3">
<li>Set the hyperparameters of our trainer. The details can be found in the documentation (and we will cover it in more detail in <a href="7de65295-dd1f-4eb3-af00-3868ed7e2df9.xhtml">Chapter 14</a>, <em>Optimizing SageMaker and Spark Machine Learning Models</em>). The main parameter to look at here is the objective, which we have set for binary classification (using a logistic regression score, which is the standard way XGBoost performs classification). XGBoost can also be used for other problems, such as regressions or multi-class classification:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">sagemaker_model</span><span class="o">.</span><span class="n">set_hyperparameters</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="s1">'binary:logistic'</span><span class="p">,<br/></span>    max_depth=5,<br/>    eta=0.2,<br/>    gamma=4,<br/>    min_child_weight=6,<br/>    subsample=0.7,<br/>    silent=0,<br/>    num_round=50)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Before fitting the model, we need to specify the location and format of the input (there are a couple of formats accepted; we have chosen CSV for our example):</li>
</ol>
<pre style="padding-left: 60px">train_data = sagemaker.session.s3_input(s3_train_data, <br/>    distribution='FullyReplicated',<br/>    content_type='text/csv', <br/>    s3_data_type='S3Prefix')<br/><br/>validation_data = sagemaker.session.s3_input(s3_validation_data, <br/>    distribution='FullyReplicated',<br/>    content_type='text/csv', <br/>    s3_data_type='S3Prefix')<br/><br/>data_channels = {'train': train_data, <br/>                 'validation': validation_data}<br/><br/>sagemaker_model.fit(inputs=data_channels, <br/>                    logs=True)</pre>
<ol start="5">
<li>Invoking the <kbd>fit</kbd> function will train the model with the data provided (that is, the data we saved in S3 through our EMR/Spark preparation):</li>
</ol>
<pre style="padding-left: 60px">INFO:sagemaker:Creating training-job with name: xgboost-2019-04-27-20-39-02-968<br/>2019-04-27 20:39:03 Starting - Starting the training job...<br/>2019-04-27 20:39:05 Starting - Launching requested ML instances......<br/>...<br/>train-error:0.169668#011validation-error:0.169047<br/>2019-04-27 20:49:02 Uploading - Uploading generated training model<br/>2019-04-27 20:49:02 Completed - Training job completed<br/>Billable seconds: 480</pre>
<p>The logs will show the some details about the training and validation error being optimized by XGBoost, as well as the status of the job and training costs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying and evaluating the model</h1>
                </header>
            
            <article>
                
<p><span><span>The following steps will show you how to use <kbd>sagemaker</kbd> to create batch predictions so you can evaluate the model.</span></span></p>
<p class="mce-root"/>
<p><span>In order to obtain predictions, we can use a batch transform job:</span></p>
<pre>transformer = sagemaker_model.transformer(instance_count=1, <br/>                                          instance_type='ml.m4.2xlarge',<br/>                                          output_path=s3_output_location)<br/>transformer.transform(s3_test_data, <br/>                      content_type='text/csv', <br/>                      split_type='Line')<br/>transformer.wait()</pre>
<p>For every file in the input <kbd>s3</kbd> directory, the batch transform job will produce a file with the scores:</p>
<pre>aws s3 ls s3://mastering-ml-aws/chapter4/sagemaker/output/xgboost/ | head</pre>
<p>The preceding command generates the following output:</p>
<pre class="mce-root">2019-04-28 01:29:58 361031 part-00000-19e45462-84f7-46ac-87bf-d53059e0c60c-c000.csv.out<br/>2019-04-28 01:29:58 361045 part-00001-19e45462-84f7-46ac-87bf-d53059e0c60c-c000.csv.out</pre>
<p>We can then load this single-column CSV file into a <kbd>pandas</kbd> dataframe:</p>
<pre>import pandas as pd<br/><br/>scores_df = pd.read_csv(output_path + \<br/>   'part-00000-19e45462-84f7-46ac-87bf-d53059e0c60c-c000.csv.out',<br/>    header=None, <br/>    names=['score'])</pre>
<p>These scores represent probabilities (derived via logistic regression). If we had set the objective to binary: hinge, we would get actual predictions instead. Choosing which kind to use depends on the type of application. In our case, it seems useful to gather probabilities, as any indication of a particular user being more likely to perform clicks would help to improve the marketing targeting.</p>
<p>One of the advantages of SageMaker XGBoost is that it provides a serialization in S3 of a compatible <span>XGBoost</span> model with Python’s standard serialization library (pickle). As an example, we will take a portion of our test data in S3 and run the model to get scores. With this, we can compute the area under the ROC curve by performing the following steps:</p>
<ol>
<li>Locate the model tarball in <kbd>s3</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>aws s3 ls --recursive s3://mastering-ml-aws/chapter4/sagemaker/output/xgboost/ | grep model</strong></pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The output looks as follows:</p>
<pre class="mce-root" style="padding-left: 60px"><strong>chapter4/sagemaker/output/xgboost/xgboost-2019-04-27-20-39-02-968/output/model.tar.gz</strong></pre>
<p>Copy the model from S3 to our local directory and uncompress the tarball:</p>
<pre style="padding-left: 60px"><strong><span>aws s3 cp s3://mastering-ml-aws/chapter4/sagemaker/output/xgboost/xgboost-2019-04-27-20-39-02-968/output/model.tar.gz /tmp/model.tar.gz<br/>tar xvf model.tar.gz</span></strong></pre>
<p style="padding-left: 60px">Here is the output of the preceding command, showing the name of the file uncompressed from the tarball:</p>
<pre class="mce-root" style="padding-left: 60px"><strong><span>xgboost-model</span></strong></pre>
<ol start="3">
<li>Once the model is locally downloaded and untared, we can load the model in memory via the <kbd>pickle</kbd> serialization library:</li>
</ol>
<pre style="padding-left: 60px">import xgboost<br/>import pickle as pkl<br/><br/>model_local = pkl.load(open('xgboost-model', 'rb'))</pre>
<ol start="4">
<li>Define the names of our columns (<kbd>f0</kbd> to <kbd>f99</kbd> for the features, and <kbd>click</kbd> as the label) and load the validation data from S3:</li>
</ol>
<pre style="padding-left: 60px">column_names = ['click'] + ['f' + str(i) for i in range(0, 100)]<br/>validation_df = pd.read_csv(s3_validation_data + \<br/>                            'part-00000-25f35551-ffff-41d8-82a9-75f429553035-c000.csv',<br/>                            header=None, <br/>                            names=column_names)</pre>
<ol start="5">
<li>To create predictions with <kbd>xgboost</kbd>, we need to assemble a matrix from our <kbd>pandas</kbd> dataframe. Select all columns except the first one (which is the label), and then construct a DMatrix. Call the predict method from the <kbd>xgboost</kbd> model to get the scores for every row:</li>
</ol>
<pre style="padding-left: 60px">import xgboost<br/>matrix = xgboost.DMatrix(validation_df[column_names[1:]])<br/>validation_df['score'] = model_local.predict(matrix)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">In the following screenshot, the reader can see how the dataframe looks:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-975 image-border" src="assets/fd2a3999-60a2-431b-966a-ff6fe1cf1c58.png" style="width:43.00em;height:10.58em;"/></p>
<ol start="6">
<li>Given the <kbd>click</kbd> column and the <kbd>score</kbd> column, we can construct the ROC AUC:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.metrics import roc_auc_score<br/>roc_auc_score(validation_df['click'], validation_df['score'])</pre>
<p>For our sample, we get a AUC value of 0.67, which is comparable to the value we got with Spark's random forests.</p>
<p>In this chapter, we did not focus on building the most optimal model for our dataset. Instead, we focused on providing simple and popular transformations and tree models you can use to classify large volumes of data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered the basic theoretical concepts for understanding tree ensembles and showed ways to train and evaluate these models in EMR, through Apache Spark, as well as through the SageMaker XGBoost service. Decision tree ensembles are one of the most popular classifiers, for many reasons:</p>
<ul>
<li>They are able to find complex patterns in relatively short training time and with few resources. The <span>XGBoost </span>library is known as the most popular classifier among Kaggle competition winners (these are competitions held to find the best model for an open dataset).</li>
<li>It's possible to understand why the classifier is predicting a given value. Following the decision tree paths or just looking at the feature importance are quick ways to understand the rationale behind the decisions made by tree ensembles. </li>
<li>Implementations of distributed training are available through Apache Spark and XGBoost. </li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span><span>In the next chapter, we will look into how to use machine learning to cluster customers based on their behavioral patterns. </span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the main difference between random forests and gradient-boosted trees?</li>
<li>Explain why the Gini Impurity may be interpreted as the misclassification rate.</li>
<li>Explain why it is necessary to perform feature encoding for categorical features.</li>
<li>In this chapter, we provided two ways to do feature encoding. Find one other way to encode categorical features. </li>
<li>Explain why the accuracy metric we used in <a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a>, <em>Classifying Twitter Feeds with Naive Bayes,</em> is not suitable for predicting clicks on our dataset.</li>
<li>Find other objectives we can use for the XGBoost algorithm. When would you use each objective?</li>
</ol>


            </article>

            
        </section>
    </body></html>