- en: Data Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The single most important thing for a beginner to know about **machine learning**
    (**ML**) is that *machine learning is not magic*. Taking a large dataset and naively
    applying a neural network to it will not automatically give you earth-shaking
    insights. ML is built on top of sound and familiar mathematical principles, such
    as probability, statistics, linear algebra, and vector calculus—voodoo not included
    (though some readers may liken vector calculus to voodoo)!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variable identification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing values treatment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outlier treatment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One misconception I would like to dispel early on is that implementing the ML
    algorithm itself is the bulk of the work you'll need to do to accomplish some
    task. If you're new to this, you may be under the impression that 95% of your
    time should be spent on implementing a neural network, and that the neural network
    is solely responsible for the results you get. Build a neural network, put data
    in, magically get results out. What could be easier?
  prefs: []
  type: TYPE_NORMAL
- en: 'The reality of ML is that the algorithm you use is only as good as the data
    you put into it. Furthermore, the results you get are only as good as your ability
    to process and interpret them. The age-old computer science acronym **GIGO** fits
    well here: *Garbage In*, *Garbage Out*.'
  prefs: []
  type: TYPE_NORMAL
- en: When implementing ML techniques, you must also pay close attention to their
    preprocessing and postprocessing of data. Data preprocessing is required for many
    reasons, and is the focus of this chapter. Postprocessing relates to your interpretation
    of the algorithm's output, whether your confidence in the algorithm's result is
    high enough to take action on it, and your ability to apply the results to your
    business problem. Since postprocessing of results strongly depends on the algorithm
    in question, we'll address postprocessing considerations as they come up in our
    specific examples throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing of data, like postprocessing of data, often depends on the algorithm
    used, as different algorithms have different requirements. One straightforward
    example is image processing with **Convolutional Neural Networks** (**CNNs**),
    covered in a later chapter. All images processed by a single CNN are expected
    to have the same dimensions, or at least the same number of pixels and the same
    number of color channels (RGB versus RGBA versus grayscale, and so on). The CNN
    was configured to expect a specific number of inputs, and so every image you give
    to it must be preprocessed to make sure it complies with the neural network's
    expectations. You may need to resize, scale, crop, or pad input images before
    feeding them to the network. You may need to convert color images to grayscale.
    You may need to detect and remove images that have been corrupted from your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some algorithms simply won''t work if you attempt to give them the wrong input.
    If a CNN expects 10,000 grayscale pixel intensity inputs (namely an image that''s
    100 x 100 pixels), there''s no way you can give it an image that''s sized 150
    x 200\. This is a best-case scenario for us: the algorithm fails loudly, and we
    are able to change our approach before attempting to use our network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other algorithms, however, will fail silently if you give them bad input. The
    algorithm will appear to be working, and even give you results that look reasonable
    but are actually wholly inaccurate. This is our worst-case scenario: we think
    the algorithm is working as expected, but in reality we''re in a GIGO situation.
    Just think about how long it will take you to discover that the algorithm is actually
    giving you nonsensical results. How many bad business decisions have you made
    based on incorrect analysis or poor data? These are the types of situations we
    must avoid, and it all starts at the beginning: making sure the data we use is
    appropriate for the application.'
  prefs: []
  type: TYPE_NORMAL
- en: Most ML algorithms make assumptions about the data they process. Some algorithms
    expect data to be of a given size and shape (as in neural networks), some algorithms
    expect data to be bucketed, some algorithms expect data to be normalized over
    a range (between 0 and 1 or between -1 and +1), some algorithms are resilient
    to missing values and others aren't. It is ultimately your responsibility to understand
    what assumptions the algorithm makes about your data, and also to align the data
    with the expectations of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the most part, the aforementioned relates to the format, shape, and size
    of data. There is another consideration: the quality of the data. A data point
    may be perfectly formatted and aligned with the expectations of an algorithm,
    but still be *wrong.* Perhaps someone wrote down the wrong value for a measurement,
    maybe there was an instrumentation failure, or maybe some environmental effect
    has contaminated or tainted your data. In these cases the format, shape, and size
    may be correct, but the data itself may harm your model and prevent it from converging
    on a stable or accurate result. In many of these cases, the data point in question
    is an **outlier**, or a data point that doesn''t seem to fit within the set.'
  prefs: []
  type: TYPE_NORMAL
- en: Outliers exist in real life, and are often valid data. It's not always apparent
    by looking at the data by itself whether an outlier is valid or not, and we must
    also consider the context and algorithm when determining how to handle the data.
    For instance, let's say you're running a meta-analysis that relates patients'
    height to their heart performance and you've got 100 medical records available
    to analyze. One of the patients is listed with a height of 7'3" (221 cm). Is this
    a typo? Did the person who recorded the data actually mean 6'3" (190 cm)? What
    are the odds that, of only 100 random individuals, one of them is actually that
    tall? Should you still use this data point in your analysis, even though it will
    skew your otherwise very clean-looking results? What if the sample size were 1
    million records instead of only 100? In that case, it's much more likely that
    you did actually select a very tall person. What if the sample size were only
    100, but they were all NBA players?
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, dealing with outliers is not straightforward. You should always
    be hesitant to discard data, especially if in doubt. By discarding data, you run
    the risk of creating a self-fulfilling prophecy by which you've consciously or
    subconsciously selected only the data that will support your hypothesis, even
    if your hypothesis is wrong. On the other hand, using legitimately bad data can
    ruin your results and prevent progress.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss a number of different considerations you must
    make when preprocessing data, including data transformations, handling missing
    data, selecting the correct parameters, handling outliers, and other forms of
    analysis that will be helpful in the data preprocessing stage.
  prefs: []
  type: TYPE_NORMAL
- en: Feature identification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine that you are responsible for placing targeted product advertisements
    on an e-commerce store that you help run. The goal is to analyze a visitor''s
    past shopping trends and select products to display that will increase the shopper''s
    likelihood to make a purchase. Given then the gift of foresight, you''ve been
    collecting 50 different metrics on all of your shoppers for months: you''ve been
    recording past purchases, the product categories of those purchases, the price
    tag on each purchase, the time on site each user spent before making a purchase,
    and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Believing that ML is a silver bullet, believing that more data is better, and
    believing that more training of your model is better, you load all 50 dimensions
    of data into an algorithm and train it for days on end. When testing your algorithm
    you find that its accuracy is very high when evaluating data points that you've
    trained the algorithm on, but also find that the algorithm fails spectacularly
    when evaluating against your validation set. Additionally, the model has taken
    a very long time to train. What went wrong here?
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you''ve made the assumption that all of your 50 dimensions of data are
    relevant to the task at hand. It turns out that not all data is relevant. ML is
    great at finding patterns within data, but not all data actually contains patterns.
    Some data is random, and other data is not random but is also uninteresting. One
    example of uninteresting data that fits a pattern might be the time of day that
    the shopper is browsing your site on: users can only shop while they''re awake,
    so most of your users shop between 7 a.m. and midnight. This data obviously follows
    a pattern, but may not actually affect the user''s purchase intent. Of course,
    there may indeed be an interesting pattern: perhaps night owls tend to make late-night
    impulse purchases—but maybe not.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, using all 50 dimensions and training your model for a long period of
    time may cause overfitting of your model: instead of being able to generalize
    behavioral patterns and making shopping predictions, your overfitted model is
    now very good at identifying that a certain behavior represents Steve Johnson
    (one specific shopper), rather than generalizing Steve''s behavior into a widely
    applicable trend. This overfit was caused by two factors: the long training time
    and the existence of irrelevant data in the training set. If one of the dimensions
    you''ve recorded is largely random and you spend a lot of time training a model
    on that data, the model may end up using that random data as an identifier for
    a user rather than filtering it out as a non-trend. The model may learn that, when
    the user''s time on site is exactly 182 seconds, they will purchase a product
    worth $120, simply because you''ve trained the model on that data point many thousands
    of times in the training process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a different example: face identification. You''ve got thousands
    of photos of peoples'' faces and want to be able to analyze a photo and determine
    who the subject is. You train a CNN on your data, and find that the accuracy of
    your algorithm is quite low, only being able to correctly identify the subject
    60% of the time. The problem here may be that your CNN, working with raw pixel
    data, has not been able to automatically identify the features of a face that
    actually matter. For instance, Sarah Jane always takes her selfies in her kitchen,
    and her favorite spatula is always on display in the background. Any other user
    who also happens to have a spatula in the picture may be falsely identified as
    Sarah Jane, even if their faces are quite different. The data has overtrained
    the neural network to recognize spatulas as Sarah Jane, rather than actually looking
    at the user''s face.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In both of these examples, the problem starts with insufficient preprocessing
    of data. In the e-commerce store example, you have not correctly identified the
    features of a shopper that actually matter, and so have trained your model with
    a lot of irrelevant data. The same problem exists in the face detection example:
    not every pixel in the photograph represents a person or their features, and in
    seeing a reliable pattern of spatulas the algorithm has learned that Sarah Jane
    is a spatula.'
  prefs: []
  type: TYPE_NORMAL
- en: To solve both of these problems, you will need to make better selections of
    the features that you give to your ML model. In the e-commerce example, it may
    turn out that only 10 of your 50 recorded dimensions are relevant, and to fix
    the problem you must identify what those 10 dimensions are and only use those
    when training your model. In the face detection example, perhaps the neural network
    should not receive raw pixel intensity data but instead facial dimensions such
    as *nose bridge length*, *mouth width*, *distance between pupils*, *distance between
    pupil and eyebrow*, *distance between earlobes*, *distance from chin to hairline*,
    and so on. Both of these examples demonstrate the need to select the most relevant
    and appropriate features of your data. Making the appropriate selection of features
    will serve to improve both the speed and accuracy of your model.
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In ML applications, we often have high-dimensional data. If we're recording
    50 different metrics for each of our shoppers, we're working in a space with 50
    dimensions. If we're analyzing grayscale images sized 100 x 100, we're working
    in a space with 10,000 dimensions. If the images are RGB-colored, the dimensionality
    increases to 30,000 dimensions (one dimension for each color channel in each pixel
    in the image)!
  prefs: []
  type: TYPE_NORMAL
- en: This problem is called the **curse of dimensionality**. On one hand, ML excels
    at analyzing data with many dimensions. Humans are not good at finding patterns
    that may be spread out across so many dimensions, especially if those dimensions
    are interrelated in counter-intuitive ways. On the other hand, as we add more
    dimensions we also increase the processing power we need to analyze the data,
    and we also increase the amount of training data required to make meaningful models.
  prefs: []
  type: TYPE_NORMAL
- en: One area that clearly demonstrates the curse of dimensionality is **natural
    language processing** (**NLP**). Imagine you are using a Bayesian classifier to
    perform sentiment analysis of tweets relating to brands or other topics. As you
    will learn in a later chapter, part of data preprocessing for NLP is tokenization
    of input strings into **n-grams**, or groups of words. Those n-grams are the features
    that are given to the Bayesian classifier algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a few input strings: `I love cheese`, `I like cheese`, `I hate cheese`,
    `I don''t love cheese`, `I don''t really like cheese`. These examples are straightforward
    to us, since we''ve been using natural language our entire lives. How would an
    algorithm view these examples, though? If we are doing a 1-gram or **unigram**
    analysis—meaning that we split the input string into individual words—we see `love`
    in the first example, `like` in the second, `hate` in the third, `love` in the
    fourth, and `like` in the fifth. Our unigram analysis may be accurate for the
    first three examples, but it fails on the fourth and fifth because it does not
    learn that `don''t love` and `don''t really like` are coherent statements; the
    algorithm is only looking at the effects of individual words. This algorithm runs
    very quickly and requires little storage space, because in the preceding example
    there are only seven unique words used in the four phrases above (`I`, `love`,
    `cheese`, `like`, `hate`, `don''t`, and `really`).'
  prefs: []
  type: TYPE_NORMAL
- en: You may then modify the tokenization preprocessing to use `bigrams`, or 2-grams—or
    groups of two words at a time. This increases the dimensionality of our data,
    requiring more storage space and processing time, but also yields better results.
    The algorithm now sees dimensions like `I love` and `love cheese`, and can now
    also recognize that `don't love` is different from `I love`. Using the bigram
    approach the algorithm may correctly identify the sentiment of the first four
    examples but still fail for the fifth, which is parsed as `I don't`, `don't really`,
    `really like`, and `like cheese`. The classification algorithm will see `really
    like` and `like cheese`, and incorrectly relate that to the positive sentiment
    in the second example. Still, the bigram approach is working for 80% of our examples.
  prefs: []
  type: TYPE_NORMAL
- en: You might now be tempted to upgrade the tokenization once more to capture trigrams,
    or groups of three words at a time. Instead of getting an increase in accuracy,
    the algorithm takes a nosedive and is unable to correctly identify anything. We
    now have too many dimensions in our data. The algorithm learns what `I love cheese` means,
    but no other training example includes the phrase `I love cheese` so that knowledge
    can't be applied in any way. The fifth example parses into the trigrams `I don't
    really`, `don't really like`, and `really like cheese`—none of which have ever
    been encountered before! This algorithm ends up giving you a 50% sentiment for
    every example, because there simply isn't enough data in the training set to capture
    all of the relevant combinations of trigrams.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the curse of dimensionality at play: the **trigram** approach may indeed
    give you better accuracy than the bigram approach, but only if you have a huge
    training set that provides data on all the different possible combinations of
    three words at a time. You also now need a tremendous amount of storage space
    because there are a much larger number of combinations of three words than there
    are of two words. Choosing the preprocessing approach will therefore depend on
    the context of the problem, the computing resources available, and also the training
    data available to you. If you have a lot of training data and tons of resources,
    the trigram approach may be more accurate, but in more realistic conditions, the
    bigram approach may be better overall, even if it does misclassify some tweets.'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding discussion relates to the concepts of **feature selection**, **feature
    extraction**, and **dimensionality**. In general, our goal is to *select* only
    relevant features (ignore shopper trends that aren't interesting to us), *extract*
    or *derive* features that better represent our data (by using facial measurements
    rather than photograph pixels), and ultimately *reduce dimensionality* such that
    we use the fewest, most relevant dimensions we can.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection and feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both feature selection and feature extraction are techniques used to reduce
    dimensionality, though they are slightly different concepts. Feature selection
    is the practice of using only variables or features that are relevant to the problem
    at hand. In general, feature selection looks at individual features (such as `time
    on site`) and makes a determination of the relevance of that single feature. Feature
    extraction is similar, however feature extraction often looks at multiple correlated
    features and combines them into a single feature (like looking at hundreds of
    individual pixels and converting them into a **distance between pupils **measurement).
    In both cases, we are reducing the dimensionality of the problem, but the difference
    between the two is whether we are simply filtering out irrelevant dimensions (feature
    selection) or combining existing features in order to derive a new representative
    feature (feature extraction).
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of feature selection is to select the subset of features or dimensions
    of your data that optimizes the accuracy of your model. Let''s take a look at
    the naive approach to solving this problem: an exhaustive, brute force search
    of all possible subsets of dimensions. This approach is not viable in real-world
    applications, but it serves to frame the problem for us. If we take the e-commerce
    store example, our goal is to find some subset of dimensions or features that
    gives us the best results from our model. We know we have 50 features to choose
    from, but we don''t know how many are in the optimum set of features. Solving
    this problem by brute force, we would first pick only one feature at a time, and
    train and evaluate our model for each feature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, we would use only `time on site` as a data point, train the model
    on that data point, evaluate the model, and record the accuracy of the model.
    Then we move on to `total past purchase amount`, train the model, evaluate the
    model, and record results. We do this 48 more times for the remaining features
    and record the performance of each. Then we have to consider combinations of two
    features at a time, for instance by training and evaluating the model on `time
    on site` and `total past purchase amount`, and then training and evaluating on
    `time on site` and `last purchase date`, and so on. There are 1,225 unique pairs
    of features out of our set of 50, and we must repeat the procedure for each pair.
    Then we must consider groups of three features at a time, of which there are 19,600
    combinations. Then we must consider groups of four features, of which there are
    230,300 unique combinations. There are 2,118,760 combinations of five features,
    and nearly 16 million combinations of six features available to us, and so on.
    Obviously this exhaustive search for the optimal set of features to use cannot
    be done in a reasonable amount of time: we''d have to train our model billions
    of times just to find out what the best subset of features to use is! We must
    find a better approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, feature selection techniques are split into three categories: filter
    methods, wrapper methods, and embedded methods. Each category has a number of
    techniques, and the technique you select will depend on the data, the context,
    and the algorithm of your specific situation.'
  prefs: []
  type: TYPE_NORMAL
- en: Filter methods are the easiest to implement and typically have the best performance.
    Filter methods for feature selection analyze a single feature at a time and attempt
    to determine that feature's relevance to the data. Filter methods typically do
    not have any relation to the ML algorithm you use afterwards, and are more typically
    statistical methods that analyze the feature itself.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, you may use the Pearson correlation coefficient to determine if
    a feature has a linear relationship with the output variable, and remove features
    with a correlation very close to zero. This family of approaches will be very
    fast in terms of computational time, but has the disadvantage of not being able
    to identify features that are cross-correlated with one another, and, depending
    on the filter algorithm you use, may not be able to identify nonlinear or complex
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapper methods are similar to the brute force approach described earlier, however
    with the goal of avoiding a full exhaustive search of every combination of features
    as we did previously. For instance, you may use a genetic algorithm to select
    subsets of features, train and evaluate the model, and then use the evaluation
    of the model as evolutionary pressure to find the next subset of features to test.
  prefs: []
  type: TYPE_NORMAL
- en: The genetic algorithm approach may not find the perfect subset of features,
    but will likely discover a very good subset of features to use. Depending on the
    actual machine learning model you use and the size of the dataset, this approach
    may still take a long time, but it will not take an intractably long amount of
    time like the exhaustive search would. The advantage of wrapper methods is that
    they interact with the actual model you're training and therefore serve to directly
    optimize your model, rather than simply attempting to independently statistically
    filter out individual features. The major disadvantage of these methods is the
    computational time it takes to achieve the desired results.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a family of methods called **embedded methods**, however this
    family of techniques relies on algorithms that have their own feature selection
    algorithm built in and are therefore quite specialized; we will not discuss them
    here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature extraction techniques focus on combining existing features into new,
    derived features that better represent your data while also eliminating extra
    or redundant dimensionality. Imagine that your e-commerce shopper data includes
    both `time on site` and `total pixel scrolling distance while browsing` as dimensions.
    Also imagine that both of these dimensions do strongly correlate to the amount
    of money a shopper spends on the site. Naturally, these two features are related
    to each other: the more time a user spends on the site, the more likely they are
    to have scrolled a farther distance. Using only feature selection techniques,
    such as the Pearson correlation analysis, you would find that you should keep
    both `time on site` and `total distance scrolled` as features. The feature selection
    technique, which analyzes these features independently, has determined that both
    are relevant to your problem, but has not understood that the two features are
    actually highly related to each other and therefore redundant.'
  prefs: []
  type: TYPE_NORMAL
- en: A more sophisticated feature extraction technique, such as **Principal Component
    Analysis** (**PCA**), would be able to identify that time on site and scroll distance
    can actually be combined into a single, new feature (let's call it `site engagement`)
    that encapsulates the data represented by what used to be two separate features.
    In this case we have *extracted* a new feature from the time on site and scrolling
    distance measurements, and we are using that single feature instead of the two
    original features separately. This differs from feature selection; in feature
    selection we are simply choosing which of the original features to use when training
    our model, however in feature extraction we are creating brand new features from
    related combinations of original features. Both feature selection and feature
    extraction therefore reduce the dimensionality of our data, but do so in different
    ways.
  prefs: []
  type: TYPE_NORMAL
- en: Pearson correlation example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s return to our example of shoppers on the e-commerce store and consider
    how we might use the Pearson correlation coefficient to select data features.
    Consider the following example data, which records purchase amounts for shoppers
    given their time spent on site and the amount of money they had spent on purchases
    previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Purchase Amount** | **Time on Site (seconds)** | **Past Purchase Amount**
    |'
  prefs: []
  type: TYPE_TB
- en: '| $10.00 | 53 | $7.00 |'
  prefs: []
  type: TYPE_TB
- en: '| $14.00 | 220 | $12.00 |'
  prefs: []
  type: TYPE_TB
- en: '| $18.00 | 252 | $22.00 |'
  prefs: []
  type: TYPE_TB
- en: '| $20.00 | 571 | $17.00 |'
  prefs: []
  type: TYPE_TB
- en: '| $22.00 | 397 | $21.00 |'
  prefs: []
  type: TYPE_TB
- en: '| $34.00 | 220 | $23.00 |'
  prefs: []
  type: TYPE_TB
- en: '| $38.00 | 776 | $29.00 |'
  prefs: []
  type: TYPE_TB
- en: '| $50.00 | 462 | $74.00 |'
  prefs: []
  type: TYPE_TB
- en: '| $52.00 | 354 | $63.00 |'
  prefs: []
  type: TYPE_TB
- en: '| $56.00 | 23 | $61.00 |'
  prefs: []
  type: TYPE_TB
- en: Of course, in a real application of this problem you may have thousands or hundreds
    of thousands of rows, and dozens of columns, each representing a different dimension
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now select features for this data manually. The `purchase amount` column
    is our output data, or the data that we want our algorithm to predict given other
    features. In this exercise, we can choose to train the model using both time on
    site and previous purchase amount, time on site alone, or previous purchase amount
    alone.
  prefs: []
  type: TYPE_NORMAL
- en: When using a filter method for feature selection we consider one feature at
    a time, so we must look at time on site's relation to purchase amount independently
    of past purchase amount's relation to purchase amount. One manual approach to
    this problem would be to chart each of our two candidate features against the
    `Purchase Amount` column, and calculate a correlation coefficient to determine
    how strongly each feature is related to the purchase amount data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll chart time on site versus purchase amount, and use our spreadsheet
    tool to calculate the Pearson correlation coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/afddac71-1293-4b6b-a716-4c16ace48d71.png)'
  prefs: []
  type: TYPE_IMG
- en: Even a simple visual inspection of the data hints to the fact that there is
    only a small relationship—if any at all—between time on site and purchase amount.
    Calculating the Pearson correlation coefficient yields a correlation of about
    +0.1, a very weak, essentially insignificant correlation between the two sets
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we chart the past purchase amount versus current purchase amount,
    we see a very different relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42d01751-2f64-434d-b706-3ef5aff2c71f.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, our visual inspection tells us that there is a linear but somewhat
    noisy relationship between the past purchase amount and the current purchase amount.
    Calculating the correlation coefficient gives us a correlation value of +0.9,
    quite a strong linear relationship!
  prefs: []
  type: TYPE_NORMAL
- en: This type of analysis tells us that we can ignore the time on site data when
    training our model, as there seems to be little to no statistical significance
    in that information. By ignoring time on site data, we can reduce the number of
    dimensions we need to train our model on by one, allowing our model to better
    generalize data and also improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: If we had 48 other numerical dimensions to consider, we could simply calculate
    the correlation coefficient for each of them and discard each dimension whose
    correlation falls beneath some threshold. Not every feature can be analyzed using
    correlation coefficients, however, so you can only apply the Pearson algorithm
    to those features where such a statistical analysis makes sense; it would not
    make sense to use Pearson correlation to analyze a feature that lists *recently
    browsed product category*, for instance. You can, and should, use other types
    of feature selection filters for different dimensions representing different types
    of data. Over time, you will develop a toolkit of analysis techniques that can
    apply to different types of data.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, a thorough explanation of all the possible feature extraction
    and feature selection algorithms and tools is not possible here; you will have
    to research various techniques and determine which ones fit the shape and style
    of your features and data.
  prefs: []
  type: TYPE_NORMAL
- en: Some algorithms to consider for filter techniques are the Pearson and Spearman
    correlation coefficients, the chi-squared test, and information gain algorithms
    such as the Kullback–Leibler divergence.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches to consider for wrapper techniques are optimization techniques such
    as genetic algorithms, tree-search algorithms such as best-first search, stochastic
    techniques such as random hill-climb algorithms, and heuristic techniques such
    as recursive feature elimination and simulated annealing. All of these techniques
    aim to select the best set of features that optimize the output of your model,
    so any optimization technique can be a candidate, however, genetic algorithms
    are quite effective and popular.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction has many algorithms to consider, and generally focuses on
    cross-correlation of features in order to determine new features that minimize
    some error function; that is, how can two or more features be combined such that
    a minimum amount of data is lost. Relevant algorithms include PCA, partial least
    squares, and autoencoding. In NLP, latent semantic analysis is popular. Image
    processing has many specialized feature extraction algorithms, such as edge detection,
    corner detection, and thresholding, and further specializations based on problem
    domain such as face identification or motion detection.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and preparing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature selection is not the only consideration required when preprocessing
    your data. There are many other things that you may need to do to prepare your
    data for the algorithm that will ultimately analyze the data. Perhaps there are
    measurement errors that create significant outliers. There can also be instrumentation
    noise in the data that needs to be smoothed out. Your data may have missing values
    for some features. These are all issues that can either be ignored or addressed,
    depending, as always, on the context, the data, and the algorithm involved.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the algorithm you use may require the data to be normalized to
    some range of values. Or perhaps your data is in a different format that the algorithm
    cannot use, as is often the case with neural networks which expect you to provide
    a vector of values, but you have JSON objects that come from a database. Sometimes
    you need to analyze only a specific subset of data from a larger source. If you're
    working with images you may need to resize, scale, pad, crop, or reduce the image
    to grayscale.
  prefs: []
  type: TYPE_NORMAL
- en: These tasks all fall into the realm of data preprocessing. Let's take a look
    at some specific scenarios and discuss possible approaches for each.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many cases, several data points may have values missing from certain features.
    If you're looking at Yes/No responses to survey questions, several participants
    may have accidentally or purposefully skipped a given question. If you're looking
    at time series data, your measurement tool may have had an error for a given period
    or measurement. If you're looking at e-commerce shopping habits, some features
    may not be relevant to a user, for instance `last login date` for users that shop
    as an anonymous guest. The individual situation and scenario, as well as your
    algorithm's tolerance for missing data, determines the approach you must take
    to remediate missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Missing categorical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the case of categorical data, such as Yes/No survey questions that may not
    have been responded to, or an image that has not yet been labeled with its category,
    often the best approach is to create a new category called *undefined*, *N/A*,
    *unknown*, or *similar*. Alternatively, you may be able to select a reasonable
    default category to use for these missing values, perhaps choosing the most frequent
    category from the set, or choosing a category that represents the data point's
    logical parent. If you're analyzing photographs uploaded by users and are missing
    the category tag for a given photograph, you may instead use the *user's* stated
    category in place of the photo's individual category. That is, if a user is tagged
    as a fashion photographer, you may use the *fashion* category for the photo, even
    though the user has also uploaded a number of *travel* photographs. This approach
    will add noise in the form of miscategorized data points to the system, but may
    in fact have a positive overall effect of forcing the algorithm to generalize
    its model; the model may eventually learn that fashion and travel photography
    are similar.
  prefs: []
  type: TYPE_NORMAL
- en: Using an *undefined* or *N/A* category is also a preferred approach, as the
    fact that a data point has no category may be significant in and of itself—*No
    category* can itself be a valid category. The size of the dataset, the algorithm
    used, and the relative size of the *N/A* category within the dataset will affect
    whether this is a reasonable approach to take. In a classification scenario, for
    instance, two effects are possible. If the uncategorized items *do* form a pattern
    (for instance, *fashion* photos are uncategorized more often than other photos),
    you may find that your classifier incorrectly learns that fashion photos should
    be categorized as N/A! In this scenario, it may be better to ignore uncategorized
    data points entirely.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the uncategorized photos are comprised of photos from various categories
    equally, your classifier may end up identifying difficult-to-classify photos as
    N/A, which could actually be a desired effect. In this scenario, you can consider
    N/A as a class of its own, being comprised of difficult, broken, or unresolvable
    photos.
  prefs: []
  type: TYPE_NORMAL
- en: Missing numerical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Missing values for numerical data is trickier to handle than categorical data,
    as there is often no reasonable default for missing numerical values. Depending
    on the dataset, you may be able to use zeros as replacements, however in some
    cases using the mean or median value of that feature is more appropriate. In other
    scenarios, and depending on the algorithm used, it may be useful to fill in missing
    values with a very large value: if that data point needs to have an error calculation
    performed on it, using a large value will mark the data point with a large error,
    and discourage the algorithm from considering that point.'
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, you can use a linear interpolation to fill in missing data points.
    This makes sense in some time series applications. If your algorithm expects 31
    data points representing the growth of some metric, and you're missing one value
    for day 12, you can use the average of day 11's and day 13's values to serve as
    an estimate for day 12's value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Often the correct approach is to ignore and filter out data points with missing
    values, however, you must consider the effects of such an action. If the data
    points with missing values strongly represent a specific category of data, you
    may end up creating a strong selection bias as a side effect, as your analysis
    would have ignored a significant group. You must balance this type of side effect
    with the possible side effects caused by the other approaches: will zeroing out
    missing values significantly skew your distribution? Will using the mean or median
    as replacements taint the rest of the analysis? These questions can only be answered
    on a case-by-case basis.'
  prefs: []
  type: TYPE_NORMAL
- en: Handling noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Noise in data can come from many sources, but is not often a significant issue
    as most machine learning techniques are resilient to noisy datasets. Noise can
    come from environmental factors (for instance, the air conditioner compressor
    turning on randomly and causing signal noise in a nearby sensor), it can come
    from transcription errors (somebody recorded the wrong data point, selected the
    wrong option in a survey, or an OCR algorithm read a `3` as an `8`), or it can
    be inherent to the data itself (such as fluctuations in temperature recordings,
    which will follow a seasonal pattern but have a noisy daily pattern).
  prefs: []
  type: TYPE_NORMAL
- en: Noise in categorical data can also be caused by category labels that aren't
    normalized, such as images that are tagged `fashion` or `fashions` when the category
    is supposed to be `Fashion`. In those scenarios, the best approach is to simply
    normalize the category label, perhaps by forcing all category labels to be made
    singular and fully lowercase—this will combine the `Fashion`, `fashion`, and `fashions`
    categories into one single `fashion` category.
  prefs: []
  type: TYPE_NORMAL
- en: Noise in time series data can be smoothed by taking a moving average of multiple
    values; however, first you should evaluate if smoothing the data is important
    to your algorithm and results in the first place. Often, the algorithm will still
    perform well enough for practical applications if there is a small amount of noise,
    and especially if the noise is random rather than systemic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example of daily measurements of some sensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Day** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.1381426172 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.5678176776 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.3564009968 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.239499423 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1.267606181 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1.440843361 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.3322843208 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.4329166745 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.5499234277 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | -0.4016070826 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.06216906816 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | -0.9689103112 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | -1.170421963 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | -0.784125647 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | -1.224217169 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | -0.4689120937 |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | -0.7458561671 |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | -0.6746415566 |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | -0.0429460593 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 0.06757010626 |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | 0.480806698 |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | 0.2019759014 |'
  prefs: []
  type: TYPE_TB
- en: '| 23 | 0.7857692899 |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | 0.725414402 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 1.188534085 |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | 0.458488458 |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | 0.3017212831 |'
  prefs: []
  type: TYPE_TB
- en: '| 28 | 0.5249332545 |'
  prefs: []
  type: TYPE_TB
- en: '| 29 | 0.3333153146 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | -0.3517342423 |'
  prefs: []
  type: TYPE_TB
- en: '| 31 | -0.721682062 |'
  prefs: []
  type: TYPE_TB
- en: 'Graphing this data shows a noisy but periodic pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77ebb25f-47d8-4c98-84c4-c899370b9c64.png)'
  prefs: []
  type: TYPE_IMG
- en: This may be acceptable in many scenarios, but other applications may require
    smoother data.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that several of the data points exceed +1 and -1, which may be of
    significance especially if your algorithm is expecting data between the -1 and
    +1 range.
  prefs: []
  type: TYPE_NORMAL
- en: We can apply a `5-Day Moving Average` to the data to generate a smoother curve.
    To perform a `5-Day Moving Average`, start with day `3`, sum the values for days
    `1` to `5`, and divide by 5\. The result becomes the moving average for day `3`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this approach, we lose days `1` and `2`, and also days `30` and
    `31`, because we cannot look two days before day `1` nor can we look two days
    after day `31`. However, if you require values for those days, you may use the
    raw values for days `1`, `2`, `30`, and `31`, or you may use `3-Day Moving Averages`
    for days `2` and `30` in addition to single values for days `1` and `31`. If you
    have more historical data, you can use data from the previous month calculating
    the `5-Day Moving Average` for days `1` and `2` (calculate day `1` by using the
    previous month's last two days). The approach to how you handle this moving average
    will depend on the data available to you and the importance of having 5-day averages
    for each data point versus combining 5-day averages with 3-day and 1-day averages
    at the boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we calculate the `5-Day Moving Average` for our month, the data becomes
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Day** | **Value** | **5-Day Moving Average** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.1381426172 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.5678176776 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.3564009968 | 0.7138933792 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.239499423 | 0.974433528 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1.267606181 | 0.9273268566 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1.440843361 | 0.9426299922 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.3322843208 | 0.8047147931 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.4329166745 | 0.4708721403 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.5499234277 | 0.1951372817 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | -0.4016070826 | -0.06510164468 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.06216906816 | -0.3857693722 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | -0.9689103112 | -0.6525791871 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | -1.170421963 | -0.8171012043 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | -0.784125647 | -0.9233174367 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | -1.224217169 | -0.8787066079 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | -0.4689120937 | -0.7795505266 |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | -0.7458561671 | -0.631314609 |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | -0.6746415566 | -0.3729571541 |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | -0.0429460593 | -0.1830133958 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 0.06757010626 | 0.006553017948 |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | 0.480806698 | 0.2986351872 |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | 0.2019759014 | 0.4523072795 |'
  prefs: []
  type: TYPE_TB
- en: '| 23 | 0.7857692899 | 0.6765000752 |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | 0.725414402 | 0.6720364272 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 1.188534085 | 0.6919855036 |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | 0.458488458 | 0.6398182965 |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | 0.3017212831 | 0.561398479 |'
  prefs: []
  type: TYPE_TB
- en: '| 28 | 0.5249332545 | 0.2533448136 |'
  prefs: []
  type: TYPE_TB
- en: '| 29 | 0.3333153146 | 0.0173107096 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | -0.3517342423 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 31 | -0.721682062 |  |'
  prefs: []
  type: TYPE_TB
- en: In some cases, the moving average differs from the day's data point by a significant
    margin. On day `3`, for instance, the moving average is double that of the day's
    measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach would not be appropriate in instances where you need to consider
    a given day''s measurement in isolation, however, when we graph the moving average
    against the daily data points, we can see the value of this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5147b1d-5741-4d10-b9d4-b6c6f2f88682.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the moving average is much smoother than the daily measurements,
    and that the moving average better represents the periodic, sinusoidal nature
    of our data. An added bonus for us is that the moving average data no longer contains
    points that lie outside our [-1, +1] range; because the noise in this data was
    random, the random fluctuations have largely canceled each other out and brought
    our data back into range.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the window of the moving average will result in broader and broader
    averages, reducing resolution; if we were to take a *31-Day Moving Average*, we
    would simply have the average measurement for the entire month. If your application
    simply needs to smooth out data rather than reduce data to lower resolutions,
    you should start by applying the smallest moving average window that serves to
    clean the data enough, for instance, a 3-point moving average.
  prefs: []
  type: TYPE_NORMAL
- en: If you're dealing with measurements that are not time series, then a moving
    average approach may not be appropriate. For instance, if you're measuring the
    value of a sensor at arbitrary and random times where the time of measurement
    is not recorded, a moving average would not be appropriate because the dimension
    to average over is unknown (that is, we do not know the time period that the average
    moves over).
  prefs: []
  type: TYPE_NORMAL
- en: If you still need to eliminate noise from your data, you can try *binning* the
    measurements by creating a histogram of the data. This approach changes the nature
    of the data itself and does not apply to every situation, however, it can serve
    to obfuscate individual measurement fluctuations while still representing the
    relative frequency of different measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Handling outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your data will often have outlying values, or data points that are far away
    from the expected value for your dataset. Sometimes, outliers are caused by noise
    or errors (somebody recording a height of 7'3" rather than 6'3"), but other times,
    outliers are legitimate data points (one celebrity with a Twitter reach of 10
    million followers joining your service where most of the users have 10,000 to
    100,000 followers). In either case, you'll first want to identify outliers so
    that you can determine what to do with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'One approach to identifying outliers is to calculate the mean and standard
    deviation of your dataset, and determine how many standard deviations away from
    the mean each data point is. The standard deviation of a dataset represents the
    overall variance or dispersion of the data. Consider the following data which
    represents the number of Twitter followers of accounts that you''re analyzing:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Followers** |'
  prefs: []
  type: TYPE_TB
- en: '| 1075 |'
  prefs: []
  type: TYPE_TB
- en: '| 1879 |'
  prefs: []
  type: TYPE_TB
- en: '| 3794 |'
  prefs: []
  type: TYPE_TB
- en: '| 4111 |'
  prefs: []
  type: TYPE_TB
- en: '| 4243 |'
  prefs: []
  type: TYPE_TB
- en: '| 4885 |'
  prefs: []
  type: TYPE_TB
- en: '| 7617 |'
  prefs: []
  type: TYPE_TB
- en: '| 8555 |'
  prefs: []
  type: TYPE_TB
- en: '| 8755 |'
  prefs: []
  type: TYPE_TB
- en: '| 19422 |'
  prefs: []
  type: TYPE_TB
- en: '| 31914 |'
  prefs: []
  type: TYPE_TB
- en: '| 36732 |'
  prefs: []
  type: TYPE_TB
- en: '| 39570 |'
  prefs: []
  type: TYPE_TB
- en: '| 1230324 |'
  prefs: []
  type: TYPE_TB
- en: 'As you can see, the last value is much larger than the other values in the
    set. However, this discrepancy may not be so obvious if you''re analyzing millions
    of records with dozens of features each. To automate our outlier identification
    we should first calculate the mean average of all our users, which in this case
    is an average of **100,205** followers. Then, we should calculate the standard
    deviation of the dataset, which for this data is **325,523** followers. Finally,
    we can inspect each data point by determining how many standard deviations away
    from the mean that data point is: find the absolute difference between the data
    point and the mean, and then divide by the standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Followers** | **Deviation** |'
  prefs: []
  type: TYPE_TB
- en: '| 1075 | 0.3045078726 |'
  prefs: []
  type: TYPE_TB
- en: '| 1879 | 0.3020381533 |'
  prefs: []
  type: TYPE_TB
- en: '| 3794 | 0.2961556752 |'
  prefs: []
  type: TYPE_TB
- en: '| 4111 | 0.2951819177 |'
  prefs: []
  type: TYPE_TB
- en: '| 4243 | 0.2947764414 |'
  prefs: []
  type: TYPE_TB
- en: '| 4885 | 0.2928043522 |'
  prefs: []
  type: TYPE_TB
- en: '| 7617 | 0.2844122215 |'
  prefs: []
  type: TYPE_TB
- en: '| 8555 | 0.2815308824 |'
  prefs: []
  type: TYPE_TB
- en: '| 8755 | 0.2809165243 |'
  prefs: []
  type: TYPE_TB
- en: '| 19422 | 0.248149739 |'
  prefs: []
  type: TYPE_TB
- en: '| 31914 | 0.2097769366 |'
  prefs: []
  type: TYPE_TB
- en: '| 36732 | 0.1949770517 |'
  prefs: []
  type: TYPE_TB
- en: '| 39570 | 0.1862593113 |'
  prefs: []
  type: TYPE_TB
- en: '| 1230324 | 3.471487079 |'
  prefs: []
  type: TYPE_TB
- en: 'The approach has yielded good results: all data points except one are found
    within one standard deviation of the mean, and our outlier is far away from the
    average with a distance of nearly 3.5 deviations. In general, you can consider
    data points more than two or three standard deviations away from the mean to be
    outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If your dataset represents a normal distribution, then you can use the **68-95-99.7** rule:
    68% of data points are expected to be within one standard deviation, 95% are expected
    to be within two deviations, and 99.7% of data points are expected to be within
    three standard deviations. In a normal distribution, therefore, only 0.3% of data
    is expected to be farther than three standard deviations from the mean.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the preceding data presented is not a normal distribution, and much
    of your data will not follow normal distributions either, but the concept of standard
    deviation may still apply (the ratios of data points expected per standard deviation
    will differ based on the distribution).
  prefs: []
  type: TYPE_NORMAL
- en: Now that an outlier is identified, a determination must be made as to how to
    handle the outlying data point. In some cases, it's better to keep the outliers
    in your dataset and continue processing as usual; outliers that are based in real
    data are often important data points that can't be ignored, because they represent
    uncommon but possible values for your data.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you're monitoring a server's CPU load average and find an average
    value of 2.0 with a standard deviation of 1.0, you would not want to ignore data
    points with load averages of 10.0—those data points still represent load averages
    that your CPU actually experienced, and for many types of analysis it would be
    self-defeating to ignore that data, even though those points are far away from
    the mean. Those points should be considered and accounted for in your analysis.
    However, in our Twitter followers example we may want to ignore the outlier, especially
    if our analysis is to determine behavioral patterns of Twitter users' audiences—our
    outlier most likely exhibits a completely separate class of behavioral patterns
    that may simply confuse our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another approach to handling outliers that works well when considering
    data that''s expected to be linear, polynomial, exponential, or periodic—the types
    of datasets where a regression can be performed. Consider data that is expected
    to be linear, like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Observation** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 10 |'
  prefs: []
  type: TYPE_TB
- en: 'When performing a linear regression on this data, we can see that the outlying
    data point skews the regression upwards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a21613d5-db9b-4a1f-9de0-31722a5c158d.png)'
  prefs: []
  type: TYPE_IMG
- en: For this small set of data points the error in the regression may not be significant,
    but if you're using the regression to extrapolate future values, for instance,
    for observation number 30, the predicted value will be far from the actual value
    as the small error introduced by the outlier compounds the further you extrapolate
    values. In this case, we would want to remove the outlier before performing the
    regression so that the regression's extrapolation is more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to identify the outlier, we can perform a linear regression as we
    have before, and then calculate the squared error from the trendline for each
    point. If the data point exceeds an error of, for instance, 25%, we can consider
    that point an outlier and remove it before performing the regression a second
    time. Once we''ve removed the outlier and re-performed the regression, the trendline
    fits the data much better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/273c129c-5c13-4fec-b3ed-61054c42fb68.png)'
  prefs: []
  type: TYPE_IMG
- en: Transforming and normalizing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most common preprocessing task is to transform and/or normalize data into
    a representation that can be used by your algorithm. For instance, you may receive
    JSON objects from an API endpoint that you need to transform into vectors used
    by your algorithm. Consider the following JSON data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Your neural network that processes the data expects input data in vector form,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In JavaScript, the easiest way to transform our JSON data in this situation
    is to use the built-in `Array.map` function. The following code will generate
    an array of vectors (an array of arrays). This form of transformation will be
    very common throughout this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are using the shortest form of ES6 arrow functions, which doesn''t
    require parentheses around the parameters nor an explicit return statement, since
    we return our array of features directly. An equivalent ES5 example would look
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Also note that the `is_verified` field was converted to an integer using the
    ternary operator, `user.is_verified ? 1 : 0`. Neural networks can only work with
    numeric values, and so we must represent the Boolean value as an integer.'
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss techniques for using natural language with neural networks in
    a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another common data transformation is to normalize data values into a given
    range, for instance between -1 and +1\. Many algorithms depend on data values
    falling within this range, however, most real-world data does not. Let''s revisit
    our noisy daily sensor data from earlier in the chapter, and let''s assume that
    we have access to this data in a simple JavaScript array called **measurements** (detail-oriented
    readers will notice I changed the value of day `15` as compared with the earlier
    example):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Day** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.1381426172 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.5678176776 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.3564009968 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.239499423 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1.267606181 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1.440843361 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.3322843208 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.4329166745 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.5499234277 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | -0.4016070826 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.06216906816 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | -0.9689103112 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | -1.170421963 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | -0.784125647 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | -1.524217169 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | -0.4689120937 |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | -0.7458561671 |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | -0.6746415566 |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | -0.0429460593 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 0.06757010626 |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | 0.480806698 |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | 0.2019759014 |'
  prefs: []
  type: TYPE_TB
- en: '| 23 | 0.7857692899 |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | 0.725414402 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 1.188534085 |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | 0.458488458 |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | 0.3017212831 |'
  prefs: []
  type: TYPE_TB
- en: '| 28 | 0.5249332545 |'
  prefs: []
  type: TYPE_TB
- en: '| 29 | 0.3333153146 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | -0.3517342423 |'
  prefs: []
  type: TYPE_TB
- en: '| 31 | -0.721682062 |'
  prefs: []
  type: TYPE_TB
- en: If we wish to normalize this data to the range [-1, +1], we must first discover
    the largest *absolute value* of all numbers in the set, which in this case is
    day 15's value of `-1.52`. If we were to simply use JavaScript's `Math.max` on
    this data, we would find the maximum value on the number line, which is day 6's
    value of `1.44`—however, day `15` is more negative than day `6` is positive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding the maximum absolute value in a JavaScript array can be accomplished
    with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The value of `absolute_max` will be +1.524217169—the number became positive
    when we called `Math.abs` using `measurements.map`. It is important that the absolute
    maximum value remains positive, because in the next step we will divide by the
    maximum and want to preserve the signs of all data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the absolute maximum value, we can normalize our data points like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'By dividing each number by the maximum value in the set, we ensure that all
    values lie in the range [-1, +1]. The maximum value will be (in this case) -1,
    and all other numbers in the set will be closer to 0 than the maximum will. After
    normalizing, our data now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Day** | **Value** | **Normalized** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.1381426172 | 0.09063184696 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.5678176776 | 0.3725306927 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.3564009968 | 0.2338256018 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.239499423 | 0.8132039508 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1.267606181 | 0.8316440777 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1.440843361 | 0.9453005718 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.3322843208 | 0.218003266 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.4329166745 | 0.284025586 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.5499234277 | 0.3607907319 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | -0.4016070826 | -0.2634841615 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.06216906816 | 0.04078753963 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | -0.9689103112 | -0.6356773373 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | -1.170421963 | -0.7678839913 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | -0.784125647 | -0.5144448332 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | -1.524217169 | -1 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | -0.4689120937 | -0.3076412623 |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | -0.7458561671 | -0.4893372037 |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | -0.6746415566 | -0.4426151145 |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | -0.0429460593 | -0.02817581391 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 0.06757010626 | 0.04433102293 |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | 0.480806698 | 0.3154450087 |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | 0.2019759014 | 0.1325112363 |'
  prefs: []
  type: TYPE_TB
- en: '| 23 | 0.7857692899 | 0.5155231854 |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | 0.725414402 | 0.4759258831 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 1.188534085 | 0.7797668924 |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | 0.458488458 | 0.3008025808 |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | 0.3017212831 | 0.1979516366 |'
  prefs: []
  type: TYPE_TB
- en: '| 28 | 0.5249332545 | 0.3443953167 |'
  prefs: []
  type: TYPE_TB
- en: '| 29 | 0.3333153146 | 0.2186796747 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | -0.3517342423 | -0.2307638633 |'
  prefs: []
  type: TYPE_TB
- en: '| 31 | -0.721682062 | -0.4734771901 |'
  prefs: []
  type: TYPE_TB
- en: 'There are no data points outside of the [-1, +1] range, and you can also see
    that day `15`, with the maximum absolute value of the data, has been normalized
    as `-1`. Graphing the data shows the relationship between the original and normalized
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4abf6168-227e-42e3-a946-48011ad77e60.png)'
  prefs: []
  type: TYPE_IMG
- en: The shape of the data has been preserved, and the chart has simply been scaled
    by a constant factor. This data is now ready to use in algorithms that require
    normalized ranges, such as PCA, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: Your data is likely much more complex than these preceding examples. Perhaps
    your JSON data is composed of complex objects with nested entities and arrays.
    You may need to run an analysis on only those items which have specific sub-elements,
    or you may need to generate dynamic subsets of data based on some user-provided
    query or filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For complex situations and datasets, you may want some help from a third-party
    library such as `DataCollection.js`, which is a library that adds SQL and NoSQL
    style query functionality to JavaScript arrays. Imagine that our preceding JSON
    data of **users** also contained an object called **locale** which gives the user''s
    country and language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To find only users whose language is `en_US`, you could perform the following
    query using `DataCollection.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, you can accomplish the aforementioned in pure JavaScript easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: However, the pure JavaScript version needs some tedious modifications to be
    resilient against undefined or null `locale` objects, and of course more complicated
    filters become ever more tedious to write in pure JavaScript. Most of the time,
    we will use pure JavaScript for the examples in this book, however, our examples
    will be contrived and much cleaner than real-world use cases; use a tool such
    as `DataCollection.js`, if you feel you need it.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed data preprocessing, or the art of delivering
    the most useful possible data to our machine learning algorithms. We discussed
    the importance of appropriate feature selection and the relevance of feature selection,
    both to overfitting and to the curse of dimensionality. We looked at correlation
    coefficients as a technique to help us determine the appropriate features to select,
    and also discussed more sophisticated wrapper methods for feature selection, such
    as using a genetic algorithm to determine the optimal set of features to choose.
    We then discussed the more advanced topic of feature extraction, which is a category
    of algorithms that can be used to combine multiple features into new individual
    features, further reducing the dimensionality of the data.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at some common scenarios you might face when dealing with real-world
    datasets, such as missing values, outliers, and measurement noise. We discussed
    various techniques you can use to correct for those issues. We also discussed
    common data transformations and normalizations you may need to perform, such as
    normalizing values to a range or vectorizing objects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at machine learning in broad strokes and begin
    to introduce specific algorithms and their applications.
  prefs: []
  type: TYPE_NORMAL
