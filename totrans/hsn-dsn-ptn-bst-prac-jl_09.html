<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Performance Patterns</h1>
                </header>
            
            <article>
                
<p>This chapter includes patterns related to improving system performance. High performance is a major requirement in scientific computing, artificial intelligence, machine learning, and big data processing. Why is that? </p>
<p>In the past decade, data has grown almost exponentially thanks to the scalability from the cloud. Think about the <strong>Internet of Things</strong> (<strong>IoT</strong>). Sensors are all around us—home security systems, personal assistants, and even room temperature controls are collecting tons of data continuously. Furthermore, the data being collected is stored and analyzed by companies that want to build smarter products. Use cases such as these demand more computing power and speed.</p>
<p>I once debated with a colleague about the use of cloud technologies for solving computationally intensive problems. Computing resources are definitely available in the cloud, but they are not free. It is therefore quite important that computer programs are designed to be more efficient and optimized to avoid unnecessary costs in the cloud. </p>
<p>Fortunately, the Julia programming language allows us to easily utilize CPU resources to the fullest extent. The way to make things fast is not difficult as long as some rules are followed. The online Julia reference manual already contains some tips. This chapter provides further patterns that are used extensively by veteran Julia developers to increase performance. </p>
<p>We will go over the following design patterns:</p>
<ul>
<li>Global constant</li>
<li>Struct of arrays</li>
<li>Shared arrays</li>
<li>Memoization</li>
<li>Barrier function</li>
</ul>
<p>Let's get started!</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>The sample source code is located at <a href="https://github.com/PacktPublishing/Hands-on-Design-Patterns-and-Best-Practices-with-Julia/tree/master/Chapter06">https://github.com/PacktPublishing/Hands-on-Design-Patterns-and-Best-Practices-with-Julia/tree/master/Chapter06</a>.</p>
<p>The code is tested in a Julia 1.3.0 environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The global constant pattern</h1>
                </header>
            
            <article>
                
<p>Global variables are generally considered evil. I'm not kidding—they are evil. If you don't believe me, just google it. There are many reasons why they are bad, but in Julia land, they can also be a contributor to poor application performance.</p>
<p>Why do we want to use global variables? In the Julia language, variables are either in the global or local scope. For example, all variable assignments at the top level of a module are considered global. Variables that appear inside functions are local. Consider an application that connects to an external system—a handle object is typically created upon connection. Such handle objects can be kept in a global variable because all functions in the module can access the variable without having to pass it around as a function argument. That's the convenience factor. Also, this handler object only needs to be created once, and then it can be used at any time for subsequent actions.</p>
<p>Unfortunately, global variables also come with a cost. It may not be obvious at first, but it does hurt performance<span>—</span>indeed, quite badly, in some cases. In this section, we will discuss how bad global variables hurt performance and how the problem can be remedied by using global constants.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benchmarking performance with global variables</h1>
                </header>
            
            <article>
                
<p>Sometimes, it is convenient to use global variables because they are accessible from anywhere in the code. However, application performance may suffer when using global variables. Let's figure out together how badly performance is affected. Here is a very simple function that just adds two numbers together:</p>
<pre>variable = 10<br/><br/>function add_using_global_variable(x)<br/>    return x + variable<br/>end</pre>
<p>To benchmark this code, we will use the great <kbd>BenchmarkTools.jl</kbd> package, which can repeatedly run the code many times and report back some performance statistics. Let's get started:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b6c8821c-a510-40bd-a34c-c358e06a8750.png" style="width:37.25em;height:5.50em;"/></p>
<p>It seems a little slow for just adding two numbers. Let's get rid of the global variable and just add the numbers using two function arguments. We can define the new function as follows:</p>
<pre>function add_using_function_arg(x, y)<br/>    return x + y<br/>end</pre>
<p>Let's benchmark this new function:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d5f2a094-899d-4de8-bfe5-5c627677127f.png" style="width:39.67em;height:3.17em;"/></p>
<p>That's <em>unbelievable</em>! Taking away the reference to the global variable sped up the function by almost 900 times. To understand where the performance hit came from, we can use the built-in introspection tool from Julia to see the generated LLVM code.</p>
<p>Here's the generated code for the faster one. It is clean and contains just a single <kbd>add</kbd> instruction:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6cb895c9-a4f4-4efc-83e3-792ae74f2b6a.png" style="width:39.67em;height:12.92em;"/></p>
<p class="mce-root"/>
<p>On the other hand, the function that uses global variable generated this ugly code:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b94445a0-33b5-423a-bc2e-6b004dc86363.png" style="width:55.50em;height:45.75em;"/></p>
<p>Why is that? Shouldn't the compiler be smarter? The answer is that the compiler cannot really assume that the global variable is always an integer. Because it is a variable, which means it can be changed at any time, the compiler must generate code that can handle any data type, to stay on the safe side. Well, such additional flexibility introduces a huge overhead in this case.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Enjoying the speed of global constants</h1>
                </header>
            
            <article>
                
<p>To improve performance, let's create a global constant by using the <kbd>const</kbd> keyword. Then, we can define a new function that accesses the constant, as follows:</p>
<pre>const constant = 10<br/><br/>function add_using_global_constant(x)<br/>    return constant + x<br/>end</pre>
<p>Let's benchmark its performance now:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/88b2394b-71ec-4415-a994-b86a09e4a250.png" style="width:41.50em;height:3.50em;"/></p>
<p><em>This is perfect!</em> If we introspect the function again, we get the following clean code:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2ca377d5-b17a-4d31-a112-55f4a111e85a.png" style="width:40.67em;height:13.67em;"/></p>
<p>Next, we will discuss how to use a global variable (not a constant) and still make it slightly better.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Annotating variables with type information</h1>
                </header>
            
            <article>
                
<p>It is best when we can just use global constants. But what if the variable <em>does</em> need to be changed during the life cycle of the application? For example, maybe it is a global counter that keeps track of the number of visitors on a website.</p>
<p class="mce-root"/>
<p>At first, we may be tempted to do the following, but we quickly realized that Julia does not support annotating global variables with type information:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4f7fa24e-cffe-4e4c-96be-43c4b440c536.png" style="width:43.42em;height:3.17em;"/></p>
<p>Instead, what we can do is to annotate the variable type within the function itself, as follows:</p>
<pre>function add_using_global_variable_typed(x)<br/>    return x + variable::Int<br/>end</pre>
<p>Let's see how it performs:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4a157480-d95c-4fcb-b082-0efe16f5dc3f.png" style="width:41.50em;height:7.58em;"/></p>
<p>That's quite a speed boost compared to the untyped version of 31 ns! However, it is still far away from the global constant solution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding why constants help performance</h1>
                </header>
            
            <article>
                
<p>The compiler has a lot more freedom when dealing with constants because of the following:</p>
<ul>
<li>The value does not change.</li>
<li>The type of the constant does not change. </li>
</ul>
<p>This will become clear after we look into some simple examples.</p>
<p>Let's take a look at the following function:</p>
<pre>function constant_folding_example()<br/>    a = 2 * 3<br/>    b = a + 1<br/>    return b &gt; 1 ? 10 : 20<br/>end</pre>
<p class="mce-root"/>
<p>If we just follow the logic, then it is not difficult to see that it always returns a value of 10. Let's just unroll it quickly here:</p>
<ul>
<li>The <kbd>a</kbd> variable has a value of 6.</li>
<li>The <kbd>b</kbd> variable has a value of <kbd>a + 1</kbd>, which is 7.</li>
<li>Because the <kbd>b</kbd> variable is greater than 1, it returns 10.</li>
</ul>
<p>From the compiler's perspective, the <kbd>a</kbd> <span>variable </span><span>can be inferred as a constant because it is assigned but never changed, and likewise for<span> the </span><kbd>b</kbd><span> variable</span></span><span>.</span></p>
<p>We can take a look at the code generated by Julia for this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c30489a9-dd5f-4b5d-bf3d-4f3332042207.png" style="width:39.08em;height:5.83em;"/></p>
<p><span>The Julia compiler goes through several stages. In this case, we can use the </span><kbd>@code_typed</kbd><span> macro, which shows the code that has been generated where all type information has been resolved.</span></p>
<p><em>Voila!</em> The compiler has figured it all out and just returns a value of <kbd>10</kbd> for this function. </p>
<p>We realize that a couple of things have happened here:</p>
<ul>
<li>When the compiler saw the multiplication of two constant values (<kbd>2 * 3</kbd>), it computed the final value of <kbd>6</kbd> for <kbd>a</kbd>. This process is called <strong>constant folding</strong>.</li>
<li>When the compiler inferred <kbd>a</kbd> as a value of <kbd>6</kbd>, it calculated <kbd>b</kbd> as a value of <kbd>7</kbd>. This process is called <strong>constant propagation</strong>.</li>
<li>When the compiler inferred <kbd>b</kbd> as a value of <kbd>7</kbd>, it pruned away the <kbd>else</kbd>-branch from the <kbd>if-then-else</kbd> operation. This process is called <strong>dead code elimination</strong>.</li>
</ul>
<p>Julia's compiler optimization is truly state of the art. These are just some of the examples that we can get a performance boost automatically without having to refactor a lot of code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Passing global variables as function arguments</h1>
                </header>
            
            <article>
                
<p>There is another way to tackle the problem of global variables. In a performance-sensitive function, rather than accessing the global variable directly, we can pass the global variable into the function as an argument. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's refactor the code earlier in this section by adding a second argument, as follows:</p>
<pre>function add_by_passing_global_variable(x, v)<br/>    return x + v<br/>end</pre>
<p>Now, we can call the function by passing in the variable. Let's benchmark the code as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d3e10bf7-9df8-4770-a931-3f43258d2429.png" style="width:42.42em;height:3.33em;"/></p>
<p><em>Fantastic!</em> It's as fast as treating it as a constant. Where's the magic? As it turns out, Julia's compiler automatically generates specialized functions according to the type of its arguments. In this case, when we pass the variable as an integer value, the function is compiled to the most optimized version because the types of the arguments are known. It is fast now for the same reason as using constants.</p>
<p>Of course, you may argue that it defeats the purpose of using global variables. Nonetheless, the flexibility is there and it can be used when you really need to get to the most optimal performance.</p>
<div class="packt_tip">When using <kbd>BenchmarkTools.jl</kbd> macros, we must interpolate global variables using the dollar-sign prefix. Otherwise, the time that it takes to reference the global variable is included in the performance test.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hiding a variable inside a global constant </h1>
                </header>
            
            <article>
                
<p>Before we conclude this section, there is yet another alternative to keep the flexibility of global variables while not losing too much performance. We can call it a <strong>global variable placeholder</strong>.</p>
<p>As it may have become clear to you by now, Julia can generate highly optimized code whenever the type of a variable is known at compilation time. Hence, one way to solve the problem is to create a constant placeholder and store a value inside the placeholder.</p>
<p>Consider this code:</p>
<pre># Initialize a constant Ref object with the value of 10<br/>const semi_constant = Ref(10)<br/><br/>function add_using_global_semi_constant(x)<br/>    return x + semi_constant[]<br/>end</pre>
<p>The global constant is assigned a <kbd>Ref</kbd> object. In Julia, a <kbd>Ref</kbd> object is nothing but a placeholder where the type of the enclosed object is known. You can try this in the Julia REPL:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dcc14ebe-314a-46d9-b985-80d71519d3b2.png" style="width:39.00em;height:6.67em;"/></p>
<p>As we can see, the value inside <kbd>Ref(10)</kbd> has a type of <kbd>Int64</kbd> according to the type signature, <kbd>Base.RefValue{Int64}</kbd>. Similarly, the type of the value inside <kbd>Ref("abc")</kbd> is <kbd>String</kbd>.</p>
<p>To fetch the value inside a <kbd>Ref</kbd> object, we can just use the index operator with no argument. Hence, in the preceding code, we use <kbd>semi_constant[]</kbd>.</p>
<p>What would be the performance overhead of this extra indirection? Let's benchmark the code as usual:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3b832665-11e3-4b60-a7e6-8bfacc12fe95.png" style="width:40.08em;height:3.75em;"/></p>
<p>That's not bad. Although it is far from the optimal performance of using global constant, it is still approximately 15 times faster than using a plain global variable.</p>
<p>Because <kbd>Ref</kbd> object is just a placeholder, the underlying value can also be assigned:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/34530746-060f-4740-83f8-f278311b3d8a.png" style="width:39.75em;height:7.25em;"/></p>
<p>In summary, <span>the use of <kbd>Ref</kbd> allows us to simulate global variables without sacrificing too much performance.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Turning to some real-life examples</h1>
                </header>
            
            <article>
                
<p>Global constants are very common among Julia packages. It is not too surprising because constants are also used to avoid hardcoding values directly in functions. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 1 – SASLib.jl package</h1>
                </header>
            
            <article>
                
<p>In the <kbd>SASLib.jl</kbd> package, most constants are defined in the <kbd>constants.jl</kbd> file located at <a href="https://github.com/tk3369/SASLib.jl/blob/master/src/constants.jl">https://github.com/tk3369/SASLib.jl/blob/master/src/constants.jl</a>.</p>
<p>Here's a fragment of the code:</p>
<pre># default settings<br/>const default_chunk_size = 0<br/>const default_verbose_level = 1 <br/><br/>const magic = [<br/>         b"\x00\x00\x00\x00\x00\x00\x00\x00" ;<br/>         b"\x00\x00\x00\x00\xc2\xea\x81\x60" ;<br/>         b"\xb3\x14\x11\xcf\xbd\x92\x08\x00" ;<br/>         b"\x09\xc7\x31\x8c\x18\x1f\x10\x11" ]<br/><br/>const align_1_checker_value = b"3"<br/>const align_1_offset = 32<br/>const align_1_length = 1<br/>const align_1_value = 4</pre>
<p>Using these constants allows the file-reading functions to perform well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 2 – PyCall.jl package</h1>
                </header>
            
            <article>
                
<p>The <kbd>PyCall.jl</kbd> package's documentation suggests the user stores a Python object using the global variable placeholder technique. The following excerpt can be found in its documentation:</p>
<div class="packt_quote">"For a type-stable global constant, initialize the constant to <kbd>PyNULL()</kbd> at the top level, and then use the <kbd>copy!</kbd> function in your module's <kbd>__init__</kbd> function to mutate it to its actual value."</div>
<p>A type-stable global constant is generally what we want for high-performance code. Basically, when the module is initialized, this global constant can be initialized with a value of <kbd>PyNULL()</kbd>. This constant is really just a placeholder object that can be mutated with the actual value later.</p>
<p>This technique is similar to the use of <kbd>Ref</kbd> as mentioned in the <em>Hiding a variable inside a global constant</em> section. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Considerations</h1>
                </header>
            
            <article>
                
<p>If a global variable can be replaced as a global constant, then it should always be done. The reason for doing that is more than performance alone. Constants have the nice property of guaranteeing that their values are unchanged throughout the application life cycle. In general, the fewer global state changes, the more robust the program. Mutating states is traditionally a source of hard-to-find bugs.</p>
<p>At times, we may get into a situation that we cannot avoid using global variables. That's too bad. However, before we feel sad about that, we could also check whether the system performance is materially affected or not.</p>
<p>In the preceding example of adding two numbers, accessing the global variable carries a relatively large cost because the actual operation is so simple and efficient. Hence, more work is done in terms of getting access to the global variable. On the other hand, if we have a more complex function that takes much longer, say, 500 nanoseconds, then the extra 25 nanosecond overhead becomes much less significant. In that case, we may as well ignore the issue as the overhead becomes immaterial.</p>
<p>Finally, we should always watch out when too many global variables are used. The problem multiplies when more global variables are used. How many are too many? It really depends on your situation, but it does not hurt to think about the application design and ask yourself whether the application is designed properly.</p>
<p>In the next section, we will discuss a pattern that helps to improve system performance just by laying out data differently in memory.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The struct of arrays pattern</h1>
                </header>
            
            <article>
                
<p>In recent years, modern CPU architecture has got fancier to meet today's demands. Due to various physical constraints, it is a lot more difficult to attain higher processor speed. <span>Many Intel processors now support a technology called <strong>Single Instruction, Multiple Data</strong> (<strong>SIMD</strong>). By utilizing <strong>Streaming SIMD Extension</strong> (<strong>SSE</strong>) and <strong>Advanced Vector Extensions</strong> (<strong>AVX</strong>) registers, several mathematical operations can be executed within a single CPU cycle. </span></p>
<p><span>That is nice, but one of the pre-requisites of utilizing these fancy CPU instructions is to make sure that the data is located in a contiguous memory block in the first place. </span>That brings us to our topic here. How do we orient our data in a contiguous memory block? You may find the solution in this section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with a business domain model</h1>
                </header>
            
            <article>
                
<p>When designing an application, we often create an object model that mimics business domain concepts. The idea is to clearly articulate data in a form that feels most natural to the programmer.</p>
<p>Let's say we need to retrieve customers' data from a relational database. A customer record may be stored in a <kbd>CUSTOMER</kbd> table, and each customer is stored as a row in the table. When we fetch customer data from the database, we can construct a <kbd>Customer</kbd> object and push that into an array. Similarly, when we work with NoSQL databases, we may receive data as JSON documents and put them into an array of objects. In both cases, we can see that data is represented as an array of objects. Applications are usually designed to work with objects as defined using the <kbd>struct</kbd> statement.</p>
<p>Let's take a look at a use case for analyzing taxi data coming from New York City. The data is publicly available as several CSV files. For illustration purposes, we have downloaded the data for December 2018 and truncated it to 100,000 records. </p>
<div class="packt_infobox">The full data file can be downloaded from <a href="https://data.cityofnewyork.us/Transportation/2018-Yellow-Taxi-Trip-Data/t29m-gskq">https://data.cityofnewyork.us/Transportation/2018-Yellow-Taxi-Trip-Data/t29m-gskq</a>. <br/>
<br/>
For convenience, a smaller file with 100,000 records is available from our GitHub site at <a href="https://github.com/PacktPublishing/Hands-On-Design-Patterns-with-Julia-1.0/raw/master/Chapter06/StructOfArraysPattern/yellow_tripdata_2018-12_100k.csv">https://github.com/PacktPublishing/Hands-On-Design-Patterns-with-Julia-1.0/raw/master/Chapter06/StructOfArraysPattern/yellow_tripdata_2018-12_100k.csv</a>. </div>
<p>First, we define a type called <kbd>TripPayment</kbd>, as follows:</p>
<pre>struct TripPayment<br/>    vendor_id::String<br/>    tpep_pickup_datetime::String<br/>    tpep_dropoff_datetime::String<br/>    passenger_count::Int<br/>    trip_distance::Float64<br/>    fare_amount::Float64<br/>    extra::Float64<br/>    mta_tax::Float64<br/>    tip_amount::Float64<br/>    tolls_amount::Float64<br/>    improvement_surcharge::Float64<br/>    total_amount::Float64<br/>end</pre>
<p>To read the data into memory, we will take advantage of the <kbd>CSV.jl</kbd> package. Let's define a function to read the file into a vector:</p>
<pre>function read_trip_payment_file(file)<br/>    f = CSV.File(file, datarow = 3)<br/>    records = Vector{TripPayment}(undef, length(f))<br/>    for (i, row) in enumerate(f)<br/>        records[i] = TripPayment(row.VendorID,<br/>                                 row.tpep_pickup_datetime,<br/>                                 row.tpep_dropoff_datetime,<br/>                                 row.passenger_count,<br/>                                 row.trip_distance,<br/>                                 row.fare_amount,<br/>                                 row.extra,<br/>                                 row.mta_tax,<br/>                                 row.tip_amount,<br/>                                 row.tolls_amount,<br/>                                 row.improvement_surcharge,<br/>                                 row.total_amount)<br/>    end<br/>    return records<br/>end</pre>
<p>Now, when we fetch the data, we end up with an array. In this example, we have downloaded 100,000 records, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1ed1f4bb-8d9c-4730-9939-6a9e396706a0.png" style="width:54.00em;height:25.50em;"/></p>
<p>Now, suppose that we need to analyze this dataset. In many data analysis use cases, we simply calculate various statistics for some of the attributes in the payment records. For example, we may want to find the average fare amount, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0e00f27a-6eae-4d7e-9144-f36e56d4915c.png" style="width:44.00em;height:3.17em;"/></p>
<p>This should be a fairly fast operation already because it uses a generator syntax and avoids allocation. </p>
<div class="packt_tip">Some Julia functions accept generator syntax, which can be written just like an array comprehension without the square brackets. It is very memory efficient because it avoids allocating memory for the intermediate object.</div>
<p><span>The only thing is that it needs to access the</span> <kbd>fare_amount</kbd> <span>field for every record. If we benchmark the function, it shows the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2351facb-3126-45e7-a9d8-5aaed47d2a0d.png" style="width:45.33em;height:3.00em;"/></div>
<p class="mce-root"/>
<p>How do we know whether it runs at optimal speed? We don't unless we try to do it differently. Because all we are doing is calculating the mean of 100,000 floating-point numbers, we can easily replicate that with a simple array. Let's replicate the data in a separate array:</p>
<pre>fare_amounts = [r.fare_amount for r in records];</pre>
<p>Then, we can benchmark the <kbd>mean</kbd> function by passing the array as is:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e767bfde-5d70-473c-a2c6-72df4905136e.png" style="width:44.92em;height:3.17em;"/></p>
<p><em>Whoa!</em> What's happening here? It is 24x faster than before. </p>
<p>In this case, the compiler was able to make use of the more advanced CPU instructions. Because Julia arrays are dense arrays, that is, data is compactly stored in a contiguous block of memory, it enables the compiler to fully optimize the operation.</p>
<p>Converting data into an array seems to be a decent solution. However, just imagine that you have to create these temporary arrays for every single field. It is not much fun anymore as there is a possibility to miss a field while doing so. Is there a better way to solve this problem?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving performance using a different data layout</h1>
                </header>
            
            <article>
                
<p>The problem we just saw is caused by the use of an array of structs. What we really want is a struct of arrays. Notice the difference between arrays of structs and structs of arrays? </p>
<p>In an array of structs, to access a field for an object, the program must first index into the object and then find the field via a predetermined offset in memory. For example, the <kbd>passenger_count</kbd> field in the <kbd>TripPayment</kbd> object is the fourth field of the struct where the preceding three fields are <kbd>Int64</kbd>, <kbd>String</kbd>, and <kbd>String types</kbd>. So, the offset to the fourth field is 24. An array of structs has a row-oriented layout as every row is stored in a contiguous block of memory.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We now introduce the concept of struct of arrays. In a struct of arrays, we take a column-oriented approach. In this case, we only maintain a single object for the entire dataset. Within the object, each field represents an array of a particular field of the original record. For example, the <kbd>fare_amount</kbd> field would be stored as an array of fare amounts in this object. The column-oriented format is optimized for high-performance computing because the data values in the array all have the same type. In addition, they are also more compact in memory. </p>
<div class="packt_tip">A struct is typically aligned into 8-byte memory blocks in a 64-bit system. For example, a struct that contains just two fields of <kbd>Int32</kbd> and <kbd>Int16</kbd> types still consumes 8 bytes even though 6 bytes are enough to store the data. The two extra bytes are used to pad the data structure to an 8-byte boundary.</div>
<p>In the following sections, we will look into how to implement this pattern and confirm that performance has improved.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constructing a struct of arrays</h1>
                </header>
            
            <article>
                
<p>It is easy and straightforward to construct a struct of arrays. After all, we were able to quickly do that for a single field earlier. For completeness, this is how we can design a new data type for storing the same trip payment data in a column-oriented format. The following code shows that this pattern helps to improve performance:</p>
<pre>struct TripPaymentColumnarData<br/>    vendor_id::Vector{Int}<br/>    tpep_pickup_datetime::Vector{String}<br/>    tpep_dropoff_datetime::Vector{String}<br/>    passenger_count::Vector{Int}<br/>    trip_distance::Vector{Float64}<br/>    fare_amount::Vector{Float64}<br/>    extra::Vector{Float64}<br/>    mta_tax::Vector{Float64}<br/>    tip_amount::Vector{Float64}<br/>    tolls_amount::Vector{Float64}<br/>    improvement_surcharge::Vector{Float64}<br/>    total_amount::Vector{Float64}<br/>end</pre>
<p>Notice that every field has been turned into <kbd>Vector{T}</kbd>, where <kbd>T</kbd> is the original data type of the particular field. It looks quite ugly but we are willing to sacrifice ugliness here for performance reasons.</p>
<p class="mce-root"/>
<div class="packt_tip">The general rule of thumb is that we should just <strong>Keep It Simple</strong> (<strong>KISS</strong>). Under certain circumstances, when we do need higher runtime performance, we could bend a little.</div>
<p>Now, although we have a data type that is more optimized for performance, we still need to populate it with data for testing. In this case, it can be achieved quite easily using array comprehension syntax:</p>
<pre>columar_records = TripPaymentColumnarData(<br/>    [r.vendor_id for r in records],<br/>    [r.tpep_pickup_datetime for r in records],<br/>    [r.tpep_dropoff_datetime for r in records],<br/>    [r.passenger_count for r in records],<br/>    [r.trip_distance for r in records],<br/>    [r.fare_amount for r in records],<br/>    [r.extra for r in records],<br/>    [r.mta_tax for r in records],<br/>    [r.tip_amount for r in records],<br/>    [r.tolls_amount for r in records],<br/>    [r.improvement_surcharge for r in records],<br/>    [r.total_amount for r in records]<br/>);</pre>
<p>When we're done, we can prove to ourselves that the new object structure is indeed optimized:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/84f64c6f-bc48-499b-8d94-e7c79571ab0c.png"/></p>
<p>Yes, it now has great performance, as we expected.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the StructArrays package</h1>
                </header>
            
            <article>
                
<p>The ugliness of the preceding columnar struct left us in a very unsatisfied state. Not only do we need to create a new data type with tons of <kbd>Vector</kbd> fields, we also have to create a constructor function to convert our array of structs into the new type.</p>
<p>We can recognize the power of Julia when we get to use powerful packages in its ecosystem. To fully implement this pattern, we will introduce the <kbd>StructArrays.jl</kbd> package, which automates most of the mundane tasks in turning an array of structs into a struct of arrays.</p>
<p class="mce-root"/>
<p>In fact, the usage of <kbd>StructArrays</kbd> is embarrassingly simple:</p>
<pre>using StructArrays<br/>sa = StructArray(records)</pre>
<p>Let's take a quick look at the content. First of all, we can treat <kbd>sa</kbd> just like the original array—for example, we can take the first three elements of the array as before:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2d939db7-ba66-4b72-aff8-2fd1cf4b068f.png" style="width:56.17em;height:16.42em;"/></p>
<p>If we pick just one record, it comes back with the original <kbd>TripPayment</kbd> object:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/45b3f966-82b6-401e-bd64-52be7717fc80.png" style="width:56.08em;height:5.17em;"/></p>
<p><span>Just to make sure that there is no mistake, we can also check the type of the first record:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/591c6b3b-eb14-40e1-8a1a-91fe8121e596.png" style="width:55.50em;height:3.83em;"/></p>
<p>Hence, the new <kbd>sa</kbd> <span>object</span><span> </span><span>works just like before. Now, the difference comes in when we need to access all of the data from a single field. For example, we can get the</span> <kbd>fare_amount</kbd> <span>field as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/21356317-9519-4dab-844e-5a6906b2d3f0.png" style="width:54.50em;height:10.58em;"/></p>
<p>Because the type is already materialized as a <em>dense array</em>, we can expect superb performance when doing numerical or statistical analysis on this field, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/49c5f557-8aa1-43b5-82f7-32a6776442e0.png" style="width:55.67em;height:4.08em;"/></p>
<div class="packt_infobox">What is a <kbd>DenseArray</kbd>? It is actually an abstract type for which all elements in the array are allocated in a contiguous block of memory. <kbd>DenseArray</kbd> is a super-type of array.<br/>
<br/>
<span>Julia supports dynamic arrays by default, which means the size of the array can grow when we push more data into it. When it allocates more memory, it copies</span> existing data over to the new memory location. <br/>
<br/>
To avoid excessive memory reallocation, the current implementation uses a sophisticated algorithm to increase the size of memory allocation—fast enough to avoid excessive reallocation but conservative enough to not over-allocate memory.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the space versus time trade-off</h1>
                </header>
            
            <article>
                
<p>The <kbd>StructArrays.jl</kbd> package provides a convenient mechanism to quickly turn an array of structs into a struct of arrays. We must recognize that the price we are paying is an additional copy of the data in memory. Hence, we are once again getting into the classic space versus time trade-off in computing.</p>
<p>Let's quickly look into our use case again. We can use the <kbd>Base.summarysize</kbd> function in the Julia REPL to see the memory footprint:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4e2b3396-19e2-43bc-ac89-72ecac1610a0.png" style="width:54.17em;height:8.33em;"/></p>
<p>The <kbd>Base.summarysize</kbd> function returns the size of the object in bytes. We divided the number <kbd>1024</kbd> twice to arrive at the mega-byte unit. It is interesting to see that the struct of arrays, <kbd>sa</kbd>, is more memory efficient than the original array of structs, <kbd>records</kbd>. Nevertheless, we have two copies of data in memory.</p>
<p class="mce-root"/>
<p>Fortunately, we do have some options here if we want to conserve memory. First, we may just discard the original data in the <kbd>records</kbd> variable if we no longer need the data in that structure. We can even force the garbage collector to run, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/27df9a41-047e-4546-8b21-7361fc6eab58.png" style="width:41.67em;height:4.75em;"/></p>
<p>Second, we can discard the <kbd>sa</kbd> variable when we are done with the computation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling nested object structures</h1>
                </header>
            
            <article>
                
<p>The preceding sample case works fine for any flat data structure. Nowadays, it is not uncommon to design types that contain other composite types. Let's drill down a little bit deeper to see how we can handle such a nested structure.</p>
<p>First, suppose that we want to separate the fields related to the fare in a separate composite data type:</p>
<pre>struct TripPayment<br/>    vendor_id::String<br/>    tpep_pickup_datetime::String<br/>    tpep_dropoff_datetime::String<br/>    passenger_count::Int<br/>    trip_distance::Float64<br/>    fare::Fare<br/>end<br/><br/>struct Fare<br/>    fare_amount::Float64<br/>    extra::Float64<br/>    mta_tax::Float64<br/>    tip_amount::Float64<br/>    tolls_amount::Float64<br/>    improvement_surcharge::Float64<br/>    total_amount::Float64<br/>end</pre>
<p>We can adjust the file reader slightly:</p>
<pre>function read_trip_payment_file(file)<br/>    f = CSV.File(file, datarow = 3)<br/>    records = Vector{TripPayment}(undef, length(f))<br/>    for (i, row) in enumerate(f)<br/>        records[i] = TripPayment(row.VendorID,<br/>                                 row.tpep_pickup_datetime,<br/>                                 row.tpep_dropoff_datetime,<br/>                                 row.passenger_count,<br/>                                 row.trip_distance,<br/>                                 Fare(row.fare_amount,<br/>                                      row.extra,<br/>                                      row.mta_tax,<br/>                                      row.tip_amount,<br/>                                      row.tolls_amount,<br/>                                      row.improvement_surcharge,<br/>                                      row.total_amount))<br/>    end<br/>    return records<br/>end</pre>
<p>After we read the data, the array of trip payment data would look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d98bcb93-8d70-4324-b6e0-f7d69dddb29d.png" style="width:52.25em;height:8.42em;"/></p>
<p>If we just create <kbd>StructArray</kbd> as before, we cannot extract the <kbd>fare_amount</kbd> field:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/97b5134f-b460-4efa-b67b-68b31c7440d8.png" style="width:52.17em;height:6.00em;"/></p>
<p>To achieve the same result at a level deeper, we can use the <kbd>unwrap</kbd> option:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ea510a52-e760-47e0-a56b-8ba0d3781cea.png" style="width:51.33em;height:2.58em;"/></p>
<p>The value of the <kbd>unwrap</kbd> keyword argument is basically a function that accepts a data type for a particular field. If the function returns <kbd>true</kbd>, then that particular field will be constructed with a nested <kbd>StructArray</kbd>.</p>
<p>We can now access the <kbd>fare_amount</kbd> field with another level of indirection as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1700346f-7b1a-4f47-a289-38f90ab5adcb.png" style="width:50.33em;height:11.83em;"/></p>
<p><span><span>Using the <kbd>unwrap</kbd> keyword argument, we can easily walk through the entire data structure and create a <kbd>StructArray</kbd> object that allows us to access any data element in a compact array structure. From this point on, application performance can be improved.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Considerations</h1>
                </header>
            
            <article>
                
<p>When designing applications, we ought to determine what is the most important thing that is valued by our users. Similarly, when working on data analysis or data science projects, we should think about what we care about the most. A customer-first approach is essential in any decision-making process.</p>
<p>Let's assume that our priority is to achieve better performance. Then, the next question is which part of the system requires optimization? If the part is slowed down due to the use of an array of structs, how much do we gain in speed when we employ the struct of arrays pattern? Is the performance gain noticeable—is it measured in milliseconds, minutes, hours, or days?</p>
<p>Further, we need to consider system constraints. We like to think that the sky is the limit. But then coming back to reality, we are limited in system resources all over the place<span>—the </span>number of CPU cores, available memory, and disk space, as well other limits imposed by our system administrators, such as, maximum number of opened files and processes.</p>
<p>While struct of arrays can improve performance, there is an overhead in allocating memory for the new arrays. If the data size is large, the allocation and data copy operation will take some time as well. </p>
<p>In the next section, we will look into another pattern that helps to conserve memory and allows distributed computing<span>—</span> shared arrays. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The shared array pattern</h1>
                </header>
            
            <article>
                
<p>Modern operating systems can handle many concurrent processes and fully utilize all processor cores. When it comes to distributed computing, a larger task is typically broken down into smaller ones such that multiple processes can execute the tasks concurrently. Sometimes, the results of these individual executions may need to be combined or aggregated for final delivery. This process is called <strong>reduction</strong>. </p>
<p>This concept is reincarnated in various forms. For example, in functional programming, it is common to implement data processes using map-reduce. The mapping process takes a list and applies a function to each element, and the reduction process combines the results. In big data processing, Hadoop uses a similar form of map-reduce, except that it runs across multiple machines in a cluster. The <kbd>DataFrames</kbd> package contains functions that perform the Split-Apply-Combine pattern. These all present pretty much the same concept.</p>
<p>Sometimes, parallel worker processes need to communicate with each other. In general, processes can talk to each other by passing data via some form of <strong>Inter-Process Communication</strong> (<strong>IPC</strong>). There are many ways to do that—sockets, Unix domain sockets, pipes, named pipes, message queues, shared memory, and memory maps.</p>
<p>Julia ships with a standard library called <kbd>SharedArrays</kbd>, which interfaces with the operating system's shared memory and memory map interface. This facility allows Julia processes to communicate with each other by sharing a central data source.</p>
<p>In this section, we will take a look at how <kbd>SharedArrays</kbd> can be used for high-performance computing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing a risk management use case</h1>
                </header>
            
            <article>
                
<p>In a risk management use case, we want to estimate the volatility of portfolio returns using a process called Monte Carlo simulation. The concept is pretty simple. First, we develop a risk model based on historical data. Second, we use the model to predict the future in 10,000 ways. Finally, we look at the distribution of security returns in the portfolio and gauge how much the portfolio gains or losses in each of those scenarios.</p>
<p>Portfolios are often measured against benchmarks. For example, a stock portfolio may be benchmarked against the S&amp;P 500 Index. The reason is that portfolio managers are typically rewarded for earning <em>alpha</em>, a term for describing the excess return that is over and above the benchmark's return. In other words, the portfolio manager is rewarded for their skills in picking the right stocks.</p>
<p class="mce-root"/>
<p>In the fixed income market, the problem is a little more challenging. Unlike the stock market, typical fixed income benchmarks are quite large, up to 10,000 bonds. In assessing portfolio risk, we often want to analyze the sources of return. Did the value of a portfolio go up because it was riding the wave in a bull market, or did it go down because everyone is selling? The risk that correlates to market movement is called <strong>systematic risk</strong>. Another source of return relates to the individual bond. For example, if the issuer of the bond is doing well and making good profit, then the bond has a lower risk and the price goes up. This kind of movement due to the specific individual bond is called <strong>idiosyncratic risk</strong>. For a global portfolio, some bonds are exposed to <strong>currency risk</strong> as well. From a computational complexity perspective, to <span>estimate the returns of the benchmark index 10,000 ways, we have to perform <em>10,000 future scenarios x 10,000 securities x 3 sources of returns = 300 million</em> pricing calculations.</span></p>
<p>Coming back to our simulation example, we can generate 10,000 possible future scenarios of the portfolio, and the results are basically a set of returns across all such scenarios. The returns data is stored on disk and is now ready for additional analysis. Here comes the problem—an asset manager has to analyze over 1,000 portfolios, and each portfolio may require access to returns data varying between 10,000 to 50,000 bonds depending on the size of the benchmark index. Unfortunately, the production server is limited in memory but has plenty of CPU resources. How can we fully utilize our hardware to perform the analysis as quickly as possible?</p>
<p>Let's quickly summarize our problem:</p>
<ul>
<li>Hardware:
<ul>
<li>16 vCPU</li>
<li>32 GB RAM</li>
</ul>
</li>
<li>Security returns data:
<ul>
<li>Stored in 100,000 individual files</li>
<li>Each file contains a 10,000 x 3 matrix (10,000 future states and 3 return sources)</li>
<li>Total memory footprint is ~22 GB</li>
</ul>
</li>
<li>Task:
<ul>
<li>Calculate statistical measures (standard deviation, skewness, and kurtosis) for all security returns across the 10,000 future states.</li>
<li>Do that as quickly as possible!</li>
</ul>
</li>
</ul>
<p>The naive way to just load all of the files sequentially. Needless to say, loading 100,000 files one by one is not going to be very fast no matter how small the files are. We are going to use the Julia distributed computing facility to get it done. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing data for the example</h1>
                </header>
            
            <article>
                
<p>To follow the subsequent code for this pattern, we can prepare some test data. Before you run the code here, make sure that you have enough disk space for the test data. You will need approximately 22 GB of free space.</p>
<p>Rather than putting 100,000 files in a single directory, we can split them into 100 sub-directories. So, let's just create the directories first. A simple function is created for that purpose:</p>
<pre>function make_data_directories()<br/>    for i in 0:99 <br/>        mkdir("$i") <br/>    end<br/>end</pre>
<p>We can assume that every security is identified by a numerical index value between 1 and 100,000. <span>Let's define a function that generates the path to find the file:</span></p>
<pre>function locate_file(index)<br/>    id = index - 1<br/>    dir = string(id % 100)<br/>    joinpath(dir, "sec$(id).dat")<br/>end</pre>
<p><span>The function is designed to hash the file into one of the 100 sub-directories. Let's see how it works:</span></p>
<pre>julia&gt; locate_file.(vcat(1:2, 100:101))<br/>4-element Array{String,1}:<br/> "0/sec0.dat"<br/> "1/sec1.dat"<br/> "99/sec99.dat"<br/> "0/sec100.dat"</pre>
<p>So, the first 100 securities are located in directories called <kbd>0</kbd>, <kbd>1</kbd>, ..., <kbd>99</kbd>. The 101<sup>st</sup> security starts wrapping and goes back to directory <kbd>0</kbd>. For consistency reasons, the filename contains the security index minus 1.</p>
<p>Now we are ready to generate the test data. Let's define a function as follows:</p>
<pre>function generate_test_data(nfiles)<br/>    for i in 1:nfiles<br/>        A = rand(10000, 3)<br/>        file = locate_file(i)<br/>        open(file, "w") do io<br/>            write(io, A)<br/>        end<br/>    end<br/>end</pre>
<p>To generate all test files, we just need to call this function by passing <kbd>nfiles</kbd> with a value of 100,000. By the end of this exercise, you should have test files scattered in all 100 sub-directories. Note that the <kbd>generate_test_data</kbd> function will take a few minutes to generate all the test data. Let's do that now:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/99083063-29a9-481c-8f3a-eda572dc7038.png" style="width:40.17em;height:10.58em;"/></p>
<p>When it is done, <span>let's quickly take a look at our data files in a Terminal:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9a49f834-4f81-44b6-920e-26d6f524b42b.png" style="width:40.00em;height:19.92em;"/></p>
<p>We're now ready to tackle the problem using the shared array pattern. Let's get started.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of a high-performance solution</h1>
                </header>
            
            <article>
                
<p>The beauty of <kbd>SharedArrays</kbd> is that the data is maintained as a single copy, and multiple processes can have both read and write access. It is a perfect solution to our problem.</p>
<p>In this solution, we will do the following:</p>
<ol>
<li>The master program creates a shared array.</li>
<li>Using a distributed <kbd>for</kbd> loop, the master program commands worker processes to read each individual file into a specific segment of the array.</li>
<li>Again, using <span>a distributed <kbd>for</kbd> loop, the master program commands worker process to perform statistical analysis.</span></li>
</ol>
<p>As we have 16 vCPUs, we can utilize all of them.</p>
<div class="packt_tip">In practice, we should probably utilize fewer vCPUs so that we can leave some room for the operating system itself. Your mileage may vary depending on what else is running on the same server. The best approach is to test various configurations and determine the optimal settings.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Populating data in the shared array</h1>
                </header>
            
            <article>
                
<p>The security return files are distributed and stored in 100 different directories. Where it gets stored is based upon a simple formula: <em>file index <strong>modulus</strong> 100</em>, where the <em>file index</em> is the numerical identifier for each security, numbered between 1 to 100,000.</p>
<p>Each data file is in a simple binary format. The upstream process has calculated three source returns for 10,000 future states, as in a 10,000 x 3 matrix. The layout is column-oriented, meaning that the first 10,000 numbers are used for the first return source, the next 10,000 numbers are for the second return source, and so on.</p>
<p>Before we start using distributed computing functions, we must spawn worker processes. Julia comes with a convenient command-line option (<kbd>-p</kbd>) that the user can specify the number of worker processes up front as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5ca28f21-f785-4ffe-90fa-082cc0f2468f.png" style="width:38.33em;height:14.67em;"/></p>
<p class="mce-root"/>
<p>When the REPL comes up, we would already have 16 processes running and ready to go.<span> The <kbd>nworkers</kbd> function confirms that all 16 worker processes are available. </span></p>
<p>Let's look into the code now. First, we must load <kbd>Distributed</kbd> and <kbd>SharedArrays</kbd> packages:</p>
<pre>using Distributed<br/>using SharedArrays</pre>
<p>To make sure that the worker processes know where to find the files, we have to change directory on all of them:</p>
<pre>@everywhere cd(joinpath(ENV["HOME"], "julia_book_ch06_data"))</pre>
<p>The <kbd>@everywhere</kbd> macro executes the statement on all worker processes.</p>
<p>The main program looks like this:</p>
<pre>nfiles = 100_000<br/>nstates = 10_000<br/>nattr = 3<br/>valuation = SharedArray{Float64}(nstates, nattr, nfiles)<br/>load_data!(nfiles, valuation)</pre>
<p>In this case, we are creating a 3-dimensional shared array. Then, we call the <kbd>load_data!</kbd> function to read all 100,000 files and shovel the data into the valuation matrix. How does the <kbd>load_data!</kbd> function work? Let's take a look:</p>
<pre>function load_data!(nfiles, dest)<br/>    @sync @distributed for i in 1:nfiles<br/>        read_val_file!(i, dest)<br/>    end<br/>end</pre>
<p>It's a very simple <kbd>for</kbd> loop that just calls the <kbd>read_val_file!</kbd> function with an index number. Notice the use of two macros here—<kbd>@distributed</kbd> and <kbd>@sync</kbd>. First, the <kbd>@distributed</kbd> macro does the magic by sending the body of the<span> </span><kbd>for</kbd><span> loop</span> to the worker processes. In general, the master program here does not wait for the worker processes to return. However, the <kbd>@sync</kbd> macro blocks until all jobs are completely finished. </p>
<p>How does it actually read the binary file? Let's see:</p>
<pre># Read a single data file into a segment of the shared array `dest`<br/># The segment size is specified as in `dims`. <br/>@everywhere function read_val_file!(index, dest)<br/>    filename = locate_file(index)<br/>    (nstates, nattrs) = size(dest)[1:2]<br/>    open(filename) do io<br/>        nbytes = nstates * nattrs * 8<br/>        buffer = read(io, nbytes)<br/>        A = reinterpret(Float64, buffer)<br/>        dest[:, :, index] = A<br/>    end<br/>end</pre>
<p>Here, the function first locates the path of the data file. Then, it opens the file and reads all the binary data into a byte array. Since the data is just 64-bit floating pointer numbers, we use the <kbd>reinterpret</kbd> function to parse the data into an array of <kbd>Float64</kbd> values. We do expect 30,000 <kbd>Float64</kbd> values here in each file, representing 10,000 future states and 3 source returns. When the data is ready, we just save them into the array for the particular index.</p>
<p>We also use the <kbd>@everywhere</kbd> macro to ensure that the function is defined and made available to all worker processes. The <kbd>locate_file</kbd> function is a little less interesting. It is included here for completeness:</p>
<pre>@everywhere function locate_file(index)<br/>    id = index - 1<br/>    dir = string(id % 100)<br/>    return joinpath(dir, "sec$(id).dat")<br/>end</pre>
<p>To load the data files in parallel, we can define a <kbd>load_data!</kbd> function as follows:</p>
<pre>function load_data!(nfiles, dest)<br/>    @sync @distributed for i in 1:nfiles<br/>        read_val_file!(i, dest)<br/>    end<br/>end</pre>
<p><span>Here, we just put the </span><kbd>@sync</kbd><span> and </span><kbd>@distributed</kbd><span> macros in front of a <kbd>for</kbd> loop. Julia automatically schedules and distributes the call among all work processes. </span>Now that everything is set up, we can run the program:</p>
<pre>nfiles = 100_000<br/>nstates = 10_000<br/>nattr = 3<br/>valuation = SharedArray{Float64}(nstates, nattr, nfiles)</pre>
<p class="mce-root"/>
<p>We simply create a valuation <kbd>SharedArray</kbd> object. Then, we pass it to the <kbd>load_data!</kbd> function for processing:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/40798cd2-7a61-4a6e-ba4b-36bba294fc47.png" style="width:38.92em;height:3.58em;"/></p>
<p>It only took about three minutes to load 100,000 files into memory using 16 parallel processes. <em>That's pretty good!</em></p>
<div class="packt_tip">If you try to run the program in your own environment but encounter an error, it may be due to system constraints. Refer to the later section, <em>Configuring system settings for shared memory usage</em>, for more information. </div>
<div class="packt_infobox">It turns out that this exercise is still IO-bound. CPU utilization hovered just around 5% during the load process. Should the problem demand incremental computation, we could possibly leverage the remaining CPU resource by spawning other asynchronous processes that operate on data and just got loaded into memory.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing data directly on a shared array</h1>
                </header>
            
            <article>
                
<p>Using shared arrays allows us to perform parallel operations on the data from a single memory space. As long as we do not mutate the data, then these operations can run independently without conflicts. This type of problem is called <em>embarrassingly parallel</em>.</p>
<p>To illustrate the power of multi-processing, let's first benchmark a simple function that calculates the standard deviation of the returns across all securities:</p>
<pre>using Statistics: std<br/><br/># Find standard deviation of each attribute for each security<br/>function std_by_security(valuation)<br/>    (nstates, nattr, n) = size(valuation)<br/>    result = zeros(n, nattr)<br/>    for i in 1:n<br/>        for j in 1:nattr<br/>            result[i, j] = std(valuation[:, j, i])<br/>        end<br/>    end<br/>    return result<br/>end</pre>
<p>The value of <kbd>n</kbd> represents number of securities.  The value of <kbd>nattr</kbd> represents number of sources of return. Let's see how much time it takes for a single process. <span>The best timing was 5.286 seconds:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6ee6bc24-1814-4c85-9f1d-578a99d03435.png" style="width:41.92em;height:16.00em;"/></p>
<div class="packt_tip">The <kbd>@benchmark</kbd> macro provides some statistics about the performance benchmark. Sometimes, it is useful to see the distribution and have an idea about how much GC impacts performance.<br/>
<br/>
The <kbd>seconds=30</kbd> parameter was specified because this function takes seconds to run. The default parameter value is 5 seconds, and that would not allow the benchmark to collect enough samples for reporting. </div>
<p>We are now ready to run the program in parallel. First, we need to make sure that all child processes have the dependent packages loaded:</p>
<pre>@everywhere using Statistics: std</pre>
<p>Then, we can define a distributed function, as follows:</p>
<pre>function std_by_security2(valuation)<br/>    (nstates, nattr, n) = size(valuation)<br/>    result = SharedArray{Float64}(n, nattr)<br/>    @sync @distributed for i in 1:n<br/>        for j in 1:nattr<br/>            result[i, j] = std(valuation[:, j, i])<br/>        end<br/>    end<br/>    return result<br/>end</pre>
<p>This function looks very similar to the previous one, with some exceptions:</p>
<ol>
<li>We have allocated a new shared array, <kbd>result</kbd>, to store the computed data. This array is 2-dimensional because we are reducing the third dimension into a single standard deviation value. This array is accessible by all worker processes.</li>
<li>The <kbd>@distributed</kbd> macro in front of the<span> </span><kbd>for</kbd><span> loop </span>is used to automatically distribute the work, in other words, the body of the<span> </span><kbd>for</kbd><span> loop</span>, to the worker processes. </li>
<li>The <kbd>@sync</kbd> macro in front of the<span> </span><kbd>for</kbd><span> loop </span>makes the system wait until all of the work is done.</li>
</ol>
<p>We can now benchmark the performance of this new function using the same 16 worker processes:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3ca5af15-5673-46dd-ae1b-b28a96bf02f8.png" style="width:41.75em;height:16.33em;"/></p>
<p>Compared to the performance of a single process, this is approximately 6x faster than before.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the overhead of parallel processing</h1>
                </header>
            
            <article>
                
<p>Have you noticed something interesting here? Since we have 16 worker processes, we would have expected that the parallel processing function to be close to 16 times faster. But the result came in at around 6 times, which is somewhat less than we expected. Why is that?</p>
<p class="mce-root"/>
<p>The answer is that it is just a matter of scale. There is some performance overhead to use the parallel processing facility. Typically, this overhead can be ignored because it is immaterial when compared to the amount of work being performed. In this particular example, calculating standard deviation is a really trivial computation. So, in relative terms, the overhead of coordinating remote function calls and collecting results overshadows the actual work itself. </p>
<p>Perhaps we should prove it. Let's just do a little more work and calculate skewness and kurtosis in addition to standard deviation:</p>
<pre>using Statistics: std, mean, median<br/>using StatsBase: skewness, kurtosis<br/><br/>function stats_by_security(valuation, funcs)<br/>    (nstates, nattr, n) = size(valuation)<br/>    result = zeros(n, nattr, length(funcs))<br/>    for i in 1:n<br/>        for j in 1:nattr<br/>            for (k, f) in enumerate(funcs)<br/>                result[i, j, k] = f(valuation[:, j, i])<br/>            end<br/>        end<br/>    end<br/>    return result<br/>end</pre>
<p>The parallel processing version is similar:</p>
<pre>@everywhere using Statistics: std, mean, median<br/>@everywhere using StatsBase: skewness, kurtosis<br/><br/>function stats_by_security2(valuation, funcs)<br/>    (nstates, nattr, n) = size(valuation)<br/>    result = SharedArray{Float64}((n, nattr, length(funcs)))<br/>    @sync @distributed for i in 1:n<br/>        for j in 1:nattr<br/>            for (k, f) in enumerate(funcs)<br/>                result[i, j, k] = f(valuation[:, j, i])<br/>            end<br/>        end<br/>    end<br/>    return result<br/>end</pre>
<p>Let's compare their performance now:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/80ed0ef1-3b83-46f8-b933-0b2a7a2f481a.png" style="width:40.00em;height:9.25em;"/></p>
<p>The parallel process is now 9x faster, as shown in the preceding. If we continue on this path and do more non-trivial computation, then we would expect a higher impact up to somewhere closer to 16x difference.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring system settings for shared memory usage</h1>
                </header>
            
            <article>
                
<p>The magic of <kbd>SharedArrays</kbd> come from the use of memory map and shared memory facilities in the operating system. When dealing with large amounts of data, we may need to configure the system to handle the volume.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adjusting system kernel parameters </h1>
                </header>
            
            <article>
                
<p>The Linux operating system has a limit on the size of shared memory. To find out what that is, we can use the <kbd>ipcs</kbd> command:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2f6ae1e3-49c1-4606-9c44-97fa10349c7d.png" style="width:42.00em;height:10.00em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The <kbd>E</kbd> unit may look a little unfamiliar. It's in exabytes, which basically mean 18 zeros: <kbd>kilo</kbd>, <kbd>mega</kbd>, <kbd>giga</kbd>, <kbd>tera</kbd>, <kbd>peta</kbd>, and <kbd>exa</kbd>. Get it? So, we're in luck here, because the limit is so high that we will probably never reach. However, if you see a small number, then you may need to reconfigure the system. The three kernel parameters are as follows:</p>
<ul>
<li>Maximum number of segments (<span>SHMMNI)</span></li>
<li>Maximum segment size (<span>SHMMAX)</span></li>
<li>Maximum total shared memory (<span>SHMALL)</span></li>
</ul>
<p>We can find out the actual values using the <kbd>sysctl</kbd> command:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3d6c6676-4fb5-4855-9fbd-dd7832911227.png" style="width:42.83em;height:5.50em;"/></p>
<p>To adjust the values, we can again use the <kbd>sysctl</kbd> command. For example, to set the maximum segment size (<kbd>shmmax</kbd>) to 128 GiB, we can do the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6c10ef33-5b2f-46c1-8147-fb6110b5ba99.png" style="width:43.17em;height:2.67em;"/></p>
<p>We can see that the kernel setting is now updated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring a shared memory device</h1>
                </header>
            
            <article>
                
<p>It is not enough to just change the system limits as shown in the preceding section. The Linux kernel actually uses the <kbd>/dev/shm</kbd> device as an in-memory backing store for shared memory. We can find out the size of the device using the regular <kbd>df</kbd> command:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b6305740-ce86-4c5f-9402-8b0ec6c1f737.png" style="width:40.92em;height:14.00em;"/></p>
<p class="mce-root"/>
<p>At the current state, the <kbd>/dev/shm</kbd> device is unused as shown in the preceding. The overall size of the block device is 16 GiB. As an exercise, let's now open a Julia REPL and create <kbd>SharedArray</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/497f7a1d-d304-47ab-8b76-dde3bd146aeb.png" style="width:41.58em;height:13.58em;"/></p>
<p>Re-running the <kbd>df</kbd> command, we can see that <kbd>/dev/shm</kbd> is now used:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/89ce04c0-03c3-47a6-95c2-9a0b0dab1035.png" style="width:43.67em;height:3.83em;"/></p>
<p>Now that we know <kbd>SharedArray</kbd> uses the <kbd>/dev/shm</kbd> device, how can we increase the size to accommodate our problem, which requires more than 22 GiB? It can be done using the <kbd>mount</kbd> command with a new size:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4bbd27b7-904d-4e2f-abba-56c95e2806d5.png" style="width:44.92em;height:6.92em;"/></p>
<p>The size of <kbd>/dev/shm</kbd> is now clearly shown as <kbd>28G</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Debugging the shared memory size issue </h1>
                </header>
            
            <article>
                
<p>What happens if we exceed the size of the shared memory device if we have forgotten to increase the size as described earlier? Let's say we need to allocate 20 GiB but there is only 16 GiB:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a79c1524-d21b-4eed-801e-62a129a38c0d.png" style="width:40.50em;height:10.67em;"/></p>
<p>There is no error even though we have exceeded the limit! Are we getting a free ride? The answer is no. It turns out that Julia does not know the limit has been breached. We can even work with the array <em>up close and personal</em> to the 16 GiB mark:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/65d91dad-4d67-4836-82d6-9577cc8a0a31.png" style="width:40.17em;height:5.67em;"/></p>
<p>The preceding code simply sets the first 15 GiB of memory to <kbd>0x01</kbd>. No error is shown so far. Going back to the shell, we can check the size of <kbd>/dev/shm</kbd> again. Clearly, 15 GiB is in use:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6a13b028-c67d-4aa6-933e-3b0ac611ebd1.png" style="width:39.17em;height:3.67em;"/></p>
<p>Now, if we continue to assign values to the later part of the array, we get an ugly <span class="packt_screen">Bus error</span> and a long stack trace:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8457ac45-5f3b-413c-941c-2f4b4906d725.png" style="width:46.83em;height:9.17em;"/></p>
<p>You may wonder why Julia cannot be smarter and tell you up front that you do not have enough shared memory space. As it turns out, it's the same behavior if you had used the underlying operating system's <kbd>mmap</kbd> function. Honestly, Julia just does not have any more information about the system constraint.</p>
<p class="mce-root"/>
<p>Sometimes, a C function's manual page can be useful and provide some hints. For example, the documentation about the <kbd>mmap</kbd> call indicates that a SIGBUS signal will be thrown when the program attempts to access an unreachable portion of the memory buffer. The manual page can be found at <a href="https://linux.die.net/man/2/mmap">https://linux.die.net/man/2/mmap</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensuring worker processes have access to code and data </h1>
                </header>
            
            <article>
                
<p class="mce-root">When developing parallel computation, a beginner often runs into the following issues:</p>
<ul>
<li class="mce-root"><strong>Functions not defined in the worker processes: </strong>This can be a symptom of a library package not being loaded, or a function that was only defined in the current process but not defined in the worker processes. Both issues can be resolved by using the <kbd>@everywhere</kbd> macro as shown in the preceding examples.</li>
<li class="mce-root"><strong>Data not available in the worker processes: </strong>This can be a symptom of the data being stored as a variable in the current processes but not passed to the worker processes. <kbd>SharedArray</kbd> is convenient because it is automatically made available to worker processes. For other cases, the programmer generally has two options:
<ul>
<li class="mce-root">Explicitly pass the data via function arguments.</li>
<li class="mce-root">If the data is in a global variable, then it can be transferred using the <kbd>@everywhere</kbd> <span>macro, as follows:</span></li>
</ul>
</li>
</ul>
<pre style="padding-left: 150px"><span>@everywhere my_global_var = whatever_value</span></pre>
<p class="mce-root">For more advanced use cases, the <kbd>ParallelDataTransfer.jl</kbd> <span>package </span>provides several helpful functions to facilitate data transfer among the master process and worker processes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Avoiding race conditions among parallel processes</h1>
                </header>
            
            <article>
                
<p><kbd>SharedArrays</kbd> provides an easy conduit for sharing data across multiple processes. At the same time, a <span><kbd>SharedArray</kbd> is by design a global variable across all worker processes. </span>As a general rule of thumb for every parallel program, extreme care should be given when the array is mutated. If the same memory address needs to be written by multiple processes, then these operations must be synchronized or the program could crash easily. </p>
<p>The best option is to avoid mutation whenever possible.</p>
<p class="mce-root"/>
<p>An alternative is to assign each worker a mutually exclusive set of slots in the array so that they do not collide with each other.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with the constraints of shared arrays</h1>
                </header>
            
            <article>
                
<p>Elements in a <kbd>SharedArray</kbd> must be <em>bits type</em>. What does that mean? The formal definition of bits type can be summarized as follows:</p>
<ul>
<li>The type is immutable.</li>
<li>The type contains only primitive types or other bits types.</li>
</ul>
<p>The following <kbd>OrderItem</kbd> type is a bits type because all fields are primitive types:</p>
<pre>struct OrderItem<br/>    order_id::Int<br/>    item_id::Int<br/>    price::Float64<br/>    quantity::Int<br/>end</pre>
<p>The following <kbd>Customer</kbd> type is not a bits type because it contains a reference to <kbd>String</kbd>, which is neither a primitive type nor a bits type:</p>
<pre>struct Customer<br/>    name::String<br/>    age::Int<br/>end</pre>
<p>Let's try to create <kbd>SharedArray</kbd> for a bits type. The following code confirms that it works properly:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4712c21d-01c9-426d-ad50-b370085cd0a9.png" style="width:40.17em;height:15.58em;"/></p>
<p class="mce-root"/>
<p>If we try to create <kbd>SharedArray</kbd> with a non-bits type such as a mutable struct type, an error will result:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d7034c0d-e743-4c89-acde-cc8e9b017467.png" style="width:40.50em;height:10.33em;"/></p>
<p>In summary, Julia's shared array is a great way to distribute data to multiple parallel processes for high-performance computing. The programming interface is also very easy to use.</p>
<p>In the next section, we will look into a pattern that improves performance by exploiting the space-time trade-off.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The memoization pattern</h1>
                </header>
            
            <article>
                
<p>In 1968, an interesting article was published—it envisioned that computers should be able to learn from experience during execution and improve their own efficiency. </p>
<p><span>In developing software, we often face a situation where the speed of execution is constrained by many factors. Maybe a function needs to read a large amount of historical data from disk (also known as I/O-bound). Or a function just needs to perform some complex calculation that takes a lot of time (also known as CPU-bound). When these functions are called repeatedly, application performance can suffer greatly.</span></p>
<p>Memoization is a powerful concept to address these problems. In recent years, it has become more popular as functional programming is becoming more mainstream. The idea is really simple. When a function is called for the first time, the return value is stored in a cache. If the function is called again with the exact same argument as before, we can look up the value from the cache and return the result immediately. </p>
<p>As you will see later in this section, memoization is a specific form of caching where the return data of a function call is cached according to the arguments being passed to the function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing the Fibonacci function</h1>
                </header>
            
            <article>
                
<p>In functional programming, recursion is a common technique for computation. Sometimes, we may fall into a performance pitfall unknowingly. A classic example is the generation of a Fibonacci sequence, which is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c33798f1-b1a1-4b1e-85cb-88cf2e05f396.png" style="width:36.33em;height:16.33em;"/></p>
<p>It works well functionally but it is not very efficient. Why? It is because the function is recursively defined, and the same function is called multiple times with the same arguments. Let's take a look at the computation graph when finding the sixth Fibonacci number, where each <kbd>f(n)</kbd> node represents a call to the <kbd>fib</kbd> function:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d77512b9-8018-4131-9c85-41ce8c6d9ce5.png" style="width:33.92em;height:16.75em;"/></p>
<p>As you can see, the function is called many times, especially for those that are at the beginning part of the sequence. To calculate <kbd>fib(6)</kbd>, we end up calling the function 15 times! And this is like a snowball, getting worse very quickly. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving the performance of the Fibonacci function</h1>
                </header>
            
            <article>
                
<p>First, let's analyze how bad the performance is by revising the function to keep track of the number of executions. The code is as follows:</p>
<pre>function fib(n)<br/>    if n &lt; 3<br/>        return (result = 1, counter = 1)<br/>    else<br/>        result1, counter1 = fib(n - 1)<br/>        result2, counter2 = fib(n - 2)<br/>        return (result = result1 + result2, counter = 1 + counter1 + counter2)<br/>    end<br/>end</pre>
<p>Every time the <kbd>fib</kbd> function is called, it keeps tracks a counter. If the value of <kbd>n</kbd> is smaller than <kbd>3</kbd>, then it returns the count of <kbd>1</kbd> along with the result. If <kbd>n</kbd> is a larger number, then it aggregates the counts from the recursive calls to <kbd>fib</kbd> function.</p>
<p>Let's run it several times with various input values:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/46c45877-c458-4688-9327-28a040dae00e.png" style="width:37.92em;height:10.17em;"/></p>
<p>This simple example just illustrated how <span>quickly</span> it turns into a disaster when the computer has no memory about what it did before. A high school student would be able to calculate <kbd>fib(20)</kbd> manually with just 18 additions, discounting the first two numbers of the sequence. Our nice little function calls itself over 13,000 items!</p>
<p>Let's now put back the original code and benchmark the function. To illustrate the problem, I will start with <kbd>fib(40)</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0a6f947b-432a-46fd-a066-e7f37228baa2.png" style="width:38.50em;height:7.75em;"/></p>
<p>For this task, the function should really return instantly. The 430 millisecond feels like an eternity in computer time!</p>
<p>We can use memoization to solve this problem. Here is our first attempt:</p>
<pre>const fib_cache = Dict()<br/><br/>_fib(n) = n &lt; 3 ? 1 : fib(n-1) + fib(n-2)<br/><br/>function fib(n)<br/>    if haskey(fib_cache, n)<br/>        return fib_cache[n]<br/>    else<br/>        value = _fib(n)<br/>        fib_cache[n] = value<br/>        return value<br/>    end<br/>end</pre>
<p>First of all, we have created a dictionary object called <kbd>fib_cache</kbd> to store the results of previous calculations. Then, the core logic for the Fibonacci sequence is captured in this private function, <kbd>_fib</kbd>. </p>
<p>The <kbd>fib</kbd> function works by first looking up the input argument from the <kbd>fib_cache</kbd> dictionary. If the value is found, it returns the value. Otherwise, it invokes the private function, <kbd>_fib</kbd>, and updates the cache before returning the value.</p>
<p>The performance should be much better now. Let's test it quickly:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bf205143-755b-4ae0-8c18-349cf1b3fdc9.png" style="width:41.92em;height:3.92em;"/></p>
<p>We should be must happier with the performance result by now.</p>
<p class="mce-root"/>
<div class="packt_tip packt_infobox">We have used a <kbd>Dict</kbd> object to cache calculation results here for demonstration purposes. In reality, we can optimize it further by using an array as a cache. The lookup from an array should be a lot faster than a dictionary key lookup.<br/>
<br/>
Note that an array cache works well for the <kbd>fib</kbd> function because it takes a positive integer argument. For more complex functions, a <kbd>Dict</kbd> cache would be more appropriate.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automating the construction of a memoization cache</h1>
                </header>
            
            <article>
                
<p>While we are quite happy with the result in the preceding implementation, it feels a little unsatisfactory because we have to write the same code every time we need to memoize a new function. Wouldn't it be nice if the cache is automatically maintained? Realistically, we just need one cache for each function that we want to memoize.</p>
<p>So, let's do it a little differently. The thought is that we should be able to build a higher-order function that takes an existing function and return a memoized version of it. Before we get there, let's first redefine our <kbd>fib</kbd> function as an anonymous function, as follows:</p>
<pre>fib = n -&gt; begin<br/>    println("called")<br/>    return n &lt; 3 ? 1 : fib(n-1) + fib(n-2)<br/>end</pre>
<p>For now, we have added a <kbd>println</kbd> statement just so that we can validate the correctness of our implementation. If it works properly, <kbd>fib</kbd> should not be called millions of times. Moving on, we can define a <kbd>memoize</kbd> function as follows:</p>
<pre>function memoize(f)<br/>    memo = Dict()<br/>    x -&gt; begin<br/>        if haskey(memo, x)<br/>            return memo[x]<br/>        else<br/>            value = f(x)<br/>            memo[x] = value<br/>            return value<br/>        end<br/>    end<br/>end</pre>
<p class="mce-root">The <kbd>memoize</kbd> function first creates a local variable called <kbd>memo</kbd> for storing previous return values. Then, it returns an anonymous function that captures the <kbd>memo</kbd> variable, performs cache lookup, and calls <kbd>f</kbd> <span>functions</span><span> </span><span>when it is needed. This coding style of capturing a variable in an anonymous function is called a</span> <strong>closure</strong><span>. Now, we can use the</span> <kbd>memoize</kbd> <span>function to build a cache-aware</span> <kbd>fib</kbd> <span>function:</span></p>
<pre>fib = memoize(fib)</pre>
<p>Let's also prove that it does not call the original <kbd>fib</kbd> function too many times. For example, running <kbd>fib(6)</kbd> should be no more than 6 calls:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b1cee8a1-3c1c-4069-b16e-31e576af7c0a.png" style="width:36.33em;height:9.25em;"/></p>
<p>That looks satisfactory. If we run the function again with any input less than or equal to 6, then the original logic should not be called at all, and all results should be returned straight from the cache. However, if the input is larger than 6, then it calculates the ones above 6. Let's try that now:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/82adab99-9b14-4c83-89d3-eb0a5e7860bd.png" style="width:35.17em;height:10.25em;"/></p>
<p>We cannot conclude what we did is good enough until we benchmark the new code. Let's do it now.</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/17ac42d4-ee15-4999-a35d-c79d011a3306.png" style="width:35.00em;height:8.25em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The original function took 433 ms to compute <kbd>fib(400)</kbd>. This memoized version only takes 50 ns. This is a huge difference.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the constraint with generic functions</h1>
                </header>
            
            <article>
                
<p>One drawback of the preceding method is that we must define the original function as an anonymous function rather than a generic function. That seems to be a major constraint. The question is why doesn't <span>it</span><span> </span><span>work with generic function?</span></p>
<p>Let's do a quick test by starting a new Julia REPL, defining the original <kbd>fib</kbd> function again, and wrapping it with the same <kbd>memoize</kbd> function:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c1bd0fc0-fdeb-495e-96b5-b2a356bab3b1.png" style="width:41.17em;height:5.33em;"/></p>
<p>The problem is that <kbd>fib</kbd> is already defined as a generic function, and it cannot be bound to a new anonymous function, which is what is being returned from the <kbd>memoize</kbd> function. To work around the issue, we may be tempted to assign the memoized function with a new name:</p>
<pre>fib_fast = memoize(fib)</pre>
<p>However, it does not really work because the original <kbd>fib</kbd> function makes a recursive call to itself rather than the new memoized version. To see it more clearly, we can unroll a call as follows:</p>
<ol>
<li>Call the function as <kbd>fib_fast(6)</kbd>.</li>
<li>In the <kbd>fib_fast</kbd> function, it checks whether the cache contains a key that equals 6. </li>
<li>The answer is no, so it calls <kbd>fib(5)</kbd>.</li>
<li>In the <kbd>fib</kbd> function, since <kbd>n</kbd> is <kbd>5</kbd> and is greater than <kbd>3</kbd>, it calls <kbd>fib(4)</kbd> and <kbd>fib(3)</kbd> recursively.</li>
</ol>
<p>As you can see, the original <kbd>fib</kbd> function got called rather than the memoized version, so we are back to the same problem before. Hence, if the function being memoized uses recursion, then we must write the function as an anonymous function. Otherwise, it would be okay to create a memoized function with a new name. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supporting functions that take multiple arguments</h1>
                </header>
            
            <article>
                
<p>In practice, we would probably encounter functions that are more complex than this. For example, the function that requires speed-up probably requires multiple arguments and possibly keyword arguments as well. Our <kbd>memoize</kbd> function in the previous section assumes a single argument, so it would not work properly.</p>
<p>A simple way to fix this is illustrated as follows:</p>
<pre>function memoize(f)<br/>    memo = Dict()<br/>    (args...; kwargs...) -&gt; begin<br/>        x = (args, kwargs)<br/>        if haskey(memo, x)<br/>            return memo[x]<br/>        else<br/>            value = f(args...; kwargs...)<br/>            memo[x] = value<br/>            return value<br/>        end<br/>    end<br/>end</pre>
<p>The anonymous function being returned now covers any number of positional arguments and keyword arguments as specified in the splatted arguments, <kbd>args...</kbd> and <kbd>kwargs...</kbd>. We can quickly test this with a dummy function as follows:</p>
<pre># Simulate a slow function with positional arguments and keyword arguments<br/>slow_op = (a, b = 2; c = 3, d) -&gt; begin<br/>    sleep(2)<br/>    a + b + c + d<br/>end</pre>
<p>Then, we can create the fast version as follows:</p>
<pre>op = memoize(slow_op)</pre>
<p>Let's test the memoized function with a few different cases:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8a2049e1-1fef-459e-bf9e-b1d1627a1ee5.png" style="width:39.25em;height:14.33em;"/></p>
<p>It's working great!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling mutable data types in the arguments</h1>
                </header>
            
            <article>
                
<p>So far, we did not pay much attention to the arguments or keyword arguments being passed to the function. Care must be taken when any of those arguments are mutable. Why? Because our current implementation uses the arguments as the key of the dictionary cache. If we mutate the key of a dictionary, it could lead to unexpected results.</p>
<p>Suppose that we have a function that takes 2 seconds to run:</p>
<pre># This is a slow implementation<br/>slow_sum_abs = (x::AbstractVector{T} where {T &lt;: Real}) -&gt; begin<br/>    sleep(2)<br/>    sum(abs(v) for v in x)<br/>end</pre>
<p><span>Knowing that it's quite slow, we happily memoize it as usual:</span></p>
<pre>sum_abs = memoize(slow_sum_abs)</pre>
<p>Initially, it seems to work perfectly, as it has <span>always</span> been:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7e526f15-b390-475b-aff1-4fc1018d7d36.png" style="width:33.33em;height:18.17em;"/></p>
<p>However, we are shocked by the following observation:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1c494dcd-0233-4a83-92ba-f550f1640f71.png" style="width:34.08em;height:13.33em;"/></p>
<p><em>Bummer!</em> Rather than returning a value of <kbd>21</kbd>, it returns the previous result as if <kbd>-6</kbd> were not inserted to the array. Out of curiosity, let's push one more value to the array and try again:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0e520ace-52df-4451-bcfa-cacda611732f.png" style="width:33.50em;height:14.33em;"/></p>
<p>It's working again. Why is that happening? To understand that, let's recap how the <kbd>memoize</kbd> function was written:</p>
<pre>function memoize(f)<br/>    memo = Dict()<br/>    (args...; kwargs...) -&gt; begin<br/>        x = (args, kwargs)<br/>        if haskey(memo, x)<br/>            return memo[x]<br/>...</pre>
<p><span>As you can see, we are caching the data using the <kbd>(args, kwargs)</kbd> </span><span>tuple</span><span> </span><span>as the key of the dictionary object. The problem is that the argument being passed to the memoized</span> <kbd>sum_abs</kbd> <span>function is a mutable object. The dictionary object gets</span> <em>confused</em> <span>when the key is mutated. In that case, it may or may not locate the key anymore.</span></p>
<p>When we added <kbd>-6</kbd> to the array, it found the same object in the dictionary and returned the cached result. When we added <kbd>7</kbd> to the array, it could not find the object. Hence, the function does not work 100% of the time.</p>
<p>To fix this issue, we need to make sure that the content of the arguments are considered, not just the memory address of the container. A common practice is to apply a <kbd>hash</kbd> function to the thing that we wish to use as a key to the dictionary. Here's one implementation:</p>
<pre>function hash_all_args(args, kwargs)<br/>    h = 0xed98007bd4471dc2<br/>    h += hash(args, h)<br/>    h += hash(kwargs, h)<br/>    return h<br/>end</pre>
<p>The initial value of the <kbd>h</kbd> <span>variable</span><span> </span><span>is randomly selected. On a 64-bit system, we can generate it with a call to</span> <kbd>rand(UInt64)</kbd><span>. </span><span>The </span><kbd>hash</kbd><span> function is a generic function defined in the <kbd>Base</kbd> module. </span><span>We will keep it simple here for illustration purposes. In reality, a better implementation would support a 32-bit system as well. </span></p>
<p>The <kbd>memoize</kbd> function can now be rewritten to utilize such a hashing scheme:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d1292b06-fc26-42cd-bfa7-6f5d6bcc1f12.png" style="width:37.50em;height:19.33em;"/></p>
<p>We can test it again more extensively. Let's redefine the <kbd>sum_abs</kbd> function again using the new <kbd>memoize</kbd> function. Then, we run a loop and capture the calculation result and timing.</p>
<p>The result is shown as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/235a693a-2447-431c-b66a-4b7174ddb109.png" style="width:38.08em;height:23.00em;"/></p>
<p><em>Fantastic!</em> It now returns the correct result even though the input data has been mutated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Memoizing generic functions with macros</h1>
                </header>
            
            <article>
                
<p>Earlier, we discussed that generic functions cannot be supported by the <kbd>memoize</kbd> function. It would be most awesome if we can just annotate the functions as memoized while they are being defined. For example, the syntax would be like this:</p>
<pre>@memoize fib(n) = n &lt; 3 ? 1 : fib(n-1) + fib(n-2)</pre>
<p>It turns out that there's already an awesome package called <kbd>Memoize.jl</kbd> that does the exact same thing. It is indeed quite convenient:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6c88cbb9-ec27-474c-9945-fda4f353f0fd.png" style="width:36.42em;height:17.50em;"/></p>
<p>Here, we can observe the following:</p>
<ol>
<li>The first call to <kbd>fib(40)</kbd> was quite fast already, which is an indication that the cache is utilized.</li>
<li>The second call to <kbd>fib(40)</kbd> was almost instant, which means that the result was just a cache lookup.</li>
<li>The third call to <kbd>fib(39)</kbd> was almost instant, which means that the result was just a cache lookup.</li>
</ol>
<div class="packt_infobox">You should be advised that <kbd>Memoize.jl</kbd> does not support mutable data as arguments either. It carries the same problem that we described in the preceding section because it uses the objects' memory addresses as the key to the dictionary.</div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Turning to real-life examples</h1>
                </header>
            
            <article>
                
<p>Memoization is used in some open source packages. The actual usage may be more common in private applications and data analysis. Let's see some use cases for memoization in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Symata.jl</h1>
                </header>
            
            <article>
                
<p>The <kbd>Symata.jl</kbd> package provides support for Fibonacci polynomials. As we may have realized, the implementation of Fibonacci polynomials is also recursive just like the Fibonacci sequence problem we discussed earlier in this section. <kbd>Symata.jl</kbd> uses the <kbd>Memoize.jl</kbd> package to create the <kbd>_fibpoly</kbd> function as follows:</p>
<pre>fibpoly(n::Int) = _fib_poly(n)<br/><br/>let myzero = 0, myone = 1, xvar = Polynomials.Poly([myzero,myone]), zerovar = Polynomials.Poly([myzero]), onevar = Polynomials.Poly([myone])<br/>    global _fib_poly<br/>    @memoize function _fib_poly(n::Int)<br/>        if n == 0<br/>            return zerovar<br/>        elseif n == 1<br/>            return onevar<br/>        else<br/>            return xvar * _fib_poly(n-1) + _fib_poly(n-2)<br/>        end<br/>    end<br/>end</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Omega.jl</h1>
                </header>
            
            <article>
                
<p>The <kbd>Omega.jl</kbd> package implements its own memoization cache. Interestingly, it ensures proper return type from the cache lookup using the <kbd>Core.Compiler.return_type</kbd> function. It is done to avoid type instability problems. In <em>The </em><em>barrier function pattern</em> section later in this chapter, we will discuss more the problem of type instability and how to deal with the issue. Check out the following code example:</p>
<pre>@inline function memapl(rv::RandVar, mω::TaggedΩ)<br/>  if dontcache(rv)<br/>    ppapl(rv, proj(mω, rv))<br/>  elseif haskey(mω.tags.cache, rv.id)<br/>    mω.tags.cache[rv.id]::(Core.Compiler).return_type(rv, typeof((mω.taggedω,)))<br/>  else<br/>    mω.tags.cache[rv.id] = ppapl(rv, proj(mω, rv))<br/>  end<br/>end</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Considerations</h1>
                </header>
            
            <article>
                
<p>Memoization can only be applied to <em>pure</em> functions. </p>
<p>What is a pure function? A function is called pure when it always returns the same value given the same input. It may seem intuitive for every function to behave that way but in practice, it is not that straightforward. Some functions are not pure due to reasons such as these:</p>
<ul>
<li>A function uses a random number generator and is expected to return random results.</li>
<li>A function relies on data from an external source that produces different data at different times.</li>
</ul>
<p>Because the memoization pattern uses function arguments as the key of the in-memory cache, it will always return the same result for the same key.</p>
<p><span>Another consideration is that we should be aware of the extra memory overhead due to the use of a cache. It is important to choose the right cache invalidation strategy for the specific use case. Typical cache invalidation strategies include <strong>Least Recently Used</strong> (<strong>LRU</strong>), <strong>First-In, First-Out</strong> (<strong>FIFO</strong>), and time-based expiration.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Utilizing the Caching.jl package</h1>
                </header>
            
            <article>
                
<p>There are several packages that can make memoization easier. Some are mentioned here:</p>
<ul>
<li><kbd>Memoize.jl</kbd> provides a <kbd>@memoize</kbd> macro. It's very easy to use.</li>
<li><kbd>Anamnesis.jl</kbd> provides a <kbd>@anamnesis</kbd> macro. It has more functionalities than <kbd><span>M</span><span>emoize.jl</span></kbd>.</li>
<li><kbd>Caching.jl</kbd> was created with the ambition to provide more functionalities such as persistence to disk, compression, and cache size management.</li>
</ul>
<p>Here, we can take a look at <span><kbd>Caching.jl</kbd> </span>as it is developed more recently and has great features.</p>
<p>Let's build a memoized CSV file reader as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/12cb7ee8-c416-4d03-b36c-c322c4bbda03.png" style="width:35.67em;height:9.42em;"/></p>
<p>The <kbd>@cache</kbd> macro makes a memoized version of the <span><kbd>read_csv</kbd> </span>function. To confirm that a file is read only once, we inserted a <kbd>println</kbd> statement and timed the file read operation.</p>
<p>For demonstration purposes, we have downloaded a copy of the film permits file from the City of New York. The file is available from <a href="https://catalog.data.gov/dataset/film-permits">https://catalog.data.gov/dataset/film-permits</a>. Let's read the data file now:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c3aa1355-bc14-491f-9beb-8a6728f00486.png" style="width:36.33em;height:13.00em;"/></p>
<p class="mce-root">Here, we can see that the file is read only once. If we call <kbd>read_csv</kbd> again with the same filename, then the same object is returned instantly. </p>
<p class="mce-root">We can examine the cache. Before doing that, let's see what properties <kbd>read_csv</kbd> supports:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9bb30aac-fd20-443a-a00e-9bf450e669db.png" style="width:38.17em;height:3.08em;"/></p>
<p class="mce-root"/>
<p>Without looking at the manual, we can guess that the <kbd>cache</kbd> property represents the cache. Let's take a quick look:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/06ccd591-911a-4a91-a157-ba8dc6e35c66.png" style="width:37.67em;height:4.25em;"/></p>
<p>We can also persist the cache to disk. Let's examine the name and size of the cache file:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fd22c071-0026-41cc-8cc2-f8cc13aea2fd.png" style="width:38.08em;height:14.42em;"/></p>
<p>The location of the cache file is found in the <kbd>filename</kbd> property. The file does not exist until the <kbd>@persist!</kbd> macro was used to persist data to disk. We can also see how many objects are present in memory or on disk by just examining the function <kbd>itself</kbd> from the REPL:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/21eedbd3-51ee-41ad-8ff3-84249d4ff7ca.png" style="width:37.67em;height:3.33em;"/></p>
<p>The <kbd>@empty!</kbd> macro can be used to purge the in-memory cache:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3f97c9cc-09ba-4765-9736-c35e380b27b4.png" style="width:36.92em;height:3.17em;"/></p>
<p class="mce-root"/>
<p>Interestingly, because the on-disk cache still exists, we can still utilize it without having to re-populate the memory cache:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6f8b658f-2b8f-446f-a8e5-9d61d32c9d5e.png" style="width:37.33em;height:8.33em;"/></p>
<p>Finally, we can synchronize the memory and disk caches:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ac89faf5-cb75-4957-a7a7-35c7a819dad3.png" style="width:37.58em;height:3.17em;"/></p>
<p>The <kbd>Caching.jl</kbd> package has more functionalities that are not shown here. Hopefully, we have got an idea of what it is capable of already.</p>
<p>Next, we will look into a pattern that can be used to address the type-instability problem, which is a common issue causing performance problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The barrier function pattern</h1>
                </header>
            
            <article>
                
<p>While Julia is designed as a dynamic language, it also aims for high performance. The magic comes from its state-of-the-art compiler. When the type of variables is known in a function, the compiler can generate highly optimized code. However, when the type of a variable is unstable, the compiler has to compile more generic code that works with any data types. In some sense, Julia can be forgiving—it never fails on you even when it comes with a cost against runtime performance.</p>
<p>What makes the type of a variable <em>unstable</em>? It means that in some circumstances the variable may be one type, and in other circumstances, it may be another type. This section will discuss such a type instability problem, how it may arise, and what we can do about it.</p>
<p><span>Barrier function is a pattern that can be used to solve performance problems due to type instability. </span><span>So, let's see how to achieve that.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying type-unstable functions</h1>
                </header>
            
            <article>
                
<p>In Julia, there is no need to specify the type of variables. In fact, to be more precise, variables are not typed. Variables are merely bindings to values, and values are typed. That is what makes Julia programs dynamic. However, such flexibility comes with a cost. Because the compiler must generate code that supports all possible types that may come up during runtime, it is unable to generate optimized code. </p>
<p><span>Consider a simple function that just returns an array of random numbers:</span></p>
<pre>random_data(n) = isodd(n) ? rand(Int, n) : rand(Float64, n)</pre>
<p>If the <kbd>n</kbd> <span>argument</span><span> </span><span>is odd, then it returns an array of random</span> <kbd>Int</kbd> <span>values. Otherwise, it returns an array of random</span> <kbd>Float64</kbd> <span>values.</span></p>
<p>This innocent function is actually type-unstable. We can use the <kbd>@code_warntype</kbd> facility to check:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/79870010-f89e-4e5d-9243-fb62731a99e6.png" style="width:39.75em;height:15.50em;"/></p>
<p>The <kbd>@code_warntype</kbd> macro displays an <strong>Intermediate Representation</strong> (<strong>IR</strong>) of the code. An IR is generated by the compiler after it understand the flow and <span>data type of every line in that code</span>. For our purpose here, we do not need to understand everything printed on screen but we can pay attention to the highlighted text as related to the data types generated from the code. In general, when you see red text, it would also be a red flag.</p>
<p>In this case, the compiler has figured that the result of this function can be an array of <kbd>Float64</kbd> or an array of <kbd>Int64</kbd>. Hence, the return type is just <span class="s1"><kbd>Union{Array{Float64,1}, Array{Int64,1}}</kbd>.</span></p>
<p class="mce-root"/>
<div class="packt_tip">In general, more red signs from the <kbd>@code_warntype</kbd> output indicates more type instability problems in the code.</div>
<div>
<p><span>The function does exactly what we want to do. But when it's used in the body of another function, the type instability problem further affects runtime performance. </span><span>We can use a barrier function to solve this problem.</span></p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding performance impact</h1>
                </header>
            
            <article>
                
<p><span>When a function is called, the type of its arguments are known and then the function is compiled with the exact data types from its arguments. This is called</span><em> specialization</em><span>.</span> What exactly is a barrier function? It simply exploits Julia's function specialization to <em>stabilize</em> the type of variable as part of a function call. <span>We will continue the preceding example to illustrate the technique.</span></p>
<p><span>First, let's create a simple function that makes use of the type unstable function, as mentioned earlier:</span></p>
<div>
<pre>function double_sum_of_random_data(n)<br/>    data = random_data(n)<br/>    total = 0<br/>    for v in data<br/>        total += 2 * v<br/>    end<br/>    return total<br/>end</pre>
<p>The <kbd>double_sum_of_random_data</kbd><span> </span>function is just a simple function that returns the sum of doubled random numbers generated by the<span> </span><kbd>random_data</kbd><span> </span>function. If we just benchmark the function with either an odd or an even number argument, it comes back with the following results:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9c281dda-c3a9-4d28-bcc0-6e8d027620ed.png" style="width:38.08em;height:6.67em;"/></p>
</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The timing is better for the call with an input value of <kbd>100001</kbd>, most likely because the random number generator for <kbd>Int</kbd> is better than the one for <kbd>Float64</kbd>. Let's see what <kbd>@code_warntype</kbd> comes back for this function:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7c4c9973-410d-4185-ab08-f6b3b767e3c7.png" style="width:41.58em;height:37.92em;"/></p>
<p>As you can see, there are tons of red marks around. The type instability issue of a single function has a larger impact on other functions that use it.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing barrier functions</h1>
                </header>
            
            <article>
                
<p>A barrier function involves refactoring a piece of logic from an existing function into a new, separate function. When it's done, all data required by the new function will be passed as function arguments. Continuing with the preceding example, we can factor out the logic that calculates the doubled sum of data as follows:</p>
<pre>function double_sum(data)<br/>    total = 0<br/>    for v in data<br/>    total += 2 * v<br/>    end<br/>    return total<br/>end</pre>
<p>Then, we just modify the original function to make use of this function:</p>
<pre>function double_sum_of_random_data(n)<br/>    data = random_data(n)<br/>    return double_sum(data)<br/>end</pre>
<p>Does it really improve performance? Let's run the test:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/158c750f-974c-47d4-891b-fbc4d57f1c59.png" style="width:41.17em;height:7.42em;"/></p>
<p>It turns out to have a huge difference for the <kbd>Float64</kbd> case—the elapsed time went from 347 to 245 microseconds. Comparing the floating-point sum versus integer sum cases, the result also makes perfect sense because summing integers is generally faster than summing floating-point numbers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dealing with a type-unstable output variable</h1>
                </header>
            
            <article>
                
<p>What we haven't noticed is another type instability problem concerning the accumulator. In the preceding example, the <kbd>double_sum</kbd> function has a <kbd>total</kbd> <span>variable</span><span> </span><span>that keeps track of the doubled numbers. The problem is that the variable was defined as an integer, but then the array may contain floating-pointer numbers instead. This problem can be easily revealed by running</span> <kbd>@code_warntype</kbd> <span>against both scenarios.</span></p>
<p><span><span>Here is the output of <kbd>@code_warntype</kbd> for when an array of integers is passed into the function:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/89141bb9-cfe6-4a36-a9aa-a02b9182dc1f.png" style="width:38.25em;height:32.83em;"/></p>
<p><span><span>Compare it with the output when an array of <kbd>Float64</kbd> is passed:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7d7a6dd4-9a34-4e2a-9c35-f4edf2f3d6e3.png" style="width:38.67em;height:33.58em;"/></p>
<p>If we call the function with an array of integers, then the type is stable. If we call the function with an array of floats, then we see the type instability issue.</p>
<p>How do we fix this? Well, there are standard <kbd>Base</kbd> functions for creating type-stable zeros or ones. For example, rather than hardcoding the initial value of <kbd>total</kbd> to be an integer zero, we can do the following instead:</p>
<pre>function double_sum(data)<br/>    total = zero(eltype(data))<br/>    for v in data<br/>        total += 2 * v<br/>    end<br/>    return total<br/>end</pre>
<p>If we look into the <kbd>@code_warntype</kbd> output of the <span><kbd>double_sum_of_random_data</kbd> function, it is much better than before. I will let you do this exercise and compare the <kbd>@code_warntype</kbd> output with the prior one.</span></p>
<p>A similar solution makes use of the parametric method:</p>
<pre>function double_sum(data::AbstractVector{T}) where {T &lt;: Number}<br/>    total = zero(T)<br/>    for v in data<br/>        total += v<br/>    end<br/>    return total<br/>end</pre>
<p>The <kbd>T</kbd> <span> </span><span>type parameter </span><span>is used to initialize the</span> <kbd>total</kbd> <span>variable to the properly typed value of zero.</span></p>
<p><span>This kind of performance gotcha is sometimes difficult to catch. To ensure optimized code is generated, it is always a good practice to use the following functions for an accumulator or an array that stores output values:</span></p>
<ul>
<li><kbd>zero</kbd> and <kbd>zeros</kbd> create a value of 0 or an array of 0s for the desired type.</li>
<li><kbd>one</kbd> and <kbd>ones</kbd> create a value of 1 or an array of 1s for the desired type.</li>
<li><span><kbd>similar</kbd> creates an array of the same type as the array argument.</span></li>
</ul>
<p>For example, we can create a value of 0 or an array of 0s for any numeric types as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/16e42dcf-9fed-4de6-9d8b-7d70389088a9.png" style="width:40.17em;height:13.17em;"/></p>
<p class="mce-root">Likewise, the <kbd>one</kbd> and <kbd>ones</kbd> functions work the same way:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/da6e68f1-dac2-42a5-9a3a-b551beef19a9.png" style="width:39.67em;height:13.33em;"/></p>
<p>If we want to create an array that looks like another one (in other words, has the same type, shape, and size), then we can use the <kbd>similar</kbd> function:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/944cce80-c2f4-49e5-8c01-e9a5c8918b00.png" style="width:39.42em;height:14.67em;"/></p>
<p>Note that the <kbd>similar</kbd> function does not zero out the content of the array.</p>
<p>The <kbd>axes</kbd> function may come in handy when we need to create an array of zeros that matches the same dimensions of another array:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0efd33ad-6ce6-4dd9-ac99-e27dd2fc44af.png" style="width:41.08em;height:7.42em;"/></p>
<p>Next, we will look into a way to debug type instability issues.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the @inferred macro</h1>
                </header>
            
            <article>
                
<p>Julia comes with a handy macro in the <kbd>Test</kbd> package that can be used to check whether the return type of a function matches the <em>inferred</em> return type of the function. The inferred return type is simply the type that we see from the <kbd>@code_warntype</kbd> output before.</p>
<p>For example, we can check the notorious <kbd>random_data</kbd> function from the beginning of this section:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dadfd917-89f6-4d9e-b427-d92337237f7a.png" style="width:43.08em;height:17.58em;"/></p>
<p>The macro reports an error whenever the actual returned type differs from the inferred return type. It could be a useful tool to validate the type instability problem as part of an automated test suite in the continuous integration pipeline.</p>
<p>The primary reason to use a barrier function is to improve performance where the type instability problem exists. If we think about it more deeply, it also has the side benefit of forcing us to create smaller functions. Smaller functions are easier to read and debug and perform better.</p>
<p>We have now concluded all patterns in this chapter.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we explored several patterns related to performance. </p>
<p>First, we discussed how global variables hurt performance and the technique of the global constant pattern. We looked into how the compiler optimizes performance by doing constant folding, constant propagation, and dead branch elimination. We also learned how to create a constant placeholder for wrapping a global variable.</p>
<p>We discussed how to utilize the struct of arrays pattern to turn an array of structs into a struct of arrays. The new layout of the data structure allows better CPU optimization and, hence, better performance. We took advantage of a very useful package, <kbd>StructArrays</kbd>, for automating such data structure transformation. We reviewed a financial services use case where a large amount of data needs to be loaded into memory and used by many parallel processes. We implemented the shared array pattern and went over some tricks to configure shared memory properly in the operating system.</p>
<p>We learned about the memoization pattern for caching function call results. We did a sample implementation using a dictionary cache and made it work with functions taking various arguments and keyword arguments. We also found a way to support mutable objects as function arguments. Finally, we discussed the barrier function pattern. We saw how performance can be degraded by type-unstable variables. We learned that splitting logic into a separate function allows the compiler to produce more optimal code. </p>
<p>In the next chapter, we will examine several patterns that improve system maintainability.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Why does the use of global variables impact performance?</li>
<li>What would be a good alternative to using a global variable when it cannot be replaced by a constant?</li>
<li>Why does a struct of arrays perform better than an array of structs?</li>
<li>What are the limitations of <kbd>SharedArray</kbd>?</li>
<li>What is an alternative to multi-core computation instead of using parallel processes?</li>
<li>What care must be taken when using the memoization pattern?</li>
<li>What is the magic behind barrier functions in improving performance?</li>
</ol>


            </article>

            
        </section>
    </body></html>