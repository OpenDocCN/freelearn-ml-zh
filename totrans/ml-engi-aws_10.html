<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer314">
<h1 class="chapter-number" id="_idParaDest-201"><a id="_idTextAnchor215"/><a id="_idTextAnchor216"/>10</h1>
<h1 id="_idParaDest-202"><a id="_idTextAnchor217"/>Machine Learning Pipelines with Kubeflow on Amazon EKS</h1>
<p>In <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a>, <em class="italic">Security, Governance, and Compliance Strategies</em>, we discussed a lot of concepts and solutions that focus on the other challenges and issues we need to worry about when dealing with <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) requirements. You have probably realized by now that ML practitioners have a lot of responsibilities and work to do outside model training and deployment! Once a model gets deployed into production, we would have to monitor the model and ensure that we are able to detect and manage a variety of issues. In addition to this, ML engineers might need to build ML pipelines to automate the different steps in the ML life cycle. To ensure that we reliably deploy ML models in production, as well as streamline the ML life cycle, it is best that we learn and apply the different principles of <strong class="bold">machine learning operations</strong> (<strong class="bold">MLOps</strong>). With MLOps, we will make use of the tried-and-tested tools and practices from <strong class="bold">software engineering</strong>, <strong class="bold">DevOps</strong>, and <strong class="bold">data engineering</strong> to <em class="italic">productionalize</em> ML models. These include utilizing a variety of automation techniques to convert manually executed Jupyter notebooks into automated ML workflows and pipelines.</p>
<p>In this chapter, we will build and run an automated MLOps pipeline using <strong class="bold">Kubeflow</strong> on top of <strong class="bold">Kubernetes</strong> and <strong class="bold">Amazon Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>). If you are wondering what these are, do not worry, as we will discuss these tools, platforms, and services in detail later! Once we have a better understanding of how they work, we will dive deeper into the recommended strategies and best practices when building more complex pipelines, along with securing and scaling our setup.</p>
<p>That said, in this chapter, we will cover the following topics:</p>
<ul>
<li>Diving deeper into Kubeflow, Kubernetes, and EKS</li>
<li>Preparing the essential prerequisites</li>
<li>Setting up Kubeflow on Amazon EKS</li>
<li>Running our first Kubeflow pipeline</li>
<li>Using the Kubeflow Pipelines SDK to build ML workflows</li>
<li>Cleaning up</li>
<li>Recommended strategies and best practices</li>
</ul>
<p>Once we reach the end of this chapter, we should have more confidence in building complex ML pipelines using the tools, platforms, and services we have learned about in this chapter.</p>
<h1 id="_idParaDest-203"><a id="_idTextAnchor218"/>Technical requirements</h1>
<p>Before we start, it is important that we have the following ready:</p>
<ul>
<li>A web browser (preferably Chrome or Firefox)</li>
<li>Access to the Cloud9 environment that was prepared in the <em class="italic">Creating your Cloud9 environment</em> and <em class="italic">Increasing the Cloud9 storage</em> sections of <a href="B18638_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>,<em class="italic"> Introduction to ML Engineering on AWS</em></li>
</ul>
<p>The Jupyter notebooks, source code, and other files used for each chapter are available at this repository: <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS</a>.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">It is recommended that you use an IAM user with limited permissions instead of the root account when running the examples in this book. If you are just starting out with using AWS, you can proceed with using the root account in the meantime.</p>
<h1 id="_idParaDest-204"><a id="_idTextAnchor219"/>Diving deeper into Kubeflow, Kubernetes, and EKS</h1>
<p>In <a href="B18638_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>, <em class="italic">Deep Learning Containers</em>, we learned that containers help guarantee the consistency of environments where applications can run. In the hands-on solutions of the said chapter, we worked with two containers—one container for training our deep learning model and another one for deploying the model. In larger applications, we will most likely encounter the usage of multiple containers running a variety of applications, databases, and automated scripts. Managing these containers is not easy and creating custom scripts to manage the uptime and scaling of the running containers is an overhead we wish to avoid. That said, it is recommended that you use a tool that helps you focus on what you need to accomplish. One of the available tools that can help us deploy, scale, and manage containerized applications is <strong class="bold">Kubernetes</strong>. This is an open source container<a id="_idIndexMarker1290"/> orchestration system that provides a framework for running resilient distributed systems. It automatically takes care of the scaling and failover work behind the scenes—this means that if your container stops working for some reason, Kubernetes will automatically replace it. <em class="italic">Cool, right?</em> Of course, this is only one of the cool features available. In addition to this, Kubernetes provides the following:</p>
<ul>
<li>Automated deployments and rollbacks</li>
<li>Secret (credentials) management</li>
<li>Managing and distributing network traffic to the containers</li>
<li>Storage orchestration</li>
<li>Making the most of servers (nodes) by fitting containers accordingly depending on the CPU and RAM requirements</li>
</ul>
<p>Note that this list is not exhaustive, and there are more features available when using Kubernetes. When using Kubernetes, it is essential that we have a good understanding of the terminology, concepts, and tools used. In <em class="italic">Figure 10.1</em>, we can see an example of a<a id="_idIndexMarker1291"/> Kubernetes cluster:</p>
<div>
<div class="IMG---Figure" id="_idContainer290">
<img alt="Figure 10.1 – An example Kubernetes cluster " height="561" src="image/B18638_10_001.jpg" width="989"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – An example Kubernetes cluster</p>
<p>Let’s quickly define and describe some of the concepts presented in <em class="italic">Figure 10.1</em>:</p>
<ul>
<li><strong class="bold">Node</strong>: This<a id="_idIndexMarker1292"/> maps to a virtual or physical machine (or an EC2 instance) that contains running containerized applications.</li>
<li><strong class="bold">Cluster</strong>: This is a <a id="_idIndexMarker1293"/>group of nodes (or servers).</li>
<li><strong class="bold">Pod</strong>: This<a id="_idIndexMarker1294"/> is a group of one or more application containers that represent a single unit of service (running inside a node).</li>
<li><strong class="bold">Control Plane</strong>: This <a id="_idIndexMarker1295"/>manages the worker nodes (servers) along with the Pods in a Kubernetes cluster.</li>
<li><strong class="bold">kubectl</strong>: This is<a id="_idIndexMarker1296"/> the command-line tool for running commands to manage Kubernetes resources.</li>
</ul>
<p>Note that this is a simplified list as we won’t dive deep into the other concepts and terminology in this chapter. Knowing them should be sufficient to help us go through the hands-on solutions of this chapter.</p>
<p>When running Kubernetes on AWS, it is recommended that you use a managed service such as <strong class="bold">Amazon EKS</strong>, which<a id="_idIndexMarker1297"/> helps manage a lot of things for us behind the scenes—including the availability and scalability of control plane nodes (which are the nodes focused on storing cluster data, ensuring application availability, and other important processes and tasks in the cluster). When using Amazon EKS, we no longer need to worry about the management of the control plane instances since AWS automatically scales these instances and replaces any unhealthy instances for us, too. In addition to these, Amazon EKS helps engineers work with other AWS services and resources (for example, <strong class="bold">AWS IAM</strong>, <strong class="bold">AWS Application Load Balancer</strong>, and <strong class="bold">Amazon CloudWatch</strong>) seamlessly when using Kubernetes.</p>
<p class="callout-heading">Note</p>
<p class="callout">It is possible to set up the autoscaling of nodes with Kubernetes and Amazon EKS. This is configured using <a id="_idIndexMarker1298"/>solutions such as <strong class="bold">Kubernetes Cluster Autoscaler</strong>. For more information, feel free to check out <a href="https://aws.github.io/aws-eks-best-practices/cluster-autoscaling/">https://aws.github.io/aws-eks-best-practices/cluster-autoscaling/</a>.</p>
<p>The primary tool when managing EKS clusters is <a id="_idIndexMarker1299"/>the <strong class="bold">eksctl</strong> command-line tool. With this tool, EKS clusters can easily be created, updated, and deleted with a single command. Once the clusters become available, we can proceed with using other tools such as <a id="_idIndexMarker1300"/>the <strong class="bold">kubectl</strong> command-line tool to create and manage Kubernetes resources inside the clusters.</p>
<p>Due to the power and <a id="_idIndexMarker1301"/>potential of Kubernetes, a lot of other tools have been built on top of it. These include <strong class="bold">Kubeflow</strong>—a popular open source ML platform focused on helping data scientists and ML engineers orchestrate and manage complex ML workflows on Kubernetes. Kubeflow brings together a collection of data science and ML tools that are already familiar to data scientists and ML engineers. These include the following:</p>
<ul>
<li><strong class="bold">JupyterHub</strong> – This is a <a id="_idIndexMarker1302"/>hub that helps spawn and manage multiple Jupyter notebooks (where data scientists can run code for ML experiments).</li>
<li><strong class="bold">Argo Workflows</strong> – This<a id="_idIndexMarker1303"/> is a workflow engine on which automated pipelines run.</li>
<li><strong class="bold">Knative Serving</strong> – This <a id="_idIndexMarker1304"/>enables rapid deployment of serverless containers (where ML models can run).</li>
<li><strong class="bold">Istio</strong> – This is a<a id="_idIndexMarker1305"/> service mesh that provides a way to easily manage network configuration and communication between the deployed microservices in the cluster.</li>
<li><strong class="bold">MinIO</strong> – This is <a id="_idIndexMarker1306"/>a multi-cloud object storage solution that is native to Kubernetes.</li>
</ul>
<p>With Kubeflow, ML practitioners can perform ML experiments and deployments without worrying about the infrastructure. At the same time, automated ML workflows and pipelines can <a id="_idIndexMarker1307"/>easily be deployed <a id="_idIndexMarker1308"/>and managed using a variety of tools available in Kubeflow (such as <strong class="bold">Kubeflow Pipelines</strong> and the <strong class="bold">Kubeflow Pipelines SDK</strong>). When properly built, these pipelines can help data scientists and ML engineers save a significant amount of time through the automation of different steps of the ML process. At the same time, these pipelines can enable automated model retraining that will help ensure deployed models are updated using the latest training data available.</p>
<p>Now that we have a better idea of the tools we are going to use, we will proceed with preparing the essential prerequisites for running ML pipelines using Kubeflow on Amazon EKS!</p>
<h1 id="_idParaDest-205"><a id="_idTextAnchor220"/>Preparing the essential prerequisites</h1>
<p>In this<a id="_idIndexMarker1309"/> section, we will work on the following:</p>
<ul>
<li>Preparing the IAM role for the EC2 instance of the Cloud9 environment</li>
<li>Attaching the IAM role to the EC2 instance of the Cloud9 environment</li>
<li>Updating the Cloud9 environment with the essential prerequisites</li>
</ul>
<p>Let’s work on and prepare the essential prerequisites one by one. </p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor221"/>Preparing the IAM role for the EC2 instance of the Cloud9 environment</h2>
<p>In order for us to securely create and <a id="_idIndexMarker1310"/>manage <strong class="bold">Amazon EKS</strong> and <strong class="bold">AWS CloudFormation</strong> resources <a id="_idIndexMarker1311"/>from inside the EC2 instance of the Cloud9 environment, we would need to attach an IAM role to the EC2 instance. In this section, we will prepare this IAM role and configure it with the permissions required to create and manage the other resources in this chapter.</p>
<p class="callout-heading">Note</p>
<p class="callout">We will discuss <strong class="bold">Amazon EKS</strong> and <strong class="bold">AWS CloudFormation</strong> in more detail in the <em class="italic">Setting up Kubeflow on Amazon EKS</em> section of this chapter.</p>
<p>In the next set of steps, we <a id="_idIndexMarker1312"/>will navigate to the IAM console and create an IAM role that will be attached to the EC2 instance (of the Cloud9 environment) later in this chapter:</p>
<ol>
<li>Navigate to the IAM console by typing <strong class="source-inline">iam</strong> into the search bar and then clicking on <strong class="bold">IAM</strong> from the list of results, as highlighted in <em class="italic">Figure 10.2</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer291">
<img alt="Figure 10.2 – Navigating to the IAM console " height="502" src="image/B18638_10_002.jpg" width="890"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Navigating to the IAM console</p>
<p class="list-inset">In <em class="italic">Figure 10.2</em>, we<a id="_idIndexMarker1313"/> have one of the ways to navigate to the IAM console. Another option would be to click on the <strong class="bold">Services</strong> drop-down menu (not shown in the preceding screenshot) and locate the <strong class="bold">IAM</strong> service under the <strong class="bold">Security, Identity, and Compliance</strong> group of services.</p>
<ol>
<li value="2">In the left-hand sidebar, locate and click on <strong class="bold">Roles</strong> (under <strong class="bold">Access management</strong>).</li>
<li>In the upper-right corner of the page, locate and click on the <strong class="bold">Create role</strong> button.</li>
<li>In the <strong class="bold">Select trusted entity</strong> page (which is step 1 of 3), select <strong class="bold">AWS service</strong> under <strong class="bold">Trusted entity type</strong>, as highlighted in <em class="italic">Figure 10.3</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer292">
<img alt="Figure 10.3 – The Select trusted entity page " height="602" src="image/B18638_10_003.jpg" width="816"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – The Select trusted entity page</p>
<p class="list-inset">Here, we also<a id="_idIndexMarker1314"/> make sure that the <strong class="bold">EC2</strong> option is selected under <strong class="bold">Use case &gt; Common use cases</strong>. Once we have reviewed the selected options, we can click on the <strong class="bold">Next</strong> button afterward.</p>
<ol>
<li value="5">In the <strong class="bold">Add permissions</strong> page (which is step 2 of 3), type <strong class="source-inline">administrator</strong> into the filter search box (as highlighted in <em class="italic">Figure 10.4</em>), and then press the <em class="italic">Enter</em> key to filter the list of results. Toggle on the checkbox corresponding to the <strong class="bold">AdministratorAccess</strong> policy, scroll down to the bottom of the page, and then click on the <strong class="bold">Next</strong> button:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer293">
<img alt="Figure 10.4 – The Add permissions page " height="590" src="image/B18638_10_004.jpg" width="939"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – The Add permissions page</p>
<p class="list-inset">Make sure that <a id="_idIndexMarker1315"/>you do not accidentally select the incorrect permission from the list of filtered results since there are permissions with similar names available. The <strong class="bold">AdministratorAccess</strong> policy should have the <strong class="bold">Description</strong> value of <strong class="bold">Provides full access to AWS services and resources</strong>.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">In this chapter, the usage of the <strong class="source-inline">AdministratorAccess</strong> policy will help us avoid different permission-related issues while we are setting things up. When setting this up in your work environment, you should use a custom policy that only adds the permission the EC2 instance needs to run the application (and nothing more).</p>
<ol>
<li value="6">In the <strong class="bold">Name, review, and create</strong> page (which is step 3 of 3), specify <strong class="source-inline">kubeflow-on-eks</strong> in the <strong class="bold">Role name</strong> input box. Scroll down to the bottom of the page and then click on the <strong class="bold">Create role</strong> button.</li>
</ol>
<p>Wasn’t that easy! At this point, we should have an IAM role we can attach to AWS resources such as EC2 instances.</p>
<h2 id="_idParaDest-207"><a id="_idTextAnchor222"/>Attaching the IAM role to the EC2 instance of the Cloud9 environment</h2>
<p>Now that we have <a id="_idIndexMarker1316"/>the IAM role ready, we can now proceed with attaching this IAM role to the EC2 instance.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">In this chapter, we will create and manage our resources in the <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>) region. Make sure that you have set the correct region before proceeding with the next steps.</p>
<p>In the next set of steps, we will use the AWS Management Console to attach the IAM role to the EC2 instance where our Cloud9 environment is running:</p>
<ol>
<li value="1">Navigate to the Cloud9 console by typing <strong class="source-inline">cloud9</strong> in the search bar and then selecting <strong class="bold">Cloud9</strong> from the list of results:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer294">
<img alt="Figure 10.5 – Navigating to the Cloud9 console " height="511" src="image/B18638_10_005.jpg" width="828"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Navigating to the Cloud9 console</p>
<p class="list-inset">In <em class="italic">Figure 10.5</em>, we have one of the ways to navigate to the Cloud9 service page. Another option would be to click on the <strong class="bold">Services</strong> drop-down menu (not shown in the preceding screenshot) and locate the <strong class="bold">Cloud9</strong> service in the <strong class="bold">Developer Tools</strong> group of services.</p>
<ol>
<li value="2">Locate and select the Cloud9 environment that we prepared in <a href="B18638_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>,<em class="italic"> Introduction to ML Engineering on AWS</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer295">
<img alt="Figure 10.6 – Locating the View details button " height="640" src="image/B18638_10_006.jpg" width="907"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Locating the View details button</p>
<p class="list-inset">Once you<a id="_idIndexMarker1317"/> have selected the Cloud9 environment, click on the <strong class="bold">View details</strong> button located in the upper-right portion of the page (as highlighted in <em class="italic">Figure 10.6</em>).</p>
<p class="callout-heading">Note</p>
<p class="callout">You might also decide to create a new Cloud9 environment from scratch and increase the size of the volume attached to the EC2 instance where the environment is running. If that’s the case, make sure to follow the step-by-step instructions specified in the <em class="italic">Creating your Cloud9 environment</em> and <em class="italic">Increasing the Cloud9 storage</em> sections of <a href="B18638_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to ML Engineering on AWS</em>.</p>
<ol>
<li value="3">Under <strong class="bold">Environment details</strong>, locate and click on the <strong class="bold">Go To Instance</strong> link, as highlighted in <em class="italic">Figure 10.7</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer296">
<img alt="Figure 10.7 – Locating and clicking on the Go To Instance button " height="395" src="image/B18638_10_007.jpg" width="832"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Locating and clicking on the Go To Instance button</p>
<p class="list-inset">This should redirect you to the EC2 console where you should see the specific EC2 instance where the Cloud9 environment is running.</p>
<ol>
<li value="4">Toggle on the<a id="_idIndexMarker1318"/> checkbox corresponding to the EC2 instance (starting with <strong class="source-inline">aws-cloud9</strong>), and then open the <strong class="bold">Actions</strong> drop-down menu, as highlighted in <em class="italic">Figure 10.8</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer297">
<img alt="Figure 10.8 – Modifying the IAM role of the EC2 instance  " height="459" src="image/B18638_10_008.jpg" width="1072"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Modifying the IAM role of the EC2 instance </p>
<ol>
<li value="5">Next, we locate and click on the <strong class="bold">Modify IAM role</strong> option under the list of options under <strong class="bold">Security</strong>. This should redirect you to a page where you can select the specific IAM role to attach to the selected EC2 instance.</li>
<li>In the IAM role drop-down menu (as highlighted in <em class="italic">Figure 10.9</em>), locate and select the IAM role we created earlier in this chapter (that is, the <strong class="source-inline">kubeflow-on-eks</strong> IAM role):</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer298">
<img alt="Figure 10.9 – Specifying kubeflow-on-eks as the IAM role " height="506" src="image/B18638_10_009.jpg" width="1047"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Specifying kubeflow-on-eks as the IAM role</p>
<p class="list-inset">Once we have <a id="_idIndexMarker1319"/>updated the IAM role drop-down value to <strong class="source-inline">kubeflow-on-eks</strong>, you can now click on the <strong class="bold">Update IAM role</strong> button (as highlighted in <em class="italic">Figure 10.9</em>).</p>
<ol>
<li value="7">Navigate back to the Cloud9 console by typing <strong class="source-inline">cloud9</strong> in the search bar and then selecting <strong class="bold">Cloud9</strong> from the list of results.</li>
<li>Locate and click on the <strong class="bold">Open IDE</strong> button associated with our Cloud9 environment. This should open a Cloud9 environment that is similar to what is shown in <em class="italic">Figure 10.10</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer299">
<img alt="Figure 10.10 – The Cloud9 environment " height="511" src="image/B18638_10_010.jpg" width="887"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – The Cloud9 environment</p>
<p class="list-inset">Here, we <a id="_idIndexMarker1320"/>should see a familiar screen (as we have used this already in <a href="B18638_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to ML Engineering on AWS</em>, and <a href="B18638_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>, <em class="italic">Deep Learning Containers</em>).</p>
<p class="list-inset">In the Terminal of the Cloud9 environment (after the $ sign in the lower part of the screen), run the following command to disable the managed temporary credentials inside the environment:</p>
<pre class="list-inset1 source-code"><strong class="bold">ENV_ID=$C9_PID</strong>
<strong class="bold">aws cloud9 update-environment --managed-credentials-action DISABLE --environment-id $ENV_ID</strong></pre>
<ol>
<li value="9">Also, let’s remove the credentials file inside the <strong class="source-inline">.aws</strong> directory to ensure that temporary credentials are not in place, too:<pre class="source-code"><strong class="bold">rm -vf /home/ubuntu/.aws/credentials</strong></pre></li>
<li>Finally, let’s verify that the Cloud9 environment is using the IAM role we prepared in this chapter (that is, the <strong class="source-inline">kubeflow-on-eks</strong> IAM role):<pre class="source-code"><strong class="bold">aws sts get-caller-identity</strong> --query Arn </pre></li>
</ol>
<p class="list-inset">This should yield a result similar to the following:</p>
<pre class="list-inset1 source-code">arn:aws:sts::1234567890:assumed-role/<strong class="bold">kubeflow-on-eks</strong>/i-abcdefgh12345</pre>
<p>Once we have <a id="_idIndexMarker1321"/>verified that we are using the correct IAM role inside the Cloud9 environment, we can proceed with the next section.</p>
<p class="callout-heading">Note</p>
<p class="callout"><em class="italic">What happened here?</em> IAM roles (attached to the AWS resources) generate and provide credentials inside environments that expire every few hours. For us to be able to work with IAM roles, we need to remove any existing set of credentials (inside the Cloud9 environment) so that the environment will use the IAM role credentials instead. For more information on this topic, feel free to check out <a href="https://docs.aws.amazon.com/cloud9/latest/user-guide/security-iam.xhtml">https://docs.aws.amazon.com/cloud9/latest/user-guide/security-iam.xhtml</a>.</p>
<h2 id="_idParaDest-208"><a id="_idTextAnchor223"/>Updating the Cloud9 environment with the essential prerequisites</h2>
<p>Before we can create our <a id="_idIndexMarker1322"/>EKS cluster and set up Kubeflow on top of it, we would need to download and<a id="_idIndexMarker1323"/> install a few prerequisites <a id="_idIndexMarker1324"/>including <a id="_idIndexMarker1325"/>several command-line tools, such as <strong class="bold">kubectl</strong>, <strong class="bold">eksctl</strong>, and <strong class="bold">kustomize</strong>.</p>
<p class="callout-heading">Note</p>
<p class="callout">We will discuss how these work in the <em class="italic">Setting up Kubeflow on Amazon EKS</em> section of this chapter.</p>
<p>In the next set of steps, we will run a couple of scripts that <a id="_idIndexMarker1326"/>will install the prerequisites needed to<a id="_idIndexMarker1327"/> get <strong class="bold">Kubernetes</strong> and <strong class="bold">Kubeflow</strong> running in our environment:</p>
<ol>
<li value="1">Let’s begin by using the <strong class="source-inline">wget</strong> command (in the Terminal of the Cloud9 environment) to download the <strong class="source-inline">prerequisites.zip</strong> file containing a variety of installation scripts. After that, we will use the <strong class="source-inline">unzip</strong> command to extract the contents of the ZIP file we just downloaded:<pre class="source-code"><strong class="bold">wget</strong> -O <strong class="bold">prerequisites.zip</strong> https://bit.ly/3ByyDGV</pre><pre class="source-code"><strong class="bold">unzip</strong> prerequisites.zip</pre></li>
</ol>
<p class="list-inset">This should extract the following files from the ZIP file: </p>
<ul>
<li><strong class="source-inline">00_install_kubectl_aws_jq_and_more.sh</strong> – This is a script that runs all the other scripts (with the prefixes of <strong class="source-inline">01</strong> to <strong class="source-inline">07</strong>) to install the prerequisites.</li>
<li><strong class="source-inline">01_install_kubectl.sh</strong> – This is a script that installs the kubectl command-line tool.</li>
<li><strong class="source-inline">02_install_aws_cli_v2.sh</strong> – This is a<a id="_idIndexMarker1328"/> script that installs v2 of the <strong class="bold">AWS CLI</strong>.</li>
<li><strong class="source-inline">03_install_jq_and_more.sh</strong> – This is a script that installs and sets up a<a id="_idIndexMarker1329"/> few prerequisites, such as <em class="italic">jq</em> and <em class="italic">yq</em>.</li>
<li><strong class="source-inline">04_check_prerequisites.sh</strong> – This is a script that checks whether the first few prerequisites have been installed successfully.</li>
<li><strong class="source-inline">05_additional_setup_instructions.sh</strong> – This is a script that sets up the Bash completion.</li>
<li><strong class="source-inline">06_download_eksctl.sh</strong> – This is a script that<a id="_idIndexMarker1330"/> installs the <strong class="bold">eksctl</strong> command-line tool.</li>
<li><strong class="source-inline">07_install_kustomize.sh</strong> – This is a <a id="_idIndexMarker1331"/>script that installs version 3.2.3 of <strong class="bold">kustomize</strong>.</li>
</ul>
<ol>
<li value="2">Navigate to the <strong class="source-inline">ch10_prerequisites</strong> folder and run the <strong class="source-inline">chmod</strong> command to make the scripts inside the folder executable:<pre class="source-code">cd ch10_prerequisites</pre><pre class="source-code"><strong class="bold">chmod +x *.sh</strong></pre></li>
<li>Now, run the following command to start the installation and setup processes:<pre class="source-code"><strong class="bold">sudo ./00_install_kubectl_aws_jq_and_more.sh</strong></pre></li>
</ol>
<p class="list-inset">This should run the other scripts inside the <strong class="source-inline">ch10_prerequisites</strong> folder starting from <strong class="source-inline">01_install_kubectl.sh</strong> to <strong class="source-inline">07_install_kustomize.sh</strong>.</p>
<p class="callout-heading">Note</p>
<p class="callout">Once the <strong class="source-inline">00_install_kubectl_aws_jq_and_more.sh</strong> script has finished running, several <a id="_idIndexMarker1332"/>prerequisites, such<a id="_idIndexMarker1333"/> as <strong class="bold">AWS CLI v2</strong>, <strong class="bold">eksctl</strong>, and <strong class="bold">kustomize</strong>, should <a id="_idIndexMarker1334"/>already be available for us to use to prepare Kubernetes clusters (if there are no errors during installation). Make sure that you review the logs generated by the script before proceeding.</p>
<ol>
<li value="4">Verify the <a id="_idIndexMarker1335"/>version of the AWS CLI we currently have:<pre class="source-code"><strong class="bold">aws --version</strong></pre></li>
</ol>
<p class="list-inset">This should yield a result similar to the following:</p>
<pre class="list-inset1 source-code">aws-cli/<strong class="bold">2.7.20</strong> Python/3.9.11 Linux/5.4.0-1081-aws exe/x86_64.ubuntu.18 prompt/off</pre>
<ol>
<li value="5">Next, let’s verify the version of <strong class="source-inline">kustomize</strong> that we will be using:<pre class="source-code"><strong class="bold">kustomize version</strong></pre></li>
</ol>
<p class="list-inset">This should yield a result similar to the following:</p>
<pre class="list-inset1 source-code">Version: {Version:kustomize/<strong class="bold">v3.2.3</strong> GitCommit:f8412aa3d39f32151525aff97a351288f5a7470b BuildDate:2019-10-08T23:30:25Z GoOs:linux GoArch:amd64}</pre>
<ol>
<li value="6">Let’s verify the version of <strong class="source-inline">eksctl</strong>, too:<pre class="source-code"><strong class="bold">eksctl version</strong></pre></li>
</ol>
<p class="list-inset">This should yield a result similar to the following:</p>
<pre class="list-inset1 source-code"><strong class="bold">0.109.0</strong></pre>
<ol>
<li value="7">Run the following so that the other changes (such as the environment variable values) from the installation scripts reflect in our current shell:<pre class="source-code"><strong class="bold">. ~/.bash_completion</strong></pre><pre class="source-code"><strong class="bold">. ~/.bash_profile</strong></pre><pre class="source-code"><strong class="bold">. ~/.bashrc</strong></pre></li>
</ol>
<p class="list-inset">Note the presence of a dot (<strong class="source-inline">.</strong>) and a space before the tilde symbol (<strong class="source-inline">~</strong>) at the start of each line.</p>
<ol>
<li value="8">Run the<a id="_idIndexMarker1336"/> following block of commands to set a few environment variables and configure the default region when using the AWS CLI:<pre class="source-code">export <strong class="bold">AWS_REGION</strong>="us-west-2"</pre><pre class="source-code">echo "export AWS_REGION=${<strong class="bold">AWS_REGION</strong>}" | tee -a ~/.bash_profile</pre><pre class="source-code">aws configure set <strong class="bold">default.region</strong> ${AWS_REGION}</pre></li>
<li>Finally, verify that the default region has been set successfully:<pre class="source-code">aws configure get <strong class="bold">default.region</strong></pre></li>
</ol>
<p class="list-inset">This should yield a value of <strong class="source-inline">us-west-2</strong> (if we are running our Cloud9 environment in Oregon).</p>
<p>Now that all prerequisites have been installed, set up, and verified, we can proceed with creating an EKS cluster and setting up Kubeflow on top of it!</p>
<h1 id="_idParaDest-209"><a id="_idTextAnchor224"/>Setting up Kubeflow on Amazon EKS</h1>
<p>With all <a id="_idIndexMarker1337"/>of the prerequisites ready, we can now proceed with <a id="_idIndexMarker1338"/>creating our EKS cluster and then installing Kubeflow on top of it. During the installation and setup process, we will use the following tools:</p>
<ul>
<li><strong class="bold">eksctl</strong> – The <a id="_idIndexMarker1339"/>CLI tool for creating and managing Amazon EKS clusters</li>
<li><strong class="bold">kubectl</strong> – The<a id="_idIndexMarker1340"/> CLI tool for creating, configuring, and deleting Kubernetes resources</li>
<li><strong class="bold">AWS CLI</strong> – The <a id="_idIndexMarker1341"/>CLI tool for creating, configuring, and deleting AWS resources</li>
<li><strong class="bold">kustomize</strong> – The<a id="_idIndexMarker1342"/> CLI tool for managing the configuration of Kubernetes objects</li>
</ul>
<p>The hands-on portion of this section involves following a high-level set of steps:</p>
<ol>
<li value="1">Preparing the <strong class="source-inline">eks.yaml</strong> file containing the EKS configuration (such as the number of nodes, desired capacity, and instance type)</li>
<li>Running the <strong class="source-inline">eks create cluster</strong> command using the <strong class="source-inline">eks.yaml</strong> file to create the Amazon EKS cluster</li>
<li>Using <strong class="bold">kustomize</strong> and <strong class="bold">kubectl</strong> to install Kubeflow inside our cluster</li>
</ol>
<p>With these in mind, we can now proceed with setting up our EKS cluster and Kubeflow:</p>
<ol>
<li value="1">Continuing where we left off in the previous section, let’s run the following commands in the Terminal of the Cloud9 environment:<pre class="source-code">cd <strong class="bold">~/environment</strong></pre><pre class="source-code">mkdir <strong class="bold">ch10</strong></pre><pre class="source-code">cd <strong class="bold">ch10</strong></pre></li>
</ol>
<p class="list-inset">Here, we create the <strong class="source-inline">ch10</strong> directory using the <strong class="source-inline">mkdir</strong> command. After that, we will navigate to the directory using the <strong class="source-inline">cd</strong> command.</p>
<ol>
<li value="2">Next, let’s use the <strong class="source-inline">touch</strong> command to create an empty <strong class="source-inline">eks.yaml</strong> file:<pre class="source-code">touch <strong class="bold">eks.yaml</strong></pre></li>
<li>In<a id="_idIndexMarker1343"/> the <strong class="bold">File Tree</strong>, locate the environment directory with the name of your Cloud9 environment. Right-click on this directory to open a context menu similar to what is shown in <em class="italic">Figure 10.11</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer300">
<img alt="Figure 10.11 – Refreshing the displayed directories and files " height="290" src="image/B18638_10_011.jpg" width="380"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – Refreshing the displayed directories and files</p>
<p class="list-inset">Select <strong class="bold">Refresh</strong> from<a id="_idIndexMarker1344"/> the list of options to <a id="_idIndexMarker1345"/>ensure that the latest changes have been reflected in the file tree.</p>
<ol>
<li value="4">Next, double-click on the <strong class="source-inline">eks.yaml</strong> file (inside the <strong class="source-inline">ch10</strong> directory) in the file tree to open the file in the <strong class="bold">Editor</strong> pane. Inside this blank file, specify the following YAML configuration:<pre class="source-code">---</pre><pre class="source-code">apiVersion: eksctl.io/v1alpha5</pre><pre class="source-code">kind: ClusterConfig</pre><pre class="source-code">metadata:</pre><pre class="source-code">  name: <strong class="bold">kubeflow-eks-000</strong></pre><pre class="source-code">  region: <strong class="bold">us-west-2</strong></pre><pre class="source-code">  version: "1.21"</pre><pre class="source-code">availabilityZones: [<strong class="bold">"us-west-2a"</strong>, <strong class="bold">"us-west-2b"</strong>, <strong class="bold">"us-west-2c"</strong>, <strong class="bold">"us-west-2d"</strong>]</pre><pre class="source-code">managedNodeGroups:</pre><pre class="source-code">- name: nodegroup</pre><pre class="source-code">  desiredCapacity: <strong class="bold">5</strong></pre><pre class="source-code">  instanceType: <strong class="bold">m5.xlarge</strong></pre><pre class="source-code">  ssh:</pre><pre class="source-code">    enableSsm: true</pre></li>
</ol>
<p class="list-inset">Make sure to <a id="_idIndexMarker1346"/>save your changes by pressing the <em class="italic">Ctrl</em> + <em class="italic">S</em> keys (or, alternatively, <em class="italic">Cmd</em> + <em class="italic">S</em> when using a Mac device). Additionally, you can use the <strong class="bold">Save</strong> option in the <strong class="bold">File</strong> menu to save your changes.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Before proceeding, it is crucial that we are aware of the resources that will be created when we run the <strong class="source-inline">eksctl create cluster</strong> command using this configuration file. Here, we specify that we want our cluster (named <strong class="source-inline">kubeflow-eks-000</strong>) to have five (<strong class="source-inline">5</strong>) <strong class="source-inline">m5.xlarge</strong> instances. Once you run the <strong class="source-inline">eksctl create cluster</strong> command in the next step, make sure that you delete the cluster within an hour or two after cluster creation to manage costs. Feel free to jump to the <em class="italic">Cleaning up</em> section at the end of this chapter once you need to delete the cluster.</p>
<ol>
<li value="5">Before <a id="_idIndexMarker1347"/>creating real resources for our cluster, let’s use the <strong class="source-inline">eksctl create cluster</strong> command with the <strong class="source-inline">--dry-run</strong> option:<pre class="source-code"><strong class="bold">eksctl create cluster -f eks.yaml --dry-run</strong></pre></li>
</ol>
<p class="list-inset">This should help us inspect the configuration before we create the actual set of resources.</p>
<ol>
<li value="6">Now, let’s create our cluster using the <strong class="source-inline">eksctl create</strong> command:<pre class="source-code"><strong class="bold">eksctl create cluster -f eks.yaml</strong></pre></li>
</ol>
<p class="list-inset">Here, we use the <strong class="source-inline">eks.yaml</strong> file we prepared in the previous step as the configuration file when running the command. </p>
<p class="callout-heading">Important Note</p>
<p class="callout">If you encounter an error with a message similar to <strong class="bold">Error: invalid version, 1.2X is no longer supported, supported values: 1.2X, 1.2X, 1.2X, 1.2X</strong>, feel free to update the <strong class="source-inline">version</strong> string value in the <strong class="source-inline">eks.yaml</strong> file with the lowest supported version specified in the error message. Once you have updated the <strong class="source-inline">eks.yaml</strong> file, you can run the <strong class="source-inline">eksctl create cluster</strong> command again and check whether the issue has been resolved. For more information on this topic, feel free to check out <a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.xhtml">https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.xhtml</a>.</p>
<p class="list-inset">Running the <strong class="source-inline">eksctl create cluster</strong> command should take about 15–30 minutes to<a id="_idIndexMarker1348"/> complete. It will use <strong class="bold">CloudFormation</strong> stacks <a id="_idIndexMarker1349"/>for launching the <a id="_idIndexMarker1350"/>AWS resources. If you are wondering what CloudFormation is, it is a service that lets you define each of your infrastructure’s components and their settings in a template. This template is then read by CloudFormation to provision the resources required by your infrastructure:</p>
<div>
<div class="IMG---Figure" id="_idContainer301">
<img alt="Figure 10.12 – How EKS resources are created using eksctl " height="531" src="image/B18638_10_012.jpg" width="1053"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – How EKS resources are created using eksctl</p>
<p class="list-inset">In <em class="italic">Figure 10.12</em>, we can see that the <strong class="source-inline">eksctl</strong> command makes use of the <strong class="source-inline">eks.yaml</strong> file to prepare the templates that will be used by the CloudFormation <a id="_idIndexMarker1351"/>service to provision resources.</p>
<p class="callout-heading">Note</p>
<p class="callout">Note that <strong class="source-inline">eksctl</strong> creates other resources outside of CloudFormation, too. This means that the CloudFormation templates used to prepare the EKS resources will <em class="italic">not</em> contain all resources created using the <strong class="source-inline">eksctl</strong> command. That said, it is best to use the <strong class="source-inline">eksctl delete cluster</strong> command when deleting the resources created in this section. Once you need to delete the resources, make sure that you follow the instructions specified in the <em class="italic">Cleaning up</em> section of this chapter.</p>
<ol>
<li value="7">Let’s quickly<a id="_idIndexMarker1352"/> inspect our setup using the <strong class="source-inline">kubectl get nodes</strong> command:<pre class="source-code"><strong class="bold">kubectl get nodes -o wide</strong></pre></li>
</ol>
<p class="list-inset">This should give us five nodes with the <strong class="bold">STATUS</strong> value of <strong class="bold">Ready</strong>. </p>
<p class="callout-heading">Important Note</p>
<p class="callout">If you encounter issues when deploying EKS clusters, make sure that you check out <a href="https://docs.aws.amazon.com/eks/latest/userguide/troubleshooting.xhtml">https://docs.aws.amazon.com/eks/latest/userguide/troubleshooting.xhtml</a>.</p>
<ol>
<li value="8">Before proceeding, let’s make sure that <strong class="source-inline">CLUSTER_NAME</strong> and <strong class="source-inline">CLUSTER_REGION</strong> have been set with the appropriate values:<pre class="source-code">CLUSTER_NAME=<strong class="bold">kubeflow-eks-000</strong></pre><pre class="source-code">CLUSTER_REGION=<strong class="bold">us-west-2</strong></pre></li>
</ol>
<p class="list-inset">Here, we specify a <strong class="source-inline">CLUSTER_NAME</strong> value equivalent to the name specified in the <strong class="source-inline">eks.yaml</strong> file. Note that if you need to experiment with another set of configuration parameters, you can specify a different cluster name (by updating both <strong class="source-inline">CLUSTER_NAME</strong> and the <strong class="source-inline">eks.yaml</strong> file) and replace <strong class="source-inline">kubeflow-eks-000</strong> with <strong class="source-inline">kubeflow-eks-001</strong> (and so on) when creating new clusters. Just make sure that you properly delete any existing clusters before creating a new one.</p>
<ol>
<li value="9">Additionally, let’s associate an IAM OIDC provider with the cluster:<pre class="source-code"><strong class="bold">eksctl utils associate-iam-oidc-provider --cluster $CLUSTER_NAME --approve -v4</strong></pre></li>
</ol>
<p class="list-inset">So, what’s an IAM OIDC provider? Well, it’s an IAM entity used to establish trust between your<a id="_idIndexMarker1353"/> AWS account and an external OpenID Connect-compatible<a id="_idIndexMarker1354"/> identity provider. This means that instead of creating IAM users, we can use IAM OIDC providers instead and give these identities permissions to work with the resources in our AWS account.</p>
<p class="callout-heading">Note</p>
<p class="callout">For more information about this topic, feel free to check out <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.xhtml">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.xhtml</a>.</p>
<ol>
<li value="10">Let’s use the <strong class="source-inline">aws eks update-kubeconfig</strong> command to configure <strong class="source-inline">kubectl</strong> so that we can connect to the Amazon EKS cluster:<pre class="source-code"><strong class="bold">aws eks update-kubeconfig --name $CLUSTER_NAME --region ${AWS_REGION}</strong></pre></li>
<li>Next, we will clone two repositories that include the manifests (files containing the specifications of the Kubernetes objects) for installing what we need:<pre class="source-code">export <strong class="bold">KUBEFLOW_VERSION=v1.5.1</strong></pre><pre class="source-code">export <strong class="bold">AWS_VERSION=v1.5.1-aws-b1.0.0</strong></pre><pre class="source-code">git clone <strong class="bold">https://github.com/awslabs/kubeflow-manifests.git</strong> &amp;&amp; cd <strong class="bold">kubeflow-manifests</strong></pre><pre class="source-code">git checkout ${AWS_VERSION}</pre><pre class="source-code">git clone --branch ${KUBEFLOW_VERSION} \</pre><pre class="source-code"><strong class="bold">https://github.com/kubeflow/manifests.git</strong> upstream</pre></li>
<li>Navigate<a id="_idIndexMarker1355"/> to the <strong class="source-inline">deployments/vanilla</strong> directory:<pre class="source-code"><strong class="bold">cd deployments/vanilla</strong></pre></li>
</ol>
<p class="list-inset">We should find a <strong class="source-inline">kustomization.yaml</strong> file inside this directory. For more information on this topic, feel free to check out <a href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/">https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/</a>.</p>
<ol>
<li value="13">With<a id="_idIndexMarker1356"/> everything ready, let’s run this single-line command to install the Kubeflow components and services:<pre class="source-code"><strong class="bold">while ! kustomize build . | kubectl apply -f -; do echo "Retrying"; sleep 30; done</strong></pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This step should take about 4–10 minutes to complete. If the output logs seem to be looping indefinitely for more than 20–30 minutes already, you might need to experiment with different values in the <strong class="source-inline">version</strong> string value in the <strong class="source-inline">eks.yaml</strong> file. <em class="italic">What values can we use?</em> Let’s say that the currently supported versions are <strong class="source-inline">1.20</strong>, <strong class="source-inline">1.21</strong>, <strong class="source-inline">1.22</strong>, and <strong class="source-inline">1.23</strong> (as indicated in <a href="https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.xhtml">https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.xhtml</a>). <em class="italic">Should we try using version 1.23?</em> If we were to use the latest supported Kubernetes version <strong class="source-inline">1.23</strong> in the <strong class="source-inline">eks.yaml</strong> file, there’s a chance that we might encounter issues installing Kubeflow. We might need to wait for a few months for the Kubeflow support to catch up (as indicated in <a href="https://awslabs.github.io/kubeflow-manifests/docs/about/eks-compatibility/">https://awslabs.github.io/kubeflow-manifests/docs/about/eks-compatibility/</a>). That said, we can try specifying <strong class="source-inline">1.20</strong>, <strong class="source-inline">1.21</strong>, or <strong class="source-inline">1.22</strong> in the <strong class="source-inline">eks.yaml</strong> file when using the <strong class="source-inline">eksctl create cluster</strong> command (starting from the lowest supported version of <strong class="source-inline">1.20</strong> first). With these in mind, the next step is to delete the cluster using the <strong class="source-inline">eksctl delete cluster</strong> command (please see the <em class="italic">Cleaning up</em> section), update the <strong class="source-inline">eks.yaml</strong> file with the desired Kubernetes version, and then repeat the steps starting from the <strong class="source-inline">eksctl create cluster</strong> command in this section.</p>
<ol>
<li value="14">Let’s quickly<a id="_idIndexMarker1357"/> inspect the created resources using the following commands:<pre class="source-code">ns_array=(<strong class="bold">kubeflow</strong> <strong class="bold">kubeflow-user-example-com</strong> <strong class="bold">kserve</strong> <strong class="bold">cert-manager</strong> <strong class="bold">istio-system</strong> <strong class="bold">auth</strong> <strong class="bold">knative-eventing</strong> <strong class="bold">knative-serving</strong>)</pre><pre class="source-code">for i in ${ns_array[@]}; do </pre><pre class="source-code">  echo "[+] kubectl get pods -n $i"</pre><pre class="source-code">  <strong class="bold">kubectl get pods -n $i</strong>; </pre><pre class="source-code">  echo "---"</pre><pre class="source-code">done</pre></li>
</ol>
<p class="list-inset">Here, we <a id="_idIndexMarker1358"/>use the <strong class="source-inline">kubectl get pods</strong> command to inspect the resources created inside the nodes of the cluster.</p>
<ol>
<li value="15">Now, we run the following command so that we can access the Kubeflow dashboard via port <strong class="source-inline">8080</strong> of the Cloud9 environment:<pre class="source-code"><strong class="bold">kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80 --address=localhost</strong></pre></li>
<li>Click on <strong class="bold">Preview</strong> (which is located at the top of the page) to open a list of drop-down menu options similar to what is shown in <em class="italic">Figure 10.13</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer302">
<img alt="Figure 10.13 – Preview Running Application " height="186" src="image/B18638_10_013.jpg" width="267"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.13 – Preview Running Application</p>
<p class="list-inset">From the list of drop-down menu options, select <strong class="bold">Preview Running Application</strong> to open <a id="_idIndexMarker1359"/>a small window just beside the Terminal pane at the bottom of the screen.</p>
<p class="callout-heading">Note</p>
<p class="callout">We were able to preview the application directly from our Cloud9 environment since the application is currently running using HTTP over port 8080. For more information about this topic, feel free to check out <a href="https://docs.aws.amazon.com/cloud9/latest/user-guide/app-preview.xhtml">https://docs.aws.amazon.com/cloud9/latest/user-guide/app-preview.xhtml</a>.</p>
<ol>
<li value="17">Click on the <a id="_idIndexMarker1360"/>button, as highlighted in <em class="italic">Figure 10.14</em>, to open the preview window in a separate browser tab:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer303">
<img alt="Figure 10.14 – Previewing in a new window " height="470" src="image/B18638_10_014.jpg" width="1051"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.14 – Previewing in a new window</p>
<p class="list-inset">Make sure that you do not close the browser tab where the Cloud9 environment is running while working with the application preview in the second browser tab.</p>
<ol>
<li value="18">Specify the following <a id="_idIndexMarker1361"/>credentials on the <strong class="bold">Log in to Your Account</strong> page: <ul><li><strong class="bold">Email Address</strong>: <strong class="source-inline">user@example.com</strong></li>
<li><strong class="bold">Password</strong>: <strong class="source-inline">12341234</strong></li>
</ul></li>
</ol>
<p class="callout-heading">Important Note</p>
<p class="callout">Do not share the URL of the application preview tab with others. To change the default password, feel free to check the following link at <a href="https://awslabs.github.io/kubeflow-manifests/docs/deployment/connect-kubeflow-dashboard/">https://awslabs.github.io/kubeflow-manifests/docs/deployment/connect-kubeflow-dashboard/</a> </p>
<p class="list-inset">This<a id="_idIndexMarker1362"/> should redirect you to the <strong class="bold">Kubeflow Central Dashboard</strong> similar<a id="_idIndexMarker1363"/> to what is shown in <em class="italic">Figure 10.15</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer304">
<img alt="Figure 10.15 – The Kubeflow central dashboard " height="871" src="image/B18638_10_015.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.15 – The Kubeflow central dashboard</p>
<p class="list-inset">In <em class="italic">Figure 10.15</em>, we can<a id="_idIndexMarker1364"/> see the <strong class="bold">Kubeflow Central Dashboard</strong>—a dashboard interface that provides immediate <a id="_idIndexMarker1365"/>access to the components and resources we have created and worked with. Feel free to navigate to the different parts of this dashboard using the sidebar. </p>
<p>Finally, all the setup work has been completed! In the next section, we will run our first custom Kubeflow pipeline. Feel free to grab a cup of coffee or tea before proceeding.</p>
<h1 id="_idParaDest-210"><a id="_idTextAnchor225"/>Running our first Kubeflow pipeline</h1>
<p>In this section, we <a id="_idIndexMarker1366"/>will run a custom pipeline that will download a sample tabular dataset and use it as training data to build our <strong class="bold">linear regression</strong> model. The<a id="_idIndexMarker1367"/> steps and instructions to be executed by the pipeline have been defined inside a YAML file. Once this YAML file has been uploaded, we would then be able to run a Kubeflow pipeline that will run the following steps:</p>
<ol>
<li value="1"><strong class="bold">Download dataset</strong>: Here, we will be downloading and working with a dataset that only has 20 records (along with the row containing the header). In addition to this, we will start with a clean version without any missing or invalid values:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer305">
<img alt="" height="644" src="image/B18638_10_016.jpg" width="881"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.16 – A sample tabular dataset</p>
<p class="list-inset">In <em class="italic">Figure 10.16</em>, we can see that our dataset has three columns:</p>
<ul>
<li><strong class="source-inline">last_name</strong> – This is the last name of the manager.</li>
<li><strong class="source-inline">management_experience_months</strong> – This is the total number of months a manager has been managing team members.</li>
<li><strong class="source-inline">monthly_salary</strong> – This is the current salary, per month, of the manager (in USD).</li>
</ul>
<p class="list-inset">To simplify<a id="_idIndexMarker1368"/> things a bit, we will be working with a dataset that only has a few records—just enough to produce a simple ML model. In addition to this, we will start with a clean version without any missing or invalid values.</p>
<ol>
<li value="2"><strong class="bold">Process data</strong>: After downloading the dataset, our pipeline will proceed with processing the data and transforming<a id="_idIndexMarker1369"/> it into a <strong class="bold">pandas</strong> DataFrame where the first column is the target column (<strong class="source-inline">monthly_salary</strong>) and the second column is the predictor column (<strong class="source-inline">management_experiment_months</strong>). At the same time, we will perform the <strong class="bold">train-test split</strong> so<a id="_idIndexMarker1370"/> that we can use 70% of the dataset for training the model and the remaining 30% for evaluating it.</li>
<li><strong class="bold">Train model</strong>: The DataFrame containing the training set would then be used to train the <strong class="bold">linear regression</strong> model. Here, we <a id="_idIndexMarker1371"/>will make use<a id="_idIndexMarker1372"/> of <strong class="bold">scikit-learn’s</strong> <strong class="source-inline">LinearRegression</strong> algorithm to fit a linear model on the training data.</li>
<li><strong class="bold">Evaluate model</strong>: Once the training step has been completed, we will evaluate it using the test set.</li>
<li><strong class="bold">Perform sample prediction</strong>: Finally, we will perform a sample prediction where the <a id="_idIndexMarker1373"/>model would yield a predicted output value (<strong class="source-inline">monthly_salary</strong>) given an input value (<strong class="source-inline">management_experiment_months</strong>).</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Note that we have full control of how our pipeline would behave. We can think of a pipeline as just a sequence of steps where each step might generate an output that would then be used by another step as input. </p>
<p>Now that we have a better idea of what our pipeline looks like, let’s proceed with running our first pipeline:</p>
<ol>
<li value="1">Let’s begin by opening the following link in another browser tab: <a href="https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter10/basic_pipeline.yaml">https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter10/basic_pipeline.yaml</a>.</li>
<li>Right-click on any part of the page to open a context menu that is similar to what is shown in <em class="italic">Figure 10.17</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer306">
<img alt="Figure 10.17 – Downloading the YAML file " height="755" src="image/B18638_10_017.jpg" width="1162"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.17 – Downloading the YAML file</p>
<p class="list-inset">Save the file as <strong class="source-inline">basic_pipeline.yaml</strong> and download it to the <strong class="source-inline">Downloads</strong> folder (or similar) of your local machine.</p>
<ol>
<li value="3">Back in <a id="_idIndexMarker1374"/>the browser tab showing the <strong class="bold">Kubeflow Central Dashboard</strong>, locate<a id="_idIndexMarker1375"/> and click on <strong class="bold">Pipelines</strong> in the sidebar.</li>
<li>Next, click on the <strong class="bold">Upload pipeline</strong> button (beside the <strong class="bold">Refresh</strong> button)</li>
<li>In the <strong class="bold">Upload Pipeline or Pipeline Version</strong> page, specify <strong class="source-inline">My first pipeline</strong> under <strong class="bold">Pipeline Name</strong>. After that, select the <strong class="bold">Upload a file</strong> checkbox, as shown in <em class="italic">Figure 10.18</em>. Locate and upload the <strong class="source-inline">basic_pipeline.yaml</strong> file (from your local machine) using the file input field provided. Finally, click on the <strong class="bold">Create</strong> button (as highlighted in <em class="italic">Figure 10.18</em>):</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer307">
<img alt="Figure 10.18 – Uploading a pipeline (file) " height="661" src="image/B18638_10_018.jpg" width="812"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.18 – Uploading a pipeline (file)</p>
<p class="list-inset">Clicking<a id="_idIndexMarker1376"/> on the <strong class="bold">Create</strong> button should create the pipeline and redirect you to a pipeline page similar to what is shown in <em class="italic">Figure 10.19</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer308">
<img alt="Figure 10.19 – A graph of the first pipeline " height="645" src="image/B18638_10_019.jpg" width="804"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.19 – A graph of the first pipeline</p>
<p class="list-inset">At this point, our pipeline should be ready! The next step would be to create an experiment and run it.</p>
<p class="callout-heading">Note</p>
<p class="callout"><em class="italic">What just happened?</em> Upon uploading the YAML file, Kubeflow Pipelines converted the YAML file into a pipeline that can be executed through a pipeline run.</p>
<ol>
<li value="6">Next, locate <a id="_idIndexMarker1377"/>and click on the <strong class="bold">Create experiment</strong> button (located in the upper-right corner of the page). Feel free to zoom in/out (and close any popups and overlays that might appear) if you cannot locate the <strong class="bold">Create experiment</strong> button.</li>
<li>Specify <strong class="source-inline">My first experiment</strong> under <strong class="bold">Experiment name</strong>. Then click on the <strong class="bold">Next</strong> button.</li>
<li>On the <strong class="bold">Start a run</strong> page, scroll down to the bottom of the page and then click on the <strong class="bold">Start</strong> button.</li>
<li>Locate and click on <strong class="bold">Run of My first pipeline</strong>, as highlighted in <em class="italic">Figure 10.20</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer309">
<img alt="Figure 10.20 – Navigating to the pipeline run " height="359" src="image/B18638_10_020.jpg" width="805"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.20 – Navigating to the pipeline run</p>
<p class="list-inset">Here, we can see that our pipeline has already started running. After navigating to the specific pipeline run page, you should see a relatively new or partially completed<a id="_idIndexMarker1378"/> pipeline similar to what is shown in <em class="italic">Figure 10.21</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer310">
<img alt="Figure 10.21 – Waiting for the pipeline to finish running " height="653" src="image/B18638_10_021.jpg" width="598"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.21 – Waiting for the pipeline to finish running</p>
<p class="list-inset">This should take around 1–2 minutes to complete. You should see a check mark on each of the steps that have been completed successfully. </p>
<ol>
<li value="10">While the pipeline is running, you might click on any of the steps to inspect the corresponding set of input and output artifacts, logs, and other details:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer311">
<img alt="Figure 10.22 – Inspecting the artifacts " height="703" src="image/B18638_10_022.jpg" width="890"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.22 – Inspecting the artifacts</p>
<p class="list-inset">In <em class="italic">Figure 10.22</em>, we<a id="_idIndexMarker1379"/> can see that we are able to view and debug the input and output artifacts after clicking on the box corresponding to the <strong class="bold">Process data</strong> step. Also, we should find other details about the current step by navigating to the other tabs (<strong class="bold">Visualizations</strong>, <strong class="bold">Details</strong>, <strong class="bold">Volumes</strong>, and <strong class="bold">Logs</strong>).</p>
<p>Congratulations on running your first pipeline! If you are wondering how we prepared this pipeline, we simply used the <strong class="bold">Kubeflow Pipelines SDK</strong> to define the steps of the pipeline and generate the YAML file containing all the instructions and configurations. In the next section, we will dive a bit deeper into using the <strong class="bold">Kubeflow Pipelines SDK</strong> when building custom ML pipelines.</p>
<h1 id="_idParaDest-211"><a id="_idTextAnchor226"/>Using the Kubeflow Pipelines SDK to build ML workflows</h1>
<p>In this <a id="_idIndexMarker1380"/>section, we will build ML workflows using the <strong class="bold">Kubeflow Pipelines SDK</strong>. The Kubeflow Pipelines SDK contains what we need to build the pipeline components containing the custom code we want to run. Using the Kubeflow Pipelines SDK, we can define the Python functions that would map to the pipeline components of a pipeline.</p>
<p>Here are some guidelines that we need to follow when building <strong class="bold">Python function-based components</strong> using<a id="_idIndexMarker1381"/> the Kubeflow Pipelines SDK:</p>
<ul>
<li>The defined Python functions should be standalone and should not use any code and variables <a id="_idIndexMarker1382"/>declared outside of the function definition. This means that <strong class="bold">import statements</strong> (for example, <strong class="source-inline">import pandas</strong>) should be implemented inside the function, too. Here’s a quick example of how imports should be implemented:<pre class="source-code">def process_data(...):</pre><pre class="source-code">    <strong class="bold">import pandas as pd    </strong></pre><pre class="source-code">    df_all_data = pd.read_csv(df_all_data_path)</pre><pre class="source-code">    # and so on...</pre></li>
<li>Data must be passed as files when passing large amounts of data (or data with complex data types) between components. Here’s a quick example of this in action:<pre class="source-code">def evaluate_model(</pre><pre class="source-code">    <strong class="bold">model_path</strong>: <strong class="bold">InputPath(str)</strong>,</pre><pre class="source-code">    <strong class="bold">df_test_data_path</strong>: <strong class="bold">InputPath(str)</strong>):</pre><pre class="source-code">    </pre><pre class="source-code">    import pandas as pd</pre><pre class="source-code">    from joblib import load</pre><pre class="source-code">    </pre><pre class="source-code">    df_test_data = pd.read_csv(<strong class="bold">df_test_data_path</strong>)</pre><pre class="source-code">    model = load(<strong class="bold">model_path</strong>)</pre><pre class="source-code">    # and so on...</pre></li>
<li>Use the <strong class="source-inline">create_component_from_func()</strong> function (from <strong class="source-inline">kfp.components</strong>) to convert the defined function into a pipeline component. A list of packages can be specified in the <strong class="source-inline">packages_to_install</strong> parameter when calling the <strong class="source-inline">create_component_from_func()</strong> function similar to what we have in the following block of code:<pre class="source-code">process_data_op = <strong class="bold">create_component_from_func</strong>(</pre><pre class="source-code">    process_data, </pre><pre class="source-code">    <strong class="bold">packages_to_install=['pandas', 'sklearn']</strong></pre><pre class="source-code">)</pre></li>
</ul>
<p class="list-inset">The packages specified would then be installed before the function is executed.</p>
<ul>
<li>Optionally, we might prepare a custom container image that will be used for the environment where the Python function will run. The custom container image can be specified in the <strong class="source-inline">base_image</strong> parameter when calling the <strong class="source-inline">create_component_from_func()</strong> function.  </li>
</ul>
<p>That said, let’s begin <a id="_idIndexMarker1383"/>defining and configuring our ML pipeline using the <strong class="bold">Kubeflow Pipelines SDK</strong>:</p>
<ol>
<li value="1">Locate and click on <strong class="bold">Notebooks</strong> in the<a id="_idIndexMarker1384"/> sidebar of the <strong class="bold">Kubeflow Central Dashboard</strong>.</li>
<li>Next, click on the <strong class="bold">New Notebook</strong> button.</li>
<li>Specify <strong class="source-inline">first-notebook</strong> for the <strong class="bold">Name</strong> input field value.</li>
<li>Scroll down to the bottom of the page, and then click on the <strong class="bold">LAUNCH</strong> button.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Wait for the notebook to become available. It should take about 1–2 minutes for the notebook to be ready.</p>
<ol>
<li value="5">Click on the <strong class="bold">CONNECT</strong> button once the notebook becomes available.</li>
<li>In the <strong class="bold">Jupyter Lab Launcher</strong>, select the <strong class="bold">Python 3</strong> option (under <strong class="bold">Notebook</strong>), as <a id="_idIndexMarker1385"/>highlighted in <em class="italic">Figure 10.23</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer312">
<img alt="Figure 10.23 – Jupyter Lab Launcher " height="502" src="image/B18638_10_023.jpg" width="756"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.23 – Jupyter Lab Launcher</p>
<p class="list-inset">This should create a new <strong class="bold">Jupyter Notebook</strong> (inside a container within a Kubernetes Pod) where<a id="_idIndexMarker1386"/> we can run our Python code.</p>
<p class="callout-heading">Note</p>
<p class="callout">We will run the blocks of code in the succeeding set of steps inside the Jupyter notebook we launched.</p>
<ol>
<li value="7">Let’s perform a<a id="_idIndexMarker1387"/> few imports from the <strong class="bold">Kubeflow Pipelines SDK</strong>:<pre class="source-code">import kfp</pre><pre class="source-code">from kfp import <strong class="bold">dsl</strong></pre><pre class="source-code">from kfp.components import <strong class="bold">InputPath</strong>, <strong class="bold">OutputPath</strong></pre><pre class="source-code">from kfp.components import <strong class="bold">create_component_from_func</strong></pre></li>
<li>For the first step in our pipeline, we define the <strong class="source-inline">download_dataset()</strong> function, which downloads a dummy dataset and converts it into a CSV file. This CSV file gets<a id="_idIndexMarker1388"/> passed to the next step through the <strong class="source-inline">df_all_data_path</strong> <strong class="source-inline">OutputPath</strong> object:<pre class="source-code">def <strong class="bold">download_dataset</strong>(</pre><pre class="source-code">    <strong class="bold">df_all_data_path</strong>: OutputPath(str)):</pre><pre class="source-code">    </pre><pre class="source-code">    import pandas as pd</pre><pre class="source-code">    </pre><pre class="source-code">    url="https://bit.ly/3POP8CI"</pre><pre class="source-code">    </pre><pre class="source-code">    df_all_data = pd.read_csv(url)</pre><pre class="source-code">    print(df_all_data)</pre><pre class="source-code">    df_all_data.to_csv(</pre><pre class="source-code">        df_all_data_path, </pre><pre class="source-code">        header=True, </pre><pre class="source-code">        index=False)</pre></li>
<li>For the second step in our pipeline, we define the <strong class="source-inline">process_data()</strong> function where we read the CSV data from the previous step and apply the train-test split, which will yield a training set and a test set. These can then be saved as CSV files and passed to the next step through the <strong class="source-inline">df_training_data_path</strong> and <strong class="source-inline">df_test_data_path</strong> <strong class="source-inline">OutputPath</strong> objects, respectively:<pre class="source-code">def <strong class="bold">process_data</strong>(</pre><pre class="source-code">    <strong class="bold">df_all_data_path</strong>: InputPath(str), </pre><pre class="source-code">    <strong class="bold">df_training_data_path</strong>: OutputPath(str), </pre><pre class="source-code">    <strong class="bold">df_test_data_path</strong>: OutputPath(str)):</pre><pre class="source-code">    </pre><pre class="source-code">    import pandas as pd</pre><pre class="source-code">    from sklearn.model_selection import \</pre><pre class="source-code">        train_test_split</pre><pre class="source-code">    </pre><pre class="source-code">    df_all_data = pd.read_csv(df_all_data_path)</pre><pre class="source-code">    print(df_all_data)</pre><pre class="source-code">    </pre><pre class="source-code">    mem = 'management_experience_months'</pre><pre class="source-code">    ms = 'monthly_salary'</pre><pre class="source-code">    X = df_all_data[mem].values </pre><pre class="source-code">    y = df_all_data[ms].values</pre><pre class="source-code">    X_train, X_test, y_train, y_test = \</pre><pre class="source-code">        <strong class="bold">train_test_split</strong>(</pre><pre class="source-code">            X, y, test_size=0.3, random_state=0</pre><pre class="source-code">        )</pre><pre class="source-code">    </pre><pre class="source-code">    df_training_data = pd.DataFrame({ </pre><pre class="source-code">        'monthly_salary': y_train, </pre><pre class="source-code">        'management_experience_months': X_train</pre><pre class="source-code">    })</pre><pre class="source-code">    df_training_data.to_csv(</pre><pre class="source-code">        <strong class="bold">df_training_data_path</strong>, </pre><pre class="source-code">        header=True, index=False</pre><pre class="source-code">    )</pre><pre class="source-code">    df_test_data = pd.DataFrame({ </pre><pre class="source-code">        'monthly_salary': y_test, </pre><pre class="source-code">        'management_experience_months': X_test</pre><pre class="source-code">    })</pre><pre class="source-code">    df_test_data.to_csv(</pre><pre class="source-code">        <strong class="bold">df_test_data_path</strong>, </pre><pre class="source-code">        header=True, index=False</pre><pre class="source-code">    )</pre></li>
<li>For the third step<a id="_idIndexMarker1389"/> in our pipeline, we define the <strong class="source-inline">train_model()</strong> function where we use the training data from the previous step to train a sample model. Then, the trained model gets saved and passed to the next step via the <strong class="source-inline">model_path</strong> <strong class="source-inline">OutputPath</strong> object:<pre class="source-code">def <strong class="bold">train_model</strong>(</pre><pre class="source-code">    <strong class="bold">df_training_data_path</strong>: InputPath(str),</pre><pre class="source-code">    <strong class="bold">model_path</strong>: OutputPath(str)):</pre><pre class="source-code">    </pre><pre class="source-code">    import pandas as pd</pre><pre class="source-code">    from sklearn.linear_model import LinearRegression</pre><pre class="source-code">    from joblib import dump</pre><pre class="source-code">    </pre><pre class="source-code">    df_training_data = pd.read_csv(</pre><pre class="source-code">        <strong class="bold">df_training_data_path</strong></pre><pre class="source-code">    )</pre><pre class="source-code">    print(df_training_data)</pre><pre class="source-code">    </pre><pre class="source-code">    mem = 'management_experience_months'</pre><pre class="source-code">    X_train = df_training_data[mem].values</pre><pre class="source-code">    ms = 'monthly_salary'</pre><pre class="source-code">    y_train = df_training_data[ms].values</pre><pre class="source-code">    </pre><pre class="source-code">    model = <strong class="bold">LinearRegression</strong>().<strong class="bold">fit</strong>(</pre><pre class="source-code">        X_train.reshape(-1, 1), y_train</pre><pre class="source-code">    )</pre><pre class="source-code">    print(model)</pre><pre class="source-code">    dump(model, <strong class="bold">model_path</strong>)</pre></li>
<li>In the fourth step, we <a id="_idIndexMarker1390"/>define the <strong class="source-inline">evaluate_model()</strong> function where we use the test data from the second step to evaluate the trained model that we obtained from the previous step:<pre class="source-code">def <strong class="bold">evaluate_model</strong>(</pre><pre class="source-code">    <strong class="bold">model_path</strong>: InputPath(str),</pre><pre class="source-code">    <strong class="bold">df_test_data_path</strong>: InputPath(str)):</pre><pre class="source-code">    </pre><pre class="source-code">    import pandas as pd</pre><pre class="source-code">    from joblib import load</pre><pre class="source-code">    </pre><pre class="source-code">    df_test_data = pd.read_csv(df_test_data_path)</pre><pre class="source-code">    mem = 'management_experience_months'</pre><pre class="source-code">    ms = 'monthly_salary'</pre><pre class="source-code">    X_test = df_test_data[mem].values</pre><pre class="source-code">    y_test = df_test_data[ms].values</pre><pre class="source-code">    </pre><pre class="source-code">    model = load(<strong class="bold">model_path</strong>)</pre><pre class="source-code">    print(model.score(X_test.reshape(-1, 1), y_test))</pre></li>
<li>For the<a id="_idIndexMarker1391"/> final step of our pipeline, we define the <strong class="source-inline">perform_sample_prediction()</strong> function where we use the trained model from the third step to perform a sample prediction (using a sample input value):<pre class="source-code">def <strong class="bold">perform_sample_prediction</strong>(</pre><pre class="source-code">    <strong class="bold">model_path</strong>: InputPath(str)):</pre><pre class="source-code">    from joblib import load</pre><pre class="source-code">    </pre><pre class="source-code">    model = load(<strong class="bold">model_path</strong>)</pre><pre class="source-code">    print(model.predict([[42]])[0])</pre></li>
<li>Then, we use the <strong class="source-inline">create_component_from_func()</strong> function for each of the functions we have prepared to create components. Here, we specify the packages to install before running these functions:<pre class="source-code"><strong class="bold">download_dataset_op</strong> = create_component_from_func(</pre><pre class="source-code">    download_dataset, </pre><pre class="source-code">    <strong class="bold">packages_to_install=['pandas']</strong></pre><pre class="source-code">)</pre><pre class="source-code"><strong class="bold">process_data_op</strong> = create_component_from_func(</pre><pre class="source-code">    process_data, </pre><pre class="source-code">    <strong class="bold">packages_to_install=['pandas', 'sklearn']</strong></pre><pre class="source-code">)</pre><pre class="source-code"><strong class="bold">train_model_op</strong> = create_component_from_func(</pre><pre class="source-code">    train_model, </pre><pre class="source-code">    <strong class="bold">packages_to_install=[</strong></pre><pre class="source-code">        <strong class="bold">'pandas', 'sklearn', 'joblib'</strong></pre><pre class="source-code">    <strong class="bold">]</strong></pre><pre class="source-code">)</pre><pre class="source-code"><strong class="bold">evaluate_model_op</strong> = create_component_from_func(</pre><pre class="source-code">    evaluate_model, </pre><pre class="source-code">    <strong class="bold">packages_to_install=[</strong></pre><pre class="source-code">        <strong class="bold">'pandas', 'joblib', 'sklearn'</strong></pre><pre class="source-code">    <strong class="bold">]</strong></pre><pre class="source-code">)</pre><pre class="source-code"><strong class="bold">perform_sample_prediction_op</strong> = \</pre><pre class="source-code">    create_component_from_func(</pre><pre class="source-code">        perform_sample_prediction, </pre><pre class="source-code">        <strong class="bold">packages_to_install=['joblib', 'sklearn']</strong></pre><pre class="source-code">    )</pre></li>
<li>Now, let’s bring <a id="_idIndexMarker1392"/>everything together and define the pipeline with the <strong class="source-inline">basic_pipeline()</strong> function:<pre class="source-code">@dsl.pipeline(</pre><pre class="source-code">    name='Basic pipeline',</pre><pre class="source-code">    description='Basic pipeline'</pre><pre class="source-code">)</pre><pre class="source-code">def <strong class="bold">basic_pipeline()</strong>:</pre><pre class="source-code">    DOWNLOAD_DATASET = download_dataset_op()</pre><pre class="source-code">    PROCESS_DATA = process_data_op(</pre><pre class="source-code">        DOWNLOAD_DATASET.<strong class="bold">output</strong></pre><pre class="source-code">    )</pre><pre class="source-code">    TRAIN_MODEL = train_model_op(</pre><pre class="source-code">        PROCESS_DATA.<strong class="bold">outputs['df_training_data']</strong></pre><pre class="source-code">    )</pre><pre class="source-code">    EVALUATE_MODEL = evaluate_model_op(</pre><pre class="source-code">        TRAIN_MODEL.outputs['model'], </pre><pre class="source-code">        PROCESS_DATA.<strong class="bold">outputs['df_test_data']</strong></pre><pre class="source-code">    )</pre><pre class="source-code">    PERFORM_SAMPLE_PREDICTION = \</pre><pre class="source-code">        perform_sample_prediction_op(</pre><pre class="source-code">            TRAIN_MODEL.<strong class="bold">outputs['model']</strong></pre><pre class="source-code">        )</pre><pre class="source-code">    PERFORM_SAMPLE_PREDICTION.after(EVALUATE_MODEL)</pre></li>
<li>Finally, let’s <a id="_idIndexMarker1393"/>generate the pipeline’s YAML file using the following block of code:<pre class="source-code">kfp.compiler.Compiler().<strong class="bold">compile</strong>(</pre><pre class="source-code">    basic_pipeline, </pre><pre class="source-code">    <strong class="bold">'basic_pipeline.yaml'</strong></pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">At this point, we should see a YAML file in the file browser. If not, feel free to use the refresh button to update the list of files displayed.</p>
<ol>
<li value="16">In the file browser, right-click on the generated <strong class="source-inline">basic_pipeline.yaml</strong> file to open a context menu similar to what is shown in <em class="italic">Figure 10.24</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer313">
<img alt="Figure 10.24 – Downloading the basic_pipeline.yaml file " height="439" src="image/B18638_10_024.jpg" width="737"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.24 – Downloading the basic_pipeline.yaml file</p>
<p class="list-inset">Select <strong class="bold">Download</strong> from<a id="_idIndexMarker1394"/> the list of options in the context menu (as highlighted in <em class="italic">Figure 10.24</em>). This should download the YAML file to the downloads folder (or similar) of your local machine.</p>
<ol>
<li value="17">After downloading the <strong class="source-inline">basic_pipeline.yaml</strong> file, navigate to the browser tab where we have <strong class="bold">Kubeflow Central Dashboard</strong> open. After that, navigate to the <strong class="bold">Pipelines</strong> page by clicking on <strong class="bold">Pipelines</strong> in the sidebar (in <strong class="bold">Kubeflow Central Dashboard</strong>).</li>
<li>Next, click on the <strong class="bold">Upload pipeline</strong> button (beside the <strong class="bold">Refresh</strong> button), and then use the <strong class="source-inline">basic_pipeline.yaml</strong> file we generated in this section to run another pipeline.</li>
</ol>
<p class="callout-heading">Important Note</p>
<p class="callout">Feel free to check and follow the steps specified in the <em class="italic">Running our first Kubeflow pipeline</em> section of this chapter when running a new pipeline. We will leave this to you as an exercise! (The resulting pipeline should be the same.) </p>
<p><em class="italic">That was easier than expected, right?</em> We should congratulate ourselves after completing the hands-on solutions in this chapter! Being able to properly set up Kubeflow on EKS along with getting custom ML pipelines to work using Kubeflow is an achievement. This should give us the confidence to build more complex ML pipelines using the technology stack that we are using right now.</p>
<p>In the next section, we will do a quick cleanup and delete the resources we created in this chapter.</p>
<h1 id="_idParaDest-212"><a id="_idTextAnchor227"/>Cleaning up</h1>
<p>Now<a id="_idIndexMarker1395"/> that we have completed working on the hands-on solutions of this chapter, it is time we clean up and turn off the resources we will no longer use. At this point in time, we have an EKS cluster running with <strong class="source-inline">5</strong> x <strong class="source-inline">m5.xlarge</strong> instances running. We need to terminate these resources to manage the cost.</p>
<p class="callout-heading">Note</p>
<p class="callout"><em class="italic">If we do not turn these off (for a month), how much would it cost?</em> At a minimum (per month), it would cost around USD 700.80 for the running EC2 instances (<em class="italic">5 instances x 0.192 USD x 730 hours in a month</em>) plus <em class="italic">USD 73</em><strong class="source-inline"> </strong>for the EKS cluster (<em class="italic">1 Cluster x 0.10 USD per hour x 730 hours per month</em>) assuming that we are running the EKS cluster in the Oregon region (<strong class="source-inline">us-west-2</strong>). Note that there will be other additional costs associated with the EBS volumes attached to these instances along with the other resources used in this chapter.</p>
<p>In the next set of steps, we will uninstall and delete the resources in the Cloud9 environment’s Terminal:</p>
<ol>
<li value="1">Let’s navigate back to the Cloud9 environment <strong class="bold">Terminal</strong> tab, where we last ran the following command (<em class="italic">NOTE: do not run the following command as we just need to navigate back to the tab where this command is running</em>):<pre class="source-code"><strong class="bold">kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80 --address=localhost</strong></pre></li>
</ol>
<p class="list-inset">We should find a few <strong class="bold">Handling connection for 8080</strong> logs in this Terminal.</p>
<ol>
<li value="2">Stop this command by pressing <em class="italic">Ctrl</em> + <em class="italic">C</em> (or, alternatively, <em class="italic">Cmd</em> + <em class="italic">C</em> when using a Mac device) inside the Terminal.</li>
<li>After that, let’s run the following command, which utilizes <strong class="source-inline">kubectl delete</strong> to delete the resources:<pre class="source-code"><strong class="bold">cd ~/environment/ch10/kubeflow-manifests/</strong></pre><pre class="source-code"><strong class="bold">cd deployments/vanilla/</strong></pre><pre class="source-code"><strong class="bold">kustomize build . | kubectl delete -f -</strong></pre></li>
<li>Let’s delete<a id="_idIndexMarker1396"/> the EKS cluster by running the following command:<pre class="source-code"><strong class="bold">eksctl delete cluster</strong> --region $CLUSTER_REGION --name $CLUSTER_NAME</pre></li>
</ol>
<p class="list-inset">Ensure that the <strong class="source-inline">CLUSTER_REGION</strong> and <strong class="source-inline">CLUSTER_NAME</strong> variables are set with the appropriate values before running the command. For example, if you are running the Kubernetes cluster in the Oregon region, <strong class="source-inline">CLUSTER_REGION</strong> should be set to <strong class="source-inline">us-west-2</strong>, while <strong class="source-inline">CLUSTER_NAME</strong> should be set to <strong class="source-inline">kubeflow-eks-000</strong> (this is similar to what’s specified in the <strong class="source-inline">eks.yaml</strong> file)</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Make sure that you verify that the CloudFormation Stack created by the <strong class="source-inline">eksctl</strong> command is completely deleted. You can do this by navigating to the CloudFormation console and checking whether there are stacks with a <strong class="bold">DELETE_FAILED</strong> status. If that’s the case, simply reattempt the deletion of these stacks until all resources have been successfully deleted.</p>
<ol>
<li value="5">Finally, detach the IAM role attached to the EC2 instance where the Cloud9 environment is running. We will leave this to you as an exercise! </li>
</ol>
<p>Make sure to review whether all delete operations have succeeded before proceeding to the next section.</p>
<h1 id="_idParaDest-213"><a id="_idTextAnchor228"/>Recommended strategies and best practices</h1>
<p>Before we <a id="_idIndexMarker1397"/>end this chapter, we will quickly discuss some of the recommended strategies and best practices when working with Kubeflow on EKS. </p>
<p>Let’s start by identifying the ways we can improve how we designed and implemented our ML pipeline. <em class="italic">What improvements can we make to the initial version of our pipeline?</em> Here are some of the possible upgrades we can implement:</p>
<ul>
<li>Making the pipeline more reusable by allowing the first step of our pipeline to accept the dataset input path as an input parameter (instead of it being hardcoded in a similar way to what we have right now)</li>
<li>Building and using a custom container image instead of using the <strong class="source-inline">packages_to_install</strong> parameter when working with pipeline components</li>
<li>Saving the model artifacts into a storage <a id="_idIndexMarker1398"/>service such as <strong class="bold">Amazon S3</strong> (which will help us make sure that we are able to keep the artifacts even if the Kubernetes cluster has been deleted)</li>
<li>Adding resource limits (such as CPU and memory limits) to specific steps in the pipeline using a <strong class="source-inline">ContainerOp</strong> object’s <strong class="source-inline">set_memory_limit()</strong> and <strong class="source-inline">set_cpu_limit()</strong></li>
<li>Utilizing <strong class="bold">SageMaker Components for Kubeflow Pipelines</strong> to move some of the <a id="_idIndexMarker1399"/>data processing and training workloads to SageMaker</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">If you are interested in learning and applying the best practices when preparing <strong class="bold">Kubeflow Pipelines’ components</strong>, feel <a id="_idIndexMarker1400"/>free to check out <a href="https://www.kubeflow.org/docs/components/pipelines/sdk/best-practices/">https://www.kubeflow.org/docs/components/pipelines/sdk/best-practices/</a>.</p>
<p>Next, let’s talk about some strategies and solutions that we can implement to upgrade our EKS cluster and Kubeflow setup:</p>
<ul>
<li>Setting <a id="_idIndexMarker1401"/>up <strong class="bold">CloudWatch Container Insights</strong> on the Amazon EKS cluster to monitor the cluster performance</li>
<li>Setting up and <a id="_idIndexMarker1402"/>deploying <strong class="bold">Kubernetes Dashboard</strong> and/or <strong class="bold">Rancher</strong> to <a id="_idIndexMarker1403"/>manage and control the Amazon EKS cluster resources</li>
<li>Setting<a id="_idIndexMarker1404"/> up <strong class="bold">Prometheus</strong> and <strong class="bold">Grafana</strong> for <a id="_idIndexMarker1405"/>monitoring<a id="_idIndexMarker1406"/> the Kubernetes cluster</li>
<li>Changing the default<a id="_idIndexMarker1407"/> user password when accessing the <strong class="bold">Kubeflow Central Dashboard</strong></li>
<li>Using <strong class="bold">AWS Cognito</strong> as an<a id="_idIndexMarker1408"/> identity provider when deploying Kubeflow (for Kubeflow user authentication)</li>
<li>Deploying Kubeflow with <a id="_idIndexMarker1409"/>Amazon <strong class="bold">Relational Database Service</strong> (<strong class="bold">RDS</strong>) and Amazon <strong class="bold">Simple Storage Service</strong> (<strong class="bold">S3</strong>) for <a id="_idIndexMarker1410"/>storing metadata and pipeline artifacts</li>
<li>Exposing and accessing Kubeflow through<a id="_idIndexMarker1411"/> an <strong class="bold">Application Load Balancer</strong></li>
<li>Using <strong class="bold">Amazon Elastic File System</strong> (<strong class="bold">EFS</strong>) with<a id="_idIndexMarker1412"/> Kubeflow for persistent storage</li>
<li>Reducing the permissions (to a minimal set of privileges) of the IAM role attached to the EC2 instance where the Cloud9 environment is running</li>
<li>Auditing and upgrading the security configuration of each of the resources used</li>
<li>Setting up autoscaling of the <a id="_idIndexMarker1413"/>EKS cluster (for example, using <strong class="bold">Cluster Autoscaler</strong>)</li>
<li>To manage the long-term costs of running EKS clusters, we can utilize the <strong class="bold">Cost Savings Plans</strong>, which <a id="_idIndexMarker1414"/>involves reducing the overall cost of running resources after making a long-term commitment (for example, a 1-year or 3-year commitment)</li>
</ul>
<p>There’s more we can add to this list, but these should do for now! Make sure to review and check the recommended solutions and strategies shared in <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a>, <em class="italic">Security, Governance, and Compliance Strategies</em>, too.</p>
<h1 id="_idParaDest-214"><a id="_idTextAnchor229"/>Summary</h1>
<p>In this chapter, we set up and configured our containerized ML environment using <strong class="bold">Kubeflow</strong>, <strong class="bold">Kubernetes</strong>, and <strong class="bold">Amazon EKS</strong>. After setting up the environment, we then prepared and ran a custom ML pipeline using the <strong class="bold">Kubeflow Pipelines SDK</strong>. After completing all the hands-on work needed, we proceeded with cleaning up the resources we created. Before ending the chapter, we discussed relevant best practices and strategies to secure, scale, and manage ML pipelines using the technology stack we used in the hands-on portion of this chapter.</p>
<p>In the next chapter, we will build and set up an ML pipeline using <strong class="bold">SageMaker Pipelines</strong>—<strong class="bold">Amazon SageMaker’s</strong> purpose-built solution for automating ML workflows using relevant MLOps practices.</p>
<h1 id="_idParaDest-215"><a id="_idTextAnchor230"/>Further reading</h1>
<p>For more information on the topics covered in this chapter, feel free to check out the following resources:</p>
<ul>
<li><em class="italic">Kubernetes concepts</em> (<a href="https://kubernetes.io/docs/concepts/">https://kubernetes.io/docs/concepts/</a>)</li>
<li><em class="italic">Getting started with Amazon EKS</em> (<a href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started.xhtml">https://docs.aws.amazon.com/eks/latest/userguide/getting-started.xhtml</a>)</li>
<li><em class="italic">eksctl – The official CLI for Amazon EKS</em> (<a href="https://eksctl.io/">https://eksctl.io/</a>)</li>
<li><em class="italic">Amazon EKS troubleshooting</em> (<a href="https://docs.aws.amazon.com/eks/latest/userguide/troubleshooting.xhtml">https://docs.aws.amazon.com/eks/latest/userguide/troubleshooting.xhtml</a>)</li>
<li><em class="italic">Kubeflow on AWS – Deployment</em> (<a href="https://awslabs.github.io/kubeflow-manifests/docs/deployment/">https://awslabs.github.io/kubeflow-manifests/docs/deployment/</a>)</li>
<li><em class="italic">Kubeflow on AWS Security</em> (<a href="https://awslabs.github.io/kubeflow-manifests/docs/about/security/">https://awslabs.github.io/kubeflow-manifests/docs/about/security/</a>)</li>
</ul>
</div>
</div>
</body></html>