["```py\nimport pandas as pdorig_df = pd.DataFrame({\n    'age': [45, 43, 54, 56, 54, 52, 41],\n    'gender': ['M', 'F', 'F', 'M', 'M', 'F', 'M'],\n    'group': ['H1', 'H1', 'H2', 'H3', 'H2', 'H1', 'H3'],\n    'target': [0, 0, 1, 0, 1, 1, 0]})\n```", "```py\n# encoding using label encodingfrom sklearn.preprocessing import LabelEncoder\n# initializing LabelEncoder\nle = LabelEncoder()\n# encoding gender and group columns\nlabel_encoded_df = orig_df.copy()\nlabel_encoded_df['gender'] = le.fit_transform(\n    label_encoded_df.gender)\nlabel_encoded_df['group'] = le.fit_transform(\n    label_encoded_df.group)\n```", "```py\n# encoding using one hot encodingfrom sklearn.preprocessing import OneHotEncoder\n# initializing OneHotEncoder\nohe = OneHotEncoder(categories = 'auto')\n# encoding gender column\ngender_ohe = ohe.fit_transform(\n    orig_df['gender'].values.reshape(-1,1)).toarray()\ngender_ohe_df = pd.DataFrame(gender_ohe)\n# encoding group column\ngroup_ohe = ohe.fit_transform(\n    orig_df['group'].values.reshape(-1,1)).toarray()\ngroup_ohe_df = pd.DataFrame(group_ohe)\n# generating the new dataframe with one hot encoded features\nonehot_encoded_df = pd.concat(\n    [orig_df, gender_ohe_df, group_ohe_df], axis =1)\nonehot_encoded_df = onehot_encoded_df.drop(\n    ['gender', 'group'], axis=1)\nonehot_encoded_df.columns = [\n    'age','target','M', 'F','H1','H2', 'H3']\n```", "```py\n# encoding using target encodingfrom category_encoders import TargetEncoder\n# initializing LabelEncoder\nte = TargetEncoder()\n# encoding gender and group columns\ntarget_encoded_df = orig_df.copy()\ntarget_encoded_df['gender'] = te.fit_transform(\n    orig_df['gender'], orig_df['target'])\ntarget_encoded_df['group'] = te.fit_transform(\n    orig_df['group'], orig_df['target'])\n```", "```py\nimport numpy as npfrom sklearn.impute import SimpleImputer\n```", "```py\nX = [[5, 1, 2, 8],    [2, 3, np.nan, 4],\n    [5, 4, 4, 6],\n    [8, 5, np.nan, 7],\n    [7, 8, 8, 3]]\n```", "```py\n# strategy options: mean, median, most_frequent, constantimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp.fit(X)\n# calculate missing values of the input\nX_no_missing = imp.transform(X)\n```", "```py\nimport numpy as npfrom sklearn.linear_model import LinearRegression as LR\n# defining input variables for feature 2 and 3\nf2 = np.array([1, 4, 8]).reshape((-1, 1))\nf3 = np.array([2, 4, 8])\n# initializing a linear regression model with sklearn LinearRegression\nmodel = LR()\n# fitting the linear regression model using f2 and f3 as input and output variables, respectively\nmodel.fit(f2, f3)\n# predicting missing values of feature 3\nmodel.predict(np.array([3, 5]).reshape((-1, 1)))\n```", "```py\nimport numpy as npimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import Isomap\nfrom sklearn.datasets import load_digits\n# loading digit dataset from sklearn\nX, _ = load_digits(return_X_y=True)\nprint('Number of features: {}'.format(X.shape[1]))\n```", "```py\n# fitting isomap and build new dataframe of feature with 5 componentsembedding = Isomap(n_components=5)\nX_transformed_isomap = embedding.fit_transform(X)\nprint('Number of features: {}'.format(\n    X_transformed_isomap.shape[1]))\n# fitting pca and build new dataframe of feature with 5 components\npca = PCA(n_components=5)\nX_transformed_pca = pca.fit_transform(X)\nprint('Number of features: {}'.format(\n    X_transformed_pca.shape[1]))\n# plotting ratio of variance explained by the first n, being between 1 and 5, components\nplt.bar(x = np.arange(0, len(\n    pca.explained_variance_ratio_)),\n    height = np.cumsum(pca.explained_variance_ratio_))\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Number of components')\nplt.show()\n```", "```py\nfrom sklearn.model_selection import cross_val_score,KFoldfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_breast_cancer\n# loading breast cancer dataset\nX, y = load_breast_cancer(return_X_y=True)\n# defining the k-fold CV\nk_CV = KFold(n_splits=5)\n# initializing a k nearest neighbor model\nknn = KNeighborsClassifier()\n# outputting validation performances using average precision across different folds of the designed CV\nscores = cross_val_score(\n    estimator = knn, X = X, y = y, cv = k_CV,\n    scoring = 'average_precision')\nprint(\"Average cross validation score: {}\".format(\n    round(scores.mean(),4)))\n```", "```py\nAverage Cross Validation score: 0.9496\n```", "```py\nfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n# loading breast cancer dataset\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n    test_size=0.30, random_state=5)\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier# initializing a random forest model\nrf_model = RandomForestClassifier(n_estimators=10,\n    max_features=10, max_depth=4)\n# training the random forest model using training set\nrf_model.fit(X_train, y_train)\n# predicting values of test set using the trained random forest model\ny_pred_rf = rf_model.predict(X_test)\n# assessing performance of the model on test setprint(\"Balanced accuracy of the predictions:\",\n    metrics.balanced_accuracy_score(y_test, y_pred_rf))\n```", "```py\nBalanced accuracy of the predictions: 0.9572\n```", "```py\nfrom sklearn import cluster# initializing a random forest model\nkmeans_model = cluster.KMeans(n_clusters=2, n_init = 10)\n# training the kmeans clustering model using training set\nkmeans_model.fit(X_train)\n# assigning new observations, that are test set datapoints here, to the identified clusters\ny_pred_kmeans = kmeans_model.predict(X_test)\n```"]