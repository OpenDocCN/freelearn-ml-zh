- en: Chapter 7. Clustering Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now shift our focus to **unsupervised learning**. In this chapter, we
    will study several **clustering** algorithms, or **clusterers**, and how we can
    implement them in Clojure. We will also demonstrate several Clojure libraries
    that provide implementations of clustering algorithms. Towards the end of the
    chapter, we explore will **dimensionality** **reduction** and how it can be used
    to provide an understandable visualization of the supplied sample data.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering or **cluster analysis** is basically a method of grouping data or
    samples together. As a form of unsupervised learning, a clustering model is trained
    using unlabeled data, by which we mean the samples in the training data will not
    contain the class or category of the input values. Rather, the training data does
    not describe values for the output variable of a given set of inputs. A clustering
    model must determine similarities between several input values and infer the classes
    of these input values on its own. The sample values can thus be partitioned into
    a number of clusters using such a model.
  prefs: []
  type: TYPE_NORMAL
- en: There are several practical applications of clustering in real-world problems.
    Clustering is often used in image analysis, image segmentation, software evolution
    systems, and social network analysis. Outside the domain of computer science,
    clustering algorithms are used in biological classification, gene analysis, and
    crime analysis.
  prefs: []
  type: TYPE_NORMAL
- en: There are several clustering algorithms that have been published till date.
    Each of these algorithms has a unique notion of how a cluster is defined and how
    input values are combined into new clusters. Unfortunately, there is no given
    solution for any clustering problem and each algorithm must be evaluated on a
    trial-and-error basis to determine which model is best suited for the supplied
    training data. Of course, this is one of the aspects of unsupervised learning,
    in the sense that there is no definite way to say that a given solution is the
    best fit for any given data.
  prefs: []
  type: TYPE_NORMAL
- en: This is due to the fact that the input data is unlabeled, and a simple yes/no
    based reward system to train cannot easily be inferred from data in which the
    output variable or class of the input values is unknown.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will describe a handful of clustering techniques that can
    be applied on unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Using K-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **K-means clustering** algorithm is a clustering technique that is based
    on vector quantization (for more information, refer to "Algorithm AS 136: A K-Means
    Clustering Algorithm"). This algorithm partitions a number of sample vectors into
    *K* clusters and hence derives its name. In this section, we will study the nature
    and implementation of the K-means algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantization**, in signal processing, is the process of mapping a large set
    of values into a smaller set of values. For example, an analog signal can be quantized
    to 8 bits and the signal can be represented by 256 levels of quantization. Assuming
    that the bits represent values within the range of 0 to 5 volts, the 8-bit quantization
    allows a resolution of 5/256 volts per bit. In the context of clustering, quantization
    of input or output can be done for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: To restrict the clustering to a finite set of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To accommodate a range of values in the sample data that need to have some level
    of tolerance while clustering is performed. This kind of flexibility is crucial
    in grouping together unknown or unexpected sample values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gist of the algorithm can be concisely described as follows. The *K* mean
    values, or *centroids*, are first randomly initialized. The distance of each sample
    value from each centroid is then calculated. A sample value is grouped into a
    given centroid's cluster depending on which centroid has the minimum distance
    from the given sample. In a multidimensional space for multiple features or input
    values, the distance of a sample input vector is measured by **Euclidean distance**
    between the input vector and a given centroid. This phase of the algorithm is
    termed as the **assignment step**.
  prefs: []
  type: TYPE_NORMAL
- en: The next phase in the *K*-means algorithm is the **update step**. The values
    of the centroids are adjusted based on the partitioned input values generated
    from the previous step. These two steps are then repeated until the difference
    between the centroid values in two consecutive iterations becomes negligible.
    Thus, the final result of the algorithm is the clusters or classes of each set
    of input values in the given training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The iterations performed by the *K*-means algorithm can be illustrated using
    the following plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using K-means clustering](img/4351OS_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Each of the plots depict the centroid and partitioned sample values produced
    by each iteration of the algorithm for a given set of input values. The clusters
    in a given iteration are shown in different colors in each plot. The final plot
    represents the final partitioned set of input values produced by the *K*-means
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization objective of the *K*-means clustering algorithm can be formally
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using K-means clustering](img/4351OS_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the optimization problem defined in the preceding equation, the terms ![Using
    K-means clustering](img/4351OS_07_03.jpg) represent the *K*-mean values around
    which the input values are clustered. The *K*-means algorithm minimizes the size
    of the clusters and also determines the mean values for which these clusters can
    be minimized in size.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm requires ![Using K-means clustering](img/4351OS_07_04.jpg) sample
    values and ![Using K-means clustering](img/4351OS_07_05.jpg) initial mean values
    as inputs. In the assignment step, the input values are assigned to clusters around
    the initial mean values supplied to the algorithm. In the later update step, the
    new mean values are calculated from the input values. In most implementations,
    the new mean values are calculated as the mean of all input values that belongs
    to a given cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Most implementations initialize the ![Using K-means clustering](img/4351OS_07_05.jpg)
    initial mean values to some randomly chosen input values. This technique is called
    the **Forgy method** of random initialization.
  prefs: []
  type: TYPE_NORMAL
- en: The *K*-means algorithm is NP-hard when either the number of clusters *K* or
    the number of dimensions in the input data *d* is unbound. When both these values
    are fixed, the *K*-means algorithm has a time complexity of ![Using K-means clustering](img/4351OS_07_06.jpg).
    There are several variations of this algorithm that vary on how the new mean values
    are calculated.
  prefs: []
  type: TYPE_NORMAL
- en: We will now demonstrate how we can implement the *K*-means algorithm in pure
    Clojure, while using no external libraries. We begin by defining bits and pieces
    of the algorithm, which are then later combined to provide a basic visualization
    of the *K*-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can say that the distance between two numbers is the absolute difference
    between their values and this can be implemented as a `distance` function, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If we are given a number of mean values, we can calculate the closest mean
    from a given number by using a composition of the `distance` and `sort-by` functions,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate the `closest` function defined in the preceding code, we will
    first need to define some data, that is, a sequence of numbers and a couple of
    mean values, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use the `data` and `guessed-means` variables with the `closest`
    function and an arbitrary number, as shown in the following REPL output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the means `0` and `10`, the `closest` function returns `0` as the closest
    mean to `2`, and `10` as that for `9` and `100`. Thus, a set of data points can
    be grouped by the means, which are closest to them. We can implement a function
    that implements this grouping operation using the `closest` and `group-by` functions
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `point-groups` function defined in the preceding code requires three arguments,
    namely the initial mean values, the collection of points to be grouped, and lastly
    a function that returns the distance of a point from a given mean. Note that the
    `group-by` function applies a function, which is passed as the first parameter,
    to a collection, which is then passed as the second parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply the `point-groups` function on the list of numbers represented
    by the `data` variable to group the given values by their distance from the guessed
    means, represented by `guessed-means` as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, the `point-groups` function partitions the
    sequence `data` into two groups. To calculate the new set of mean values from
    these groups of input values, we must calculate their average value, which can
    be implemented using the `reduce` and `count` functions, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We implement a function to apply the `average` function defined in the preceding
    code to the previous mean values and the map of groups returned by the `point-groups`
    function. We will do this with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `new-means` function defined in the preceding code, for each value in
    the previous mean values, we apply the `average` function to the points that are
    grouped by the mean value. Of course, the `average` function must be applied to
    the points of a given mean only if the mean has any points grouped by it. This
    is checked using the `contains?` function in the `new-means` function. We can
    inspect the value returned by the `new-means` function on our sample data in the
    REPL, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding output, the new mean values are calculated as `(10/3
    55)` from the initial mean values `(0 10)`. To implement the *K*-means algorithm,
    we must apply the `new-means` function iteratively over the new mean values returned
    by it. This iteration can be performed using the `iterate` function, which requires
    a function that takes a single argument to be passed to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a function to use with the `iterate` function by currying the
    `new-means` function over the old mean values passed to it, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `iterate-means` function defined in the preceding code returns a function
    that calculates the new mean values from a given set of initial mean values, as
    shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding output, the mean value is observed to change on applying
    the function returned by the `iterate-means` function a couple of times. This
    returned function can be passed to the `iterate` function and we can inspect the
    iterated mean values using the `take` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s observed that the mean value changes only in the first three iterations
    and converges to the value `(37/6 10)` for the sample data that we have defined.
    The termination condition of the *K*-means algorithm is the convergence of the
    mean values and thus we must iterate over the values returned by the `iterate-means`
    function until the returned mean value does not differ from the previously returned
    mean value. Since the `iterate` function lazily returns an infinite sequence,
    we must implement a function that limits this sequence by the convergence of the
    elements in the sequence. This behavior can be implemented by lazy realization
    using the `lazy-seq` and `seq` functions as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `take-while-unstable` function defined in the preceding code splits a lazy
    sequence into its head and tail terms and then compares the first element of the
    sequence with the first element of the tail of the sequence to return an empty
    list, or `nil`, if the two elements are equal. However, if they are not equal,
    the `take-while-unstable` function is invoked again on the tail of the sequence.
    Note the use of the `if-let` macro, which is simply a `let` form with an `if`
    expression as its body to check if the sequence `sq` is empty. We can inspect
    the value returned by the `take-while-unstable` function in the REPL as shown
    in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the final mean value we have calculated, we can determine the clusters
    of input values using the `vals` function on the map returned by the `point-groups`
    function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `vals` function returns all the values in a given map as a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `k-cluster` function defined in the preceding code produces the final clusters
    of input values returned by the *K*-means algorithm. We can apply the `k-cluster`
    function on the final mean value `(37/6 101)` to return the final clusters of
    input values, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To visualize the change in the clusters of input values, we can apply the `k-cluster`
    function on the sequence of values returned by composing the `iterate` and `iterate-means`
    functions. We must limit this sequence by convergence of the values in all clusters
    and this can be done using the `take-while-unstable` function, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can refactor the preceding expression into a function that requires only
    the initial set of guessed mean values by binding the `iterate-means` function
    to the sample data. The functions used to calculate the distance of a given input
    value from a mean value and the average mean value from a set of input values
    are as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can bind the `k-groups` function defined in the preceding code with our
    sample data and the `distance` and `average` functions, which operate on numeric
    values as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can apply the `grouper` function on any arbitrary set of mean values
    to visualize the changes in the clusters over the various iterations of the *K*-means
    algorithm, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned earlier, if the number of mean values is greater than the number
    of inputs, we end up with a number of clusters equal to the number of input values,
    in which each cluster contains a single input value. This can be verified in the
    REPL using the `grouper` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can extend the preceding implementation to apply to vector values and not
    just numeric values, by changing the `distance` and `average` distance, which
    are parameters to the `k-groups` function. We can implement these two functions
    for vector values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `vec-distance` function defined in the preceding code implements the squared
    Euclidean distance between two vector values as the sum of the squared differences
    between the corresponding elements in the two vectors. We can also calculate the
    average of some vector values by adding them together and dividing each resulting
    element by the number of vectors that were added together, as shown in the `vec-average`
    function defined in the preceding code. We can inspect the returned values of
    these functions in the REPL as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now define some of the following vector values to use as sample data
    for our clustering algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use the `k-groups` function with the `vector-data`, `vec-distance`,
    and `vec-average` variables to print the various clusters iterated through to
    produce the final set of clusters, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Another improvement we can add to this implementation is updating identical
    mean values by the `new-means` function. If we pass a list of identical mean values
    to the `new-means` function, both the mean values will get updated. However, in
    the classic *K*-means algorithm, only one mean from two identical mean values
    is updated. This behavior can be verified in the REPL, by passing a list of identical
    means such as `''(0 0)` to the `new-means` function, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can avoid this problem by checking the number of occurrences of a given
    mean in the set of mean values and only updating a single mean value if multiple
    occurrences of it are found. We can implement this using the `frequencies` function,
    which returns a map with keys as elements from the original collection passed
    to the `frequencies` function and values as the frequencies of occurrences of
    these elements. We can thus redefine the `new-means` function, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `update-seq` function defined in the preceding code applies a function
    `f` to the elements in a sequence `sq`. The function `f` is only applied to a
    single element if the element is repeated in the sequence. We can now observe
    that only a single mean value changes when we apply the redefined `new-means`
    function on the sequence of identical means `''(0 0)`, as shown in the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'A consequence of the preceding redefinition of the `new-means` function is
    that the `k-groups` function now produces identical clusters when applied to both
    distinct and identical initial mean values, such as `''(0 1)` and `''(0 0)`, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This new behavior of the `new-means` function with respect to identical initial
    mean values also extends to vector values as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In conclusion, the `k-cluster` and `k-groups` functions defined in the preceding
    example depict how *K*-means clustering can be implemented in idiomatic Clojure.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering data using clj-ml
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `clj-ml` library provides several implementations of clustering algorithms
    derived from the Java Weka library. We will now demonstrate how we can use the
    `clj-ml` library to build a *K*-means clusterer.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `clj-ml` and Incanter libraries can be added to a Leiningen project by
    adding the following dependency to the `project.clj` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'For the example that will follow, the namespace declaration should look similar
    to the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: For the examples that use the `clj-ml library` in this chapter, we will use
    the **Iris** dataset from the Incanter library as our training data. This dataset
    is essentially a sample of 150 flowers and four feature variables that are measured
    for these samples. The features of the flowers that are measured in the Iris dataset
    are the width and length of petal and sepal of the flowers. The sample values
    are distributed over three species or categories, namely Virginica, Setosa, and
    Versicolor. The data is available as a ![Clustering data using clj-ml](img/4351OS_07_07.jpg)
    sized matrix in which the species of a given flower is represented as the last
    column in this matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can select the features from the Iris dataset as a vector using the `get-dataset`,
    `sel`, and `to-vector` functions from the Incanter library, as shown in the following
    code. We can then convert this vector into a `clj-ml` dataset using the `make-dataset`
    function from the `clj-ml` library. This is done by passing the keyword names
    of the feature values as a template to the `make-dataset` function as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can print the `iris-dataset` variable defined in the preceding code in the
    REPL to give us some information on what it contains as shown in the following
    code and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create a clusterer using the `make-clusterer` function from the `clj-ml.clusterers`
    namespace. We can specify the type of cluster to create as the first argument
    to the `make-cluster` function. The second optional argument is a map of options
    to be used to create the specified clusterer. We can train a given clusterer using
    the `cluster-build` function from the `clj-ml` library. In the following code,
    we create a new *K*-means clusterer using the `make-clusterer` function with the
    `:k-means` keyword and define a simple helper function to help train this clusterer
    with any given dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train-clusterer` function can be applied to the clusterer instance defined
    by the `k-means-clusterer` variable and the sample data represented by the `iris-dataset`
    variable, as shown in the following code and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding output, the trained clusterer contains `61` values
    in the first cluster (cluster `0`), `50` values in the second cluster (cluster
    `1`), and `39` values in the third cluster (cluster `2`). The preceding output
    also gives us some information about the mean values of the individual features
    in the training data. We can now predict the classes of the input data using the
    trained clusterer and the `clusterer-cluster` function as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The `clusterer-cluster` function uses the trained clusterer to return a new
    dataset that contains an additional fifth attribute that represents the category
    of a given sample value. As shown in the preceding code, this new attribute has
    the values `0`, `1`, and `2`, and the sample values also contain valid values
    for this new feature. In conclusion, the `clj-ml` library provides a good framework
    for working with clustering algorithms. In the preceding example, we created a
    *K*-means clusterer using the `clj-ml` library.
  prefs: []
  type: TYPE_NORMAL
- en: Using hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hierarchical clustering** is another method of cluster analysis in which
    input values from the training data are grouped together into a hierarchy. The
    process of creating the hierarchy can be done in a top-down approach, where in
    all observations are first part of a single cluster and are then divided into
    smaller clusters. Alternatively, we can group the input values using a bottom-up
    methodology, where each cluster is initially a sample value from the training
    data and these clusters are then combined together. The former top-down approach
    is termed as **divisive clustering** and the later bottom-up method is called
    **agglomerative clustering**.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in agglomerative clustering, we combine clusters into larger clusters,
    whereas we divide clusters into smaller ones in divisive clustering. In terms
    of performance, modern implementations of agglomerative clustering algorithms
    have a time complexity of ![Using hierarchical clustering](img/4351OS_07_08.jpg),
    while those of divisive clustering have much higher complexity of ![Using hierarchical
    clustering](img/4351OS_07_09.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have six input values in our training data. In the following illustration,
    assume that these input values are positioned according to some two-dimensional
    metric to measure the overall value of a given input value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using hierarchical clustering](img/4351OS_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can apply agglomerative clustering on these input values to produce the
    following hierarchy of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using hierarchical clustering](img/4351OS_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The values *b* and *c* are observed to be the closest to each other in the spatial
    distribution and are hence grouped into a cluster. Similarly, the nodes *d* and
    *e* are also grouped into another cluster. The final result of hierarchically
    clustering the input value is a single binary tree or a **dendogram** of the sample
    values. In effect, clusters such as *bc* and *def* are added to the hierarchy
    as binary subtrees of values or of other clusters. Although this process tends
    to appear very simple in a two-dimensional space, the solution to the problem
    of determining the distance and hierarchy between input values is much less trivial
    when applied over several dimensions of features.
  prefs: []
  type: TYPE_NORMAL
- en: In both agglomerative and divisive clustering techniques, the similarity between
    input values from the sample data has to be calculated. This can be done by measuring
    the distance between two sets of input values, grouping them into clusters using
    the calculated distance, and then determining the linkage or similarity between
    two clusters of input values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of the distance metric in a hierarchical clustering algorithm will
    determine the shape of the clusters that are produced by the algorithm. A couple
    of commonly used measures of the distance between two input vectors ![Using hierarchical
    clustering](img/4351OS_07_12.jpg) and ![Using hierarchical clustering](img/4351OS_07_13.jpg)
    are the Euclidean distance ![Using hierarchical clustering](img/4351OS_07_14.jpg)
    and the squared Euclidean distance ![Using hierarchical clustering](img/4351OS_07_15.jpg),
    which can be formally expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using hierarchical clustering](img/4351OS_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another commonly used metric of the distance between input values is the maximum
    distance ![Using hierarchical clustering](img/4351OS_07_17.jpg), which calculates
    the maximum absolute difference of corresponding elements in two given vectors.
    This function can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using hierarchical clustering](img/4351OS_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The second aspect of a hierarchical clustering algorithm is the linkage criteria,
    which is an effective measure of similarity or dissimilarity between two clusters
    of input values. Two commonly used methods of determining the linkage between
    two input values are **complete linkage clustering** and **single linkage clustering**.
    Both of these methods are forms of agglomerative clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'In agglomerative clustering, two input values or clusters with the shortest
    distance metric are combined into a new cluster. Of course, the definition of
    "shortest distance" is what is unique in any agglomerative clustering technique.
    In complete linkage clustering, input values farthest from each other are used
    to determine the grouping. Hence, this method is also termed as **farthest neighbor
    clustering**. This metric of the distance ![Using hierarchical clustering](img/4351OS_07_19.jpg)
    between two values can be formally expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using hierarchical clustering](img/4351OS_07_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the function ![Using hierarchical clustering](img/4351OS_07_21.jpg)
    is the selected metric of distance between two input vectors. Complete linkage
    clustering will essentially group together values or clusters that have the maximum
    value of the distance metric ![Using hierarchical clustering](img/4351OS_07_22.jpg).
    This operation of grouping together clusters is repeated until a single cluster
    is produced.
  prefs: []
  type: TYPE_NORMAL
- en: 'In single linkage clustering, values that are nearest to each other are grouped
    together. Hence, single linkage clustering is also called **nearest neighbor clustering**.
    This can be be formally stated using the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using hierarchical clustering](img/4351OS_07_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Another popular hierarchical clustering technique is the **Cobweb algorithm**.
    This algorithm is a form of **conceptual clustering**, in which a concept is created
    for each cluster produced by the clustering method used. By the term "concept",
    we mean a concise formal description of the data clustered together. Interestingly,
    conceptual clustering is closely related to decision tree learning, which we already
    discussed in [Chapter 3](ch03.html "Chapter 3. Categorizing Data"), *Categorizing
    Data*. The Cobweb algorithm groups all clusters into a **classification tree**,
    in which each node contains a formal summary of the values or clusters that are
    its child nodes. This information can then be used to determine and predict the
    category of an input value with some missing features. In this sense, this technique
    can be used when some of the samples in the test data have missing or unknown
    features.
  prefs: []
  type: TYPE_NORMAL
- en: We now demonstrate a simple implementation of hierarchical clustering. In this
    implementation, we take a slightly different approach where we embed part of the
    required functionality into the standard vector data structure provided by the
    Clojure language.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the upcoming example, we require the `clojure.math.numeric-tower` library
    that can be added to a Leiningen project by adding the following dependency to
    the `project.clj` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The namespace declaration for the example should look similar to the following
    declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For this implementation, we will use the Euclidean distance between two points
    as a distance metric. We can calculate this distance from the sum of squares of
    the elements in an input vector, which can be computed using a composition of
    the `reduce` and `map` functions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The `sum-of-squares` function defined in the preceding code will be used to
    determine the distance metric. We will define two protocols that abstract the
    operations we perform on a particular data type. From an engineering perspective,
    these two protocols could be combined into a single protocol, since both the protocols
    will be used in combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we use the following two protocols for this example for the sake of
    clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The `each` function defined in the `Each` protocol applies a given operation
    `op` on corresponding elements in two collections `v` and `w`. The `each` function
    is quite similar to the standard `map` function, but `each` allows the data type
    of `v` to decide how to apply the function `op`. The `distance` function defined
    in the `Distance` protocol calculates the distance between any two collections
    `v` and `w`. Note that we use the generic term "collection" since we are dealing
    with abstract protocols and not concrete implementations of the functions of these
    protocols. For this example, we will implement the preceding protocols as part
    of the vector data type. Of course, these protocols could also be extended to
    other data types such as sets and maps.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will implement single linkage clustering as the linkage
    criteria. First, we will have to define a function to determine the two closest
    vectors from a set of vector values. To do this, we can apply the `min-key` function,
    which returns the key with the least associated value in a collection, on a vector.
    Interestingly, this is possible in Clojure since we can treat a vector as a map
    with the index values of the various elements in the vector as its keys. We will
    implement this with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The `closest-vectors` function defined in the preceding code determines all
    possible combinations of the indexes of the vector `vs` using the `for` form.
    Note that the vector `vs` is a vector of vectors. The `distance` function is then
    applied over the values of the possible index combinations and these distances
    are then compared using the `min-key` function. The function finally returns the
    index values of the two inner vector values that have the least distance from
    each other, thus implementing single linkage clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also need to calculate the mean value of two vectors that have to be
    clustered together. We can implement this using the `each` function we had previously
    defined in the `Each` protocol and the `reduce` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The `centroid` function defined in the preceding code will calculate the mean
    value of a sequence of vector values. Note the use of the `double` function to
    ensure that the value returned by the `centroid` function is a double-precision
    number.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now implement the `Each` and `Distance` protocols as part of the vector
    data type, which is fully qualified as `clojure.lang.PersistentVector`. This is
    done using the `extend-type` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The `each` function is implemented such that it applies the `op` operation to
    each element in the `v` vector and a second argument `w`. The `w` parameter could
    be either a vector or a number. In case `w` is a number, we simply map the function
    `op` over `v` and the repeated value of the number `w`. If `w` is a vector, we
    pad the smaller vector with `0` values using the `lazy-cat` function and map `op`
    over the two vectors. Also, we wrap the entire expression in a `vec` function
    to ensure that the value returned is always a vector.
  prefs: []
  type: TYPE_NORMAL
- en: The `distance` function is implemented as the Euclidean distance between two
    vector values `v` and `w` using the `sum-of-squares` function that we previously
    defined and the `sqrt` function from the `clojure.math.numeric-tower` namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have all the pieces needed to implement a function that performs hierarchical
    clustering on vector values. We can implement hierarchical clustering primarily
    using the centroid and `closest-vectors` functions that we had previously defined,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We can pass a vector of maps to the `h-cluster` function defined in the preceding
    code. Each map in this vector contains a vector as the value of the keyword `:vec`.
    The `h-cluster` function combines all the vector values from the `:vec` keywords
    in these maps and determines the two closest vectors using the `closest-vectors`
    function. Since the value returned by the `closest-vectors` function is a vector
    of two index values, we determine all the vectors with indexes other than the
    two index values returned by the `closest-vectors` function. This is done using
    a special form of the `for` macro that allows a conditional clause to be specified
    with the `:when` key parameter. The mean value of the two closest vectors is then
    calculated using the `centroid` function. A new map is created using the mean
    value and then added to the original vector to replace the two closest vector
    values. The process is repeated until the vector contains a single cluster, using
    the `loop` form. We can inspect the behavior of the `h-cluster` function in the
    REPL as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: When applied to three vector values `[1 2 3]`, `[3 4 5]`, and `[7 9 9]`, as
    shown in the preceding code, the `h-cluster` function groups the vectors `[1 2
    3]` and `[3 4 5]` into a single cluster. This cluster has the mean value of `[2.0
    3.0 4.0]`, which is calculated from the vectors `[1 2 3]` and `[3 4 5]`. This
    new cluster is then grouped with the vector `[7 9 9]` in the next iteration, thus
    producing a single cluster with a mean value of `[4.5 6.0 6.5]`. In conclusion,
    the `h-cluster` function can be used to hierarchically cluster vector values into
    a single hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: The `clj-ml` library provides an implementation of the Cobweb hierarchical clustering
    algorithm. We can instantiate such a clusterer using the `make-clusterer` function
    with the `:cobweb` argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The clusterer defined by the `h-clusterer` variable shown in the preceding
    code can be trained using the `train-clusterer` function and `iris-dataset` dataset,
    which we had previously defined, as follows: The `train-clusterer` function and
    `iris-dataset` can be implemented as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding REPL output, the Cobweb clustering algorithm partitions
    the input data into two clusters. One cluster has 96 samples and the other cluster
    has 54 samples, which is quite a different result compared to the *K*-means clusterer,
    we had previously used. In summary, the `clj-ml` library provides an easy-to-use
    implementation of the Cobweb clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Using Expectation-Maximization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Expectation-Maximization** (**EM**) algorithm is a probabilistic approach
    for determining a clustering model that fits the supplied training data. This
    algorithm determines the **Maximum Likelihood Estimate** (**MLE**) of the parameters
    of a formulated clustering model (for more information, refer to *Maximum likelihood
    theory and applications for distributions generated when observing a function
    of an exponential family variable*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to determine the probability of a coin toss being a head or
    a tail. If we flip the coin ![Using Expectation-Maximization](img/4351OS_07_24.jpg)
    times, we end up with ![Using Expectation-Maximization](img/4351OS_07_25.jpg)
    occurrences of heads and ![Using Expectation-Maximization](img/4351OS_07_26.jpg)
    occurrences of tails. We can estimate the actual probability of occurrence of
    a head ![Using Expectation-Maximization](img/4351OS_07_27.jpg) as the ratio of
    the number of occurrences of a head to the total number of coin tosses performed,
    using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using Expectation-Maximization](img/4351OS_07_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The probability ![Using Expectation-Maximization](img/4351OS_07_29.jpg) defined
    in the preceding equation is the MLE of the probability ![Using Expectation-Maximization](img/4351OS_07_30.jpg).
    In the context of machine learning, the MLE can be maximized to determine the
    probability of occurrence of a given class or category. However, this estimated
    probability may not be statistically distributed in a well-defined way over the
    available training data, which makes it hard to determine the MLE efficiently.
    The problem is simplified by introducing a set of hidden values to account for
    the unobserved values in the training data. The hidden values are not directly
    measured from the data, but are determined from factors that influence the data.
    The likelihood function of the parameters ![Using Expectation-Maximization](img/4351OS_07_31.jpg)
    for a given set of observed values ![Using Expectation-Maximization](img/4351OS_07_12.jpg)
    and a set of hidden values ![Using Expectation-Maximization](img/4351OS_07_32.jpg)
    is defined as the probability of occurrence of ![Using Expectation-Maximization](img/4351OS_07_12.jpg)
    and ![Using Expectation-Maximization](img/4351OS_07_32.jpg) for a given set of
    parameters ![Using Expectation-Maximization](img/4351OS_07_31.jpg). The likelihood
    is mathematically written as ![Using Expectation-Maximization](img/4351OS_07_33.jpg),
    and can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using Expectation-Maximization](img/4351OS_07_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The EM algorithm comprises two steps—the expectation step and the maximization
    step. In the expectation step, we calculate the expected value of the **log likelihood**
    function. This step determines a metric ![Using Expectation-Maximization](img/4351OS_07_35.jpg),
    which must be maximized in the next step, that is, the maximization step of the
    algorithm. These two steps can be formally summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using Expectation-Maximization](img/4351OS_07_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the value of ![Using Expectation-Maximization](img/4351OS_07_31.jpg)
    that maximizes the value of the function *Q* is iteratively calculated until it
    converges to a particular value. The term ![Using Expectation-Maximization](img/4351OS_07_37.jpg)
    represents the estimated parameters in the ![Using Expectation-Maximization](img/4351OS_07_38.jpg)
    iteration of the algorithm. Also, the term ![Using Expectation-Maximization](img/4351OS_07_39.jpg)
    is the expected value of the log likelihood function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `clj-ml` library also provides an EM clusterer. We can create an EM clusterer
    using the `make-clusterer` function with the `:expectation-maximization` keyword
    as its argument, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Note that we must also specify the number of clusters to produce as an option
    to the `make-clusterer` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can train the clusterer defined by the `em-clusterer` variable in the preceding
    code using the `train-clusterer` function and `iris-dataset` dataset, which we
    had previously defined, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding output, the EM clusterer partitions the given dataset
    into three clusters in which the clusters are distributed as approximately 41
    percent, 25 percent, and 35 percent of the samples in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Using SOMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned earlier in [Chapter 4](ch04.html "Chapter 4. Building Neural
    Networks"), *Building Neural Networks*, SOMs can be used to model unsupervised
    machine learning problems such as clustering (for more information, refer to *Self-organizing
    Maps as Substitutes for K-Means Clustering*). To quickly recap, an SOM is a type
    of ANN that maps input values with a high number of dimensions to a low-dimensional
    output space. This mapping preserves patterns and topological relations between
    the input values. The neurons in the output space of an SOM will have higher activation
    values for input values that are spatially close to each other. Thus, SOMs are
    a good solution for clustering input data with a large number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The Incanter library provides a concise SOM implementation that we can use to
    cluster the input variables from the Iris dataset. We will demonstrate how we
    can use this SOM implementation for clustering in the example that will follow.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Incanter library can be added to a Leiningen project by adding the following
    dependency to the `project.clj` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'For the upcoming example, the namespace declaration should look similar to
    the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We first define the sample data to cluster using the `get-dataset`, `sel`,
    and `to-matrix` functions from the Incanter library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The `iris-features` variable defined in the preceding code is in fact a ![Using
    SOMs](img/4351OS_07_40.jpg) sized matrix that represents the values of the four
    input variables that we have selected from the Iris dataset. Now, we can use the
    `som-batch-train` function from the `incanter.som` namespace to create and train
    an SOM using these selected features, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The `som` variable defined is actually a map with several key-value pairs.
    The `:dims` key in this map contains a vector that represents the dimensions in
    the lattice of neurons in the trained SOM, as shown in the following code and
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, we can say that the neural lattice of the trained SOM is a ![Using SOMs](img/4351OS_07_41.jpg)
    matrix. The `:sets` key of the map represented by the `som` variable gives us
    the positional grouping of the various indexes of the input values in the lattice
    of neurons of the SOM, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding REPL output, the input data is partitioned into three
    clusters. We can calculate the mean values of each feature using the `mean` function
    from the `incanter.stats` namespace as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We can implement a function to plot these mean values using the `xy-plot`,
    `add-lines`, and `view` functions from the Incanter library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The following linear plot is produced on calling the `plot-means` function
    defined in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using SOMs](img/4351OS_07_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding plot gives us an idea of the mean values of the various features
    in the three clusters determined by the SOM. The plot shows that two of the clusters
    (*Cluster 0* and *Cluster 1*) have similar features. The third cluster, however,
    has significantly different mean values for these set of features and is thus
    shown as a different shape in the plot. Of course, this plot doesn't give us much
    information about the distribution or variance of input values around these mean
    values. To visualize these features, we need to somehow transform the number of
    dimensions of the input data to two or three dimensions, which can be easily visualized.
    We will discuss more on this concept of reducing the number of features in the
    training data in the next section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also print the clusters and the actual categories of the input values
    using the `frequencies` and `sel` functions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We can call the function `print-clusters` defined in the preceding code to
    produce the following REPL output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding output, the `virginica` and `setosa` species seem
    to be appropriately classified into two clusters. However, the cluster containing
    the input values of the `versicolor` species also contains 27 samples of the `virginica`
    species. This problem could be remedied by using more sample data to train the
    SOM or by modeling a higher number of features.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, the Incanter library provides us with a concise implementation
    of an SOM, which we can train using the Iris dataset as shown in the preceding
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing dimensions in the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to easily visualize the distribution of some unlabeled data in which
    the input values have multiple dimensions, we must reduce the number of feature
    dimensions to two or three. Once we have reduced the number of dimensions of the
    input data to two or three dimensions, we can trivially plot the data to provide
    a more understandable visualization of it. This process of reducing the number
    of dimensions in the input data is known as **dimensionality reduction**. As this
    process reduces the total number of dimensions used to represent the sample data,
    it is also useful for data compression.
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**) is a form of dimensionality reduction
    in which the input variables in the sample data are transformed into linear uncorrelated
    variables (for more information, refer to *Principal Component Analysis*). These
    transformed features are called the **principal components** of the sample data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA uses a covariance matrix and a matrix operation called **Singular Value
    Decomposition** (**SVD**) to calculate the principal components of a given set
    of input values. The covariance matrix denoted as ![Reducing dimensions in the
    data](img/4351OS_07_43.jpg), can be determined from a set of input vectors ![Reducing
    dimensions in the data](img/4351OS_07_12.jpg) with ![Reducing dimensions in the
    data](img/4351OS_07_44.jpg) samples as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reducing dimensions in the data](img/4351OS_07_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The covariance matrix is generally calculated from the input values after mean
    normalization, which is simply ensuring that each feature has a zero mean value.
    Also, the features could be scaled before determining the covariance matrix. Next,
    the SVD of the covariance matrix is determined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reducing dimensions in the data](img/4351OS_07_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: SVD can be thought of as factorization of a matrix ![Reducing dimensions in
    the data](img/4351OS_07_47.jpg) with size ![Reducing dimensions in the data](img/4351OS_07_48.jpg)
    into three matrices ![Reducing dimensions in the data](img/4351OS_07_49.jpg),
    ![Reducing dimensions in the data](img/4351OS_07_50.jpg), and ![Reducing dimensions
    in the data](img/4351OS_07_51.jpg). The matrix ![Reducing dimensions in the data](img/4351OS_07_49.jpg)
    has a size of ![Reducing dimensions in the data](img/4351OS_07_52.jpg), the matrix
    ![Reducing dimensions in the data](img/4351OS_07_50.jpg) has a size of ![Reducing
    dimensions in the data](img/4351OS_07_48.jpg), and the matrix ![Reducing dimensions
    in the data](img/4351OS_07_51.jpg) has a size of ![Reducing dimensions in the
    data](img/4351OS_07_53.jpg). The matrix ![Reducing dimensions in the data](img/4351OS_07_47.jpg)
    actually represents the ![Reducing dimensions in the data](img/4351OS_07_54.jpg)
    input vectors with ![Reducing dimensions in the data](img/4351OS_07_55.jpg) dimensions
    in the sample data. The matrix ![Reducing dimensions in the data](img/4351OS_07_50.jpg)
    is a diagonal matrix and is called the **singular value** of the matrix ![Reducing
    dimensions in the data](img/4351OS_07_47.jpg), and the matrices ![Reducing dimensions
    in the data](img/4351OS_07_49.jpg) and ![Reducing dimensions in the data](img/4351OS_07_51.jpg)
    are called the **left and right singular vectors** of ![Reducing dimensions in
    the data](img/4351OS_07_47.jpg), respectively. In the context of PCA, the matrix
    ![Reducing dimensions in the data](img/4351OS_07_50.jpg) is termed as the **reduction
    component** and the matrix ![Reducing dimensions in the data](img/4351OS_07_49.jpg)
    is termed as the **rotation component** of the sample data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PCA algorithm to reduce the ![Reducing dimensions in the data](img/4351OS_07_55.jpg)
    dimensions in the ![Reducing dimensions in the data](img/4351OS_07_54.jpg) input
    vectors to ![Reducing dimensions in the data](img/4351OS_07_56.jpg) dimensions
    can be summarized using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the covariance matrix ![Reducing dimensions in the data](img/4351OS_07_43.jpg)
    from the input vectors ![Reducing dimensions in the data](img/4351OS_07_12.jpg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the matrices ![Reducing dimensions in the data](img/4351OS_07_49.jpg),
    ![Reducing dimensions in the data](img/4351OS_07_50.jpg), and ![Reducing dimensions
    in the data](img/4351OS_07_51.jpg) by applying SVD on the covariance matrix ![Reducing
    dimensions in the data](img/4351OS_07_43.jpg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the ![Reducing dimensions in the data](img/4351OS_07_52.jpg) matrix ![Reducing
    dimensions in the data](img/4351OS_07_49.jpg), select the first ![Reducing dimensions
    in the data](img/4351OS_07_56.jpg) columns to produce the matrix ![Reducing dimensions
    in the data](img/4351OS_07_57.jpg), which is termed as the **reduced left singular
    vector** or **reduced rotation matrix** of the matrix ![Reducing dimensions in
    the data](img/4351OS_07_43.jpg). This matrix represents the ![Reducing dimensions
    in the data](img/4351OS_07_56.jpg) principal components of the sample data and
    will have a size of ![Reducing dimensions in the data](img/4351OS_07_58.jpg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the vectors with ![Reducing dimensions in the data](img/4351OS_07_56.jpg)
    dimensions, denoted by ![Reducing dimensions in the data](img/4351OS_07_32.jpg),
    as follows:![Reducing dimensions in the data](img/4351OS_07_59.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the input to the PCA algorithm is the set of input vectors ![Reducing
    dimensions in the data](img/4351OS_07_12.jpg) from the sample data after mean
    normalization and feature scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Since the matrix ![Reducing dimensions in the data](img/4351OS_07_57.jpg) calculated
    in the preceding steps has ![Reducing dimensions in the data](img/4351OS_07_56.jpg)
    columns, the matrix ![Reducing dimensions in the data](img/4351OS_07_32.jpg) will
    have a size of ![Reducing dimensions in the data](img/4351OS_07_60.jpg), which
    represents the ![Reducing dimensions in the data](img/4351OS_07_54.jpg) input
    vectors in ![Reducing dimensions in the data](img/4351OS_07_56.jpg) dimensions.
    We should note that a lower value of the number of dimensions ![Reducing dimensions
    in the data](img/4351OS_07_56.jpg) could result in a higher loss of variance in
    the data. Hence, we should choose ![Reducing dimensions in the data](img/4351OS_07_56.jpg)
    such that only a small fraction of the variance is lost.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original input vectors ![Reducing dimensions in the data](img/4351OS_07_12.jpg)
    can be recreated from the matrix ![Reducing dimensions in the data](img/4351OS_07_32.jpg)
    and the reduced left singular vector ![Reducing dimensions in the data](img/4351OS_07_57.jpg)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reducing dimensions in the data](img/4351OS_07_61.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Incanter library includes some functions to perform PCA. In the example
    that will follow, we will use PCA to provide a better visualization of the Iris
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The namespace declaration of the upcoming example should look similar to the
    following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We first define the training data using the `get-dataset`, `to-matrix`, and
    `sel` functions, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the previous example, we will use the first four columns of the Iris
    dataset as sample data for the input variables of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA is performed by the `principal-components` function from the `incanter.stats`
    namespace. This function returns a map that contains the rotation matrix ![Reducing
    dimensions in the data](img/4351OS_07_49.jpg) and the reduction matrix ![Reducing
    dimensions in the data](img/4351OS_07_50.jpg) from PCA, which we described earlier.
    We can select columns from the reduction matrix of the input data using the `sel`
    function as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, the rotation matrix of the PCA of the input
    data can be fetched using the `:rotation` keyword on the value returned by the
    `principal-components` function. We can now calculate the reduced features *Z*
    using the reduced rotation matrix and the original matrix of features represented
    by the `iris-features` variable, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The reduced features can then be visualized by selecting the first two columns
    of the `reduced-features` matrix and plotting them using the `scatter-plot` function,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot is generated on calling the `plot-reduced-features` function
    defined in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reducing dimensions in the data](img/4351OS_07_62.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The scatter plot illustrated in the preceding diagram gives us a good visualization
    of the distribution of the input data. The blue and green clusters in the preceding
    plot are shown to have similar values for the given set of features. In summary,
    the Incanter library supports PCA, which allows for the easy visualization of
    some sample data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explored several clustering algorithms that can be used
    to model some unlabeled data. The following are some of the other points that
    we have covered:'
  prefs: []
  type: TYPE_NORMAL
- en: We explored the *K*-means algorithm and hierarchical clustering techniques while
    providing sample implementations of these methods in pure Clojure. We also described
    how we can leverage these techniques through the `clj-ml` library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discussed the EM algorithm, which is a probabilistic clustering technique,
    and also described how we can use the `clj-ml` library to build an EM clusterer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also explored how we can use SOMs to fit clustering problems with a high
    number of dimensions. We also demonstrated how we can use the Incanter library
    to build an SOM that can be used for clustering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we studied dimensionality reduction and PCA, and how we can use PCA
    to provide a better visualization of the Iris dataset using the Incanter library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following chapter, we will explore the concepts of anomaly detection
    and recommendation systems using machine learning techniques.
  prefs: []
  type: TYPE_NORMAL
