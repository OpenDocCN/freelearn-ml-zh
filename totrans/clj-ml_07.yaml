- en: Chapter 7. Clustering Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 聚类数据
- en: We will now shift our focus to **unsupervised learning**. In this chapter, we
    will study several **clustering** algorithms, or **clusterers**, and how we can
    implement them in Clojure. We will also demonstrate several Clojure libraries
    that provide implementations of clustering algorithms. Towards the end of the
    chapter, we explore will **dimensionality** **reduction** and how it can be used
    to provide an understandable visualization of the supplied sample data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将关注点转向**无监督学习**。在本章中，我们将研究几种**聚类**算法，或称为**聚类器**，以及如何在Clojure中实现它们。我们还将演示几个提供聚类算法实现的Clojure库。在章节的末尾，我们将探讨**降维**及其如何被用来提供对提供的样本数据的可理解的可视化。
- en: Clustering or **cluster analysis** is basically a method of grouping data or
    samples together. As a form of unsupervised learning, a clustering model is trained
    using unlabeled data, by which we mean the samples in the training data will not
    contain the class or category of the input values. Rather, the training data does
    not describe values for the output variable of a given set of inputs. A clustering
    model must determine similarities between several input values and infer the classes
    of these input values on its own. The sample values can thus be partitioned into
    a number of clusters using such a model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类或**聚类分析**基本上是一种将数据或样本分组的方法。作为一种无监督学习形式，聚类模型使用未标记数据进行训练，这意味着训练数据中的样本将不包含输入值的类别或类别。相反，训练数据不描述给定输入集的输出变量的值。聚类模型必须确定几个输入值之间的相似性，并自行推断这些输入值的类别。因此，可以使用这种模型将样本值划分为多个簇。
- en: There are several practical applications of clustering in real-world problems.
    Clustering is often used in image analysis, image segmentation, software evolution
    systems, and social network analysis. Outside the domain of computer science,
    clustering algorithms are used in biological classification, gene analysis, and
    crime analysis.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类在现实世界问题中有几种实际应用。聚类常用于图像分析、图像分割、软件演化系统和社交网络分析。在计算机科学领域之外，聚类算法被用于生物分类、基因分析和犯罪分析。
- en: There are several clustering algorithms that have been published till date.
    Each of these algorithms has a unique notion of how a cluster is defined and how
    input values are combined into new clusters. Unfortunately, there is no given
    solution for any clustering problem and each algorithm must be evaluated on a
    trial-and-error basis to determine which model is best suited for the supplied
    training data. Of course, this is one of the aspects of unsupervised learning,
    in the sense that there is no definite way to say that a given solution is the
    best fit for any given data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，已经发表了多种聚类算法。每种算法都有其独特的关于如何定义簇以及如何将输入值组合成新簇的概念。不幸的是，对于任何聚类问题都没有给出解决方案，每个算法都必须通过试错法来评估，以确定哪个模型最适合提供的训练数据。当然，这是无监督学习的一个方面，即没有明确的方法可以说一个给定的解决方案是任何给定数据的最佳匹配。
- en: This is due to the fact that the input data is unlabeled, and a simple yes/no
    based reward system to train cannot easily be inferred from data in which the
    output variable or class of the input values is unknown.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为输入数据未标记，并且无法从输出变量或输入值的类别未知的数据中推断出一个简单的基于奖励的yes/no训练系统。
- en: In this chapter, we will describe a handful of clustering techniques that can
    be applied on unlabeled data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将描述一些可以应用于未标记数据的聚类技术。
- en: Using K-means clustering
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用K-means聚类
- en: 'The **K-means clustering** algorithm is a clustering technique that is based
    on vector quantization (for more information, refer to "Algorithm AS 136: A K-Means
    Clustering Algorithm"). This algorithm partitions a number of sample vectors into
    *K* clusters and hence derives its name. In this section, we will study the nature
    and implementation of the K-means algorithm.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-means聚类**算法是一种基于矢量量化的聚类技术（更多信息，请参阅“算法AS 136：K-Means聚类算法”）。该算法将多个样本向量划分为*K*个簇，因此得名。在本节中，我们将研究K-means算法的性质和实现。'
- en: '**Quantization**, in signal processing, is the process of mapping a large set
    of values into a smaller set of values. For example, an analog signal can be quantized
    to 8 bits and the signal can be represented by 256 levels of quantization. Assuming
    that the bits represent values within the range of 0 to 5 volts, the 8-bit quantization
    allows a resolution of 5/256 volts per bit. In the context of clustering, quantization
    of input or output can be done for the following reasons:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**量化**，在信号处理中，是将一组大量值映射到一组较小值的过程。例如，一个模拟信号可以被量化为8位，信号可以用256个量化级别来表示。假设这些位代表0到5伏特的值，8位量化允许每位的分辨率为5/256伏特。在聚类的上下文中，输入或输出的量化可以出于以下原因进行：'
- en: To restrict the clustering to a finite set of clusters.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了将聚类限制在有限的聚类集合中。
- en: To accommodate a range of values in the sample data that need to have some level
    of tolerance while clustering is performed. This kind of flexibility is crucial
    in grouping together unknown or unexpected sample values.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了适应样本数据中的值范围，在聚类执行时需要有一定的容差。这种灵活性对于将未知或意外的样本值分组在一起至关重要。
- en: The gist of the algorithm can be concisely described as follows. The *K* mean
    values, or *centroids*, are first randomly initialized. The distance of each sample
    value from each centroid is then calculated. A sample value is grouped into a
    given centroid's cluster depending on which centroid has the minimum distance
    from the given sample. In a multidimensional space for multiple features or input
    values, the distance of a sample input vector is measured by **Euclidean distance**
    between the input vector and a given centroid. This phase of the algorithm is
    termed as the **assignment step**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的精髓可以简洁地描述如下。首先随机初始化K个均值值，或称为**质心**。然后计算每个样本值与每个质心的距离。根据哪个质心与给定样本的距离最小，将样本值分组到给定质心的聚类中。在多维空间中，对于多个特征或输入值，样本输入向量的距离是通过输入向量与给定质心之间的**欧几里得距离**来衡量的。这个算法阶段被称为**分配步骤**。
- en: The next phase in the *K*-means algorithm is the **update step**. The values
    of the centroids are adjusted based on the partitioned input values generated
    from the previous step. These two steps are then repeated until the difference
    between the centroid values in two consecutive iterations becomes negligible.
    Thus, the final result of the algorithm is the clusters or classes of each set
    of input values in the given training data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: K均值算法的下一阶段是**更新步骤**。根据前一步生成的分割输入值调整质心的值。然后，这两个步骤重复进行，直到连续两次迭代中质心值之间的差异变得可以忽略不计。因此，算法的最终结果是给定训练数据中每组输入值的聚类或类别。
- en: 'The iterations performed by the *K*-means algorithm can be illustrated using
    the following plots:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下图表来展示K均值算法的迭代过程：
- en: '![Using K-means clustering](img/4351OS_07_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![使用K均值聚类](img/4351OS_07_01.jpg)'
- en: Each of the plots depict the centroid and partitioned sample values produced
    by each iteration of the algorithm for a given set of input values. The clusters
    in a given iteration are shown in different colors in each plot. The final plot
    represents the final partitioned set of input values produced by the *K*-means
    algorithm.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图描绘了算法针对一组输入值在每次迭代中产生的质心和分割样本值。在每张图中，给定迭代的聚类以不同的颜色显示。最后一张图代表了K均值算法产生的最终分割输入值集。
- en: 'The optimization objective of the *K*-means clustering algorithm can be formally
    defined as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: K均值聚类算法的优化目标可以正式定义为如下：
- en: '![Using K-means clustering](img/4351OS_07_02.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![使用K均值聚类](img/4351OS_07_02.jpg)'
- en: In the optimization problem defined in the preceding equation, the terms ![Using
    K-means clustering](img/4351OS_07_03.jpg) represent the *K*-mean values around
    which the input values are clustered. The *K*-means algorithm minimizes the size
    of the clusters and also determines the mean values for which these clusters can
    be minimized in size.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面方程定义的优化问题中，![使用K均值聚类](img/4351OS_07_03.jpg)这些项代表围绕输入值聚类的K均值。K均值算法最小化聚类的尺寸，并确定可以最小化这些聚类尺寸的均值。
- en: This algorithm requires ![Using K-means clustering](img/4351OS_07_04.jpg) sample
    values and ![Using K-means clustering](img/4351OS_07_05.jpg) initial mean values
    as inputs. In the assignment step, the input values are assigned to clusters around
    the initial mean values supplied to the algorithm. In the later update step, the
    new mean values are calculated from the input values. In most implementations,
    the new mean values are calculated as the mean of all input values that belongs
    to a given cluster.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法需要![使用K-means聚类](img/4351OS_07_04.jpg)样本值和![使用K-means聚类](img/4351OS_07_05.jpg)初始均值作为输入。在分配步骤中，输入值被分配到算法提供的初始均值周围的聚类中。在后续的更新步骤中，从输入值计算新的均值。在大多数实现中，新的均值被计算为属于给定聚类的所有输入值的平均值。
- en: Most implementations initialize the ![Using K-means clustering](img/4351OS_07_05.jpg)
    initial mean values to some randomly chosen input values. This technique is called
    the **Forgy method** of random initialization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数实现将![使用K-means聚类](img/4351OS_07_05.jpg)初始均值设置为一些随机选择的输入值。这种技术被称为**Forgy方法**的随机初始化。
- en: The *K*-means algorithm is NP-hard when either the number of clusters *K* or
    the number of dimensions in the input data *d* is unbound. When both these values
    are fixed, the *K*-means algorithm has a time complexity of ![Using K-means clustering](img/4351OS_07_06.jpg).
    There are several variations of this algorithm that vary on how the new mean values
    are calculated.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当聚类数*K*或输入数据的维度*d*未定义时，*K*-means算法是NP难的。当这两个值都固定时，*K*-means算法的时间复杂度为![使用K-means聚类](img/4351OS_07_06.jpg)。该算法有几种变体，这些变体在计算新均值的方式上有所不同。
- en: We will now demonstrate how we can implement the *K*-means algorithm in pure
    Clojure, while using no external libraries. We begin by defining bits and pieces
    of the algorithm, which are then later combined to provide a basic visualization
    of the *K*-means algorithm.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将演示如何在纯Clojure中实现*K*-means算法，而不使用任何外部库。我们首先定义算法的各个部分，然后稍后将其组合以提供*K*-means算法的基本可视化。
- en: 'We can say that the distance between two numbers is the absolute difference
    between their values and this can be implemented as a `distance` function, as
    shown in the following code:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说两个数字之间的距离是它们值之间的绝对差，这可以通过以下代码中的`distance`函数实现：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If we are given a number of mean values, we can calculate the closest mean
    from a given number by using a composition of the `distance` and `sort-by` functions,
    as shown in the following code:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们给定一些均值，我们可以通过使用`distance`和`sort-by`函数的组合来计算给定数字的最近均值，如下面的代码所示：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To demonstrate the `closest` function defined in the preceding code, we will
    first need to define some data, that is, a sequence of numbers and a couple of
    mean values, as shown in the following code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示前面代码中定义的`closest`函数，我们首先需要定义一些数据，即一系列数字和一些均值，如下面的代码所示：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now use the `data` and `guessed-means` variables with the `closest`
    function and an arbitrary number, as shown in the following REPL output:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`data`和`guessed-means`变量与`closest`函数以及任意数字一起使用，如下面的REPL输出所示：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Given the means `0` and `10`, the `closest` function returns `0` as the closest
    mean to `2`, and `10` as that for `9` and `100`. Thus, a set of data points can
    be grouped by the means, which are closest to them. We can implement a function
    that implements this grouping operation using the `closest` and `group-by` functions
    as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 给定均值`0`和`10`，`closest`函数返回`0`作为`2`最近的均值，以及`10`作为`9`和`100`的均值。因此，可以通过与它们最近的均值来对数据点进行分组。我们可以通过使用`closest`和`group-by`函数来实现以下分组操作：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `point-groups` function defined in the preceding code requires three arguments,
    namely the initial mean values, the collection of points to be grouped, and lastly
    a function that returns the distance of a point from a given mean. Note that the
    `group-by` function applies a function, which is passed as the first parameter,
    to a collection, which is then passed as the second parameter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码中定义的`point-groups`函数需要三个参数，即初始均值、要分组的点集合，以及最后是一个返回点与给定均值距离的函数。请注意，`group-by`函数将一个函数（作为第一个参数传递）应用于一个集合，然后将该集合作为第二个参数传递。
- en: 'We can apply the `point-groups` function on the list of numbers represented
    by the `data` variable to group the given values by their distance from the guessed
    means, represented by `guessed-means` as shown in the following code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 `data` 变量表示的数字列表上应用 `point-groups` 函数，根据它们与猜测的均值（由 `guessed-means` 表示）的距离将给定的值分组，如下面的代码所示：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As shown in the preceding code, the `point-groups` function partitions the
    sequence `data` into two groups. To calculate the new set of mean values from
    these groups of input values, we must calculate their average value, which can
    be implemented using the `reduce` and `count` functions, as shown in the following
    code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，`point-groups` 函数将序列 `data` 分成两个组。为了从这些输入值的组中计算新的均值集，我们必须计算它们的平均值，这可以通过使用
    `reduce` 和 `count` 函数来实现，如下面的代码所示：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We implement a function to apply the `average` function defined in the preceding
    code to the previous mean values and the map of groups returned by the `point-groups`
    function. We will do this with the help of the following code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了一个函数，将前面代码中定义的 `average` 函数应用于前一个平均值和 `point-groups` 函数返回的组映射。我们将通过以下代码来完成这项工作：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the `new-means` function defined in the preceding code, for each value in
    the previous mean values, we apply the `average` function to the points that are
    grouped by the mean value. Of course, the `average` function must be applied to
    the points of a given mean only if the mean has any points grouped by it. This
    is checked using the `contains?` function in the `new-means` function. We can
    inspect the value returned by the `new-means` function on our sample data in the
    REPL, as shown in the following output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码中定义的 `new-means` 函数中，对于前一个平均值中的每个值，我们应用 `average` 函数到按平均值分组的点。当然，只有当平均值有按其分组的一些点时，才需要对给定均值的点应用
    `average` 函数。这是通过在 `new-means` 函数中使用 `contains?` 函数来检查的。我们可以在 REPL 中检查 `new-means`
    函数返回的值，如下面的输出所示：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As shown in the preceding output, the new mean values are calculated as `(10/3
    55)` from the initial mean values `(0 10)`. To implement the *K*-means algorithm,
    we must apply the `new-means` function iteratively over the new mean values returned
    by it. This iteration can be performed using the `iterate` function, which requires
    a function that takes a single argument to be passed to it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个输出所示，新的平均值是根据初始平均值 `(0 10)` 计算得出的 `(10/3 55)`。为了实现 *K*-means 算法，我们必须迭代地应用
    `new-means` 函数到它返回的新平均值上。这个迭代可以通过 `iterate` 函数来完成，该函数需要一个接受单个参数的函数作为输入。
- en: 'We can define a function to use with the `iterate` function by currying the
    `new-means` function over the old mean values passed to it, as shown in the following
    code:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将 `new-means` 函数对传递给它的旧均值进行柯里化来定义一个与 `iterate` 函数一起使用的函数，如下面的代码所示：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `iterate-means` function defined in the preceding code returns a function
    that calculates the new mean values from a given set of initial mean values, as
    shown in the following output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码中定义的 `iterate-means` 函数返回一个函数，该函数从给定的一组初始平均值计算新的平均值，如下面的输出所示：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As shown in the preceding output, the mean value is observed to change on applying
    the function returned by the `iterate-means` function a couple of times. This
    returned function can be passed to the `iterate` function and we can inspect the
    iterated mean values using the `take` function, as shown in the following code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个输出所示，观察到在应用 `iterate-means` 函数返回的函数几次后，平均值发生了变化。这个返回的函数可以传递给 `iterate` 函数，我们可以使用
    `take` 函数检查迭代的平均值，如下面的代码所示：
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'It''s observed that the mean value changes only in the first three iterations
    and converges to the value `(37/6 10)` for the sample data that we have defined.
    The termination condition of the *K*-means algorithm is the convergence of the
    mean values and thus we must iterate over the values returned by the `iterate-means`
    function until the returned mean value does not differ from the previously returned
    mean value. Since the `iterate` function lazily returns an infinite sequence,
    we must implement a function that limits this sequence by the convergence of the
    elements in the sequence. This behavior can be implemented by lazy realization
    using the `lazy-seq` and `seq` functions as shown in the following code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到均值值仅在第一次迭代中的前三次发生变化，并收敛到我们定义的样本数据的值`(37/6 10)`。K-means算法的终止条件是均值值的收敛，因此我们必须迭代`iterate-means`函数返回的值，直到返回的均值值与之前返回的均值值不再不同。由于`iterate`函数惰性地返回一个无限序列，我们必须实现一个函数来通过序列中元素的收敛来限制这个序列。这种行为可以通过使用`lazy-seq`和`seq`函数的惰性实现来实现，如下所示：
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `take-while-unstable` function defined in the preceding code splits a lazy
    sequence into its head and tail terms and then compares the first element of the
    sequence with the first element of the tail of the sequence to return an empty
    list, or `nil`, if the two elements are equal. However, if they are not equal,
    the `take-while-unstable` function is invoked again on the tail of the sequence.
    Note the use of the `if-let` macro, which is simply a `let` form with an `if`
    expression as its body to check if the sequence `sq` is empty. We can inspect
    the value returned by the `take-while-unstable` function in the REPL as shown
    in the following output:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码中定义的`take-while-unstable`函数将惰性序列分割成其头部和尾部项，然后比较序列的第一个元素与序列尾部的第一个元素，如果两个元素相等则返回一个空列表，或`nil`。然而，如果它们不相等，则`take-while-unstable`函数会在序列的尾部再次被调用。注意`if-let`宏的使用，它只是一个带有`if`表达式的`let`形式，用于检查序列`sq`是否为空。我们可以在以下输出中检查`take-while-unstable`函数返回的值：
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using the final mean value we have calculated, we can determine the clusters
    of input values using the `vals` function on the map returned by the `point-groups`
    function, as shown in the following code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们计算出的最终均值值，我们可以使用`point-groups`函数返回的映射上的`vals`函数来确定输入值的聚类，如下所示：
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that the `vals` function returns all the values in a given map as a sequence.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`vals`函数返回给定映射中的所有值作为一个序列。
- en: 'The `k-cluster` function defined in the preceding code produces the final clusters
    of input values returned by the *K*-means algorithm. We can apply the `k-cluster`
    function on the final mean value `(37/6 101)` to return the final clusters of
    input values, as shown in the following output:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码中定义的`k-cluster`函数生成了由K-means算法返回的输入值的最终聚类。我们可以将`k-cluster`函数应用于最终均值值`(37/6
    101)`，以返回输入值的最终聚类，如下所示：
- en: '[PRE15]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To visualize the change in the clusters of input values, we can apply the `k-cluster`
    function on the sequence of values returned by composing the `iterate` and `iterate-means`
    functions. We must limit this sequence by convergence of the values in all clusters
    and this can be done using the `take-while-unstable` function, as shown in the
    following code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化输入值聚类的变化，我们可以将`k-cluster`函数应用于由组合`iterate`和`iterate-means`函数返回的值序列。我们必须通过所有聚类中值的收敛来限制这个序列，这可以通过使用`take-while-unstable`函数来实现，如下所示：
- en: '[PRE16]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can refactor the preceding expression into a function that requires only
    the initial set of guessed mean values by binding the `iterate-means` function
    to the sample data. The functions used to calculate the distance of a given input
    value from a mean value and the average mean value from a set of input values
    are as shown in the following code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面的表达式重构为一个函数，该函数只需要初始猜测的均值值集合，通过将`iterate-means`函数绑定到样本数据来实现。用于计算给定输入值与均值值距离以及从输入值集合中计算平均均值值的函数如下所示：
- en: '[PRE17]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can bind the `k-groups` function defined in the preceding code with our
    sample data and the `distance` and `average` functions, which operate on numeric
    values as shown in the following code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面代码中定义的`k-groups`函数与我们的样本数据和`distance`以及`average`函数绑定，这些函数在以下代码中展示了它们对数值的操作：
- en: '[PRE18]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we can apply the `grouper` function on any arbitrary set of mean values
    to visualize the changes in the clusters over the various iterations of the *K*-means
    algorithm, as shown in the following code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以对任何任意集合的均值应用`grouper`函数，以可视化在K-均值算法的各个迭代过程中聚类的变化，如下面的代码所示：
- en: '[PRE19]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As we mentioned earlier, if the number of mean values is greater than the number
    of inputs, we end up with a number of clusters equal to the number of input values,
    in which each cluster contains a single input value. This can be verified in the
    REPL using the `grouper` function, as shown in the following code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，如果平均值数量大于输入数量，我们最终会得到与输入值数量相等的聚类数量，其中每个聚类包含一个单独的输入值。这可以通过使用`grouper`函数在REPL中进行验证，如下面的代码所示：
- en: '[PRE20]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can extend the preceding implementation to apply to vector values and not
    just numeric values, by changing the `distance` and `average` distance, which
    are parameters to the `k-groups` function. We can implement these two functions
    for vector values as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过更改`k-groups`函数的参数`distance`和`average`距离来扩展前面的实现，使其适用于向量值而不是仅限于数值。我们可以如下实现这两个函数：
- en: '[PRE21]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `vec-distance` function defined in the preceding code implements the squared
    Euclidean distance between two vector values as the sum of the squared differences
    between the corresponding elements in the two vectors. We can also calculate the
    average of some vector values by adding them together and dividing each resulting
    element by the number of vectors that were added together, as shown in the `vec-average`
    function defined in the preceding code. We can inspect the returned values of
    these functions in the REPL as shown in the following output:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`vec-distance`函数实现了两个向量值之间的平方欧几里得距离，即两个向量中对应元素平方差的和。我们还可以通过将它们相加并除以相加的向量数量来计算一些向量值的平均值，如前面代码中定义的`vec-average`函数所示。我们可以在REPL中检查这些函数的返回值，如下面的输出所示：
- en: '[PRE22]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can now define some of the following vector values to use as sample data
    for our clustering algorithm:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义一些以下向量值作为我们的聚类算法的样本数据：
- en: '[PRE23]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can now use the `k-groups` function with the `vector-data`, `vec-distance`,
    and `vec-average` variables to print the various clusters iterated through to
    produce the final set of clusters, as shown in the following code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`k-groups`函数以及`vector-data`、`vec-distance`和`vec-average`变量来打印出迭代产生的各种聚类，从而得到最终的聚类集合，如下面的代码所示：
- en: '[PRE24]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Another improvement we can add to this implementation is updating identical
    mean values by the `new-means` function. If we pass a list of identical mean values
    to the `new-means` function, both the mean values will get updated. However, in
    the classic *K*-means algorithm, only one mean from two identical mean values
    is updated. This behavior can be verified in the REPL, by passing a list of identical
    means such as `''(0 0)` to the `new-means` function, as shown in the following
    code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以添加到这个实现中的另一个改进是使用`new-means`函数更新相同的均值值。如果我们向`new-means`函数传递一个相同的均值值的列表，两个均值值都将得到更新。然而，在经典的K-均值算法中，只有两个相同均值值中的一个会被更新。这种行为可以通过在REPL中传递一个如`'(0
    0)'`的相同均值值的列表到`new-means`函数来验证，如下面的代码所示：
- en: '[PRE25]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can avoid this problem by checking the number of occurrences of a given
    mean in the set of mean values and only updating a single mean value if multiple
    occurrences of it are found. We can implement this using the `frequencies` function,
    which returns a map with keys as elements from the original collection passed
    to the `frequencies` function and values as the frequencies of occurrences of
    these elements. We can thus redefine the `new-means` function, as shown in the
    following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查给定平均值在平均值集合中的出现次数来避免这个问题，并且只有在发现多个出现时才更新单个平均值。我们可以使用`frequencies`函数来实现这一点，该函数返回一个映射，其键是传递给`frequencies`函数的原始集合中的元素，其值是这些元素出现的频率。因此，我们可以重新定义`new-means`函数，如下面的代码所示：
- en: '[PRE26]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `update-seq` function defined in the preceding code applies a function
    `f` to the elements in a sequence `sq`. The function `f` is only applied to a
    single element if the element is repeated in the sequence. We can now observe
    that only a single mean value changes when we apply the redefined `new-means`
    function on the sequence of identical means `''(0 0)`, as shown in the following
    output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码中定义的`update-seq`函数将函数`f`应用于序列`sq`中的元素。只有当元素在序列中重复时，才会对单个元素应用函数`f`。现在我们可以观察到，当我们对相同的均值序列`'(0
    0)`应用重新定义的`new-means`函数时，只有一个均值值发生变化，如下面的输出所示：
- en: '[PRE27]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'A consequence of the preceding redefinition of the `new-means` function is
    that the `k-groups` function now produces identical clusters when applied to both
    distinct and identical initial mean values, such as `''(0 1)` and `''(0 0)`, as
    shown in the following code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`new-means`函数先前重新定义的结果是，`k-groups`函数现在在应用于不同的和相同的初始均值时，如`''(0 1)`和`''(0 0)`，会产生相同的聚类，如下面的代码所示：'
- en: '[PRE28]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This new behavior of the `new-means` function with respect to identical initial
    mean values also extends to vector values as shown in the following output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`new-means`函数在相同初始均值方面的这种新行为也扩展到向量值，如下面的输出所示：
- en: '[PRE29]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In conclusion, the `k-cluster` and `k-groups` functions defined in the preceding
    example depict how *K*-means clustering can be implemented in idiomatic Clojure.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，前例中定义的`k-cluster`和`k-groups`函数描述了如何在Clojure中实现*K*-means聚类。
- en: Clustering data using clj-ml
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用clj-ml进行聚类数据
- en: The `clj-ml` library provides several implementations of clustering algorithms
    derived from the Java Weka library. We will now demonstrate how we can use the
    `clj-ml` library to build a *K*-means clusterer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`clj-ml`库提供了从Java Weka库派生出的几个聚类算法的实现。现在我们将演示如何使用`clj-ml`库构建一个*K*-means聚类器。'
- en: Note
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The `clj-ml` and Incanter libraries can be added to a Leiningen project by
    adding the following dependency to the `project.clj` file:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在`project.clj`文件中添加以下依赖项将`clj-ml`和Incanter库添加到Leiningen项目中：
- en: '[PRE30]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'For the example that will follow, the namespace declaration should look similar
    to the following declaration:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的示例，命名空间声明应类似于以下声明：
- en: '[PRE31]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: For the examples that use the `clj-ml library` in this chapter, we will use
    the **Iris** dataset from the Incanter library as our training data. This dataset
    is essentially a sample of 150 flowers and four feature variables that are measured
    for these samples. The features of the flowers that are measured in the Iris dataset
    are the width and length of petal and sepal of the flowers. The sample values
    are distributed over three species or categories, namely Virginica, Setosa, and
    Versicolor. The data is available as a ![Clustering data using clj-ml](img/4351OS_07_07.jpg)
    sized matrix in which the species of a given flower is represented as the last
    column in this matrix.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章中使用的`clj-ml`库的示例，我们将使用Incanter库中的**Iris**数据集作为我们的训练数据。这个数据集本质上是一个150朵花的样本，以及为这些样本测量的四个特征变量。在Iris数据集中测量的花的特征是花瓣和花萼的宽度和长度。样本值分布在三个物种或类别中，即Virginica、Setosa和Versicolor。数据以![使用clj-ml进行聚类数据](img/4351OS_07_07.jpg)大小的矩阵形式提供，其中给定花的物种在该矩阵的最后一列中表示。
- en: 'We can select the features from the Iris dataset as a vector using the `get-dataset`,
    `sel`, and `to-vector` functions from the Incanter library, as shown in the following
    code. We can then convert this vector into a `clj-ml` dataset using the `make-dataset`
    function from the `clj-ml` library. This is done by passing the keyword names
    of the feature values as a template to the `make-dataset` function as shown in
    the following code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Incanter库中的`get-dataset`、`sel`和`to-vector`函数从Iris数据集中选择特征作为向量，如下面的代码所示。然后我们可以使用`clj-ml`库中的`make-dataset`函数将此向量转换为`clj-ml`数据集。这是通过将特征值的键名作为模板传递给`make-dataset`函数来完成的，如下面的代码所示：
- en: '[PRE32]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can print the `iris-dataset` variable defined in the preceding code in the
    REPL to give us some information on what it contains as shown in the following
    code and output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在REPL中打印前述代码中定义的`iris-dataset`变量，以获取有关其包含内容的一些信息，如下面的代码和输出所示：
- en: '[PRE33]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can create a clusterer using the `make-clusterer` function from the `clj-ml.clusterers`
    namespace. We can specify the type of cluster to create as the first argument
    to the `make-cluster` function. The second optional argument is a map of options
    to be used to create the specified clusterer. We can train a given clusterer using
    the `cluster-build` function from the `clj-ml` library. In the following code,
    we create a new *K*-means clusterer using the `make-clusterer` function with the
    `:k-means` keyword and define a simple helper function to help train this clusterer
    with any given dataset:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`clj-ml.clusterers`命名空间中的`make-clusterer`函数来创建一个聚类器。我们可以将创建的聚类器类型作为`make-cluster`函数的第一个参数。第二个可选参数是一个选项映射，用于创建指定的聚类器。我们可以使用`clj-ml`库中的`cluster-build`函数来训练一个给定的聚类器。在下面的代码中，我们使用`make-clusterer`函数和`:k-means`关键字创建一个新的*K*-means聚类器，并定义一个简单的辅助函数来帮助使用任何给定的数据集训练这个聚类器：
- en: '[PRE34]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `train-clusterer` function can be applied to the clusterer instance defined
    by the `k-means-clusterer` variable and the sample data represented by the `iris-dataset`
    variable, as shown in the following code and output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`train-clusterer`函数可以应用于由`k-means-clusterer`变量定义的聚类器实例和由`iris-dataset`变量表示的样本数据，如下面的代码和输出所示：'
- en: '[PRE35]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'As shown in the preceding output, the trained clusterer contains `61` values
    in the first cluster (cluster `0`), `50` values in the second cluster (cluster
    `1`), and `39` values in the third cluster (cluster `2`). The preceding output
    also gives us some information about the mean values of the individual features
    in the training data. We can now predict the classes of the input data using the
    trained clusterer and the `clusterer-cluster` function as shown in the following
    code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个输出所示，训练好的聚类器在第一个聚类（聚类`0`）中包含`61`个值，在第二个聚类（聚类`1`）中包含`50`个值，在第三个聚类（聚类`2`）中包含`39`个值。前一个输出还提供了关于训练数据中各个特征平均值的一些信息。现在我们可以使用训练好的聚类器和`clusterer-cluster`函数来预测输入数据的类别，如下面的代码所示：
- en: '[PRE36]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The `clusterer-cluster` function uses the trained clusterer to return a new
    dataset that contains an additional fifth attribute that represents the category
    of a given sample value. As shown in the preceding code, this new attribute has
    the values `0`, `1`, and `2`, and the sample values also contain valid values
    for this new feature. In conclusion, the `clj-ml` library provides a good framework
    for working with clustering algorithms. In the preceding example, we created a
    *K*-means clusterer using the `clj-ml` library.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`clusterer-cluster`函数使用训练好的聚类器返回一个新的数据集，该数据集包含一个额外的第五个属性，表示给定样本值的类别。如前述代码所示，这个新属性的值为`0`、`1`和`2`，样本值也包含这个新特征的合法值。总之，`clj-ml`库提供了一个良好的框架来处理聚类算法。在前面的例子中，我们使用`clj-ml`库创建了一个*K*-means聚类器。'
- en: Using hierarchical clustering
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用层次聚类
- en: '**Hierarchical clustering** is another method of cluster analysis in which
    input values from the training data are grouped together into a hierarchy. The
    process of creating the hierarchy can be done in a top-down approach, where in
    all observations are first part of a single cluster and are then divided into
    smaller clusters. Alternatively, we can group the input values using a bottom-up
    methodology, where each cluster is initially a sample value from the training
    data and these clusters are then combined together. The former top-down approach
    is termed as **divisive clustering** and the later bottom-up method is called
    **agglomerative clustering**.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**层次聚类**是另一种聚类分析方法，其中训练数据的输入值被分组到一个层次结构中。创建层次结构的过程可以采用自上而下的方法，其中所有观测值最初都是单个聚类的部分，然后被划分为更小的聚类。或者，我们可以使用自下而上的方法来分组输入值，其中每个聚类最初都是训练数据中的一个样本值，然后这些聚类被合并在一起。前者自上而下的方法被称为**划分聚类**，后者自下而上的方法被称为**聚合聚类**。'
- en: Thus, in agglomerative clustering, we combine clusters into larger clusters,
    whereas we divide clusters into smaller ones in divisive clustering. In terms
    of performance, modern implementations of agglomerative clustering algorithms
    have a time complexity of ![Using hierarchical clustering](img/4351OS_07_08.jpg),
    while those of divisive clustering have much higher complexity of ![Using hierarchical
    clustering](img/4351OS_07_09.jpg).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在聚合聚类中，我们将聚类合并成更大的聚类，而在划分聚类中，我们将聚类划分为更小的聚类。在性能方面，现代聚合聚类算法的实现具有![使用层次聚类](img/4351OS_07_08.jpg)的时间复杂度，而划分聚类的时间复杂度则要高得多。
- en: 'Suppose we have six input values in our training data. In the following illustration,
    assume that these input values are positioned according to some two-dimensional
    metric to measure the overall value of a given input value:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在训练数据中有六个输入值。在下图说明中，假设这些输入值是根据某种二维度量来衡量给定输入值的整体值的位置：
- en: '![Using hierarchical clustering](img/4351OS_07_10.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![使用层次聚类](img/4351OS_07_10.jpg)'
- en: 'We can apply agglomerative clustering on these input values to produce the
    following hierarchy of clusters:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对这些输入值应用凝聚聚类，以产生以下聚类层次：
- en: '![Using hierarchical clustering](img/4351OS_07_11.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![使用层次聚类](img/4351OS_07_11.jpg)'
- en: The values *b* and *c* are observed to be the closest to each other in the spatial
    distribution and are hence grouped into a cluster. Similarly, the nodes *d* and
    *e* are also grouped into another cluster. The final result of hierarchically
    clustering the input value is a single binary tree or a **dendogram** of the sample
    values. In effect, clusters such as *bc* and *def* are added to the hierarchy
    as binary subtrees of values or of other clusters. Although this process tends
    to appear very simple in a two-dimensional space, the solution to the problem
    of determining the distance and hierarchy between input values is much less trivial
    when applied over several dimensions of features.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到值 *b* 和 *c* 在空间分布上彼此最接近，因此被分组到一个聚类中。同样，节点 *d* 和 *e* 也被分组到另一个聚类中。层次聚类输入值的最终结果是单个二叉树或样本值的**树状图**。实际上，如
    *bc* 和 *def* 这样的聚类作为值的二叉子树或其他聚类的子树被添加到层次中。尽管这个过程在二维空间中看起来非常简单，但当应用于多个维度的特征时，确定输入值之间距离和层次的问题的解决方案就不再那么简单了。
- en: In both agglomerative and divisive clustering techniques, the similarity between
    input values from the sample data has to be calculated. This can be done by measuring
    the distance between two sets of input values, grouping them into clusters using
    the calculated distance, and then determining the linkage or similarity between
    two clusters of input values.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在凝聚和分裂聚类技术中，必须计算样本数据中输入值之间的相似性。这可以通过测量两组输入值之间的距离，使用计算出的距离将它们分组到聚类中，然后确定输入值聚类之间的连接或相似性来完成。
- en: 'The choice of the distance metric in a hierarchical clustering algorithm will
    determine the shape of the clusters that are produced by the algorithm. A couple
    of commonly used measures of the distance between two input vectors ![Using hierarchical
    clustering](img/4351OS_07_12.jpg) and ![Using hierarchical clustering](img/4351OS_07_13.jpg)
    are the Euclidean distance ![Using hierarchical clustering](img/4351OS_07_14.jpg)
    and the squared Euclidean distance ![Using hierarchical clustering](img/4351OS_07_15.jpg),
    which can be formally expressed as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在层次聚类算法中，距离度量的选择将决定算法产生的聚类形状。两个常用的衡量两个输入向量 ![使用层次聚类](img/4351OS_07_12.jpg) 和
    ![使用层次聚类](img/4351OS_07_13.jpg) 之间距离的度量是欧几里得距离 ![使用层次聚类](img/4351OS_07_14.jpg)
    和平方欧几里得距离 ![使用层次聚类](img/4351OS_07_15.jpg)，其形式可以表示如下：
- en: '![Using hierarchical clustering](img/4351OS_07_16.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![使用层次聚类](img/4351OS_07_16.jpg)'
- en: 'Another commonly used metric of the distance between input values is the maximum
    distance ![Using hierarchical clustering](img/4351OS_07_17.jpg), which calculates
    the maximum absolute difference of corresponding elements in two given vectors.
    This function can be expressed as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用于衡量输入值之间距离的度量标准是最大距离 ![使用层次聚类](img/4351OS_07_17.jpg)，它计算两个给定向量中对应元素的绝对最大差值。此函数可以表示如下：
- en: '![Using hierarchical clustering](img/4351OS_07_18.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![使用层次聚类](img/4351OS_07_18.jpg)'
- en: The second aspect of a hierarchical clustering algorithm is the linkage criteria,
    which is an effective measure of similarity or dissimilarity between two clusters
    of input values. Two commonly used methods of determining the linkage between
    two input values are **complete linkage clustering** and **single linkage clustering**.
    Both of these methods are forms of agglomerative clustering.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法的第二个方面是连接标准，它是衡量两个输入值聚类之间相似性或差异性的有效度量。确定两个输入值之间连接的两种常用方法是**完全连接聚类**和**单连接聚类**。这两种方法都是凝聚聚类的形式。
- en: 'In agglomerative clustering, two input values or clusters with the shortest
    distance metric are combined into a new cluster. Of course, the definition of
    "shortest distance" is what is unique in any agglomerative clustering technique.
    In complete linkage clustering, input values farthest from each other are used
    to determine the grouping. Hence, this method is also termed as **farthest neighbor
    clustering**. This metric of the distance ![Using hierarchical clustering](img/4351OS_07_19.jpg)
    between two values can be formally expressed as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚合聚类中，两个具有最短距离度量的输入值或聚类被合并成一个新的聚类。当然，“最短距离”的定义是任何聚合聚类技术中独特的地方。在完全链接聚类中，使用彼此最远的输入值来确定分组。因此，这种方法也被称为**最远邻聚类**。两个值之间的距离![使用层次聚类](img/4351OS_07_19.jpg)的度量可以如下正式表达：
- en: '![Using hierarchical clustering](img/4351OS_07_20.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![使用层次聚类](img/4351OS_07_20.jpg)'
- en: In the preceding equation, the function ![Using hierarchical clustering](img/4351OS_07_21.jpg)
    is the selected metric of distance between two input vectors. Complete linkage
    clustering will essentially group together values or clusters that have the maximum
    value of the distance metric ![Using hierarchical clustering](img/4351OS_07_22.jpg).
    This operation of grouping together clusters is repeated until a single cluster
    is produced.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，函数![使用层次聚类](img/4351OS_07_21.jpg)是两个输入向量之间选择的距离度量。完全链接聚类将本质上将具有最大距离度量![使用层次聚类](img/4351OS_07_22.jpg)的值或聚类分组在一起。这种将聚类分组在一起的操作会重复进行，直到产生单个聚类。
- en: 'In single linkage clustering, values that are nearest to each other are grouped
    together. Hence, single linkage clustering is also called **nearest neighbor clustering**.
    This can be be formally stated using the following expression:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在单链接聚类中，彼此最近的值被分组在一起。因此，单链接聚类也称为**最近邻聚类**。这可以用以下表达式正式表述：
- en: '![Using hierarchical clustering](img/4351OS_07_23.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![使用层次聚类](img/4351OS_07_23.jpg)'
- en: Another popular hierarchical clustering technique is the **Cobweb algorithm**.
    This algorithm is a form of **conceptual clustering**, in which a concept is created
    for each cluster produced by the clustering method used. By the term "concept",
    we mean a concise formal description of the data clustered together. Interestingly,
    conceptual clustering is closely related to decision tree learning, which we already
    discussed in [Chapter 3](ch03.html "Chapter 3. Categorizing Data"), *Categorizing
    Data*. The Cobweb algorithm groups all clusters into a **classification tree**,
    in which each node contains a formal summary of the values or clusters that are
    its child nodes. This information can then be used to determine and predict the
    category of an input value with some missing features. In this sense, this technique
    can be used when some of the samples in the test data have missing or unknown
    features.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的层次聚类技术是**蜘蛛网算法**。该算法是一种**概念聚类**，其中为聚类方法产生的每个聚类创建一个概念。术语“概念”指的是聚在一起的数据的简洁形式化描述。有趣的是，概念聚类与决策树学习密切相关，我们已经在[第3章](ch03.html
    "第3章。数据分类")中讨论过，即*数据分类*。蜘蛛网算法将所有聚类分组到一个**分类树**中，其中每个节点包含其子节点（即值或聚类）的正式摘要。然后可以使用这些信息来确定和预测具有一些缺失特征的输入值的类别。在这种情况下，当测试数据中的某些样本具有缺失或未知特征时，可以使用这种技术。
- en: We now demonstrate a simple implementation of hierarchical clustering. In this
    implementation, we take a slightly different approach where we embed part of the
    required functionality into the standard vector data structure provided by the
    Clojure language.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们演示层次聚类的简单实现。在这个实现中，我们采取了一种略有不同的方法，将部分所需功能嵌入到Clojure语言提供的标准向量数据结构中。
- en: Note
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the upcoming example, we require the `clojure.math.numeric-tower` library
    that can be added to a Leiningen project by adding the following dependency to
    the `project.clj` file:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于即将到来的示例，我们需要`clojure.math.numeric-tower`库，可以通过在`project.clj`文件中添加以下依赖项将此库添加到Leiningen项目中：
- en: '[PRE37]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The namespace declaration for the example should look similar to the following
    declaration:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 示例中的命名空间声明应类似于以下声明：
- en: '[PRE38]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'For this implementation, we will use the Euclidean distance between two points
    as a distance metric. We can calculate this distance from the sum of squares of
    the elements in an input vector, which can be computed using a composition of
    the `reduce` and `map` functions as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实现，我们将使用两点之间的欧几里得距离作为距离度量。我们可以通过输入向量中元素的平方和来计算这个距离，这可以通过`reduce`和`map`函数的组合来计算，如下所示：
- en: '[PRE39]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The `sum-of-squares` function defined in the preceding code will be used to
    determine the distance metric. We will define two protocols that abstract the
    operations we perform on a particular data type. From an engineering perspective,
    these two protocols could be combined into a single protocol, since both the protocols
    will be used in combination.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`sum-of-squares`函数将用于确定距离度量。我们将定义两个协议来抽象我们对特定数据类型执行的操作。从工程角度来看，这两个协议可以合并为一个单一协议，因为这两个协议都将组合使用。
- en: 'However, we use the following two protocols for this example for the sake of
    clarity:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了清晰起见，我们在这个例子中使用以下两个协议：
- en: '[PRE40]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The `each` function defined in the `Each` protocol applies a given operation
    `op` on corresponding elements in two collections `v` and `w`. The `each` function
    is quite similar to the standard `map` function, but `each` allows the data type
    of `v` to decide how to apply the function `op`. The `distance` function defined
    in the `Distance` protocol calculates the distance between any two collections
    `v` and `w`. Note that we use the generic term "collection" since we are dealing
    with abstract protocols and not concrete implementations of the functions of these
    protocols. For this example, we will implement the preceding protocols as part
    of the vector data type. Of course, these protocols could also be extended to
    other data types such as sets and maps.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Each`协议中定义的`each`函数将给定的操作`op`应用于两个集合`v`和`w`中的对应元素。`each`函数与标准`map`函数非常相似，但`each`允许`v`的数据类型决定如何应用函数`op`。在`Distance`协议中定义的`distance`函数计算任何两个集合`v`和`w`之间的距离。请注意，我们使用通用术语“集合”，因为我们处理的是抽象协议，而不是这些协议函数的具体实现。对于这个例子，我们将实现前面的协议作为向量数据类型的一部分。当然，这些协议也可以扩展到其他数据类型，如集合和映射。
- en: 'In this example, we will implement single linkage clustering as the linkage
    criteria. First, we will have to define a function to determine the two closest
    vectors from a set of vector values. To do this, we can apply the `min-key` function,
    which returns the key with the least associated value in a collection, on a vector.
    Interestingly, this is possible in Clojure since we can treat a vector as a map
    with the index values of the various elements in the vector as its keys. We will
    implement this with the help of the following code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将实现单链接聚类作为链接标准。首先，我们必须定义一个函数来确定从一组向量值中距离最近的两个向量。为此，我们可以对一个向量应用`min-key`函数，该函数返回集合中与最少关联值的键。有趣的是，在Clojure中这是可能的，因为我们可以将向量视为一个映射，其中向量中各种元素的索引值作为其键。我们将借助以下代码来实现这一点：
- en: '[PRE41]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The `closest-vectors` function defined in the preceding code determines all
    possible combinations of the indexes of the vector `vs` using the `for` form.
    Note that the vector `vs` is a vector of vectors. The `distance` function is then
    applied over the values of the possible index combinations and these distances
    are then compared using the `min-key` function. The function finally returns the
    index values of the two inner vector values that have the least distance from
    each other, thus implementing single linkage clustering.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`closest-vectors`函数使用`for`形式确定向量`vs`的所有可能的索引组合。请注意，向量`vs`是一个向量向量的向量。然后，将`distance`函数应用于可能的索引组合的值，并使用`min-key`函数比较这些距离。该函数最终返回两个内部向量值的最小距离的索引值，从而实现单链接聚类。
- en: 'We will also need to calculate the mean value of two vectors that have to be
    clustered together. We can implement this using the `each` function we had previously
    defined in the `Each` protocol and the `reduce` function, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要计算必须聚在一起的两个向量的平均值。我们可以使用在`Each`协议中先前定义的`each`函数和`reduce`函数来实现这一点，如下所示：
- en: '[PRE42]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The `centroid` function defined in the preceding code will calculate the mean
    value of a sequence of vector values. Note the use of the `double` function to
    ensure that the value returned by the `centroid` function is a double-precision
    number.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一段代码中定义的 `centroid` 函数将计算一系列向量值的平均值。请注意，使用 `double` 函数以确保 `centroid` 函数返回的值是一个双精度浮点数。
- en: 'We now implement the `Each` and `Distance` protocols as part of the vector
    data type, which is fully qualified as `clojure.lang.PersistentVector`. This is
    done using the `extend-type` function as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将 `Each` 和 `Distance` 协议作为向量数据类型的一部分实现，该类型完全限定为 `clojure.lang.PersistentVector`。这是通过使用
    `extend-type` 函数实现的，如下所示：
- en: '[PRE43]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The `each` function is implemented such that it applies the `op` operation to
    each element in the `v` vector and a second argument `w`. The `w` parameter could
    be either a vector or a number. In case `w` is a number, we simply map the function
    `op` over `v` and the repeated value of the number `w`. If `w` is a vector, we
    pad the smaller vector with `0` values using the `lazy-cat` function and map `op`
    over the two vectors. Also, we wrap the entire expression in a `vec` function
    to ensure that the value returned is always a vector.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`each` 函数的实现方式是，将 `op` 操作应用于 `v` 向量中的每个元素和第二个参数 `w`。`w` 参数可以是向量或数字。如果 `w` 是一个数字，我们只需将函数
    `op` 映射到 `v` 和数字 `w` 的重复值上。如果 `w` 是一个向量，我们使用 `lazy-cat` 函数用 `0` 值填充较短的向量，并将 `op`
    映射到两个向量上。此外，我们用 `vec` 函数包装整个表达式，以确保返回的值始终是向量。'
- en: The `distance` function is implemented as the Euclidean distance between two
    vector values `v` and `w` using the `sum-of-squares` function that we previously
    defined and the `sqrt` function from the `clojure.math.numeric-tower` namespace.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`distance` 函数是通过使用我们之前定义的 `sum-of-squares` 函数和来自 `clojure.math.numeric-tower`
    命名空间的 `sqrt` 函数来计算两个向量值 `v` 和 `w` 之间的欧几里得距离。'
- en: 'We have all the pieces needed to implement a function that performs hierarchical
    clustering on vector values. We can implement hierarchical clustering primarily
    using the centroid and `closest-vectors` functions that we had previously defined,
    as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经拥有了实现一个对向量值进行层次聚类功能的所有组件。我们可以主要使用之前定义的 `centroid` 和 `closest-vectors` 函数来实现层次聚类，如下所示：
- en: '[PRE44]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can pass a vector of maps to the `h-cluster` function defined in the preceding
    code. Each map in this vector contains a vector as the value of the keyword `:vec`.
    The `h-cluster` function combines all the vector values from the `:vec` keywords
    in these maps and determines the two closest vectors using the `closest-vectors`
    function. Since the value returned by the `closest-vectors` function is a vector
    of two index values, we determine all the vectors with indexes other than the
    two index values returned by the `closest-vectors` function. This is done using
    a special form of the `for` macro that allows a conditional clause to be specified
    with the `:when` key parameter. The mean value of the two closest vectors is then
    calculated using the `centroid` function. A new map is created using the mean
    value and then added to the original vector to replace the two closest vector
    values. The process is repeated until the vector contains a single cluster, using
    the `loop` form. We can inspect the behavior of the `h-cluster` function in the
    REPL as shown in the following code:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将映射到前一段代码中定义的 `h-cluster` 函数的向量传递给。这个向量中的每个映射都包含一个向量作为 `:vec` 关键字的值。`h-cluster`
    函数结合了这些映射中 `:vec` 关键字的所有向量值，并使用 `closest-vectors` 函数确定两个最近的向量。由于 `closest-vectors`
    函数返回的是一个包含两个索引值的向量，我们确定除了 `closest-vectors` 函数返回的两个索引值之外的所有向量。这是通过一个特殊的 `for`
    宏形式实现的，该宏允许使用 `:when` 关键字参数指定条件子句。然后使用 `centroid` 函数计算两个最近向量值的平均值。使用平均值创建一个新的映射，并将其添加到原始向量中以替换两个最近的向量值。使用
    `loop` 形式重复此过程，直到向量中只剩下一个簇。我们可以在以下代码中检查 `h-cluster` 函数的行为：
- en: '[PRE45]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: When applied to three vector values `[1 2 3]`, `[3 4 5]`, and `[7 9 9]`, as
    shown in the preceding code, the `h-cluster` function groups the vectors `[1 2
    3]` and `[3 4 5]` into a single cluster. This cluster has the mean value of `[2.0
    3.0 4.0]`, which is calculated from the vectors `[1 2 3]` and `[3 4 5]`. This
    new cluster is then grouped with the vector `[7 9 9]` in the next iteration, thus
    producing a single cluster with a mean value of `[4.5 6.0 6.5]`. In conclusion,
    the `h-cluster` function can be used to hierarchically cluster vector values into
    a single hierarchy.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于三个向量值 `[1 2 3]`、`[3 4 5]` 和 `[7 9 9]`，如前述代码所示时，`h-cluster` 函数将向量 `[1 2 3]`
    和 `[3 4 5]` 分组到一个单独的簇中。这个簇的平均值为 `[2.0 3.0 4.0]`，这是从向量 `[1 2 3]` 和 `[3 4 5]` 计算得出的。然后，这个新的簇在下一轮迭代中与向量
    `[7 9 9]` 分组，从而产生一个平均值为 `[4.5 6.0 6.5]` 的单个簇。总之，`h-cluster` 函数可以用来将向量值分层聚类到一个单独的层次结构中。
- en: The `clj-ml` library provides an implementation of the Cobweb hierarchical clustering
    algorithm. We can instantiate such a clusterer using the `make-clusterer` function
    with the `:cobweb` argument.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`clj-ml` 库提供了一个 Cobweb 层次聚类算法的实现。我们可以使用带有 `:cobweb` 参数的 `make-clusterer` 函数实例化这样的聚类器。'
- en: '[PRE46]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The clusterer defined by the `h-clusterer` variable shown in the preceding
    code can be trained using the `train-clusterer` function and `iris-dataset` dataset,
    which we had previously defined, as follows: The `train-clusterer` function and
    `iris-dataset` can be implemented as shown in the following code:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由前述代码中显示的 `h-clusterer` 变量定义的聚类器可以使用 `train-clusterer` 函数和之前定义的 `iris-dataset`
    数据集进行训练，如下所示：`train-clusterer` 函数和 `iris-dataset` 可以按照以下代码实现：
- en: '[PRE47]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As shown in the preceding REPL output, the Cobweb clustering algorithm partitions
    the input data into two clusters. One cluster has 96 samples and the other cluster
    has 54 samples, which is quite a different result compared to the *K*-means clusterer,
    we had previously used. In summary, the `clj-ml` library provides an easy-to-use
    implementation of the Cobweb clustering algorithm.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述 REPL 输出所示，Cobweb 聚类算法将输入数据划分为两个簇。一个簇包含 96 个样本，另一个簇包含 54 个样本，这与我们之前使用的 *K*-means
    聚类器得到的结果相当不同。总之，`clj-ml` 库提供了一个易于使用的 Cobweb 聚类算法的实现。
- en: Using Expectation-Maximization
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用期望最大化
- en: The **Expectation-Maximization** (**EM**) algorithm is a probabilistic approach
    for determining a clustering model that fits the supplied training data. This
    algorithm determines the **Maximum Likelihood Estimate** (**MLE**) of the parameters
    of a formulated clustering model (for more information, refer to *Maximum likelihood
    theory and applications for distributions generated when observing a function
    of an exponential family variable*).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**期望最大化**（**EM**）算法是一种概率方法，用于确定适合提供的训练数据的聚类模型。此算法确定了一个公式化聚类模型参数的**最大似然估计**（**MLE**）（有关更多信息，请参阅*观察指数族变量函数时的分布的最大似然理论和应用*）。'
- en: 'Suppose we want to determine the probability of a coin toss being a head or
    a tail. If we flip the coin ![Using Expectation-Maximization](img/4351OS_07_24.jpg)
    times, we end up with ![Using Expectation-Maximization](img/4351OS_07_25.jpg)
    occurrences of heads and ![Using Expectation-Maximization](img/4351OS_07_26.jpg)
    occurrences of tails. We can estimate the actual probability of occurrence of
    a head ![Using Expectation-Maximization](img/4351OS_07_27.jpg) as the ratio of
    the number of occurrences of a head to the total number of coin tosses performed,
    using the following equation:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要确定抛硬币得到正面或反面的概率。如果我们抛硬币 ![使用期望最大化](img/4351OS_07_24.jpg) 次，我们将得到 ![使用期望最大化](img/4351OS_07_25.jpg)
    次正面和 ![使用期望最大化](img/4351OS_07_26.jpg) 次反面的出现。我们可以通过以下方程估计出现正面的实际概率 ![使用期望最大化](img/4351OS_07_27.jpg)，即正面出现次数与抛硬币总次数的比值：
- en: '![Using Expectation-Maximization](img/4351OS_07_28.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![使用期望最大化](img/4351OS_07_28.jpg)'
- en: 'The probability ![Using Expectation-Maximization](img/4351OS_07_29.jpg) defined
    in the preceding equation is the MLE of the probability ![Using Expectation-Maximization](img/4351OS_07_30.jpg).
    In the context of machine learning, the MLE can be maximized to determine the
    probability of occurrence of a given class or category. However, this estimated
    probability may not be statistically distributed in a well-defined way over the
    available training data, which makes it hard to determine the MLE efficiently.
    The problem is simplified by introducing a set of hidden values to account for
    the unobserved values in the training data. The hidden values are not directly
    measured from the data, but are determined from factors that influence the data.
    The likelihood function of the parameters ![Using Expectation-Maximization](img/4351OS_07_31.jpg)
    for a given set of observed values ![Using Expectation-Maximization](img/4351OS_07_12.jpg)
    and a set of hidden values ![Using Expectation-Maximization](img/4351OS_07_32.jpg)
    is defined as the probability of occurrence of ![Using Expectation-Maximization](img/4351OS_07_12.jpg)
    and ![Using Expectation-Maximization](img/4351OS_07_32.jpg) for a given set of
    parameters ![Using Expectation-Maximization](img/4351OS_07_31.jpg). The likelihood
    is mathematically written as ![Using Expectation-Maximization](img/4351OS_07_33.jpg),
    and can be expressed as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个方程中定义的概率 ![使用期望最大化](img/4351OS_07_29.jpg) 是概率 ![使用期望最大化](img/4351OS_07_30.jpg)
    的最大似然估计。在机器学习的背景下，最大似然估计可以被最大化以确定给定类别或类别的发生概率。然而，这个估计的概率可能不会在可用的训练数据上以良好的定义方式统计分布，这使得难以有效地确定最大似然估计。通过引入一组隐藏值来解释训练数据中的未观察值，问题得到了简化。隐藏值不是直接从数据中测量的，而是从影响数据的因素中确定的。对于给定的一组观察值
    ![使用期望最大化](img/4351OS_07_12.jpg) 和一组隐藏值 ![使用期望最大化](img/4351OS_07_32.jpg)，参数 ![使用期望最大化](img/4351OS_07_31.jpg)
    的似然函数定义为 ![使用期望最大化](img/4351OS_07_12.jpg) 和 ![使用期望最大化](img/4351OS_07_32.jpg) 发生的概率，对于给定的一组参数
    ![使用期望最大化](img/4351OS_07_31.jpg)。似然函数可以用 ![使用期望最大化](img/4351OS_07_33.jpg) 的数学表达式表示，可以表示如下：
- en: '![Using Expectation-Maximization](img/4351OS_07_34.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![使用期望最大化](img/4351OS_07_34.jpg)'
- en: 'The EM algorithm comprises two steps—the expectation step and the maximization
    step. In the expectation step, we calculate the expected value of the **log likelihood**
    function. This step determines a metric ![Using Expectation-Maximization](img/4351OS_07_35.jpg),
    which must be maximized in the next step, that is, the maximization step of the
    algorithm. These two steps can be formally summarized as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: EM 算法包括两个步骤——期望步骤和最大化步骤。在期望步骤中，我们计算 **对数似然** 函数的期望值。这一步确定了一个必须在下一步中最大化的度量 ![使用期望最大化](img/4351OS_07_35.jpg)，即算法的最大化步骤。这两个步骤可以正式总结如下：
- en: '![Using Expectation-Maximization](img/4351OS_07_36.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![使用期望最大化](img/4351OS_07_36.jpg)'
- en: In the preceding equation, the value of ![Using Expectation-Maximization](img/4351OS_07_31.jpg)
    that maximizes the value of the function *Q* is iteratively calculated until it
    converges to a particular value. The term ![Using Expectation-Maximization](img/4351OS_07_37.jpg)
    represents the estimated parameters in the ![Using Expectation-Maximization](img/4351OS_07_38.jpg)
    iteration of the algorithm. Also, the term ![Using Expectation-Maximization](img/4351OS_07_39.jpg)
    is the expected value of the log likelihood function.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个方程中，通过迭代计算最大化函数 *Q* 值的 ![使用期望最大化](img/4351OS_07_31.jpg) 值，直到它收敛到一个特定的值。术语
    ![使用期望最大化](img/4351OS_07_37.jpg) 代表算法 ![使用期望最大化](img/4351OS_07_38.jpg) 迭代中的估计参数。此外，术语
    ![使用期望最大化](img/4351OS_07_39.jpg) 是对数似然函数的期望值。
- en: 'The `clj-ml` library also provides an EM clusterer. We can create an EM clusterer
    using the `make-clusterer` function with the `:expectation-maximization` keyword
    as its argument, as shown in the following code:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`clj-ml` 库还提供了一个 EM 聚类器。我们可以使用 `make-clusterer` 函数和 `:expectation-maximization`
    关键字作为其参数来创建一个 EM 聚类器，如下面的代码所示：'
- en: '[PRE48]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Note that we must also specify the number of clusters to produce as an option
    to the `make-clusterer` function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们还必须指定要生成的聚类数量作为 `make-clusterer` 函数的选项。
- en: 'We can train the clusterer defined by the `em-clusterer` variable in the preceding
    code using the `train-clusterer` function and `iris-dataset` dataset, which we
    had previously defined, as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `train-clusterer` 函数和之前定义的 `iris-dataset` 数据集来训练由 `em-clusterer` 变量定义的聚类器，如下所示：
- en: '[PRE49]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: As shown in the preceding output, the EM clusterer partitions the given dataset
    into three clusters in which the clusters are distributed as approximately 41
    percent, 25 percent, and 35 percent of the samples in the training data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述输出所示，EM 聚类器将给定的数据集划分为三个簇，其中簇的分布大约为训练数据样本的 41%、25% 和 35%。
- en: Using SOMs
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SOMs
- en: As we mentioned earlier in [Chapter 4](ch04.html "Chapter 4. Building Neural
    Networks"), *Building Neural Networks*, SOMs can be used to model unsupervised
    machine learning problems such as clustering (for more information, refer to *Self-organizing
    Maps as Substitutes for K-Means Clustering*). To quickly recap, an SOM is a type
    of ANN that maps input values with a high number of dimensions to a low-dimensional
    output space. This mapping preserves patterns and topological relations between
    the input values. The neurons in the output space of an SOM will have higher activation
    values for input values that are spatially close to each other. Thus, SOMs are
    a good solution for clustering input data with a large number of dimensions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前在[第 4 章](ch04.html "第 4 章。构建神经网络")中提到的，*构建神经网络*，SOM 可以用于建模无监督机器学习问题，例如聚类（更多信息，请参阅
    *自组织映射作为 K-Means 聚类的替代方案*）。为了快速回顾，SOM 是一种将高维输入值映射到低维输出空间的 ANN 类型。这种映射保留了输入值之间的模式和拓扑关系。SOM
    的输出空间中的神经元对于空间上彼此接近的输入值将具有更高的激活值。因此，SOM 是聚类具有大量维度的输入数据的好方法。
- en: The Incanter library provides a concise SOM implementation that we can use to
    cluster the input variables from the Iris dataset. We will demonstrate how we
    can use this SOM implementation for clustering in the example that will follow.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Incanter 库提供了一个简洁的 SOM 实现，我们可以使用它来聚类 Iris 数据集中的输入变量。我们将在接下来的示例中演示如何使用这个 SOM
    实现进行聚类。
- en: Note
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Incanter library can be added to a Leiningen project by adding the following
    dependency to the `project.clj` file:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在 `project.clj` 文件中添加以下依赖项将 Incanter 库添加到 Leiningen 项目中：
- en: '[PRE50]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'For the upcoming example, the namespace declaration should look similar to
    the following declaration:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于即将到来的示例，命名空间声明应类似于以下声明：
- en: '[PRE51]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We first define the sample data to cluster using the `get-dataset`, `sel`,
    and `to-matrix` functions from the Incanter library as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用 Incanter 库中的 `get-dataset`、`sel` 和 `to-matrix` 函数定义用于聚类的样本数据，如下所示：
- en: '[PRE52]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The `iris-features` variable defined in the preceding code is in fact a ![Using
    SOMs](img/4351OS_07_40.jpg) sized matrix that represents the values of the four
    input variables that we have selected from the Iris dataset. Now, we can use the
    `som-batch-train` function from the `incanter.som` namespace to create and train
    an SOM using these selected features, as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的 `iris-features` 变量实际上是一个 ![使用 SOMs](img/4351OS_07_40.jpg) 大小的矩阵，它表示我们从
    Iris 数据集中选出的四个输入变量的值。现在，我们可以使用 `incanter.som` 命名空间中的 `som-batch-train` 函数创建并训练一个
    SOM，如下所示：
- en: '[PRE53]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The `som` variable defined is actually a map with several key-value pairs.
    The `:dims` key in this map contains a vector that represents the dimensions in
    the lattice of neurons in the trained SOM, as shown in the following code and
    output:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 定义为 `som` 变量的实际上是一个包含多个键值对的映射。该映射中的 `:dims` 键包含一个向量，它表示训练好的 SOM 中神经元格子的维度，如下面的代码和输出所示：
- en: '[PRE54]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Thus, we can say that the neural lattice of the trained SOM is a ![Using SOMs](img/4351OS_07_41.jpg)
    matrix. The `:sets` key of the map represented by the `som` variable gives us
    the positional grouping of the various indexes of the input values in the lattice
    of neurons of the SOM, as shown in the following output:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说训练好的 SOM 的神经网络是一个 ![使用 SOMs](img/4351OS_07_41.jpg) 矩阵。`som` 变量表示的映射中的
    `:sets` 键给出了输入值在 SOM 神经元格子中的各种索引的位置分组，如下面的输出所示：
- en: '[PRE55]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'As shown in the preceding REPL output, the input data is partitioned into three
    clusters. We can calculate the mean values of each feature using the `mean` function
    from the `incanter.stats` namespace as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述 REPL 输出所示，输入数据被划分为三个簇。我们可以使用 `incanter.stats` 命名空间中的 `mean` 函数计算每个特征的均值，如下所示：
- en: '[PRE56]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We can implement a function to plot these mean values using the `xy-plot`,
    `add-lines`, and `view` functions from the Incanter library as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Incanter 库中的 `xy-plot`、`add-lines` 和 `view` 函数来实现一个函数来绘制这些均值，如下所示：
- en: '[PRE57]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The following linear plot is produced on calling the `plot-means` function
    defined in the preceding code:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 调用前面代码中定义的 `plot-means` 函数时，产生了以下线性图：
- en: '![Using SOMs](img/4351OS_07_42.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![使用SOMs](img/4351OS_07_42.jpg)'
- en: The preceding plot gives us an idea of the mean values of the various features
    in the three clusters determined by the SOM. The plot shows that two of the clusters
    (*Cluster 0* and *Cluster 1*) have similar features. The third cluster, however,
    has significantly different mean values for these set of features and is thus
    shown as a different shape in the plot. Of course, this plot doesn't give us much
    information about the distribution or variance of input values around these mean
    values. To visualize these features, we need to somehow transform the number of
    dimensions of the input data to two or three dimensions, which can be easily visualized.
    We will discuss more on this concept of reducing the number of features in the
    training data in the next section of this chapter.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表展示了SOM确定的三个聚类中各种特征的均值。图表显示，其中两个聚类（*Cluster 0*和*Cluster 1*）具有相似的特征。然而，第三个聚类在这些特征集合上的均值有显著差异，因此在图表中显示为不同的形状。当然，这个图表并没有给我们关于这些均值周围输入值分布或方差的信息。为了可视化这些特征，我们需要以某种方式将输入数据的维度数转换为两个或三个维度，这样就可以轻松地可视化。我们将在本章下一节中进一步讨论在训练数据中减少特征数量的概念。
- en: 'We can also print the clusters and the actual categories of the input values
    using the `frequencies` and `sel` functions as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用`frequencies`和`sel`函数来打印聚类和输入值的实际类别，如下所示：
- en: '[PRE58]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We can call the function `print-clusters` defined in the preceding code to
    produce the following REPL output:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调用前面代码中定义的`print-clusters`函数来生成以下REPL输出：
- en: '[PRE59]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: As shown in the preceding output, the `virginica` and `setosa` species seem
    to be appropriately classified into two clusters. However, the cluster containing
    the input values of the `versicolor` species also contains 27 samples of the `virginica`
    species. This problem could be remedied by using more sample data to train the
    SOM or by modeling a higher number of features.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的输出所示，`virginica`和`setosa`物种似乎被适当地分类到两个聚类中。然而，包含`versicolor`物种输入值的聚类也包含了27个`virginica`物种的样本。这个问题可以通过使用更多的样本数据来训练SOM或通过建模更多的特征来解决。
- en: In conclusion, the Incanter library provides us with a concise implementation
    of an SOM, which we can train using the Iris dataset as shown in the preceding
    example.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，Incanter库为我们提供了一个简洁的SOM实现，我们可以使用前面示例中的Iris数据集进行训练。
- en: Reducing dimensions in the data
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在数据中降低维度
- en: In order to easily visualize the distribution of some unlabeled data in which
    the input values have multiple dimensions, we must reduce the number of feature
    dimensions to two or three. Once we have reduced the number of dimensions of the
    input data to two or three dimensions, we can trivially plot the data to provide
    a more understandable visualization of it. This process of reducing the number
    of dimensions in the input data is known as **dimensionality reduction**. As this
    process reduces the total number of dimensions used to represent the sample data,
    it is also useful for data compression.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了轻松可视化某些未标记数据的分布，其中输入值具有多个维度，我们必须将特征维度数降低到两个或三个。一旦我们将输入数据的维度数降低到两个或三个维度，我们就可以简单地绘制数据，以提供更易于理解的可视化。在输入数据中减少维度数的过程被称为**降维**。由于这个过程减少了表示样本数据所使用的总维度数，因此它也有助于数据压缩。
- en: '**Principal Component Analysis** (**PCA**) is a form of dimensionality reduction
    in which the input variables in the sample data are transformed into linear uncorrelated
    variables (for more information, refer to *Principal Component Analysis*). These
    transformed features are called the **principal components** of the sample data.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）是一种降维方法，其中样本数据中的输入变量被转换为线性不相关的变量（更多信息，请参阅*主成分分析*）。这些转换后的特征被称为样本数据的**主成分**。'
- en: 'PCA uses a covariance matrix and a matrix operation called **Singular Value
    Decomposition** (**SVD**) to calculate the principal components of a given set
    of input values. The covariance matrix denoted as ![Reducing dimensions in the
    data](img/4351OS_07_43.jpg), can be determined from a set of input vectors ![Reducing
    dimensions in the data](img/4351OS_07_12.jpg) with ![Reducing dimensions in the
    data](img/4351OS_07_44.jpg) samples as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 使用协方差矩阵和称为 **奇异值分解**（**SVD**）的矩阵运算来计算给定输入值的特征值。表示为 ![降低数据维度](img/4351OS_07_43.jpg)
    的协方差矩阵，可以从具有 ![降低数据维度](img/4351OS_07_12.jpg) 个样本的输入向量 ![降低数据维度](img/4351OS_07_44.jpg)
    中确定如下：
- en: '![Reducing dimensions in the data](img/4351OS_07_45.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![降低数据维度](img/4351OS_07_45.jpg)'
- en: 'The covariance matrix is generally calculated from the input values after mean
    normalization, which is simply ensuring that each feature has a zero mean value.
    Also, the features could be scaled before determining the covariance matrix. Next,
    the SVD of the covariance matrix is determined as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵通常在均值归一化后的输入值上计算，这仅仅是确保每个特征具有零均值值。此外，在确定协方差矩阵之前，特征可以被缩放。接下来，协方差矩阵的 SVD
    如下确定：
- en: '![Reducing dimensions in the data](img/4351OS_07_46.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![降低数据维度](img/4351OS_07_46.jpg)'
- en: SVD can be thought of as factorization of a matrix ![Reducing dimensions in
    the data](img/4351OS_07_47.jpg) with size ![Reducing dimensions in the data](img/4351OS_07_48.jpg)
    into three matrices ![Reducing dimensions in the data](img/4351OS_07_49.jpg),
    ![Reducing dimensions in the data](img/4351OS_07_50.jpg), and ![Reducing dimensions
    in the data](img/4351OS_07_51.jpg). The matrix ![Reducing dimensions in the data](img/4351OS_07_49.jpg)
    has a size of ![Reducing dimensions in the data](img/4351OS_07_52.jpg), the matrix
    ![Reducing dimensions in the data](img/4351OS_07_50.jpg) has a size of ![Reducing
    dimensions in the data](img/4351OS_07_48.jpg), and the matrix ![Reducing dimensions
    in the data](img/4351OS_07_51.jpg) has a size of ![Reducing dimensions in the
    data](img/4351OS_07_53.jpg). The matrix ![Reducing dimensions in the data](img/4351OS_07_47.jpg)
    actually represents the ![Reducing dimensions in the data](img/4351OS_07_54.jpg)
    input vectors with ![Reducing dimensions in the data](img/4351OS_07_55.jpg) dimensions
    in the sample data. The matrix ![Reducing dimensions in the data](img/4351OS_07_50.jpg)
    is a diagonal matrix and is called the **singular value** of the matrix ![Reducing
    dimensions in the data](img/4351OS_07_47.jpg), and the matrices ![Reducing dimensions
    in the data](img/4351OS_07_49.jpg) and ![Reducing dimensions in the data](img/4351OS_07_51.jpg)
    are called the **left and right singular vectors** of ![Reducing dimensions in
    the data](img/4351OS_07_47.jpg), respectively. In the context of PCA, the matrix
    ![Reducing dimensions in the data](img/4351OS_07_50.jpg) is termed as the **reduction
    component** and the matrix ![Reducing dimensions in the data](img/4351OS_07_49.jpg)
    is termed as the **rotation component** of the sample data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将 SVD 视为将大小为 ![降低数据维度](img/4351OS_07_48.jpg) 的矩阵 ![降低数据维度](img/4351OS_07_47.jpg)
    分解为三个矩阵 ![降低数据维度](img/4351OS_07_49.jpg)，![降低数据维度](img/4351OS_07_50.jpg)，和 ![降低数据维度](img/4351OS_07_51.jpg)。矩阵
    ![降低数据维度](img/4351OS_07_49.jpg) 的大小为 ![降低数据维度](img/4351OS_07_52.jpg)，矩阵 ![降低数据维度](img/4351OS_07_50.jpg)
    的大小为 ![降低数据维度](img/4351OS_07_48.jpg)，矩阵 ![降低数据维度](img/4351OS_07_51.jpg) 的大小为 ![降低数据维度](img/4351OS_07_53.jpg)。矩阵
    ![降低数据维度](img/4351OS_07_47.jpg) 实际上代表了具有 ![降低数据维度](img/4351OS_07_55.jpg) 维度的 ![降低数据维度](img/4351OS_07_54.jpg)
    输入向量。矩阵 ![降低数据维度](img/4351OS_07_50.jpg) 是一个对角矩阵，被称为矩阵 ![降低数据维度](img/4351OS_07_47.jpg)
    的 **奇异值**，而矩阵 ![降低数据维度](img/4351OS_07_49.jpg) 和 ![降低数据维度](img/4351OS_07_51.jpg)
    分别被称为 ![降低数据维度](img/4351OS_07_47.jpg) 的 **左奇异向量** 和 **右奇异向量**。在 PCA 的上下文中，矩阵 ![降低数据维度](img/4351OS_07_50.jpg)
    被称为 **降维成分**，而矩阵 ![降低数据维度](img/4351OS_07_49.jpg) 被称为样本数据的 **旋转成分**。
- en: 'The PCA algorithm to reduce the ![Reducing dimensions in the data](img/4351OS_07_55.jpg)
    dimensions in the ![Reducing dimensions in the data](img/4351OS_07_54.jpg) input
    vectors to ![Reducing dimensions in the data](img/4351OS_07_56.jpg) dimensions
    can be summarized using the following steps:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入向量的 ![降低数据维度](img/4351OS_07_55.jpg) 维度降低到 ![降低数据维度](img/4351OS_07_56.jpg)
    维度的 PCA 算法可以总结如下：
- en: Calculate the covariance matrix ![Reducing dimensions in the data](img/4351OS_07_43.jpg)
    from the input vectors ![Reducing dimensions in the data](img/4351OS_07_12.jpg).
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输入向量![数据降维](img/4351OS_07_12.jpg)计算协方差矩阵![数据降维](img/4351OS_07_43.jpg)。
- en: Calculate the matrices ![Reducing dimensions in the data](img/4351OS_07_49.jpg),
    ![Reducing dimensions in the data](img/4351OS_07_50.jpg), and ![Reducing dimensions
    in the data](img/4351OS_07_51.jpg) by applying SVD on the covariance matrix ![Reducing
    dimensions in the data](img/4351OS_07_43.jpg).
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对协方差矩阵![数据降维](img/4351OS_07_43.jpg)应用奇异值分解（SVD），计算矩阵![数据降维](img/4351OS_07_49.jpg)，![数据降维](img/4351OS_07_50.jpg)，和![数据降维](img/4351OS_07_51.jpg)。
- en: From the ![Reducing dimensions in the data](img/4351OS_07_52.jpg) matrix ![Reducing
    dimensions in the data](img/4351OS_07_49.jpg), select the first ![Reducing dimensions
    in the data](img/4351OS_07_56.jpg) columns to produce the matrix ![Reducing dimensions
    in the data](img/4351OS_07_57.jpg), which is termed as the **reduced left singular
    vector** or **reduced rotation matrix** of the matrix ![Reducing dimensions in
    the data](img/4351OS_07_43.jpg). This matrix represents the ![Reducing dimensions
    in the data](img/4351OS_07_56.jpg) principal components of the sample data and
    will have a size of ![Reducing dimensions in the data](img/4351OS_07_58.jpg).
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从![数据降维](img/4351OS_07_52.jpg)矩阵![数据降维](img/4351OS_07_49.jpg)中，选择前![数据降维](img/4351OS_07_56.jpg)列以生成矩阵![数据降维](img/4351OS_07_57.jpg)，该矩阵被称为矩阵![数据降维](img/4351OS_07_43.jpg)的**降维左奇异向量**或**降维旋转矩阵**。此矩阵代表了样本数据的![数据降维](img/4351OS_07_56.jpg)个主成分，并将具有![数据降维](img/4351OS_07_58.jpg)的大小。
- en: Calculate the vectors with ![Reducing dimensions in the data](img/4351OS_07_56.jpg)
    dimensions, denoted by ![Reducing dimensions in the data](img/4351OS_07_32.jpg),
    as follows:![Reducing dimensions in the data](img/4351OS_07_59.jpg)
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算具有![数据降维](img/4351OS_07_56.jpg)维度的向量，用![数据降维](img/4351OS_07_32.jpg)表示，如下所示：![数据降维](img/4351OS_07_59.jpg)
- en: Note that the input to the PCA algorithm is the set of input vectors ![Reducing
    dimensions in the data](img/4351OS_07_12.jpg) from the sample data after mean
    normalization and feature scaling.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，PCA算法的输入是经过均值归一化和特征缩放后的样本数据集的输入向量集![数据降维](img/4351OS_07_12.jpg)。
- en: Since the matrix ![Reducing dimensions in the data](img/4351OS_07_57.jpg) calculated
    in the preceding steps has ![Reducing dimensions in the data](img/4351OS_07_56.jpg)
    columns, the matrix ![Reducing dimensions in the data](img/4351OS_07_32.jpg) will
    have a size of ![Reducing dimensions in the data](img/4351OS_07_60.jpg), which
    represents the ![Reducing dimensions in the data](img/4351OS_07_54.jpg) input
    vectors in ![Reducing dimensions in the data](img/4351OS_07_56.jpg) dimensions.
    We should note that a lower value of the number of dimensions ![Reducing dimensions
    in the data](img/4351OS_07_56.jpg) could result in a higher loss of variance in
    the data. Hence, we should choose ![Reducing dimensions in the data](img/4351OS_07_56.jpg)
    such that only a small fraction of the variance is lost.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在前面步骤中计算的矩阵![数据降维](img/4351OS_07_57.jpg)有![数据降维](img/4351OS_07_56.jpg)列，矩阵![数据降维](img/4351OS_07_32.jpg)将具有![数据降维](img/4351OS_07_60.jpg)的大小，它代表了![数据降维](img/4351OS_07_56.jpg)维度的![数据降维](img/4351OS_07_54.jpg)个输入向量。我们应该注意，维度数![数据降维](img/4351OS_07_56.jpg)的降低可能会导致数据方差损失增加。因此，我们应该选择![数据降维](img/4351OS_07_56.jpg)，使得方差损失尽可能小。
- en: 'The original input vectors ![Reducing dimensions in the data](img/4351OS_07_12.jpg)
    can be recreated from the matrix ![Reducing dimensions in the data](img/4351OS_07_32.jpg)
    and the reduced left singular vector ![Reducing dimensions in the data](img/4351OS_07_57.jpg)
    as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 原始输入向量![数据降维](img/4351OS_07_12.jpg)可以通过矩阵![数据降维](img/4351OS_07_32.jpg)和降维后的左奇异向量![数据降维](img/4351OS_07_57.jpg)如下重建：
- en: '![Reducing dimensions in the data](img/4351OS_07_61.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![数据降维](img/4351OS_07_61.jpg)'
- en: The Incanter library includes some functions to perform PCA. In the example
    that will follow, we will use PCA to provide a better visualization of the Iris
    dataset.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Incanter库包含一些执行PCA的函数。在接下来的示例中，我们将使用PCA来提供Iris数据集的更好可视化。
- en: Note
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The namespace declaration of the upcoming example should look similar to the
    following declaration:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例中的命名空间声明应类似于以下声明：
- en: '[PRE60]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We first define the training data using the `get-dataset`, `to-matrix`, and
    `sel` functions, as shown in the following code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用 `get-dataset`、`to-matrix` 和 `sel` 函数定义训练数据，如下面的代码所示：
- en: '[PRE61]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Similar to the previous example, we will use the first four columns of the Iris
    dataset as sample data for the input variables of the training data.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的例子类似，我们将使用 Iris 数据集的前四列作为训练数据的输入变量样本数据。
- en: 'PCA is performed by the `principal-components` function from the `incanter.stats`
    namespace. This function returns a map that contains the rotation matrix ![Reducing
    dimensions in the data](img/4351OS_07_49.jpg) and the reduction matrix ![Reducing
    dimensions in the data](img/4351OS_07_50.jpg) from PCA, which we described earlier.
    We can select columns from the reduction matrix of the input data using the `sel`
    function as shown in the following code:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 通过 `incanter.stats` 命名空间中的 `principal-components` 函数执行。此函数返回一个包含旋转矩阵 ![数据维度降低](img/4351OS_07_49.jpg)
    和降低矩阵 ![数据维度降低](img/4351OS_07_50.jpg) 的映射，这些矩阵是我们之前描述的。我们可以使用 `sel` 函数从输入数据的降低矩阵中选择列，如下面的代码所示：
- en: '[PRE62]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'As shown in the preceding code, the rotation matrix of the PCA of the input
    data can be fetched using the `:rotation` keyword on the value returned by the
    `principal-components` function. We can now calculate the reduced features *Z*
    using the reduced rotation matrix and the original matrix of features represented
    by the `iris-features` variable, as shown in the following code:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面代码所示，可以通过 `principal-components` 函数返回的值上的 `:rotation` 关键字获取输入数据的 PCA 旋转矩阵。现在我们可以使用降低旋转矩阵和由
    `iris-features` 变量表示的特征原始矩阵来计算降低特征 *Z*，如下面的代码所示：
- en: '[PRE63]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The reduced features can then be visualized by selecting the first two columns
    of the `reduced-features` matrix and plotting them using the `scatter-plot` function,
    as shown in the following code:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择 `reduced-features` 矩阵的前两列并使用 `scatter-plot` 函数进行绘图，可以可视化降低特征，如下面的代码所示：
- en: '[PRE64]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The following plot is generated on calling the `plot-reduced-features` function
    defined in the preceding code:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表是在调用前面代码中定义的 `plot-reduced-features` 函数时生成的：
- en: '![Reducing dimensions in the data](img/4351OS_07_62.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![数据维度降低](img/4351OS_07_62.jpg)'
- en: The scatter plot illustrated in the preceding diagram gives us a good visualization
    of the distribution of the input data. The blue and green clusters in the preceding
    plot are shown to have similar values for the given set of features. In summary,
    the Incanter library supports PCA, which allows for the easy visualization of
    some sample data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 前面图表中展示的散点图为我们提供了输入数据分布的良好可视化。前面图表中的蓝色和绿色簇显示，对于给定的特征集，这些簇具有相似的价值。总之，Incanter
    库支持主成分分析（PCA），这使得可视化一些样本数据变得简单。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we explored several clustering algorithms that can be used
    to model some unlabeled data. The following are some of the other points that
    we have covered:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了可以用于建模一些未标记数据的几种聚类算法。以下是我们已经涵盖的一些其他要点：
- en: We explored the *K*-means algorithm and hierarchical clustering techniques while
    providing sample implementations of these methods in pure Clojure. We also described
    how we can leverage these techniques through the `clj-ml` library.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们探讨了 *K*-均值算法和层次聚类技术，同时提供了这些方法在纯 Clojure 中的示例实现。我们还描述了如何通过 `clj-ml` 库利用这些技术。
- en: We discussed the EM algorithm, which is a probabilistic clustering technique,
    and also described how we can use the `clj-ml` library to build an EM clusterer.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了 EM 算法，这是一种概率聚类技术，并描述了如何使用 `clj-ml` 库构建一个 EM 聚类器。
- en: We also explored how we can use SOMs to fit clustering problems with a high
    number of dimensions. We also demonstrated how we can use the Incanter library
    to build an SOM that can be used for clustering.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还探讨了如何使用自组织映射（SOMs）来拟合具有高维度的聚类问题。我们还演示了如何使用 Incanter 库构建一个用于聚类的 SOM。
- en: Lastly, we studied dimensionality reduction and PCA, and how we can use PCA
    to provide a better visualization of the Iris dataset using the Incanter library.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们研究了降维和 PCA，以及如何使用 Incanter 库通过 PCA 提供更好的 Iris 数据集可视化。
- en: In the following chapter, we will explore the concepts of anomaly detection
    and recommendation systems using machine learning techniques.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨使用机器学习技术来探索异常检测和推荐系统概念。
