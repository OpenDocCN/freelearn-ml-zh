<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer263">
<h1 class="chapter-number" id="_idParaDest-127"><a id="_idTextAnchor169"/>8</h1>
<h1 id="_idParaDest-128"><a id="_idTextAnchor170"/>Exploring Optional Parameters for H2O AutoML</h1>
<p>As we explored in <a href="B17298_02.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with H2O Flow (H2O’s Web UI)</em>, when training models using H2O AutoML, we had plenty of parameters to select. All these parameters gave us the capability to control how H2O AutoML should train our models. This control helps us get the best possible use of AutoML based on our requirements. Most of the parameters we explored were pretty straightforward to understand. However, there were some parameters whose purpose and effects were slightly complex to be understood at the very start of this book.</p>
<p>In this chapter, we shall explore these parameters by learning about the <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) concepts behind them, and then understand how we can use them in an AutoML setting.</p>
<p>By the end of this chapter, you will not only be educated in some of the advanced ML concepts, but you will also be able to implement them using the parametric provisions made in H2O AutoML.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Experimenting with parameters that support imbalanced classes</li>
<li>Experimenting with parameters that support early stopping </li>
<li>Experimenting with parameters that support cross-validation</li>
</ul>
<p>We will start by understanding what imbalanced classes are.</p>
<h1 id="_idParaDest-129"><a id="_idTextAnchor171"/>Technical requirements</h1>
<p>You will require the following to complete this chapter:</p>
<ul>
<li>The latest version of your preferred web browser.</li>
<li>The H2O software installed on your system. Refer to <a href="B17298_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Understanding H2O AutoML Basics</em>, for instructions on how to install H2O on your system.</li>
</ul>
<p class="callout-heading">Tip</p>
<p class="callout">All the H2O AutoML function parameters shown in this chapter are shown using H2O Flow to keep things simple. The equivalent parameters are also available in the Python and R programming languages for software engineers to code into their services. You can find these details at <a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/parameters.xhtml">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/parameters.xhtml</a>.</p>
<h1 id="_idParaDest-130"><a id="_idTextAnchor172"/>Experimenting with parameters that support imbalanced classes</h1>
<p>One common problem<a id="_idIndexMarker904"/> you will often face in the field of ML is classifying rare events. Consider the case of large earthquakes. Large earthquakes of magnitude 7 and higher occur about once every year. If you had a dataset containing the Earth’s tectonic activity of each day since the last decade with the response column containing whether or not an earthquake occurred, then you would have approximately 3,650 rows of data; that is, one row for each day in the decade, with around 8-12 rows showing large earthquakes. That is less than a 0.3% chance that this event will occur. 99.7% of the time, there will be no large earthquakes. This dataset, where<a id="_idIndexMarker905"/> the number of large earthquake events is so small, is called an <strong class="bold">imbalanced dataset</strong>.</p>
<p>The problem with the imbalanced dataset is that even if you write a simple <strong class="source-inline">if-else</strong> function that marks all tectonic events as not earthquakes and call this a model, it will still show the accuracy as 99.7% accuracy since the majority of the events are not earthquake-causing. However, in actuality, this so-called model is very bad as it is not correctly informing you whether it is an earthquake or not.</p>
<p>Such imbalance in the <strong class="bold">target class</strong> creates a lot of issues when<a id="_idIndexMarker906"/> training ML models. The ML models are more likely to assume that these events are so rare that they will never occur and will not learn the distinction between those events.</p>
<p>However, there are ways to tackle this issue. One way is to undersample the majority class and the other way is to oversample the minority class. We shall learn more about these techniques in the upcoming sections. </p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor173"/>Understanding undersampling the majority class</h2>
<p>In the scenario of predicting<a id="_idIndexMarker907"/> the occurrence of earthquakes, the dataset contains<a id="_idIndexMarker908"/> a large number of events that have been identified as <em class="italic">not-earthquake</em>. This event is known as the majority class. The few<a id="_idIndexMarker909"/> events that mark the activity as an <em class="italic">earthquake</em> are known as the<a id="_idIndexMarker910"/> minority class.</p>
<p>Let’s see how <strong class="bold">undersampling the majority class</strong> can solve the problems caused by an imbalance in the classes. Consider the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer252">
<img alt="Figure 8.1 – Undersampling an imbalanced dataset " height="348" src="image/B17298_08_001.jpg" width="1198"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Undersampling an imbalanced dataset</p>
<p>Let’s assume you have 3,640 data samples of tectonic activity that indicate no earthquakes happened and only 10 samples that indicate earthquakes happened. In this case, to tackle this imbalance issue, you must create a bootstrapped dataset containing all 10 samples of the minority class, and 10 samples of the majority class chosen at random from the 3,640 data samples. Then, you can feed this new dataset to H2O AutoML for training. In this case, we have undersampled the majority class and equalized the <em class="italic">earthquake</em> and <em class="italic">not-earthquake</em> data samples before training the model.</p>
<p>The drawback of this approach is that we end up tossing away huge amounts of data, and the model won’t be able to learn a lot from the reduced data. </p>
<h2 id="_idParaDest-132"><a id="_idTextAnchor174"/>Understanding oversampling the minority class</h2>
<p>The second approach<a id="_idIndexMarker911"/> to tackling the imbalanced dataset issue is to <strong class="bold">oversample the minority class</strong>. One obvious way is to duplicate the minority<a id="_idIndexMarker912"/> class data samples and append them to the dataset so that the number of data samples between the majority and minority classes is equal. Refer to the following diagram for a better understanding:</p>
<div>
<div class="IMG---Figure" id="_idContainer253">
<img alt="Figure 8.2 – Oversampling an imbalanced dataset " height="369" src="image/B17298_08_002.jpg" width="1250"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Oversampling an imbalanced dataset</p>
<p>In the preceding diagram, you can see that we replicated the minority class data samples and appended them to the dataset so that we ended up with 3,640 rows for each of the classes.</p>
<p>This approach can work; however, oversampling will lead to an explosion in the size of the dataset. You need to make sure that it does not exceed your computation and memory limits and end up failing.</p>
<p>Now that we’ve covered the basics of class balancing using undersampling and oversampling, let’s see how H2O AutoML handles it using its class balancing parameters.</p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor175"/>Working with class balancing parameters in H2O AutoML</h2>
<p>H2O AutoML has a parameter<a id="_idIndexMarker913"/> called <strong class="source-inline">balance_classes</strong> that accepts a boolean value. If set to <em class="italic">True</em>, H2O performs oversampling on the minority class and undersampling on the majority class. The balancing is performed in such a way that eventually, each class contains the same number of data samples.</p>
<p>Both undersampling and oversampling of the respective classes is done randomly. Additionally, oversampling of the minority class is done with replacement. This means that data samples from the minority class can be chosen and added to the new training dataset multiple times and can be repeated.</p>
<p>H2O AutoML has the following parameters that support class balancing functionality:</p>
<ul>
<li><strong class="source-inline">balance_classes</strong>: This parameter accepts a boolean value. It is <em class="italic">False</em> by default, but if you want to perform class balancing on your dataset before feeding it to H2O AutoML for training, then you can set the boolean value to <em class="italic">True</em>.</li>
</ul>
<p>In H2O Flow, you get a checkbox besides the parameter. Refer to the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer254">
<img alt="Figure 8.3 – The balance_classes checkbox in H2O Flow " height="60" src="image/B17298_08_003.jpg" width="1142"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – The balance_classes checkbox in H2O Flow</p>
<p>Checking it makes the <strong class="source-inline">class_sampling_factors</strong> and <strong class="source-inline">max_after_balance_size</strong> parameters available in the <strong class="bold">EXPERT</strong> section of the <strong class="bold">Run AutoML</strong> parameters, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer255">
<img alt="Figure 8.4 – The class_sampling_factors and max_after_balance_size parameters in the EXPERT section " height="181" src="image/B17298_08_004.jpg" width="1204"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – The class_sampling_factors and max_after_balance_size parameters in the EXPERT section</p>
<ul>
<li><strong class="source-inline">class_sampling_factors</strong>: This parameter requires <strong class="source-inline">balance_classes</strong> to be <em class="italic">True</em>. This parameter takes a list of float values as input that will represent the sampling rate for that class. A sampling rate of value <em class="italic">1.0</em> for a given class will not change its sample rate during class balancing. A sampling rate of <em class="italic">0.5</em> will halve the sample rate of a class during class balancing while a sampling rate of <em class="italic">2.0</em> will double it.</li>
<li><strong class="source-inline">max_after_balance_size</strong>: This parameter requires <strong class="source-inline">balance_classes</strong> to be <em class="italic">True</em> and specifies the maximum relative size of the training dataset after balancing. This parameter accepts a <strong class="source-inline">float</strong> value as input, which would limit the size your training dataset can grow to. The default value is <em class="italic">5.0</em>, which indicates that the training dataset will grow a maximum of <em class="italic">5</em> times its size. This value can also be less than <em class="italic">1.0</em>.</li>
</ul>
<p>In the Python programming<a id="_idIndexMarker914"/> language, you can set these parameters as follows:</p>
<pre class="source-code">
aml = h2o.automl.H2OAutoML(balance_classes = True, class_sampling_factors =[0.3, 2.0], max_after_balance_size=0.95, seed = 123)
aml.train(x = features, y = label, training_frame = train_dataframe)</pre>
<p>Similarly, in the R programming language, you can set these parameters as follows:</p>
<pre class="source-code">
aml &lt;- h2o.automl(x = features, y = label, training_frame = train_dataframe, seed = 123, balance_classes = TRUE, class_sampling_factors = c(0.3, 2.0), max_after_balance_size=0.95)</pre>
<p>To perform class balancing when training models using AutoML, you can set the <strong class="source-inline">balance_classes</strong> parameter to true in the H2O AutoML estimator object. In that same object, you can specify your <strong class="source-inline">class_sampling_factors</strong> and <strong class="source-inline">max_after_balance_size</strong> parameters. Then, you can use this initialized AutoML estimator object to trigger AutoML on your training dataset.</p>
<p>Now that you understand how we can tackle the class imbalance issue using the <strong class="source-inline">balance_classes</strong>, <strong class="source-inline">class_sampling_factors</strong>, and <strong class="source-inline">max_after_balance_size</strong> parameters, let’s understand<a id="_idIndexMarker915"/> the next optional parameters in AutoML – that is, stopping criteria.</p>
<h1 id="_idParaDest-134"><a id="_idTextAnchor176"/>Experimenting with parameters that support early stopping</h1>
<p><strong class="bold">Overfitting</strong> models is one of the common issues often faced<a id="_idIndexMarker916"/> when trying to solve an ML problem. Overfitting is said to have occurred when the ML model tries to adapt to your training set too much, so much so that it is only able to make predictions on values that it has seen before in the training set and is unable to make a generalized prediction on unseen data.</p>
<p>Overfitting occurs due to a variety of reasons, one of them being that the model learns so much from the dataset that it even incorporates and learns the noise in the dataset. This learning negatively impacts predictions on new data that may not have that noise. So, how do we tackle this issue and prevent the model from overfitting? Stop the model early before it learns the noise.</p>
<p>In the following sub-sections, we shall understand what early stopping is and how it is done. Then, we will learn how the early stopping parameters offered by H2O AutoML work. </p>
<h2 id="_idParaDest-135"><a id="_idTextAnchor177"/>Understanding early stopping</h2>
<p><strong class="bold">Early stopping</strong> is a form of <strong class="bold">regularization</strong> that stops a model’s training<a id="_idIndexMarker917"/> once it has achieved a satisfactory understanding of the data and further prevents it from overfitting. Early stopping aims to observe the model’s performance as it improves using an appropriate performance metric and stop the model’s training once deterioration is observed due to overfitting.</p>
<p>When training a model using algorithms that use iterative optimization to minimize the loss function, the training dataset is passed through the algorithm during each iteration. Observations and understandings that pass are then used during the next iteration. This iteration<a id="_idIndexMarker918"/> of passing the training dataset through the algorithm is called an <strong class="bold">epoch</strong>.</p>
<p>For early stopping, at the end of every epoch, we can calculate the performance of the model and note down the metric value. Comparing these values during every iteration helps us understand whether the model is improving its performance after every epoch or whether it is learning noise and losing performance. We can monitor this and stop the model training at the epoch where we start seeing a decrease in performance. Refer to the following diagram to gain a better understanding of early stopping:</p>
<div>
<div class="IMG---Figure" id="_idContainer256">
<img alt="Figure 8.5 – Early stopping to avoid model overfitting " height="411" src="image/B17298_08_005.jpg" width="831"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Early stopping to avoid model overfitting</p>
<p>In the preceding diagram, on the <em class="italic">Y</em> axis, we have the <strong class="bold">Performance</strong> value of the model. On the <em class="italic">X</em>-axis, we have the <strong class="bold">Epoch</strong> value. So, as time goes<a id="_idIndexMarker919"/> on and we iterate through the number of epochs, we see that the performance of the model on the training set and the validation set continues to increase. But after a certain point, the performance of the model on the validation dataset starts decreasing, while the performance of the training dataset continues to increase. This is where overfitting starts. The model learns too much from the training dataset and starts incorporating noise into its learning. This might show high performance on the training dataset, but the model fails to generalize the predictions. This leads to bad predictions on unseen data, such as the ones in the validation dataset.</p>
<p>So, the best thing to do is to stop the model at the exact point where the performance of the model is highest for both the training and validation dataset.</p>
<p>Now that we have a basic understanding of how early stopping<a id="_idIndexMarker920"/> of model training works, let’s learn how we can perform it using the early stopping parameter offered by the H2O AutoML function.</p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor178"/>Working with early stopping parameters in H2O AutoML</h2>
<p>H2O AutoML has provisions<a id="_idIndexMarker921"/> for you to implement and control the early stopping of your models that it will auto train for you.</p>
<p>You can use the following parameters to implement early stopping:</p>
<ul>
<li><strong class="source-inline">stopping_rounds</strong>: This parameter indicates the number of training rounds over which if the stopping metric fails to improve, we stop the model training.</li>
<li><strong class="source-inline">stopping_metric</strong>: This parameter is used to select the performance metric to consider when early stopping. It is available if <strong class="source-inline">stopping_rounds</strong> is set and is greater than <em class="italic">0</em>. We studied performance metrics in <a href="B17298_06.xhtml#_idTextAnchor129"><em class="italic">Chapter 6</em></a>, <em class="italic">Understanding H2O AutoML Leaderboard and Other Performance Metrics</em>, so kindly refer to that chapter if you wish to revise how the different metrics measure performance. The available options for this parameter are as follows:<ul><li><strong class="source-inline">AUTO</strong>: This is the default value and further defaults to the following values, depending on the type of ML problem:<ul><li><strong class="source-inline">logloss</strong>: The default stopping metric for classification problems.</li><li><strong class="source-inline">deviance</strong>: The default stopping metric for regression problems. This stands for mean residual deviance.</li><li><strong class="source-inline">anomaly_score</strong>: The default stopping metric for Isolation Forest models, which are a type of ensemble model.</li></ul></li><li><strong class="source-inline">anomaly_score</strong>: The default stopping metric for Isolation Forest models (ensemble models). It is the measure of normality of an observation equivalent to the number of splits in a decision tree needed to isolate a point in a given tree where that point is at max depth.</li><li><strong class="source-inline">deviance</strong>: This stands for mean residual deviance. This value tells us how well the label value can be predicted by a model based on the number of features in the dataset.</li><li><strong class="source-inline">logloss</strong>: Log loss is a metric that is a way of measuring the performance of a classification model that outputs classification results in the form of probability values.</li><li><strong class="source-inline">MSE</strong> (<strong class="bold">Mean Squared Error</strong>): This is a metric that measures<a id="_idIndexMarker922"/> the mean of the squares of errors of the predicted value against the actual value.</li><li><strong class="source-inline">RMSE</strong> (<strong class="bold">Root Mean Squared Error</strong>): This is a metric that calculates<a id="_idIndexMarker923"/> the root value of the MSE.</li><li><strong class="source-inline">MAE</strong> (<strong class="bold">Mean Absolute Error</strong>): This is a metric that calculates<a id="_idIndexMarker924"/> the average magnitude of errors in each set of observations.</li><li><strong class="source-inline">RMSLE</strong> (<strong class="bold">Root Mean Squared Logarithmic Error</strong>): This is a metric that calculates the RMSE<a id="_idIndexMarker925"/> of the log-transformed observed and log-transformed actual values.</li><li><strong class="source-inline">AUC</strong> (<strong class="bold">Area Under the ROC Curve</strong>): AUC-ROC is a metric that helps us compare which classification<a id="_idIndexMarker926"/> algorithm performed better, depending on whose ROC curve covers the most area.</li><li><strong class="source-inline">AUCPR</strong> (<strong class="bold">Area Under the Precision-Recall Curve</strong>): AUCPR is similar to the AUC-ROC<a id="_idIndexMarker927"/> curve, with the only difference being that the PR curve is a function that uses precision on the <em class="italic">Y</em> axis and recall on the <em class="italic">X</em> axis.</li><li><strong class="source-inline">lift_top_group</strong>: This parameter configures AutoML in such a way that the model<a id="_idIndexMarker928"/> being trained must improve its lift within the top 1% of the training data. Lift is nothing but the measure of performance of a model in making accurate predictions, compared to a model that randomly makes predictions. The top 1% of the dataset are the observations with the highest predicted values.</li><li><strong class="source-inline">misclassification</strong>: This metric is used to measure the fraction of the predictions that were incorrectly predicted without distinguishing between positive and negative predictions.</li><li><strong class="source-inline">mean_per_class_error</strong>: This is a metric that calculates the average of all errors per class in a dataset containing multiple classes.</li><li><strong class="source-inline">custom</strong>: This parameter <a id="_idIndexMarker929"/>is used to set any custom metric as the stopping metric during AutoML training. The custom metric should be of the behavior <em class="italic">less is better</em>, meaning the lower the value of the custom metric, the better the performance of the model. The lower bound value of the custom metric is assumed to be 0.</li><li><strong class="source-inline">custom_increasing</strong>: This parameter is for custom performance metrics that have the behavior as <em class="italic">more is better</em>, meaning the higher the value of these metrics, the better the model performance. At the time of writing, this parameter is only supported in the Python client for GBM and DRF.</li></ul></li>
<li><strong class="source-inline">stopping_tolerance</strong>: This parameter indicates the tolerance value by which the model’s performance metric must improve before stopping the model training. It is available if <strong class="source-inline">stopping_rounds</strong> is set and is greater than <em class="italic">0</em>. The default stopping tolerance for AutoML is <em class="italic">0.001</em> if the dataset contains at least 1 million rows; otherwise, the value is determined by the size of the dataset and the amount of non-NA data in the dataset, which leads to a value greater than <em class="italic">0.001</em></li>
</ul>
<p>In H2O Flow, these parameters are available in the <strong class="bold">ADVANCED</strong> section of the <strong class="bold">Run AutoML</strong> parameters, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer257">
<img alt="Figure 8.6 – Early stopping parameters in H2O Flow " height="193" src="image/B17298_08_006.jpg" width="1185"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Early stopping pa<a id="_idTextAnchor179"/>rame<a id="_idTextAnchor180"/>ters in H2O Flow</p>
<p>In the Python programming language, you can set these parameters as follows:</p>
<p class="source-code">aml = h2o.automl.H2OAutoML(stopping_metric = "mse", stopping_rounds = 5, stopping_tolerance = 0.001)</p>
<p class="source-code">aml.train(x = features, y = label, training_frame = train_dataframe)</p>
<p>In the R programming<a id="_idIndexMarker930"/> language, you can set these parameters as follows:</p>
<p class="source-code">aml &lt;- h2o.automl(x = features, y = label, training_frame = train_dataframe, seed = 123, stopping_metric = "mse", stopping_rounds = 5, stopping_tolerance = 0.001)</p>
<p>To better understand how AutoML will stop a model's training early, consider the same Python and R example values. We have <strong class="source-inline">stopping_metric</strong> as <strong class="bold">mse</strong>, <strong class="source-inline">stopping_rounds</strong> as <strong class="bold">5</strong>, and <strong class="source-inline">stopping_tolerance</strong> as <strong class="bold">0.001</strong>.</p>
<p>When implementing early stopping, H2O will calculate the moving average of the last <strong class="bold">6</strong> stopping rounds, where the average of the very first round is used as a reference for the next rounds. If the ratio between the best moving average and the reference moving average is greater than or equal to a <strong class="source-inline">stopping_tolerance</strong> of <em class="italic">0.001</em>, then H2O will stop the model training. For performance metrics that have the <em class="italic">more is better</em> behavior, the ratio between the best moving average and reference moving average should be less than or equal to the stopping tolerance.</p>
<p>Now that we understand how to stop model<a id="_idIndexMarker931"/> training early using the <strong class="source-inline">stopping_rounds</strong>, <strong class="source-inline">stopping_metrics</strong>, and <strong class="source-inline">stopping tolerance</strong> parameters, let’s understand the next optional parameter in AutoML – that is, cross-validation.</p>
<h1 id="_idParaDest-137"><a id="_idTextAnchor181"/>Experimenting with parameters that support cross-validation</h1>
<p>When performing model training<a id="_idIndexMarker932"/> on a dataset, we usually perform a train-test split on the dataset. Let’s assume we split it in the ratio of 70% and 30%, where 70% is used to create the training dataset and the remaining 30% is used to create the test dataset. Then, we pass the training dataset to the ML system for training and use the test dataset to calculate the performance of the model. A train-test split is often performed in a random state, meaning 70% of the data that was used to create the training dataset is often chosen at random from the original dataset without replacement, except in the case of time-series data, where the order of the events needs to be maintained or in the case where we need to keep the classes stratified. Similarly, for the test dataset, 30% of the data is chosen at random from the original dataset to create the test dataset.</p>
<p>The following diagram shows how data from the dataset is randomly picked to create the training and testing datasets for their respective purposes:</p>
<div>
<div class="IMG---Figure" id="_idContainer258">
<img alt="Figure 8.7 – Train-test split on the dataset " height="227" src="image/B17298_08_007.jpg" width="752"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Train-test split on the dataset</p>
<p>Now, the issue with the train-test split is that when 30% of the data kept outside the testing dataset is not used to train the model, any missing knowledge that could be derived from this data isn’t available to train the model. This leads to a loss in the performance of the model. If you retrain a model using a different random state for the train-test split, then the model will end up having a different performance level as it has been trained on different data records. Thus, the performance of the models depends on the random assignment of the training dataset. So, how can we provide the test data for training as well as keeping some<a id="_idIndexMarker933"/> test data for performance measurement? This is where cross-validation comes into play.</p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor182"/>Understanding cross-validation</h2>
<p><strong class="bold">Cross-validation</strong> is a model validation technique<a id="_idIndexMarker934"/> that resamples data to train and test models. The technique uses different parts of the dataset for training and testing during each iteration. Multiple iterations of model training and testing are performed using different parts of the dataset. The performance results are combined to give an average estimation of the model’s performance.</p>
<p>Let’s try to understand this with an example. Let’s assume your dataset contains around 1,000 records. To perform cross-validation, you must split the dataset into a ratio – let’s assume a 1:9 ratio where we have 100 records for the test dataset and 900 records for the training dataset. Then, you perform model training on the training dataset. Once the model has been trained, you must test the model on the test dataset and note its performance. This is your first iteration of cross-validation.</p>
<p>In the next iteration, you split the dataset in the same ratio of 1/9 records for the testing and training datasets, respectively, but this time, you choose different data records to form your test dataset and use the remaining records as the training dataset. Then, you perform model training on the training dataset and calculate the model’s performance on the testing dataset. You repeat the same experiment using different data records until all the dataset has been used for training as well as testing. You will need to perform around 10 iterations of cross-validation so that, during the entire cross-validation process, the model is trained and tested on the entire dataset each iteration while containing different data records in the testing DataFrame. </p>
<p>Once all the iterations have finished, you must combine the performance results of the experiments and provide the average estimation of the model’s performance. This technique is called cross-validation.</p>
<p>You may have noticed that during cross-validation, we perform model training multiple times on the same dataset. This is expected to increase the overall ML process time. This is especially true when performing cross-validation on a large dataset with a very high ratio between the training and testing partition. For example, if we have a dataset that contains 30,000 rows and we split the dataset into 29,000 rows for training and 1,000 rows for testing, then this will lead to a total of 3,000 iterations of model training and testing. Hence, there is an alternative form of cross-validation that lets you choose how<a id="_idIndexMarker935"/> many iterations to perform: called <strong class="bold">K-fold cross-validation</strong>.</p>
<p>In K-fold cross-validation, you decide the value of <strong class="bold">K</strong>, which is used to determine the number of cross-validation<a id="_idIndexMarker936"/> iterations to perform. Depending on the value of K, the ML service will randomly partition the dataset into K equal subsamples that will be resampled over the cross-validation iterations. The following diagram will help you understand this:</p>
<div>
<div class="IMG---Figure" id="_idContainer259">
<img alt="Figure 8.8 – K-fold cross-validation where K=3 " height="943" src="image/B17298_08_008.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – K-fold cross-validation where K=3</p>
<p>As you can see, we have a dataset that contains 30,000 data records and the chosen value of K in the K-fold cross-validation is 3. Accordingly, the dataset will be split into 20,000 records for the test dataset and 10,000 records for training, which will be resampled during the following iterations, leading to a total of three cross-validations.</p>
<p>The benefit of using K-fold cross-validation to perform your model validation is that the model is trained on the entire dataset without missing out on data during training. This is especially beneficial in multi-class classification problems where there are chances that the model might miss out training  on some of the prediction classes because it got split out from the training dataset to be used in the testing dataset.</p>
<p>Now that we have a better understanding of the basics of cross-validation and how it works, let’s see how we can perform<a id="_idIndexMarker937"/> it using special parameters in the H2O AutoML training function.</p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor183"/>Working with cross-validation parameters in H2O AutoML</h2>
<p>H2O AutoML has provisions<a id="_idIndexMarker938"/> for you to implement K-fold cross-validation on your data for all ML algorithms that support it, along with some additional information that may help support the implementation.</p>
<p>You can use the following parameters to implement cross-validation:</p>
<ul>
<li><strong class="source-inline">nfolds</strong>: This parameter sets the number of folds to use for K-fold cross-validation.</li>
</ul>
<p>In H2O Flow, this parameter will be available in the <strong class="bold">ADVANCED</strong> section of the <strong class="bold">Run AutoML</strong> parameters, as shown in the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer260">
<img alt="Figure 8.9 – The nfolds parameter in H2O Flow " height="95" src="image/B17298_08_009.jpg" width="1332"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – The nfolds parameter in H2O Flow</p>
<ul>
<li><strong class="source-inline">fold_assignment</strong>: This parameter is used to specify the fold assignment scheme to use to perform K-fold cross-validation. The various types of fold assignments you can set are as follows:<ul><li><strong class="source-inline">AUTO</strong>: This assignment value lets the model training algorithm choose the fold assignment to use. <strong class="source-inline">AUTO</strong> currently uses <strong class="source-inline">Random</strong> as the fold assignment.</li><li><strong class="source-inline">Random</strong>: This assignment value is used to randomly split the dataset based on the <strong class="source-inline">nfolds</strong> value. This value is set by default if <strong class="source-inline">nfolds &gt; 0</strong> and <strong class="source-inline">fold_column</strong> is not specified.</li><li><strong class="source-inline">Modulo</strong>: This assignment value is used to perform a modulo operation when splitting the folds based on the <strong class="source-inline">nfolds</strong> value.</li><li><strong class="source-inline">Stratified</strong>: This assignment value is used to arrange the folds based on the response variable for<a id="_idTextAnchor184"/> classification problems.</li></ul></li>
</ul>
<p>In the Python programming<a id="_idIndexMarker939"/> language, you can set these parameters as follows:</p>
<p class="source-code">aml = h2o.automl.H2OAutoML(nfolds = 10, fold_assignment = "AUTO", seed = 123)</p>
<p class="source-code">aml.train(x = features, y = label, training_frame = train_dataframe)</p>
<p>In the R programming language, you can set these parameters as follows:</p>
<p class="source-code">aml &lt;- h2o.automl(x = features, y = label, training_frame = train_dataframe, seed = 123, nfolds = 10, fold_assignment = "AUTO")</p>
<ul>
<li><strong class="source-inline">fold_column</strong>: This parameter is used to specify the fold assignment based on the contents of a column rather than any procedural assignment technique. You can custom set the fold values per row in the dataset by creating a separate column containing the fold IDs and then setting <strong class="source-inline">fold_column</strong> to the custom column’s name.</li>
</ul>
<p>In H2O Flow, this parameter will be available in the <strong class="bold">ADVANCED</strong> section of the <strong class="bold">Run AutoML</strong> parameters, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer261">
<img alt="Figure 8.10 – The fold_column parameter in H2O Flow " height="53" src="image/B17298_08_010.jpg" width="1151"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – The fold_column parameter in H2O Flow</p>
<p>In the Python programming language, you can set these parameters as follows:</p>
<p class="source-code">aml.train(x = features, y = label, training_frame = train_dataframe, fold_column = "fold_column_name")</p>
<p>In the R programming language, you can set these parameters as follows:</p>
<p class="source-code">aml &lt;- h2o.automl(x = features, y = label, training_frame = train_dataframe, seed = 123, fold_column="fold_numbers")</p>
<ul>
<li><strong class="source-inline">keep_cross_validation_predictions</strong>: When performing K-fold cross-validation, H2O will train <em class="italic">K+1</em> number of models, where <em class="italic">K</em> number of models are trained<a id="_idIndexMarker940"/> as a part of cross-validation and <em class="italic">1</em> additional model is trained on the entire dataset. Each of the cross-validation models makes predictions on the test DataFrame for that iteration and the predicted values are stored in a prediction frame. You can save these prediction frames by setting this parameter to <em class="italic">True</em>. By default, this parameter is set to <em class="italic">False</em>.</li>
<li><strong class="source-inline">keep_cross_validation_models</strong>: Similar to <strong class="source-inline">keep_cross_validation_predictions</strong>, you can also choose to keep the models trained during cross-validation for further inspection and experimentation by enabling this parameter to <em class="italic">True</em>. By default, this parameter is set to <em class="italic">False</em>.</li>
<li><strong class="source-inline">keep_cross_validation_fold_assignment</strong>: During cross-validation, the data is split either by the <strong class="source-inline">fold_cloumn</strong> or <strong class="source-inline">fold_assignment</strong> parameter. You can save the fold assignment that was used in cross-validation by setting this parameter to <em class="italic">True</em>. By default, this parameter is set to <em class="italic">False</em>.</li>
</ul>
<p>In H2O Flow, these parameters will be available in the <strong class="bold">EXPERT</strong> section of the <strong class="bold">Run AutoML</strong> parameters, as shown in the following screenshot.</p>
<div>
<div class="IMG---Figure" id="_idContainer262">
<img alt="Figure 8.11 – Advanced cross-validation parameters in H2O Flow " height="217" src="image/B17298_08_011.jpg" width="1402"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Advanced cross-validation parameters in H2O Flow</p>
<p>In the Python programming language, you can set these parameters as follows:</p>
<p class="source-code">aml = h2o.automl.H2OAutoML(nfolds = 10, keep_cross_validation_fold_assignment = True, keep_cross_validation_models = True, keep_cross_validation_predictions= True, seed = 123)</p>
<p class="source-code">aml.train(x = features, y = label, training_frame = train_dataframe)</p>
<p>In the R programming language, you can set these parameters as follows:</p>
<p class="source-code">aml &lt;- h2o.automl(x = features, y = label, training_frame = train_dataframe, seed = 123, nfolds = 10, keep_cross_validation_fold_assignment = TRUE, keep_cross_validation_models = TRUE, keep_cross_validation_predictions= TRUE)</p>
<p>Congratulations – you have now understood<a id="_idIndexMarker941"/> a few more advanced ML concepts and how to use them in H2O AutoML!</p>
<h1 id="_idParaDest-140"><a id="_idTextAnchor185"/>Summary</h1>
<p>In this chapter, we learned about some of the optional parameters that are available to us in H2O AutoML. We started by understanding what imbalanced classes in a dataset are and how they can cause trouble when training models. Then, we understood oversampling and undersampling, which we can use to tackle this. After that, we learned how H2O AutoML provides parameters for us to control the sampling techniques so that we can handle imbalanced classes in datasets.</p>
<p>After that, we understood another concept, called early stopping. We understood how overtraining can lead to an overfitted ML model that performs very poorly against unseen new data. We also learned that early stopping is a method that we can use to stop model training once we start noticing that the model has started overfitting by monitoring the performance of the model against the validation dataset. We then learned about the various parameters that H2O AutoML has that we can use to automatically stop model training once overfitting occurs during model training.</p>
<p>Next, we understood what cross-validation is and how it helps us train the model on the entire dataset, as well as validate the model’s performance as if the model had seen the data for the first time. We also learned how K-fold cross-validation helps us control the number of cross-validation iterations to be performed during model training. Then, we explored how H2O AutoML has various provisions for performing cross-validation during AutoML training. Finally, we learned how we can keep the cross-validation models and predictions if we wish to perform more experiments on them, as well as how we can store the cross-validation fold assignments.</p>
<p>In the next chapter, we shall explore some of the miscellaneous features that H2O AutoML has that can be useful to us in certain scenarios.</p>
</div>
<div>
<div id="_idContainer264">
</div>
</div>
</div></body></html>