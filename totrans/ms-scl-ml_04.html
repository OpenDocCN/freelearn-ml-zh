<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 4. Supervised and Unsupervised Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Supervised and Unsupervised Learning</h1></div></div></div><p>I covered the basics of the MLlib library in the previous chapter, but MLlib, at least at the time of writing this book, is more like a fast-moving target that is gaining the lead rather than a well-structured implementation that everyone uses in production or even has a consistent and tested documentation. In this situation, as people say, rather than giving you the fish, I will try to focus on well-established concepts behind the libraries and teach the process of fishing in this book in order to avoid the need to drastically modify the chapters with each new MLlib release. For better or worse, this increasingly seems to be a skill that a data scientist needs to possess.</p><p>Statistics and machine learning inherently deal with uncertainty, due to one or another reason we covered in <a class="link" href="ch02.xhtml" title="Chapter 2. Data Pipelines and Modeling">Chapter 2</a>, <span class="emphasis"><em>Data Pipelines and Modeling</em></span>. While some datasets might be completely random, the goal here is to find trends, structure, and patterns beyond what a random number generator will provide you. The fundamental value of ML is that we can generalize these patterns and improve on at least some metrics. Let's see what basic tools are available within Scala/Spark.</p><p>In this chapter, I am covering supervised and unsupervised leaning, the two historically different approaches. Supervised learning is traditionally used when we have a specific goal to predict a label, or a specific attribute of a dataset. Unsupervised learning can be used to understand internal structure and dependencies between any attributes of a dataset, and is often used to group the records or attributes in meaningful clusters.  In practice, both methods can be used to complement and aid each other.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Learning standard models for supervised learning – decision trees and logistic regression</li><li class="listitem" style="list-style-type: disc">Discussing the staple of unsupervised learning – k-means clustering and its derivatives</li><li class="listitem" style="list-style-type: disc">Understanding metrics and methods to evaluate the effectiveness of the above algorithms</li><li class="listitem" style="list-style-type: disc">Having a glimpse of extending the above methods on special cases of streaming data, sparse data, and non-structured data</li></ul></div><div class="section" title="Records and supervised learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"/>Records and supervised learning</h1></div></div></div><p>For the purpose <a id="id293" class="indexterm"/>of this chapter, a record is an observation or measurement of one or several attributes. We assume that the observations might contain noise <span class="inlinemediaobject"><img src="Images/B04935_04_01F.jpg" alt="Records and supervised learning" width="23" height="32"/></span> (or be inaccurate for one or other reason):</p><div class="mediaobject"><img src="Images/B04935_04_02F.jpg" alt="Records and supervised learning" width="88" height="37"/></div><p>While we believe that there is some pattern or correlation between the attributes, the one that we are after and want to uncover, the noise is uncorrelated across either the attributes or the records. In statistical terms, we say that the values for each record are drawn from the same distribution and are independent (or <span class="emphasis"><em>i.i.d</em></span>. in statistical terms). The order of records does not matter. One of the attributes, usually the first, might be designated to be the label.</p><p>Supervised learning is when the goal is to predict the label <span class="emphasis"><em>yi</em></span>:</p><div class="mediaobject"><img src="Images/B04935_04_03F.jpg" alt="Records and supervised learning" width="143" height="33"/></div><p>Here, <span class="emphasis"><em>N</em></span> is the number of remaining attributes. In other words, the goal is to generalize the patterns so that we can predict the label by just knowing the other attributes, whether because we cannot physically get the measurement or just want to explore the structure of the dataset without having the immediate goal to predict the label.</p><p>The unsupervised learning is when we don't use the label—we just try to explore the structure and correlations to understand the dataset to, potentially, predict the label better. The number of problems in this latter category has increased recently with the emergence of learning for unstructured data and streams, each of which, I'll be covering later in the book in separate chapters.</p><div class="section" title="Iris dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec26"/>Iris dataset</h2></div></div></div><p>I will demonstrate<a id="id294" class="indexterm"/> the concept of records and labels based on<a id="id295" class="indexterm"/> one of the most famous datasets in machine learning, the Iris dataset (<a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Iris">https://archive.ics.uci.edu/ml/datasets/Iris</a>). The Iris dataset <a id="id296" class="indexterm"/>contains 50 records for each of the three types of Iris flower, 150 lines of total five fields. Each line is a measurement of the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Sepal length in cm</li><li class="listitem" style="list-style-type: disc">Sepal width in cm</li><li class="listitem" style="list-style-type: disc">Petal length in cm</li><li class="listitem" style="list-style-type: disc">Petal width in cm</li></ul></div><p>With the final field being the type of the flower (<span class="emphasis"><em>setosa</em></span>, <span class="emphasis"><em>versicolor</em></span>, or <span class="emphasis"><em>virginica</em></span>). The classic problem is to predict the label, which, in this case, is a categorical attribute with three possible values as a function of the first four attributes:</p><div class="mediaobject"><img src="Images/B04935_04_04F.jpg" alt="Iris dataset" width="187" height="33"/></div><p>One option would be to draw a plane in the four-dimensional space that separates all four labels. Unfortunately, as one can find out, while one of the classes is clearly separable, the remaining two are not, as shown in the following multidimensional scatterplot (we have used Data Desk software to create it):</p><div class="mediaobject"><img src="Images/B04935_04_01.jpg" alt="Iris dataset" width="800" height="538"/><div class="caption"><p>Figure 04-1. The Iris dataset as a three-dimensional plot. The Iris setosa records, shown by crosses, can be separated from the other two types based on petal length and width.</p></div></div><p>The colors and shapes are assigned according to the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Label</p>
</th><th style="text-align: left" valign="bottom">
<p>Color</p>
</th><th style="text-align: left" valign="bottom">
<p>Shape</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>Iris setosa</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Blue</p>
</td><td style="text-align: left" valign="top">
<p>x</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>Iris versicolor</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Green</p>
</td><td style="text-align: left" valign="top">
<p>Vertical bar</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>Iris virginica</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Purple</p>
</td><td style="text-align: left" valign="top">
<p>Horizontal bar</p>
</td></tr></tbody></table></div><p>The <span class="emphasis"><em>Iris setosa</em></span> is separable because it happens to have a very short petal length and width compared to the two other types.</p><p>Let's see how <a id="id297" class="indexterm"/>we can use MLlib to find that separating <a id="id298" class="indexterm"/>multidimensional plane.</p></div><div class="section" title="Labeled point"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec27"/>Labeled point</h2></div></div></div><p>The labeled datasets <a id="id299" class="indexterm"/>used to have a very special place<a id="id300" class="indexterm"/> in ML—we will discuss unsupervised learning later in the chapter, where we do not need a label, so MLlib has a special data type to represent a record <a id="id301" class="indexterm"/>with a <code class="literal">org.apache.spark.mllib.regression.LabeledPoint</code> label (refer to <a class="ulink" href="https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point">https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point</a>). To read the Iris dataset from a text file, we need to transform the original UCI repository file into the so-called LIBSVM text format. While there are plenty of converters from CSV to LIBSVM format, I'd like to use a simple AWK script to do the job:</p><div class="informalexample"><pre class="programlisting">awk -F, '/setosa/ {print "0 1:"$1" 2:"$2" 3:"$3" 4:"$4;}; /versicolor/ {print "1 1:"$1" 2:"$2" 3:"$3" 4:"$4;}; /virginica/ {print "1 1:"$1" 2:"$2" 3:"$3" 4:"$4;};' iris.csv &gt; iris-libsvm.txt
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>
<span class="strong"><strong>Why do we need LIBSVM format?</strong></span>
</p><p>LIBSVM is <a id="id302" class="indexterm"/>the format that many libraries use. First, LIBSVM takes only continuous attributes. While a lot of datasets in the real world contain discrete or categorical attributes, internally they are always converted to a numerical representation for efficiency reasons, even if the L1 or L2 metrics on the resulting numerical attribute does not make much sense in the unordered discrete values. Second, the LIBSVM format allows for efficient sparse data representation. While the Iris dataset is not sparse, almost all of the modern big data sources are sparse, and the format allows for efficient storage by only storing the provided values. Many modern big data key-value and traditional RDBMS databases actually do the same for efficiency reasons.</p></div></div><p>The code<a id="id303" class="indexterm"/> might be more complex for missing values, but we know<a id="id304" class="indexterm"/> that the Iris dataset is not sparse—otherwise we'd complement our code with a bunch of if statements. We mapped the last two labels to 1 for our purpose now.</p></div><div class="section" title="SVMWithSGD"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec28"/>SVMWithSGD</h2></div></div></div><p>Now, let's<a id="id305" class="indexterm"/> run the <span class="strong"><strong>Linear Support Vector Machine</strong></span> (<span class="strong"><strong>SVM</strong></span>) <a id="id306" class="indexterm"/>SVMWithSGD code<a id="id307" class="indexterm"/> from MLlib:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>scala&gt; val data = MLUtils.loadLibSVMFile(sc, "iris-libsvm.txt")</strong></span>
<span class="strong"><strong>data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112</strong></span>
<span class="strong"><strong>scala&gt; val splits = data.randomSplit(Array(0.6, 0.4), seed = 123L)</strong></span>
<span class="strong"><strong>splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:26, MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:26)</strong></span>
<span class="strong"><strong>scala&gt; val training = splits(0).cache()</strong></span>
<span class="strong"><strong>training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:26</strong></span>
<span class="strong"><strong>scala&gt; val test = splits(1)</strong></span>
<span class="strong"><strong>test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:26</strong></span>
<span class="strong"><strong>scala&gt; val numIterations = 100</strong></span>
<span class="strong"><strong>numIterations: Int = 100</strong></span>
<span class="strong"><strong>scala&gt; val model = SVMWithSGD.train(training, numIterations)</strong></span>
<span class="strong"><strong>model: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 4, numClasses = 2, threshold = 0.0</strong></span>
<span class="strong"><strong>scala&gt; model.clearThreshold()</strong></span>
<span class="strong"><strong>res0: model.type = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 4, numClasses = 2, threshold = None</strong></span>
<span class="strong"><strong>scala&gt; val scoreAndLabels = test.map { point =&gt;</strong></span>
<span class="strong"><strong>     |   val score = model.predict(point.features)</strong></span>
<span class="strong"><strong>     |   (score, point.label)</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>scoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[212] at map at &lt;console&gt;:36</strong></span>
<span class="strong"><strong>scala&gt; val metrics = new BinaryClassificationMetrics(scoreAndLabels)</strong></span>
<span class="strong"><strong>metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@692e4a35</strong></span>
<span class="strong"><strong>scala&gt; val auROC = metrics.areaUnderROC()</strong></span>
<span class="strong"><strong>auROC: Double = 1.0</strong></span>

<span class="strong"><strong>scala&gt; println("Area under ROC = " + auROC)</strong></span>
<span class="strong"><strong>Area under ROC = 1.0</strong></span>
<span class="strong"><strong>scala&gt; model.save(sc, "model")</strong></span>
<span class="strong"><strong>SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".</strong></span>
<span class="strong"><strong>SLF4J: Defaulting to no-operation (NOP) logger implementation</strong></span>
<span class="strong"><strong>SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</strong></span>
</pre></div><p>So, you just<a id="id308" class="indexterm"/> run one of the most complex algorithms in the <a id="id309" class="indexterm"/>machine learning toolbox: SVM. The result is a separating plane that distinguishes <span class="emphasis"><em>Iris setosa</em></span> flowers from the other two types. The model in this case is exactly the intercept and the coefficients of the plane that best separates the labels:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; model.intercept</strong></span>
<span class="strong"><strong>res5: Double = 0.0</strong></span>

<span class="strong"><strong>scala&gt; model.weights</strong></span>
<span class="strong"><strong>res6: org.apache.spark.mllib.linalg.Vector = [-0.2469448809675877,-1.0692729424287566,1.7500423423258127,0.8105712661836376]</strong></span>
</pre></div><p>If one looks under the hood, the model is stored in a <code class="literal">parquet</code> file, which can be dumped using <code class="literal">parquet-tool</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ parquet-tools dump model/data/part-r-00000-7a86b825-569d-4c80-8796-8ee6972fd3b1.gz.parquet</strong></span>
<span class="strong"><strong>…</strong></span>
<span class="strong"><strong>DOUBLE weights.values.array </strong></span>
<span class="strong"><strong>----------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>*** row group 1 of 1, values 1 to 4 *** </strong></span>
<span class="strong"><strong>value 1: R:0 D:3 V:-0.2469448809675877</strong></span>
<span class="strong"><strong>value 2: R:1 D:3 V:-1.0692729424287566</strong></span>
<span class="strong"><strong>value 3: R:1 D:3 V:1.7500423423258127</strong></span>
<span class="strong"><strong>value 4: R:1 D:3 V:0.8105712661836376</strong></span>

<span class="strong"><strong>DOUBLE intercept </strong></span>
<span class="strong"><strong>----------------------------------------------------------------------------------------------------------------------------------------------</strong></span>
<span class="strong"><strong>*** row group 1 of 1, values 1 to 1 *** </strong></span>
<span class="strong"><strong>value 1: R:0 D:1 V:0.0</strong></span>
<span class="strong"><strong>…</strong></span>
</pre></div><p>The <span class="strong"><strong>Receiver Operating Characteristic</strong></span> (<span class="strong"><strong>ROC</strong></span>) is a common measure of the classifier to be<a id="id310" class="indexterm"/> able to correctly rank the records <a id="id311" class="indexterm"/>according to their numeric label. We will consider precision metrics in more detail in <a class="link" href="ch09.xhtml" title="Chapter 9. NLP in Scala">Chapter 9</a>, <span class="emphasis"><em>NLP in Scala</em></span>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip07"/>Tip</h3><p>
<span class="strong"><strong>What is ROC?</strong></span>
</p><p>ROC has <a id="id312" class="indexterm"/>emerged in signal processing with the first application to<a id="id313" class="indexterm"/> measure the accuracy of analog radars. The common measure of accuracy is area under ROC, which, shortly, is the probability of two randomly chosen points to be ranked correctly according to their labels (the <span class="emphasis"><em>0</em></span> label should always have a lower rank than the <span class="emphasis"><em>1</em></span> label). AUROC has a number of attractive characteristics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The value, at least theoretically, does not depend on the oversampling rate, that is, the rate at which we see <span class="emphasis"><em>0</em></span> labels as opposed to <span class="emphasis"><em>1</em></span> labels.</li><li class="listitem" style="list-style-type: disc">The value does not depend on the sample size, excluding the expected variance due to the limited sample size.</li><li class="listitem" style="list-style-type: disc">Adding a constant to the final score does not change the ROC, thus the intercept can always be set to <span class="emphasis"><em>0</em></span>. Computing the ROC requires a sort with respect to the generated score.</li></ul></div></div></div><p>Of course, separating the remaining two labels is a harder problem since the plane that separated <span class="emphasis"><em>Iris versicolor</em></span> from <span class="emphasis"><em>Iris virginica</em></span> does not exist: the AUROC score will be less than <span class="emphasis"><em>1.0</em></span>. However, the SVM method will find the plane that best differentiates between the latter two classes.</p></div><div class="section" title="Logistic regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec29"/>Logistic regression</h2></div></div></div><p>Logistic regression<a id="id314" class="indexterm"/> is one of the oldest classification<a id="id315" class="indexterm"/> methods. The outcome of the logistic regression is also a set of weights, which define the hyperplane, but the loss function is logistic loss instead of <span class="emphasis"><em>L2</em></span>:</p><div class="mediaobject"><img src="Images/B04935_04_05F.jpg" alt="Logistic regression" width="165" height="40"/></div><p>Logit function is a frequent choice when the label is binary (as <span class="emphasis"><em>y = +/- 1</em></span> in the above equation):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.SparkContext</strong></span>
<span class="strong"><strong>import org.apache.spark.SparkContext</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.evaluation.MulticlassMetrics</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.evaluation.MulticlassMetrics</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>scala&gt; val data = MLUtils.loadLibSVMFile(sc, "iris-libsvm-3.txt")</strong></span>
<span class="strong"><strong>data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112</strong></span>
<span class="strong"><strong>scala&gt; val splits = data.randomSplit(Array(0.6, 0.4))</strong></span>
<span class="strong"><strong>splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:29, MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:29)</strong></span>
<span class="strong"><strong>scala&gt; val training = splits(0).cache()</strong></span>
<span class="strong"><strong>training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:29</strong></span>
<span class="strong"><strong>scala&gt; val test = splits(1)</strong></span>
<span class="strong"><strong>test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:29</strong></span>
<span class="strong"><strong>scala&gt; val model = new LogisticRegressionWithLBFGS().setNumClasses(3).run(training)</strong></span>
<span class="strong"><strong>model: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 8, numClasses = 3, threshold = 0.5</strong></span>
<span class="strong"><strong>scala&gt; val predictionAndLabels = test.map { case LabeledPoint(label, features) =&gt;</strong></span>
<span class="strong"><strong>     |   val prediction = model.predict(features)</strong></span>
<span class="strong"><strong>     |   (prediction, label)</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>predictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[67] at map at &lt;console&gt;:37</strong></span>
<span class="strong"><strong>scala&gt; val metrics = new MulticlassMetrics(predictionAndLabels)</strong></span>
<span class="strong"><strong>metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@6d5254f3</strong></span>
<span class="strong"><strong>scala&gt; val precision = metrics.precision</strong></span>
<span class="strong"><strong>precision: Double = 0.9516129032258065</strong></span>
<span class="strong"><strong>scala&gt; println("Precision = " + precision)</strong></span>
<span class="strong"><strong>Precision = 0.9516129032258065</strong></span>
<span class="strong"><strong>scala&gt; model.intercept</strong></span>
<span class="strong"><strong>res5: Double = 0.0</strong></span>
<span class="strong"><strong>scala&gt; model.weights</strong></span>
<span class="strong"><strong>res7: org.apache.spark.mllib.linalg.Vector = [10.644978886788556,-26.850171485157578,3.852594349297618,8.74629386938248,4.288703063075211,-31.029289381858273,9.790312529377474,22.058196856491996]</strong></span>
</pre></div><p>The<a id="id316" class="indexterm"/> labels in this case can be any integer in the<a id="id317" class="indexterm"/> range <span class="emphasis"><em>[0, k)</em></span>, where <span class="emphasis"><em>k</em></span> is the total number of classes (the correct class will be determined by building multiple binary logistic regression models against the pivot class, which in this case, is the class with the <span class="emphasis"><em>0</em></span> label) (<span class="emphasis"><em>The Elements of Statistical Learning</em></span> by <span class="emphasis"><em>Trevor Hastie</em></span>, <span class="emphasis"><em>Robert Tibshirani</em></span>, <span class="emphasis"><em>Jerome Friedman</em></span>, <span class="emphasis"><em>Springer Series in Statistics</em></span>).</p><p>The accuracy metric is precision, or the percentage of records predicted correctly (which is 95% in our case).</p></div><div class="section" title="Decision tree"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec30"/>Decision tree</h2></div></div></div><p>The <a id="id318" class="indexterm"/>preceding two methods describe linear models. Unfortunately, the <a id="id319" class="indexterm"/>linear approach does not always work for complex interactions between attributes. Assume that the label looks like an exclusive <span class="emphasis"><em>OR: 0</em></span> if <span class="emphasis"><em>X ≠ Y</em></span> and <span class="emphasis"><em>1</em></span> if <span class="emphasis"><em>X = Y</em></span>:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>X</p>
</th><th style="text-align: left" valign="bottom">
<p>Y</p>
</th><th style="text-align: left" valign="bottom">
<p>Label</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr></tbody></table></div><p>There is no hyperplane that can differentiate between the two labels in the <span class="emphasis"><em>XY</em></span> space. Recursive split solution, where the split on each level is made on only one variable or a linear combination thereof might work a bit better in these case. Decision trees are also known to <a id="id320" class="indexterm"/>work well with sparse and interaction-rich <a id="id321" class="indexterm"/>datasets:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.DecisionTree</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.DecisionTree</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.model.DecisionTreeModel</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.model.DecisionTreeModel</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.configuration.Strategy</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.configuration.Strategy</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.configuration.Algo.Classification</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.configuration.Algo.Classification</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.impurity.{Entropy, Gini}</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.impurity.{Entropy, Gini}</strong></span>
<span class="strong"><strong>scala&gt; val data = MLUtils.loadLibSVMFile(sc, "iris-libsvm-3.txt")</strong></span>
<span class="strong"><strong>data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112</strong></span>

<span class="strong"><strong>scala&gt; val splits = data.randomSplit(Array(0.7, 0.3), 11L)</strong></span>
<span class="strong"><strong>splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:30, MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:30)</strong></span>
<span class="strong"><strong>scala&gt; val (trainingData, testData) = (splits(0), splits(1))</strong></span>
<span class="strong"><strong>trainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:30</strong></span>
<span class="strong"><strong>testData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:30</strong></span>
<span class="strong"><strong>scala&gt; val strategy = new Strategy(Classification, Gini, 10, 3, 10)</strong></span>
<span class="strong"><strong>strategy: org.apache.spark.mllib.tree.configuration.Strategy = org.apache.spark.mllib.tree.configuration.Strategy@4110e631</strong></span>
<span class="strong"><strong>scala&gt; val dt = new DecisionTree(strategy)</strong></span>
<span class="strong"><strong>dt: org.apache.spark.mllib.tree.DecisionTree = org.apache.spark.mllib.tree.DecisionTree@33d89052</strong></span>
<span class="strong"><strong>scala&gt; val model = dt.run(trainingData)</strong></span>
<span class="strong"><strong>model: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 6 with 21 nodes</strong></span>
<span class="strong"><strong>scala&gt; val labelAndPreds = testData.map { point =&gt;</strong></span>
<span class="strong"><strong>     |   val prediction = model.predict(point.features)</strong></span>
<span class="strong"><strong>     |   (point.label, prediction)</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>labelAndPreds: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[32] at map at &lt;console&gt;:36</strong></span>
<span class="strong"><strong>scala&gt; val testErr = labelAndPreds.filter(r =&gt; r._1 != r._2).count.toDouble / testData.count()</strong></span>
<span class="strong"><strong>testErr: Double = 0.02631578947368421</strong></span>
<span class="strong"><strong>scala&gt; println("Test Error = " + testErr)</strong></span>
<span class="strong"><strong>Test Error = 0.02631578947368421</strong></span>

<span class="strong"><strong>scala&gt; println("Learned classification tree model:\n" + model.toDebugString)</strong></span>
<span class="strong"><strong>Learned classification tree model:</strong></span>
<span class="strong"><strong>DecisionTreeModel classifier of depth 6 with 21 nodes</strong></span>
<span class="strong"><strong>  If (feature 3 &lt;= 0.4)</strong></span>
<span class="strong"><strong>   Predict: 0.0</strong></span>
<span class="strong"><strong>  Else (feature 3 &gt; 0.4)</strong></span>
<span class="strong"><strong>   If (feature 3 &lt;= 1.7)</strong></span>
<span class="strong"><strong>    If (feature 2 &lt;= 4.9)</strong></span>
<span class="strong"><strong>     If (feature 0 &lt;= 5.3)</strong></span>
<span class="strong"><strong>      If (feature 1 &lt;= 2.8)</strong></span>
<span class="strong"><strong>       If (feature 2 &lt;= 3.9)</strong></span>
<span class="strong"><strong>        Predict: 1.0</strong></span>
<span class="strong"><strong>       Else (feature 2 &gt; 3.9)</strong></span>
<span class="strong"><strong>        Predict: 2.0</strong></span>
<span class="strong"><strong>      Else (feature 1 &gt; 2.8)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>     Else (feature 0 &gt; 5.3)</strong></span>
<span class="strong"><strong>      Predict: 1.0</strong></span>
<span class="strong"><strong>    Else (feature 2 &gt; 4.9)</strong></span>
<span class="strong"><strong>     If (feature 0 &lt;= 6.0)</strong></span>
<span class="strong"><strong>      If (feature 1 &lt;= 2.4)</strong></span>
<span class="strong"><strong>       Predict: 2.0</strong></span>
<span class="strong"><strong>      Else (feature 1 &gt; 2.4)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>     Else (feature 0 &gt; 6.0)</strong></span>
<span class="strong"><strong>      Predict: 2.0</strong></span>
<span class="strong"><strong>   Else (feature 3 &gt; 1.7)</strong></span>
<span class="strong"><strong>    If (feature 2 &lt;= 4.9)</strong></span>
<span class="strong"><strong>     If (feature 1 &lt;= 3.0)</strong></span>
<span class="strong"><strong>      Predict: 2.0</strong></span>
<span class="strong"><strong>     Else (feature 1 &gt; 3.0)</strong></span>
<span class="strong"><strong>      Predict: 1.0</strong></span>
<span class="strong"><strong>    Else (feature 2 &gt; 4.9)</strong></span>
<span class="strong"><strong>     Predict: 2.0</strong></span>
<span class="strong"><strong>scala&gt; model.save(sc, "dt-model")</strong></span>
<span class="strong"><strong>SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".</strong></span>
<span class="strong"><strong>SLF4J: Defaulting to no-operation (NOP) logger implementation</strong></span>
<span class="strong"><strong>SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</strong></span>
</pre></div><p>As you<a id="id322" class="indexterm"/> can see, the error (misprediction) rate on hold-out 30% sample is only 2.6%. The 30% sample of 150 is only 45 records, which means we missed <a id="id323" class="indexterm"/>only 1 record from the whole test set. Certainly, the result might and will change with a different seed, and we need a more rigorous cross-validation technique to prove the accuracy of the model, but this is enough for a rough estimate of model performance.</p><p>Decision tree generalizes on regression case, that is, when the label is continuous in nature. In this case, the splitting criterion is minimization of weighted variance, as opposed to entropy gain or gini in the case of classification. I will talk more about the differences in <a class="link" href="ch05.xhtml" title="Chapter 5. Regression and Classification">Chapter 5</a>, <span class="emphasis"><em>Regression and Classification</em></span>.</p><p>There are a number of parameters, which can be tuned to improve the performance:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Parameter</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th><th style="text-align: left" valign="bottom">
<p>Recommended value</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">maxDepth</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is<a id="id324" class="indexterm"/> the maximum depth of the tree. Deep trees are costly and usually are more likely to overfit. Shallow trees are more efficient and better for bagging/boosting algorithms such as AdaBoost.</p>
</td><td style="text-align: left" valign="top">
<p>This depends on the size of the original dataset. It is worth experimenting and plotting the accuracy of the resulting tree versus the parameter to find out the optimum.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">minInstancesPerNode</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id325" class="indexterm"/> also limits the size of the tree: once the number of instances falls under this threshold, no further splitting occurs.</p>
</td><td style="text-align: left" valign="top">
<p>The value is usually 10-100, depending on the complexity of the original dataset and the number of potential labels.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">maxBins</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is<a id="id326" class="indexterm"/> used only for continuous attributes: the number of bins to split the original range.</p>
</td><td style="text-align: left" valign="top">
<p>Large number of bins increase computation and communication cost. One can also consider the option of pre-discretizing the attribute based on domain knowledge.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">minInfoGain</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id327" class="indexterm"/> is the amount of information gain (entropy), impurity (gini), or variance (regression) gain to split a node.</p>
</td><td style="text-align: left" valign="top">
<p>The default is <span class="emphasis"><em>0</em></span>, but you can increase the default to limit the tree size and reduce the risk of overfitting.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">maxMemoryInMB</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is<a id="id328" class="indexterm"/> the amount of memory to be used for collecting sufficient statistics.</p>
</td><td style="text-align: left" valign="top">
<p>The default value is conservatively chosen to be 256 MB to allow the decision algorithm to work in most scenarios. Increasing <code class="literal">maxMemoryInMB</code> can lead to faster training (if the memory is available) by allowing fewer passes over the data. However, there may be decreasing returns as <code class="literal">maxMemoryInMB</code> grows, as the amount of communication on each iteration can be proportional to <code class="literal">maxMemoryInMB</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">subsamplingRate</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is <a id="id329" class="indexterm"/>the fraction of the training data used for learning the decision tree.</p>
</td><td style="text-align: left" valign="top">
<p>This parameter is most relevant for training ensembles of trees (using <code class="literal">RandomForest</code> and <code class="literal">GradientBoostedTrees</code>), where it can be useful to subsample the original data. For training a single decision tree, this parameter is less useful since the number of training instances is generally not the main constraint.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">useNodeIdCache</code>
</p>
</td><td style="text-align: left" valign="top">
<p>If this <a id="id330" class="indexterm"/>is set to true, the algorithm will avoid passing the current model (tree or trees) to executors on each iteration.</p>
</td><td style="text-align: left" valign="top">
<p>This can be useful with deep trees (speeding up computation on workers) and for large random forests (reducing communication on each iteration).</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">checkpointDir:</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is <a id="id331" class="indexterm"/>the directory for checkpointing the node ID cache RDDs.</p>
</td><td style="text-align: left" valign="top">
<p>This is an optimization to save intermediate results to avoid recomputation in case of node failure. Set it in large clusters or with unreliable nodes.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">checkpointInterval</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is<a id="id332" class="indexterm"/> the frequency for checkpointing the node ID cache RDDs.</p>
</td><td style="text-align: left" valign="top">
<p>Setting this too low will cause extra overhead from writing to HDFS and setting this too high can cause problems if executors fail and the RDD needs to be recomputed.</p>
</td></tr></tbody></table></div></div><div class="section" title="Bagging and boosting – ensemble learning methods"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec31"/>Bagging and boosting – ensemble learning methods</h2></div></div></div><p>As a portfolio of stocks has better characteristics compared to individual equities, models can be<a id="id333" class="indexterm"/> combined to produce better <a id="id334" class="indexterm"/>classifiers. Usually, these methods work really well with decision trees as the training technique can be modified to produce models with large variations. One way is to train the model on random subsets of the original data or random subsets of attributes, which is called random forest. Another way is to generate a sequence of models, where misclassified instances are reweighted to get a larger weight in each subsequent iteration. It has been shown that this method has a relation to gradient descent methods in the model parameter space. While these are valid and interesting techniques, they usually require much more space in terms of model storage and <a id="id335" class="indexterm"/>are less interpretable compared to<a id="id336" class="indexterm"/> bare decision tree models. For Spark, the <a id="id337" class="indexterm"/>ensemble models are currently under development—the umbrella issue is SPARK-3703 (<a class="ulink" href="https://issues.apache.org/jira/browse/SPARK-3703">https://issues.apache.org/jira/browse/SPARK-3703</a>).</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Unsupervised learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Unsupervised learning</h1></div></div></div><p>If we get rid <a id="id338" class="indexterm"/>of the label in the Iris dataset, it would be nice if some algorithm could recover the original grouping, maybe without the exact label names—<span class="emphasis"><em>setosa</em></span>, <span class="emphasis"><em>versicolor</em></span>, and <span class="emphasis"><em>virginica</em></span>. Unsupervised learning has multiple applications in compression and encoding, CRM, recommendation engines, and security to uncover internal structure without actually having the exact labels. The labels sometimes can be given base on the singularity in attribute value distributions. For example, <span class="emphasis"><em>Iris setosa</em></span> can be described as a <span class="emphasis"><em>Flower with Small Leaves</em></span>.</p><p>While a supervised learning problem can always be cast as unsupervised by disregarding the label, the reverse is also true. A clustering algorithm can be cast as a density-estimation problem by assigning label <span class="emphasis"><em>1</em></span> to all vectors and generating random vectors with label <span class="emphasis"><em>0</em></span> (<span class="emphasis"><em>The Elements of Statistical Learning</em></span> by <span class="emphasis"><em>Trevor Hastie</em></span>, <span class="emphasis"><em>Robert Tibshirani</em></span>, <span class="emphasis"><em>Jerome Friedman</em></span>, <span class="emphasis"><em>Springer Series in Statistics</em></span>). The difference between the two is formal and it's even fuzzier with non-structured and nested data. Often, running unsupervised algorithms in labeled datasets leads to a better understanding of the dependencies and thus a better selection and performance of the supervised algorithm.</p><p>One of the most popular algorithms for clustering and unsupervised learning in k-means (and its variants, k-median and k-center, will be described later):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell</strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>
<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; val iris = sc.textFile("iris.txt")</strong></span>
<span class="strong"><strong>iris: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at textFile at &lt;console&gt;:23</strong></span>

<span class="strong"><strong>scala&gt; val vectors = data.map(s =&gt; Vectors.dense(s.split('\t').map(_.toDouble))).cache()</strong></span>
<span class="strong"><strong>vectors: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[5] at map at &lt;console&gt;:25</strong></span>

<span class="strong"><strong>scala&gt; val numClusters = 3</strong></span>
<span class="strong"><strong>numClusters: Int = 3</strong></span>
<span class="strong"><strong>scala&gt; val numIterations = 20</strong></span>
<span class="strong"><strong>numIterations: Int = 20</strong></span>
<span class="strong"><strong>scala&gt; val clusters = KMeans.train(vectors, numClusters, numIterations)</strong></span>
<span class="strong"><strong>clusters: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@5dc9cb99</strong></span>
<span class="strong"><strong>scala&gt; val centers = clusters.clusterCenters</strong></span>
<span class="strong"><strong>centers: Array[org.apache.spark.mllib.linalg.Vector] = Array([5.005999999999999,3.4180000000000006,1.4640000000000002,0.2439999999999999], [6.8538461538461535,3.076923076923076,5.715384615384614,2.0538461538461537], [5.883606557377049,2.740983606557377,4.388524590163936,1.4344262295081966])</strong></span>
<span class="strong"><strong>scala&gt; val SSE = clusters.computeCost(vectors)</strong></span>
<span class="strong"><strong>WSSSE: Double = 78.94506582597859</strong></span>
<span class="strong"><strong>scala&gt; vectors.collect.map(x =&gt; clusters.predict(x))</strong></span>
<span class="strong"><strong>res18: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2)</strong></span>
<span class="strong"><strong>scala&gt; println("Sum of Squared Errors = " + SSE)</strong></span>
<span class="strong"><strong>Sum of Squared Errors = 78.94506582597859</strong></span>
<span class="strong"><strong>scala&gt; clusters.save(sc, "model")</strong></span>
<span class="strong"><strong>SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".</strong></span>
<span class="strong"><strong>SLF4J: Defaulting to no-operation (NOP) logger implementation</strong></span>
<span class="strong"><strong>SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</strong></span>
</pre></div><p>One can see that the first center, the one with index <code class="literal">0</code>, has petal length and width of <code class="literal">1.464</code> and <code class="literal">0.244</code>, which is much shorter than the other two—<code class="literal">5.715</code> and <code class="literal">2.054</code>, <code class="literal">4.389</code> and <code class="literal">1.434</code>). The prediction completely matches the first cluster, corresponding to <span class="emphasis"><em>Iris setosa</em></span>, but has a few mispredictions for the other two.</p><p>The measure <a id="id339" class="indexterm"/>of cluster quality might depend on the (desired) labels if we want to achieve a desired classification result, but since the algorithm has no information about the labeling, a more common measure is the sum of distances from centroids to the points in each of the clusters. Here is a graph of <code class="literal">WSSSE</code>, depending on the number of clusters:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; 1.to(10).foreach(i =&gt; println("i: " + i + " SSE: " + KMeans.train(vectors, i, numIterations).computeCost(vectors)))</strong></span>
<span class="strong"><strong>i: 1 WSSSE: 680.8244</strong></span>
<span class="strong"><strong>i: 2 WSSSE: 152.3687064773393</strong></span>
<span class="strong"><strong>i: 3 WSSSE: 78.94506582597859</strong></span>
<span class="strong"><strong>i: 4 WSSSE: 57.47327326549501</strong></span>
<span class="strong"><strong>i: 5 WSSSE: 46.53558205128235</strong></span>
<span class="strong"><strong>i: 6 WSSSE: 38.9647878510374</strong></span>
<span class="strong"><strong>i: 7 WSSSE: 34.311167589868646</strong></span>
<span class="strong"><strong>i: 8 WSSSE: 32.607859500805034</strong></span>
<span class="strong"><strong>i: 9 WSSSE: 28.231729411088438</strong></span>
<span class="strong"><strong>i: 10 WSSSE: 29.435054384424078</strong></span>
</pre></div><p>As expected, the average distance is decreasing as more clusters are configured. A common method to determine the optimal number of clusters—in our example, we know that there are three types of flowers—is to add a penalty function. A common penalty is the log of the number of clusters as we expect a convex function. What would be the coefficient in front of log? If each vector is associated with its own cluster, the sum of all distances will be zero, so if we would like a metric that achieves approximately the same value at both ends of the set of possible values, <code class="literal">1</code> to <code class="literal">150</code>, the coefficient should be <code class="literal">680.8244/log(150)</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; for (i &lt;- 1.to(10)) println(i + " -&gt; " + ((KMeans.train(vectors, i, numIterations).computeCost(vectors)) + 680 * scala.math.log(i) / scala.math.log(150)))</strong></span>
<span class="strong"><strong>1 -&gt; 680.8244</strong></span>
<span class="strong"><strong>2 -&gt; 246.436635016484</strong></span>
<span class="strong"><strong>3 -&gt; 228.03498068120865</strong></span>
<span class="strong"><strong>4 -&gt; 245.48126639400738</strong></span>
<span class="strong"><strong>5 -&gt; 264.9805962616268</strong></span>
<span class="strong"><strong>6 -&gt; 285.48857890531764</strong></span>
<span class="strong"><strong>7 -&gt; 301.56808340425164</strong></span>
<span class="strong"><strong>8 -&gt; 315.321639004243</strong></span>
<span class="strong"><strong>9 -&gt; 326.47262191671723</strong></span>
<span class="strong"><strong>10 -&gt; 344.87130979355675</strong></span>
</pre></div><p>Here is how the sum of the squared distances with penalty looks as a graph:</p><div class="mediaobject"><img src="Images/B04935_04_02.jpg" alt="Unsupervised learning" width="800" height="515"/><div class="caption"><p>Figure 04-2. The measure of the clustering quality as a function of the number of clusters</p></div></div><p>Besides <a id="id340" class="indexterm"/>k-means clustering, MLlib also has implementations of the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Gaussian mixture</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Power Iteration Clustering</strong></span> (<span class="strong"><strong>PIC</strong></span>)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>)</li><li class="listitem" style="list-style-type: disc">Streaming k-means</li></ul></div><p>The<a id="id341" class="indexterm"/> Gaussian mixture is another classical mechanism, particularly<a id="id342" class="indexterm"/> known for spectral analysis. Gaussian <a id="id343" class="indexterm"/>mixture decomposition is appropriate, where the attributes are continuous and we know that they are likely to come from a set of Gaussian distributions. For example, while the potential groups of points corresponding to clusters may have the average for all attributes, say <span class="strong"><strong>Var1</strong></span> and <span class="strong"><strong>Var2</strong></span>, the points might be centered around two intersecting hyperplanes, as shown in the following diagram:</p><div class="mediaobject"><img src="Images/B04935_04_03.jpg" alt="Unsupervised learning" width="800" height="479"/><div class="caption"><p>Figure 04-3. A mixture of two Gaussians that cannot be properly described by k-means clustering</p></div></div><p>This renders the k-means algorithm ineffective as it will not be able to distinguish between the two (of course a simple non-linear transformation such as a distance to one of the<a id="id344" class="indexterm"/> hyperplanes will solve the problem, but this is where domain knowledge and expertise as a data scientist are handy).</p><p>PIC is using<a id="id345" class="indexterm"/> clustering vertices of a graph provided pairwise similarity measures given as edge properties. It computes a pseudo-eigenvector of the normalized affinity matrix of the graph via power iteration and uses it to cluster vertices. MLlib includes an implementation of PIC using GraphX as its backend. It takes an RDD of (<code class="literal">srcId</code>, <code class="literal">dstId</code>, similarity) tuples and outputs a model with the clustering assignments. The similarities must be non-negative. PIC assumes that the similarity measure is symmetric. A pair (<code class="literal">srcId</code>, <code class="literal">dstId</code>) regardless of the ordering should appear at most once in the input data. If a pair is missing from the input, their similarity is treated as zero.</p><p>LDA can be <a id="id346" class="indexterm"/>used for clustering documents based on keyword frequencies. Rather than estimating a clustering using a traditional distance, LDA uses a function based on a statistical model of how text documents are generated.</p><p>Finally, streaming k-means is a modification of the k-means algorithm, where the clusters can be adjusted with <a id="id347" class="indexterm"/>new batches of data. For each batch of data, we assign all points to their nearest cluster, compute new cluster centers based on the assignment, and then update each cluster parameters using the equations:</p><div class="mediaobject"><img src="Images/B04935_04_06F.jpg" alt="Unsupervised learning" width="147" height="60"/></div><div class="mediaobject"><img src="Images/B04935_04_07F.jpg" alt="Unsupervised learning" width="115" height="32"/></div><p>Here, <span class="emphasis"><em>c</em></span>
<span class="emphasis"><em>t</em></span> and <span class="emphasis"><em>c'</em></span>
<span class="emphasis"><em>t</em></span> are the centers of from the old model and the ones computed for the new batch and <span class="emphasis"><em>n</em></span>
<span class="emphasis"><em>t</em></span> and <span class="emphasis"><em>n'</em></span>
<span class="emphasis"><em>t</em></span> are the number of vectors from the old model and for the new batch. By changing the <span class="emphasis"><em>α</em></span> parameter, we can control how much information from the old runs can<a id="id348" class="indexterm"/> influence the clustering—<span class="emphasis"><em>0</em></span> means the new cluster centers are totally based on the points in the new batch, while <span class="emphasis"><em>1</em></span> means that we accommodate for all points that we have seen so far.</p><p>k-means clustering<a id="id349" class="indexterm"/> has many modifications. For example, k-medians computes the cluster centers as medians of the attribute values, not mean, which works much better for some distributions and with <span class="emphasis"><em>L1</em></span> target distance metric (absolute value of the difference) as opposed to <span class="emphasis"><em>L2</em></span> (the sum of squares). K-medians centers are not necessarily present as a specific point in the dataset. K-medoids is another algorithm from the same family, where the resulting cluster center has to be an actual instance in the input set and we actually do not need to have the global sort, only the pairwise distances between the points. Many variations of the techniques exist on how to choose the original seed cluster centers and converge on the optimal number of clusters (besides the simple log trick I have shown).</p><p>Another big class of clustering algorithms is hierarchical clustering. Hierarchical clustering is either done from the top—akin to the decision tree algorithms—or from the bottom; we first find the closest neighbors, pair them, and continue the pairing process up the hierarchy until all records are merged. The advantage of hierarchical clustering is that it can be made deterministic and relatively fast, even though the cost of one iteration in k-means is probably going to be better. However, as mentioned, the unsupervised problem can actually <a id="id350" class="indexterm"/>be converted to a density-estimation supervised problem, with all the supervised learning techniques available. So have fun understanding the data!</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Problem dimensionality"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Problem dimensionality</h1></div></div></div><p>The larger the attribute space or the number of dimensions, the harder it is to usually predict the label for a given combination of attribute values. This is mostly due to the fact that the total number of possible distinct combinations of attributes increases exponentially <a id="id351" class="indexterm"/>with the dimensionality of the attribute space—at least in the case of discrete variables (in case of continuous variables, the situation is more complex and depends on the metrics used), and it is becoming harder to generalize.</p><p>The effective dimensionality of the problem might be different from the dimensionality of the input space. For example, if the label depends only on the linear combination of the (continuous) input attributes, the problem is called linearly separable and its internal dimensionality is one—we still have to find the coefficients for this linear combination like in logistic regression though.</p><p>This idea is also<a id="id352" class="indexterm"/> sometimes referred to as a <span class="strong"><strong>Vapnik–Chervonenkis</strong></span> (<span class="strong"><strong>VC</strong></span>) dimension of a problem, model, or algorithm—the expressive power of the model depending on how complex the dependencies that it can solve, or shatter, might be. More complex problems require algorithms with higher VC dimensions and larger training sets. However, using an algorithm with higher VC dimension on a simple problem can lead to overfitting and worse generalization to new data.</p><p>If the units of input attributes are comparable, say all of them are meters or units of time, PCA, or more generally, kernel methods, can be used to reduce the dimensionality of the input space:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.feature.PCA</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.feature.PCA</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>scala&gt; val pca = new PCA(2).fit(data.map(_.features))</strong></span>
<span class="strong"><strong>pca: org.apache.spark.mllib.feature.PCAModel = org.apache.spark.mllib.feature.PCAModel@4eee0b1a</strong></span>

<span class="strong"><strong>scala&gt; val reduced = data.map(p =&gt; p.copy(features = pca.transform(p.features)))</strong></span>
<span class="strong"><strong>reduced: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[311] at map at &lt;console&gt;:39</strong></span>
<span class="strong"><strong>scala&gt; reduced.collect().take(10)</strong></span>
<span class="strong"><strong>res4: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,[-2.827135972679021,-5.641331045573367]), (0.0,[-2.7959524821488393,-5.145166883252959]), (0.0,[-2.621523558165053,-5.177378121203953]), (0.0,[-2.764905900474235,-5.0035994150569865]), (0.0,[-2.7827501159516546,-5.6486482943774305]), (0.0,[-3.231445736773371,-6.062506444034109]), (0.0,[-2.6904524156023393,-5.232619219784292]), (0.0,[-2.8848611044591506,-5.485129079769268]), (0.0,[-2.6233845324473357,-4.743925704477387]), (0.0,[-2.8374984110638493,-5.208032027056245]))</strong></span>

<span class="strong"><strong>scala&gt; import scala.language.postfixOps</strong></span>
<span class="strong"><strong>import scala.language.postfixOps</strong></span>

<span class="strong"><strong>scala&gt; pca pc</strong></span>
<span class="strong"><strong>res24: org.apache.spark.mllib.linalg.DenseMatrix = </strong></span>
<span class="strong"><strong>-0.36158967738145065  -0.6565398832858496  </strong></span>
<span class="strong"><strong>0.08226888989221656   -0.7297123713264776  </strong></span>
<span class="strong"><strong>-0.856572105290527    0.17576740342866465  </strong></span>
<span class="strong"><strong>-0.35884392624821626  0.07470647013502865</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</strong></span>
<span class="strong"><strong>scala&gt; val splits = reduced.randomSplit(Array(0.6, 0.4), seed = 1L)</strong></span>
<span class="strong"><strong>splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[312] at randomSplit at &lt;console&gt;:44, MapPartitionsRDD[313] at randomSplit at &lt;console&gt;:44)</strong></span>
<span class="strong"><strong>scala&gt; val training = splits(0).cache()</strong></span>
<span class="strong"><strong>training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[312] at randomSplit at &lt;console&gt;:44</strong></span>
<span class="strong"><strong>scala&gt; val test = splits(1)</strong></span>
<span class="strong"><strong>test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[313] at randomSplit at &lt;console&gt;:44</strong></span>
<span class="strong"><strong>scala&gt; val numIterations = 100</strong></span>
<span class="strong"><strong>numIterations: Int = 100</strong></span>
<span class="strong"><strong>scala&gt; val model = SVMWithSGD.train(training, numIterations)</strong></span>
<span class="strong"><strong>model: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 2, numClasses = 2, threshold = 0.0</strong></span>
<span class="strong"><strong>scala&gt; model.clearThreshold()</strong></span>
<span class="strong"><strong>res30: model.type = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 2, numClasses = 2, threshold = None</strong></span>
<span class="strong"><strong>scala&gt; val scoreAndLabels = test.map { point =&gt;</strong></span>
<span class="strong"><strong>     |   val score = model.predict(point.features)</strong></span>
<span class="strong"><strong>     |   (score, point.label)</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>scoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[517] at map at &lt;console&gt;:54</strong></span>
<span class="strong"><strong>scala&gt; val metrics = new BinaryClassificationMetrics(scoreAndLabels)</strong></span>
<span class="strong"><strong>metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@27f49b8c</strong></span>

<span class="strong"><strong>scala&gt; val auROC = metrics.areaUnderROC()</strong></span>
<span class="strong"><strong>auROC: Double = 1.0</strong></span>
<span class="strong"><strong>scala&gt; println("Area under ROC = " + auROC)</strong></span>
<span class="strong"><strong>Area under ROC = 1.0</strong></span>
</pre></div><p>Here, we <a id="id353" class="indexterm"/>reduced the original four-dimensional problem to two-dimensional. Like averaging, computing linear combinations of input attributes and selecting only those that describe most of the variance helps to reduce noise.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec32"/>Summary</h1></div></div></div><p>In this chapter, we looked at supervised and unsupervised learning and a few examples of how to run them in Spark/Scala. We considered SVM, logistic regression, decision tree, and k-means in the example of UCI Iris dataset. This is in no way a complete guide, and many other libraries either exist or are being made as we speak, but I would bet that you can solve 99% of the immediate data analysis problems just with these tools.</p><p>This will give you a very fast shortcut on how to start being productive with a new dataset. There are many other ways to look at the datasets, but before we get into more advanced topics, let's discuss regression and classification in the next chapter, that is, how to predict continuous and discrete labels.</p></div></div>



  </body></html>