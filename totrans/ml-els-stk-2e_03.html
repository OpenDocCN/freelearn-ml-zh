<html><head></head><body>
		<div id="_idContainer029">
			<h1 id="_idParaDest-32"><em class="italic"><a id="_idTextAnchor033"/>Chapter 2</em>: Enabling and Operationalization</h1>
			<p>We have just learned the basics of what Elastic ML is doing to accomplish both unsupervised automated anomaly detection and supervised data frame analysis. Now it is time to get detailed about how Elastic ML works inside the Elastic Stack (Elasticsearch and Kibana).</p>
			<p>This chapter will focus on both the installation (really, the enablement) of Elastic ML features and a detailed discussion of the logistics of the operation, especially with respect to anomaly detection. Specifically, we will cover the following topics:</p>
			<ul>
				<li>Enabling Elastic ML features</li>
				<li>Understanding operationalization</li>
			</ul>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor034"/>Technical requirements</h1>
			<p>The information in this chapter will use the Elastic Stack as it exists in v7.10 and the workflow of the Elasticsearch Service of Elastic Cloud as of November 2020.</p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor035"/>Enabling Elastic ML features</h1>
			<p>The <a id="_idIndexMarker046"/>process for enabling Elastic ML features inside the Elastic Stack is slightly different if you are doing so within a self-managed cluster <a id="_idIndexMarker047"/>versus using the <strong class="bold">Elasticsearch</strong> <strong class="bold">Service</strong> (<strong class="bold">ESS</strong>) of Elastic Cloud. In short, on a self-managed cluster, the features of ML are enabled via a license key (either a commercial key or a trial key). In ESS, a dedicated ML node needs to be provisioned within the cluster in order to utilize Elastic ML. In the following sections, we will explain the details of how this is accomplished in both scenarios.</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor036"/>Enabling ML on a self-managed cluster</h2>
			<p>If you <a id="_idIndexMarker048"/>have a self-managed cluster that <a id="_idIndexMarker049"/>was created from the downloading of Elastic's default distributions of Elasticsearch and Kibana (available at <a href="http://elastic.co/downloads/">elastic.co/downloads/</a>), enabling Elastic ML features via a license key is very simple. Be sure to not use the Apache 2.0 licensed open source distributions that do not contain the X-Pack code base. </p>
			<p>Elastic ML, unlike the bulk of the capabilities of the Elastic Stack, is not free – it requires a commercial (specifically, a <strong class="bold">Platinum</strong> level) license. It is, however, open source in that the source code is out in the open on GitHub (<a href="http://github.com/elastic/ml-cpp">github.com/elastic/ml-cpp</a>) and that users can look at the code, file issues, make comments, or even execute pull requests. However, the usage of Elastic ML is governed by a commercial agreement with Elastic, the company.</p>
			<p>When <a id="_idIndexMarker050"/>Elastic ML was first released (back <a id="_idIndexMarker051"/>in the v5.x days), it was part of the closed <a id="_idIndexMarker052"/>source features known as <strong class="bold">X-Pack</strong> that required a separate installation step. However, as of version 6.3, the code of X-Pack was "opened" (<a href="http://elastic.co/what-is/open-x-pack">elastic.co/what-is/open-x-pack</a>) and <a id="_idIndexMarker053"/>folded into the default distribution of Elasticsearch and Kibana. Therefore, a separate X-Pack installation step was no longer necessary, just the "enablement" of the features via a commercial license (or a trial license).</p>
			<p>The installation procedure for Elasticsearch and Kibana itself is beyond the scope of this book, but it is easily accomplished by following the online documentation on the Elastic website (available at <a href="http://elastic.co/guide/">elastic.co/guide/</a>).</p>
			<p>Once Elasticsearch and Kibana are running, navigate to the <strong class="bold">Stack</strong> option from the left-side navigation menu and select <strong class="bold">License Management</strong>. You will see a screen like the following:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B17040_02_001.jpg" alt="Figure 2.1 – The License management screen in Kibana&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – The License management screen in Kibana</p>
			<p>Notice that, by <a id="_idIndexMarker054"/>default, the license level <a id="_idIndexMarker055"/>applied is the free <strong class="bold">Basic</strong> tier. This enables you to use some of the advanced features not found in the Apache 2.0 licensed open source distribution, or on third-party services (such as the Amazon Elasticsearch Service). A handy guide for comparing the features that exist at the different license levels can be found on the Elastic website at <a href="http://elastic.co/subscriptions">elastic.co/subscriptions</a>.</p>
			<p>As previously stated, Elastic ML requires a Platinum tier license. If you have purchased a Platinum license from Elastic, you can apply that license by clicking on the <strong class="bold">Update license</strong> button, as shown on the screen in <em class="italic">Figure 2.1</em>. If you do not have a Platinum license, you can start a free 30-day trial by clicking the <strong class="bold">Start my trial</strong> button to enable Elastic ML and the other Platinum features (assuming you agree to the license terms and conditions):</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B17040_02_002.jpg" alt="Figure 2.2 – Starting a free 30-day trial&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Starting a free 30-day trial</p>
			<p>Once this is complete, the licensing screen will indicate that you are now in an active trial of the Platinum features of the Elastic Stack:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B17040_02_003.jpg" alt="Figure 2.3 – Trial license activated&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – Trial license activated</p>
			<p>Once this is <a id="_idIndexMarker056"/>done, you can start to use Elastic ML <a id="_idIndexMarker057"/>right away. Additional configuration steps are needed to take advantage of the other Platinum features, but those steps are outside the scope of this book. Consult the Elastic documentation for further assistance on configuring those features.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor037"/>Enabling ML in the cloud – Elasticsearch Service</h2>
			<p>If <a id="_idIndexMarker058"/>downloading, installing, and self-managing <a id="_idIndexMarker059"/>the Elastic Stack is less interesting <a id="_idIndexMarker060"/>than just getting the Elastic Stack platform offered as a service, then head on <a id="_idIndexMarker061"/>over to Elastic Cloud (<a href="http://cloud.elastic.co">cloud.elastic.co</a>) and sign up for a free trial, using only your email:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B17040_02_004.jpg" alt="Figure 2.4 – Elastic Cloud welcome screen&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Elastic Cloud welcome screen</p>
			<p>You <a id="_idIndexMarker062"/>can then perform <a id="_idIndexMarker063"/>the following <a id="_idIndexMarker064"/>steps:</p>
			<ol>
				<li>Once inside the Elastic Cloud interface after logging in, you will have the ability to start a free trial by clicking the <strong class="bold">Start your free trial</strong> button:<div id="_idContainer020" class="IMG---Figure"><img src="image/B17040_02_005.jpg" alt="Figure 2.5 – Elastic Cloud home screen&#13;&#10;"/></div><p class="figure-caption">Figure 2.5 – Elastic Cloud home screen</p><p>Once <a id="_idIndexMarker065"/>the button is <a id="_idIndexMarker066"/>clicked, you will see that <a id="_idIndexMarker067"/>your 14-day free trial of ESS has started:</p><div id="_idContainer021" class="IMG---Figure"><img src="image/B17040_02_006.jpg" alt="Figure 2.6 – Elasticsearch Service trial enabled&#13;&#10;"/></div><p class="figure-caption">Figure 2.6 – Elasticsearch Service trial enabled</p></li>
				<li>Of <a id="_idIndexMarker068"/>course, in <a id="_idIndexMarker069"/>order to try out Elastic ML, you <a id="_idIndexMarker070"/>first need an Elastic Stack cluster provisioned. There are a few options to create <a id="_idIndexMarker071"/>what ESS refers to as <strong class="bold">deployments</strong>, with some that are tailored to specific use cases. For this example, we will use the <strong class="bold">Elastic Stack</strong> template on the left of <em class="italic">Figure 2.6</em> and choose the <strong class="bold">I/O Optimized</strong> hardware profile, but feel free to experiment with the other options during your trial:<div id="_idContainer022" class="IMG---Figure"><img src="image/B17040_02_007.jpg" alt="Figure 2.7 – Creating an ESS deployment&#13;&#10;"/></div><p class="figure-caption">Figure 2.7 – Creating an ESS deployment</p></li>
				<li>You can also <a id="_idIndexMarker072"/>choose what cloud provider <a id="_idIndexMarker073"/>and which region <a id="_idIndexMarker074"/>to start your cluster in, but most importantly, if you want to use ML features, you must enable an ML node by first clicking on the <strong class="bold">Customize</strong> button near the bottom-right corner.</li>
				<li>After clicking the <strong class="bold">Customize</strong> button, you will see a new screen that allows you to add an ML node:<div id="_idContainer023" class="IMG---Figure"><img src="image/B17040_02_008.jpg" alt="Figure 2.8 – Customizing deployment to add an ML node&#13;&#10;"/></div><p class="figure-caption">Figure 2.8 – Customizing deployment to add an ML node</p></li>
				<li>Near the bottom of <em class="italic">Figure 2.8</em> is a link to <strong class="bold">Add Machine Learning nodes</strong> to your cluster. Clicking on this will reveal the ML node configuration:<div id="_idContainer024" class="IMG---Figure"><img src="image/B17040_02_009.jpg" alt="Figure 2.9 – Adding ML node(s)&#13;&#10;"/></div><p class="figure-caption">Figure 2.9 – Adding ML node(s)</p><p class="callout-heading">Note</p><p class="callout">During the <a id="_idIndexMarker075"/>free 14-day trial period of ESS, you can only <a id="_idIndexMarker076"/>add one 1 GB ML <a id="_idIndexMarker077"/>node (in one or two availability zones). If you move from a free trial to a paid subscription, you can obviously create more or larger ML nodes.</p></li>
				<li>Once the ML node is added to the configuration, click on the <strong class="bold">Create Deployment</strong> button to initiate the process for ESS to create your cluster for you, which will take a few minutes. In the meantime, you will be shown the default credentials that you will use to access the cluster:<div id="_idContainer025" class="IMG---Figure"><img src="image/B17040_02_010.jpg" alt="Figure 2.10 – Default assigned credentials&#13;&#10;"/></div><p class="figure-caption">Figure 2.10 – Default assigned credentials</p><p>You can <a id="_idIndexMarker078"/>download these credentials <a id="_idIndexMarker079"/>for use later. Don't worry if you <a id="_idIndexMarker080"/>forgot to download them – you can always reset the password later if needed.</p></li>
				<li>Once the cluster is up and running as shown in <em class="italic">Figure 2.11</em> (usually only after a few minutes), you will see the following view of your deployment, with an <strong class="bold">Open Kibana</strong> button that will allow you to launch into your deployment:</li>
			</ol>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B17040_02_011.jpg" alt="Figure 2.11 – Deployment successfully created&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.11 – Deployment successfully created</p>
			<p>Once the <strong class="bold">Open Kibana</strong> button <a id="_idIndexMarker081"/>is clicked, you <a id="_idIndexMarker082"/>will be automatically authenticated <a id="_idIndexMarker083"/>into Kibana, where you will be ready to use ML straight away – no additional configuration steps are necessary. </p>
			<p>At this point, from the perspective of the user who wants to use Elastic ML, there is little difference between the self-managed configuration shown earlier and the setup created in ESS. The one major difference, however, is that the configuration here in ESS has Elastic ML always isolated to a <strong class="bold">dedicated ML node</strong>. In a self-managed configuration, ML nodes can be dedicated or in a shared role (such as <strong class="source-inline">data</strong>, <strong class="source-inline">ingest</strong>, and <strong class="source-inline">ml</strong> roles all on the same node). We will discuss this concept later in this chapter.</p>
			<p>Now that <a id="_idIndexMarker084"/>we have a functioning <a id="_idIndexMarker085"/>Elastic Stack with ML <a id="_idIndexMarker086"/>enabled, we are getting closer to being able to start analyzing data, which will begin in <a href="B17040_03_Epub_AM.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a>, <em class="italic">Anomaly Detection</em>. But first, let's understand the operationalization of Elastic ML.</p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor038"/>Understanding operationalization</h1>
			<p>At some point on your journey with using Elastic ML, it will be helpful to understand a number of key concepts <a id="_idIndexMarker087"/>regarding how Elastic ML is operationalized within the Elastic Stack. This includes information about how the analytics run on the cluster nodes and how data that is to be analyzed by ML is retrieved and processed. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Some concepts in this section may not be intuitive until you actually start using Elastic ML on some real examples. Don't worry if you feel like you prefer to skim (or even skip) this section now and return to it later following some genuine experience of using Elastic ML.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor039"/>ML nodes</h2>
			<p>First and <a id="_idIndexMarker088"/>foremost, since Elasticsearch is, by nature, a distributed multi-node solution, it is only natural that the ML feature of the Elastic Stack works as a native plugin that obeys many of the same operational concepts. As described in the documentation (<a href="http://elastic.co/guide/en/elasticsearch/reference/current/ml-settings.html">elastic.co/guide/en/elasticsearch/reference/current/ml-settings.html</a>), ML can be enabled on any or all nodes, but it is a best practice in a production system to have dedicated ML nodes. We saw this best practice forced on the user in Elastic Cloud ESS – the user must create dedicated ML nodes if ML is desired to be used.</p>
			<p>Having dedicated ML nodes is also helpful in optimizing the types of resources specifically required by ML. Unlike data nodes that are involved in a fair amount of disk I/O loads due to indexing and searching, ML nodes are more compute- and memory-intensive. With this knowledge, you can size the hardware appropriately for dedicated ML nodes. </p>
			<p>One key thing to note—the ML algorithms do not run in the <strong class="bold">Java Virtual Machine </strong>(<strong class="bold">JVM</strong>). They are C++-based executables that will use the RAM that is left over from whatever is allocated for the JVM heap. When running ML jobs, a process that invokes the analysis (called <strong class="source-inline">autodetect</strong> for anomaly <a id="_idIndexMarker089"/>detection and <strong class="source-inline">data_frame_analyzer</strong> for data frame analytics) can be seen in the process list (if you were to run the <strong class="source-inline">ps</strong> command on Linux, for example). There will be one process for every actively running ML job. In multi-node setups, ML will distribute the jobs to each of the ML-enabled nodes to balance the load of the work.</p>
			<p>Elastic ML <a id="_idIndexMarker090"/>obeys a setting called <strong class="source-inline">xpack.ml.max_machine_memory_percent</strong>, which governs <a id="_idIndexMarker091"/>how much system memory can be used by ML jobs. The default value of this setting is 30%. The limit is based on the total memory of the machine, not memory that is currently free. Don't forget that the Elasticsearch JVM may take up to around 50% of the available machine memory, so leaving 30% to ML and the remaining 20% for the operating system and other ancillary processes is prudent, albeit conservative. Jobs are not allocated to a node if doing so would cause the estimated memory use of ML jobs to exceed the limit defined by this setting. </p>
			<p>While there is no empirical formula to determine the size and number of dedicated ML nodes, some good rules of thumb are as follows:</p>
			<ul>
				<li>Have one dedicated ML node (two for high availability/fault tolerance if a single node becomes unavailable) for cluster sizes of up to 10 data nodes.</li>
				<li>Have at least two ML nodes for clusters of up to 20 nodes.</li>
				<li>Add an additional ML node for every additional 10 data nodes.</li>
			</ul>
			<p>This general approach of reserving about 10-20% of your cluster capacity to dedicated ML nodes is certainly a reasonable suggestion, but it does not obviate the need to do your own sizing, characterization testing, and resource monitoring. As we will see in several later chapters, the resource demands on your ML tasks will greatly depend on what kind(s) of analyses are being invoked, as well as the density and volume of the data being analyzed.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor040"/>Jobs</h2>
			<p>In Elastic ML, the <a id="_idIndexMarker092"/>job is the unit of work. There are both <strong class="bold">anomaly detection</strong> jobs and <strong class="bold">data frame analytics</strong> jobs. Both <a id="_idIndexMarker093"/>take some kind of data as input and produce new information as <a id="_idIndexMarker094"/>output. Jobs can be created using the ML UI in Kibana, or programmatically via the API. They also require ML-enabled nodes.</p>
			<p>In general, anomaly detection jobs can be run as a single-shot batch analysis (over a swath of historical data) or continuously run in real time on time series data – data that is constantly being indexed by your Elastic Stack (or both, really). </p>
			<p>Alternatively, data frame analytics jobs are not continuous – they are single-shot executions that produce output results and/or an output model that is used for subsequent <strong class="bold">inferencing</strong>, discussed in more depth in chapters 9 to 13. </p>
			<p>Therefore, from an operationalization standpoint, anomaly detection jobs are a bit more complex – as multiple can be running simultaneously, doing independent things and analyzing data from different indices. In other words, anomaly detection jobs are likely to be continuously busy within a typical cluster.</p>
			<p>As we will see in more depth later, the main configuration elements of an anomaly detection job are as follows:</p>
			<ul>
				<li>Job name/ID</li>
				<li>Analysis bucketization window (the <strong class="bold">bucket span</strong>)</li>
				<li>The definition and settings for the query to obtain the raw data to be analyzed (the <strong class="bold">datafeed</strong>)</li>
				<li>The anomaly detection configuration recipe (the <strong class="bold">detector</strong>)</li>
			</ul>
			<p>With the notion of jobs understood, we'll next focus on how the bucketing of time series data is an important concept in the analysis of real-time data.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor041"/>Bucketing data in a time series analysis</h2>
			<p>Bucketing <a id="_idIndexMarker095"/>input data is an important concept <a id="_idIndexMarker096"/>to understand in Elastic ML's anomaly detection. Set with a key parameter at the job level called <strong class="source-inline">bucket_span</strong>, the input <a id="_idIndexMarker097"/>data from the datafeed (described next) is collected into mini batches for processing. Think of the bucket span as a pre-analysis aggregation interval—the window of time in which a portion of the data is aggregated over for the purposes of analysis. The shorter the duration of <strong class="source-inline">bucket_span</strong>, the more granular the analysis, but also the higher the potential for noisy artifacts in the data.</p>
			<p>To illustrate, the following graph shows the same dataset aggregated over three different intervals:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B17040_02_012.jpg" alt="Figure 2.12 – Aggregations of the same data over different time intervals&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.12 – Aggregations of the same data over different time intervals</p>
			<p>Notice that the prominent anomalous spike seen in the version aggregated over the 5- minute interval becomes all but lost if the data is aggregated over a 60-minute interval due to the fact of the spike's short (&lt;2 minute) duration. In fact, at this 60-minute interval, the spike doesn't even seem that anomalous any more.</p>
			<p>This is a practical consideration behind the choice of <strong class="source-inline">bucket_span</strong>. On the one hand, having a shorter aggregation period is helpful because it will increase the frequency of the analysis (and thus reduce the interval of notification if there is something anomalous), but <a id="_idIndexMarker098"/>making it too short may highlight <a id="_idIndexMarker099"/>features in the data that you don't really care about. If the brief spike that's shown in the preceding data is a meaningful anomaly for you, then the 5-minute view of the data is sufficient. If, however, a perturbation of the data that is very brief seems like an unnecessary distraction, then avoid a low value of <strong class="source-inline">bucket_span</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Some additional practical considerations can be found on Elastic's blog: <a href="http://elastic.co/blog/explaining-the-bucket-span-in-machine-learning-for-elasticsearch">elastic.co/blog/explaining-the-bucket-span-in-machine-learning-for-elasticsearch</a>.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor042"/>Feeding data to Elastic ML</h2>
			<p>Anomaly <a id="_idIndexMarker100"/>detection jobs obviously need <a id="_idIndexMarker101"/>data to analyze (and use to build and mature the statistical models). This data comes from your time series indices in Elasticsearch. The datafeed is the <a id="_idIndexMarker102"/>mechanism by which this data is retrieved (searched) on a routine basis and presented to the ML algorithms. Its configuration is mostly obscured from the user, except in the case of the creation of an advanced job in the UI (or by using the anomaly detection API). However, it is important to understand what the datafeed is doing behind the scenes.</p>
			<p>Similar to the concept of a <strong class="bold">Watch</strong> input in <strong class="bold">Watcher</strong>, the datafeed will routinely query for data against an index pattern (or <strong class="bold">saved search</strong>) that contains the data to be analyzed. How often (and how much data at a time) the datafeed queries the data depends on a number of factors:</p>
			<ul>
				<li><strong class="source-inline">query</strong>: The actual query (expressed in Elasticsearch DSL) that will be used to retrieve data from the source index for analysis. The user can choose to query all documents in the source index or to selectively filter and/or aggregate the data.</li>
				<li><strong class="source-inline">bucket_span</strong>: We have already established that <strong class="source-inline">bucket_span</strong> controls the width of the ongoing analysis window. Therefore, the job of the datafeed is to make sure that the buckets are full of chronologically ordered data. You can therefore see that the datafeed will make a date range query to Elasticsearch.</li>
				<li><strong class="source-inline">frequency</strong>: A parameter that controls how often the raw data is physically queried. If this is between 2 and 20 minutes, <strong class="source-inline">frequency</strong> will equal <strong class="source-inline">bucket_span</strong> (as in, query every 5 minutes for the last 5 minutes' worth of data). If <strong class="source-inline">bucket_span</strong> is longer, <strong class="source-inline">frequency</strong>, by default, will be a smaller number (more frequent) so that the overall long interval is not expected to be queried all at once. This is helpful if the dataset is rather voluminous. In other words, the interval of a long <strong class="source-inline">bucket_span</strong> will be chopped up into smaller intervals simply for the purposes of querying.</li>
				<li><strong class="source-inline">query_delay</strong>: This controls the amount of time "behind now" that the datafeed should query for a <a id="_idIndexMarker103"/>bucket span's worth of data. The <a id="_idIndexMarker104"/>default is 60 seconds when the job is configured via the API, or a randomized value between 60 seconds and 120 seconds if the job is configured via the UI. Therefore, with a <strong class="source-inline">bucket_span</strong> value of 5 minutes and a <strong class="source-inline">query_delay</strong> value of 60 seconds at 12:01 P.M., the datafeed will request data in the range of 11:55 A.M. to midnight. This extra little delay allows for delays in the ingest pipeline to ensure that no data is excluded from the analysis if its ingestion is delayed for any reason. If the system detects that the anomaly detection job is missing data due to possible ingest delays, a system-generated <strong class="bold">annotation</strong> will be created to warn the user that this is occurring and that <strong class="source-inline">query_delay</strong> might need to be increased to remedy it.</li>
				<li><strong class="source-inline">scroll_size</strong>: In most cases, the type of search that the datafeed executes to Elasticsearch uses the scroll API (<a href="http://elastic.co/guide/en/elasticsearch/reference/current/scroll-api.html">elastic.co/guide/en/elasticsearch/reference/current/scroll-api.html</a>). The scroll size defines how much the datafeed queries to Elasticsearch at a time. For example, if the datafeed is set to query for log data every 5 minutes, but in a typical 5-minute window there are 1 million events, the idea of scrolling that data means that not all 1 million events will be expected to be fetched with one giant query. Rather, it will do it with many queries in increments of <strong class="source-inline">scroll_size</strong>. By default, this scroll size is set conservatively to 1,000. So, to get 1 million records returned to ML, the datafeed will ask Elasticsearch for 1,000 rows, 1,000 times. Increasing <strong class="source-inline">scroll_size</strong> to 10,000 will reduce the number of scrolls to 100. In general, beefier clusters should be able to handle a larger <strong class="source-inline">scroll_size</strong> and thus be more efficient in the overall process.</li>
			</ul>
			<p>There is an exception, however, in the case of a single metric job. The single metric job (described more in <a href="B17040_03_Epub_AM.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a>, <em class="italic">Anomaly Detection</em>) is a simple ML job that allows only one-time series metrics to be analyzed. In this case, the scroll API is not used to obtain the raw data—rather, the datafeed will automatically create a query aggregation (using the <strong class="source-inline">date_histogram</strong> aggregation). This <a id="_idIndexMarker105"/>aggregation technique can also <a id="_idIndexMarker106"/>be used for any anomaly detection job, but it currently requires direct editing of the job's JSON configuration and should be reserved for expert users.</p>
			<p>In terms of feeding data to Elastic ML for data frame analytics jobs, the paradigm is different from anomaly detection because data isn't being fed to the analytics continuously, in real time. The specifics on how to feed data to a data frame analytics job will be covered in chapters 9-13.</p>
			<p>Now that we have a deeper understanding of how data flows into Elastic ML for analysis, let's now look at some of the indices that are used to support Elastic ML's operation.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor043"/>The supporting indices</h2>
			<p>For Elastic ML to <a id="_idIndexMarker107"/>function, there are several supporting indices that exist and serve specific purposes: </p>
			<ul>
				<li><strong class="source-inline">.ml-config </strong></li>
				<li><strong class="source-inline">.ml-state-*</strong></li>
				<li><strong class="source-inline">.ml-notifications-*</strong></li>
				<li><strong class="source-inline">.ml-annotations-*</strong></li>
				<li><strong class="source-inline">.ml-stats-*</strong></li>
				<li><strong class="source-inline">.ml-anomalies-*</strong></li>
			</ul>
			<p>All of these indices are <strong class="bold">system indices</strong> (and most are <strong class="bold">hidden indices</strong>), meaning that they are not <a id="_idIndexMarker108"/>intended to be written to or manipulated by the end user. However, it is often helpful to understand their function/role, so let's take each one in turn.</p>
			<h3>.ml-config</h3>
			<p>The <strong class="source-inline">.ml-config</strong> index <a id="_idIndexMarker109"/>contains configuration information <a id="_idIndexMarker110"/>about all of the ML jobs that are currently defined in the system. The information contained in this index is readable and interpretable by the average user.</p>
			<h3>.ml-state-*</h3>
			<p>The <strong class="source-inline">.ml-state</strong> <a id="_idIndexMarker111"/>index is the place where Elastic ML <a id="_idIndexMarker112"/>keeps the internal information about the progress of data frame analytics jobs and anomaly detection statistical models that have been learned for a specific dataset, plus additional logistical information. This index is <em class="italic">not</em> meant to be understandable by a user—it is the backend algorithms of ML that will read and write entries in this index.</p>
			<h3>.ml-notifications-*</h3>
			<p>This index is the <a id="_idIndexMarker113"/>place where Elastic ML stores the <a id="_idIndexMarker114"/>audit messages that appear in the <strong class="bold">Job</strong> <strong class="bold">Messages</strong> section of the <strong class="bold">Job</strong> <strong class="bold">Management</strong> page of the anomaly detection UI. These messages convey basic information about the job's creation and activity. Additionally, basic operational errors can be found here. These messages can also be viewed under the<strong class="source-inline"> </strong><strong class="bold">Job Messages</strong> section on the <strong class="bold">Job Management</strong> page in the anomaly detection UI. Detailed information about the execution of ML jobs, however, can be found in the <strong class="source-inline">elasticsearch.log</strong> file.</p>
			<h3>.ml-annotations-*</h3>
			<p>This index <a id="_idIndexMarker115"/>stores records of annotations <a id="_idIndexMarker116"/>associated with anomaly detection jobs. This includes user-created annotations that can be defined with the anomaly detection UI, but also system-created annotations, such as ingest delay warnings and model snapshot notifications.</p>
			<h3>.ml-stats-*</h3>
			<p>This <a id="_idIndexMarker117"/>index retains information about the progress <a id="_idIndexMarker118"/>and performance of data frame analytics jobs.</p>
			<h3>.ml-anomalies-*</h3>
			<p>The .<strong class="source-inline">ml-anomalies-*</strong> indices contain the <a id="_idIndexMarker119"/>detailed results of ML jobs. These indices are instrumental in leveraging the output of the ML <a id="_idIndexMarker120"/>algorithms. All information displayed in the ML UI will be driven from this result data. Additionally, proactive alerting on anomalies will be accomplished by having queries configured against these indices. More information on this will be presented in <a href="B17040_06_Epub_AM.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>, <em class="italic">Alerting on ML Analysis</em>.</p>
			<p>Now that we know the names and roles of the system indices owned and managed by Elastic ML, let's next look specifically at <strong class="source-inline">.ml-state</strong> and <strong class="source-inline">.ml-anomalies</strong> and how they contribute to the runtime orchestration of the anomaly detection jobs.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor044"/>Anomaly detection orchestration</h2>
			<p>Because <a id="_idIndexMarker121"/>anomaly detection jobs can be <a id="_idIndexMarker122"/>run continuously on live, time series data, a rather complex orchestration occurs. A simplified diagram of this process is shown in <em class="italic">Figure 2.13</em>:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B17040_02_013.jpg" alt="Figure 2.13 – Simplified sequence of an anomaly detection job's operation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.13 – Simplified sequence of an anomaly detection job's operation</p>
			<p>The <strong class="source-inline">autodetect</strong> process, which is the physical manifestation of the anomaly detection job, is what is represented by the analyze versus model step in <em class="italic">Figure 2.13</em>. The <strong class="source-inline">.ml-state</strong> index is read and written to by the <strong class="source-inline">autodetect</strong> process occasionally (as described in the next section). The output of the <strong class="source-inline">autodetect</strong> process (the results of the analysis) is stored in the <strong class="source-inline">.ml-anomalies-*</strong> indices.</p>
			<p>In general, the preceding procedures are done <a id="_idIndexMarker123"/>once per <strong class="source-inline">bucket_span</strong> (except for the actual read/write <a id="_idIndexMarker124"/>from <strong class="source-inline">.ml-state</strong>). The key takeaway is that this orchestration enables the anomaly detection job to be online (that is, not offline/batch) and constantly learning on newly ingested data. This process is also handled automatically by Elastic ML, so that the user doesn't have to worry about the complex logistics required to make it all happen.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor045"/>Anomaly detection model snapshots</h2>
			<p>As mentioned in the <a id="_idIndexMarker125"/>previous section, the "state" of the <a id="_idIndexMarker126"/>anomaly detection model is stored in the <strong class="source-inline">.ml-state</strong> index. However, it is not actually read or written with every bucket span. Instead, the model state is mostly kept in the memory of the <strong class="source-inline">autodetect</strong> process and is only periodically serialized to <strong class="source-inline">.ml-state</strong>. If the anomaly detection job is asked to run over a large swath of historical data, or is running in real time, then the model is serialized in the following ways:</p>
			<ul>
				<li>Periodically, on a schedule of about every 3 to 4 hours (or at an interval defined by <strong class="source-inline">background_persist_interval</strong>, if explicitly set)</li>
				<li>When the anomaly detection job is put in the <strong class="bold">closed</strong> state</li>
			</ul>
			<p>Because of this periodic serialization of the model, older snapshots are automatically deleted with a nightly system maintenance job. <a id="_idTextAnchor046"/>By default, if there are snapshots over 1 day older than the newest snapshot in the <strong class="source-inline">.ml-state</strong> index, they are deleted except for the first snapshot each day. Additionally, all snapshots over 10 <a id="_idIndexMarker127"/>days older than the newest <a id="_idIndexMarker128"/>snapshot are deleted. If you want to exempt a specific snapshot from this cleanup and keep it around indefinitely, use the UI in Kibana or the updated model snapshots API to set the value of the <strong class="source-inline">retain</strong> setting to <strong class="source-inline">true</strong>.</p>
			<p>It may also be apparent that having saved snapshots now allows the user to revert the job to use one of these previously taken snapshots of the model in the event of something going wrong operationally, or an unexpected situation arising. In one of the <em class="italic">Tips and tricks</em> sections of the <a href="B17040_14_Epub_AM.xhtml#_idTextAnchor248"><em class="italic">Appendix</em></a>, we will work through an example that demonstrates how to ignore time periods and revert a job to use a model snapshot.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor047"/>Summary</h1>
			<p>To summarize, in this chapter, we covered the procedures around the enabling of Elastic ML's features in both a self-managed on-premises Elastic Stack and within the Elasticsearch Service of Elastic Cloud. Additionally, we looked under the hood to see the deep integration points with the rest of the Elastic Stack and how Elastic ML works from an operational perspective.</p>
			<p>As we look ahead to future chapters, the focus will now shift away from the conceptual and background information into the realm of practical usage. Starting with the next chapter, we will jump right into the comprehensive capabilities of Elastic ML's anomaly detection and we will learn how to configure jobs to solve some practical use cases in log analytics, metric analysis, and user behavior analytics.</p>
		</div>
	</body></html>