["```py\nimport numpy as np\nfrom functools import reduce \n```", "```py\ndef add3(input_array): \n    return map(lambda x: x+3, input_array) \n```", "```py\ndef mul2(input_array): return map(lambda x: x*2, input_array) \n```", "```py\ndef sub5(input_array): \n    return map(lambda x: x-5, input_array)\n```", "```py\ndef function_composer(*args): \n    return reduce(lambda f, g: lambda x: f(g(x)), args) \n```", "```py\nif __name__=='__main__': \n    arr = np.array([2,5,4,7]) \n\n    print(\"Operation: add3(mul2(sub5(arr)))\") \n```", "```py\n    arr1 = add3(arr) \n    arr2 = mul2(arr1) \n    arr3 = sub5(arr2) \n    print(\"Output using the lengthy way:\", list(arr3)) \n```", "```py\n    func_composed = function_composer(sub5, mul2, add3) \n    print(\"Output using function composition:\", list(func_composed(arr)))  \n```", "```py\n    print(\"Operation: sub5(add3(mul2(sub5(mul2(arr)))))\\nOutput:\", \\\n            list(function_composer(mul2, sub5, mul2, add3, sub5)(arr)))\n```", "```py\nOperation: add3(mul2(sub5(arr)))\nOutput using the lengthy way: [5, 11, 9, 15]\nOutput using function composition: [5, 11, 9, 15]\nOperation: sub5(add3(mul2(sub5(mul2(arr)))))\nOutput: [-10, 2, -2, 10]\n```", "```py\nfrom sklearn.datasets import samples_generator \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.feature_selection import SelectKBest, f_regression \nfrom sklearn.pipeline import Pipeline \n```", "```py\n# generate sample data \nX, y = samples_generator.make_classification( \n        n_informative=4, n_features=20, n_redundant=0, random_state=5) \n```", "```py\n# Feature selector  \nselector_k_best = SelectKBest(f_regression, k=10)\n```", "```py\n# Random forest classifier \nclassifier = RandomForestClassifier(n_estimators=50, max_depth=4)\n```", "```py\n# Build the machine learning pipeline \npipeline_classifier = Pipeline([('selector', selector_k_best), ('rf', classifier)]) \n```", "```py\npipeline_classifier.set_params(selector__k=6,  \n        rf__n_estimators=25) \n```", "```py\n# Training the classifier \npipeline_classifier.fit(X, y) \n```", "```py\n# Predict the output prediction = pipeline_classifier.predict(X) print(\"Predictions:\\n\", prediction) \n```", "```py\n# Print score \nprint(\"Score:\", pipeline_classifier.score(X, y))                         \n```", "```py\n# Print the selected features chosen by the selector features_status = pipeline_classifier.named_steps['selector'].get_support() selected_features = [] for count, item in enumerate(features_status): if item: selected_features.append(count) print(\"Selected features (0-indexed):\", ', '.join([str(x) for x in selected_features]))\n```", "```py\nPredictions:\n [1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1\n 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1\n 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1]\nScore: 0.95\nSelected features (0-indexed): 0, 5, 9, 10, 11, 15\n```", "```py\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.neighbors import NearestNeighbors \n```", "```py\n# Input data \nX = np.array([[1, 1], [1, 3], [2, 2], [2.5, 5], [3, 1],  \n        [4, 2], [2, 3.5], [3, 3], [3.5, 4]])\n```", "```py\n# Number of neighbors we want to find \nnum_neighbors = 3 \n```", "```py\n# Input point \ninput_point = [2.6, 1.7] \n```", "```py\n# Plot datapoints \nplt.figure() \nplt.scatter(X[:,0], X[:,1], marker='o', s=25, color='k') \n```", "```py\n# Build nearest neighbors model \nknn = NearestNeighbors(n_neighbors=num_neighbors, algorithm='ball_tree').fit(X) \n```", "```py\ndistances, indices = knn.kneighbors(input_point) \n```", "```py\n# Print the 'k' nearest neighbors \nprint(\"k nearest neighbors\")\nfor rank, index in enumerate(indices[0][:num_neighbors]):\n    print(str(rank+1) + \" -->\", X[index])\n```", "```py\n# Plot the nearest neighbors  \nplt.figure() \nplt.scatter(X[:,0], X[:,1], marker='o', s=25, color='k') \nplt.scatter(X[indices][0][:][:,0], X[indices][0][:][:,1],  \n        marker='o', s=150, color='k', facecolors='none') \nplt.scatter(input_point[0], input_point[1], \n        marker='x', s=150, color='k', facecolors='none') \n\nplt.show() \n```", "```py\nk nearest neighbors\n1 --> [2\\. 2.]\n2 --> [3\\. 1.]\n3 --> [3\\. 3.]\n```", "```py\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport matplotlib.cm as cm \nfrom sklearn import neighbors, datasets \n\nfrom utilities import load_data \n```", "```py\n# Load input data \ninput_file = 'data_nn_classifier.txt' \ndata = load_data(input_file) \nX, y = data[:,:-1], data[:,-1].astype(np.int) \n```", "```py\n# Plot input data \nplt.figure() \nplt.title('Input datapoints') \nmarkers = '^sov<>hp' \nmapper = np.array([markers[i] for i in y]) \nfor i in range(X.shape[0]): \n    plt.scatter(X[i, 0], X[i, 1], marker=mapper[i],  \n            s=50, edgecolors='black', facecolors='none') \n```", "```py\n# Number of nearest neighbors to consider \nnum_neighbors = 10\n```", "```py\n# step size of the grid \nh = 0.01   \n```", "```py\n# Create a K-Neighbours Classifier model and train it \nclassifier = neighbors.KNeighborsClassifier(num_neighbors, weights='distance') \nclassifier.fit(X, y)\n```", "```py\n# Create the mesh to plot the boundaries \nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 \ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 \nx_grid, y_grid = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) \n```", "```py\n# Compute the outputs for all the points on the mesh \npredicted_values = classifier.predict(np.c_[x_grid.ravel(), y_grid.ravel()]) \n```", "```py\n# Put the computed results on the map \npredicted_values = predicted_values.reshape(x_grid.shape) \nplt.figure() \nplt.pcolormesh(x_grid, y_grid, predicted_values, cmap=cm.Pastel1) \n```", "```py\n# Overlay the training points on the map \nfor i in range(X.shape[0]): \n    plt.scatter(X[i, 0], X[i, 1], marker=mapper[i],  \n            s=50, edgecolors='black', facecolors='none') \n\nplt.xlim(x_grid.min(), x_grid.max()) \nplt.ylim(y_grid.min(), y_grid.max()) \nplt.title('k nearest neighbors classifier boundaries')\n```", "```py\n# Test input datapoint \ntest_datapoint = [4.5, 3.6] \nplt.figure() \nplt.title('Test datapoint') \nfor i in range(X.shape[0]): \n    plt.scatter(X[i, 0], X[i, 1], marker=mapper[i],  \n            s=50, edgecolors='black', facecolors='none') \n\nplt.scatter(test_datapoint[0], test_datapoint[1], marker='x',  \n        linewidth=3, s=200, facecolors='black') \n```", "```py\n# Extract k nearest neighbors \ndist, indices = classifier.kneighbors(test_datapoint) \n```", "```py\n# Plot k nearest neighbors \nplt.figure() \nplt.title('k nearest neighbors') \n\nfor i in indices: \n    plt.scatter(X[i, 0], X[i, 1], marker='o',  \n            linewidth=3, s=100, facecolors='black') \n\nplt.scatter(test_datapoint[0], test_datapoint[1], marker='x',  \n        linewidth=3, s=200, facecolors='black') \n\nfor i in range(X.shape[0]): \n    plt.scatter(X[i, 0], X[i, 1], marker=mapper[i],  \n            s=50, edgecolors='black', facecolors='none') \n\nplt.show() \n```", "```py\nprint(\"Predicted output:\", classifier.predict(test_datapoint)[0])\n```", "```py\nPredicted output: 2\n```", "```py\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn import neighbors \n```", "```py\n# Generate sample data \namplitude = 10 \nnum_points = 100 \nX = amplitude * np.random.rand(num_points, 1) - 0.5 * amplitude \n```", "```py\n# Compute target and add noise \ny = np.sinc(X).ravel()  \ny += 0.2 * (0.5 - np.random.rand(y.size))\n```", "```py\n# Plot input data \nplt.figure() \nplt.scatter(X, y, s=40, c='k', facecolors='none') \nplt.title('Input data') \n```", "```py\n# Create the 1D grid with 10 times the density of the input data \nx_values = np.linspace(-0.5*amplitude, 0.5*amplitude, 10*num_points)[:, np.newaxis] \n```", "```py\n# Number of neighbors to consider  \nn_neighbors = 8 \n```", "```py\n# Define and train the regressor \nknn_regressor = neighbors.KNeighborsRegressor(n_neighbors, weights='distance') \ny_values = knn_regressor.fit(X, y).predict(x_values) \n```", "```py\nplt.figure() \nplt.scatter(X, y, s=40, c='k', facecolors='none', label='input data') \nplt.plot(x_values, y_values, c='k', linestyle='--', label='predicted values') \nplt.xlim(X.min() - 1, X.max() + 1) \nplt.ylim(y.min() - 0.2, y.max() + 0.2) \nplt.axis('tight') \nplt.legend() \nplt.title('K Nearest Neighbors Regressor') \n\nplt.show()\n```", "```py\nimport json \nimport numpy as np \n```", "```py\n# Returns the Euclidean distance score between user1 and user2  \ndef euclidean_score(dataset, user1, user2): \n    if user1 not in dataset: \n        raise TypeError('User ' + user1 + ' not present in the dataset') \n\n    if user2 not in dataset: \n        raise TypeError('User ' + user2 + ' not present in the dataset') \n```", "```py\n    # Movies rated by both user1 and user2 \n    rated_by_both = {}  \n\n    for item in dataset[user1]: \n        if item in dataset[user2]: \n            rated_by_both[item] = 1 \n```", "```py\n    # If there are no common movies, the score is 0  \n    if len(rated_by_both) == 0: \n        return 0 \n```", "```py\n    squared_differences = []  \n\n    for item in dataset[user1]: \n        if item in dataset[user2]: \n            squared_differences.append(np.square(dataset[user1][item] - dataset[user2][item])) \n\n    return 1 / (1 + np.sqrt(np.sum(squared_differences)))  \n```", "```py\nif __name__=='__main__': \n    data_file = 'movie_ratings.json' \n\n    with open(data_file, 'r') as f: \n        data = json.loads(f.read()) \n```", "```py\n    user1 = 'John Carson' \n    user2 = 'Michelle Peterson' \n\n    print(\"Euclidean score:\")\n    print(euclidean_score(data, user1, user2)) \n```", "```py\n0.29429805508554946\n```", "```py\nimport json \nimport numpy as np \n```", "```py\n# Returns the Pearson correlation score between user1 and user2  \ndef pearson_score(dataset, user1, user2): \n    if user1 not in dataset: \n        raise TypeError('User ' + user1 + ' not present in the dataset') \n\n    if user2 not in dataset: \n        raise TypeError('User ' + user2 + ' not present in the dataset') \n```", "```py\n    # Movies rated by both user1 and user2 \n    rated_by_both = {} \n\n    for item in dataset[user1]: \n        if item in dataset[user2]: \n            rated_by_both[item] = 1 \n\n    num_ratings = len(rated_by_both)  \n```", "```py\n    # If there are no common movies, the score is 0  \n    if num_ratings == 0: \n        return 0\n```", "```py\n    # Compute the sum of ratings of all the common preferences  \n    user1_sum = np.sum([dataset[user1][item] for item in rated_by_both]) \n    user2_sum = np.sum([dataset[user2][item] for item in rated_by_both]) \n```", "```py\n    # Compute the sum of squared ratings of all the common preferences  \n    user1_squared_sum = np.sum([np.square(dataset[user1][item]) for item in rated_by_both]) \n    user2_squared_sum = np.sum([np.square(dataset[user2][item]) for item in rated_by_both]) \n```", "```py\n    # Compute the sum of products of the common ratings  \n    product_sum = np.sum([dataset[user1][item] * dataset[user2][item] for item in rated_by_both]) \n```", "```py\n    # Compute the Pearson correlation \n    Sxy = product_sum - (user1_sum * user2_sum / num_ratings) \n    Sxx = user1_squared_sum - np.square(user1_sum) / num_ratings \n    Syy = user2_squared_sum - np.square(user2_sum) / num_ratings \n```", "```py\n    if Sxx * Syy == 0: \n        return 0 \n```", "```py\n    return Sxy / np.sqrt(Sxx * Syy) \n```", "```py\nif __name__=='__main__': \n    data_file = 'movie_ratings.json' \n\n    with open(data_file, 'r') as f: \n        data = json.loads(f.read()) \n\n    user1 = 'John Carson' \n    user2 = 'Michelle Peterson' \n\n    print(\"Pearson score:\")\n    print(pearson_score(data, user1, user2)) \n```", "```py\nPearson score:\n0.39605901719066977\n```", "```py\nimport json \nimport numpy as np \n\nfrom pearson_score import pearson_score \n```", "```py\n# Finds a specified number of users who are similar to the input user \ndef find_similar_users(dataset, user, num_users): \n    if user not in dataset: \n        raise TypeError('User ' + user + ' not present in the dataset') \n\n    # Compute Pearson scores for all the users \n    scores = np.array([[x, pearson_score(dataset, user, x)] for x in dataset if user != x]) \n```", "```py\n    # Sort the scores based on second column \n    scores_sorted = np.argsort(scores[:, 1]) \n\n    # Sort the scores in decreasing order (highest score first)  \n    scored_sorted_dec = scores_sorted[::-1] \n```", "```py\n    # Extract top 'k' indices \n    top_k = scored_sorted_dec[0:num_users]  \n\n    return scores[top_k]  \n```", "```py\nif __name__=='__main__': \n    data_file = 'movie_ratings.json' \n\n    with open(data_file, 'r') as f: \n        data = json.loads(f.read()) \n```", "```py\n    user = 'John Carson'\n    print(\"Users similar to \" + user + \":\\n\")\n    similar_users = find_similar_users(data, user, 3) \n    print(\"User\\t\\t\\tSimilarity score\\n\")\n    for item in similar_users:\n        print(item[0], '\\t\\t', round(float(item[1]), 2))\n```", "```py\nUsers similar to John Carson:\n\nUser               Similarity score\nMichael Henry                  0.99\nAlex Roberts                   0.75\nMelissa Jones                  0.59\n```", "```py\nimport json \nimport numpy as np \n\nfrom pearson_score import pearson_score \n```", "```py\n# Generate recommendations for a given user \ndef generate_recommendations(dataset, user): \n    if user not in dataset: \n        raise TypeError('User ' + user + ' not present in the dataset') \n```", "```py\n    total_scores = {} \n    similarity_sums = {} \n\n    for u in [x for x in dataset if x != user]: \n        similarity_score = pearson_score(dataset, user, u) \n\n        if similarity_score <= 0: \n            continue \n```", "```py\n        for item in [x for x in dataset[u] if x not in dataset[user] or dataset[user][x] == 0]: \n            total_scores.update({item: dataset[u][item] * similarity_score}) \n            similarity_sums.update({item: similarity_score}) \n```", "```py\n    if len(total_scores) == 0: \n        return ['No recommendations possible'] \n```", "```py\n    # Create the normalized list \n    movie_ranks = np.array([[total/similarity_sums[item], item]  \n            for item, total in total_scores.items()]) \n```", "```py\n    # Sort in decreasing order based on the first column \n    movie_ranks = movie_ranks[np.argsort(movie_ranks[:, 0])[::-1]] \n```", "```py\n    # Extract the recommended movies \n    recommendations = [movie for _, movie in movie_ranks] \n\n    return recommendations\n```", "```py\nif __name__=='__main__': \n    data_file = 'movie_ratings.json' \n\n    with open(data_file, 'r') as f: \n        data = json.loads(f.read()) \n```", "```py\n    user = 'Michael Henry'\n    print(\"Recommendations for \" + user + \":\")\n    movies = generate_recommendations(data, user) \n    for i, movie in enumerate(movies):\n        print(str(i+1) + '. ' + movie)\n```", "```py\n    user = 'John Carson' \n    print(\"Recommendations for \" + user + \":\")\n    movies = generate_recommendations(data, user) \n    for i, movie in enumerate(movies):\n        print(str(i+1) + '. ' + movie)\n```", "```py\nRecommendations for Michael Henry:\n1\\. Jerry Maguire\n2\\. Inception\n3\\. Anger Management\nRecommendations for John Carson:\n1\\. No recommendations possible\n```", "```py\nimport pyltr\n```", "```py\nwith open('train.txt') as trainfile, \\\n        open('vali.txt') as valifile, \\\n        open('test.txt') as testfile:\n    TrainX, Trainy, Trainqids, _ = pyltr.data.letor.read_dataset(trainfile)\n    ValX, Valy, Valqids, _ = pyltr.data.letor.read_dataset(valifile)\n    TestX, Testy, Testqids, _ = pyltr.data.letor.read_dataset(testfile)\n    metric = pyltr.metrics.NDCG(k=10)\n```", "```py\n    monitor = pyltr.models.monitors.ValidationMonitor(\n                ValX, Valy, Valqids, metric=metric, stop_after=250)\n```", "```py\n model = pyltr.models.LambdaMART(\n    metric=metric,\n    n_estimators=1000,\n    learning_rate=0.02,\n    max_features=0.5,\n    query_subsample=0.5,\n    max_leaf_nodes=10,\n    min_samples_leaf=64,\n    verbose=1,\n)\n```", "```py\nmodel.fit(TestX, Testy, Testqids, monitor=monitor)\n```", "```py\nTestpred = model.predict(TestX)\n```", "```py\nprint('Random ranking:', metric.calc_mean_random(Testqids, Testy))\nprint('Our model:', metric.calc_mean(Testqids, Testy, Testpred))\n```", "```py\nEarly termination at iteration 480\nRandom ranking: 0.27258472902087394\nOur model: 0.5487673789992693\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n```", "```py\nData = pd.read_csv('ratings.csv', sep=';', names=['user', 'item', 'rating', 'timestamp'], header=None)\n\nData = Data.iloc[:,0:3]\n\nNumItems = Data.item.nunique() \nNumUsers = Data.user.nunique()\n\nprint('Item: ', NumItems)\nprint('Users: ', NumUsers)\n```", "```py\nItem: 3706\nUsers: 6040\n```", "```py\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nData['rating'] = Data['rating'].values.astype(float)\nDataScaled = pd.DataFrame(scaler.fit_transform(Data['rating'].values.reshape(-1,1)))\nData['rating'] = DataScaled\n```", "```py\nUserItemMatrix = Data.pivot(index='user', columns='item', values='rating')\nUserItemMatrix.fillna(0, inplace=True)\n\nUsers = UserItemMatrix.index.tolist()\nItems = UserItemMatrix.columns.tolist()\n\nUserItemMatrix = UserItemMatrix.as_matrix()\n```", "```py\nNumInput = NumItems\nNumHidden1 = 10\nNumHidden2 = 5\n```", "```py\nX = tf.placeholder(tf.float64, [None, NumInput])\n\nweights = {\n    'EncoderH1': tf.Variable(tf.random_normal([NumInput, NumHidden1], dtype=tf.float64)),\n    'EncoderH2': tf.Variable(tf.random_normal([NumHidden1, NumHidden2], dtype=tf.float64)),\n    'DecoderH1': tf.Variable(tf.random_normal([NumHidden2, NumHidden1], dtype=tf.float64)),\n    'DecoderH2': tf.Variable(tf.random_normal([NumHidden1, NumInput], dtype=tf.float64)),\n}\n\nbiases = {\n    'EncoderB1': tf.Variable(tf.random_normal([NumHidden1], dtype=tf.float64)),\n    'EncoderB2': tf.Variable(tf.random_normal([NumHidden2], dtype=tf.float64)),\n    'DecoderB1': tf.Variable(tf.random_normal([NumHidden1], dtype=tf.float64)),\n    'DecoderB2': tf.Variable(tf.random_normal([NumInput], dtype=tf.float64)),\n}\n```", "```py\ndef encoder(x):\n    Layer1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['EncoderH1']), biases['EncoderB1']))\n    Layer2 = tf.nn.sigmoid(tf.add(tf.matmul(Layer1, weights['EncoderH2']), biases['EncoderB2']))\n    return Layer2\n\ndef decoder(x):\n    Layer1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['DecoderH1']), biases['DecoderB1']))\n    Layer2 = tf.nn.sigmoid(tf.add(tf.matmul(Layer1, weights['DecoderH2']), biases['DecoderB2']))\n    return Layer2\n```", "```py\nEncoderOp = encoder(X)\nDecoderOp = decoder(EncoderOp)\n\nYPred = DecoderOp\n\nYTrue = X\n```", "```py\nloss = tf.losses.mean_squared_error(YTrue, YPred)\nOptimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\nEvalX = tf.placeholder(tf.int32, )\nEvalY = tf.placeholder(tf.int32, )\nPre, PreOp = tf.metrics.precision(labels=EvalX, predictions=EvalY)\n```", "```py\nInit = tf.global_variables_initializer()\nLocalInit = tf.local_variables_initializer()\nPredData = pd.DataFrame()\n```", "```py\nwith tf.Session() as session:\n    Epochs = 120\n    BatchSize = 200\n\n    session.run(Init)\n    session.run(LocalInit)\n\n    NumBatches = int(UserItemMatrix.shape[0] / BatchSize)\n    UserItemMatrix = np.array_split(UserItemMatrix, NumBatches)\n\n    for i in range(Epochs):\n\n        AvgCost = 0\n\n        for batch in UserItemMatrix:\n            _, l = session.run([Optimizer, loss], feed_dict={X: batch})\n            AvgCost += l\n\n        AvgCost /= NumBatches\n\n        print(\"Epoch: {} Loss: {}\".format(i + 1, AvgCost))\n\n    UserItemMatrix = np.concatenate(UserItemMatrix, axis=0)\n\n    Preds = session.run(DecoderOp, feed_dict={X: UserItemMatrix})\n\n    PredData = PredData.append(pd.DataFrame(Preds))\n\n    PredData = PredData.stack().reset_index(name='rating')\n    PredData.columns = ['user', 'item', 'rating']\n    PredData['user'] = PredData['user'].map(lambda value: Users[value])\n    PredData['item'] = PredData['item'].map(lambda value: Items[value])\n\n    keys = ['user', 'item']\n    Index1 = PredData.set_index(keys).index\n    Index2 = Data.set_index(keys).index\n\n    TopTenRanked = PredData[~Index1.isin(Index2)]\n    TopTenRanked = TopTenRanked.sort_values(['user', 'rating'], ascending=[True, False])\n    TopTenRanked = TopTenRanked.groupby('user').head(10)\n\n    print(TopTenRanked.head(n=10))\n```", "```py\n\n user item rating\n2651 1 2858 0.295800\n1106 1 1196 0.278715\n1120 1 1210 0.251717\n2203 1 2396 0.227491\n1108 1 1198 0.213989\n579  1 593  0.201507\n802  1 858  0.196411\n2374 1 2571 0.195712\n309  1 318  0.191919\n2785 1 2997 0.188679\n```"]