["```py\n    // output vectors of image points\n    std::vector<cv::Point2f> imageCorners;\n    // number of inner corners on the chessboard\n    cv::Size boardSize(6,4);\n    // Get the chessboard corners\n    bool found = cv::findChessboardCorners(image, \n                                 boardSize, imageCorners);\n```", "```py\n    //Draw the corners\n    cv::drawChessboardCorners(image, \n                    boardSize, imageCorners, \n                    found); // corners have been found\n```", "```py\nclass CameraCalibrator {\n\n    // input points:\n    // the points in world coordinates\n    std::vector<std::vector<cv::Point3f>> objectPoints;\n    // the point positions in pixels\n    std::vector<std::vector<cv::Point2f>> imagePoints;\n    // output Matrices\n    cv::Mat cameraMatrix;\n    cv::Mat distCoeffs;\n    // flag to specify how calibration is done\n    int flag;\n```", "```py\n// Open chessboard images and extract corner points\nint CameraCalibrator::addChessboardPoints(\n         const std::vector<std::string>& filelist, \n         cv::Size & boardSize) {\n\n   // the points on the chessboard\n   std::vector<cv::Point2f> imageCorners;\n   std::vector<cv::Point3f> objectCorners;\n\n   // 3D Scene Points:\n   // Initialize the chessboard corners \n   // in the chessboard reference frame\n   // The corners are at 3D location (X,Y,Z)= (i,j,0)\n   for (int i=0; i<boardSize.height; i++) {\n      for (int j=0; j<boardSize.width; j++) {\n         objectCorners.push_back(cv::Point3f(i, j, 0.0f));\n      }\n    }\n\n    // 2D Image points:\n    cv::Mat image; // to contain chessboard image\n    int successes = 0;\n    // for all viewpoints\n    for (int i=0; i<filelist.size(); i++) {\n        // Open the image\n        image = cv::imread(filelist[i],0);\n        // Get the chessboard corners\n        bool found = cv::findChessboardCorners(\n                        image, boardSize, imageCorners);\n        // Get subpixel accuracy on the corners\n        cv::cornerSubPix(image, imageCorners, \n                  cv::Size(5,5), \n                  cv::Size(-1,-1), \n         cv::TermCriteria(cv::TermCriteria::MAX_ITER +\n                          cv::TermCriteria::EPS, \n                  30,     // max number of iterations \n                  0.1));  // min accuracy\n\n        //If we have a good board, add it to our data\n        if (imageCorners.size() == boardSize.area()) {\n            // Add image and scene points from one view\n            addPoints(imageCorners, objectCorners);\n            successes++;\n        }\n    }\n   return successes;\n}\n```", "```py\n// Calibrate the camera\n// returns the re-projection error\ndouble CameraCalibrator::calibrate(cv::Size &imageSize)\n{\n   //Output rotations and translations\n    std::vector<cv::Mat> rvecs, tvecs;\n\n   // start calibration\n   return \n     calibrateCamera(objectPoints, // the 3D points\n               imagePoints,  // the image points\n               imageSize,    // image size\n               cameraMatrix, // output camera matrix\n               distCoeffs,   // output distortion matrix\n               rvecs, tvecs, // Rs, Ts \n               flag);        // set options\n}\n```", "```py\n// remove distortion in an image (after calibration)\ncv::Mat CameraCalibrator::remap(const cv::Mat &image) {\n\n   cv::Mat undistorted;\n\n   if (mustInitUndistort) { // called once per calibration\n\n    cv::initUndistortRectifyMap(\n      cameraMatrix,  // computed camera matrix\n      distCoeffs,    // computed distortion matrix\n      cv::Mat(),     // optional rectification (none) \n      cv::Mat(),     // camera matrix to generate undistorted\n      image.size(),  // size of undistorted\n      CV_32FC1,      // type of output map\n      map1, map2);   // the x and y mapping functions\n\n    mustInitUndistort= false;\n   }\n\n   // Apply mapping functions\n   cv::remap(image, undistorted, map1, map2, \n      cv::INTER_LINEAR); // interpolation type\n\n   return undistorted;\n}\n```", "```py\n      cv::Size boardSize(7,7);\n      std::vector<cv::Point2f> centers;\n      bool found = cv:: findCirclesGrid(\n                          image, boardSize, centers);\n```", "```py\n   // Convert keypoints into Point2f\n   std::vector<cv::Point2f> selPoints1, selPoints2;\n   std::vector<int> pointIndexes1, pointIndexes2;\n   cv::KeyPoint::convert(keypoints1,selPoints1,pointIndexes1);\n   cv::KeyPoint::convert(keypoints2,selPoints2,pointIndexes2);\n```", "```py\n   // Compute F matrix from 7 matches\n   cv::Mat fundamental= cv::findFundamentalMat(\n      selPoints1,    // 7 points in first image\n      selPoints2,    // 7 points in second image\n      CV_FM_7POINT); // 7-point method\n```", "```py\n   // draw the left points corresponding epipolar \n   // lines in right image \n   std::vector<cv::Vec3f> lines1; \n   cv::computeCorrespondEpilines(\n      selPoints1,  // image points \n      1,           // in image 1 (can also be 2)\n      fundamental, // F matrix\n      lines1);     // vector of epipolar lines\n\n   // for all epipolar lines\n   for (vector<cv::Vec3f>::const_iterator it= lines1.begin();\n       it!=lines1.end(); ++it) {\n          // draw the line between first and last column\n          cv::line(image2,\n            cv::Point(0,-(*it)[2]/(*it)[1]),\n            cv::Point(image2.cols,-((*it)[2]+\n                      (*it)[0]*image2.cols)/(*it)[1]),\n                      cv::Scalar(255,255,255));\n   }\n```", "```py\nclass RobustMatcher {\n  private:\n    // pointer to the feature point detector object\n    cv::Ptr<cv::FeatureDetector> detector;\n    // pointer to the feature descriptor extractor object\n    cv::Ptr<cv::DescriptorExtractor> extractor;\n    int normType;\n    float ratio; // max ratio between 1st and 2nd NN\n    bool refineF; // if true will refine the F matrix\n    double distance; // min distance to epipolar\n    double confidence; // confidence level (probability)\n\n  public:\n    RobustMatcher(std::string detectorName, // specify by name\n                   std::string descriptorName) \n      : normType(cv::NORM_L2), ratio(0.8f), \n          refineF(true), confidence(0.98), distance(3.0) {    \n\n      // construct by name\n      if (detectorName.length()>0) {\n      detector= cv::FeatureDetector::create(detectorName); \n      extractor= cv::DescriptorExtractor::\n                           create(descriptorName);\n      }\n    }\n```", "```py\n// Match feature points using RANSAC\n// returns fundamental matrix and output match set\ncv::Mat match(cv::Mat& image1, cv::Mat& image2, // input images \n    std::vector<cv::DMatch>& matches,           // output matches\n    std::vector<cv::KeyPoint>& keypoints1,      // output keypoints\n    std::vector<cv::KeyPoint>& keypoints2) { \n\n    // 1\\. Detection of the feature points\n    detector->detect(image1,keypoints1);\n    detector->detect(image2,keypoints2);\n\n    // 2\\. Extraction of the feature descriptors\n    cv::Mat descriptors1, descriptors2;\n    extractor->compute(image1,keypoints1,descriptors1);\n    extractor->compute(image2,keypoints2,descriptors2);\n\n    // 3\\. Match the two image descriptors\n    //    (optionnally apply some checking method)\n\n    // Construction of the matcher with crosscheck \n    cv::BFMatcher matcher(normType,   //distance measure\n                            true);    // crosscheck flag\n    // match descriptors\n    std::vector<cv::DMatch> outputMatches;\n    matcher.match(descriptors1,descriptors2,outputMatches);\n\n    // 4\\. Validate matches using RANSAC\n    cv::Mat fundaental= ransacTest(outputMatches, \n                              keypoints1, keypoints2, matches);\n    // return the found fundamental matrix\n    return fundamental;\n  }\n```", "```py\n// Identify good matches using RANSAC\n// Return fundamental matrix and output matches\ncv::Mat ransacTest(const std::vector<cv::DMatch>& matches,\n                   const std::vector<cv::KeyPoint>& keypoints1, \n                   const std::vector<cv::KeyPoint>& keypoints2,\n                   std::vector<cv::DMatch>& outMatches) {\n\n// Convert keypoints into Point2f  \n  std::vector<cv::Point2f> points1, points2;    \n  for (std::vector<cv::DMatch>::const_iterator it= \n  matches.begin(); it!= matches.end(); ++it) {\n\n       // Get the position of left keypoints\n       points1.push_back(keypoints1[it->queryIdx].pt);\n       // Get the position of right keypoints\n       points2.push_back(keypoints2[it->trainIdx].pt);\n    }\n\n  // Compute F matrix using RANSAC\n  std::vector<uchar> inliers(points1.size(),0);\n  cv::Mat fundamental= cv::findFundamentalMat(\n      points1,points2, // matching points\n      inliers,      // match status (inlier or outlier)  \n      CV_FM_RANSAC, // RANSAC method\n      distance,     // distance to epipolar line\n      confidence);  // confidence probability\n\n  // extract the surviving (inliers) matches\n  std::vector<uchar>::const_iterator itIn= inliers.begin();\n  std::vector<cv::DMatch>::const_iterator itM= matches.begin();\n  // for all matches\n  for ( ;itIn!= inliers.end(); ++itIn, ++itM) {\n    if (*itIn) { // it is a valid match\n      outMatches.push_back(*itM);\n    }\n  }\n  return fundamental;\n}\n```", "```py\n  // Prepare the matcher (with default parameters)\n  RobustMatcher rmatcher(\"SURF\"); // we use SURF features here\n  // Match the two images\n  std::vector<cv::DMatch> matches;\n  std::vector<cv::KeyPoint> keypoints1, keypoints2;\n  cv::Mat fundamental= rmatcher.match(image1,image2,\n                           matches, keypoints1, keypoints2);\n```", "```py\n    if (refineF) {\n      // The F matrix will \n      // be recomputed with all accepted matches\n\n      // Convert keypoints into Point2f \n      points1.clear();\n      points2.clear();\n      for (std::vector<cv::DMatch>::\n                  const_iterator it= outMatches.begin();\n         it!= outMatches.end(); ++it) {\n\n         // Get the position of left keypoints\n         points1.push_back(keypoints1[it->queryIdx].pt);\n\n         // Get the position of right keypoints\n         points2.push_back(keypoints2[it->trainIdx].pt);\n      }\n\n      // Compute 8-point F from all accepted matches\n      fundamental= cv::findFundamentalMat(\n        points1,points2, // matching points\n        CV_FM_8POINT); // 8-point method solved using SVD\n    }\n```", "```py\n  std::vector<cv::Point2f> newPoints1, newPoints2;  \n  // refine the matches\n  correctMatches(fundamental,            // F matrix\n                points1, points2,        // original position\n                newPoints1, newPoints2); // new position\n```", "```py\n// Find the homography between image 1 and image 2\nstd::vector<uchar> inliers(points1.size(),0);\ncv::Mat homography= cv::findHomography(\n  points1, points2, // corresponding points\n  inliers,   // outputed inliers matches \n  CV_RANSAC, // RANSAC method\n  1.);       // max distance to reprojection point\n```", "```py\n   // Draw the inlier points\n   std::vector<cv::Point2f>::const_iterator itPts=  \n                                            points1.begin();\n   std::vector<uchar>::const_iterator itIn= inliers.begin();\n   while (itPts!=points1.end()) {\n\n      // draw a circle at each inlier location\n      if (*itIn) \n          cv::circle(image1,*itPts,3,\n                    cv::Scalar(255,255,255));\n      ++itPts;\n      ++itIn;\n   }\n```", "```py\n   // Warp image 1 to image 2\n   cv::Mat result;\n   cv::warpPerspective(image1,  // input image\n      result,                   // output image\n      homography,               // homography\n      cv::Size(2*image1.cols,\n                 image1.rows)); // size of output image\n```", "```py\n   // Copy image 1 on the first half of full image\n   cv::Mat half(result,cv::Rect(0,0,image2.cols,image2.rows));\n   image2.copyTo(half); // copy image2 to image1 roi\n```", "```py\nclass TargetMatcher {\n\n  private:\n\n    // pointer to the feature point detector object\n    cv::Ptr<cv::FeatureDetector> detector;\n    // pointer to the feature descriptor extractor object\n    cv::Ptr<cv::DescriptorExtractor> extractor;\n    cv::Mat target; // target image\n    int normType;\n    double distance; // min reprojection error\n```", "```py\n  // detect the defined planar target in an image\n  // returns the homography \n  // the 4 corners of the detected target\n  // plus matches and keypoints\n  cv::Mat detectTarget(const cv::Mat& image, \n    // position of the target corners (clock-wise)\n    std::vector<cv::Point2f>& detectedCorners,       \n    std::vector<cv::DMatch>& matches,\n    std::vector<cv::KeyPoint>& keypoints1,\n    std::vector<cv::KeyPoint>& keypoints2) {\n    // find a RANSAC homography between target and image\n    cv::Mat homography= match(target,image,matches, \n                                keypoints1, keypoints2);\n    // target corners\n    std::vector<cv::Point2f> corners;  \n    corners.push_back(cv::Point2f(0,0));\n    corners.push_back(cv::Point2f(target.cols-1,0));\n    corners.push_back(cv::Point2f(target.cols-1,target.rows-1));\n    corners.push_back(cv::Point2f(0,target.rows-1));\n\n    // reproject the target corners\n    cv::perspectiveTransform(corners,detectedCorners,\n                                      homography);\n    return homography;\n  }\n```", "```py\n// Prepare the matcher \nTargetMatcher tmatcher(\"FAST\",\"FREAK\");\ntmatcher.setNormType(cv::NORM_HAMMING);\n\n// definition of the output data\nstd::vector<cv::DMatch> matches;\nstd::vector<cv::KeyPoint> keypoints1, keypoints2;\nstd::vector<cv::Point2f> corners;\n// the reference image\ntmatcher.setTarget(target); \n// match image with target\ntmatcher.detectTarget(image,corners,matches,\n                            keypoints1,keypoints2);\n// draw the target corners on the image\ncv::Point pt= cv::Point(corners[0]);\ncv::line(image,cv::Point(corners[0]),cv::Point(corners[1]),\n               cv::Scalar(255,255,255),3);\ncv::line(image,cv::Point(corners[1]),cv::Point(corners[2]),\n               cv::Scalar(255,255,255),3);\ncv::line(image,cv::Point(corners[2]),cv::Point(corners[3]),\n               cv::Scalar(255,255,255),3);\ncv::line(image,cv::Point(corners[3]),cv::Point(corners[0]),\n               cv::Scalar(255,255,255),3);\n```"]