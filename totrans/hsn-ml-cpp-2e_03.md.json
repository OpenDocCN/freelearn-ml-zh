["```py\ntemplate<typename MLAlgorithm, typename Metric,\n    typename CV,...>\nclass HyperParameterTuner{…};\n```", "```py\ndouble validation_size = 0.2;\nHyperParameterTuner<LinearRegression, \n  MSE, \n  SimpleCV> parameters_tuner(\n    validation_size, samples, labels);\n```", "```py\nstd::pair<arma::mat, arma::rowvec> GenerateData(\n  size_t num_samples) {\n  arma::mat samples = arma::randn<arma::mat>(1, num_samples);\n  arma::rowvec labels = samples + arma::randn<arma::rowvec(\n    num_samples, arma::distr_param(1.0, 1.5));\n  return {samples, labels};\n}\n...\nsize_t num_samples = 1000;\nauto [raw_samples, raw_labels] = GenerateData(num_samples);\n```", "```py\ndata::StandardScaler sample_scaler;\nsample_scaler.Fit(raw_samples);\narma::mat samples(1, num_samples);\nsample_scaler.Transform(raw_samples, samples);\ndata::StandardScaler label_scaler;\nlabel_scaler.Fit(raw_labels);\narma::rowvec labels(num_samples);\nlabel_scaler.Transform(raw_labels, labels);\n```", "```py\narma::vec lambdas{0.0, 0.001, 0.01, 0.1, 1.0};\ndouble best_lambda = 0;\nstd::tie(best_lambda) = parameters_tuner.Optimize(lambdas);\n```", "```py\nLinearRegression& linear_regression =  parameters_tuner.BestModel();\n```", "```py\nsize_t num_new_samples = 50;\narma::dvec new_samples_values = arma::linspace<\n  arma::dvec>(x_minmax.first, x_minmax.second, num_new_samples);\narma::mat new_samples(1, num_new_samples);\nnew_samples.row(0) = arma::trans(new_samples_values);\narma::mat norm_new_samples(1, num_new_samples);\nsample_scaler.Transform(new_samples, norm_new_samples);\n```", "```py\narma::rowvec predictions(num_new_samples);\nlinear_regression.Predict(norm_new_samples, predictions);\n```", "```py\nimport optuna\nimport subprocess\n```", "```py\ndef objective(trial: optuna.trial.Trial):\n  lr = trial.suggest_float(\"learning_rate\", low=0.01, high=0.05)\n  d = trial.suggest_int(\"polynomial_degree\", low=8, high=16)\n  bs = trial.suggest_int(\"batch_size\", low=16, high=64)\n  result = subprocess.run(\n    [binary_path, str(d), str(lr), str(bs)],\n    stdout=subprocess.PIPE\n  )\n  mse = float(result.stdout)\n  return mse\n```", "```py\nsearch_space = {\n  \"learning_rate\": [0.01, 0.025, 0.045],],],\n  \"polynomial_degree\": [8, 14, 16],\n  \"batch_size\": [16, 32, 64],\n}\n```", "```py\nstudy = optuna.create_study(\n  study_name=\"PolyFit\",\n  direction=\"minimize\",\n  sampler=optuna.samplers.GridSampler(search_space),\n)\n```", "```py\nstudy.optimize(objective)\nprint(f\"Best value: {study.best_value}\n     (params: {study.best_params})\\n\")\n```", "```py\nint main(int argc, char** argv) {\n  if (argc < 3) {\n    std::cout << \"Usage: \" << argv[0] <<\n    \" polynomial_degree learning_rate batch_size\" << std::endl;\n    return 0;\n  } else {\n    // Hyper parameters\n    int polynomial_degree = std::atoi(argv[1]);\n    double learning_rate = std::atof(argv[2]);\n    int batch_size = std::atoi(argv[3]);\n    // Other code...\n  }\n}\n```", "```py\n// define learnable variables\nauto weight = fl::Variable(fl::rand({polynomial_degree, 1}),\n                           /*calcGrad*/ true);\nauto bias = fl::Variable(fl::full({1}, 0.0),\n                         /*calcGrad*/ true);\nfloat mse = 0;\nfl::MeanSquaredError mse_func;\nfor (int e = 1; e <= num_epochs; ++e) {\n  fl::Tensor error = fl::fromScalar(0);\n  for (auto& batch : *batch_dataset) {\n    auto input = fl::Variable(batch[0],\n                              /*calcGrad*/ false);\n    auto local_batch_size = batch[0].shape().dim(1);\n    auto predictions = fl::matmul(fl::transpose(\n                             weight), input) + fl::tile(\n                             bias, {1, local_batch_size});\n    auto targets = fl::Variable(\n        fl::reshape(batch[1], {1, local_batch_size}),\n        /*calcGrad*/ false);\n    // Mean Squared Error Loss\n    auto loss = mse_func.forward(predictions, targets);\n    // Compute gradients using backprop\n    loss.backward();\n    // Update the weight and bias\n    weight.tensor() -= learning_rate * weight.grad().tensor();\n    bias.tensor() -= learning_rate * bias.grad().tensor();\n    // Clear the gradients for next iteration\n    weight.zeroGrad();\n    bias.zeroGrad();\n    mse_func.zeroGrad();\n    error += loss.tensor();\n  }\n// Mean Squared Error for the epoch\nerror /= batch_dataset->size();\nmse = error.scalar<float>();\n```", "```py\nstd::cout << mse;\n```", "```py\nauto CrossValidationScore = [&](const double gamma,\n  const double c,\n  const double degree_in) {\n  auto degree = std::floor(degree_in);\n  using KernelType = Dlib::polynomial_kernel<SampleType>;\n  Dlib::svr_trainer<KernelType> trainer;\n  trainer.set_kernel(KernelType(gamma, c, degree));\n  Dlib::matrix<double> result = Dlib::\n    cross_validate_regression_trainer(\n      trainer, samples, raw_labels, 10);\n  return result(0, 0);\n};\n```", "```py\nauto result = find_min_global(\n    CrossValidationScore,\n    {0.01, 1e-8, 5}, // minimum values for gamma, c, and degree\n    {0.1, 1, 15}, // maximum values for gamma, c, and degree\n    max_function_calls(50));\n```", "```py\ndouble gamma = result.x(0);\ndouble c = result.x(1);\ndouble degree = result.x(2);\nusing KernelType = Dlib::polynomial_kernel<SampleType>;\nDlib::svr_trainer<KernelType> trainer;\ntrainer.set_kernel(KernelType(gamma, c, degree));\nauto descision_func = trainer.train(samples, raw_labels)\n```"]