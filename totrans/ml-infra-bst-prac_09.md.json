["```py\n# read the file with data using openpyxl\nimport pandas as pd\n# we read the data from the excel file,\n# which is the defect data from the ant 1.3 system\ndfDataAnt13 = pd.read_excel('./chapter_6_dataset_numerical.\n              xlsx',sheet_name='ant_1_3', index_col=0)\ndfDataAnt13\n```", "```py\n# let's remove the defect column, as this is the one that\n# we could potentially predict\ndfDataAnt13Pred = dfDataAnt13.drop(['Defect'], axis = 1)\n```", "```py\n# now, let's import PCA and find a few components\nfrom sklearn.decomposition import PCA\n# previously, we used 2 components, now, let's go with\n# three\npca = PCA(n_components=3)\n# now, the transformation to the new components\ndfDataAnt13PCA = pca.fit_transform(dfDataAnt13Pred)\n# and printing the resulting array\n# or at least the three first elements\ndfDataAnt13PCA[:3]\n```", "```py\n# and let's visualize that using the seaborn library\nimport seaborn as sns\nsns.set(rc={\"figure.figsize\":(8, 8)})\nsns.set_style(\"white\")\nsns.set_palette('rocket')\nsns.barplot(x=['PC 1', 'PC 2', 'PC 3'], y=pca.explained_variance_ratio_)\n```", "```py\n# for t-SNE, we use the same data as we used previously\n# i.e., the predictor dfDataAnt13Pred\nfrom sklearn.manifold import TSNE\n# we create the t-sne transformation with three components\n# just like we did with the PCA\ntsne = TSNE(n_components = 3)\n# we fit and transform the data\ndfDataAnt13TSNE = tsne.fit_transform(dfDataAnt13Pred)\n# and print the three first rows\ndfDataAnt13TSNE[:3]\n```", "```py\n# we import the package\nfrom sklearn.decomposition import FastICA\n# instantiate the ICA\nica = FastICA(n_components=3)\n# transform the data\ndfDataAnt13ICA = ica.fit_transform(dfDataAnt13Pred)\n# and check the first three rows\ndfDataAnt13ICA[:3]\n```", "```py\nfrom sklearn.manifold import LocallyLinearEmbedding\n# instantiate the classifier\nlle = LocallyLinearEmbedding(n_components=3)\n# transform the data\ndfDataAnt13LLE = lle.fit_transform(dfDataAnt13Pred)\n# print the three first rows\ndfDataAnt13LLE[:3]\n```", "```py\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# create the classifier\n# please note that we can only use one component, because\n# we have only one predicted variable\nlda = LinearDiscriminantAnalysis(n_components=1)\n# fit to the data\n# please note that this transformation requires the predicted\n# variable too\ndfDataAnt13LDA = lda.fit(dfDataAnt13Pred, dfDataAnt13.Defect).transform(dfDataAnt13Pred)\n# print the transformed data\ndfDataAnt13LDA[:3]\n```", "```py\n# read the file with data using openpyxl\nimport pandas as pd\n# we read the data from the excel file,\n# which is the defect data from the ant 1.3 system\ndfDataAnt13 = pd.read_excel('./chapter_6_dataset_numerical.\n              xlsx',sheet_name='ant_1_3',index_col=0)\n# let's remove the defect column, as this is the one that we could\n# potentially predict\nX = dfDataAnt13.drop(['Defect'], axis = 1)\ny = dfDataAnt13.Defect\n```", "```py\n# split into train test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# scale data\nt = MinMaxScaler()\nt.fit(X_train)\nX_train = t.transform(X_train)\nX_test = t.transform(X_test)\n```", "```py\n# number of input columns\nn_inputs = X.shape[1]\n# the first layer - the visible one\nvisible = Input(shape=(n_inputs,))\n# encoder level 1\ne = Dense(n_inputs*2)(visible)\ne = BatchNormalization()(e)\ne = LeakyReLU()(e)\n# encoder level 2\ne = Dense(n_inputs)(e)\ne = BatchNormalization()(e)\ne = LeakyReLU()(e)\n```", "```py\nn_bottleneck = 3\nbottleneck = Dense(n_bottleneck)(e)\n```", "```py\n# define decoder, level 1\nd = Dense(n_inputs)(bottleneck)\nd = BatchNormalization()(d)\nd = LeakyReLU()(d)\n# decoder level 2\nd = Dense(n_inputs*2)(d)\nd = BatchNormalization()(d)\nd = LeakyReLU()(d)\n# output layer\noutput = Dense(n_inputs, activation='linear')(d)\n```", "```py\n# we place both of these into one model\n# define autoencoder model\nmodel = Model(inputs=visible, outputs=output)\n# compile autoencoder model\nmodel.compile(optimizer='adam', loss='mse')\n```", "```py\n# we train the autoencoder model\nhistory = model.fit(X_train, X_train,\n                    epochs=100,\n                    batch_size=16,\n                    verbose=2,\n                    validation_data=(X_test,X_test))\n```", "```py\nsubmodel = Model(model.inputs, model.get_layer(\"dense_8\").output)\n# this is the actual feature extraction -\n# where we make prediction for the train dataset\n# please note that the autoencoder requires a two dimensional array\n# so we need to take one datapoint and make it into a two dimensional array\n# with only one row\nresults = submodel.predict(np.array([X_train[0]]))\nresults[0]\n```", "```py\n# first, let's read the image data from the Keras library\nfrom tensorflow.keras.datasets import mnist\n# and load it with the pre-defined train/test splits\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train/255.0\nX_test = X_test/255.0\n```", "```py\n# image size is 28 pixels\nn_inputs = 28\n# the first layer - the visible one\nvisible = Input(shape=(n_inputs,n_inputs,))\n# encoder level 1\ne = Flatten(input_shape = (28, 28))(visible)\ne = LeakyReLU()(e)\ne = Dense(n_inputs*2)(e)\ne = BatchNormalization()(e)\ne = LeakyReLU()(e)\n# encoder level 2\ne = Dense(n_inputs)(e)\ne = BatchNormalization()(e)\ne = LeakyReLU()(e)\n```", "```py\nn_bottleneck = 32\nbottleneck = Dense(n_bottleneck)(e)\n```", "```py\n# and now, we define the decoder part\n# define decoder, level 1\nd = Dense(n_inputs)(bottleneck)\nd = BatchNormalization()(d)\nd = LeakyReLU()(d)\n# decoder level 2\nd = Dense(n_inputs*2)(d)\nd = BatchNormalization()(d)\nd = LeakyReLU()(d)\n# output layer\nd = Dense(n_inputs*n_inputs, activation='linear')(d)\noutput = Reshape((28,28))(d)\n```", "```py\n# we place both of these into one model\n# define autoencoder model\nmodel = Model(inputs=visible, outputs=output)\n# compile autoencoder model\nmodel.compile(optimizer='adam', loss='mse')\n# we train the autoencoder model\nhistory = model.fit(X_train, X_train,\n                    epochs=100,\n                    batch_size=16,\n                    verbose=2,\n                    validation_data=(X_test,X_test))\n```", "```py\nsubmodel = Model(model.inputs, bottleneck)\n# this is the actual feature extraction -\n# where we make prediction for the train dataset\n# please note that the autoencoder requires a two dimensional array\n# so we need to take one datapoint and make it into a two dimensional array\n# with only one row\nresults = submodel.predict(np.array([X_train[0]]))\nresults[0]\n```"]