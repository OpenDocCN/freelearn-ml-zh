<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-66"><a id="_idTextAnchor070"/>4</h1>
<h1 id="_idParaDest-67"><a id="_idTextAnchor071"/>Parametric Tests</h1>
<p>In the previous chapter, we introduced the concept of a hypothesis test and showed several applications of the z-test. The z-test is a type of hypothesis test in a family of hypothesis tests called parametric tests. Parametric tests are powerful hypothesis tests, but the application of parametric tests requires certain assumptions to be met by the data. While the z-test is a useful test, it is limited by the required assumptions. In this chapter, we will discuss several more parametric tests, which will expand our parametric tool set. More specifically, we will discuss the various applications of the t-test, how to perform tests when more than two subgroups of data are present, and the hypothesis test for Pearson’s correlation coefficient. We will complete the chapter with a discussion on power analysis for parametric tests.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>Assumptions of <strong class="bold">parametric tests</strong></li>
<li><strong class="bold">T-test</strong>—a parametric hypothesis test</li>
<li>Tests with more than two groups and <strong class="bold">analysis of </strong><strong class="bold">variance</strong> (<strong class="bold">ANOVA</strong>)</li>
<li><strong class="bold">Pearson’s </strong><strong class="bold">correlation coefficient</strong></li>
<li><strong class="bold">Power </strong><strong class="bold">analysis</strong> examples</li>
</ul>
<h1 id="_idParaDest-68"><a id="_idTextAnchor072"/>Assumptions of parametric tests</h1>
<p>Parametric tests <a id="_idIndexMarker295"/>make assumptions about population data that require the statistics practitioner to perform analysis of data prior to modeling, especially when using sample data because the sample statistics are leveraged as estimates for the population parameters when the true population parameters are unknown. These are the three primary assumptions of parametric hypothesis tests:</p>
<ul>
<li>Normally distributed population data</li>
<li>Samples are independent</li>
<li>Equal population variances (when comparing two or more groups)</li>
</ul>
<p>In this <a id="_idIndexMarker296"/>chapter, we discuss the z-test, t-test, ANOVA, and Pearson’s correlation. These tests are used on continuous data. In addition to these assumptions, Pearson’s correlation requires data to contain paired samples. In other words, there must be an equal number of samples in each group being compared as Pearson’s correlation is based on pairwise comparisons.</p>
<p>While these assumptions are ideal, there are many occasions where these cannot be ensured. Consequently, it is useful to understand there is some robustness to these assumptions, depending on the test.</p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor073"/>Normally distributed population data</h2>
<p>Because <a id="_idIndexMarker297"/>in parametric hypothesis tests we are interested in gaining inferences about population parameters, such as <a id="_idIndexMarker298"/>the mean or standard deviation, we must assume the parameter of choice is representative of the distribution and that it is safe to assume a central tendency in the data. We must also assume the statistic (parametric value taken from a sample or sampling distribution) is representative of its respective population parameter. Therefore, since we assume in parametric hypothesis tests that the population is normally distributed, the sample should also be normally distributed as well. Otherwise, it is not safe to assume the sample is representative of the population.</p>
<p>Parametric hypothesis tests rely heavily on the mean and assume it is strongly representative of the data’s central point (all population data is centrally distributed around the mean). Consider where the means of two distributions are being compared to test if there is a statistically significant difference between them. If the distributions are skewed, the mean will not be the center point of the data and, consequently, cannot represent the distributions very well. Since this would be the case, inference obtained from a test comparing the means would not be reliable.</p>
<h3>Robustness to normally distributed data</h3>
<p>Many<a id="_idIndexMarker299"/> hypothesis tests specify degrees of freedom when using samples to make estimates about populations. Degrees of freedom force models to assume there is extra variance in the distributions used than actually present. While the statistical parameters in the analysis remain the same, the assumed extra variance forces measures of central tendency closer. Stated differently, using degrees of freedom forces measures of central tendency to be more centrally representative of the distributions from which they are calculated. The reason for this is that it is assumed samples—while representative of their overall populations—represent their populations with a margin of error. Consequently, parametric hypothesis tests using degrees of freedom have some robustness to violations of the requirement for normally distributed data.</p>
<p>In the plots shown in <em class="italic">Figure 4</em><em class="italic">.1</em>, we have a slightly skewed distribution. One applies degrees of freedom while the other does not. We can see the mean and median have the same distance between them whether degrees of freedom are used or not. However, the distribution using degrees of freedom takes on more errors (more variance):</p>
<div><div><img alt="Figure 4.1 – Visualizing the influence of degrees of freedom" height="441" src="img/B18945_04_001.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Visualizing the influence of degrees of freedom</p>
<p>When using a hypothesis test that considers the mean, we can see that the mean, while not centered (as is the median), approximates the center of the distribution much more closely, relative to all data points, when degrees of freedom are used. Since parametric hypothesis tests use the mean as the central point, this is important for the usefulness of the model as the mean is more representative of the central point of the data when degrees of freedom are used. This is a primary reason there is some robustness to normality. Some other robustness is in the statistical interpretation, such as in choosing the level of <a id="_idIndexMarker300"/>confidence; if a distribution is not perfectly normally distributed, it may be beneficial to use a 90% level of confidence rather than a 99% level of confidence, for example.</p>
<h3>Testing for normally distributed data</h3>
<p>There are<a id="_idIndexMarker301"/> multiple methods for determining whether a distribution is normally distributed and thus can be used in<a id="_idIndexMarker302"/> parametric hypothesis testing. Generally, the level of adherence to normality is up to the discretion of the researcher. The methods in this section leave some margin for debate on normality based on visual inspection as well as levels of statistical significance applied.</p>
<h4>Visual inspection</h4>
<p>The<a id="_idIndexMarker303"/> best tests to identify whether a distribution <a id="_idIndexMarker304"/>is normally distributed or not are based on visual inspection. We can use <strong class="bold">Quantile-Quantile</strong> (<strong class="bold">QQ</strong>) plots <a id="_idIndexMarker305"/>and histograms—among other tests—to visually inspect the distributions.</p>
<p>In the following code snippet, we generate plots of the original data as well as the QQ plots using the <code>scipy.stats</code> module’s <code>probplot</code> function:</p>
<pre class="source-code">
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np
mu, sigma = 0, 1.1
normally_distributed = np.random.normal(mu, sigma, 1000)</pre>
<p>In <em class="italic">Figure 4</em><em class="italic">.2</em>, we can see in the first column a histogram of exponentially distributed data and, beneath it, its QQ plot. As the points are very far from approximating adherence to the 45-degree red line, which represents a pure normal distribution, we can conclude the data is not normally distributed. By visually inspecting the data in the second column, we can see the histogram exhibits an approximately normally distributed dataset. This is backed up by the QQ plot below it, where the points mostly approximate the 45-degree red line. With respect to the tails of the QQ plot, these data points represent the density of skewness. We expect with a normally distributed dataset that the bulk of data points will tend toward the center of the red line. With the exponential distribution, we can see a heavy density toward the left, lower tail of the red line, and a sparse <a id="_idIndexMarker306"/>scattering of points toward the <a id="_idIndexMarker307"/>upper-right side of the line. The QQ plot can be read left to right, mirroring the spread seen in the histogram, where the smallest values appear on the left-hand side of the <em class="italic">x</em> axis and the largest on the right-hand side:</p>
<div><div><img alt="Figure 4.2 – Visually assessing normality with QQ and histogram plots" height="855" src="img/B18945_04_002.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Visually assessing normality with QQ and histogram plots</p>
<p>Visual inspection of the QQ plots and histograms should be enough to help a researcher conclude whether the normality assumption has been violated or not. However, in cases where one might <a id="_idIndexMarker308"/>not want to perform visual inspection—such as when constructing a data science pipeline—there are alternative approaches that provide specific<a id="_idIndexMarker309"/> measurements of normality. Three of the most commonly used<a id="_idIndexMarker310"/> tests are<a id="_idIndexMarker311"/> the <strong class="bold">Kolmogorov-Smirnov</strong>, <strong class="bold">Anderson-Darling</strong>, and <strong class="bold">Shapiro-Wilk</strong> tests.</p>
<p>The<a id="_idIndexMarker312"/> Kolmogorov-Smirnov test focuses more on the centrality of the data. Consequently, however, the test has less power if there is a wide variance around the center of the data. Anderson-Darling focuses more on the tails of the data than the center and is more likely to identify non-conformity to normality if data is heavy-tailed with extreme outliers. These two tests perform well on large sample sizes but do not have as much power when sample sizes are lower. The third test we consider, Shapiro-Wilk, is more general than the Kolmogorov-Smirnov and Anderson-Darling tests and therefore more robust to small sample sizes. Based on these traits, it may be more useful to use Shapiro-Wilk tests in an automated pipeline. Alternatively, it may be better to lower the level of confidence for the test being applied.</p>
<h4>Kolmogorov-Smirnov</h4>
<p>The <code>kstest</code> function in the <code>scipy.stats</code> module, using <code>stats.norm.cdf</code> (<code>scipy</code>’s cumulative density function) performs this one-sample version of the test. The two-sample version tests against a specified distribution to determine whether the two distributions match. In the two-sample case, the distribution to be tested must be provided as a <code>numpy</code> array instead of the <code>stats.norm.cdf</code> function used in the code snippet shown below <em class="italic">Figure 4</em><em class="italic">.3</em>. However, this is outside of the scope of testing for normality, so we will not look at this.</p>
<p>Kolmogorov-Smirnov measures a calculated test statistic against a table-based critical value (<code>kstest</code> calculates this internally). As with other hypothesis tests, if the test statistic is larger than the critical value, the null hypothesis that the given distribution is normally distributed can be rejected. This can also be assessed if the p-value is low enough to be significant. The test statistic is calculated as the absolute value of the maximum distance between all data points in the given distribution against the cumulative density<a id="_idIndexMarker315"/> function.</p>
<p class="callout-heading">Kolmogorov-Smirnov special requirement</p>
<p class="callout">The Kolmogorov-Smirnov test requires data to be centered around zero and scaled to a standard deviation of one. All data must be transformed for the test, but inference can be applied to the pre-transformed distribution; the centered and scaled distribution does not need to be the distribution used in further statistical testing or analysis.</p>
<p>In the<a id="_idIndexMarker316"/> following code snippet, we test to confirm whether a normally distributed dataset, <code>normally_distributed</code>, is normally distributed. The dataset has a mean of 0 and a standard deviation of 1. The output confirms the data is normally distributed. The plots in <em class="italic">Figure 4</em><em class="italic">.3</em> show the normally distributed distribution centered around a mean of 0 with a standard deviation of 1, and on the right of it is the exponentially transformed version of the same distribution:</p>
<pre class="source-code">
from scipy import stats
import numpy as np
mu, sigma = 0, 1
normally_distributed = np.random.normal(mu, sigma, 1000)</pre>
<div><div><img alt="Figure 4.3 – Normally distributed and exponential data" height="448" src="img/B18945_04_003.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Normally distributed and exponential data</p>
<p>Here, we<a id="_idIndexMarker317"/> run the Kolmogorov-Smirnov <a id="_idIndexMarker318"/>test:</p>
<pre class="source-code">
stats.kstest(normally_distributed,
             stats.norm.cdf)</pre>
<p>The <code>statsmodels</code> Kolmorogov-Smirnov test yielded the following results for our data:</p>
<p><code>KstestResult(statistic=0.0191570377833315, pvalue=0.849436919292824)</code></p>
<p>If we use the same data, but transform it exponentially to be right-skewed, the same test indicates the data is no longer normally distributed:</p>
<pre class="source-code">
stats.kstest(np.exp(normally_distributed), stats.norm.cdf)</pre>
<p>The signficant p-value confirms non-normality:</p>
<p><code>KstestResult(statistic=0.5375205782404135, pvalue=9.59979841227121e-271)</code></p>
<p>Next, let us take a distribution of 1,000 samples with a mean of 100 and a standard deviation of 2. We need to center it to a mean of 0 with unit variance (standard deviation of 1). In the following code snippet, we generate the data, then perform the scaling and save it to the <code>normally_distributed_scaled</code> variable:</p>
<pre class="source-code">
mu, sigma = 100, 2
normally_distributed = np.random.normal(mu, sigma, 1000)
normally_distributed_scaled = (
    normally_distributed-normally_distributed.mean()) /
    normally_distributed.std()</pre>
<p>Now that <a id="_idIndexMarker319"/>the data is centered and scaled as required, we check it using the Kolmogorov-Smirnov test. As expected, the <a id="_idIndexMarker320"/>data is confirmed normally distributed:</p>
<pre class="source-code">
stats.kstest(normally_distributed_scaled, stats.norm.cdf)</pre>
<p>This is the output:</p>
<p><code>KstestResult(statistic=0.02597307287070466, pvalue=0.5016041053535877)</code></p>
<h4>Anderson-Darling</h4>
<p>Similar to <a id="_idIndexMarker321"/>the Kolmogorov-Smirnov test, the <code>scipy</code>’s <code>anderson</code> test, we can test against other distributions, but the default argument specifying a normal distribution, <code>dist="norm"</code>, assumes a null hypothesis that the given distribution is statistically the same as a normally distributed distribution. For each distribution tested against, a different set of critical values must be calculated.</p>
<p class="callout-heading">Anderson-Darling compared to Kolmogorov-Smirnov</p>
<p class="callout">Note that while both the Anderson-Darling and Kolmogorov-Smirnov tests use the cumulative density frequency distributions to test for normality, the Anderson-Darling test is different from the Kolmogorov-Smirnov test because it weights the variance in the tails of the cumulative density frequency distribution more than the middle. This is because the variance in the tails can be measured in smaller increments than in the middle of the distribution. Consequently, the Anderson-Darling test is more sensitive to tails than the Kolmogorov-Smirnov test. In line with the Kolmogorov-Smirnov test, a test statistic is calculated and measured against a critical value. If the test statistic is larger than the critical value, the null hypothesis that the given distribution is normally distributed can be rejected at the specified level of significance.</p>
<p>Here, we are using the Anderson-Darling test to test a random normal probability distribution<a id="_idIndexMarker323"/> generated with a mean of 19 and a standard deviation of 1.7. We also test an exponentially<a id="_idIndexMarker324"/> transformed version of this data:</p>
<pre class="source-code">
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
mu, sigma = 19, 1.7
normally_distributed = np.random.normal(mu, sigma, 1000)
not_normally_distributed = np.exp(normally_distributed);</pre>
<p><em class="italic">Figure 4</em><em class="italic">.4</em> shows plots of the data:</p>
<div><div><img alt="Figure 4.4 – Normal distribution versus heavy-tailed exponential distribution" height="465" src="img/B18945_04_004.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Normal distribution versus heavy-tailed exponential distribution</p>
<p>In the code and output shown next, in <em class="italic">Figure 4</em><em class="italic">.5</em>, we can see the distribution is normally distributed at all levels of significance. Recall that the level of significance is the p-value (that is, a level of significance = 15.0 means a p-value of 0.15 or smaller is significant):</p>
<pre class="source-code">
<code>from scipy import stats</code>
<code>import pandas as pd</code>
<code>import numpy as np</code>
<code>def anderson_test(data):</code>
<code>    </code><code>data = np.array(data)</code>
<code>    test_statistic, critical_values, significance_levels = stats.anderson(normally_distributed, dist='norm')</code>
<code>    df_anderson = pd.DataFrame({'Test Statistic':np.repeat(test_statistic, len(critical_values)), 'Critical Value':critical_values, 'Significance Level': significance_levels})</code>
<code>    df_anderson.loc[df_anderson['Test Statistic'] &gt;= </code><code>df_anderson['Critical Value'], 'Normally Distributed'] = 'No'</code>
<code>    df_anderson.loc[df_anderson['Test Statistic'] &lt;df_anderson['Critical Value'], 'Normally Distributed'] = 'Yes'</code>
<code>    return df_anderson;</code>
<code>mu, sigma = 19, 1.7</code>
<code>normally_distributed = np.random.normal(mu, sigma, 1000)</code>
<code>anderson_test(normally_distributed)</code></pre>
<p>Here, the data generated through the <code>numpy</code> <code>random.normal</code> function is tested with the Anderson-Darling method and confirmed to be normally distributed:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Test statistic</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Critical value</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Significance level</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Normally distributed</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0.191482344</p>
</td>
<td class="No-Table-Style">
<p>0.574</p>
</td>
<td class="No-Table-Style">
<p>15</p>
</td>
<td class="No-Table-Style">
<p>Yes</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0.191482344</p>
</td>
<td class="No-Table-Style">
<p>0.653</p>
</td>
<td class="No-Table-Style">
<p>10</p>
</td>
<td class="No-Table-Style">
<p>Yes</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0.191482344</p>
</td>
<td class="No-Table-Style">
<p>0.784</p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p>Yes</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0.191482344</p>
</td>
<td class="No-Table-Style">
<p>0.914</p>
</td>
<td class="No-Table-Style">
<p>2.5</p>
</td>
<td class="No-Table-Style">
<p>Yes</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0.191482344</p>
</td>
<td class="No-Table-Style">
<p>1.088</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>Yes</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 4.5 – Anderson-Darling results for normally distributed data</p>
<p>Here, we <a id="_idIndexMarker325"/>test an exponential<a id="_idIndexMarker326"/> transformation of the normally distributed data to check for normality. The data is exponentially distributed and should reject at all levels of significance. However, we see in <em class="italic">Figure 4</em><em class="italic">.6</em> that it has failed to reject at the 0.01 level of significance (99% confidence). Therefore, depending on the use case, it may be prudent to check all levels of significance, use a different test, or make a decision based on multiple tests:</p>
<pre class="source-code">
<code>not_normally_distributed = np.exp(normally_distributed)</code>
<code>anderson_test(not_normally_distributed)</code></pre>
<p>Our Anderson-Darling test of non-normally distributed data outputs are as follows in <em class="italic">Figure 4</em><em class="italic">.6</em>:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Test statistic</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Critical value</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Significance level</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Normally distributed</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0.96277351</p>
</td>
<td class="No-Table-Style">
<p>0.574</p>
</td>
<td class="No-Table-Style">
<p>15</p>
</td>
<td class="No-Table-Style">
<p>No</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0.96277351</p>
</td>
<td class="No-Table-Style">
<p>0.653</p>
</td>
<td class="No-Table-Style">
<p>10</p>
</td>
<td class="No-Table-Style">
<p>No</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0.96277351</p>
</td>
<td class="No-Table-Style">
<p>0.784</p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p>No</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0.96277351</p>
</td>
<td class="No-Table-Style">
<p>0.914</p>
</td>
<td class="No-Table-Style">
<p>2.5</p>
</td>
<td class="No-Table-Style">
<p>No</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0.96277351</p>
</td>
<td class="No-Table-Style">
<p>1.088</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>Yes</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 4.6 – Anderson-Darling results for non-normally distributed data</p>
<h4>Shapiro-Wilk</h4>
<p>The <code>scipy.stats shapiro</code> module, so input data does not need to be altered prior to testing. The level of significance for this test in <code>scipy</code> is 0.05.</p>
<p class="callout-heading">Shapiro-Wilk compared to Kolmogorov-Smirnov and Anderson-Darling</p>
<p class="callout">Shapiro-Wilk is ideal, compared to Kolmogorov-Smirnov and Anderson-Darling, for testing small sample sizes of roughly less than 50. However, one drawback is that since Shapiro-Wilk uses repeated sampling and testing for the calculated test statistic by applying Monte Carlo simulation, the law of large numbers poses a risk that as the sample size increases, there is an inherent increase in the risk of encountering a <em class="italic">type II</em> error (a loss of power) and failing to reject the null hypothesis, where the null hypothesis states the given distribution is normally distributed.</p>
<p>Using the same distributions as in the Anderson-Darling test, we test with Shapiro-Wilk. We can see with the random normal distribution with a mean of 19 and a standard deviation of 1.7, the Shapiro-Wilk test has confirmed with a p-value of 0.99 that the null hypothesis that the input distribution is normally distributed should not be rejected:</p>
<pre class="source-code">
mu, sigma = 19, 1.7
normally_distributed = np.random.normal(mu, sigma, 1000)
stats.shapiro(normally_distributed)</pre>
<p>This is the output:</p>
<p><code>ShapiroResult(statistic=0.9993802905082703, pvalue=0.9900037050247192)</code></p>
<p>When testing using the exponentially transformed version of the normally distributed data, we find a significant p-value (p = 0.0), indicating we have enough evidence to reject the null hypothesis <a id="_idIndexMarker330"/>and conclude the distribution is not normally distributed:</p>
<pre class="source-code">
not_normally_distributed = np.exp(normally_distributed)
stats.shapiro(not_normally_distributed)</pre>
<p>This is the output:</p>
<p><code>ShapiroResult(statistic=0.37320804595947266, pvalue=0.0)</code></p>
<h3>Independent samples</h3>
<p>In <a id="_idIndexMarker331"/>parametric hypothesis testing, the independence of samples is another important assumption. Two effects can occur from non-independent sampling. One effect occurs when subgroup sampling is performed. The issue here is that responses in one subgroup of the population may be different than responses from another subgroup of the same population or even more similar to those of a different population. However, when sampling representative of the overall population is taken, this type of subgroup difference may not be very representative of the population.</p>
<p>Another effect of non-independent sampling is when samples are taken close enough together in time that the occurrence of one precludes or excludes the occurrence of another. This is called serial (or auto-) correlation.</p>
<p>Parametric tests are not typically robust to violations of this requirement as it has direct, categorical implications on the interpretability of test outcomes. With respect to subgroup sampling, this can be prevented through a well-structured sampling approach such as those outlined in <a href="B18945_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a><em class="italic">, Sampling and Generalization</em>. However, as regards the serial effect, we can test for autoregressive correlation (also called serial correlation) in the data.</p>
<h3>Durbin-Watson</h3>
<p>One of the <a id="_idIndexMarker332"/>most common<a id="_idIndexMarker333"/> tests performed to assess a lack of independence in sampling is the first-order (also referred to as lag-one) autoregressive test called the <strong class="bold">Durbin-Watson</strong> test. <strong class="bold">Autoregressive</strong> means<a id="_idIndexMarker334"/> previous data points are used to predict the current data point. First-order means the last sampled data point (lag one) is the point most significantly correlated to the most recently sampled data point (lag zero) in a sequence of sampled data. In first-order autocorrelation, the correlation for each data point is strongest with the previous data point. The Durbin-Watson test does not test whether any value is correlated to the value before it, but instead if, overall, there is a strong enough relationship between each value and the value before it to conclude there is significant autocorrelation. In that sense, there is some robustness to non-independent sampling such that an accident or two may not completely invalidate a hypothesis test, but a consistent recurrence of this type of violation will.</p>
<p>A Durbin-Watson <a id="_idIndexMarker335"/>value of 2 indicates no significant autocorrelation, a value between 0 and 2 represents positive (direct) autocorrelation, and a value between 2 and 4 represents negative (inverse) autocorrelation.</p>
<p>In the following example, we have two distributions, each with 1,000 samples. The distribution on the left is a sinusoidal distribution that exhibits strong autoregressive correlation, and the distribution on the right is a set of randomly generated data displaying as white-noise variance (random points centered around a mean of 0). Using the <code>durbin_watson()</code> function from the <code>statsmodels.stats</code> module, we are able to confirm direct, positive lag-one autocorrelation in the sinusoidal pattern (a very small Durbin-Watson value) and a Durbin-Watson statistic of 2.1 with the random noise, indicating no autocorrelation. Therefore, in <em class="italic">Figure 4</em><em class="italic">.7</em>, the plot on the left is not composed of <a id="_idIndexMarker336"/>independent samples whereas the plot on the right is:</p>
<pre class="source-code">
<code>from statsmodels.stats.stattools import durbin_watson</code>
<code>import matplotlib.pyplot as plt</code>
<code>import numpy as np</code>
<code>mu, sigma = 0, 1.1</code>
<code>independent_samples = np.random.normal(mu, sigma, 1000)</code>
<code>correlated_samples = np.linspace(-np.pi, np.pi, num=1000)</code>
<code>fig, ax = plt.subplots(1,2, figsize=(10,5))</code>
<code>ax[0].plot(correlated_samples, np.sin(correlated_samples))</code>
<code>ax[0].set_title('Durbin Watson = {}'.format(</code>
    <code>durbin_watson(correlated_samples)))</code>
<code>ax[1].plot(independent_samples)</code>
<code>ax[1].set_title('Durbin Watson = {}'.format(</code>
    <code>durbin_watson(independent_samples)))</code></pre>
<div><div><img alt="Figure 4.7 – Serially correlated and normally distributed sequence data" height="448" src="img/B18945_04_007.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – Serially correlated and normally distributed sequence data</p>
<h2 id="_idParaDest-70"><a id="_idTextAnchor074"/>Equal population variance</h2>
<p>Similar to<a id="_idIndexMarker337"/> the assumption of normally distributed data, the assumption of equal population variance—also referred to as homogeneity of variance—is about the shape of the physical properties of the distributions being compared. Assuming <a id="_idIndexMarker338"/>equal population variance helps increase the power of a parametric test. This is because there is confidence when means are identified as being different; we also know the degree of potential distribution overlap. When a test has an intuition about the location of the full distributions—in effect, true knowledge of the effect size—power increases. Conversely, as variances diverge, power decreases.</p>
<h3>Robustness to equal population variance</h3>
<p>While equal population variance is useful in parametric testing, modifications to these tests exist <a id="_idIndexMarker339"/>that help results be robust to deviance from equal variance. One prominent modified version of these tests uses the <strong class="bold">Welch-Satterthwaite</strong> adjustment<a id="_idIndexMarker340"/> to the degrees of freedom used. Because applying the same degree of freedom to each group when each group has a different variance would result in a misrepresentation of the data, the Welch-Satterthwaite adjustment accounts for variance differences when allocating degrees of freedom to parametric tests that assume equal variance. Two common tests that use the Welch-Satterthwaite adjustment are Welch’s t-test and Welch’s ANOVA test. When used on small samples, these tests may not be reliable, but when used on sample sizes large enough to have sufficient power, the results should be approximately the same as their non-Welch counterparts.</p>
<h3>Testing for equal variance</h3>
<p>When <a id="_idIndexMarker341"/>testing for equal variance among distributions, we have two prominent tests: <strong class="bold">Levene’s test for equality of variances</strong> and <strong class="bold">Fisher’s F-test</strong>.</p>
<h3>Levene’s test for equality of variances</h3>
<p>Levene’s test <a id="_idIndexMarker342"/>for equality of variances is useful when<a id="_idIndexMarker343"/> testing for homogeneity of variance of two or more groups. In the code snippet shown below <em class="italic">Figure 4</em><em class="italic">.8</em>, we test with three distributions, each having a sample size of 100, a mean of 0, and standard deviations of 0.9, 1.1, and 2. <em class="italic">Figure 4</em><em class="italic">.8</em> is a plot of the three distributions generated using the data output from the code above <em class="italic">Figure 4</em><em class="italic">.8</em>.</p>
<pre class="source-code">
<code>from scipy.stats import levene</code>
<code>np.random.seed(26)</code>
<code>mu1, sigma1, mu2, sigma2, mu3, sigma3 = 0,0.9,0,1.1,0,2</code>
<code>distro1, distro2, distro3 = pd.DataFrame(), pd.DataFrame(),</code>
    <code>pd.DataFrame()</code>
<code>distro1['x'] = np.random.normal(mu1, sigma1, 100)</code>
<code>distro2['x'] = np.random.normal(mu2, sigma2, 100)</code>
<code>distro3['x'] = np.random.normal(mu3, sigma3, 100)</code></pre>
<p>We can see how their different standard deviations impact their range.</p>
<div><div><img alt="Figure 4.8 – Distributions for multiple equality of variance testing" height="319" src="img/B18945_04_008.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – Distributions for multiple equality of variance testing</p>
<p>We can<a id="_idIndexMarker344"/> see the test is sensitive to violations of <a id="_idIndexMarker345"/>non-homogenous variance because the result of this is a statistically significant p-value indicating non-homogenous variance:</p>
<pre class="source-code">
<code>f_statistic, p_value = levene(distro1['x'], distro2['x'], distro3['x'])</code>
<code>if p_value &lt;= 0.05:</code>
<code>    print('The distributions do not have homogenous variance. P-value = %.4f, F-statistic = %.4f'%(p_value, f_statistic))</code>
<code>else:</code>
<code>    print('The distributions have homogenous variance.P-value = %.4f, F-statistic = %.4f'%(p_value, f_statistic))</code></pre>
<p>This is the output:</p>
<p><code>The distributions do not have homogenous variance. P-value = </code><code>0.0000</code></p>
<h3>Fisher’s F-test</h3>
<p>Fisher’s F-test is <a id="_idIndexMarker346"/>useful when testing for homogeneity of variance <a id="_idIndexMarker347"/>for two groups at a time. This test compares a test statistic to a critical value to determine whether the variances are statistically the same or not. The calculated F-statistic is the variance of group one divided by the variance of group two. Group one is always the group with the larger variance. Using the preceding data, let us compare distribution 1 with distribution 3. Distribution 3 has a larger variance of 2, so that group’s variance will be the numerator when calculating the F-statistic. Since each group has a sample size of 100, their degrees of freedom for the table lookup will each be 99. However, since we will use the <code>scipy</code> Python package to compute the test, here, the table lookup is not needed as <code>scipy</code> does this for us with the <code>f.cdf()</code> function. In line with the results of the Levene test, the F-test indicates distribution 1 and distribution 3 do not have homogenous variance:</p>
<pre class="source-code">
<code>from scipy.stats import f</code>
<code>def f_test(inputA, inputB):</code>
<code>    group1 = np.array(inputA)</code>
<code>    group2 = np.array(inputB)</code>
<code>    if np.var(group1) &gt; np.var(group2):</code>
<code>        f_statistic = np.var(group1) / np.var(group2)</code>
<code>        numeratorDegreesOfFreedom = group1.shape[0] - 1</code>
<code>        denominatorDegreesOfFreedom = group2.shape[0] - 1</code>
<code>    else:</code>
<code>        f_statistic = np.var(group2)/np.var(group1)</code>
<code>        numeratorDegreesOfFreedom = group2.shape[0] - 1</code>
<code>        denominatorDegreesOfFreedom = group1.shape[0] - 1</code>
<code>    p_value = 1 - f.cdf(f_statistic,numeratorDegreesOfFreedom, denominatorDegreesOfFreedom)</code>
<code>    if p_value &lt;= 0.05:</code>
<code>        print('The distributions do not have homogenous variance. P-value = %.4f, </code><code>F-statistic = %.4f'%(p_value, f_statistic))</code>
<code>    else:</code>
<code>        print('The distributions have homogenous variance. P-value = %.4f, F-statistic = %.4f'%(p_value, f_statistic))</code>
<code>f_test(distro3['x'], distro1['x'])</code></pre>
<p>This F-test <a id="_idIndexMarker348"/>output<a id="_idIndexMarker349"/> is as follows:</p>
<p><code>The distributions do not have homogenous variance. P-value = 0.0000, F-statistic = </code><code>102622.9745</code></p>
<h1 id="_idParaDest-71"><a id="_idTextAnchor075"/>T-test – a parametric hypothesis test</h1>
<p>In the last <a id="_idIndexMarker350"/>chapter, the z-test for means was applied when population standard <a id="_idIndexMarker351"/>deviations were known. However, in the real world, it is not easy (or virtually impossible) to obtain the population standard deviation. In this section, we will discuss another hypothesis test called the <strong class="bold">t-test</strong>, which is used when the population standard deviations are unknown. The mean and the standard deviation of a population are estimated by taking the mean and the standard deviation of sample data representative of this population.</p>
<p>Broadly speaking, the method for the t-test for means is very similar to the one for the z-test for means, but the calculations for the test statistic and p-value are not the same as for the z-test. The test statistic is computed by the following formula:</p>
<p>t =   _ x  − μ _ s/ √ _ n  </p>
<p>Here,  _ x  , μ, s, and n are the sample mean, population mean, sample standard deviation, and sample size, respectively, which <a id="_idIndexMarker352"/>has a t<strong class="bold">-distribution</strong> when the sample data, x, is normally distributed. The following code illustrates the standard normal distribution (blue curve) with 1,000 samples and t-distribution (green and red curves) with <a id="_idIndexMarker353"/>two <a id="_idIndexMarker354"/>sample sizes—3 and 16 samples:</p>
<pre class="source-code">
# libraries
import numpy as np
import scipy.stats as stats
# creating normal distribution
x =np.linspace(-5, 5, 1000) #create 1000 point from -5 to 5
y = stats.norm.pdf(x) # create probability density for each point x  - normal distribution
# creating Student t distributions for 2 sample sizes n =3 and n =15
degree_freedom1 = 2
t_dis1 = stats.t.pdf(x, degree_freedom1)
degree_freedom2 = 15
t_dis2 = stats.t.pdf(x, degree_freedom2)</pre>
<p>The following visuallization is for these 3 distributions considered.</p>
<div><div><img alt="Figure 4.9 – Normal and t-distributions" height="776" src="img/B18945_04_009.jpg" width="1264"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – Normal and t-distributions</p>
<p>Observe that the<a id="_idIndexMarker355"/> three curves have similar symmetry and shapes but there is more variability (or, in other words, heavier tails) for a sample with a smaller size. Historically, researchers <a id="_idIndexMarker356"/>considered a sample standard deviation to represent the population when the sample size was greater than 30, that is, the red curve approximates the blue curve when n &gt; 30. It was also common to use the z-test if the sample distribution overlapped the standard normal distribution. This practice has some reasoning behind it because, previously, critical value tables were stored up to a sample size of 50, but nowadays, with the power of computation and the internet, the <em class="italic">t</em> values can be obtained easily with any sample size.</p>
<h2 id="_idParaDest-72"><a id="_idTextAnchor076"/>T-test for means</h2>
<p>One-sample <a id="_idIndexMarker357"/>and two-sample t-tests related to a population mean or means of two populations where the population variances or population standard deviations are unknown will be considered in this part.</p>
<p>To perform a t-test, the <a id="_idIndexMarker358"/>following assumptions need to be satisfied:</p>
<p><strong class="bold">Normality</strong>: The sample is normally distributed</p>
<p><strong class="bold">Independence</strong>: Observations are randomly selected from a population to form a sample or, in other<a id="_idIndexMarker359"/> words, they are independent</p>
<p>Let us consider the one-sample t-test in the following section.</p>
<h3>One-sample t-test</h3>
<p>Similar <a id="_idIndexMarker360"/>to the <a id="_idIndexMarker361"/>one-sample z-test, the null and alternative hypotheses need to be considered in order to perform the hypothesis test. Three null and alternative hypotheses corresponding to left-tailed, right-tailed, and two-tailed t-tests are presented as follows:</p>
<p>H 0 :  μ ≥ μ 0 H 0 :  μ ≤ μ 0  H 0 :  μ = μ 0</p>
<p>H a :  μ &lt; μ 0 H a :  μ &gt; μ 0  H a :  μ ≠ μ 0</p>
<p>Next, the level of significance, α, needs to be specified following the research purpose. There are two approaches: the p-value approach and the critical value approach. In the p-value approach, the rejection rule (reject H 0—the null hypothesis) is when the p-value is less than or equal to the specified level of significance chosen. In the critical value approach, the rejection rule is when the test statistic is less than or equal to the critical value − t α for the left-tailed t-test, the test statistic is greater than or equal to t α for a right-tailed t-test, and the test statistic is less than or equal to − t α/2 or greater than or equal to t α/2 for a two-tailed test. The last step is to interpret the statistical conclusion for the hypothesis test.</p>
<p>To find the p-value based on the value of the student t distribution, we can use the following syntax:</p>
<pre class="source-code">
scipy.stats.t.sf(abs(x), df)</pre>
<p>Here, <code>x</code> is the test statistic and <code>df</code> is the degree of freedom (<code>df</code> = n-1 where <em class="italic">n</em> is the sample size) in the formula.</p>
<p>For example, to find the p-value associated with a t-score of 1.9 with the degree of freedom 14 in a left-tailed test, this would be the Python implementation:</p>
<pre class="source-code">
import scipy.stats
round(scipy.stats.t.sf(abs(1.9), df=14),4)</pre>
<p>The output would be 0.0391. If the level of significance α = 0.05, then we reject the null hypothesis because the p-value is less than α. For a right-tailed t-test, similar Python code as in the left-tailed t-test is implemented to find the p-value. For a two-tailed test, we need<a id="_idIndexMarker362"/> to multiply the value by 2, as follows:</p>
<pre class="source-code">
scipy.stats.t.sf(abs(t), df)*2</pre>
<p>Here, <code>t</code> is the test statistic and <code>df</code> is the degree of freedom (<code>df</code> = n-1 where <em class="italic">n</em> is the sample size).</p>
<p>To<a id="_idIndexMarker363"/> compute the critical value in Python, we use the following syntax:</p>
<pre class="source-code">
scipy.stats.t.ppf(q, df)</pre>
<p>Here, <code>q</code> is the level of significance and <code>df</code> is the degree of freedom to be used in the formula. Here is the implementation of the code in Python for left-tailed, right-tailed, and two-tailed tests:</p>
<pre class="source-code">
import scipy.stats as stats
alpha = 0.05 # level of significance
df= 15 # degree of freedom
#find t critical value for left-tailed test
print(f" The critical value is {stats.t.ppf(q= alpha, df =df)}")
#find t critical value for right-tailed test
print(f" The critical value is {stats.t.ppf(q= 1-alpha, df =df)}")
##find t critical value for two-tailed test
print(f" The critical values are {-stats.t.ppf(q= 1-alpha/2, df =df)} and {stats.t.ppf(q= 1-alpha/2, df =df)}")</pre>
<p>This is the output of the preceding code:</p>
<ul>
<li>The critical value is -1.7530503556925552</li>
<li>The critical value is 1.7530503556925547</li>
<li>The critical values are -2.131449545559323 and 2.131449545559323</li>
</ul>
<p>At the level <a id="_idIndexMarker364"/>of significance α = 0.05, for the left-tailed test, the critical value is about -1.753. Since this is a left-tailed test, if the test statistic is less than or equal to this critical value, we reject the null hypothesis. Similarly, for the right-tailed test, if the test statistic is greater than or equal to 1.753, we reject the null hypothesis. For the two-tailed test, we reject the null hypothesis if the test statistic is greater than or equal to 2.1314 or less than or equal to -2.1314. Finally, we interpret the statistical conclusion for the hypothesis testing.</p>
<p>Let us <a id="_idIndexMarker365"/>randomly choose 30 students from a high school and score their IQ. We would like to test the claim that the mean IQ score of the distribution of the students from this high school is higher than 100. This means that we will perform a right-tailed t-test. The IQ scores of 30 students are given here:</p>
<pre class="source-code">
IQscores = [113, 107, 106, 115, 103, 103, 107, 102, 108, 107, 104, 104, 99, 102, 102, 105, 109, 97, 109, 103, 103, 100, 97, 107,116, 117, 105, 107, 104, 107]</pre>
<p>Before conducting the hypothesis testing, we will check normality and independence assumptions. The assumption of independence is satisfied if the sample is randomly selected from the population of high school students at this school. For normality, we will check the histogram and QQ plots of IQ score data:</p>
<div><div><img alt="Figure 4.10 – Visually assessing normality of student IQ scores" height="1030" src="img/B18945_04_010.jpg" width="1263"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – Visually assessing normality of student IQ scores</p>
<p>There is little <a id="_idIndexMarker366"/>to no evidence from the histogram and QQ plot that the<a id="_idIndexMarker367"/> population IQ score distribution of the students at the high school is not normal. Since the distribution is assumed to be normal, we will proceed with the t-test.</p>
<p>First, we define the null hypothesis and the alternative hypothesis:</p>
<p>H 0 :  μ ≤ 100</p>
<p>H a :  μ &gt; 100</p>
<p>We choose the level of significance α=0.05. You can calculate the test statistic by using its mathematical formula by hand or by implementing Python. For the critical value and p-value, the implemented code was shown in the previous part. Here, we will use another function from the <code>scipy</code> library to find the test statistic and p-value:</p>
<pre class="source-code">
scipy.stats.ttest_1samp(data, popmean, alternative='greater')</pre>
<p>Here, the following applies:</p>
<ul>
<li><code>data</code>: The observations from the sample</li>
<li><code>popmean</code>: The expected value in the null hypothesis</li>
<li><code>alternative</code>: <code>'two-sided'</code> for a two-tailed t-test, <code>'less'</code> for a left-tailed t-test, and <code>'greater'</code> for a right-tailed t-test</li>
</ul>
<p>The <a id="_idIndexMarker368"/>Python code<a id="_idIndexMarker369"/> is implemented as follows:</p>
<pre class="source-code">
import scipy.stats as stats
#perform one sample t-test
t_statistic, p_value = stats.ttest_1samp(IQscores, popmean =100, axis=0,  alternative='greater')
print(f"The test statistic is {t_statistic} and the corresponding p-value is {p_value}.")</pre>
<p>This is the output:</p>
<p><code>The test statistic is 6.159178830896832 and the corresponding p-value </code><code>is 5.15076734562176e-07.</code></p>
<p>Because the p-value &lt; 0.05 where 0.05 is the level of significance, we have enough evidence to reject the null hypothesis and conclude that the true mean IQ scores of the students from this school is higher than 100.</p>
<p>In addition, with 95% confidence, the mean IQ score lies between 104.08 and 107.12. We can perform the calculation for the confidence interval in Python as follows:</p>
<pre class="source-code">
IQmean = np.array(IQscores).mean() # sample mean
IQsd = np.array(IQscores).std() # sample standard deviation
sample_size = len(np.array(IQscores)) # sample size
df = sample_size-1 # degree of freedom
alpha = 0.05 # level of significance
t_crit = stats.t.ppf(q=1-alpha, df =df) # critical
confidence_interval = (IQmean-IQsd*t_crit/np.sqrt(sample_size), IQmean+IQsd*t_crit/np.sqrt(sample_size))</pre>
<p>The steps<a id="_idIndexMarker370"/> to <a id="_idIndexMarker371"/>perform a hypothesis test in Python using the left-tailed t-test are similar to those of the right-tailed and two-tailed t-tests.</p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor077"/>Two-sample t-test – pooled t-test</h2>
<p>Similar to <a id="_idIndexMarker372"/>what <a id="_idIndexMarker373"/>was covered in <a href="B18945_03.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, Hypothesis Testing,</em> (two-sample z-test for means), the <a id="_idIndexMarker374"/>two-sample t-test for<a id="_idIndexMarker375"/> means has three forms for the null and alternative hypotheses. Some assumptions need to be <a id="_idIndexMarker376"/>satisfied before conducting the test, as follows:</p>
<ul>
<li><strong class="bold">Normality</strong>: Two samples are drawn from their normally distributed populations</li>
<li><strong class="bold">Independence</strong>: The observations of one sample are independent of one another</li>
<li><strong class="bold">Homogeneity of variance</strong>: Both populations are assumed to have similar standard deviations</li>
</ul>
<p>For normality, we use visual histograms and also QQ plots of the two samples and compare them. Let us assume independence is satisfied. In order to check equal standard deviations between the two samples, we could use visualization by observing their histograms and also use an F-test to have additional evidence if the visualization is inconclusive. This is a hypothesis test to check whether two sample variances are equal.</p>
<p>Let us look at the IQ scores between two high schools, A and B. The following are the scores of 30 students from each school, randomly selected:</p>
<pre class="source-code">
IQscoresA=[113, 107, 106, 115, 103, 103, 107, 102,108, 107,
            104, 104, 99, 102, 102, 105, 109, 97, 109, 103,
            103, 100, 97, 107, 116, 117, 105, 107, 104, 107]
IQscoresB = [102, 108, 110, 101, 98, 98, 97, 102, 102, 103,
             100, 99, 97, 97, 94, 100, 104, 98, 92, 104,
            98, 95, 92, 111, 102, 112, 100, 103, 103, 100]</pre>
<p>The histograms and QQ plots shown in Figure 4.11 are generated by the IQ data above.</p>
<div><div><img alt="Figure 4.11 – Assessing normality of two schools’ IQ scores" height="920" src="img/B18945_04_011.jpg" width="1205"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – Assessing normality of two schools’ IQ scores</p>
<p>We can<a id="_idIndexMarker377"/> see that the normality assumption is satisfied. We also can <a id="_idIndexMarker378"/>assume by observing the histograms that the equal variance assumption is supported. Another F-test to check the equal variance assumption (if necessary) follows:</p>
<pre class="source-code">
# F-test
import numpy as np
import scipy.stats as stats
IQscoresA = np.array(IQscoresA)
IQscoresB = np.array(IQscoresB)
f = np.var(IQscoresA, ddof=1)/np.var(IQscoresB, ddof=1) # F statistic
dfA = IQscoresA.size-1 #degrees of freedom A
dfB = IQscoresB.size-1 #degrees of freedom B
p = 1-stats.f.cdf(f, dfA, dfB) #p-value</pre>
<p>The output of <a id="_idIndexMarker379"/>the preceding code tells us the F-test statistic is 0.9963 and the corresponding p-value is 0.50394 &gt; 0.05 (0.05 is the level of significance), then we fail to reject the null hypothesis. This means that there is enough evidence to say that the standard deviations of these two samples are equal.</p>
<p>We now define the null hypothesis and the alternative hypothesis:</p>
<p>H 0 : μ A = μ B,</p>
<p>H a : μ A ≠ μ B.</p>
<p>We choose <a id="_idIndexMarker380"/>the level of significance α=0.05. We use the <code>statsmodels.stats.weightstats.ttest_ind</code> function to conduct the t-test. The documentation can be found here: <a href="https://www.statsmodels.org/dev/generated/statsmodels.stats.weightstats.ttest_ind.xhtml">https://www.statsmodels.org/dev/generated/statsmodels.stats.weightstats.ttest_ind.xhtml</a>.</p>
<p>We can use this function to perform three forms of the alternate hypothesis with <code>alternative='two-sided'</code>, <code>'larger'</code>, or <code>'smaller'</code>. In the pooled-variance t-test, when the assumption for equal variances is satisfied, the test statistic is computed as follows:</p>
<p>t =  (‾ x 1 − ‾ x 2) − (μ 1 − μ 2)  _____________  s p √ _  1 _ n 1 +  1 _ n 2  </p>
<p>Here, _ x 1, _ x 2, μ 1, μ 2, n 1,and n 2 are sample means, population means, and sample sizes of two samples, 1 and 2 respectively, and the pooled standard deviation is given here:</p>
<p>s p = √ ________________   (n 1 − 1) s 1 2 + (n 2 − 1) s 2 2  ________________  n 1 + n 2 − 2  </p>
<p>The degree of freedom is shown here:</p>
<p>df = n 1 + n 2 − 2.</p>
<p>Let’s go back to the example:</p>
<pre class="source-code">
from statsmodels.stats.weightstats import ttest_ind as ttest
t_statistic, p_value, degree_freedom = ttest(IQscoresA,
    IQscoresB, alternative='two-sided', usevar='pooled')</pre>
<p>The output <a id="_idIndexMarker381"/>returns the test statistic 3.78 and the p-value 0.00037, and the<a id="_idIndexMarker382"/> degrees of freedom used in the t-test are 58 (each sample size has 30 observations, then the degrees of freedom are calculated as 30 + 30 - 2 = 58).</p>
<p>Because the p-value &lt;0.05, we reject the null hypothesis. There is sufficient evidence to suggest that there is a difference in mean IQ score between students at high schools A and B. To perform the confidence level, you can adapt the Python code in the last part of the one-sample t-test.</p>
<p>Recall that in some situations, if the histograms and QQ plots show some evidence of skewness, we can consider testing the medians instead of the means for hypothesis testing. As shown in <a href="B18945_02.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a><em class="italic">, Distributions of Data</em>, we can perform a data transformation (for example, log transformation) to obtain the normality assumption. After the transformation, the median of <code>log(data)</code> is equal to the mean of <code>log(data)</code>. This means that the test is performed on means of the transformed data.</p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor078"/>Two-sample t-test – Welch’s t-test</h2>
<p>This is a <a id="_idIndexMarker383"/>practical two-sample t-test when the data is normally distributed <a id="_idIndexMarker384"/>but the population standard deviations are <a id="_idIndexMarker385"/>unknown and unequal. We have the same assumption for normality as with a pooled t-test but we can relax the assumption for equal variance when performing Welch’s t-test. Let us consider the following example where we have two sample datasets:</p>
<pre class="source-code">
sample1 = np.array([2,3,4,2,3,4,2,3,5,8,7,10])
sample2 = np.array([30,26,32,34,28,29,31,35,36,33,32,27])</pre>
<p>We assume the independence is satisfied, but we will check the normality and equal standard deviation assumptions for these two samples, as follows:</p>
<div><div><img alt="Figure 4.12 – Checking equal variance for Welch’s t-test" height="1012" src="img/B18945_04_012.jpg" width="1212"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – Checking equal variance for Welch’s t-test</p>
<p>There is<a id="_idIndexMarker386"/> strong visual evidence against equal standard deviations <a id="_idIndexMarker387"/>by looking <a id="_idIndexMarker388"/>at the <em class="italic">x</em> axis scale of the histograms in <em class="italic">Figure 4</em><em class="italic">.12</em>. Let us assume the normality assumption is satisfied. In this case, a two-sample pooled t-test is not a good idea, but Welch’s t-test would suffice. The null and alternative hypotheses are given here:</p>
<p>H 0 :  μ 1 = μ 2</p>
<p>H a :  μ 1 ≠ μ 2</p>
<p>We specify the level of significance 0.05. To calculate the test statistic and p-value, we implement the code as follows:</p>
<pre class="source-code">
import scipy.stats as stats
t_statistic, p_value = stats.ttest_ind(sample1, sample2,
    equal_var = False)</pre>
<p>The test <a id="_idIndexMarker389"/>statistic is -22.47 and the p-value is &lt;0.05 (the level of significance). We reject <a id="_idIndexMarker390"/>the null hypothesis. There is <a id="_idIndexMarker391"/>strong evidence to suggest the mean of sample data 1 is different from the mean of sample data 2.</p>
<h2 id="_idParaDest-75"><a id="_idTextAnchor079"/>Paired t-test</h2>
<p>The paired t-test<a id="_idIndexMarker392"/> is also known as a matched pairs or dependent t-test and is <a id="_idIndexMarker393"/>used in studies when each element in a sample is tested twice (pre-test and post-test or repeated measures) and when the researcher thinks that there are some similarities, such as family. The assumptions are set out here:</p>
<ul>
<li>Differences are normally distributed</li>
<li>Differences are independent between observations but dependent from one test to another test</li>
</ul>
<p>The paired t-test is used in many studies, especially in medical reasoning tests related to pre- and post-treatments. Let’s go back to IQ test scores—a researcher recruits a number of students to see whether there is a score difference before and after a training section, as represented in the following table:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table003-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Students</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Pre-training score</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Post-training score</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Differences</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>A</p>
</td>
<td class="No-Table-Style">
<p>95</p>
</td>
<td class="No-Table-Style">
<p>95</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>B</p>
</td>
<td class="No-Table-Style">
<p>98</p>
</td>
<td class="No-Table-Style">
<p>110</p>
</td>
<td class="No-Table-Style">
<p>12</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>C</p>
</td>
<td class="No-Table-Style">
<p>90</p>
</td>
<td class="No-Table-Style">
<p>97</p>
</td>
<td class="No-Table-Style">
<p>7</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>D</p>
</td>
<td class="No-Table-Style">
<p>115</p>
</td>
<td class="No-Table-Style">
<p>112</p>
</td>
<td class="No-Table-Style">
<p>-3</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>E</p>
</td>
<td class="No-Table-Style">
<p>112</p>
</td>
<td class="No-Table-Style">
<p>117</p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – Pre-training and post-training scores</p>
<p>In this case, we should not use an independent two-sample t-test. The mean of the differences should be tested here. We can check the assumption about normal distribution by using<a id="_idIndexMarker394"/> histogram<a id="_idIndexMarker395"/> and QQ plots, as follows:</p>
<div><div><img alt="Figure 4.14 – Checking normality for the paired t-test" height="960" src="img/B18945_04_014.jpg" width="1201"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – Checking normality for the paired t-test</p>
<p>Evidence for the data being normally distributed is more obvious by looking at the QQ plot than the histogram.</p>
<p>The differences are assumed to be independent. The null and alternative hypotheses are given here:</p>
<p>H 0 :  μ pos − μ pre = 0</p>
<p>H a :  μ pos − μ pre &gt; 0</p>
<p>d i denotes the difference between the pre-training score and the post-training score of each student. The null and alternative hypotheses can be rewritten as follows:</p>
<p>H 0 :  μ d = 0</p>
<p>H a :  μ d &gt; 0</p>
<p>Then, the test statistic is computed like so:</p>
<p>t =   _ d  − μ d _  s d _ √ _ n  </p>
<p>Here,  _ d  is the <a id="_idIndexMarker396"/>sample mean of the differences and s d is the sample standard<a id="_idIndexMarker397"/> deviation of differences. In other words, a paired t-test is reduced to a one-sample t-test. However, we can use the following function in <code>scipy</code> directly:</p>
<p><code>stats.ttest_rel(data_pos, data_pre, alternative = {'two-sided', '</code><code>less', 'greater'})</code></p>
<p>The alternative hypothesis corresponds to a left-tailed, right-tailed, or two-tailed test. Here is the Python implementation for the IQ test score study example:</p>
<pre class="source-code">
from scipy import stats
IQ_pre = [95, 98, 90, 115, 112]
IQ_pos = [95, 110, 97, 112, 117]
t_statistic, p_value = stats.ttest_rel(IQ_pos, IQ_pre, alternative = 'greater')</pre>
<p>The test statistic is 1.594 and the p-value is 0.093. Therefore, given the p-value is &lt;0.05 and the level of significance α = 0.05, we reject the null hypothesis. There is sufficient evidence to suggest that training has a significant effect on IQ scores.</p>
<h1 id="_idParaDest-76"><a id="_idTextAnchor080"/>Tests with more than two groups and ANOVA</h1>
<p>In the previous chapter and previous sections, we covered tests between two groups. In this section, we will cover two methods for testing differences between groups, as follows:</p>
<ul>
<li>Pairwise tests with the <strong class="bold">Bonferroni correction</strong></li>
<li>ANOVA</li>
</ul>
<p>When testing for differences between more than two groups, we will have to use multiple tests, which will affect our <em class="italic">type I</em> error rate. There are several methods to control the error rate. We will see how to utilize the Bonferroni correction to control the <em class="italic">Type I</em> error rate. We will also discuss ANOVA in this section, which is used to test for a difference in means of multiple groups.</p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor081"/>Multiple tests for significance</h2>
<p>In the previous sections, we <a id="_idIndexMarker398"/>looked at making a comparison between two groups. In this section, we will consider how to perform tests when there are more than two groups present. Let’s again consider the factory example where we have several models (model A, model B, and model C) of machines on a factory floor, and these machines are used to perform the same operation in the factory. A plausible question of interest is: <em class="italic">Does one machine model have a higher mean output than the other two models?</em> To make this determination, we would need to do three tests comparing the difference in means of each model to the other models, testing that the difference in means is different than zero. These are the null hypotheses we would need to test:</p>
<p>μ output,A − μ output,B = 0</p>
<p>μ output,B − μ output,C = 0</p>
<p>μ output,A − μ output,C = 0</p>
<p>When performing multiple tests, we will need to apply p-value corrections for our expected error rate. Recall that in <a href="B18945_03.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, Hypothesis Testing</em>, we defined the expected error rate for a hypothesis test as α. This is the rate at which we expect a <em class="italic">single</em> hypothesis test to result in a <em class="italic">Type I</em> error. In our example with factory machines, we are making three hypothesis tests, which means <em class="italic">we are three times more likely to see a Type I error</em>. While our example specifically considers multiple tests for differences in means, this applies to any type of hypothesis test. In these situations with multiple tests, we will generally define <a id="_idIndexMarker399"/>a <strong class="bold">familywise error rate</strong> (<strong class="bold">FWER</strong>) and apply p-value corrections to control for the FWER. The FWER is the probability of making a <em class="italic">Type I</em> error from a group of hypothesis tests. The error rate from tests within the group is <a id="_idIndexMarker400"/>the <strong class="bold">individual error rate</strong> (<strong class="bold">IER</strong>). We will define the IER and FWER as follows:</p>
<ul>
<li><strong class="bold">IER</strong>: The expected <em class="italic">Type I</em> error rate for an individual hypothesis test</li>
<li><strong class="bold">FWER</strong>: The <a id="_idIndexMarker401"/>expected <em class="italic">Type I</em> error rate for a group of hypothesis tests</li>
</ul>
<p>We will discuss one method for p-value correction in this section to provide intuition for the rationale.</p>
<h3>The Bonferroni correction</h3>
<p>One method<a id="_idIndexMarker402"/> for adjusting the p-value to control multiple hypothesis tests is the Bonferroni correction. The Bonferroni correction controls the FWER by uniformly reducing the significance level of each individual test in the family of tests. Given that we have m tests in a family each with the p-value p i, then the p-value correction is given as follows:</p>
<p>p i ≤  α _ m </p>
<p>Taking our previous example of three models of machines, we have a family of three tests, making m = 3. If we let FWER be 0.05, then, with the Bonferroni correction, the level of significance for the three individual tests is this:</p>
<p> 0.05 _ 3  = 0.0167</p>
<p>Thus, in this example, any of the individual tests would be required to have a p-value of 0.0167 to be considered significant.</p>
<p class="callout-heading">Effects on Type I and Type II errors</p>
<p class="callout">As discussed, the Bonferroni correction reduces the significance levels of individual tests to control the <em class="italic">Type I</em> error rate at<a id="_idIndexMarker403"/> the family level. We should also consider how this change impacts <a id="_idIndexMarker404"/>the <em class="italic">Type II</em> error rate. In general, reducing the significance level of individual tests will increase the chance of making a <em class="italic">Type II</em> error for that test (as is done in the Bonferroni correction). While we have only discussed the Bonferroni correction in this section, there are other methods for p-value correction that provide different trade-offs. Check the documentation of <code>multipletests</code> in <code>statsmodels</code> to see a list of p-value corrections implemented in <code>statsmodels</code>.</p>
<p>Let’s take a look at an example using the miles per gallon (MPG) data from the <em class="italic">Auto MPG</em> dataset from the <em class="italic">UCI Machine Learning Repository</em>, which can be found at this link: <a href="https://archive.ics.uci.edu/ml/datasets/Auto+MPG">https://archive.ics.uci.edu/ml/datasets/Auto+MPG</a> [1]. This dataset contains various attributes, including <code>origin</code>, <code>mpg</code>, <code>cylinders</code>, and <code>displacement</code>, for vehicles manufactured between 1970 and 1982. We will show an abbreviated form of the analysis here; the full analysis is included in the notebook in the code repository for this chapter.</p>
<p>For this<a id="_idIndexMarker405"/> example, we will use the <code>mpg</code> and <code>origin</code> variables, and test whether there is a difference in <code>mpg</code> from the different origins with a significance level of 0.01. The group means are shown in the following table (<code>origin</code> is an integer-encoded label in this dataset).</p>
<table class="No-Table-Style _idGenTablePara-1" id="table004">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>origin</code>)</p>
</td>
<td class="No-Table-Style">
<p><code>mpg</code>)</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>20.0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>27.9</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>30.5</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – Vehicle MPG means for each origin group</p>
<p>Running a t-test to compare each mean, we get the following p-values:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table005">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Null hypothesis</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Uncorrected p-value</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>μ 1 − μ 2 = 0</p>
</td>
<td class="No-Table-Style">
<p>7.946116336281346e-12</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>μ 1 − μ 3 = 0</p>
</td>
<td class="No-Table-Style">
<p>4.608511957238898e-19</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>μ 2 − μ 3 = 0</p>
</td>
<td class="No-Table-Style">
<p>0.0420926104552266</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.16 – Uncorrected p-values for t-tests on the difference between each group</p>
<p>Applying the Bonferroni correction to the p-values, we get the following p-values:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table006">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Null hypothesis</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Corrected </strong><strong class="bold">p-value (Bonferroni)</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>μ 1 − μ 2 = 0</p>
</td>
<td class="No-Table-Style">
<p>2.38383490e-11</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>μ 1 − μ 3 = 0</p>
</td>
<td class="No-Table-Style">
<p>1.38255359e-18</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>μ 2 − μ 3 = 0</p>
</td>
<td class="No-Table-Style">
<p>0.126277831</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.17 – Corrected p-values for t-tests on the difference between each group</p>
<p>With the preceding p-values, reject the null hypothesis for a difference in the means of groups 1 and 2 and a<a id="_idIndexMarker406"/> difference in the means of groups 1 and 3, but fail to reject the null hypothesis for a difference in the means of groups 2 and 3 at our significance level of 0.01.</p>
<p class="callout-heading">Other p-value correction methods</p>
<p class="callout">In this section, we only discussed one method for p-value correction—the Bonferroni correction—to provide intuition for the rationale of p-value corrections. However, there are other correction methods available that might be better suited for your problem. To see a list of p-value correction methods implemented within <code>statsmodels</code>, check the documentation of <code>statsmodels.stats.multitest.multipletests</code>.</p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor082"/>ANOVA</h2>
<p>In the previous<a id="_idIndexMarker407"/> section on multiple tests for significance, we saw how to perform multiple tests to determine whether means differed between groups. When dealing with means of groups, a useful first task is to conduct an analysis of variance. ANOVA is a statistical test for determining whether there is a difference between means of several groups. The null hypothesis is there is no difference in means, and the alternative hypothesis is the means are not all equal. Since ANOVA tests for a difference in means, it is commonly used before testing for a difference in means with pairwise hypothesis tests. If the ANOVA null hypothesis fails to be rejected, then there is no need to perform the pairwise tests. However, if the ANOVA null hypothesis is rejected, then pairwise tests can be performed to determine which specific means differ.</p>
<p class="callout-heading">ANOVA versus pairwise tests</p>
<p class="callout">While <a id="_idIndexMarker408"/>pairwise testing is a general procedure for testing for differences <a id="_idIndexMarker409"/>between groups, ANOVA can only be used to test for differences in means.</p>
<p>In this example, we will again consider the MPG of vehicles from the <em class="italic">Auto MPG</em> dataset. Since we have already run pairwise tests and found a significant difference in the mean mpg of vehicles based on origin, we expect that ANOVA will provide a positive test result (reject the null hypothesis). Performing the ANOVA calculation, we get the following output. The small p-value suggests that we should reject the null hypothesis:</p>
<pre class="source-code">
anova = anova_oneway(data.mpg, data.origin, use_var='equal')
print(anova)
# statistic = 98.54179491075868
# pvalue = 1.915486418412936e-35</pre>
<p>The ANOVA analysis <a id="_idIndexMarker410"/>shown here is abbreviated. For the full code, see the associated notebook in the code repository for this chapter.</p>
<p>In this section, we covered methods for performing hypothesis tests for more than two groups of data. The first method was pairwise testing with p-value correction, which is a general method that can be used for any type of hypothesis test. The other method we covered was ANOVA, which is a specific test for differences in the means of groups. This is not a general method such as pairwise testing but can be used as a first step before performing pairwise tests for differences in means. In the next section, we cover another type of parametric test that can be used to determine whether two sets of data are correlated.</p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor083"/>Pearson’s correlation coefficient</h2>
<p><strong class="bold">Pearson’s correlation coefficient</strong>, also called Pearson’s <em class="italic">r</em> (or Pearson’s rho (<em class="italic">ρ</em>) when applied to population data) or the <strong class="bold">Pearson product-moment sample coefficient of correlation (PPMCC)</strong>, is a <a id="_idIndexMarker411"/>bivariate test that measures the linear correlation between two<a id="_idIndexMarker412"/> variables. The coefficient produces a value ranging from -1 to 1 where -1 is a strong, inverse correlation and 1 is a strong, direct correlation. A zero-valued coefficient indicates no correlation between the two variables. Weak correlation is generally considered to be correlation between +/- 0.1 and +/- 0.3, moderate correlation is between +/- 0.3 and +/- 0.5, and strong correlation is between +/- 0.5 to +/- 1.0.</p>
<p>This test<a id="_idIndexMarker413"/> is considered parametric but does not require assumptions of normal distribution or homogeneity of variance. It is, however, required that data be independently sampled (both randomly selected and without serial correlation), have finite variance—such as with a distribution that has a very heavy tail—and be of a continuous data type. The test does not indicate an input variable and a response variable; it is simply a measure of the linear relation between two variables. The test uses standardized covariance to derive correlation. Recall that standardization requires dividing a value by the standard deviation.</p>
<p>The equation for the population Pearson’s coefficient, ρ, is shown here:</p>
<p>ρ =  σ xy _ σ x σ y </p>
<p>Here, σ xy is the population covariance, calculated as follows:</p>
<p>σ xy =  ∑ i=1 N  (x i − μ x)(y i − μ y)  ______________ N </p>
<p>The equation for the sample Pearson’s coefficient, <em class="italic">r</em>, is shown here:</p>
<p>r =  S xy _ S x S y </p>
<p>Here, S xy is the sample covariance, calculated as follows:</p>
<p>S xy =  ∑ i=1 n  (x i −  _ x )(y i −  _ y )  _____________  n − 1 </p>
<p>In Python, we can perform this test using the <code>scipy</code> <code>scipy.stats.pearsonr</code> function. In the following code snippet, we generate two normally distributed datasets of random numbers using <code>numpy</code>. We want to test the hypothesis that there is a correlation between the two groups since there is some significant overlap:</p>
<pre class="source-code">
<code>from scipy.stats import pearsonr</code>
<code>import matplotlib.pyplot as plt</code>
<code>import scipy.stats as stats</code>
<code>import seaborn as sns</code>
<code>import pandas as pd</code>
<code>import numpy as np</code>
<code>mu1, sigma1 = 0, 1.1</code>
<code>normally_distributed_1 = np.random.normal(mu1, sigma1, 1000)</code>
<code>mu2, sigma2 = 0, 0.7</code>
<code>normally_distributed_2 = np.random.normal(mu2, sigma2,</code>
    <code>1000)</code>
<code>df_norm = pd.DataFrame({'Distribution':['Distribution 1' for i in range(len(normally_distributed_1))] + ['Distribution 2' for i in range(len(normally_distributed_2))], 'X':np.concatenate([normally_distributed_1, normally_distributed_2])})</code></pre>
<p>In <em class="italic">Figure 4</em><em class="italic">.18</em>, we <a id="_idIndexMarker414"/>can observe the overlapping variance of the two correlated distributions:</p>
<div><div><img alt="Figure 4.18 – Correlated distributions" height="566" src="img/B18945_04_018.jpg" width="1080"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.18 – Correlated distributions</p>
<p>In the plot <a id="_idIndexMarker415"/>shown in <em class="italic">Figure 4</em><em class="italic">.18</em>, we can see the overlap of the populations. Now, we want to test the correlation using the <code>pearsonr()</code> function, as follows:</p>
<pre class="source-code">
<code>p, r = pearsonr(df_norm.loc[df_norm['Distribution'] == 'Distribution 1', 'X'], df_norm.loc[df_norm['Distribution'] == 'Distribution 2', 'X'])</code>
<code>print("p-value = %.4f"%p)</code>
<code>print("Correlation coefficient = %.4f"%r)</code></pre>
<p>The following output indicates that at a 0.05 level of significance, we have a 0.9327 level of correlation (p-value is 0.0027):</p>
<p><code>p-value = </code><code>0.0027</code></p>
<p><code>Correlation coefficient = </code><code>0.9327</code></p>
<p>To frame the correlation differently, we could say the level of variance explained (r 2, also <a id="_idIndexMarker416"/>called <strong class="bold">goodness-of-fit</strong> or the <strong class="bold">coefficient of determination</strong>) in <a id="_idIndexMarker417"/>distribution 2 by distribution 1 is 0.9327 2 = 87%, assuming we know that distribution 2 is a response to distribution 1. Otherwise, we could simply say there is a correlation of 0.93 or an 87% level of variance explained in the relationship between the two variables.</p>
<p>Now, let us look at the <em class="italic">Motor Trend Car Road Tests</em> dataset from R, which we import using the <code>statsmodels datasets.get_rdataset</code> function. Here, we have the first five rows, which have the variables for miles per gallon (<code>mpg</code>), number of cylinders (<code>cyl</code>), engine displacement (<code>disp</code>), horsepower (<code>hp</code>), rear-axle gear ratio (<code>drat</code>), weight (<code>wt</code>), minimum time to drive a quarter of a mile (<code>qsec</code>), engine shape (<code>vs=0</code> for v-shaped and <code>vs=1</code> for inline), transmission (<code>am=0</code> for automatic and <code>am=1</code> for manual), number <a id="_idIndexMarker418"/>of gears (<code>gear</code>), and number of carburetors (<code>carb</code>) (if not fuel injected):</p>
<pre class="source-code">
<code>import statsmodels.api as sm</code>
<code>df_cars = sm.datasets.get_rdataset("mtcars","datasets").data</code></pre>
<p>In Figure 4.19, we can see the first five rows of the data set, which contains data suitable for Pearson's correlation analysis.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table007">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">mpg</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">cyl</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">disp</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">hp</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">drat</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">wt</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">qsec</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Vs</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">am</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">gear</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">carb</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>21</p>
</td>
<td class="No-Table-Style">
<p>6</p>
</td>
<td class="No-Table-Style">
<p>160</p>
</td>
<td class="No-Table-Style">
<p>110</p>
</td>
<td class="No-Table-Style">
<p>3.9</p>
</td>
<td class="No-Table-Style">
<p>2.62</p>
</td>
<td class="No-Table-Style">
<p>16.46</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>21</p>
</td>
<td class="No-Table-Style">
<p>6</p>
</td>
<td class="No-Table-Style">
<p>160</p>
</td>
<td class="No-Table-Style">
<p>110</p>
</td>
<td class="No-Table-Style">
<p>3.9</p>
</td>
<td class="No-Table-Style">
<p>2.875</p>
</td>
<td class="No-Table-Style">
<p>17.02</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>22.8</p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p>108</p>
</td>
<td class="No-Table-Style">
<p>93</p>
</td>
<td class="No-Table-Style">
<p>3.85</p>
</td>
<td class="No-Table-Style">
<p>2.32</p>
</td>
<td class="No-Table-Style">
<p>18.61</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>21.4</p>
</td>
<td class="No-Table-Style">
<p>6</p>
</td>
<td class="No-Table-Style">
<p>258</p>
</td>
<td class="No-Table-Style">
<p>110</p>
</td>
<td class="No-Table-Style">
<p>3.08</p>
</td>
<td class="No-Table-Style">
<p>3.215</p>
</td>
<td class="No-Table-Style">
<p>19.44</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>18.7</p>
</td>
<td class="No-Table-Style">
<p>8</p>
</td>
<td class="No-Table-Style">
<p>360</p>
</td>
<td class="No-Table-Style">
<p>175</p>
</td>
<td class="No-Table-Style">
<p>3.15</p>
</td>
<td class="No-Table-Style">
<p>3.44</p>
</td>
<td class="No-Table-Style">
<p>17.02</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.19 – first five rows from the mtcars dataset</p>
<p>Using the dataset, we can plot a correlation matrix with the following code, which shows each pairwise correlation for all features in the dataset to see how they relate to one another:</p>
<pre class="source-code">
<code>sns.set_theme(style="white")</code>
<code>corr = df_cars.corr()</code>
<code>f, ax = plt.subplots(figsize=(15, 10))</code>
<code>cmap = sns.diverging_palette(250, 20, as_cmap=True)</code>
<code>sns.heatmap(corr, cmap=cmap, vmax=.4, center=0,</code>
<code>            square=True, linewidths=.5, annot=True)</code></pre>
<p>Suppose we are curious about the variables most meaningful for quarter-mile time (<code>qsec</code>). In <em class="italic">Figure 4</em><em class="italic">.20</em>, we can see by looking at the line for <code>qsec</code> that <code>vs</code> (v-shaped) is positively correlated at 0.74. Since this is a binary variable, we can assume, based on this dataset, that<a id="_idIndexMarker419"/> inline engines are faster than v-shaped engines. However, there are other covariates involved with significant correlation. For example, almost as strongly correlated with speed as engine shape is horsepower, such that as horsepower goes up, quarter-mile runtime goes down:</p>
<div><div><img alt="Figure 4.20 – Correlation matrix heatmap" height="945" src="img/B18945_04_020.jpg" width="1099"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.20 – Correlation matrix heatmap</p>
<p>A correlation matrix is useful for exploring the relationships between multiple variables at one time. It is also a useful tool <a id="_idIndexMarker420"/>for feature selection when building statistical <a id="_idIndexMarker421"/>and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models, such as linear regression.</p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor084"/>Power analysis examples</h2>
<p>Power analysis is<a id="_idIndexMarker422"/> a statistical method for identifying an appropriate sample size required for a hypothesis test to have sufficient power in preventing <em class="italic">Type II</em> errors – or failing to reject the null hypothesis when the null hypothesis should be rejected. Power analysis can also be used for identifying, based on sample size, a detectable effect size (or difference) between samples tested. In other words, based on a specific sample size and distribution, a power analysis can provide the analyst with a specific minimum difference the researcher may be able to reliably identify with a given test. In this section, we will demonstrate a power analysis using a one-sample t-test.</p>
<h3>One-sample t-test</h3>
<p>Let’s <a id="_idIndexMarker423"/>assume a manufacturer sells a type of machine capable <a id="_idIndexMarker424"/>of producing 100,000 units per month with a standard deviation of 2,800 units. A company has bought a number of these machines and has found them to only be producing 90,000 units. The company wants to know how many of the machines are needed to determine with a high level of confidence the machines are not capable of producing 100,000 units. The following power analysis indicates that for a t-test, a sample of three machines is required to prevent, with an 85% probability, failing to identify a statistically significant difference in actual versus marketed machine performance when there is one:</p>
<pre class="source-code">
<code>from statsmodels.stats.power import TTestPower</code>
<code>import numpy as np</code>
<code># Difference of distribution mean and the value to be assessed divided by the distribution standard deviation</code>
<code>effect_size = abs(100000-90000) / 2800</code>
<code>powersTT = TTestPower()</code>
<code>result = powersTT.solve_power(effect_size, nobs=3, alpha=0.05, alternative='two-sided')</code>
<code>print('Power based on sample size:{}'.format(round(result,2)))</code>
<code># Power based on sample size: 0.85</code></pre>
<h3>Additional power analysis example</h3>
<p>To <a id="_idIndexMarker425"/>see additional examples of power analysis in Python, please refer to this book’s GitHub repository. There, we have examples for additional t-tests and F-tests, which focus on analyzing variance between sample groups.</p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor085"/>Summary</h1>
<p>This chapter covered topics of parametric tests. Starting with the assumptions of parametric tests, we identified and applied methods for testing the violation of these assumptions and discussed scenarios where robustness can be assumed when the required assumptions are not met. We then looked at one of the most popular alternatives to the z-test, the t-test. We iterated through multiple applications of this test, covering one-sample and two-sample versions of this test using pooling, pairing, and Welch’s non-pooled version of the two-sample analysis. Next, we explored ANOVA techniques, where we looked at using data from multiple groups to identify statistically significant differences between them. This included one of the most popular adjustments to the p-value for when a high volume of groups is present—the Bonferroni correction, which helps prevent inflating the <em class="italic">Type I</em> error when performing multiple tests. We then looked at performing correlation analysis on continuous data using Pearson’s correlation coefficient and how to visualize correlation using a correlation matrix and accompanying heatmap. Finally, we briefly overviewed power analysis, with an example of performing this with a one-sample t-test. In the next chapter, we will discuss non-parametric hypothesis testing, including new tests in addition to those that pair with the parametric tests in this chapter for when assumptions cannot be safely assumed.</p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor086"/>References</h1>
<p>[1] <em class="italic">Dua, D.</em> and <em class="italic">Graff, C.</em> (<em class="italic">2019</em>). <em class="italic">UCI Machine Learning Repository</em> [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. <em class="italic">Irvine, CA: University of California, School of Information and </em><em class="italic">Computer Science</em>.</p>
</div>
</div></body></html>