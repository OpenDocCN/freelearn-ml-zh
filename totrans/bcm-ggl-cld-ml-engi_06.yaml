- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning BQ/BQML, TensorFlow, and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After building the GCP and Python foundations in part one and understanding
    the ML concepts and development process in part two, we are now entering part
    three of the book: *Mastering ML in GCP*. We will start by learning how Google
    does ML for structured data and the Google ML frameworks TensorFlow and Keras.
    In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: GCP BQ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCP BQML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In recent years, relational databases have been widely used in many enterprises,
    and thus structural data is a big portion of the big data available for many businesses.
    Google’s BQ and BQML play a big role in relational/structural data processing
    and analytics.
  prefs: []
  type: TYPE_NORMAL
- en: GCP BQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we mentioned in the *Google Cloud BigQuery* section in [*Chapter 1*](B18333_01.xhtml#_idTextAnchor015),
    *Comprehending Google Cloud Services*, BigQuery is a petabyte cloud enterprise
    data warehouse. BigQuery has the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully managed GCP service** – you do not concern yourself with the underlying
    backend data processing infrastructure, including compute, network storage, and
    other resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serverless** – you do not manage any servers in BigQuery. All of the data
    processing engines are taken care of by Google, including the invisible BigQuery
    BI Engine and ML Engine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Highly scalable** – it is incredibly elastic and can scale to any size, quickly
    and seamlessly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-effective** – you are only charged for the BigQuery resources you consume.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the Google Cloud big data processing pipeline, BigQuery is a key service
    for data ingestion, storing, analyzing, and visualization, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'BigQuery ingests data from data sources in three ways: archive, batch, and
    real-time streaming. With archived data, you can create a dataset with tables
    generated from data sources such as your computer, GCS, other GCP databases such
    as Bigtable, and Amazon **Simple Storage Services** (**S3**). With batch processing,
    you can load data into BigQuery from cloud storage or local storage, and the source
    data can be in the format of Avro, CSV, ORC, JSON, Parquet, or Firestore exports
    stored in GCS. Real-time events can be streamed into BigQuery. A common pattern
    is to push the events to GCP Pub/Sub, process them using a dataflow job, and ingest
    the outputs to BigQuery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BigQuery stores data with scalable storage, which is ACID compliant and cost-effective.
    The separation of storage and compute in BigQuery provides high performance and
    service decoupling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BigQuery processes data with an in-memory business intelligence engine – BigQuery
    BI Engine. Because BigQuery supports standard SQL, which is compliant with the
    ANSI SQL 2011 standard, it opens an avenue for the traditional relational databases
    and professionals to transform to BQ and BQML platforms. With SQL, BigQuery allows
    you to run queries, create reports and dashboards quickly, and export the results
    to Google Sheets or GCS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With BigQuery, you can visualize your data with its integrated Google Data Studio
    tool. Leveraging the BigQuery connector in Data Studio, you can create a data
    source, a report, and charts that visualize data in BigQuery data warehouses.
    You can leverage other tools such as Google Datalab, Looker, and Tableau.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can launch the BigQuery service from the GCP web console, or the command-line
    tools from Cloud Shell – `bq` is a Python-based command-line tool. There are also
    a number of client libraries for programmatic access to BigQuery, using C#, Go,
    Java, Node.js, PHP, Python, Ruby, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: GCP BQML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BQML enables data scientists to create and train ML models directly in BigQuery
    using standard SQL queries. BQML improves the ML model development speed by eliminating
    the need to move data and directly using BigQuery datasets as training and testing
    datasets. BQML-trained models can be exported directly to Vertex AI (to be discussed
    in later chapters) or other cloud serving layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'BQML can be accessed and used in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: The GCP console via a web browser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `bq` command-line tool via Google Cloud Shell or a VM shell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The BigQuery REST API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: External tools such as Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 3*](B18333_03.xhtml#_idTextAnchor072), *Preparing
    for ML Development*, and [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094), *ML
    Model Developing and Deploying*, the ML process includes data preparation, model
    creation and training, model validation/evaluation, and model deployment/prediction.
    Let’s go over this process with BQML.
  prefs: []
  type: TYPE_NORMAL
- en: The *first step* is data preparation. With BQML, you can prepare the training
    dataset by loading CSV files in the BigQuery console and directly running SQL
    statements after data is imported to BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *second step* is model creation and training. BigQuery ML supports the
    following ML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear regression** – where we have a number of data points, and we basically
    fit a line to those data points to minimize the error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binary logistic regression** – where we have two classes and we assign each
    example to one of the classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiclass logistic regression** – where we have more than two classes and
    we assign each example to one of the classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-means clustering** – where we have a number of points and we are able to
    separate them into different clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other models are supported by BQML. For more details, please refer to [https://cloud.google.com/bigquery-ml/docs/introduction#supported_models_in](https://cloud.google.com/bigquery-ml/docs/introduction#supported_models_in).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BQML implements the model creation and training in a single step using the BQML
    `create model` statement. Using *example 2* that we discussed in the previous
    chapters, *Table 6.1* shows two samples for the loan application processing model
    where the target is a binary value, *approval or not*, and the features include
    the application date, applicant credit score, loan amount, annual income, age,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 6.1 – Sample table structure ](img/Figure_6.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 6.1 – Sample table structure
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is sample code for creating a model with logistic regression
    using the dataset from a sample table `t`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: When the preceding code is run, BQML will execute the `SELECT` statement to
    filter all of the samples ranging from August 01, 2016 to July 31, 2019, and then
    use the results as the dataset input to train the logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *third step* is model validation/evaluation to determine how good the model
    is. With BQML, model evaluation is done using the `ML.EVALUATE` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When the preceding code is run, BQML will execute the `SELECT` statement to
    filter all of the samples ranging from August 01, 2019 to July 31, 2020, and then
    use the results as the dataset input to evaluate the performance of the classifier
    (the logistic regression model). When the code is completed, you can view the
    results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 6.2 – Sample BQML model evaluation result ](img/Figure_6.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 6.2 – Sample BQML model evaluation result
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 6.2* shows the sample metrics for our binary classification model as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Precision` – a metric that identifies the frequency with which a model was
    correct when predicting the positive class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recall` – a metric that answers the following question: out of all the possible
    positive labels, how many did the model correctly identify?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accuracy` – the fraction of predictions that a classification model got right.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`f1_score` – the harmonic average of the precision and recall.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_loss` – the loss function used in a logistic regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roc_auc` – the area under the ROC curve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the business use case, we can review the evaluation results, measure
    the model performance, and find the business indications. During the model evaluation
    process, we can tune model perimeters using the `model create` statement. More
    details are at [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-hyperparameter-tuning](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-hyperparameter-tuning).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *fourth step*, we can use the model to predict production outcomes.
    A sample query to predict the outcome is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding four steps, BQML completes the full process
    of ML development within the BigQuery cloud service. For structured data, BQML
    has many advantages for our data scientists to train and develop ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud BQ and BQML provide services for structured data processing and
    learning, and they are widely used in many business use cases. In the following
    section, we will introduce Google’s ML frameworks, TensorFlow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is an end-to-end open source platform for ML, developed by Google
    Brain, and it is one of the most widely used ML frameworks by data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow flow tensors** – TensorFlow’s name is directly derived from its
    core framework components: tensors. Let’s start by understanding **tensors**.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the concept of tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A tensor is a container that holds data of various sizes and shapes in an N-dimensional
    space. A tensor can be originated from the input data or a computation of the
    input data. In ML, we call the tensor components **features**. A tensor has three
    main characters to describe itself, called a tensor’s **rank**, **shape**, and
    **dtype** as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Rank is the number of directions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shape is the number of elements in each direction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dtype is the data type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The rank of a tensor specifies the number of directions being measured for
    a tensor. From the number of ranks, a tensor can be categorized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rank 0**: A tensor that only has a magnitude and 0 directions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rank 1**: A tensor that has one direction and a magnitude.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rank 2**: A tensor that has two directions (rows and columns), with each
    element having a magnitude.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rank 3**: A tensor that has three directions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rank 4**: A tensor with four directions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-rank tensors**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 6.3* illustrates the ranks of the tensors using the basic geometrical
    objects as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A rank 0 tensor is a scalar with a magnitude but no directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A rank 1 tensor is a vector with one direction and a magnitude.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A rank 2 tensor is a matrix with two directions – its elements have magnitudes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A rank 3 tensor has three directions – its elements have two-dimensional magnitudes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A rank 4 tensor is a list of rank 3 sensors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Ranks of tensors ](img/Figure_6.3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Ranks of tensors
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use the `list` data type, which we discussed in The *Python Basic Data
    Structure* section in [*Chapter 2*](B18333_02.xhtml#_idTextAnchor054), *Mastering
    Python Programming*, to define the rank of a tensor, then *each rank of tensors
    gives us a list of the objects from the previous rank of sensors* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Rank 1 tensors (vectors) gave us a list of rank 0 tensors (scalars).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank 2 tensors (matrixes) gave us a list of rank 1 tensors (vectors).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank 3 tensors (tensors) gave us a list of rank 2 tensors (matrixes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank 4 tensors gave us a list of rank 3 tensors (tensors).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s use a color image as an example to illustrate the concept of *rank* for
    a tensor. For a color image, we can use three channels to describe each pixel:
    a red channel, a green channel, and a blue channel. Each channel measures the
    magnitude of the pixel within that color channel. The red channel is a *rank 2*
    tensor since it uses a matrix to represent the image pixel map in red light (0
    means zero light and 255 means maximum light), and so are the green channel and
    the blue channel. Combining these three channels, we have a *rank 3* tensor. If
    we add a time axis for the order of the color frames/images to form a video, then
    it becomes a *rank 4* tensor. If we then batch the videos, it will then come up
    with a list of *rank 4* tensors – a *rank 5* tensor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have examined the tensor ranks, let’s check out the shape and dtype
    of a tensor – shape is the number of elements of a tensor and dtype is the element’s
    data type. With the tensors of ranks 0, 1, 2, 3, and n, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The shape for the *rank 0* tensor (scalar) is empty *()*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shape for a *rank 1* tensor, for example, a vector ([3, 5, 7]), is *(3)*
    since it has three elements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shape for a *rank 2* tensor, for example, a matrix ([3, 5, 7], [4, 6, 8])
    is *(2,3)* since it has two elements in one direction (rows) and three elements
    in the other direction (columns).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a good understanding of the tensor concept, let’s look at the
    second part of the name *TensorFlow*, flow, and see how tensors flow.
  prefs: []
  type: TYPE_NORMAL
- en: How tensors flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To describe the flowing of tensors in the **TensorFlow** (**TF**) framework,
    we adopt a computational graph with nodes and edges called a **Directed Acyclic
    Graph** (**DAG**). Directed means that the tensor (data) moves in a given order
    along a path in the graph. Acyclic means that the moving path does not form any
    cycles. So, tensors flow in no-loop DAGs to transform data.
  prefs: []
  type: TYPE_NORMAL
- en: Using DAGs, a tensor has a node and an edge. The nodes represent the operations
    we perform on the tensor/data, and the edges represent the path that the tensor/data
    flows along. Let’s use a DAG to describe a sample algorithm. As shown in *Figure
    6.4*, we input two numbers, multiply them to get their product, and add the numbers
    to get their sum. Then, we divide the product by the sum and print the result.
    If we replace the constant values with variables and add more complex mathematical
    operations, we can see that this tensor flowing process is really an ML model
    – we can change the variables or the weights to produce the expected output from
    the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – A sample algorithm with DAG  ](img/Figure_6.4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – A sample algorithm with DAG
  prefs: []
  type: TYPE_NORMAL
- en: Since DAGs are mono-directional and have an order of execution, we can parallelize
    the operations when tensors flow in the diagram. For example, If you want to add
    1,000 tensors and multiply the same 1,000 tensors, the addition and multiplication
    operations can be performed in parallel by distributing the operations to multiple
    computing resources by utilizing many CPU, **Graphics Processing Unit** (**GPU**),
    or **Tensor Processing Unit** (**TPU**) cores, or even **Quantum Processors**
    (**QU**). Since DAG allows the execution of parallel operations on different physical
    machines or platforms, we can further execute these operations in parallel on
    distributed server farms or edge devices. As we know, cloud computing is featured
    as on-demand, globally distributed, auto-scalable, and pay-as-you-go and thus
    it provides a perfect environment to parallelize TensorFlow operations and train
    ML models. As we have also discussed in the previous chapters, Google Colab has
    preinstalled TensorFlow packages, and you can practice TensorFlow in Colab, with
    GPU and TPU free of cost.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have grasped the concepts of tensors and understand how tensors
    flow in the TF frame, it’s time to introduce Keras—a high-level API that is designed
    for us to develop ML models with TensorFlow very easily.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras is a Google platform and a high-level interface to build ML/DL models
    with TensorFlow. Keras provides a high-level API that encapsulates data transformations
    and operations using logic units, called **layers**, as the building blocks to
    create neural networks. A layer performs data manipulation operations such as
    taking an average, calculating the minimum, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Keras, ML models are built from layers. During the ML model training process,
    the variables in the layers are adjusted, via backpropagation, to optimize the
    model cost function. Behind the scenes, TensorFlow and Keras complete detailed
    data operations, such as linear algebra and calculus calculations, in the background.
    Keras provides the following two APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: The **sequential API** provides the simplest interface and least complexity.
    With the sequential API, we can create the model layer by layer and thus build
    an ML/DL model as a simple list of layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **functional API** is more flexible and powerful than the sequential API
    since it allows thebranching or sharing of layers. With the functional API, we
    can have multiple inputs and outputs for ML models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following snippet shows an ML model training with the sequential API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following snippet shows an ML model training with the functional API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the preceding two Keras APIs have their own pros and cons. The
    sequential API is simple and straightforward, and the functional API can be used
    to build complex models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed Google Cloud BQ and BQML and introduced some examples
    of using BQ/BQML for data processing and ML model development. We also learned
    about the concepts of TensorFlow and Keras – Google’s frameworks for building
    ML models and projects.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will focus on Vertex AI, which provides an end-to-end
    platform for ML in the Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For further insights on the learning of the chapter, you can refer to the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/bigquery/docs/](https://cloud.google.com/bigquery/docs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/bigquery-ml/docs/](https://cloud.google.com/bigquery-ml/docs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/](https://www.tensorflow.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://keras.io/about/](https://keras.io/about/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
