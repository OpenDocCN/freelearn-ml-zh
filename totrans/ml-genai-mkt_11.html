<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer349">
    <h1 class="chapterNumber">11</h1>
    <h1 class="chapterTitle" id="_idParaDest-237">Micro-Targeting with Retrieval-Augmented Generation</h1>
    <p class="normal">This chapter introduces the advanced capabilities of <strong class="keyWord">retrieval-augmented generation</strong> (<strong class="keyWord">RAG</strong>) and its<a id="_idIndexMarker888"/> strategic application in precision marketing, building <a id="_idIndexMarker889"/>on the foundations laid by <strong class="keyWord">zero-shot learning</strong> (<strong class="keyWord">ZSL</strong>) and <strong class="keyWord">few-shot learning</strong> (<strong class="keyWord">FSL</strong>) discussed<a id="_idIndexMarker890"/> in the previous two chapters. Unlike ZSL, which operates without prior examples, and FSL, which relies on a minimal dataset, RAG leverages a real-time retrieval system to enhance generative models, enabling them to access and incorporate the most current and specific information available. This ability allows RAG to surpass the limitations of ZSL and FSL by providing personalized content tailored to individual consumer profiles or current market conditions – capabilities crucial for micro-targeting in marketing.</p>
    <p class="normal">The chapter will detail the operational framework of RAG, emphasizing its hybrid structure, which merges generative AI with dynamic information retrieval. This synthesis not only ensures the generation of contextually appropriate and accurate content but also introduces a level of personalization that was previously unattainable with either ZSL or FSL alone. We will explore how RAG’s real-time data retrieval component plays a critical role in adapting marketing strategies swiftly to align with consumer behavior and preferences using the consumer interaction data from a multi-product e-commerce platform as an example.</p>
    <p class="normal">By the conclusion of this chapter, you will be equipped with the knowledge to:</p>
    <ul>
      <li class="bulletList">Understand the integration of RAG with traditional generative models and its superiority in handling real-time, relevant data compared to ZSL and FSL.</li>
      <li class="bulletList">Recognize the enhanced capability of RAG in micro-targeting and personalizing content, which can dramatically improve consumer engagement and conversion.</li>
      <li class="bulletList">Apply RAG concepts to develop cutting-edge marketing strategies that are not only data-driven but also highly adaptable to the nuances of consumers.</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-238">Introduction to RAG for precision marketing</h1>
    <p class="normal">Generative models, particularly<a id="_idIndexMarker891"/> those developed on transformer frameworks like <strong class="keyWord">generative pre-trained transformer</strong> (<strong class="keyWord">GPT</strong>), have revolutionized how machines understand and generate human-like text. These models are trained on vast corpora of text data and are capable of learning complex patterns and structures of language that enable them to predict and generate coherent and contextually appropriate text sequences. However, despite their sophistication, pure generative models often lack the ability to incorporate real-time, specific information that isn’t explicitly present in their training data.</p>
    <p class="normal">This is where the “retrieval” component of RAG comes into play. RAG is a fusion of <strong class="keyWord">Generative AI</strong> (<strong class="keyWord">GenAI</strong>) with information retrieval systems, forming a hybrid model designed to enhance the quality and relevance of generated content. RAG achieves this by incorporating a dynamic retrieval component that pulls relevant information from an external dataset or knowledge base during the generation process. The retrieval system in RAG is designed to query a structured or unstructured database to fetch information that is relevant to the context of the generation task. This approach ensures that the generative model has access to the most current and relevant data to enhance the accuracy and relevance of the content it produces.</p>
    <h2 class="heading-2" id="_idParaDest-239">How RAG works</h2>
    <p class="normal">In addition<a id="_idIndexMarker892"/> to its role in enhancing content relevance, RAG enables a bidirectional flow of information between generative models and external databases. As illustrated in<strong class="keyWord"> </strong><em class="italic">Figure 11.1</em>, the process of RAG can be broken down into several key steps, with each step discussed further:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_11_01.png"/></figure>
    <p class="packt_figref">Figure 11.1: The key components of the RAG workflow</p>
    <p class="normal">Let’s<a id="_idIndexMarker893"/> explore these steps further:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Input prompt</strong>: In this step, the<a id="_idIndexMarker894"/> generative model is fed the user’s input prompt or the ongoing (fine-tuned) generation context.</li>
      <li class="numberedList"><strong class="keyWord">Query generation</strong>: Initially, the generative model produces a query based on the input provided in step 1. This query is written to retrieve the most relevant information from the knowledge base. This bidirectional flow of information between the generative model and the external database fosters a symbiotic relationship, enriching both data utilization and model performance.</li>
      <li class="numberedList"><strong class="keyWord">Information retrieval</strong>: In step 3, the query is then processed by the retrieval system, which searches through the database to find matches or closely related information. The sophistication of this system can vary, from simple keyword-based searches to more complex neural network models that understand semantic meanings. The retrieved data is then integrated into the content generation process in the next step.</li>
      <li class="numberedList"><strong class="keyWord">Content generation</strong>: Armed with the retrieved data, in step 4, the generative model then incorporates this information into its ongoing text generation process. This step is crucial as it involves blending the newly retrieved data with the generated content to maintain coherence and flow. This content may serve as the basis for an initiative like a marketing campaign, as illustrated above.</li>
      <li class="numberedList"><strong class="keyWord">Iterative refinement</strong>: Often, the process is iterative, with the generative model adjusting the queries based on the feedback loop between what has been generated<a id="_idIndexMarker895"/> and what needs to be generated next. This is captured in step 5, where the iterative refinement loop ensures the continuous adaptation and optimization of the generative process based on feedback such as marketing KPIs or customer feedback.</li>
    </ol>
    <p class="normal">Next, we will discuss how the query generation and retrieval steps work from a mathematical perspective.</p>
    <h2 class="heading-2" id="_idParaDest-240">Mathematical model of RAG retrieval</h2>
    <p class="normal">The<a id="_idIndexMarker896"/> mathematical model of RAG captures the relationship between a document query and its retrieval components (steps 2 and 3 in <em class="italic">Figure 11.1</em>, respectively), laying the groundwork for its dynamic content generation process. At its core, RAG combines probabilistic frameworks to integrate text generation with information retrieval, which can be expressed through equations that quantify the probabilities involved in generating text given an input and retrieved documents.</p>
    <p class="normal">The process begins with the generative model creating a query <em class="italic">q </em>based on the input prompt or ongoing context. Simultaneously, the retrieval system processes this query to fetch relevant documents from the database. The retrieval process is governed by a probability model that estimates the relevance of each document in the database given the query <em class="italic">q</em>. This estimation incorporates factors such as semantic similarity and document recency, ensuring that the retrieved information aligns closely with the context of the generation task. Mathematically, the probability of selecting document <em class="italic">r</em> from the database given the query <em class="italic">q</em> can be expressed as:</p>
    <p class="center"><a id="_idIndexMarker897"/><img alt="" src="../Images/B30999_11_001.png"/></p>
    <p class="normal">Where <em class="italic">D</em> represents the set of all possible documents in the database and <em class="italic">score(q,r)</em> is a function that computes the relevance of document <em class="italic">r</em> to the query <em class="italic">q</em>. The score function is typically learned through training and can involve complex embeddings that capture the semantic similarity between the query and the documents. The denominator in the equation ensures<a id="_idIndexMarker898"/> that the probability of selecting any document from the database sums to <code class="inlineCode">1</code>.</p>
    <p class="normal">Now that we know how the model works, let’s discuss the critical importance of the external data used in RAG systems.</p>
    <h2 class="heading-2" id="_idParaDest-241">The importance of data in RAG</h2>
    <p class="normal">External data <a id="_idIndexMarker899"/>acts as the cornerstone of the RAG framework, influencing both the quality of the content generated and the accuracy of the information retrieved. In marketing, where precision and relevance directly correlate with consumer engagement and conversion, the proper application and management of data within a RAG system becomes critical. There are two fundamental components embedded within the retrieval framework that warrant deeper exploration and should be prioritized due to their profound impact on the resulting outcome:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Data freshness</strong>: Keeping data up to date allows the model to capitalize on current trends, events, and behaviors, enhancing consumer engagement.</li>
      <li class="bulletList"><strong class="keyWord">Data specificity</strong>: By incorporating detailed consumer data, content can be precisely tailored to individual preferences and behaviors. This specificity not only increases the relevance of marketing messages but also can significantly boost conversion rates.</li>
    </ul>
    <p class="normal">Now let’s look at these concepts in detail.</p>
    <h3 class="heading-" id="_idParaDest-242">Data freshness</h3>
    <p class="normal">In practical terms, data freshness <a id="_idIndexMarker900"/>means the system consistently<a id="_idIndexMarker901"/> accesses and prioritizes the most recent information available. This is particularly significant in marketing, where information can quickly become outdated due to rapidly changing market conditions or consumer preferences. For example, during a major sales event like Black Friday, having the latest data allows RAG systems to produce content that highlights current promotions, available stock levels, or last-minute deals, greatly enhancing the effectiveness of marketing campaigns. Unlike previously discussed GenAI approaches such as ZSL and FSL, which primarily rely on pre-existing datasets and may not update their knowledge base in real time, RAG systems integrate a dynamic retrieval mechanism that actively fetches and utilizes the most current data available.</p>
    <p class="normal">To ensure content remains relevant, RAG systems can adjust their retrieval processes to favor newer documents. Mathematically, this adjustment can be represented by modifying the relevance score to include a term that increases the weight of more recent documents:</p>
    <p class="center"><a id="_idIndexMarker902"/><img alt="" src="../Images/B30999_11_002.png"/></p>
    <p class="normal">Where <a id="_idIndexMarker903"/><img alt="" src="../Images/B30999_11_003.png"/> is a parameter that determines the impact of a<a id="_idIndexMarker904"/> document’s recency on its score. By prioritizing recent data, RAG systems can respond more dynamically to the latest trends, ensuring that the content they generate resonates with current consumer behaviors and preferences.</p>
    <h3 class="heading-" id="_idParaDest-243">Data specificity</h3>
    <p class="normal">Data specificity refers <a id="_idIndexMarker905"/>to how detailed and <a id="_idIndexMarker906"/>relevant the information is in relation to a specific query. In RAG systems, high data specificity ensures that the content retrieved and generated is directly aligned with the user’s current needs or questions, therefore enhancing user engagement and satisfaction. While other methods like transfer learning also allow access to detailed datasets, RAG integrates real-time data retrieval with generative capabilities, making it particularly well-suited for applications where up-to-the-minute information is crucial.</p>
    <p class="normal">In technical terms, RAG systems can use advanced semantic matching algorithms to ensure that the content they retrieve matches the user’s query not just by keywords but in overall meaning and context. This approach involves adjusting the scoring mechanism to prioritize documents that are not only relevant but also contextually specific to the user’s inquiry:</p>
    <p class="center"><a id="_idIndexMarker907"/><img alt="" src="../Images/B30999_11_004.png"/></p>
    <p class="normal">For example, consider a user searching for “best skincare for sensitive skin”. A RAG system with high data specificity would be able to pull and generate content that not only mentions skincare products but specifically addresses products designed for sensitive skin, potentially including user reviews, expert advice, and the latest product innovations. This level of detail in content generation can significantly improve the effectiveness of <a id="_idIndexMarker908"/>personalized marketing campaigns, enhancing customer conversion rates and building brand loyalty.</p>
    <h2 class="heading-2" id="_idParaDest-244">Understanding the retrieval index</h2>
    <p class="normal">The <strong class="keyWord">retrieval index</strong> is <a id="_idIndexMarker909"/>a fundamental<a id="_idIndexMarker910"/> component of any RAG system, acting as the bedrock on which the retrieval functionality operates. It ensures that the queries processed by the system are matched with accurate and contextually relevant responses. Managing this index effectively is crucial for the system’s performance and can involve several key strategies. Let’s look at some management approaches.</p>
    <h3 class="heading-" id="_idParaDest-245">Indexing strategies</h3>
    <p class="normal">Effective indexing is <a id="_idIndexMarker911"/>vital for the swift and accurate retrieval of information. The backbone of a robust RAG system lies in its ability to quickly sift through vast amounts of data and find information that best matches the user’s query. This is achieved through sophisticated indexing strategies that organize data in a way that optimizes search processes, such as:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Inverted indices</strong>: These<a id="_idIndexMarker912"/> are used to store a mapping from content keywords to their locations in the database. For example, in an e-commerce setting, an inverted index might link terms like “wireless headphones” or “thermal jacket” directly to the relevant product listings. This allows the system to quickly gather all relevant documents during a query, enhancing search efficiency and response speed.</li>
      <li class="bulletList"><strong class="keyWord">Vector space models</strong>: This<a id="_idIndexMarker913"/> approach involves converting text into vector forms or embeddings that are easy to compare and analyze. Using algorithms like TF-IDF or neural network-based embeddings, such as those introduced in <em class="chapterRef">Chapters 5</em>, <em class="chapterRef">9</em>, and <em class="chapterRef">10</em>, these models help in understanding and comparing the semantic similarity between the user’s query and available documents, leading to more nuanced and contextually appropriate responses.</li>
      <li class="bulletList"><strong class="keyWord">Graph-based indexing</strong>: This <a id="_idIndexMarker914"/>approach is useful for applications like social media analytics where understanding complex user relationships enhances content targeting. More generally, this approach is valuable when relationships between data points are as important as the data itself.</li>
      <li class="bulletList"><strong class="keyWord">Multi-dimensional indexing</strong>: This<a id="_idIndexMarker915"/> technique is beneficial in cases such as geographic data applications such as real estate analysis where efficient searches across multiple attributes like location and time are needed. This method is particularly valuable for queries that involve ranges or require simultaneous consideration of several dimensions.</li>
    </ul>
    <p class="normal">Choosing the appropriate <a id="_idIndexMarker916"/>indexing strategy depends on the application’s data needs and query complexity. Inverted indices are optimized for quick keyword lookups, ideal for environments where speed is crucial. Vector space models offer richer semantic analysis and are better suited for contexts requiring a deeper understanding of content. For handling more complex data relationships or multiple query dimensions, graph-based and multi-dimensional indexing provide valuable solutions.</p>
    <h3 class="heading-" id="_idParaDest-246">Data curation and updating</h3>
    <p class="normal">To maintain its<a id="_idIndexMarker917"/> efficacy, the retrieval index must be regularly updated and curated. The following are a couple of concepts to consider:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Dynamic updating</strong>: As new<a id="_idIndexMarker918"/> information becomes available or old information becomes obsolete, the index needs to be updated to reflect these changes. This ensures that the RAG system remains effective and relevant over time. For instance, in a marketing campaign, promotional content might need frequent updates to reflect the most current offers by the company so that the RAG system can adjust its responses.</li>
      <li class="bulletList"><strong class="keyWord">Automated monitoring</strong>: Implementing automated systems to continuously monitor and update <a id="_idIndexMarker919"/>indices can greatly enhance a RAG system’s responsiveness to changes. In digital marketing, this could include adjusting strategies based on new consumer trend analyses to make sure the marketing content is aligned with the economic environment.</li>
    </ul>
    <p class="normal">Proper data updating and curation are key for keeping RAG systems useful, especially in marketing, given that these systems currently do not include advanced data management features and often need separate practices to manage data curation and updates.</p>
    <h2 class="heading-2" id="_idParaDest-247">RAG implementation challenges</h2>
    <p class="normal">There are <a id="_idIndexMarker920"/>several challenges in RAG implementation that are worth highlighting. As discussed in the previous section, ensuring the quality and relevance of the underlying external database is crucial, and often separate data engineering practices are needed to achieve this. The following are a couple of other major issues that RAG systems can face:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Handling long documents</strong>: RAG systems need to break retrieval documents into smaller chunks to avoid memory issues and slow response times. However, breaking longer documents into smaller chunks can lead to a loss of context. To address this, solutions such as summarization algorithms and context-aware chunking methods can be used.</li>
      <li class="bulletList"><strong class="keyWord">Retrieving relevant data from multiple sources</strong>: Combining data from various sources can be challenging due to inconsistencies and differences in data formats. For instance, if customer data comes from both chat history and tabular purchase data, the system might face issues linking the two sources and generate marketing messages based on inconsistent information. Techniques such as multi-modal AI models (see <em class="chapterRef">Chapter 12</em>) and knowledge graphs can help address this.</li>
    </ul>
    <p class="normal">Despite these challenges, major organizations such as Meta AI Research, Google AI, and Microsoft have successfully implemented RAG systems. For instance, Google has integrated RAG to enhance the relevance of its search results and Microsoft integrates RAG within Azure AI services to improve virtual assistants’ capabilities. RAG is also becoming increasingly accessible to smaller organizations through services that offer to curate their existing enterprise data into formats that are compatible with RAG applications.</p>
    <h2 class="heading-2" id="_idParaDest-248">Applications of RAG in marketing</h2>
    <p class="normal">RAG systems<a id="_idIndexMarker921"/> that uniquely combine real-time data retrieval with advanced content generation offer a number of transformative applications in marketing. </p>
    <p class="normal">Given that these systems enable marketers to craft personalized and contextually aware content by integrating the most relevant data into the content creation process, there are some pivotal areas where RAG systems can significantly enhance marketing strategies:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Dynamic personalized content creation</strong>: RAG systems excel in generating advertising content that adapts in real time to changes in user interactions, browsing behaviors, or purchase histories. By accessing data specific to an individual’s recent activities, RAG can tailor advertisements to match personal preferences and interests.</li>
    </ul>
    <p class="normal"><strong class="keyWord">Example</strong>: Imagine a user recently explored camping gear online. A RAG system could use<a id="_idIndexMarker922"/> this data to dynamically create ads for related items like hiking boots or travel guides. This targeted advertising not only increases the relevance and appeal of the ads but also boosts engagement rates and potential conversions. We will discuss other examples of this in the hands-on exercise presented later in this chapter.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Segment-specific content generation</strong>: RAG enables marketers to produce content finely targeted to specific demographic segments, enriched with the latest data relevant to these groups. This strategy ensures the content deeply resonates with its intended audience, therefore enhancing reader engagement and brand loyalty.</li>
    </ul>
    <p class="normal"><strong class="keyWord">Example</strong>: A RAG system might generate blog posts for first-time home buyers that include up-to-date mortgage rates and real-time housing market trends. This not only provides valuable information but also establishes the brand as a credible resource in the home-buying journey.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Enhanced customer engagement: </strong>By harnessing the power of real-time data retrieval and advanced content generation, these systems enable the production of highly personalized content that resonates deeply with individual customers. Whether through personalized emails, targeted social media posts, or bespoke newsletters, RAG systems ensure that all communications are timely, relevant, and tailored to each customer’s unique profile.</li>
    </ul>
    <p class="normal"><strong class="keyWord">Example</strong>: For a customer who recently celebrated a significant event like an anniversary, RAG can generate personalized greetings or offers that reflect the customer’s preferences and past interactions with the brand. This could include curated content, special promotions, or personalized messages from brand ambassadors, creating a highly personalized and memorable customer experience.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Automated response generation for customers</strong>: RAG enhances customer support by generating informative and contextually relevant responses to inquiries. By pulling information from FAQs, product manuals, or customer databases, RAG systems deliver precise, customized answers.</li>
    </ul>
    <p class="normal"><strong class="keyWord">Example</strong>: When a<a id="_idIndexMarker923"/> customer inquires about the return policy for an online purchase, RAG can generate a response that includes details tailored to the item’s category or the date of purchase. This application of RAG in customer support not only improves response times but also boosts overall customer satisfaction.</p>
    <p class="normal">Now that you have the theoretical knowledge in place, let’s move on to the application and get familiar with <a id="_idIndexMarker924"/>the process of building a knowledge retrieval system.</p>
    <div class="packt_tip">
      <p class="normal">If you would like to explore RAG further, here are a couple of valuable resources:</p>
      <ul>
        <li class="bulletList"><em class="italic">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</em> (<a href="https://arxiv.org/pdf/2005.11401"><span class="url">https://arxiv.org/pdf/2005.11401</span></a>) is a seminal paper by researchers from Meta AI that explores the integration of retrieval and generation to enhance language models for knowledge-intensive tasks.</li>
        <li class="bulletList"><em class="italic">Searching for Best Practices in Retrieval-Augmented Generation</em> (<a href="https://arxiv.org/abs/2407.01219"><span class="url">https://arxiv.org/abs/2407.01219</span></a>) is a more recent review/survey of the literature covering key advancements and current challenges.</li>
      </ul>
    </div>
    <h1 class="heading-1" id="_idParaDest-249">Building a knowledge retrieval system for marketing with LangChain</h1>
    <p class="normal">This section <a id="_idIndexMarker925"/>will explore the practical aspects of setting up a knowledge retrieval system specifically tailored for marketing purposes using LangChain, a library designed to facilitate the integration of language models with retrieval systems. We will do this via an example that readers can follow to construct their own retrieval systems for enhancing the capabilities of generative AI in their own marketing strategies.</p>
    <h2 class="heading-2" id="_idParaDest-250">Introduction to LangChain</h2>
    <p class="normal">LangChain is a <a id="_idIndexMarker926"/>framework that significantly enhances the capabilities of language models by integrating them with retrieval systems. Its flexible design allows it to support a variety of backend databases and customizable retrieval strategies using modules like <code class="inlineCode">langchain-retrieval</code> and <code class="inlineCode">langchain-storage</code>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">LangChain essentials</strong></p>
      <p class="normal">LangChain facilitates the combination of language models with robust retrieval systems. This integration captures the principles of RAG, enabling applications to generate content that is not only human-like but also highly relevant and informed by the latest data.</p>
      <p class="normal">Further documentation<a id="_idIndexMarker927"/> and resources can be found on LangChain’s official documentation page: <a href="https://python.langchain.com/docs/get_started/introduction/"><span class="url">https://python.langchain.com/docs/get_started/introduction/</span></a></p>
    </div>
    <p class="normal">LangChain stands out as a top choice for Python developers looking to integrate advanced AI capabilities into their marketing strategies. While other tools such as LlamaIndex specialize in data indexing and retrieval for quick access to large datasets, LangChain offers a more versatile platform for creating complex NLP applications. Some of the key features <a id="_idIndexMarker928"/>making LangChain ideal for Python developers are the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Ease of integration</strong>: LangChain simplifies the incorporation of complex AI models with retrieval databases through high-level abstraction, allowing you to focus more on creating unique application logic rather than wrestling with backend complexities, speeding up development and deployment processes.</li>
      <li class="bulletList"><strong class="keyWord">Modularity</strong>: The framework’s high modularity supports a diverse range of language models and retrieval databases, alongside custom logic for specific interactions.</li>
      <li class="bulletList"><strong class="keyWord">Scalability</strong>: LangChain is designed to scale effortlessly from handling small tasks to managing large-scale applications, such as real-time personalized advertising and extensive email marketing campaigns. This scalability ensures that as marketing strategies grow and evolve, their technological infrastructure can grow without needing a complete overhaul.</li>
    </ul>
    <p class="normal">In the following <a id="_idIndexMarker929"/>sections, we will dive deeper into setting up LangChain, designing a retrieval model, and integrating this system with generative models to demonstrate its application in real-world marketing scenarios.</p>
    <h2 class="heading-2" id="_idParaDest-251">Understanding the external dataset</h2>
    <p class="normal">The <a id="_idIndexMarker930"/>dataset that we’ll be using as the source of external data for our retrieval model comes from a large multi-category e-commerce platform and captures user interactions over the course of one month (December 2019). It includes different types of events related to product interactions by users, such as viewing, adding to the cart, removing from the cart, and purchasing. </p>
    <p class="normal">We will use this as a basis for our setup process of a knowledge retrieval system with LangChain as it will set the stage for the hands-on example we will walk through in the final part of the chapter, where we will be using the user interactions from this same database for the purpose of RAG micro-targeting of consumers.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Source code and data</strong>: <a href="https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.11"><span class="url">https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.11</span></a></p>
      <p class="normal"><strong class="keyWord">Data source</strong>: <a href="https://www.kaggle.com/code/annettecatherinepaul/ecommerce-analysis-and-recommendations/input"><span class="url">https://www.kaggle.com/code/annettecatherinepaul/ecommerce-analysis-and-recommendations/input</span></a></p>
    </div>
    <p class="normal">Before incorporating this dataset into our retrieval model, it’s important to thoroughly understand its characteristics to ensure effective data ingestion and utilization in the retrieval system. A preliminary examination of the data can tailor the retrieval architecture and refine the indexing strategy, facilitating optimized query responses. To start, let’s examine the first few entries to understand the type of data each column contains:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
df = pd.read_csv(<span class="hljs-string">"2019-Dec.csv"</span>)
df.head(<span class="hljs-number">3</span>))
</code></pre>
    <p class="normal">This yields the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_11_02.png"/></figure>
    <p class="packt_figref">Figure 11.2: First three rows from the Kaggle e-commerce user interactions dataset</p>
    <p class="normal">The<a id="_idIndexMarker931"/> dataset features columns such as <code class="inlineCode">event_time</code>, which marks when an event occurred, <code class="inlineCode">event_type</code>, indicating the nature of the interaction (viewing, purchasing, or adding or removing from the cart), and other identifiers such as <code class="inlineCode">product_id</code>, <code class="inlineCode">category_id</code>, <code class="inlineCode">brand</code>, and <code class="inlineCode">price</code>. Taken together, this will help us understand the user’s journey through the e-commerce platform, including their actions, the products involved, and the pricing dynamics during their sessions.</p>
    <p class="normal">To deepen our understanding, we can perform basic statistical analyses that highlight the distribution and range of key data points by running the <code class="inlineCode">describe</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">df.describe()
</code></pre>
    <p class="normal">The output shows that the dataset comprises over 3.5 million events:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_11_03.png"/></figure>
    <p class="packt_figref">Figure 11.3: Key statistics for numeric columns in the dataset</p>
    <p class="normal">The product<a id="_idIndexMarker932"/> prices range significantly from as low as -$79.37 (possibly indicating returns or pricing errors) to a high of $327.80, with a mean of around $8.87. Such variability suggests diverse consumer behavior and purchasing power, which are crucial for segmenting and targeting marketing campaigns.</p>
    <p class="normal">We can also obtain a deeper understanding of two more key columns that are not included in the above summary, <code class="inlineCode">event_type</code> and <code class="inlineCode">brand</code>, which help us identify consumer preferences and buying patterns:</p>
    <pre class="programlisting code"><code class="hljs-code">df[<span class="hljs-string">'event_type'</span>].value_counts()
</code></pre>
    <p class="normal">This gives us the following output:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_11_04.png"/></figure>
    <p class="packt_figref">Figure 11.4: Counts of event_type values in the dataset </p>
    <p class="normal">This first output shows that views are the most common <code class="inlineCode">event_type</code> (approximately 1.73 million instances), followed by additions to the cart, removals from the cart, and purchases. The substantial drop from cart interactions to actual purchases (over 700,000 fewer purchases than cart additions) is a point of interest for improving conversion rates. More generally, the reduction of counts from views to cart interactions to overall purchases highlights a typical e-commerce sales funnel.</p>
    <p class="normal">By running our second command of <code class="inlineCode">value_counts</code> on the <code class="inlineCode">brand</code> column, we can also see which brands appear most commonly throughout the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">df[<span class="hljs-string">'brand'</span>].value_counts()
</code></pre>
    <p class="normal">The following is the output:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_11_05.png"/></figure>
    <p class="packt_figref">Figure 11.5: Counts of brand values in the dataset</p>
    <p class="normal">It is also <a id="_idIndexMarker933"/>important to understand the null values in the data. This gives us a way to understand both the data quality and key information around missing values that will need to be accounted for when indexing data for our retrieval index:</p>
    <pre class="programlisting code"><code class="hljs-code">df.isnull().<span class="hljs-built_in">sum</span>()
</code></pre>
    <p class="normal">The following result indicates significant gaps in <code class="inlineCode">category_code</code> (almost 3.5 million missing values) and <code class="inlineCode">brand</code> (over 1.5 million missing values) data:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_11_06.png"/></figure>
    <p class="packt_figref">Figure 11.6: Counts of missing values across columns in the dataset</p>
    <p class="normal">This could hinder our ability to perform detailed product category analysis and brand-specific marketing <a id="_idIndexMarker934"/>using this dataset. The minor missing count in <code class="inlineCode">user_session</code> (<code class="inlineCode">779</code>) suggests nearly complete tracking of user sessions, which is most vital for our goal of analyzing user behavior throughout their sessions.</p>
    <h2 class="heading-2" id="_idParaDest-252">Designing the retrieval model with LangChain</h2>
    <p class="normal">As we move <a id="_idIndexMarker935"/>forward in constructing our knowledge retrieval system using LangChain, the next step involves the effective indexing of <a id="_idIndexMarker936"/>our e-commerce dataset. Indexing is a foundational process that enables the efficient retrieval of data. This section outlines the setup of an indexing system to support the queries that will allow us to achieve our goal of micro-targeting based on their shopping behaviors. To implement our retrieval system, we will utilize Elasticsearch for our backend system due to its robust full-text search capabilities and suitability for handling large datasets.</p>
    <h3 class="heading-" id="_idParaDest-253">Install and connect to Elasticsearch</h3>
    <p class="normal">Before creating the index, we<a id="_idIndexMarker937"/> need to ensure that Elasticsearch is installed and running on our system. Follow these steps based on your operating system:</p>
    <p class="normal">Here are the steps to be followed by Windows users:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Download</strong>: Visit the Elasticsearch official download page (<a href="https://www.elastic.co/downloads/elasticsearch"><span class="url">https://www.elastic.co/downloads/elasticsearch</span></a>) and download the latest version for Windows.</li>
      <li class="numberedList"><strong class="keyWord">Install</strong>: Extract the downloaded ZIP file to your desired location and navigate to the extracted folder.</li>
      <li class="numberedList"><strong class="keyWord">Start</strong>: Open Command Prompt as <code class="inlineCode">Administrator</code>, change directory to where Elasticsearch is installed (e.g., <code class="inlineCode">cd C:\path\to\elasticsearch-&lt;version&gt;\bin</code>), and execute <code class="inlineCode">elasticsearch.bat</code> to start Elasticsearch. Substitute <code class="inlineCode">&lt;version&gt;</code> with the specific version of Elasticsearch that was downloaded from their official download page.</li>
    </ol>
    <p class="normal">macOS/Linux users can follow these steps:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Download</strong>: Use the below command in a terminal window, replacing <code class="inlineCode">&lt;version&gt;</code> with the latest version:
        <pre class="programlisting con"><code class="hljs-con">curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-&lt;version&gt;-darwin-x86_64.tar.gz
</code></pre>
      </li>
      <li class="numberedList"><strong class="keyWord">Install</strong>: Extract<a id="_idIndexMarker938"/> the file with the below command and navigate to the newly created directory:
        <pre class="programlisting con"><code class="hljs-con">tar -xzf elasticsearch-&lt;version&gt;-darwin-x86_64.tar.gz 
</code></pre>
      </li>
      <li class="numberedList"><strong class="keyWord">Start</strong>: Open a terminal window and change to the <code class="inlineCode">bin</code> directory:
        <pre class="programlisting con"><code class="hljs-con">cd elasticsearch-&lt;version&gt;/bin
</code></pre>
      </li>
    </ol>
    <p class="normal">Start Elasticsearch by running:</p>
    <pre class="programlisting con"><code class="hljs-con">./elasticsearch
</code></pre>
    <p class="normal">If you have an ARM-powered MacBook, you may need to install via Homebrew instead. Instructions for this, as well as alternative ways of installing Elasticsearch, can be found on their website: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/install-elasticsearch.html"><span class="url">https://www.elastic.co/guide/en/elasticsearch/reference/7.17/install-elasticsearch.html</span></a>.</p>
    <p class="normal">After starting Elasticsearch, verify that it is running by using the following <code class="inlineCode">curl</code> command in your terminal or command prompt:</p>
    <pre class="programlisting con"><code class="hljs-con">curl http://localhost:9200
</code></pre>
    <p class="normal">You should see a JSON response from Elasticsearch indicating that it is running correctly, such as the following:</p>
    <pre class="programlisting con"><code class="hljs-con">{
  "name" : "Device-name.local",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "2QND1H4pxAyi75_21C6rhw",
  "version" : {
    "number" : "7.17.4",
    "build_flavor" : "default",
    "build_type" : "tar",
    "build_hash" : "79878662c54c126ae89206c685d9f1051a9d6411",
    "build_date" : "2022-05-18T18:04:20.964345128Z",
    "build_snapshot" : false,
    "lucene_version" : "8.11.1",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
</code></pre>
    <p class="normal">With Elasticsearch<a id="_idIndexMarker939"/> running, you can now connect from Python using the Elasticsearch client. Here’s how to set it up:</p>
    <pre class="programlisting code"><code class="hljs-code">!conda install elasticsearch
<span class="hljs-keyword">from</span> elasticsearch <span class="hljs-keyword">import</span> Elasticsearch
es = Elasticsearch(["http://localhost:<span class="hljs-number">9200</span>"])
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> es.ping():
    <span class="hljs-keyword">raise</span> ValueError("Connection failed")
<span class="hljs-keyword">else</span>:
    <span class="hljs-built_in">print</span>("Connected to Elasticsearch!")
</code></pre>
    <h3 class="heading-" id="_idParaDest-254">Indexing data in Elasticsearch</h3>
    <p class="normal">Next, we can define the <a id="_idIndexMarker940"/>data schema and index data from the dataset. Given the diversity of data types and volume of data captured within the dataset, it is important to configure our indexing mappings to efficiently handle the range and type of data present. This mapping for our index is informed by our earlier exploratory analysis that we performed on this dataset, where we discovered that our dataset consists of over 3.5 million entries with data types ranging from integers and floating-point numbers to categorical and textual data. This diversity necessitates a robust indexing strategy to ensure efficient data retrieval and query performance:</p>
    <pre class="programlisting code"><code class="hljs-code">mapping = <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">"mappings"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
        <span class="hljs-attr">"properties"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
            <span class="hljs-attr">"event_time"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"date"</span><span class="hljs-punctuation">},</span>
            <span class="hljs-attr">"event_type"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"</span><span class="hljs-string">keyword"</span><span class="hljs-punctuation">},</span>
            <span class="hljs-attr">"product_id"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"integer"</span><span class="hljs-punctuation">},</span>
            <span class="hljs-attr">"category_id"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"</span><span class="hljs-string">long"</span><span class="hljs-punctuation">},</span>
            <span class="hljs-attr">"category_code"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
                <span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"text"</span><span class="hljs-punctuation">,</span>
                <span class="hljs-attr">"fields"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
                    <span class="hljs-attr">"keyword"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
                        <span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"keyword"</span><span class="hljs-punctuation">,</span>
                        <span class="hljs-attr">"ignore_above"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">256</span>
                    <span class="hljs-punctuation">}</span>
                <span class="hljs-punctuation">}</span>
            <span class="hljs-punctuation">},</span>
            <span class="hljs-attr">"brand"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
                <span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"text"</span><span class="hljs-punctuation">,</span>
                <span class="hljs-attr">"fields"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
                    <span class="hljs-attr">"keyword"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
                        <span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"keyword"</span><span class="hljs-punctuation">,</span>
                        <span class="hljs-attr">"ignore_above"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">256</span>
                    <span class="hljs-punctuation">}</span>
                <span class="hljs-punctuation">}</span>
            <span class="hljs-punctuation">},</span>
            <span class="hljs-attr">"price"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"float"</span><span class="hljs-punctuation">},</span>
            <span class="hljs-attr">"user_id"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"long"</span><span class="hljs-punctuation">},</span>
            <span class="hljs-attr">"user_session"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"keyword"</span><span class="hljs-punctuation">}</span>
        <span class="hljs-punctuation">}</span>
    <span class="hljs-punctuation">}</span>
<span class="hljs-punctuation">}</span>
</code></pre>
    <p class="normal">Some key aspects<a id="_idIndexMarker941"/> of the decisions that were made in defining the above mapping types are the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Numeric and identifier fields</strong>: <code class="inlineCode">product_id</code>, <code class="inlineCode">category_id</code>, and <code class="inlineCode">user_id</code> are all stored as integers with significant ranges. Given their use as identifiers, they are mapped to Elasticsearch’s integer and long data types, which are optimized for numeric operations and comparisons. <code class="inlineCode">price</code>, a floating-point field, shows a wide range of values, from negative (likely indicating refunds) to over 300. This field is mapped as a float to accurately represent and query price data.</li>
      <li class="bulletList"><strong class="keyWord">Categorical and textual fields</strong>: <code class="inlineCode">event_type</code> and <code class="inlineCode">user_session</code> are categorical fields with high cardinality. These are mapped as keyword types, which support the exact matching necessary for filtering and aggregations. <code class="inlineCode">brand</code> and <code class="inlineCode">category_code</code>, however, contain textual data that might be used for full-text search as well as for exact matches. These fields are defined with the text type and a keyword multi-field to allow both full-text searching and aggregations.</li>
      <li class="bulletList"><strong class="keyWord">Date fields</strong>: <code class="inlineCode">event_time</code> represents timestamps and is mapped as a date. This allows for time-based queries, which are essential for analyzing trends over time.</li>
    </ul>
    <p class="normal">Next, we will create the index based on the mapping defined above using:</p>
    <pre class="programlisting code"><code class="hljs-code">es.indices.create(index=<span class="hljs-string">'ecommerce_data'</span>, body=mapping, ignore=<span class="hljs-number">400</span>)
</code></pre>
    <p class="normal">Upon execution, this<a id="_idIndexMarker942"/> should result in a response such as the following:</p>
    <pre class="programlisting con"><code class="hljs-con">ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'ecommerce_data'})
</code></pre>
    <p class="normal">With these configurations, your Elasticsearch environment is now fully equipped for indexing and querying our dataset.</p>
    <h3 class="heading-" id="_idParaDest-255">Data ingestion into Elasticsearch</h3>
    <p class="normal">Ingesting<a id="_idIndexMarker943"/> the dataset into the Elasticsearch index is the step that involves transferring data from our DataFrame into Elasticsearch. This process uses an efficient bulk indexing method, which is essential for handling large volumes of data effectively.</p>
    <p class="normal">Starting from the dataset load step, we include handling of the inconsistent and missing values that we identified earlier and replace these with <code class="inlineCode">None</code> types to avoid errors in the indexing process. Depending on your device hardware, the indexing process may take some time, ranging from minutes to hours, given the dataset size. You can track this progress using the <code class="inlineCode">tqdm</code> progress bar.</p>
    <p class="normal">To further optimize the indexing performance, we include the <code class="inlineCode">chunk_size</code> parameter in the bulk indexing method. This parameter controls the number of documents processed in each bulk request and adjusting its value can impact the speed of indexing and memory usage. In addition to <code class="inlineCode">chunk_size</code>, we also suggest truncating the data before indexing if you still experience memory issues. To truncate the data, you can use a <code class="inlineCode">pandas</code> function such as <code class="inlineCode">df = df.head(n)</code> to limit the DataFrame to the first <code class="inlineCode">n</code> rows. Let’s take a look at the code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> elasticsearch <span class="hljs-keyword">import</span> helpers
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">def</span> <span class="hljs-title">generate_data</span>(<span class="hljs-params">df</span>):
    <span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> tqdm(df.iterrows(), total=<span class="hljs-built_in">len</span>(df), desc=<span class="hljs-string">"Indexing documents"</span>):
        doc = {
            <span class="hljs-string">"</span><span class="hljs-string">_index"</span>: <span class="hljs-string">"ecommerce_data"</span>,
            <span class="hljs-string">"_source"</span>: {
                <span class="hljs-string">"event_time"</span>: pd.to_datetime(row[<span class="hljs-string">'event_time'</span>]).isoformat() <span class="hljs-keyword">if</span> pd.notna(row[<span class="hljs-string">'event_time'</span>]) <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,
                <span class="hljs-string">"event_type"</span>: row[<span class="hljs-string">'</span><span class="hljs-string">event_type'</span>],
  <span class="hljs-string">"product_id"</span>: <span class="hljs-built_in">int</span>(row[<span class="hljs-string">'product_id'</span>]) <span class="hljs-keyword">if</span> pd.notna(row[<span class="hljs-string">'product_id'</span>]) <span class="hljs-keyword">else</span>
<span class="hljs-literal">None</span>,
                <span class="hljs-string">"category_id"</span>: <span class="hljs-built_in">int</span>(row[<span class="hljs-string">'</span><span class="hljs-string">category_id'</span>]) <span class="hljs-keyword">if</span> pd.notna(row[<span class="hljs-string">'category_id'</span>]) <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,
                <span class="hljs-string">"category_code"</span>: row[<span class="hljs-string">'category_code'</span>] <span class="hljs-keyword">if</span> pd.notna(row[<span class="hljs-string">'category_code'</span>]) <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,
                <span class="hljs-string">"brand"</span>: row[<span class="hljs-string">'brand'</span>] <span class="hljs-keyword">if</span> pd.notna(row[<span class="hljs-string">'brand'</span>]) <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,
 <span class="hljs-string">"price"</span>: <span class="hljs-built_in">float</span>(row[<span class="hljs-string">'price'</span>]) <span class="hljs-keyword">if</span> pd.notna(row[<span class="hljs-string">'price'</span>]) <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,
                <span class="hljs-string">"user_id"</span>: <span class="hljs-built_in">int</span>(row[<span class="hljs-string">'user_id'</span>]) <span class="hljs-keyword">if</span> pd.notna(row[<span class="hljs-string">'user_id'</span>]) <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,
                <span class="hljs-string">"user_session"</span>: row[<span class="hljs-string">'user_session'</span>] <span class="hljs-keyword">if</span> pd.notna(row[<span class="hljs-string">'user_session'</span>]) <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>
            }
        }
        <span class="hljs-keyword">yield</span> doc
success, _ = helpers.bulk(es, generate_data(df), chunk_size=<span class="hljs-number">500</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Indexed </span><span class="hljs-subst">{success}</span><span class="hljs-string"> documents successfully."</span>)
</code></pre>
    <p class="normal">Upon <a id="_idIndexMarker944"/>completion of the indexing process, you should see an output similar to the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Indexing documents: 100%|██████████████████████████████████| 3533286/3533286 [19:36&lt;00:00, 3004.23it/s]
Indexed 3533286 documents successfully.
</code></pre>
    <h3 class="heading-" id="_idParaDest-256">Integrating with LangChain using GPT</h3>
    <p class="normal">Integrating <a id="_idIndexMarker945"/>LangChain with your Elasticsearch backend involves setting up the environment so that LangChain can use an LLM to dynamically generate content based on the data retrieved from Elasticsearch. We will demonstrate this using the gpt-3.5-turbo LLM, but you should refer to the latest LangChain documentation for the latest models available (<a href="https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html"><span class="url">https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html</span></a>):</p>
    <ol>
      <li class="numberedList" value="1">The first step involves initializing the GPT model as a LangChain model. You may also need to generate an OpenAI API key for the model if you haven’t done this step already, a process that currently includes the creation of an OpenAI account. Further details on the instructions for this can be found on the OpenAI website (<a href="https://platform.openai.com/docs/introduction"><span class="url">https://platform.openai.com/docs/introduction</span></a>):
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> langchain_openai <span class="hljs-keyword">import</span> ChatOpenAI
<span class="hljs-comment"># Replace 'XXX' with your OpenAI API key</span>
llm = ChatOpenAI(openai_api_key='XXX', model= "gpt-<span class="hljs-number">3.5</span>-turbo")
</code></pre>
      </li>
      <li class="numberedList">Once <a id="_idIndexMarker946"/>the model is set up, the next step involves constructing queries to fetch relevant data from Elasticsearch. This data serves as the input for GPT, allowing it to generate contextually relevant content based on user behavior.</li>
    </ol>
    <p class="normal">For example, here is how you can define a function to retrieve data based on any matching query from Elasticsearch:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">retrieve_data_from_es</span>(<span class="hljs-params">query</span>):
    response = es.search(index="ecommerce_data", body={"query": {"<span class="hljs-keyword">match</span>": query}})
    <span class="hljs-keyword">return</span> response['hits']['hits']
</code></pre>
    <p class="normal">This function <code class="inlineCode">retrieve_data_from_es</code> takes a dictionary representing the Elasticsearch query and returns a list of documents that match this query. The example provided fetches records associated with a specific user ID, allowing for the generation of personalized content based on the user’s previous interactions, such as products viewed or added to the cart. For example, to grab content related to <code class="inlineCode">user_id</code> <code class="inlineCode">576802932</code>, we can execute the following:</p>
    <pre class="programlisting code"><code class="hljs-code">query = {"user_id": "<span class="hljs-number">576802932</span>"}
data = retrieve_data_from_es(query)
</code></pre>
    <ol>
      <li class="numberedList" value="3">The <code class="inlineCode">data</code> response retrieved from Elasticsearch includes detailed records of the user’s activities, each of which is tagged with precise timestamps, product identifiers, category details, and session information. We can see examples of these types of interactions via the following:
        <pre class="programlisting code"><code class="hljs-code">removal_example = <span class="hljs-built_in">next</span>(item <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data <span class="hljs-keyword">if</span> item['_source']['event_type'] == 'remove_from_cart')
view_example = <span class="hljs-built_in">next</span>(item <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data <span class="hljs-keyword">if</span> item['_source']['event_type'] == 'view')
<span class="hljs-built_in">print</span>("Removal Example:\n", removal_example)
<span class="hljs-built_in">print</span>("\nView Example:\n", view_example)
</code></pre>
      </li>
    </ol>
    <p class="normal">The following <a id="_idIndexMarker947"/>screenshot shows the output:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_11_07.png"/></figure>
    <p class="packt_figref">Figure 11.7: example entries for remove_from_cart and view event types from the dataset</p>
    <p class="normal">In these examples, one entry shows a user removing an item from the cart around midnight, suggesting reconsideration or a preference change. The second entry captures a view event, where a user is browsing products but has not yet made a purchase decision. Understanding these interactions can help in designing personalized re-engagement strategies to encourage users to complete their purchases and enhance conversion rates.</p>
    <p class="normal">Now that we have designed, implemented, and tested the framework for our knowledge retrieval model, let’s explore the application of this model via an example.</p>
    <h1 class="heading-1" id="_idParaDest-257">Implementing RAG for micro-targeting based on customer data</h1>
    <p class="normal">Having<a id="_idIndexMarker948"/> thoroughly analyzed the dataset and constructed a robust retrieval system, we now transition from theoretical frameworks to practical implementation. In this section, we will learn how to apply RAG to dynamically address common challenges in digital marketing, such as outdated information in trained models and capturing recent user interactions. Traditional <strong class="keyWord">zero-shot learning</strong> (<strong class="keyWord">ZSL</strong>) and <strong class="keyWord">few-shot learning</strong> (<strong class="keyWord">FSL</strong>) models, while powerful, often lag in real-time responsiveness and rely on pre-existing data, limiting their effectiveness in such a fast-paced marketing scenario.</p>
    <p class="normal">To overcome these<a id="_idIndexMarker949"/> limitations, we will utilize RAG to generate marketing content that is not only up to date but also deeply relevant to current consumer behaviors. By integrating our retrieval system with GPT, we can pull the latest user interaction data directly from our database. With RAG, we can also generate real-time content tailored to individual user scenarios, effectively re-engaging customers who have abandoned their carts. This approach allows us to generate marketing strategies that are informed by the most recent and relevant user activities, something that static models cannot achieve.</p>
    <h2 class="heading-2" id="_idParaDest-258">Determining the campaign strategy</h2>
    <p class="normal">As we transition from <a id="_idIndexMarker950"/>the theoretical frameworks discussed earlier, it’s important to understand the strategic underpinnings of the upcoming example where we demonstrate how RAG can be applied. Before diving into the real-world examples, we will set the stage for the basis of our micro-targeting strategies by performing a couple more crucial elements <a id="_idIndexMarker951"/>of exploratory analysis on the dataset in order to determine our optimal campaign strategy and its goals.</p>
    <h3 class="heading-" id="_idParaDest-259">Message timing</h3>
    <p class="normal">Efficiency<a id="_idIndexMarker952"/> in digital marketing not only reduces costs but also enhances the effectiveness of campaigns. One strategic component for our campaign will be the optimization of the timing for our marketing messages. As discussed in <em class="chapterRef">Chapter 4</em>, understanding the time-series trends in user interactions can be particularly insightful for this. Rather than performing a date-based analysis, since we only have access to one month of data, we will instead examine how interactions vary by time of day so that we can use this information to give us insight into the optimal time of day for user engagement for the following months. We can extract the insights needed to make this decision via the following:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
df['event_time'] = pd.to_datetime(df['event_time'])
df['time_of_day'] = df['event_time'].dt.hour
time_of_day_data = df.groupby(['time_of_day', 'event_type']).size().unstack()
fig, ax = plt.subplots(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
time_of_day_data.plot(ax=ax, title='User Interactions by Time of Day')
plt.xlabel('Hour of the Day')
plt.ylabel('Number of Events')
plt.xticks(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">24</span>))
plt.grid(<span class="hljs-literal">True</span>)
plt.show()
</code></pre>
    <p class="normal">This gives us <a id="_idIndexMarker953"/>the following graph:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_11_08.png"/></figure>
    <p class="packt_figref">Figure 11.8: Number of user interactions by event type across different hours of the day</p>
    <p class="normal">From the plotted data, we observe a few distinct user behavior patterns throughout the day that have important implications for the timing of our micro-targeting strategy:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Peak viewing hours</strong>: The highest user activity, specifically views, occurs between <code class="inlineCode">17:00</code> and <code class="inlineCode">20:00</code>. This peak period is optimal for pushing advertisements and personalized content to maximize exposure and engagement.</li>
      <li class="numberedList"><strong class="keyWord">High transaction activities</strong>: Cart additions and purchases are also generally high from <code class="inlineCode">17:00</code> to <code class="inlineCode">20:00</code>. This indicates not only active browsing but also a higher propensity to finalize purchases, making it an ideal time for promotional offers.</li>
      <li class="numberedList"><strong class="keyWord">Cart removal insights</strong>:<strong class="keyWord"> </strong>Cart removals closely mirror the two preceding trends and peak between <code class="inlineCode">18:00</code> and <code class="inlineCode">20:00</code>, suggesting a reconsideration phase among users. This period could be strategically targeted with reminders or incentives such as discount offers or free shipping to convert hesitations into sales.</li>
      <li class="numberedList"><strong class="keyWord">Early morning and late night trends</strong>: While there’s a gradual increase in activity from the morning, it sharply declines after <code class="inlineCode">21:00</code>, indicating late night hours might not be as effective for targeted campaigns.</li>
    </ol>
    <p class="normal">Given these <a id="_idIndexMarker954"/>patterns, a targeted strategy can be specifically implemented during the <code class="inlineCode">19:00</code> to <code class="inlineCode">21:00</code> window to mitigate cart abandonment rates. During this time, deploying personalized re-engagement campaigns such as sending reminder emails, offering limited-time discount codes, or displaying pop-up messages with special offers can be particularly effective.</p>
    <p class="normal">For your own analysis, it is important to consider that behavior patterns will differ when analyzing user interactions across different time zones. It is therefore important to confirm that user time zones are accurately recorded and stored in your database, or that time zone differences are accounted for after the fact based on the region of the user. Failing to do so could result in analysis that involves a convolution of user behaviors across regions, leading to poor message timing.</p>
    <h3 class="heading-" id="_idParaDest-260">Choosing the brand</h3>
    <p class="normal">As we <a id="_idIndexMarker955"/>continue to refine our marketing strategy, it’s crucial to identify which brands within our dataset present the highest potential for increasing conversion rates. In previous analyses, we explored the frequency of brand appearances to gauge their prevalence. Let’s now go deeper by examining the top five brands to understand how different types of interactions vary across these brands:</p>
    <pre class="programlisting code"><code class="hljs-code">top_brands = df['brand'].value_counts().nlargest(<span class="hljs-number">5</span>).index
brand_event_type_counts = df[df['brand'].isin(top_brands)].groupby(['brand', 'event_type']).size().unstack()
Brand_event_type_counts
</code></pre>
    <p class="normal">This yields the following output:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_11_09.png"/></figure>
    <p class="packt_figref">Figure 11.9: Number of user interactions by event type for the five most popular brands</p>
    <p class="normal">Analyzing<a id="_idIndexMarker956"/> the brand interactions, <code class="inlineCode">bpw.style</code> stands out as a candidate for focused improvement, especially in terms of cart abandonment metrics. While <code class="inlineCode">bpw.style</code> shows a substantial presence in the dataset, there’s a noticeable discrepancy between the number of items added to carts (<code class="inlineCode">21995</code>) and those removed (<code class="inlineCode">18014</code>). This pattern suggests a significant gap between initial interest and final purchasing decisions.</p>
    <p class="normal">To quantify the opportunity for improvement, let’s calculate the cart abandonment rate for the <code class="inlineCode">bpw.style</code> brand:</p>
    <pre class="programlisting code"><code class="hljs-code">abandon_rate_bpw = brand_event_type_counts.loc['bpw.style', 'remove_from_cart'] / brand_event_type_counts.loc['bpw.style', 'cart']
<span class="hljs-built_in">print</span>(f"Cart Abandonment Rate <span class="hljs-keyword">for</span> bpw.style: {abandon_rate_bpw:<span class="hljs-number">.2</span>f}")
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Cart Abandonment Rate for bpw.style: 0.82
</code></pre>
    <p class="normal">Addressing the gap in cart abandonment could significantly boost <code class="inlineCode">bpw.style</code>'s performance by converting potential sales into actual purchases.</p>
    <h2 class="heading-2" id="_idParaDest-261">Using LangChain for micro-targeting</h2>
    <p class="normal">Earlier, we<a id="_idIndexMarker957"/> established a retrieval system integrated with GPT to dynamically generate content based on the<a id="_idIndexMarker958"/> data from Elasticsearch. We will now utilize the marketing campaign goals that we just defined to leverage the insights gained from our analysis of user behavior by brand and timing to create highly personalized content aimed at enhancing customer engagement. We will start with two examples that are user-specific, before transitioning to a third example that is targeted specifically toward consumers of the <code class="inlineCode">bpw.style</code> brand.</p>
    <h3 class="heading-" id="_idParaDest-262">Case study 1: Targeted product discounts</h3>
    <p class="normal">Given that<a id="_idIndexMarker959"/> our data indicates that user interactions, especially cart additions and removals, peak roughly between <code class="inlineCode">17:00</code> and <code class="inlineCode">21:00</code>, we will use this timeframe as the optimal window for sending out marketing messages, capturing when users are most active and likely reconsidering their purchase decisions. The preferred medium for this outreach will be email, which allows for rich, personalized content and can effectively re-engage users by reminding them of items left in their carts or providing time-sensitive offers.</p>
    <p class="normal">To generate tailored marketing content, we deploy a function that constructs prompts for GPT based on user interactions, transforming these data points into actionable marketing strategies:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_content</span>(<span class="hljs-params">data</span>):
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> data:
        <span class="hljs-keyword">return</span> "No data available to generate content."
    messages = [
        ("system", "You are an assistant that generates marketing strategies based on user activities.")
    ]
    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data:
        source = item['_source']
        product_description = f"{source['event_type']} the product {source['brand']} priced at ${source['price']} on {source['event_time']}."
        messages.append(("human", product_description))
    messages.append(("human", "Based on these interactions, suggest a targeted marketing message to improve engagement that focuses on product discounts."))
    <span class="hljs-keyword">try</span>:
        response = llm.invoke(messages)
        <span class="hljs-keyword">return</span> response.content
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        <span class="hljs-keyword">return</span> f"Error generating content: {<span class="hljs-built_in">str</span>(e)}"
</code></pre>
    <p class="normal">This <a id="_idIndexMarker960"/>function constructs a series of messages to feed into the LLM, each representing a significant user interaction, such as adding to a cart or removing from it. These descriptions culminate in a request for a marketing strategy tailored to these activities, prompting the LLM to generate a custom message.</p>
    <p class="normal">Going back to our earlier analysis of the data stored in our retrieval index for “<code class="inlineCode">user_id</code>": “<code class="inlineCode">576802932</code>", we can run an analysis on this customer and obtain a response such as the following:</p>
    <pre class="programlisting code"><code class="hljs-code">query = {"user_id": "<span class="hljs-number">576802932</span>"}
data = retrieve_data_from_es(query)
generate_content(data)
</code></pre>
    <p class="normal">This yields the following:</p>
    <pre class="programlisting code"><code class="hljs-code">Dear customer, we have noticed that you've recently removed a few items from your cart. We'd like to offer you a special discount on those products to help complete your purchase. This discount of 10% is valid only for the next 24 hours and can be claimed using discount code LOVE10 at checkout. Add them to your cart now and let's get those items home!
</code></pre>
    <p class="normal">Our marketing campaign could time this prompt generation to take place based on the most recent customer interaction data for this consumer as of roughly <code class="inlineCode">17:00</code> and then send the email shortly thereafter. This not only provides a direct incentive to reduce abandonment but also creates a sense of urgency that the customer is more likely to act upon.</p>
    <h3 class="heading-" id="_idParaDest-263">Case study 2: Product upselling</h3>
    <p class="normal">Building on the <a id="_idIndexMarker961"/>insights from the first case study, which focused on reducing cart abandonment rates through targeted discounts during peak interaction hours, this second case study explores a complementary strategy. Here, we aim to enhance product upselling by offering personalized product recommendations based on user behavior and preferences during similar peak hours.</p>
    <p class="normal">This function is designed to harness user data to not only remind them of their abandoned carts but also to upsell related or complementary products based on their browsing and purchasing history. By doing this, we not only aim to recover abandoned carts but also<a id="_idIndexMarker962"/> increase the average order value:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_upsell_content</span>(<span class="hljs-params">data</span>):
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> data:
        <span class="hljs-keyword">return</span> "No data available to generate content."
    messages = [("system", "You are an assistant that generates upsell opportunities based on user purchase history.")]
    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data:
        source = item['_source']
        messages.append(("human", f"Identify complementary products <span class="hljs-keyword">for</span> {source['brand']} priced at ${source['price']} that were viewed but <span class="hljs-keyword">not</span> purchased on {source['event_time']}."))
    messages.append(("human", "Suggest an upselling strategy that could be included <span class="hljs-keyword">in</span> a follow-up marketing email."))
    <span class="hljs-keyword">try</span>:
        response = llm.invoke(messages)
        <span class="hljs-keyword">return</span> response.content
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        <span class="hljs-keyword">return</span> f"Error generating content: {<span class="hljs-built_in">str</span>(e)}"
</code></pre>
    <p class="normal">This function is designed to construct a narrative that not only addresses the immediate need to reduce cart abandonment but also strategically suggests additional products that enhance the user’s initial choice, potentially increasing the transaction value.</p>
    <p class="normal">We can now apply this function to the same consumer as last time and see the result, this time prompting the model to return a full email message in its entirety:</p>
    <pre class="programlisting code"><code class="hljs-code">query = {"user_id": "<span class="hljs-number">576802932</span>"}
data = retrieve_data_from_es(query)
output = generate_upsell_content(data)
</code></pre>
    <p class="normal">This produces the following email content:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_11_10.png"/></figure>
    <p class="packt_figref">Figure 11.10: Tailored upselling content strategy generated for user 576802932, based on their interaction data retrieved from the Elasticsearch database</p>
    <h3 class="heading-" id="_idParaDest-264">Case study 3: Real-time content customization for bpw.style</h3>
    <p class="normal">Building on our <a id="_idIndexMarker963"/>strategy to utilize RAG for micro-targeting, this case study focuses specifically on enhancing engagement and reducing cart abandonment rates for the <code class="inlineCode">bpw.style</code> brand. Recognizing the disparity between cart additions and completions, as previously analyzed, we target users who might benefit from a gentle reminder or an incentive to complete their transactions. This query will specifically look for cart addition and removal events:</p>
    <ol>
      <li class="numberedList" value="1">Here is a function with the Elasticsearch query to fetch this specific type of brand interaction data:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">retrieve_bpw_style_data</span>(<span class="hljs-params">es_client</span>):
    query = {
        "<span class="hljs-built_in">bool</span>": {
            "must": [
                {"<span class="hljs-keyword">match</span>": {"brand": "bpw.style"}},
                {"terms": {"event_type": ["cart", "remove_from_cart"]}}
            ]
        }
    }
    response = es_client.search(index="ecommerce_data", body={"query": query, "size": <span class="hljs-number">100</span>})
    <span class="hljs-keyword">return</span> response['hits']['hits']
</code></pre>
      </li>
      <li class="numberedList">With the<a id="_idIndexMarker964"/> targeted data retrieved, we can now integrate this into our <code class="inlineCode">generate_reengagement_content</code> function. The modified function will use the data fetched by our new query function to generate personalized re-engagement strategies:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_reengagement_content</span>(<span class="hljs-params">es_client</span>):
    data = retrieve_bpw_style_data(es_client)
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> data:
        <span class="hljs-keyword">return</span> "No data available to generate content."
    messages = [
        ("system", "You are an assistant that creates re-engagement strategies <span class="hljs-keyword">for</span> users who have shown interest <span class="hljs-keyword">in</span> bpw.style products but abandoned their carts.")
    ]
    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data:
        source = item['_source']
        interaction_desc = f"User showed interest <span class="hljs-keyword">in</span> {source['brand']} priced at ${source['price']} but abandoned the cart on {source['event_time']}."
        messages.append(("human", interaction_desc))
    messages.append(("human", "Generate a short personalized email to re-engage the user <span class="hljs-keyword">and</span> encourage them to complete their purchase."))
   
    <span class="hljs-keyword">try</span>:
        response = llm.invoke(messages)
        <span class="hljs-keyword">return</span> response.content
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        <span class="hljs-keyword">return</span> f"Error generating content: {<span class="hljs-built_in">str</span>(e)}"
</code></pre>
      </li>
      <li class="numberedList">We can run our new query and content generation commands via the following:
        <pre class="programlisting code"><code class="hljs-code">marketing_message = generate_reengagement_content(es)
<span class="hljs-built_in">print</span>(marketing_message)
</code></pre>
      </li>
    </ol>
    <p class="normal">This yields the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_11_11.png"/></figure>
    <p class="packt_figref">Figure 11.11: Personalized re-engagement strategy created for users interested in bpw.style products who have abandoned their carts</p>
    <p class="normal">As shown <a id="_idIndexMarker965"/>by the output, by honing in on specific brands that exhibit high cart abandonment rates, we can tailor marketing strategies that address unique challenges related to brand perception and customer engagement. This approach allows for the creation of highly specific content that resonates with the brand’s audience, potentially transforming browsing behaviors into completed transactions. While this strategy emphasizes brand-specific data, it can seamlessly integrate with user-specific insights illustrated earlier, including demographic information, if available, to enhance the precision and relevance of the marketing messages.</p>
    <h1 class="heading-1" id="_idParaDest-265">Summary</h1>
    <p class="normal">This chapter has provided a detailed exploration of RAG and its transformative impact on precision marketing. By integrating generative AI with dynamic retrieval systems, RAG overcomes the limitations inherent in previous models like ZSL and FSL by incorporating real-time, context-specific data into content generation. This enables an unprecedented level of personalization in marketing strategies, enhancing the relevance and efficacy of marketing content tailored to individual consumer preferences and current market conditions. </p>
    <p class="normal">We’ve used practical examples and mathematical models to demonstrate how RAG effectively combines data freshness and specificity, thereby elevating consumer engagement and optimizing conversion rates.</p>
    <p class="normal">The discussion also covered the technical mechanisms that underpin RAG, from query generation and information retrieval to the iterative refinement of generated content. These elements ensure that the content not only resonates with the audience but also stays aligned with the latest trends and data insights, a crucial advantage in today’s rapidly evolving digital marketing landscape.</p>
    <p class="normal">As we look to the future, in the next chapter, we will move on to the broader landscape of AI and ML in marketing, consolidating the knowledge acquired throughout this book while exploring emerging technologies. We will combine current methods and predicted advancements in AI that will revolutionize marketing even further. Additionally, we will examine how AI is becoming integral to emerging digital platforms that offer novel ways to engage customers and personalize marketing efforts. This forward-looking perspective will give you the insights and skills that you will need to navigate the evolving AI landscape, preparing you for the future of digital marketing.</p>
    <h1 class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 5000 members at:</p>
    <p class="normal"><a href="https://packt.link/genai"><span class="url">https://packt.link/genai</span></a></p>
    <p class="normal"><img alt="" src="../Images/QR_Code12856128601808671.png"/></p>
  </div>
</body></html>