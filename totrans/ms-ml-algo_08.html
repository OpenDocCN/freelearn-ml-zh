<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Ensemble Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we are going to discuss some important algorithms that exploit different estimators to improve the overall performance of an ensemble or committee. These techniques work either by introducing a medium level of randomness in every estimator belonging to a predefined set or by creating a sequence of estimators where, each new model is forced to improve the performance of the previous ones. These techniques allow us to reduce both the bias and the variance (thereby increasing validation accuracy) when employing models with a limited capacity or more prone to overfit the training set.</p>
<p class="mce-root">In particular, the topics covered in the chapter are as follows:</p>
<ul>
<li>Introduction to ensemble learning</li>
<li class="mce-root">A brief introduction to decision trees</li>
<li class="mce-root">Random forest and extra randomized forests</li>
<li class="mce-root">AdaBoost (algorithms M1, SAMME, SAMME.R, and R2)</li>
<li class="mce-root">Gradient boosting</li>
<li class="mce-root">Ensembles of voting classifiers, stacking, and bucketing</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensemble learning fundamentals</h1>
                </header>
            
            <article>
                
<p>The main concept behind ensemble learning is the distinction between strong and weak learners. In particular, a strong learner is a classifier or a regressor which has enough capacity to reach the highest potential accuracy, minimizing both bias and variance (thus achieving also a satisfactory level of generalization). More formally, if we consider a parametrized binary classifier <em>f(x; θ)</em>, we define it as a strong learner if the following is true:</p>
<div style="padding-left: 30px" class="mce-root"><img src="assets/e85388b3-c547-42a0-a706-4e9975ae1a92.png" style="width:42.42em;height:3.33em;"/></div>
<p>This expression can appear cryptic; however, it's very easy to understand. It simply expresses the concept that a strong learner is theoretically able to achieve any non-null probability of misclassification with a probability greater than or equal to 0.5 (that is, the threshold for a binary random guess). All the models normally employed in Machine Learning tasks are normally strong learners, even if their domain can be limited (for example, a logistic regression cannot solve non-linear problems). On the other hand, a weak learner is a model that is generically able to achieve an accuracy slightly higher than a random guess, but whose complexity is very low (they can be trained very quickly, but can never be used alone to solve complex problems). There is a formal definition also in this case, but it's simpler to consider that the real main property of a weak learner is a limited ability to achieve a reasonable accuracy. In some very particular and small regions of the training space, a weak learner could reach a low probability of misclassification, but in the whole space its performance is only a little bit superior to a random guess. The previous one is more a theoretical definition than a practice one, because all the models currently available are normally quite better than a random oracle. However, an ensemble is defined as a set of weak learners that are trained together (or in a sequence) to make up a committee. Both in classification and regression problems, the final result is obtained by averaging the predictions or employing a majority vote.</p>
<p>At this point, a reasonable question is—Why do we need to train many weak learners instead of a single strong one? The answer is double—in ensemble learning, we normally work with medium-strong learners (such as decision trees or <strong>support vector machines</strong> (<strong>SVMs</strong>)) and we use them as a committee to increase the overall accuracy and reduce the variance thanks to a wider exploration of the sample space. In fact, while a single strong learner is often able to overfit the training set, it's more difficult to keep a high accuracy over the whole sample subspace without saturating the capacity. In order to avoid overfitting, a trade-off must be found and the result is a less accurate classifier/regressor with a simpler separation hyperplane. The adoption of many weak learners (that are actually quite strong, because even the simplest models are more accurate than a random guess), allows us to force them to focus only on a limited subspace, so as to be able to reach a very high local accuracy with a low variance. The committee, employing an averaging technique, can easily find out which prediction is the most suitable. Alternatively, it can ask each learner to vote, assuming that a successful training process must always lead the majority to propose the most accurate classification or prediction.</p>
<p>The most common approaches to ensemble learning are as follows:</p>
<ul>
<li><strong>Bagging (bootstrap aggregating)</strong>: This approach trains <em>n</em> weak learners <em>fw1, fw2, ..., fwn</em> (very often they are decision trees) using <em>n</em> training sets (<em>D1, D2, ..., Dn</em>) created by randomly sampling the original dataset <em>D</em>. The sampling process (called <strong>bootstrap sampling</strong>) is normally performed with replacement, so as to determine different data distributions. Moreover, in many real algorithms, the weak learners are also initialized and trained using a medium degree of randomness. In this way, the probability of having clones becomes very small and, at the same, time it's possible to increase the accuracy by keeping the variance under a tolerable threshold (thus avoiding overfitting).</li>
<li><strong>Boosting</strong>: This is an alternative approach that builds an incremental ensemble starting with a single weak learner <em>fw1</em> and adding a new one <em>fwi</em> at each iteration. The goal is to reweight the dataset, so as to force the new learner to focus on the samples that were previously misclassified. This strategy yields a very high accuracy because the new learners are trained with a positively-biased dataset that allows them to adapt to the most difficult internal conditions. However, in this way, the control over the variance is weakened and the ensemble can more easily overfit the training set. It's possible to mitigate this problem by reducing the complexity of the weak learners or imposing a regularization constraint.</li>
<li><strong>Stacking</strong>: This method can be implemented in different ways but the philosophy is always the same—use different algorithms (normally a few strong learners) trained on the same dataset and filter the final result using another classifier, averaging the predictions or using a majority vote. This strategy can be very powerful if the dataset has a structure that can be partially managed with different approaches. Each classifier or regressor should discover some data aspects that are peculiar; that's why the algorithms must be structurally different. For example, it can be useful to mix a decision tree with a SVM or linear and kernel models. The evaluation performed on the test set should clearly show the prevalence of a classifier only in some cases. If an algorithm is finally the only one that produces the best prediction, the ensemble becomes useless and it's better to focus on a single strong learner.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forests</h1>
                </header>
            
            <article>
                
<p>A random forest is the bagging ensemble model based on decision trees. If the reader is not familiar with this kind of model, I suggest reading the <em>Introduction to Machine Learning</em>, <em>Alpaydin E.</em>, <em>The MIT Press</em>, where a complete explanation can be found. However, for our purposes, it's useful to provide a brief explanation of the most important concepts. A decision tree is a model that resembles a standard hierarchical decision process. In the majority of cases, a special family is employed, called binary decision trees, as each decision yields only two outcomes. This kind of tree is often the simplest and most reasonable choice and the training process (which consists in building the tree itself) is very intuitive. The root contains the whole dataset:</p>
<div style="padding-left: 150px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/207960e3-1f7e-440d-9a30-14e364aae79c.png" style="width:25.25em;height:1.83em;"/></div>
<p>Each level is obtained by applying a selection tuple, defined as follows:</p>
<p style="padding-left: 90px"><img src="assets/94702ff1-52eb-4f64-a05e-d6520cfe18fc.png" style="width:36.00em;height:2.67em;"/></p>
<p>The first index of the tuple corresponds to an input feature, while the threshold <em>t<sub>i</sub></em> is a value chosen in the specific range of each feature. The application of a selection tuple leads to a split and two nodes that contain each a non-overlapping subset of the input dataset. In the following diagram, there's an example of a slip performed at the level of the root (initial split):</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/a04f6f3f-79da-4289-b0a7-db4ddb65353d.png" style="width:24.42em;height:14.25em;"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Example of initial split in a decision tree</div>
<p>The set <em>X</em> is split into two subsets defined as <em>X11</em> and <em>X12</em> whose samples have respectively the feature with <em>i=2</em> less or greater than the threshold <em>ti=0.8</em>. The intuition behind classification decision trees is to continue splitting until the leaves contain samples belonging to a single category <em>yi</em> (these nodes are defined as pure). In this way, a new sample <em>xj</em> can traverse the tree with a computation complexity <em>O(log(M))</em> and reach a final node that determines its category. In a very similar way, it's possible to build regression trees whose output is continuous (even if, for our purposes, we are going to consider only classification scenarios).</p>
<p>At this point, the main problem is how to perform each split. We cannot pick any feature and any threshold, because the final tree will be completely unbalanced and very deep. Our goal is to find the optimal selection tuple at each node considering the final goal, which is classification into discrete categories (the process is almost identical for regressions). The technique is very similar to a problem based on a cost function that must be minimized, but, in this case, we operate locally, applying an impurity measure proportional to the heterogeneity of a node. A high impurity indicates that samples belonging to many different categories are present, while an impurity equal to 0 indicates that a single category is present. As we need to continue splitting until a pure leaf appears, the optimal choice is based on a function that scores each selection tuple, allowing us to select the one that yields the lowest impurity (theoretically, the process should continue until all the leaves are pure, but normally a maximum depth is provided, so as to avoid excessive complexity). If there are p classes, the category set can be defined as follows:</p>
<p style="padding-left: 150px"><img src="assets/f687377f-feb0-482c-88b0-290fcb4a533f.png" style="width:25.75em;height:1.83em;"/></p>
<p>A very common impurity measure is called <strong>Gini impurity</strong> and it's based on the probability of a misclassification if a sample is categorized using a label randomly chosen from the node subset distribution. Intuitively, if all the samples belong to the same category, any random choice leads to a correct classification (and the impurity becomes <em>0</em>). On the other side, if the node contains samples from many categories, the probability of a misclassification increases. Formally, the measure is defined as follows:</p>
<p style="padding-left: 150px"><img src="assets/420ffd2d-0948-4cc2-99fe-6b3a5e64f1b1.png" style="width:20.42em;height:4.33em;"/></p>
<p class="mce-root">The subset is indicated by <em>Xk</em> and <em>p(j|k)</em> is obtained as the ratio of the samples belonging to the class <em>j</em> over the total number of samples. The selection tuple must be chosen so as to minimize the Gini impurity of the children. Another common approach is the cross-entropy impurity, defined as follows:</p>
<div style="padding-left: 120px" class="mce-root"><img src="assets/95e8f999-d39c-40ee-ad7e-405a812b0875.png" style="width:24.25em;height:4.17em;"/></div>
<p class="mce-root">The main difference between this measure and the previous one is provided by some fundamental information theory concepts. In particular, the goal we want to reach is the minimization of the uncertainty, which is measured using the <em>(Cross-)Entropy</em>. If we have a discrete distribution and all the samples belong to the same category, a random choice is can fully describe the distribution; therefore, the uncertainty is null. On the contrary, if, for example, we have a fair die, the probability of each outcome is 1/6 and the corresponding entropy is about 2.58 bits (if the base of the logarithm is 2). When the nodes become purer and purer, the cross-entropy impurity decreases and reaches 0 in an optimal scenario. Moreover, adopting the concept of mutual information, we can define the information gain obtained after a split has been performed:</p>
<p style="padding-left: 120px"><img src="assets/bcec7570-adf4-445f-b1db-51ee487b1e0f.png" style="width:25.83em;height:1.67em;"/></p>
<p class="mce-root">Given a node, we want to create two children to maximize the information gain. In other words, by choosing the cross-entropy impurity we implicitly grow the tree until the information gain becomes null. Considering again the example of a fair die, we need 2.58 bits of information to decide which is the right outcome. If, instead, the die is loaded and the probability of an outcome is 1.0, we need no information to make a decision. In a decision tree, we'd like to resemble this situation, so that, when a new sample has completely traversed the tree, we don't need any further information to classify it. If a maximum depth is imposed, the final information gain cannot be null. This means that we need to pay an extra cost to finalize the classification. This cost is proportional to the residual uncertainty and should be minimized to increase the accuracy.</p>
<p>Other methods can also be employed (even if Gini and cross-entropy are the most common) and I invite the reader to check the references for further information. However, at this point, a consideration naturally arises. Decision trees are simple models (they are not weak learners!), but the procedure for building them is more complex than, for example, training a logistic regression or a SVM. Why are they so popular? One reason is already clear—they represent a structural process that can be shown using a diagram; however, this is not enough to justify their usage. Two important properties allow the employment of decision trees without any data preprocessing.</p>
<p>In fact, it's easy to understand that, contrary to other methods, there's no need for any scaling or whitening and it's possible to use continuous and categorical features at the same time. For example, if in a bidimensional dataset a feature has a variance equal to 1 and the other equal to 100, the majority of classifiers will achieve a low accuracy; therefore, a preprocessing step becomes necessary. In a decision tree, a selection tuple has the same effect also when the ranges are very different. It goes without saying that a split can be easily performed considering also categorical features and there's no need, for example, to use techniques such as one-hot encoding (which is necessary in most cases to avoid generalization errors). However, unfortunately, the separation hypersurface obtained with a decision tree is normally much more complex than the one obtained using other algorithms and this drives to a higher variance with a consequential loss of generalization ability.</p>
<p>To understand the reason, it's possible to imagine a very simple bidimensional dataset made up of two blobs located in the second and fourth quarters. The first set is characterized by <em>(x &lt; 0, y &gt; 0)</em>, but the second one by <em>(x &lt; 0, y &lt; 0)</em>. Let's also suppose that we have a few outliers, but our knowledge about the data generating process is not enough to qualify them as noisy samples (the original distribution can have tails that are extended over the axes; for example, it may be a mixture of two Gaussians). In this scenario, the simplest separation line is a diagonal splitting the plane into two subplanes containing regions belonging also to the first and third quarters. However, this decision can be made only considering both coordinates at the same time. Using a decision tree, we need to split initially, for example, using the first feature and again with the second one. The result is a piece-wise separation line (for example, splitting the plane into the region corresponding to the second quarter and its complement), leading to a very high classification variance. Paradoxically, a better solution can be obtained with an incomplete tree (limiting the process, for example, to a single split) and with the selection of the <em>y</em>-axis as the separation line (this is why it's important to impose a maximum depth), but the price you pay is an increased bias (and a consequently worse accuracy).</p>
<p>Another important element to consider when working with decision trees (and related models) is the maximum depth. It's possible to grow the tree until the all leaves are pure, but sometimes it's preferable to impose a maximum depth (and, consequently, a maximum number of terminal nodes). A maximum depth equal to 1 drives to binary models called <strong>decision stumps</strong>, which don't allow any interaction among the features (they can simply be represented as <em>If... Then</em> conditions). Higher values yield more terminal nodes and allow an increasing interaction among features (it's possible to think about a combination of many <em>If... Then</em> statements together with <kbd>AND</kbd> logical operators). The right value must be tuned considering every single problem and it's important to remember that very deep trees are more prone to overfitting than pruned ones.</p>
<p>In some contexts, it's preferable to achieve a slightly worse accuracy with a higher generalization ability and, in those case, a maximum depth should be imposed. The common tool to determine the best value is always a grid search together with a cross-validation technique.</p>
<p>Random forests provide us with a powerful tool to solve the bias-variance trade-off problem. They were proposed by L. Breiman (in <em>Breiman L.</em>, <em>Random Forests</em>, <em>Machine Learning</em>, <em>45</em>, <em>2001</em>) and their logic is very simple. As already explained in the previous section, the bagging method starts with the choice of the number of weak learners, <em>Nc</em>. The second step is the generation of Nc datasets (called bootstrap samples) <em>D1, D2, ..., DNc</em>:</p>
<p class="mce-root"><img src="assets/7223f375-7714-439a-99f1-c5927a22dd01.png" style="width:46.17em;height:2.17em;"/></p>
<p>Each decision tree is trained using the corresponding dataset using a common impurity criterion; however, in a random forest, in order to reduce the variance, the selection splits are not computed considering all the features, but only via a random subset containing a quite smaller number of features (common choices are the rounded square root, log2 or natural logarithm). This approach indeed weakens each learner, as the optimality is partially lost, but allows us to obtain a drastic variance reduction by limiting the over-specialization. At the same time, a bias reduction and an increased accuracy are a result of the ensemble (in particular for a large number of estimators). In fact, as the learners are trained with slightly different data distributions, the average of a prediction converges to the right value when <em>Nc → ∞</em> (in practice, it's not always necessary to employ a very large number of decision trees, however, the correct value must be tuned using a grid search with cross-validation). Once all the models, represented with a function <em>di(x)</em>, have been trained, the final prediction can be obtained as an average:</p>
<p style="padding-left: 210px"><img src="assets/3a7ab209-96a9-4ae5-bd03-3328a219e30f.png" style="width:11.58em;height:4.83em;"/></p>
<p class="mce-root">Alternatively, it's possible to employ a majority vote (but only for classifications):</p>
<p style="padding-left: 180px"><img src="assets/1db39b66-3c55-486d-946e-99d8a17d431f.png" style="width:15.75em;height:2.00em;"/></p>
<p>These two methods are very similar and, in most cases, they yield the same result. However, averaging is more robust and allows an improved flexibility when the samples are almost on the boundaries. Moreover, it can be used for both classification and regression tasks.</p>
<p>Random forests limit their randomness by picking the best selection tuple from a smaller sample subset. In some cases, for example, when the number of features is not very large, this strategy drives to a minimum variance reduction and the computational cost is no longer justified by the result. It's possible to achieve better performances with a variant called extra-randomized trees (or simply extra-trees). The procedure is almost the same; however, in this case, before performing a split, <em>n</em> random thresholds are computed (for each feature) and the one which leads to the least impurity is chosen. This approach further weakens the learners but, at the same time, reduces residual variance and prevents overfitting. The dynamic is not very different from many techniques such as regularization or dropout (we're going to discuss this approach in the next chapter); in fact, the extra-randomness reduces the capacity of the model, forcing it to a more linearized solution (which is clearly sub-optimal). The price to pay for this limitation is a consequent bias worsening, which, however, is compensated by the presence of many different learners. Even with random splits, when <em>Nc</em> is large enough, the probability of a wrong classification (or regression prediction) becomes smaller and smaller because both the average and the majority vote tend to compensate the outcome of trees whose structure is strongly sub-optimal in particular regions. This result is easier to obtain, in particular, when the number of training samples is large. In this case, in fact, sampling with replacement leads to slightly different distributions that could be considered (even if this is not formally correct) as partially and randomly boosted. Therefore, every weak learner will implicitly focus on the whole dataset with extra-attention to a smaller subset that, however, is randomly selected (differently from actual boosting).</p>
<p>The complete random forest algorithm is as follows:</p>
<ol>
<li>Set the number of decision trees <em>Nc</em></li>
<li>For <em>i=1</em> to <em>Nc</em>:
<ol>
<li>Create a dataset <em>Di</em> sampling with replacements from the original dataset <em>X</em></li>
</ol>
</li>
</ol>
<ol start="3">
<li>Set the number of features to consider during each split <em>Nf</em> (for example, <em>sqrt(n)</em>)</li>
<li>Set an impurity measure (for example, Gini impurity)</li>
<li>Define an optional maximum depth for each tree</li>
<li>For <em>i=1</em> to <em>Nc</em>:
<ol>
<li>Random forest:
<ol>
<li>Train the decision tree <em>di(x)</em> using the dataset <em>Di</em> and selecting the best split among <em>Nf</em> features randomly sampled</li>
</ol>
</li>
<li>Extra-trees:
<ol>
<li>Train the decision tree <em>di(x)</em> using the dataset <em>Di</em>, computing before each split <em>n</em> random thresholds and selecting the one that yields the least impurity</li>
</ol>
</li>
</ol>
</li>
<li>Define an output function averaging the single outputs or employing a majority vote</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of random forest with Scikit-Learn</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this example, we are going to use the famous Wine dataset (178 13-dimensional samples split into three classes) that is directly available in Scikit-Learn. Unfortunately, it's not so easy to find good and simple datasets for ensemble learning algorithms, as they are normally employed with large and complex sets that require too long a computational time. As the Wine dataset is not particularly complex, the first step is to assess the performances of different classifiers (logistic regression, decision tree, and polynomial SVM) using a k-fold cross-validation:</p>
<pre class="mce-root">import numpy as np<br/><br/>from sklearn.datasets import load_wine<br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.svm import SVC<br/><br/>X, Y = load_wine(return_X_y=True)<br/><br/>lr = LogisticRegression(max_iter=1000, random_state=1000)<br/>print(np.mean(cross_val_score(lr, X, Y, cv=10)))<br/>0.956432748538<br/><br/>dt = DecisionTreeClassifier(criterion='entropy', random_state=1000)<br/>print(np.mean(cross_val_score(dt, X, Y, cv=10)))<br/>0.933298933609<br/><br/>svm = SVC(kernel='poly', random_state=1000)<br/>print(np.mean(cross_val_score(svm, X, Y, cv=10)))<br/>0.961403508772</pre>
<p>As expected, the performances are quite good, with a top value of average cross-validation accuracy equal to about 96% achieved by the polynomial (the default degree is 3) SVM. A very interesting element is the performance of the decision tree, the worst of the set (with Gini impurity it's lower). Even if it's not correct, we can define this model as the weakest of the group and it's a perfect candidate for our bagging test. We can now fit a Random Forest by instantiating the class <kbd>RandomForestClassifier</kbd> and selecting <kbd>n_estimators=50</kbd> (I invite the reader to try different values):</p>
<pre>from multiprocessing import cpu_count<br/><br/>from sklearn.ensemble import RandomForestClassifier<br/><br/>rf = RandomForestClassifier(n_estimators=50, n_jobs=cpu_count(), random_state=1000)<br/>print(np.mean(cross_val_score(rf, X, Y, cv=10)))<br/>0.983333333333</pre>
<p>As expected, the average cross-validation accuracy is the highest, about 98.3%. Therefore, the random forest has successfully found a global configuration of decision trees, so as to specialize them in almost any region of the sample space. The parameter <kbd>n_jobs=cpu_count()</kbd> tells Scikit-Learn to parallelize the training process using all of the CPU cores available in the machine.</p>
<p>To better understand the dynamics of this model, it's useful to plot the cross-validation accuracy as a function of the number of trees:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/458c8030-ceab-46d5-89e5-8183299e051d.png" style="width:44.17em;height:20.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Cross-validation accuracy of a random forest as a function of the number of trees</div>
<p>It's not surprising to observe some oscillations and a plateau when the number of trees becomes greater at about 320. The effect of the randomness can cause a performance loss, even increasing the number of learners. In fact, even if the training accuracy grows, the validation accuracy on different folds can be affected by an over-specialization. Moreover, in this case, it's very interesting to notice that the top accuracy is achievable with 50 trees instead of 400 or more. For this reason, I always suggest performing at least a grid search, in order not only to achieve the best accuracy but also to minimize the complexity of the model.</p>
<p>Another important element to consider when working with decision trees and random forests is feature importance (also called Gini importance when this criterion is chosen), which is a measure proportional to the impurity reduction that a particular feature allows us achieve. For a decision tree, it is defined as follows:</p>
<p style="padding-left: 150px"><img src="assets/a8f75f6a-5542-415c-91c0-43f2cd907dd6.png" style="width:22.08em;height:4.50em;"/></p>
<p class="mce-root">In the previous formula, <em>n(j)</em> denotes the number of samples reaching the node <em>j</em> (the sum must be extended to all nodes where the feature is chosen) and <em>ΔIi</em> is the impurity reduction achieved at node <em>j</em> after splitting using the feature <em>i</em>. In a random forest, the importance must be computed by averaging over all trees:</p>
<p style="padding-left: 120px"><img src="assets/fc2e83bd-307a-45cc-a990-135ad97e7518.png" style="width:27.25em;height:5.00em;"/></p>
<p>After fitting a model (decision tree or random forest), Scikit-Learn outputs the feature importance vector in the <kbd>feature_importances_ instance</kbd> variable. In the following graph, there's a plot showing the importance of each feature (the labels can be obtained with the command <kbd>load_wine()['feature_names'])</kbd> in descending order:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/22a4f588-560e-44c4-af5e-1dcdee9ed168.png" style="width:72.42em;height:48.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Feature importances for Wine dataset</div>
<p>We don't want to analyze the chemical meaning of each element, but it's clear that, for example, the presence of proline and the color intensity are much more important than the presence of non-flavonoid phenols. As the model is working with features that are semantically independent (it's not the same for the pixels of an image), it's possible to reduce the dimensionality of a dataset by removing all those features whose importance doesn't have a high impact on the final accuracy. This process, called <strong>feature selection</strong>, should be performed using more complex statistical techniques, such as Chi-squared, but when a classifier is able to produce an importance index, it's also possible to use a Scikit-Learn class called <kbd>SelectFromModel</kbd>. Passing an estimator (that can be fitted or not) and a threshold, it's possible to transform the dataset by filtering out all the features whose value is below the threshold. Applying it to our model and setting a minimum importance equal to <kbd>0.02</kbd>, we get the following:</p>
<pre>from sklearn.feature_selection import SelectFromModel<br/><br/>sfm = SelectFromModel(estimator=rf, prefit=True, threshold=0.02)<br/>X_sfm = sfm.transform(X)<br/><br/>print(X_sfm.shape)<br/>(178, 10)</pre>
<p>The new dataset now contains 10 features instead of the 13 of the original Wine dataset (for example., it's easy to verify that ash and non-flavonoid phenols have been removed). Of course, as for any other dimensionality reduction method, it's always suggested you verify the final accuracy with a cross-validation and make decisions only if the trade-off between loss of accuracy and complexity reduction is reasonable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AdaBoost</h1>
                </header>
            
            <article>
                
<p>In the previous section, we have seen that sampling with a replacement leads to datasets where the samples are randomly reweighted. However, if <em>M</em> is very large, most of the samples will appear only once and, moreover, all the choices are totally random. AdaBoost is an algorithm proposed by Schapire and Freund that tries to maximize the efficiency of each weak learner by employing adaptive boosting (the name derives from this). In particular, the ensemble is grown sequentially and the data distribution is recomputed at each step so as to increase the weight of those samples that were misclassified and reduce the weight of the ones that were correctly classified. In this way, every new learner is forced to focus on those regions that were more problematic for the previous estimators. The reader can immediately understand that, contrary to random forests and other bagging methods, boosting doesn't rely on randomness to reduce the variance and improve the accuracy. Rather, it works in a deterministic way and each new data distribution is chosen with a precise goal. In this paragraph, we are going to consider a variant called <strong>Discrete AdaBoost</strong> (formally <em>AdaBoost.M1</em>), which needs a classifier whose output is thresholded (for example, <em>-1</em> and <em>1</em>). However, real-valued versions (whose output behaves like a probability) have been developed (a classical example is shown in <em>Additive Logistic Regression: a Statistical View of Boosting</em>,<em> <span>Friedman J.</span></em>, <em><span>Hastie T.</span></em>, <em><span>Tibshirani R.</span></em>,<em><span> </span>Annals of Statistics</em>, 28/1998). As the main concepts are always the same, the reader interested in the theoretical details of other variants can immediately find them in the referenced papers.</p>
<p>For simplicity, the training dataset of <strong>AdaBoost.M1</strong> is defined as follow:</p>
<p style="padding-left: 90px"><img src="assets/9cca3c68-157b-4ca9-ab53-baeee3d284f2.png" style="width:27.92em;height:3.92em;"/></p>
<p>This choice is not a limitation because, in multi-class problems, a one-versus-the-rest strategy can be easily employed, even if algorithms like <strong>AdaBoost.SAMME</strong> guarantee a much better performance. In order to manipulate the data distribution, we need to define a weight set:</p>
<p style="padding-left: 60px"><img src="assets/532490e8-b784-4762-ac4c-74c24c91f4db.png" style="width:37.50em;height:5.67em;"/></p>
<p>The weight set allows defining an implicit data distribution <em>D(t)(x)</em>, which initially is equivalent to the original one but that can be easily reshaped by changing the values <em>wi</em>. Once the family and the number of estimators, <em>Nc</em>, have been chosen, it's possible to start the global training process. The algorithm can be applied to any kind of learner that is able to produce thresholded estimations (while the real-valued variants can work with probabilities, for example, obtained through the Platt scaling method).</p>
<p>The first instance <em>d1(x)</em> is trained with the original dataset, which means with the data distribution <em>D(1)(x)</em>. The next instances, instead, are trained with the reweighted distributions <em>D(2)(x), D(3)(x), ..., D(Nc)(x)</em>. In order to compute them, after each training process, the normalized weighted error sum is computed:</p>
<p style="padding-left: 180px"><img src="assets/95ea5305-93ea-4421-a85a-25ad479a6bc2.png" style="width:12.92em;height:4.75em;"/></p>
<p>This value is bounded between <em>0</em> (no misclassifications) and <em>1</em> (all samples have been misclassified) and it's employed to compute the estimator weight <em>α(t)</em>:</p>
<p style="padding-left: 180px"><img src="assets/15597875-3154-412d-a332-0d145c51c7ce.png" style="width:14.25em;height:4.17em;"/></p>
<p>To understand how this function works, it's useful to consider its plot (shown in the following diagram):</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/3267ea61-5b68-4668-b290-b59ca43cd89d.png" style="width:46.08em;height:28.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Estimator weight plot as a function of the normalized weighted error sum</div>
<p>This diagram unveils an implicit assumption: the worst classifier is not the one that misclassifies all samples (<em>ε(t) = 1</em>), but a totally random binary guess (corresponding to <em>ε(t) = 0.5</em>). In this case, <em>α(t)</em> is null and, therefore, the outcome if the estimator is completely discarded. When <em>ε(t) &lt; 0.5</em>, a boosting is applied (between about 0.05 and 0.5, the trend is almost linear), but it becomes greater than 1 only when <em>ε(t)</em> &lt; about 0.25 (larger values drive to a penalty because the weight is smaller than 1). This value is a threshold to qualify an estimator as trusted or very strong and <em>α(t) → +∞</em> in the particular case of a perfect estimator (no errors).</p>
<p>In practice, an upper bound should be imposed in order to avoid overflows or divisions by zero. Instead, when <em>ε(t) &gt; 0.5</em>, the estimator is unacceptably weak, because it's worse than a random guess and the resulting boosting would be negative. To avoid this problem, real implementations must invert the output of such estimators, transforming them de facto into learners with <em>ε(t) &lt; 0.5</em> (this is not an issue, as the transformation is applied to all output values in the same way). It's important to consider that this algorithm shouldn't be directly applied to multi-class scenarios because, as pointed out in <em>Multi-class AdaBoost</em>,<em> <span>Zhu J.</span></em>, <em><span>Rosset S.</span></em>, <em><span>Zou H.</span></em>, <em><span>Hastie T.</span></em>,<em><span> </span>01/2006</em>, the threshold 0.5 corresponds to a random guess accuracy only for binary choices. When the number of classes is larger than two, a random estimator outputs a class with a probability <em>1/Ny</em> (where <em>Ny</em> is the number of classes) and, therefore, AdaBoost.M1 will boost the classifiers in a wrong way, yielding poor final accuracies (the real threshold should be <em>1 - 1/Ny</em>, which is larger than 0.5 when <em>Ny &gt; 2</em>). The AdaBoost.SAMME algorithm (implemented by Scikit-Learn) has been proposed to solve this problem and exploit the power of boosting also in multi-class scenarios.</p>
<p>The global decision function is defined as follows:</p>
<p style="padding-left: 180px"><img src="assets/621e3cfe-d929-4bf7-93c0-31b9cb09cf09.png" style="width:18.92em;height:4.92em;"/></p>
<p class="mce-root">In this way, as the estimators are added sequentially, the importance of each of them will decrease while the accuracy of <em>di(x)</em> increases. However, it's also possible to observe a plateau if the complexity of <em>X</em> is very high. In this case, many learners will have a high weight, because the final prediction must take into account a sub-combination of learners in order to achieve an acceptable accuracy. As this algorithm specializes the learners at each step, a good practice is to start with a small number of estimators (for example, 10 or 20) and increase the number until no improvement is achieved. Sometimes, a minimum number of good learners (like SVM or decision trees) is sufficient to reach the highest possible accuracy (limited to this kind of algorithm), but in some other cases, the number of estimators can be some thousands. Grid search and cross-validation are again the only good strategies to make the right choice.</p>
<p>After each training step it is necessary to update the weights in order to produce a boosted distribution. This is achieved using an exponential function (based on bipolar outputs <em>{-1, 1}</em>):</p>
<p style="padding-left: 60px"><img src="assets/affbe3ea-ac1a-433d-8490-f2260c52d34b.png" style="width:35.67em;height:4.17em;"/></p>
<p class="mce-root">Given a sample <em>x<sub>i</sub></em>, if it has been misclassified, its weight will be increased considering the overall estimator weight. This approach allows a further adaptive behavior because a classifier with a high <em>α(t)</em> is already very accurate and it's necessary a higher attention level to focus only on the (few) misclassified samples. On the contrary, if <em>α(t)</em> is small, the estimator must improve its overall performance and the over-weighting process must be applied to a large subset (therefore, the distribution won't peak around a few samples, but will penalize only the small subset that has been correctly classified, leaving the estimator free to explore the remaining space with the same probability). Even if not present in the original proposal, it's also possible to include a learning rate <em>η</em> that multiplies the exponent:</p>
<p style="padding-left: 210px"><img src="assets/065c5ca6-a718-4d26-94df-93c1bc6ba57d.png" style="width:12.50em;height:2.33em;"/></p>
<p>A value <em>η = 1</em> has no effect, while smaller values have been proven to increase the accuracy by avoiding a premature specialization. Of course, when <em>η &lt;&lt; 1</em>, the number of estimators must be increased in order to compensate the minor reweighting and this can drive to a training performance loss. As for the other hyperparameters, the right value for <em>η</em> must be discovered using a cross-validation technique (alternatively, if it's the only value that must be fine-tuned, it's possible to start with one and proceed by decreasing its value until the maximum accuracy has been reached).</p>
<p>The complete AdaBoost.M1 algorithm is as follows:</p>
<ol>
<li>Set the family and the number of estimators <em>Nc</em></li>
<li>Set the initial weights <em>W(1)</em> equal to <em>1/M</em></li>
<li>Set the learning rate <em>η</em> (for example, <em>η = 1</em>)</li>
<li>Set the initial distribution <em>D(1)</em> equal to the dataset <em>X</em></li>
</ol>
<ol start="5">
<li>For <em>i=1</em> to <em>Nc</em>:
<ol>
<li>Train the <em>i<sup>th</sup></em> estimator <em>di(x)</em> with the data distribution <em>D(i)</em></li>
<li>Compute the normalized weighted error sum <em>ε(i)</em>:
<ol>
<li>If <em>ε(i) &gt; 0.5</em>, invert all estimator outputs</li>
</ol>
</li>
<li>Compute the estimator weight <em>α(i)</em></li>
<li>Update the weights using the exponential formula (with or without the learning rate)</li>
<li>Normalize the weights</li>
</ol>
</li>
<li>Create the global estimator applying the sign(•) function to the weighted sum <em>α(i)di(x)</em> (for <em>i=1</em> to <em>Nc</em>)</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AdaBoost.SAMME</h1>
                </header>
            
            <article>
                
<p>This variant, called <strong>Stagewise Additive Modeling using a Multi-class Exponential loss</strong> (<strong>SAMME</strong>), was proposed by Zhu, Rosset, Zou, and Hastie in <em>Multi-class AdaBoost</em>,<em> <span>Zhu J.</span></em>, <em><span>Rosset S.</span></em>, <em><span>Zou H.</span></em>, <em><span>Hastie T.</span></em>,<em><span> </span>01/2006</em>. The goal is to adapt AdaBoost.M1 in order to work properly in multi-class scenarios. As this is a discrete version, its structure is almost the same, with a difference in the estimator weight computation. Let's consider a label dataset, <em>Y</em>:</p>
<p style="padding-left: 150px"><img src="assets/93ea80f7-59b0-413b-9c59-1b65285610d9.png" style="width:24.67em;height:1.83em;"/></p>
<p>Now, there are <em>p</em> different classes and it's necessary to consider that a random guess estimator cannot reach an accuracy equal to 0.5; therefore, the new estimator weights are computed as follows:</p>
<p style="padding-left: 60px"><img src="assets/b32c95f1-abcb-4c3d-b36c-cfba45033445.png" style="width:37.25em;height:4.50em;"/></p>
<p>In this way, the threshold is pushed forward and <em>α(t)</em> will be zero when the following is true:</p>
<p style="padding-left: 210px"><img src="assets/c30ed323-a649-4852-b175-540e93085a9b.png" style="width:8.08em;height:3.67em;"/> </p>
<p class="mce-root">The following graph shows the plot of <em>α(t)</em> with <em>p = 10</em>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/7dbe4459-d05f-4e36-a488-70cf4fb0ca60.png" style="width:41.83em;height:25.25em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Estimator weight plot as a function of the normalized weighted error sum when p = 10</div>
<p>Employing this correction, the boosting process can successfully cope with multi-class problems without the bias normally introduced by AdaBoost.M1 when <em>p &gt; 2 (α(t) &gt; 0</em> when the error is less than an actual random guess, which is a function of the number of classes). As the performance of this algorithm is clearly superior, the majority of AdaBoost implementations aren't based on the original algorithm anymore (as already mentioned, for example, Scikit-Learn implements AdaBoost.SAMME and the real-valued version AdaBoost.SAMME.R). Of course, when <em>p = 2</em>, AdaBoost.SAMME is exactly equivalent to AdaBoost.M1.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AdaBoost.SAMME.R</h1>
                </header>
            
            <article>
                
<p>AdaBoost.SAMME.R is a variant that works with classifiers that can output prediction probabilities. This is normally possible employing techniques such as Platt scaling, but it's important to check whether a specific classifier implementation is able to output the probabilities without any further action. For example, SVM implementations provided by Scikit-Learn don't compute the probabilities unless the parameter <kbd>probability=True</kbd> (because they require an extra step that could be useless in some cases).</p>
<p>In this case, we assume that the output of each classifier is a probability vector:</p>
<p style="padding-left: 30px"><img src="assets/f489c04a-b017-42e0-9615-0d5b81d71d07.png" style="width:52.17em;height:3.08em;"/></p>
<p class="mce-root">Each component is the conditional probability that the <em>j<sup>th</sup></em> class is output given the input xi. When working with a single estimator, the winning class is obtained through the argmax(•) function; however, in this case, we want to re-weight each learner so as to obtain a sequentially grown ensemble. The basic idea is the same as AdaBoost.M1, but, as now we manage probability vectors, we also need an estimator weighting function that depends on the single sample <em>xi</em> (this function indeed wraps every single estimator that is now expressed as a probability vectorial function <em>pi(t)(y=i|x))</em>: </p>
<p style="padding-left: 30px"><img src="assets/6d766343-dccd-48c3-8891-3188e83156b0.png" style="width:41.92em;height:4.58em;"/></p>
<p class="mce-root">Considering the properties of logarithms, the previous expression is equivalent to a discrete <em>α(t)</em>; however, in this case, we don't rely on a weighted error sum (the theoretical explanation is rather complex and is beyond the scope of this book. The reader can find it in the aforementioned paper, even if the method presented in the next chapter discloses a fundamental part of the logic). To better understand the behavior of this function, let's consider a simple scenario with <em>p = 2</em>. The first case is a sample that the learner isn't able to classify (<em>p=(0.5, 0.5)</em>):</p>
<p style="padding-left: 90px" class="mce-root"><img src="assets/c1d2fce3-0e5c-4ac9-9955-d38081974cd9.png" style="width:30.67em;height:3.58em;"/></p>
<p>In this case, the uncertainty is maximal and the classifier cannot be trusted for this sample, so the weight becomes null for all output probabilities. Now, let's apply the boosting, obtaining the probability vector <em>p=(0.7, 0.3)</em>:</p>
<p style="padding-left: 60px" class="mce-root"><img src="assets/3f301973-7a85-426d-8907-c2d27deee853.png" style="width:34.33em;height:4.92em;"/></p>
<p class="mce-root">The first class will become positive and its magnitude will increase when <em>p → 1</em>, while the other one is the opposite value. Therefore, the functions are symmetric and allow working with a sum:</p>
<div style="padding-left: 180px"><img src="assets/cc89b32f-6c44-49fc-9ac7-65811bb4cb08.png" style="width:18.17em;height:4.83em;"/></div>
<p>This approach is very similar to a weighted majority vote because the winning class <em>yi</em> is computed taking into account not only the number of estimators whose output is <em>yi</em> but also their relative weight and the negative weight of the remaining classifiers. A class can be selected only if the strongest classifiers predicted it and the impact of the other learners is not sufficient to overturn this result.</p>
<p>In order to update the weights, we need to consider the impact of all probabilities. In particular, we want to reduce the uncertainty (which can degenerate to a purely random guess) and force a superior attention focused on all those samples that have been misclassified. To achieve this goal, we need to define the <em>yi</em> and <em>p(t)(xi)</em> vectors, which contain, respectively, the one-hot encoding of the true class (for example, <em>(0, 0, 1, ..., 0)</em>) and the output probabilities yielded by the estimator (as a column vector). Hence, the update rule becomes as follows:</p>
<p style="padding-left: 150px"><img src="assets/bf93290c-bb10-4179-9b2c-268f90f047dc.png" style="width:25.17em;height:3.08em;"/></p>
<p class="mce-root">If, for example, the true vector is (1, 0) and the output probabilities are (0.1, 0.9), with η=1, the weight of the sample will be multiplied by about 3.16. If instead, the output probabilities are (0.9, 0.1), meaning the sample has been successfully classified, the multiplication factor will become closer to 1. In this way, the new data distribution <em>D(t+1)</em>, analogously to AdaBoost.M1, will be more peaked on the samples that need more attention. All implementations include the learning rate as a hyperparameter because, as already explained, the default value equal to 1.0 cannot be the best choice for specific problems. In general, a lower learning rate allows reducing the instability when there are many outliers and improves the generalization ability thanks to a slower convergence towards the optimum. When <em>η &lt; 1</em>, every new distribution is slightly more focused on the misclassified samples, allowing the estimators to search for a better parameter set without big jumps (that can lead the estimator to skip an optimal point). However, contrary to Neural Networks that normally work with small batches, AdaBoost can often perform quite well also with <em>η=1</em> because the correction is applied only after a full training step. As usual, I recommend performing a grid search to select the right values for each specific problem.</p>
<p>The complete AdaBoost.SAMME.R algorithm is as follows:</p>
<ol>
<li>Set the family and the number of estimators <em>Nc</em></li>
<li>Set the initial weights <em>W(1)</em> equal to <em>1/M</em></li>
<li>Set the learning rate <em>η</em> (for example, <em>η = 1</em>)</li>
<li>Set the initial distribution <em>D(1)</em> equal to the dataset <em>X</em></li>
<li>For <em>i=1</em> to <em>Nc</em>:
<ol>
<li>Train the <em>i<sup>th</sup></em> estimator <em>di(x)</em> with the data distribution <em>D(i)</em></li>
<li>Compute the output probabilities for each class and each training sample</li>
<li>Compute the estimator weights <em>αj(i)</em></li>
<li>Update the weights using the exponential formula (with or without the learning rate)</li>
<li>Normalize the weights</li>
</ol>
</li>
<li>Create the global estimator applying the argmax(•) function to the sum <em>αj(i)</em> (for <em>i=1</em> to <em>Nc</em>)</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AdaBoost.R2</h1>
                </header>
            
            <article>
                
<p>A slightly more complex variant has been proposed by Drucker (in <em>Improving Regressors using Boosting Techniques</em>,<em> <span>Drucker H.</span></em>,<em><span> </span>ICML 1997</em>) to manage regression problems. The weak learners are commonly decision trees and the main concepts are very similar to the other variants (in particular, the re-weighting process applied to the training dataset). The real difference is the strategy adopted in order to choose the final prediction <em>yi</em> given the input sample <em>xi</em>. Assuming that there are Nc estimators and each of them is represented as function <em>dt(x)</em>, we can compute the absolute residual <em>ri(t)</em> for every input sample:</p>
<p style="padding-left: 210px"><img src="assets/ebdc8cc2-5fe8-4a61-92bb-27b556ce0f13.png" style="width:12.17em;height:2.33em;"/></p>
<p>Once the set <em>Ri</em> containing all the absolute residuals has been populated, we can compute the quantity <em>Sr = sup Ri</em> and compute the values of a cost function that must be proportional to the error. The common choice that is normally implemented (and suggested by the author himself) is a linear loss:</p>
<p style="padding-left: 210px" class="mce-root"><img src="assets/3ef3a9e6-36f9-47f8-b553-0aeb08f9e2fb.png" style="width:7.17em;height:4.42em;"/></p>
<p class="mce-root">This loss is very flat and it's directly proportional to the error. In most cases, it's a good choice because it avoids premature over-specialization and allows the estimators to readapt their structure in a gentler way. The most obvious alternative is the square loss, which starts giving more importance to those samples whose prediction error is larger. It is defined as follows:</p>
<p style="padding-left: 210px" class="mce-root"> <img src="assets/9257d79a-f66f-45c2-a3df-1c03dcb51383.png" style="width:7.67em;height:4.75em;"/></p>
<p>The last cost function is strictly related to AdaBoost.M1 and it's exponential:</p>
<p style="padding-left: 210px"><img src="assets/e10feb45-b692-4c0d-aa25-ccb2a6ba9ca1.png" style="width:8.08em;height:3.42em;"/></p>
<p>This is normally a less robust choice because, as we are also going to discuss in the next section, it penalizes small errors in favor of larger ones. Considering that these functions are also employed in the re-weighting process, an exponential loss can force the distribution to assign very high probabilities to samples whose misclassification error is high, driving the estimators to become over-specialized with effect from the first iterations. In many cases (such as in neural networks), the loss functions are normally chosen according to their specific properties but, above all, to the ease to minimize them. In this particular scenario, loss functions are a fundamental part of the boosting process and they must be chosen considering the impact on the data distribution. Testing and cross-validation provide the best tool to make a reasonable decision.</p>
<p>Once the loss function has been evaluated for all training samples, it's possible to build the global cost function as the weighted average of all losses. Contrary to many algorithms that simply sum or average the losses, in this case, it's necessary to consider the structure of the distribution. As the boosting process reweights the samples, also the corresponding loss values must be filtered to avoid a bias. At the iteration <em>t</em>, the cost function is computed as follows:</p>
<p style="padding-left: 180px"><img src="assets/b7adb16a-f916-4e5e-916d-bb2b39af3039.png" style="width:16.08em;height:5.33em;"/></p>
<p>This function is proportional to the weighted errors, which can be either linearly filtered or emphasized using a quadratic or exponential function. However, in all cases, a sample whose weight is lower will yield a smaller contribution, letting the algorithm focus on the samples more difficult to be predicted. Consider that, in this case, we are working with classifications; therefore, the only measure we can exploit is the loss. Good samples yield low losses, hard samples yield proportionally higher losses. Even if it's possible to use <em>C(t)</em> directly, it's preferable to define a confidence measure:</p>
<p style="padding-left: 210px"><img src="assets/4f8193e2-f1a9-48c6-9d13-bb99d2ad7077.png" style="width:9.08em;height:3.58em;"/></p>
<p>This index is inversely proportional to the average confidence at the iteration <em>t</em>. In fact, when <em>C(t) → 0</em>, <em>γ(t) → 0</em> and when <em>C(t) → ∞</em>, <em>γ(t) → 1</em>. The weight update is performed considering the overall confidence and the specific loss value:</p>
<p style="padding-left: 210px"><img src="assets/3958f502-bacd-4402-a368-9a88e4802cab.png" style="width:12.50em;height:2.92em;"/></p>
<p>A weight will be decreased proportionally to the loss associated with the corresponding absolute residual. However, instead of using a fixed base, the global confidence index is chosen. This strategy allows a further degree of adaptability, because an estimator with a low confidence doesn't need to focus only on a small subset and, considering that <em>γ(t)</em> is bounded between 0 and 1 (worst condition), the exponential becomes ineffective when the cost function is very high (1x = 1), so that the weights remain unchanged. This procedure is not very dissimilar to the one adopted in other variants, but it tries to find a trade-off between global accuracy and local misclassification problems, providing an extra degree of robustness.</p>
<p class="mce-root">The most complex part of this algorithm is the approach employed to output a global prediction. Contrary to classification algorithms, we cannot easily compute an average, because it's necessary to consider the global confidence at each iteration. Drucker proposed a method based on the weighted median of all outputs. In particular, given a sample xi, we define the set of predictions:</p>
<p style="padding-left: 180px"><img src="assets/dacfd8db-452f-45d5-88ae-f1edff8371b0.png" style="width:16.75em;height:3.00em;"/></p>
<p>As weights, we consider the <em>log(1 / γ(t))</em>, so we can define a weight set:</p>
<p style="padding-left: 120px"><img src="assets/8dc09994-0ed6-4fd1-be70-75bbb1f51c47.png" style="width:27.08em;height:2.92em;"/></p>
<p class="mce-root">The final output is the median of <em>Y</em> weighted according to <em>Γ</em> (normalized so that the sum is 1.0). As <em>γ(t) → 1</em> when the confidence is low, the corresponding weight will tend to 0. In the same way, when the confidence is high (close to 1.0), the weight will increase proportionally and the chance to pick the output associated with it will be higher. For example, if the outputs are <em>Y = {1, 1.2, 1.3, 2.0, 2.2, 2.5, 2.6}</em> and the weights are <em>Γ = { 0.35, 0.15, 0.12, 0.11, 0.1, 0.09, 0.08 }</em>, the weighted median corresponds to the second index, therefore the global estimator will output 1.2 (which is, also intuitively, the most reasonable choice).</p>
<p class="mce-root">The procedure to find the median is quite simple:</p>
<ol>
<li>The <em>yi(t)</em> must be sorted in ascending order, so that <em>yi(1) &lt; yi(2) &lt; ... &lt; yi(Nc)</em></li>
<li>The set <em>Γ</em> is sorted accordingly to the index of <em>yi(t)</em> (each output <em>yi(t)</em> must carry its own weight)</li>
<li>The set <em>Γ</em> is normalized, dividing it by its sum</li>
<li>The index corresponding to the smallest element that splits <em>Γ</em> into two blocks (whose sums are less than or equal to 0.5) is selected</li>
<li>The output corresponding to this index is chosen</li>
</ol>
<p>The complete AdaBoost.R2 algorithm is as follows:</p>
<ol>
<li>Set the family and the number of estimators <em>Nc</em></li>
<li>Set the initial weights <em>W(1)</em> equal to <em>1/M</em></li>
<li>Set the initial distribution <em>D(1)</em> equal to the dataset <em>X</em></li>
<li>Select a loss function <em>L</em></li>
<li>For <em>i=1</em> to <em>Nc</em>:
<ol>
<li>Train the <em>i<sup>th</sup></em> estimator <em>di(x)</em> with the data distribution <em>D(i)</em></li>
<li>Compute the absolute residuals, the loss values, and the confidence measures</li>
<li>Compute the global cost function</li>
<li>Update the weights using the exponential formula</li>
</ol>
</li>
<li>Create the global estimator using the weighted median</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of AdaBoost with Scikit-Learn</h1>
                </header>
            
            <article>
                
<p>Let's continue using the Wine dataset in order to analyze the performance of AdaBoost with different parameters. Scikit-Learn, like almost all algorithms, implements both a classifier <kbd>AdaBoostClassfier</kbd> (based on the algorithm SAMME and SAMME.R) and a regressor <kbd>AdaBoostRegressor</kbd> (based on the algorithm R2). In this case, we are going to use the classifier, but I invite the reader to test the regressor using a custom dataset or one of the built-in toy datasets. In both classes, the most important parameters are <kbd>n_estimators</kbd> and <kbd>learning_rate</kbd> (default value set to <kbd>1.0</kbd>). The default underlying weak learner is always a decision tree, but it's possible to employ other models creating a base instance and passing it through the parameter <kbd>base_estimator</kbd>. As explained in the chapter, real-valued AdaBoost algorithms require an output based on a probability vector. In Scikit-Learn, some classifiers/regressors (such as SVM) don't compute the probabilities unless it is explicitly required (setting the parameter <kbd>probability=True</kbd>); therefore, if an exception is raised, I invite you to check the documentation in order to learn how to force the algorithm to compute them.</p>
<p>The examples we are going to discuss have only a didactic purpose because they focus on a single parameter. In a real-world scenario, it's always better to perform a grid search (which is more expensive), so as to analyze a set of combinations. Let's start analyzing the cross-validation score as a function of the number of estimators (the vectors <em>X</em> and <em>Y</em> are the ones defined in the previous example):</p>
<pre>import numpy as np<br/><br/>from sklearn.ensemble import AdaBoostClassifier<br/>from sklearn.model_selection import cross_val_score <br/><br/>scores_ne = []<br/><br/>for ne in range(10, 201, 10):<br/>    adc = AdaBoostClassifier(n_estimators=ne, learning_rate=0.8, random_state=1000)<br/>    scores_ne.append(np.mean(cross_val_score(adc, X, Y, cv=10)))</pre>
<p>We have considered a range starting from 10 trees and ending with 200 trees with steps of 10 trees. The learning rate is kept constant and equal to 0.8. The resulting plot is shown in the following graph:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/7ac290ef-4dc9-44c8-96e3-90f49af5d90d.png" style="width:61.17em;height:32.58em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">10-fold cross-validation accuracy as a function of the number of estimators</div>
<p>The maximum is reached with 50 estimators. Larger values cause performance worsening due to the over-specialization and a consequent variance increase. As explained also in other chapters, the capacity of a model must be tuned according to the Occam's Razor principle, not only because the resulting model can be faster to train, but also because a capacity excess is normally saturated, overfitting the training set and reducing the scope for generalization. Cross-validation can immediately show this effect, which, instead, can remain hidden when a standard training/test set split is done (above all when the samples are not shuffled).</p>
<p>Let's now check the performance with different learning rates (keeping the number of trees fixed):</p>
<pre>import numpy as np<br/><br/>scores_eta_adc = []<br/><br/>for eta in np.linspace(0.01, 1.0, 100):<br/>    adc = AdaBoostClassifier(n_estimators=50, learning_rate=eta, random_state=1000)<br/>    scores_eta_adc.append(np.mean(cross_val_score(adc, X, Y, cv=10)))</pre>
<p>The final plot is shown in the following graph:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/0341b161-a7b8-4bbe-9fbc-855f635049ab.png" style="width:60.58em;height:32.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">10-fold Cross-validation accuracy as a function of the learning rate (number of estimators = 50)</div>
<p>Again, different learning rates yield different accuracies. The choice of <em>η = 0.8</em> seems to be the most effective, as higher and lower values lead to performance worsening. As explained, the learning rate has a direct impact on the re-weighting process. Very small values require a larger number of estimators because subsequent distributions are very similar. On the other side, large values can lead to a premature over-specialization. Even if the default value is <kbd>1.0</kbd>, I always suggest checking the accuracy also with smaller values. There's no golden rule for picking the right learning rate in every case, but it's important to remember that lower values allow the algorithm to smoothly adapt to fit the training set in a gentler way, while higher values reduce the robustness to outliers, because the samples that have been misclassified are immediately boosted and the probability of sampling them increases very rapidly. The result of this behavior is a constant focus on those samples that may be affected by noise, almost forgetting the structure of the remaining sample space.</p>
<p>The last experiment we want to make is analyzing the performance after a dimensionality reduction performed with <strong>Principal Component Analysis</strong> (<strong>PCA</strong>) and <strong>Factor Analysis</strong> (<strong>FA</strong>) (with 50 estimators and <kbd>η = 0.8</kbd>):</p>
<pre>import numpy as np<br/><br/>from sklearn.decomposition import PCA, FactorAnalysis<br/><br/>scores_pca = []<br/><br/>for i in range(13, 1, -1):<br/>    if i &lt; 12:<br/>        pca = PCA(n_components=i, random_state=1000)<br/>        X_pca = pca.fit_transform(X)<br/>    else:<br/>        X_pca = X<br/>        <br/>    adc = AdaBoostClassifier(n_estimators=50, learning_rate=0.8, random_state=1000)<br/>    scores_pca.append(np.mean(cross_val_score(adc, X_pca, Y, cv=10)))  <br/><br/>scores_fa = []<br/><br/>for i in range(13, 1, -1):<br/>    if i &lt; 12:<br/>        fa = FactorAnalysis(n_components=i, random_state=1000)<br/>        X_fa = fa.fit_transform(X)<br/>    else:<br/>        X_fa = X<br/>        <br/>    adc = AdaBoostClassifier(n_estimators=50, learning_rate=0.8, random_state=1000)<br/>    scores_fa.append(np.mean(cross_val_score(adc, X_fa, Y, cv=10)))</pre>
<p>The resulting plot is shown in the following graph:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/00546deb-0510-444a-b198-e697936e69d2.png" style="width:64.17em;height:34.50em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"> 10-fold cross-validation accuracy as a function of the number of components (PCA and factor analysis)</div>
<p>This exercise confirms some important features analyzed in <a href="8d541a43-8790-4a91-a79b-e48496f75d90.xhtml" target="_blank">Chapter 5</a>, <em>EM Algorithm and Applications</em>. First of all, performances are not dramatically affected even by a 50% dimensionality reduction. This consideration is further confirmed by the feature importance analysis performed in the previous example. Decision trees can perform quite a good classification considering only 6/7 features because the remaining ones offer a marginal contribution to the characterization of a sample. Moreover, FA is almost always superior to PCA. With 7 components, the accuracy achieved using the FA algorithm is higher than 0.95 (very close to the value achieved with no reduction), while a PCA reaches this value with 12 components. The reader should remember that PCA is a particular case of FA, with the assumption of homoscedastic noise. The diagram confirms that this condition is not acceptable with the Wine dataset. Assuming different noise variances allows remodeling the reduced dataset in a more accurate way, minimizing the cross-effect of the missing features. Even if PCA is normally the first choice, with large datasets, I suggest you always compare the performance with a Factor Analysis and choose the technique that guarantees the best result (given also that FA is more expensive in terms of computational complexity). </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient boosting</h1>
                </header>
            
            <article>
                
<p>At this point, we can introduce a more general method of creating boosted ensembles. Let's choose a generic algorithm family, represented as follows:</p>
<p style="padding-left: 210px"><img src="assets/5d3d5c8e-4f1c-4184-9ac3-9a275caf6f87.png" style="width:10.58em;height:1.92em;"/></p>
<p>Each model is parametrized using the vector <em>θi</em> and there are no restrictions on the kind of method that is employed. In this case, we are going to consider decision trees (which is one of the most diffused algorithms when this boosting strategy is employed—in this case, the algorithm is known as gradient tree boosting), but the theory is generic and can be easily applied to more complex models, such as neural networks. In a decision tree, the parameter vector <em>θi</em> is made up of selection tuples, so the reader can think of this method as a pseudo-random forest where, instead of randomness, we look for extra optimality exploiting the previous experience. In fact, as with AdaBoost, a gradient boosting ensemble is built sequentially, using a technique that is formally defined as <strong>Forward Stage-wise Additive Modeling</strong>. The resulting estimator is represented as a weighted sum:</p>
<p style="padding-left: 150px"><img src="assets/a543a7f1-1bdb-4f5d-b624-8e37fd1b852a.png" style="width:23.58em;height:4.83em;"/></p>
<p class="mce-root">Therefore the variables to manage are the single estimator weights <em>αi</em> and the parameter vectors <em>θi</em>. However, we don't have to work with the whole set, but with a single tuple <em>(αi, θi)</em>, without modifying the values already chosen during the previous iterations. The general procedure can be summarized with a loop:</p>
<ol>
<li class="mce-root">The estimator sum is initialized to a null value</li>
<li class="mce-root">For <em>i=1</em> to <em>Nc</em>:
<ol>
<li class="mce-root"> The best <em>tuple(αi, θi)</em> is chosen and the estimator <em>f(x; θi)</em> is trained</li>
<li><em>di(x) = di-1(x) + αif(x; θi)</em></li>
</ol>
</li>
<li>The final estimator <em>d(x)</em> is output</li>
</ol>
<p>How is it possible to find out the best tuple? We have already presented a strategy for improving the performance of every learner through boosting the dataset. In this case, instead, the algorithm is based on a cost function that we need to minimize:</p>
<p style="padding-left: 150px"><img src="assets/184477c8-9617-4b24-90e9-29f44aafd30c.png" style="width:24.25em;height:4.83em;"/></p>
<p>In particular, the generic optimal tuple is obtained as follows:</p>
<p style="padding-left: 150px"><img src="assets/65fb3164-1039-4186-92f9-63c2bfc83488.png" style="width:23.42em;height:2.25em;"/></p>
<p class="mce-root">As the process is sequential, each estimator is optimized to improve the previous one's accuracy. However, contrary to AdaBoost, we are not constrained to impose a specific loss function (it's possible to prove that AdaBoost.M1 is equivalent to this algorithm with an exponential loss but the proof is beyond the scope of this book). As we are going to discuss, other cost functions can yield better performances in several different scenarios, because they avoid the premature convergence towards sub-optimal minima.</p>
<p>The problem could be considered as solved by employing the previous formula to optimize each new learner; however, the <kbd>argmin(•)</kbd> function needs a complete exploration of the cost function space and, as <kbd>C(•)</kbd> depends on each specific model instance and, therefore, on <em>θi</em>, it's necessary to perform several retraining processes in order to find the optimal solution. Moreover, the problem is generally non-convex and the number of variables can be very high. Numerical algorithms such as L-BFGS or other quasi-Newton methods need too many iterations and a prohibitive computational time. It's clear that such an approach is not affordable in the vast majority of cases and the Gradient Boosting algorithm has been proposed as an intermediate solution. The idea is to find a sub-optimal solution with a gradient descent strategy limited to a single step for each iteration.</p>
<p>In order to present the algorithm, it's useful to rewrite the additive model with an explicit reference to the optimal goal:</p>
<p style="padding-left: 90px"><img src="assets/a5ca1d40-bc8e-4be4-b5f4-81ba1e81e284.png" style="width:38.17em;height:5.08em;"/></p>
<p>Note that the cost function is computed carrying on all the previously trained models; therefore, the correction is always incremental. If the cost function <em>L</em> is differentiable (a fundamental condition that is not difficult to meet), it's possible to compute the gradient with respect to the current additive model (at the <em>i<sup>th</sup></em> iteration, we need to consider the additive model obtained summing all the previous <em>i-1</em> models):</p>
<p style="padding-left: 90px"><img src="assets/b61fc950-e403-4b93-a086-4ecd6d187c32.png" style="width:34.67em;height:4.58em;"/></p>
<p>At this point, a new classifier can be added by moving the current additive model into the negative direction of the gradient:</p>
<p style="padding-left: 120px"><img src="assets/7f2f22b0-32fa-43b4-abc7-08f1946018df.png" style="width:29.42em;height:5.08em;"/></p>
<p>We haven't considered the parameter αi yet (nor the learning rate <em>η</em>, which is a constant), however the reader familiar with some basic calculus can immediately understand the effect of an update is to reduce the value of the global loss function by forcing the next model to improve its accuracy with respect to its predecessors. However, a single gradient step isn't enough to guarantee an appropriate boosting strategy. In fact, as discussed previously, we also need to weight each classifier according to its ability to reduce the loss. Once the gradient has been computed, it's possible to determine the best value for the weight αi with a direct minimization of the loss function (using a line search algorithm) computed considering the current additive model with <em>α</em> as an extra variable:</p>
<p><img src="assets/3b76af08-36a2-41e9-b6d7-2ad04db82758.png" style="width:57.17em;height:5.08em;"/></p>
<p class="mce-root">When using the gradient tree boosting variant, an improvement can be achieved by splitting the weight <em>αi</em> into <em>m</em> sub-weights <em>αi(j)</em> associated with each terminal node of the tree. The computational complexity is slightly increased, but the final accuracy can be higher than the one obtained with a single weight. The reason derives from the functional structure of a tree. As the boosting forces a specialization in specific regions, a single weight could drive to an over-estimation of a learner also when a specific sample cannot be correctly classified. Instead, using different weights, it's possible to operate a fine-grained filtering of the result, accepting or discarding an outcome according to its value and to the properties of the specific tree. </p>
<p class="mce-root">This solution cannot provide the same accuracy of a complete optimization, but it's rather fast and it's possible to compensate for this loss using more estimators and a lower learning rate. Like many other algorithms, gradient boosting must be tuned up in order to yield the maximum accuracy with a low variance. The learning rate is normally quite smaller than 1.0 and its value should be found by validating the results and considering the total number of estimators (it's better to reduce it when more learners are employed). Moreover, a regularization technique could be added in order to prevent overfitting. When working with specific classifier families (such as logistic regression or neural networks), it's very easy to include an <em>L1</em> or <em>L2</em> penalty, but it's not so easy with other estimators. For this reason, a common regularization technique (implemented also by Scikit-Learn) is the downsampling of the training dataset. Selecting <em>P &lt; N</em> random samples allows the estimators to reduce the variance and prevent overfitting. Alternatively, it's possible to employ a random feature selection (for gradient tree boosting only) as in a random forest; picking a fraction of the total number of features increases the uncertainty and avoids over-specialization. Of course, the main drawback to these techniques is a loss of accuracy (proportional to the downsampling/feature selection ratio) that must be analyzed in order to find the most appropriate trade-off. </p>
<p>Before moving to the next section, it's useful to briefly discuss the main cost functions that are normally employed with this kind of algorithms. In the first chapter, we have presented some common cost functions, like mean squared error, Huber Loss (very robust in regression contexts), and cross-entropy. They are all valid examples, but there are other functions that are peculiar to classification problems. The first one is Exponential Loss, defined as follows:</p>
<p style="padding-left: 180px"><img src="assets/90bfefe0-decf-497b-aab8-9add8054ad98.png" style="width:17.50em;height:2.25em;"/></p>
<p class="mce-root">As pointed out by Hastie, Tibshirani and, Friedman, this function transforms the gradient boosting into an AdaBoost.M1 algorithm. The corresponding cost function has a very precise behavior that sometimes is not the most adequate to solve particular problems. In fact, the result of an exponential loss has a very high impact when the error is large, yielding distributions that are strongly peaked around a few samples. The subsequent classifiers can be consequently driven to over-specialize their structure to cope only with a small data region, with a concrete risk of losing the ability to correctly classify other samples. In many situations, this behavior is not dangerous and the final bias-variance trade-off is absolutely reasonable; however, there are problems where a softer loss function can allow a better final accuracy and generalization ability. The most common choice for real-valued binary classification problems is Binomial Negative Log-Likelihood Loss (deviance), defined as follows (in this case we are assuming that the classifier <em>f(•)</em> is not thresholded, but outputs a positive-class probability):</p>
<p style="padding-left: 60px"><img src="assets/fd2ed604-b82b-4fe5-9585-c1585c579e3a.png" style="width:32.17em;height:1.58em;"/></p>
<p>This loss function is the same employed in Logistic Regressions and, contrary to Exponential Loss, doesn't yield peaked distributions. Two misclassified samples with different probabilities are boosted proportionally to the error (not the exponential value), so as to force the classifiers to focus on all the misclassified population with almost the same probability (of course, a higher probability assigned to samples whose error is very large is desirable, assuming that all the other misclassified samples have always a good chance to be selected). The natural extension of the Binomial Negative Log-Likelihood Loss to multi-class problems is the Multinomial Negative Log-Likelihood Loss, defined as follows (the classifier <em>f(•)</em> is represented as probability vector with <em>p</em> components):</p>
<p style="padding-left: 120px"><img src="assets/0dd27751-38b1-48d8-bb41-3f25a9466aa9.png" style="width:27.83em;height:4.83em;"/></p>
<p>In the previous formula, the notation <em>Iy=j</em> must be interpreted as an indicator function, which is equal to 1 when <em>y=j</em> and 0 otherwise. The behavior of this loss function is perfectly analogous to the binomial variant and, in general, it is the default choice for classification problems. The reader is invited to test the examples with both exponential loss and deviance and compare the results.</p>
<p>The complete gradient boosting algorithm is as follows:</p>
<ol>
<li>Set the family and the number of estimators <em>Nc</em></li>
<li>Select a loss function <em>L</em> (for example, deviance)</li>
<li>Initialize the base estimator <em>d0(x)</em> as a constant (such as 0) or using another model</li>
<li>Set the learning rate <em>η</em> (such as <em>η = 1</em>)</li>
<li>For <em>i=1</em> to <em>Nc</em>:
<ol>
<li>Compute the gradient <em>∇d L(•)</em> using the additive model at the step <em>i-1</em></li>
<li>Train the <em>i<sup>th</sup></em> estimator <em>di(x)</em> with the data distribution <em>{ (xi, ∇d L(yi, di-1(xi)) }</em></li>
<li>Perform a line search to compute <em>αi</em></li>
<li>Add the estimator to the ensemble</li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of gradient tree boosting with Scikit-Learn</h1>
                </header>
            
            <article>
                
<p>In this example, we want to employ a gradient tree boosting classifier (class <kbd>GradientBoostingClassifier</kbd>) and check the impact of the maximum tree depth (<kbd>parameter max_depth</kbd>) on the performance. Considering the previous example, we start by setting <kbd>n_estimators=50</kbd> and <kbd>learning_rate=0.8</kbd>:</p>
<pre>import numpy as np<br/><br/>from sklearn.ensemble import GradientBoostingClassifier<br/>from sklearn.model_selection import cross_val_score<br/><br/>scores_md = []<br/>eta = 0.8<br/><br/>for md in range(2, 13):<br/>    gbc = GradientBoostingClassifier(n_estimators=50, learning_rate=eta, max_depth=md, random_state=1000)<br/>    scores_md.append(np.mean(cross_val_score(gbc, X, Y, cv=10)))</pre>
<p>The result is shown in the following diagram:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/c5252e92-7501-42d6-911c-b59617e11481.png" style="width:51.58em;height:27.58em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">10-fold Cross-validation accuracy as a function of the maximum tree depth</div>
<p>As explained in the first section, the maximum depth of a decision tree is strictly related to the possibility of interaction among features. This can be a positive or negative aspect when the trees are employed in an ensemble. A very high interaction level can create over-complex separation hyperplanes and reduce the overall variance. In other cases, a limited interaction results in a higher bias. With this particular (and simple) dataset, the gradient boosting algorithm can achieve better performances when the max depth is 2 (consider that the root has a depth equal to zero) and this is partially confirmed by both the feature importance analysis and dimensionality reductions. In many real-world situations, the result of such a research could be completely different, with increased performance, therefore I suggest you cross-validate the results (it's better to employ a grid search) starting from a minimum depth and increasing the value until the maximum accuracy has been achieved. With <kbd>max_depth=2</kbd>, we want now to tune up the learning rate, which is a fundamental parameter in this algorithm:</p>
<pre>import numpy as np<br/><br/>scores_eta = []<br/><br/>for eta in np.linspace(0.01, 1.0, 100):<br/>    gbr = GradientBoostingClassifier(n_estimators=50, learning_rate=eta, max_depth=2, random_state=1000)<br/>    scores_eta.append(np.mean(cross_val_score(gbr, X, Y, cv=10)))</pre>
<p>The corresponding plot is shown in the following diagram:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/0af543ef-0cee-4572-ba36-71ce1ba7e228.png" style="width:63.08em;height:34.50em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">10-fold Cross-validation accuracy as a function of the learning rate (max depth equal to 2)</div>
<p>Unsurprisingly, gradient tree boosting outperforms AdaBoost with <em>η ≈ 0.9</em>, achieving a cross-validation accuracy slightly lower than 0.99. The example is very simple, but it clearly shows the power of this kind of techniques. The main drawback is the complexity. Contrary to single models, ensembles are more sensitive to changes to the hyperparameters and more detailed research must be conducted in order to optimize the models. When the datasets are not excessively large, cross-validation remains the best choice. If, instead, we are pretty sure that the dataset represents almost perfectly the underlying data generating process, it's possible to shuffle it and split it into two (training/test) or three blocks (training/test/validation) and proceed by optimizing the hyperparameters and trying to overfit the test set (this expression can seem strange, but overfitting the test set means maximizing the generalization ability while learning perfectly the training set structure).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensembles of voting classifiers</h1>
                </header>
            
            <article>
                
<p>A simpler but no less effective way to create an ensemble is based on the idea of exploiting a limited number of strong learners whose peculiarities allow them to yield better performances in particular regions of the sample space. Let's start considering a set of <em>Nc</em> discrete-valued classifiers <em>f1(x), f2(x), ..., fNc(x)</em>. The algorithms are different, but they are all trained with the same dataset and output the same label set. The simplest strategy is based on a hard-voting approach:</p>
<p style="padding-left: 150px"><img src="assets/81f2b833-ad97-48b4-9fa1-893d280000be.png" style="width:26.00em;height:1.83em;"/></p>
<p class="mce-root">In this case, the function <em>n(•)</em> counts the number of estimators that output the label <em>yi</em>. This method is rather powerful in many cases, but has some limitations. If we rely only on a majority vote, we are implicitly assuming that a correct classification is obtained by a large number of estimators. Even if, <em>Nc/2 + 1</em> votes are necessary to output a result, in many cases their number is much higher. Moreover, when k is not very large, also <em>Nc/2 + 1</em> votes imply a symmetry that involves a large part of the population. This condition often drives to the training of useless models that could be simply replaced by a single well-fitted strong learner. In fact, let's suppose that the ensemble is made up of three classifiers and one of them is more specialized in regions where the other two can easily be driven to misclassifications. A hard-voting strategy applied to this ensemble could continuously penalize the more complex estimator in favor of the other classifiers. A more accurate solution can be obtained by considering real-valued outcomes. If each estimator outputs a probability vector, the confidence of a decision is implicitly encoded in the values. For example, a binary classifier whose output is <em>(0.52, 0.48)</em> is much more uncertain than another classifier outputting <em>(0.95, 0.05)</em>. Applying a threshold is equivalent to flattening the probability vectors and discarding the uncertainty. Let's consider an ensemble with three classifiers and a sample that is hard to classify because it's very close to the separation hyperplane. A hard-voting strategy decides for the first class because the thresholded output is <em>(1, 1, 2)</em>. Then we check the output probabilities, obtaining <em>(0.51, 0.49)</em>, <em>(0.52, 0.48)</em>, <em>(0.1, 0.9)</em>. After averaging the probabilities, the ensemble output becomes about (0.38, 062) and by applying <kbd>argmax(•)</kbd>, we get the second class as the final decision. In general, it's also a good practice to consider a weighted average, so that the final class is obtained as follows (assuming the output of the classifier is a probability vector): </p>
<p class="mce-root"><img src="assets/d745df4d-35df-4a4e-b758-3be4cfde5cc3.png" style="width:58.50em;height:5.08em;"/></p>
<p>The weights can be simply equal to 1.0 if no weighting is required or they can reflect the level of trust we have for each classifier. An important rule is to avoid the dominance of a classifier in the majority of cases because it would be an implicit fallback to a single estimator scenario. A good voting example should always allow a minority to overturn a result when their confidence is quite higher than the majority. In this strategies, the weights can be considered as hyperparameters and tuned up using a grid search with cross-validation. However, contrary to other ensemble methods, they are not fine-grained, therefore the optimal value is often a compromise among some different possibilities.</p>
<p>A slightly more complex technique is called <strong>stacking</strong> and consists of using an extra classifier as a post-filtering step. The classical approach consists of training the classifiers separately, then the whole dataset is transformed into a prediction set (based on class labels or probabilities) and the combining classifier is trained to associate the predictions to the final classes. Using even very simple models like Logistic Regressions or Perceptrons, it's possible to mix up the predictions so as to implement a dynamic reweighting that is a function of the input values. A more complex approach is feasible only when a single training strategy can be used to train the whole ensemble (including the combiner). For example, it could be employed with neural networks that, however, have already an implicit flexibility and can often perform quite better than complex ensembles.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of voting classifiers with Scikit-Learn</h1>
                </header>
            
            <article>
                
<p>In this example, we are going to employ the MNIST handwritten digits dataset. As the concept is very simple, our goal is to show how to combine two completely different estimators to improve the overall cross-validation accuracy. For this reason, we have selected a Logistic Regression and a decision tree, which are structurally different. In particular, while the former is a linear model that works with the whole vectors, the latter is a feature-wise estimator that can support the decision only in particular cases (images are not made up of semantically consistent features, but the over-complexity of a Decision Tree can help with particular samples which are very close to the separation hyperplane and, therefore, more difficult to classify with a linear method). The first step is loading and normalizing the dataset (this operation is not important with a Decision Tree, but has a strong impact on the performances of a Logistic Regression):</p>
<pre>import numpy as np<br/><br/>from sklearn.datasets import load_digits<br/><br/>X, Y = load_digits(return_X_y=True)<br/>X /= np.max(X)</pre>
<p>At this point, we need to evaluate the performances of both estimators individually:</p>
<pre>import numpy as np<br/><br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_val_score<br/><br/>dt = DecisionTreeClassifier(criterion='entropy', random_state=1000)<br/>print(np.mean(cross_val_score(dt, X, Y, cv=10)))<br/>0.830880960443<br/><br/>lr = LogisticRegression(C=2.0, random_state=1000)<br/>print(np.mean(cross_val_score(lr, X, Y, cv=10)))<br/>0.937021649942</pre>
<p>As expected, the Logistic Regression (∼94% accuracy) outperforms the decision tree (83% accuracy); therefore, a hard-voting strategy is not the best choice. As we trust the Logistic Regression more, we can employ soft voting with a weight vector set to <em>(0.9, 0.1)</em>. The class <kbd>VotingClassifier</kbd> accepts a list of tuples (name of the estimator, instance) that must be supplied through the <kbd>estimators</kbd> parameter. The strategy can be specified using parameter voting (it can be either "soft" or "hard") and the optional weights, using the parameter with the same name:</p>
<pre>import numpy as np<br/><br/>from sklearn.ensemble import VotingClassifier<br/><br/>vc = VotingClassifier(estimators=[<br/>    ('LR', LogisticRegression(C=2.0, random_state=1000)),<br/>    ('DT', DecisionTreeClassifier(criterion='entropy', random_state=1000))], <br/>     voting='soft', weights=(0.9, 0.1))<br/><br/>print(np.mean(cross_val_score(vc, X, Y, cv=10)))<br/>0.944835154373</pre>
<p>Using a soft-voting strategy, the estimator is able to outperform Logistic Regression by reducing the global uncertainty. I invite the reader to test this algorithm with other datasets, using more estimators, and try to find out the optimal combination using both the hard and soft voting strategies. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensemble learning as model selection</h1>
                </header>
            
            <article>
                
<p>This is not a proper ensemble learning technique, but it is sometimes known as <strong>bucketing</strong>. In the previous section, we have discussed how a few strong learners with different peculiarities can be employed to make up a committee. However, in many cases, a single learner is enough to achieve a good bias-variance trade-off but it's not so easy to choose among the whole Machine Learning algorithm population. For this reason, when a family of similar problems must be solved (they can differ but it's better to consider scenarios that can be easily compared), it's possible to create an ensemble containing several models and use cross-validation to find the one whose performances are the best. At the end of the process, a single learner will be used, but its choice can be considered like a grid search with a voting system. Sometimes this technique can unveil important differences even using similar datasets. For example, during the development of a system, a first dataset (<em>X1, Y1</em>) is provided. Everybody expects that it is correctly sampled from an underlying data generating process p<sub>data</sub> and, so, a generic model is fitted and evaluated. Let's imagine that a SVM achieves a very high validation accuracy (evaluated using a k-fold cross-validation) and, therefore, it is chosen as the final model. Unfortunately, a second, larger dataset (<em>X2, Y2</em>) is provided and the final mean accuracy worsens. We might simply think that the residual variance of the model cannot let it generalize correctly or, as sometimes happens, we can say the second dataset contains many outliers which are not correctly classified. The real situation is a little more complex: given a dataset, we can only suppose that it represents a complete data distribution. Even when the number of samples is very high or we use data augmentation techniques, the population might not represent some particular samples that will be analyzed by the system we are developing. Bucketing is a good way to create a security buffer that can be exploited whenever the scenario changes. The ensemble can be made up of completely different models, models belonging to the same family but differently parametrized (for example, different kernel SVMs) or a mixture of composite algorithms (like PCA + SVM, PCA + decision trees/random forests, and so on). The most important element is the cross-validation. As explained in the first chapter, splitting the dataset into training and test sets can be an acceptable solution only when the number of samples and their variability is high enough to justify the belief that it correctly represents the final data distribution. This often happens in deep learning, where the dimensions of the datasets are quite large and the computational complexity doesn't allow retraining the model too many times. Instead, in classical Machine Learning contexts, cross-validation is the only way to check the behavior of a model when trained with a large random subset and tested on the remaining samples. Ideally, we'd like to observe the same performances, but it can also happen that the accuracy is higher in some folds and quite lower in other. When this phenomenon is observed and the dataset is the final one, it probably means that the model is not able to manage one or more regions of the sample space and a boosting approach could dramatically improve the final accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced the main concepts of ensemble learning, focusing on both bagging and boosting techniques. In the first section, we explained the difference between strong and weak learners and we presented the big picture of how it's possible to combine the estimators to achieve specific goals.</p>
<p>The next topic focused on the properties of decision trees and their main strengths and weaknesses. In particular, we explained that the structure of a tree causes a natural increase in the variance. The bagging technique called random forests allow mitigating this problem, improving at the same time the overall accuracy. A further variance reduction can be achieved by increasing the randomness and employing a variant called <strong>extra randomized trees</strong>. In the example, we have also seen how it's possible to evaluate the importance of each input feature and perform dimensionality reduction without involving complex statistical techniques.</p>
<p>In the third section, we presented the most famous boosting techniques, AdaBoost, which is based on the concept of creating a sequential additive model, when each new estimator is trained using a reweighted (boosted) data distribution. In this way, every learner is added to focus on the misclassified samples without interfering with the previously added models. We analyzed the original M1 discrete variant and the most effective alternatives called SAMME and SAMME.R (real-valued), and R2 (for regressions), which are implemented in many Machine Learning packages.</p>
<p>After AdaBoost, we extended the concept to a generic Forward Stage-wise Additive Model, where the task of each new estimator is to minimize a generic cost function. Considering the complexity of a full optimization, a gradient descent technique was presented that, combined with an estimator weight line search, can yield excellent performances both in classification and in regression problems.</p>
<p>The final topics concerned how to build ensembles using a few strong learners, averaging their prediction or considering a majority vote. We discussed the main drawback of thresholded classifiers and we showed how it's possible to build a soft-voting model that is able to trust the estimator that show less uncertainty. Other useful topics are the Stacking method, which consists of using an extra classifier to process the prediction of each member of the ensemble and how it's possible to create candidate ensembles that are evaluated using a cross-validation technique to find out the best estimator for each specific problem.</p>
<p>In the next chapter, we are going to begin discussing the most important deep learning techniques, introducing the fundamental concepts regarding neural networks and the algorithms involved in their training processes.</p>


            </article>

            
        </section>
    </body></html>