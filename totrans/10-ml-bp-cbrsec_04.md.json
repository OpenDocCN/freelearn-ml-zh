["```py\nimport bert\nfrom bert import run_classifier\nfrom bert import optimization\nfrom bert import tokenization\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport tensorflow_hub as hub\n```", "```py\nBERT_MODEL_HUB = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\ndef create_tokenizer_from_hub_module():\n  with tf.Graph().as_default():\n    bert_module = hub.Module(BERT_MODEL_HUB)\n    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n  with tf.Session() as sess:\n    vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n                                                 tokenization_info[\"do_lower_case\"]])\n  return bert.tokenization.FullTokenizer(\n      vocab_file=vocab_file, do_lower_case=do_lower_case)\ntokenizer = create_tokenizer_from_hub_module()\n```", "```py\ndef create_model(is_predicting, input_ids, input_mask, segment_ids, labels,num_labels):\n  \"\"\"Creates a classification model.\"\"\"\n  bert_module = hub.Module(\n      BERT_MODEL_HUB,\n      trainable=True)\n  bert_inputs = dict(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids)\n  bert_outputs = bert_module(\n      inputs=bert_inputs,\n      signature=\"tokens\",as_dict=True)\n  output_layer = bert_outputs[\"pooled_output\"]\n  hidden_size = output_layer.shape[-1].value\n  output_weights = tf.get_variable(\"output_weights\", [num_labels, hidden_size],      initializer=tf.truncated_normal_initializer(stddev=0.02))\n  output_bias = tf.get_variable(\"output_bias\",[num_labels], initializer=tf.zeros_initializer())\n  with tf.variable_scope(\"loss\"):\n    # Dropout helps prevent overfitting\n  output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    # Convert labels into one-hot encoding\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n    # If we're predicting, we want predicted labels and the probabilties.\n    if is_predicting:\n      return (predicted_labels, log_probs)\n    # If we're train/eval, compute loss between predicted and actual label\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, predicted_labels, log_probs)\n```", "```py\ndef model_fn_builder(num_labels, learning_rate, num_train_steps,num_warmup_steps):\n  def model_fn(features, labels, mode, params):\n    input_ids = features[\"input_ids\"]\n    input_mask = features[\"input_mask\"]\n    segment_ids = features[\"segment_ids\"]\n    label_ids = features[\"label_ids\"]\n    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n    # TRAIN and EVAL\n    if not is_predicting:\n      (loss, predicted_labels, log_probs) = create_model(is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n      train_op = bert.optimization.create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n    # Calculate evaluation metrics.\n    def metric_fn(label_ids, predicted_labels):\n       accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n        f1_score = tf.contrib.metrics.f1_score(\n            label_ids,\n            predicted_labels)\n        auc = tf.metrics.auc(\n            label_ids,\n            predicted_labels)\n        recall = tf.metrics.recall(\n            label_ids,\n            predicted_labels)\n        precision = tf.metrics.precision(\n            label_ids,\n            predicted_labels)\n        true_pos = tf.metrics.true_positives(\n            label_ids,\n            predicted_labels)\n        true_neg = tf.metrics.true_negatives(\n            label_ids,\n            predicted_labels)\n        false_pos = tf.metrics.false_positives(\n            label_ids,\n            predicted_labels)\n        false_neg = tf.metrics.false_negatives(\n            label_ids,\n            predicted_labels)\n        return {\n            \"eval_accuracy\": accuracy,\n            \"f1_score\": f1_score,\n            \"auc\": auc,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"true_positives\": true_pos,\n            \"true_negatives\": true_neg,\n            \"false_positives\": false_pos,\n            \"false_negatives\": false_neg\n        }\n      eval_metrics = metric_fn(label_ids, predicted_labels)\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(mode=mode,\n          loss=loss,\n          train_op=train_op)\n      else:\n        return tf.estimator.EstimatorSpec(mode=mode,\n            loss=loss,\n            eval_metric_ops=eval_metrics)\n      else:\n        (predicted_labels, log_probs) = create_model(\n        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n      predictions = {\n          'probabilities': log_probs,\n          'labels': predicted_labels\n      }\n     return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n  # Return the actual model function in the closure\nreturn model_fn\n```", "```py\nBATCH_SIZE = 32\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 3.0\nWARMUP_PROPORTION = 0.1\nSAVE_CHECKPOINTS_STEPS = 500\nSAVE_SUMMARY_STEPS = 100\nPOSRATIO=0.001 # 0.1%\nNPOS=10000*POSRATIO\n```", "```py\ndf_a=pd.read_csv('dataset.csv')\ndf_a_1=df_a[df_a['VERDICT']==1]\ndf_a_0=df_a[df_a['VERDICT']==0]\ndf_a_sampled=pd.concat([df_a_1[:nPos],df_a_0[:NPOS]])\n```", "```py\nfrom sklearn.model_selection import train_test_split\ndf_train_n,df_test_n=train_test_split(df_a_sampled,stratify=df_a_sampled['VERDICT'])\n```", "```py\nDATA_COLUMN='API_CALL_SEQ'\nLABEL_COLUMN='VERDICT'\ntrain_InputExamples_a = df_train_n.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n                        text_a = x[DATA_COLUMN],\n                        text_b = None,\n                        label = x[LABEL_COLUMN]),\n                        axis = 1)\ntest_InputExamples_a = df_test_n.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n                        text_a = x[DATA_COLUMN],\n                        text_b = None,\n                        label = x[LABEL_COLUMN]),\n                        axis = 1)\n```", "```py\nlabel_list=[0,1]\nMAX_SEQ_LENGTH = 128\ntrain_features_a = bert.run_classifier.convert_examples_to_features(\n                train_InputExamples_a,\n                label_list,\n                MAX_SEQ_LENGTH,\n                tokenizer)\ntest_features_a = bert.run_classifier.convert_examples_to_features(\n                test_InputExamples_a,\n                label_list,\n                MAX_SEQ_LENGTH,\n                tokenizer)\n```", "```py\nOUTPUT_DIR='saved_models/rate_'+str(posRatio*100)\nnum_train_steps = int(len(train_features_a) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\nrun_config = tf.estimator.RunConfig(\n    model_dir=OUTPUT_DIR,\n    save_summary_steps=SAVE_SUMMARY_STEPS,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\nmodel_fn = model_fn_builder(\n  num_labels=len(label_list),\n  learning_rate=LEARNING_RATE,\n  num_train_steps=num_train_steps,\n  num_warmup_steps=num_warmup_steps)\nestimator_android = tf.estimator.Estimator(\n  model_fn=model_fn,\n  config=run_config,\n  params={\"batch_size\": BATCH_SIZE})\ntrain_input_fn_a = bert.run_classifier.input_fn_builder(\n    features=train_features_a,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=False)\nimport time\nprint(f'Beginning Training!')\ncurrent_time = time.time()\nestimator_android.train(input_fn=train_input_fn_a, max_steps=num_train_steps)\nprint(\"Training took time \", time.time() - current_time)\n```", "```py\ntest_input_fn_a = bert.run_classifier.input_fn_builder(\n    features=test_features_a,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)\nmetrics = estimator_android.evaluate(input_fn=test_input_fn_a, steps=None)\n```", "```py\n{'auc': 0.95666675,\n 'eval_accuracy': 0.99920094,\n 'f1_score': 0.49999997,\n 'false_negatives': 2.0,\n 'false_positives': 0.0,\n 'loss': 0.0076462436,\n 'precision': 0.974,\n 'recall': 0.871,\n 'true_negatives': 2500.0,\n 'true_positives': 1.0,\n 'global_step': 703}\n```"]