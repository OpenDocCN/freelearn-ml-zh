- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting Machine-Generated Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed deepfakes, which are synthetic media that
    can depict a person in a video and show the person to be saying or doing things
    that they did not say or do. Using powerful deep learning methods, it has been
    possible to create realistic deepfakes that cannot be distinguished from real
    media. Similar to such deepfakes, machine learning models have also succeeded
    in creating fake text – text that is generated by a model but appears to be written
    by a human. While the technology has been used to power chatbots and develop question-answering
    systems, it has also found its use in several nefarious applications.
  prefs: []
  type: TYPE_NORMAL
- en: Generative text models can be used to enhance bots and fake profiles on social
    networking sites. Given a prompt text, the model can be used to write messages,
    posts, and articles, thus adding credibility to the bot. A bot can now pretend
    to be a real person, and a victim might be fooled because of the realistic-appearing
    chat messages. These models allow customization by style, tone, sentiment, domain,
    and even political leaning. It is easily possible to provide a prompt and generate
    a news-style article; such articles can be used to spread misinformation. Models
    can be automated and deployed at scale on the internet, which means that there
    can be millions of fake profiles pretending to be real people, and millions of
    Twitter accounts generating and posting misleading articles. Detecting automated
    text is an important problem on the internet today.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will explore the fundamentals of generative models, how they can
    be used to create text, and techniques to detect them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Text generation models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer methods for detecting automated text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a firm understanding of text generation
    models and approaches to detecting bot-generated text.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%206](
    https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%206).
  prefs: []
  type: TYPE_NORMAL
- en: Text generation models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we saw how machine learning models can be trained to
    generate images of people. The images generated were so realistic that it was
    impossible in most cases to tell them apart from real images with the naked eye.
    Along similar lines, machine learning models have made great progress in the area
    of text generation as well. It is now possible to generate high-quality text in
    an automated fashion using deep learning models. Just like images, this text is
    so well written that it is not possible to distinguish it from human-generated
    text.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentally, a language model is a machine learning system that is able to
    look at a part of a sentence and predict what comes next. The words predicted
    are appended to the existing sentence, and this newly formed sentence is used
    to predict what will come next. The process continues recursively until a specific
    token denoting the end of the text is generated. Note that when we say that the
    next word is predicted, in reality, the model generates a probability distribution
    over possible output words. Language models can also operate at the character
    level.
  prefs: []
  type: TYPE_NORMAL
- en: Most text generation models take in a prompt text as input. Trained on massive
    datasets (such as all Wikipedia articles or entire books), the models have learned
    to produce text based on these prompts. Training on different kinds of text (stories,
    biographies, technical articles, and news articles) enables models to generate
    those specific kinds of text.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the power of AI-based text generation with your own eyes, explore the
    open source text generator called **Grover**. This is a tool that was produced
    by researchers at the University of Washington and allows you to produce a real-looking
    news article based on any given prompt. The website provides an interface as shown
    in the following figure, where you can provide a prompt title and an author name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The Grover interface to generate text](img/B19327_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – The Grover interface to generate text
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you set these parameters, Grover will generate an article that appears
    real. Here is the article that was generated with our given prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Link found between autism** **and vaccines**May 29, 2019 - Sofia OjedaWritten
    by Sofia Ojeda, CNNA paper published in the journal Genetics and Human Behavior
    claims to find a “significant link” between autism and a type of booster vaccine
    given in childhood. The paper focuses on the mercury-based preservative thimerosal
    and considers the possibility that the injectable vaccine contributes to autism-related
    symptoms. These include poor social skills, language delays and seizures, according
    to the paper.Thimerosal, which is commonly used as a preservative in vaccines,
    was phased out of children’s vaccines in 2010\. In 2010, the Food and Drug Administration
    declared that thimerosal was safe. “We don’t know of any adverse effects associated
    with the endocrine-disrupting, mercury-containing preservative in vaccines, and
    believe that this outcome is beneficial to public health,” said Dr. Lisa M. Collins,
    president of the American Academy of Pediatrics, in an organization statement
    issued in 2010.The new study, led by researchers at Tel Aviv University, is based
    on case studies of two women with autistic children and one non-autistic mother.
    The researchers found that autism symptoms tended to worsen after only one dose
    of the thimerosal-containing hepatitis B booster vaccine. “The use of unadjuvanted
    Hepatitis B booster vaccines, against which the only adverse effects are fatigue,
    headache, cough, and fever, remains a substantial and unrefined source of vaccine-related
    injury and disability worldwide,” the study authors wrote.Using a mathematical
    model, the study authors described multiple risks associated with the use of unadjuvanted
    injectable vaccines. Among them, vaccines increased the risk of seizures, such
    as those associated with autism; autoimmune disorders and cancer.“There are other
    types of vaccines that pose similar risk to autism, including rotavirus vaccines
    and the polio vaccine,” said lead author Professor Uri Ayalon, head of the pediatrics
    department at Tel Aviv University.The authors also found that the autism risk
    increased by twofold in children aged 3 to 10 who received the hepatitis B booster
    vaccine against Hepatitis B, regardless of the following vaccination schedules:
    aged 9 months or older with only one dose; or under 4 months old and only one
    dose. “Since these vaccines are widely used, regardless of age, it may be important
    to limit its utilization,” the researchers wrote.Lead author Dr. Yonatan Schulmann
    said there were no apparent risks associated with a standard influenza vaccination.
    “The flu vaccine probably represents an acceptable source of vaccine-related injury
    and disability,” he said. “This is not true for most vaccines. The flu vaccine
    is relatively inexpensive (free of charges) and has no significant health effects,”
    he said.The timing of vaccination is also important, said Schulmann. “Autism spectrum
    disorders are most often diagnosed in early adolescence, the upper age range at
    which it is most likely that vaccination data is available,” he said. Furthermore,
    the authors said they found no clear differences between children who received
    hepatitis B vaccine against Hepatitis B and other children. |'
  prefs: []
  type: TYPE_TB
- en: Note how the article has the stylistic features that you would typically expect
    from journalistic writings. The sentence construction is grammatically correct
    and the whole text reads as a coherent article. There are quotes from researchers
    and professors who are subject matter experts, complete with statistics and experimental
    results cited. Overall, the article could pass off as something written by a human.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT stands for **Generative Pretrained Transformer**, and GPT models have dazzled
    the NLP world because they can generate coherent essays that are beyond those
    produced by traditional language models such as those based on **Recurrent Neural
    Networks** (**RNNs**). GPT models are also based on the transformer architecture
    (recall the BERT architecture that we used for malware detection was also based
    on the transformer).
  prefs: []
  type: TYPE_NORMAL
- en: Recall the concepts of attention that we introduced in [*Chapter 3*](B19327_03.xhtml#_idTextAnchor015),
    *Malware Detection Using Transformers and BERT*. We introduced two kinds of blocks
    – the encoder and decoder – both of which were built using transformers that leveraged
    the attention mechanism. The transformer encoder had a self-attention layer followed
    by a fully connected feed-forward neural network. The decoder layer was similar
    except that it had an additional masked self-attention layer that ensured that
    the transformer did not attend to the future tokens (which would defeat the purpose
    of the language model). For example, if the decoder decodes the fourth word, it
    will attend to all words up to the third predicted word and all the words in the
    input.
  prefs: []
  type: TYPE_NORMAL
- en: In general, GPT models use only the decoder blocks, which are stacked one after
    the other. When a token is fed into the model, it is converted into an embedding
    representation using a matrix lookup. Additionally, a positional encoding is added
    to it to indicate the sequence of words/tokens. The two matrices (embedding and
    positional encoding) are parts of the pretrained models we use. When the first
    token is passed to the model, it gets converted into a vector using the embedding
    lookup and positional encoding matrices. It passes through the first decoder block,
    which performs self-attention, passes the output to the neural network layer,
    and forwards the output to the next decoder block.
  prefs: []
  type: TYPE_NORMAL
- en: After processing by the final decoder, the output vector is multiplied with
    the embedding matrix to obtain a probability distribution over the output token
    to be produced. This probability distribution can be used to select the next word.
    The most straightforward strategy is to choose the word with the highest probability
    – however, we run the risk of being stuck in a loop. For instance, if the tokens
    produced so far are “*The man and*” and we always select the word with the highest
    probability, we might end up producing “*The man and the man and the man and the*
    *man…..*” indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this, we apply a top-*K* sampling. We select the top *K* words (based
    on the probability) and sample a word from them, where words with a higher score
    have a higher chance of being selected. Since this process is non-deterministic,
    the model does not end up in the loop of choosing the same set of words again
    and again. The process continues until a certain number of tokens has been produced,
    or the end-of-string token is found.
  prefs: []
  type: TYPE_NORMAL
- en: Generation by GPT models can be either conditional or unconditional. To see
    generation in action, we can use the Write with Transformer ([https://transformer.huggingface.co/doc/gpt2-large](https://transformer.huggingface.co/doc/gpt2-large))
    web app developed by Hugging Face, which uses GPT-2\. The website allows you to
    simulate both conditional and unconditional generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In conditional generation, we provide the model with a set of words as a prompt,
    which is used to seed the generation. This initial set of words provides the context
    used to drive the rest of the text, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Generating text with a prompt](img/B19327_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Generating text with a prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, in unconditional generation, we just provide the `<s>` token,
    which is used to indicate the start of a string, and allow the model to freely
    produce what it wants. If you press the *Tab* key on Write With Transformer, you
    should see such unconditional samples generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Generating text without prompts](img/B19327_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Generating text without prompts
  prefs: []
  type: TYPE_NORMAL
- en: There have been multiple versions of GPT models released by OpenAI, the latest
    one that has made the news being ChatGPT, based on GPT 3.5\. In an upcoming section,
    we will use ChatGPT to create our own dataset of fake news.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will focus on naïve methods for detecting bot-generated
    text. We will first create our own dataset, extract features, and then apply machine
    learning models to determine whether a particular text is machine-generated or
    not.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The task we will focus on is detecting bot-generated fake news. However, the
    concepts and techniques we will learn are fairly generic and can be applied to
    parallel tasks such as detecting bot-generated tweets, reviews, posts, and so
    on. As such a dataset is not readily available to the public, we will create our
    own.
  prefs: []
  type: TYPE_NORMAL
- en: How are we creating our dataset? We will use the News Aggregator dataset ([https://archive.ics.uci.edu/ml/datasets/News+Aggregator](https://archive.ics.uci.edu/ml/datasets/News+Aggregator))
    from the UCI Dataset Repository. The dataset contains a set of news articles (that
    is, links to the articles on the web). We will scrape these articles, and these
    are our human-generated articles. Then, we will use the article title as a prompt
    to seed generation by GPT-2, and generate an article that will be on the same
    theme and topic, but generated by GPT-2! This makes up our positive class.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping real articles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The News Aggregator dataset from UCI contains information on over 420k news
    articles. It was developed for research purposes by scientists at the Roma Tre
    University in Italy. News articles span multiple categories such as business,
    health, entertainment, and science and technology. For each article, we have the
    title and the URL of the article online. You will need to download the dataset
    from the UCI Machine Learning Repository website ([https://archive.ics.uci.edu/ml/datasets/News+Aggregator](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the data using the `head()` functionality (note that you will
    have to change the path according to how you store the file locally):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will show you the first five rows of the DataFrame. As you can see in the
    following screenshot, we have an ID to refer to each row and the title and URL
    of the news article. We also have the hostname (the website where the article
    appeared) and the timestamp, which denotes the time when the news was published.
    The **STORY** field contains an ID that is used to indicate a cluster containing
    similar news stories.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – UCI News Aggregator data](img/B19327_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – UCI News Aggregator data
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us take a look at the distribution of the articles across categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – News article distribution by category](img/B19327_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – News article distribution by category
  prefs: []
  type: TYPE_NORMAL
- en: From the documentation, we see that the categories **e**, **b**, **t**, and
    **m** represent entertainment, business, technology, and health, respectively.
    Entertainment has the highest number of articles, followed by business and technology
    (which are similar), and health has the least.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can also inspect the top domains where the articles come from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Distribution of news articles across sources](img/B19327_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Distribution of news articles across sources
  prefs: []
  type: TYPE_NORMAL
- en: In order to scrape the article from the website, we would need to simulate a
    browser session using a browser tool such as Selenium, find the article text by
    parsing the HTML source, and then extract it. Fortunately, there is a library
    in Python that does all of this for us. The `Newspaper` Python package ([https://github.com/codelucas/newspaper/](https://github.com/codelucas/newspaper/))
    provides an interface for downloading and parsing news articles. It can extract
    text, keywords, author names, summaries, and images from the HTML source of an
    article. It has support for multiple languages including English, Spanish, Russian,
    and German. You can also use a general-purpose web scraping library such as `BeautifulSoup`,
    but the `Newspaper` library is designed specifically to capture news articles
    and hence provides a lot of functions that we would have had to write custom if
    using `BeautifulSoup`.
  prefs: []
  type: TYPE_NORMAL
- en: To create our dataset of real articles, we will iterate through the News Aggregator
    DataFrame and use the `Newspaper` library to extract the text for each article.
    Note that the dataset has upward of 420k articles – for the purposes of demonstration,
    we will sample 1,000 articles randomly from the dataset. For each article, we
    will use the `Newspaper` library to scrape the text. We will create a directory
    to hold these articles.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us create the directory structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let us sample articles from the 400k articles we have. In order to avoid
    bias and overfitting, we should not focus on a particular category. Rather, our
    goal should be to sample uniformly at random so we have a well-distributed dataset
    across all four categories. This general principle also applies to other areas
    where you are designing machine learning models; the more diverse your dataset
    is, the better the generalization. We will sample 250 articles from each of our
    4 categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you check the distribution now, you will see that it is equal across all
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the distribution clearly in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Distribution of sampled articles](img/B19327_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Distribution of sampled articles
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now iterate through this DataFrame and scrape each article. We will
    scrape the article, read the text, and save it into a file in the real directory
    we created earlier. Note that this is essentially a web scraper – as different
    websites have different HTML structures, the newspaper library may hit some errors.
    Certain websites may also block scrapers. For such articles, we will print out
    a message with the article URL. In practice, when such a situation is encountered,
    data scientists will fill the gap manually if the number of missing articles is
    small enough:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now we have our real articles downloaded locally. It’s time to get into the
    good stuff – creating our set of fake articles!
  prefs: []
  type: TYPE_NORMAL
- en: Using GPT to create a dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will use GPT-3 to create our own dataset of machine-generated
    text. OpenAI, a San Francisco-based artificial intelligence research lab, developed
    GPT-3, a pretrained universal language model that utilizes deep learning transformers
    to create text that is remarkably human-like. Released in 2020, GPT-3 has made
    headlines in various industries, as its potential use cases are virtually limitless.
    With the help of the GPT-3 API family and ChatGPT, individuals have used it to
    write fiction and poetry, code websites, respond to customer feedback, improve
    grammar, translate languages, generate dialog, optimize tax deductions, and automate
    A/B testing, among other things. The model’s high-quality results have impressed
    many.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `transformers` library from HuggingFace to download and run
    inference on ChatGPT models. To do this, we can first load the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This will download the model for you locally. Note that this involves downloading
    a sizeable model from the online repository, and hence will take quite some time.
    The time taken to execute will depend on your system usage, resources, and network
    speed at the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can generate a sample text using this new model. For example, if we want
    to generate a poem about flowers, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And this gave me the following poem (note that the results may differ for you):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We have already downloaded and initialized the model we want. Now, we can iterate
    through our list of article titles and generate articles one by one by passing
    the title as a seed prefix. Just like the scraped articles, each article must
    be saved into a text file so that we can later access it for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'All that is left now is to read all of the data we have into a common array
    or list, which can then be used in all of our experiments. We will read each file
    in the real directory and add it to an array. At the same time, we will keep appending
    `0` (indicating a real article) to another array that holds labels. We will repeat
    the same process with the fake articles and append `1` as the label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have our text in the `X` list and associated labels in the `Y` list.
    Our dataset is ready!
  prefs: []
  type: TYPE_NORMAL
- en: Feature exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have our dataset, we want to build a machine learning model to detect
    bot-generated news articles. Recall that machine learning algorithms are mathematical
    models and, therefore, operate on numbers; they cannot operate directly on text!
    Let us now extract some features from the text.
  prefs: []
  type: TYPE_NORMAL
- en: This section will focus on hand-crafting features – the process where subject
    matter experts theorize potential differences between the two classes and build
    features that will effectively capture the differences. There is no unified technique
    for doing this; data scientists experiment with several features based on domain
    knowledge to identify the best ones.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are concerned with text data – so let us engineer a few features from
    that domain. Prior work in NLP and linguistics has analyzed human writing and
    identified certain characteristics. We will engineer three features based on prior
    research.
  prefs: []
  type: TYPE_NORMAL
- en: Function words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These are supporting words in the text that do not contribute to meaning but
    add continuity and flow to the sentence. They are generally determiners (*the*,
    *an*, *many*, *a little*, and *none*), conjunctions (*and* and *but*), prepositions
    (*around*, *within*, and *on*), pronouns (*he*, *her*, and *their*), auxiliary
    verbs (*be*, *have*, and *do*), modal auxiliary (*can*, *should*, *could*, and
    *would*), qualifiers (*really* and *quite*), or question words (*how* and *why*).
    Linguistic studies have shown that every human uses these unpredictably, so there
    might be randomness in the usage pattern. As our feature, we will count the number
    of function words that we see in the sentence, and then normalize it by the length
    of the sentence in words.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a file that contains a list of the top function words and read
    the list of all function words. Then, we will count the function words in each
    text and normalize this count by the length. We will wrap this up in a function
    that can be used to featurize multiple instances of text. Note that as the list
    of function words would be the same for all texts, we do not need to repeat it
    in each function call – we will keep that part outside the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Punctuation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Punctuation symbols (commas, periods, question marks, exclamations, and semi-colons)
    set the tone of the text and inform how it should be read. Prior research has
    shown that the count of punctuation symbols may be an important feature in detecting
    bot-generated text. We will first compile a list of punctuation symbols (readily
    available in the Python `string` package). Similar to the function words, we will
    count the occurrences of punctuation and normalize them by length. Note that this
    time, however, we need to normalize by the length in terms of the number of characters
    as opposed to words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Readability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Research in early childhood education has studied text in detail and derived
    several metrics that indicate how readable a particular blob of text is. These
    metrics analyze the vocabulary and complexity of the text and determine the ease
    with which a reader can read and understand the text. There are several measures
    of readability defined in prior literature ([https://en.wikipedia.org/wiki/Readability](https://en.wikipedia.org/wiki/Readability)),
    but we will be using the most popular one called the **Automated Readability Index**
    (**ARI**) ([https://readabilityformulas.com/automated-readability-index.php](https://readabilityformulas.com/automated-readability-index.php)).
    It depends on two factors – word difficulty (the number of letters per word) and
    sentence difficulty (the number of words per sentence), and is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ARI = 4.71 ( # characters _  # words ) + 0.5 ( # words _ # sentences ) − 21.43'
  prefs: []
  type: TYPE_NORMAL
- en: 'In theory, the ARI represents the approximate age needed to understand the
    text. We will now develop a function that calculates the ARI for our input text,
    and wrap it into a function like we did for the previous features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This completes our discussion of naive feature extraction. In the next section,
    we will use these features to train and evaluate machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Using machine learning models for detecting text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have now hand-crafted three different features: punctuation counts, function
    word counts, and the readability index. We also defined functions for each. Now,
    we are ready to apply these to our dataset and build models. Recall that the `X`
    array contains all of our text. We want to represent each text sample using a
    three-element vector (as we have three features):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now, each text sample is represented by a three-element vector in `X_Features`.
    The first, second, and third elements represent the normalized function word count,
    punctuation count, and ARI, respectively. Note that this order is arbitrary –
    you may choose your own order as it does not affect the final model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our features are ready, so now we will do the usual. We begin by splitting
    our data into training and test sets. We then fit a model on the training data
    and evaluate its performance on the test data. In previous chapters, we used the
    confusion matrix function to plot the confusion matrix and visually observe the
    true positives, false positives, true negatives, and false negatives. We will
    now build another function on top of it that will take in these values and calculate
    metrics of interest. We will calculate the true positives, false positives, true
    negatives, and false negatives, and then calculate the accuracy, precision, recall,
    and F1 score. We will return all of these as a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us split the data into training and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will fit a model on the training data, and evaluate its performance
    on the test data. Here, we will use random forests, logistic regression, SVM,
    and a deep neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic regression classifier is a statistical model that expresses the
    probability of an input belonging to a particular class as a linear combination
    of features. Specifically, the model produces a linear combination of inputs (just
    like linear regression) and applies a sigmoid to this combination to obtain an
    output probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Random forests are ensemble classifiers consisting of multiple decision trees.
    Each tree is a hierarchical structure with nodes as conditions and leaves as class
    labels. A classification label is derived by following the path of the tree through
    the root. The random forest contains multiple such trees, each trained on a random
    sample of data and features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'A **multilayer perceptron** (**MLP**) is a fully connected deep neural network,
    with multiple hidden layers. The input data undergoes transformations through
    these layers, and the final layer is a sigmoid or softmax function, which generates
    the probability of the data belonging to a particular class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The SVM constructs a decision boundary between two classes such that the best
    classification accuracy is obtained. In case the boundary is not linear, the SVM
    transforms the features into a higher dimensional space and obtains a non-linear
    boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Running this code should print out the evaluation dictionaries for each model,
    which tells you the accuracy, recall, and precision. You can also plot the confusion
    matrix (as we did in previous chapters) to visually see the false positives and
    negatives, and get an overall sense of how good the model is.
  prefs: []
  type: TYPE_NORMAL
- en: Playing around with the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have explored here only three features – however, the possibilities for
    hand-crafted features are endless. I encourage you to experiment by adding more
    features to the mix. Examples of some features are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Length of the text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of proper nouns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of numeric characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average sentence length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of times the letter *q* was used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is certainly not an exhaustive list, and you should experiment by adding
    other features to see whether the model's performance improves.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic feature extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we discussed how features can be engineered from text.
    However, hand-crafting features might not always be the best idea. This is because
    it requires expert knowledge. In this case, data scientists or machine learning
    engineers alone will not be able to design these features – they will need experts
    from linguistics and language studies to identify the nuances of language and
    suggest appropriate features such as the readability index. Additionally, the
    process is time-consuming; each feature has to be identified, implemented, and
    tested one after the other.
  prefs: []
  type: TYPE_NORMAL
- en: We will now explore some methods for automatic feature extraction from text.
    This means that we do not manually design features such as the punctuation count,
    readability index, and so on. We will use existing models and techniques, which
    can take in the input text and generate a feature vector for us.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Term Frequency – Inverse Document Frequency** (**TF-IDF**) is a commonly
    used technique in natural language processing to convert text into numeric features.
    Every word in the text is assigned a score that indicates how important the word
    is in that text. This is done by multiplying two metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Term Frequency**: How frequently does the word appear in the text sample?
    This can be normalized by the length of the text in words, as texts that differ
    in length by a large number can cause skews. The term frequency measures how common
    a word is in this particular text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inverse Document Frequency**: How frequently does the word appear in the
    rest of the corpus? First, the number of text samples containing this word is
    obtained. The total number of samples is divided by this number. Simply put, IDF
    is the inverse of the fraction of text samples containing the word. IDF measures
    how common the word is in the rest of the corpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For every word in each text, the TF-IDF score is a statistical measure of the
    importance of the word to the sentence. A word that is common in a text but rare
    in the rest of the corpus is surely important and a distinguishing characteristic
    of the text, and will have a high TF-IDF score. Alternately, a word that is very
    common in the corpus (that is, present in nearly all text samples) will not be
    a distinguishing one – it will have a low TF-IDF score.
  prefs: []
  type: TYPE_NORMAL
- en: In order to convert the text into a vector, we first calculate the TF-IDF score
    of each word in each text. Then, we replace the word with a sequence of TF-IDF
    scores corresponding to the words. The `scikit-learn` library provides us with
    an implementation of TF-IDF vectorization out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note a fine nuance here: the goal of our experiment is to build a model for
    bot detection that can be used to classify new text as being generated by bots
    or not. Thus, when we are training, we have no idea about the test data that will
    come in the future. To ensure that we simulate this, we will do the TF-IDF score
    calculation over only the training data. When we vectorize the test data, we will
    simply use the calculated scores as a lookup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You can manually inspect a few samples from the generated list. What do they
    look like?
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the feature vectors, we can use them to train the classification
    models. The overall procedure remains the same: initialize a model, fit a model
    on the training data, and evaluate it on the testing data. The MLP example is
    shown here; however, you could replace this with any of the models we discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: How does the performance of this model compare to the performance of the same
    model with handcrafted features? How about the performance of the other models?
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The TF-IDF approach is considered to be what we call a *bag of words* approach
    in machine learning terms. Each word is scored based on its presence, irrespective
    of the order in which it appears. Word embeddings are numeric representations
    of words assigned such that words that are similar in meaning have similar embeddings
    – the numeric representations are close to each other in the feature space. The
    most fundamental technique used to to generate word embeddings is called **Word2Vec**.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec embeddings are produced by a shallow neural network. Recall that the
    last layer of a classification model is a sigmoid or softmax layer for producing
    an output probability distribution. This softmax layer operates on the features
    it receives from the pre-final layer – these features can be treated as high-dimensional
    representations of the input. If we chop off the last layer, the neural network
    without the classification layer can be used to extract these embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Word2Vec can work in one of two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '`X` = *I went to walk the* and `Y` = *dog* would be one training example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Skip-Gram**: This is the more widely used technique. Instead of predicting
    the target word, we train a model to predict the surrounding words. For example,
    if the text corpus contains the sentence *I went to walk the dog*, then our input
    would be *walk* and the output would be a prediction (or probabilistic prediction)
    of the surrounding two or more words. Because of this design, the model learns
    to generate similar embeddings for similar words. After the model is trained,
    we can pass the word of interest as an input, and use the features of the final
    layer as our embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that while this is still a classification task, it is not supervised learning.
    Rather, it is a self-supervised approach. We have no ground truth, but by framing
    the problem uniquely, we generate our own ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: We will now build our word embedding model using the `gensim` Python library.
    We will fit the model on our training data, and then vectorize each sentence using
    the embeddings. After we have the vectors, we can fit and evaluate the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we fit the model on training data. Because of the way Word2Vec operates,
    we need to combine our texts into a list of sentences and then tokenize it into
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can fit the embedding model. By passing in the `vector_size` parameter,
    we control the size of the generated embedding. The larger the size, the more
    the expressive the power of the embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have the embedding model and can start using it to tokenize the text.
    Here, we have two strategies. One strategy is that we can calculate the embedding
    for all the words in the text and simply average them to find the mean embedding
    for the text. Here’s how we would do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `X_train_vector_mean` array now holds an embedding representation for each
    text in our corpus. The same process can be repeated to generate the feature set
    with test data.
  prefs: []
  type: TYPE_NORMAL
- en: The second strategy is, instead of averaging the vectors, we append them one
    after the other. This retains more expressive power as it takes into account the
    order of words in the sentence. However, each text will have a different length
    and we require a fixed-size vector. Therefore, we take only a fixed number of
    words from the text and concatenate their embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we set the maximum number of words to be `40`. If a text has more than
    40 words, we will consider only the first 40\. If it has less than 40 words, we
    will consider all of the words and pad the remaining elements of the vector with
    zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The same code snippet can be repeated with the test data as well. Remember that
    the approach you use (averaging or appending) has to be consistent across training
    and testing data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the features are ready, we train and evaluate the model as usual.
    Here’s how you would do it with an MLP; this is easily extensible to other models
    we have seen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note that, here, the dimensions of the hidden layers we passed to the model
    are different from before. In the very first example with hand-crafted features,
    our feature vector was only three-dimensional. However, in this instance, every
    text instance will be represented by 40 words, and each word represented by a
    30-dimensional embedding, meaning that the feature vector has 1,200 elements.
    The higher number of neurons in the hidden layer helps handle the high-dimensional
    feature space.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an exercise, you are encouraged to experiment with three changes and check
    whether there is an improvement in the model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the word embeddings, which has been set to `30` for now. What happens
    to the model performance as you increase or decrease this number?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of words has been chosen as 0\. What happens if this is reduced or
    increased?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the MLP, how does the model performance change as you vary the number
    of layers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine word embeddings and TF-IDF. Instead of a simple average, calculate a
    weighted average where the embedding for each word is weighted by the TF-IDF score.
    This will ensure that more important words influence the average more. How does
    this affect model performance?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer methods for detecting automated text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we have used traditional hand-crafted features, automated
    bag of words features, as well as embedding representations for text classification.
    We saw the power of BERT as a language model in the previous chapter. While describing
    BERT, we referenced that the embeddings generated by BERT can be used for downstream
    classification tasks. In this section, we will extract BERT embeddings for our
    classification task.
  prefs: []
  type: TYPE_NORMAL
- en: The embeddings generated by BERT are different from those generated by the Word2Vec
    model. Recall that in BERT, we use the masked language model and a transformer-based
    architecture based on attention. This means that the embedding of a word depends
    on the context in which it occurs; based on the surrounding words, BERT knows
    which other words to pay attention to and generate the embedding.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional word embeddings, a word will have the same embedding, irrespective
    of the context. The word *match* will have the same embedding in the sentence
    *They were a perfect match!* and *I lit a match last night*. BERT, on the other
    hand, conditions the embeddings based on context. The word *match* would have
    different embeddings in these two sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that we have already used BERT once, for malware detection. There are
    two major differences in how we use it now versus when we implemented it for malware
    detection:'
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we used BERT in the fine-tuning mode. This means that we used the
    entire transformer architecture initialized with pretrained weights, added a neural
    network on top of it, and trained the whole model end to end. The pretrained model
    enabled learning sequence features, and the fine-tuning helped adapt it to the
    specific task. However, now we will use BERT only as a feature extractor. We will
    load a pretrained model, run the sentence through it, and use the pre-final layer
    to construct our features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, we used TensorFlow for implementing BERT. Now, we will
    use PyTorch, a deep learning framework developed by researchers from Facebook.
    This provides a much more intuitive, straightforward, and understandable interface
    to design and run deep neural networks. It also has a `transformers` library,
    which provides easy implementations of all pretrained models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, we will initialize the BERT model and set it to evaluation mode. In
    the evaluation mode, there is no learning, just inferencing. Therefore, we need
    only the forward pass and no backpropagation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now prepare our data in the format needed by BERT. This includes adding
    the two special tokens to indicate the start and separation. Then, we will run
    the model in inference mode to obtain the embeddings (hidden states). Recall that
    when we used Word2Vec embeddings, we averaged the embeddings for each word. In
    the case of BERT embeddings, we have multiple choices:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use just the last hidden state as the embedding:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the sum of all hidden states as the embedding:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the sum of the last four layers as an embedding:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Concatenate the last four layers and use that as the embedding:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we have the BERT features, we train and evaluate the model using our usual
    methodology. We will show an example of an MLP here, but the same process can
    be repeated for all the classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: This completes our analysis of how transformers can be used to detect machine-generated
    text.
  prefs: []
  type: TYPE_NORMAL
- en: Compare and contrast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By now, we have explored several techniques for detecting bot-generated news.
    Here’s a list of all of them:'
  prefs: []
  type: TYPE_NORMAL
- en: Hand-crafted features such as function words, punctuation words, and automated
    readability index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-IDF scores for words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Word2Vec embeddings:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Averaged across the text for all words
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Concatenated for each word across the text
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BERT embeddings:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using only the last hidden state
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the sum of all hidden states
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the sum of the last four hidden states
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the concatenation of the last four hidden states
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see that we have eight feature sets at our disposal. Additionally, we
    experimented with four different models: random forests, logistic regression,
    SVM, and deep neural network (MLP). This means that we have a total of 32 configurations
    (feature set `x` model) that we can use for building a classifier to detect bot-generated
    fake news.'
  prefs: []
  type: TYPE_NORMAL
- en: I leave it up to you to construct this 8x4 matrix and determine which is the
    best approach among all of them!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we described approaches and techniques for detecting bot-generated
    fake news. With the rising prowess of artificial intelligence and the widespread
    availability of language models, attackers are using automated text generation
    to run bots on social media. These sock-puppet accounts can generate real-looking
    responses, posts, and, as we saw, even news-style articles. Data scientists in
    the security space, particularly those working in the social media domain, will
    often be up against attackers who leverage AI to spew out text and carpet-bomb
    a platform.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter aims to equip practitioners against such adversaries. We began
    by understanding how text generation exactly works and created our own dataset
    for machine learning experiments. We then used a variety of features (hand-crafted,
    TF-IDF, and word embeddings) to detect the bot-generated text. Finally, we used
    contextual embeddings to build improved mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study the problem of authorship attribution and
    obfuscation and the social and technical issues surrounding it.
  prefs: []
  type: TYPE_NORMAL
