["```py\n    https://xxxxxxxxx@dev.azure.com/xxxxx/Learn_MLOps/_git/Learn_MLOps\n    ```", "```py\n    git clone https://user:password_hash@dev.azure.com/user/repo_created\n    ```", "```py\n    git clone https://xxxxxxxxx@dev.azure.com/xxxxx/Learn_MLOps/_git/Learn_MLOps\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import warnings\n    from math import sqrt\n    warnings.filterwarnings('ignore')\n    from azureml.core.run import Run\n    from azureml.core.experiment import Experiment\n    from azureml.core.workspace import Workspace\n    from azureml.core.model import Model\n    from azureml.core.authentication import ServicePrincipalAuthentication\n    from azureml.train.automl import AutoMLConfig\n    import pickle\n    from matplotlib import pyplot as plt\n    from matplotlib.pyplot import figure\n    import mlflow\n    ```", "```py\n    uri = workspace.mlruns folder where MLflow artifacts and logs will be saved for experiments. \n    ```", "```py\n    from azureml.core import Workspace, Dataset\n    subscription_id = 'xxxxxx-xxxxxx-xxxxxxx-xxxxxxx'\n    resource_group = 'Learn_MLOps'\n    workspace_name = 'MLOps_WS'\n    workspace = subscription_id, resource_group, and workspace_name and initiate a workspace object using these credentials. When these instructions are successfully executed in the JupyterLab, you can run the remaining blocks of code in the next cells.\n    ```", "```py\n    # Importing pre-processed dataset\n    dataset = Dataset.get_by_name (workspace, name='processed_weather_data_portofTurku')\n    print(dataset.name, dataset.version)\n    ```", "```py\n    df_training = df.iloc[:77160]\n    df_test = df.drop(df_training.index)\n    df_training.to_csv('Data/training_data.csv',index=False)\n    df_test.to_csv('Data/test_data.csv',index=False)\n    ```", "```py\n    datastore = workspace.get_default_datastore()\n    datastore.upload(src_dir='Data', target_path='data')\n    training_dataset = /\n    Dataset.Tabular.from_delimited_files(datastore.path('data/training_data.csv'))\n    validation_dataset = /\n    Dataset.Tabular.from_delimited_files(datastore.path('data/validation_data.csv'))\n    training_ds = training_dataset.register(workspace=workspace, name='training_dataset',\n    description='Dataset to use for ML training')\n    test_ds = validation_dataset.register(workspace=workspace,\n                                     name='test_dataset',\n    description='Dataset for validation ML models')\n    ```", "```py\ndataset = Dataset.get_by_name (workspace, name='training_dataset')\nprint(dataset.name, dataset.version)\ndf = dataset.to_pandas_dataframe ( )\n```", "```py\n    X = df[['Temperature_C', 'Humidity', 'Wind_speed_kmph', 'Wind_bearing_degrees', 'Visibility_km', 'Pressure_millibars', 'Current_weather_condition']].values\n    y = df['Future_weather_condition'].values\n    ```", "```py\n    # Splitting the Training dataset into Train and Test set for ML training\n    from sklearn.model_selection import train_test_split\n    X_train, X_val,  y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n    ```", "```py\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_val = sc.transform(X_val)\n    ```", "```py\n    myexperiment = Experiment(workspace, \"support-vector-machine\")\n    ```", "```py\n    mlflow.set_experiment(\"mlflow-support-vector-machine\")\n    ```", "```py\n    from sklearn.svm import SVC\n    from sklearn import svm \n    from sklearn.model_selection import GridSearchCV\n    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n    svc = svm.STANDARD_DS11_V2 (2 cores, 14 GB RAM) compute machine). The result or the output of the Grid Search suggests the best performing parameters to be C=1 and the kernel as rbf. Using run.log(), we have logged the dataset used to train the model (the training set) and keep track of the experiment. This data is logged to the Azure ML workspace and the MLflow experiments. \n    ```", "```py\n    svc = SVC(C=svc_grid.get_params(deep=True)['estimator__C'], kernel=svc_grid.get_params(deep=True)['estimator__kernel'])\n    svc.fit(X_train, y_train)\n    # Logging training parameters to AzureML and MLFlow experiments\n    run.log(\"C\", svc_grid.get_params(deep=True)['estimator__C'])\n    run.log(\"Kernel\", svc_grid.get_params(deep=True)['estimator__kernel'])\n    After training the SVC classifier, the following output is shown:\n    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n      kernel='rbf', max_iter=-1, probability=False, random_state=None,\n      shrinking=True, tol=0.001, verbose=False)\n    ```", "```py\n    myexperiment = Experiment(workspace, \"support-vector-machine\")\n    mlflow.set_experiment(\"mlflow-support-vector-machine\")\n    ```", "```py\n    from sklearn.ensemble import RandomForestClassifier\n    rf = RandomForestClassifier (max_depth=10, random_state=0, n_estimators=100)\n    ```", "```py\n    # initialize runs in Azureml and mlflow\n    run = myexperiment.start_logging()\n    mlflow.start_run()\n    # Log dataset used \n    run.log(\"dataset name\", dataset.name)\n    run.log(\"dataset Version\", dataset.version)\n    rf.fit(X_train, y_train)\n    # Logging training parameters to AzureML and MLFlow experiments\n    run.log(\"max_depth\", 10)\n    run.log(\"random_state\", 0)\n    run.log(\"n_estimators\", 100)\n    ```", "```py\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n    max_depth=10, max_features='auto', max_leaf_nodes=None,\n    min_impurity_decrease=0.0, min_impurity_split=None,\n    min_samples_leaf=1, min_samples_split=2,\n    min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n    oob_score=False, random_state=0, verbose=0, warm_start=False)\n    ```", "```py\npredicted_svc = svc.predict(X_test)\nacc = accuracy_score(y_test, predicted_svc)\nfscore = f1_score(y_test, predicted_svc, average=\"macro\")\nprecision = precision_score(y_test, predicted_svc, average=\"macro\")\nrecall = recall_score(y_test, predicted_svc, average=\"macro\")\nrun.log(\"Test_accuracy\", acc)\nrun.log(\"Precision\", precision)\nrun.log(\"Recall\", recall)\nrun.log(\"F-Score\", fscore)\nrun.log(\"Git-sha\", sha)\n```", "```py\nacc = accuracy_score(y_test, predicted_rf)\nfscore = f1_score(y_test, predicted_rf, average=\"macro\")\nprecision = precision_score(y_test, predicted_rf, average=\"macro\")\nrecall = recall_score(y_test, predicted_rf, average=\"macro\")\nrun.log(\"Test_accuracy\", acc)\nrun.log(\"Precision\", precision)\nrun.log(\"Recall\", recall)\nrun.log(\"F-Score\", fscore)\nrun.log(\"Git-sha\", sha)\n```", "```py\n# Convert into SVC model into ONNX format file\nfrom skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\ninitial_type = [('float_input', FloatTensorType([None, 6]))]\nonx = convert_sklearn(svc, initial_types=initial_type)\nwith open(\"outputs/svc.onnx\", \"wb\") as f:\n    f.write(onx.SerializeToString())\n```", "```py\n# Convert into RF model into ONNX format file\nfrom skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\ninitial_type = [('float_input', FloatTensorType([None, 6]))]\nonx = convert_sklearn(rf, initial_types=initial_type)\nwith open(\"outputs/rf.onnx\", \"wb\") as f:\n    f.write(onx.SerializeToString())\n```", "```py\n# Register Model on AzureML WS\nmodel = Model.register (model_path = './outputs/svc.onnx', # this points to a local file \n                       model_name = \"support-vector-classifier\", \n                       tags = {'dataset': dataset.name, 'version': dataset.version, 'hyparameter-C': '1', 'testdata-accuracy': '0.9519'}, \n                       model_framework='pandas==0.23.4',\n                       description = \"Support vector classifier to predict weather at port of Turku\",\n                       workspace = workspace)\nprint('Name:', model.name)\nprint('Version:', model.version)\n```", "```py\n# Register Model on AzureML WS\nmodel = Model.register (model_path = './outputs/rf.onnx', # this points to a local file \n                       model_name = \"random-forest-classifier\", \n                       tags = {'dataset': dataset.name, 'version': dataset.version, 'hyparameter-C': '1', 'testdata-accuracy': '0.9548'}, \n                       model_framework='pandas==0.23.4',\n                       description = \"Random forest classifier to predict weather at port of Turku\",\n                       workspace = workspace)\nprint('Name:', model.name)\nprint('Version:', model.version)\n```", "```py\n    pickle.dump(sc, scaler_pkl)\n```", "```py\n# Register Model on AzureML WS\nscaler = Model.register(model_path = './outputs/scaler.pkl', # this points to a local file \n                       model_name = \"scaler\", # this is the name the model is registered as\n                       tags = {'dataset': dataset.name, 'version': dataset.version}, \n                       model_framework='pandas==0.23.4',\n                       description = \"Scaler used for scaling incoming inference data\",\n                       workspace = workspace)\nprint('Name:', scaler.name)\nprint('Version:', scaler.version)\n```"]