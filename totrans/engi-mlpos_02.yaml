- en: 'Chapter 1: Fundamentals of an MLOps Workflow'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) is maturing from research to applied business
    solutions. However, the grim reality is that only 2% of companies using ML have
    successfully deployed a model in production to enhance their business processes,
    reported by DeepLearning.AI ([https://info.deeplearning.ai/the-batch-companies-slipping-on-ai-goals-self-training-for-better-vision-muppets-and-models-china-vs-us-only-the-best-examples-proliferating-patents](https://info.deeplearning.ai/the-batch-companies-slipping-on-ai-goals-self-training-for-better-vision-muppets-and-models-china-vs-us-only-the-best-examples-proliferating-patents)).
    What makes it so hard? And what do we need to do to improve the situation?'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: To get a solid understanding of this problem and its solution, in this chapter,
    we will delve into the evolution and intersection of software development and
    ML. We'll begin by reflecting on some of the trends in traditional software development,
    starting from the waterfall model to agile to DevOps practices, and how these
    are evolving to industrialize ML-centric applications. You will be introduced
    to a systematic approach to operationalizing AI using **Machine Learning Operations**
    (**MLOps**). By the end of this chapter, you will have a solid understanding of
    MLOps and you will be equipped to implement a generic MLOps workflow that can
    be used to build, deploy, and monitor a wide range of ML applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of infrastructure and software development
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional software development challenges
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trends of ML adoption in software development
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding MLOps
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concepts and workflow of MLOps
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evolution of infrastructure and software development
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the genesis of the modern internet age (around 1995), we witnessed a rise
    in software applications, ranging from operating systems such as Windows 95 to
    the Linux operating system and websites such as Google and Amazon, which have
    been serving the world (online) for over two decades. This has resulted in a culture
    of continuously improving services by collecting, storing, and processing a massive
    amount of data from user interactions. Such developments have been shaping the
    evolution of IT infrastructure and software development.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Transformation in IT infrastructure has picked up pace since the start of this
    millennium. Since then, businesses have increasingly adopted cloud computing as
    it opens up new possibilities for businesses to outsource IT infrastructure maintenance
    while provisioning necessary IT resources such as storage and computation resources
    and services required to run and scale their operations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing offers on-demand provisioning and the availability of IT resources
    such as data storage and computing resources without the need for active management
    by the user of the IT resources. For example, businesses provisioning computation
    and storage resources do not have to manage these resources directly and are not
    responsible for keeping them running – the maintenance is outsourced to the cloud
    service provider.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算提供了按需提供和IT资源（如数据存储和计算资源）的可用性，无需用户对IT资源进行主动管理。例如，提供计算和存储资源的公司无需直接管理这些资源，也不负责保持它们运行——维护工作外包给了云服务提供商。
- en: Businesses using cloud computing can reap benefits as there's no need to buy
    and maintain IT resources; it enables them to have less in-house expertise for
    IT resource maintenance and this allows businesses to optimize costs and resources.
    Cloud computing enables scaling on demand and users pay as per the usage of resources.
    As a result, we have seen companies adopting cloud computing as part of their
    businesses and IT infrastructures.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用云计算的企业可以从中获益，因为它们无需购买和维护IT资源；这使它们在IT资源维护方面的内部专业知识需求减少，从而允许企业优化成本和资源。云计算使按需扩展成为可能，用户按资源使用量付费。因此，我们看到许多公司已经开始将云计算作为其业务和IT基础设施的一部分。
- en: Cloud computing became popular in the industry from 2006 onward when Sun Microsystems
    launched Sun Grid in March 2006\. It is a hardware and data resource sharing service.
    This service was acquired by Oracle and was later named Sun Cloud. Parallelly,
    in the same year (2006), another cloud computing service was launched by Amazon
    called Elastic Compute Cloud. This enabled new possibilities for businesses to
    provision computation, storage, and scaling capabilities on demand. Since then,
    the transformation across industries has been organic toward adopting cloud computing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从2006年开始，云计算在业界变得流行，当时太阳微系统公司在2006年3月推出了Sun Grid。这是一个硬件和数据资源共享服务。这项服务被甲骨文公司收购，后来更名为Sun
    Cloud。同时，在同年（2006年），亚马逊推出了另一项名为弹性计算云（Elastic Compute Cloud）的云计算服务。这为企业在按需提供计算、存储和扩展能力方面开辟了新的可能性。从那时起，各行业向采用云计算的转变已经是有机发生的。
- en: In the last decade, many companies on a global and regional scale have catalyzed
    the cloud transformation, with companies such as Google, IBM, Microsoft, UpCloud,
    Alibaba, and others heavily investing in the research and development of cloud
    services. As a result, a shift from localized computing (companies having their
    own servers and data centers) to on-demand computing has taken place due to the
    availability of robust and scalable cloud services. Now businesses and organizations
    are able to provision resources on-demand on the cloud to satisfy their data processing
    needs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，许多全球和区域性的公司推动了云转型，例如谷歌、IBM、微软、UpCloud、阿里巴巴等公司都在云计算服务的研发上进行了大量投资。因此，由于强大的可扩展云服务的可用性，从本地化计算（公司拥有自己的服务器和数据中心）向按需计算转变已经发生。现在，企业和组织能够在云上按需提供资源，以满足他们的数据处理需求。
- en: With these developments, we have witnessed **Moore's law** in operation, which
    states that the number of transistors on a microchip doubles every 2 years – though
    the cost of computers has halved, this has been true so far. Subsequently, some
    trends are developing as follows.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这些发展，我们见证了**摩尔定律**在发挥作用，该定律指出，每两年微芯片上的晶体管数量翻一番——尽管计算机的成本减半，但这至今一直如此。随后，一些趋势正在如下发展。
- en: The rise of machine learning and deep learning
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习和深度学习的兴起
- en: Over the last decade, we have witnessed the adoption of ML in everyday life
    applications. Not only for esoteric applications such as **Dota** or **AlphaGo**,
    but ML has also made its way to pretty standard applications such as machine translation,
    image processing, and voice recognition.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，我们见证了机器学习在日常生活中的应用。不仅限于像**Dota**或**AlphaGo**这样的专业应用，机器学习也进入了相当标准的应用，如机器翻译、图像处理和语音识别。
- en: 'This adoption is powered by developments in infrastructure, especially in terms
    of the utilization of computation power. It has unlocked the potential of deep
    learning and ML.. We can observe deep learning breakthroughs correlated with computation
    developments in *Figure 1.1* (sourced from OpenAI: [https://openai.com/blog/ai-and-compute](https://openai.com/blog/ai-and-compute)):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这种采用得益于基础设施的发展，特别是在计算能力的利用方面。它释放了深度学习和机器学习的潜力。我们可以从*图1.1*（来源：OpenAI：[https://openai.com/blog/ai-and-compute](https://openai.com/blog/ai-and-compute)）中观察到与计算发展相关的深度学习突破：
- en: '![Figure 1.1 – Demand for deep learning over time supported by computation'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1 – 随时间推移由计算支持的深度学习需求'
- en: '](img/B16572_01_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_01_01.jpg)'
- en: Figure 1.1 – Demand for deep learning over time supported by computation
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 随时间推移由计算支持的深度学习需求
- en: These breakthroughs in deep learning are enabled by the exponential growth in
    computing, which increases around 35 times every 18 months. Looking ahead in time,
    with such demands we may hit roadblocks in terms of scaling up central computing
    for CPUs, GPUs, or TPUs. This has forced us to look at alternatives such as **distributed
    learning** where computation for data processing is distributed across multiple
    computation nodes. We have seen some breakthroughs in distributed learning, such
    as federated learning and edge computing approaches. Distributed learning has
    shown promise to serve the growing demands of deep learning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域的这些突破得益于计算能力的指数级增长，大约每18个月增长35倍。展望未来，面对这样的需求，我们可能会在扩大CPU、GPU或TPU的中心计算规模方面遇到瓶颈。这迫使我们考虑替代方案，例如**分布式学习**，其中数据处理计算分布在多个计算节点上。我们已经看到了分布式学习的一些突破，例如联邦学习和边缘计算方法。分布式学习有望满足深度学习不断增长的需求。
- en: The end of Moore's law
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摩尔定律的终结
- en: Prior to 2012, AI results closely tracked Moore's law, with compute doubling
    every 2 years. Post-2012, compute has been doubling every 3.4 months (sourced
    from AI Index 2019 – [https://hai.stanford.edu/research/ai-index-2019](https://hai.stanford.edu/research/ai-index-2019)).
    We can observe from *Figure 1.1* that demand for deep learning and **high-performance
    computing** (**HPC**) has been increasing exponentially with around 35x growth
    in computing every 18 months whereas Moore's law is seen to be outpaced (2x every
    18 months). Moore's law is still applicable to the case of CPUs (single-core performance)
    but not to new hardware architectures such as GPUs and TPUs. This makes Moore's
    law obsolete and outpaced in contrast to current demands and trends.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在2012年之前，AI的结果与摩尔定律紧密相关，计算能力每两年翻一番。2012年之后，计算能力每3.4个月翻一番（来源：2019年AI指数 – [https://hai.stanford.edu/research/ai-index-2019](https://hai.stanford.edu/research/ai-index-2019)）。从*图1.1*中我们可以观察到，深度学习和高性能计算（HPC）的需求正以每18个月约35倍的速度呈指数增长，而摩尔定律似乎已经落后（每18个月翻一番）。摩尔定律仍然适用于CPU（单核性能）的情况，但不适用于GPU和TPU等新的硬件架构。这使得摩尔定律在当前需求和趋势面前变得过时和落后。
- en: AI-centric applications
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以AI为中心的应用程序
- en: 'Applications are becoming AI-centric – we see that across multiple industries.
    Virtually every application is starting to use AI, and these applications are
    running separately on distributed workloads such as **HPC**, **microservices**,
    and **big data**, as shown in *Figure 1.2*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序正变得越来越以AI为中心——我们在多个行业中都能看到这一点。几乎每个应用程序都开始使用AI，并且这些应用程序正在分布式工作负载上分别运行，如图*图1.2*所示：**高性能计算**（HPC）、**微服务**和**大数据**：
- en: '![Figure 1.2 – Applications running on distributed workloads'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.2 – 在分布式工作负载上运行的应用程序'
- en: '](img/B16572_01_002.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16572_01_002.jpg)'
- en: Figure 1.2 – Applications running on distributed workloads
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 在分布式工作负载上运行的应用程序
- en: By combining HPC and AI, we can enable the benefits of computation needed to
    train deep learning and ML models. With the overlapping of big data and AI, we
    can leverage extracting required data at scale for AI model training, and with
    the overlap of microservices and AI we can serve the AI models for inference to
    enhance business operations and impact. This way, distributed applications have
    become the new norm. Developing AI-centric applications at scale requires a synergy
    of distributed applications (HPC, microservices, and big data) and for this, a
    new way of developing software is required.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Software development evolution
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Software development has evolved hand in hand with infrastructural developments
    to facilitate the efficient development of applications using the infrastructure.
    Traditionally, software development started with the waterfall method of development
    where development is done linearly by gathering requirements to design and develop.
    The waterfall model has many limitations, which led to the evolution of software
    development over the years in the form of Agile methodologies and the DevOps method,
    as shown in *Figure 1.3*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Software development evolution'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_003.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.3 – Software development evolution
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: The waterfall method
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **waterfall method** was used to develop software from the onset of the
    internet age (~1995). It is a non-iterative way of developing software. It is
    delivered in a unidirectional way. Every stage is pre-organized and executed one
    after another, starting from requirements gathering to software design, development,
    and testing. The waterfall method is feasible and suitable when requirements are
    well-defined, specific, and do not change over time. Hence this is not suitable
    for dynamic projects where requirements change and evolve as per user demands.
    In such cases, where there is continuous modification, the waterfall method cannot
    be used to develop software. These are the major disadvantages of waterfall development
    methods:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The entire set of requirements has to be given before starting the development;
    modifying them during or after the project development is not possible.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are fewer chances to create or implement reusable components.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing can only be done after the development is finished. Testing is not intended
    to be iterable; it is not possible to go back and fix anything once it is done.
    Moreover, customer acceptance tests often introduced changes, resulting in a delay
    in delivery and high costs. This way of development and testing can have a negative
    impact on the project delivery timeline and costs.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the time, users of the system are provisioned with a system based on
    the developer's understanding, which is not user-centric and can come short of
    meeting their needs.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Agile method
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **Agile method** facilitates an iterative and progressive approach to software
    development. Unlike the waterfall method, Agile approaches are precise and user-centric.
    The method is bidirectional and often involves end users or customers in the development
    and testing process so they have the opportunity to test, give feedback, and suggest
    improvements throughout the project development process and phases. Agile has
    several advantages over the waterfall method:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Requirements are defined before starting the development, but they can be modified
    at any time.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to create or implement reusable components.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution or project can be modular by segregating the project into different
    modules that are delivered periodically.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The users or customers can co-create by testing and evaluating developed solution
    modules periodically to ensure the business needs are satisfied. Such a user-centric
    process ensures quality outcomes focused on meeting customer and business needs.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the difference between **Waterfall** and **Agile**
    methodologies:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Difference between waterfall and agile methods'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_004.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.4 – Difference between waterfall and agile methods
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The DevOps method
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **DevOps method** extends agile development practices by further streamlining
    the movement of software change through the *build*, *test*, *deploy*, and *delivery*
    stages. DevOps empowers cross-functional teams with the autonomy to execute their
    software applications driven by continuous integration, continuous deployment,
    and continuous delivery. It encourages collaboration, integration, and automation
    among software developers and IT operators to improve the efficiency, speed, and
    quality of delivering customer-centric software. DevOps provides a streamlined
    software development framework for designing, testing, deploying, and monitoring
    systems in production. DevOps has made it possible to ship software to production
    in minutes and to keep it running reliably.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Traditional software development challenges
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we observed the shift in traditional software development
    from the waterfall model to agile and DevOps practices. Agile and DevOps practices
    have enabled companies to ship software reliably. DevOps has made it possible
    to ship software to production in minutes and to keep it running reliably. This
    approach has been so successful that many companies are already adopting it, so
    why can't we keep doing the same thing for ML applications?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'The leading cause is that there''s a fundamental difference between ML development
    and traditional software development: *Machine learning is not just code; it is
    code plus data*. A ML model is created by applying an algorithm (via code) to
    fit the data to result in a ML model, as shown in *Figure 1.5*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Machine learning = data + code'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_005.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.5 – Machine learning = data + code
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'While code is meticulously crafted in the development environment, data comes
    from multiple sources for training, testing, and inference. It is robust and changing
    over time in terms of volume, velocity, veracity, and variety. To keep up with
    evolving data, code evolves over time. For perspective, their relationship can
    be observed as if code and data live in separate planes that share the time dimension
    but are independent in all other aspects. The challenge of an ML development process
    is to create a bridge between these two planes in a controlled way:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Data and code progression over time'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_006.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.6 – Data and code progression over time
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Data and code, with the progression of time, end up going in two directions
    with one objective of building and maintaining a robust and scalable ML system.
    This disconnect causes several challenges that need to be solved by anyone trying
    to put a ML model in production. It comes with challenges such as slow, brittle,
    fragmented, and inconsistent deployment, and a lack of reproducibility and traceability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome these challenges, MLOps offers a systematic approach by bridging
    data and code together over the progression of time. This is the solution to challenges
    posed by traditional software development methods with regard to ML applications.
    Using the MLOps method, data and code progress over time in one direction with
    one objective of building and maintaining a robust and scalable ML system:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – MLOps – data and code progressing together'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_007.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.7 – MLOps – data and code progressing together
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: MLOps facilitates ML model development, deployment, and monitoring in a streamlined
    and systematic approach. It empowers data science and IT teams to collaborate,
    validate, and govern their operations. All the operations executed by the teams
    are recorded or audited, end-to-end traceable, and repeatable. In the coming sections,
    we will learn how MLOps enables data science and IT teams to build and maintain
    robust and scalable ML systems.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Trends of ML adoption in software development
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we delve into the workings of the MLOps method and workflow, it is beneficial
    to understand the big picture and trends as to where and how MLOps is disrupting
    the world. As many applications are becoming AI-centric, software development
    is evolving to facilitate ML. ML will increasingly become part of software development,
    mainly due to the following reasons:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '**Investments**: In 2019, investments in global private AI clocked over $70
    billion, with start-up investments related to AI over $37 billion, M&A $34 billion,
    IPOs $5 billion, and minority stake valued at around $2 billion. The forecast
    for AI globally shows fast growth in market value as AI reached $9.5 billion in
    2018 and is anticipated to reach a market value of $118 billion by 2025\. It has
    been assessed that growth in economic activity resulting from AI until 2030 will
    be of high value and significance. Currently, the US attracts ~50% of global VC
    funding, China ~39%, and 11% goes to Europe.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投资**：2019年，全球私人人工智能投资超过700亿美元，其中与人工智能相关的初创企业投资超过370亿美元，并购投资340亿美元，首次公开募股（IPO）50亿美元，以及约20亿美元的少数股权。全球人工智能的市场价值预测显示，随着2018年人工智能达到95亿美元，预计到2025年将达到1180亿美元。据评估，到2030年，由人工智能引发的经济活动增长将具有很高的价值和意义。目前，美国吸引了约50%的全球风险投资（VC），中国约39%，11%流向欧洲。'
- en: '**Big data**: Data is exponentially growing in volume, velocity, veracity,
    and variety. For instance, observations suggest data growing in volume at 61%
    per annum in Europe, and it is anticipated that four times more data will be created
    by 2025 than exists today. Data is a requisite raw material for developing AI.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大数据**：数据在量、速度、真实性和多样性方面呈指数级增长。例如，观察表明，在欧洲，数据量每年增长61%，预计到2025年，将比现在多出四倍的数据。数据是开发人工智能的必需原材料。'
- en: '**Infrastructural developments and adoption**: Moore''s law has been closely
    tracked and observed to have been realized prior to 2012\. Post-2012, compute
    has been doubling every 3.4 months.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施发展和采用**：摩尔定律一直被密切关注，并在2012年之前得以实现。2012年之后，计算能力每3.4个月翻一番。'
- en: '**Increasing research and development**: AI research has been prospering in
    quality and quantity. A prominent growth of 300% is observed in the volume of
    peer-reviewed AI papers from 1998 to 2018, summing up to 9% of published conference
    papers and 3% of peer-reviewed journal publications.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**研发增长**：人工智能研究在质量和数量上都在蓬勃发展。从1998年到2018年，同行评审的人工智能论文数量增长了300%，总计占已发表会议论文的9%和同行评审期刊发表的3%。'
- en: '**Industry**: Based on a surveyed report, 47% of large companies have reported
    having adopted AI in at least one function or business unit. In 2019, it went
    up to 58% and is expected to increase.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**产业**：根据一份调查报告，47%的大公司报告称至少在一个职能或业务单元中采用了人工智能。到2019年，这一比例上升至58%，预计还将继续增长。'
- en: Information
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息
- en: These points have been sourced from policy and investment recommendations for
    trustworthy AI – European commission ([https://ec.europa.eu/digital-single-market/en/news/policy-and-investment-recommendations-trustworthy-artificial-intelligence](https://ec.europa.eu/digital-single-market/en/news/policy-and-investment-recommendations-trustworthy-artificial-intelligence))
    and AI Index 2019 ([https://hai.stanford.edu/research/ai-index-2019](https://hai.stanford.edu/research/ai-index-2019)).
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些观点来源于对可信赖人工智能的政策和投资建议——欧洲委员会([https://ec.europa.eu/digital-single-market/en/news/policy-and-investment-recommendations-trustworthy-artificial-intelligence](https://ec.europa.eu/digital-single-market/en/news/policy-and-investment-recommendations-trustworthy-artificial-intelligence))和AI指数2019([https://hai.stanford.edu/research/ai-index-2019](https://hai.stanford.edu/research/ai-index-2019))。
- en: All these developments indicate a strong push toward the industrialization of
    AI, and this is possible by bridging industry and research. MLOps will play a
    key role in the industrialization of AI. If you invest in learning this method,
    it will give you a headstart in your company or team and you could be a catalyst
    for operationalizing ML and industrializing AI.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些发展都表明，人工智能的工业化进程正在加速，这是通过连接产业和研究实现的。MLOps将在人工智能的工业化中扮演关键角色。如果你投资学习这种方法，它将使你在公司或团队中领先一步，你可能会成为实现机器学习和工业化的催化剂。
- en: So far, we have learned about some challenges and developments in IT, software
    development, and AI. Next, we will delve into understanding MLOps conceptually
    and learn in detail about a generic MLOps workflow that can be used commonly for
    any use case. These fundamentals will help you get a firm grasp of MLOps.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了IT、软件开发和人工智能的一些挑战和发展。接下来，我们将从概念上深入了解MLOps，并详细学习一个通用的MLOps工作流程，该流程可以用于任何用例。这些基础知识将帮助你牢固掌握MLOps。
- en: Understanding MLOps
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解MLOps
- en: Software development is interdisciplinary and is evolving to facilitate ML.
    MLOps is an emerging method to fuse ML with software development by integrating
    multiple domains as MLOps combines ML, DevOps, and data engineering, which aims
    to build, deploy, and maintain ML systems in production reliably and efficiently.
    Thus, MLOps can be expounded by this intersection.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – MLOps intersection'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_008.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.8 – MLOps intersection
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this intersection (MLOps) operational, I have designed a modular framework
    by following the systematic *design science method proposed by Wieringa* ([https://doi.org/10.1007/978-3-662-43839-8](https://doi.org/10.1007/978-3-662-43839-8))
    to develop a workflow to bring these three together (Data Engineering, Machine
    Learning, and *DevOps*). Design science goes with the application of design to
    problems and context. Design science is the design and investigation of artifacts
    in a context. The artifact in this case is the MLOps workflow, which is designed
    iteratively by interacting with problem contexts (industry use cases for the application
    of AI):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Design science workflow'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_009.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.9 – Design science workflow
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: In a structured and iterative approach, the implementation of two cycles (the
    design cycle and the empirical cycle) was done for qualitative and quantitative
    analysis for MLOps workflow design through iterations. As a result of these cycles,
    an MLOps workflow is developed and validated by applying it to multiple problem
    contexts, that is, tens of ML use cases (for example, anomaly detection, real-time
    trading, predictive maintenance, recommender systems, virtual assistants, and
    so on) across multiple industries (for example, finance, manufacturing, healthcare,
    retail, the automotive industry, energy, and so on). I have applied and validated
    this MLOps workflow successfully in various projects across multiple industries
    to operationalize ML. In the next section, we will go through the concepts of
    the MLOps workflow designed as a result of the design science process.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Concepts and workflow of MLOps
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn about a generic MLOps workflow; it is the result
    of many design cycle iterations as discussed in the previous section. It brings
    together data engineering, ML, and DevOps in a streamlined fashion. *Figure 1.10*
    is a generic MLOps workflow; it is modular and flexible and can be used to build
    proofs of concept or to operationalize ML solutions in any business or industry:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10 – MLOps workflow'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_010.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.10 – MLOps workflow
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'This workflow is segmented into two modules:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '**MLOps pipeline** (build, deploy, and monitor) – the upper layer'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drivers**: Data, code, artifacts, middleware, and infrastructure – mid and
    lower layers'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The upper layer is the MLOps pipeline (build, deploy, and monitor), which is
    enabled by drivers such as data, code, artifacts, middleware, and infrastructure.
    The MLOps pipeline is powered by an array of services, drivers, middleware, and
    infrastructure, and it crafts ML-driven solutions. By using this pipeline, a business
    or individual(s) can do quick prototyping, testing, and validating and deploy
    the model(s) to production at scale frugally and efficiently.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: To understand the workings and implementation of the MLOps workflow, we will
    look at the implementation of each layer and step using a figurative business
    use case.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Discussing a use case
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this use case, we are to operationalize (prototyping and deploying for production)
    an image classification service to classify cats and dogs in a pet park in Barcelona,
    Spain. The service will identify cats and dogs in real time from the inference
    data coming from a CCTV camera installed in the pet park.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'The pet park provide you access to the data and infrastructure needed to operationalize
    the service:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '**Data**: The pet park has given you access to their data lake containing 100,000
    labeled images of cats and dogs, which we will use for training the model.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Infrastructure**: Public cloud (IaaS).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This use case resembles a real-life use case for operationalizing ML and is
    used to explain the workings and implementation of the MLOps workflow. Remember
    to look for an explanation for the implementation of this use case at every segment
    and step of the MLOps workflow. Now, let's look at the workings of every layer
    and step in detail.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The MLOps pipeline
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The MLOps pipeline is the upper layer, which performs operations such as build,
    deploy, and monitor, which work modularly in sync with each other. Let's look
    into each module's functionality.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Build
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The build module has the core ML pipeline, and this is purely for training,
    packaging, and versioning the ML models. It is powered by the required compute
    (for example, the CPU or GPU on the cloud or distributed computing) resources
    to run the ML training and pipeline:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – MLOps – build pipeline'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_11.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.11 – MLOps – build pipeline
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline works from left to right. Let''s look at the functionality of
    each step in detail:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ingestion**: This step is a trigger step for the ML pipeline. It deals
    with the volume, velocity, veracity, and variety of data by extracting data from
    various data sources (for example, databases, data warehouses, or data lakes)
    and ingesting the required data for the model training step. Robust data pipelines
    connected to multiple data sources enable it to perform **extract, transform,
    and load** (**ETL**) operations to provide the necessary data for ML training
    purposes. In this step, we can split and version data for model training in the
    required format (for example, the training or test set). As a result of this step,
    any experiment (that is, model training) can be audited and is back-traceable.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a better understanding of the data ingestion step, here is the previously
    described use case implementation:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As you have access to the pet park''s data lake, you can now procure data to
    get started. Using data pipelines (part of the data ingestion step), you do the
    following:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1\. Extract, transform, and load 100,000 images of cats and dogs.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Split and version this data into a train and test split (with an 80% and
    20% split).
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Versioning this data will enable end-to-end traceability for trained models.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Congrats – now you are ready to start training and testing the ML model using
    this data.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model training**: After procuring the required data for ML model training
    in the previous step, this step will enable model training; it has modular scripts
    or code that perform all the traditional steps in ML, such as data preprocessing,
    feature engineering, and feature scaling before training or retraining any model.
    Following this, the ML model is trained while performing hyperparameter tuning
    to fit the model to the dataset (training set). This step can be done manually,
    but efficient and automatic solutions such as **Grid Search** or **Random Search**
    exist. As a result, all important steps of ML model training are executed with
    a ML model as the output of this step.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this step, we implement all the important steps to train the image classification
    model. The goal is to train a ML model to classify cats and dogs. For this case,
    we train a **convolutional neural network** (**CNN** – [https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb](https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb))
    for the image classification service. The following steps are implemented: data
    preprocessing, feature engineering, and feature scaling before training, followed
    by training the model with hyperparameter tuning. As a result, we have a CNN model
    to classify cats and dogs with 97% accuracy.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model testing**: In this step, we evaluate the trained model performance
    on a separated set of data points named test data (which was split and versioned
    in the data ingestion step). The inference of the trained model is evaluated according
    to selected metrics as per the use case. The output of this step is a report on
    the trained model''s performance.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We test the trained model on test data (we split data earlier in the *Data ingestion*
    step) to evaluate the trained model's performance. In this case, we look for precision
    and the recall score to validate the model's performance in classifying cats and
    dogs to assess false positives and true positives to get a realistic understanding
    of the model's performance. If and when we are satisfied with the results, we
    can proceed to the next step, or else reiterate the previous steps to get a decent
    performing model for the pet park image classification service.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model packaging**: After the trained model has been tested in the previous
    step, the model can be serialized into a file or containerized (using Docker)
    to be exported to the production environment.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The model we trained and tested in the previous steps is serialized to an ONNX
    file and is ready to be deployed in the production environment.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model registering**: In this step, the model that was serialized or containerized
    in the previous step is registered and stored in the model registry. A registered
    model is a logical collection or package of one or more files that assemble, represent,
    and execute your ML model. For example, multiple files can be registered as one
    model. For instance, a classification model can be comprised of a vectorizer,
    model weights, and serialized model files. All these files can be registered as
    one single model. After registering, the model (all files or a single file) can
    be downloaded and deployed as needed.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The serialized model in the previous step is registered on the model registry
    and is available for quick deployment into the pet park production environment.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By implementing the preceding steps, we successfully execute the ML pipeline
    designed for our use case. As a result, we have trained models on the model registry
    ready to be deployed in the production setup. Next, we will look into the workings
    of the deployment pipeline.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deploy
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The deploy module enables operationalizing the ML models we developed in the
    previous module (build). In this module, we test our model performance and behavior
    in a production or production-like (test) environment to ensure the robustness
    and scalability of the ML model for production use. *Figure 1.12* depicts the
    deploy pipeline, which has two components – production testing and production
    release – and the deployment pipeline is enabled by streamlined CI/CD pipelines
    connecting the development to production environments:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12 – MLOps – deploy pipeline'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_12.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.12 – MLOps – deploy pipeline
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'It works from left to right. Let''s look at the functionality of each step
    in detail:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '**Application testing**: Before deploying an ML model to production, it is
    vital to test its robustness and performance via testing. Hence we have the "application
    testing" phase where we rigorously test all the trained models for robustness
    and performance in a production-like environment called a test environment. In
    the application testing phase, we deploy the models in the test environment (pre-production),
    which replicates the production environment.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ML model for testing is deployed as an API or streaming service in the test
    environment to deployment targets such as Kubernetes clusters, container instances,
    or scalable virtual machines or edge devices as per the need and use case. After
    the model is deployed for testing, we perform predictions using test data (which
    is not used for training the model; test data is sample data from a production
    environment) for the deployed model, during which model inference in batch or
    periodically is done to test the model deployed in the test environment for robustness
    and performance.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The performance results are automatically or manually reviewed by a quality
    assurance expert. When the ML model's performance meets the standards, then it
    is approved to be deployed in the production environment where the model will
    be used to infer in batches or real time to make business decisions.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We deploy the model as an API service on an on-premises computer in the pet
    park, which is set up for testing purposes. This computer is connected to a CCTV
    camera in the park to fetch real-time inference data to predict cats or dogs in
    the video frames. The model deployment is enabled by the CI/CD pipeline. In this
    step, we test the robustness of the model in a production-like environment, that
    is, whether the model is performing inference consistently, and an accuracy, fairness,
    and error analysis. At the end of this step, a quality assurance expert certifies
    the model if it meets the standards.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Production release**: Previously tested and approved models are deployed
    in the production environment for model inference to generate business or operational
    value. This production release is deployed to the production environment enabled
    by CI/CD pipelines.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We deploy a previously tested and approved model (by a quality assurance expert)
    as an API service on a computer connected to CCTV in the pet park (production
    setup). This deployed model performs ML inference on the incoming video data from
    the CCTV camera in the pet park to classify cats or dogs in real time.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Monitor
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The monitor module works in sync with the deploy module. Using explainable
    monitoring (discussed later in detail, in [*Chapter 11*](B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206),
    *Key Principles for Monitoring Your ML System*), we can monitor, analyze, and
    govern the deployed ML application (ML model and application). Firstly, we can
    monitor the performance of the ML model (using pre-defined metrics) and the deployed
    application (using telemetry data). Secondly, model performance can be analyzed
    using a pre-defined explainability framework, and lastly, the ML application can
    be governed using alerts and actions based on the model''s quality assurance and
    control. This ensures a robust monitoring mechanism for the production system:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13 – MLOps – monitor pipeline'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_13.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.13 – MLOps – monitor pipeline
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see each of the abilities of the monitor module in detail:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitor**: The monitoring module captures critical information to monitor
    data integrity, model drift, and application performance. Application performance
    can be monitored using telemetry data. It depicts the device performance of a
    production system over a period of time. With telemetry data such as accelerometer,
    gyroscope, humidity, magnetometer, pressure, and temperature we can keep a check
    on the production system''s performance, health, and longevity.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In real time, we will monitor three things – data integrity, model drift, and
    application performance – for the deployed API service on the park's computer.
    Metrics such as accuracy, F1 score, precision, and recall are tracked to data
    integrity and model drift. We monitor application performance by tracking the
    telemetry data of the production system (the on-premises computer in the park)
    running the deployed ML model to ensure the proper functioning of the production
    system. Telemetry data is monitored to foresee any anomalies or potential failures
    and fix them in advance. Telemetry data is logged and can be used to assess production
    system performance over time to check its health and longevity.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Analyze**: It is critical to analyze the model performance of ML models deployed
    in production systems to ensure optimal performance and governance in correlation
    to business decisions or impact. We use model explainability techniques to measure
    the model performance in real time. Using this, we evaluate important aspects
    such as model fairness, trust, bias, transparency, and error analysis with the
    intention of improving the model in correlation to business.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Over time, the statistical properties of the target variable we are trying
    to predict can change in unforeseen ways. This change is called "model drift,"
    for example, in a case where we have deployed a recommender system model to suggest
    suitable items for users. User behavior may change due to unforeseeable trends
    that could not be observed in historical data that was used for training the model.
    It is essential to consider such unforeseen factors to ensure deployed models
    provide the best and most relevant business value. When model drift is observed,
    then any of these actions should be performed:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) The product owner or the quality assurance expert needs to be alerted.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The model needs to be switched or updated.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Re-training the pipeline should be triggered to re-train and update the model
    as per the latest data or needs.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We monitor the deployed model's performance in the production system (a computer
    connected to the CCTV in the pet park). We will analyze the accuracy, precision,
    and recall scores for the model periodically (once a day) to ensure the model's
    performance does not deteriorate below the threshold. When the model performance
    deteriorates below the threshold, we initiate system governing mechanisms (for
    example, a trigger to retrain the model).
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Govern**: Monitoring and analyzing is done to govern the deployed application
    to drive optimal performance for the business (or the purpose of the ML system).
    After monitoring and analyzing the production data, we can generate certain alerts
    and actions to govern the system. For example, the product owner or the quality
    assurance expert gets alerted when model performance deteriorates (for example,
    low accuracy, high bias, and so on) below a pre-defined threshold. The product
    owner initiates a trigger to retrain and deploy an alternative model. Lastly,
    an important aspect of governance is "compliance" with the local and global laws
    and rules. For compliance, model explainability and transparency are vital. For
    this, model auditing and reporting are done to provide end-to-end traceability
    and explainability for production models.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We monitor and analyze the deployed model's performance in the production system
    (a computer connected to the CCTV in the pet park). Based on the analysis of accuracy,
    precision, and recall scores for the deployed model, periodically (once a day),
    alerts are generated when the model's performance deteriorates below the pre-defined
    threshold. The product owner of the park generates actions, and these actions
    are based on the alerts. For example, an alert is generated notifying the product
    owner that the production model is 30% biased to detect dogs more than cats. The
    product owner then triggers the model re-training pipeline to update the model
    using the latest data to reduce the bias, resulting in a fair and robust model
    in production. This way, the ML system at the pet park in Barcelona is well-governed
    to serve the business needs.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This brings us to the end of the MLOps pipeline. All models trained, deployed,
    and monitored using the MLOps method are end-to-end traceable and their lineage
    is logged in order to trace the origins of the model, which includes the source
    code the model used to train, the data used to train and test the model, and parameters
    used to converge the model. Full lineage is useful to audit operations or to replicate
    the model, or when a blocker is hit, the logged ML model lineage is useful to
    backtrack the origins of the model or to observe and debug the cause of the blocker.
    As ML models generate data in production during inference, this data can be tied
    to the model training and deployment lineage to ensure the end-to-end lineage,
    and this is important for certain compliance requirements. Next, we will look
    into key drivers enabling the MLOps pipeline.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Drivers
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These are the key drivers for the MLOps pipeline: data, code, artifacts, middleware,
    and infrastructure. Let''s look into each of the drivers to get an overview of
    how they enable the MLOps pipeline:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.14 – MLOps drivers'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_14.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.14 – MLOps drivers
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the key drivers for the MLOps pipeline are defined as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '**Data**: Data can be in multiple forms, such as text, audio, video, and images.
    In traditional software applications, data quite often tends to be structured,
    whereas, for ML applications, it can be structured or unstructured. To manage
    data in ML applications, data is handled in these steps: data acquisition, data
    annotation, data cataloging, data preparation, data quality checking, data sampling,
    and data augmentation. Each step involves its own life cycle. This makes a whole
    new set of processes and tools necessary for ML applications. For efficient functioning
    of the ML pipeline, data is segmented and versioned into training data, testing
    data, and monitoring data (collected in production, for example, model inputs,
    outputs, and telemetry data). These data operations are part of the MLOps pipeline.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code**: There are three essential modules of code that drive the MLOps pipeline:
    training code, testing code, and application code. These scripts or code are executed
    using the CI/CD and data pipelines to ensure the robust working of the MLOps pipeline.
    The source code management system (for example, using Git or Mercurial) will enable
    orchestration and play a vital role in managing and integrating seamlessly with
    CI, CD, and data pipelines. All of the code is staged and versioned in the source
    code management setup (for example, Git).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifacts**: The MLOps pipeline generates artifacts such as data, serialized
    models, code snippets, system logs, ML model training, and testing metrics information.
    All these artifacts are useful for the successful working of the MLOps pipeline,
    ensuring its traceability and sustainability. These artifacts are managed using
    middleware services such as the model registry, workspaces, logging services,
    source code management services, databases, and so on.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Middleware**: Middleware is computer software that offers services to software
    applications that are more than those available from the operating systems. Middleware
    services ensure multiple applications to automate and orchestrate processes for
    the MLOps pipeline. We can use a diverse set of middleware software and services
    depending on the use cases, for example, Git for source code management, VNets
    to enable the required network configurations, Docker for containerizing our models,
    and Kubernetes for container orchestration to automate application deployment,
    scaling, and management.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Infrastructure**: To ensure the successful working of the MLOps pipeline,
    we need essential compute and storage resources to train Test and deploy the ML
    models. Compute resources enable us to train, deploy and monitor our ML models.
    Two types of storages resources can facilitate ML operations, central storage
    and feature stores. A central storage stores the logs, artifacts, training, testing
    and monitoring data. A feature store is optional and complementary to central
    storage. It extracts, transforms and stores needed features for ML model training
    and inference using a feature pipeline. When it comes to the infrastructure, there
    are various options such as on-premises resources or **infrastructure as a service
    (IaaS)**, which is cloud services. These days, there are many cloud players providing
    IaaS, such as Amazon, Microsoft, Google, Alibaba, and so on. Having the right
    infrastructure for your use case will enable robust, efficient, and frugal operations
    for your team and company.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fully automated workflow is achievable with smart optimization and synergy
    of all these drivers with the MLOps pipeline. Some direct advantages of implementing
    an automated MLOps workflow is a spike in IT teams' efficiency (by reducing the
    time spent by data scientists and developers on mundane and repeatable tasks)
    and the optimization of resources, resulting in cost reductions, and both of these
    are great for any business.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about the evolution of software development
    and infrastructure to facilitate ML. We delved into the concepts of MLOps, followed
    by getting acquainted with a generic MLOps workflow that can be implemented in
    a wide range of ML solutions across multiple industries.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to characterize any ML problem into
    an MLOps-driven solution and start developing it using an MLOps workflow.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
