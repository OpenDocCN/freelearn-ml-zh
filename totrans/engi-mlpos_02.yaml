- en: 'Chapter 1: Fundamentals of an MLOps Workflow'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) is maturing from research to applied business
    solutions. However, the grim reality is that only 2% of companies using ML have
    successfully deployed a model in production to enhance their business processes,
    reported by DeepLearning.AI ([https://info.deeplearning.ai/the-batch-companies-slipping-on-ai-goals-self-training-for-better-vision-muppets-and-models-china-vs-us-only-the-best-examples-proliferating-patents](https://info.deeplearning.ai/the-batch-companies-slipping-on-ai-goals-self-training-for-better-vision-muppets-and-models-china-vs-us-only-the-best-examples-proliferating-patents)).
    What makes it so hard? And what do we need to do to improve the situation?'
  prefs: []
  type: TYPE_NORMAL
- en: To get a solid understanding of this problem and its solution, in this chapter,
    we will delve into the evolution and intersection of software development and
    ML. We'll begin by reflecting on some of the trends in traditional software development,
    starting from the waterfall model to agile to DevOps practices, and how these
    are evolving to industrialize ML-centric applications. You will be introduced
    to a systematic approach to operationalizing AI using **Machine Learning Operations**
    (**MLOps**). By the end of this chapter, you will have a solid understanding of
    MLOps and you will be equipped to implement a generic MLOps workflow that can
    be used to build, deploy, and monitor a wide range of ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of infrastructure and software development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional software development challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trends of ML adoption in software development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding MLOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concepts and workflow of MLOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evolution of infrastructure and software development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the genesis of the modern internet age (around 1995), we witnessed a rise
    in software applications, ranging from operating systems such as Windows 95 to
    the Linux operating system and websites such as Google and Amazon, which have
    been serving the world (online) for over two decades. This has resulted in a culture
    of continuously improving services by collecting, storing, and processing a massive
    amount of data from user interactions. Such developments have been shaping the
    evolution of IT infrastructure and software development.
  prefs: []
  type: TYPE_NORMAL
- en: Transformation in IT infrastructure has picked up pace since the start of this
    millennium. Since then, businesses have increasingly adopted cloud computing as
    it opens up new possibilities for businesses to outsource IT infrastructure maintenance
    while provisioning necessary IT resources such as storage and computation resources
    and services required to run and scale their operations.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing offers on-demand provisioning and the availability of IT resources
    such as data storage and computing resources without the need for active management
    by the user of the IT resources. For example, businesses provisioning computation
    and storage resources do not have to manage these resources directly and are not
    responsible for keeping them running – the maintenance is outsourced to the cloud
    service provider.
  prefs: []
  type: TYPE_NORMAL
- en: Businesses using cloud computing can reap benefits as there's no need to buy
    and maintain IT resources; it enables them to have less in-house expertise for
    IT resource maintenance and this allows businesses to optimize costs and resources.
    Cloud computing enables scaling on demand and users pay as per the usage of resources.
    As a result, we have seen companies adopting cloud computing as part of their
    businesses and IT infrastructures.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing became popular in the industry from 2006 onward when Sun Microsystems
    launched Sun Grid in March 2006\. It is a hardware and data resource sharing service.
    This service was acquired by Oracle and was later named Sun Cloud. Parallelly,
    in the same year (2006), another cloud computing service was launched by Amazon
    called Elastic Compute Cloud. This enabled new possibilities for businesses to
    provision computation, storage, and scaling capabilities on demand. Since then,
    the transformation across industries has been organic toward adopting cloud computing.
  prefs: []
  type: TYPE_NORMAL
- en: In the last decade, many companies on a global and regional scale have catalyzed
    the cloud transformation, with companies such as Google, IBM, Microsoft, UpCloud,
    Alibaba, and others heavily investing in the research and development of cloud
    services. As a result, a shift from localized computing (companies having their
    own servers and data centers) to on-demand computing has taken place due to the
    availability of robust and scalable cloud services. Now businesses and organizations
    are able to provision resources on-demand on the cloud to satisfy their data processing
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: With these developments, we have witnessed **Moore's law** in operation, which
    states that the number of transistors on a microchip doubles every 2 years – though
    the cost of computers has halved, this has been true so far. Subsequently, some
    trends are developing as follows.
  prefs: []
  type: TYPE_NORMAL
- en: The rise of machine learning and deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the last decade, we have witnessed the adoption of ML in everyday life
    applications. Not only for esoteric applications such as **Dota** or **AlphaGo**,
    but ML has also made its way to pretty standard applications such as machine translation,
    image processing, and voice recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'This adoption is powered by developments in infrastructure, especially in terms
    of the utilization of computation power. It has unlocked the potential of deep
    learning and ML.. We can observe deep learning breakthroughs correlated with computation
    developments in *Figure 1.1* (sourced from OpenAI: [https://openai.com/blog/ai-and-compute](https://openai.com/blog/ai-and-compute)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Demand for deep learning over time supported by computation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.1 – Demand for deep learning over time supported by computation
  prefs: []
  type: TYPE_NORMAL
- en: These breakthroughs in deep learning are enabled by the exponential growth in
    computing, which increases around 35 times every 18 months. Looking ahead in time,
    with such demands we may hit roadblocks in terms of scaling up central computing
    for CPUs, GPUs, or TPUs. This has forced us to look at alternatives such as **distributed
    learning** where computation for data processing is distributed across multiple
    computation nodes. We have seen some breakthroughs in distributed learning, such
    as federated learning and edge computing approaches. Distributed learning has
    shown promise to serve the growing demands of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: The end of Moore's law
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prior to 2012, AI results closely tracked Moore's law, with compute doubling
    every 2 years. Post-2012, compute has been doubling every 3.4 months (sourced
    from AI Index 2019 – [https://hai.stanford.edu/research/ai-index-2019](https://hai.stanford.edu/research/ai-index-2019)).
    We can observe from *Figure 1.1* that demand for deep learning and **high-performance
    computing** (**HPC**) has been increasing exponentially with around 35x growth
    in computing every 18 months whereas Moore's law is seen to be outpaced (2x every
    18 months). Moore's law is still applicable to the case of CPUs (single-core performance)
    but not to new hardware architectures such as GPUs and TPUs. This makes Moore's
    law obsolete and outpaced in contrast to current demands and trends.
  prefs: []
  type: TYPE_NORMAL
- en: AI-centric applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Applications are becoming AI-centric – we see that across multiple industries.
    Virtually every application is starting to use AI, and these applications are
    running separately on distributed workloads such as **HPC**, **microservices**,
    and **big data**, as shown in *Figure 1.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Applications running on distributed workloads'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.2 – Applications running on distributed workloads
  prefs: []
  type: TYPE_NORMAL
- en: By combining HPC and AI, we can enable the benefits of computation needed to
    train deep learning and ML models. With the overlapping of big data and AI, we
    can leverage extracting required data at scale for AI model training, and with
    the overlap of microservices and AI we can serve the AI models for inference to
    enhance business operations and impact. This way, distributed applications have
    become the new norm. Developing AI-centric applications at scale requires a synergy
    of distributed applications (HPC, microservices, and big data) and for this, a
    new way of developing software is required.
  prefs: []
  type: TYPE_NORMAL
- en: Software development evolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Software development has evolved hand in hand with infrastructural developments
    to facilitate the efficient development of applications using the infrastructure.
    Traditionally, software development started with the waterfall method of development
    where development is done linearly by gathering requirements to design and develop.
    The waterfall model has many limitations, which led to the evolution of software
    development over the years in the form of Agile methodologies and the DevOps method,
    as shown in *Figure 1.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Software development evolution'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.3 – Software development evolution
  prefs: []
  type: TYPE_NORMAL
- en: The waterfall method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **waterfall method** was used to develop software from the onset of the
    internet age (~1995). It is a non-iterative way of developing software. It is
    delivered in a unidirectional way. Every stage is pre-organized and executed one
    after another, starting from requirements gathering to software design, development,
    and testing. The waterfall method is feasible and suitable when requirements are
    well-defined, specific, and do not change over time. Hence this is not suitable
    for dynamic projects where requirements change and evolve as per user demands.
    In such cases, where there is continuous modification, the waterfall method cannot
    be used to develop software. These are the major disadvantages of waterfall development
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: The entire set of requirements has to be given before starting the development;
    modifying them during or after the project development is not possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are fewer chances to create or implement reusable components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing can only be done after the development is finished. Testing is not intended
    to be iterable; it is not possible to go back and fix anything once it is done.
    Moreover, customer acceptance tests often introduced changes, resulting in a delay
    in delivery and high costs. This way of development and testing can have a negative
    impact on the project delivery timeline and costs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the time, users of the system are provisioned with a system based on
    the developer's understanding, which is not user-centric and can come short of
    meeting their needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Agile method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **Agile method** facilitates an iterative and progressive approach to software
    development. Unlike the waterfall method, Agile approaches are precise and user-centric.
    The method is bidirectional and often involves end users or customers in the development
    and testing process so they have the opportunity to test, give feedback, and suggest
    improvements throughout the project development process and phases. Agile has
    several advantages over the waterfall method:'
  prefs: []
  type: TYPE_NORMAL
- en: Requirements are defined before starting the development, but they can be modified
    at any time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to create or implement reusable components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution or project can be modular by segregating the project into different
    modules that are delivered periodically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The users or customers can co-create by testing and evaluating developed solution
    modules periodically to ensure the business needs are satisfied. Such a user-centric
    process ensures quality outcomes focused on meeting customer and business needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the difference between **Waterfall** and **Agile**
    methodologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Difference between waterfall and agile methods'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.4 – Difference between waterfall and agile methods
  prefs: []
  type: TYPE_NORMAL
- en: The DevOps method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **DevOps method** extends agile development practices by further streamlining
    the movement of software change through the *build*, *test*, *deploy*, and *delivery*
    stages. DevOps empowers cross-functional teams with the autonomy to execute their
    software applications driven by continuous integration, continuous deployment,
    and continuous delivery. It encourages collaboration, integration, and automation
    among software developers and IT operators to improve the efficiency, speed, and
    quality of delivering customer-centric software. DevOps provides a streamlined
    software development framework for designing, testing, deploying, and monitoring
    systems in production. DevOps has made it possible to ship software to production
    in minutes and to keep it running reliably.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional software development challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we observed the shift in traditional software development
    from the waterfall model to agile and DevOps practices. Agile and DevOps practices
    have enabled companies to ship software reliably. DevOps has made it possible
    to ship software to production in minutes and to keep it running reliably. This
    approach has been so successful that many companies are already adopting it, so
    why can't we keep doing the same thing for ML applications?
  prefs: []
  type: TYPE_NORMAL
- en: 'The leading cause is that there''s a fundamental difference between ML development
    and traditional software development: *Machine learning is not just code; it is
    code plus data*. A ML model is created by applying an algorithm (via code) to
    fit the data to result in a ML model, as shown in *Figure 1.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Machine learning = data + code'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.5 – Machine learning = data + code
  prefs: []
  type: TYPE_NORMAL
- en: 'While code is meticulously crafted in the development environment, data comes
    from multiple sources for training, testing, and inference. It is robust and changing
    over time in terms of volume, velocity, veracity, and variety. To keep up with
    evolving data, code evolves over time. For perspective, their relationship can
    be observed as if code and data live in separate planes that share the time dimension
    but are independent in all other aspects. The challenge of an ML development process
    is to create a bridge between these two planes in a controlled way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Data and code progression over time'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.6 – Data and code progression over time
  prefs: []
  type: TYPE_NORMAL
- en: Data and code, with the progression of time, end up going in two directions
    with one objective of building and maintaining a robust and scalable ML system.
    This disconnect causes several challenges that need to be solved by anyone trying
    to put a ML model in production. It comes with challenges such as slow, brittle,
    fragmented, and inconsistent deployment, and a lack of reproducibility and traceability.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome these challenges, MLOps offers a systematic approach by bridging
    data and code together over the progression of time. This is the solution to challenges
    posed by traditional software development methods with regard to ML applications.
    Using the MLOps method, data and code progress over time in one direction with
    one objective of building and maintaining a robust and scalable ML system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – MLOps – data and code progressing together'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.7 – MLOps – data and code progressing together
  prefs: []
  type: TYPE_NORMAL
- en: MLOps facilitates ML model development, deployment, and monitoring in a streamlined
    and systematic approach. It empowers data science and IT teams to collaborate,
    validate, and govern their operations. All the operations executed by the teams
    are recorded or audited, end-to-end traceable, and repeatable. In the coming sections,
    we will learn how MLOps enables data science and IT teams to build and maintain
    robust and scalable ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: Trends of ML adoption in software development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we delve into the workings of the MLOps method and workflow, it is beneficial
    to understand the big picture and trends as to where and how MLOps is disrupting
    the world. As many applications are becoming AI-centric, software development
    is evolving to facilitate ML. ML will increasingly become part of software development,
    mainly due to the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Investments**: In 2019, investments in global private AI clocked over $70
    billion, with start-up investments related to AI over $37 billion, M&A $34 billion,
    IPOs $5 billion, and minority stake valued at around $2 billion. The forecast
    for AI globally shows fast growth in market value as AI reached $9.5 billion in
    2018 and is anticipated to reach a market value of $118 billion by 2025\. It has
    been assessed that growth in economic activity resulting from AI until 2030 will
    be of high value and significance. Currently, the US attracts ~50% of global VC
    funding, China ~39%, and 11% goes to Europe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Big data**: Data is exponentially growing in volume, velocity, veracity,
    and variety. For instance, observations suggest data growing in volume at 61%
    per annum in Europe, and it is anticipated that four times more data will be created
    by 2025 than exists today. Data is a requisite raw material for developing AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Infrastructural developments and adoption**: Moore''s law has been closely
    tracked and observed to have been realized prior to 2012\. Post-2012, compute
    has been doubling every 3.4 months.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increasing research and development**: AI research has been prospering in
    quality and quantity. A prominent growth of 300% is observed in the volume of
    peer-reviewed AI papers from 1998 to 2018, summing up to 9% of published conference
    papers and 3% of peer-reviewed journal publications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Industry**: Based on a surveyed report, 47% of large companies have reported
    having adopted AI in at least one function or business unit. In 2019, it went
    up to 58% and is expected to increase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These points have been sourced from policy and investment recommendations for
    trustworthy AI – European commission ([https://ec.europa.eu/digital-single-market/en/news/policy-and-investment-recommendations-trustworthy-artificial-intelligence](https://ec.europa.eu/digital-single-market/en/news/policy-and-investment-recommendations-trustworthy-artificial-intelligence))
    and AI Index 2019 ([https://hai.stanford.edu/research/ai-index-2019](https://hai.stanford.edu/research/ai-index-2019)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: All these developments indicate a strong push toward the industrialization of
    AI, and this is possible by bridging industry and research. MLOps will play a
    key role in the industrialization of AI. If you invest in learning this method,
    it will give you a headstart in your company or team and you could be a catalyst
    for operationalizing ML and industrializing AI.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned about some challenges and developments in IT, software
    development, and AI. Next, we will delve into understanding MLOps conceptually
    and learn in detail about a generic MLOps workflow that can be used commonly for
    any use case. These fundamentals will help you get a firm grasp of MLOps.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MLOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software development is interdisciplinary and is evolving to facilitate ML.
    MLOps is an emerging method to fuse ML with software development by integrating
    multiple domains as MLOps combines ML, DevOps, and data engineering, which aims
    to build, deploy, and maintain ML systems in production reliably and efficiently.
    Thus, MLOps can be expounded by this intersection.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – MLOps intersection'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.8 – MLOps intersection
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this intersection (MLOps) operational, I have designed a modular framework
    by following the systematic *design science method proposed by Wieringa* ([https://doi.org/10.1007/978-3-662-43839-8](https://doi.org/10.1007/978-3-662-43839-8))
    to develop a workflow to bring these three together (Data Engineering, Machine
    Learning, and *DevOps*). Design science goes with the application of design to
    problems and context. Design science is the design and investigation of artifacts
    in a context. The artifact in this case is the MLOps workflow, which is designed
    iteratively by interacting with problem contexts (industry use cases for the application
    of AI):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Design science workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.9 – Design science workflow
  prefs: []
  type: TYPE_NORMAL
- en: In a structured and iterative approach, the implementation of two cycles (the
    design cycle and the empirical cycle) was done for qualitative and quantitative
    analysis for MLOps workflow design through iterations. As a result of these cycles,
    an MLOps workflow is developed and validated by applying it to multiple problem
    contexts, that is, tens of ML use cases (for example, anomaly detection, real-time
    trading, predictive maintenance, recommender systems, virtual assistants, and
    so on) across multiple industries (for example, finance, manufacturing, healthcare,
    retail, the automotive industry, energy, and so on). I have applied and validated
    this MLOps workflow successfully in various projects across multiple industries
    to operationalize ML. In the next section, we will go through the concepts of
    the MLOps workflow designed as a result of the design science process.
  prefs: []
  type: TYPE_NORMAL
- en: Concepts and workflow of MLOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn about a generic MLOps workflow; it is the result
    of many design cycle iterations as discussed in the previous section. It brings
    together data engineering, ML, and DevOps in a streamlined fashion. *Figure 1.10*
    is a generic MLOps workflow; it is modular and flexible and can be used to build
    proofs of concept or to operationalize ML solutions in any business or industry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10 – MLOps workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.10 – MLOps workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'This workflow is segmented into two modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MLOps pipeline** (build, deploy, and monitor) – the upper layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drivers**: Data, code, artifacts, middleware, and infrastructure – mid and
    lower layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The upper layer is the MLOps pipeline (build, deploy, and monitor), which is
    enabled by drivers such as data, code, artifacts, middleware, and infrastructure.
    The MLOps pipeline is powered by an array of services, drivers, middleware, and
    infrastructure, and it crafts ML-driven solutions. By using this pipeline, a business
    or individual(s) can do quick prototyping, testing, and validating and deploy
    the model(s) to production at scale frugally and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the workings and implementation of the MLOps workflow, we will
    look at the implementation of each layer and step using a figurative business
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: Discussing a use case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this use case, we are to operationalize (prototyping and deploying for production)
    an image classification service to classify cats and dogs in a pet park in Barcelona,
    Spain. The service will identify cats and dogs in real time from the inference
    data coming from a CCTV camera installed in the pet park.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pet park provide you access to the data and infrastructure needed to operationalize
    the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data**: The pet park has given you access to their data lake containing 100,000
    labeled images of cats and dogs, which we will use for training the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Infrastructure**: Public cloud (IaaS).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This use case resembles a real-life use case for operationalizing ML and is
    used to explain the workings and implementation of the MLOps workflow. Remember
    to look for an explanation for the implementation of this use case at every segment
    and step of the MLOps workflow. Now, let's look at the workings of every layer
    and step in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The MLOps pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The MLOps pipeline is the upper layer, which performs operations such as build,
    deploy, and monitor, which work modularly in sync with each other. Let's look
    into each module's functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Build
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The build module has the core ML pipeline, and this is purely for training,
    packaging, and versioning the ML models. It is powered by the required compute
    (for example, the CPU or GPU on the cloud or distributed computing) resources
    to run the ML training and pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – MLOps – build pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.11 – MLOps – build pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline works from left to right. Let''s look at the functionality of
    each step in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ingestion**: This step is a trigger step for the ML pipeline. It deals
    with the volume, velocity, veracity, and variety of data by extracting data from
    various data sources (for example, databases, data warehouses, or data lakes)
    and ingesting the required data for the model training step. Robust data pipelines
    connected to multiple data sources enable it to perform **extract, transform,
    and load** (**ETL**) operations to provide the necessary data for ML training
    purposes. In this step, we can split and version data for model training in the
    required format (for example, the training or test set). As a result of this step,
    any experiment (that is, model training) can be audited and is back-traceable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a better understanding of the data ingestion step, here is the previously
    described use case implementation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As you have access to the pet park''s data lake, you can now procure data to
    get started. Using data pipelines (part of the data ingestion step), you do the
    following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1\. Extract, transform, and load 100,000 images of cats and dogs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Split and version this data into a train and test split (with an 80% and
    20% split).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Versioning this data will enable end-to-end traceability for trained models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Congrats – now you are ready to start training and testing the ML model using
    this data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model training**: After procuring the required data for ML model training
    in the previous step, this step will enable model training; it has modular scripts
    or code that perform all the traditional steps in ML, such as data preprocessing,
    feature engineering, and feature scaling before training or retraining any model.
    Following this, the ML model is trained while performing hyperparameter tuning
    to fit the model to the dataset (training set). This step can be done manually,
    but efficient and automatic solutions such as **Grid Search** or **Random Search**
    exist. As a result, all important steps of ML model training are executed with
    a ML model as the output of this step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this step, we implement all the important steps to train the image classification
    model. The goal is to train a ML model to classify cats and dogs. For this case,
    we train a **convolutional neural network** (**CNN** – [https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb](https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb))
    for the image classification service. The following steps are implemented: data
    preprocessing, feature engineering, and feature scaling before training, followed
    by training the model with hyperparameter tuning. As a result, we have a CNN model
    to classify cats and dogs with 97% accuracy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model testing**: In this step, we evaluate the trained model performance
    on a separated set of data points named test data (which was split and versioned
    in the data ingestion step). The inference of the trained model is evaluated according
    to selected metrics as per the use case. The output of this step is a report on
    the trained model''s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We test the trained model on test data (we split data earlier in the *Data ingestion*
    step) to evaluate the trained model's performance. In this case, we look for precision
    and the recall score to validate the model's performance in classifying cats and
    dogs to assess false positives and true positives to get a realistic understanding
    of the model's performance. If and when we are satisfied with the results, we
    can proceed to the next step, or else reiterate the previous steps to get a decent
    performing model for the pet park image classification service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model packaging**: After the trained model has been tested in the previous
    step, the model can be serialized into a file or containerized (using Docker)
    to be exported to the production environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The model we trained and tested in the previous steps is serialized to an ONNX
    file and is ready to be deployed in the production environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model registering**: In this step, the model that was serialized or containerized
    in the previous step is registered and stored in the model registry. A registered
    model is a logical collection or package of one or more files that assemble, represent,
    and execute your ML model. For example, multiple files can be registered as one
    model. For instance, a classification model can be comprised of a vectorizer,
    model weights, and serialized model files. All these files can be registered as
    one single model. After registering, the model (all files or a single file) can
    be downloaded and deployed as needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The serialized model in the previous step is registered on the model registry
    and is available for quick deployment into the pet park production environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By implementing the preceding steps, we successfully execute the ML pipeline
    designed for our use case. As a result, we have trained models on the model registry
    ready to be deployed in the production setup. Next, we will look into the workings
    of the deployment pipeline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deploy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The deploy module enables operationalizing the ML models we developed in the
    previous module (build). In this module, we test our model performance and behavior
    in a production or production-like (test) environment to ensure the robustness
    and scalability of the ML model for production use. *Figure 1.12* depicts the
    deploy pipeline, which has two components – production testing and production
    release – and the deployment pipeline is enabled by streamlined CI/CD pipelines
    connecting the development to production environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12 – MLOps – deploy pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.12 – MLOps – deploy pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'It works from left to right. Let''s look at the functionality of each step
    in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Application testing**: Before deploying an ML model to production, it is
    vital to test its robustness and performance via testing. Hence we have the "application
    testing" phase where we rigorously test all the trained models for robustness
    and performance in a production-like environment called a test environment. In
    the application testing phase, we deploy the models in the test environment (pre-production),
    which replicates the production environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ML model for testing is deployed as an API or streaming service in the test
    environment to deployment targets such as Kubernetes clusters, container instances,
    or scalable virtual machines or edge devices as per the need and use case. After
    the model is deployed for testing, we perform predictions using test data (which
    is not used for training the model; test data is sample data from a production
    environment) for the deployed model, during which model inference in batch or
    periodically is done to test the model deployed in the test environment for robustness
    and performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The performance results are automatically or manually reviewed by a quality
    assurance expert. When the ML model's performance meets the standards, then it
    is approved to be deployed in the production environment where the model will
    be used to infer in batches or real time to make business decisions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We deploy the model as an API service on an on-premises computer in the pet
    park, which is set up for testing purposes. This computer is connected to a CCTV
    camera in the park to fetch real-time inference data to predict cats or dogs in
    the video frames. The model deployment is enabled by the CI/CD pipeline. In this
    step, we test the robustness of the model in a production-like environment, that
    is, whether the model is performing inference consistently, and an accuracy, fairness,
    and error analysis. At the end of this step, a quality assurance expert certifies
    the model if it meets the standards.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Production release**: Previously tested and approved models are deployed
    in the production environment for model inference to generate business or operational
    value. This production release is deployed to the production environment enabled
    by CI/CD pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We deploy a previously tested and approved model (by a quality assurance expert)
    as an API service on a computer connected to CCTV in the pet park (production
    setup). This deployed model performs ML inference on the incoming video data from
    the CCTV camera in the pet park to classify cats or dogs in real time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Monitor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The monitor module works in sync with the deploy module. Using explainable
    monitoring (discussed later in detail, in [*Chapter 11*](B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206),
    *Key Principles for Monitoring Your ML System*), we can monitor, analyze, and
    govern the deployed ML application (ML model and application). Firstly, we can
    monitor the performance of the ML model (using pre-defined metrics) and the deployed
    application (using telemetry data). Secondly, model performance can be analyzed
    using a pre-defined explainability framework, and lastly, the ML application can
    be governed using alerts and actions based on the model''s quality assurance and
    control. This ensures a robust monitoring mechanism for the production system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13 – MLOps – monitor pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.13 – MLOps – monitor pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see each of the abilities of the monitor module in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitor**: The monitoring module captures critical information to monitor
    data integrity, model drift, and application performance. Application performance
    can be monitored using telemetry data. It depicts the device performance of a
    production system over a period of time. With telemetry data such as accelerometer,
    gyroscope, humidity, magnetometer, pressure, and temperature we can keep a check
    on the production system''s performance, health, and longevity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In real time, we will monitor three things – data integrity, model drift, and
    application performance – for the deployed API service on the park's computer.
    Metrics such as accuracy, F1 score, precision, and recall are tracked to data
    integrity and model drift. We monitor application performance by tracking the
    telemetry data of the production system (the on-premises computer in the park)
    running the deployed ML model to ensure the proper functioning of the production
    system. Telemetry data is monitored to foresee any anomalies or potential failures
    and fix them in advance. Telemetry data is logged and can be used to assess production
    system performance over time to check its health and longevity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Analyze**: It is critical to analyze the model performance of ML models deployed
    in production systems to ensure optimal performance and governance in correlation
    to business decisions or impact. We use model explainability techniques to measure
    the model performance in real time. Using this, we evaluate important aspects
    such as model fairness, trust, bias, transparency, and error analysis with the
    intention of improving the model in correlation to business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Over time, the statistical properties of the target variable we are trying
    to predict can change in unforeseen ways. This change is called "model drift,"
    for example, in a case where we have deployed a recommender system model to suggest
    suitable items for users. User behavior may change due to unforeseeable trends
    that could not be observed in historical data that was used for training the model.
    It is essential to consider such unforeseen factors to ensure deployed models
    provide the best and most relevant business value. When model drift is observed,
    then any of these actions should be performed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) The product owner or the quality assurance expert needs to be alerted.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The model needs to be switched or updated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Re-training the pipeline should be triggered to re-train and update the model
    as per the latest data or needs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We monitor the deployed model's performance in the production system (a computer
    connected to the CCTV in the pet park). We will analyze the accuracy, precision,
    and recall scores for the model periodically (once a day) to ensure the model's
    performance does not deteriorate below the threshold. When the model performance
    deteriorates below the threshold, we initiate system governing mechanisms (for
    example, a trigger to retrain the model).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Govern**: Monitoring and analyzing is done to govern the deployed application
    to drive optimal performance for the business (or the purpose of the ML system).
    After monitoring and analyzing the production data, we can generate certain alerts
    and actions to govern the system. For example, the product owner or the quality
    assurance expert gets alerted when model performance deteriorates (for example,
    low accuracy, high bias, and so on) below a pre-defined threshold. The product
    owner initiates a trigger to retrain and deploy an alternative model. Lastly,
    an important aspect of governance is "compliance" with the local and global laws
    and rules. For compliance, model explainability and transparency are vital. For
    this, model auditing and reporting are done to provide end-to-end traceability
    and explainability for production models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case implementation*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We monitor and analyze the deployed model's performance in the production system
    (a computer connected to the CCTV in the pet park). Based on the analysis of accuracy,
    precision, and recall scores for the deployed model, periodically (once a day),
    alerts are generated when the model's performance deteriorates below the pre-defined
    threshold. The product owner of the park generates actions, and these actions
    are based on the alerts. For example, an alert is generated notifying the product
    owner that the production model is 30% biased to detect dogs more than cats. The
    product owner then triggers the model re-training pipeline to update the model
    using the latest data to reduce the bias, resulting in a fair and robust model
    in production. This way, the ML system at the pet park in Barcelona is well-governed
    to serve the business needs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This brings us to the end of the MLOps pipeline. All models trained, deployed,
    and monitored using the MLOps method are end-to-end traceable and their lineage
    is logged in order to trace the origins of the model, which includes the source
    code the model used to train, the data used to train and test the model, and parameters
    used to converge the model. Full lineage is useful to audit operations or to replicate
    the model, or when a blocker is hit, the logged ML model lineage is useful to
    backtrack the origins of the model or to observe and debug the cause of the blocker.
    As ML models generate data in production during inference, this data can be tied
    to the model training and deployment lineage to ensure the end-to-end lineage,
    and this is important for certain compliance requirements. Next, we will look
    into key drivers enabling the MLOps pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Drivers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These are the key drivers for the MLOps pipeline: data, code, artifacts, middleware,
    and infrastructure. Let''s look into each of the drivers to get an overview of
    how they enable the MLOps pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.14 – MLOps drivers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_01_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.14 – MLOps drivers
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the key drivers for the MLOps pipeline are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data**: Data can be in multiple forms, such as text, audio, video, and images.
    In traditional software applications, data quite often tends to be structured,
    whereas, for ML applications, it can be structured or unstructured. To manage
    data in ML applications, data is handled in these steps: data acquisition, data
    annotation, data cataloging, data preparation, data quality checking, data sampling,
    and data augmentation. Each step involves its own life cycle. This makes a whole
    new set of processes and tools necessary for ML applications. For efficient functioning
    of the ML pipeline, data is segmented and versioned into training data, testing
    data, and monitoring data (collected in production, for example, model inputs,
    outputs, and telemetry data). These data operations are part of the MLOps pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code**: There are three essential modules of code that drive the MLOps pipeline:
    training code, testing code, and application code. These scripts or code are executed
    using the CI/CD and data pipelines to ensure the robust working of the MLOps pipeline.
    The source code management system (for example, using Git or Mercurial) will enable
    orchestration and play a vital role in managing and integrating seamlessly with
    CI, CD, and data pipelines. All of the code is staged and versioned in the source
    code management setup (for example, Git).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifacts**: The MLOps pipeline generates artifacts such as data, serialized
    models, code snippets, system logs, ML model training, and testing metrics information.
    All these artifacts are useful for the successful working of the MLOps pipeline,
    ensuring its traceability and sustainability. These artifacts are managed using
    middleware services such as the model registry, workspaces, logging services,
    source code management services, databases, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Middleware**: Middleware is computer software that offers services to software
    applications that are more than those available from the operating systems. Middleware
    services ensure multiple applications to automate and orchestrate processes for
    the MLOps pipeline. We can use a diverse set of middleware software and services
    depending on the use cases, for example, Git for source code management, VNets
    to enable the required network configurations, Docker for containerizing our models,
    and Kubernetes for container orchestration to automate application deployment,
    scaling, and management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Infrastructure**: To ensure the successful working of the MLOps pipeline,
    we need essential compute and storage resources to train Test and deploy the ML
    models. Compute resources enable us to train, deploy and monitor our ML models.
    Two types of storages resources can facilitate ML operations, central storage
    and feature stores. A central storage stores the logs, artifacts, training, testing
    and monitoring data. A feature store is optional and complementary to central
    storage. It extracts, transforms and stores needed features for ML model training
    and inference using a feature pipeline. When it comes to the infrastructure, there
    are various options such as on-premises resources or **infrastructure as a service
    (IaaS)**, which is cloud services. These days, there are many cloud players providing
    IaaS, such as Amazon, Microsoft, Google, Alibaba, and so on. Having the right
    infrastructure for your use case will enable robust, efficient, and frugal operations
    for your team and company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fully automated workflow is achievable with smart optimization and synergy
    of all these drivers with the MLOps pipeline. Some direct advantages of implementing
    an automated MLOps workflow is a spike in IT teams' efficiency (by reducing the
    time spent by data scientists and developers on mundane and repeatable tasks)
    and the optimization of resources, resulting in cost reductions, and both of these
    are great for any business.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about the evolution of software development
    and infrastructure to facilitate ML. We delved into the concepts of MLOps, followed
    by getting acquainted with a generic MLOps workflow that can be implemented in
    a wide range of ML solutions across multiple industries.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to characterize any ML problem into
    an MLOps-driven solution and start developing it using an MLOps workflow.
  prefs: []
  type: TYPE_NORMAL
