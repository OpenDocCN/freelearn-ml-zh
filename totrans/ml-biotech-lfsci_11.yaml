- en: 'Chapter 9: Natural Language Processing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章：自然语言处理
- en: In the previous chapter, we discussed using deep learning to not only address
    structured data in the form of tables but also sequence-based data where the order
    of the elements matters. In this chapter, we will be discussing another form of
    sequence-based data – text, within a field known as **Natural Language Processing**
    (**NLP**). We can define NLP as a subset of artificial intelligence that overlaps
    with both the realms of machine learning and deep learning, specifically when
    it comes to interactions between the areas of linguistics and computer science.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了使用深度学习不仅解决以表格形式存在的结构化数据，还包括顺序数据，其中元素的顺序很重要。在本章中，我们将讨论另一种形式的顺序数据——文本，这属于一个被称为**自然语言处理**（**NLP**）的领域。我们可以将NLP定义为人工智能的一个子集，它与机器学习和深度学习的领域都有重叠，特别是在语言学和计算机科学领域之间的交互。
- en: There are many well-known and well-documented applications and success stories
    of using NLP for various tasks. Products ranging from spam detectors all the way
    to document analyzers involve NLP to some extent. Throughout this chapter, we
    will explore several different areas and applications involving NLP.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NLP进行各种任务的许多知名和有详细记录的应用和成功案例。从垃圾邮件检测器到文档分析器等不同产品都涉及一定程度上的NLP。在本章中，我们将探讨涉及NLP的几个不同领域和应用。
- en: As we have observed with many other areas of data science we have explored thus
    far, the field of NLP is just as vast and sparse, with endless tools and applications
    that a single book would never be able to fully cover. Throughout this chapter,
    we will aim to highlight as many of the most common and useful applications you
    will likely encounter as we can.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们观察到的许多其他数据科学领域一样，NLP领域同样广阔且分散，有无数的工具和应用，一本书根本无法完全涵盖。在本章中，我们将努力突出您可能会遇到的最常见和最有用的应用。
- en: Throughout this chapter, we will explore many of the popular areas relating
    to NLP from the perspective of both structured and unstructured data. We will
    explore several topics, such as entity recognition, sentence analysis, topic modeling,
    sentiment analysis, and natural language search engines.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从结构化和非结构化数据的角度探讨许多与NLP相关的流行领域。我们将探讨几个主题，如实体识别、句子分析、主题建模、情感分析和自然语言搜索引擎。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to NLP
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP简介
- en: Getting started with NLP using NLTK and SciPy
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NLTK和SciPy开始NLP
- en: Working with structured data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与结构化数据一起工作
- en: Tutorial – abstract clustering and topic modeling
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教程——抽象聚类和主题建模
- en: Working with unstructured data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与非结构化数据一起工作
- en: Tutorial – developing a scientific data search engine using transformers
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教程——使用transformers开发科学数据搜索引擎
- en: With these objectives in mind, let's go ahead and get started.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这些目标，让我们开始吧。
- en: Introduction to NLP
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP简介
- en: 'Within the scope of **biotechnology**, we often turn to **NLP** for numerous
    reasons, which generally involve the need to organize data and develop models
    to find answers to scientific questions. As opposed to the many other areas we
    have investigated so far, NLP is unique in the sense that we focus on one type
    of data at hand: text data. When we think of text data within the realm of NLP,
    we can divide things into two general categories: **structured data** and **unstructured
    data**. We can think of structured data as text fields living within tables and
    databases in which items are organized, labeled, and linked together for easier
    retrieval, such as a SQL or DynamoDB database. On the other hand, we have what
    is known as unstructured data such as documents, PDFs, and images, which can contain
    static content that is neither searchable nor easily accessible. An example of
    this can be seen in the following diagram:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在**生物技术**领域内，我们经常出于多种原因转向**NLP**，这通常涉及组织数据和开发模型以找到科学问题的答案。与迄今为止我们调查的许多其他领域不同，NLP的独特之处在于我们专注于一种手头的数据类型：文本数据。当我们想到NLP领域的文本数据时，我们可以将其分为两大类：**结构化数据**和**非结构化数据**。我们可以将结构化数据视为存在于表格和数据库中的文本字段，其中项目是有组织的、标记的，并且相互链接以便于检索，例如SQL或DynamoDB数据库。另一方面，我们有所谓的非结构化数据，如文档、PDF和图像，它们可以包含既不可搜索也不易访问的静态内容。以下图表中可以看到一个例子：
- en: '![Figure 9.1 – Structured and unstructured data in NLP ](img/B17761_09_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – NLP中的结构化和非结构化数据](img/B17761_09_001.jpg)'
- en: Figure 9.1 – Structured and unstructured data in NLP
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – NLP中的结构化和非结构化数据
- en: 'Often, we wish to use documents or text-based data for various purposes, such
    as the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们希望将文档或基于文本的数据用于各种目的，如下所示：
- en: '**Generating insights**: Looking for trends, keywords, or key phrases'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成洞察**：寻找趋势、关键词或关键短语'
- en: '**Classification**: Automatically labeling documents for various purposes'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：自动为各种目的标记文档'
- en: '**Clustering**: Grouping documents together based on features and characteristics'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：根据特征和特性将文档分组在一起'
- en: '**Searching**: Quickly finding important knowledge in historical documents'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索**：快速在历史文档中找到重要知识'
- en: In each of these examples, we would need to take our data from unstructured
    data and move it toward a structured state to implement these tasks. In this chapter,
    we will look at some of the most important and useful concepts and tools you should
    know about concerning the NLP space.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些示例中，我们需要将我们的数据从非结构化数据转换为结构化状态，以实现这些任务。在本章中，我们将探讨一些关于NLP空间的重要和有用的概念和工具，您应该了解。
- en: Getting started with NLP using NLTK and SciPy
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NLTK和SciPy开始NLP之旅
- en: 'There are many different NLP libraries available in the Python language that
    allow users to accomplish a variety of different tasks for analyzing data, generating
    insights, or preparing predictive models. To begin our journey in the realm of
    NLP, we will take advantage of two popular libraries known as **NLTK** and **SciPy**.
    We will begin by importing these two libraries:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Python语言中有许多不同的NLP库可供用户使用，以完成各种不同的数据分析、洞察生成或预测模型准备任务。为了开始我们在NLP领域的旅程，我们将利用两个流行的库，即**NLTK**和**SciPy**。我们将从导入这两个库开始：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Often, we will want to parse and analyze raw pieces of text for particular
    purposes. Take, for example, the following paragraph regarding the field of biotechnology:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会想要为了特定目的解析和分析原始文本片段。以以下关于生物技术领域的段落为例：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, there is a single string that''s been assigned to the `paragraph` variable.
    Paragraphs can be separated into sentences using the `sent_tokenize()` function,
    as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，有一个字符串被分配给了`paragraph`变量。段落可以使用`sent_tokenize()`函数分割成句子，如下所示：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Upon printing these sentences, we will get the following output:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 打印这些句子后，我们将得到以下输出：
- en: '![Figure 9.2 – A sample list of sentences that were split from the initial
    paragraph ](img/B17761_09_002.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – 从初始段落中分割出的句子样本列表](img/B17761_09_002.jpg)'
- en: Figure 9.2 – A sample list of sentences that were split from the initial paragraph
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 从初始段落中分割出的句子样本列表
- en: 'Similarly, we can use the `word_tokenize()` function to separate the paragraph
    into individual words:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以使用`word_tokenize()`函数将段落分割成单个单词：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Upon printing the results, you will get a list similar to the one shown in
    the following screenshot:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 打印结果后，您将得到一个类似于以下屏幕截图中的列表：
- en: '![Figure 9.3 – A sample list of words that were split from the first sentence
    ](img/B17761_09_003.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 从第一句中分割出的单词样本列表](img/B17761_09_003.jpg)'
- en: Figure 9.3 – A sample list of words that were split from the first sentence
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 从第一句中分割出的单词样本列表
- en: 'Often, we will want to know the part of speech for a given word in a sentence.
    We can use the `pos_tag()` function on a given string to do this – in this case,
    the first sentence of the paragraph:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们想知道句子中给定单词的词性。我们可以使用`pos_tag()`函数在给定的字符串上执行此操作 – 在这种情况下，段落的第一个句子：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Upon printing the tags, we get a list of sets where the word or token is listed
    on the left and the associated part of speech is on the right. For example, `Biotechnology`
    and `biology` were tagged as proper nouns, whereas `involving` and `develop` were
    tagged as verbs. We can see an example of these results in the following screenshot:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 打印标签后，我们得到一个列表，其中单词或标记列在左侧，而相关的词性列在右侧。例如，`Biotechnology`和`biology`被标记为专有名词，而`involving`和`develop`被标记为动词。我们可以在以下屏幕截图中看到这些结果的示例：
- en: '![Figure 9.4 – Results of the part-of-speech tagging method ](img/B17761_09_004.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – 领词性标注方法的结果](img/B17761_09_004.jpg)'
- en: Figure 9.4 – Results of the part-of-speech tagging method
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 领词性标注方法的结果
- en: 'In addition to understanding the parts of speech, we often want to know the
    frequency of the given words in a particular string of text. For this, we could
    either tokenize the paragraph into words, group by word, count the instances and
    plot them, or use NLTK''s built-in functionality:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 除了理解词性之外，我们通常还想知道特定文本字符串中给定单词的频率。为此，我们可以将段落标记为单词，按单词分组，计算实例并绘制它们，或者使用NLTK的内置功能：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'By doing this, you will receive the following output:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，您将收到以下输出：
- en: '![Figure 9.5 – Results of calculating the frequency (with stop words) ](img/B17761_09_005.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – 计算频率（含停用词）的结果](img/B17761_09_005.jpg)'
- en: Figure 9.5 – Results of calculating the frequency (with stop words)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – 计算频率（含停用词）的结果
- en: 'Here, we can see that the most common elements are commas, periods, and other
    irrelevant words. Words that are irrelevant to any given analysis are known as
    `regex`. Let''s go ahead and prepare a function to clean our text:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到最常见的元素是逗号、句号和其他无关紧要的词。对于任何给定分析无关紧要的词被称为`regex`。让我们继续准备一个函数来清理我们的文本：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There are four main steps within this function. First, we convert the text into
    lowercase for consistency; then, we use regex to remove all punctuation and numbers.
    After, we split the string into individual tokens and remove the words if they
    are in our list of stop words, before finally joining the words back together
    into a single string.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，有四个主要步骤。首先，我们将文本转换为小写以保持一致性；然后，我们使用正则表达式删除所有标点符号和数字。之后，我们将字符串拆分为单个标记，并删除如果它们在我们的停用词列表中的单词，最后将单词重新组合成一个字符串。
- en: Important note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Please note that text cleaning scripts are often specific to the use case in
    the sense that not all use cases require the same steps.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，文本清理脚本通常针对特定用例是特定的，这意味着并非所有用例都需要相同的步骤。
- en: 'Now, we can apply the `cleaner` function to our paragraph:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将`cleaner`函数应用于我们的段落：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can see the output of this function in the following screenshot:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下屏幕截图中看到此函数的输出：
- en: '![Figure 9.6 – Output of the text cleaning function ](img/B17761_09_006.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – 文本清理函数的输出](img/B17761_09_006.jpg)'
- en: Figure 9.6 – Output of the text cleaning function
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 文本清理函数的输出
- en: 'Upon recalculating the frequencies with the clean text, we can replot the data
    and view the results:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新计算使用干净文本的频率后，我们可以重新绘制数据并查看结果：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output of this code can be seen in the following screenshot:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的输出可以在以下屏幕截图中看到：
- en: '![Figure 9.7 – Results of calculating the frequency (without stop words) ](img/B17761_09_007.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 计算频率（不含停用词）的结果](img/B17761_09_007.jpg)'
- en: Figure 9.7 – Results of calculating the frequency (without stop words)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 计算频率（不含停用词）的结果
- en: 'As we begin to dive deeper into our text, we will often want to tag items not
    only by their parts of speech, but also by their entities, allowing us to parse
    dates, names, and many others in a process known as `spacy` library:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们开始深入挖掘我们的文本，我们通常会希望不仅通过它们的词性来标记项目，还要通过它们的实体来标记，这样我们就可以在称为`spacy`库的过程中解析日期、人名以及许多其他内容：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Upon printing the results, we obtain a list of items and their associated entity
    tags. Notice that the model not only picked up the year `1919` as a `DATE` entity
    but also picked up descriptions such as `21st centuries` as `DATE` entities:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 打印结果后，我们获得一个项目及其相关实体标签的列表。请注意，该模型不仅识别了年份`1919`作为`DATE`实体，还识别了描述如`21st centuries`作为`DATE`实体：
- en: '![Figure 9.8 – Results of the NER model showing the text and its subsequent
    tag ](img/B17761_09_008.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – 显示文本及其后续标签的NER模型的结果](img/B17761_09_008.jpg)'
- en: Figure 9.8 – Results of the NER model showing the text and its subsequent tag
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 显示文本及其后续标签的NER模型的结果
- en: 'We can also display the tags from a visual perspective within Jupyter Notebook
    using the `render` function:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`render`函数在Jupyter Notebook中从视觉角度显示标签：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Upon executing this code, we will receive the original paragraph, which has
    been color-coded based on the identified entity tags, allowing us to view the
    results visually:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，我们将收到原始段落，它已被根据识别的实体标签着色，使我们能够直观地查看结果：
- en: '![Figure 9.9 – Results of the NER model when rendered in Jupyter Notebook ](img/B17761_09_009.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图9.9 – 在Jupyter Notebook中渲染的NER模型的结果](img/B17761_09_009.jpg)'
- en: Figure 9.9 – Results of the NER model when rendered in Jupyter Notebook
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – 在Jupyter Notebook中渲染的NER模型的结果
- en: 'We can also use SciPy to implement a visual understanding of our text when
    it comes to parts of speech using the same `render()` function:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 SciPy 通过相同的 `render()` 函数实现对文本中词性的视觉理解：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can see the output of this command in the following diagram:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图表中看到此命令的输出：
- en: '![Figure 9.10 – Results of the POS model when rendered in Jupyter Notebook
    ](img/B17761_09_010.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.10 – POS 模型在 Jupyter Notebook 中渲染的结果](img/B17761_09_010.jpg)'
- en: Figure 9.10 – Results of the POS model when rendered in Jupyter Notebook
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 – POS 模型在 Jupyter Notebook 中渲染的结果
- en: This gives us a great way to understand and visualize the structure of a sentence
    before implementing any NLP models. With some of the basic analysis out of the
    way, let's go ahead and explore some applications of NLP using structured data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们理解并可视化句子结构提供了一个很好的方法。在完成一些基本分析后，让我们继续探索使用结构化数据的 NLP 应用。
- en: Working with structured data
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理结构化数据
- en: Now that we have explored some of the basics of NLP, let's dive into some more
    complex and common use cases that are often observed in the biotech and life sciences
    fields. When working with text-based data, it is much more common to work with
    larger datasets rather than single strings. More often than not, we generally
    want these datasets to involve scientific data regarding specific areas of interest
    relating to a particular research topic. Let's go ahead and learn how to retrieve
    scientific data using Python.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了一些 NLP 的基础知识，让我们深入研究一些更复杂且在生物技术和生命科学领域常见的用例。当处理基于文本的数据时，与单个字符串相比，更常见的是处理更大的数据集。通常情况下，我们希望这些数据集涉及特定研究主题相关领域的科学数据。让我们继续学习如何使用
    Python 检索科学数据。
- en: Searching for scientific articles
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 搜索科学文章
- en: 'To programmatically retrieve scientific publication data using Python, we can
    make use of the `pymed` library from *PubMed* ([https://pubmed.ncbi.nlm.nih.gov/](https://pubmed.ncbi.nlm.nih.gov/)).
    Let''s go ahead and build a sample dataset:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Python 程序化检索科学出版物数据，我们可以利用来自 *PubMed* 的 `pymed` 库([https://pubmed.ncbi.nlm.nih.gov/](https://pubmed.ncbi.nlm.nih.gov/))。让我们继续构建一个示例数据集：
- en: 'First, let''s import our libraries and instantiate a new `PubMed` object:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入我们的库并实例化一个新的 `PubMed` 对象：
- en: '[PRE12]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we will need to define and run our query. Let''s go ahead and search
    for all the items related to monoclonal antibodies and retrieve `100` results:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义并运行我们的查询。让我们继续搜索与单克隆抗体相关的一切，并检索 `100` 个结果：
- en: '[PRE13]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With the results found, we can iterate over our results to retrieve all the
    available fields for each of the given articles:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在找到结果后，我们可以遍历我们的结果以检索给定文章的所有可用字段：
- en: '[PRE14]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we can go ahead and convert our list into a DataFrame for ease of
    use:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以将我们的列表转换为 DataFrame 以便于使用：
- en: '[PRE15]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following is the output:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下为输出：
- en: '![Figure 9.11 – A DataFrame showing the results of the PubMed search ](img/B17761_09_011.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11 – 显示 PubMed 搜索结果的 DataFrame](img/B17761_09_011.jpg)'
- en: Figure 9.11 – A DataFrame showing the results of the PubMed search
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 – 显示 PubMed 搜索结果的 DataFrame
- en: With the final step complete, we have a dataset full of scientific abstracts
    and their associated metadata. In the next section, we will explore this data
    in more depth and develop a few visuals to represent it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成最后一步后，我们有一个包含科学摘要及其相关元数据的完整数据集。在下一节中，我们将更深入地探索这些数据，并开发一些可视化来表示它。
- en: Exploring our datasets
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索我们的数据集
- en: Now that we have some data to work with, let's go ahead and explore it. If you
    recall from many of the previous chapters, we often explore our numerical datasets
    in various ways. We can group columns, explore trends, and find correlations –
    tasks we cannot necessarily do when working with text. Let's implement a few NLP
    methods to explore data in a slightly different way.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一些数据可以处理了，让我们继续探索它。如果你还记得前面许多章节的内容，我们经常以各种方式探索我们的数值数据集。我们可以分组列，探索趋势，并找到相关性——这些任务在处理文本时可能无法完成。让我们实现一些
    NLP 方法，以稍微不同的方式探索数据。
- en: Checking string lengths
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查字符串长度
- en: 'Since we have our dataset structured within a pandas DataFrame, one of the
    first items we generally want to explore is the distribution of string lengths
    for our text-based data. In the current dataset, the two main columns containing
    text are `title` and `abstract` – let''s go ahead and plot the distribution of
    lengths:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经在pandas DataFrame中结构化我们的数据集，我们通常想要探索的第一个项目之一就是基于文本数据的字符串长度分布。在当前数据集中，包含文本的两个主要列是`title`和`abstract`
    – 让我们继续绘制长度分布图：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following is the output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为输出：
- en: '![Figure 9.12 – Frequency distributions of the average length of abstracts
    (left) and titles (right) ](img/B17761_09_012.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图9.12 – 摘要平均长度（左侧）和标题平均长度（右侧）的频率分布](img/B17761_09_012.jpg)'
- en: Figure 9.12 – Frequency distributions of the average length of abstracts (left)
    and titles (right)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 – 摘要平均长度（左侧）和标题平均长度（右侧）的频率分布
- en: 'Here, we can see that the average length of most abstracts is around 1,500
    characters, whereas titles are around 100\. Since the titles may contain important
    keywords or identifiers for a given article, similar to that of the abstracts,
    it would be wise to combine the two into a single column to analyze them together.
    We can simply combine them using the `+` operator:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到大多数摘要的平均长度约为1,500个字符，而标题约为100个字符。由于标题可能包含文章的重要关键词或标识符，类似于摘要，因此将两者合并为一个单独的列以一起分析是明智的。我们可以简单地使用`+`运算符将它们合并：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can see this new column in the following screenshot:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下屏幕截图中看到这个新列：
- en: '![Figure 9.13 – A sample DataFrame showing the title, abstract, and text columns
    ](img/B17761_09_013.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图9.13 – 显示标题、摘要和文本列的样本DataFrame](img/B17761_09_013.jpg)'
- en: Figure 9.13 – A sample DataFrame showing the title, abstract, and text columns
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 – 显示标题、摘要和文本列的样本DataFrame
- en: Using the `mean()` function on each of the columns, we can see that the titles
    have, on average, 108 characters, the abstracts have 1,277 characters, and the
    combined text column has 1,388 characters.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对每一列使用`mean()`函数，我们可以看到标题平均有108个字符，摘要有1,277个字符，而组合文本列有1,388个字符。
- en: 'Similar to other datasets, we can use the `value_counts()` function to get
    a quick sense of the most common words:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他数据集类似，我们可以使用`value_counts()`函数来快速了解最常见的单词：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Immediately, we notice that our dataset is flooded with stop words:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 立即，我们注意到我们的数据集中充满了停用词：
- en: '![Figure 9.14 – A sample of the most frequent words in the dataset ](img/B17761_09_014.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图9.14 – 数据集中最频繁出现的单词样本](img/B17761_09_014.jpg)'
- en: Figure 9.14 – A sample of the most frequent words in the dataset
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 – 数据集中最频繁出现的单词样本
- en: We can implement the same `cleaner` function as we did previously to remove
    these stop words and any other undesirable values. Note that some of the cells
    within the DataFrame may be empty, depending on the query made and the results
    returned. We'll take a closer look at this in the next section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以实施与之前相同的`cleaner`函数来删除这些停用词和任何其他不希望存在的值。请注意，DataFrame中的某些单元格可能为空，这取决于查询方式和返回的结果。我们将在下一节中对此进行更详细的探讨。
- en: Cleaning text data
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清理文本数据
- en: 'We can add a quick check at the top of the function by checking the type of
    the value to ensure that no errors are encountered:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在函数顶部添加一个快速检查，通过检查值的类型来确保不会遇到错误：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can quickly test this out on a sample string to test the functionality:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在一个样本字符串上快速测试这个功能：
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output of this function can be seen in the following screenshot:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的输出可以在以下屏幕截图中看到：
- en: '![Figure 9.15 – Results of the cleaning function ](img/B17761_09_015.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图9.15 – 清理函数的结果](img/B17761_09_015.jpg)'
- en: Figure 9.15 – Results of the cleaning function
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15 – 清理函数的结果
- en: 'With the function working, we can go ahead and apply this to the `text` column
    within the DataFrame and create a new column consisting of the cleaned text using
    the `apply()` function, which allows us to apply a given function iteratively
    down through all rows of a DataFrame:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 函数运行正常后，我们可以将其应用于DataFrame中的`text`列，并创建一个新列，该列包含使用`apply()`函数清理后的文本，该函数允许我们对DataFrame的所有行迭代应用给定函数：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can check the performance of our function by checking the columns of interest:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查感兴趣的列来检查我们函数的性能：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can see these two columns in the following screenshot:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下屏幕截图中看到这两个列：
- en: '![Figure 9.16 – A DataFrame showing the original and cleaned texts ](img/B17761_09_016.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图9.16 – 显示原始和清理文本的DataFrame](img/B17761_09_016.jpg)'
- en: Figure 9.16 – A DataFrame showing the original and cleaned texts
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 – 显示原始和清理文本的DataFrame
- en: If we go ahead and check `value_counts()`of the `clean_text` column, as we did
    previously, you will notice that the stop words were removed and that more useful
    keywords are now populated at the top.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们继续检查`clean_text`列的`value_counts()`，就像我们之前做的那样，你会注意到停用词已被移除，并且现在更有用的关键词已填充在顶部。
- en: Creating word clouds
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建词云
- en: 'Another popular and useful method to get a quick sense of the content for a
    given text-based dataset is by using word clouds. `wordclouds` library:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行且有用的方法，可以通过使用词云来快速了解给定基于文本数据集的内容。`wordclouds`库：
- en: 'First, we need to import the function and then create a `wordcloud` object
    that we will specify a number of parameters in. We can adjust the dimensions of
    the image, colors, and the data as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入函数，然后创建一个`wordcloud`对象，我们将指定一系列参数。我们可以调整图像的尺寸、颜色和数据，如下所示：
- en: '[PRE23]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we can use the `imshow()` function from `matplotlib` to render the image:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`imshow()`函数从`matplotlib`库中渲染图像：
- en: '[PRE24]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following is the output:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下为输出结果：
- en: '![Figure 9.17 – A word cloud representing the frequency of words in the dataset
    ](img/B17761_09_017.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图9.17 – 表示数据集中单词频率的词云](img/B17761_09_017.jpg)'
- en: Figure 9.17 – A word cloud representing the frequency of words in the dataset
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 – 表示数据集中单词频率的词云
- en: In this section, we investigated a few of the most popular methods for quickly
    analyzing text-based data as a preliminary step before performing any type of
    rigorous analysis or model development process. In the next section, we will train
    a model using this dataset to investigate topics.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们调查了几种快速分析基于文本数据的流行方法，作为在执行任何类型严格分析或模型开发过程之前的初步步骤。在下一节中，我们将使用这个数据集训练一个模型来研究主题。
- en: Tutorial – clustering and topic modeling
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教程 – 聚类和主题建模
- en: Similar to some of the previous examples we have seen so far, much of our data
    can either be classified in a supervised setting or clustered in an unsupervised
    one. In most cases, text-based data is generally made available to us in the form
    of real-world data in the sense that it is in a raw and unlabeled form.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们迄今为止看到的一些前例类似，我们的大部分数据要么可以在监督设置中分类，要么在无监督设置中聚类。在大多数情况下，基于文本的数据通常以原始和未标记的形式提供给我们，这意味着它是现实世界数据。
- en: Let's look at an example where we can make sense of our data and label it from
    an unsupervised perspective. Our main objective here will be to preprocess our
    raw text, cluster the data into five clusters, and then determine the main topics
    for each of those clusters. If you are following along using the provided code
    and documentation, please note that your results may vary as the dataset is dynamic,
    and its contents change as new data is populated into the PubMed database. I would
    urge you to customize the queries to topics that interest you. With that in mind,
    let's go ahead and begin.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个例子，我们可以从无监督的角度理解我们的数据并对它进行标记。我们在这里的主要目标将是预处理我们的原始文本，将数据聚类成五个簇，然后确定每个簇的主要主题。如果你正在使用提供的代码和文档进行操作，请注意，你的结果可能会有所不同，因为数据集是动态的，其内容会随着新数据被填充到PubMed数据库中而变化。我敦促你根据你感兴趣的主题定制查询。考虑到这一点，让我们继续开始。
- en: 'We will begin by querying some data using the `pymed` library and retrieving
    a few hundred abstracts and titles to work with:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用`pymed`库查询一些数据，检索几百篇摘要和标题来工作：
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Instead of making a single query, let''s make a few and combine the results
    into a single DataFrame:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是做一个单一的查询，而是做几个查询，并将结果合并到一个单独的DataFrame中：
- en: '[PRE26]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Taking a look at the data, we can see that some of the cells have missing (`nan`)
    values. Given that our objective concerns the text-based fields only (titles and
    abstracts), let''s limit the scope of any cleaning methods to those columns alone:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 观察数据，我们可以看到一些单元格有缺失（`nan`）值。鉴于我们的目标仅涉及基于文本的字段（标题和摘要），让我们将任何清理方法的范围限制在这些列上：
- en: '[PRE27]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Given that we are concerned with the contents of each article as a whole, we
    can combine the titles and abstracts together into a new column called `text`:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们关注的是每篇文章的整体内容，我们可以将标题和摘要合并到一个新的列中，称为`text`：
- en: '[PRE28]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Taking a look at the dataset, we can see that we have 560 rows and 3 columns.
    Scientific articles can be very descriptive, encompassing many stop words. Given
    that our objective here is to detect topics that are represented by keywords,
    let''s remove any punctuation, numerical values, and stopwords from our text:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 观察数据集，我们可以看到我们有560行和3列。科学文章可以非常详细，包含许多停用词。鉴于我们的目标是检测由关键词表示的主题，让我们从我们的文本中移除任何标点符号、数值和停用词：
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We can check the average number of words per article before and after implementing
    the script to ensure that the data was cleaned. In our case, we can see that we
    started with an average of 190 words and ended up with an average of 123.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查在实施脚本前后每篇文章的平均单词数来确保数据已被清理。在我们的例子中，我们可以看到我们开始时的平均单词数是190，最终的平均单词数是123。
- en: 'With the data now clean, we can go ahead and extract our features. We will
    use a relatively simple and common method known as **TFIDF** – a measure of originality
    of words in which each word is compared to the number of times it appears in an
    article, relative to the number of articles the same word appears in. We can think
    of TFIDF as two separate items – **Term Frequency** (**TF**) and **Inverse Document
    Frequency** (**IDF**) – which we can represent as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现在已清理，我们可以继续提取我们的特征。我们将使用一种相对简单且常见的方法，称为**TFIDF** – 一种衡量单词原创性的度量，其中每个单词都与它在文章中出现的次数进行比较，相对于该单词在文章中出现的次数。我们可以将TFIDF视为两个单独的项目
    – **词频**（**TF**）和**逆文档频率**（**IDF**） – 我们可以如下表示：
- en: '![](img/Formula_09_001.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_001.jpg)'
- en: 'In the preceding equation, *t* is the term or keyword, while *d* is the document
    or – in our case – the article. The main idea here is to capture important keywords
    that would be descriptive as main topics but ignore those that appear in almost
    every article. We will begin by importing `TfidfVectorizer` from `sklearn`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，*t*是术语或关键词，而*d*是文档或 – 在我们的情况下 – 文章。这里的主要思想是捕捉那些可以作为主要主题描述的重要关键词，但忽略那些几乎在每篇文章中都出现的关键词。我们将首先从`sklearn`导入`TfidfVectorizer`：
- en: '[PRE30]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we will convert our text-based data into numerical features by fitting
    our dataset and transforming the values:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过拟合我们的数据集并转换值将基于文本的数据转换为数值特征：
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We can check the shape of the `features` variable to confirm that we have 560
    rows, just as we did before applying TFIDF, and 5,500 columns worth of features
    to go with it. Next, we can go ahead and cluster our documents using one of the
    many clustering methods we have explored so far.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查`features`变量的形状来确认我们确实有560行，正如我们在应用TFIDF之前所做的那样，以及与之相对应的5500个特征列。接下来，我们可以继续使用我们迄今为止探索的许多聚类方法之一来聚类我们的文档。
- en: 'Let''s implement `MiniBatchKMeans` and specify `4` as the number of clusters
    we want to retrieve:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现`MiniBatchKMeans`并指定我们想要检索的聚类数量为`4`：
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'When working with larger datasets, especially in production, it is generally
    advisable to avoid using pandas DataFrames as there are more efficient methods
    available, depending on the processes you need to implement. Given that we are
    only working with 560 rows of data, and our objective is to cluster our data and
    retrieve topics, we will once again make use of DataFrames to manage our data.
    Let''s go ahead and add our predicted clusters to our DataFrame:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理大型数据集时，特别是在生产环境中，通常建议避免使用pandas DataFrame，因为根据您需要实现的过程，有更高效的方法可用。鉴于我们只处理560行数据，并且我们的目标是聚类数据和检索主题，我们再次将使用DataFrame来管理我们的数据。让我们继续将我们的预测聚类添加到我们的DataFrame中：
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can see the output of this command in the following screenshot:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的屏幕截图中看到这个命令的输出：
- en: '![Figure 9.18 – A DataFrame showing the cleaned texts and their associated
    clusters ](img/B17761_09_018.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图9.18 – 显示清洗后的文本及其相关聚类的DataFrame](img/B17761_09_018.jpg)'
- en: Figure 9.18 – A DataFrame showing the cleaned texts and their associated clusters
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 – 显示清洗后的文本及其相关聚类的DataFrame
- en: 'With the data clustered, let''s plot this data in a 2D scatterplot. Given that
    we have several thousand features, we can make use of the **PCA** algorithm to
    reduce these down to only two features for our visualization:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据聚类后，让我们将此数据绘制在二维散点图上。鉴于我们有数千个特征，我们可以使用**PCA**算法将这些特征减少到仅用于可视化的两个特征：
- en: '[PRE34]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s go ahead and add these two principal components to our DataFrame:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续将这些两个主成分添加到我们的DataFrame中：
- en: '[PRE35]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can see the output of this command in the following screenshot:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的屏幕截图中看到这个命令的输出：
- en: '![Figure 9.19 – A DataFrame showing the texts, clusters, and principal components
    ](img/B17761_09_019.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图9.19 – 显示文本、簇和主成分的DataFrame](img/B17761_09_019.jpg)'
- en: Figure 9.19 – A DataFrame showing the texts, clusters, and principal components
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19 – 显示文本、簇和主成分的DataFrame
- en: 'Here, we can see that each row of text now has a cluster, as well as a set
    of coordinates, in the form of principal components. Next, we will plot our data
    and color by cluster:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到每一行文本现在都有一个簇，以及一组以主成分形式表示的坐标。接下来，我们将绘制我们的数据，并按簇着色：
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Upon executing this code, we will get the following output:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，我们将得到以下输出：
- en: '![Figure 9.20 – A scatterplot of the principal components colored by cluster,
    with stars representing the cluster centers](img/B17761_09_020.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图9.20 – 按簇着色的主成分散点图，星号代表簇中心](img/B17761_09_020.jpg)'
- en: Figure 9.20 – A scatterplot of the principal components colored by cluster,
    with stars representing the cluster centers
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20 – 按簇着色的主成分散点图，星号代表簇中心
- en: Here, we can see that there seems to be some adequate separation between clusters!
    The two clusters on the far left seem to have smaller variance in their distributions,
    whereas the other two are much more spread out. Given that we reduced a considerable
    number of features down to only two principal components, it makes sense that
    there is a certain degree of overlap between them, especially given the fact that
    all the articles were scientific.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到簇之间似乎有一些适当的分离！最左边的两个簇似乎在其分布中具有较小的方差，而其他两个则分布得更广。鉴于我们将相当数量的特征降低到只有两个主成分，它们之间一定程度的重叠是合理的，尤其是考虑到所有文章都是科学性的。
- en: 'Now, let''s go ahead and calculate some of the most prominent topics that were
    found in our dataset:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算一下在我们数据集中发现的一些最突出的主题：
- en: 'First, we will begin by implementing TFIDF:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将开始实现TFIDF：
- en: '[PRE37]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, we will reduce the dimensionality. However, this time, we will use **Non-Negative
    Matrix Factorization** (**NMF**) to reduce our data instead of PCA. We will need
    to specify the number of topics we are interested in:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将降低数据的维度。然而，这次，我们将使用**非负矩阵分解**（**NMF**）来降低我们的数据，而不是PCA。我们需要指定我们感兴趣的主题数量：
- en: '[PRE38]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, we can specify the number of keywords to retrieve per topic. After that,
    we will iterate over the components and retrieve the keywords of interest:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以指定每个主题要检索的关键词数量。之后，我们将遍历组件并检索感兴趣的关键词：
- en: '[PRE39]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Upon executing this loop, we retrieve the following as output:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行此循环后，我们将检索以下输出：
- en: '![Figure 9.21 – Top 10 topics of the dataset, each represented by three keywords
    ](img/B17761_09_021.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图9.21 – 数据集的前10个主题，每个主题由三个关键词表示](img/B17761_09_021.jpg)'
- en: Figure 9.21 – Top 10 topics of the dataset, each represented by three keywords
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.21 – 数据集的前10个主题，每个主题由三个关键词表示
- en: 'We can use these topic modeling methods to extract insights and trends from
    the dataset, allowing users to have high-level interpretations without the need
    to dive into the datasets as a whole. Throughout this tutorial, we examined one
    of the classical methods for clustering and topic modeling: using **TFIDF** and
    **NMF**. However, many other methods exist that use language models, such as **BERT**
    and **BioBERT** and libraries such as **Gensim** and **LDA**. If this is an area
    you find interesting, I highly urge you to explore these libraries for more information.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些主题建模方法从数据集中提取见解和趋势，使用户能够进行高级解释，而无需深入整个数据集。在整个教程中，我们考察了聚类和主题建模的经典方法之一：使用**TFIDF**和**NMF**。然而，存在许多其他使用语言模型的方法，例如**BERT**和**BioBERT**，以及**Gensim**和**LDA**等库。如果您对这个领域感兴趣，我强烈建议您探索这些库以获取更多信息。
- en: Often, you will not have your data already existing in a usable format. In this
    tutorial, we had our dataset structured within a DataFrame, ready for use to slice
    and dice. However, in many cases, our data of interest will be unstructured, such
    as in PDFs. We will explore how to handle situations such as these in the next
    section.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您可能不会以可用的格式拥有您的数据。在本教程中，我们的数据集结构在DataFrame中，准备好使用以进行切片和切块。然而，在许多情况下，我们感兴趣的数据可能是未结构化的，例如在PDF中。我们将在下一节中探讨如何处理这些情况。
- en: Working with unstructured data
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理未结构化数据
- en: In the previous section, we explored some of the most common tasks and processes
    that are conducted when handing text-based data. More often than not, you will
    find that the data you work with is generally not of a structured nature, or perhaps
    not of a digital nature. Take, for example, a company that has decided to move
    all printed documents to a digital state. Or perhaps a company that maintains
    a large repository of documents, none of which are structured or organized. For
    tasks such as these, we can rely on several AWS products to come to our rescue.
    We will explore two of the most useful NLP tools in the next few sections.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: OCR using AWS Textract
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In my opinion, one of the most useful tools available within **AWS** is an **Optical
    Character Recognition** (**OCR**) tool known as **AWS Textract**. The main idea
    behind this tool is to enable users to extract text, tables, and other useful
    items from images or static PDF documents using pre-built machine learning models
    implemented within Textract.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, users can upload images or scanned PDF documents to Textract that
    are otherwise unsearchable and extract all the text-based content from them, as
    shown in the following diagram:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.22 – A schematic showing structuring raw PDFs into organized digital
    text ](img/B17761_09_022.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: Figure 9.22 – A schematic showing structuring raw PDFs into organized digital
    text
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to extracting text, users can extract key-value pairs such as those
    found in both printed and handwritten forms:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.23 – A schematic showing the structuring of handwritten data to
    organized tables ](img/B17761_09_023.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Figure 9.23 – A schematic showing the structuring of handwritten data to organized
    tables
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'To use `boto3`. We can install Boto3 using `pip`. When using Boto3 within SageMaker,
    you will not be required to use any access keys to utilize the service. However,
    if you are using a local implementation of Jupyter Notebook, you will need to
    be able to authenticate using access keys. **Access keys** can easily be created
    in a few simple steps:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the AWS console and select **IAM** from the **Services** menu.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the **Access Management** tab on the left, click on **Users**, then **Add
    Users**.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go ahead and give your user a name, such as `ml-biotech-user`, and enable the
    **Programmatic access** option:![Figure 9.24 – Setting the username for AWS IAM
    roles ](img/B17761_09_024.jpg)
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.24 – Setting the username for AWS IAM roles
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, select the **Attach existing policies directly** option at the top and
    add the policies of interest. Go ahead and add Textract, Comprehend, and S3 as
    we will require all three for the role:![Figure 9.25 – Setting the policies for
    AWS IAM roles ](img/B17761_09_025.jpg)
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.25 – Setting the policies for AWS IAM roles
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After labeling your new user with some descriptive tags, you will gain access
    to two items: your **access key ID** and **AWS secret access k**. Be sure to copy
    those two items to a safe space. For security, you will not be able to retrieve
    them from AWS after leaving this page.'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have our access keys, let's go ahead and start implementing Textract
    on a document of interest. We can complete this in a few steps.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will need to upload our data to our **S3 bucket**. We can recycle
    the same S3 bucket we used earlier in this book. We will need to specify our keys,
    and then connect to AWS using the Boto3 client:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'With our connection set up, we can go ahead and upload a sample PDF file. Note
    that you can submit PDFs as well as image files (PNG). Let''s go ahead and upload
    our PDF file using the `upload_fileobj()` function:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'With our PDF now uploaded, we can use Textract. First, we will need to connect
    using the Boto3 client. Note that we changed the desired resource from `''s3''`
    to `''textract''` since we are using a different service now:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we can send our file to Textract using the `start_document_text_detection()`
    method, where we specify the name of the bucket and the name of the document:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can confirm that the task was started successfully by checking the status
    code in the response variable. After a few moments (depending on the duration
    of the job), we retrieve the results by specifying `JobId`:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Immediately, we will notice that the `results` variable is simply one large
    JSON that we can parse and iterate over. Notice that the structure of the JSON
    is quite complex and detailed.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we can gather all the text by iterating over `Blocks` and collecting
    all the texts for blocks of the `LINE` type:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: If you print the `documentText` variable, you will see all of the text that
    was successfully collected from that document! Textract can be an extremely useful
    tool for moving documents from an unstructured and unsearchable state to a more
    structured and searchable state. Often, most text-based data will exist in an
    unstructured format, and you will find Textract to be one of the most useful resources
    for these types of applications. Textract is generally coupled with other AWS
    resources to maximize the utility of the tool, such as **DynamoDB** for storage
    or **Comprehend** for analysis. We will explore Comprehend in the next section.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Entity recognition using AWS Comprehend
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier in this chapter, we implemented a NER model using the SciPy library
    to detect entities in a given section of text. Now, let's explore a more powerful
    implementation of NER known as **AWS Comprehend**. Comprehend is an NLP service
    that's designed to discover insights in unstructured text data, allowing users
    to extract key phrases, calculate sentiment, identify entities, and much more.
    Let's go ahead and explore this tool.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to other `boto3` client:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we can go ahead and use the `detect_entities()` function to identify
    entities in our text. We can use the `documentText` string we generated using
    Textract earlier:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Upon printing the response, we will see the results for each of the entities
    that were detected in our block of text. In addition, we can organize the results
    in a DataFrame:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Upon sorting the values by score, we can see our results listed in a structured
    manner:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.26 – A sample DataFrame showing the results of the AWS Comprehend
    entities API ](img/B17761_09_026.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: Figure 9.26 – A sample DataFrame showing the results of the AWS Comprehend entities
    API
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to entities, Comprehend can also detect key phrases within text:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Upon printing the first item in the list, we can see the score, phrase, and
    position in the string:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.27 – Results of the AWS Comprehend key phrases API ](img/B17761_09_027.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: Figure 9.27 – Results of the AWS Comprehend key phrases API
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we can also detect the sentiment using the `detect_sentiment()`
    function:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We can print the response variable to get the results of the string. We can
    see that the sentiment was noted as neutral, which makes sense for a statement
    concerning scientific data that is generally not written with a positive or negative
    tone:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.28 – Results of the AWS Comprehend sentiment API ](img/B17761_09_028.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Figure 9.28 – Results of the AWS Comprehend sentiment API
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, Comprehend can also detect dominant languages within text using the
    `detect_dominant_language()` function:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Here, we can see that, upon printing the response, we get a sense of the language,
    as well as the associated score or probability from the model:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.29 – Results of the AWS Comprehend language detection API ](img/B17761_09_029.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: Figure 9.29 – Results of the AWS Comprehend language detection API
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: AWS Textract and AWS Comprehend are two of the top NLP tools available today
    and have been instrumental in structuring and analyzing vast amounts of unstructured
    text documents. Most NLP-based applications today generally use at least one,
    if not both, of these types of technologies. For more information about Textract
    and Comprehend, I highly recommend that you visit the AWS website ([https://aws.amazon.com/](https://aws.amazon.com/)).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned how to analyze and transform text-based data, especially
    when it comes to moving data from an unstructured state to a more structured state.
    Now that the documents are more organized, the next step is to be able to use
    them in one way or another, such as through a search engine. We will learn how
    to create a **semantic search** engine using **transformers** in the next section.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Tutorial – developing a scientific data search engine using transformers
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have looked at text from a word-by-word perspective in the sense
    that we kept our text *as is*, without the need to convert or embed it in any
    way. In some cases, converting words into numerical values or **embeddings** can
    open many new doors and unlock many new possibilities, especially when it comes
    to deep learning. Our main objective within this tutorial will be to develop a
    search engine to find and retrieve scientific data. We will do so by implementing
    an important and useful deep learning NLP architecture known as a transformer.
    The main benefit here is that we will be designing a powerful semantic search
    engine in the sense that we can now search for ideas or semantic meaning rather
    than only keywords.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: We can think of transformers as deep learning models designed to solve sequence-based
    tasks using a mechanism known as **self-attention**. We can think of self-attention
    as a method to help relate different portions of text within a sentence or embedding
    in an attempt to create a representation. Simply put, the model attempts to view
    sentences as ideas, rather than a collection of single words.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin to work with transformers, let''s talk a little more about
    the idea of **embeddings**. We can think of embeddings as low-dimensional numerical
    values or vectors of continuous numbers representing an item, which in our case
    would be a word or sentence. We commonly convert words and sentences into embeddings
    to allow models to carry out machine learning tasks more easily when working with
    larger datasets. Within the context of NLP and neural networks, there are three
    main reasons embeddings are used:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the **dimensionality** of large segments of text data
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To compute the **similarity** between two different texts
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To **visualize** relationships between portions of text
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have gained a better sense of embeddings and the role they play
    in NLP, let''s go ahead and get started with a real-world example of a scientific
    search engine. We will begin by importing a few libraries that we will need:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'To create our embeddings, we will need a model. We have the option to create
    a customized model concerning our dataset. The benefit here is that our results
    would likely improve, given that the model was trained on text about our domain.
    Alternatively, we could use other pre-trained models available from the `SentenceTransformer`
    website ([https://www.sbert.net/](https://www.sbert.net/)). Let''s download one
    of these pre-trained models:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, we can create a testing database and populate it with a few sentences:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, we can call the `encode()` function to convert our list of strings into
    a list of embeddings:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'If we check the length of the database and the length of `database_embeddings`
    using the `len()` function, we will find that they both contain the same number
    of elements since there should be one embedding for every piece of text. If we
    print the contents of the first element of the embeddings database, we will find
    that the content is now simply a list of vectors:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.30 – A view of an embedded piece of text ](img/B17761_09_030.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: Figure 9.30 – A view of an embedded piece of text
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'With each of our documents now embedded, the idea would be that a user would
    want to search for or query a particular phrase. We can take a user''s query and
    encode it as we did with the others, but assign that value to a new variable that
    we will call `query_embedding`:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'With the query and sentences embedded, we can compute the distance between
    the items. The idea here is that documents that were more similar to the user''s
    query would have shorter distances, and those that were less similar would have
    longer ones. Notice that we are using cosine here as a measure of distance, and
    therefore, similarity. We can use other methods as well, such as the `euclidean`
    distance:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Let''s go ahead and prepare a single `runSearch` function that incorporates
    the query, the encoder, as well as a method to display our results. The process
    begins with a few print statements, and then encodes the new query into a variable
    called `query_embedding`. The distances are then calculated, and the results are
    sorted according to their distance. Finally, the results are iterated over and
    the scores, titles, and abstracts for each are printed:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now that we have prepared our function, we can call it with our query of interest:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Upon calling the function, we retrieve several results printed similarly. We
    can see one of the results in the following screenshot, showing us the `score`,
    `title`, and `abstract` properties of the article:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.31 – Results of the semantic searching model for scientific text
    ](img/B17761_09_031.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: Figure 9.31 – Results of the semantic searching model for scientific text
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have managed to successfully develop a semantic searching model
    capable of searching through scientific literature. Notice that the query itself
    is not a direct string match to the top result the model returned. Again, the
    idea here is not to match keywords but to calculate the distance between embeddings,
    which is representative of similarities.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we made an adventurous attempt to cover a wide range of NLP
    topics. We explored a range of introductory topics such as NER, tokenization,
    and parts of speech using the NLTK and spaCy libraries. We then explored NLP through
    the lens of structured datasets, in which we utilized the `pymed` library as a
    source for scientific literature and proceeded to analyze and clean the data in
    our preprocessing steps. Next, we developed a word cloud to visualize the frequency
    of words in a given dataset. Finally, we developed a clustering model to group
    our abstracts and a topic modeling model to identify prominent topics.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: We then explored NLP through the lens of unstructured data in which we explored
    two common AWS NLP products. We used Textract to convert PDFs and images into
    searchable and structured text and Comprehend to analyze and provide insights.
    Finally, we learned how to develop a semantic search engine using deep learning
    transformers to find pertinent information.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: What was particularly unique about this chapter is that we learned that text
    is a sequence-based type of data, which makes its uses and applications drastically
    different from many of the other datasets we had previously worked with. As companies
    across the world begin to migrate legacy documents into the digital space, the
    ability to search for documents and identify insights will be of great value.
    In the next chapter, we will examine another type of sequence-based data known
    as time series.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
