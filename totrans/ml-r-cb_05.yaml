- en: Chapter 5. Classification (I) – Tree, Lazy, and Probabilistic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the training and testing datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a classification model with recursive partitioning trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing a recursive partitioning tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the prediction performance of a recursive partitioning tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning a recursive partitioning tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a classification model with a conditional inference tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing a conditional inference tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the prediction performance of a conditional inference tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying data with a k-nearest neighbor classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying data with logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying data with the Naïve Bayes classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification is used to identify a category of new observations (testing datasets)
    based on a classification model built from the training dataset, of which the
    categories are already known. Similar to regression, classification is categorized
    as a supervised learning method as it employs known answers (label) of a training
    dataset to predict the answer (label) of the testing dataset. The main difference
    between regression and classification is that regression is used to predict continuous
    values.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to this, classification is used to identify the category of a given
    observation. For example, one may use regression to predict the future price of
    a given stock based on historical prices. However, one should use the classification
    method to predict whether the stock price will rise or fall.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will illustrate how to use R to perform classification.
    We first build a training dataset and a testing dataset from the churn dataset,
    and then apply different classification methods to classify the churn dataset.
    In the following recipes, we will introduce the tree-based classification method
    using a traditional classification tree and a conditional inference tree, lazy-based
    algorithm, and a probabilistic-based method using the training dataset to build
    up a classification model, and then use the model to predict the category (class
    label) of the testing dataset. We will also use a confusion matrix to measure
    the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the training and testing datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a classification model requires a training dataset to train the classification
    model, and testing data is needed to then validate the prediction performance.
    In the following recipe, we will demonstrate how to split the telecom churn dataset
    into training and testing datasets, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use the telecom churn dataset as the input data source,
    and split the data into training and testing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to split the churn dataset into training and testing
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can retrieve the churn dataset from the `C50` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `str` to read the structure of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can remove the `state`, `area_code`, and `account_length` attributes, which
    are not appropriate for classification features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, split 70 percent of the data into the training dataset and 30 percent
    of the data into the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, use `dim` to explore the dimensions of both the training and testing
    datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we use the telecom churn dataset as our example data source.
    The dataset contains 20 variables with 3,333 observations. We would like to build
    a classification model to predict whether a customer will churn, which is very
    important to the telecom company as the cost of acquiring a new customer is significantly
    more than retaining one.
  prefs: []
  type: TYPE_NORMAL
- en: Before building the classification model, we need to preprocess the data first.
    Thus, we load the churn data from the `C50` package into the R session with the
    variable name as `churn`. As we determined that attributes such as `state`, `area_code`,
    and `account_length` are not useful features for building the classification model,
    we remove these attributes.
  prefs: []
  type: TYPE_NORMAL
- en: After preprocessing the data, we split it into training and testing datasets,
    respectively. We then use a sample function to randomly generate a sequence containing
    70 percent of the training dataset and 30 percent of the testing dataset with
    a size equal to the number of observations. Then, we use a generated sequence
    to split the churn dataset into the training dataset, `trainset`, and the testing
    dataset, `testset`. Lastly, by using the `dim` function, we found that 2,315 out
    of the 3,333 observations are categorized into the training dataset, `trainset`,
    while the other 1,018 are categorized into the testing dataset, `testset`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can combine the split process of the training and testing datasets into
    the `split.data` function. Therefore, you can easily split the data into the two
    datasets by calling this function and specifying the proportion and seed in the
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Building a classification model with recursive partitioning trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A classification tree uses a split condition to predict class labels based on
    one or multiple input variables. The classification process starts from the root
    node of the tree; at each node, the process will check whether the input value
    should recursively continue to the right or left sub-branch according to the split
    condition, and stops when meeting any leaf (terminal) nodes of the decision tree.
    In this recipe, we will introduce how to apply a recursive partitioning tree on
    the customer churn dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have completed the previous recipe by splitting the churn dataset
    into the training dataset (`trainset`) and testing dataset (`testset`), and each
    dataset should contain exactly 17 variables.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to split the churn dataset into training and testing
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `rpart` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `rpart` function to build a classification tree model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Type `churn.rp` to retrieve the node detail of the classification tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, use the `printcp` function to examine the complexity parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, use the `plotcp` function to plot the cost complexity parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00101.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1: The cost complexity parameter plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Lastly, use the `summary` function to examine the built model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we use a recursive partitioning tree from the `rpart` package
    to build a tree-based classification model. The recursive portioning tree includes
    two processes: recursion and partitioning. During the process of decision induction,
    we have to consider a statistic evaluation question (or simply a yes/no question)
    to partition the data into different partitions in accordance with the assessment
    result. Then, as we have determined the child node, we can repeatedly perform
    the splitting until the stop criteria is satisfied.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the data (shown in the following figure) in the root node can
    be partitioned into two groups with regard to the question of whether **f1** is
    smaller than **X**. If so, the data is divided into the left-hand side. Otherwise,
    it is split into the right-hand side. Then, we can continue to partition the left-hand
    side data with the question of whether **f2** is smaller than **Y**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Recursive partioning tree'
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, we load the `rpart` package with the `library` function.
    Next, we build a classification model using the `churn` variable as a classification
    category (class label) and the remaining variables as input features.
  prefs: []
  type: TYPE_NORMAL
- en: After the model is built, you can type the variable name of the built model,
    `churn.rp`, to display the tree node details. In the printed node detail, `n`
    indicates the sample size, `loss` indicates the misclassification cost, `yval`
    stands for the classified membership (`no` or `yes`, in this case), and `yprob`
    stands for the probabilities of two classes (the left value refers to the probability
    reaching label `no`, and the right value refers to the probability reaching label,
    `yes`).
  prefs: []
  type: TYPE_NORMAL
- en: Then, we use the `printcp` function to print the complexity parameters of the
    built tree model. From the output of `printcp`, one should find the value of CP,
    a complexity parameter, which serves as a penalty to control the size of the tree.
    In short, the greater the CP value, the fewer the number of splits there are (`nsplit`).
    The output value (the `rel` error) represents the average deviance of the current
    tree divided by the average deviance of the null tree. A `xerror` value represents
    the relative error estimated by a 10-fold classification. `xstd` stands for the
    standard error of the relative error.
  prefs: []
  type: TYPE_NORMAL
- en: To make the **CP** (**cost complexity parameter**) table more readable, we use
    `plotcp` to generate an information graphic of the CP table. As per the screenshot
    (step 5), the x-axis at the bottom illustrates the `cp` value, the y-axis illustrates
    the relative error, and the upper x-axis displays the size of the tree. The dotted
    line indicates the upper limit of a standard deviation. From the screenshot, we
    can determine that minimum cross-validation error occurs when the tree is at a
    size of 12.
  prefs: []
  type: TYPE_NORMAL
- en: We can also use the `summary` function to display the function call, complexity
    parameter table for the fitted tree model, variable importance, which helps identify
    the most important variable for the tree classification (summing up to 100), and
    detailed information of each node.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using the decision tree is that it is very flexible and easy
    to interpret. It works on both classification and regression problems, and more;
    it is nonparametric. Therefore, one does not have to worry about whether the data
    is linear separable. As for the disadvantage of using the decision tree, it is
    that it tends to be biased and over-fitted. However, you can conquer the bias
    problem through the use of a conditional inference tree, and solve the problem
    of over-fitting through a random forest method or tree pruning.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information about the `rpart`, `printcp`, and `summary` functions,
    please use the `help` function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`C50` is another package that provides a decision tree and a rule-based model.
    If you are interested in the package, you may refer to the document at [http://cran.r-project.org/web/packages/C50/C50.pdf](http://cran.r-project.org/web/packages/C50/C50.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing a recursive partitioning tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the last recipe, we learned how to print the classification tree in a text
    format. To make the tree more readable, we can use the `plot` function to obtain
    the graphical display of a built classification tree.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One needs to have the previous recipe completed by generating a classification
    model, and assign the model into the `churn.rp` variable.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to visualize the classification tree:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `plot` function and the `text` function to plot the classification
    tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00103.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3: The graphical display of a classification tree'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can also specify the `uniform`, `branch`, and `margin` parameter to adjust
    the layout:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00104.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4: Adjust the layout of the classification tree'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we demonstrate how to use the `plot` function to graphically display a
    classification tree. The `plot` function can simply visualize the classification
    tree, and you can then use the `text` function to add text to the plot.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3*, we assign margin = 0.1 as a parameter to add extra white space
    around the border to prevent the displayed text being truncated by the margin.
    It shows that the length of the branches displays the relative magnitude of the
    drop in deviance. We then use the text function to add labels for the nodes and
    branches. By default, the text function will add a split condition on each split,
    and add a category label in each terminal node. In order to add extra information
    on the tree plot, we set the parameter as all equal to `TRUE` to add a label to
    all the nodes. In addition to this, we add a parameter by specifying `use.n =
    TRUE` to add extra information, which shows that the actual number of observations
    fall into two different categories (no and yes).
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4*, we set the option branch to 0.6 to add a shoulder to each plotted
    branch. In addition to this, in order to display branches of an equal length rather
    than relative magnitude of the drop in deviance, we set the option uniform to
    `TRUE`. As a result, *Figure 4* shows a classification tree with short shoulders
    and branches of equal length.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may use `?plot.rpart` to read more about the plotting of the classification
    tree. This document also includes information on how to specify the parameters,
    `uniform`, `branch`, `compress`, `nspace`, `margin`, and `minbranch`, to adjust
    the layout of the classification tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the prediction performance of a recursive partitioning tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we have built a classification tree in the previous recipes, we can use
    it to predict the category (class label) of new observations. Before making a
    prediction, we first validate the prediction power of the classification tree,
    which can be done by generating a classification table on the testing dataset.
    In this recipe, we will introduce how to generate a predicted label versus a real
    label table with the `predict` function and the `table` function, and explain
    how to generate a confusion matrix to measure the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have the previous recipe completed by generating the classification
    model, `churn.rp`. In addition to this, you have to prepare the training dataset,
    `trainset`, and the testing dataset, `testset`, generated in the first recipe
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to validate the prediction performance of a classification
    tree:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `predict` function to generate a predicted label of testing
    the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `table` function to generate a classification table for the testing
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'One can further generate a confusion matrix using the `confusionMatrix` function
    provided in the `caret` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we use a `predict` function and built up classification model,
    `churn.rp`, to predict the possible class labels of the testing dataset, `testset`.
    The predicted categories (class labels) are coded as either no or yes. Then, we
    use the `table` function to generate a classification table on the testing dataset.
    From the table, we discover that there are 859 correctly predicted as no, while
    18 are misclassified as yes. 100 of the yes predictions are correctly predicted,
    but 41 observations are misclassified into no. Further, we use the `confusionMatrix`
    function from the `caret` package to produce a measurement of the classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may use `?confusionMatrix` to read more about the performance measurement
    using the confusion matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For those who are interested in the definition output by the confusion matrix,
    please refer to the Wikipedia entry, **Confusion_matrix** ([http://en.wikipedia.org/wiki/Confusion_matrix](http://en.wikipedia.org/wiki/Confusion_matrix))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning a recursive partitioning tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous recipes, we have built a complex decision tree for the churn dataset.
    However, sometimes we have to remove sections that are not powerful in classifying
    instances to avoid over-fitting, and to improve the prediction accuracy. Therefore,
    in this recipe, we introduce the cost complexity pruning method to prune the classification
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have the previous recipe completed by generating a classification
    model, and assign the model into the `churn.rp` variable.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to prune the classification tree:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the minimum cross-validation error of the classification tree model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Locate the record with the minimum cross-validation errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the cost complexity parameter of the record with the minimum cross-validation
    errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prune the tree by setting the `cp` parameter to the CP value of the record
    with minimum cross-validation errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the classification tree by using the `plot` and `text` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00105.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5: The pruned classification tree'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you can generate a classification table based on the pruned classification
    tree model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can generate a confusion matrix based on the classification table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we discussed pruning a classification tree to avoid over-fitting
    and producing a more robust classification model. We first located the record
    with the minimum cross-validation errors within the `cptable`, and we then extracted
    the CP of the record and assigned the value to `churn.cp`. Next, we used the `prune`
    function to prune the classification tree with `churn.cp` as the parameter. Then,
    by using the `plot` function, we graphically displayed the pruned classification
    tree. From *Figure 5*, it is clear that the split of the tree is less than the
    original classification tree (*Figure 3*). Lastly, we produced a classification
    table and used the confusion matrix to validate the performance of the pruned
    tree. The result shows that the accuracy (0.9411) is slightly lower than the original
    model (0.942), and also suggests that the pruned tree may not perform better than
    the original classification tree as we have pruned some split conditions (Still,
    one should examine the change in sensitivity and specificity). However, the pruned
    tree model is more robust as it removes some split conditions that may lead to
    over-fitting.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For those who would like to know more about cost complexity pruning, please
    refer to the Wikipedia article for **Pruning (decision_trees)**: [http://en.wikipedia.org/wiki/Pruning_(decision_trees](http://en.wikipedia.org/wiki/Pruning_(decision_trees)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a classification model with a conditional inference tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to traditional decision trees (`rpart`), conditional inference trees
    (`ctree`) are another popular tree-based classification method. Similar to traditional
    decision trees, conditional inference trees also recursively partition the data
    by performing a univariate split on the dependent variable. However, what makes
    conditional inference trees different from traditional decision trees is that
    conditional inference trees adapt the significance test procedures to select variables
    rather than selecting variables by maximizing information measures (`rpart` employs
    a Gini coefficient). In this recipe, we will introduce how to adapt a conditional
    inference tree to build a classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have the first recipe completed by generating the training dataset,
    `trainset`, and the testing dataset, `testset`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to build the conditional inference tree:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we use `ctree` from the `party` package to build the classification
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we examine the built tree model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we used a conditional inference tree to build a classification
    tree. The use of `ctree` is similar to `rpart`. Therefore, you can easily test
    the classification power using either a traditional decision tree or a conditional
    inference tree while confronting classification problems. Next, we obtain the
    node details of the classification tree by examining the built model. Within the
    model, we discover that `ctree` provides information similar to a split condition,
    criterion (1 – p-value), statistics (test statistics), and weight (the case weight
    corresponding to the node). However, it does not offer as much information as
    `rpart` does through the use of the `summary` function.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may use the `help` function to refer to the definition of **Binary** **Tree**
    **Class** and read more about the properties of binary trees:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Visualizing a conditional inference tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to `rpart`, the `party` package also provides a visualization method
    for users to plot conditional inference trees. In the following recipe, we will
    introduce how to use the `plot` function to visualize conditional inference trees.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have the first recipe completed by generating the conditional inference
    tree model, `ctree.model`. In addition to this, you need to have both, `trainset`
    and `testset`, loaded in an R session.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to visualize the conditional inference tree:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `plot` function to plot `ctree.model` built in the last recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00106.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6: A conditional inference tree of churn data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To obtain a simple conditional inference tree, one can reduce the built model
    with less input features, and redraw the classification tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00107.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7: A conditional inference tree using the total_day_charge variable
    as only split condition'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To visualize the node detail of the conditional inference tree, we can apply
    the `plot` function on a built classification model. The output figure reveals
    that every intermediate node shows the dependent variable name and the p-value.
    The split condition is displayed on the left and right branches. The terminal
    nodes show the number of categorized observations, *n*, and the probability of
    a class label of either 0 or 1.
  prefs: []
  type: TYPE_NORMAL
- en: Taking *Figure 7* as an example, we first build a classification model using
    `total_day_charge` as the only feature and `churn` as the class label. The built
    classification tree shows that when `total_day_charge` is above 48.18, the lighter
    gray area is greater than the darker gray in node 9, which indicates that the
    customer with a day charge of over 48.18 has a greater likelihood to churn (label
    = yes).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The visualization of the conditional inference tree comes from the `plot.BinaryTree`
    function. If you are interested in adjusting the layout of the classification
    tree, you may use the `help` function to read the following document:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Measuring the prediction performance of a conditional inference tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After building a conditional inference tree as a classification model, we can
    use the `treeresponse` and `predict` functions to predict categories of the testing
    dataset, `testset`, and further validate the prediction power with a classification
    table and a confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have the previous recipe completed by generating the conditional
    inference tree model, `ctree.model`. In addition to this, you need to have both
    `trainset` and `testset` loaded in an R session.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to measure the prediction performance of a conditional
    inference tree:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `predict` function to predict the category of the testing dataset,
    `testset`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Furthermore, you can use `confusionMatrix` from the caret package to generate
    the performance measurements of the prediction result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also use the `treeresponse` function, which will tell you the list
    of class probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we first demonstrate that one can use the `prediction` function
    to predict the category (class label) of the testing dataset, `testset`, and then
    employ a `table` function to generate a classification table. Next, you can use
    the `confusionMatrix` function built into the caret package to determine the performance
    measurements.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the `predict` function, `treeresponse` is also capable of estimating
    the class probability, which will often classify labels with a higher probability.
    In this example, we demonstrated how to obtain the estimated class probability
    using the top five records of the testing dataset, `testset`. The `treeresponse`
    function returns a list of five probabilities. You can use the list to determine
    the label of instance.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the `predict` function, you can specify the type as `response`, `prob`,
    or `node`. If you specify the type as `prob` when using the `predict` function
    (for example, `predict(… type="prob")`), you will get exactly the same result
    as what `treeresponse` returns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying data with the k-nearest neighbor classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**K-nearest neighbor** (**knn**) is a nonparametric lazy learning method. From
    a nonparametric view, it does not make any assumptions about data distribution.
    In terms of lazy learning, it does not require an explicit learning phase for
    generalization. The following recipe will introduce how to apply the k-nearest
    neighbor algorithm on the churn dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have the previous recipe completed by generating the training and
    testing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to classify the churn data with the k-nearest neighbor
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, one has to install the `class` package and have it loaded in an R session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Replace `yes` and `no` of the `voice_mail_plan` and `international_plan` attributes
    in both the training dataset and testing dataset to 1 and 0:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `knn` classification method on the training dataset and the testing
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can use the `summary` function to retrieve the number of predicted
    labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can generate the classification matrix using the `table` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can generate a confusion matrix by using the `confusionMatrix`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**knn** trains all samples and classifies new instances based on a similarity
    (distance) measure. For example, the similarity measure can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Euclidian Distance**: ![How it works...](img/00108.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manhattan Distance**:![How it works...](img/00109.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In knn, a new instance is classified to a label (class) that is common among
    the k-nearest neighbors. If *k = 1*, then the new instance is assigned to the
    class where its nearest neighbor belongs. The only required input for the algorithm
    is k. If we give a small k input, it may lead to over-fitting. On the other hand,
    if we give a large k input, it may result in under-fitting. To choose a proper
    k-value, one can count on cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of knn are:'
  prefs: []
  type: TYPE_NORMAL
- en: The cost of the learning process is zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is nonparametric, which means that you do not have to make the assumption
    of data distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can classify any data whenever you can find similarity measures of given
    instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main disadvantages of knn are:'
  prefs: []
  type: TYPE_NORMAL
- en: It is hard to interpret the classified result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is an expensive computation for a large dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance relies on the number of dimensions. Therefore, for a high dimension
    problem, you should reduce the dimension first to increase the process performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of knn does not vary significantly from applying a tree-based algorithm
    mentioned in the previous recipes. However, while a tree-based algorithm may show
    you the decision tree model, the output produced by knn only reveals classification
    category factors. However, before building a classification model, one should
    replace the attribute with a string type to an integer since the k-nearest neighbor
    algorithm needs to calculate the distance between observations. Then, we build
    up a classification model by specifying *k=3*, which means choosing the three
    nearest neighbors. After the classification model is built, we can generate a
    classification table using predicted factors and the testing dataset label as
    the input. Lastly, we can generate a confusion matrix from the classification
    table. The confusion matrix output reveals an accuracy result of (0.8723), which
    suggests that both the tree-based methods mentioned in previous recipes outperform
    the accuracy of the k-nearest neighbor classification method in this case. Still,
    we cannot determine which model is better depending merely on accuracy, one should
    also examine the specificity and sensitivity from the output.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is another package named `kknn`, which provides a weighted k-nearest
    neighbor classification, regression, and clustering. You can learn more about
    the package by reading this document: [http://cran.r-project.org/web/packages/kknn/kknn.pdf](http://cran.r-project.org/web/packages/kknn/kknn.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying data with logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is a form of probabilistic statistical classification model,
    which can be used to predict class labels based on one or more features. The classification
    is done by using the `logit` function to estimate the outcome probability. One
    can use logistic regression by specifying the family as a binomial while using
    the `glm` function. In this recipe, we will introduce how to classify data using
    logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have completed the first recipe by generating training and testing
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to classify the churn data with logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the specification of family as a binomial, we apply the `glm` function
    on the dataset, `trainset`, by using churn as a class label and the rest of the
    variables as input features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `summary` function to obtain summary information of the built logistic
    regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we find that the built model contains insignificant variables, which
    would lead to misclassification. Therefore, we use significant variables only
    to train the classification model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can then use a fitted model, `fit`, to predict the outcome of `testset`.
    You can also determine the class by judging whether the probability is above 0.5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, the use of the `summary` function will show you the binary outcome count,
    and reveal whether the probability is above 0.5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can generate the counting statistics based on the testing dataset label
    and predicted result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can turn the statistics of the previous step into a classification table,
    and then generate the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression is very similar to linear regression; the main difference
    is that the dependent variable in linear regression is continuous, but the dependent
    variable in logistic regression is dichotomous (or nominal). The primary goal
    of logistic regression is to use logit to yield the probability of a nominal variable
    is related to the measurement variable. We can formulate logit in following equation:
    ln(P/(1-P)), where P is the probability that certain event occurs.'
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of logistic regression is that it is easy to interpret, it directs
    model logistic probability, and provides a confidence interval for the result.
    Unlike the decision tree, which is hard to update the model, you can quickly update
    the classification model to incorporate new data in logistic regression. The main
    drawback of the algorithm is that it suffers from multicollinearity and, therefore,
    the explanatory variables must be linear independent. `glm` provides a generalized
    linear regression model, which enables specifying the model in the option family.
    If the family is specified to a binomial logistic, you can set the family as a
    binomial to classify the dependent variable of the category.
  prefs: []
  type: TYPE_NORMAL
- en: The classification process begins by generating a logistic regression model
    with the use of the training dataset by specifying `Churn` as the class label,
    the other variables as training features, and family set as binomial. We then
    use the `summary` function to generate the model's summary information. From the
    summary information, we may find some insignificant variables (p-values > 0.05),
    which may lead to misclassification. Therefore, we should consider only significant
    variables for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use the `fit` function to predict the categorical dependent variable
    of the testing dataset, `testset`. The `fit` function outputs the probability
    of a class label, with a result equal to 0.5 and below, suggesting that the predicted
    label does not match the label of the testing dataset, and a probability above
    0.5 indicates that the predicted label matches the label of the testing dataset.
    Further, we can use the `summary` function to obtain the statistics of whether
    the predicted label matches the label of the testing dataset. Lastly, in order
    to generate a confusion matrix, we first generate a classification table, and
    then use `confusionMatrix` to generate the performance measurement.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more information of how to use the `glm` function, please refer to [Chapter
    4](part0046_split_000.html#page "Chapter 4. Understanding Regression Analysis"),
    *Understanding Regression Analysis*, which covers how to interpret the output
    of the `glm` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying data with the Naïve Bayes classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Naïve Bayes classifier is also a probability-based classifier, which is
    based on applying the Bayes theorem with a strong independent assumption. In this
    recipe, we will introduce how to classify data with the Naïve Bayes classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have the first recipe completed by generating training and testing
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to classify the churn data with the Naïve Bayes
    classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `e1071` library and employ the `naiveBayes` function to build the
    classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Type `classifier` to examine the function call, a-priori probability, and conditional
    probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can generate a classification table for the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can generate a confusion matrix from the classification table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Naive Bayes assumes that features are conditionally independent, which the
    effect of a predictor(x) to class (c) is independent of the effect of other predictors
    to class(c). It computes the posterior probability, *P(c|x)*, as the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *P(x|c)* is called likelihood, *p(x)* is called the marginal likelihood,
    and *p(c)* is called the prior probability. If there are many predictors, we can
    formulate the posterior probability as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The advantage of Naïve Bayes is that it is relatively simple and straightforward
    to use. It is suitable when the training set is relative small, and may contain
    some noisy and missing data. Moreover, you can easily obtain the probability for
    a prediction. The drawbacks of Naïve Bayes are that it assumes that all features
    are independent and equally important, which is very unlikely in real-world cases.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we use the Naïve Bayes classifier from the `e1071` package to
    build a classification model. First, we specify all the variables (excluding the
    `churn` class label) as the first input parameters, and specify the `churn` class
    label as the second parameter in the `naiveBayes` function call. Next, we assign
    the classification model into the variable classifier. Then, we print the variable
    classifier to obtain information, such as function call, A-priori probabilities,
    and conditional probabilities. We can also use the `predict` function to obtain
    the predicted outcome and the `table` function to retrieve the classification
    table of the testing dataset. Finally, we use a confusion matrix to calculate
    the performance measurement of the classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'At last, we list a comparison table of all the mentioned algorithms in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Advantage | Disadvantage |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Recursive partitioning tree |'
  prefs: []
  type: TYPE_TB
- en: Very flexible and easy to interpret
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works on both classification and regression problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonparametric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Prone to bias and over-fitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Conditional inference tree |'
  prefs: []
  type: TYPE_TB
- en: Very flexible and easy to interpret
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works on both classification and regression problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonparametric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less prone to bias than a recursive partitioning tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Prone to over-fitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| K-nearest neighbor classifier |'
  prefs: []
  type: TYPE_TB
- en: The cost of the learning process is zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonparametric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can classify any data whenever you can find similarity measures of any given
    instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Hard to interpret the classified result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computation is expensive for a large dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance relies on the number of dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Logistic regression |'
  prefs: []
  type: TYPE_TB
- en: Easy to interpret
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides model logistic probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides confidence interval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can quickly update the classification model to incorporate new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Suffers multicollinearity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not handle the missing value of continuous variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensitive to extreme values of continuous variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Naïve Bayes |'
  prefs: []
  type: TYPE_TB
- en: Relatively simple and straightforward to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suitable when the training set is relative small
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can deal with some noisy and missing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can easily obtain the probability for a prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Assumes all features are independent and equally important, which is very unlikely
    in real-world cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prone to bias when the number of training sets increase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To learn more about the Bayes theorem, you can refer to the following Wikipedia
    article: [http://en.wikipedia.org/wiki/Bayes''_theorem](http://en.wikipedia.org/wiki/Bayes''_theorem)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
