- en: Chapter 7. Gyroscopic Video Stabilization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Video stabilization is a classic problem in computer vision. The idea is simple
    – you have a video stream that's shaky, and you're trying to identify the best
    way to negate the motion of the camera to produce a smooth motion across images.
    The resulting video is easier to view and has a cinematic look.
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, there have been a number of approaches being tried to solve
    this. Videos have traditionally been stabilized by using data available only from
    images, or using specialized hardware to negate physical motion in the camera.
    Gyroscopes in mobile devices are the middle ground between these two approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An Android camera application to record media and gyroscope traces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the video and gyroscope trace to find mathematical unknowns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the physical camera unknowns to compensate for camera motion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying rolling shutter in the camera
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rolling shutter on a camera sensor produces unwanted effects. We'll cover this
    in detail in a later section. Also, refer to [Chapter 1](part0015_split_000.html#E9OE2-940925703e144daa867f510896bffb69
    "Chapter 1. Getting the Most out of Your Camera System"), *Getting the Most out
    of Your Camera System*, for a detailed discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get started, let's take a look at some techniques that were used in
    the past to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Stabilization with images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Video stabilization with images alone seems like the first logical step. Indeed,
    initial research on stabilizing video captured by cameras was based on using information
    readily available from the camera sensors to understand how they move image by
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to find keypoints in an image sequence to understand how they move
    image by image. Keypoints are pixel locations on an image that match these criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: Keypoints should be easily recognizable and distinguishable from each other.
    Corners of objects are good keypoints while a point on a blank wall is not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be possible to track keypoints across multiple images to calculate
    motion. You should be able to tell exactly where the keypoint has moved from one
    frame to another.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For performance, identifying these keypoints should be fast and memory-efficient.
    This is usually a bottleneck on low memory and low power devices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Research with these criteria led to several unique approaches like including
    some famous algorithms such as SIFT, SURF, ORB, FREAK, and so on. These techniques
    often work well.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV comes with several common keypoint detectors. These include ORB, FAST,
    BRISK, SURF, and so on. Check the 2D Features Framework documentation page for
    more information on how to use these at: [http://docs.opencv.org/3.0-beta/modules/features2d/doc/feature_detection_and_description.html](http://docs.opencv.org/3.0-beta/modules/features2d/doc/feature_detection_and_description.html).'
  prefs: []
  type: TYPE_NORMAL
- en: The keypoint detectors, however, have their own set of drawbacks. Firstly, the
    results of stabilization are highly dependent on the quality of the images. For
    example, a low resolution image might not produce the best set of features. Out
    of focus and blurry images are another concern. This puts a constraint on the
    types of sequences that can be stabilized. A scene with a clear blue sky and yellow
    sand might not contain enough features.
  prefs: []
  type: TYPE_NORMAL
- en: A surface with a repetitive pattern will confuse the algorithm because the same
    features keep showing up in different positions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Stabilization with images](img/00116.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The image above is taken from: [https://d2v9y0dukr6mq2.cloudfront.net/video/thumbnail/kG-5Wkc/crowd-of-people-walking-crossing-street-at-night-in-times-square-slow-motion-30p_ekqzvese__S0000.jpg](https://d2v9y0dukr6mq2.cloudfront.net/video/thumbnail/kG-5Wkc/crowd-of-people-walking-crossing-street-at-night-in-times-square-slow-motion-30p_ekqzvese__S0000.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, if there is a large amount of motion in the image (such as people
    walking in the background or a truck moving across the road), the stabilization
    will be skewed because of it. The keypoint is tracking the moving object and not
    the motion of the camera itself. This limits the types of videos that can be successfully
    stabilized with such an approach. There are ways of getting around such constraints
    – however, it makes the algorithm more complex.
  prefs: []
  type: TYPE_NORMAL
- en: Stabilization with hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Certain industries, such as the movie industry and the military, expect high
    quality video stabilization. Using just images in those varied environments would
    not work. This led industries to create hardware-based image stabilization rigs.
    For example, a quadcopter with a camera needs to have a high quality video output
    despite (potentially) bad lighting conditions, wind, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Stabilization with hardware](img/00117.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The image above is taken from [http://g01.a.alicdn.com/kf/HTB1.ogPIpXXXXaXXVXXq6xXFXXXw/GH4-A7S-SUMMER-DYS-3-axis-3-Axis-Gimbal-dslr-camera-Stabilizer-gyro-brushless-gimbal-steadicam.jpg](http://g01.a.alicdn.com/kf/HTB1.ogPIpXXXXaXXVXXq6xXFXXXw/GH4-A7S-SUMMER-DYS-3-axis-3-Axis-Gimbal-dslr-camera-Stabilizer-gyro-brushless-gimbal-steadicam.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: These devices use a gyroscope to physically move and rotate the camera so that
    the image sequence stays stable. The results look excellent since you're actually
    compensating for the motion of the camera.
  prefs: []
  type: TYPE_NORMAL
- en: These devices tend to be on the more expensive side and thus unaffordable for
    the common consumer. They also tend to be quite bulky. The average person would
    not want to carry a two kilogram rig on his vacation.
  prefs: []
  type: TYPE_NORMAL
- en: A hybrid of hardware and software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers a hybrid solution between the original software-only approach
    and hardware devices. This became possible only recently, with the advent of the
    smartphone. People now had a high quality camera and a gyroscope in a small form.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind this approach is to use the gyroscope to capture motion and
    the camera sensor to capture light. These two streams are then fused so that the
    image is always stable.
  prefs: []
  type: TYPE_NORMAL
- en: As the sensors' density increases and we head to 4K cameras, selecting a (stable)
    subregion of the image becomes an increasingly viable option as we can discard
    more of the image without compromising on the quality.
  prefs: []
  type: TYPE_NORMAL
- en: The math
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we jump into the code, let's take an overview of the algorithm. There
    are four key components.
  prefs: []
  type: TYPE_NORMAL
- en: The first is the pinhole camera model. We try and approximate real world positions
    to pixels using this matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second is the camera motion estimate. We need to use data from the gyroscope
    to figure out the orientation of the phone at any given moment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third is the rolling shutter computation. We need to specify the direction
    of the rolling shutter and estimate the duration of the rolling shutter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth is the image warping expression. Using all the information from the
    previous calculations, we need to generate a new image so that it becomes stable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The camera model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use the standard pinhole camera model. This model is used in several algorithms
    and is a good approximation of an actual camera.
  prefs: []
  type: TYPE_NORMAL
- en: '![The camera model](img/00118.jpeg)![The camera model](img/00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There are three unknowns. The *o* variables indicate the origin of the camera
    axis in the image plane (these can be assumed to be 0). The two 1s in the matrix
    indicate the aspect ratio of the pixels (we're assuming square pixels). The *f*
    indicates the focal length of the lens. We're assuming the focal length is the
    same in both horizontal and vertical directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this model, we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The camera model](img/00120.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, X is the point in the real world. There is also an unknown scaling factor,
    *q*, present.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Estimating this unknown is not possible for monocular vision unless the physical
    dimensions of an object are known.
  prefs: []
  type: TYPE_NORMAL
- en: '*K* is the intrinsic matrix and *x* is the point on the image.'
  prefs: []
  type: TYPE_NORMAL
- en: The Camera motion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can assume that the world origin is the same as the camera origin. Then,
    the motion of the camera can be described in terms of the orientation of the camera.
    Thus, at any given time *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Camera motion](img/00121.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The rotation matrix *R* can be calculated by integrating the angular velocity
    of the camera (obtained from the gyroscope).
  prefs: []
  type: TYPE_NORMAL
- en: '![The Camera motion](img/00122.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *ω[d]* is the gyroscope drift and *t[d]* is the delay between the gyroscope
    and frame timestamps. These are unknowns as well; we need a mechanism to calculate
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling shutter compensation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you click a picture, the common assumption is that the entire image is
    captured in one go. This is indeed the case for images captured with CCD sensors
    (which were prevalent a while back). With the commercialization of CMOS image
    sensors, this is no longer the case. Some CMOS sensors support a global shutter
    too but, in this chapter, we'll assume the sensor has a rolling shutter.
  prefs: []
  type: TYPE_NORMAL
- en: Images are captured one row at a time—usually the first row is captured first,
    then the second row, and so on. There's a very slight delay between the consecutive
    rows of an image.
  prefs: []
  type: TYPE_NORMAL
- en: This leads to strange effects. This is very visible when we're correcting camera
    shake (for example if there's a lot of motion in the camera).
  prefs: []
  type: TYPE_NORMAL
- en: '![Rolling shutter compensation](img/00123.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The fan blades are the same size; however due to the fast motion, the rolling
    shutter causes artifacts in the image recorded by the sensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'To model the rolling shutter, we need to identify at what time a specific row
    was captured. This can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rolling shutter compensation](img/00124.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *t[i]* is the time when the i^(th) frame was captured, *h* is the height
    of the image frame, and *t[s]* is the duration of the rolling shutter, that is,
    the time it takes to scan from top to bottom. Assuming each row takes the same
    time, the y^(th) row would take *t[s] * y / h* additional time to get scanned.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This assumes the rolling shutter happens from top to bottom. A rolling shutter
    from bottom to top can be modeled with a negative value for t[s]. Also, a rolling
    shutter from left to right can be modeled by replacing *y / h* with *x / w* where
    *w* is the width of the frame.
  prefs: []
  type: TYPE_NORMAL
- en: Image warping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have the estimated camera motion and a model for correcting the
    rolling shutter. We''ll combine both and identify a relationship across multiple
    frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image warping](img/00125.jpeg) (for frame i with rotation configuration 1)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Image warping](img/00126.jpeg) (for frame j with rotation configuration 2)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'We can combine these two equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image warping](img/00127.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'From here, we can calculate a warping matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image warping](img/00128.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, the relationship between points x[i] and x[j] can be more succinctly described
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image warping](img/00129.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This warp matrix simultaneously corrects both the video shake and the rolling
    shutter.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can map the original video to an artificial camera that has smooth motion
    and a global shutter (no rolling shutter artifacts).
  prefs: []
  type: TYPE_NORMAL
- en: This artificial camera can be simulated by low-pass filtering the input camera's
    motion and setting the rolling shutter duration to zero. A low pass filter removes
    high frequency noise from the camera orientation. Thus, the artificial camera's
    motion will appear much smoother.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, this matrix can be calculated for each row in the image. However, in
    practice, subdividing the image into five subsections produces good results as
    well (with better performance).
  prefs: []
  type: TYPE_NORMAL
- en: Project overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a moment to understand how the code in this chapter is organized.
    We have two moving pieces. One is the mobile application and the second is the
    video stabilizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mobile app only records video and stores the gyroscope signals during the
    video. It dumps this data into two files: a `.mp4` and a `.csv` file. These two
    files are the input for the next step. There is no computation on the mobile device.
    In this chapter, we''ll use Android as our platform. Moving to any other platform
    should be fairly easy—we are doing only basic tasks that any platform should support.'
  prefs: []
  type: TYPE_NORMAL
- en: The video stabilizer runs on a desktop. This is to help you figure out what's
    happening in the stabilization algorithm much more easily. Debugging, stepping
    through code and viewing images on a mobile device is relatively slower than iterating
    on a desktop. We have some really good scientific modules available for free (from
    the Python community. In this project, we will use Scipy, Numpy, and Matplotlib).
  prefs: []
  type: TYPE_NORMAL
- en: Capturing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we need to create an app for a mobile device that can capture both images
    and gyroscope signals simultaneously. Interestingly, these aren't readily available
    (at least on Android).
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a video and gyro stream, we'll look at how to use that data.
  prefs: []
  type: TYPE_NORMAL
- en: Create a standard Android application (I use Android Studio). We'll start by
    creating a blank application. The goal is to create a simple app that starts recording
    video and gyro signals on touching the screen. On touching again, the recording
    stops and a video file and a text file are saved on the phone. These two files
    can then be used by OpenCV to compute the best stabilization.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Code in this section is available in the GitHub repository for this book: [https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_7](https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_7)'
  prefs: []
  type: TYPE_NORMAL
- en: Recording video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll start by implementing a simple video recording utility. Create a new
    blank project in Android Studio (I named it GyroRecorder and named the activity
    Recorder). First, we start by adding permissions to our app. Open `AndroidManifest.xml`
    in your project and add these permissions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This simply lets our app access the camera and write to storage (the gyro file
    and the video). Next, open the main activity visual editor and add a TextureView
    and a Button element inside a vertical LinearLayout.
  prefs: []
  type: TYPE_NORMAL
- en: Change the names of these elements to `texturePreview` and `btnRecord` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Recording video](img/00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we start with some code. In the main activity class, add these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: These objects will be used to communicate with Android to indicate when to start
    recording.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Android Studio automatically adds imports to your code as you type. For example,
    the above piece results in the addition of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to initialize these objects. We do that in the `onCreate` event.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `onCreate` method already contains some methods (`super.onCreate`, `setContentView`,
    and so on; we will add a few lines after that). Now, we need to define what `onCaptureClick`
    does.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to explore `strings.xml`, refer to [Chapter 2](part0023_split_000.html#LTSU2-940925703e144daa867f510896bffb69
    "Chapter 2. Photographing Nature and Wildlife with an Automated Camera"), *Working
    with Camera Frames*, of the PacktPub book *Android Application Programming with
    OpenCV*.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use the internal `isRecording` variable to notify the media recorder
    and camera to start saving the stream. We need to create a new thread because
    initializing the media recorder and camera usually takes a few milliseconds. This
    lag would be noticeable on the UI if we did it in the main thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we''re done recording (the user taps the Stop button and we need to release
    the media recorder. This happens in the `releaseMediaRecorder` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now, we look at creating a new thread. Create this class in your main activity
    class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This creates an object of the type `AsyncTask`. Creating a new object of this
    class automatically creates a new thread and runs `doInBackground` in that thread.
    We want to prepare the media recorder in this thread. Preparing the media recorder
    involves identifying supported image sizes from the camera, finding the suitable
    height, setting the bitrate of the video and specifying the destination video
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In your main activity class, create a new method called `prepareVideoRecorder`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have supported video sizes, we need to find the optimal image size
    for the camera. This is done here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With the optimal size in hand, we can now set up the camera recorder settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we try to contact the camera hardware and set up these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, along with setting the camera parameters, we also specify a preview surface.
    The preview surface is used to display what the camera sees live.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the camera setup done, we can now set up the media recorder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This just sets whatever we already know about the video stream—we're simply
    passing information from what we've gathered into the media recorder.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This configuration does not record audio. In this project, we're not concerned
    with the audio signals. However, it should be straightforward to configure the
    media recorder to store audio as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'With everything in place, we try to start the media recorder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: And that's the end of the `prepareVideoRecorder` method. We've referenced a
    bunch of variables and functions that do not exist yet, so we'll define some of
    them now.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first is `getOptimalPreviewSize`. Define this method in your activity''s
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This function simply tries to match all possible image sizes against an expected
    aspect ratio. If it cannot find a close match, it returns the closest match (based
    on the expected height).
  prefs: []
  type: TYPE_NORMAL
- en: 'The second is `getOutputMediaFile`. This function uses the Android API to find
    an acceptable location to store our videos. Define this method in the main activity
    class as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: It finds the media storage location for pictures and appends a timestamp to
    the filename.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have almost everything to start recording videos. Two more method definitions
    and we'll have a working video recorder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `onPause` method is called whenever the user switches to another app. It's
    being a good citizen to release hardware dependencies when you're not using them.
  prefs: []
  type: TYPE_NORMAL
- en: Recording gyro signals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we only looked at recording video. For this project,
    we also need to record gyroscope signals. With Android, this is accomplished by
    using a sensor event listener. We''ll modify the main activity class for this.
    Add this `implements` clause:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to add a few new objects to our class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `SensorManager` object manages all sensors on the hardware. We''re only
    interested in the gyroscope, so we have a `Sensor` object for it. `PrintStream`
    writes a text file with the gyroscope signals. We now need to initialize these
    objects. We do that in the `onCreate` method. Modify the method so that it looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, we're fetching the gyroscope sensor and registering that this class should
    receive events (`registerListener`). We're also mentioning the frequency we want
    data to flow in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we initialize the `PrintStream` in the `prepareVideoRecorder` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This tries to open a new stream to a text file. We fetch the text file name
    using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This is almost the same code as `getOutputMediaFile`, except that it returns
    a `.csv` file (instead of an `.mp4`) in the same directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'One last thing and we''ll be recording gyroscope signals as well. Add this
    method to the main activity class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The idea is to store values returned by the sensor into the file as soon as
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: Android specifics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll look at some Android-specific tasks: one is rendering
    an overlay on top of the camera view and the second is reading media files on
    Android.'
  prefs: []
  type: TYPE_NORMAL
- en: The overlay is helpful for general information and debugging, and looks nice
    too! Think of it like the heads up display on a consumer camera.
  prefs: []
  type: TYPE_NORMAL
- en: The reading media files section is something we don't use in this chapter (we
    read media files using Python). However, if you decide to write an Android app
    that processes videos on the device itself, this section should get you started.
  prefs: []
  type: TYPE_NORMAL
- en: Threaded overlay
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the camera preview working, we want to render some additional
    information on top of it. We'll be drawing three things; first, a red circle to
    indicate whether recording is active, second, the current gyroscope values (angular
    velocity and estimated theta) just for information, and third, a safety rectangle.
    When stabilizing, we'll probably be cropping the image a bit. The rectangle will
    guide your video recording to stay within a relatively safe zone.
  prefs: []
  type: TYPE_NORMAL
- en: Along with this, we'll also be setting it up so that you can create buttons
    on this overlay. Simple touch events can be used to execute specific functions.
  prefs: []
  type: TYPE_NORMAL
- en: You don't need this section for the application to work, but it's a good idea
    to know how to render on top of an OpenCV camera view while recording.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start working on the overlay widget, let''s define a supporting class,
    `Point3`. Create a new class called `Point3` with three double attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We start by defining a new class, `CameraOverlayWidget`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We've subclassed this from `SurfaceView` to be able to render things on it.
    It also implements the gesture detector class so that we'll be able to monitor
    touch events on this widget.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We define a bunch of variables to be used by the class. Some of them are `Paint`
    objects – which are used by the `SurfaceView` to render things. We've created
    different paints for the safety rectangle, the red recording circle, and the text.
  prefs: []
  type: TYPE_NORMAL
- en: Next, there are variables that describe the current state of the recorder. These
    variables answer questions like, is it currently recording? What's the size of
    the video? What is the latest gyro reading? We'll use these state variables to
    render the appropriate overlay.
  prefs: []
  type: TYPE_NORMAL
- en: We also define some safety fractions – the safety rectangle will have a margin
    of 0.15 on each edge.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: And finally, we add a few event listeners – we'll use these to detect touches
    in specific areas of the overlay (we won't be using these though).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the constructor for this class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Here, we set up some basics when the object is initialized. We create the paint
    objects in `initializePaints`, create a new thread for rendering the overlay and
    also create a gesture detector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, paints describe the physical attributes of the things to draw.
    For example, `paintRecordCircle` is red and fills whatever shape we draw. Similarly,
    the record text shows up white with a text size of 20.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at the `RenderThread` class—the thing that does the actual drawing
    of the overlay. We start by defining the class itself and defining the `run` method.
    The `run` method is executed when the thread is spawned. On returning from this
    method, the thread stops.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now let's add the `renderOverlay` method to `RenderThread`. We start by getting
    a lock on the canvas and drawing a transparent color background. This clears anything
    that already exists on the overlay.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now, we draw the safety bounds of the camera view. While stabilizing the video,
    we'll inevitably have to crop certain parts of the image. The safe lines mark
    this boundary. In our case, we take a certain percentage of the view as safe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: If we're recording, we want to blink the red recording circle and the recording
    text. We do this by taking the current time and the start time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now we draw a button that says "Record" on it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: While recording the video, we will also display some useful information—the
    current angular velocity and estimated angle. You can verify if the algorithm
    is working as expected or not.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: And, with this, the render overlay method is complete!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This class can be used to spawn a new thread and this thread simply keeps the
    overlay updated. We've added a special logic for the recording circle so that
    it makes the red circle blink.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's look at some of the supporting functions in `CameraOverlayWidget`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Two simple set and unset methods enable or disable the red circle.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: If the size of the widget changes (we'll be setting it fullscreen on the preview
    pane), we should know about it and capture the size in these variables. This will
    affect the positioning of the various elements and the safety rectangle.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We also have a few set methods that let you change the values to be displayed
    on the overlay.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: There are other functions that can be used to modify the overlay being displayed.
    These functions set the gyroscope values.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at some Android-specific lifecycle events such as pause, resume,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: These two methods ensure we're not using processor cycles when the app isn't
    in the foreground. We simply stop the rendering thread if the app goes to a paused
    state and resume painting when it's back.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: When the surface is created, we set up the pixel format (we want it to be transparent,
    we make the surface of the type RGBA). Also, we should spawn a new thread to get
    the overlay rendering going.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we''re almost ready with our overlay display. One last thing remains—responding
    to touch events. Let''s do that now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'These functions do nothing but pass on events to the event listener, if there
    is any. We''re responding to the following events: `onTouchEvent`, `onDown`, `onShowPress`,
    `onFlight`, `onLongPress`, `onScroll`, `onSingleTapUp`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One final piece of code remains for the overlay class. We''ve used something
    called `OverlayEventListener` at certain places in the class but have not yet
    defined it. Here''s what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: With this defined, we will now be able to create event handlers for specific
    buttons being touched on the overlay (the calibrate and record buttons).
  prefs: []
  type: TYPE_NORMAL
- en: Reading media files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you've written the media file, you need a mechanism to read individual
    frames from the movie. We can use Android's Media Decoder to extract frames and
    convert them into OpenCV's native Mat data structure. We'll start by creating
    a new class called `SequentialFrameExtractor`.
  prefs: []
  type: TYPE_NORMAL
- en: Most of this section is based on Andy McFadden's tutorial on using the MediaCodec
    at bigflake.com.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned earlier, you don't need this class to get through this chapter's
    project. If you decide to write an Android app that reads media files, this class
    should get you started. Feel free to skip this if you like!
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the Android app only to record the video and gyro signals.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '`mFilename` is the name of the file that''s being read, it should only be set
    when an object of `SequentialFrameExtractor` is created. `CodecOutputSurface`
    is a construct borrowed from [http://bigflake.com](http://bigflake.com) that encapsulates
    logic to render a frame using OpenGL and fetches raw bytes for us to use. It is
    available on the website and also in the accompanying code. The next is `MediaCodec`—Android''s
    way of letting you access the decoding pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: '`FrameAvailableListener` is an interface we''ll create in just a moment. It
    allows us to respond whenever a frame becomes available.'
  prefs: []
  type: TYPE_NORMAL
- en: What is `TIMEOUT_USEC` and `decodeCount`?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We've created a constructor and a new `start` method. The `start` method is
    when the decoding begins and it will start firing the `onFrameAvailable` method
    as new frames become available.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Here, we loop over all the tracks available in the given file (audio, video,
    and so on) and identify a video track to work with. We're assuming this is a mono-video
    file, so we should be good to select the first video track that shows up.
  prefs: []
  type: TYPE_NORMAL
- en: With the track selected, we can now start the actual decoding process. Before
    that, we must set up a decoding surface and some buffers. The way MediaCodec works
    is that it keeps accumulating data into a buffer. Once it accumulates an entire
    frame, the data is passed onto a surface to be rendered.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'With the initial setup done, we now get into the decoding loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the input half of the media extraction. This loop reads the file and
    queues chunks for the decoder. As things get decoded, we need to route it to the
    places we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This completes the output half of the decode loop. Whenever a frame is complete,
    it converts the raw data into a Mat structure and creates a new `Frame` object.
    This is then passed to the `onFrameAvailable` method.
  prefs: []
  type: TYPE_NORMAL
- en: Once the decoding is complete, the media extractor is released and we're done!
  prefs: []
  type: TYPE_NORMAL
- en: 'The only thing left is to define what `FrameAvailableListener` is. We shall
    do that now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This is a common pattern in Java when defining such listeners. The listeners
    contain methods that are fired on specific events (in our case, when a frame is
    available, or when the processing of a frame is complete).
  prefs: []
  type: TYPE_NORMAL
- en: Calibration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the section that discusses the mathematical basis, we found several unknown
    camera parameters. These parameters need to be figured out so we can process each
    image and stabilize it. As with any calibration process, we need to use a predefined
    scene. Using this scene and a relative handshake, we will try to estimate the
    unknown parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The unknown parameters are:'
  prefs: []
  type: TYPE_NORMAL
- en: Focal length of the lens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delay between gyroscope and frame timestamps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias in the gyroscope
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duration of the rolling shutter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is often possible to detect the focal length of a phone camera (in terms
    of millimeters) using the platform API (`getFocalLength()` for Android). However,
    we're interested in the camera space focal length. This number is a product of
    the physical focal length and a conversion ratio that depends on the image resolution
    and the physical size of the camera sensor, which might differ across cameras.
    It is also possible to find the conversion ratio by trigonometry if the field
    of view (`getVerticalViewAngle()` and `getHorizontalViewAngle()` for Android)
    is known for a sensor and lens setup. However, we'll just leave it as an unknown
    and let the calibration find it for us.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you're interested in more information on this, refer to [Chapter 5](part0043_split_000.html#190862-940925703e144daa867f510896bffb69
    "Chapter 5. Generic Object Detection for Industrial Applications"), *Combining
    Image Tracking with 3D Rendering*, of PacktPub's *Android Application Programming
    with OpenCV*.
  prefs: []
  type: TYPE_NORMAL
- en: We need to estimate the delay between gyro and frame timestamps to improve the
    quality of the output on sharp turns. This also offsets any lag introduced by
    the phone when recording the video.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling shutter effects are visible at high speed and the estimated parameter
    tries to correct these.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to calculate these parameters with a short clip that's shaky.
    We use a feature detector from OpenCV to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During this phase, we'll be using Python. The SciPy library provides us with
    mathematical functions that we can use out of the box. It is possible to implement
    these on your own, but that would require a more in-depth explanation of how mathematical
    optimization works. Along with this, we'll use Matplotlib to generate graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Data structures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll set up three key data structures: first, the unknown parameters, second,
    something to read the gyro data file generated by the Android app, and third,
    a representation of the video being processed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first structure is to store the estimates from the calibration. It contains
    four values:'
  prefs: []
  type: TYPE_NORMAL
- en: An estimate of the focal length of the camera (in camera units, not physical
    units)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The delay between the gyroscope timestamps and the frame timestamps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gyroscope bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rolling shutter estimated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by creating a new file called `calibration.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Reading the gyroscope trace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we'll define a class to read in the `.csv` file generated by the Android
    app.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We initialize the class with two main variables: the file path to read, and
    a dictionary of angular velocities. This dictionary will store mappings between
    the timestamp and the angular velocity at that instant. We''ll eventually need
    to calculate actual angles from the angular velocity, but that will happen outside
    this class.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we add the `parse` method. This method will actually read the file and populate
    the Omega dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We validate that the first line of the csv file matches our expectation. If
    not, the csv file was probably not compatible and will error out over the next
    few lines.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Here, we initiate a loop over the entire file. The `strip` function removed
    any additional whitespace (tabs, spaces, newline characters, among others) that
    might be stored in the file.
  prefs: []
  type: TYPE_NORMAL
- en: After removing the whitespace, we split the string with commas (this is a comma-separated
    file!).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Information read from the file is plain strings so we convert that into the
    appropriate numeric type and store it in `self.omega`. We're now ready to parse
    the csv files and get started with numeric calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Before we do that, we'll define a few more useful functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The `get_timestamps` method on this class will return a sorted list of timestamps.
    Building on this, we also define a function called `get_signal`. The angular velocity
    is composed of three signals. These signals are packed together in `self.omega`.
    The `get_signal` function lets us extract a specific component of the signal.
  prefs: []
  type: TYPE_NORMAL
- en: For example, `get_signal(0)` returns the X component of angular velocity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: These utility functions return only the specific signal we're looking at. We'll
    be using these signals to smooth out individual signals, calculate the angle,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue we need to address is that the timestamps are discrete. For example,
    we might have angular velocities at timestamp N and the next reading might exist
    at N+500000 (remember, the timestamps are in nanoseconds). However, the video
    file might have a frame at N+250000\. We need a way to interpolate between two
    angular velocity readings.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use simple linear interpolation to estimate the angular velocity at any
    given moment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: This method takes in a timestamp and returns the estimated angular velocity.
    If the exact timestamp already exists, there is no estimation to do.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Here we're walking over the timestamps and finding the timestamp that is closest
    to the one requested.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Once we have the two closest timestamps (`i` and `i+1` in the list `sorted_timestamps`),
    we're ready to start linear interpolation. We calculate the estimated X, Y, and
    Z angular velocities and return these values.
  prefs: []
  type: TYPE_NORMAL
- en: This finishes our work on reading the gyroscope file!
  prefs: []
  type: TYPE_NORMAL
- en: The training video
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We'll also create a new class that lets us treat the entire video sequence as
    a single entity. We'll extract useful information from the video in a single pass
    and store it for future reference, making our code both faster and more memory
    efficient.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We initialize the class with some variables we'll be using throughout. Most
    of the variables are self-explanatory. `frameInfo` stores details about every
    frame—like the timestamp of a given frame and keypoints (useful for calibration).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We define a new method that will do all the heavy lifting for us. We start by
    creating the OpenCV video reading object (`VideoCapture`) and try to read a single
    frame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The `get` method on a `VideoCapture` object returns information about the video
    sequence. Zero (0) happens to be the constant for fetching the timestamp in milliseconds.
    We convert this into nanoseconds and print out a helpful message!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: If this is the first frame being read, we won't have a previous frame. We're
    also not interested in storing keypoints for the first frame. So we just move
    on to the next frame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: If you set the `skip_keypoints` parameter to `true`, it'll just store the timestamps
    of each frame. You might then use this parameter to read a video after you've
    already calibrated your device and already have the values of the various unknowns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: We convert the previous and the current frame into grayscale and extract some
    good features to track. We'll use these features and track them in the new frame.
    This gives us a visual estimate of how the orientation of the camera changed.
    We already have the gyroscope data for this; we just need to calibrate some unknowns.
    We achieve this by using the visual estimate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: If no corners were found in the old frame, that's not a good sign. Was it a
    very blurry frame? Were there no good features to track? So we simply skip processing
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we did find keypoints to track, we use optical flow to identify where they
    are in the new frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: This gives us the position of the corners in the new frame. We can then estimate
    the motion that happened between the previous frame and the current, and correlate
    it with the gyroscope data.
  prefs: []
  type: TYPE_NORMAL
- en: A common issue with `goodFeaturesToTrack` is that the features aren't robust.
    They often move around, losing the position they were tracking. To get around
    this, we add another test just to ensure such random outliers don't make it to
    the calibration phase. This is done with the help of RANSAC.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**RANSAC** stands for **Ran**dom **Sa**mple **C**onsensus. The key idea of
    RANSAC is that the given dataset contains a set of inliers that fit perfectly
    to a given model. It gives you the set of points that most closely satisfy a given
    constraint. In our case, these inliers would account for the points moving from
    one set of positions to another. It does not matter how numerous the outliers
    of the data set are.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV comes with a utility function to calculate the perspective transform
    between two frames. While the transform is being estimated, the function also
    tries to figure out which points are outliers. We'll hook into this functionality
    for our purposes too!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: We need at least four keypoints to calculate the perspective transform between
    two frames. If there aren't enough points, we just store whatever we have. We
    get a better result if there are more points and some are eliminated.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Once we have everything figured out, we just store it in the frame information
    list. And this marks the end of our method!
  prefs: []
  type: TYPE_NORMAL
- en: Handling rotations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's take a look at how we rotate frames to stabilize them.
  prefs: []
  type: TYPE_NORMAL
- en: Rotating an image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we get into how images can be rotated for our project, let''s look at
    rotating images in general. The goal is to produce images like the one below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rotating an image](img/00131.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Rotating an image in 2D is simple, there's only one axis. A 2D rotation can
    be achieved by using an affine transform.
  prefs: []
  type: TYPE_NORMAL
- en: '![Rotating an image](img/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In our project, we need to rotate images around all three axes. An affine transform
    is not sufficient to produce these, so we need to go towards a perspective transform.
    Also, rotations are linear transformations; this means we can split an arbitrary
    rotation into its component X, Y, and Z rotations and use that to compose the
    rotation.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we'll use the OpenCV Rodrigues function call to generate these
    transformation matrices. Let's start by writing a function that arbitrarily rotates
    an image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: This method accepts a source image that needs to be rotated, the three rotation
    angles, an optional translation, the focal length in pixels, and whether the angles
    are in radians or not. If the angles are in degrees, we need to convert them to
    radians. We also force convert these into `float`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll calculate the width and the height of the source image. These, along
    with the focal length, are used to transform the rotation matrix (which is in
    real world space) into image space.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we use the Rodrigues function to generate the rotation matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The Rodrigues function takes a vector (a list) that contains the three rotation
    angles and returns the rotation matrix. The matrix returned is a 3x3 matrix. We'll
    convert that into a 4x4 homogeneous matrix so that we can apply transformations
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's usually a good idea to keep dz equal to the focal length. This implies
    that the image was captured at just the right focal length and needs to be rotated
    about that point. You are free to change dz to other values, but usually setting
    it equal to F gives good results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now apply a simple translation to the matrix. The translation matrix is
    easily evaluated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Until now, all transformations have happened in world space. We need to change
    these into image space. This is accomplished by the simple pinhole model of a
    camera.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Combining these transforms is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: This matrix can now be used in OpenCV's `warpPerspective` method to rotate the
    source image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: The output of this isn't exactly what you want though, the images are rotated
    about (0, 0) in the image. We need to rotate the image about the center. To achieve
    this, we need to insert an additional translation matrix right *before* the rotations
    happen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we insert the matrix A1 at the very beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Now images should rotate around the center; this is exactly what we want and
    is a self-contained method that we can use to rotate images arbitrarily in 3D
    space using OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Accumulated rotations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rotating an image with a single rotation vector is quite straightforward. In
    this section, we'll extend that method so it is better suited for our project.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two data sources active when recording a video: the image capture and
    the gyroscope trace. These are captured at different rates—images every few milliseconds
    and gyroscope signals every few microseconds. To calculate the exact rotation
    required to stabilize an image, we need to accumulate the rotation of dozens of
    gyroscope signals. This means that the rotation matrix needs to have information
    on several different gyroscope data samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, the gyroscope and image sensors are not in sync; we will need to use linear
    interpolation on the gyroscope signals to bring them in sync.
  prefs: []
  type: TYPE_NORMAL
- en: Let's write a function that returns the transformation matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes a lot of parameters. Let''s go over each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '`w`, `h`: We need to know the size of the image to convert it from world space
    to image space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'theta_*: Currently, we have access to angular velocity. From there, we can
    evaluate actual angles and that is what this function accepts as parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Timestamps: The time each sample was taken.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prev`, `current`: Accumulate rotations between these timestamps. This will
    usually provide the timestamp of the previous frame and the current frame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`f`, `gyro_delay`, `gyro_drift`, and `shutter_duration` are used to improve
    the estimate of the rotation matrix. The last three of these are optional (and
    they get set to zero if you don''t pass them).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the previous section, we know that we need to start by translating (or
    we'll get rotations about (0, 0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: We'll use the "transform" matrix to accumulate rotations across multiple gyroscope
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we offset the timestamps by using `gyro_delay`. This is just adding (or
    subtracting, based on the sign of its value) to the timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: If the updated `prev` and `current` values exist in the timestamps (meaning
    we have values captured from the sensor at that time instant) – great! No need
    to interpolate. Otherwise, we use the function `fetch_closest_trio` to interpolate
    the signals to the given timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: 'This helper function returns three things:'
  prefs: []
  type: TYPE_NORMAL
- en: The interpolated rotation for the requested timestamp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The closest timestamp in the sensor data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The timestamp right after it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use `start_timestamp` and `end_timestamp` for iterating now.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: We iterate over each timestamp in the physical data. We add the gyroscope delay
    and use that to fetch the closest (interpolated) signals. Once that's done, we
    add the gyroscope drift per component. This is just a constant that should be
    added to compensate for errors in the gyroscope.
  prefs: []
  type: TYPE_NORMAL
- en: Using these rotation angles, we now calculate the rotation matrix, as in the
    previous section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: This piece of code is almost the same as that in the previous section. There
    are a few key differences though. Firstly, we're providing negative values to
    Rodrigues. This is to negate the effect of motion. Secondly, the X and Y values
    are swapped. (`gyro_drifted[1]` comes first, followed by `gyro_drifted[0]`). This
    is required because the axes of the gyroscope and the ones used by these matrices
    are different.
  prefs: []
  type: TYPE_NORMAL
- en: 'This completes the iteration over the gyroscope samples between the specified
    timestamps. To complete this, we need to translate in the Z direction just like
    in the previous section. Since this can be hardcoded, let''s do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: We also need to use the camera matrix to convert from world space to image space.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: We first translate in the Z direction and then convert to image space. This
    section essentially lets you rotate frames of your video with the gyroscope parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The calibration class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With our data structures ready, we're in a good place to start the key piece
    of this project. The calibration builds on all the previously mentioned classes.
  prefs: []
  type: TYPE_NORMAL
- en: As always, we'll create a new class which encapsulates all the calibration-related
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The object requires two things: the video file and the gyroscope data file.
    These get stored in the object.'
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping directly into the calibration method, let's create some utility
    functions that will be helpful when calibrating.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: This method generates a Gaussian kernel of a given size. We'll use this to smooth
    out the angular velocity signals in a bit.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: This function does the actual smoothing of a signal. Given an input signal,
    it generates the Gaussian kernel and convolves it with the input signal.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutions are a mathematical tool to produce new functions. You can think
    of the gyroscope signal as a function; you give it a timestamp and it returns
    a value. To smooth it out, we need to combine it with another function. This function,
    called the Gaussian function, is a smooth bell curve. Both these functions have
    different time ranges on which they operate (the gyroscope function might return
    values between a time of 0 seconds and 50 seconds while the Gaussian function
    might just work for a time of 0 seconds to 5 seconds). Convolving these two functions
    produces a third function that behaves a bit like both, thereby effectively smoothing
    out the minor variations in the gyroscope signal.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we write a function that calculates an error score giving two sets of
    points. This will be a building block in estimating how good the calibration has
    been.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'This error score is straightforward: you have two lists of points and you calculate
    the distance between the corresponding points on the two lists and sum it up.
    A higher error score means the points on the two lists don''t correspond perfectly.'
  prefs: []
  type: TYPE_NORMAL
- en: This method, however, only gives us the error on a single frame. We're concerned
    about errors across the whole video. We therefore write another method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: We pass in the video object and all the details we have estimated (the theta,
    timestamps, focal length, gyroscope delay, and so on). With these details, we
    try to do the video stabilization and see what differences exists between the
    visually tracked keypoints and the gyroscope-based transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we''re calculating the error across the whole video, we need to iterate
    over each frame. If the frame''s information does not have any keypoints in it,
    we simply ignore the frame. If the frame does have keypoints, here''s what we
    do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: The `getAccumulatedRotation` function is something we'll write soon. The key
    idea of the function is to return a transformation matrix for the given theta
    (the angles we need to rotate to stabilize the video). We can apply this transform
    to `old_corners` and compare it to `new_corners`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since `new_corners` was obtained visually, it is the ground truth. We want
    `getAccumulatedRotation` to return a transformation that matches the visual ground
    truth perfectly. This means the error between `new_corners` and the transformed
    `old_corners` should be minimal. This is where `calcErrorScore` helps us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re ready to calculate the error across the whole video! Now let''s move
    to the calibration function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: The first step is to smooth out the noise in the angular velocity signals. This
    is the desired signal with smooth motion.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: We'll be writing the `gaussian_filter` method soon; for now, let's just keep
    in mind that it returns a smoothed out signal.
  prefs: []
  type: TYPE_NORMAL
- en: '![The calibration class](img/00133.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Next, we calculate the difference between the physical signal and the desired
    signal. We need to do this separately for each component.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: We also need to calculate the delta between the timestamps. We'll be using this
    for integration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Next, we integrate the angular velocities to get actual angles. Integration
    introduces errors into our equations but that's okay. It is good enough for our
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: And that's it. We have calculated the amount of theta that will stabilize the
    image! However, this is purely from the gyroscope's view. We still need to calculate
    the unknowns so that we can use these thetas to stabilize the image.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we initialize some of the unknowns as variables with an arbitrary
    initial value (0 in most cases). Also, we load the video and process the keypoint
    information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: Now, we use SciPy's optimize method to minimize the error. To do this, we must
    first convert these unknowns into a Numpy array.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we''ve not yet incorporated fixing the rolling shutter, we ignore that
    in the parameters list. Next, we call the actual optimization function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing this function takes a few seconds, but it produces the values of
    the unknowns for us. We can then extract these from the result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: With this, we're done with calibration! All we need to do is return all the
    relevant calculations we've done just now.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: And that's a wrap!
  prefs: []
  type: TYPE_NORMAL
- en: Undistorting images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we calculated all the unknowns in our equations. Now,
    we can go ahead with fixing the shaky video.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start off by creating a new method called `stabilize_video`. This method
    will take a video file and a corresponding csv file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: We create an object of the calibration class we just defined and pass it the
    required information. Now, we just need to call the calibrate function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: This method call may take a while to execute, but we need to run this only once
    for every device. Once calculated, we can store these values in a text file and
    read them from there.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have estimated all the unknowns, we start by reading the video file
    for each frame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: Now we start iterating over each frame and correcting the rotations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: Next, we fetch the timestamp from the video stream and use that to fetch the
    closest rotation sample.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: The `VideoCapture` class returns timestamps in milliseconds. We convert that
    into nanoseconds to keep consistent units.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: With these pieces, we now fetch the accumulated rotation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we write the transformed frame into a file and move on to the next frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: And this finishes our simple function to negate the shakiness of the device.
    Once we have all the images, we can combine them into a single video with `ffmpeg`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: Testing calibration results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The effectiveness of the calibration depends on how accurately it can replicate
    motion on the video. For any frame, we have matching keypoints in the previous
    and current frames. This gives a sense of the general motion of the scene.
  prefs: []
  type: TYPE_NORMAL
- en: Using the estimated parameters, if we are able to use previous frames' keypoints
    to generate the current frames' keypoints, we can assume the calibration has been
    successful.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling shutter compensation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, our video is stable, however, when objects in the scene are moving
    quickly, the rolling shutter effects become more pronounced.
  prefs: []
  type: TYPE_NORMAL
- en: To fix this, we'll need to do a few things. First, incorporate the rolling shutter
    speed into our calibration code. Second, when warping images, we need to unwarp
    the rolling shutter as well.
  prefs: []
  type: TYPE_NORMAL
- en: Calibrating the rolling shutter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start calibrating the rolling shutter duration, we need to tweak the error
    function to incorporate another term. Let''s start by looking at the `calcErrorAcrossVideo`
    method. The part we''re interested in is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: Also, we'll need to add logic to transform a corner based on its location—a
    corner in the upper part of the image is transformed differently from a corner
    in the lower half.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have had a single transformation matrix and that was usually sufficient.
    However, now, we need to have multiple transformation matrices, one for each row.
    We could choose to do this for every row of pixels, however that is a bit excessive.
    We only need transforms for rows that contain a corner we're tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by replacing the two lines mentioned above. We need to loop over
    each corner individually and warp it. Let''s do this with a simple `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: Here, we extract the x and y coordinates of the old corner and try to estimate
    the timestamp when this particular pixel was captured. Here, I'm assuming the
    rolling shutter is in the vertical direction, from the top of the frame to the
    bottom.
  prefs: []
  type: TYPE_NORMAL
- en: We use the current estimate of the rolling shutter duration and estimate subtract
    and add time based on the row the corner belongs to. It should be simple to adapt
    this for a horizontal rolling shutter as well. Instead of using `y` and `frameHeight`,
    you would have to use `x` and `frameWidth`—the calculation would stay the same.
    For now, we'll just assume this is going to be a vertical rolling shutter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the estimated timestamp of capture, we can get the rotation
    matrix for that instant (remember, the gyroscope produces a higher resolution
    data than the camera sensor).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: This line is almost the same as the original we had; the only difference is
    that we've replaced `current_timestamp` with `pt_timestamp`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to transform this point based on the rolling shutter duration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: After transforming, we simply append it to the `transformed_corners` list (just
    like we did earlier).
  prefs: []
  type: TYPE_NORMAL
- en: With this, we're done with the calibration part. Now, we move onto warping images.
  prefs: []
  type: TYPE_NORMAL
- en: Warping with grid points
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start by writing a function that will do the warping for us. This function
    takes in these inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The original image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bunch of points that should ideally line up in a perfect grid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the point list gives us the number of rows and columns to expect
    and the function returns a perfectly aligned image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Warping with grid points](img/00134.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s define the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned earlier, this takes in an image and the list of control points.
    We store the size of the image for future reference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: The size we stored earlier will most likely have three channels in it. So we
    create a new variable called `mapsize`; this stores the size of the image but
    only one channel. We'll use this later for creating matrices for use by the remap
    function in OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: We also create a blank image of the same size as the original. Next, we look
    at calculating the number of rows and columns in the grid.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: We'll use the variables in some loops soon.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: These lists store all the source (distorted) points and the destination (perfectly
    aligned) points. We'll have to use `distorted_grid` to populate `pt_src_all`.
    We'll procedurally generate the destination based on the number of rows and columns
    in the input data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: The distorted grid should be a list of lists. Each row is a list that contains
    its points.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we generate the procedural destination points using the `quads_per_*` variables
    we calculated earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: This generates the ideal grid based on the number of points we passed to the
    method.
  prefs: []
  type: TYPE_NORMAL
- en: We then have all the required information to calculate the interpolation between
    the source and destination grids. We'll be using `scipy` to calculate the interpolation
    for us. We then pass this to OpenCV's remap method and that applies it to an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, `scipy` needs a representation of the expected output grid so
    we need to specify a dense grid that contains all the pixels of the image. This
    is done with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: Once we have the base grid defined, we can use Scipy's `interpolate` module
    to calculate the mapping for us.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '`g_out` contains both the `x` and `y` coordinates of the remapping; we need
    to split this into individual components for OpenCV''s `remap` method to work.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: These matrices are exactly what remap expects and we can now simply run it with
    the appropriate parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: And that completes our method. We can use this in our stabilization code and
    fix the rolling shutter as well.
  prefs: []
  type: TYPE_NORMAL
- en: Unwarping with calibration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we discuss how to warp images given a mesh for stabilizing the video.
    We split each frame into a 10x10 mesh. We warp the mesh and that results in warping
    the image (like control points). Using this approach, we should get good results
    and decent performance as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual unwarp happens in the `accumulateRotation` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, there''s a single perspective transform happening. Now, instead, we have
    to do a different transform for each of the 10x10 control points and use the `meshwarp`
    method to fix the rolling shutter. So replace the `transform =` line with the
    contents below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now generated the original grid in the `pts` list. Now, we need to
    generate the transformed coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'If a shutter duration is passed, we generate the timestamp at which this specific
    pixel was recorded. Now we can transform (`pixel_x`, `pixel_y`) based on the shutter
    rotation and append that to `current_row_transformed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'This completes the grid for `meshwarp`. Now all we need to do is generate the
    warped image. This is simple since we already have the required method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: And this completes our transformation. We now have rolling shutter incorporated
    into our undistortion as well.
  prefs: []
  type: TYPE_NORMAL
- en: What's next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What we have right now is a very barebones implementation of video stabilization.
    There are a few more things you can add to it to make it more robust, more automated
    and the output more pleasing to the eye. Here are a few things to get you started.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying gyroscope axes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we've hard-coded the axes of the gyroscope. This might not
    be the case for all mobile phone manufacturers. Using a similar calibration technique,
    you should be able to find an axes configuration that minimizes errors across
    the video.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the rolling shutter direction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've hard-coded the direction of the rolling shutter. Using specific techniques
    (like blinking an LED really fast at the camera), it is possible to estimate the
    direction of the rolling shutter and incorporate that into the calibration code.
    Certain camera sensors don't have the rolling shutter artifacts at all. This test
    can also identify if such a sensor is being used.
  prefs: []
  type: TYPE_NORMAL
- en: Smoother timelapses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we've stabilized the video, we can speed up (or slow down) the video
    much better. There are commercial packages that do similar tasks – now your OpenCV
    code can do it too!
  prefs: []
  type: TYPE_NORMAL
- en: Repository of calibration parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will have to calibrate every new device type you come across. If you move
    from one device type (say, a Samsung S5 to an iPhone 6), you'll have to run a
    calibration for this combination of lens and sensor. However, moving between different
    devices of the same kind does not require a re-calibration (such as moving from
    one iPhone 6 to another). If you're able to collect enough calibration results,
    your code can run perfectly on pretty much any device.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You could also figure out a fallback mechanism if the repository does not have
    the required parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating translations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Currently, we're only using rotations. This means that if you shake the camera
    in a single plane, the algorithm won't do much. By using inputs from the accelerometer
    and using the translation of keypoints, it should be possible to compensate for
    translation as well. This should produce higher quality video.
  prefs: []
  type: TYPE_NORMAL
- en: Additional tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here are some additional things to keep in mind while working with Python and
    computer vision in general. They should help speed up your work and keep you safe
    from unexpected crashes!
  prefs: []
  type: TYPE_NORMAL
- en: Use the Python pickle module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python gives us a neat way to store Python objects as files on disk. In our
    project, we have the gyroscope calibration class. This class stores information
    like the video dimensions and keypoints across different frames. Calculating this
    information from scratch everytime you want to test your code is cumbersome. You
    can easily pickle this object into a file and read back the data when required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is some sample code for pickling the video object in our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'To read the object back into the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: This saves time when iterating on code and verifying if something works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Write out single images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When working with videos, you most often end up using something like the VideoWriter
    class from OpenCV. You feed it frames and it writes out a video file. While this
    is a perfectly valid way to get things done, you have more control if you write
    out individual frames to disk and then use a video encoder to combine the images
    into a video stream.
  prefs: []
  type: TYPE_NORMAL
- en: A simple way to combine multiple images is to use `ffmpeg`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: Testing without the delta
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the project, we're trying to stabilize video – thus we're calculating the
    delta between the actual gyroscope signal and a smoothed out version of the signal.
  prefs: []
  type: TYPE_NORMAL
- en: You might want to try it out with just the actual gyroscope signal; this will
    totally keep the video still. This might be useful for situations where you want
    the camera to appear completely still.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ve covered quite a bit: talking to your gyroscope, using
    that to find unknowns, negating the effects of camera shake and rolling shutter.'
  prefs: []
  type: TYPE_NORMAL
- en: We started out by creating an Android app that uses background tasks to initiate
    recording media into a video file. While doing this, we figured out how to extend
    OpenCV's camera view class to incorporate custom UI and responsiveness. With this,
    you can now create very sophisticated UIs with an OpenCV backend. Along with this,
    we also captured the gyroscope trace and stored it in a single file. The sampling
    rate of the gyroscope and the media were different – however, we did not care
    about it at this stage. We'll let the app store a higher density of gyroscope
    traces (every few hundred microseconds versus every few dozen milliseconds for
    the media).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we had the media/csv pair, we used Python and the numpy/scipy libraries
    to calibrate the system. We had three unknowns initially: the pixel focal length
    of the camera, the gyroscope delay (the offset between the gyroscope recordings
    and the media timestamps) and the gyroscope drift.'
  prefs: []
  type: TYPE_NORMAL
- en: We devised an error function that takes the expected keypoints and the transformed
    keypoints and returns the amount of errors. We then used this to calculate errors
    across the whole video. Using this, and Scipy's optimize method, we were able
    to find the values for these unknowns. This calibration needs to happen only once
    for each device type.
  prefs: []
  type: TYPE_NORMAL
- en: Then we added another parameter to our calibration—the rolling shutter. Estimating
    the value of this unknown was similar to the previous three, however, incorporating
    the undistortion was a bit tricky. We had to create a new method called `meshwarp`
    that takes a distorted grid. This method rectifies the grid and removes artifacts
    due to the rolling shutter. We worked on a vertical rolling shutter, however it
    should be easy to convert it to a horizontal rolling shutter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We touched upon a lot of different areas: sensors, calibration, and geometric
    distortions. I hope this chapter gives you an insight into designing your own
    pipelines for working with images.'
  prefs: []
  type: TYPE_NORMAL
