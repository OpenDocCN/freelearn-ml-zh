<html><head></head><body>
		<div id="_idContainer344">
			<h1 id="_idParaDest-131"><a id="_idTextAnchor132"/><span class="koboSpan" id="kobo.1.1">Chapter 9: Natural Language Processing</span></h1>
			<p><span class="koboSpan" id="kobo.2.1">In the previous chapter, we discussed using deep learning to not only address structured data in the form of tables but also sequence-based data where the order of the elements matters. </span><span class="koboSpan" id="kobo.2.2">In this chapter, we will be discussing another form of sequence-based data – text, within a field known as </span><strong class="bold"><span class="koboSpan" id="kobo.3.1">Natural Language Processing</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.5.1">NLP</span></strong><span class="koboSpan" id="kobo.6.1">). </span><span class="koboSpan" id="kobo.6.2">We can define NLP as a subset of artificial intelligence that overlaps with both the realms of machine learning and deep learning, specifically when it comes to interactions between the areas of linguistics and computer science.</span></p>
			<p><span class="koboSpan" id="kobo.7.1">There are many well-known and well-documented applications and success stories of using NLP for various tasks. </span><span class="koboSpan" id="kobo.7.2">Products ranging from spam detectors all the way to document analyzers involve NLP to some extent. </span><span class="koboSpan" id="kobo.7.3">Throughout this chapter, we will explore several different areas and applications involving NLP. </span></p>
			<p><span class="koboSpan" id="kobo.8.1">As we have observed with many other areas of data science we have explored thus far, the field of NLP is just as vast and sparse, with endless tools and applications that a single book would never be able to fully cover. </span><span class="koboSpan" id="kobo.8.2">Throughout this chapter, we will aim to highlight as many of the most common and useful applications you will likely encounter as we can. </span></p>
			<p><span class="koboSpan" id="kobo.9.1">Throughout this chapter, we will explore many of the popular areas relating to NLP from the perspective of both structured and unstructured data. </span><span class="koboSpan" id="kobo.9.2">We will explore several topics, such as entity recognition, sentence analysis, topic modeling, sentiment analysis, and natural language search engines. </span></p>
			<p><span class="koboSpan" id="kobo.10.1">In this chapter, we will cover the following topics:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.11.1">Introduction to NLP</span></li>
				<li><span class="koboSpan" id="kobo.12.1">Getting started with NLP using NLTK and SciPy</span></li>
				<li><span class="koboSpan" id="kobo.13.1">Working with structured data</span></li>
				<li><span class="koboSpan" id="kobo.14.1">Tutorial – abstract clustering and topic modeling </span></li>
				<li><span class="koboSpan" id="kobo.15.1">Working with unstructured data</span></li>
				<li><span class="koboSpan" id="kobo.16.1">Tutorial – developing a scientific data search engine using transformers</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.17.1">With these objectives in mind, let's go ahead and get started.</span></p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.18.1">Introduction to NLP</span></h1>
			<p><span class="koboSpan" id="kobo.19.1">Within the scope of </span><strong class="bold"><span class="koboSpan" id="kobo.20.1">biotechnology</span></strong><span class="koboSpan" id="kobo.21.1">, we often turn to </span><strong class="bold"><span class="koboSpan" id="kobo.22.1">NLP</span></strong><span class="koboSpan" id="kobo.23.1"> for numerous reasons, which generally involve the need</span><a id="_idIndexMarker937"/><span class="koboSpan" id="kobo.24.1"> to organize data and develop models to find answers to scientific questions. </span><span class="koboSpan" id="kobo.24.2">As opposed to the many other areas we have investigated so far, NLP is unique in the sense that we focus on one type of data</span><a id="_idIndexMarker938"/><span class="koboSpan" id="kobo.25.1"> at hand: text data. </span><span class="koboSpan" id="kobo.25.2">When we think of text data within the realm of NLP, we can divide things into two general categories: </span><strong class="bold"><span class="koboSpan" id="kobo.26.1">structured data</span></strong><span class="koboSpan" id="kobo.27.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.28.1">unstructured data</span></strong><span class="koboSpan" id="kobo.29.1">. </span><span class="koboSpan" id="kobo.29.2">We can think of structured data as text fields living within tables and databases in which items are organized, labeled, and linked together</span><a id="_idIndexMarker939"/><span class="koboSpan" id="kobo.30.1"> for easier retrieval, such as a SQL or DynamoDB database. </span><span class="koboSpan" id="kobo.30.2">On the other hand, we have what is known as unstructured data such as documents, PDFs, and images, which can contain static content that is neither searchable nor easily accessible. </span><span class="koboSpan" id="kobo.30.3">An example of this can be seen in the following diagram:</span></p>
			<div>
				<div id="_idContainer312" class="IMG---Figure">
					<span class="koboSpan" id="kobo.31.1"><img src="image/B17761_09_001.jpg" alt="Figure 9.1 – Structured and unstructured data in NLP "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.32.1">Figure 9.1 – Structured and unstructured data in NLP</span></p>
			<p><span class="koboSpan" id="kobo.33.1">Often, we wish to use documents or text-based data for various purposes, such as the following:</span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.34.1">Generating insights</span></strong><span class="koboSpan" id="kobo.35.1">: Looking for trends, keywords, or key</span><a id="_idIndexMarker940"/><span class="koboSpan" id="kobo.36.1"> phrases</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.37.1">Classification</span></strong><span class="koboSpan" id="kobo.38.1">: Automatically labeling</span><a id="_idIndexMarker941"/><span class="koboSpan" id="kobo.39.1"> documents for various purposes</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.40.1">Clustering</span></strong><span class="koboSpan" id="kobo.41.1">: Grouping documents</span><a id="_idIndexMarker942"/><span class="koboSpan" id="kobo.42.1"> together based on features and characteristics</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.43.1">Searching</span></strong><span class="koboSpan" id="kobo.44.1">: Quickly finding important knowledge</span><a id="_idIndexMarker943"/><span class="koboSpan" id="kobo.45.1"> in historical documents</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.46.1">In each of these examples, we would need to take our data from unstructured data and move it toward a structured</span><a id="_idIndexMarker944"/><span class="koboSpan" id="kobo.47.1"> state to implement these tasks. </span><span class="koboSpan" id="kobo.47.2">In this chapter, we will look at some of the most important and useful concepts and tools you should know about concerning the NLP space.</span></p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor134"/><span class="koboSpan" id="kobo.48.1">Getting started with NLP using NLTK and SciPy</span></h1>
			<p><span class="koboSpan" id="kobo.49.1">There are many different NLP libraries</span><a id="_idIndexMarker945"/><span class="koboSpan" id="kobo.50.1"> available in the Python language that allow users to accomplish a variety of different</span><a id="_idIndexMarker946"/><span class="koboSpan" id="kobo.51.1"> tasks for analyzing da</span><a id="_idIndexMarker947"/><span class="koboSpan" id="kobo.52.1">ta, generating</span><a id="_idIndexMarker948"/><span class="koboSpan" id="kobo.53.1"> insights, or preparing predictive models. </span><span class="koboSpan" id="kobo.53.2">To begin our journey in the realm of NLP, we will take advantage of two popular libraries known as </span><strong class="bold"><span class="koboSpan" id="kobo.54.1">NLTK</span></strong><span class="koboSpan" id="kobo.55.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.56.1">SciPy</span></strong><span class="koboSpan" id="kobo.57.1">. </span><span class="koboSpan" id="kobo.57.2">We will begin by importing these two libraries:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.58.1">import nltk</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.59.1">import scipy</span></p>
			<p><span class="koboSpan" id="kobo.60.1">Often, we will want to parse and analyze raw pieces of text for particular purposes. </span><span class="koboSpan" id="kobo.60.2">Take, for example, the following paragraph regarding the field of biotechnology:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.61.1">paragraph = """Biotechnology is a broad area of biology, involving the use of living systems and organisms to develop or make products. </span><span class="koboSpan" id="kobo.61.2">Depending on the tools and applications, it often overlaps with related scientific fields. </span><span class="koboSpan" id="kobo.61.3">In the late 20th and early 21st centuries, biotechnology has expanded to include new and diverse sciences, such as genomics, recombinant gene techniques, applied immunology, and development of pharmaceutical therapies and diagnostic tests. </span><span class="koboSpan" id="kobo.61.4">The term biotechnology was first used by Karl Ereky in 1919, meaning the production of products from raw materials with the aid of living organisms."""</span></p>
			<p><span class="koboSpan" id="kobo.62.1">Here, there is a single</span><a id="_idIndexMarker949"/><span class="koboSpan" id="kobo.63.1"> string that's been</span><a id="_idIndexMarker950"/><span class="koboSpan" id="kobo.64.1"> assigned to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.65.1">paragraph</span></strong><span class="koboSpan" id="kobo.66.1"> variable. </span><span class="koboSpan" id="kobo.66.2">Paragraphs</span><a id="_idIndexMarker951"/><span class="koboSpan" id="kobo.67.1"> can be separated into sentences</span><a id="_idIndexMarker952"/><span class="koboSpan" id="kobo.68.1"> using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.69.1">sent_tokenize()</span></strong><span class="koboSpan" id="kobo.70.1"> function, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.71.1">fr</span><strong class="source-inline"><span class="koboSpan" id="kobo.72.1">om nltk.tokenize import sent_tokenize</span></strong></p>
			<p class="source-code"><strong class="source-inline"><span class="koboSpan" id="kobo.73.1">nltk.download('popular')</span></strong></p>
			<p class="source-code"><strong class="source-inline"><span class="koboSpan" id="kobo.74.1">sentences</span></strong><span class="koboSpan" id="kobo.75.1"> = sent_tokenize(paragraph)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.76.1">print(sentences)</span></p>
			<p><span class="koboSpan" id="kobo.77.1">Upon printing these sentences, we will get the following output:</span></p>
			<div>
				<div id="_idContainer313" class="IMG---Figure">
					<span class="koboSpan" id="kobo.78.1"><img src="image/B17761_09_002.jpg" alt="Figure 9.2 – A sample list of sentences that were split from the initial paragraph "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.79.1">Figure 9.2 – A sample list of sentences that were split from the initial paragraph</span></p>
			<p><span class="koboSpan" id="kobo.80.1">Similarly, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.81.1">word_tokenize()</span></strong><span class="koboSpan" id="kobo.82.1"> function to separate the paragraph into individual words:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.83.1">from nltk.tokenize import word_tokenize</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.84.1">words = word_tokenize(sentences[0])</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.85.1">print(words)</span></p>
			<p><span class="koboSpan" id="kobo.86.1">Upon printing the results, you will get a list similar to the one shown in the following screenshot:</span></p>
			<div>
				<div id="_idContainer314" class="IMG---Figure">
					<span class="koboSpan" id="kobo.87.1"><img src="image/B17761_09_003.jpg" alt="Figure 9.3 – A sample list of words that were split from the first sentence "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.88.1">Figure 9.3 – A sample list of words that were split from the first sentence</span></p>
			<p><span class="koboSpan" id="kobo.89.1">Often, we will want to know the part of speech for a given word in a sentence. </span><span class="koboSpan" id="kobo.89.2">We can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.90.1">pos_tag()</span></strong><span class="koboSpan" id="kobo.91.1"> function on a given string to do this – in this case, the first sentence of the paragraph:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.92.1">tokens = word_tokenize(sentences[0])</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.93.1">tags = nltk.pos_tag(tokens)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.94.1">print(tags)</span></p>
			<p><span class="koboSpan" id="kobo.95.1">Upon printing the tags, we get</span><a id="_idIndexMarker953"/><span class="koboSpan" id="kobo.96.1"> a list of sets where the word or token</span><a id="_idIndexMarker954"/><span class="koboSpan" id="kobo.97.1"> is listed on the left and the associated</span><a id="_idIndexMarker955"/><span class="koboSpan" id="kobo.98.1"> part of speech is on the</span><a id="_idIndexMarker956"/><span class="koboSpan" id="kobo.99.1"> right. </span><span class="koboSpan" id="kobo.99.2">For example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.100.1">Biotechnology</span></strong><span class="koboSpan" id="kobo.101.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.102.1">biology</span></strong><span class="koboSpan" id="kobo.103.1"> were tagged as proper nouns, whereas </span><strong class="source-inline"><span class="koboSpan" id="kobo.104.1">involving</span></strong><span class="koboSpan" id="kobo.105.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.106.1">develop</span></strong><span class="koboSpan" id="kobo.107.1"> were tagged as verbs. </span><span class="koboSpan" id="kobo.107.2">We can see an example of these results in the following screenshot:</span></p>
			<div>
				<div id="_idContainer315" class="IMG---Figure">
					<span class="koboSpan" id="kobo.108.1"><img src="image/B17761_09_004.jpg" alt="Figure 9.4 – Results of the part-of-speech tagging method "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.109.1">Figure 9.4 – Results of the part-of-speech tagging method</span></p>
			<p><span class="koboSpan" id="kobo.110.1">In addition to understanding the parts of speech, we often want to know the frequency of the given words in a particular</span><a id="_idIndexMarker957"/><span class="koboSpan" id="kobo.111.1"> string of text. </span><span class="koboSpan" id="kobo.111.2">For this, we could either tokenize</span><a id="_idIndexMarker958"/><span class="koboSpan" id="kobo.112.1"> the paragraph into words, group</span><a id="_idIndexMarker959"/><span class="koboSpan" id="kobo.113.1"> by word, count the instances</span><a id="_idIndexMarker960"/><span class="koboSpan" id="kobo.114.1"> and plot them, or use NLTK's built-in functionality:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.115.1">freqdist = nltk.FreqDist(word_tokenize(paragraph))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.116.1">import matplotlib.pyplot as plt</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.117.1">import seaborn as sns</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.118.1">plt.figure(figsize=(10,3))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.119.1">plt.xlabel("Samples", fontsize=20)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.120.1">plt.xticks(fontsize=14)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.121.1">plt.ylabel("Counts", fontsize=20)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.122.1">plt.yticks(fontsize=14)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.123.1">sns.set_style("darkgrid")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.124.1">freqdist.plot(30,cumulative=False) </span></p>
			<p><span class="koboSpan" id="kobo.125.1">By doing this, you will receive the following output:</span></p>
			<div>
				<div id="_idContainer316" class="IMG---Figure">
					<span class="koboSpan" id="kobo.126.1"><img src="image/B17761_09_005.jpg" alt="Figure 9.5 – Results of calculating the frequency (with stop words) "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.127.1">Figure 9.5 – Results of calculating the frequency (with stop words)</span></p>
			<p><span class="koboSpan" id="kobo.128.1">Here, we can see that the most</span><a id="_idIndexMarker961"/><span class="koboSpan" id="kobo.129.1"> common elements are commas, periods, and other irrelevant words. </span><span class="koboSpan" id="kobo.129.2">Words that are irrelevant</span><a id="_idIndexMarker962"/><span class="koboSpan" id="kobo.130.1"> to any given analysis are known as </span><strong class="bold"><span class="koboSpan" id="kobo.131.1">stop words</span></strong><span class="koboSpan" id="kobo.132.1"> and are generally removed as part</span><a id="_idIndexMarker963"/><span class="koboSpan" id="kobo.133.1"> of the preprocessing step. </span><span class="koboSpan" id="kobo.133.2">Punctuation, on the other hand, can</span><a id="_idIndexMarker964"/><span class="koboSpan" id="kobo.134.1"> be handled with </span><strong class="source-inline"><span class="koboSpan" id="kobo.135.1">regex</span></strong><span class="koboSpan" id="kobo.136.1">. </span><span class="koboSpan" id="kobo.136.2">Let's go ahead</span><a id="_idIndexMarker965"/><span class="koboSpan" id="kobo.137.1"> and prepare a function to clean our text:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.138.1">from nltk.corpus import stopwords</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.139.1">from nltk.tokenize import word_tokenize</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.140.1">nltk.download('punkt')</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.141.1">nltk.download('stopwords')</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.142.1">import re</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.143.1">STOP_WORDS = stopwords.words()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.144.1">def cleaner(text):</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.145.1">    text = text.lower() #Convert to lower case</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.146.1">    text = re.sub("[^a-zA-Z]+", ' ', text) # Only keep text, remove punctuation and numbers</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.147.1">    text_tokens = word_tokenize(text) #Tokenize the words</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.148.1">    tokens_without_sw = [word for word in text_tokens if not word in STOP_WORDS] #Remove the stop words</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.149.1">    filtered_sentence = (" ").join(tokens_without_sw) # Join all the words or tokens back to a single string</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.150.1">    return filtered_sentence</span></p>
			<p><span class="koboSpan" id="kobo.151.1">There are four main steps within this function. </span><span class="koboSpan" id="kobo.151.2">First, we convert the text into lowercase for consistency; then, we use regex to remove all punctuation and numbers. </span><span class="koboSpan" id="kobo.151.3">After, we split the string into individual tokens and remove the words if they are in our list of stop words, before finally joining the words back together into a single string. </span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.152.1">Important note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.153.1">Please note that text cleaning scripts are often specific to the use case in the sense that not all use cases require the same steps. </span></p>
			<p><span class="koboSpan" id="kobo.154.1">Now, we can apply the </span><strong class="source-inline"><span class="koboSpan" id="kobo.155.1">cleaner</span></strong><span class="koboSpan" id="kobo.156.1"> function to our paragraph:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.157.1">clean_paragraph = cleaner(paragraph)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.158.1">clean_paragraph</span></p>
			<p><span class="koboSpan" id="kobo.159.1">We can see the output of this function in the following screenshot:</span></p>
			<div>
				<div id="_idContainer317" class="IMG---Figure">
					<span class="koboSpan" id="kobo.160.1"><img src="image/B17761_09_006.jpg" alt="Figure 9.6 – Output of the text cleaning function "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.161.1">Figure 9.6 – Output of the text cleaning function</span></p>
			<p><span class="koboSpan" id="kobo.162.1">Upon recalculating</span><a id="_idIndexMarker966"/><span class="koboSpan" id="kobo.163.1"> the frequencies</span><a id="_idIndexMarker967"/><span class="koboSpan" id="kobo.164.1"> with the clean text, we can replot</span><a id="_idIndexMarker968"/><span class="koboSpan" id="kobo.165.1"> the data and view</span><a id="_idIndexMarker969"/><span class="koboSpan" id="kobo.166.1"> the results:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.167.1">import matplotlib.pyplot as plt</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.168.1">import seaborn as sns</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.169.1">plt.figure(figsize=(10,3))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.170.1">plt.xlabel("Samples", fontsize=20)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.171.1">plt.xticks(fontsize=14)</span></p>
			<p class="source-code"> </p>
			<p class="source-code"><span class="koboSpan" id="kobo.172.1">plt.ylabel("Counts", fontsize=20)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.173.1">plt.yticks(fontsize=14)</span></p>
			<p class="source-code"> </p>
			<p class="source-code"><span class="koboSpan" id="kobo.174.1">sns.set_style("darkgrid")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.175.1">freqdist.plot(30,cumulative=False)</span></p>
			<p><span class="koboSpan" id="kobo.176.1">The output of this code can be seen in the following screenshot:</span></p>
			<div>
				<div id="_idContainer318" class="IMG---Figure">
					<span class="koboSpan" id="kobo.177.1"><img src="image/B17761_09_007.jpg" alt="Figure 9.7 – Results of calculating the frequency (without stop words) "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.178.1">Figure 9.7 – Results of calculating the frequency (without stop words)</span></p>
			<p><span class="koboSpan" id="kobo.179.1">As we begin to dive deeper</span><a id="_idIndexMarker970"/><span class="koboSpan" id="kobo.180.1"> into our text, we will often want to tag items</span><a id="_idIndexMarker971"/><span class="koboSpan" id="kobo.181.1"> not only by their parts</span><a id="_idIndexMarker972"/><span class="koboSpan" id="kobo.182.1"> of speech, but also by their</span><a id="_idIndexMarker973"/><span class="koboSpan" id="kobo.183.1"> entities, allowing us to parse</span><a id="_idIndexMarker974"/><span class="koboSpan" id="kobo.184.1"> dates, names, and many others</span><a id="_idIndexMarker975"/><span class="koboSpan" id="kobo.185.1"> in a process known as </span><strong class="bold"><span class="koboSpan" id="kobo.186.1">Named Entity Recognition</span></strong><span class="koboSpan" id="kobo.187.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.188.1">NER</span></strong><span class="koboSpan" id="kobo.189.1">). </span><span class="koboSpan" id="kobo.189.2">To accomplish this, we can make use of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.190.1">spacy</span></strong><span class="koboSpan" id="kobo.191.1"> library:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.192.1">import spacy</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.193.1">spacy_paragraph = nlp(paragraph)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.194.1">spacy_paragraph = nlp(paragraph)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.195.1">print([(X.text, X.label_) for X in spacy_paragraph.ents])</span></p>
			<p><span class="koboSpan" id="kobo.196.1">Upon printing the results, we obtain a list of items and their associated entity tags. </span><span class="koboSpan" id="kobo.196.2">Notice that the model not only picked up the year </span><strong class="source-inline"><span class="koboSpan" id="kobo.197.1">1919</span></strong><span class="koboSpan" id="kobo.198.1"> as a </span><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">DATE</span></strong><span class="koboSpan" id="kobo.200.1"> entity but also picked up descriptions such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.201.1">21st centuries</span></strong><span class="koboSpan" id="kobo.202.1"> as </span><strong class="source-inline"><span class="koboSpan" id="kobo.203.1">DATE</span></strong><span class="koboSpan" id="kobo.204.1"> entities:</span></p>
			<div>
				<div id="_idContainer319" class="IMG---Figure">
					<span class="koboSpan" id="kobo.205.1"><img src="image/B17761_09_008.jpg" alt="Figure 9.8 – Results of the NER model showing the text and its subsequent tag "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.206.1">Figure 9.8 – Results of the NER model showing the text and its subsequent tag</span></p>
			<p><span class="koboSpan" id="kobo.207.1">We can also display the tags from a visual perspective within Jupyter Notebook using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.208.1">render</span></strong><span class="koboSpan" id="kobo.209.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.210.1">from spacy import displacy</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.211.1">displacy.render(nlp(str(sentences)), jupyter=True, style='ent')</span></p>
			<p><span class="koboSpan" id="kobo.212.1">Upon executing this code, we</span><a id="_idIndexMarker976"/><span class="koboSpan" id="kobo.213.1"> will receive the original paragraph, which</span><a id="_idIndexMarker977"/><span class="koboSpan" id="kobo.214.1"> has been color-coded based</span><a id="_idIndexMarker978"/><span class="koboSpan" id="kobo.215.1"> on the identified entity tags, allowing</span><a id="_idIndexMarker979"/><span class="koboSpan" id="kobo.216.1"> us to view the results visually:</span></p>
			<div>
				<div id="_idContainer320" class="IMG---Figure">
					<span class="koboSpan" id="kobo.217.1"><img src="image/B17761_09_009.jpg" alt="Figure 9.9 – Results of the NER model when rendered in Jupyter Notebook "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.218.1">Figure 9.9 – Results of the NER model when rendered in Jupyter Notebook</span></p>
			<p><span class="koboSpan" id="kobo.219.1">We can also use SciPy to implement a visual understanding of our text when it comes to parts of speech using the same </span><strong class="source-inline"><span class="koboSpan" id="kobo.220.1">render()</span></strong><span class="koboSpan" id="kobo.221.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.222.1">displacy.render(nlp(str(sentences[0])), style='dep', jupyter = True, options = {'distance': 120})</span></p>
			<p><span class="koboSpan" id="kobo.223.1">We can see the output of this command in the following diagram:</span></p>
			<div>
				<div id="_idContainer321" class="IMG---Figure">
					<span class="koboSpan" id="kobo.224.1"><img src="image/B17761_09_010.jpg" alt="Figure 9.10 – Results of the POS model when rendered in Jupyter Notebook "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.225.1">Figure 9.10 – Results of the POS model when rendered in Jupyter Notebook</span></p>
			<p><span class="koboSpan" id="kobo.226.1">This gives us a great</span><a id="_idIndexMarker980"/><span class="koboSpan" id="kobo.227.1"> way to understand and visualize the structure of a sentence before implementing any NLP</span><a id="_idIndexMarker981"/><span class="koboSpan" id="kobo.228.1"> models. </span><span class="koboSpan" id="kobo.228.2">With some of the basic</span><a id="_idIndexMarker982"/><span class="koboSpan" id="kobo.229.1"> analysis out of the</span><a id="_idIndexMarker983"/><span class="koboSpan" id="kobo.230.1"> way, let's go ahead and explore some applications of NLP using structured data.</span></p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor135"/><span class="koboSpan" id="kobo.231.1">Working with structured data</span></h1>
			<p><span class="koboSpan" id="kobo.232.1">Now that we have explored some of the basics of NLP, let's dive into some more complex and common</span><a id="_idIndexMarker984"/><span class="koboSpan" id="kobo.233.1"> use cases that are often observed in the biotech and life sciences fields. </span><span class="koboSpan" id="kobo.233.2">When working with text-based data, it is much more common to work with larger datasets rather than single strings. </span><span class="koboSpan" id="kobo.233.3">More often than not, we generally want these datasets to involve scientific data regarding specific areas of interest relating to a particular research topic. </span><span class="koboSpan" id="kobo.233.4">Let's go ahead and learn how to retrieve scientific</span><a id="_idIndexMarker985"/><span class="koboSpan" id="kobo.234.1"> data using Python.</span></p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor136"/><span class="koboSpan" id="kobo.235.1">Searching for scientific articles</span></h2>
			<p><span class="koboSpan" id="kobo.236.1">To programmatically</span><a id="_idIndexMarker986"/><span class="koboSpan" id="kobo.237.1"> retrieve scientific publication data using Python, we can make use of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.238.1">pymed</span></strong><span class="koboSpan" id="kobo.239.1"> library from </span><em class="italic"><span class="koboSpan" id="kobo.240.1">PubMed</span></em><span class="koboSpan" id="kobo.241.1"> (</span><a href="https://pubmed.ncbi.nlm.nih.gov/"><span class="koboSpan" id="kobo.242.1">https://pubmed.ncbi.nlm.nih.gov/</span></a><span class="koboSpan" id="kobo.243.1">). </span><span class="koboSpan" id="kobo.243.2">Let's go ahead and build</span><a id="_idIndexMarker987"/><span class="koboSpan" id="kobo.244.1"> a sample dataset:</span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.245.1">First, let's import our libraries and instantiate a new </span><strong class="source-inline"><span class="koboSpan" id="kobo.246.1">PubMed</span></strong><span class="koboSpan" id="kobo.247.1"> object:</span><p class="source-code"><span class="koboSpan" id="kobo.248.1">from pymed import PubMed</span></p><p class="source-code"><span class="koboSpan" id="kobo.249.1">pubmed = PubMed()</span></p></li>
				<li><span class="koboSpan" id="kobo.250.1">Next, we will need to define and run our query. </span><span class="koboSpan" id="kobo.250.2">Let's go ahead and search for all the items related to monoclonal antibodies and retrieve </span><strong class="source-inline"><span class="koboSpan" id="kobo.251.1">100</span></strong><span class="koboSpan" id="kobo.252.1"> results:</span><p class="source-code"><span class="koboSpan" id="kobo.253.1">query = "monoclonal antibody"</span></p><p class="source-code"><span class="koboSpan" id="kobo.254.1">results = pubmed.query(query, max_results=100)</span></p></li>
				<li><span class="koboSpan" id="kobo.255.1">With the results found, we can iterate over our results to retrieve all the available fields for each of the given articles:</span><p class="source-code"><span class="koboSpan" id="kobo.256.1">articleList = []</span></p><p class="source-code"><span class="koboSpan" id="kobo.257.1">for article in results:</span></p><p class="source-code"><span class="koboSpan" id="kobo.258.1">    articleDict = article.toDict()</span></p><p class="source-code"><span class="koboSpan" id="kobo.259.1">    articleList.append(articleDict)</span></p></li>
				<li><span class="koboSpan" id="kobo.260.1">Finally, we can go ahead and convert our list into a DataFrame for ease of use:</span><p class="source-code"><span class="koboSpan" id="kobo.261.1">df = pd.DataFrame(articleList)</span></p><p class="source-code"><span class="koboSpan" id="kobo.262.1">df.head()</span></p><p><span class="koboSpan" id="kobo.263.1">The following is the output:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer322" class="IMG---Figure">
					<span class="koboSpan" id="kobo.264.1"><img src="image/B17761_09_011.jpg" alt="Figure 9.11 – A DataFrame showing the results of the PubMed search "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.265.1">Figure 9.11 – A DataFrame showing the results of the PubMed search</span></p>
			<p><span class="koboSpan" id="kobo.266.1">With the final step</span><a id="_idIndexMarker988"/><span class="koboSpan" id="kobo.267.1"> complete, we have a dataset full of scientific</span><a id="_idIndexMarker989"/><span class="koboSpan" id="kobo.268.1"> abstracts and their associated metadata. </span><span class="koboSpan" id="kobo.268.2">In the next section, we will explore this data in more depth and develop a few visuals to represent it.</span></p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.269.1">Exploring our datasets</span></h2>
			<p><span class="koboSpan" id="kobo.270.1">Now that we have some data to work with, let's go ahead and explore it. </span><span class="koboSpan" id="kobo.270.2">If you recall from many of the previous</span><a id="_idIndexMarker990"/><span class="koboSpan" id="kobo.271.1"> chapters, we often explore our numerical datasets</span><a id="_idIndexMarker991"/><span class="koboSpan" id="kobo.272.1"> in various ways. </span><span class="koboSpan" id="kobo.272.2">We can group columns, explore trends, and find correlations – tasks we cannot necessarily do when working with text. </span><span class="koboSpan" id="kobo.272.3">Let's implement a few NLP methods to explore data in a slightly different way.</span></p>
			<h3><span class="koboSpan" id="kobo.273.1">Checking string lengths</span></h3>
			<p><span class="koboSpan" id="kobo.274.1">Since we have our dataset</span><a id="_idIndexMarker992"/><span class="koboSpan" id="kobo.275.1"> structured within a pandas DataFrame, one of the first items we generally want to explore is the distribution of string lengths for our text-based data. </span><span class="koboSpan" id="kobo.275.2">In the current dataset, the two main columns containing text are </span><strong class="source-inline"><span class="koboSpan" id="kobo.276.1">title</span></strong><span class="koboSpan" id="kobo.277.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.278.1">abstract</span></strong><span class="koboSpan" id="kobo.279.1"> – let's go ahead and plot the distribution of lengths:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.280.1">sns.displot(df.abstract.str.len(), bins=25)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.281.1">sns.displot(df.title.str.len(), bins=25)</span></p>
			<p><span class="koboSpan" id="kobo.282.1">The following is the output:</span></p>
			<div>
				<div id="_idContainer323" class="IMG---Figure">
					<span class="koboSpan" id="kobo.283.1"><img src="image/B17761_09_012.jpg" alt="Figure 9.12 – Frequency distributions of the average length of abstracts (left) and titles (right) "/></span>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption"><span class="koboSpan" id="kobo.284.1">Figure 9.12 – Frequency distributions of the average length of abstracts (left) and titles (right)</span></p>
			<p><span class="koboSpan" id="kobo.285.1">Here, we can see that the average</span><a id="_idIndexMarker993"/><span class="koboSpan" id="kobo.286.1"> length of most abstracts is around 1,500 characters, whereas titles are around 100. </span><span class="koboSpan" id="kobo.286.2">Since the titles may contain important keywords or identifiers for a given article, similar to that of the abstracts, it would be wise to combine the two into a single column to analyze them together. </span><span class="koboSpan" id="kobo.286.3">We can simply combine them using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.287.1">+</span></strong><span class="koboSpan" id="kobo.288.1"> operator:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.289.1">df["text"] = df["title"] + " " + df["abstract"]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.290.1">df[["title", "abstract", "text"]]</span></p>
			<p><span class="koboSpan" id="kobo.291.1">We can see this new column in the following screenshot:</span></p>
			<div>
				<div id="_idContainer324" class="IMG---Figure">
					<span class="koboSpan" id="kobo.292.1"><img src="image/B17761_09_013.jpg" alt="Figure 9.13 – A sample DataFrame showing the title, abstract, and text columns "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.293.1">Figure 9.13 – A sample DataFrame showing the title, abstract, and text columns</span></p>
			<p><span class="koboSpan" id="kobo.294.1">Using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.295.1">mean()</span></strong><span class="koboSpan" id="kobo.296.1"> function on each of the columns, we can see that the titles have, on average, 108 characters, the abstracts have 1,277 characters, and the combined text column has 1,388 characters. </span></p>
			<p><span class="koboSpan" id="kobo.297.1">Similar to other</span><a id="_idIndexMarker994"/><span class="koboSpan" id="kobo.298.1"> datasets, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.299.1">value_counts()</span></strong><span class="koboSpan" id="kobo.300.1"> function to get a quick sense of the most common words:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.301.1">df.text.str.split(expand=True).stack().value_counts()</span></p>
			<p><span class="koboSpan" id="kobo.302.1">Immediately, we notice that our dataset is flooded with stop words: </span></p>
			<div>
				<div id="_idContainer325" class="IMG---Figure">
					<span class="koboSpan" id="kobo.303.1"><img src="image/B17761_09_014.jpg" alt="Figure 9.14 – A sample of the most frequent words in the dataset "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.304.1">Figure 9.14 – A sample of the most frequent words in the dataset</span></p>
			<p><span class="koboSpan" id="kobo.305.1">We can implement the same </span><strong class="source-inline"><span class="koboSpan" id="kobo.306.1">cleaner</span></strong><span class="koboSpan" id="kobo.307.1"> function as we did previously to remove these stop words and any other undesirable values. </span><span class="koboSpan" id="kobo.307.2">Note that some of the cells within the DataFrame may be empty, depending on the query made and the results returned. </span><span class="koboSpan" id="kobo.307.3">We'll take a closer look at this in the next section.</span></p>
			<h3><span class="koboSpan" id="kobo.308.1">Cleaning text data</span></h3>
			<p><span class="koboSpan" id="kobo.309.1">We can add a quick check at the top</span><a id="_idIndexMarker995"/><span class="koboSpan" id="kobo.310.1"> of the function by checking the type of the value to ensure that no errors are encountered:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.311.1">from nltk.corpus import stopwords</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.312.1">STOP_WORDS = stopwords.words()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.313.1">def cleaner(text):</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.314.1">    if type(text) == str:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.315.1">        text = text.lower()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.316.1">        text = re.sub("[^a-zA-Z]+", ' ', text)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.317.1">        text_tokens = word_tokenize(text)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.318.1">        tokens_without_sw = [word for word in text_tokens if not word in STOP_WORDS]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.319.1">        filtered_sentence = (" ").join(tokens_without_sw)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.320.1">        return filtered_sentence</span></p>
			<p><span class="koboSpan" id="kobo.321.1">We can quickly test this out on a sample string to test the functionality:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.322.1">cleaner("Biotech in 2021 is a wonderful field to work and study in!")</span></p>
			<p><span class="koboSpan" id="kobo.323.1">The output of this function can be seen in the following screenshot:</span></p>
			<div>
				<div id="_idContainer326" class="IMG---Figure">
					<span class="koboSpan" id="kobo.324.1"><img src="image/B17761_09_015.jpg" alt="Figure 9.15 – Results of the cleaning function "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.325.1">Figure 9.15 – Results of the cleaning function</span></p>
			<p><span class="koboSpan" id="kobo.326.1">With the function working, we can go ahead and apply this to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.327.1">text</span></strong><span class="koboSpan" id="kobo.328.1"> column within the DataFrame and create a new column consisting of the cleaned text using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.329.1">apply()</span></strong><span class="koboSpan" id="kobo.330.1"> function, which allows us to apply a given function iteratively down through all rows of a DataFrame:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.331.1">df["clean_text"] = df["text"].apply(lambda x: cleaner(x))</span></p>
			<p><span class="koboSpan" id="kobo.332.1">We can check the performance of our function by checking the columns of interest:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.333.1">df[["text", "clean_text"]].head()</span></p>
			<p><span class="koboSpan" id="kobo.334.1">We can see these two</span><a id="_idIndexMarker996"/><span class="koboSpan" id="kobo.335.1"> columns in the following screenshot:</span></p>
			<div>
				<div id="_idContainer327" class="IMG---Figure">
					<span class="koboSpan" id="kobo.336.1"><img src="image/B17761_09_016.jpg" alt="Figure 9.16 – A DataFrame showing the original and cleaned texts "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.337.1">Figure 9.16 – A DataFrame showing the original and cleaned texts</span></p>
			<p><span class="koboSpan" id="kobo.338.1">If we go ahead and check </span><strong class="source-inline"><span class="koboSpan" id="kobo.339.1">value_counts()</span></strong><span class="koboSpan" id="kobo.340.1">of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.341.1">clean_text</span></strong><span class="koboSpan" id="kobo.342.1"> column, as we did previously, you will notice</span><a id="_idIndexMarker997"/><span class="koboSpan" id="kobo.343.1"> that the stop words were removed and that more useful keywords are now populated at the top.</span></p>
			<h3><span class="koboSpan" id="kobo.344.1">Creating word clouds</span></h3>
			<p><span class="koboSpan" id="kobo.345.1">Another popular and useful method to get</span><a id="_idIndexMarker998"/><span class="koboSpan" id="kobo.346.1"> a quick sense of the content for a given text-based dataset is by using word clouds. </span><strong class="bold"><span class="koboSpan" id="kobo.347.1">Word clouds</span></strong><span class="koboSpan" id="kobo.348.1"> are images that are populated with the content</span><a id="_idIndexMarker999"/><span class="koboSpan" id="kobo.349.1"> of a dataset in which words are rendered in larger fonts when they are more frequent, and smaller fonts when they are less frequent. </span><span class="koboSpan" id="kobo.349.2">To accomplish this, we can make</span><a id="_idIndexMarker1000"/><span class="koboSpan" id="kobo.350.1"> use of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.351.1">wordclouds</span></strong><span class="koboSpan" id="kobo.352.1"> library:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.353.1">First, we need to import the function and then create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.354.1">wordcloud</span></strong><span class="koboSpan" id="kobo.355.1"> object that we will specify a number of parameters in. </span><span class="koboSpan" id="kobo.355.2">We can adjust the dimensions of the image, colors, and the data as follows:</span><p class="source-code"><span class="koboSpan" id="kobo.356.1">from wordcloud import WordCloud, STOPWORDS</span></p><p class="source-code"><span class="koboSpan" id="kobo.357.1">plt.figure(figsize=(20,10))</span></p><p class="source-code"><span class="koboSpan" id="kobo.358.1"># Drop nans</span></p><p class="source-code"><span class="koboSpan" id="kobo.359.1">df2 = df[["clean_text"]].dropna()</span></p><p class="source-code"><span class="koboSpan" id="kobo.360.1"># Create word cloud</span></p><p class="source-code"><span class="koboSpan" id="kobo.361.1">wordcloud = WordCloud(width = 5000, </span></p><p class="source-code"><span class="koboSpan" id="kobo.362.1">                      height = 3000, </span></p><p class="source-code"><span class="koboSpan" id="kobo.363.1">                      random_state=1, </span></p><p class="source-code"><span class="koboSpan" id="kobo.364.1">                      background_color='white', </span></p><p class="source-code"><span class="koboSpan" id="kobo.365.1">                      colormap='Blues', </span></p><p class="source-code"><span class="koboSpan" id="kobo.366.1">                      collocations=False, </span></p><p class="source-code"><span class="koboSpan" id="kobo.367.1">                      stopwords = STOPWORDS).generate(' '.join(df2['clean_text']))</span></p></li>
				<li><span class="koboSpan" id="kobo.368.1">Now, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.369.1">imshow()</span></strong><span class="koboSpan" id="kobo.370.1"> function from </span><strong class="source-inline"><span class="koboSpan" id="kobo.371.1">matplotlib</span></strong><span class="koboSpan" id="kobo.372.1"> to render the image:</span><p class="source-code"><span class="koboSpan" id="kobo.373.1">plt.figure( figsize=(15,10) )</span></p><p class="source-code"><span class="koboSpan" id="kobo.374.1">plt.imshow(wordcloud)</span></p><p><span class="koboSpan" id="kobo.375.1">The following is the output:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer328" class="IMG---Figure">
					<span class="koboSpan" id="kobo.376.1"><img src="image/B17761_09_017.jpg" alt="Figure 9.17 – A word cloud representing the frequency of words in the dataset "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.377.1">Figure 9.17 – A word cloud representing the frequency of words in the dataset</span></p>
			<p><span class="koboSpan" id="kobo.378.1">In this section, we investigated a few of the most popular methods for quickly analyzing text-based</span><a id="_idIndexMarker1001"/><span class="koboSpan" id="kobo.379.1"> data as a preliminary step before performing any type of rigorous analysis or model development process. </span><span class="koboSpan" id="kobo.379.2">In the next section, we will train a model using this dataset to investigate topics.</span></p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor138"/><span class="koboSpan" id="kobo.380.1">Tutorial – clustering and topic modeling</span></h1>
			<p><span class="koboSpan" id="kobo.381.1">Similar to some of the previous examples we have seen so far, much of our data can either be classified</span><a id="_idIndexMarker1002"/><span class="koboSpan" id="kobo.382.1"> in a supervised setting or clustered in an unsupervised one. </span><span class="koboSpan" id="kobo.382.2">In most</span><a id="_idIndexMarker1003"/><span class="koboSpan" id="kobo.383.1"> cases, text-based data is generally made available to us in the form of real-world data in the sense that it is in a raw and unlabeled form. </span></p>
			<p><span class="koboSpan" id="kobo.384.1">Let's look at an example where we can make sense of our data and label it from an unsupervised perspective. </span><span class="koboSpan" id="kobo.384.2">Our main objective here will be to preprocess our raw text, cluster the data</span><a id="_idIndexMarker1004"/><span class="koboSpan" id="kobo.385.1"> into five clusters, and then determine the main topics</span><a id="_idIndexMarker1005"/><span class="koboSpan" id="kobo.386.1"> for each of those clusters. </span><span class="koboSpan" id="kobo.386.2">If you are following along using the provided code and documentation, please note that your results may vary as the dataset is dynamic, and its contents change as new data is populated into the PubMed database. </span><span class="koboSpan" id="kobo.386.3">I would urge you to customize the queries to topics that interest you. </span><span class="koboSpan" id="kobo.386.4">With that in mind, let's go ahead and begin.</span></p>
			<p><span class="koboSpan" id="kobo.387.1">We will begin by querying some data using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.388.1">pymed</span></strong><span class="koboSpan" id="kobo.389.1"> library and retrieving a few hundred abstracts and titles to work with:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.390.1">def dataset_generator(query, num_results, ):</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.391.1">    results = pubmed.query(query, max_results=num_results)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.392.1">    articleList = []</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.393.1">    for article in results:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.394.1">        articleDict = article.toDict()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.395.1">        articleList.append(articleDict)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.396.1">    print(f"Found {len(articleList)} results for the query '{query}'.")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.397.1">    return pd.DataFrame(articleList)</span></p>
			<p><span class="koboSpan" id="kobo.398.1">Instead of making a single query, let's make a few and combine the results into a single DataFrame:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.399.1">df1 = dataset_generator("monoclonal antibodies", 600)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.400.1">df2 = dataset_generator("machine learning", 600)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.401.1">df3 = dataset_generator("covid-19", 600)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.402.1">df4 = dataset_generator("particle physics", 600)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.403.1">df = pd.concat([df1, df2, df3, df4])</span></p>
			<p><span class="koboSpan" id="kobo.404.1">Taking a look at the data, we can see that some of the cells have missing (</span><strong class="source-inline"><span class="koboSpan" id="kobo.405.1">nan</span></strong><span class="koboSpan" id="kobo.406.1">) values. </span><span class="koboSpan" id="kobo.406.2">Given that our objective</span><a id="_idIndexMarker1006"/><span class="koboSpan" id="kobo.407.1"> concerns the text-based fields only (titles and abstracts), let's limit</span><a id="_idIndexMarker1007"/><span class="koboSpan" id="kobo.408.1"> the scope of any cleaning methods to those columns alone:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.409.1">df = df[["title", "abstract"]]</span></p>
			<p><span class="koboSpan" id="kobo.410.1">Given that we are concerned with the contents of each article as a whole, we can combine the titles and abstracts together into a new column called </span><strong class="source-inline"><span class="koboSpan" id="kobo.411.1">text</span></strong><span class="koboSpan" id="kobo.412.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.413.1">df["text"] = df["title"] + " " + df["abstract"]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.414.1">df = df.dropna()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.415.1">print(df.shape)</span></p>
			<p><span class="koboSpan" id="kobo.416.1">Taking a look at the dataset, we can see that we have 560 rows and 3 columns. </span><span class="koboSpan" id="kobo.416.2">Scientific articles can be very descriptive, encompassing many stop words. </span><span class="koboSpan" id="kobo.416.3">Given that our objective here is to detect topics that are represented by keywords, let's remove any punctuation, numerical values, and stopwords from our text:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.417.1">def cleaner(text):</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.418.1">    if type(text) == str:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.419.1">        text = text.lower()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.420.1">        text = re.sub("[^a-zA-Z]+", ' ', text)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.421.1">        text_tokens = word_tokenize(text)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.422.1">        tokens_without_sw = [word for word in text_tokens if not word in STOP_WORDS]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.423.1">        filtered_sentence = (" ").join(tokens_without_sw)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.424.1">        return filtered_sentence</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.425.1">df["text"] = df["text"].apply(lambda x: cleaner(x))</span></p>
			<p><span class="koboSpan" id="kobo.426.1">We can check the average number of words per article before and after implementing the script to ensure that the data was cleaned. </span><span class="koboSpan" id="kobo.426.2">In our case, we can see that we started with an average of 190 words and ended up with an average of 123.</span></p>
			<p><span class="koboSpan" id="kobo.427.1">With the data now clean, we can go ahead and extract our features. </span><span class="koboSpan" id="kobo.427.2">We will use a relatively</span><a id="_idIndexMarker1008"/><span class="koboSpan" id="kobo.428.1"> simple and common method known as </span><strong class="bold"><span class="koboSpan" id="kobo.429.1">TFIDF</span></strong><span class="koboSpan" id="kobo.430.1"> – a measure of originality of words in which each word is compared to the number of times it appears in an article, relative</span><a id="_idIndexMarker1009"/><span class="koboSpan" id="kobo.431.1"> to the number of articles the same word appears in. </span><span class="koboSpan" id="kobo.431.2">We can think</span><a id="_idIndexMarker1010"/><span class="koboSpan" id="kobo.432.1"> of TFIDF as two separate items – </span><strong class="bold"><span class="koboSpan" id="kobo.433.1">Term Frequency</span></strong><span class="koboSpan" id="kobo.434.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.435.1">TF</span></strong><span class="koboSpan" id="kobo.436.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.437.1">Inverse Document Frequency</span></strong><span class="koboSpan" id="kobo.438.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.439.1">IDF</span></strong><span class="koboSpan" id="kobo.440.1">) – which we can represent as follows:</span></p>
			<div>
				<div id="_idContainer329" class="IMG---Figure">
					<span class="koboSpan" id="kobo.441.1"><img src="image/Formula_09_001.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.442.1">In the preceding equation, </span><em class="italic"><span class="koboSpan" id="kobo.443.1">t</span></em><span class="koboSpan" id="kobo.444.1"> is the term or keyword, while </span><em class="italic"><span class="koboSpan" id="kobo.445.1">d</span></em><span class="koboSpan" id="kobo.446.1"> is the document or – in our case – the article. </span><span class="koboSpan" id="kobo.446.2">The main</span><a id="_idIndexMarker1011"/><span class="koboSpan" id="kobo.447.1"> idea here is to capture important keywords that</span><a id="_idIndexMarker1012"/><span class="koboSpan" id="kobo.448.1"> would be descriptive as main topics but ignore those that appear in almost every article. </span><span class="koboSpan" id="kobo.448.2">We will begin by importing </span><strong class="source-inline"><span class="koboSpan" id="kobo.449.1">TfidfVectorizer</span></strong><span class="koboSpan" id="kobo.450.1"> from </span><strong class="source-inline"><span class="koboSpan" id="kobo.451.1">sklearn</span></strong><span class="koboSpan" id="kobo.452.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.453.1">from sklearn.feature_extraction.text import TfidfVectorizer</span></p>
			<p><span class="koboSpan" id="kobo.454.1">Next, we will convert our text-based data into numerical features by fitting our dataset and transforming the values:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.455.1">vectors = TfidfVectorizer(stop_words="english", max_features=5500)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.456.1">vectors.fit(df.text.values)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.457.1">features = vectors.transform(df.text.values)</span></p>
			<p><span class="koboSpan" id="kobo.458.1">We can check the shape of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.459.1">features</span></strong><span class="koboSpan" id="kobo.460.1"> variable to confirm that we have 560 rows, just as we did before applying TFIDF, and 5,500 columns worth of features to go with it. </span><span class="koboSpan" id="kobo.460.2">Next, we can go ahead and cluster our documents using one of the many clustering methods we have explored so far. </span></p>
			<p><span class="koboSpan" id="kobo.461.1">Let's implement </span><strong class="source-inline"><span class="koboSpan" id="kobo.462.1">MiniBatchKMeans</span></strong><span class="koboSpan" id="kobo.463.1"> and specify </span><strong class="source-inline"><span class="koboSpan" id="kobo.464.1">4</span></strong><span class="koboSpan" id="kobo.465.1"> as the number of clusters we want to retrieve:</span></p>
			<p class="source-code"><strong class="source-inline"><span class="koboSpan" id="kobo.466.1">from sklearn.cluster import MiniBatchKMeans</span></strong></p>
			<p class="source-code"><strong class="source-inline"><span class="koboSpan" id="kobo.467.1">cls = M</span></strong><span class="koboSpan" id="kobo.468.1">iniBatchKMeans(n_clusters=4)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.469.1">cls.fit(features)</span></p>
			<p><span class="koboSpan" id="kobo.470.1">When working with larger datasets, especially in production, it is generally advisable to avoid using pandas DataFrames as there are more efficient methods available, depending on the processes you need to implement. </span><span class="koboSpan" id="kobo.470.2">Given that we are only working with 560 rows of data, and our objective is to cluster our data and retrieve topics, we will once again make use of DataFrames to manage our data. </span><span class="koboSpan" id="kobo.470.3">Let's go ahead and add our predicted clusters to our DataFrame:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.471.1">df["cluster"] = cls.predict(features)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.472.1">df[["text", "cluster"]].head()</span></p>
			<p><span class="koboSpan" id="kobo.473.1">We can see the output of this command in the following screenshot:</span></p>
			<div>
				<div id="_idContainer330" class="IMG---Figure">
					<span class="koboSpan" id="kobo.474.1"><img src="image/B17761_09_018.jpg" alt="Figure 9.18 – A DataFrame showing the cleaned texts and their associated clusters "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.475.1">Figure 9.18 – A DataFrame showing the cleaned texts and their associated clusters</span></p>
			<p><span class="koboSpan" id="kobo.476.1">With the data</span><a id="_idIndexMarker1013"/><span class="koboSpan" id="kobo.477.1"> clustered, let's plot this data in a 2D scatterplot. </span><span class="koboSpan" id="kobo.477.2">Given</span><a id="_idIndexMarker1014"/><span class="koboSpan" id="kobo.478.1"> that we have several thousand features, we can make use of the </span><strong class="bold"><span class="koboSpan" id="kobo.479.1">PCA</span></strong><span class="koboSpan" id="kobo.480.1"> algorithm to reduce these down to only</span><a id="_idIndexMarker1015"/><span class="koboSpan" id="kobo.481.1"> two features for our visualization:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.482.1">from sklearn.decomposition import PCA</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.483.1">pca = PCA(n_components=2)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.484.1">pca_features_2d = pca.fit_transform(features.toarray())</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.485.1">pca_features_2d_centers = pca.transform(cls.cluster_centers_)</span></p>
			<p><span class="koboSpan" id="kobo.486.1">Let's go ahead and add these two principal components to our DataFrame:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.487.1">df["pc1"], df["pc2"] = pca_features_2d[:,0], pca_features_2d[:,1]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.488.1">df[["text", "cluster", "pc1", "pc2"]].head()</span></p>
			<p><span class="koboSpan" id="kobo.489.1">We can see the output of this command in the following screenshot:</span></p>
			<div>
				<div id="_idContainer331" class="IMG---Figure">
					<span class="koboSpan" id="kobo.490.1"><img src="image/B17761_09_019.jpg" alt="Figure 9.19 – A DataFrame showing the texts, clusters, and principal components "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.491.1">Figure 9.19 – A DataFrame showing the texts, clusters, and principal components</span></p>
			<p><span class="koboSpan" id="kobo.492.1">Here, we can see that</span><a id="_idIndexMarker1016"/><span class="koboSpan" id="kobo.493.1"> each row of text now has a cluster, as well as a set of</span><a id="_idIndexMarker1017"/><span class="koboSpan" id="kobo.494.1"> coordinates, in the form of principal components. </span><span class="koboSpan" id="kobo.494.2">Next, we will plot our data and color by cluster:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.495.1">plt.figure(f</span><strong class="source-inline"><span class="koboSpan" id="kobo.496.1">igsize=(15,8))</span></strong></p>
			<p class="source-code"><strong class="source-inline"><span class="koboSpan" id="kobo.497.1">new_cmap = matplotlib.colors.LinearSegmentedColormap.from_list("mycmap", colors)</span></strong></p>
			<p class="source-code"><strong class="source-inline"><span class="koboSpan" id="kobo.498.1">plt.scatter(df["pc1"], df["pc2"], c=df["cluster"], cmap=new_cmap)</span></strong></p>
			<p class="source-code"><span class="koboSpan" id="kobo.499.1">plt.scatter(pca_features_2d_centers[:, 0], pca_features_2d_centers[:,1], marker='*', s=500, c='r')</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.500.1">plt.xlabel("PC1", fontsize=20)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.501.1">plt.ylabel("PC2", fontsize=20)</span></p>
			<p><span class="koboSpan" id="kobo.502.1">Upon executing this code, we will get the following output:</span></p>
			<div>
				<div id="_idContainer332" class="IMG---Figure">
					<span class="koboSpan" id="kobo.503.1"><img src="image/B17761_09_020.jpg" alt="Figure 9.20 – A scatterplot of the principal components colored by cluster, with stars representing the cluster centers"/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.504.1">Figure 9.20 – A scatterplot of the principal components colored by cluster, with stars representing the cluster centers</span></p>
			<p><span class="koboSpan" id="kobo.505.1">Here, we can see that there seems to be some adequate separation between clusters! </span><span class="koboSpan" id="kobo.505.2">The two clusters</span><a id="_idIndexMarker1018"/><span class="koboSpan" id="kobo.506.1"> on the far left seem to have smaller variance in their</span><a id="_idIndexMarker1019"/><span class="koboSpan" id="kobo.507.1"> distributions, whereas the other two are much more spread out. </span><span class="koboSpan" id="kobo.507.2">Given that we reduced a considerable number of features down to only two principal components, it makes sense that there is a certain degree of overlap between them, especially given the fact that all the articles were scientific.</span></p>
			<p><span class="koboSpan" id="kobo.508.1">Now, let's go ahead and calculate some of the most prominent topics that were found in our dataset:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.509.1">First, we will begin by implementing TFIDF:</span><p class="source-code"><span class="koboSpan" id="kobo.510.1">from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer</span></p><p class="source-code"><span class="koboSpan" id="kobo.511.1">vectors = TfidfVectorizer(max_features=5500, stop_words="english")</span></p><p class="source-code"><span class="koboSpan" id="kobo.512.1">nmf_features = vectors.fit_transform(df.text)</span></p></li>
				<li><span class="koboSpan" id="kobo.513.1">Next, we will reduce</span><a id="_idIndexMarker1020"/><span class="koboSpan" id="kobo.514.1"> the dimensionality. </span><span class="koboSpan" id="kobo.514.2">However, this time, we will use </span><strong class="bold"><span class="koboSpan" id="kobo.515.1">Non-Negative Matrix Factorization</span></strong><span class="koboSpan" id="kobo.516.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.517.1">NMF</span></strong><span class="koboSpan" id="kobo.518.1">) to reduce our data instead of PCA. </span><span class="koboSpan" id="kobo.518.2">We will need to specify the number of topics we are interested in:</span><p class="source-code"><span class="koboSpan" id="kobo.519.1">from sklearn.decomposition import NMF</span></p><p class="source-code"><span class="koboSpan" id="kobo.520.1">n_topics = 10</span></p><p class="source-code"><span class="koboSpan" id="kobo.521.1">cls = NMF(n_components=n_topics)</span></p><p class="source-code"><span class="koboSpan" id="kobo.522.1">cls.fit(features)</span></p></li>
				<li><span class="koboSpan" id="kobo.523.1">Now, we can</span><a id="_idIndexMarker1021"/><span class="koboSpan" id="kobo.524.1"> specify the number of keywords to retrieve per</span><a id="_idIndexMarker1022"/><span class="koboSpan" id="kobo.525.1"> topic. </span><span class="koboSpan" id="kobo.525.2">After that, we will iterate over the components and retrieve the keywords of interest:</span><p class="source-code"><span class="koboSpan" id="kobo.526.1">num_topic_words = 3</span></p><p class="source-code"><span class="koboSpan" id="kobo.527.1">feature_names = vectors.get_feature_names()</span></p><p class="source-code"><span class="koboSpan" id="kobo.528.1">for i, j in enumerate(cls.components_):</span></p><p class="source-code"><span class="koboSpan" id="kobo.529.1">    print(i, end=' ')</span></p><p class="source-code"><span class="koboSpan" id="kobo.530.1">    for k in j.argsort()[-1:-num_topic_words-1:-1]:</span></p><p class="source-code"><span class="koboSpan" id="kobo.531.1">        print(feature_names[k], end=' ')</span></p><p><span class="koboSpan" id="kobo.532.1">Upon executing this loop, we retrieve the following as output:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer333" class="IMG---Figure">
					<span class="koboSpan" id="kobo.533.1"><img src="image/B17761_09_021.jpg" alt="Figure 9.21 – Top 10 topics of the dataset, each represented by three keywords "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.534.1">Figure 9.21 – Top 10 topics of the dataset, each represented by three keywords</span></p>
			<p><span class="koboSpan" id="kobo.535.1">We can use these topic modeling methods to extract insights and trends from the dataset, allowing users to have high-level interpretations without the need to dive into the datasets as a whole. </span><span class="koboSpan" id="kobo.535.2">Throughout</span><a id="_idIndexMarker1023"/><span class="koboSpan" id="kobo.536.1"> this tutorial, we examined one of the classical methods</span><a id="_idIndexMarker1024"/><span class="koboSpan" id="kobo.537.1"> for clustering and topic modeling: using </span><strong class="bold"><span class="koboSpan" id="kobo.538.1">TFIDF</span></strong><span class="koboSpan" id="kobo.539.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.540.1">NMF</span></strong><span class="koboSpan" id="kobo.541.1">. </span><span class="koboSpan" id="kobo.541.2">However, many other</span><a id="_idIndexMarker1025"/><span class="koboSpan" id="kobo.542.1"> methods exist that use language models, such as </span><strong class="bold"><span class="koboSpan" id="kobo.543.1">BERT</span></strong><span class="koboSpan" id="kobo.544.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.545.1">BioBERT</span></strong><span class="koboSpan" id="kobo.546.1"> and libraries such as </span><strong class="bold"><span class="koboSpan" id="kobo.547.1">Gensim</span></strong><span class="koboSpan" id="kobo.548.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.549.1">LDA</span></strong><span class="koboSpan" id="kobo.550.1">. </span><span class="koboSpan" id="kobo.550.2">If this is an area you find interesting, I highly</span><a id="_idIndexMarker1026"/><span class="koboSpan" id="kobo.551.1"> urge you to explore</span><a id="_idIndexMarker1027"/><span class="koboSpan" id="kobo.552.1"> these libraries for more</span><a id="_idIndexMarker1028"/><span class="koboSpan" id="kobo.553.1"> information. </span></p>
			<p><span class="koboSpan" id="kobo.554.1">Often, you will not</span><a id="_idIndexMarker1029"/><span class="koboSpan" id="kobo.555.1"> have your data already existing in a usable format. </span><span class="koboSpan" id="kobo.555.2">In this tutorial, we had our dataset structured within a DataFrame, ready for use to slice</span><a id="_idIndexMarker1030"/><span class="koboSpan" id="kobo.556.1"> and dice. </span><span class="koboSpan" id="kobo.556.2">However, in many cases, our data of interest will be unstructured, such as in PDFs. </span><span class="koboSpan" id="kobo.556.3">We will explore how to handle situations such as these in the next section.</span></p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.557.1">Working with unstructured data</span></h1>
			<p><span class="koboSpan" id="kobo.558.1">In the previous section, we explored some of the most common tasks and processes that are conducted when</span><a id="_idIndexMarker1031"/><span class="koboSpan" id="kobo.559.1"> handing text-based data. </span><span class="koboSpan" id="kobo.559.2">More often than not, you will find that the data you work with is generally not of a structured nature, or perhaps not of a digital nature. </span><span class="koboSpan" id="kobo.559.3">Take, for example, a company that has decided to move all printed documents to a digital state. </span><span class="koboSpan" id="kobo.559.4">Or perhaps a company that maintains a large repository of documents, none of which are structured or organized. </span><span class="koboSpan" id="kobo.559.5">For tasks such as these, we can rely on several AWS products to come to our rescue. </span><span class="koboSpan" id="kobo.559.6">We will explore two of the most useful NLP tools in the next few sections.</span></p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.560.1">OCR using AWS Textract</span></h2>
			<p><span class="koboSpan" id="kobo.561.1">In my opinion, one of the most useful tools available within </span><strong class="bold"><span class="koboSpan" id="kobo.562.1">AWS</span></strong><span class="koboSpan" id="kobo.563.1"> is an </span><strong class="bold"><span class="koboSpan" id="kobo.564.1">Optical Character Recognition</span></strong><span class="koboSpan" id="kobo.565.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.566.1">OCR</span></strong><span class="koboSpan" id="kobo.567.1">) tool known as </span><strong class="bold"><span class="koboSpan" id="kobo.568.1">AWS Textract</span></strong><span class="koboSpan" id="kobo.569.1">. </span><span class="koboSpan" id="kobo.569.2">The main idea behind this tool is to enable users</span><a id="_idIndexMarker1032"/><span class="koboSpan" id="kobo.570.1"> to extract text, tables, and other useful items from images or static PDF documents</span><a id="_idIndexMarker1033"/><span class="koboSpan" id="kobo.571.1"> using pre-built machine learning models implemented within Textract.</span></p>
			<p><span class="koboSpan" id="kobo.572.1">For example, users can</span><a id="_idIndexMarker1034"/><span class="koboSpan" id="kobo.573.1"> upload images or scanned PDF</span><a id="_idIndexMarker1035"/><span class="koboSpan" id="kobo.574.1"> documents to Textract that are otherwise unsearchable and extract all the text-based content from them, as shown in the following diagram:</span></p>
			<div>
				<div id="_idContainer334" class="IMG---Figure">
					<span class="koboSpan" id="kobo.575.1"><img src="image/B17761_09_022.jpg" alt="Figure 9.22 – A schematic showing structuring raw PDFs into organized digital text "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.576.1">Figure 9.22 – A schematic showing structuring raw PDFs into organized digital text</span></p>
			<p><span class="koboSpan" id="kobo.577.1">In addition to extracting</span><a id="_idIndexMarker1036"/><span class="koboSpan" id="kobo.578.1"> text, users can extract</span><a id="_idIndexMarker1037"/><span class="koboSpan" id="kobo.579.1"> key-value pairs such as those found in both printed and handwritten forms:</span></p>
			<div>
				<div id="_idContainer335" class="IMG---Figure">
					<span class="koboSpan" id="kobo.580.1"><img src="image/B17761_09_023.jpg" alt="Figure 9.23 – A schematic showing the structuring of handwritten data to organized tables "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.581.1">Figure 9.23 – A schematic showing the structuring of handwritten data to organized tables</span></p>
			<p><span class="koboSpan" id="kobo.582.1">To use </span><strong class="bold"><span class="koboSpan" id="kobo.583.1">Textract</span></strong><span class="koboSpan" id="kobo.584.1"> in Python, we will need</span><a id="_idIndexMarker1038"/><span class="koboSpan" id="kobo.585.1"> to use an AWS Python library known as </span><strong class="source-inline"><span class="koboSpan" id="kobo.586.1">boto3</span></strong><span class="koboSpan" id="kobo.587.1">. </span><span class="koboSpan" id="kobo.587.2">We can install Boto3 using </span><strong class="source-inline"><span class="koboSpan" id="kobo.588.1">pip</span></strong><span class="koboSpan" id="kobo.589.1">. </span><span class="koboSpan" id="kobo.589.2">When using Boto3 within SageMaker, you will not be required to use any access keys to utilize the service. </span><span class="koboSpan" id="kobo.589.3">However, if you are using a local implementation of Jupyter Notebook, you will need to be able to authenticate using access keys. </span><strong class="bold"><span class="koboSpan" id="kobo.590.1">Access keys</span></strong><span class="koboSpan" id="kobo.591.1"> can easily be created in a few simple</span><a id="_idIndexMarker1039"/><span class="koboSpan" id="kobo.592.1"> steps:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.593.1">Navigate to the AWS console and select </span><strong class="bold"><span class="koboSpan" id="kobo.594.1">IAM</span></strong><span class="koboSpan" id="kobo.595.1"> from the </span><strong class="bold"><span class="koboSpan" id="kobo.596.1">Services</span></strong><span class="koboSpan" id="kobo.597.1"> menu.</span></li>
				<li><span class="koboSpan" id="kobo.598.1">Under the </span><strong class="bold"><span class="koboSpan" id="kobo.599.1">Access Management</span></strong><span class="koboSpan" id="kobo.600.1"> tab on the left, click on </span><strong class="bold"><span class="koboSpan" id="kobo.601.1">Users</span></strong><span class="koboSpan" id="kobo.602.1">, then </span><strong class="bold"><span class="koboSpan" id="kobo.603.1">Add Users</span></strong><span class="koboSpan" id="kobo.604.1">.</span></li>
				<li><span class="koboSpan" id="kobo.605.1">Go ahead and give your user a name, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.606.1">ml-biotech-user</span></strong><span class="koboSpan" id="kobo.607.1">, and enable the </span><strong class="bold"><span class="koboSpan" id="kobo.608.1">Programmatic access</span></strong><span class="koboSpan" id="kobo.609.1"> option:</span><div id="_idContainer336" class="IMG---Figure"><span class="koboSpan" id="kobo.610.1"><img src="image/B17761_09_024.jpg" alt="Figure 9.24 – Setting the username for AWS IAM roles "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.611.1">Figure 9.24 – Setting the username for AWS IAM roles</span></p></li>
				<li><span class="koboSpan" id="kobo.612.1">Next, select the </span><strong class="bold"><span class="koboSpan" id="kobo.613.1">Attach existing policies directly</span></strong><span class="koboSpan" id="kobo.614.1"> option at the top and add the policies</span><a id="_idIndexMarker1040"/><span class="koboSpan" id="kobo.615.1"> of interest. </span><span class="koboSpan" id="kobo.615.2">Go ahead and add Textract, Comprehend, and S3 as we will require all three for the role:</span><div id="_idContainer337" class="IMG---Figure"><span class="koboSpan" id="kobo.616.1"><img src="image/B17761_09_025.jpg" alt="Figure 9.25 – Setting the policies for AWS IAM roles "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.617.1">Figure 9.25 – Setting the policies for AWS IAM roles</span></p></li>
				<li><span class="koboSpan" id="kobo.618.1">After labeling your new</span><a id="_idIndexMarker1041"/><span class="koboSpan" id="kobo.619.1"> user with some descriptive tags, you</span><a id="_idIndexMarker1042"/><span class="koboSpan" id="kobo.620.1"> will gain access to two items: your </span><strong class="bold"><span class="koboSpan" id="kobo.621.1">access key ID </span></strong><span class="koboSpan" id="kobo.622.1">and</span><strong class="bold"><span class="koboSpan" id="kobo.623.1"> AWS secret access k</span></strong><span class="koboSpan" id="kobo.624.1">. </span><span class="koboSpan" id="kobo.624.2">Be sure to copy those two items to a safe space. </span><span class="koboSpan" id="kobo.624.3">For security, you will not be able to retrieve</span><a id="_idIndexMarker1043"/><span class="koboSpan" id="kobo.625.1"> them from AWS after leaving this page.</span></li>
			</ol>
			<p><span class="koboSpan" id="kobo.626.1">Now that we have our access keys, let's go ahead and start implementing Textract on a document of interest. </span><span class="koboSpan" id="kobo.626.2">We can complete this in a few steps. </span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.627.1">First, we will need to upload our data to our </span><strong class="bold"><span class="koboSpan" id="kobo.628.1">S3 bucket</span></strong><span class="koboSpan" id="kobo.629.1">. </span><span class="koboSpan" id="kobo.629.2">We can recycle the same S3 bucket we used earlier</span><a id="_idIndexMarker1044"/><span class="koboSpan" id="kobo.630.1"> in this book. </span><span class="koboSpan" id="kobo.630.2">We will need to specify our keys, and then</span><a id="_idIndexMarker1045"/><span class="koboSpan" id="kobo.631.1"> connect to AWS using the Boto3 client:</span><p class="source-code"><span class="koboSpan" id="kobo.632.1">AWS_ACCESS_KEY_ID = "add-access-key-here"</span></p><p class="source-code"><span class="koboSpan" id="kobo.633.1">AWS_SECRET_ACCESS_KEY = "add-secret-access-key-here"</span></p><p class="source-code"><span class="koboSpan" id="kobo.634.1">AWS_REGION = "us-east-2"</span></p><p class="source-code"><span class="koboSpan" id="kobo.635.1">s3_client = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name=AWS_REGION)</span></p></li>
				<li><span class="koboSpan" id="kobo.636.1">With our connection set up, we can go ahead and upload a sample PDF file. </span><span class="koboSpan" id="kobo.636.2">Note that you can submit PDFs as well as image files (PNG). </span><span class="koboSpan" id="kobo.636.3">Let's go ahead and upload our PDF file using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.637.1">upload_fileobj()</span></strong><span class="koboSpan" id="kobo.638.1"> function:</span><p class="source-code"><span class="koboSpan" id="kobo.639.1">with open("Monoclonal Production Article.pdf", "rb") as f:</span></p><p class="source-code"><span class="koboSpan" id="kobo.640.1">    s3_client.upload_fileobj(f, "biotech-machine-learning", "pdfs/Monoclonal Production Article.pdf")</span></p></li>
				<li><span class="koboSpan" id="kobo.641.1">With our PDF now uploaded, we can use Textract. </span><span class="koboSpan" id="kobo.641.2">First, we will need to connect using the Boto3 client. </span><span class="koboSpan" id="kobo.641.3">Note that we changed the desired resource from </span><strong class="source-inline"><span class="koboSpan" id="kobo.642.1">'s3'</span></strong><span class="koboSpan" id="kobo.643.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">'textract'</span></strong><span class="koboSpan" id="kobo.645.1"> since we are using a different service now:</span><p class="source-code"><span class="koboSpan" id="kobo.646.1">textract_client = boto3.client('textract', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name=AWS_REGION)</span></p></li>
				<li><span class="koboSpan" id="kobo.647.1">Next, we can send our file to Textract using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.648.1">start_document_text_detection()</span></strong><span class="koboSpan" id="kobo.649.1"> method, where we specify the name of the bucket and the name of the document:</span><p class="source-code"><span class="koboSpan" id="kobo.650.1">response = textract_client.start_document_text_detection(</span></p><p class="source-code"><span class="koboSpan" id="kobo.651.1">                   DocumentLocation={'S3Object': {'Bucket': "biotech-machine-learning", 'Name': "pdfs/Monoclonal Production Article.pdf"} })</span></p></li>
				<li><span class="koboSpan" id="kobo.652.1">We can confirm that the task was started successfully by checking the status code in the response</span><a id="_idIndexMarker1046"/><span class="koboSpan" id="kobo.653.1"> variable. </span><span class="koboSpan" id="kobo.653.2">After a few moments (depending on the duration of the job), we retrieve the results by specifying </span><strong class="source-inline"><span class="koboSpan" id="kobo.654.1">JobId</span></strong><span class="koboSpan" id="kobo.655.1">:</span><p class="source-code"><span class="koboSpan" id="kobo.656.1">results = textract_client.get_document_text_detection(JobId=response["JobId"])</span></p><p><span class="koboSpan" id="kobo.657.1">Immediately, we will notice that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.658.1">results</span></strong><span class="koboSpan" id="kobo.659.1"> variable is simply one large JSON that we can parse and iterate over. </span><span class="koboSpan" id="kobo.659.2">Notice that the structure of the JSON is quite complex and detailed. </span></p></li>
				<li><span class="koboSpan" id="kobo.660.1">Finally, we can gather all the text by iterating over </span><strong class="source-inline"><span class="koboSpan" id="kobo.661.1">Blocks</span></strong><span class="koboSpan" id="kobo.662.1"> and collecting all the texts for blocks of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.663.1">LINE</span></strong><span class="koboSpan" id="kobo.664.1"> type:</span><p class="source-code"><span class="koboSpan" id="kobo.665.1">documentText = ""</span></p><p class="source-code"><span class="koboSpan" id="kobo.666.1">for item in results["Blocks"]:</span></p><p class="source-code"><span class="koboSpan" id="kobo.667.1">    if item["BlockType"] == "LINE":</span></p><p class="source-code"><span class="koboSpan" id="kobo.668.1">        documentText = documentText + item["Text"]</span></p></li>
			</ol>
			<p><span class="koboSpan" id="kobo.669.1">If you print the </span><strong class="source-inline"><span class="koboSpan" id="kobo.670.1">documentText</span></strong><span class="koboSpan" id="kobo.671.1"> variable, you will see all of the text that was successfully collected from that document! </span><span class="koboSpan" id="kobo.671.2">Textract can be an extremely useful tool for moving documents</span><a id="_idIndexMarker1047"/><span class="koboSpan" id="kobo.672.1"> from an unstructured and unsearchable state to a more structured and searchable state. </span><span class="koboSpan" id="kobo.672.2">Often, most text-based data will exist in an unstructured format, and you will find Textract to be one of the most useful resources for these types of applications. </span><span class="koboSpan" id="kobo.672.3">Textract is generally coupled with other AWS resources to maximize the utility of the tool, such as </span><strong class="bold"><span class="koboSpan" id="kobo.673.1">DynamoDB</span></strong><span class="koboSpan" id="kobo.674.1"> for storage or </span><strong class="bold"><span class="koboSpan" id="kobo.675.1">Comprehend</span></strong><span class="koboSpan" id="kobo.676.1"> for analysis. </span><span class="koboSpan" id="kobo.676.2">We will explore Comprehend in the next section.</span></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.677.1">Entity recognition using AWS Comprehend</span></h2>
			<p><span class="koboSpan" id="kobo.678.1">Earlier in this chapter, we implemented a NER</span><a id="_idIndexMarker1048"/><span class="koboSpan" id="kobo.679.1"> model using the SciPy library to detect entities in a given section of text. </span><span class="koboSpan" id="kobo.679.2">Now, let's explore a more powerful implementation of NER known as </span><strong class="bold"><span class="koboSpan" id="kobo.680.1">AWS Comprehend</span></strong><span class="koboSpan" id="kobo.681.1">. </span><span class="koboSpan" id="kobo.681.2">Comprehend is an NLP service</span><a id="_idIndexMarker1049"/><span class="koboSpan" id="kobo.682.1"> that's designed to discover</span><a id="_idIndexMarker1050"/><span class="koboSpan" id="kobo.683.1"> insights in unstructured text data, allowing</span><a id="_idIndexMarker1051"/><span class="koboSpan" id="kobo.684.1"> users to extract key phrases, calculate sentiment, identify entities, and much more. </span><span class="koboSpan" id="kobo.684.2">Let's go ahead and explore this tool.</span></p>
			<p><span class="koboSpan" id="kobo.685.1">Similar to other </span><strong class="bold"><span class="koboSpan" id="kobo.686.1">AWS</span></strong><span class="koboSpan" id="kobo.687.1"> resources, we will need to connect using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.688.1">boto3</span></strong><span class="koboSpan" id="kobo.689.1"> client:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.690.1">comprehend_client = boto3.client('comprehend', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name=AWS_REGION)</span></p>
			<p><span class="koboSpan" id="kobo.691.1">Next, we can go ahead and use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.692.1">detect_entities()</span></strong><span class="koboSpan" id="kobo.693.1"> function to identify entities in our text. </span><span class="koboSpan" id="kobo.693.2">We can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.694.1">documentText</span></strong><span class="koboSpan" id="kobo.695.1"> string we generated using Textract earlier:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.696.1">response = comprehend_client.detect_entities(</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.697.1">    Text=documentText[:5000],</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.698.1">    LanguageCode='en',</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.699.1">)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.700.1">print(response["Entities"])</span></p>
			<p><span class="koboSpan" id="kobo.701.1">Upon printing the response, we will see the results for each of the entities that were detected in our block of text. </span><span class="koboSpan" id="kobo.701.2">In addition, we can organize the results in a DataFrame:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.702.1">pd.DataFrame(response["Entities"]).sort_values(by='Score', ascending=False).head()</span></p>
			<p><span class="koboSpan" id="kobo.703.1">Upon sorting the</span><a id="_idIndexMarker1052"/><span class="koboSpan" id="kobo.704.1"> values by score, we can see our results listed</span><a id="_idIndexMarker1053"/><span class="koboSpan" id="kobo.705.1"> in a structured manner:</span></p>
			<div>
				<div id="_idContainer338" class="IMG---Figure">
					<span class="koboSpan" id="kobo.706.1"><img src="image/B17761_09_026.jpg" alt="Figure 9.26 – A sample DataFrame showing the results of the AWS Comprehend entities API "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.707.1">Figure 9.26 – A sample DataFrame showing the results of the AWS Comprehend entities API</span></p>
			<p><span class="koboSpan" id="kobo.708.1">In addition to entities, Comprehend can also detect key phrases within text:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.709.1">response = comprehend_client.detect_key_phrases(</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.710.1">    Text=documentText[:5000],</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.711.1">    LanguageCode='en',</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.712.1">)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.713.1">response["KeyPhrases"][0]</span></p>
			<p><span class="koboSpan" id="kobo.714.1">Upon printing the first item in the list, we can see the score, phrase, and position in the string:</span></p>
			<div>
				<div id="_idContainer339" class="IMG---Figure">
					<span class="koboSpan" id="kobo.715.1"><img src="image/B17761_09_027.jpg" alt="Figure 9.27 – Results of the AWS Comprehend key phrases API "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.716.1">Figure 9.27 – Results of the AWS Comprehend key phrases API</span></p>
			<p><span class="koboSpan" id="kobo.717.1">In addition, we can also detect the sentiment using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.718.1">detect_sentiment()</span></strong><span class="koboSpan" id="kobo.719.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.720.1">response = comprehend_client.detect_sentiment(</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.721.1">    Text=documentText[:5000],</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.722.1">    LanguageCode='en',</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.723.1">)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.724.1">print(response)</span></p>
			<p><span class="koboSpan" id="kobo.725.1">We can print the response variable to get the results of the string. </span><span class="koboSpan" id="kobo.725.2">We can see that the sentiment was noted</span><a id="_idIndexMarker1054"/><span class="koboSpan" id="kobo.726.1"> as neutral, which makes sense</span><a id="_idIndexMarker1055"/><span class="koboSpan" id="kobo.727.1"> for a statement concerning scientific data that is generally not written with a positive or negative tone:</span></p>
			<div>
				<div id="_idContainer340" class="IMG---Figure">
					<span class="koboSpan" id="kobo.728.1"><img src="image/B17761_09_028.jpg" alt="Figure 9.28 – Results of the AWS Comprehend sentiment API "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.729.1">Figure 9.28 – Results of the AWS Comprehend sentiment API</span></p>
			<p><span class="koboSpan" id="kobo.730.1">Lastly, Comprehend can also detect dominant languages within text using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.731.1">detect_dominant_language()</span></strong><span class="koboSpan" id="kobo.732.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.733.1">response = comprehend_client.detect_dominant_language(</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.734.1">    Text=documentText[:5000],</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.735.1">)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.736.1">response</span></p>
			<p><span class="koboSpan" id="kobo.737.1">Here, we can see that, upon printing the response, we get a sense of the language, as well as the associated score or probability from the model:</span></p>
			<div>
				<div id="_idContainer341" class="IMG---Figure">
					<span class="koboSpan" id="kobo.738.1"><img src="image/B17761_09_029.jpg" alt="Figure 9.29 – Results of the AWS Comprehend language detection API "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.739.1">Figure 9.29 – Results of the AWS Comprehend language detection API</span></p>
			<p><span class="koboSpan" id="kobo.740.1">AWS Textract and AWS Comprehend are two of the top NLP tools available today and have been instrumental</span><a id="_idIndexMarker1056"/><span class="koboSpan" id="kobo.741.1"> in structuring and analyzing</span><a id="_idIndexMarker1057"/><span class="koboSpan" id="kobo.742.1"> vast amounts of unstructured text documents. </span><span class="koboSpan" id="kobo.742.2">Most NLP-based applications today generally use at least one, if not both, of these types</span><a id="_idIndexMarker1058"/><span class="koboSpan" id="kobo.743.1"> of technologies. </span><span class="koboSpan" id="kobo.743.2">For more information</span><a id="_idIndexMarker1059"/><span class="koboSpan" id="kobo.744.1"> about Textract and Comprehend, I highly recommend that you visit the AWS website (</span><a href="https://aws.amazon.com/"><span class="koboSpan" id="kobo.745.1">https://aws.amazon.com/</span></a><span class="koboSpan" id="kobo.746.1">).</span></p>
			<p><span class="koboSpan" id="kobo.747.1">So far, we have learned how to analyze and transform text-based data, especially when it comes to moving data from an unstructured state to a more structured state. </span><span class="koboSpan" id="kobo.747.2">Now that the documents are more organized, the next step is to be able to use them in one way or another, such as through a search engine. </span><span class="koboSpan" id="kobo.747.3">We will learn how to create a </span><strong class="bold"><span class="koboSpan" id="kobo.748.1">semantic search</span></strong><span class="koboSpan" id="kobo.749.1"> engine using </span><strong class="bold"><span class="koboSpan" id="kobo.750.1">transformers</span></strong><span class="koboSpan" id="kobo.751.1"> in the next section.</span></p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.752.1">Tutorial – developing a scientific data search engine using transformers</span></h1>
			<p><span class="koboSpan" id="kobo.753.1">So far, we have looked</span><a id="_idIndexMarker1060"/><span class="koboSpan" id="kobo.754.1"> at text from a word-by-word perspective</span><a id="_idIndexMarker1061"/><span class="koboSpan" id="kobo.755.1"> in the sense that we kept our text </span><em class="italic"><span class="koboSpan" id="kobo.756.1">as is</span></em><span class="koboSpan" id="kobo.757.1">, without the need to convert or embed it in any way. </span><span class="koboSpan" id="kobo.757.2">In some cases, converting words into numerical values or </span><strong class="bold"><span class="koboSpan" id="kobo.758.1">embeddings</span></strong><span class="koboSpan" id="kobo.759.1"> can open many new doors and unlock many new possibilities, especially when it comes to deep learning. </span><span class="koboSpan" id="kobo.759.2">Our main objective within this tutorial will be to develop a search engine to find and retrieve scientific data. </span><span class="koboSpan" id="kobo.759.3">We will do so by implementing</span><a id="_idIndexMarker1062"/><span class="koboSpan" id="kobo.760.1"> an important and useful deep learning NLP architecture known as a transformer. </span><span class="koboSpan" id="kobo.760.2">The main benefit here is that we will be designing a powerful semantic search engine in the sense that we can now search for ideas or semantic meaning rather than only keywords.</span></p>
			<p><span class="koboSpan" id="kobo.761.1">We can think of transformers as deep learning models designed to solve sequence-based</span><a id="_idIndexMarker1063"/><span class="koboSpan" id="kobo.762.1"> tasks using a mechanism known as </span><strong class="bold"><span class="koboSpan" id="kobo.763.1">self-attention</span></strong><span class="koboSpan" id="kobo.764.1">. </span><span class="koboSpan" id="kobo.764.2">We can think of self-attention as a method to help relate different portions of text within a sentence or embedding in an attempt to create a representation. </span><span class="koboSpan" id="kobo.764.3">Simply put, the model attempts to view sentences as ideas, rather than a collection of single words.</span></p>
			<p><span class="koboSpan" id="kobo.765.1">Before we begin to work with transformers, let's talk a little more about the idea of </span><strong class="bold"><span class="koboSpan" id="kobo.766.1">embeddings</span></strong><span class="koboSpan" id="kobo.767.1">. </span><span class="koboSpan" id="kobo.767.2">We can think of embeddings</span><a id="_idIndexMarker1064"/><span class="koboSpan" id="kobo.768.1"> as low-dimensional numerical values or vectors of continuous</span><a id="_idIndexMarker1065"/><span class="koboSpan" id="kobo.769.1"> numbers representing an item, which in our case</span><a id="_idIndexMarker1066"/><span class="koboSpan" id="kobo.770.1"> would be a word or sentence. </span><span class="koboSpan" id="kobo.770.2">We commonly convert words and sentences into embeddings to allow models to carry out machine learning tasks more easily when working with larger datasets. </span><span class="koboSpan" id="kobo.770.3">Within the context of NLP and neural networks, there are three main reasons embeddings</span><a id="_idIndexMarker1067"/><span class="koboSpan" id="kobo.771.1"> are used:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.772.1">To reduce the </span><strong class="bold"><span class="koboSpan" id="kobo.773.1">dimensionality</span></strong><span class="koboSpan" id="kobo.774.1"> of large segments of text data</span></li>
				<li><span class="koboSpan" id="kobo.775.1">To compute the </span><strong class="bold"><span class="koboSpan" id="kobo.776.1">similarity</span></strong><span class="koboSpan" id="kobo.777.1"> between two different texts</span></li>
				<li><span class="koboSpan" id="kobo.778.1">To </span><strong class="bold"><span class="koboSpan" id="kobo.779.1">visualize</span></strong><span class="koboSpan" id="kobo.780.1"> relationships between portions of text</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.781.1">Now that we have gained a better sense of embeddings and the role they play in NLP, let's go ahead and get started with a real-world example of a scientific search engine. </span><span class="koboSpan" id="kobo.781.2">We will begin by importing a few libraries that we will need:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.782.1">import scipy</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.783.1">import torch</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.784.1">import pandas as pd</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.785.1">from sentence_transformers import SentenceTransformer, util</span></p>
			<p><span class="koboSpan" id="kobo.786.1">To create our embeddings, we will need a model. </span><span class="koboSpan" id="kobo.786.2">We have the option to create a customized model concerning our dataset. </span><span class="koboSpan" id="kobo.786.3">The benefit here is that our results would likely improve, given that the model was trained on text about our domain. </span><span class="koboSpan" id="kobo.786.4">Alternatively, we could use other pre-trained models</span><a id="_idIndexMarker1068"/><span class="koboSpan" id="kobo.787.1"> available from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.788.1">SentenceTransformer</span></strong><span class="koboSpan" id="kobo.789.1"> website (</span><a href="https://www.sbert.net/"><span class="koboSpan" id="kobo.790.1">https://www.sbert.net/</span></a><span class="koboSpan" id="kobo.791.1">). </span><span class="koboSpan" id="kobo.791.2">Let's download one of these pre-trained models:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.792.1">model = SentenceTransformer('msmarco-distilbert-base-v4')</span></p>
			<p><span class="koboSpan" id="kobo.793.1">Next, we can create a testing database and populate it with a few sentences:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.794.1">database = df["abstract"].values </span></p>
			<p><span class="koboSpan" id="kobo.795.1">Next, we can call the </span><strong class="source-inline"><span class="koboSpan" id="kobo.796.1">encode()</span></strong><span class="koboSpan" id="kobo.797.1"> function to convert our list of strings into a list of embeddings:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.798.1">database_embeddings = model.encode(database)</span></p>
			<p><span class="koboSpan" id="kobo.799.1">If we check the length of the database and the length of </span><strong class="source-inline"><span class="koboSpan" id="kobo.800.1">database_embeddings</span></strong><span class="koboSpan" id="kobo.801.1"> using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.802.1">len()</span></strong><span class="koboSpan" id="kobo.803.1"> function, we will find that they both contain the same number of elements since there should</span><a id="_idIndexMarker1069"/><span class="koboSpan" id="kobo.804.1"> be one embedding for every piece</span><a id="_idIndexMarker1070"/><span class="koboSpan" id="kobo.805.1"> of text. </span><span class="koboSpan" id="kobo.805.2">If we print the contents of the first element of the embeddings database, we will find that the content is now simply a list of vectors:</span></p>
			<div>
				<div id="_idContainer342" class="IMG---Figure">
					<span class="koboSpan" id="kobo.806.1"><img src="image/B17761_09_030.jpg" alt="Figure 9.30 – A view of an embedded piece of text "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.807.1">Figure 9.30 – A view of an embedded piece of text</span></p>
			<p><span class="koboSpan" id="kobo.808.1">With each of our documents now embedded, the idea would be that a user would want to search for or query a particular phrase. </span><span class="koboSpan" id="kobo.808.2">We can take a user's query and encode it as we did with the others, but assign that value to a new variable that we will call </span><strong class="source-inline"><span class="koboSpan" id="kobo.809.1">query_embedding</span></strong><span class="koboSpan" id="kobo.810.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.811.1">query = "One of the best discoveries were monoclonal antibodies"</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.812.1">query_embedding = model.encode(query)</span></p>
			<p><span class="koboSpan" id="kobo.813.1">With the query and sentences embedded, we can compute the distance between the items. </span><span class="koboSpan" id="kobo.813.2">The idea here is that documents that were more similar to the user's query would have shorter distances, and those that were less similar would have longer ones. </span><span class="koboSpan" id="kobo.813.3">Notice that we are using cosine here as a measure of distance, and therefore, similarity. </span><span class="koboSpan" id="kobo.813.4">We can use other methods as well, such as the </span><strong class="source-inline"><span class="koboSpan" id="kobo.814.1">euclidean</span></strong><span class="koboSpan" id="kobo.815.1"> distance:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.816.1">import scipy</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.817.1">cos_scores = util.pytorch_cos_sim(query_embedding, </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.818.1">                             database_embeddings)[0]</span></p>
			<p><span class="koboSpan" id="kobo.819.1">Let's go ahead and prepare a single </span><strong class="source-inline"><span class="koboSpan" id="kobo.820.1">runSearch</span></strong><span class="koboSpan" id="kobo.821.1"> function that incorporates the query, the encoder, as well as a method</span><a id="_idIndexMarker1071"/><span class="koboSpan" id="kobo.822.1"> to display our results. </span><span class="koboSpan" id="kobo.822.2">The process begins</span><a id="_idIndexMarker1072"/><span class="koboSpan" id="kobo.823.1"> with a few print statements, and then encodes the new query into a variable called </span><strong class="source-inline"><span class="koboSpan" id="kobo.824.1">query_embedding</span></strong><span class="koboSpan" id="kobo.825.1">. </span><span class="koboSpan" id="kobo.825.2">The distances are then calculated, and the results are sorted according to their distance. </span><span class="koboSpan" id="kobo.825.3">Finally, the results are iterated over and the scores, titles, and abstracts for each are printed:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.826.1">def askQuestion(query, top_k):</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.827.1">    print(f"#########################################")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.828.1">    print(f"#### {query} ####")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.829.1">    print(f"#########################################")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.830.1">    query_embedding = model.encode(query, convert_to_tensor=True)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.831.1">    cos_scores = util.pytorch_cos_sim(query_embedding, </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.832.1">                 database_embeddings)[0]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.833.1">    top_results = torch.topk(cos_scores, k=top_k)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.834.1">    </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.835.1">    for score, idx in zip(top_results[0], top_results[1]):</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.836.1">        print("#### Score: {:.4f}".format(score))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.837.1">        print("#### Title: ", df.loc[float(idx)].title)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.838.1">        print("#### Abstract: ", df.loc[float(idx)].abstract)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.839.1">        print("#################################")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.840.1">        </span></p>
			<p><span class="koboSpan" id="kobo.841.1">Now that we have prepared our function, we can call it with our query of interest:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.842.1">query = ' What is known about the removal of harmful cyanobacteria?</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.843.1">askQuestion(query, 5)        </span></p>
			<p><span class="koboSpan" id="kobo.844.1">Upon calling the</span><a id="_idIndexMarker1073"/><span class="koboSpan" id="kobo.845.1"> function, we retrieve</span><a id="_idIndexMarker1074"/><span class="koboSpan" id="kobo.846.1"> several results printed similarly. </span><span class="koboSpan" id="kobo.846.2">We can see one of the results in the following screenshot, showing us the </span><strong class="source-inline"><span class="koboSpan" id="kobo.847.1">score</span></strong><span class="koboSpan" id="kobo.848.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.849.1">title</span></strong><span class="koboSpan" id="kobo.850.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.851.1">abstract</span></strong><span class="koboSpan" id="kobo.852.1"> properties of the article:</span></p>
			<div>
				<div id="_idContainer343" class="IMG---Figure">
					<span class="koboSpan" id="kobo.853.1"><img src="image/B17761_09_031.jpg" alt="Figure 9.31 – Results of the semantic searching model for scientific text "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.854.1">Figure 9.31 – Results of the semantic searching model for scientific text</span></p>
			<p><span class="koboSpan" id="kobo.855.1">With that, we have managed to successfully develop a semantic searching model capable of searching through scientific literature. </span><span class="koboSpan" id="kobo.855.2">Notice that the query itself is not a direct string match to the top result the model returned. </span><span class="koboSpan" id="kobo.855.3">Again, the idea here is not to match keywords but to calculate the distance between embeddings, which is representative of similarities.</span></p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.856.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.857.1">In this chapter, we made an adventurous attempt to cover a wide range of NLP topics. </span><span class="koboSpan" id="kobo.857.2">We explored a range of introductory topics such as NER, tokenization, and parts of speech using the NLTK and spaCy libraries. </span><span class="koboSpan" id="kobo.857.3">We then explored NLP through the lens of structured datasets, in which we utilized the </span><strong class="source-inline"><span class="koboSpan" id="kobo.858.1">pymed</span></strong><span class="koboSpan" id="kobo.859.1"> library as a source for scientific literature and proceeded to analyze and clean the data in our preprocessing steps. </span><span class="koboSpan" id="kobo.859.2">Next, we developed a word cloud to visualize the frequency of words in a given dataset. </span><span class="koboSpan" id="kobo.859.3">Finally, we developed a clustering model to group our abstracts and a topic modeling model to identify prominent topics.</span></p>
			<p><span class="koboSpan" id="kobo.860.1">We then explored NLP through the lens of unstructured data in which we explored two common AWS NLP products. </span><span class="koboSpan" id="kobo.860.2">We used Textract to convert PDFs and images into searchable and structured text and Comprehend to analyze and provide insights. </span><span class="koboSpan" id="kobo.860.3">Finally, we learned how to develop a semantic search engine using deep learning transformers to find pertinent information.</span></p>
			<p><span class="koboSpan" id="kobo.861.1">What was particularly unique about this chapter is that we learned that text is a sequence-based type of data, which makes its uses and applications drastically different from many of the other datasets we had previously worked with. </span><span class="koboSpan" id="kobo.861.2">As companies across the world begin to migrate legacy documents into the digital space, the ability to search for documents and identify insights will be of great value. </span><span class="koboSpan" id="kobo.861.3">In the next chapter, we will examine another type of sequence-based data known as time series.</span></p>
		</div>
	</body></html>