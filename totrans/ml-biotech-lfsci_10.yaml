- en: 'Chapter 8: Understanding Deep Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章：理解深度学习
- en: Throughout this book, we have examined the many tools and methods within the
    fields of supervised and unsupervised machine learning. Within the field of unsupervised
    learning, we explored **clustering** and **dimensionality reduction**, while within
    the field of supervised learning, we explored **classification** and **regression**.
    Within all of these fields, we explored many of the most popular algorithms for
    developing powerful predictive models for our datasets. However, as we have seen
    with some of the data we have worked with, there are numerous limitations when
    it comes to these models' performance that cannot be overcome by additional tuning
    and hyperparameter optimization. In cases such as these, data scientists often
    turn to the field of **deep learning**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们考察了监督学习和无监督机器学习领域的许多工具和方法。在无监督学习领域，我们探讨了**聚类**和**降维**，而在监督学习领域，我们探讨了**分类**和**回归**。在所有这些领域，我们探讨了为我们的数据集开发强大预测模型的最流行算法。然而，正如我们处理的一些数据所看到的，这些模型的表现存在许多限制，这些限制不能通过额外的调整和超参数优化来克服。在这些情况下，数据科学家通常会转向**深度学习**领域。
- en: If you recall our overarching diagram of the artificial intelligence space that
    we saw in *Chapter 5*, *Introduction to Machine Learning*, we noted that the overall
    space is known as **Artificial Intelligence** (**AI**). Within the AI space, we
    defined machine learning as the ability to develop models to learn or generalize
    from data and make predictions. We will now explore a subset of machine learning
    known as **deep learning**, which focuses on developing models and extracting
    patterns within data using deep neural networks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得我们在第5章“机器学习简介”中看到的整体人工智能空间图，我们会注意到这个整体空间被称为**人工智能**（**AI**）。在人工智能空间内，我们将机器学习定义为开发模型从数据中学习或泛化并做出预测的能力。现在，我们将探讨机器学习的一个子集，称为**深度学习**，它专注于使用深度神经网络开发和提取数据中的模式。
- en: 'Throughout this chapter, we will explore the ideas of neural networks and deep
    learning as they relate to the field of biotechnology. In particular, we will
    be covering the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨神经网络和深度学习与生物技术领域的相关思想。特别是，我们将涵盖以下主题：
- en: Understanding the field of deep learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解深度学习领域
- en: Exploring the types of deep learning models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索深度学习模型的类型
- en: Selecting an activation function
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择激活函数
- en: Measuring progress with loss
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用损失来衡量进度
- en: Developing models with the Keras library
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras库开发模型
- en: Tutorial – protein sequence classification via LSTMs using Keras and MLflow
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教程 – 使用Keras和MLflow通过LSTMs进行蛋白质序列分类
- en: Tutorial – anomaly detection using AWS Lookout for Vision
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教程 – 使用AWS Lookout for Vision进行异常检测
- en: With these sections in mind, let's get started!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些部分，让我们开始吧！
- en: Understanding the field of deep learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解深度学习领域
- en: As we mentioned in the introduction, deep learning is a subset or branch of
    the machine learning space that focuses on developing models using neural networks.
    The idea behind using neural networks for deep learning derives from neural networks
    found in the human brain. Let's learn more about this.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在引言中提到的，深度学习是机器学习空间的一个子集或分支，它专注于使用神经网络开发模型。使用神经网络进行深度学习的理念源于人类大脑中的神经网络。让我们了解更多关于这个内容。
- en: Neural networks
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'Similar to machine learning, the idea behind developing deep learning models
    is not to explicitly define the steps in which a decision or prediction is made.
    The main idea here is to generalize from the data. Deep learning makes this possible
    by drawing a parallel between the dendrites, cell body, and synapses of the human
    brain, which, within the context of deep learning, act as inputs, nodes, and outputs
    for a given model, as shown in the following diagram:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与机器学习类似，开发深度学习模型的理念不是明确定义决策或预测的步骤。这里的主要思想是从数据中泛化。深度学习通过将人脑的树突、细胞体和突触之间的平行关系，在深度学习的背景下，它们作为给定模型的输入、节点和输出，如图所示，使得这一点成为可能：
- en: '![Figure 8.1 – Comparison between the human brain and a neural network ](img/B17761_08_001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 人脑与神经网络的比较](img/B17761_08_001.jpg)'
- en: Figure 8.1 – Comparison between the human brain and a neural network
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 人脑与神经网络的比较
- en: Some of the biggest benefits behind such an implementation revolve around the
    idea of feature engineering. Earlier in this book, we saw how features can be
    created or summarized using various methods such as basic mathematical operations
    (x2) or through complex algorithms such as **Principal Component Analysis** (**PCA**).
    Manually engineered features can be very time-consuming and not feasible in practice,
    which is where the field of deep learning can come in, with the ability to learn
    the many underlying features in a given dataset directly from the data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实现背后的最大好处之一是特征工程的概念。在这本书的早期，我们看到了如何使用各种方法创建或总结特征，例如基本的数学运算（x2）或通过如**主成分分析**（PCA）等复杂算法。手动构建的特征可能非常耗时，在实践中不可行，这就是深度学习领域可以发挥作用的地方，它能够直接从数据中学习给定数据集的许多潜在特征。
- en: Within the field of **biotechnology**, most applications, ranging from the early
    stages of therapeutic discovery all the way downstream to manufacturing, are generally
    data-rich processes. However, much of the data that's been collected will have
    little to no use on its own, or perhaps the data that's been collected is for
    different batches of a particular molecule. Perhaps the data is extensive for
    some molecules and less extensive for others. In many of these cases, using deep
    learning models can come to your aid when it comes to features that are relative
    to the traditional machine learning models we have discussed so far.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物技术领域，大多数应用，从治疗发现的早期阶段到下游的制造，通常是数据丰富的过程。然而，收集到的许多数据本身可能几乎没有任何用途，或者收集的数据可能是针对特定分子的不同批次。也许对于某些分子数据量很大，而对于其他分子则较少。在这些许多情况下，使用深度学习模型可以帮助我们处理与之前讨论的传统机器学习模型相关的特征。
- en: 'We can think of features at three different levels:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将特征视为三个不同级别：
- en: '**Low-level features**, such as individual amino acids, a protein, or the elements
    of a small molecule.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低级特征**，例如单个氨基酸、一个蛋白质或小分子的元素。'
- en: '**Mid-level features**, such as the amino acid sequences of a protein and the
    functional groups of a small molecule.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中级特征**，例如蛋白质的氨基酸序列和小分子的官能团。'
- en: '**High-level features**, such as the overall structure or the classification
    of a protein or the geometric shape of a small molecule.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级特征**，例如蛋白质的整体结构或分类，或小分子的几何形状。'
- en: 'The following diagram shows a graphical representation of these features:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这些特征的图形表示：
- en: '![Figure 8.2 – The three types of features and some associated examples ](img/B17761_08_002.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 三种类型的特征及其一些相关示例](img/B17761_08_002.jpg)'
- en: Figure 8.2 – The three types of features and some associated examples
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 三种类型的特征及其一些相关示例
- en: 'In many instances, architecting a robust deep learning model can unlock a more
    powerful predictive model relative to its machine learning counterpart. In many
    of the machine learning models we have explored, we attempted to improve the model''s
    performance not only by tuning and adjusting the hyperparameters but also by making
    a conscious decision to use datasets with a sufficient amount of data. Increasing
    the size of the datasets will likely not lead to any significant improvement in
    our machine learning models. However, this is not always the case with deep learning
    models, which tend to improve in performance when more data is made available.
    We can see a visual depiction of this in the following diagram:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，构建一个健壮的深度学习模型可以解锁相对于其机器学习对应模型更强大的预测模型。在我们已经探索的许多机器学习模型中，我们尝试通过调整和调整超参数来提高模型性能，并且有意识地决定使用具有足够数据量的数据集。增加数据集的大小可能不会导致我们的机器学习模型有任何显著的改进。然而，这并不总是适用于深度学习模型，当有更多数据可用时，深度学习模型往往会提高性能。我们可以在以下图表中看到这种视觉表示：
- en: '![Figure 8.3 – A graphical representation of machine learning versus deep learning
    ](img/B17761_08_003.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 机器学习与深度学习的图形表示](img/B17761_08_003.jpg)'
- en: Figure 8.3 – A graphical representation of machine learning versus deep learning
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 机器学习与深度学习的图形表示
- en: Using neural networks within the context of machine learning has seen a major
    surge in recent years, which can be attributed to the increased use of big data
    within most industries, the decreased expense of computational hardware such as
    CPUs and GPUs, and the growing community that supports much of the open source
    software and packages that are available today. Two of the most common packages
    out there for developing deep learning models are TensorFlow and Keras – we will
    explore these two later in this chapter. Before we do, let's go ahead and talk
    about the architecture behind a deep learning model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下使用神经网络在近年来经历了重大增长，这可以归因于大多数行业大数据使用的增加，计算硬件（如CPU和GPU）成本的降低，以及支持今天大多数开源软件和包的社区的增长。用于开发深度学习模型的最常见的两个包是TensorFlow和Keras
    – 我们将在本章后面探讨这两个包。在我们这样做之前，让我们先谈谈深度学习模型背后的架构。
- en: The perceptron
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知器
- en: 'One of the most important building blocks of any deep learning model is the
    perceptron. A perceptron is an algorithm that''s used for developing supervised
    binary classifiers, first invented in 1958 by Frank Rosenblatt, who is sometimes
    called the father of deep learning. A perceptron generally consists of four major
    parts:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型最重要的构建块之一是感知器。感知器是一种用于开发监督二分类算法的算法，由弗兰克·罗森布拉特于1958年首次发明，有时被称为深度学习的之父。感知器通常由四个主要部分组成：
- en: The **input** values, which are generally taken from a given dataset.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**值，通常是从给定的数据集中获取的。'
- en: The **weights**, which are values by which the input values are multiplied.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**，即乘以输入值的值。'
- en: The **net sum**, which is the sum of all the values from each of the inputs.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**净和**，即所有输入值的总和。'
- en: The **activation function**, which maps a resulting value to an output.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**，它将结果值映射到输出。'
- en: 'The following diagram shows a graphical representation of these four parts
    of a perceptron:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了感知器这四个部分的图形表示：
- en: '![Figure 8.4 – A graphical representation of a perceptron ](img/B17761_08_004.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – 一个感知器的图形表示](img/B17761_08_004.jpg)'
- en: Figure 8.4 – A graphical representation of a perceptron
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 一个感知器的图形表示
- en: 'There are three main steps that a perceptron takes to arrive at a predicted
    output from a given set of input values:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器到达给定输入值预测输出的三个主要步骤如下：
- en: The **input values** (x1, x2, and so on) are multiplied by their respective
    weights (w1, w2, and so on). These weights are determined in the training process
    for this model so that a different weight is assigned to each of the input values.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入值**（x1，x2等）乘以其相应的权重（w1，w2等）。这些权重在模型的训练过程中确定，以便为每个输入值分配不同的权重。'
- en: All the values from each of the calculations are summed together in a value
    known as the **weighted sum**.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次计算的所有值都汇总到一个称为**加权求和**的值中。
- en: The weighted sum is then applied to the **activation function** to map the value
    to a given output. The specific activation function that's used is dependent on
    the given situation. For example, within the context of a unit step activation
    function, values would either be mapped to 0 or 1.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将加权求和应用于**激活函数**，将值映射到给定的输出。所使用的特定激活函数取决于给定的情况。例如，在单位步长激活函数的背景下，值将被映射到0或1。
- en: 'When viewed from a mathematical perspective, we can define the output value,
    ![](img/Formula_B17761__08_016.png), as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，我们可以定义输出值，![](img/Formula_B17761__08_016.png)，如下所示：
- en: '![](img/Formula_B17761__08_001.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761__08_001.jpg)'
- en: 'In this equation, *g* is the activation function, *w*o is the bias, and the
    final components are the sum of the **linear combination** of input values:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*g*是激活函数，*w*o是偏置，最后的组成部分是输入值的**线性组合**的总和：
- en: '![](img/Formula_B17761__08_002.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761__08_002.jpg)'
- en: So, in this equation, ![](img/Formula_B17761__08_003.png) and ![](img/Formula_B17761__08_004.png)
    account for the final output value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个方程中，![](img/Formula_B17761__08_003.png)和![](img/Formula_B17761__08_004.png)考虑了最终的输出值。
- en: A perceptron is one of the simplest deep learning building blocks out there
    and can be expanded quite drastically by increasing the number of **hidden layers**.
    Hidden layers are the layers that lay in-between the input and output layers.
    Models with very few hidden layers are generally referred to as **neural networks**
    or multilayer perceptrons, whereas models with many hidden layers are referred
    to as **deep neural networks**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是现有最简单的深度学习构建块之一，可以通过增加**隐藏层**的数量来大幅扩展。隐藏层是位于输入层和输出层之间的层。具有很少隐藏层的模型通常被称为**神经网络**或多层感知器，而具有许多隐藏层的模型被称为**深度神经网络**。
- en: Each of these layers consists of several **nodes**, and the flow of data is
    similar to that of the perceptron we saw previously. The number of input nodes
    (*x*1*, x*2*, x*3) generally corresponds to the number of **features** in a given
    dataset, whereas the number of output nodes generally corresponds to the number
    of outputs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些每一层都由几个**节点**组成，数据流与之前看到的感知器类似。输入节点的数量（*x*1*, x*2*, x*3）通常对应于给定数据集中的**特征**数量，而输出节点的数量通常对应于输出数量。
- en: 'The following diagram is a graphical representation of the difference between
    neural networks and deep learning:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图是神经网络与深度学习之间差异的图形表示：
- en: '![Figure 8.5 – Difference between neural networks and deep learning ](img/B17761_08_005.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 神经网络与深度学习之间的差异](img/B17761_08_005.jpg)'
- en: Figure 8.5 – Difference between neural networks and deep learning
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 神经网络与深度学习之间的差异
- en: In the previous diagram, we can see the neural network or multilayer perceptron
    (on the left) consisting of an input layer, a single hidden layer with four nodes,
    and an output layer with four nodes. Similar to the single perceptron we saw earlier,
    the idea here is that each of the nodes within the hidden layer will intake the
    input nodes, multiplied by some value, and then pass them through an **activation
    function** to yield output. On the right, we can see a similar model, but the
    values are passed through several hidden layers before determining a final output
    value.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一张图中，我们可以看到由输入层、一个包含四个节点的单个隐藏层和一个包含四个节点的输出层组成的神经网络或多层感知器（在左侧）。与之前看到的单个感知器类似，这里的想法是隐藏层中的每个节点都会接收输入节点，乘以某个值，然后通过一个**激活函数**传递以产生输出。在右侧，我们可以看到一个类似模型，但值在确定最终输出值之前会通过几个隐藏层。
- en: Exploring the different types of deep learning models
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索不同类型的深度学习模型
- en: There are many different types of neural networks and deep learning architectures
    out there that differ in function, shape, data flow, and much more. There are
    three types of neural networks that have gained a great deal of popularity in
    recent years, given their promise and robustness with various types of data. First,
    we will explore the simplest of these architectures, known as a multilayer perceptron.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有各种各样的神经网络和深度学习架构，它们在功能、形状、数据流等方面有所不同。近年来，有三种类型的神经网络因其对各种类型数据的承诺和鲁棒性而获得了极大的普及。首先，我们将探讨这些架构中最简单的一种，被称为多层感知器。
- en: Multilayer perceptron
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'A **Multilayer Perceptron** (**MLP**) is one of the most basic types of **Artificial
    Neural Networks** (**ANNs**). This type of network is simply composed of layers
    in which data flows in a forward manner, as shown in the previous diagram. Data
    flows from the input layer to one or more hidden layers, and then finally to an
    output layer in which a prediction is produced. In essence, each layer attempts
    to learn and calculate certain weights. ANNs and MLPs come in many different shapes
    and sizes: they can have a different number of nodes in each layer, a different
    number of inputs, or even a different number of outputs. We can see a visual depiction
    of this in the following diagram:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）是**人工神经网络**（**ANNs**）中最基本的一种类型。这种网络简单地由层组成，数据以正向方式在这些层中流动，如前一张图所示。数据从输入层流向一个或多个隐藏层，然后最终流向输出层，在那里产生预测。本质上，每一层都试图学习和计算某些权重。ANNs和MLPs有多种不同的形状和大小：它们可以在每一层有不同的节点数、不同的输入数，甚至不同的输出数。我们可以在以下图中看到这种视觉描述：'
- en: '![Figure 8.6 – Two examples of MLPs ](img/B17761_08_006.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – MLP的两个示例](img/B17761_08_006.jpg)'
- en: Figure 8.6 – Two examples of MLPs
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – MLP的两个示例
- en: MLP models are generally very versatile but are most commonly used for structured
    tabular data, such as the structured protein classification dataset we have been
    working with. In addition, they can be used for image data or even text data.
    MLPs, however, generally tend to suffer when it comes to sequential data such
    as protein sequences and time-series datasets.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: MLP模型通常非常通用，但最常用于结构化表格数据，例如我们一直在工作的结构化蛋白质分类数据集。此外，它们还可以用于图像数据或甚至文本数据。然而，MLPs在处理如蛋白质序列和时间序列数据等序列数据时通常会遇到困难。
- en: Convolutional neural networks
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '**Convolutional Neural Networks** (**CNNs**) are deep learning algorithms that
    are commonly used for processing and analyzing image data. CNNs can take in images
    as input data and restructure them to determine the importance through weights
    and biases, allowing it to distinguish between the features of one image relative
    to another. Similar to our earlier discussion of how deep learning is similar
    to neurons in the brain, CNNs are also analogous to the connectivity of neurons
    in the human brain and the visual cortex when it comes to the sensitivity of regions,
    similar to the concept of receptive fields. One of the biggest areas of success
    for CNN models is their ability to capture spatial dependencies, as well as temporal
    dependencies, in images through the use of filters. We can see a visual representation
    of this in the following diagram:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）是常用的深度学习算法，用于处理和分析图像数据。CNNs可以接受图像作为输入数据，并通过权重和偏置重新结构化它们以确定重要性，从而使其能够区分一个图像相对于另一个图像的特征。类似于我们之前讨论的深度学习如何类似于大脑中的神经元，CNNs在区域敏感性方面也类似于人脑和视觉皮层的神经元连接，类似于感受野的概念。CNN模型最大的成功之一是它们能够通过使用过滤器捕获图像中的空间依赖性和时间依赖性。我们可以在以下图表中看到这一视觉表示：'
- en: '![Figure 8.7 – A representation of a CNN ](img/B17761_08_007.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – CNN的表示](img/B17761_08_007.jpg)'
- en: Figure 8.7 – A representation of a CNN
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – CNN的表示
- en: 'Let''s take, for example, the idea of creating an image classification model.
    We could use an ANN and convert a 2D image of pixels by flattening it. An image
    with a 4x4 matrix of pixels would now become a 1x16 vector instead. This change
    would cause two main drawbacks:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以创建图像分类模型的想法为例。我们可以使用ANN并将2D像素图像通过展平转换为向量。一个4x4像素矩阵的图像现在将变成一个1x16的向量。这种变化将导致两个主要缺点：
- en: The spatial features of the image would be lost, and thereby reduce the robustness
    of any trained model.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像的空间特征将会丢失，从而降低任何训练模型的鲁棒性。
- en: The number of input features would increase quite drastically as the image size
    grows.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着图像尺寸的增长，输入特征的数量将急剧增加。
- en: CNNs can overcome this by extracting high-level features from the images, allowing
    them to be quite effective with image-based datasets.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs可以通过从图像中提取高级特征来克服这一点，这使得它们在基于图像的数据集中非常有效。
- en: Recurrent neural networks
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: '**Recurrent Neural Networks** (**RNNs**) are commonly used algorithms that
    are generally applied to sequence-based datasets. They are quite similar in architecture
    to the ANNs we discussed earlier, but RNNs can remember their input using internal
    memory, making them quite effective with sequential datasets in which previous
    data is of great importance.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）是常用的算法，通常应用于基于序列的数据集。它们的架构与我们之前讨论的ANNs非常相似，但RNNs可以通过内部记忆记住它们的输入，这使得它们在先前数据非常重要的序列数据集中非常有效。'
- en: 'Take, for example, a protein sequence consisting of various amino acids. To
    predict the class of the protein, or its general structure, the model would not
    only need to know which amino acids were used but the order in which they were
    used as well. RNNs and their many derivatives have been central to the many advances
    in deep learning within the field of biology and biotechnology. We can see a visual
    representation of this in the following diagram:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个由各种氨基酸组成的蛋白质序列为例。为了预测蛋白质的类别或其一般结构，模型不仅需要知道使用了哪些氨基酸，还需要知道它们的使用顺序。RNNs及其许多衍生品在生物技术和生物技术领域的深度学习许多进步中起到了核心作用。我们可以在以下图表中看到这一视觉表示：
- en: '![Figure 8.8 – A representation of an ANN node versus an RNN node ](img/B17761_08_008.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – ANN节点与RNN节点的表示](img/B17761_08_008.jpg)'
- en: Figure 8.8 – A representation of an ANN node versus an RNN node
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – ANN节点与RNN节点的表示
- en: 'There are several advantages when it comes to using RNNs as predictor models,
    with the main benefits being as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RNN作为预测模型有几个优点，主要好处如下：
- en: Their ability to capture the dependency between data points such as words in
    a sentence
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们能够捕捉数据点之间的依赖关系，例如句子中的单词
- en: Their ability to share parameters across time steps, thus decreasing the overall
    computational cost
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们能够在时间步长之间共享参数，从而降低整体计算成本
- en: Because of this, RNNs have become increasingly popular architectures for developing
    models that solve problems related to scientific sequence data such as proteins
    and DNA, as well as text and time-series data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个原因，RNN已成为开发解决与蛋白质和DNA等科学序列数据以及文本和时间序列数据相关问题的模型时越来越受欢迎的架构。
- en: Long short-term memory
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长短期记忆
- en: '**Long Short-Term Memory** (**LSTM**) models are a type of RNN designed with
    the capability of learning long-term dependencies when handling sequence-based
    problems. Commonly used with text-based data for classification, translation,
    and recognition, LSTMs have gained an unprecedented surge in popularity over the
    years. We can depict the structure of a standard RNN as we did previously, but
    structured slightly differently:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）模型是一种设计用于处理基于序列问题的长期依赖能力的RNN。通常与基于文本的数据一起用于分类、翻译和识别，LSTMs在近年来获得了前所未有的流行。我们可以像之前那样描绘标准RNN的结构，但结构略有不同：'
- en: '![Figure 8.9 – The inner workings of an RNN versus an LSTM ](img/B17761_08_009.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图8.9 – RNN与LSTM的内部工作原理](img/B17761_08_009.jpg)'
- en: Figure 8.9 – The inner workings of an RNN versus an LSTM
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – RNN与LSTM的内部工作原理
- en: In the preceding diagram, *X*t is an input vector, *h*tis a hidden layer vector,
    and *o*t is an output vector. On the other hand, and using some of the same elements,
    an LSTM can be structured quite similarly. Without diving into too much detail,
    the core idea behind an LSTM is the cell state (the top horizontal line). This
    state operates similarly to a conveyor belt in which data flows linearly through
    it. Gates within the cell are methods that optionally allow information to be
    added to the state. An LSTM has three gates, all leading to the cell state.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，*X*t是一个输入向量，*h*t是一个隐藏层向量，*o*t是一个输出向量。另一方面，使用一些相同的元素，LSTM的结构可以相当相似。不深入细节，LSTM的核心思想是细胞状态（顶部水平线）。这个状态的工作方式类似于传送带，数据线性地通过它。细胞内的门是可选地允许信息添加到状态的方法。LSTM有三个门，都指向细胞状态。
- en: 'Although LSTM models and their associated diagrams can be quite intimidating
    at first, they have proven their worth time and time again in various areas. Most
    recently, LSTM models have been used as generative models for antibody design,
    as well as classification models for protein sequence-structure classification.
    Now that we have explored several common deep learning architectures, let''s go
    ahead and explore their main components: activation functions.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LSTM模型及其相关图表一开始可能让人感到有些令人畏惧，但它们在各种领域一次又一次地证明了它们的价值。最近，LSTM模型被用作抗体设计的生成模型，以及蛋白质序列-结构分类的分类模型。现在我们已经探讨了几个常见的深度学习架构，让我们继续探索它们的主要组件：激活函数。
- en: Selecting an activation function
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择激活函数
- en: 'Recall that, in the previous section, we used an activation function to map
    a value to a particular output, depending on the value. We will define an activation
    function as a mathematical function that defines the output of an individual node
    using an input value. Using the analogy of the human brain, these functions simply
    act as gatekeepers, deciding what will be *fired off* to the next neuron. There
    are several features that an activation function should have to allow the model
    to learn most effectively from it:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在前一节中，我们使用激活函数将一个值映射到特定的输出，这取决于该值。我们将激活函数定义为一种数学函数，它使用输入值定义单个节点的输出。用人类大脑的类比，这些函数简单地充当守门人，决定什么将被*触发*到下一个神经元。激活函数应该具有一些特性，以便模型能够从它那里最有效地学习：
- en: The avoidance of a vanishing gradient
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免梯度消失
- en: A low computational expense
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低计算成本
- en: 'Artificial neural networks are trained using a process known as gradient descent.
    For this example, let''s assume that there is a two-layer neural network:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络使用称为梯度下降的过程进行训练。在这个例子中，让我们假设有一个两层神经网络：
- en: '![](img/Formula_B17761__08_005.jpg)![](img/Formula_B17761__08_006.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/Formula_B17761__08_005.jpg)![公式图片](img/Formula_B17761__08_006.jpg)'
- en: 'The overall network can be represented as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 整个网络可以表示如下：
- en: '![](img/Formula_B17761__08_007.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761__08_007.jpg)'
- en: 'When the weights are calculated in a step known as a backward pass, the result
    becomes as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播步骤中计算权重时，结果如下：
- en: '![](img/Formula_B17761__08_008.jpg)![](img/Formula_B17761__08_009.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761__08_008.jpg)![](img/Formula_B17761__08_009.jpg)'
- en: 'Upon determining the derivative, the function becomes as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 确定导数后，函数变为如下：
- en: '![](img/Formula_B17761__08_010.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761__08_010.jpg)'
- en: If this process were to continue through many layers during the backpropagation
    step, there would be a considerable reduction in the value of the gradient for
    the initial layers, thus halting the model's ability to learn. This is the concept
    of **vanishing gradients**.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个过程在反向传播步骤中通过许多层继续进行，那么初始层的梯度值将会有相当大的减少，从而阻碍模型的学习能力。这就是**梯度消失**的概念。
- en: On the other hand, **computational expense** is also a feature that must be
    considered before designing and deploying any given model. The activation functions
    that are applied from one layer to another must be calculated many times, so the
    expense of the calculation should be kept to a minimum to avoid longer training
    periods. The flow of information from an input layer to an output layer is called
    **forward propagation**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**计算成本**也是在设计并部署任何给定模型之前必须考虑的一个特性。从一层到另一层的激活函数必须多次计算，因此计算成本应保持在最低，以避免更长的训练周期。从输入层到输出层的信息流动被称为**正向传播**。
- en: 'There are many different types of activation functions out there that are commonly
    used for various purposes. Although this is not a hard rule, some activation functions
    are generally used with specific deep learning layers. For example, `sigmoid`
    and **Tanh** activation functions are commonly used with **RNNs**. Let''s take
    a moment and look at the three most common activation functions you will likely
    encounter in your journey:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同类型的激活函数，它们通常用于各种目的。尽管这不是一条硬性规则，但一些激活函数通常与特定的深度学习层一起使用。例如，`sigmoid` 和 **Tanh**
    激活函数通常与 **RNNs** 一起使用。让我们花点时间看看你可能在旅途中遇到的三种最常见的激活函数：
- en: '![Figure 8.10 – Various types of activation functions by model type ](img/B17761_08_010.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 8.10 – Various types of activation functions by model type ](img/B17761_08_010.jpg)'
- en: Figure 8.10 – Various types of activation functions by model type
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 按模型类型划分的各种激活函数
- en: 'With some of these types now in mind, let''s go ahead and explore them in a
    little more detail. `sigmoid` functions are probably some of the most commonly
    used functions within the field of deep learning. It is a non-linear activation
    function that is also sometimes referred to as a logistic function (remember logistic
    regression? Hint hint). `sigmoid` functions are unique in the sense that they
    can map values to either a 0 or a 1\. Using the `numpy` library, we can easily
    put together a `Python` function to calculate it:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在心中有了这些类型的一些概念后，让我们更深入地探讨它们。`sigmoid` 函数可能是深度学习领域中应用最广泛的一些函数。它是一种非线性激活函数，有时也被称为逻辑函数（还记得逻辑回归吗？提示提示）。`sigmoid`
    函数的独特之处在于它们可以将值映射到 0 或 1。使用 `numpy` 库，我们可以轻松地编写一个 `Python` 函数来计算它：
- en: '[PRE0]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using this function alongside the `numpy` library, we can generate some data
    and plot our `sigmoid` function accordingly:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数以及 `numpy` 库，我们可以生成一些数据并相应地绘制 `sigmoid` 函数：
- en: '[PRE1]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In return, we yield the following diagram, showing the curved nature of a `sigmoid`
    function. Notice how the upper and lower ranges are 1 and 0, respectively:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回报，我们得到以下图表，显示了 `sigmoid` 函数的曲线性质。注意上下限分别是 1 和 0：
- en: '![Figure 8.11 – A simple sigmoid function ](img/B17761_08_011.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 8.11 – A simple sigmoid function ](img/B17761_08_011.jpg)'
- en: Figure 8.11 – A simple sigmoid function
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 一个简单的 sigmoid 函数
- en: One of the biggest issues with a `sigmoid` activation function is that the outputs
    can **saturate** in the sense that values greater than 1.0 are mapped to one,
    and values that are smaller than 0 are mapped to 0\. This can cause some models
    to fail to generalize or learn from the data and is related to the vanishing gradients
    issue we discussed earlier in this chapter.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid` 激活函数的一个最大问题是输出可能会**饱和**，即大于 1.0 的值被映射到 1，而小于 0 的值被映射到 0。这可能导致某些模型无法泛化或从数据中学习，这与我们在本章前面讨论的梯度消失问题有关。'
- en: 'On the other hand, another common activation function is `sigmoid` function.
    The Tanh function is symmetric in the sense that it passes through the point (0,
    0) and it ranges to the values of 1 and -1, unlike its `sigmoid` counterpart,
    making it a slightly better function. Instead of defining our functions in Python,
    as we did previously, we can take advantage of the optimized functions in the
    `numpy` library:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Upon executing this code, we retrieve the following diagram. Notice how the
    center of the diagram is the point (0, 0), while the upper and lower values are
    1.00 and -1.00, respectively:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – A simple Tanh function ](img/B17761_08_012.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – A simple Tanh function
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Similar to its `sigmoid` counterpart, the `sigmoid`, making it a slightly better
    function to use.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, yet another commonly used activation function is the **Rectified Linear
    Unit** (**ReLU**). **ReLU** activation functions were specifically developed to
    avoid saturation when handling larger numbers. The non-linear nature of this function
    allows it to learn the patterns within the data, whereas the linear nature of
    the function allows it to be easily interpretable relative to the other functions
    we have seen so far. Let''s go ahead and explore this in Python:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Executing this code yields the following diagram. Notice how the **ReLU** function
    takes advantage of both the linear and non-linear nature of activation functions,
    giving it the best of both worlds:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – A simple ReLU function ](img/B17761_08_013.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – A simple ReLU function
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The **ReLU** activation function has become one of the most popular, if not
    **the** most popular, activation function among data scientists because of its
    ease of implementation, and its robust speed within the model development and
    training process. **ReLU** activation functions do, however, have their downsides.
    For example, the function cannot be differentiable when x = 0 (at point 0, 0),
    so **gradient descent** cannot be computed for that value.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Yet another activation function worth mentioning is known as **Softmax**. **Softmax**
    is very different from the other activation functions we have looked at so far
    because it computes a probability distribution for a list of values that are proportional
    to the relative scale of each of the values in the vector, the sum of which always
    equals 1\.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Commonly used for `numpy` library:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Upon printing the values, we retrieve the following results:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Results of a Softmax function ](img/B17761_08_014.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Results of a Softmax function
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The two main advantages that come from using **Softmax** as an activation function
    are that the output values range between 0 and 1 and that they always sum to a
    value of 1.0\. In return, this allows the function to be used to understand cross-entropy
    when it comes to the idea of divergence. We will visit this topic in more detail
    later in this chapter.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'The various activation functions we have visited so far each have their pros
    and cons when it comes to using them in various applications. For example, `sigmoid`
    functions are commonly used for binary and multilabel classification applications,
    whereas **Softmax** functions are generally used for multiclass classification.
    This is not a hard rule, but simply a guide to help you match a function with
    the highest chance of success with its respective application:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – Activation functions by problem type ](img/B17761_08_015.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – Activation functions by problem type
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions are a vital part of any deep learning model and are often
    regarded as *game changers* since simply changing one function for another can
    boost the performance of a model quite drastically. We will take a closer look
    at how model performance can be quantified within the scope of deep learning in
    the following section.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Measuring progress with loss
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we discussed the areas of classification and regression, we outlined a
    few measures to measure and quantify the performance of our models relative to
    one another. When it came to classification, we used **precision** and **accuracy**,
    whereas, in regression, we used **MAE** and **MSE**. Within the confines of deep
    learning, we will use a metric known as **loss**. The **loss** of a neural network
    is simply a measure of the cost that''s incurred from making an incorrect prediction.
    Take, for example, a simple neural network with three input values and a single
    output value:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – A neural network showing input and output values ](img/B17761_08_016.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – A neural network showing input and output values
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we have the values *[2.3, 3.3, 1.2]* being used as input values
    to the model, with a predicted value of 0.2 relative to the actual value of 1.0\.
    We can demonstrate the loss as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17761__08_011.jpg)![](img/Formula_B17761__08_012.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: In this function, ![](img/Formula_B17761__08_013.png)is the predicted value,
    while ![](img/Formula_B17761__08_014.png) is the actual value.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '**Empirical loss**, on the other hand, is a measure of the total loss for the
    entirety of the dataset. We can represent empirical loss as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B17761__08_015.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: In this function, we sum the total losses for all calculations.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout the model training process, our main objective will be to minimize
    this loss in a process known as **loss optimization**. The main idea behind loss
    optimization is to identify a set of weights that help achieve the lowest loss
    possible. We can visualize the idea of gradient descent as the process of moving
    from an initial starting value with a high loss, to a final value with a low loss.
    Our objective will be to ensure that converge in a global minimum rather than
    a local minimum, as depicted in the following diagram:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17 – The process of loss optimization ](img/B17761_08_017.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – The process of loss optimization
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Each step we take to get closer to the minimum value is known as a **learning
    step**, the **learning rate** of which is generally determined by the user. This
    parameter is just one of many that we can specify using Keras, which we will learn
    about in the following section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning with Keras
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Within the realm of data science, the availability and use of various **frameworks**
    will always be crucial to standardizing the methods we use to develop and deploy
    models. So far, we have focused our machine learning efforts on using the scikit-learn
    framework. Throughout this section, we will learn about three new frameworks specifically
    focused on deep learning: **Keras**, **TensorFlow**, and **PyTorch**. These two
    frameworks are the two most popular amongst data scientists when it comes to developing
    various deep learning models as they offer a comprehensive list of APIs for numerous
    problems and use cases.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the differences between Keras and TensorFlow
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although these two platforms allow users to develop deep learning models, there
    are a few differences to know about. TensorFlow is known as an end-to-end machine
    learning platform that offers a comprehensive list of libraries, tools, and numerous
    resources. Users can manage data, develop models, and deploy solutions. Unlike
    most other libraries, **TensorFlow** offers both low and high levels of abstractions
    through their APIs, giving users lots of flexibility when it comes to developing
    models. On the other hand, **Keras** offers high-level APIs for developing neural
    networks, which run using **TensorFlow**. The high-level nature of this library
    allows users to begin developing and training complex neural networks with only
    a few lines of Python code. Keras is generally regarded as user-friendly, modular,
    and extendable. A third library exists that is commonly used in the deep learning
    space known as **PyTorch**. **PyTorch** is a low-level API known for its remarkable
    speed and optimization in the model training process. The architectures within
    this library are generally complex and not appropriate for introductory material,
    so they are not within the scope of this book. However, it is worth mentioning
    as it is one of the most common libraries in the machine learning space that you
    will likely encounter. Let''s take a closer look at all three:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18 – A comparison between three of the most common deep learning
    frameworks ](img/013.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – A comparison between three of the most common deep learning frameworks
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: There are pros and cons for each of these libraries and you should select one
    of these libraries based on the task you set out to accomplish. Given that we
    are exploring the development of deep learning models for the first time, we will
    focus on using the Keras library.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Keras and ANNs
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we move on to a full tutorial, let''s look at an example of using the
    `Keras` library since we have not explored its functionality and code:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: First, we will need some sample data to use. Let's take advantage of the `make_blobs`
    class in `sklearn` to create a `classification` dataset.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will specify the need for two classes (binary classification) and a cluster
    standard deviation of `5` to ensure that the two clusters overlap, making it a
    more difficult dataset to work with:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we can scale the data using the `MinMaxScaler()` class:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Following this transformation, we can split the data into training and testing
    sets, similar to how we have done previously:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s go ahead and convert the array into a DataFrame to check the first few
    rows of data beforehand:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will render the following table:'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.19 – An example of the data within the DataFrame of features ](img/B17761_08_019.jpg)'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.19 – An example of the data within the DataFrame of features
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can check the overlap of the two clusters by using the `seaborn` library
    to plot the first two of the four features of the training dataset:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following diagram shows the preceding code''s output:'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.20 – A scatterplot of the dataset showing the overlapping nature
    of the two classes ](img/B17761_08_020.jpg)'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.20 – A scatterplot of the dataset showing the overlapping nature of
    the two classes
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we can see that the data is quite well blended, making it difficult for
    some of the machine learning models we have explored to be able to separate the
    two classes with a high degree of **accuracy**.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With the data ready, we can go ahead and use the Keras library. One of the
    most popular methods for setting up a model is using the `Sequential()` class
    from `Keras`. Let''s go ahead and import the class and instantiate a new model:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With the model now instantiated, we can add a new layer to our model using
    the `Dense` class. We can also specify the number of `nodes` (`4`), `input_shape`
    (`4` for the four features), `activation` (`relu`), and a unique `name` for the
    layer:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To review the model we have built so far, we can use the `summary()` function:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will give us some information and details about the model so far:'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.21 – The sample output of the model''s summary ](img/B17761_08_021.jpg)'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.21 – The sample output of the model's summary
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can add a few more layers to our model by simply using the `model.add()`
    function again after the fact, perhaps even with a different number of nodes:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Since we are developing a `0` and `1`, we can only have a single output value
    come from the model. Therefore, we will need to add one more layer that reduces
    the number of nodes from 8 to 1\. In addition, we will change the activation to
    `sigmoid`:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now that our model''s general architecture has been set up, we will need to
    use the compile function and specify our loss. Since we are creating a binary
    classifier, we can use the `binary_crossentropy` loss and specify accuracy as
    our main metric of interest:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With the model ready, let''s use the summary function to check it once more:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.22 – The sample output of the model''s summary ](img/B17761_08_022.jpg)'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.22 – The sample output of the model's summary
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So far, the model is quite simple. It will intake a dataset with four features
    in the first layer, expand that to eight nodes in the second layer, and then reduce
    it down to a single output in the third layer. With the model all set, we can
    go ahead and train it.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can train a model by using the `model.fit()` function and by specifying the
    `X_train` and `y_train` sets. In addition, we will specify 50 `epochs` to train
    over. **Epochs** are simply the number of passes or iterations. We can also control
    the verbosity of the model, allowing us to control the amount of output data we
    want to see in the training process.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Recall that, in our earlier machine learning models, we only used the training
    data to train the model and kept the testing data to test the model after the
    training was completed. We will use the same methodology here as well; however,
    we will take advantage of the high-level nature of `validation split` to be used
    in the training process. Deep learning models will almost always overfit your
    data. Using a validation split in the training process can help mitigate this:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As the model begins the training process, it will begin to produce the following
    output. You can monitor the performance here by looking at the number of **epochs**
    on the left and the **metrics** on the right. When training a model, our objective
    is to ensure that the **loss** metric is constantly decreasing, whereas the accuracy
    is increasing. We can see an example of this in the following screenshot:'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.23 – A sample of a model''s output ](img/B17761_08_023.jpg)'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.23 – A sample of a model's output
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With the model trained, let''s quickly examine the classification metrics,
    as we did previously, to get a sense of the performance. We can begin by making
    predictions using the testing data and using the `classification_report` to calculate
    our metric. Note that the `predict()` method does not return a class but a probability
    that needs to be rounded to either `0` or `1`, given that this is a **binary classification**
    problem:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Upon printing the report, we will get the following results:'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.24 – The results of the model ](img/B17761_08_024.jpg)'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.24 – The results of the model
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can see that the `history` variable, which contains the model''s training
    history:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Upon executing this code, we will receive the following diagram, which shows
    the change in accuracy and loss over the course of the model training process:'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.25 – The accuracy and loss of the model ](img/B17761_08_025.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: Figure 8.25 – The accuracy and loss of the model
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: When training a model, recall that the main objective is to ensure that the
    loss is decreasing, and never increasing over time. In addition, our secondary
    objective is to ensure that the accuracy of our model slowly and steadily increases.
    When trying to diagnose a model that is not performing well, the first step is
    to generate graphs such as these to get a sense of any potential problems before
    altering the model in an attempt to improve the metrics.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'When we worked with most of the machine learning models in the previous chapters,
    we learned that we could alter these metrics by doing the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Improving our data preprocessing.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning our hyperparameters or changing the model. Within the confines of deep
    learning, in addition to the options mentioned previously, there are a few more
    tools we have to change to suit our needs. For instance, we can change the overall
    architecture by adding or removing layers and nodes. In addition, we can change
    the activation functions within each of the layers to whatever would complement
    our problem statement the most. We can also change the optimizer or the learning
    rate of our optimizer (Adam, in this model).
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With so many changes that can be made that could have high impacts on any given
    model, we will need to organize our work. We could either create numerous models
    and record our metrics manually in a spreadsheet, or we could take advantage of
    a library specifically designed to handle use cases such as these: **MLflow**.
    We will take a closer look at **MLflow** in the next section.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Tutorial – protein sequence classification via LSTMs using Keras and MLflow
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has gained a surge of popularity in recent years, prompting many
    scientists to turn to the field as a new means for solving and optimizing scientific
    problems. One of the most popular applications for deep learning within the biotechnology
    space involves **protein sequence** data. So far within this book, we have focused
    our efforts on developing predictive models when it comes to **structured** data.
    We will now turn our attention to data that's **sequential** in the sense that
    the elements within a sequence bear some relation to their previous element. Within
    this tutorial, we will attempt to develop a protein **sequence classification**
    model in which we will classify protein sequences based on their known family
    accession using the **Pfam** ([https://pfam.xfam.org/](https://pfam.xfam.org/))
    dataset.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '`Pfam` dataset: Pfam: The protein families database in 2021 J. Mistry, S. Chuguransky,
    L. Williams, M. Qureshi, G.A. Salazar, E.L.L. Sonnhammer, S.C.E. Tosatto, L. Paladin,
    S. Raj, L.J. Richardson, R.D. Finn, A. BatemanNucleic Acids Research (2020) doi:
    10.1093/nar/gkaa913 (`Pfam: The protein families database in 2021`).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Pfam` dataset consists of several columns, as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '`Family_id`: The name of the family that the sequence belongs to (for example,
    filamin)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Family Accession`: The class or output that our model will aim to predict'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sequence`: The amino acid sequence we will use as input for our model'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this tutorial, we will use the sequence data to develop several predictive
    models to determine each sequence's associated family accession. The sequences
    are in their raw state with different lengths and sizes. We will need to pre-process
    the data and structure it in such a way as to prepare it for sequence classification.
    When it comes to the labels, we will develop a model using a **balanced** set
    of different labels to ensure the model does not learn any particular bias.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'As we begin to develop our ideal classification model, we will need to alter
    the many possible parameters to maximize the performance. To keep track of these
    changes, we will make use of the MLflow ([https://mlflow.org](https://mlflow.org))
    library. There are four main components within **MLflow**:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '**MLflow Tracking**: Allows users to record and query experiments'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLflow Projects**: Packages data science code'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLflow Models**: Deploys trained machine learning models'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLflow Registry**: Stores and manages your models'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Within this tutorial, we will explore how to use MLflow tracking to track and
    manage the development of a protein sequence classification model. With these
    items in mind, let's get started.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Importing the necessary libraries and datasets
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will begin with the standard set of library imports, followed by the dataset
    in the format of a CSV document:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With the libraries now imported, we can import the dataset as well. We will
    begin by specifying the path, and then concatenate the dataset using a `for` loop:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Upon importing the dataset, we immediately notice that it contains five columns
    and ~1.3 million rows of data – slightly larger than what we have worked with
    so far. We can take a quick glimpse at the dataset using the `.head()` function:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.26 – A sample of the data from the protein sequence dataset ](img/B17761_08_026.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: Figure 8.26 – A sample of the data from the protein sequence dataset
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: With the dataset successfully imported, let's go ahead and explore the dataset
    in more detail.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Checking the dataset
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can confirm the completeness of the data in this DataFrame using the `isna()`
    function, followed by the `sum()` function to summarize by column:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let''s take a closer look at the `family_accession` column (our model''s
    output) of this dataset. We can check the total number of instances by grouping
    the column and using the `value_counts()` function, followed by the `n_largest()`
    function, to get the top 10 most common entries in this column:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Grouping the data will yield the following results:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.27 – A summary of the classes in the dataset with value counts higher
    than 1,200 ](img/B17761_08_027.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: Figure 8.27 – A summary of the classes in the dataset with value counts higher
    than 1,200
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that 1,500 entries seems to be the cutoff point for the top
    10 values. We can also take a closer look at the sequence column (our model''s
    input) by getting a sense of the average lengths of the sequences. We can plot
    the count of each sequence length using the `displot()` function from the `seaborn`
    library:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Executing this code will yield the following results:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.28 – A histogram of the counts of the sequence lengths in the dataset
    ](img/B17761_08_028.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Figure 8.28 – A histogram of the counts of the sequence lengths in the dataset
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: From this graph, as well as by using the `mean()` and `median()` functions,
    we can see that the average and most common lengths are approximately 155 and
    100 amino acids. We will use these numbers later when determining what the cutoff
    should be for the input sequences.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have gained a better sense of the data, it is time to prepare the
    dataset for our classification models. We could theoretically train the model
    on the dataset as a whole without limits – however, the models would require a
    much longer duration to train. In addition, by training across all the data without
    accounting for balance, we may introduce bias within the model. To mitigate both
    of these situations, let''s reduce this dataset by filtering for the classifications
    with at least 1,200 **observations**:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Given that some classes have significantly more than 1,200 observations, we
    can randomly select exactly 1,200 observations using the `sample()` function:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can now check the filtered and balanced dataset using the `head()` function:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `head()` function will yield the following results:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.29 – A sample of the data in the form of a DataFrame ](img/B17761_08_029.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: Figure 8.29 – A sample of the data in the form of a DataFrame
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check the number of classes we will have in this dataset by checking
    the length of the `value_counts()` function:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If we check the `num_classes` variable, we will see that we have 28 possible
    classes in total.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the dataset
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the data prepared, our next step will be to split the dataset into training,
    testing, and validation sets. We will once again make use of the `train_test_split`
    function from `sklearn` to accomplish this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With the data now split, let's go ahead and preprocess it.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the data
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the data split up, we need to preprocess the datasets to use on our neural
    network models. First, we will need to reduce the sequences down to the 20 most
    common amino acids and convert the sequences into integers. This will speed up
    the training process. First, we will create a dictionary of amino acids that contains
    their corresponding values:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we can iterate over the sequences and convert the string values into
    their corresponding integers. Note that we will complete this for the training,
    testing, and validation sets:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we will need to pad the sequences to ensure they are all of an equal
    length. To accomplish this, we can use the `pad_sequences` function from `keras`.
    We will specify `max_length` for each of the sequences as 100, given that it approximates
    the median value we saw earlier. In addition, we will pad the sequences with `''post''`
    to ensure that we pad them at the end instead of the front:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can take a quick glance of the changes we have made using one of the sequences.
    First, we have the raw sequence as a `string`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we can encode the sequence to remove uncommon amino acids and convert
    the string to a list of integers:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we can limit the lengths of the sequences to either truncate them
    at 100 elements or **pad** them with zeros to reach 100 elements:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now that we have preprocessed the input data, we will need to preprocess the
    output values as well. We can do so using the `LabelEncoder` class from `sklearn`.
    Our main objective here will be to transform the values from a list of labels
    in a dataframe column into an encoded list:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we can use the `to_categorical` function from `sklearn` to transform
    a class vector into a binary class matrix:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To review the changes we''ve made here, we can use a single column in a `DataFrame`:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can see the results of this in the following diagram:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.30 – A list of the classes ](img/B17761_08_030.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: Figure 8.30 – A list of the classes
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must encode the classes into a list of numerical values, with each
    value representing a specific class:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, we must convert the structure into a **binary class matrix** so that
    each row consists of a list of 27 values of zero, and one value of 1, representing
    the class it belongs to:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: With that, our datasets have been fully preprocessed and ready to be used in
    the model development phase of the tutorial.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Developing models with Keras and MLflow
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to our previous example of developing models with the `Keras` library,
    we will once again be using the `Sequential` class:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by importing the layers and other items we will need from Keras:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, we can create a new instance of a model using the sequential class and
    begin populating it by adding a few layers of interest. We will start by adding
    an embedding layer to convert positive integers into dense vectors. We will specify
    an `input_dim` of `21` to represent the size of the amino acid index + 1, and
    an `output_dim` of 32\. In addition, we will assign an `input_length` equal to
    that of `max_length` – the length of the sequences:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we will add an LSTM layer, wrapped in a `Bidirectional` layer, to run
    inputs in both directions – from past to future and from future to past:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we will add a `Dropout` layer to help prevent the model from overfitting:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, we will end with a `Dense` layer and set the number of nodes to `28`
    so that this corresponds with the shape of the outputs. Notice that we use a Softmax
    activation here:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'With the model''s architecture prepared, we can assign an optimizer (Adam),
    compile the model, and check the summary:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, let''s go ahead and train our model using the `fit()` function and assign
    30 epochs. Notice from our previous tutorial that training deep learning models
    can be very time-consuming and expensive, so training a model that is not learning
    can be a major waste of time. To mitigate situations such as these, we can implement
    what is known as a callback in the sense that Keras can end the training period
    when a model is no longer learning (that is, the loss is no longer decreasing):'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, we can go ahead and log our new run in MLflow by calling the `autolog()`
    function and fitting the model, as we did previously. MLflow offers many different
    methods to log both parameters and metrics, and you are not limited to using just
    `autolog()`:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Assuming you followed these steps correctly, the model will print a note stating
    that MLflow is being used, and you should see a new directory appear next to your
    current notebook. Upon completing the training process, you can plot the results,
    as we did previously, to arrive at the following diagram:'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.31 – The accuracy and results of the first iteration of this model
    ](img/B17761_08_031.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: Figure 8.31 – The accuracy and results of the first iteration of this model
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that the accuracy seems to remain stagnant at around 80-85%,
    while the loss remains stagnant at 0.6 to 0.8\. We can see that the model is not
    learning. Perhaps a change of parameters is needed? Let''s go ahead and change
    the number of nodes from `8` to `12` and the learning rate from `0.1` to `0.01`.
    Upon compiling the new model, calling the `autolog()` function, and training the
    new dataset, we will arrive at a new diagram:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.32 – The accuracy and results of the next iteration of this model
    ](img/B17761_08_032.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: Figure 8.32 – The accuracy and results of the next iteration of this model
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that the model''s loss, both for training and validation,
    decreased quite nicely until the callback stopped the training at around 30 epochs
    in. Alternatively, the accuracy shows a sharp increase at the beginning, followed
    by a stable increase toward the end, also stopping at 30 epochs into the process.
    We can keep making our changes and calling the `autolog()` function over and over,
    allowing the system to log the changes and the resulting metrics on our behalf.
    After several iterations, we can review the performance of our models using `mlflow
    ui`. Within the notebook itself, enter the following command:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Next, navigate to `http://localhost:5000/`. There, you will be able to see
    the `MLflow` UI, where you will be able to view the models, their parameters,
    and their associated metrics:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.33 – An example of the MLflow UI ](img/B17761_08_033.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: Figure 8.33 – An example of the MLflow UI
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: With that, you can select the best model and move forward with your project.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the model's performance
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that the best-performing model has been selected, let''s get a better sense
    of its associated `classification_report`, as we did previously, showing almost
    99% for both precision and recall. Alternatively, we can use a confusion matrix
    to get a better sense of the data, given that we have 28 classes in total:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'With the confusion matrix calculated, we can use a heatmap to visualize the
    results:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Upon executing this, we will get the following diagram:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.34 – A confusion matrix of the results of the model ](img/B17761_08_034.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
- en: Figure 8.34 – A confusion matrix of the results of the model
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the performance of the model is quite robust as it is
    giving us great results! Keras, TensorFlow, and PyTorch are great packages that
    can help us develop robust and high-impact models to solve specific solutions.
    Often, we will find that there may be a model (or set of models) that already
    exists through AWS that can solve our complex problem with little to no code.
    We will explore an example of this in the next section.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Tutorial – anomaly detection in manufacturing using AWS Lookout for Vision
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we prepared and trained a deep learning model to classify
    proteins in their given categories. We went through the process of preprocessing
    our data, developing a model, testing the parameters, editing the architecture,
    and selecting a combination that maximized our metrics of interest. While this
    process can generally produce good results, we can sometimes utilize platform
    architectures such as those from AWS to automatically develop models on our behalf.
    Within this tutorial, we will take advantage of a tool known **AWS Lookout for
    Vision** ([https://aws.amazon.com/lookout-for-vision/](https://aws.amazon.com/lookout-for-vision/))
    to help us prepare a model capable of detecting anomalies within a dataset.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this tutorial, we will be working with a dataset consisting of images
    concerned with manufacturing a of **Drug Product** (**DP**). Each of the images
    consists of a vial whose image was captured at the end of the manufacturing cycle.
    Most of the vials are clean and don''t have any impurities. However, some of the
    vials contain minor impurities, as illustrated in the following diagram:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.35 – An example of an accepted vial versus a damaged vial ](img/B17761_08_035.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
- en: Figure 8.35 – An example of an accepted vial versus a damaged vial
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: The process of rejecting damaged or impure vials is often done manually and
    can be quite time-consuming. We have been tasked with implementing an automated
    solution to this problem and we only have a few days to do so. Rather than developing
    our own custom deep learning model for detecting anomalies in images, we can utilize
    **Amazon Lookout for Vision**. In this tutorial, we will begin by uploading our
    dataset of images to S3, importing the images into the framework, and begin training
    our model. With that in mind, let's go ahead and get started!
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Within this book''s GitHub repository, you can find a directory called `vials_input_dataset_s3`,
    which contains a collection of both normal and damaged vials. If we take a closer
    look at our dataset, we will notice that it is constructed using a directory hierarchy,
    as shown in the following diagram:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.36 – An example of an accepted vial versus a damaged vial ](img/B17761_08_036.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
- en: Figure 8.36 – An example of an accepted vial versus a damaged vial
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by importing the images into the same S3 bucket we have been
    working with throughout this book:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: First, navigate to S3 from within the AWS console and select the bucket of interest.
    In this case, I will select **biotech-machine-learning**.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click the orange **Upload** button, select the **vials_input_dataset_s3**
    folder, and click **Upload**. This process may take a few moments, depending on
    your internet connection.
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, click on the **Copy S3 URI** button at the top right-hand side of the page.
    We will need this URI in a few moments.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, our data is available for use to use in our S3 bucket. Next, we can focus
    on getting the data imported with the model and start the model training process:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: To begin, navigate to Amazon Lookout for Vision, which is located in the AWS
    console. Then, click the **Get started** button:![Figure 8.37 – The front page
    of Amazon Lookout for Vision ](img/B17761_08_037.jpg)
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 8.37 – The front page of Amazon Lookout for Vision
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the **Create project** button on the right-hand side of the page and
    give your project a name.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the project has been created, go ahead and click the **Create dataset**
    button on the left-hand side of the page.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the second option to **Create a training dataset and test dataset**:![Figure
    8.38 – Creating a dataset in AWS Lookout for Vision ](img/B17761_08_038.jpg)
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 8.38 – Creating a dataset in AWS Lookout for Vision
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, within the `training` to the path, as shown in the following screenshot:![Figure
    8.39 – Creating a dataset in AWS Lookout for Vision ](img/B17761_08_039.jpg)
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 8.39 – Creating a dataset in AWS Lookout for Vision
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition, be sure to select the **Automatic labeling** option to ensure our
    labels are taken in by AWS.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat this same process for the test dataset but be sure to add the word `validation`
    instead of training in the S3 URI path. Then, click on **Create dataset**.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the dataset has been created, you will be taken to a new page where you
    can visually inspect the dataset''s readiness. Then, you can click the **Train
    model** button located in the top right-hand corner of the page to begin the model
    training process. This process can be time-consuming and may take a few hours:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.40 – The dataset before training the model ](img/B17761_08_040.jpg)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
- en: Figure 8.40 – The dataset before training the model
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'By doing this, you will be presented with the final results for the model,
    which will show you the precision, recall, and F1 score, as shown in the following
    screenshot:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.41 – The Model performance metrics page ](img/B17761_08_041.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
- en: Figure 8.41 – The Model performance metrics page
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: With that final step completed, we have successfully developed a robust model
    capable of detecting anomalies in the manufacturing process! Not only were we
    able to create the models in just a few hours, but we managed to do so without
    any code!
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we made a major stride to cover a respectable portion
    of the *must-know* elements of deep learning and neural networks. First, we investigated
    the roots of neural networks and how they came about and then dove into the idea
    of a perceptron and its basic form of functionality. We then embarked on a journey
    to explore four of the most common neural networks out there: MLP, CNN, RNN, and
    LSTM. We gained a better sense of how to select activation functions, measure
    loss, and implement our understandings using the Keras library.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Next, we took a less theoretical and much more hands-on approach as we tackled
    our first dataset that was sequential nature. We spent a considerable amount of
    time preprocessing our data, developing our model, getting our model development
    organized with MLflow, and reviewing its performance. Following these steps allowed
    us to create a custom and well-suited model for the problem at hand. Finally,
    we took a no-code approach by using AWS Lookout for Vision to train a model capable
    of detecting anomalies in images of vials.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning and the application of neural networks have most certainly seen
    a major surge over the last few years, and in the next chapter, we will see an
    application of deep learning as it relates to natural language processing.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
