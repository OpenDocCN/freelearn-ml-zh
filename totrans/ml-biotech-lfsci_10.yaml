- en: 'Chapter 8: Understanding Deep Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章：理解深度学习
- en: Throughout this book, we have examined the many tools and methods within the
    fields of supervised and unsupervised machine learning. Within the field of unsupervised
    learning, we explored **clustering** and **dimensionality reduction**, while within
    the field of supervised learning, we explored **classification** and **regression**.
    Within all of these fields, we explored many of the most popular algorithms for
    developing powerful predictive models for our datasets. However, as we have seen
    with some of the data we have worked with, there are numerous limitations when
    it comes to these models' performance that cannot be overcome by additional tuning
    and hyperparameter optimization. In cases such as these, data scientists often
    turn to the field of **deep learning**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们考察了监督学习和无监督机器学习领域的许多工具和方法。在无监督学习领域，我们探讨了**聚类**和**降维**，而在监督学习领域，我们探讨了**分类**和**回归**。在所有这些领域，我们探讨了为我们的数据集开发强大预测模型的最流行算法。然而，正如我们处理的一些数据所看到的，这些模型的表现存在许多限制，这些限制不能通过额外的调整和超参数优化来克服。在这些情况下，数据科学家通常会转向**深度学习**领域。
- en: If you recall our overarching diagram of the artificial intelligence space that
    we saw in *Chapter 5*, *Introduction to Machine Learning*, we noted that the overall
    space is known as **Artificial Intelligence** (**AI**). Within the AI space, we
    defined machine learning as the ability to develop models to learn or generalize
    from data and make predictions. We will now explore a subset of machine learning
    known as **deep learning**, which focuses on developing models and extracting
    patterns within data using deep neural networks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得我们在第5章“机器学习简介”中看到的整体人工智能空间图，我们会注意到这个整体空间被称为**人工智能**（**AI**）。在人工智能空间内，我们将机器学习定义为开发模型从数据中学习或泛化并做出预测的能力。现在，我们将探讨机器学习的一个子集，称为**深度学习**，它专注于使用深度神经网络开发和提取数据中的模式。
- en: 'Throughout this chapter, we will explore the ideas of neural networks and deep
    learning as they relate to the field of biotechnology. In particular, we will
    be covering the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨神经网络和深度学习与生物技术领域的相关思想。特别是，我们将涵盖以下主题：
- en: Understanding the field of deep learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解深度学习领域
- en: Exploring the types of deep learning models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索深度学习模型的类型
- en: Selecting an activation function
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择激活函数
- en: Measuring progress with loss
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用损失来衡量进度
- en: Developing models with the Keras library
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras库开发模型
- en: Tutorial – protein sequence classification via LSTMs using Keras and MLflow
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教程 – 使用Keras和MLflow通过LSTMs进行蛋白质序列分类
- en: Tutorial – anomaly detection using AWS Lookout for Vision
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教程 – 使用AWS Lookout for Vision进行异常检测
- en: With these sections in mind, let's get started!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些部分，让我们开始吧！
- en: Understanding the field of deep learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解深度学习领域
- en: As we mentioned in the introduction, deep learning is a subset or branch of
    the machine learning space that focuses on developing models using neural networks.
    The idea behind using neural networks for deep learning derives from neural networks
    found in the human brain. Let's learn more about this.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在引言中提到的，深度学习是机器学习空间的一个子集或分支，它专注于使用神经网络开发模型。使用神经网络进行深度学习的理念源于人类大脑中的神经网络。让我们了解更多关于这个内容。
- en: Neural networks
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'Similar to machine learning, the idea behind developing deep learning models
    is not to explicitly define the steps in which a decision or prediction is made.
    The main idea here is to generalize from the data. Deep learning makes this possible
    by drawing a parallel between the dendrites, cell body, and synapses of the human
    brain, which, within the context of deep learning, act as inputs, nodes, and outputs
    for a given model, as shown in the following diagram:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与机器学习类似，开发深度学习模型的理念不是明确定义决策或预测的步骤。这里的主要思想是从数据中泛化。深度学习通过将人脑的树突、细胞体和突触之间的平行关系，在深度学习的背景下，它们作为给定模型的输入、节点和输出，如图所示，使得这一点成为可能：
- en: '![Figure 8.1 – Comparison between the human brain and a neural network ](img/B17761_08_001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 人脑与神经网络的比较](img/B17761_08_001.jpg)'
- en: Figure 8.1 – Comparison between the human brain and a neural network
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 人脑与神经网络的比较
- en: Some of the biggest benefits behind such an implementation revolve around the
    idea of feature engineering. Earlier in this book, we saw how features can be
    created or summarized using various methods such as basic mathematical operations
    (x2) or through complex algorithms such as **Principal Component Analysis** (**PCA**).
    Manually engineered features can be very time-consuming and not feasible in practice,
    which is where the field of deep learning can come in, with the ability to learn
    the many underlying features in a given dataset directly from the data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实现背后的最大好处之一是特征工程的概念。在这本书的早期，我们看到了如何使用各种方法创建或总结特征，例如基本的数学运算（x2）或通过如**主成分分析**（PCA）等复杂算法。手动构建的特征可能非常耗时，在实践中不可行，这就是深度学习领域可以发挥作用的地方，它能够直接从数据中学习给定数据集的许多潜在特征。
- en: Within the field of **biotechnology**, most applications, ranging from the early
    stages of therapeutic discovery all the way downstream to manufacturing, are generally
    data-rich processes. However, much of the data that's been collected will have
    little to no use on its own, or perhaps the data that's been collected is for
    different batches of a particular molecule. Perhaps the data is extensive for
    some molecules and less extensive for others. In many of these cases, using deep
    learning models can come to your aid when it comes to features that are relative
    to the traditional machine learning models we have discussed so far.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物技术领域，大多数应用，从治疗发现的早期阶段到下游的制造，通常是数据丰富的过程。然而，收集到的许多数据本身可能几乎没有任何用途，或者收集的数据可能是针对特定分子的不同批次。也许对于某些分子数据量很大，而对于其他分子则较少。在这些许多情况下，使用深度学习模型可以帮助我们处理与之前讨论的传统机器学习模型相关的特征。
- en: 'We can think of features at three different levels:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将特征视为三个不同级别：
- en: '**Low-level features**, such as individual amino acids, a protein, or the elements
    of a small molecule.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低级特征**，例如单个氨基酸、一个蛋白质或小分子的元素。'
- en: '**Mid-level features**, such as the amino acid sequences of a protein and the
    functional groups of a small molecule.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中级特征**，例如蛋白质的氨基酸序列和小分子的官能团。'
- en: '**High-level features**, such as the overall structure or the classification
    of a protein or the geometric shape of a small molecule.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级特征**，例如蛋白质的整体结构或分类，或小分子的几何形状。'
- en: 'The following diagram shows a graphical representation of these features:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这些特征的图形表示：
- en: '![Figure 8.2 – The three types of features and some associated examples ](img/B17761_08_002.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 三种类型的特征及其一些相关示例](img/B17761_08_002.jpg)'
- en: Figure 8.2 – The three types of features and some associated examples
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 三种类型的特征及其一些相关示例
- en: 'In many instances, architecting a robust deep learning model can unlock a more
    powerful predictive model relative to its machine learning counterpart. In many
    of the machine learning models we have explored, we attempted to improve the model''s
    performance not only by tuning and adjusting the hyperparameters but also by making
    a conscious decision to use datasets with a sufficient amount of data. Increasing
    the size of the datasets will likely not lead to any significant improvement in
    our machine learning models. However, this is not always the case with deep learning
    models, which tend to improve in performance when more data is made available.
    We can see a visual depiction of this in the following diagram:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，构建一个健壮的深度学习模型可以解锁相对于其机器学习对应模型更强大的预测模型。在我们已经探索的许多机器学习模型中，我们尝试通过调整和调整超参数来提高模型性能，并且有意识地决定使用具有足够数据量的数据集。增加数据集的大小可能不会导致我们的机器学习模型有任何显著的改进。然而，这并不总是适用于深度学习模型，当有更多数据可用时，深度学习模型往往会提高性能。我们可以在以下图表中看到这种视觉表示：
- en: '![Figure 8.3 – A graphical representation of machine learning versus deep learning
    ](img/B17761_08_003.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 机器学习与深度学习的图形表示](img/B17761_08_003.jpg)'
- en: Figure 8.3 – A graphical representation of machine learning versus deep learning
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 机器学习与深度学习的图形表示
- en: Using neural networks within the context of machine learning has seen a major
    surge in recent years, which can be attributed to the increased use of big data
    within most industries, the decreased expense of computational hardware such as
    CPUs and GPUs, and the growing community that supports much of the open source
    software and packages that are available today. Two of the most common packages
    out there for developing deep learning models are TensorFlow and Keras – we will
    explore these two later in this chapter. Before we do, let's go ahead and talk
    about the architecture behind a deep learning model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下使用神经网络在近年来经历了重大增长，这可以归因于大多数行业大数据使用的增加，计算硬件（如CPU和GPU）成本的降低，以及支持今天大多数开源软件和包的社区的增长。用于开发深度学习模型的最常见的两个包是TensorFlow和Keras
    – 我们将在本章后面探讨这两个包。在我们这样做之前，让我们先谈谈深度学习模型背后的架构。
- en: The perceptron
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知器
- en: 'One of the most important building blocks of any deep learning model is the
    perceptron. A perceptron is an algorithm that''s used for developing supervised
    binary classifiers, first invented in 1958 by Frank Rosenblatt, who is sometimes
    called the father of deep learning. A perceptron generally consists of four major
    parts:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型最重要的构建块之一是感知器。感知器是一种用于开发监督二分类算法的算法，由弗兰克·罗森布拉特于1958年首次发明，有时被称为深度学习的之父。感知器通常由四个主要部分组成：
- en: The **input** values, which are generally taken from a given dataset.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**值，通常是从给定的数据集中获取的。'
- en: The **weights**, which are values by which the input values are multiplied.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**，即乘以输入值的值。'
- en: The **net sum**, which is the sum of all the values from each of the inputs.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**净和**，即所有输入值的总和。'
- en: The **activation function**, which maps a resulting value to an output.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**，它将结果值映射到输出。'
- en: 'The following diagram shows a graphical representation of these four parts
    of a perceptron:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了感知器这四个部分的图形表示：
- en: '![Figure 8.4 – A graphical representation of a perceptron ](img/B17761_08_004.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – 一个感知器的图形表示](img/B17761_08_004.jpg)'
- en: Figure 8.4 – A graphical representation of a perceptron
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 一个感知器的图形表示
- en: 'There are three main steps that a perceptron takes to arrive at a predicted
    output from a given set of input values:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器到达给定输入值预测输出的三个主要步骤如下：
- en: The **input values** (x1, x2, and so on) are multiplied by their respective
    weights (w1, w2, and so on). These weights are determined in the training process
    for this model so that a different weight is assigned to each of the input values.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入值**（x1，x2等）乘以其相应的权重（w1，w2等）。这些权重在模型的训练过程中确定，以便为每个输入值分配不同的权重。'
- en: All the values from each of the calculations are summed together in a value
    known as the **weighted sum**.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次计算的所有值都汇总到一个称为**加权求和**的值中。
- en: The weighted sum is then applied to the **activation function** to map the value
    to a given output. The specific activation function that's used is dependent on
    the given situation. For example, within the context of a unit step activation
    function, values would either be mapped to 0 or 1.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将加权求和应用于**激活函数**，将值映射到给定的输出。所使用的特定激活函数取决于给定的情况。例如，在单位步长激活函数的背景下，值将被映射到0或1。
- en: 'When viewed from a mathematical perspective, we can define the output value,
    ![](img/Formula_B17761__08_016.png), as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，我们可以定义输出值，![](img/Formula_B17761__08_016.png)，如下所示：
- en: '![](img/Formula_B17761__08_001.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761__08_001.jpg)'
- en: 'In this equation, *g* is the activation function, *w*o is the bias, and the
    final components are the sum of the **linear combination** of input values:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*g*是激活函数，*w*o是偏置，最后的组成部分是输入值的**线性组合**的总和：
- en: '![](img/Formula_B17761__08_002.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761__08_002.jpg)'
- en: So, in this equation, ![](img/Formula_B17761__08_003.png) and ![](img/Formula_B17761__08_004.png)
    account for the final output value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个方程中，![](img/Formula_B17761__08_003.png)和![](img/Formula_B17761__08_004.png)考虑了最终的输出值。
- en: A perceptron is one of the simplest deep learning building blocks out there
    and can be expanded quite drastically by increasing the number of **hidden layers**.
    Hidden layers are the layers that lay in-between the input and output layers.
    Models with very few hidden layers are generally referred to as **neural networks**
    or multilayer perceptrons, whereas models with many hidden layers are referred
    to as **deep neural networks**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是现有最简单的深度学习构建块之一，可以通过增加**隐藏层**的数量来大幅扩展。隐藏层是位于输入层和输出层之间的层。具有很少隐藏层的模型通常被称为**神经网络**或多层感知器，而具有许多隐藏层的模型被称为**深度神经网络**。
- en: Each of these layers consists of several **nodes**, and the flow of data is
    similar to that of the perceptron we saw previously. The number of input nodes
    (*x*1*, x*2*, x*3) generally corresponds to the number of **features** in a given
    dataset, whereas the number of output nodes generally corresponds to the number
    of outputs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些每一层都由几个**节点**组成，数据流与之前看到的感知器类似。输入节点的数量（*x*1*, x*2*, x*3）通常对应于给定数据集中的**特征**数量，而输出节点的数量通常对应于输出数量。
- en: 'The following diagram is a graphical representation of the difference between
    neural networks and deep learning:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图是神经网络与深度学习之间差异的图形表示：
- en: '![Figure 8.5 – Difference between neural networks and deep learning ](img/B17761_08_005.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 神经网络与深度学习之间的差异](img/B17761_08_005.jpg)'
- en: Figure 8.5 – Difference between neural networks and deep learning
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 神经网络与深度学习之间的差异
- en: In the previous diagram, we can see the neural network or multilayer perceptron
    (on the left) consisting of an input layer, a single hidden layer with four nodes,
    and an output layer with four nodes. Similar to the single perceptron we saw earlier,
    the idea here is that each of the nodes within the hidden layer will intake the
    input nodes, multiplied by some value, and then pass them through an **activation
    function** to yield output. On the right, we can see a similar model, but the
    values are passed through several hidden layers before determining a final output
    value.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一张图中，我们可以看到由输入层、一个包含四个节点的单个隐藏层和一个包含四个节点的输出层组成的神经网络或多层感知器（在左侧）。与之前看到的单个感知器类似，这里的想法是隐藏层中的每个节点都会接收输入节点，乘以某个值，然后通过一个**激活函数**传递以产生输出。在右侧，我们可以看到一个类似模型，但值在确定最终输出值之前会通过几个隐藏层。
- en: Exploring the different types of deep learning models
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索不同类型的深度学习模型
- en: There are many different types of neural networks and deep learning architectures
    out there that differ in function, shape, data flow, and much more. There are
    three types of neural networks that have gained a great deal of popularity in
    recent years, given their promise and robustness with various types of data. First,
    we will explore the simplest of these architectures, known as a multilayer perceptron.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有各种各样的神经网络和深度学习架构，它们在功能、形状、数据流等方面有所不同。近年来，有三种类型的神经网络因其对各种类型数据的承诺和鲁棒性而获得了极大的普及。首先，我们将探讨这些架构中最简单的一种，被称为多层感知器。
- en: Multilayer perceptron
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'A **Multilayer Perceptron** (**MLP**) is one of the most basic types of **Artificial
    Neural Networks** (**ANNs**). This type of network is simply composed of layers
    in which data flows in a forward manner, as shown in the previous diagram. Data
    flows from the input layer to one or more hidden layers, and then finally to an
    output layer in which a prediction is produced. In essence, each layer attempts
    to learn and calculate certain weights. ANNs and MLPs come in many different shapes
    and sizes: they can have a different number of nodes in each layer, a different
    number of inputs, or even a different number of outputs. We can see a visual depiction
    of this in the following diagram:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）是**人工神经网络**（**ANNs**）中最基本的一种类型。这种网络简单地由层组成，数据以正向方式在这些层中流动，如前一张图所示。数据从输入层流向一个或多个隐藏层，然后最终流向输出层，在那里产生预测。本质上，每一层都试图学习和计算某些权重。ANNs和MLPs有多种不同的形状和大小：它们可以在每一层有不同的节点数、不同的输入数，甚至不同的输出数。我们可以在以下图中看到这种视觉描述：'
- en: '![Figure 8.6 – Two examples of MLPs ](img/B17761_08_006.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – MLP的两个示例](img/B17761_08_006.jpg)'
- en: Figure 8.6 – Two examples of MLPs
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – MLP的两个示例
- en: MLP models are generally very versatile but are most commonly used for structured
    tabular data, such as the structured protein classification dataset we have been
    working with. In addition, they can be used for image data or even text data.
    MLPs, however, generally tend to suffer when it comes to sequential data such
    as protein sequences and time-series datasets.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: MLP模型通常非常通用，但最常用于结构化表格数据，例如我们一直在工作的结构化蛋白质分类数据集。此外，它们还可以用于图像数据或甚至文本数据。然而，MLPs在处理如蛋白质序列和时间序列数据等序列数据时通常会遇到困难。
- en: Convolutional neural networks
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '**Convolutional Neural Networks** (**CNNs**) are deep learning algorithms that
    are commonly used for processing and analyzing image data. CNNs can take in images
    as input data and restructure them to determine the importance through weights
    and biases, allowing it to distinguish between the features of one image relative
    to another. Similar to our earlier discussion of how deep learning is similar
    to neurons in the brain, CNNs are also analogous to the connectivity of neurons
    in the human brain and the visual cortex when it comes to the sensitivity of regions,
    similar to the concept of receptive fields. One of the biggest areas of success
    for CNN models is their ability to capture spatial dependencies, as well as temporal
    dependencies, in images through the use of filters. We can see a visual representation
    of this in the following diagram:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）是常用的深度学习算法，用于处理和分析图像数据。CNNs可以接受图像作为输入数据，并通过权重和偏置重新结构化它们以确定重要性，从而使其能够区分一个图像相对于另一个图像的特征。类似于我们之前讨论的深度学习如何类似于大脑中的神经元，CNNs在区域敏感性方面也类似于人脑和视觉皮层的神经元连接，类似于感受野的概念。CNN模型最大的成功之一是它们能够通过使用过滤器捕获图像中的空间依赖性和时间依赖性。我们可以在以下图表中看到这一视觉表示：'
- en: '![Figure 8.7 – A representation of a CNN ](img/B17761_08_007.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – CNN的表示](img/B17761_08_007.jpg)'
- en: Figure 8.7 – A representation of a CNN
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – CNN的表示
- en: 'Let''s take, for example, the idea of creating an image classification model.
    We could use an ANN and convert a 2D image of pixels by flattening it. An image
    with a 4x4 matrix of pixels would now become a 1x16 vector instead. This change
    would cause two main drawbacks:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以创建图像分类模型的想法为例。我们可以使用ANN并将2D像素图像通过展平转换为向量。一个4x4像素矩阵的图像现在将变成一个1x16的向量。这种变化将导致两个主要缺点：
- en: The spatial features of the image would be lost, and thereby reduce the robustness
    of any trained model.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像的空间特征将会丢失，从而降低任何训练模型的鲁棒性。
- en: The number of input features would increase quite drastically as the image size
    grows.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着图像尺寸的增长，输入特征的数量将急剧增加。
- en: CNNs can overcome this by extracting high-level features from the images, allowing
    them to be quite effective with image-based datasets.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs可以通过从图像中提取高级特征来克服这一点，这使得它们在基于图像的数据集中非常有效。
- en: Recurrent neural networks
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: '**Recurrent Neural Networks** (**RNNs**) are commonly used algorithms that
    are generally applied to sequence-based datasets. They are quite similar in architecture
    to the ANNs we discussed earlier, but RNNs can remember their input using internal
    memory, making them quite effective with sequential datasets in which previous
    data is of great importance.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）是常用的算法，通常应用于基于序列的数据集。它们的架构与我们之前讨论的ANNs非常相似，但RNNs可以通过内部记忆记住它们的输入，这使得它们在先前数据非常重要的序列数据集中非常有效。'
- en: 'Take, for example, a protein sequence consisting of various amino acids. To
    predict the class of the protein, or its general structure, the model would not
    only need to know which amino acids were used but the order in which they were
    used as well. RNNs and their many derivatives have been central to the many advances
    in deep learning within the field of biology and biotechnology. We can see a visual
    representation of this in the following diagram:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个由各种氨基酸组成的蛋白质序列为例。为了预测蛋白质的类别或其一般结构，模型不仅需要知道使用了哪些氨基酸，还需要知道它们的使用顺序。RNNs及其许多衍生品在生物技术和生物技术领域的深度学习许多进步中起到了核心作用。我们可以在以下图表中看到这一视觉表示：
- en: '![Figure 8.8 – A representation of an ANN node versus an RNN node ](img/B17761_08_008.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – ANN节点与RNN节点的表示](img/B17761_08_008.jpg)'
- en: Figure 8.8 – A representation of an ANN node versus an RNN node
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – ANN节点与RNN节点的表示
- en: 'There are several advantages when it comes to using RNNs as predictor models,
    with the main benefits being as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RNN作为预测模型有几个优点，主要好处如下：
- en: Their ability to capture the dependency between data points such as words in
    a sentence
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们能够捕捉数据点之间的依赖关系，例如句子中的单词
- en: Their ability to share parameters across time steps, thus decreasing the overall
    computational cost
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们能够在时间步长之间共享参数，从而降低整体计算成本
- en: Because of this, RNNs have become increasingly popular architectures for developing
    models that solve problems related to scientific sequence data such as proteins
    and DNA, as well as text and time-series data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个原因，RNN已成为开发解决与蛋白质和DNA等科学序列数据以及文本和时间序列数据相关问题的模型时越来越受欢迎的架构。
- en: Long short-term memory
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长短期记忆
- en: '**Long Short-Term Memory** (**LSTM**) models are a type of RNN designed with
    the capability of learning long-term dependencies when handling sequence-based
    problems. Commonly used with text-based data for classification, translation,
    and recognition, LSTMs have gained an unprecedented surge in popularity over the
    years. We can depict the structure of a standard RNN as we did previously, but
    structured slightly differently:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）模型是一种设计用于处理基于序列问题的长期依赖能力的RNN。通常与基于文本的数据一起用于分类、翻译和识别，LSTMs在近年来获得了前所未有的流行。我们可以像之前那样描绘标准RNN的结构，但结构略有不同：'
- en: '![Figure 8.9 – The inner workings of an RNN versus an LSTM ](img/B17761_08_009.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图8.9 – RNN与LSTM的内部工作原理](img/B17761_08_009.jpg)'
- en: Figure 8.9 – The inner workings of an RNN versus an LSTM
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – RNN与LSTM的内部工作原理
- en: In the preceding diagram, *X*t is an input vector, *h*tis a hidden layer vector,
    and *o*t is an output vector. On the other hand, and using some of the same elements,
    an LSTM can be structured quite similarly. Without diving into too much detail,
    the core idea behind an LSTM is the cell state (the top horizontal line). This
    state operates similarly to a conveyor belt in which data flows linearly through
    it. Gates within the cell are methods that optionally allow information to be
    added to the state. An LSTM has three gates, all leading to the cell state.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，*X*t是一个输入向量，*h*t是一个隐藏层向量，*o*t是一个输出向量。另一方面，使用一些相同的元素，LSTM的结构可以相当相似。不深入细节，LSTM的核心思想是细胞状态（顶部水平线）。这个状态的工作方式类似于传送带，数据线性地通过它。细胞内的门是可选地允许信息添加到状态的方法。LSTM有三个门，都指向细胞状态。
- en: 'Although LSTM models and their associated diagrams can be quite intimidating
    at first, they have proven their worth time and time again in various areas. Most
    recently, LSTM models have been used as generative models for antibody design,
    as well as classification models for protein sequence-structure classification.
    Now that we have explored several common deep learning architectures, let''s go
    ahead and explore their main components: activation functions.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LSTM模型及其相关图表一开始可能让人感到有些令人畏惧，但它们在各种领域一次又一次地证明了它们的价值。最近，LSTM模型被用作抗体设计的生成模型，以及蛋白质序列-结构分类的分类模型。现在我们已经探讨了几个常见的深度学习架构，让我们继续探索它们的主要组件：激活函数。
- en: Selecting an activation function
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择激活函数
- en: 'Recall that, in the previous section, we used an activation function to map
    a value to a particular output, depending on the value. We will define an activation
    function as a mathematical function that defines the output of an individual node
    using an input value. Using the analogy of the human brain, these functions simply
    act as gatekeepers, deciding what will be *fired off* to the next neuron. There
    are several features that an activation function should have to allow the model
    to learn most effectively from it:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在前一节中，我们使用激活函数将一个值映射到特定的输出，这取决于该值。我们将激活函数定义为一种数学函数，它使用输入值定义单个节点的输出。用人类大脑的类比，这些函数简单地充当守门人，决定什么将被*触发*到下一个神经元。激活函数应该具有一些特性，以便模型能够从它那里最有效地学习：
- en: The avoidance of a vanishing gradient
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免梯度消失
- en: A low computational expense
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低计算成本
- en: 'Artificial neural networks are trained using a process known as gradient descent.
    For this example, let''s assume that there is a two-layer neural network:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络使用称为梯度下降的过程进行训练。在这个例子中，让我们假设有一个两层神经网络：
- en: '![](img/Formula_B17761__08_005.jpg)![](img/Formula_B17761__08_006.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/Formula_B17761__08_005.jpg)![公式图片](img/Formula_B17761__08_006.jpg)'
- en: 'The overall network can be represented as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 整个网络可以表示如下：
- en: '![](img/Formula_B17761__08_007.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761__08_007.jpg)'
- en: 'When the weights are calculated in a step known as a backward pass, the result
    becomes as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播步骤中计算权重时，结果如下：
- en: '![](img/Formula_B17761__08_008.jpg)![](img/Formula_B17761__08_009.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761__08_008.jpg)![](img/Formula_B17761__08_009.jpg)'
- en: 'Upon determining the derivative, the function becomes as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 确定导数后，函数变为如下：
- en: '![](img/Formula_B17761__08_010.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761__08_010.jpg)'
- en: If this process were to continue through many layers during the backpropagation
    step, there would be a considerable reduction in the value of the gradient for
    the initial layers, thus halting the model's ability to learn. This is the concept
    of **vanishing gradients**.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个过程在反向传播步骤中通过许多层继续进行，那么初始层的梯度值将会有相当大的减少，从而阻碍模型的学习能力。这就是**梯度消失**的概念。
- en: On the other hand, **computational expense** is also a feature that must be
    considered before designing and deploying any given model. The activation functions
    that are applied from one layer to another must be calculated many times, so the
    expense of the calculation should be kept to a minimum to avoid longer training
    periods. The flow of information from an input layer to an output layer is called
    **forward propagation**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**计算成本**也是在设计并部署任何给定模型之前必须考虑的一个特性。从一层到另一层的激活函数必须多次计算，因此计算成本应保持在最低，以避免更长的训练周期。从输入层到输出层的信息流动被称为**正向传播**。
- en: 'There are many different types of activation functions out there that are commonly
    used for various purposes. Although this is not a hard rule, some activation functions
    are generally used with specific deep learning layers. For example, `sigmoid`
    and **Tanh** activation functions are commonly used with **RNNs**. Let''s take
    a moment and look at the three most common activation functions you will likely
    encounter in your journey:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同类型的激活函数，它们通常用于各种目的。尽管这不是一条硬性规则，但一些激活函数通常与特定的深度学习层一起使用。例如，`sigmoid` 和 **Tanh**
    激活函数通常与 **RNNs** 一起使用。让我们花点时间看看你可能在旅途中遇到的三种最常见的激活函数：
- en: '![Figure 8.10 – Various types of activation functions by model type ](img/B17761_08_010.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 8.10 – Various types of activation functions by model type ](img/B17761_08_010.jpg)'
- en: Figure 8.10 – Various types of activation functions by model type
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 按模型类型划分的各种激活函数
- en: 'With some of these types now in mind, let''s go ahead and explore them in a
    little more detail. `sigmoid` functions are probably some of the most commonly
    used functions within the field of deep learning. It is a non-linear activation
    function that is also sometimes referred to as a logistic function (remember logistic
    regression? Hint hint). `sigmoid` functions are unique in the sense that they
    can map values to either a 0 or a 1\. Using the `numpy` library, we can easily
    put together a `Python` function to calculate it:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在心中有了这些类型的一些概念后，让我们更深入地探讨它们。`sigmoid` 函数可能是深度学习领域中应用最广泛的一些函数。它是一种非线性激活函数，有时也被称为逻辑函数（还记得逻辑回归吗？提示提示）。`sigmoid`
    函数的独特之处在于它们可以将值映射到 0 或 1。使用 `numpy` 库，我们可以轻松地编写一个 `Python` 函数来计算它：
- en: '[PRE0]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using this function alongside the `numpy` library, we can generate some data
    and plot our `sigmoid` function accordingly:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数以及 `numpy` 库，我们可以生成一些数据并相应地绘制 `sigmoid` 函数：
- en: '[PRE1]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In return, we yield the following diagram, showing the curved nature of a `sigmoid`
    function. Notice how the upper and lower ranges are 1 and 0, respectively:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回报，我们得到以下图表，显示了 `sigmoid` 函数的曲线性质。注意上下限分别是 1 和 0：
- en: '![Figure 8.11 – A simple sigmoid function ](img/B17761_08_011.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 8.11 – A simple sigmoid function ](img/B17761_08_011.jpg)'
- en: Figure 8.11 – A simple sigmoid function
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 一个简单的 sigmoid 函数
- en: One of the biggest issues with a `sigmoid` activation function is that the outputs
    can **saturate** in the sense that values greater than 1.0 are mapped to one,
    and values that are smaller than 0 are mapped to 0\. This can cause some models
    to fail to generalize or learn from the data and is related to the vanishing gradients
    issue we discussed earlier in this chapter.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid` 激活函数的一个最大问题是输出可能会**饱和**，即大于 1.0 的值被映射到 1，而小于 0 的值被映射到 0。这可能导致某些模型无法泛化或从数据中学习，这与我们在本章前面讨论的梯度消失问题有关。'
- en: 'On the other hand, another common activation function is `sigmoid` function.
    The Tanh function is symmetric in the sense that it passes through the point (0,
    0) and it ranges to the values of 1 and -1, unlike its `sigmoid` counterpart,
    making it a slightly better function. Instead of defining our functions in Python,
    as we did previously, we can take advantage of the optimized functions in the
    `numpy` library:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，另一个常见的激活函数是`sigmoid`函数。Tanh函数在意义上是对称的，因为它通过点(0, 0)，并且它的值范围在1和-1之间，与它的`sigmoid`对应函数不同，使其成为一个稍微更好的函数。我们不必像之前那样在Python中定义我们的函数，而是可以利用`numpy`库中的优化函数：
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Upon executing this code, we retrieve the following diagram. Notice how the
    center of the diagram is the point (0, 0), while the upper and lower values are
    1.00 and -1.00, respectively:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，我们得到以下图表。注意图表的中心是点(0, 0)，而上下值分别为1.00和-1.00：
- en: '![Figure 8.12 – A simple Tanh function ](img/B17761_08_012.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图8.12 – 简单的Tanh函数](img/B17761_08_012.jpg)'
- en: Figure 8.12 – A simple Tanh function
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 – 简单的Tanh函数
- en: Similar to its `sigmoid` counterpart, the `sigmoid`, making it a slightly better
    function to use.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与其`sigmoid`对应函数类似，`sigmoid`，使其成为一个稍微更好的函数来使用。
- en: 'Finally, yet another commonly used activation function is the **Rectified Linear
    Unit** (**ReLU**). **ReLU** activation functions were specifically developed to
    avoid saturation when handling larger numbers. The non-linear nature of this function
    allows it to learn the patterns within the data, whereas the linear nature of
    the function allows it to be easily interpretable relative to the other functions
    we have seen so far. Let''s go ahead and explore this in Python:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一个常用的激活函数是**Rectified Linear Unit**（**ReLU**）。**ReLU**激活函数是专门开发来避免处理较大数值时的饱和现象的。该函数的非线性特性使其能够学习数据中的模式，而其线性特性使得它相对于我们迄今为止看到的其他函数更容易解释。让我们继续在Python中探索这一点：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Executing this code yields the following diagram. Notice how the **ReLU** function
    takes advantage of both the linear and non-linear nature of activation functions,
    giving it the best of both worlds:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后得到以下图表。注意**ReLU**函数利用了激活函数的线性和非线性特性，使其兼具两者的优点：
- en: '![Figure 8.13 – A simple ReLU function ](img/B17761_08_013.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图8.13 – 简单的ReLU函数](img/B17761_08_013.jpg)'
- en: Figure 8.13 – A simple ReLU function
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 – 简单的ReLU函数
- en: The **ReLU** activation function has become one of the most popular, if not
    **the** most popular, activation function among data scientists because of its
    ease of implementation, and its robust speed within the model development and
    training process. **ReLU** activation functions do, however, have their downsides.
    For example, the function cannot be differentiable when x = 0 (at point 0, 0),
    so **gradient descent** cannot be computed for that value.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLU**激活函数已经成为数据科学家中最受欢迎的激活函数之一，如果不是**最受欢迎**的，这得益于其实施的简便性和在模型开发和训练过程中的稳健速度。然而，**ReLU**激活函数确实有其缺点。例如，当x
    = 0（在点0, 0）时，该函数不可微分，因此无法计算该值的梯度下降。'
- en: Yet another activation function worth mentioning is known as **Softmax**. **Softmax**
    is very different from the other activation functions we have looked at so far
    because it computes a probability distribution for a list of values that are proportional
    to the relative scale of each of the values in the vector, the sum of which always
    equals 1\.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得提及的激活函数被称为**Softmax**。**Softmax**与我们迄今为止看到的其他激活函数非常不同，因为它为一系列值计算一个概率分布，这些值与向量中每个值的相对尺度成比例，它们的总和始终等于1。
- en: 'Commonly used for `numpy` library:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 常用于`numpy`库：
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Upon printing the values, we retrieve the following results:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 打印值后，我们得到以下结果：
- en: '![Figure 8.14 – Results of a Softmax function ](img/B17761_08_014.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图8.14 – Softmax函数的结果](img/B17761_08_014.jpg)'
- en: Figure 8.14 – Results of a Softmax function
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 – Softmax函数的结果
- en: The two main advantages that come from using **Softmax** as an activation function
    are that the output values range between 0 and 1 and that they always sum to a
    value of 1.0\. In return, this allows the function to be used to understand cross-entropy
    when it comes to the idea of divergence. We will visit this topic in more detail
    later in this chapter.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**Softmax**作为激活函数的两个主要优势是输出值介于0和1之间，并且它们总是加起来等于1.0。作为回报，这使得函数可以用来理解交叉熵中的发散概念。我们将在本章后面更详细地探讨这个话题。
- en: 'The various activation functions we have visited so far each have their pros
    and cons when it comes to using them in various applications. For example, `sigmoid`
    functions are commonly used for binary and multilabel classification applications,
    whereas **Softmax** functions are generally used for multiclass classification.
    This is not a hard rule, but simply a guide to help you match a function with
    the highest chance of success with its respective application:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论的各种激活函数在应用于各种应用时各有优缺点。例如，`sigmoid`函数通常用于二元和多标签分类应用，而**Softmax**函数通常用于多类分类。这并不是一个硬性规则，而只是一个指南，帮助你将函数与其最有可能成功的应用相匹配：
- en: '![Figure 8.15 – Activation functions by problem type ](img/B17761_08_015.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图8.15 – 按问题类型划分的激活函数](img/B17761_08_015.jpg)'
- en: Figure 8.15 – Activation functions by problem type
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 – 按问题类型划分的激活函数
- en: Activation functions are a vital part of any deep learning model and are often
    regarded as *game changers* since simply changing one function for another can
    boost the performance of a model quite drastically. We will take a closer look
    at how model performance can be quantified within the scope of deep learning in
    the following section.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是任何深度学习模型的重要组成部分，并且通常被视为*变革者*，因为仅仅改变一个函数为另一个函数就可以极大地提高模型的表现。在下一节中，我们将更详细地探讨如何在深度学习的范围内量化模型性能。
- en: Measuring progress with loss
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用损失衡量进度
- en: 'When we discussed the areas of classification and regression, we outlined a
    few measures to measure and quantify the performance of our models relative to
    one another. When it came to classification, we used **precision** and **accuracy**,
    whereas, in regression, we used **MAE** and **MSE**. Within the confines of deep
    learning, we will use a metric known as **loss**. The **loss** of a neural network
    is simply a measure of the cost that''s incurred from making an incorrect prediction.
    Take, for example, a simple neural network with three input values and a single
    output value:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论分类和回归的领域时，我们概述了一些衡量和量化我们模型之间性能的度量。当涉及到分类时，我们使用了**精确度**和**准确度**，而在回归中，我们使用了**MAE**和**MSE**。在深度学习的范围内，我们将使用一个称为**损失**的度量。神经网络的**损失**简单地说就是由于做出错误预测而产生的成本的度量。以一个简单的具有三个输入值和一个输出值的神经网络为例：
- en: '![Figure 8.16 – A neural network showing input and output values ](img/B17761_08_016.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图8.16 – 展示输入和输出值的神经网络](img/B17761_08_016.jpg)'
- en: Figure 8.16 – A neural network showing input and output values
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 – 展示输入和输出值的神经网络
- en: 'In this case, we have the values *[2.3, 3.3, 1.2]* being used as input values
    to the model, with a predicted value of 0.2 relative to the actual value of 1.0\.
    We can demonstrate the loss as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用*[2.3, 3.3, 1.2]*作为模型的输入值，相对于实际值1.0的预测值为0.2。我们可以如下展示损失：
- en: '![](img/Formula_B17761__08_011.jpg)![](img/Formula_B17761__08_012.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761__08_011.jpg)![](img/Formula_B17761__08_012.jpg)'
- en: In this function, ![](img/Formula_B17761__08_013.png)is the predicted value,
    while ![](img/Formula_B17761__08_014.png) is the actual value.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，![](img/Formula_B17761__08_013.png)是预测值，而![](img/Formula_B17761__08_014.png)是实际值。
- en: '**Empirical loss**, on the other hand, is a measure of the total loss for the
    entirety of the dataset. We can represent empirical loss as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**经验损失**是整个数据集总损失的度量。我们可以如下表示经验损失：
- en: '![](img/Formula_B17761__08_015.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17761__08_015.jpg)'
- en: In this function, we sum the total losses for all calculations.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们计算所有计算的损失总和。
- en: 'Throughout the model training process, our main objective will be to minimize
    this loss in a process known as **loss optimization**. The main idea behind loss
    optimization is to identify a set of weights that help achieve the lowest loss
    possible. We can visualize the idea of gradient descent as the process of moving
    from an initial starting value with a high loss, to a final value with a low loss.
    Our objective will be to ensure that converge in a global minimum rather than
    a local minimum, as depicted in the following diagram:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个模型训练过程中，我们的主要目标将是通过一个称为**损失优化**的过程来最小化这个损失。损失优化的主要思想是确定一组权重，以帮助实现可能的最小损失。我们可以将梯度下降的概念可视化为一个从初始起始值（损失高）移动到最终值（损失低）的过程。我们的目标是确保收敛到全局最小值而不是局部最小值，如下面的图所示：
- en: '![Figure 8.17 – The process of loss optimization ](img/B17761_08_017.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图8.17 – 损失优化的过程](img/B17761_08_017.jpg)'
- en: Figure 8.17 – The process of loss optimization
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 – 损失优化过程
- en: Each step we take to get closer to the minimum value is known as a **learning
    step**, the **learning rate** of which is generally determined by the user. This
    parameter is just one of many that we can specify using Keras, which we will learn
    about in the following section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们每向前迈出一步以接近最小值，都称为一个**学习步**，其**学习率**通常由用户决定。这个参数只是我们可以使用Keras指定的许多参数之一，我们将在下一节中了解它。
- en: Deep learning with Keras
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras进行深度学习
- en: 'Within the realm of data science, the availability and use of various **frameworks**
    will always be crucial to standardizing the methods we use to develop and deploy
    models. So far, we have focused our machine learning efforts on using the scikit-learn
    framework. Throughout this section, we will learn about three new frameworks specifically
    focused on deep learning: **Keras**, **TensorFlow**, and **PyTorch**. These two
    frameworks are the two most popular amongst data scientists when it comes to developing
    various deep learning models as they offer a comprehensive list of APIs for numerous
    problems and use cases.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学的领域内，各种**框架**的可用性和使用始终是我们用来标准化开发和部署模型的方法的关键。到目前为止，我们已将机器学习努力集中在使用scikit-learn框架上。在本节中，我们将了解三个专注于深度学习的全新框架：**Keras**、**TensorFlow**和**PyTorch**。这两个框架在数据科学家开发各种深度学习模型时最为流行，因为它们提供了针对众多问题和用例的全面API列表。
- en: Understanding the differences between Keras and TensorFlow
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Keras和TensorFlow之间的区别
- en: 'Although these two platforms allow users to develop deep learning models, there
    are a few differences to know about. TensorFlow is known as an end-to-end machine
    learning platform that offers a comprehensive list of libraries, tools, and numerous
    resources. Users can manage data, develop models, and deploy solutions. Unlike
    most other libraries, **TensorFlow** offers both low and high levels of abstractions
    through their APIs, giving users lots of flexibility when it comes to developing
    models. On the other hand, **Keras** offers high-level APIs for developing neural
    networks, which run using **TensorFlow**. The high-level nature of this library
    allows users to begin developing and training complex neural networks with only
    a few lines of Python code. Keras is generally regarded as user-friendly, modular,
    and extendable. A third library exists that is commonly used in the deep learning
    space known as **PyTorch**. **PyTorch** is a low-level API known for its remarkable
    speed and optimization in the model training process. The architectures within
    this library are generally complex and not appropriate for introductory material,
    so they are not within the scope of this book. However, it is worth mentioning
    as it is one of the most common libraries in the machine learning space that you
    will likely encounter. Let''s take a closer look at all three:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两个平台允许用户开发深度学习模型，但有一些区别需要了解。TensorFlow被称为端到端的机器学习平台，提供了全面的库、工具和众多资源。用户可以管理数据、开发模型和部署解决方案。与大多数其他库不同，**TensorFlow**通过其API提供低级和高级抽象，使用户在开发模型时具有很大的灵活性。另一方面，**Keras**提供用于开发神经网络的API，这些API使用**TensorFlow**运行。这个库的高级特性使得用户只需几行Python代码就可以开始开发和训练复杂的神经网络。Keras通常被认为用户友好、模块化和可扩展。还有一个在深度学习空间中常用的第三方库，称为**PyTorch**。**PyTorch**是一个低级API，以其在模型训练过程中的卓越速度和优化而闻名。这个库中的架构通常很复杂，不适合初学者材料，因此它们不在这个书的范围内。然而，它值得提及，因为它是在机器学习空间中最常见的库之一，你可能会遇到。让我们更详细地看看这三个：
- en: '![Figure 8.18 – A comparison between three of the most common deep learning
    frameworks ](img/013.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图8.18 – 三种最常见的深度学习框架的比较](img/013.jpg)'
- en: Figure 8.18 – A comparison between three of the most common deep learning frameworks
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 – 三种最常见的深度学习框架的比较
- en: There are pros and cons for each of these libraries and you should select one
    of these libraries based on the task you set out to accomplish. Given that we
    are exploring the development of deep learning models for the first time, we will
    focus on using the Keras library.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 每个库都有其优缺点，你应该根据你设定的任务选择其中一个库。鉴于我们第一次探索深度学习模型的发展，我们将专注于使用Keras库。
- en: Getting started with Keras and ANNs
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras和人工神经网络入门
- en: 'Before we move on to a full tutorial, let''s look at an example of using the
    `Keras` library since we have not explored its functionality and code:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行完整教程之前，让我们看看使用`Keras`库的一个示例，因为我们尚未探索其功能性和代码：
- en: First, we will need some sample data to use. Let's take advantage of the `make_blobs`
    class in `sklearn` to create a `classification` dataset.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要一些样本数据来使用。让我们利用`sklearn`中的`make_blobs`类来创建一个`分类`数据集。
- en: 'We will specify the need for two classes (binary classification) and a cluster
    standard deviation of `5` to ensure that the two clusters overlap, making it a
    more difficult dataset to work with:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将指定需要两个类别（二分类）和`5`的聚类标准差，以确保两个聚类重叠，使数据集更具挑战性：
- en: '[PRE5]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we can scale the data using the `MinMaxScaler()` class:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用`MinMaxScaler()`类对数据进行缩放：
- en: '[PRE6]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Following this transformation, we can split the data into training and testing
    sets, similar to how we have done previously:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此转换之后，我们可以将数据分为训练集和测试集，类似于我们之前所做的那样：
- en: '[PRE7]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s go ahead and convert the array into a DataFrame to check the first few
    rows of data beforehand:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先转换数组为DataFrame，以便在之前检查数据的前几行：
- en: '[PRE8]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will render the following table:'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下表格：
- en: '![Figure 8.19 – An example of the data within the DataFrame of features ](img/B17761_08_019.jpg)'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.19 – 特征DataFrame中的数据示例](img/B17761_08_019.jpg)'
- en: Figure 8.19 – An example of the data within the DataFrame of features
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.19 – 特征DataFrame中的数据示例
- en: 'We can check the overlap of the two clusters by using the `seaborn` library
    to plot the first two of the four features of the training dataset:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`seaborn`库来绘制训练数据集的前两个特征，以检查两个聚类的重叠情况：
- en: '[PRE9]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following diagram shows the preceding code''s output:'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面的图表显示了前面代码的输出：
- en: '![Figure 8.20 – A scatterplot of the dataset showing the overlapping nature
    of the two classes ](img/B17761_08_020.jpg)'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.20 – 显示两个类别重叠性质的散点图](img/B17761_08_020.jpg)'
- en: Figure 8.20 – A scatterplot of the dataset showing the overlapping nature of
    the two classes
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.20 – 显示两个类别重叠性质的散点图
- en: Here, we can see that the data is quite well blended, making it difficult for
    some of the machine learning models we have explored to be able to separate the
    two classes with a high degree of **accuracy**.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到数据混合得相当好，这使得我们探索的一些机器学习模型难以以高精度**准确度**将两个类别分开。
- en: 'With the data ready, we can go ahead and use the Keras library. One of the
    most popular methods for setting up a model is using the `Sequential()` class
    from `Keras`. Let''s go ahead and import the class and instantiate a new model:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据准备就绪后，我们可以继续使用Keras库。设置模型最流行的方法之一是使用`Keras`中的`Sequential()`类。让我们继续导入该类并实例化一个新的模型：
- en: '[PRE10]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With the model now instantiated, we can add a new layer to our model using
    the `Dense` class. We can also specify the number of `nodes` (`4`), `input_shape`
    (`4` for the four features), `activation` (`relu`), and a unique `name` for the
    layer:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在模型已经实例化，我们可以使用`Dense`类向我们的模型添加一个新层。我们还可以指定`节点`（`4`），`input_shape`（`4`，对应四个特征），`activation`（`relu`）以及层的唯一`名称`：
- en: '[PRE11]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To review the model we have built so far, we can use the `summary()` function:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要回顾我们迄今为止构建的模型，我们可以使用`summary()`函数：
- en: '[PRE12]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will give us some information and details about the model so far:'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将给我们提供一些关于迄今为止模型的信息和细节：
- en: '![Figure 8.21 – The sample output of the model''s summary ](img/B17761_08_021.jpg)'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.21 – 模型摘要的样本输出](img/B17761_08_021.jpg)'
- en: Figure 8.21 – The sample output of the model's summary
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.21 – 模型摘要的样本输出
- en: 'We can add a few more layers to our model by simply using the `model.add()`
    function again after the fact, perhaps even with a different number of nodes:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过在事实之后再次使用`model.add()`函数向我们的模型添加更多层，也许会有不同数量的节点：
- en: '[PRE13]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Since we are developing a `0` and `1`, we can only have a single output value
    come from the model. Therefore, we will need to add one more layer that reduces
    the number of nodes from 8 to 1\. In addition, we will change the activation to
    `sigmoid`:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们正在开发一个`0`和`1`，模型只能有一个输出值。因此，我们需要添加一个额外的层，将节点数量从8减少到1。此外，我们将激活函数改为`sigmoid`：
- en: '[PRE14]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now that our model''s general architecture has been set up, we will need to
    use the compile function and specify our loss. Since we are creating a binary
    classifier, we can use the `binary_crossentropy` loss and specify accuracy as
    our main metric of interest:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经设置了模型的总体架构，我们需要使用`compile`函数并指定我们的损失。由于我们正在创建一个二元分类器，我们可以使用`binary_crossentropy`损失，并将准确率指定为我们感兴趣的主要指标：
- en: '[PRE15]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With the model ready, let''s use the summary function to check it once more:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型准备就绪后，让我们使用摘要函数再次检查它：
- en: '![Figure 8.22 – The sample output of the model''s summary ](img/B17761_08_022.jpg)'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.22 – 模型摘要的样本输出](img/B17761_08_022.jpg)'
- en: Figure 8.22 – The sample output of the model's summary
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.22 – 模型摘要的样本输出
- en: So far, the model is quite simple. It will intake a dataset with four features
    in the first layer, expand that to eight nodes in the second layer, and then reduce
    it down to a single output in the third layer. With the model all set, we can
    go ahead and train it.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到目前为止，模型相当简单。它将在第一层接收具有四个特征的数据库，将其扩展到第二层的八个节点，然后在第三层减少到一个单一输出。模型设置完毕后，我们可以继续训练它。
- en: We can train a model by using the `model.fit()` function and by specifying the
    `X_train` and `y_train` sets. In addition, we will specify 50 `epochs` to train
    over. **Epochs** are simply the number of passes or iterations. We can also control
    the verbosity of the model, allowing us to control the amount of output data we
    want to see in the training process.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以通过使用`model.fit()`函数并指定`X_train`和`y_train`集合来训练一个模型。此外，我们将指定50个`epochs`进行训练。**epochs**简单地是遍历或迭代的次数。我们还可以控制模型的详细程度，以便我们可以控制训练过程中想要看到的数据输出量。
- en: 'Recall that, in our earlier machine learning models, we only used the training
    data to train the model and kept the testing data to test the model after the
    training was completed. We will use the same methodology here as well; however,
    we will take advantage of the high-level nature of `validation split` to be used
    in the training process. Deep learning models will almost always overfit your
    data. Using a validation split in the training process can help mitigate this:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回想一下，在我们早期的机器学习模型中，我们只使用了训练数据来训练模型，并在训练完成后保留了测试数据来测试模型。这里我们将使用相同的方法；然而，我们将利用`validation
    split`在训练过程中的高级特性。深度学习模型几乎总是会过拟合你的数据。在训练过程中使用验证分割可以帮助减轻这一点：
- en: '[PRE16]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As the model begins the training process, it will begin to produce the following
    output. You can monitor the performance here by looking at the number of **epochs**
    on the left and the **metrics** on the right. When training a model, our objective
    is to ensure that the **loss** metric is constantly decreasing, whereas the accuracy
    is increasing. We can see an example of this in the following screenshot:'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当模型开始训练过程时，它将开始产生以下输出。您可以通过查看左侧的**epochs**数量和右侧的**指标**来监控性能。在训练模型时，我们的目标是确保**损失**指标持续下降，而准确率在上升。以下屏幕截图展示了这一点的示例：
- en: '![Figure 8.23 – A sample of a model''s output ](img/B17761_08_023.jpg)'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.23 – 模型输出的样本](img/B17761_08_023.jpg)'
- en: Figure 8.23 – A sample of a model's output
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.23 – 模型输出的样本
- en: 'With the model trained, let''s quickly examine the classification metrics,
    as we did previously, to get a sense of the performance. We can begin by making
    predictions using the testing data and using the `classification_report` to calculate
    our metric. Note that the `predict()` method does not return a class but a probability
    that needs to be rounded to either `0` or `1`, given that this is a **binary classification**
    problem:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练完成后，让我们快速检查分类指标，就像之前做的那样，以了解性能。我们可以通过使用测试数据并使用`classification_report`来计算我们的指标开始。请注意，`predict()`方法不返回一个类别，而是一个概率，需要将其四舍五入到`0`或`1`，因为这是一个**二元分类**问题：
- en: '[PRE17]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Upon printing the report, we will get the following results:'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 打印报告后，我们将得到以下结果：
- en: '![Figure 8.24 – The results of the model ](img/B17761_08_024.jpg)'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.24 – 模型的结果](img/B17761_08_024.jpg)'
- en: Figure 8.24 – The results of the model
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.24 – 模型的结果
- en: 'We can see that the `history` variable, which contains the model''s training
    history:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到包含模型训练历史的`history`变量：
- en: '[PRE18]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Upon executing this code, we will receive the following diagram, which shows
    the change in accuracy and loss over the course of the model training process:'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行此代码后，我们将收到以下图表，它显示了模型训练过程中的准确率和损失的变化：
- en: '![Figure 8.25 – The accuracy and loss of the model ](img/B17761_08_025.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图8.25 – 模型的准确率和损失](img/B17761_08_025.jpg)'
- en: Figure 8.25 – The accuracy and loss of the model
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.25 – 模型的准确率和损失
- en: When training a model, recall that the main objective is to ensure that the
    loss is decreasing, and never increasing over time. In addition, our secondary
    objective is to ensure that the accuracy of our model slowly and steadily increases.
    When trying to diagnose a model that is not performing well, the first step is
    to generate graphs such as these to get a sense of any potential problems before
    altering the model in an attempt to improve the metrics.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，请记住，主要目标是确保损失随着时间逐渐减少，而不是增加。此外，我们的次要目标是确保模型的准确率缓慢而稳定地增加。在尝试诊断表现不佳的模型时，第一步是生成此类图表，在尝试通过改变模型来改善指标之前，了解任何潜在的问题。
- en: 'When we worked with most of the machine learning models in the previous chapters,
    we learned that we could alter these metrics by doing the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在前几章中与大多数机器学习模型合作时，我们了解到我们可以通过以下方式改变这些指标：
- en: Improving our data preprocessing.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进我们的数据预处理。
- en: Tuning our hyperparameters or changing the model. Within the confines of deep
    learning, in addition to the options mentioned previously, there are a few more
    tools we have to change to suit our needs. For instance, we can change the overall
    architecture by adding or removing layers and nodes. In addition, we can change
    the activation functions within each of the layers to whatever would complement
    our problem statement the most. We can also change the optimizer or the learning
    rate of our optimizer (Adam, in this model).
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整我们的超参数或更改模型。在深度学习的范畴内，除了之前提到的选项外，我们还有一些额外的工具可以调整以满足我们的需求。例如，我们可以通过添加或删除层和节点来改变整体架构。此外，我们还可以更改每一层的激活函数，使其与我们的问题陈述最为契合。我们还可以更改优化器或优化器的学习率（在这个模型中是Adam）。
- en: 'With so many changes that can be made that could have high impacts on any given
    model, we will need to organize our work. We could either create numerous models
    and record our metrics manually in a spreadsheet, or we could take advantage of
    a library specifically designed to handle use cases such as these: **MLflow**.
    We will take a closer look at **MLflow** in the next section.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可以做出许多可能对任何给定模型产生重大影响的更改，我们需要组织我们的工作。我们既可以创建多个模型并在电子表格中手动记录我们的指标，也可以利用专门为处理此类用例设计的库：**MLflow**。我们将在下一节中更详细地了解**MLflow**。
- en: Tutorial – protein sequence classification via LSTMs using Keras and MLflow
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教程 – 使用Keras和MLflow进行蛋白质序列分类
- en: Deep learning has gained a surge of popularity in recent years, prompting many
    scientists to turn to the field as a new means for solving and optimizing scientific
    problems. One of the most popular applications for deep learning within the biotechnology
    space involves **protein sequence** data. So far within this book, we have focused
    our efforts on developing predictive models when it comes to **structured** data.
    We will now turn our attention to data that's **sequential** in the sense that
    the elements within a sequence bear some relation to their previous element. Within
    this tutorial, we will attempt to develop a protein **sequence classification**
    model in which we will classify protein sequences based on their known family
    accession using the **Pfam** ([https://pfam.xfam.org/](https://pfam.xfam.org/))
    dataset.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在近年来获得了极大的流行，促使许多科学家转向该领域作为解决和优化科学问题的新的手段。在生物技术领域，深度学习最流行的应用之一涉及**蛋白质序列**数据。到目前为止，在这本书中，我们专注于开发针对**结构化**数据的预测模型。现在，我们将把注意力转向那些在某种意义上是**序列性**的数据，即序列中的元素与其前一个元素之间存在某种关系。在本教程中，我们将尝试开发一个蛋白质**序列分类**模型，我们将根据已知的家族访问号对蛋白质序列进行分类，使用**Pfam**([https://pfam.xfam.org/](https://pfam.xfam.org/))数据集。
- en: Important note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '`Pfam` dataset: Pfam: The protein families database in 2021 J. Mistry, S. Chuguransky,
    L. Williams, M. Qureshi, G.A. Salazar, E.L.L. Sonnhammer, S.C.E. Tosatto, L. Paladin,
    S. Raj, L.J. Richardson, R.D. Finn, A. BatemanNucleic Acids Research (2020) doi:
    10.1093/nar/gkaa913 (`Pfam: The protein families database in 2021`).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pfam`数据集：Pfam：2021年的蛋白质家族数据库 J. Mistry, S. Chuguransky, L. Williams, M. Qureshi,
    G.A. Salazar, E.L.L. Sonnhammer, S.C.E. Tosatto, L. Paladin, S. Raj, L.J. Richardson,
    R.D. Finn, A. Bateman Nucleic Acids Research (2020) doi: 10.1093/nar/gkaa913 (`Pfam：2021年的蛋白质家族数据库`).'
- en: 'The `Pfam` dataset consists of several columns, as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pfam`数据集包含几个列，如下所示：'
- en: '`Family_id`: The name of the family that the sequence belongs to (for example,
    filamin)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Family_id`：序列所属家族的名称（例如，filamin）'
- en: '`Family Accession`: The class or output that our model will aim to predict'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Family Accession`：我们的模型将尝试预测的类别或输出'
- en: '`Sequence`: The amino acid sequence we will use as input for our model'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Sequence`：我们将用作模型输入的氨基酸序列'
- en: Throughout this tutorial, we will use the sequence data to develop several predictive
    models to determine each sequence's associated family accession. The sequences
    are in their raw state with different lengths and sizes. We will need to pre-process
    the data and structure it in such a way as to prepare it for sequence classification.
    When it comes to the labels, we will develop a model using a **balanced** set
    of different labels to ensure the model does not learn any particular bias.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用序列数据开发几个预测模型，以确定每个序列的相关家族访问号。这些序列处于原始状态，长度和大小不同。我们需要预处理数据并按这种方式构建它，以便为序列分类做准备。至于标签，我们将使用一组**平衡**的不同标签来开发模型，以确保模型不会学习任何特定的偏差。
- en: 'As we begin to develop our ideal classification model, we will need to alter
    the many possible parameters to maximize the performance. To keep track of these
    changes, we will make use of the MLflow ([https://mlflow.org](https://mlflow.org))
    library. There are four main components within **MLflow**:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始开发理想的分类模型时，我们需要调整许多可能的参数以最大化性能。为了跟踪这些变化，我们将使用 MLflow ([https://mlflow.org](https://mlflow.org))
    库。**MLflow** 中有四个主要组件：
- en: '**MLflow Tracking**: Allows users to record and query experiments'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow 跟踪**：允许用户记录和查询实验'
- en: '**MLflow Projects**: Packages data science code'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow 项目**：打包数据科学代码'
- en: '**MLflow Models**: Deploys trained machine learning models'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow 模型**：部署训练好的机器学习模型'
- en: '**MLflow Registry**: Stores and manages your models'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow 注册表**：存储和管理你的模型'
- en: Within this tutorial, we will explore how to use MLflow tracking to track and
    manage the development of a protein sequence classification model. With these
    items in mind, let's get started.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将探讨如何使用 MLflow 跟踪来跟踪和管理蛋白质序列分类模型的发展。考虑到这些事项，让我们开始吧。
- en: Importing the necessary libraries and datasets
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入必要的库和数据集
- en: 'We will begin with the standard set of library imports, followed by the dataset
    in the format of a CSV document:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始导入标准的库集合，然后是格式为 CSV 文档的数据集：
- en: '[PRE19]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With the libraries now imported, we can import the dataset as well. We will
    begin by specifying the path, and then concatenate the dataset using a `for` loop:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在库已经导入，我们也可以导入数据集。我们将首先指定路径，然后使用 `for` 循环连接数据集：
- en: '[PRE20]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Upon importing the dataset, we immediately notice that it contains five columns
    and ~1.3 million rows of data – slightly larger than what we have worked with
    so far. We can take a quick glimpse at the dataset using the `.head()` function:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入数据集后，我们立即注意到它包含五个列和约 1.3 百万行数据 – 比我们之前处理的数据略大。我们可以使用 `.head()` 函数快速查看数据集：
- en: '![Figure 8.26 – A sample of the data from the protein sequence dataset ](img/B17761_08_026.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.26 – 蛋白质序列数据集的数据样本](img/B17761_08_026.jpg)'
- en: Figure 8.26 – A sample of the data from the protein sequence dataset
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.26 – 蛋白质序列数据集的数据样本
- en: With the dataset successfully imported, let's go ahead and explore the dataset
    in more detail.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集成功导入后，让我们进一步探索数据集的详细信息。
- en: Checking the dataset
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查数据集
- en: 'We can confirm the completeness of the data in this DataFrame using the `isna()`
    function, followed by the `sum()` function to summarize by column:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `isna()` 函数确认 DataFrame 中数据的完整性，然后使用 `sum()` 函数按列进行总结：
- en: '[PRE21]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let''s take a closer look at the `family_accession` column (our model''s
    output) of this dataset. We can check the total number of instances by grouping
    the column and using the `value_counts()` function, followed by the `n_largest()`
    function, to get the top 10 most common entries in this column:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更仔细地查看数据集的 `family_accession` 列（我们的模型输出）。我们可以通过分组列并使用 `value_counts()`
    函数，然后使用 `n_largest()` 函数来检查实例总数，以获取该列中最常见的 10 个条目：
- en: '[PRE22]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Grouping the data will yield the following results:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分组将产生以下结果：
- en: '![Figure 8.27 – A summary of the classes in the dataset with value counts higher
    than 1,200 ](img/B17761_08_027.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.27 – 数据集中值计数高于 1,200 的类别摘要](img/B17761_08_027.jpg)'
- en: Figure 8.27 – A summary of the classes in the dataset with value counts higher
    than 1,200
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.27 – 数据集中值计数高于 1,200 的类别摘要
- en: 'Here, we can see that 1,500 entries seems to be the cutoff point for the top
    10 values. We can also take a closer look at the sequence column (our model''s
    input) by getting a sense of the average lengths of the sequences. We can plot
    the count of each sequence length using the `displot()` function from the `seaborn`
    library:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到1,500条条目似乎是前10个值截止点。我们还可以通过了解序列的平均长度来更仔细地查看序列列（我们模型的输入），我们可以使用`seaborn`库中的`displot()`函数绘制每个序列长度的计数：
- en: '[PRE23]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Executing this code will yield the following results:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码将产生以下结果：
- en: '![Figure 8.28 – A histogram of the counts of the sequence lengths in the dataset
    ](img/B17761_08_028.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图8.28 – 数据集中序列长度计数的直方图](img/B17761_08_028.jpg)'
- en: Figure 8.28 – A histogram of the counts of the sequence lengths in the dataset
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.28 – 数据集中序列长度计数的直方图
- en: From this graph, as well as by using the `mean()` and `median()` functions,
    we can see that the average and most common lengths are approximately 155 and
    100 amino acids. We will use these numbers later when determining what the cutoff
    should be for the input sequences.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张图以及使用`mean()`和`median()`函数，我们可以看到平均长度和最常见的长度大约是155和100个氨基酸。我们将在确定输入序列的截止值时使用这些数字。
- en: 'Now that we have gained a better sense of the data, it is time to prepare the
    dataset for our classification models. We could theoretically train the model
    on the dataset as a whole without limits – however, the models would require a
    much longer duration to train. In addition, by training across all the data without
    accounting for balance, we may introduce bias within the model. To mitigate both
    of these situations, let''s reduce this dataset by filtering for the classifications
    with at least 1,200 **observations**:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们对数据有了更好的了解，现在是时候为我们的分类模型准备数据集了。理论上，我们可以在整个数据集上无限制地训练模型 - 然而，模型将需要更长的时间来训练。此外，如果不考虑平衡性而在所有数据上训练，我们可能会在模型中引入偏差。为了减轻这两种情况，让我们通过过滤至少有1,200个**观测值**的分类来减少这个数据集：
- en: '[PRE24]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Given that some classes have significantly more than 1,200 observations, we
    can randomly select exactly 1,200 observations using the `sample()` function:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些类别的观测值明显多于1,200个，我们可以使用`sample()`函数随机选择恰好1,200个观测值：
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can now check the filtered and balanced dataset using the `head()` function:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`head()`函数检查过滤和平衡后的数据集：
- en: '[PRE26]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `head()` function will yield the following results:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`head()`函数将产生以下结果：'
- en: '![Figure 8.29 – A sample of the data in the form of a DataFrame ](img/B17761_08_029.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图8.29 – 以DataFrame形式的数据样本](img/B17761_08_029.jpg)'
- en: Figure 8.29 – A sample of the data in the form of a DataFrame
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.29 – 以DataFrame形式的数据样本
- en: 'We can check the number of classes we will have in this dataset by checking
    the length of the `value_counts()` function:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查`value_counts()`函数的长度来检查这个数据集中我们将有多少个类别：
- en: '[PRE27]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If we check the `num_classes` variable, we will see that we have 28 possible
    classes in total.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查`num_classes`变量，我们将看到总共有28个可能的类别。
- en: Splitting the dataset
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集拆分
- en: 'With the data prepared, our next step will be to split the dataset into training,
    testing, and validation sets. We will once again make use of the `train_test_split`
    function from `sklearn` to accomplish this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备就绪后，我们的下一步将是将数据集拆分为训练、测试和验证集。我们将再次使用`sklearn`中的`train_test_split`函数来完成此操作：
- en: '[PRE28]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With the data now split, let's go ahead and preprocess it.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现在已拆分，让我们继续预处理它。
- en: Preprocessing the data
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'With the data split up, we need to preprocess the datasets to use on our neural
    network models. First, we will need to reduce the sequences down to the 20 most
    common amino acids and convert the sequences into integers. This will speed up
    the training process. First, we will create a dictionary of amino acids that contains
    their corresponding values:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 数据拆分后，我们需要预处理数据集以便用于我们的神经网络模型。首先，我们需要将序列减少到最常见的20种氨基酸，并将序列转换为整数。这将加快训练过程。首先，我们将创建一个包含氨基酸及其对应值的字典：
- en: '[PRE29]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we can iterate over the sequences and convert the string values into
    their corresponding integers. Note that we will complete this for the training,
    testing, and validation sets:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以遍历序列并将字符串值转换为相应的整数。请注意，我们将为此完成训练、测试和验证集：
- en: '[PRE30]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we will need to pad the sequences to ensure they are all of an equal
    length. To accomplish this, we can use the `pad_sequences` function from `keras`.
    We will specify `max_length` for each of the sequences as 100, given that it approximates
    the median value we saw earlier. In addition, we will pad the sequences with `''post''`
    to ensure that we pad them at the end instead of the front:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要填充序列以确保它们的长度都相同。为了实现这一点，我们可以使用`keras`中的`pad_sequences`函数。我们将为每个序列指定`max_length`为100，因为这与我们之前看到的中间值相近。此外，我们将使用`'post'`来填充序列，以确保我们在序列的末尾而不是开头进行填充：
- en: '[PRE31]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can take a quick glance of the changes we have made using one of the sequences.
    First, we have the raw sequence as a `string`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用其中一个序列快速查看我们所做的更改。首先，我们有原始序列作为一个`字符串`：
- en: '[PRE32]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we can encode the sequence to remove uncommon amino acids and convert
    the string to a list of integers:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将序列编码以去除不常见的氨基酸，并将字符串转换为整数列表：
- en: '[PRE33]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we can limit the lengths of the sequences to either truncate them
    at 100 elements or **pad** them with zeros to reach 100 elements:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将序列的长度限制为100个元素，或者用零**填充**以达到100个元素：
- en: '[PRE34]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now that we have preprocessed the input data, we will need to preprocess the
    output values as well. We can do so using the `LabelEncoder` class from `sklearn`.
    Our main objective here will be to transform the values from a list of labels
    in a dataframe column into an encoded list:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经预处理了输入数据，我们还需要预处理输出值。我们可以使用`sklearn`中的`LabelEncoder`类来完成这个任务。我们的主要目标是将数据框列中的标签列表转换为编码后的列表：
- en: '[PRE35]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we can use the `to_categorical` function from `sklearn` to transform
    a class vector into a binary class matrix:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`sklearn`中的`to_categorical`函数将类别向量转换为二进制类别矩阵：
- en: '[PRE36]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To review the changes we''ve made here, we can use a single column in a `DataFrame`:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 要回顾我们在这里所做的更改，我们可以使用`DataFrame`的单个列：
- en: '[PRE37]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can see the results of this in the following diagram:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图表中看到这个结果：
- en: '![Figure 8.30 – A list of the classes ](img/B17761_08_030.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图8.30 – 类别列表](img/B17761_08_030.jpg)'
- en: Figure 8.30 – A list of the classes
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.30 – 类别列表
- en: 'Next, we must encode the classes into a list of numerical values, with each
    value representing a specific class:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须将类别编码成一系列数值，每个数值代表一个特定的类别：
- en: '[PRE38]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, we must convert the structure into a **binary class matrix** so that
    each row consists of a list of 27 values of zero, and one value of 1, representing
    the class it belongs to:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们必须将结构转换为**二进制类别矩阵**，使得每一行都是一个包含27个零和一个1的列表，1代表它所属的类别：
- en: '[PRE39]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: With that, our datasets have been fully preprocessed and ready to be used in
    the model development phase of the tutorial.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们的数据集已经完全预处理完毕，准备好在教程的模型开发阶段使用。
- en: Developing models with Keras and MLflow
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras和MLflow开发模型
- en: 'Similar to our previous example of developing models with the `Keras` library,
    we will once again be using the `Sequential` class:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前使用`Keras`库开发模型的例子类似，我们再次将使用`Sequential`类：
- en: 'We will begin by importing the layers and other items we will need from Keras:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先从Keras导入我们需要的层和其他项目：
- en: '[PRE40]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, we can create a new instance of a model using the sequential class and
    begin populating it by adding a few layers of interest. We will start by adding
    an embedding layer to convert positive integers into dense vectors. We will specify
    an `input_dim` of `21` to represent the size of the amino acid index + 1, and
    an `output_dim` of 32\. In addition, we will assign an `input_length` equal to
    that of `max_length` – the length of the sequences:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用序列类创建一个新的模型实例，并通过添加一些感兴趣的层来填充它。我们首先添加一个嵌入层，将正整数转换为密集向量。我们将指定`input_dim`为`21`来表示氨基酸索引的大小加1，以及`output_dim`为32。此外，我们将`input_length`设置为`max_length`减去序列的长度：
- en: '[PRE41]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we will add an LSTM layer, wrapped in a `Bidirectional` layer, to run
    inputs in both directions – from past to future and from future to past:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将添加一个LSTM层，并用`Bidirectional`层包装，以在两个方向上运行输入——从过去到未来和从未来到过去：
- en: '[PRE42]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we will add a `Dropout` layer to help prevent the model from overfitting:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将添加一个`Dropout`层，以帮助防止模型过拟合：
- en: '[PRE43]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, we will end with a `Dense` layer and set the number of nodes to `28`
    so that this corresponds with the shape of the outputs. Notice that we use a Softmax
    activation here:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将以一个`Dense`层结束，并将节点数设置为`28`，以便与输出形状相对应。注意，我们在这里使用Softmax激活函数：
- en: '[PRE44]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'With the model''s architecture prepared, we can assign an optimizer (Adam),
    compile the model, and check the summary:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型的架构准备就绪后，我们可以分配一个优化器（Adam），编译模型，并检查摘要：
- en: '[PRE45]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, let''s go ahead and train our model using the `fit()` function and assign
    30 epochs. Notice from our previous tutorial that training deep learning models
    can be very time-consuming and expensive, so training a model that is not learning
    can be a major waste of time. To mitigate situations such as these, we can implement
    what is known as a callback in the sense that Keras can end the training period
    when a model is no longer learning (that is, the loss is no longer decreasing):'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们继续使用`fit()`函数训练我们的模型并分配30个epoch。注意，从我们之前的教程中可以看出，训练深度学习模型可能非常耗时且昂贵，因此训练一个没有学习的模型可能是一种巨大的时间浪费。为了减轻这种情况，我们可以在Keras中实现所谓的回调，即在模型不再学习（即损失不再下降）时结束训练周期：
- en: '[PRE46]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, we can go ahead and log our new run in MLflow by calling the `autolog()`
    function and fitting the model, as we did previously. MLflow offers many different
    methods to log both parameters and metrics, and you are not limited to using just
    `autolog()`:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以继续通过调用`autolog()`函数和拟合模型来记录我们的新运行，就像我们之前做的那样。MLflow提供了许多不同的方法来记录参数和指标，你不仅限于使用`autolog()`：
- en: '[PRE47]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Assuming you followed these steps correctly, the model will print a note stating
    that MLflow is being used, and you should see a new directory appear next to your
    current notebook. Upon completing the training process, you can plot the results,
    as we did previously, to arrive at the following diagram:'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设你正确地遵循了这些步骤，模型将打印一条消息说明正在使用MLflow，你应该看到一个新的目录出现在你的当前笔记本旁边。在完成训练过程后，你可以像之前那样绘制结果，得到以下图表：
- en: '![Figure 8.31 – The accuracy and results of the first iteration of this model
    ](img/B17761_08_031.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图8.31 – 该模型第一次迭代的准确率和结果](img/B17761_08_031.jpg)'
- en: Figure 8.31 – The accuracy and results of the first iteration of this model
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.31 – 该模型第一次迭代的准确率和结果
- en: 'Here, we can see that the accuracy seems to remain stagnant at around 80-85%,
    while the loss remains stagnant at 0.6 to 0.8\. We can see that the model is not
    learning. Perhaps a change of parameters is needed? Let''s go ahead and change
    the number of nodes from `8` to `12` and the learning rate from `0.1` to `0.01`.
    Upon compiling the new model, calling the `autolog()` function, and training the
    new dataset, we will arrive at a new diagram:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到准确率似乎保持在80-85%左右，而损失保持在0.6到0.8之间。我们可以看到模型没有在学习。也许需要改变参数？让我们继续将节点数从`8`改为`12`，将学习率从`0.1`改为`0.01`。在编译新模型、调用`autolog()`函数和训练新数据集后，我们将得到一个新的图表：
- en: '![Figure 8.32 – The accuracy and results of the next iteration of this model
    ](img/B17761_08_032.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图8.32 – 该模型下一次迭代的准确率和结果](img/B17761_08_032.jpg)'
- en: Figure 8.32 – The accuracy and results of the next iteration of this model
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.32 – 该模型下一次迭代的准确率和结果
- en: 'Here, we can see that the model''s loss, both for training and validation,
    decreased quite nicely until the callback stopped the training at around 30 epochs
    in. Alternatively, the accuracy shows a sharp increase at the beginning, followed
    by a stable increase toward the end, also stopping at 30 epochs into the process.
    We can keep making our changes and calling the `autolog()` function over and over,
    allowing the system to log the changes and the resulting metrics on our behalf.
    After several iterations, we can review the performance of our models using `mlflow
    ui`. Within the notebook itself, enter the following command:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到模型的损失，无论是训练还是验证，都相当好地下降，直到回调在约30个epoch时停止训练。另一方面，准确率在开始时急剧上升，随后稳定上升，也停止在过程的30个epoch处。我们可以不断地做出改变并多次调用`autolog()`函数，让系统代表我们记录变化和产生的指标。经过几次迭代后，我们可以使用`mlflow
    ui`来检查我们模型的性能。在笔记本本身中，输入以下命令：
- en: '[PRE48]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Next, navigate to `http://localhost:5000/`. There, you will be able to see
    the `MLflow` UI, where you will be able to view the models, their parameters,
    and their associated metrics:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，导航到`http://localhost:5000/`。在那里，你将能够看到`MLflow` UI，你将能够查看模型、它们的参数和相关的指标：
- en: '![Figure 8.33 – An example of the MLflow UI ](img/B17761_08_033.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图8.33 – MLflow UI的示例](img/B17761_08_033.jpg)'
- en: Figure 8.33 – An example of the MLflow UI
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.33 – MLflow UI的示例
- en: With that, you can select the best model and move forward with your project.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你可以选择最佳模型并继续你的项目。
- en: Reviewing the model's performance
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查模型的性能
- en: 'Now that the best-performing model has been selected, let''s get a better sense
    of its associated `classification_report`, as we did previously, showing almost
    99% for both precision and recall. Alternatively, we can use a confusion matrix
    to get a better sense of the data, given that we have 28 classes in total:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经选出了表现最好的模型，让我们更好地了解其相关的`classification_report`，正如我们之前所做的那样，几乎达到了99%的精确率和召回率。或者，鉴于我们总共有28个类别，我们可以使用混淆矩阵来更好地了解数据：
- en: '[PRE49]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'With the confusion matrix calculated, we can use a heatmap to visualize the
    results:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了混淆矩阵后，我们可以使用热图来可视化结果：
- en: '[PRE50]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Upon executing this, we will get the following diagram:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作后，我们将得到以下图表：
- en: '![Figure 8.34 – A confusion matrix of the results of the model ](img/B17761_08_034.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![图8.34 – 模型结果的混淆矩阵](img/B17761_08_034.jpg)'
- en: Figure 8.34 – A confusion matrix of the results of the model
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.34 – 模型结果的混淆矩阵
- en: Here, we can see that the performance of the model is quite robust as it is
    giving us great results! Keras, TensorFlow, and PyTorch are great packages that
    can help us develop robust and high-impact models to solve specific solutions.
    Often, we will find that there may be a model (or set of models) that already
    exists through AWS that can solve our complex problem with little to no code.
    We will explore an example of this in the next section.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到模型的性能相当稳健，因为它给出了非常好的结果！Keras、TensorFlow和PyTorch是伟大的包，可以帮助我们开发稳健且具有高影响力的模型来解决特定问题。通常，我们会发现AWS可能已经存在一个（或一组）模型，可以用很少或没有代码来解决我们的复杂问题。我们将在下一节中探讨这个例子。
- en: Tutorial – anomaly detection in manufacturing using AWS Lookout for Vision
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教程 – 使用AWS Lookout for Vision进行制造中的异常检测
- en: In the previous section, we prepared and trained a deep learning model to classify
    proteins in their given categories. We went through the process of preprocessing
    our data, developing a model, testing the parameters, editing the architecture,
    and selecting a combination that maximized our metrics of interest. While this
    process can generally produce good results, we can sometimes utilize platform
    architectures such as those from AWS to automatically develop models on our behalf.
    Within this tutorial, we will take advantage of a tool known **AWS Lookout for
    Vision** ([https://aws.amazon.com/lookout-for-vision/](https://aws.amazon.com/lookout-for-vision/))
    to help us prepare a model capable of detecting anomalies within a dataset.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们准备并训练了一个深度学习模型来对给定的蛋白质类别进行分类。我们经历了数据预处理、模型开发、测试参数、编辑架构以及选择最大化我们感兴趣指标的组合的过程。虽然这个过程通常会产生良好的结果，但我们有时可以利用AWS等平台架构自动为我们开发模型。在本教程中，我们将利用一个名为**AWS
    Lookout for Vision**的工具([https://aws.amazon.com/lookout-for-vision/](https://aws.amazon.com/lookout-for-vision/))来帮助我们准备一个能够检测数据集中异常的模型。
- en: 'Throughout this tutorial, we will be working with a dataset consisting of images
    concerned with manufacturing a of **Drug Product** (**DP**). Each of the images
    consists of a vial whose image was captured at the end of the manufacturing cycle.
    Most of the vials are clean and don''t have any impurities. However, some of the
    vials contain minor impurities, as illustrated in the following diagram:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个教程中，我们将使用一个包含与制造**药物产品**（**DP**）相关的图像的数据集。每张图像都包含一个在制造周期结束时捕获的瓶子的图像。大多数瓶子都很干净，没有杂质。然而，一些瓶子含有轻微的杂质，如下面的图所示：
- en: '![Figure 8.35 – An example of an accepted vial versus a damaged vial ](img/B17761_08_035.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![图8.35 – 合格瓶与损坏瓶的示例](img/B17761_08_035.jpg)'
- en: Figure 8.35 – An example of an accepted vial versus a damaged vial
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.35 – 一个合格瓶与损坏瓶的示例
- en: The process of rejecting damaged or impure vials is often done manually and
    can be quite time-consuming. We have been tasked with implementing an automated
    solution to this problem and we only have a few days to do so. Rather than developing
    our own custom deep learning model for detecting anomalies in images, we can utilize
    **Amazon Lookout for Vision**. In this tutorial, we will begin by uploading our
    dataset of images to S3, importing the images into the framework, and begin training
    our model. With that in mind, let's go ahead and get started!
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 拒绝损坏或杂质瓶子的过程通常是通过手工完成的，可能相当耗时。我们被要求实施一个自动化的解决方案来解决这个问题，而我们只有几天的时间来完成。与其开发我们自己的定制深度学习模型来检测图像中的异常，我们可以利用**Amazon
    Lookout for Vision**。在本教程中，我们将首先将我们的图像数据集上传到S3，将图像导入框架，并开始训练我们的模型。考虑到这一点，让我们开始吧！
- en: 'Within this book''s GitHub repository, you can find a directory called `vials_input_dataset_s3`,
    which contains a collection of both normal and damaged vials. If we take a closer
    look at our dataset, we will notice that it is constructed using a directory hierarchy,
    as shown in the following diagram:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的GitHub仓库中，您可以找到一个名为`vials_input_dataset_s3`的目录，其中包含了一组正常和损坏的试管。如果我们仔细查看我们的数据集，我们会注意到它使用目录层次结构构建，如图所示：
- en: '![Figure 8.36 – An example of an accepted vial versus a damaged vial ](img/B17761_08_036.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![图8.36 – 一个合格的试管与一个损坏的试管的示例](img/B17761_08_036.jpg)'
- en: Figure 8.36 – An example of an accepted vial versus a damaged vial
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.36 – 一个合格的试管与一个损坏的试管的示例
- en: 'We will begin by importing the images into the same S3 bucket we have been
    working with throughout this book:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将图像导入到本书中一直使用的同一个S3存储桶中：
- en: First, navigate to S3 from within the AWS console and select the bucket of interest.
    In this case, I will select **biotech-machine-learning**.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从AWS控制台中导航到S3并选择感兴趣的存储桶。在这种情况下，我将选择**biotech-machine-learning**。
- en: Next, click the orange **Upload** button, select the **vials_input_dataset_s3**
    folder, and click **Upload**. This process may take a few moments, depending on
    your internet connection.
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击橙色**上传**按钮，选择**vials_input_dataset_s3**文件夹，然后点击**上传**。这个过程可能需要几分钟，具体取决于您的网络连接。
- en: Now, click on the **Copy S3 URI** button at the top right-hand side of the page.
    We will need this URI in a few moments.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，点击页面右上角的**复制S3 URI**按钮。我们将在几分钟后需要这个URI。
- en: 'Now, our data is available for use to use in our S3 bucket. Next, we can focus
    on getting the data imported with the model and start the model training process:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的数据已可用于在S3存储桶中使用。接下来，我们可以专注于将数据导入模型并开始模型训练过程：
- en: To begin, navigate to Amazon Lookout for Vision, which is located in the AWS
    console. Then, click the **Get started** button:![Figure 8.37 – The front page
    of Amazon Lookout for Vision ](img/B17761_08_037.jpg)
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导航到AWS控制台中的Amazon Lookout for Vision。然后，点击**开始使用**按钮：![图8.37 – Amazon Lookout
    for Vision的前页](img/B17761_08_037.jpg)
- en: Figure 8.37 – The front page of Amazon Lookout for Vision
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.37 – Amazon Lookout for Vision的前页
- en: Click on the **Create project** button on the right-hand side of the page and
    give your project a name.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击页面右侧的**创建项目**按钮，并为您的项目命名。
- en: Once the project has been created, go ahead and click the **Create dataset**
    button on the left-hand side of the page.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦项目创建完成，请继续点击页面左侧的**创建数据集**按钮。
- en: Select the second option to **Create a training dataset and test dataset**:![Figure
    8.38 – Creating a dataset in AWS Lookout for Vision ](img/B17761_08_038.jpg)
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择第二个选项以**创建训练数据集和测试数据集**：![图8.38 – 在AWS Lookout for Vision中创建数据集](img/B17761_08_038.jpg)
- en: Figure 8.38 – Creating a dataset in AWS Lookout for Vision
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.38 – 在AWS Lookout for Vision中创建数据集
- en: Next, within the `training` to the path, as shown in the following screenshot:![Figure
    8.39 – Creating a dataset in AWS Lookout for Vision ](img/B17761_08_039.jpg)
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在`training`路径内，如图所示：![图8.39 – 在AWS Lookout for Vision中创建数据集](img/B17761_08_039.jpg)
- en: Figure 8.39 – Creating a dataset in AWS Lookout for Vision
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.39 – 在AWS Lookout for Vision中创建数据集
- en: In addition, be sure to select the **Automatic labeling** option to ensure our
    labels are taken in by AWS.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，请确保选择**自动标注**选项，以确保我们的标签被AWS接收。
- en: Repeat this same process for the test dataset but be sure to add the word `validation`
    instead of training in the S3 URI path. Then, click on **Create dataset**.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于测试数据集，重复此过程，但请确保在S3 URI路径中将“training”替换为“validation”。然后，点击**创建数据集**。
- en: 'Once the dataset has been created, you will be taken to a new page where you
    can visually inspect the dataset''s readiness. Then, you can click the **Train
    model** button located in the top right-hand corner of the page to begin the model
    training process. This process can be time-consuming and may take a few hours:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据集创建完成，您将被带到一个新的页面，您可以在此页面上直观地检查数据集的准备工作。然后，您可以在页面的右上角点击**训练模型**按钮以开始模型训练过程。这个过程可能耗时，可能需要几个小时：
- en: '![Figure 8.40 – The dataset before training the model ](img/B17761_08_040.jpg)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![图8.40 – 训练模型前的数据集](img/B17761_08_040.jpg)'
- en: Figure 8.40 – The dataset before training the model
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.40 – 训练模型前的数据集
- en: 'By doing this, you will be presented with the final results for the model,
    which will show you the precision, recall, and F1 score, as shown in the following
    screenshot:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，您将看到模型的最终结果，它将显示精确度、召回率和 F1 分数，如下面的截图所示：
- en: '![Figure 8.41 – The Model performance metrics page ](img/B17761_08_041.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.41 – 模型性能指标页面](img/B17761_08_041.jpg)'
- en: Figure 8.41 – The Model performance metrics page
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.41 – 模型性能指标页面
- en: With that final step completed, we have successfully developed a robust model
    capable of detecting anomalies in the manufacturing process! Not only were we
    able to create the models in just a few hours, but we managed to do so without
    any code!
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 完成最后一步后，我们成功开发了一个能够检测制造过程中异常的稳健模型！我们不仅能够在短短几小时内创建模型，而且还做到了无需任何代码！
- en: Summary
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Throughout this chapter, we made a major stride to cover a respectable portion
    of the *must-know* elements of deep learning and neural networks. First, we investigated
    the roots of neural networks and how they came about and then dove into the idea
    of a perceptron and its basic form of functionality. We then embarked on a journey
    to explore four of the most common neural networks out there: MLP, CNN, RNN, and
    LSTM. We gained a better sense of how to select activation functions, measure
    loss, and implement our understandings using the Keras library.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们迈出了重大步伐，涵盖了深度学习和神经网络*必须了解*的相当一部分元素。首先，我们研究了神经网络的根源以及它们是如何产生的，然后深入探讨了感知器及其基本功能形式。然后，我们开始了一段探索最常见四种神经网络的旅程：MLP、CNN、RNN
    和 LSTM。我们更好地理解了如何选择激活函数、衡量损失以及使用 Keras 库实现我们的理解。
- en: Next, we took a less theoretical and much more hands-on approach as we tackled
    our first dataset that was sequential nature. We spent a considerable amount of
    time preprocessing our data, developing our model, getting our model development
    organized with MLflow, and reviewing its performance. Following these steps allowed
    us to create a custom and well-suited model for the problem at hand. Finally,
    we took a no-code approach by using AWS Lookout for Vision to train a model capable
    of detecting anomalies in images of vials.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们采取了一种更少理论性和更多实践性的方法，因为我们处理了第一个具有序列性质的数据集。我们花费了大量时间进行数据预处理，开发我们的模型，使用 MLflow
    组织模型开发，并审查其性能。遵循这些步骤使我们能够为当前问题创建一个定制且合适的模型。最后，我们通过使用 AWS Lookout for Vision 训练一个能够检测安瓿瓶图像中异常的模型，采取了一种无代码的方法。
- en: Deep learning and the application of neural networks have most certainly seen
    a major surge over the last few years, and in the next chapter, we will see an
    application of deep learning as it relates to natural language processing.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习和神经网络的应用在过去几年中确实经历了重大增长，在下一章中，我们将看到与自然语言处理相关的深度学习应用。
