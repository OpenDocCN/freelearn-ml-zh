<html><head></head><body>
<div id="_idContainer076" class="Content">
<p class="hidden" data-amznremoved-m8="true" data-amznremoved="mobi7">5</p>
</div>
<div id="_idContainer077" class="Content">
<h1 id="_idParaDest-118"><a id="_idTextAnchor123"></a>
 Using Trees for Predictive Analysis</h1>
</div>
<div id="_idContainer078" class="Content">
<h2>Learning Objectives</h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class="bullets">Understand the metrics used for evaluating the utility of a data model</li>
<li class="bullets">Classify datapoints based on decision trees</li>
<li class="bullets">Classify datapoints based on the random forest algorithm</li>
</ul>
<p>In this chapter, we will learn about two types of supervised learning algorithm in detail. The first algorithm will help us to classify data points using decision trees, while the other algorithm will help us classify using random forests.</p>
</div>
<div id="_idContainer085" class="Content">
<h2 id="_idParaDest-119"><a id="_idTextAnchor124"></a>
 Introduction to Decision Trees</h2>
<p>In decision trees, we have input and corresponding output in the training data. A decision tree, like any tree, has leaves, branches, and nodes. Leaves are the end nodes like a yes or no. Nodes are where a decision is taken. A decision tree consists of rules that we use to formulate a decision on the prediction of a data point.</p>
<p>Every node of the decision tree represents a feature and every edge coming out of an internal node represents a possible value or a possible interval of values of the tree. Each leaf of the tree represents a label value of the tree.</p>
<p>As we learned in the previous chapters, data points have features and labels. A task of a decision tree is to predict the label value based on fixed rules. The rules come from observing patterns on the training data.</p>
<p>Let's consider an example of determining the label values</p>
<p>Suppose the following training dataset is given. Formulate rules that help you determine the label value:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer079" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00049.jpg" alt="" />
</div>
</div>
<h6>Figure 5.1: Dataset to formulate the rules</h6>
<p>In this example, we predict the label value based on four features. To set up a decision tree, we have to make observations on the available data. Based on the data that's available to us, we can conclude the following:</p>
<ul>
<li>All house loans are determined as credit-worthy.</li>
<li class="_idGenParaOverride-1">Studies loans are credit-worthy as long as the debtor is employed. If the debtor is not employed, he/she is not creditworthy.</li>
<li style="list-style: none; display: inline"><div></div>
</li>
<li>Loans are credit-worthy above 75,000/year income.</li>
<li>At or below 75,000/year, car loans are credit-worthy whenever the debtor is not employed.</li>
</ul>
<p>Depending on the order of how we take these rules into consideration, we can build a tree and describe one possible way of credit scoring. For instance, the following tree maps the preceding four rules:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer080" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00050.jpg" alt="" />
</div>
</div>
<h6>Figure 5.2: Decision Tree for the loan type</h6>
<p>We first determine the loan type. House loans are automatically credit-worthy according to the first rule. Studies loans are described by the second rule, resulting in a subtree containing another decision on employment. As we have covered both house and studies loans, there are only car loans left. The third rule describes an income decision, while the fourth rule describes a decision on employment.</p>
<p>Whenever we have to score a new debtor to determine if he/she is credit-worthy, we have to go through the decision tree from top to bottom and observe the true or false value at the bottom.</p>
<p class="_idGenParaOverride-1">Obviously, a model based on seven data points is highly inaccurate, because we can generalize rules that simply do not match reality. Therefore, rules are often determined based on large amounts of data.</p>
<div></div>
<p>This is not the only way that we can create a decision tree. We can build decision trees based on other sequences of rules, too. Let's extract some other rules from the Dataset in Figure 5.1.</p>
<p>Observation 1: Notice that individual salaries that are strictly greater than 75,000 are all credit-worthy. This means that we can classify four out of seven data points with one decision.</p>
<p>
<strong class="bold _idGenCharOverride-1">Rule 1:</strong>
 Income &gt; 75,000 =&gt; CreditWorthy is true.</p>
<p>
<strong class="bold _idGenCharOverride-1">Rule 1</strong>
 classifies four out of seven data points; we need more rules for the remaining three data points.</p>
<p>
<strong class="bold _idGenCharOverride-1">Observation 2:</strong>
 Out of the remaining three data points, two are not employed. One is employed and is not credit worthy. With a vague generalization, we can claim the following rule:</p>
<p>
<strong class="bold _idGenCharOverride-1">Rule 2:</strong>
 Assuming Income &lt;= 75,000, the following holds: Employed == true =&gt; CreditWorthy is false.</p>
<p>The first two rules classify five data points. Only two data points are left. We know that their income is less than or equal to 75,000 and that none of them are employed. There are some differences between them, though:</p>
<ul>
<li>The credit-worthy person makes 75,000, while the non-credit-worthy person makes 25,000.</li>
<li>The credit-worthy person took a car loan, while the non-credit-worthy person took a studies loan.</li>
<li>The credit-worthy person took a loan of 30,000, while the non-credit-worthy person took a loan of 15,000</li>
</ul>
<p>Any of these differences can be extracted into a rule. For discrete ranges, such as car, studies, and house, the rule is a simple membership check. In the case of continuous ranges such as salary and loan amount, we need to determine a range to branch off.</p>
<p>Let's suppose that we chose the loan amount as a basis for our third rule.</p>
<p>
<strong class="bold _idGenCharOverride-1">Rule 3:</strong>
</p>
<p>Assuming <strong class="inline _idGenCharOverride-2">Income &lt;= 75,000</strong>
 and <strong class="inline _idGenCharOverride-2">Employed == false</strong>
 ,</p>
<p>If <strong class="inline _idGenCharOverride-2">LoanAmount &lt;= AMOUNT</strong>
</p>
<p>Then <strong class="inline _idGenCharOverride-2">CreditWorthy</strong>
 is <strong class="inline _idGenCharOverride-2">false</strong>
</p>
<p class="_idGenParaOverride-1">Else <strong class="inline _idGenCharOverride-2">CreditWorthy</strong>
 is <strong class="inline _idGenCharOverride-2">true</strong>
</p>
<div></div>
<p>The first line describes the path that leads to this decision. The second line formulates the condition, and the last two lines describe the result.</p>
<p>Notice that there is a constant AMOUNT in the rule. What should AMOUNT be equal to?</p>
<p>The answer is, any number is fine in the range 15,000 &lt;= AMOUNT &lt; 30,000. We are free to select any number. In this example, we chose the bottom end of the range:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer081" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00051.jpg" alt="" />
</div>
</div>
<h6>Figure 5.3: Decision Tree for Income</h6>
<p>The second decision tree is less complex. At the same time, we cannot overlook the fact that the model says, "higher loan amounts are more likely to be repaid than lower loan amounts." It is also hard to overlook the fact that employed people with a lower income never pay back their loans. Unfortunately, there is not enough training data available, which makes it likely that we end up with false conclusions.</p>
<p>Overfitting is a frequent problem in decision trees when making a decision based on a few data points. This decision is rarely representative.</p>
<p class="_idGenParaOverride-1">Since we can build decision trees in any possible order, it makes sense to define the desired way of algorithmically constructing a decision tree. Therefore, we will now explore a good measure for optimally ordering the features in the decision process.</p>
<div></div>
<h3 id="_idParaDest-120"><a id="_idTextAnchor125"></a>
 Entropy</h3>
<p>In information theory, entropy measures how randomly distributed the possible values of an attribute are. The higher the degree of randomness, the higher the entropy of the attribute.</p>
<p>Entropy is the highest possibility of an event. If we know beforehand what the outcome will be then the event has no randomness. So, entropy is zero.</p>
<p>When measuring the entropy of a system to be classified, we measure the entropy of the label.</p>
<p>Entropy is defined as follows:</p>
<ul>
<li>
<strong class="inline _idGenCharOverride-2">[v1, v2, ..., vn]</strong>
 are the possible values of an attribute</li>
<li>
<strong class="inline _idGenCharOverride-2">[p1, p2, ..., pn]</strong>
 is the probability of these values occurring for that attribute assuming the values are equally distributed</li>
<li>
<strong class="inline _idGenCharOverride-2">p1 + p2 + ... + pn = 1</strong>
<div id="_idContainer082" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00052.jpg" alt="" />
</div>
</li>
</ul>
<h6>Figure 5.4: Entropy Formula</h6>
<p>The symbol of entropy is H in information theory. Not because entropy has anything to do with the h sound, but because H is the symbol for the upper-case Greek letter eta. Eta is, the symbol of entropy.</p>
<h4>Note</h4>
<p class="callout _idGenParaOverride-1">We use entropy to order the nodes in the decision tree because the lower the entropy, the less randomly its values are distributed. The less randomness there is in the distribution, the more likely it is that the value of the label can be determined.</p>
<div></div>
<p>To calculate the entropy of a distribution in Python, we have to use the NumPy library:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">distribution = list(range(1,4))</p>
<p class="snippet">minus_distribution = [-x for x in distribution]</p>
<p class="snippet">log_distribution = [x for x in map(np.log2,distribution)]</p>
<p class="snippet">entropy_value = np.dot(minus_distribution, log_distribution)</p>
<p>The distribution is given as a NumPy array or a regular list. On line 2, you have to insert your own distribution in place of <strong class="inline _idGenCharOverride-2">[p1, p2, …, pn]</strong>
 .</p>
<p>We need to create a vector of the negated values of the distribution in line 3.</p>
<p>On line 4, we must take the base 2 logarithm of each value in the distribution list</p>
<p>Finally, we calculate the sum with the scalar product, also known as the dot product of the two vectors.</p>
<p>Let's define the preceding calculation in the form of a function:</p>
<p class="snippet">def entropy(distribution):</p>
<p class="snippet">    minus_distribution = [-x for x in distribution]</p>
<p class="snippet">    log_distribution = [x for x in map(np.log2, distribution)]</p>
<p class="snippet">    return np.dot(minus_distribution, log_distribution)</p>
<h4>Note</h4>
<p class="callout">You first learned about the dot product in <em class="italics _idGenCharOverride-3">Chapter 3</em>
 , <em class="italics _idGenCharOverride-3">Regression</em>
 . The dot product of two vectors is calculated by multiplying the ith coordinate of the first vector by the ith coordinate of the second vector, for each i. Once we have all of the products, we sum the values:</p>
<p class="callout _idGenParaOverride-1">
<strong class="inline _idGenCharOverride-4">np.dot([1, 2, 3], [4, 5, 6]) # 1*4 + 2*5 + 3*6 = 32</strong>
</p>
<div></div>
<h3 id="_idParaDest-121"><a id="_idTextAnchor126"></a>
 Exercise 15: Calculating the Entropy</h3>
<p>Calculate the entropy of the features in the dataset in <em class="italics _idGenCharOverride-5">Figure 5.1</em>
 .</p>
<p>We will calculate entropy for all features.</p>
<ol>
<li value="1">We have four features: <strong class="bold _idGenCharOverride-1">Employed</strong>
 , <strong class="bold _idGenCharOverride-1">Income</strong>
 , <strong class="bold _idGenCharOverride-1">LoanType</strong>
 , and <strong class="bold _idGenCharOverride-1">LoanAmount</strong>
 . For simplicity, we will treat the values in <strong class="bold _idGenCharOverride-1">Income</strong>
 and <strong class="bold _idGenCharOverride-1">LoanAmount</strong>
 as discrete values for now.</li>
<li value="2">The following is the distribution of values for <strong class="bold _idGenCharOverride-1">Employed</strong>
 :<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">true 4/7 times</strong>
</p>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">false 3/7 times</strong>
</p>
</li>
<li value="3">Let's use the entropy function to calculate the entropy of the Employed column:<p class="snippet _idGenParaOverride-3">H_employed = entropy([4/7, 3/7])</p>
<p class="_idGenParaOverride-2">The output is <strong class="inline _idGenCharOverride-2">0.9852</strong>
 .</p>
</li>
<li value="4">The following is the distribution of values for <strong class="bold _idGenCharOverride-1">Income</strong>
 :<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">25,000 1/7 times</strong>
</p>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">75,000 2/7 times</strong>
</p>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">80,000 1/7 times</strong>
</p>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">100,000 2/7 times</strong>
</p>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">125,000 1/7 times</strong>
</p>
</li>
<li value="5">Let's use the entropy function to calculate the entropy of the Income column:<p class="snippet _idGenParaOverride-3">H_income = entropy([1/7, 2/7, 1/7, 2/7, 1/7])</p>
<p class="_idGenParaOverride-2">The output is <strong class="inline _idGenCharOverride-2">2.2359</strong>
 .</p>
</li>
<li value="6">The following is the distribution of values for <strong class="bold _idGenCharOverride-1">LoanType</strong>
 :<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">car 3/7 times</strong>
</p>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">studies 2/7 times</strong>
</p>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">house 2/7 times</strong>
</p>
</li>
<li value="7">Let's use the entropy function to calculate the entropy of the LoanType column:<p class="snippet _idGenParaOverride-3">H_loanType = entropy([3/7, 2/7, 2/7])</p>
<p class="_idGenParaOverride-4">The output is <strong class="inline _idGenCharOverride-2">1.5567</strong>
 .</p>
<div></div>
</li>
<li value="8">The following is the distribution of values for <strong class="bold _idGenCharOverride-1">LoanAmount</strong>
 :<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">15,000 1/7 times</strong>
</p>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">25,000 1/7 times</strong>
</p>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">30,000 3/7 times</strong>
</p>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">125,000 1/7 times</strong>
</p>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">150,000 1/7 times</strong>
</p>
</li>
<li value="9">Let's use the entropy function to calculate the entropy of the LoanAmount column:<p class="snippet _idGenParaOverride-3">H_LoanAmount = entropy([1/7, 1/7, 3/7, 1/7, 1/7])</p>
<p class="_idGenParaOverride-2">The output is <strong class="inline _idGenCharOverride-2">2.1281</strong>
 .</p>
</li>
<li value="10">As you can see, the closer the distribution is to the uniform distribution, the higher the entropy.</li>
<li value="11">In this exercise, we were cheating a bit, because these are not the entropies that we will be using to construct the tree. In both trees, we had conditions like "greater than 75,000". We will therefore calculate the entropies belonging to the decision points we used in our original trees as well.</li>
<li value="12">The following is the distribution of values for <strong class="bold _idGenCharOverride-1">Income&gt;75000</strong>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">true 4/7 times</strong>
</p>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">false 3/7 times</strong>
</p>
</li>
<li value="13">Let's use the entropy function to calculate the entropy of the Income&gt;75,000 column:<p class="snippet _idGenParaOverride-3">H_incomeMoreThan75K = entropy([4/7, 3/7])</p>
<p class="_idGenParaOverride-2">The output is <strong class="inline _idGenCharOverride-2">0.9852</strong>
 .</p>
</li>
<li value="14">The following is the distribution of values for <strong class="bold _idGenCharOverride-1">LoanAmount&gt;15000</strong>
<p class="_idGenParaOverride-2">
<strong class="inline _idGenCharOverride-2">true 6/7 times</strong>
</p>
<p class="_idGenParaOverride-4">
<strong class="inline _idGenCharOverride-2">false 1/7 times</strong>
</p>
<div></div>
</li>
<li value="15">Let's use the entropy function to calculate the entropy of the <strong class="bold _idGenCharOverride-1">LoanAmount</strong>
<p class="_idGenParaOverride-2">&gt;15,000 column:</p>
<p class="snippet _idGenParaOverride-3">H_loanMoreThan15K = entropy([6/7, 1/7])</p>
<p class="_idGenParaOverride-2">The output is <strong class="inline _idGenCharOverride-2">0.5917</strong>
 .</p>
</li>
</ol>
<p>Intuitively, the distribution [1] is the most deterministic distribution. This is because we know for a fact that there is 100% chance that the value of a feature stays fixed.</p>
<p>
<strong class="inline _idGenCharOverride-2">H([1]) = 1 * np.log2( 1 ) # 1*0 =0</strong>
</p>
<p>We can conclude that the entropy of a distribution is strictly non-negative.</p>
<h3 id="_idParaDest-122"><a id="_idTextAnchor127"></a>
 Information Gain</h3>
<p>When we partition the data points in a dataset according to the values of an attribute, we reduce the entropy of the system.</p>
<p>To describe information gain, we can calculate the distribution of the labels. Initially, we have five credit-worthy and two not credit-worthy individuals in our dataset. The entropy belonging to the initial distribution is as follows:</p>
<p class="snippet">H_label = entropy([5/7, 2/7])</p>
<p class="snippet">0.863120568566631</p>
<p>Let's see what happens if we partition the dataset based on whether the loan amount is greater than 15,000 or not.</p>
<ul>
<li>In group 1, we get one data point belonging to the 15,000 loan amount. This data point is not credit-worthy.</li>
<li>In group 2, we have 5 credit-worthy and 1 non-credit-worthy individuals.</li>
</ul>
<p>The entropy of the labels in each group is as follows:</p>
<p class="snippet">H_group1 = entropy([1]) #0</p>
<p class="snippet _idGenParaOverride-1">H_group2 = entropy([5/6, 1/6]) #0.65</p>
<div></div>
<p>To calculate the information gain, let's calculate the weighted average of the group entropies:</p>
<p class="snippet">H_group1 * 1/7 + H_group2 * 6/7 #0.55</p>
<p class="snippet">Information_gain = 0.8631 – 0.5572 #0.30</p>
<p>When creating the decision tree, on each node, our job is to partition the dataset using a rule that maximizes the information gain.</p>
<p>We could also use Gini Impurity instead of entropy-based information gain to construct the best rules for splitting decision trees.</p>
<h3 id="_idParaDest-123"><a id="_idTextAnchor128"></a>
 Gini Impurity</h3>
<p>Instead of entropy, there is another widely used metric that can be used to measure the randomness of a distribution: Gini Impurity.</p>
<p>Gini Impurity is defined as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer083" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00053.jpg" alt="" />
</div>
</div>
<h6>Fig 5.5: Gini Impurity</h6>
<p>For two variables, the Gini Impurity is:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer084" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00054.jpg" alt="" />
</div>
</div>
<h6 class="_idGenParaOverride-1">Fig 5.6: Gini Impurity for two variables</h6>
<div></div>
<p>Entropy may be a bit slower to calculate because of the usage of the logarithm. Gini Impurity, on the other hand, is less precise when it comes to measuring randomness.</p>
<h4>Note</h4>
<p class="callout">Is information gain with entropy or Gini Impurity better for creating a decision tree?</p>
<p class="callout">Some people prefer Gini Impurity, because you don't have to calculate with logarithms. Computation-wise, none of the solutions are particularly complex, and so both of them can be used. When it comes to performance, the following study concluded that there are often just minimal differences between the two metrics: <a href="https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf">https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf</a>
 .</p>
<p>We have learned that we can optimize a decision tree based on information gain or Gini Impurity. Unfortunately, these metrics are only available for discrete values. What if the label is defined on a continuous interval such as a price range or salary range?</p>
<p>We have to use other metrics. You can technically understand the idea behind creating a decision tree based on a continuous label, which was about regression. The metric we can reuse from this chapter is the mean squared error. Instead of Gini Impurity or information gain, we have to minimize the mean squared error to optimize the decision tree. As this is a beginner's book, we will omit this metric.</p>
<h3 id="_idParaDest-124"><a id="_idTextAnchor129"></a>
 Exit Condition</h3>
<p>We can continuously split the data points according to rule values until each leaf of the decision tree has an entropy of zero. The question is whether this end state is desirable.</p>
<p>Often, this state is not desirable, because we risk overfitting the model. When our rules for the model are too specific and too nitpicky, and the sample size on which the decision was made is too small, we risk making a false conclusion, thus recognizing a pattern in the dataset that simply does not exist in real life.</p>
<p class="_idGenParaOverride-1">For instance, if we spin a roulette wheel three times and we get 12, 25, 12, concluding that every odd spin result in the value 12 is not a sensible strategy. By assuming that every odd spin equals 12, we find a rule that is exclusively due to random noise.</p>
<div></div>
<p>Therefore, posing a restriction on the minimum size of the dataset that we can still split is an exit condition that works well in practice. For instance, if you stop splitting as soon as you have a dataset that's lower than 50, 100, 200, or 500 in size, you avoid drawing conclusions on random noise, and so you minimize the risk of overfitting the model.</p>
<p>Another popular exit condition is a maximum restriction on the depth of the tree. Once we reach a fixed tree depth, we classify the data points in the leaves.</p>
<h3 id="_idParaDest-125"><a id="_idTextAnchor130"></a>
 Building Decision Tree Classifiers using scikit-learn</h3>
<p>We have already learned how to load data from a <strong class="inline _idGenCharOverride-2">.csv</strong>
 file, how to apply preprocessing on the data, and how to split data into a training and testing dataset. If you need to refresh yourself on this knowledge, go back to previous chapters, where you go through this process in the context of regression and classification.</p>
<p>We will now assume that a set of training features, training labels, testing features, and testing labels are given as a return value of the scikit-learn train-test-split call.</p>
<p>Notice that, in older versions of scikit-learn, you have to import cross_validation instead of model selection:</p>
<p class="snippet">features_train, features_test, label_train, label_test =</p>
<p class="snippet">    model_selection.train_test_split(</p>
<p class="snippet">        features,</p>
<p class="snippet">        label,</p>
<p class="snippet">        test_size=0.1</p>
<p class="snippet">    )</p>
<p>We will not focus on how we got these data points, because the process is exactly the same as in the case of regression and classification.</p>
<p>It's time to import and use the decision tree classifier of scikit-learn:</p>
<p class="snippet">from sklearn.tree import DecisionTreeClassifier</p>
<p class="snippet">decision_tree = DecisionTreeClassifier(max_depth=6)</p>
<p class="snippet _idGenParaOverride-1">decision_tree.fit( features_train, label_train )</p>
<div></div>
<p>We set one optional parameter in the <strong class="inline _idGenCharOverride-2">DecisionTreeClassifier</strong>
 , that is, <strong class="inline _idGenCharOverride-2">max_depth</strong>
 , to limit the depth of the decision tree. You can read the official documentation for a full list of parameters: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a>
 . Some of the more important parameters are as follows:</p>
<ul>
<li>
<strong class="bold _idGenCharOverride-1">criterion</strong>
 : Gini stands for Gini Impurity, while entropy stands for information gain.</li>
<li>
<strong class="bold _idGenCharOverride-1">max_depth</strong>
 : This is the maximum depth of the tree.</li>
<li>
<strong class="bold _idGenCharOverride-1">min_samples_split</strong>
 : This is the minimum number of samples needed to split an internal node.</li>
</ul>
<p>You can also experiment with all of the other parameters enumerated in the documentation. We will omit them in this topic.</p>
<p>Once the model has been built, we can use the decision tree classifier to predict data:</p>
<p class="snippet">decision_tree.predict(features_test)</p>
<p>You will build a decision tree classifier in the activity at the end of this topic.</p>
<h3 id="_idParaDest-126"><a id="_idTextAnchor131"></a>
 Evaluating the Performance of Classifiers</h3>
<p>After splitting training and testing data, the decision tree model has a score method to evaluate how well testing data is classified by the model. We already learned how to use the score method in Chapters 3 and 4:</p>
<p class="snippet">decision_tree.score(features_test, label_test)</p>
<p>The return value of the score method is a number that's less than or equal to 1. The closer we get to 1, the better our model is.</p>
<p>We will now learn another way to evaluate the model. Feel free to use this method on the models you constructed in the previous chapter as well.</p>
<p>Suppose we have one test feature and one test label:</p>
<p class="snippet"># testLabel denotes the test label</p>
<p class="snippet">predicted_label = decision_tree.predict(testFeature)</p>
<p class="_idGenParaOverride-1">Suppose we are investigating a label value, positiveValue.</p>
<div></div>
<p>We will use the following definitions to define some metrics that help you evaluate how good your classifier is:</p>
<ul>
<li>
<strong class="bold _idGenCharOverride-1">Definition (True Positive)</strong>
 : <strong class="inline _idGenCharOverride-2">positiveValue == predictedLabel == testLabel</strong>
</li>
<li>
<strong class="bold _idGenCharOverride-1">Definition (True Negative)</strong>
 : <strong class="inline _idGenCharOverride-2">positiveValue != predictedLabel == testLabel</strong>
</li>
<li>
<strong class="bold _idGenCharOverride-1">Definition (False Positive)</strong>
 : <strong class="inline _idGenCharOverride-2">positiveValue == predictedLabel != testLabel</strong>
</li>
<li>
<strong class="bold _idGenCharOverride-1">Definition (False Negative)</strong>
 : <strong class="inline _idGenCharOverride-2">positiveValue != predictedLabel != testLabel</strong>
</li>
</ul>
<p>A false positive is a prediction that is equal to the positive value, but the actual label in the test data is not equal to this positive value. For instance, in a tech interview, a false positive is an incompetent software developer who got hired because he acted in a convincing manner, hiding his complete lack of competence.</p>
<p>Don't confuse a false positive with a false negative. Using the tech interview example, a false negative is a software developer who was competent enough to do the job, but he did not get hired.</p>
<p>Using the preceding four definitions, we can define three metrics that describe how well our model predicts reality. The symbol #( X ) denotes the number of values in X. Using technical terms, #( X ) denotes the cardinality of X:</p>
<p>
<strong class="bold _idGenCharOverride-1">Definition (Precision):</strong>
</p>
<p>
<strong class="inline _idGenCharOverride-2">#( True Positives ) / (#( True Positives ) + #( False Positives ))</strong>
</p>
<p>
<strong class="bold _idGenCharOverride-1">Definition (Recall):</strong>
</p>
<p>
<strong class="inline _idGenCharOverride-2">#( True Positives ) / (#( True Positives ) + #( False Negatives ))</strong>
</p>
<p>Precision centers around values that our classifier found to be positive. Some of these results are true positive, while others are false positive. A high precision means that the number of false positive results are very low compared to true positive results. This means that a precise classifier rarely makes a mistake when finding a positive result.</p>
<p class="_idGenParaOverride-1">Recall that centers around values are positive among the test data. Some of these results are found by the classifier. These are the true positive values. Those positive values that are not found by the classifier are false negatives. A classifier with a high recall value finds most of the positive values.</p>
<div></div>
<h3 id="_idParaDest-127"><a id="_idTextAnchor132"></a>
 Exercise 16: Precision and Recall</h3>
<p>Find the precision and the recall value of the following two classifiers:</p>
<p class="snippet"># Classifier 1</p>
<p class="snippet">TestLabels1 = [True, True, False, True, True]</p>
<p class="snippet">PredictedLabels1 = [True, False, False, False, False]</p>
<p class="snippet"># Classifier 2</p>
<p class="snippet">TestLabels2 = [True, True, False, True, True]</p>
<p class="snippet">PredictedLabels = [True, True, True, True, True]</p>
<ol>
<li class="ParaOverride-1" value="1">According to the formula let's calculate the number of true positives, false positives, and false negatives for classifier 1:<p class="snippet _idGenParaOverride-3">TruePositives1 = 1 # both the predicted and test labels are true</p>
<p class="snippet _idGenParaOverride-3">FalsePositives1 = 0 # predicted label is true, test label is false</p>
<p class="snippet _idGenParaOverride-3">FalseNegatives1 = 3 # predicted label is false, test label is true</p>
<p class="snippet _idGenParaOverride-3">Precision1 = TruePositives1 / (TruePositives1 + FalsePositives1)</p>
<p class="snippet _idGenParaOverride-3">Precision1 # 1/1 = 1</p>
<p class="snippet _idGenParaOverride-3">Recall1 = TruePositives1 / (TruePositives1 + FalseNegatives1)</p>
<p class="snippet _idGenParaOverride-3">Recall1 #1/4 = 0.25</p>
</li>
<li value="2">The first classifier has excellent precision, but bad recall. Let's calculate the same for the second classifier.<p class="snippet _idGenParaOverride-3">TruePositives2 = 4</p>
<p class="snippet _idGenParaOverride-3">FalsePositives2 = 1</p>
<p class="snippet _idGenParaOverride-3">FalseNegatives2 = 0</p>
<p class="snippet _idGenParaOverride-3">Precision2 = TruePositives2 / (TruePositives2 + FalsePositives2) Precision2 #4/5 = 0.8</p>
<p class="snippet _idGenParaOverride-3">Recall2 = TruePositives2 / (TruePositives2 + FalseNegatives2)</p>
<p class="snippet _idGenParaOverride-3">Recall2 # 4/4 = 1</p>
</li>
<li value="3">The second classifier has excellent recall, but its precision is not perfect.</li>
<li class="_idGenParaOverride-1" value="4">The F1 score is the harmonic mean of precision and recall. Its value ranges between 0 and 1. The advantage of the F1 score is that it considers both false positives and false negatives.<div></div>
</li>
</ol>
<h3 id="_idParaDest-128"><a id="_idTextAnchor133"></a>
 Exercise 17: Calculating the F1 Score</h3>
<p>Calculate the F1 Score of the two classifiers from the previous exercise:</p>
<ol>
<li class="ParaOverride-1" value="1">The formula for calculating the F1 Score is as follows:<p class="snippet _idGenParaOverride-3">2*Precision*Recall / (Precision + Recall)</p>
</li>
<li value="2">The first classifier has the following F1 Score:<p class="snippet _idGenParaOverride-3">2 * 1 * 0.25 / (1 + 0.25) # 0.4</p>
</li>
<li value="3">The second classifier has the following F1 Score:<p class="snippet _idGenParaOverride-3">2 * 0.8 * 1 / (0.8 + 1) # 0.888888888888889</p>
</li>
</ol>
<p>Now that we know what precision, recall, and F1 score mean, let's use a scikit-learn utility to calculate and print these values:</p>
<p class="snippet">from sklearn.metrics import classification_report</p>
<p class="snippet">print(</p>
<p class="snippet">    classification_report(</p>
<p class="snippet">        label_test,</p>
<p class="snippet">        decision_tree.predict(features_test)</p>
<p class="snippet">    )</p>
<p class="snippet">)</p>
<p>The output will be as follows:</p>
<p class="snippet">             precision    recall f1-score support</p>
<p class="snippet">          0     0.97     0.97     0.97        36</p>
<p class="snippet">          1     1.00     1.00     1.00         5</p>
<p class="snippet">          2     1.00     0.99     1.00     127</p>
<p class="snippet">          3     0.83     1.00     0.91         5</p>
<p class="snippet _idGenParaOverride-1">avg / total     0.99     0.99     0.99     173</p>
<div></div>
<p>In this example, there are four possible label values, denoted by 0, 1, 2, and 3. In each row, you get a precision, recall, and F1 score value belonging to each possible label value. You can also see in the support column how many of these label values exist in the dataset. The last row contains an aggregated precision, recall, and f1-score.</p>
<p>If you used label encoding to encode string labels to numbers, you may want to perform an inverse transformation to find out which row belongs to which label. In the following example, Class is the name of the label, and <strong class="inline _idGenCharOverride-2">labelEncoders['Class']</strong>
 is the label encoder belonging to the Class label:</p>
<p class="snippet">labelEncoders['Class'].inverse_transform([0, 1, 2, 3])</p>
<p class="snippet">array(['acc', 'good', 'unacc', 'vgood'])</p>
<p>If you prefer calculating the precision, recall, and F1 Score on its own, you can use individual calls. Note that in the next example, we will call each score function twice: once with <strong class="inline _idGenCharOverride-2">average=None</strong>
 , and once with <strong class="inline _idGenCharOverride-2">average='weighted'</strong>
 .</p>
<p>When the average is specified as None, we get the score value belonging to each possible label value. You can see the same values rounded in the table if you compare the results to the first four values of the corresponding column.</p>
<p>When the average is specified as weighted, you get the cell value belonging to the column of the score name and the avg/total row:</p>
<p class="snippet">from sklearn.metrics import recall_score, precision_score, f1_score</p>
<p class="snippet">label_predicted = decision_tree.predict(features_test)</p>
<p>Calculating the precision score with no average can be done like so:</p>
<p class="snippet">precision_score(label_test, label_predicted, average=None)</p>
<p>The output is as follows:</p>
<p class="snippet"> array([0.97222222, 1.        , 1.        , 0.83333333])</p>
<p>Calculating the precision score with a weighted average can be done like so:</p>
<p class="snippet">precision_score(label_test, label_predicted, average='weighted')</p>
<p>The output is <strong class="inline _idGenCharOverride-2">0.989402697495183</strong>
 .</p>
<p>Calculating the recall score with no average can be done like so:</p>
<p class="snippet _idGenParaOverride-1">recall_score(label_test, label_predicted, average=None)</p>
<div></div>
<p>The output is as follows:</p>
<p class="snippet"> array([0.97222222, 1.        , 0.99212598, 1.        ])</p>
<p>Calculating the recall score with a weighted average can be done like so:</p>
<p class="snippet">recall_score(label_test, label_predicted, average='weighted')</p>
<p>The output is <strong class="inline _idGenCharOverride-2">0.9884393063583815</strong>
 .</p>
<p>Calculating the f1_score with no average can be done like so:</p>
<p class="snippet">f1_score(label_test, label_predicted, average=None)</p>
<p>The output is as follows:</p>
<p class="snippet"> array([0.97222222, 1.        , 0.99604743, 0.90909091])</p>
<p>Calculating the f1_score with a weighted average can be done like so:</p>
<p class="snippet">f1_score(label_test, label_predicted, average='weighted')</p>
<p>The output is <strong class="inline _idGenCharOverride-2">0.988690625785373</strong>
 .</p>
<p>There is one more score worth investigating: the accuracy score. Suppose #( Dataset ) denotes the length of the total dataset, or in other words, the sum of true positives, true negatives, false positives, and false negatives.</p>
<p>Accuracy is defined as follows:</p>
<p>
<strong class="bold _idGenCharOverride-1">Definition (Accuracy)</strong>
 : #( True Positives ) + #( True Negatives ) / #( Dataset )</p>
<p>Accuracy is a metric that's used for determining how many times the classifier gives us the correct answer. This is the first metric we used to evaluate the score of a classifier. Whenever we called the score method of a classifier model, we calculated its accuracy:</p>
<p class="snippet">from sklearn.metrics import accuracy_score</p>
<p class="snippet">accuracy_score(label_test, label_predicted )</p>
<p>The output is <strong class="inline _idGenCharOverride-2">0.9884393063583815</strong>
 .</p>
<p>Calculating the decision tree score can be done like so:</p>
<p class="snippet">decisionTree.score(features_test, label_test)</p>
<p class="_idGenParaOverride-1">The output is <strong class="inline _idGenCharOverride-2">0.9884393063583815</strong>
 .</p>
<div></div>
<h3 id="_idParaDest-129"><a id="_idTextAnchor134"></a>
 Confusion Matrix</h3>
<p>We will conclude this topic with one data structure that helps you evaluate the performance of a classification model: the confusion matrix.</p>
<p>A confusion matrix is a square matrix, where the number of rows and columns equals the number of distinct label values. In the columns of the matrix, we place each test label value. In the rows of the matrix, we place each predicted label value.</p>
<p>For each data point, we add one to the corresponding cells of the confusion matrix based on the predicted and actual label value.</p>
<h3 id="_idParaDest-130"><a id="_idTextAnchor135"></a>
 Exercise 18: Confusion Matrix</h3>
<p>Construct the confusion matrix of the following two distributions:</p>
<p class="snippet"># Classifier 1</p>
<p class="snippet">TestLabels1 = [True, True, False, True, True]</p>
<p class="snippet">PredictedLabels1 = [True, False, False, False, False]</p>
<p class="snippet"># Classifier 2</p>
<p class="snippet">TestLabels2 = [True, True, False, True, True]</p>
<p class="snippet">PredictedLabels = [True, True, True, True, True]</p>
<ol>
<li class="ParaOverride-1" value="1">We will start with the first classifier. The columns determine the place of the test labels, while the rows determine the place of the predicted labels. The first entry is <strong class="inline _idGenCharOverride-2">TestLabels1[0]</strong>
 and <strong class="inline _idGenCharOverride-2">PredictedLabels1[0]</strong>
 . The values are true and true, and so we add 1 to the top-left column.</li>
<li value="2">The second values are <strong class="inline _idGenCharOverride-2">TestLabels1[1] = True</strong>
 and <strong class="inline _idGenCharOverride-2">PredictedLabels1[1] = False.</strong>
 These values determine the bottom-left cell of the 2x2 matrix.</li>
<li value="3">After finishing the placement of all five label pairs, we get the following confusion matrix:<p class="snippet _idGenParaOverride-3">       True False</p>
<p class="snippet _idGenParaOverride-3">True     1     0</p>
<p class="snippet _idGenParaOverride-3">False     3     1</p>
</li>
<li value="4">After finishing the placement of all five label pairs, we get the following confusion matrix:<p class="snippet _idGenParaOverride-3">       True False</p>
<p class="snippet _idGenParaOverride-3">True     4     1</p>
<p class="snippet _idGenParaOverride-5">False     0     0</p>
<div></div>
</li>
<li value="5">In a 2x2 matrix, we have the following distribution:<p class="snippet _idGenParaOverride-3">                True            False</p>
<p class="snippet _idGenParaOverride-3">True    TruePositives FalsePositives</p>
<p class="snippet _idGenParaOverride-3">False FalseNegatives TrueNegatives    </p>
</li>
<li value="6">The confusion matrix can be used to calculate precision, recall, accuracy, and f1_score metrics. The calculation is straightforward and is implied by the definitions of the metrics.</li>
<li value="7">The confusion matrix can be calculated by scikit-learn:<p class="snippet _idGenParaOverride-3">from sklearn.metrics import confusion_matrix</p>
<p class="snippet _idGenParaOverride-3">confusion_matrix(label_test, label_predicted)</p>
<p class="snippet _idGenParaOverride-3">array([[ 25, 0, 11, 0],</p>
<p class="snippet _idGenParaOverride-3">     [ 5, 0, 0, 0],</p>
<p class="snippet _idGenParaOverride-3">     [ 0, 0, 127, 0],</p>
<p class="snippet _idGenParaOverride-3">       [ 5, 0, 0, 0]])</p>
</li>
<li value="8">Note that this is not the same example as the one we used in the previous section. Therefore, if you use the values inside the confusion matrix, you will get different precision, recall, and f1_score values.</li>
<li value="9">You can also use pandas to create the confusion matrix:<p class="snippet _idGenParaOverride-3">import pandas</p>
<p class="snippet _idGenParaOverride-3">pandas.crosstab(label_test, label_predicted)</p>
<p class="snippet _idGenParaOverride-3">col_0 0    2</p>
<p class="snippet _idGenParaOverride-3">row_0        </p>
<p class="snippet _idGenParaOverride-3">0     25 11</p>
<p class="snippet _idGenParaOverride-3">1     5    0</p>
<p class="snippet _idGenParaOverride-3">2     0 127</p>
<p class="snippet _idGenParaOverride-3">3     5    0</p>
</li>
</ol>
<p>Let's verify these values by calculating the accuracy score of the model:</p>
<ul>
<li>We have 127 + 25 = 152 data points that were classified correctly.</li>
<li>The total number of data points is 152 + 11 + 5 + 5 = 173.</li>
<li class="_idGenParaOverride-1">152/173 is 0.8786127167630058.</li>
<li style="list-style: none; display: inline"><div></div>
</li>
</ul>
<p>Let's calculate the accuracy score by using the scikit-learn utility we used before:</p>
<p class="snippet">from sklearn.metrics import accuracy_score</p>
<p class="snippet">accuracy_score(label_test, label_predicted)</p>
<p>The output is as follows:</p>
<p class="snippet">0.8786127167630058</p>
<p>We got the same value. All of the metrics can be derived from the confusion matrix.</p>
<h3 id="_idParaDest-131"><a id="_idTextAnchor136"></a>
 Activity 10: Car Data Classification</h3>
<p>In this<a id="_idTextAnchor137"></a>
 section, we will discuss how to build a reliable decision tree model that's capable of aiding your company in finding cars that clients are likely to buy. We will be assuming that you are employed by a car rental agency who's focusing on building a lasting relationship with its clients. Your task is to build a decision tree model that classifies cars into one of four categories: unacceptable, acceptable, good, and very good.</p>
<p>The dataset for this activity can be accessed here: <a href="https://archive.ics.uci.edu/ml/datasets/Car+Evaluation">https://archive.ics.uci.edu/ml/datasets/Car+Evaluation</a>
 . Click the Data Folder link to download the dataset. Then, click the Dataset Description link to access the description of the attributes.</p>
<p>Let's evaluate the utility of your decision tree model:</p>
<ol>
<li class="ParaOverride-1" value="1">Download the car data file from here: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data">https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data</a>
 . Add a header line to the front of the CSV file so that you can reference it in Python easily. We simply call the label Class. We named the six features after their descriptions in <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names">https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names</a>
 .</li>
<li value="2">Load the dataset into Python and check if it has loaded properly.<p class="_idGenParaOverride-2">It's time to separate the training and testing data with the cross-validation (in newer versions, this is model-selection) feature of scikit-learn. We will use 10% test data.</p>
<p class="_idGenParaOverride-2">Note that the <strong class="inline _idGenCharOverride-2">train_test_split</strong>
 method will be available in the <strong class="inline _idGenCharOverride-2">model_selection</strong>
 module, and not in the <strong class="inline _idGenCharOverride-2">cross_validation</strong>
 module, starting from scikit-learn 0.20. In previous versions, <strong class="inline _idGenCharOverride-2">model_selection</strong>
 already contains the <strong class="inline _idGenCharOverride-2">train_test_split</strong>
 method.</p>
<p class="_idGenParaOverride-2">Build the decision tree classifier.</p>
</li>
<li class="_idGenParaOverride-1" value="3">Check the score of our model based on the test data.<div></div>
</li>
<li value="4">Create a deeper evaluation of the model based on the <strong class="inline _idGenCharOverride-2">classification_report</strong>
 feature.<h4 class="_idGenParaOverride-6">Note</h4>
<p class="callout _idGenParaOverride-6">The solution to this activity is available at page 282.</p>
</li>
</ol>
<h2 id="_idParaDest-132"><a id="_idTextAnchor138"></a>
 Random Forest Classifier</h2>
<p>If you <a id="_idTextAnchor139"></a>
 think about the name Random forest classifier, it makes sense to conclude the following:</p>
<ul>
<li>A forest consists of multiple trees.</li>
<li>These trees can be used for classification.</li>
<li>Since the only tree we have used so far for classification is a decision tree, it makes sense that the random forest is a forest of decision trees.</li>
<li>The random nature of the trees means that our decision trees are constructed in a randomized manner.</li>
<li>As a consequence, we will base our decision tree construction on information gain or Gini Impurity.</li>
</ul>
<p>Once you understand these basic concepts, you essentially know what a Random forest classifier is all about. The more trees you have in the forest, the more accurate prediction is going to be. When performing prediction, each tree performs classification. We collect the results, and the class that gets the most votes wins.</p>
<p class="_idGenParaOverride-1">Random forests can be used for regression as well as for classification. When using random forests for regression, instead of counting the most votes for a class, we take the average of the arithmetic mean (average) of the prediction results and return it. Random forests are not as ideal for regression as they are for classification, though, because the models used to predict values are often out of control, and often return a wide range of values. The average of these values is often not too meaningful. Managing the noise in a regression exercise is harder than in classification.</p>
<div></div>
<p>Random forests are often better than one simple decision tree because they provide redundancy. They treat outlier values better and have a lower probability of overfitting the model. Decision trees seem to behave great as long as you are using them on data that you used when creating the model. Once you use them to predict new data, random forests lose their edge. Random forests are widely used for classification problems, whether it be customer segmentation for banks or e-commerce, classifying images, or medicine. If you own an Xbox with Kinect, your Kinect device contains a random forest classifier to detect your body parts.</p>
<p>Random Forest Classification and regression are ensemble algorithms. The idea behind ensemble learning is that we take an aggregated view over a decision of multiple agents that potentially have different weaknesses. Due to the aggregated vote, these weaknesses cancel out, and the majority vote likely represents the correct result.</p>
<h3 id="_idParaDest-133"><a id="_idTextAnchor140"></a>
 Constructing a Random Forest</h3>
<p>One way to construct the trees of a random forest is to limit the number of features used in the classification task. Suppose you have a feature set, F. The length of the feature set is <strong class="inline _idGenCharOverride-2">#(F)</strong>
 . The number of features in the feature set is <strong class="inline _idGenCharOverride-2">dim(F)</strong>
 , where dim stands for dimension.</p>
<p>Suppose we limit the training data to a different subset of size <em class="italics _idGenCharOverride-5">s &lt; #(F)</em>
 , and each random forest receives a different training data set of size s. Suppose we specify that we will use <em class="italics _idGenCharOverride-5">k &lt; dim(F)</em>
 features out of the possible features to construct a tree in the random forest. The selection of k features is chosen at random.</p>
<p>We construct each decision tree completely. Once we get a new data po<a id="_idTextAnchor141"></a>
 int to classify, we execute each tree in the random forest to perform the prediction. Once the prediction results are in, we count the votes, and the most voted class is going to be the class of the data point predicted by the random forest.</p>
<p class="_idGenParaOverride-1">In random forest terminology, we describe the performance benefits of random forests with one word: bagging. Bagging is a technique that consists of bootstrapping and using aggregated decision making. Bootstrapping is responsible for creating a dataset that contains a subset of the entries of the original dataset. The size of the original dataset and the bootstrapped dataset is still the same because we are allowed to select the same data points multiple times in the bootstrapped dataset.</p>
<div></div>
<p>Out of bag data points are ones that don't end up in some bootstrapped datasets. To measure the out of bag error of a random forest classifier, we have to run all out of bag data points on trees of the random forest classifier that were built without considering the out of bag data points. The margin of error is the ratio between correctly classified out of bag data points and all out of bag data points.</p>
<h3 id="_idParaDest-134"><a id="_idTextAnchor142"></a>
 Random Forest Classification Using scikit-learn</h3>
<p>Our starting point is the result of the train-test splitting:</p>
<p class="snippet">from sklearn import model_selection</p>
<p class="snippet">features_train, features_test, label_train, label_test =</p>
<p class="snippet">    model_selection.train_test_split(</p>
<p class="snippet">        features,</p>
<p class="snippet">        label,</p>
<p class="snippet">        test_size=0.1</p>
<p class="snippet">    )</p>
<p>The random forest classifier can be implemented as follows:</p>
<p class="snippet">from sklearn.ensemble import RandomForestClassifier</p>
<p class="snippet">random_forest_classifier = RandomForestClassifier(</p>
<p class="snippet">    n_estimators=100,</p>
<p class="snippet">    max_depth=6</p>
<p class="snippet">)</p>
<p class="snippet">randomForestClassifier.fit(features_train, label_train)</p>
<p class="snippet">labels_predicted = random_forest_classifier.predict(features_test)</p>
<p class="_idGenParaOverride-1">The interface of scikit-learn makes it easy to handle the random forest classifier. Throughout the last three chapters, we have already gotten used to this way of calling a classifier or a regression model for prediction.</p>
<div></div>
<h3 id="_idParaDest-135"><a id="_idTextAnchor143"></a>
 Parameterization of the random forest classifier</h3>
<p>As usual, consult the documentation for the full list of parameters. You can find the documentation here: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.</p>
<p>We will only consider a subset of the possible parameters, based on what you already know, which is based on the description of constructing random forests:</p>
<ul>
<li>
<strong class="bold _idGenCharOverride-1">n_estimators</strong>
 : The number of trees in the random forest. The default value is 10.</li>
<li>
<strong class="bold _idGenCharOverride-1">criterion</strong>
 : Use Gini or entropy to determine whether you use Gini Impurity or information gain using entropy in each tree.</li>
<li>
<strong class="bold _idGenCharOverride-1">max_features</strong>
 : The maximum number of features considered in any tree of the forest. Possible values include an integer. You can also add some strings such as "<strong class="inline _idGenCharOverride-2">sqrt</strong>
 " for the square root of the number of features.</li>
<li>
<strong class="bold _idGenCharOverride-1">max_depth</strong>
 : The maximum depth of each tree.</li>
<li>
<strong class="bold _idGenCharOverride-1">min_samples_split</strong>
 : The minimum number of samples in the dataset in a given node to perform a split. This may also reduce the tree's size.</li>
<li>
<strong class="bold _idGenCharOverride-1">bootstrap</strong>
 : A Boolean indicating whether to use bootstrapping on data points when constructing trees.</li>
</ul>
<h3 id="_idParaDest-136"><a id="_idTextAnchor144"></a>
 Feature Importance</h3>
<p>A random forest classifier gives you information on how important each feature in data classification is. Remember, we use a lot of randomly constructed decision trees to classify data points. We can measure how accurately these data points behave, and we can also see which features are vital in decision making.</p>
<p>We can retrieve the array of feature importance scores with the following query:</p>
<p class="snippet">random_forest_classifier.feature_importances_</p>
<p>The output is as follows:</p>
<p class="snippet">array([0.12794765, 0.1022992 , 0.02165415, 0.35186759, 0.05486389,</p>
<p class="snippet">       0.34136752])</p>
<p class="_idGenParaOverride-1">In this six-feature classifier, the fourth and the sixth features are clearly a lot more important than any other features. The third feature has a very low importance score.</p>
<div></div>
<p>Feature importance scores come in handy when we have a lot of features and we want to reduce the feature size to avoid the classifier getting lost in the details. When we have a lot of features, we risk overfitting the model. Therefore, reducing the number of features by dropping the least significant ones is often helpful.</p>
<h3 id="_idParaDest-137"><a id="_idTextAnchor145"></a>
 Extremely Randomized Trees</h3>
<p>Extremely randomized trees increase randomization inside random forests by randomizing the splitting rules on top of the already randomized factors in random forests.</p>
<p>The parameterization is similar to the random forest classifier. You can see the full list of parameters here: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html</a>
 .</p>
<p>The Python implementation is as follows:</p>
<p class="snippet">from sklearn.ensemble import ExtraTreesClassifier</p>
<p class="snippet">extra_trees_classifier = ExtraTreesClassifier(</p>
<p class="snippet">    n_estimators=100,</p>
<p class="snippet">    max_depth=6</p>
<p class="snippet">)</p>
<p class="snippet">extra_trees_classifier.fit(features_train, label_train)</p>
<p class="snippet">labels_predicted = extra_trees_classifier.predict(features_test)&amp;#9;</p>
<h3 id="_idParaDest-138"><a id="_idTextAnchor146"></a>
 Activity 11: Random Forest Classification for Your Car Rental Company</h3>
<p>In this section, we will optimize your classifier so that you satisfy your clients more when selecting future cars for your car fleet. We will be performing random forest and Extreme random forest classification on the car dealership dataset that you worked on in previous activity of this chapter. Suggest further improvements for the model to improve the performance of the classifier:</p>
<ol>
<li class="ParaOverride-1" value="1">Follow steps 1 to 5 of previous activity.</li>
<li value="2">If you are using <strong class="inline _idGenCharOverride-2">IPython</strong>
 , your variables may already be accessible in your console.</li>
<li class="_idGenParaOverride-1" value="3">Create a random forest and an extremely randomized trees classifier and train the models.<div></div>
</li>
<li value="4">Estimate how well the two models perform on the test data. We can also calculate the accuracy scores.</li>
<li value="5">As a first optimization technique, let's see which features more important and which features are less important. Due to randomization, removing the least important features may reduce the random noise in the model.</li>
<li value="6">Remove the third feature from the model and retrain the classifier. Compare how well the new models fare compared to the original ones.</li>
<li value="7">Tweak the parameterization of the classifiers a bit more.</li>
</ol>
<p>Note that we reduced the amount of nondeterminism by allowing the maximum number of features to go up to this could eventually lead to some degree of overfitting.</p>
<h4>Note</h4>
<p class="callout">The solution to this activity is available at page 285.</p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor147"></a>
 Summary</h2>
<p>In this chapter, we have learned how to use decision trees for prediction. Using ensemble learning techniques, we created complex reinforcement learning models to predict the class of an arbitrary data point.</p>
<p>Decision trees on their own proved to be very accurate on the surface, but they were prone to overfitting the model. Random Forests and Extremely Randomized Trees combat overfitting by introducing some random elements and a voting algorithm, where the majority wins.</p>
<p>Beyond decision trees, random forests, and Extremely Randomized Trees, we also learned about new methods for evaluating the utility of a model. After using the well-known accuracy score, we started using the precision, recall, and F1 score metrics to evaluate how well our classifier works. All of these values were derived from the confusion matrix.</p>
<p>In the next chapter, we will describe the clustering problem and compare and contrast two clustering algorithms.</p>
</div>
</body></html>