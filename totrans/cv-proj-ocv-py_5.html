<html><head></head><body><div><h1 class="header-title">Handwritten Digit Recognition with scikit-learn and TensorFlow</h1>
                
            
            
                
<p>In this chapter, we are going to learn how machine learning can be applied to computer vision projects, using a couple of different Python modules. We will also create and train a support vector machine that will actually perform our digit classification.</p>
<p>In this chapter, we will be covering the following topics:</p>
<ul>
<li>Acquiring and processing MNIST digit data</li>
<li>Creating and training a support vector machine</li>
<li>Applying the support vector machine to new data</li>
<li>Introducing TensorFlow with digit classification</li>
<li>Evaluating the results</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Acquiring and processing MNIST digit data</h1>
                
            
            
                
<p>As mentioned, we will be covering handwritten digit recognition with scikit-learn and TensorFlow. Here, we're going to learn how machine learning can be applied to computer vision projects, and we're going to learn a couple of different ways and models, using a couple of different Python modules. Let's get started.</p>
<p>You have probably heard about machine learning. Here, we will be particularly talking about supervised machine learning, where we have a bunch of examples that we want to accomplish. So, rather than explicitly telling the computer what we want, we give an example.</p>
<p>Let's take the case of the handwritten digits 0 through 9, which have labels that are created by humans indicating what those digits are supposed to be. So, rather than hand-coding features and explicitly telling the computer what the algorithm is, we are going to construct a model where we take those inputs, optimize some functions like a set of variables, and then train the computer to put the outputs to be what we want them to be.</p>
<p>So, we will go through handwritten digits, starting with 0, 1, 2, 3, and so on. That's the general paradigm of machine learning, and we're going to cover three different algorithms here.</p>
<p>So, let's start running some code.</p>
<p>Open up your Jupyter Notebook and, as we did in the previous chapter, let's start fresh in this chapter. As you can observe in the following code, we will be importing our essential modules, such as <kbd>numpy</kbd>, which is the foundation of numerical computing in Python:</p>
<pre>#import necessary modules here<br/>#--the final notebook will have complete codes that can be<br/>#--copied out into self-contained .py scripts<br/><br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import cv2 <br/>import sys<br/>import tempfile<br/><br/>from sklearn import svm, metrics<br/>import tensorflow as tf<br/><br/>from tensorflow.examples.tutorials.mnist import input_data</pre>
<p>As you can see in the preceding code, we are importing <kbd>pyplot</kbd>, so that we can visualize what we are doing. We will also use a little bit of OpenCV for converting some images. We will also be using scikit-learn, which is abbreviated as <kbd>sklearn</kbd> in the actual module, while importing a support vector machine, as well as some tools that will give us our metrics. This will tell us how well things have actually worked. We will also be importing TensorFlow, with the abbreviation as <kbd>tf</kbd>, as we will be obtaining our data from it.</p>
<p>One main advantage of scikit-learn and TensorFlow is that they have built-in functionality for getting digit recognition, which is such a common thing in computer vision and machine learning packages. So, you don't need to go to websites and download it, and then write the lines yourself. It will be taken care of for you. Hence, scikit-learn actually has a good number of inbuilt datasets, some for computer vision, some for other tasks. It has a digit example, and we can then choose which datasets are available from the inbuilt datasets by writing <kbd>datasets</kbd> and then pressing <em>Tab</em>, as shown in the following screenshot:</p>
<div><img src="img/e26c33b4-1d2f-4905-827a-a886bd59b62c.png" style="width:20.75em;height:17.17em;" width="286" height="237"/></div>
<p>Now, we have a list of inbuilt datasets. For example, you want to know <kbd>california_housing</kbd> prices; that is, you want estimated housing prices based on things like square footage and the number of bedrooms in the house—there's a dataset for that. Some of this is image data, some is not. So, this might be something you want to check out if you want to experiment with different machine learning techniques, but for the <kbd>dataset.load_digits()</kbd> one, we have the following code that shows what it does:</p>
<pre>#what kind of data do we already have?<br/>from sklearn import datasets<br/>digits=datasets.load_digits()<br/><br/><br/>example_image=digits.images[0]<br/>print(type(example_image))<br/>plt.imshow(example_image); plt.show()<br/>example_image.reshape((8*8,1))</pre>
<p class="mce-root"/>
<p>Let's break it down and understand the code. Firstly, we load an example image, just the first image in the set, as follows:</p>
<pre>example_image=digits.images[0]<br/>print(type(example_image))</pre>
<p>The data is actually stored in images and it's an array of examples where each one is an 8 x 8 handwritten digit image.</p>
<p>Next, we plot the image as follows:</p>
<pre>plt.imshow(example_image); plt.show()<br/>example_image.reshape((8*8,1))</pre>
<p>We should see the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c6d7d2c1-ee8a-456f-adcc-ef9f4da83ae8.png" style="width:20.17em;height:23.33em;" width="245" height="283"/></p>
<p>But I like to work with a slightly higher resolution example that we're going to see from MNIST later. The lower resolution images are a little computationally faster to use because they're smaller images. If we want to preprocess these images, these are stored as 8 x 8, and we need to convert each of them to a 1D array. We can do that easily using the <kbd>reshape</kbd> function, which we have used in our previous code:</p>
<pre>example_image.reshape((8*8,1))</pre>
<p>This will provide us with an output where, instead of an 8 x 8 array, we get a 1 x 64 array, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/1f1f2016-6359-475b-8c87-cc232bc53330.png" style="width:10.75em;height:29.00em;" width="172" height="465"/></p>
<p>Now, we are going to use the MNIST data that is available from the following website:</p>
<p><a href="http://yann.lecun.com/exdb/mnist/" target="_blank">http://yann.lecun.com/exdb/mnist/</a></p>
<p>It is a fairly standard dataset. TensorFlow is nice enough to provide some functionality for getting that data, so you don't have to go to the website and manually download it. We need to define <kbd>data_dir</kbd> and specify a location to save the data to. So, just create this <kbd>/tmp/tensorflow/mnist/input_data</kbd> directory and this will be fine, regardless of the operating system you're running, and then we have some <kbd>input_data</kbd> that we imported from <kbd>tensorflow</kbd> and <kbd>read_data_sets</kbd>. Now, let's run the following code:</p>
<pre>#acquire standard MNIST handwritten digit data<br/>#http://yann.lecun.com/exdb/mnist/<br/><br/>data_dir = '/tmp/tensorflow/mnist/input_data'<br/>mnist = input_data.read_data_sets(data_dir, one_hot=True)</pre>
<p>We should get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/3c828acb-7b21-4136-b8fe-34bce96b048e.png" style="width:34.33em;height:14.58em;" width="564" height="240"/></p>
<p>If you don't have the files, the code will download the gzip files and, if you do already have them, it just reads the existing gzip files and stores them in the <kbd>mnist</kbd> variable. <kbd>one_hot=True</kbd> ensures you get the labels, in terms of vectors, which means instead of being labeled with an American numerals like zero, one, two, three, four, and so on, it's going to be an array of mostly zeros. It's going to be an array of length 10, where everything is 0 except for one thing, which will be 1. So, if we have, for example, 0, 1, 0, 0, 0, 0, and so on, that would represent a 1 and, if it was a 9, it would be all zeros until the last one, which would be a 1. So, it's one useful way for machine learning to label an output. This is the way we got the data and we're going to be using it; it's more helpful for when we actually use TensorFlow, but for scikit-learn it actually does need the numerics.</p>
<p>Let's understand the data before we dive in and do some actual machine learning. We have the <kbd>mnist</kbd> variable, and it's already separated into training and testing data. With machine learning, you don't want to train on all of your data; you don't want to build your model with all of your data because then you won't know how well it's going to handle new data examples that it hasn't seen before. What you want to do is split it into training data and testing data. So, the training data is going to build a model, and the test data is going to validate it. So, the splitting of the data is already done for us, just with the following variables:</p>
<pre>#now we load and examine the data<br/>train_data=mnist.train.images<br/>print(train_data.shape)<br/>n_samples = train_data.shape[0]<br/><br/><br/>train_labels=np.array(np.where(mnist.train.labels==1))[1]<br/><br/>plt.imshow(train_data[1000].reshape((28,28))); plt.show()</pre>
<p>Let's break down the code for better understanding.</p>
<p>Firstly, we load <kbd>train_data</kbd> from <kbd>train.images</kbd>, as follows:</p>
<pre class="mce-root">#now we load and examine the data<br/>train_data=mnist.train.images</pre>
<p>We're going to see what the shape is to understand it using <kbd>.shape</kbd>, as follows:</p>
<pre>print(train_data.shape)</pre>
<p>If you need to know the number of samples, we can extract that from the <kbd>shape</kbd> output, as follows:</p>
<pre>n_samples = train_data.shape[0]</pre>
<p>Again, it is a NumPy array, so all NumPy functions and features are there.</p>
<p>Then, execute the following code for <kbd>train_labels</kbd>:</p>
<pre>train_labels=np.array(np.where(mnist.train.labels==1))[1]<br/>plt.imshow(train_data[1000].reshape((28,28))); plt.show()</pre>
<p>Here, we just see where the <kbd>train.label</kbd> equals <kbd>1</kbd> and we extract that to create an array of those values, which will give us our <kbd>train_labels</kbd>. So, a 1D array corresponds to the number of examples where it contains the actual output of each one. We'll see just an example; let's take <kbd>1000</kbd> out of <kbd>55000</kbd> training examples. </p>
<p>Running this code gives us the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/3e3c2879-f179-4de7-94b3-99c325736b94.png" style="width:16.33em;height:18.00em;" width="256" height="282"/></p>
<p><kbd>784</kbd> is the number of pixels in the image and that's because they are <kbd>28</kbd> by <kbd>28</kbd> squares, <em>28 x 28 = 784</em>. So, we have, <kbd>55000</kbd> examples <kbd>784</kbd> pixels, or we call them features, and then the <kbd>train_labels</kbd> is going to be of length <kbd>55000</kbd> and the other dimension is just <kbd>1</kbd>. Here's an example. This data already comes in a 1D array, so that was why we used the <kbd>reshape</kbd> function and passed the <kbd>28</kbd> by <kbd>28</kbd> value, in order to convert it to an actual image that we can see.</p>
<p>Great, our data is loaded and processed and is ready to be used, so we can begin actual machine learning. Now that our data is set up and ready to go, we can move on to our next section, in which we will create and train our support vector machine and perform our digit classification.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Creating and training a support vector machine</h1>
                
            
            
                
<p>In this section, we're going to create and train a support vector machine that will actually perform our digit classification.</p>
<p>In the very first example, we're going to use scikit-learn, and we're going to use what's called a support vector machine, which is a very powerful, very versatile classic machine learning technique that can learn all kinds of functions and all kinds of mappings from inputs to outputs. We're going to do classification, which is mapping inputs as an array of pixels, and in our case we're going to classify each input into one of ten classes, corresponding to ten digits. But we can classify different kinds of things as continuous ordered functions, which is called regression, and that can be useful, for example, if you want to extract position or an area of volume where it doesn't just fit into a neat category.</p>
<p>For this section, we're going to be doing primarily classification. So, scikit-learn makes it very easy to create such a model. A support vector classifier can be called with <kbd>svm.SVC</kbd>, in which support vector machine came from <kbd>sklearn</kbd> package and we have this meta parameter for the model called <kbd>gamma</kbd>, just kind of an inverse radius, the area of influence of the sport vectors, as shown in the following code:</p>
<pre># Create a classifier: a support vector classifier<br/>classifier = svm.SVC(gamma=0.001)<br/># Learn about gamma and other SVM parameters here:<br/># http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html<br/># Exercise: Experiment with the parameters to see how they affect execution <br/># time and accuracy<br/><br/># Train the model -- we're only going to use the training data (and not<br/># the test data) to ensure that our model generalizes to unseen cases.<br/># This (training) is typically what takes the most computational time<br/># when doing machine learning.<br/>classifier.fit(train_data, train_labels)</pre>
<p>How the support vector machine works is not covered here, as there is plenty of literature available on that subject, and it's not absolutely necessary to understand it fully in order to learn it. Now, we're just going to see how we can apply this for some cases.</p>
<p>The <kbd>gamma</kbd> parameter is something I recommend you experiment with as an exercise. We're going to start with a known <kbd>gamma</kbd> parameter that will work well for our case, namely <kbd>.001</kbd>, but you should learn about the other parameters that are available. I recommend going to <a href="http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html" target="_blank">http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html</a> and again I recommend playing with this to see how it affects execution time and accuracy. But, what's important to take away here is that we can create our model with just one line. It defines the model but we haven't actually trained it. We haven't actually given any data and made it fit its parameters such that it will actually produce a desirable output. Now, if we feed it an image of five, it will say, that's OK, this is a 5. So, in order to do that, we have to fit it.</p>
<p>In the preceding code, we have created our classifier and it's very simple: <kbd>classifier. fit</kbd>. We give it the <kbd>train_data</kbd> and the <kbd>train_labels</kbd> that we got from our previous code execution. Just a heads up, this execution is going to take a few minutes; it generally does. Usually, the training process is the slowest part of machine learning. That's typically the case but this shouldn't be too bad. This only takes a couple of minutes and, again, we're just using your training data so that we can verify that this will generalize to unseen cases.</p>
<p>Now that we've seen our support vector machine and it's actually been trained, we can move on to our next section, where we apply the support vector machine to new data that it was not trained upon.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Applying the support vector machine to new data</h1>
                
            
            
                
<p>Now that we have our trained support vector machine, we can actually apply the support vector machine to new data that hasn't been seen and see that our digit classifier is actually working.</p>
<p>After the cell has executed successfully and if everything worked correctly, we should see the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b80dd4a3-bab1-4049-b30c-1781e67d40e6.png" style="width:37.67em;height:17.50em;" width="617" height="288"/></p>
<p>This is just the output from creating the support vector classifier. This just gives information about the metadata parameters that we used; we used what's known as a radial basis function kernel, and fitting the data did not produce any error messages. So, that means the code has worked. So, now we have our trained model, we want to see how well it's going to work on data that it hasn't seen.</p>
<p>Now, we're going to get our test data, as follows:</p>
<pre># Now predict the value of the digit on the test data:<br/>test_data=mnist.test.images<br/>test_labels=np.array(np.where(mnist.test.labels==1))[1]<br/><br/>expected = test_labels<br/>predicted = classifier.predict(test_data)</pre>
<p>We get our <kbd>mnist.test.images</kbd>, which is equal to <kbd>mnist.train.images</kbd>, and extract the labels the same way, by calling the <kbd>expected</kbd> variable, and then we're going to compute <kbd>predicted</kbd> from the <kbd>classifier</kbd> model, using <kbd>classifier.predict(test_data)</kbd>. So, this is going to take just a little bit of time to execute. After execution, there should be no error messages, which indicates that our prediction ran successfully.</p>
<p>So, now we can see how well we did. We're going to use the built-in metrics functions from scikit-learn. We're going to record some of the metrics, such as <em>precision</em> and <em>recall</em>, and if you want to understand what those mean, I recommend the following Wikipedia article:</p>
<p><a href="https://en.wikipedia.org/wiki/Precision_and_recall_to_understand_metric_definitions" target="_blank">https://en.wikipedia.org/wiki/Precision_and_recall_to_understand_metric_definitions</a></p>
<p><a href="https://en.wikipedia.org/wiki/Precision_and_recall_to_understand_metric_definitions" target="_blank"/></p>
<p>Just in short, they are different metrics for evaluating how well your machine learning algorithm did. Accuracy is probably the most common. It's simple: the correct data points divided by the total. But there's also precision recall that weighs the pros and cons with true positives, true negatives, false positives, and false negatives, and which one is the best depends on your application. It depends on which is worse between the false positive and the false negative and so forth and, on top of that, we're going to output what's known as a confusion matrix, which tells you which ones were successful and which ones were misclassified. Let's run the following code:</p>
<pre># And display the results<br/>print("See https://en.wikipedia.org/wiki/Precision_and_recall to understand metric definitions")<br/>print("Classification report for classifier %s:\n%s\n"<br/>      % (classifier, metrics.classification_report(expected, predicted)))<br/>print("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))<br/><br/>images_and_predictions = list(zip(test_data, predicted))<br/>for index, (image, prediction) in enumerate(images_and_predictions[:4]):<br/>    plt.subplot(2, 4, index + 5)<br/>    plt.axis('off')<br/>    plt.imshow(image.reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')<br/>    plt.title('Prediction: %i' % prediction)<br/><br/>plt.show()</pre>
<p>It should give us the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/634bd8ef-8432-49da-b72a-381338a357ba.png" style="width:35.75em;height:32.92em;" width="748" height="685"/></p>
<p>OK, so we get the classification reports and we can see <kbd>precision</kbd>, <kbd>recall</kbd>, and another metric called <kbd>f1-score</kbd> that you can read about in that same Wikipedia article. In short, zero is the worst case and one is the best case. In the preceding screenshot, we can see <kbd>precision</kbd>, <kbd>recall</kbd>, and <kbd>f1-score</kbd> for the different digits and we can see we're in the 90% range; it varies, which is OK. It depends on your application, but that might be good enough or that might be abysmally bad. It depends. We're actually going to see how we can do better a little later on using a more powerful model. We can see that it generally worked. We look at the confusion matrix here, where the columns tell you what the actual value is and the rows tell you what the predicted value is. Ideally, we would see all large values along the diagonal and all zeroes otherwise. There's always going to be some errors, we're human beings so that's going to happen but like I said, we're going to see if we can do a little bit better, in vast majority of cases it did work. Now, we can see some example random outputs, where we had some digits as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/1677df97-8bed-4199-9ac3-bb9ac2f3ae4b.png" style="width:21.75em;height:5.67em;" width="346" height="90"/></p>
<p>As we can see, all of the predictions are correct according to their images. OK, that's all well and good but I kind of feel like I'm taking the computer's word for it at this point. I'd like to throw my own data at it. I'd like to see how well this is really working, and this is something generally recommended with machine learning. You want to test it with your own data to really know if it's working and, if nothing else, it's much more satisfying. So, here is a little snippet of code that's going to use Jupyter's widget capabilities, its interactive capabilities:</p>
<pre>#Let's test our model on images we draw ourselves!<br/><br/>from matplotlib.lines import Line2D<br/>%pylab notebook <br/>#This is needed for plot widgets<br/><br/>class Annotator(object):<br/>    def __init__(self, axes):<br/>        self.axes = axes<br/><br/>        self.xdata = []<br/>        self.ydata = []<br/>        self.xy = []<br/>        self.drawon = False<br/><br/>    def mouse_move(self, event):<br/>        if not event.inaxes:<br/>            return<br/><br/>        x, y = event.xdata, event.ydata<br/>        if self.drawon:<br/>            self.xdata.append(x)<br/>            self.ydata.append(y)<br/>            self.xy.append((int(x),int(y)))<br/>            line = Line2D(self.xdata,self.ydata)<br/>            line.set_color('r')<br/>            self.axes.add_line(line)<br/><br/>            plt.draw()<br/><br/>    def mouse_release(self, event):<br/>        # Erase x and y data for new line<br/>        self.xdata = []<br/>        self.ydata = []<br/>        self.drawon = False<br/>        <br/>    def mouse_press(self, event):<br/>        self.drawon = True<br/><br/><br/>img = np.zeros((28,28,3),dtype='uint8')<br/><br/>fig, axes = plt.subplots(figsize=(3,3))<br/>axes.imshow(img)<br/>plt.axis("off")<br/>plt.gray()<br/>annotator = Annotator(axes)<br/>plt.connect('motion_notify_event', annotator.mouse_move)<br/>plt.connect('button_release_event', annotator.mouse_release)<br/>plt.connect('button_press_event', annotator.mouse_press)<br/><br/>axes.plot()<br/><br/>plt.show()</pre>
<p>So, now we're actually going to create a little drawing widget. It's going to let us produce our own digits. Let's look at the code.</p>
<p>Let's import <kbd>Line2D</kbd> from <kbd>matpllotlib.line</kbd>, this is going to let us draw individual lines, like creating a kind of a vector image based on our mouse movements:</p>
<pre>#Let's test our model on images we draw ourselves!<br/><br/>from matplotlib.lines import Line2D</pre>
<p>We execute <kbd>%pylab notebook</kbd>; the percent sign indicates the following magic command:</p>
<pre>%pylab notebook</pre>
<p>It's kind of a meta command within Jupyter and Pylab Notebook, and it loads a bunch of stuff into your namespace for plotting and numerics. It's not necessary because we already did that with NumPy and Matplotlib, but to enable the widgets, we use this command.</p>
<p>Then, create this <kbd>Annotator</kbd> class, which contains call back for what happens if we move a mouse over our displayed image, as follows:</p>
<pre>class Annotator(object):<br/>    def __init__(self, axes):<br/>        self.axes = axes<br/><br/>        self.xdata = []<br/>        self.ydata = []<br/>        self.xy = []<br/>        self.drawon = False<br/><br/>    def mouse_move(self, event):<br/>        if not event.inaxes:<br/>            return<br/><br/>        x, y = event.xdata, event.ydata<br/>        if self.drawon:<br/>            self.xdata.append(x)<br/>            self.ydata.append(y)<br/>            self.xy.append((int(x),int(y)))<br/>            line = Line2D(self.xdata,self.ydata)<br/>            line.set_color('r')<br/>            self.axes.add_line(line)<br/><br/>            plt.draw()<br/><br/>    def mouse_release(self, event):<br/>        # Erase x and y data for new line<br/>        self.xdata = []<br/>        self.ydata = []<br/>        self.drawon = False<br/>        <br/>    def mouse_press(self, event):<br/>        self.drawon = True</pre>
<p>We don't have to understand the <kbd>Annotator</kbd> class, but this might be something useful in the future if you want to make an annotation or draw something, and to seize a full snippet of code.</p>
<p>Then, we're going to create a blank image, the same size as our images. It's just going to be three RGBs for the time being. It just looks a little bit nicer, even though we're going to make it black and white in the end, because that's what our data is. Create the image as follows:</p>
<pre>img = np.zeros((28,28,3),dtype='uint8')</pre>
<p>Now, create a plot, show it, and hook up our <kbd>annotator</kbd> functions to that, as follows:</p>
<pre>fig, axes = plt.subplots(figsize=(3,3))<br/>axes.imshow(img)<br/>plt.axis("off")<br/>plt.gray()<br/>annotator = Annotator(axes)<br/>plt.connect('motion_notify_event', annotator.mouse_move)<br/>plt.connect('button_release_event', annotator.mouse_release)<br/>plt.connect('button_press_event', annotator.mouse_press)<br/><br/>axes.plot()<br/><br/>plt.show()</pre>
<p>After running the code, we should get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/7e873b37-046c-4f77-aaaf-caad496ca0d9.png" style="width:24.58em;height:19.67em;" width="501" height="400"/></p>
<p>So, let's draw, say, the numeral three:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/3e1d18ae-a7b8-44d6-a4d5-27d8be21a696.png" style="width:25.58em;height:20.42em;" width="507" height="402"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, that's sluggish you know, and isn't exactly going to replace Photoshop, but this still beats going into a separate program and creating your image file, making sure it's in the right format, saving it, and then writing code to load it and get it right. So, this will allow us to quickly play and experiment with our models. We just created a kind of array of lines, so we need to rasterize that and process it so that it looks more like actual handwritten digits, something that would come from either a scanned pencil drawing or a pressure-sensitive tablet. The following will do this:</p>
<pre># Now we see how our model "sees" (predicts the digit from)<br/># our hand drawn image...<br/># First, we rasterize (convert to pixels) our vector data<br/># and process the image to more closely resemble something<br/># drawn with a pencil or pressure-sensitive tablet.<br/><br/>digimg = np.zeros((28,28,3),dtype='uint8')<br/>for ind, points in enumerate(annotator.xy[:-1]):<br/>    digimg=cv2.line(digimg, annotator.xy[ind], annotator.xy[ind+1],(255,0,0),1)<br/>digimg = cv2.GaussianBlur(digimg,(5,5),1.0)<br/>digimg = (digimg.astype('float') *1.0/np.amax(digimg)).astype('float')[:,:,0]<br/>digimg **= 0.5; digimg[digimg&gt;0.9]=1.0<br/><br/>#The model is expecting the input in a particular format<br/>testim = digimg.reshape((-1,28*28))<br/><br/>print("Support vector machine prediction:",classifier.predict( testim ))<br/><br/>outimg = testim.reshape((28,28))<br/>figure(figsize=(3,3)); imshow(outimg);</pre>
<p>Let's look at the code. First, we create a blank image, as follows:</p>
<pre>digimg = np.zeros((28,28,3),dtype='uint8')</pre>
<p>We iterate over the <kbd>xy</kbd> pairs that came from our <kbd>annotator</kbd> and then we're going to draw lines there in raster format on our rasterized images, as follows:</p>
<pre>for ind, points in enumerate(annotator.xy[:-1]):<br/>    digimg=cv2.line(digimg, annotator.xy[ind], annotator.xy[ind+1],(255,0,0),1)<br/>digimg = cv2.GaussianBlur(digimg,(5,5),1.0)</pre>
<p>Then, we convert the image to a <kbd>float</kbd>, from range <kbd>0</kbd> to <kbd>1</kbd>, just like our input data, as follows:</p>
<pre>digimg = (digimg.astype('float') *1.0/np.amax(digimg)).astype('float')[:,:,0]</pre>
<p>Then, we are going to bring it a little bit closer to <kbd>1</kbd>, because that's just what our input images look like and what our model would be expecting:</p>
<pre>digimg **= 0.5; digimg[digimg&gt;0.9]=1.0</pre>
<p>Then, we have our two-dimensional image but, of course, to run it through our model, we need to flatten it into <kbd>1</kbd> x <kbd>784</kbd>, so that's what this <kbd>reshape</kbd> function does:</p>
<pre>#The model is expecting the input in a particular format<br/>testim = digimg.reshape((-1,28*28))</pre>
<p>Then, we're going to run that through our <kbd>classifier</kbd> and print the output as well. We will create a figure where we can see what our rasterized image looks like, as follows:</p>
<pre>print("Support vector machine prediction:",classifier.predict( testim ))<br/><br/>outimg = testim.reshape((28,28))<br/>figure(figsize=(3,3)); imshow(outimg);</pre>
<p>We should get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/64e0f2af-31ab-4e3d-8457-e65f326b3aaa.png" style="width:20.50em;height:20.33em;" width="406" height="402"/><br/></p>
<p>We drew a three and we predicted a <kbd>3</kbd>. Excellent. Let's try something else. Clear the previous output by hitting <em>Ctrl</em> + <em>Enter</em>, and we get a warning message; it's just telling us that it has clobbered some of the variables that were created. That's not a big deal. You can safely ignore that. Just a fair warning, your mileage may vary on this, depending on what your handwriting is like and what was in your training data. If you wanted this to work perfectly every time, or as close to that as possible, you want to train it probably on your own handwriting.</p>
<p>Let's try a zero:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/54ef84b4-a1ba-4453-b443-57ad38f50a2d.png" style="width:18.42em;height:20.17em;" width="343" height="373"/></p>
<p>The following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/980d11b9-8013-4631-8cba-0c53e53a8f3a.png" style="width:20.58em;height:19.33em;" width="432" height="405"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>So, here you can see an example of it not working. The prediction is supposed to be zero but the model predicted a three for some reason. It's possible that if you redraw it, it might work. So, again, your mileage may vary. Experiment with it. You can also play with the preprocessing, although this, as far as I can tell, works pretty well. But anyway, we can see that our model is, at least for the most part, working. So, that's going to be it for the scikit-learn support vector machines. Now, in our next section, we're going to introduce TensorFlow and perform digit classification with that.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Introducing TensorFlow with digit classification</h1>
                
            
            
                
<p class="mce-root">We're going to see TensorFlow in action and see how we can perform digit classification with a tractable amount of code. TensorFlow is Google's machine learning library, for numerical analysis in general. It is called TensorFlow because it supposedly flows tensors, where tensors are defined to be arrays of <em>n</em> dimensions. Tensors have a real geometric meaning that just multidimensional arrays don't necessarily classify, but we're just going to use that term. A tensor is just a multidimensional array.</p>
<p class="mce-root">Here, we're going to do a simple <kbd>softmax</kbd> example. It's a very simple model; you can visit TensorFlow's own website (<a href="https://www.tensorflow.org/get_started/mnist/beginners" target="_blank">https://www.tensorflow.org/get_started/mnist/beginners</a>) for more information. Let's have a look at the following code:</p>
<pre>data_dir = '/tmp/tensorflow/mnist/input_data'<br/>mnist = input_data.read_data_sets(data_dir, one_hot=True)<br/>    <br/># Create the model<br/>x = tf.placeholder(tf.float32, [None, 784])<br/>W = tf.Variable(tf.zeros([784, 10]))<br/>b = tf.Variable(tf.zeros([10]))<br/>y = tf.matmul(x, W) + b<br/>    <br/># Define loss and optimizer<br/>y_ = tf.placeholder(tf.float32, [None, 10])<br/>    <br/>    <br/>cross_entropy = tf.reduce_mean(<br/>  tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))<br/>train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)<br/>    <br/>sess = tf.InteractiveSession()<br/>tf.global_variables_initializer().run()<br/># Train<br/>for _ in range(1000):<br/>    batch_xs, batch_ys = mnist.train.next_batch(100)<br/>    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})<br/>    <br/># Test trained model<br/>correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))<br/>accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))<br/>print(\"Model accuracy:\",sess.run(accuracy, feed_dict={x: mnist.test.images,<br/>                                    y_: mnist.test.labels}))</pre>
<p class="mce-root">In short, you're going to take your input data and you're going to multiply it by a matrix. The data has <kbd>784</kbd> points. Each point is going to have a matrix value and, for each of the <kbd>10</kbd> classes, you're going to compute an inner product by multiplying 784 × 784 and sum them up. There will be <kbd>10</kbd> outputs. It will be a 1 by 10 array, and you're going to add a bias variable to the output of the array and run it through the <kbd>softmax</kbd> function, which will convert it to something. The output of the matrix plus the bias will compute something in the range of <kbd>0</kbd> to <kbd>1</kbd>, which loosely corresponds to the probability of that data being in that class. For example, there might be a 0.4% probability or 40% probability of being a <kbd>1</kbd>, a 2% probability of being it <kbd>2</kbd>, and 90% probability of it being a <kbd>9</kbd>, and the output is going to be the maximum output of that.</p>
<p class="mce-root">TensorFlow is very sophisticated. There's a little bit more setup here than there was with the scikit-learn example. You can learn more about that on their own website. Now, let's go through the following code in detail:</p>
<pre>data_dir = '/tmp/tensorflow/mnist/input_data'<br/>mnist = input_data.read_data_sets(data_dir, one_hot=True)</pre>
<p class="mce-root">We've already done this in the previous example. Now, we're going to get the data from <kbd>data_dir</kbd>; make sure it's in our <kbd>mnist</kbd> variable.</p>
<p class="mce-root">Then, we create the model where <kbd>x</kbd> corresponds to our input data and, although we're not loading the data just yet, we just need to create a placeholder so TensorFlow knows where stuff is. We don't need to know how many examples there are, and that's what the <kbd>None</kbd> dimension corresponds to, but we do need to know how big each example is, which in this case is <kbd>784</kbd>. <kbd>W</kbd> is the matrix that's going to multiply <kbd>x</kbd> classes to the inner product over the image, <kbd>784</kbd> <em>dot</em> <kbd>784</kbd>, and you do that <kbd>10</kbd> times. So, that corresponds to a 784/10 matrix, <kbd>10</kbd> being the number of classes; then, you add the <kbd>b</kbd> bias variable to that. The values of <kbd>W</kbd> and <kbd>b</kbd> are what TensorFlow is going to produce for us based on our inputs, and <kbd>y</kbd> defines the actual operation that's going to be performed on our data from the matrix multiplication. We add the <kbd>b</kbd> bias variable to it as follows:</p>
<pre>x = tf.placeholder(tf.float32, [None, 784])<br/>W = tf.Variable(tf.zeros([784, 10]))<br/>b = tf.Variable(tf.zeros([10]))<br/>y = tf.matmul(x, W) + b</pre>
<p class="mce-root">We need to create a placeholder for our labeled data, as follows:</p>
<pre>y_ = tf.placeholder(tf.float32, [None, 10])</pre>
<p class="mce-root">In order to do machine learning, you need a <kbd>loss</kbd> function or a <kbd>fitness</kbd> function, which tells you how well your model is doing given learning parameters like those given in <kbd>W</kbd> and <kbd>b</kbd>. Hence, we're going to use something called <kbd>cross-entropy</kbd>; we'll not go into much detail about <kbd>cross-entropy</kbd> but that's going to give us some criteria for letting us know that we're getting closer to a working model, as shown in the following lines of code:</p>
<pre>cross_entropy = tf.reduce_mean(<br/>  tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))<br/>train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)</pre>
<p class="mce-root">As we add more and more data, we're going to use what's known as <kbd>GradientDescentOptimizer</kbd> in order to minimize the error, minimize the cross entropy, and make our model fit as well as possible.</p>
<p class="mce-root">In the following code, we're actually going to start by creating an interactive session, as follows:</p>
<pre>sess = tf.InteractiveSession()<br/>tf.global_variables_initializer().run()<br/>for _ in range(1000):<br/>    batch_xs, batch_ys = mnist.train.next_batch(100)<br/>    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})</pre>
<p class="mce-root">We want to make it an interactive session so that we can use our model and buy new data to it afterwards. We're going to initialize <kbd>run()</kbd> and then we're going to compute the data in batches. TensorFlow is a very powerful program and it allows you to break up your data. We're not going to do it here, but you can run parallelized code fairly easily with it. Here, we're just going to iterate <kbd>1000</kbd> times and compete stuff in batches feeding in our training data.</p>
<p class="mce-root">After this runs, we're going to see how well we did and just see where our predicted data is equal to the given labels to it. We can compute the <kbd>accuracy</kbd> by just seeing how many of the predictions were correct on average. Then, print that data, as follows:</p>
<pre>correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))<br/>accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))<br/>print(\"Model accuracy:\",sess.run(accuracy, feed_dict={x: mnist.test.images,<br/>                                    y_: mnist.test.labels}))</pre>
<p class="mce-root">The following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/4d8cf5fb-5bb4-4041-8402-1629d6cee9ee.png" width="966" height="246"/></p>
<p class="mce-root">Being a really simple model, it runs a lot faster and we can see that we got a little less than 92% accuracy. The code executed a lot faster, but a little bit less accurately than our <strong>Support Vector Machine</strong> (<strong>SVM</strong>), but that's OK. This code just provides a very simple example of how TensorFlow works.</p>
<p class="mce-root">You get a little bit more advanced momentarily, but let's test the following code the way we did before:</p>
<pre>img = np.zeros((28,28,3),dtype='uint8')<br/>fig, axes = plt.subplots(figsize=(3,3))<br/>axes.imshow(img)<br/>plt.axis("off")<br/>plt.gray()<br/>annotator = Annotator(axes)<br/>plt.connect('motion_notify_event', annotator.mouse_move)<br/>plt.connect('button_release_event', annotator.mouse_release)<br/>plt.connect('button_press_event', annotator.mouse_press)<br/>axes.plot()<br/>plt.show()</pre>
<p class="mce-root">We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/1af2c010-209e-41b9-913d-923e77aec493.png" style="width:15.33em;height:18.75em;" width="305" height="373"/></p>
<p class="mce-root">Our annotator is initiated and let's put in a digit. Try a <kbd>3</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/15a25209-0eed-4937-9a5a-7256ffe39d97.png" style="width:15.00em;height:18.50em;" width="304" height="374"/></p>
<p class="mce-root">Now, we're going to preprocess the drawn digit and it's almost the same code as before, which is going to go through the classes and possible classes for our data and see which one the TensorFlow <kbd>softmax</kbd> model thought was the best:</p>
<pre>for tindex in range(10):<br/>    testlab = np.zeros((1,10))<br/>    testlab[0,tindex] = 1<br/>    if sess.run(accuracy, feed_dict={x: testim, y_ : testlab}) == 1:<br/>        break</pre>
<p class="mce-root">So, we'll run the preceding block and, as shown, it predicts a <kbd>3</kbd> from a <kbd>3</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/a71f2e2c-549b-42f8-9450-31496778a708.png" style="width:17.83em;height:20.42em;" width="353" height="402"/></p>
<p class="mce-root">Sometimes it might not predict correctly, and that is unfortunately going to happen. So, there are two ways to improve that: train on your own handwriting or use a better model.</p>
<p class="mce-root">We are going to move on to the most powerful model in this section. Here, we're going to just briefly touch on deep learning with, <strong>convolutional neural networks</strong> (<strong>CNNs</strong>). We are not covering the theory here. There's a lot to know about deep learning and multi-layered neural networks in general. Deep learning is a deep subject but, for this chapter, we're just going to see how we can actually apply state-of-the-art machine learning techniques to digit recognition with a relatively simple block of code.</p>
<p class="mce-root">So, we have a <kbd>deepnn(x)</kbd> function here that creates our deep neural network, finds our hidden layers or convolutional layers, pools, and so forth, and defines all our necessary stuff from our input:</p>
<pre>def deepnn(x):<br/>     with tf.name_scope('reshape'):<br/>     x_image = tf.reshape(x, [-1, 28, 28, 1])</pre>
<p><kbd>deepnn</kbd> builds the graph for a deep net for classifying digits and <kbd>reshape</kbd> function is to be used within a convolutional neural net. The arguments used here are:</p>
<ul>
<li><kbd>x</kbd>: An input tensor with the dimensions (<kbd>N_examples</kbd>, <kbd>784</kbd>), where <kbd>784</kbd> is the number of pixels in a standard MNIST image.</li>
<li><kbd>y</kbd>: A tensor of shape (<kbd>N_examples</kbd>, <kbd>10</kbd>), with values equal to the logic's of classifying the digit into one of 10 classes (the digits 0-9). <kbd>keep_prob</kbd> is a scalar placeholder for the probability of dropout.</li>
</ul>
<p class="mce-root"/>
<p>This returns a tuple (<kbd>y</kbd>, <kbd>keep_prob</kbd>). The last dimension is for <em>features</em>—there is only one here, since images are grayscale—it would be 3 for an RGB image, 4 for RGBA, and so on.</p>
<p>The first convolutional layer maps one grayscale image to <kbd>32</kbd> feature maps:</p>
<pre>     with tf.name_scope('conv1'):<br/>     W_conv1 = weight_variable([5, 5, 1, 32])<br/>     b_conv1 = bias_variable([32])<br/>     h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)<br/>    <br/>     # Pooling layer - downsamples by 2X.<br/>     with tf.name_scope('pool1'):<br/>     h_pool1 = max_pool_2x2(h_conv1)<br/>    <br/>     # Second convolutional layer -- maps 32 feature maps to 64.<br/>     with tf.name_scope('conv2'):<br/>     W_conv2 = weight_variable([5, 5, 32, 64])<br/>     b_conv2 = bias_variable([64])<br/>     h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)<br/>    <br/>     # Second pooling layer.<br/>     with tf.name_scope('pool2'):<br/>     h_pool2 = max_pool_2x2(h_conv2)<br/>    <br/>     # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image<br/>     # is down to 7x7x64 feature maps -- maps this to 1024 features.<br/>     with tf.name_scope('fc1'):<br/>     W_fc1 = weight_variable([7 * 7 * 64, 1024])<br/>     b_fc1 = bias_variable([1024])<br/>    <br/>     h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])<br/>     h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)<br/>    <br/>     # Dropout - controls the complexity of the model, prevents co-adaptation of<br/>     # features.<br/>     with tf.name_scope('dropout'):<br/>     keep_prob = tf.placeholder(tf.float32)<br/>     h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)<br/>    <br/>     # Map the 1024 features to 10 classes, one for each digit<br/>     with tf.name_scope('fc2'):<br/>     W_fc2 = weight_variable([1024, 10])<br/>     b_fc2 = bias_variable([10])<br/>    <br/>     y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2<br/>     return y_conv, keep_prob<br/>    <br/>    <br/>    def conv2d(x, W):<br/> """conv2d returns a 2d convolution layer with full stride."""<br/>     return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')<br/>    <br/>    <br/>    def max_pool_2x2(x):<br/>     """max_pool_2x2 downsamples a feature map by 2X."""<br/>     return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],<br/>     strides=[1, 2, 2, 1], padding='SAME')<br/>    <br/>    <br/>    def weight_variable(shape):<br/>    """weight_variable generates a weight variable of a given shape."""<br/>     initial = tf.truncated_normal(shape, stddev=0.1)<br/>     return tf.Variable(initial)<br/>    <br/>    <br/>    def bias_variable(shape):<br/>    """bias_variable generates a bias variable of a given shape."""<br/>     initial = tf.constant(0.1, shape=shape)<br/>     return tf.Variable(initial)<br/>    <br/> </pre>
<p class="mce-root">We have our functions that do convolutions, weight variables, bias variables, and so forth. Then, we have our main code here:</p>
<pre>  ###begin main code<br/>    <br/>    data_dir= '/tmp/tensorflow/mnist/input_data'<br/>    # Import data<br/>    mnist = input_data.read_data_sets(data_dir, one_hot=True)<br/>    <br/>    # Create the model<br/>    x = tf.placeholder(tf.float32, [None, 784])<br/>    <br/>    # Define loss and optimizer<br/>    y_ = tf.placeholder(tf.float32, [None, 10])<br/>    <br/>    # Build the graph for the deep net<br/>    y_conv, keep_prob = deepnn(x)<br/>    <br/>    with tf.name_scope('loss'):<br/>     cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,<br/>     logits=y_conv)<br/>    cross_entropy = tf.reduce_mean(cross_entropy)<br/>    <br/>    with tf.name_scope('adam_optimizer'):<br/>     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)<br/>    <br/>    with tf.name_scope('accuracy'):<br/>     correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))<br/>     correct_prediction = tf.cast(correct_prediction, tf.float32)<br/>     accuracy = tf.reduce_mean(correct_prediction)<br/>    <br/>    graph_location = tempfile.mkdtemp()<br/>    print('Saving graph to: %s' % graph_location)<br/>    train_writer = tf.summary.FileWriter(graph_location)<br/>    train_writer.add_graph(tf.get_default_graph())<br/>    <br/>    # Let's run the model<br/>    sess = tf.InteractiveSession()<br/>    sess.run(tf.global_variables_initializer())<br/>    for i in range(20000):<br/>     batch = mnist.train.next_batch(50)<br/>     if i % 100 == 0:<br/>     train_accuracy = accuracy.eval(feed_dict={<br/>     x: batch[0], y_: batch[1], keep_prob: 1.0})<br/>     print('step %d, training accuracy %g' % (i, train_accuracy))<br/>     train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})<br/>    <br/>    # How did we do?<br/>    print('test accuracy %g' % accuracy.eval(feed_dict={<br/>     x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))</pre>
<p class="mce-root">The <kbd>mnist</kbd> variable gets the data in case we don't already have it. We define our placeholder for the input, and the outputs build the graph. We then define the <kbd>fitness</kbd> function and <kbd>cross-entropy</kbd> and create our graph. We have to be careful while creating the session; in the example on their website, they just created a normal session. We want an interactive session here, so that we can apply our model to our own generated data, and we're going to break this up into batches. We'll run it, and every <kbd>100</kbd> iterations it's going to tell us what exactly it's doing and then, at the end, it will tell us our accuracy.</p>
<p class="mce-root">Let's run the code and extract the data, and you can see the statistics as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/820ee86a-10a9-4c9d-bac7-d4e93158c64e.png" style="width:34.92em;height:15.92em;" width="565" height="258"/></p>
<p class="mce-root">It starts off with very bad training accuracy but it quickly gets up to over 90% and shoots up to <kbd>1</kbd>. It's not exactly 100%, but that usually means it's something like 99%, so pretty close to <kbd>1</kbd>. This usually takes a few minutes. OK, now we have created our TensorFlow classifier. </p>


            

            
        
    </div>



  
<div><h1 class="header-title">Evaluating the results</h1>
                
            
            
                
<p>After we finished training, as we can see from the following screenshot, we get a result of over 99%, so that is significantly better than what we got with <kbd>softmax</kbd> or our SVM:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/4eabb374-6994-4a4b-a598-fa2b2b9b38bd.png" style="width:17.00em;height:18.25em;" width="254" height="272"/></p>
<p>Deep learning is probably the most powerful machine learning technique, due to its ability to learn very complex pattern recognition. It's just dominating everything else for advanced computer vision, speech processing, and more—stuff that conventional machine learning techniques haven't been all that successful at. However, that doesn't necessarily mean that you want to use deep learning for everything. Deep learning generally acquires a large number of examples—many thousands, if not millions sometimes—and it can also be very computationally expensive. So, it's not always the best solution, although it is very powerful, as we have seen right here. So, 99% is about as good as you can get.</p>
<p>The following is the code to draw digits:</p>
<pre># Test on handwritten digits again<br/>img = np.zeros((28,28,3),dtype='uint8')<br/>fig, axes = plt.subplots(figsize=(3,3))<br/>axes.imshow(img)<br/>plt.axis("off")<br/>plt.gray()<br/>annotator = Annotator(axes)<br/>plt.connect('motion_notify_event', annotator.mouse_move)<br/>plt.connect('button_release_event', annotator.mouse_release)<br/>plt.connect('button_press_event', annotator.mouse_press)<br/>axes.plot()<br/>plt.show()</pre>
<p>The following code rasterizes and preprocesses the handwritten digit image:</p>
<pre># Rasterize and preprocess the above<br/>digimg = np.zeros((28,28,3),dtype='uint8')<br/>for ind, points in enumerate(annotator.xy[:-1]):<br/>    digimg=cv2.line(digimg, annotator.xy[ind], annotator.xy[ind+1],(255,0,0),1)<br/>digimg = cv2.GaussianBlur(digimg,(5,5),1.0)<br/>digimg = (digimg.astype('float') *1.0/np.amax(digimg)).astype('float')[:,:,0]<br/>digimg **= 0.5; digimg[digimg&gt;0.9]=1.0<br/>testim = digimg.reshape((-1,28*28))<br/><br/># And run through our model<br/>for tindex in range(10):<br/>    testlab = np.zeros((1,10))<br/>    testlab[0,tindex] = 1<br/>    if accuracy.eval(feed_dict={x: testim, y_: testlab, <br/>                                keep_prob: 1.0}) == 1:<br/>        break<br/><br/>print("Predicted #:",tindex) #tindex = TF model prediction<br/><br/># Display our rasterized digit<br/>outimg = testim.reshape((28,28))<br/>figure(figsize=(3,3)); imshow(outimg)</pre>
<p>So, let's test it again on our handwritten digit <kbd>0</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/1880122f-8b2d-4199-b02f-e02ef3c0a43b.png" style="width:17.00em;height:19.50em;" width="375" height="432"/></p>
<p class="mce-root"/>
<p>Again, we have similar code for processing the vectorized image, and we're getting the output and raster form and running it through our model, doing <kbd>accuracy.eval</kbd> here. As we can see in the preceding screenshot, we've got a zero as expected, and that's perfect. So,  we're going to be talking more about deep learning with CNNs in the next chapters, but we've already seen how powerful it is with relatively little code and we were able to fly it towards our particular problem of digit recognition. Alright, so, with that, we're going to move on to our next chapter, which is <a href="c59fb392-c966-4da6-987a-625378474e71.xhtml" target="_blank">Chapter 6</a>, <em>Facial Feature Tracking and Classification with dlib</em>.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we learned how to perform digit classification with TensorFlow using <kbd>softmax</kbd>. We learned how to acquire and process MNIST digit data. We then learned how to create and train a support vector machine, and apply it to new data.</p>
<p>In the next chapter, we will learn facial feature tracking and classification using dlib.</p>


            

            
        
    </div>



  </body></html>