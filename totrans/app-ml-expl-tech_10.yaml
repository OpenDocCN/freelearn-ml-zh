- en: '*Chapter 8*: Human-Friendly Explanations with TCAV'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*：使用TCAV进行人性化解释'
- en: In the previous few chapters, we have extensively discussed **LIME** and **SHAP**.
    You have also seen the practical aspect of applying the Python frameworks of LIME
    and SHAP to explain black-box models. One major limitation of both frameworks
    is that the method of explanation is not extremely consistent and intuitive with
    how non-technical end users would explain an observation. For example, if you
    have an image of a glass filled with Coke and use LIME and SHAP to explain a black-box
    model used to correctly classify the image as Coke, both LIME and SHAP would highlight
    regions of the image that lead to the correct prediction by the trained model.
    But if you ask a non-technical user to describe the image, the user would classify
    the image as Coke due to the presence of a dark-colored carbonated liquid in a
    glass that resembles a Cola drink. In other words, human beings tend to relate
    any observation with known *concepts* to explain it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们广泛讨论了**LIME**和**SHAP**。您也看到了将LIME和SHAP的Python框架应用于解释黑盒模型的实际应用方面。这两个框架的一个主要局限性是，解释方法与非技术性最终用户解释观察结果的方式并不极端一致和直观。例如，如果您有一张装满可乐的玻璃杯的图片，并使用LIME和SHAP来解释用于正确分类图片为可乐的黑盒模型，LIME和SHAP都会突出显示图片中导致训练模型做出正确预测的区域。但如果你要求一个非技术用户描述这张图片，用户会因为玻璃杯中存在类似可乐饮料的深色碳酸饮料而将其归类为可乐。换句话说，人类倾向于将任何观察与已知的概念联系起来以解释它。
- en: '**Testing with Concept Activation Vector (TCAV)** from *Google AI* also follows
    a similar approach in terms of explaining model predictions with known *human
    concepts*. So, in this chapter, we will cover how TCAV can be used to provide
    concept-based human-friendly explanations. Unlike LIME and SHAP, TCAV works beyond
    *feature attribution* and refers to concepts such as *color*, *gender*, *race*,
    *shape*, *any known object*, or an *abstract idea* to explain model predictions.
    In this chapter, we will discuss the workings of the TCAV algorithm. I will cover
    some of the advantages and disadvantages of the framework. We will also discuss
    using this framework for practical problem-solving. In [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033),
    *Model Explainability Methods*, under *Representation-based explanation*, you
    did get some exposure to TCAV, but in this chapter, we will cover the following
    topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用概念激活向量（TCAV）**进行**测试**，这是由**Google AI**提出的，在用已知**人类概念**解释模型预测方面也采取了类似的方法。因此，在本章中，我们将介绍如何使用TCAV提供基于概念的人性化解释。与LIME和SHAP不同，TCAV超越了**特征归因**，并涉及诸如**颜色**、**性别**、**种族**、**形状**、**任何已知对象**或**抽象想法**等概念来解释模型预测。在本章中，我们将讨论TCAV算法的工作原理。我将介绍该框架的一些优缺点。我们还将讨论使用此框架进行实际问题解决。在[*第2章*](B18216_02_ePub.xhtml#_idTextAnchor033)的*模型可解释性方法*部分，在*基于表示的解释*中，您已经接触到了TCAV，但本章我们将涵盖以下主题：'
- en: Understanding TCAV intuitively
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直观理解TCAV
- en: Exploring the practical applications of TCAV
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索TCAV的实际应用
- en: Advantages and limitations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Potential applications of concept-based explanations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于概念解释的潜在应用
- en: It's time to get started now!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候开始行动了！
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This code tutorial and the requisite resources can be downloaded or cloned from
    the GitHub repository for this chapter at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08).
    Similar to other chapters, Python and Jupyter notebooks are used to implement
    the practical application of the theoretical concepts covered in this chapter.
    However, I would recommend you run the notebooks only after you have gone through
    this chapter for a better understanding.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码教程和必需的资源可以从本章的GitHub仓库[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08)下载或克隆。与其他章节类似，Python和Jupyter笔记本用于实现本章涵盖的理论概念的实际应用。然而，我建议您在阅读完本章后再运行笔记本，以便更好地理解。
- en: Understanding TCAV intuitively
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直观理解TCAV
- en: 'The idea of TCAV was first introduced by *Kim et al.* in their work – *Interpretability
    beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors
    (TCAV)* ([https://arxiv.org/pdf/1711.11279.pdf](https://arxiv.org/pdf/1711.11279.pdf)).
    The framework was designed to provide interpretability beyond feature attribution,
    particularly for deep learning models that rely on low-level transformed features
    that are not human-interpretable. TCAV aims to explain the opaque internal state
    of the deep learning model using abstract, high-level, human-friendly concepts.
    In this section, I will present you with an intuitive understanding of TCAV and
    explain how it works to provide human-friendly explanations.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TCAV的概念最初由*Kim等人*在其工作中提出 – *超越特征归因的解释性：使用概念激活向量（TCAV）的定量测试* ([https://arxiv.org/pdf/1711.11279.pdf](https://arxiv.org/pdf/1711.11279.pdf))。该框架旨在提供超越特征归因的解释性，特别是对于依赖于低级转换特征且这些特征对人类不可解释的深度学习模型。TCAV旨在使用抽象的、高级的、对人类友好的概念来解释深度学习模型的内部状态。在本节中，我将向您展示对TCAV的直观理解，并解释它是如何提供对人类友好的解释的。
- en: What is TCAV?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是TCAV？
- en: So far, we have covered many methods and frameworks to explain ML models through
    feature-based approaches. But it might occur to you that since most ML models
    operate on low-level features, the feature-based explanation approaches might
    highlight features that are not human-interpretable. For example, for explaining
    image classifiers, pixel intensity values or pixels coordinates in an image might
    not be useful for end users without any technical background in data science and
    ML. So, these features are not user-friendly. Moreover, feature-based explanations
    are always restricted by the selection of features and the number of features
    present in the dataset. Out of all the features selected by the feature-based
    explanation methods, end users might be interested in a particular feature that
    is not picked by the algorithm.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了许多通过基于特征的方法解释机器学习模型的方法和框架。但您可能会想到，由于大多数机器学习模型在低级特征上运行，基于特征的解释方法可能会突出那些对没有数据科学和机器学习技术背景的最终用户来说没有用的特征。例如，在解释图像分类器时，图像中的像素强度值或像素坐标可能对没有技术背景的最终用户来说没有用。因此，这些特征不是用户友好的。此外，基于特征的解释总是受限于特征的选择和数据集中存在的特征数量。在所有由基于特征的解释方法选择的特征中，最终用户可能对算法未选择的某个特定特征感兴趣。
- en: So, instead of this approach, concept-based approaches provide a much wider
    abstraction that is human-friendly and more relevant as interpretability is provided
    in terms of the importance of high-level concepts. So, **TCAV** is a model interpretability
    framework from Google AI that implements the idea of a concept-based explanation
    method in practice. The algorithm depends on **Concept Activation Vectors (CAV)**,
    which provide an interpretation of the internal state of ML models using human-friendly
    concepts. In a more technical sense, TCAV uses directional derivatives to quantify
    the importance of human-friendly, high-level concepts for model predictions. For
    example, while describing hairstyles, concepts such as *curly hair*, *straight
    hair*, or *hair color* can be used by TCAV. These user-defined concepts are not
    the input features of the dataset that are used by the algorithm during the training
    process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，而不是这种方法，基于概念的方法提供了一种更广泛的抽象，它对人类友好且更相关，因为解释是以高级概念的重要性为依据提供的。所以，**TCAV**是Google
    AI的一个模型可解释性框架，它在实践中实现了基于概念解释方法的想法。该算法依赖于**概念激活向量（CAV**），它使用对人类友好的概念来解释机器学习模型的内部状态。在更技术性的意义上，TCAV使用方向导数来量化对模型预测的人类友好、高级概念的重要性。例如，在描述发型时，TCAV可以使用诸如*卷发*、*直发*或*发色*等概念。这些用户定义的概念不是算法在训练过程中使用的输入特征集。
- en: 'The following figure illustrates the key question addressed by TCAV:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了TCAV解决的关键问题：
- en: '![Figure 8.1 – TCAV helps us to address the key question of concept importance
    of a user-defined concept for image classification by a neural network'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1 – TCAV帮助我们通过神经网络对用户定义的概念进行图像分类，解决该概念重要性的关键问题'
- en: '](img/B18216_08_001.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_08_001.jpg]'
- en: Figure 8.1 – TCAV helps us to address the key question of concept importance
    of a user-defined concept for image classification by a neural network
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – TCAV帮助我们通过神经网络对用户定义的概念进行图像分类，解决该概念重要性的关键问题
- en: In the next section, let's try to understand the idea of model explanation using
    abstract concepts.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们尝试理解使用抽象概念进行模型解释的想法。
- en: Explaining with abstract concepts
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用抽象概念进行解释
- en: By now, you may have an intuitive understanding of the method of providing explanations
    with abstract concepts. But why do you think this is an effective approach? Let's
    take another example. Suppose you are working on building a deep learning-based
    image classifier for detecting doctors from images. After applying TCAV, let's
    say that you have found out that the *concept importance* of the concept *white
    male* is maximum, followed by *stethoscope* and *white coat*. The concept importance
    of *stethoscope* and *white coat* is expected, but the high concept importance
    of *white male* indicates a biased dataset. Hence, TCAV can help to evaluate **fairness**
    in trained models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能已经对使用抽象概念提供解释的方法有了直观的理解。但为什么你认为这是一种有效的方法呢？让我们再举一个例子。假设你正在构建一个基于深度学习的图像分类器，用于从图像中检测医生。在应用TCAV之后，比如说，你发现*概念重要性*最高的概念是*白人男性*，其次是*听诊器*和*白大褂*。*听诊器*和*白大褂*的概念重要性是可以预料的，但*白人男性*概念的高重要性表明数据集存在偏见。因此，TCAV可以帮助评估训练模型中的**公平性**。
- en: Essentially, the goal of CAVs is to estimate the importance of a concept (such
    as color, gender, and race) for the prediction of a trained model, even though
    the *concepts* were not used during the model training process. This is because
    TCAV learns *concepts* from a few example samples. For example, in order to learn
    a *gender* concept, TCAV needs a few data instances that have a *male* concept
    and a few *non-male* examples. Hence, TCAV can quantitatively estimate the trained
    model's sensitivity to a particular *concept* for that class. For generating explanations,
    TCAV perturbs data points toward a *concept* that is relatable to humans, and
    so it is a type of **global perturbation method**. Next, let's try to learn the
    main objectives of TCAV.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，CAV的目标是估计一个概念（如颜色、性别和种族）对于训练模型预测的重要性，即使这些*概念*在模型训练过程中没有被使用。这是因为TCAV从几个示例样本中学习*概念*。例如，为了学习一个*性别*概念，TCAV需要一些具有*男性*概念的样本和一些*非男性*的例子。因此，TCAV可以定量地估计训练模型对该类别的特定*概念*的敏感性。为了生成解释，TCAV会扰动数据点，使其向与人类相关的*概念*靠近，因此它是一种**全局扰动方法**。接下来，让我们尝试了解TCAV的主要目标。
- en: Goals of TCAV
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TCAV的目标
- en: 'I found the approach of TCAV to be very unique as compared to other explanation
    methods. One of the main reasons is because the developers of this framework established
    clear goals that resonate with my own understanding of human-friendly explanations.
    The following are the established goals of TCAV:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为TCAV的方法与其他解释方法相比非常独特。其中一个主要原因是，这个框架的开发者确立了与我自己的对人类友好解释的理解相呼应的明确目标。以下是TCAV确立的目标：
- en: '**Accessibility**: The developers of TCAV wanted this approach to be accessible
    to any end user, irrespective of their knowledge of ML or data science.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可访问性**：TCAV的开发者希望这种方法对任何最终用户都是可访问的，无论他们是否了解机器学习或数据科学。'
- en: '**Customization**: The framework can adapt to any user-defined concept. This
    is not limited to concepts considered during the training process.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制性**：该框架可以适应任何用户定义的概念。这不仅仅限于训练过程中考虑的概念。'
- en: '**Plug-in readiness**: The developers wanted this approach to work without
    the need to retrain or fine-tune trained ML models.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插件就绪性**：开发者希望这种方法能够在不重新训练或微调训练的机器学习模型的情况下工作。'
- en: '**Global interpretability**: TCAV can interpret the entire class or multiple
    samples of the dataset with a single quantitative measure. It is not restricted
    to the local explainability of data instances.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局可解释性**：TCAV可以通过一个单一的定量指标来解释整个类别或数据集的多个样本。它不仅限于数据实例的局部可解释性。'
- en: Now that we have an idea of what can be achieved using TCAV, let's discuss the
    general approach to how TCAV works.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了使用TCAV可以取得什么成果，让我们来讨论TCAV工作的一般方法。
- en: Approach of TCAV
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TCAV的方法
- en: 'In this section, we will cover the workings of TCAV in more depth. The overall
    workings of this algorithm can be summarized in the following methods:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更深入地探讨TCAV的工作原理。该算法的整体工作原理可以总结为以下方法：
- en: Applying directional derivatives to quantitatively estimate the sensitivity
    of predictions of trained ML models for various user-defined concepts.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将方向导数应用于定量估计训练的机器学习模型对各种用户定义概念的预测敏感性。
- en: Computing the final quantitative explanation, which is termed **TCAVq measure**,
    without any model re-training or fine-tuning. This measure is the relative importance
    of each concept to each model prediction class.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算最终的定量解释，称为**TCAVq度量**，而无需重新训练或微调模型。这个度量是每个概念对每个模型预测类别的相对重要性。
- en: 'Now, I will try to further simplify the approach of TCAV without using too
    many mathematical notions. Let''s assume we have a model for identifying zebras
    from images. To apply TCAV, the following approach can be taken:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我将尝试进一步简化TCAV的方法，而不使用过多的数学概念。假设我们有一个从图像中识别斑马的模型。要应用TCAV，可以采取以下方法：
- en: '**Defining a concept of interest**: The very first step is to consider the
    concepts of interest. For our zebra classifier, either we can have a given set
    of examples that represent the concept (such as black stripes are important in
    identifying a zebra) or we can have an independent dataset with the concepts labeled.
    The major benefit of this step is that it does not limit the algorithm from using
    features used by the model. Even non-technical users or domain experts can define
    the concepts based on their existing knowledge.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义感兴趣的概念**：第一步是考虑感兴趣的概念。对于我们的斑马分类器，我们既可以有一个代表该概念的给定示例集（例如，黑色条纹对于识别斑马很重要）或者我们可以有一个带有概念标签的独立数据集。这一步骤的主要好处是它不会限制算法使用模型使用的特征。即使是非技术用户或领域专家也可以根据他们现有的知识来定义概念。'
- en: '**Learning concept activation vectors**: The algorithm tries to learn a vector
    in the space of activation of the layers by training a linear classifier to differentiate
    between activations generated by a concept''s instances and instances present
    in any layer. So, a **CAV** is defined as the normal projection to a hyperplane
    that separates instances with a concept and instances without a concept in the
    model''s activation. For our zebra classifier, CAVs help to distinguish representations
    that denote *black stripes* and representations that do not denote *black stripes*.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**学习概念激活向量**：算法试图通过训练一个线性分类器来区分由概念实例生成的激活和任何层中存在的实例的激活，从而在学习层的激活空间中学习一个向量。因此，**CAV**被定义为将具有概念和没有概念的实例在模型激活中分离的超平面的正交投影。对于我们的斑马分类器，CAVs有助于区分表示*黑色条纹*和表示不表示*黑色条纹*的表示。'
- en: '**Estimating directional derivatives**: Directional derivatives are used to
    quantify the sensitivity of a model prediction toward a concept. So, for our zebra
    classifier, directional directives help us to measure the importance of the *black
    stripes* representation in predicting zebras. Unlike saliency maps, which use
    per-pixel saliency, directional derivatives are computed on the entire dataset
    or a set of inputs but for a specific concept. This helps to give a global perspective
    for the explanation.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**估计方向导数**：方向导数用于量化模型预测对概念的敏感性。因此，对于我们的斑马分类器，方向导数帮助我们测量*黑色条纹*表示在预测斑马中的重要性。与使用每个像素显著性的显著性图不同，方向导数是在整个数据集或一组输入上计算的，但针对特定概念。这有助于提供一个全局的解释视角。'
- en: '**Estimating the TCAV score**: To quantify the concept importance of a particular
    class, the TCAV score (**TCAVq**) is calculated. This metric helps to measure
    the positive or negative influence of a defined concept on a particular activation
    layer of a model.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**估计TCAV分数**：为了量化特定类别的概念重要性，计算TCAV分数（**TCAVq**）。这个指标有助于衡量定义的概念对模型特定激活层的影响，无论是正面的还是负面的。'
- en: '**CAV validation**: CAV can be produced from randomly selected data. But unfortunately,
    this might not produce meaningful concepts. So, in order to improve the generated
    concepts, TCAV runs multiple iterations for finding concepts from different batches
    of data, instead of training CAV once, on a single batch of data. Then, a **statistical
    significance test** is performed using *two-side t-test* for selecting the statistically
    significant concepts. Necessary corrections, such as the *Bonferroni correction*,
    are also performed to control the false discovery rate.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**CAV验证**：CAV可以从随机选择的数据中生成。但不幸的是，这可能不会产生有意义的概念。因此，为了提高生成的概念，TCAV通过从不同的数据批次中寻找概念进行多次迭代，而不是在单个数据批次上一次性训练CAV。然后，使用**双尾t检验**进行**统计显著性测试**，以选择具有统计显著性的概念。还执行了必要的校正，如**Bonferroni校正**，以控制假发现率。'
- en: Thus, we have covered the intuitive workings of the TCAV algorithm. Next, let's
    cover how TCAV can actually be implemented in practice.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经介绍了TCAV算法的直观工作原理。接下来，让我们看看TCAV如何在实践中实际应用。
- en: Exploring the practical applications of TCAV
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索TCAV的实际应用
- en: In this section, we will explore the practical applications of TCAV for explaining
    pre-trained image explainers with concept importance. The entire notebook tutorial
    is available in the code repository of this chapter at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter08/Intro_to_TCAV.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter08/Intro_to_TCAV.ipynb).
    This tutorial is presented based on the notebook provided in the original GitHub
    project repository of TCAV [https://github.com/tensorflow/tcav](https://github.com/tensorflow/tcav).
    I recommend that you all refer to the main project repository of TCAV since the
    credit for implementation should go to the developers and contributors of TCAV.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨TCAV在解释预训练图像解释器时，如何应用于概念重要性的实际应用。整个笔记本教程可在本章代码库中找到，地址为[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter08/Intro_to_TCAV.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter08/Intro_to_TCAV.ipynb)。本教程基于TCAV原始GitHub项目仓库中提供的笔记本[https://github.com/tensorflow/tcav](https://github.com/tensorflow/tcav)。我建议您都参考TCAV的主要项目仓库，因为实现上的功劳应归于TCAV的开发者和贡献者。
- en: 'In this tutorial, we will cover how to apply TCAV to validate the concept importance
    of the concept of *stripes* as compared to the *honeycomb* pattern for identifying
    *tigers*. The following images illustrate the flow of the approach used by TCAV
    to ascertain concept importance using a simple visualization:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将介绍如何将TCAV应用于验证“条纹”概念相对于“蜂窝”图案在识别老虎方面的概念重要性。以下图像展示了TCAV使用简单可视化来确定概念重要性的方法流程：
- en: '![Figure 8.2 – Using TCAV to estimate the concept importance of stripes in
    a tiger image classifier'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.2 – 使用TCAV估计老虎图像分类器中条纹的概念重要性'
- en: '](img/B18216_08_002.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_08_002.jpg]'
- en: Figure 8.2 – Using TCAV to estimate the concept importance of stripes in a tiger
    image classifier
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 使用TCAV估计老虎图像分类器中条纹的概念重要性
- en: Let's begin by setting up our Jupyter notebook.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先设置我们的Jupyter笔记本。
- en: Getting started
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始
- en: 'Similar to the other tutorial examples covered in the previous chapters, to
    install the necessary Python modules required to run the notebook, you can use
    the `pip` `install` command in a Jupyter notebook:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章中涵盖的其他教程示例类似，为了安装运行笔记本所需的Python模块，您可以在Jupyter笔记本中使用`pip install`命令：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can import all the modules to validate the successful installation of these
    frameworks:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以导入所有模块以验证这些框架的成功安装：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, let's take a look at the data that we'll be working with.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看我们将要处理的数据。
- en: About the data
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于数据
- en: 'I felt that the data preparation process, which is provided in the original
    project repository of TCAV, is slightly time-consuming. So, I have already prepared
    the necessary datasets, which you can refer to from this project repository. Since
    we will be validating the importance of the concept of *stripes* for images of
    *tigers*, we will need an image dataset for tigers. The data is collected from
    the ImageNet collection and is provided in the project repository at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/images/tiger](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/images/tiger).
    The images are randomly curated and collected using the *data collection script*
    provided in the TCAV repository: [https://github.com/tensorflow/tcav/tree/master/tcav/tcav_examples/image_models/imagenet](https://github.com/tensorflow/tcav/tree/master/tcav/tcav_examples/image_models/imagenet).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得在TCAV原始项目仓库中提供的数据准备过程稍微有些耗时。因此，我已经准备了必要的数据集，您可以从这个项目仓库中参考。由于我们将验证“条纹”概念对于老虎图像的重要性，我们需要一个老虎图像数据集。数据来自ImageNet收集，并在项目仓库[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/images/tiger](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/images/tiger)中提供。这些图像是随机精选和收集的，使用了TCAV仓库中提供的*数据收集脚本*[https://github.com/tensorflow/tcav/tree/master/tcav/tcav_examples/image_models/imagenet](https://github.com/tensorflow/tcav/tree/master/tcav/tcav_examples/image_models/imagenet)。
- en: 'In order to run TCAV, you would need to have the necessary *concept images*,
    *target class images,* and *random dataset images*. For this tutorial, I have
    prepared the concept images from the *Broden dataset* ([http://netdissect.csail.mit.edu/data/broden1_224.zip](http://netdissect.csail.mit.edu/data/broden1_224.zip)),
    as suggested in the main project example. Please go through the research work
    that led to the creation of this dataset: [https://github.com/CSAILVision/NetDissect](https://github.com/CSAILVision/NetDissect).
    You can also explore the *Broden dataset texture images* provided at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/concepts/broden_concepts](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/concepts/broden_concepts)
    to learn more. I recommend you to experiment with other concepts or other images
    and play around with TCAV-based concept importance!'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行TCAV，你需要准备必要的*概念图像*、*目标类图像*和*随机数据集图像*。对于这个教程，我已从*Broden数据集*（[http://netdissect.csail.mit.edu/data/broden1_224.zip](http://netdissect.csail.mit.edu/data/broden1_224.zip)）中准备了概念图像，正如主项目示例中所建议的。请查阅创建此数据集的研究工作：[https://github.com/CSAILVision/NetDissect](https://github.com/CSAILVision/NetDissect)。你还可以探索在[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/concepts/broden_concepts](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/concepts/broden_concepts)提供的*Broden数据集纹理图像*，以了解更多信息。我建议你尝试其他概念或其他图像，并围绕基于TCAV的概念重要性进行实验！
- en: Broden dataset
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Broden数据集
- en: '*David Bau*, Bolei Zhou*, Aditya Khosla, Aude Oliva, and Antonio Torralba.
    Network Dissection: Quantifying Interpretability of Deep Visual Representations.
    Computer Vision and Pattern Recognition (CVPR), 2017.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*David Bau*、Bolei Zhou*、Aditya Khosla、Aude Oliva和Antonio Torralba。网络剖析：量化深度视觉表示的可解释性。计算机视觉和模式识别（CVPR），2017。'
- en: 'As TCAV also requires some random datasets to ascertain the statistical significance
    of the concepts learned from target image examples, I have provided some sample
    random images in the project repository, thereby simplifying the running of the
    tutorial notebook! But as always, you should experiment with other random image
    examples as well. These random images are also collected using the image fetcher
    script provided in the main project: [https://github.com/tensorflow/tcav/blob/master/tcav/tcav_examples/image_models/imagenet/download_and_make_datasets.py](https://github.com/tensorflow/tcav/blob/master/tcav/tcav_examples/image_models/imagenet/download_and_make_datasets.py).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TCAV还需要一些随机数据集来确保从目标图像示例中学习到的概念统计意义的显著性，我在项目仓库中提供了一些示例随机图像，从而简化了教程笔记本的运行！但就像往常一样，你也应该尝试其他随机图像示例。这些随机图像也是使用主项目中提供的图像获取脚本收集的：[https://github.com/tensorflow/tcav/blob/master/tcav/tcav_examples/image_models/imagenet/download_and_make_datasets.py](https://github.com/tensorflow/tcav/blob/master/tcav/tcav_examples/image_models/imagenet/download_and_make_datasets.py)。
- en: 'To proceed further, you need to define the variables for the target class and
    the concepts:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步进行，你需要定义目标类和概念的变量：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can also create the necessary paths and directories to store the generated
    activations and CAVs as mentioned in the notebook tutorial. Next, let's discuss
    the model used in this example.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以创建必要的路径和目录来存储笔记本教程中提到的生成的激活和CAVs。接下来，让我们讨论这个示例中使用的模型。
- en: Discussions about the deep learning model used
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于所使用的深度学习模型的讨论
- en: In this example, we will use a pre-trained deep learning model to highlight
    the fact that even though TCAV is considered to be a model-specific approach,
    as it is only applicable to neural networks, it does not make an assumption of
    the network architecture as such and can work well with most deep neural network
    models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用预训练的深度学习模型来强调即使TCAV被认为是一种特定于模型的方案，因为它仅适用于神经网络，但它并不对网络架构做出假设，可以很好地与大多数深度神经网络模型协同工作。
- en: 'For this example, we will use the pre-trained GoogleNet model, [https://paperswithcode.com/method/googlenet](https://paperswithcode.com/method/googlenet),
    based on the ImageNet dataset ([https://www.image-net.org/](https://www.image-net.org/)).
    The model files are provided in the code repository: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/models/inception5h](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/models/inception5h).
    You can load the trained model using the following lines of code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用基于ImageNet数据集的预训练GoogleNet模型，[https://paperswithcode.com/method/googlenet](https://paperswithcode.com/method/googlenet)。模型文件在代码仓库中提供：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/models/inception5h](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter08/models/inception5h)。您可以使用以下代码行加载训练好的模型：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The model wrapper is actually used to get the internal state and tensors of
    the trained model. Concept importance is actually computed based on the internal
    neuron activations and hence, this model wrapper is important. For more details
    about the workings of the internal API, please refer to the following link: [https://github.com/tensorflow/tcav/blob/master/Run_TCAV_on_colab.ipynb](https://github.com/tensorflow/tcav/blob/master/Run_TCAV_on_colab.ipynb).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 模型包装器实际上用于获取训练模型的内部状态和张量。概念重要性实际上是基于内部神经元激活计算的，因此，这个模型包装器很重要。有关内部API工作原理的更多详细信息，请参阅以下链接：[https://github.com/tensorflow/tcav/blob/master/Run_TCAV_on_colab.ipynb](https://github.com/tensorflow/tcav/blob/master/Run_TCAV_on_colab.ipynb)。
- en: 'Next, we would need to generate the concept activations using the `ImageActivationGenerator`
    method:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用`ImageActivationGenerator`方法生成概念激活：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, we will explore model explainability using TCAV.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用TCAV来探索模型的可解释性。
- en: Model explainability using TCAV
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TCAV进行模型可解释性
- en: 'As discussed before, TCAV is currently used to explain neural networks and
    the inner layers of a neural network. So, it is not model-agnostic, but rather
    a model-centric explainability method. This requires us to define the bottleneck
    layer of the network:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，TCAV目前用于解释神经网络及其神经网络内部层。因此，它不是模型无关的，而是一种以模型为中心的可解释性方法。这要求我们定义网络的瓶颈层：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The next step will be to apply the TCAV algorithm to create the concept activation
    vectors. The process also includes performing statistical significance testing
    using two side t-test between the concept importance of the target class and the
    random samples:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步将是应用TCAV算法来创建概念激活向量。此过程还包括使用目标类概念重要性和随机样本之间的双尾t检验进行统计显著性测试：
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The original experiment mentioned in the TCAV paper, [https://arxiv.org/abs/1711.11279](https://arxiv.org/abs/1711.11279),
    mentioned using at least 500 random experiments to identify the statistically
    significant concepts. But for the sake of simplicity, and to achieve faster results,
    we are using 15 random experiments. You can experiment with more random experiments
    as well.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: TCAV论文中提到的原始实验，[https://arxiv.org/abs/1711.11279](https://arxiv.org/abs/1711.11279)，提到至少使用500次随机实验来识别具有统计显著性的概念。但为了简化，并实现更快的结果，我们正在使用15次随机实验。您也可以尝试更多的随机实验。
- en: 'Finally, we can get the results and visualize the concept importance:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以获取结果并可视化概念重要性：
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This will generate the following plot that helps us to compare concept importance:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表，帮助我们比较概念的重要性：
- en: '![Figure 8.3 – TCAV concept importance of the concepts of striped and honeycombed'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.3 – TCAV concept importance of the concepts of striped and honeycombed]'
- en: for identifying tiger images
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 用于识别老虎图像
- en: '](img/B18216_08_003.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_08_003.jpg]'
- en: Figure 8.3 – TCAV concept importance of the concepts of striped and honeycombed
    for identifying tiger images
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 用于识别老虎图像的条纹和蜂窝状概念的概念重要性
- en: As you can observe from *Figure 8.3*, the *striped* concept has significantly
    higher concept importance than the *honeycombed* concept for identifying *tigers*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从*图8.3*中观察到的，对于识别*老虎*，*条纹*概念的概念重要性显著高于*蜂窝状*概念。
- en: Now that we have covered the practical application part, let me give you a similar
    challenge as an exercise. Can you now use the ImageNet dataset and ascertain the
    importance of the concept of *water* to *ships* and of *clouds* or *sky* to *airplanes*?
    This will help you understand this concept in more depth and give you more confidence
    to apply TCAV. Next, we will discuss some advantages and limitations of TCAV.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了实际应用部分，让我给你一个类似的挑战作为练习。你现在能否使用ImageNet数据集，并确定“水”对“船只”以及“云”或“天空”对“飞机”的概念的重要性？这将帮助你更深入地理解这个概念，并增强你应用TCAV的信心。接下来，我们将讨论TCAV的一些优势和局限性。
- en: Advantages and limitations
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势和局限性
- en: In the previous section, we covered the practical aspects of TCAV. TCAV is indeed
    a very interesting and novel approach to explaining complex deep learning models.
    Although it has many advantages, unfortunately, I did find some limitations in
    terms of the current framework that can definitely be improved in the revised
    version.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了TCAV的实际应用方面。TCAV确实是一种解释复杂深度学习模型非常有趣且新颖的方法。尽管它有很多优势，但不幸的是，我在当前框架中确实发现了一些局限性，这些局限性在修订版本中肯定可以改进。
- en: Advantages
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势
- en: 'Let''s discuss the following advantages first:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先讨论以下优势：
- en: As you have previously seen with the LIME framework in [*Chapter 4*](B18216_04_ePub.xhtml#_idTextAnchor076),
    *LIME for Model Interpretability* (which generates explanations using a **global
    perturbation method**), there can be contradicting explanations for two data instances
    for the same class. Even though TCAV is also a type of global perturbation method,
    unlike LIME, TCAV-generated explanations are not only true for a single data instance
    but also true for the entire class. This is a major advantage of TCAV over LIME,
    which increases the user's trust in the explanation method.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如你在[第4章](B18216_04_ePub.xhtml#_idTextAnchor076)中之前看到的LIME框架一样，*LIME用于模型可解释性*（使用**全局扰动方法**生成解释），对于同一类别的两个数据实例可能会有相互矛盾的解释。尽管TCAV也是一种全局扰动方法，但与LIME不同，TCAV生成的解释不仅对单个数据实例是真实的，而且对整个类别也是真实的。这是TCAV相对于LIME的一个主要优势，这增加了用户对解释方法的信任。
- en: Concept-based explanations are closer to how humans would explain an unknown
    observation, rather than feature-based explanations as adopted in LIME and SHAP.
    So, TCAV-generated explanations are indeed more human-friendly.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于概念的解释更接近人类解释未知观察的方式，而不是LIME和SHAP采用的基于特征的解释。因此，TCAV生成的解释确实更符合人性。
- en: Feature-based explanations are limited to the features used in the model. To
    introduce any new feature for model explainability, we would need to re-train
    the model, whereas a concept-based explanation is more flexible and is not limited
    to features used during model training. To introduce a new concept, we do not
    need to retrain the model. Also, for introducing the concepts, you don't have
    to know anything about ML. You would just have to make the necessary datasets
    to generate concepts.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于特征的解释仅限于模型中使用的特征。为了引入任何新的特征以实现模型可解释性，我们需要重新训练模型，而基于概念的解释则更加灵活，并且不受模型训练期间使用的特征的限制。要引入一个新概念，我们不需要重新训练模型。此外，在引入概念时，你不必了解任何关于机器学习（ML）的知识。你只需要创建必要的数据集来生成概念。
- en: Model explainability is not the only benefit of TCAV. TCAV can help to detect
    issues during the training process, such as **imbalanced datasets** leading to
    *bias* in *the dataset vis-à-vis the majority class*. In fact, concept importance
    can be used as a *metric* to compare models. For example, suppose you are using
    a *VGG19* model and a *ResNet50* model. Let's say both these models have similar
    accuracy and model performance, yet concept importance for a user-defined concept
    is much higher for the VGG19 model as compared to the ResNet50 model. In such
    a case, it is better to use the VGG19 model as compared to ResNet50\. Hence, TCAV
    can be used to improve the model training process.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可解释性不是TCAV的唯一好处。TCAV可以帮助检测训练过程中的问题，例如**不平衡数据集**导致**相对于多数类的数据集**中的**偏差**。实际上，概念重要性可以用作**指标**来比较模型。例如，假设你正在使用*VGG19*模型和*ResNet50*模型。假设这两个模型具有相似的准确性和模型性能，但与ResNet50模型相比，用户定义的概念的重要性对于VGG19模型要高得多。在这种情况下，使用VGG19模型比使用ResNet50模型更好。因此，TCAV可以用来改进模型训练过程。
- en: These are some of the distinct advantages of TCAV, which makes it more human-friendly
    than LIME and SHAP. Next, let's discuss some known limitations of TCAV.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是TCAV的一些独特优势，使其比LIME和SHAP更符合人性。接下来，让我们讨论一下TCAV的一些已知局限性。
- en: Limitations
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: 'The following are some of the known disadvantages of the TCAV approach:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些已知TCAV方法的缺点：
- en: Currently, the approach of concept-based explanation using TCAV is limited to
    just neural networks. In order to increase its adoption, TCAV would need an implementation
    that can work with *classical machine learning algorithms* such as *Decision Trees*,
    *Support Vector Machines*, and *Ensemble Learning algorithms*. Both LIME and SHAP
    can be applied with classical ML algorithms to solve standard ML problems and
    that is probably why LIME and SHAP have more adoption. Similarly, with text data,
    too, TCAV has very limited applications.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前，使用TCAV进行概念解释的方法仅限于神经网络。为了增加其采用率，TCAV需要一个可以与*经典机器学习算法*（如*决策树*、*支持向量机*和*集成学习算法*）一起工作的实现。LIME和SHAP都可以应用于经典机器学习算法来解决标准机器学习问题，这也是为什么LIME和SHAP有更多采用的原因。同样，在文本数据方面，TCAV的应用也非常有限。
- en: 'TCAV is highly prone to *data drift*, *adversarial effects*, and *other data
    quality issues* discussed in [*Chapter 3*](B18216_03_ePub.xhtml#_idTextAnchor053)*,
    Data-Centric Approaches*. If you are using TCAV, you would need to ensure that
    training data, inference data, and even concept data have similar statistical
    properties. Otherwise, the concepts generated can become affected due to noise
    or data impurity issues:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: TCAV非常容易受到[*第3章*](B18216_03_ePub.xhtml#_idTextAnchor053)中讨论的*数据漂移*、*对抗性影响*和其他数据质量问题的影响，“数据为中心的方法”。如果您使用TCAV，您需要确保训练数据、推理数据和甚至概念数据具有相似的统计特性。否则，由于噪声或数据不纯问题，生成的概念可能会受到影响：
- en: '*Guillaume Alain* and *Yoshua Bengio*, in their paper *Understanding intermediate
    layers using linear classifier probes* ([https://arxiv.org/abs/1610.01644](https://arxiv.org/abs/1610.01644)),
    have expressed some concern about applying TCAV to shallower neural networks.
    Many similar research papers have suggested that concepts in deeper layers are
    more separable as compared to concepts in shallower networks and, hence, the use
    of TCAV is limited to mostly deep neural networks.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Guillaume Alain* 和 *Yoshua Bengio* 在他们的论文 *Understanding intermediate layers
    using linear classifier probes* ([https://arxiv.org/abs/1610.01644](https://arxiv.org/abs/1610.01644))
    中，表达了对将TCAV应用于较浅层神经网络的担忧。许多类似的研究论文都建议，与较浅层网络中的概念相比，深层网络中的概念更具可分性，因此TCAV的使用主要限于深度神经网络。'
- en: Preparing a concept dataset can be a challenging and expensive task. Although
    you don't need ML knowledge to prepare a concept dataset, still, in practice,
    you do not expect any common end user to spend time creating an annotated concept
    dataset for any customized user-defined concept.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备一个概念数据集可能是一项具有挑战性和昂贵的任务。尽管您不需要机器学习知识来准备概念数据集，但在实践中，您也不期望任何普通用户花费时间创建任何定制用户定义概念的有标注概念数据集。
- en: I felt that the TCAV Python framework would require further improvements before
    being used in any production-level system. In my opinion, at the time of writing
    this chapter, this framework would need to mature further so that it can be used
    easily with any production-level ML system.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我觉得TCAV的Python框架在使用于任何生产级系统之前还需要进一步改进。在我看来，在撰写本章时，这个框架需要进一步成熟，以便可以轻松地与任何生产级机器学习系统一起使用。
- en: I think all these limitations can indeed be solved to make TCAV a much more
    robust framework that is widely adopted. If you are interested, you can also reach
    out to authors and developers of the TCAV framework and contribute to the open
    source community! In the next section, let's discuss some potential applications
    of concept-based explanations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为所有这些限制都可以解决，使TCAV成为一个更加稳健的框架，被广泛采用。如果您感兴趣，您也可以联系TCAV框架的作者和开发者，为开源社区做出贡献！在下一节中，让我们讨论一些基于概念解释的潜在应用。
- en: Potential applications of concept-based explanations
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于概念解释的潜在应用
- en: 'I do see great potential for concept-based explanations such as TCAV! In this
    section, you will get exposure to some potential applications of concept-based
    explanations that can be important research topics for the entire AI community,
    which are as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我确实看到了基于概念的解释，如TCAV的巨大潜力！在本节中，您将接触到一些基于概念解释的潜在应用，这些可能成为整个AI社区的重要研究课题，具体如下：
- en: '**Estimation of transparency and fairness in AI**: Most regulatory concerns
    for black-box AI models are related to concepts such as gender, color, and race.
    Concept-based explanations can actually help to estimate whether an AI algorithm
    is fair in terms of these abstract concepts. The detection of bias for AI models
    can actually improve its transparency and help to address certain regulatory concerns.
    For example, in terms of doctors using deep learning models, TCAV can be used
    to detect whether the model is biased toward a specific gender, color, or race
    as ideally, these concepts are not important as regards the model''s decision.
    High concept importance for these concepts indicates the presence of bias. *Figure
    8.4* illustrates an example where TCAV is used to detect model bias.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI中透明度和公平性的估计**：大多数针对黑盒AI模型的监管担忧都与性别、颜色和种族等概念相关。基于概念的解释实际上可以帮助估计AI算法在这些抽象概念方面是否公平。检测AI模型的偏差实际上可以提高其透明度，并有助于解决某些监管问题。例如，在医生使用深度学习模型的情况下，TCAV可以用来检测模型是否对特定性别、颜色或种族存在偏见，因为这些概念在模型决策方面理想上并不重要。这些概念的高概念重要性表明存在偏见。*图8.4*展示了使用TCAV检测模型偏差的一个示例。'
- en: '![Figure 8.4 – TCAV can be used to detect model bias based on concept importance'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.4 – TCAV可以根据概念重要性检测模型偏差'
- en: '](img/B18216_08_004.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_08_004.jpg)'
- en: Figure 8.4 – TCAV can be used to detect model bias based on concept importance
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – TCAV可以根据概念重要性检测模型偏差
- en: '**Detection of adversarial attacks with CAV**: If you go through the appendix
    of the TCAV research paper ([https://arxiv.org/pdf/1711.11279.pdf](https://arxiv.org/pdf/1711.11279.pdf)),
    the authors have mentioned that the concept importance of actual samples and adversarial
    samples are quite different. This means that if an image gets impacted by an adversarial
    attack, the concept importance would also change. So, CAVs can be a potential
    method in detecting adversarial attacks, as discussed in [*Chapter 3*](B18216_03_ePub.xhtml#_idTextAnchor053),
    *Data-Centric Approaches*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用CAV检测对抗攻击**：如果你阅读了TCAV研究论文的附录([https://arxiv.org/pdf/1711.11279.pdf](https://arxiv.org/pdf/1711.11279.pdf))，作者们提到实际样本和对抗样本的概念重要性相当不同。这意味着如果一个图像受到对抗攻击的影响，其概念重要性也会发生变化。因此，CAV可以作为一种潜在的方法来检测对抗攻击，正如在第3章*数据中心方法*中讨论的那样。'
- en: '**Concept-based image clustering**: Using CAVs to cluster images based on similar
    concepts can be an interesting application. Deep learning-based image search engines
    are a common application in which clustering or similarity algorithms are applied
    to feature vectors to locate similar images. However, these are feature-based
    methods. Similarly, there is a potential to apply concept-based image clustering
    using CAVs.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于概念图像聚类**：使用CAV根据相似概念对图像进行聚类可以是一个有趣的应用。基于深度学习的图像搜索引擎是一个常见的应用，其中聚类或相似性算法被应用于特征向量以定位相似图像。然而，这些都是基于特征的方法。同样，使用CAV进行基于概念图像聚类的潜力也是存在的。'
- en: '**Automated concept-based explanations (ACE)**: *Ghorbani, Amirata*, *James
    Wexler*, *James Zou*, *and Been Kim*, in their research work – *Towards automatic
    concept-based explanations*, mentioned an automated version of TCAV that goes
    through the training images and automatically discovers prominent concepts. This
    is an interesting work, as I think it can have an important application in identifying
    incorrectly labeled training data. In industrial applications, getting a perfectly
    labeled curated dataset is extremely challenging. This problem can be solved to
    a great extent using ACE.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于概念自动解释（ACE）**：*Ghorbani, Amirata*，*James Wexler*，*James Zou*，*和Been Kim*，在他们关于《**向自动概念解释**》的研究工作中，提到了一种自动化的TCAV版本，该版本会遍历训练图像并自动发现显著的概念。这是一项有趣的工作，我认为它在识别错误标记的训练数据方面可以发挥重要作用。在工业应用中，获取一个完美标记的精选数据集极具挑战性。这个问题在很大程度上可以通过ACE来解决。'
- en: '**Concept-based Counterfactual Explanation**: In [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033),
    *Model Explainability Methods*, we discussed **counterfactual explanation (CFE)**
    as a mechanism for generating actionable insights by suggesting changes to the
    input features that can change the overall outcome. However, CFE is a feature-based
    explanation method. It would be a really interesting topic of research to have
    a concept-based counterfactual explanation, which is one step closer to human-friendly
    explanations.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if we say that it is going to *rain* today although there is a
    *clear sky* now, we usually add a further explanation that suggests that the *clear
    sky* can be covered with *clouds*, which increases the probability of rainfall.
    In other words, a *clear sky* is a concept related to a *sunny* day, while a *cloudy
    sky* is a concept related to *rainfall*. This example suggests that the forecast
    can be flipped if the concept describing the situation is also flipped. Hence,
    this is the idea of the concept-based counterfactual. The idea is not very far-fetched
    as **concept bottleneck models (CBMs)** presented in the research work by *Koh
    et al.*, in [https://arxiv.org/abs/2007.04612](https://arxiv.org/abs/2007.04612),
    can implement a similar idea of generated concept-based counterfactuals by manipulating
    the neuron action of the bottleneck layer.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.5* illustrates an example of using a concept-based counterfactual
    example. There is no existing algorithm or framework that can help us achieve
    this, yet this can be a useful application of concept-based approaches in computer
    vision.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – An illustration of the idea of concept-based counterfactual
    examples'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_08_005.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – An illustration of the idea of concept-based counterfactual examples
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: I feel this is a wide-open research field and the potential to come up with
    game-changing applications using concept-based explanations is immense. I do sincerely
    hope that more and more researchers and AI developers start working on this area
    to make significant progress in the coming years! Thus, we have arrived at the
    end of this chapter. Let me now summarize what has been covered in this chapter
    in the next section.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers the concepts of TCAV, a novel approach, and a framework
    developed by Google AI. You have received a conceptual understanding of TCAV,
    practical exposure to applying the Python TCAV framework, learned about some key
    advantages and limitations of TCAV, and finally, I presented some interesting
    ideas regarding potential research problems that can be solved using concept-based
    explanations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore other popular XAI frameworks and apply
    these frameworks to solving practical problems.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following resources to gain additional information:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '*Interpretability Beyond Feature Attribution: Quantitative Testing with Concept
    Activation Vectors (TCAV)*: [https://arxiv.org/pdf/1711.11279.pdf](https://arxiv.org/pdf/1711.11279.pdf)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*超越特征归因的可解释性：使用概念激活向量（TCAV）的定量测试*: [https://arxiv.org/pdf/1711.11279.pdf](https://arxiv.org/pdf/1711.11279.pdf)'
- en: '*TCAV Python framework -* [https://github.com/tensorflow/tcav](https://github.com/tensorflow/tcav)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TCAV Python 框架*: [https://github.com/tensorflow/tcav](https://github.com/tensorflow/tcav)'
- en: '*Koh et al. "Concept Bottleneck Models"*: [https://arxiv.org/abs/2007.04612](https://arxiv.org/abs/2007.04612)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Koh 等人 "概念瓶颈模型"*: [https://arxiv.org/abs/2007.04612](https://arxiv.org/abs/2007.04612)'
- en: '*Guillaume Alain and Yoshua Bengio*, "*Understanding intermediate layers using
    linear classifier probes"*: [https://arxiv.org/abs/1610.01644](https://arxiv.org/abs/1610.01644)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Guillaume Alain 和 Yoshua Bengio*, "*使用线性分类器探针理解中间层"*: [https://arxiv.org/abs/1610.01644](https://arxiv.org/abs/1610.01644)'
- en: '*Ghorbani, Amirata, James Wexler, James Zou and Been Kim, "Towards automatic
    concept-based explanations"*: [https://arxiv.org/abs/1902.03129](https://arxiv.org/abs/1902.03129)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ghorbani, Amirata, James Wexler, James Zou 和 Been Kim, "迈向基于概念自动解释"*: [https://arxiv.org/abs/1902.03129](https://arxiv.org/abs/1902.03129)'
- en: '*Detecting Concepts, Chapter 10.3 Molnar, C. (2022). Interpretable Machine
    Learning: A Guide for Making Black Box Models Explainable (2nd ed.).*: [https://christophm.github.io/interpretable-ml-book/detecting-concepts.html](https://christophm.github.io/interpretable-ml-book/detecting-concepts.html)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检测概念，第10.3章 Molnar, C. (2022). 可解释机器学习：构建可解释黑盒模型的指南（第2版）*: [https://christophm.github.io/interpretable-ml-book/detecting-concepts.html](https://christophm.github.io/interpretable-ml-book/detecting-concepts.html)'
