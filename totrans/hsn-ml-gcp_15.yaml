- en: Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Nowadays, most computers are based on a symbolic elaboration. The problem is
    first encoded in a set of variables and then processed using an explicit algorithm
    that, for each possible input of the problem, offers an adequate output. However,
    there are problems in which resolution by an explicit algorithm is inefficient
    or even unnatural, for example, a speech recognizer; tackling this kind of problem
    with the classic approach is inefficient. This and other similar problems, such
    as autonomous navigation of a robot or voice assistance in performing an operation,
    are part of a very diverse set of problems that can be addressed directly through
    solutions based on reinforcement learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，大多数计算机都是基于符号化处理。问题是首先编码在一系列变量中，然后使用一个显式算法进行处理，该算法为问题的每个可能输入提供适当的输出。然而，有些问题通过显式算法的解决既低效甚至不自然，例如，语音识别器；用经典方法处理这类问题是不高效的。这类问题和其他类似问题，如机器人的自主导航或操作中的语音助手，是可以通过基于强化学习的解决方案直接解决的一个非常多样化的问题集。
- en: Reinforcement learning is based on a psychology theory, elaborated after a series
    of experiments performed on animals. Defining a goal to be achieved, reinforcement
    learning tries to maximize the rewards received for the execution of the action
    or set of actions that allow us to reach the designated goal. Reinforcement learning
    is a very exciting sector of machine learning, used in everything from autonomous
    cars to video games. It aims to create algorithms that can learn and adapt to
    environmental changes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习基于心理学理论，该理论是在对动物进行一系列实验之后详细阐述的。定义一个要实现的目标，强化学习试图最大化执行动作或动作集以使我们达到指定目标所获得的奖励。强化学习是机器学习的一个非常激动人心的领域，它被用于从自动驾驶汽车到视频游戏的各种应用中。它的目标是创建能够学习和适应环境变化的算法。
- en: 'The topics covered in this chapter are:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题包括：
- en: Reinforcement learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: '**Markov Decision Process** (**MDP**)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程**（**MDP**）'
- en: Q-learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习
- en: '**Temporal difference** (**TD**) learning'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间差分**（**TD**）学习'
- en: Deep Q-learning networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度Q学习网络
- en: At the end of the chapter, you will be fully introduced to the power of reinforcement
    learning and will learn the different approaches to this technique. Several reinforcement
    learning methods will be covered.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将全面了解强化学习的力量，并学习这种技术的不同方法。将涵盖几种强化学习方法。
- en: Reinforcement learning introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习简介
- en: Reinforcement learning aims to create algorithms that can learn and adapt to
    environmental changes. This programming technique is based on the concept of receiving
    external stimuli depending on the algorithm choices. A correct choice will involve
    a premium while an incorrect choice will lead to a penalty. The goal of the system
    is to achieve the best possible result, of course.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是创建能够学习和适应环境变化的算法。这种编程技术基于根据算法选择接收外部刺激的概念。正确的选择将涉及奖励，而错误的选择将导致惩罚。当然，系统的目标是实现最佳可能的结果。
- en: In supervised learning, there is a teacher that tells the system which is the
    correct output (learning with a teacher). This is not always possible. Often we
    have only qualitative information (sometimes binary, right/wrong, or success/failure).
    The information available is called **reinforcement signals**. But the system
    does not give any information on how to update the agent's behavior (that is,
    weights). You cannot define a cost function or a gradient. The goal of the system
    is to create the smart agents that are able to learn from their experience.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，有一个教师告诉系统哪个是正确的输出（有教师的学习）。这并不总是可能的。通常我们只有定性信息（有时是二元的，对/错，或成功/失败）。可用的信息被称为**强化信号**。但系统不会提供任何关于如何更新代理行为（即权重）的信息。你不能定义一个成本函数或梯度。系统的目标是创建能够从经验中学习的智能代理。
- en: 'The following is a flowchart that displays reinforcement learning interaction
    with the environment:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是一个流程图，显示了强化学习与环境之间的交互：
- en: '![](img/2b492b5e-a4d9-4cf9-b7e5-719a5f97220b.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2b492b5e-a4d9-4cf9-b7e5-719a5f97220b.png)'
- en: Scientific literature has taken an uncertain stance on the classification of
    learning by reinforcement as a paradigm. In fact, in an initial phase it was considered
    as a special case of supervised learning, and then it was fully promoted as the
    third paradigm of machine learning algorithms. It is applied in different contexts
    in which supervised learning is inefficient; the problems of interaction with
    the environment are clear examples.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 科学文献对强化学习的分类持不确定的态度，将其视为一种范例。事实上，在最初阶段，它被视为监督学习的特殊情况，然后被完全提升为机器学习算法的第三范式。它在监督学习效率低下的不同环境中得到应用；与环境交互的问题是很明显的例子。
- en: 'The following flow shows the steps to follow to correctly apply a reinforcement
    learning algorithm:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下流程显示了正确应用强化学习算法的步骤：
- en: Preparation of the agent
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理的准备
- en: Observation of the environment
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监测环境
- en: Selection of the optimal strategy
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最佳策略
- en: Execution of actions
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作
- en: Calculation of the corresponding reward (or penalty)
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算相应的奖励（或惩罚）
- en: Development of updating strategies (if necessary)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新策略的开发（如果需要）
- en: Repeating steps 2-5 iteratively until the agent learns the optimal strategies
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2-5，直到代理学会最佳策略
- en: Reinforcement learning is based on some theory of psychology, elaborated after
    a series of experiments performed on animals. In particular, American psychologist
    Edward Thorndike noted that if a cat is given a reward immediately after the execution
    of a behavior considered correct, it increases the probability that this behavior
    will repeat itself. While in the face of unwanted behavior, the application of
    a punishment decreases the probability of a repetition of error.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习基于一些心理学理论，这些理论是在一系列对动物进行的实验之后详细阐述的。特别是，美国心理学家爱德华·桑代克指出，如果一只猫在执行被认为正确的行为后立即得到奖励，那么这种行为再次发生的概率会增加。而面对不受欢迎的行为时，应用惩罚会降低错误重复的概率。
- en: On the basis of this theory, and with a goal to be achieved defined, reinforcement
    learning tries to maximize the rewards received for execution of the action or
    set of actions that allow us to reach the designated goal.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在此理论的基础上，并定义了一个要实现的目标，强化学习试图最大化执行动作或动作集以实现指定目标的奖励。
- en: Agent-Environment interface
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理-环境接口
- en: Reinforcement learning can be seen as a special case of the interaction problem
    for achieving a goal. The entity that must reach the goal is called an **agent**.
    The entity with which the agent must interact is called the **environment**, which
    corresponds to everything that is external to the agent.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习可以看作是达到目标交互问题的特殊情况。必须达到目标的是被称为**代理**的实体。与代理必须交互的实体被称为**环境**，它对应于代理外部的一切。
- en: So far, we have focused more on the term agent, but what does it represent?
    The agent is a software entity that performs services on behalf of another program,
    usually automatically and invisibly. These software are also called **smart agents**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们更多地关注了“代理”这个术语，但它究竟代表了什么？代理是一种代表其他程序执行服务的软件实体，通常自动且无形。这些软件也被称为**智能代理**。
- en: 'The following are the most important features of an agent:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个代理最重要的特征：
- en: It can choose an action on the environment between a continuous and a discrete
    set
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以在连续集和离散集之间选择对环境采取的行动
- en: Action depends on the situation. The situation is summarized in the system state
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行动取决于情况。情况总结在系统状态中
- en: The agent continuously monitors the environment (input) and continuously changes
    the status
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理持续监控环境（输入）并持续改变状态
- en: The choice of action is not trivial and requires a certain degree of **intelligence**
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行动选择并非易事，需要一定程度的**智能**
- en: The agent has a smart memory
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理具有智能记忆
- en: The agent has a goal-directed behavior but acts in an uncertain environment
    not known a priori or partially known. An agent learns by interacting with the
    environment. Planning can be developed while learning about the environment through
    the measurements made by the agent itself. The strategy is close to trial-and-error
    theory.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 代理具有目标导向的行为，但在一个事先未知或部分已知的不确定环境中行动。代理通过与环境的交互来学习。在通过代理自身进行的测量中了解环境的同时，可以制定计划。策略接近试错理论。
- en: Trial and error is a fundamental method of problem solving. It is characterized
    by repeated, varied attempts that are continued until success, or until the agent
    stops trying.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试和错误是解决问题的基本方法。它以重复、变化的尝试为特征，直到成功，或者直到智能体停止尝试。
- en: The Agent-Environment interaction is continuous; the agent chooses an action
    to be taken, and in response, the environment changes states, presenting a new
    situation to be faced.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体-环境交互是连续的；智能体选择一个要采取的动作，然后环境改变状态，呈现一个新的情况来面对。
- en: In the particular case of reinforcement learning, the environment provides the
    agent with a reward; it is essential that the source of the reward is the environment
    to avoid the formation of a personal reinforcement mechanism within the agent
    that would compromise learning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习的特定情况下，环境为智能体提供奖励；确保奖励来源是环境，以避免在智能体内部形成会损害学习的个人强化机制。
- en: The value of the reward is proportional to the influence that the action has
    in reaching the objective; so it is positive or high in the case of a correct
    action, or negative or low action for an incorrect action.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励的价值与动作在达到目标时产生的影响成比例；因此，在正确动作的情况下是正的或高的，在错误动作的情况下是负的或低的。
- en: 'The following are some examples from real life in which there is an interaction
    between the agent and environment to solve the problem:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些现实生活中智能体和环境之间交互以解决问题的例子：
- en: A chess player, for each move, has information on the configurations of pieces
    that can create and on the possible countermoves of the opponent
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国际象棋选手，对于每一个移动，都有关于可以创造和对手可能的反击动作的棋子配置的信息
- en: A little giraffe learns to get up and run at 50 km/h in a few hours
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一只小长颈鹿在几小时内学会以50公里/小时的速度站起来和奔跑
- en: A truly autonomous robot learns to move in a room to get out of it
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个真正自主的机器人学会在房间里移动以走出房间
- en: The parameters of a refinery (oil pressure, flow, and so on) are set in real
    time so as to obtain the maximum yield or maximum quality
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 炼油厂的参数（如油压、流量等）在实时设置，以便获得最大产量或最大质量
- en: 'All the examples we have analyzed have the following characteristics in common:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所分析的所有例子都具有以下共同特征：
- en: Interaction with the environment
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与环境的交互
- en: Objective of the agent
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体的目标
- en: Uncertainty or partial knowledge of the environment
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境的不确定性或部分知识
- en: 'From the analysis of these examples, it is possible to make the following observations:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些例子的分析中，可以得出以下观察：
- en: The agent learns from its own experience.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体从自己的经验中学习。
- en: When the actions change the status (the situation), the possibilities of choices
    in the future change (delayed reward).
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当动作改变状态（情况）时，未来选择的可能性会改变（延迟奖励）。
- en: The effect of an action cannot be completely predicted.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作的效果不能完全预测。
- en: The agent has a global assessment of it behavior.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体对其行为的整体评估。
- en: It must exploit this information to improve his choices. Choices improve with
    experience.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须利用这些信息来改善其选择。选择随着经验而提高。
- en: Problems can have a finite or infinite time horizon.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题可以有一个有限或无限的时间范围。
- en: Markov Decision Process
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: To avoid load problems and computational difficulties, the Agent-Environment
    interaction is considered as a MDP. MDP is a discrete time stochastic control
    process. At each time step, the process is in a state *s*, and the decision maker
    may choose any action *a* that is available in state *s*. The process responds
    at the next time step by randomly moving into a new state *s'* and giving the
    decision maker a corresponding reward, *r(s,s')*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免负载问题和计算困难，智能体-环境交互被视为一个MDP。MDP是一个离散时间随机控制过程。在每一个时间步，过程处于一个状态 *s*，决策者可以选择在状态
    *s* 中可用的任何动作 *a*。在下一个时间步，过程通过随机移动到新的状态 *s'* 并给决策者一个相应的奖励，*r(s,s')* 来响应。
- en: 'Under these hypotheses, the Agent-Environment interaction can be schematized
    as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些假设下，智能体-环境交互可以概括如下：
- en: The agent and the environment interact at discrete intervals over time, *t =
    0, 1, 2, … n*.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体和环境在时间上的离散间隔内进行交互，*t = 0, 1, 2, … n*。
- en: At each interval, the agent receives a representation of the state *st* of the
    environment.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个间隔，智能体接收环境状态 *st* 的表示。
- en: Each element *s[t]* of *S*, where *S* is the set of possible states.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S* 集合中每个元素 *s[t]*，其中 *S* 是可能状态集合。'
- en: Once the state is recognized, the agent must take an action a[t] of *A(s[t])*,
    where *A(s[t])* is the set of possible actions in the state *s[t]*.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦识别出状态，智能体必须采取 *A(s[t])* 中的行动 *a[t]*，其中 *A(s[t])* 是状态 *s[t]* 中的可能行动集合。
- en: The choice of the action to be taken depends on the objective to be achieved
    and is mapped through the policy indicated with the symbol *π* (discounted cumulative
    reward), which associates the action with a[t] of *A(s)* for each state *s*. The
    term *π[t](s,a)* represents the probability that action *a* is carried out in
    the state *s*.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要采取的行动的选择取决于要实现的目标，并通过带有符号 *π*（折现累积奖励）的策略来映射，该策略将行动与 *A(s)* 中的 *a[t]* 相关联，对于每个状态
    *s*。术语 *π[t](s,a)* 表示在状态 *s* 中执行行动 *a* 的概率。
- en: During the next time interval *t + 1*, as part of the consequence of the action
    at, the agent receives a numerical reward *r[t] + 1* *R* corresponding to the
    action previously taken a[t].
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下一个时间间隔 *t + 1* 中，作为行动后果的一部分，智能体收到一个数值奖励 *r[t] + 1* *R*，对应于之前采取的行动 *a[t]*。
- en: The consequence of the action represents, instead, the new state *s[t]*. At
    this point, the agent must again code the state and make the choice of the action.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行动的后果代表的是新的状态 *s[t]*。在这个时候，智能体必须再次编码状态并选择行动。
- en: This iteration repeats itself until the achievement of the objective by the
    agent.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种迭代会一直重复，直到智能体达到目标。
- en: 'The definition of the status *s[t] + 1* depends from the previous state and
    the action taken MDP, that is:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 *s[t] + 1* 的定义取决于之前的状态和采取的行动 MDP，即：
- en: '*s[t] + 1 = δ (s[t],a[t])*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*s[t] + 1 = δ (s[t],a[t])*'
- en: Here *δ* represents the status function.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *δ* 代表状态函数。
- en: 'In summary:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说：
- en: In an MDP, the agent can perceive the status *s S* in which he is and has an
    *A* set of actions at his disposal
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个MDP中，智能体可以感知到他所处的状态 *s S*，并且有一个 *A* 集合的行动可供选择。
- en: At each discrete interval *t* of time, the agent detects the current status
    *st* and decides to implement an action at *A*
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个离散的时间间隔 *t* 中，智能体检测到当前状态 *st* 并决定在 *A* 实施行动。
- en: The environment responds by providing a reward (a reinforcement) *r[t] = r (s[t],
    a[t])* and moving into the state *s[t] + 1 = δ (s[t], a[t])*
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境通过提供奖励（强化）*r[t] = r (s[t], a[t])* 并移动到状态 *s[t] + 1 = δ (s[t], a[t])* 来做出反应。
- en: The *r* and *δ* functions are part of the environment; they depend only on the
    current state and action (not the previous ones) and are not necessarily known
    to the agent
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*r* 和 *δ* 函数是环境的一部分；它们只依赖于当前状态和行动（不是之前的），并且不一定为智能体所知。'
- en: The goal of reinforcement learning is to learn a policy that, for each state
    *s* in which the system is located, indicates to the agent an action to maximize
    the total reinforcement received during the entire action sequence
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的目标是学习一个策略，对于系统所处的每个状态 *s*，指示智能体采取一个行动以最大化在整个行动序列中收到的总强化。
- en: 'Let''s go deeper into some of the terms used:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一些使用的术语：
- en: A **reward function** defines the goal in a reinforcement learning problem.
    It maps the detected states of the environment into a single number, thus defining
    a reward. As already mentioned, the only goal is to maximize the total reward
    it receives in the long term. The reward function then defines what the good and
    bad events are for the agent. The reward function has the need to be correct,
    and it can be used as a basis for changing the policy. For example, if an action
    selected by the policy is followed by a low reward, the policy can be changed
    to select other actions in that situation in the next step.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励函数**定义了强化学习问题中的目标。它将环境检测到的状态映射到一个单一的数字，从而定义了一个奖励。如前所述，唯一的目标是在长期内最大化它所收到的总奖励。奖励函数定义了对于智能体来说什么是好事和坏事。奖励函数需要是正确的，并且可以用作改变策略的基础。例如，如果策略选择的行动导致低奖励，策略可以在下一步改变以选择其他行动。'
- en: A **policy** defines the behavior of the learning agent at a given time. It
    maps both the detected states of the environment and the actions to take when
    they are in those states. Corresponds to what in psychology would be called a
    **set of rules** or associations of stimulus response. Policy is the fundamental
    part of a reinforcing learning agent, in the sense that it alone is enough to
    determine behavior.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**定义了学习智能体在给定时间的行为。它将环境检测到的状态和在这些状态下采取的行动进行映射。对应于心理学中所谓的**规则集**或刺激反应的关联。策略是强化学习智能体的基本部分，因为它单独就足以确定行为。'
- en: A **value function** represents how good a state is for an agent. It is equal
    to the total reward expected for an agent from the status *s*. The value function
    depends on the policy with which the agent selects the actions to be performed.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值函数**表示一个状态对智能体有多好。它等于智能体从状态 *s* 预期得到的总奖励。价值函数取决于智能体选择执行的动作的策略。'
- en: Discounted cumulative reward
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 折扣累积奖励
- en: 'In the previous section, we said this: the goal of reinforcement learning is
    to learn a policy that, for each state *s* in which the system is located, indicates
    to the agent an action to maximize the total reinforcement received during the
    entire action sequence. But how can we maximize the total reinforcement received
    during the entire sequence of actions?'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们提到这一点：强化学习的目标是学习一个策略，对于系统所处的每个状态 *s*，指示智能体一个动作，以在整个动作序列中最大化收到的总强化。但如何最大化整个动作序列中收到的总强化呢？
- en: 'The total reinforcement derived from the policy is calculated as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 根据策略计算得到的总强化如下：
- en: '![](img/2bce7ad4-a3fe-4ee6-92e6-560dff764b36.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2bce7ad4-a3fe-4ee6-92e6-560dff764b36.png)'
- en: Here, *r[T]* represents the reward of the action that drives the environment
    in the terminal state *s[T]*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*r[T]* 表示将环境驱动到终端状态 *s[T]* 的动作的奖励。
- en: A possible solution to the problem is to associate the action that provides
    the highest reward to each individual state; that is, we must determine an optimal
    policy such that the previous quantity is maximized.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的可能方法是将提供最高奖励的动作与每个个体状态关联起来；也就是说，我们必须确定一个最优策略，使得之前的数量最大化。
- en: For problems that do not reach the goal or terminal state in a finite number
    of steps (continuing tasks), *R[t]* tends to infinity.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在有限步数内无法达到目标或终端状态的问题（持续任务），*R[t]* 趋向于无穷大。
- en: In these cases, the sum of the rewards that one wants to maximize diverges at
    infinity, so this approach is not applicable. Then, it is necessary to develop
    an alternative reinforcement technique.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，想要最大化的奖励总和在无穷大处发散，因此这种方法不适用。那么，就需要开发一种替代的强化技术。
- en: 'The technique that best suits the reinforcement learning paradigm turns out
    to be discounted cumulative reward, which tries to maximize the following quantity:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 适合强化学习范式的技术最终证明是折扣累积奖励，它试图最大化以下数量：
- en: '![](img/94c8312c-b46e-461c-9180-85f557269910.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/94c8312c-b46e-461c-9180-85f557269910.png)'
- en: 'Here, *γ* is called **discount factor** and it represents the importance for
    future rewards. This parameter can take the values *0 ≤ γ ≤ 1*, with the following
    meanings:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*γ* 被称为**折扣因子**，它表示对未来奖励的重要性。此参数可以取值 *0 ≤ γ ≤ 1*，具有以下含义：
- en: If *γ <1*, the sequence *r[t]* will converge to a finite value
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *γ < 1*，序列 *r[t]* 将收敛到一个有限值。
- en: If *γ = 0*, the agent will have no interest in future rewards, but will try
    to maximize the reward only for the current state
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *γ = 0*，智能体将不会对未来的奖励感兴趣，但会尝试仅最大化当前状态的奖励。
- en: If *γ = 1*, the agent will try to increase future rewards even at the expense
    of immediate ones
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *γ = 1*，智能体将试图增加未来的奖励，即使是以牺牲当前的奖励为代价。
- en: The discount factor can be modified during the learning process to highlight
    or not particular actions or states. An optimal policy can cause the reinforcement
    obtained in performing a single action to be even low (or even negative), provided
    that overall this leads to greater reinforcement.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣因子可以在学习过程中修改，以突出或忽略特定的动作或状态。一个最优策略可以使执行单个动作获得的强化甚至很低（或甚至为负），只要这总体上导致更大的强化。
- en: Exploration versus exploitation
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索与利用
- en: Ideally, the agent must associate with each action at the respective reward
    *r* in order to then choose the most rewarded behavior for achieving the goal.
    This approach is therefore impracticable for complex problems, in which the number
    of states is particularly high and consequently the possible associations increase
    exponentially.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，智能体必须将每个动作与相应的奖励 *r* 关联起来，然后选择最被奖励的行为以实现目标。因此，这种方法对于复杂问题来说不切实际，在这些问题中，状态的数量特别高，因此可能的关联呈指数增长。
- en: This problem is called the **exploration-exploitation dilemma**. Ideally, the
    agent must explore all possible actions for each state, finding the one that is
    actually most rewarded for exploiting it in achieving its goal.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题被称为**探索-利用困境**。理想情况下，智能体必须探索每个状态的所有可能动作，找到实际最被奖励的动作，以利用它来实现其目标。
- en: 'Thus, decision-making involves a fundamental choice:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，决策涉及一个基本的选择：
- en: '**Exploitation**: Make the best decision given current information'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用**：根据当前信息做出最佳决策'
- en: '**Exploration**: Collect more information'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索**：收集更多信息'
- en: In this process, the best long-term strategy can lead to considerable sacrifices
    in the short term. Therefore, it is necessary to gather enough information to
    make the best decisions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，最佳长期策略可能会在短期内带来相当大的牺牲。因此，有必要收集足够的信息来做出最佳决策。
- en: 'Here are some examples of adopting this technique for real-life cases:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些采用这种技术解决现实生活案例的例子：
- en: '**Selection of a store**:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**选择商店**：'
- en: '**Exploitation**: Go to your favorite store'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用**：前往你最喜欢的商店'
- en: '**Exploration**: Try a new store'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索**：尝试一个新的商店'
- en: '**Choice of a route**:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**选择路线**：'
- en: '**Exploitation**: Choose the best route so far'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用**：选择迄今为止的最佳路线'
- en: '**Exploration**: Try a new route'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索**：尝试一条新路线'
- en: In practice, in very complex problems, convergence to a very good strategy would
    be too slow.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，对于非常复杂的问题，收敛到非常好的策略会太慢。
- en: 'A good solution to the problem is to find a balance between exploration and
    exploitation:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的好方法是在探索和利用之间找到平衡：
- en: An agent who limits himself to exploring will always act in a casual way in
    every state, and it is evident that convergence to an optimal strategy is impossible
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个仅限于探索的代理将始终在每个状态下以随意的方式行事，显然收敛到最优策略是不可能的
- en: If an agent explores little, it will always use the usual actions, which may
    not be optimal ones
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个代理很少探索，它将始终使用常规动作，这些动作可能不是最优的
- en: Reinforcement learning techniques
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习技术
- en: 'As we have seen in the previous sections, reinforcement learning is a programming
    philosophy that aims to develop algorithms that can learn and adapt to changes
    in the environment. This programming technique is based on the assumption of being
    able to receive stimuli from the outside according to the choices of the algorithm.
    So, a correct choice will result in a prize while an incorrect choice will lead
    to a penalization of the system. The goal of the system is to achieve the highest
    possible prize and consequently the best possible result. The techniques related
    to learning by reinforcement are divided into two categories:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几节中看到的，强化学习是一种编程哲学，旨在开发能够学习和适应环境变化的算法。这种编程技术基于能够根据算法的选择从外部接收刺激的假设。因此，正确的选择将带来奖励，而错误的选择将导致系统的惩罚。系统的目标是实现尽可能高的奖励，从而获得最佳结果。与强化学习相关的技术分为两大类：
- en: '**Continuous learning algorithms**: These techniques start from the assumption
    of having a simple mechanism able to evaluate the choices of the algorithm and
    then reward or punish the algorithm depending on the result. These techniques
    can also adapt to substantial changes in the environment. An example is speech
    recognition programs or OCR programs that improve their performance with use.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续学习算法**：这些技术从假设有一个简单的机制来评估算法的选择，然后根据结果奖励或惩罚算法。这些技术也可以适应环境中的重大变化。例如，语音识别程序或OCR程序在使用过程中提高其性能。'
- en: '**Preventive training algorithms**: These algorithms start from the observation
    that constantly evaluating the actions of the algorithm can be a process that
    cannot be automated or very expensive. In this case, a first phase is applied,
    in which the algorithm is taught; when the system is considered reliable, it is
    crystallized and no more editable. Many electronic components use neural networks
    within them, and the synaptic weights of these networks are not changeable since
    they are fixed during the construction of the circuit.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预防性训练算法**：这些算法从观察出发，认为不断评估算法的行为可能是一个无法自动化或非常昂贵的流程。在这种情况下，应用一个第一阶段，在这个阶段中，算法被教授；当系统被认为可靠时，它被固定下来，就不再可编辑。许多电子组件在其内部使用神经网络，这些网络的突触权重是不可变的，因为它们在电路构建过程中是固定的。'
- en: It should be noted that the categories mentioned previously are implementation
    choices rather than conceptual differences in the algorithm. Therefore, an algorithm
    can often be in the first or second category depending on how it is implemented
    by the designer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，前面提到的类别是实现选择而不是算法概念上的差异。因此，一个算法可以根据设计者的实现方式经常位于第一类或第二类。
- en: Q-learning
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习
- en: Q-learning is one of the most-used reinforcement learning algorithms. This is
    due to its ability to compare the expected utility of the available actions without
    requiring an environment model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 是最常用的强化学习算法之一。这得益于它能够在不要求环境模型的情况下比较可用动作的预期效用。
- en: Thanks to this technique, it is possible to find an optimal action for every
    given state in a finished MDP.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这项技术，我们可以找到在完成 MDP 中每个给定状态的优化动作。
- en: 'A general solution to the reinforcement learning problem is to estimate, thanks
    to the learning process, an evaluation function. This function must be able to
    evaluate, through the sum of the rewards, the convenience or otherwise of a particular
    policy. In fact, Q-learning tries to maximize the value of the *Q* function (action-value
    function), which represents the maximum discounted future reward when we perform
    actions *a* in state *s*, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题的一般解决方案是，通过学习过程估计一个评估函数。这个函数必须能够通过奖励的总和评估特定策略的便利性或不利性。实际上，Q-learning试图最大化
    *Q* 函数（动作值函数）的值，它代表我们在状态 *s* 执行动作 *a* 时的最大折现未来奖励，如下所示：
- en: '*Q(S[t],a[t]) = max(R[t+1])*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q(S[t],a[t]) = max(R[t+1])* '
- en: 'Knowing the *Q* function, the optimal action *a* in a state *s* is the one
    with the highest *Q* value. At this point, we can define a policy *π(s)* that
    provides us with the best action in any state. Recalling that the policy *π* associates
    the pair *(s; a)* with the probability *(s; a)* that action is carried out in
    the state *s*, we can write the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 知道 *Q* 函数，状态 *s* 中的最佳动作 *a* 是具有最高 *Q* 值的动作。在这个点上，我们可以定义一个策略 *π(s)*，它为我们提供任何状态下的最佳动作。回忆一下，策略
    *π* 将 *(s; a)* 对与在状态 *s* 中执行动作的概率 *(s; a)* 相关联，我们可以写出以下内容：
- en: '![](img/0bf8dd3d-0c65-4b9a-809a-75214b472d33.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bf8dd3d-0c65-4b9a-809a-75214b472d33.png)'
- en: 'The problem is reduced to the evaluation of the *Q* function. We can then estimate
    the *Q* function for a transition point in terms of the *Q* function at the next
    point through a recursive process. The following is the equation used in a single
    step of the process. This equation is known as **Bellman''s equation** and represents
    the transition rule of Q-learning:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 问题被简化为评估 *Q* 函数。然后我们可以通过递归过程，用下一个点的 *Q* 函数来估计过渡点的 *Q* 函数。以下是在过程的单步中使用的方程。这个方程被称为
    **贝尔曼方程**，代表了 Q-learning 的转换规则：
- en: '![](img/4e95528e-5650-490d-8dce-f82aeb359907.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e95528e-5650-490d-8dce-f82aeb359907.png)'
- en: 'The terms are defined as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 术语定义如下：
- en: '*Q(s[t],a[t])* is the current policy of action *a* from state *s.*'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q(s[t],a[t])* 是从状态 *s* 出发的动作 *a* 的当前策略。'
- en: '*r* is the reward for the action.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*r* 是动作的奖励。'
- en: '*max[t+1](Q(s[t+1],a[t+1]))* defines the maximum future reward. We performed
    the *a[t]* action to state *s[t]* to reach the *s[t+1]* state. From here, we may
    have multiple actions, each corresponding to some rewards. The maximum of that
    reward is computed.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*max[t+1](Q(s[t+1],a[t+1]))* 定义了最大未来奖励。我们执行了 *a[t]* 动作，从状态 *s[t]* 到达状态 *s[t+1]*。从这里，我们可能有多个动作，每个动作对应一些奖励。计算这些奖励中的最大值。'
- en: '*γ* is the discount factor. The *γ* value varies from 0 to 1; if the value
    is near 0, an immediate reward is given preference. If it goes near 1, the importance
    of future rewards increases until 1, where it is considered equal to immediate
    rewards.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*γ* 是折扣因子。*γ* 的值在 0 到 1 之间变化；如果值接近 0，则优先考虑即时奖励。如果它接近 1，则未来奖励的重要性增加，直到 1，此时它被认为与即时奖励相等。'
- en: On the basis of the previous equation, the evaluation function *Q* is given
    by the sum of the immediate reward and the maximum reward obtainable starting
    from the next state.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程的基础上，评估函数 *Q* 是即时奖励和从下一个状态开始可获得的最高奖励的总和。
- en: 'Applying the previous formula, we are trying to formulate the delayed rewards
    into immediate rewards. We have previously said that the evaluation of the *Q*
    function represents a recursive process. We can then enter the values obtained
    during this process in a table that we will, of course, call table *Q*. In this
    table, the rows are the states and the columns are the actions. As a starting
    table *Q*, we can use a matrix containing all zeros (we have initialized table
    *Q*), as shown in the following figure:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 应用前面的公式，我们试图将延迟奖励转化为即时奖励。我们之前提到，*Q* 函数的评估代表一个递归过程。然后我们可以将这个过程中获得的价值输入到一个表格中，当然，我们将这个表格称为
    *Q* 表。在这个表格中，行是状态，列是动作。作为一个起始的 *Q* 表，我们可以使用一个包含所有零的矩阵（我们已经初始化了 *Q* 表），如下面的图所示：
- en: '![](img/31e9ee64-1d2c-48d9-a410-2db4756872d1.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31e9ee64-1d2c-48d9-a410-2db4756872d1.png)'
- en: 'The elements of this table *Q* (cells) are the rewards that are obtained if
    one is in the state given by the row and the action given by the column is executed.
    The best action to take in any state is the one with the highest reward. Our task
    now is to update this table with new values. To do this, we can adopt the following
    algorithm:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此表 *Q* 的元素（单元格）是在给定行状态和列动作的情况下获得的奖励。在任何状态下采取的最佳动作是具有最高奖励的动作。我们的任务是使用新值更新此表。为此，我们可以采用以下算法：
- en: The status *s[t]* is decoded
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码状态 *s[t]*
- en: An action *a[t]* is selected
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择动作 *a[t]*
- en: Action *a[t]* is performed and the reward *r* is received
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作 *a[t]* 并获得奖励 *r*
- en: The element of table *Q(s[t]; a[t])* is updated with the training rule provided
    by Bellman's equation
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 表 *Q(s[t]; a[t])* 的元素根据Bellman方程提供的训练规则进行更新。
- en: The execution of the action a moves the environment in the state *s[t+1]*
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作a将环境状态移动到 *s[t+1]*
- en: Set the next state as the current state (*s[t] = s[t+1]*)
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将下一个状态设置为当前状态 (*s[t] = s[t+1]*)
- en: Start again from point 1 and repeat the process until a terminal state is reached
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从点1重新开始并重复该过程，直到达到终端状态。
- en: In more complex and efficient formulations, it is possible to replace the table,
    whose iteration is still inefficient for complex problems, with a neural network
    where the learning process will change the weights of the synaptic connections.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在更复杂和高效的公式中，可以用神经网络代替迭代效率仍然不高的表，学习过程将改变突触连接的权重。
- en: Temporal difference learning
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间差分学习
- en: TD learning algorithms are based on reducing the differences between estimates
    made by the agent at different times. Q-learning, seen in the previous section,
    is a TD algorithm, but it is based on the difference between states in immediately
    adjacent instants. TD is more generic and may consider moments and states further
    away.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: TD学习算法基于减少代理在不同时间做出的估计之间的差异。在前一节中看到的Q-learning是TD算法，但它基于相邻瞬间状态之间的差异。TD更通用，可能考虑更远的时间和状态。
- en: It is a combination of the ideas of the **Monte Carlo** (**MC**) method and
    the **Dynamic Programming** (**DP**).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 它是**蒙特卡洛**（**MC**）方法和**动态规划**（**DP**）思想的结合。
- en: MC methods allow solving reinforcement learning problems based on the average
    of the results obtained.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: MC方法允许基于获得结果的平均值解决强化学习问题。
- en: DP represents a set of algorithms that can be used to calculate an optimal policy
    given a perfect model of the environment in the form of an MDP.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: DP代表一组算法，可以在给定环境完美模型（以MDP形式）的情况下计算最优策略。
- en: A TD algorithm can learn directly from raw data, without a model of the dynamics
    of the environment (such as MC). This algorithm updates the estimates based partly
    on previously learned estimates, without waiting for the final result (bootstrap,
    such as DP). It is suitable for learning without a model of dynamic environments.
    Converge using a fixed policy if the time step is sufficiently small, or if it
    reduces over time.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: TD算法可以直接从原始数据中学习，而无需环境动态模型（如MC）。该算法基于先前学习的估计部分更新估计，而不需要等待最终结果（如DP中的自举）。它适用于没有动态环境模型的场景。如果时间步长足够小，或者随着时间的推移而减少，则使用固定策略收敛。
- en: 'As we saw in the previous section, Q-learning calculates its values according
    to the following formula:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所述，Q-learning根据以下公式计算其值：
- en: '![](img/23d2a832-5cd5-4ef2-b9d9-de942401f24b.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/23d2a832-5cd5-4ef2-b9d9-de942401f24b.png)'
- en: By adopting a one-step look-ahead.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用一步前瞻。
- en: Look-ahead is the generic term for a procedure that attempts to foresee the
    effects of choosing a branching variable to evaluate one of its values. The two
    main aims of look-ahead are to choose a variable to evaluate next and the order
    of values to assign to it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 前瞻是尝试预测选择一个分支变量以评估其值的影响的通用术语。前瞻的两个主要目标是选择下一个要评估的变量以及分配给它的值的顺序。
- en: 'It is clear that a two-step formula can also be used, as shown in the following
    line:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，也可以使用两步公式，如下所示：
- en: '![](img/dcdee0c9-9322-4a98-89ad-1cadf49d66e6.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dcdee0c9-9322-4a98-89ad-1cadf49d66e6.png)'
- en: 'More generally with n-step look-ahead, we obtain the following formula:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，使用n步前瞻，我们得到以下公式：
- en: '![](img/ad4e7e0d-46c9-493b-b774-b8b5d7b84fe1.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ad4e7e0d-46c9-493b-b774-b8b5d7b84fe1.png)'
- en: Dynamic Programming
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态规划
- en: DP represents a set of algorithms that can be used to calculate an optimal policy
    given a perfect model of the environment in the form of a MDP. The fundamental
    idea of DP, as well as reinforcement learning in general, is the use of state
    values and actions, to look for good policies.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: DP代表一组算法，可以用来在给定环境完美模型（以MDP形式）的情况下计算最优策略。DP的基本思想，以及一般强化学习，是使用状态值和动作来寻找好的策略。
- en: The DP methods approach the resolution of Markov decision-making processes through
    the iteration of two processes called **policy evaluation** and **policy improvement**.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 'DP方法通过迭代两个称为**政策评估**和**政策改进**的过程来接近解决马尔可夫决策过程。 '
- en: Policy evaluation algorithm consists in applying an iterative method to the
    resolution of the Bellman equation. Since convergence is guaranteed to us only
    for *k → ∞*, we must be content to have good approximations by imposing a stopping
    condition.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政策评估算法在于将迭代方法应用于Bellman方程的求解。由于只有当*k → ∞*时我们才能保证收敛，我们必须满足于通过设置停止条件来获得良好的近似值。
- en: Policy improvement algorithm improves policy based on current values.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政策改进算法基于当前值来改进策略。
- en: A disadvantage of the policy iteration algorithm is that we have to evaluate
    a policy at every step. This involves an iterative process whose time of convergence
    we do not know a priori. This will depend on, among other things, how the starting
    policy was chosen.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 政策迭代算法的一个缺点是我们必须在每一步评估策略。这涉及一个迭代过程，其收敛时间我们事先不知道。这将取决于许多其他因素，包括起始策略的选择。
- en: One way to overcome this drawback is to cut off the evaluation of the policy
    at a specific step. This operation does not change the guarantee of convergence
    to the optimal value. A special case in which the assessment of the policy is
    blocked by a step by state (also called **sweep**) defines the value iteration
    algorithm. In the value iteration algorithm, a single iteration of calculation
    of the values is performed between each step of the policy improvement.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这种缺点的一种方法是在特定步骤中断策略的评估。这种操作不会改变收敛到最优值的保证。一种特殊情况是，通过状态（也称为**sweep**）的步骤阻止策略评估，定义了值迭代算法。在值迭代算法中，在策略改进的每一步之间执行一次值的计算迭代。
- en: 'The DP algorithms are therefore essentially based on two processes that take
    place in parallel: policy evaluation and policy improvement. The repeated execution
    of these two processes makes the general process converge towards the optimal
    solution. In the policy iteration algorithm the two phases alternate and each
    ends before the other begins.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DP算法基本上基于两个并行发生的过程：政策评估和政策改进。这两个过程的重复执行使整个过程收敛到最优解。在策略迭代算法中，这两个阶段交替进行，每个阶段都在另一个开始之前结束。
- en: DP methods operate through the entire set of states that can be assumed by the
    environment, performing a complete backup for each state at each iteration. Each
    update operation performed by the backup updates the value of a status based on
    the values ​​of all possible successor states, weighted for their probability
    of occurrence and induced by the policy of choice and dynamics of the environment.
    Full backups are closely related to the Bellman equation; they are nothing more
    than the transformation of the equation into assignment instructions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: DP方法在整个环境可能假设的状态集中操作，在每个迭代中对每个状态进行完全备份。备份操作中执行的每次更新操作都是基于所有可能的后继状态值，这些值根据它们发生的概率加权，并受到选择策略和环境动态的影响。完全备份与Bellman方程密切相关；它们不过是将方程转换为赋值指令。
- en: When a complete backup iteration does not bring any change to the state values,
    convergence is obtained; therefore the final state values ​​fully satisfy the
    Bellman equation. The DP methods are applicable only if there is a perfect model
    of the alternator, which must be equivalent to a MDP.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当完整的备份迭代不会对状态值带来任何变化时，就达到了收敛；因此，最终状态值完全满足Bellman方程。DP方法仅在存在交替器的完美模型时适用，该模型必须等同于MDP。
- en: Precisely for this reason, the DP algorithms are of little use in reinforcement
    learning, both for their assumption of a perfect model of the environment, and
    for the high and expensive computation, but it is still opportune to mention them
    because they represent the theoretical basis of reinforcement learning. In fact,
    all the methods of reinforcement learning try to achieve the same goal of the
    DP methods, only with lower computational cost and without the assumption of a
    perfect model of the environment.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 正是因为这个原因，动态规划算法在强化学习中用处不大，这不仅是因为它们假设了一个完美的环境模型，还因为计算成本高且昂贵。但仍然有必要提及它们，因为它们代表了强化学习的理论基础。实际上，所有强化学习方法都试图实现动态规划方法相同的目标，只是计算成本更低，且不假设一个完美的环境模型。
- en: The DP methods converge to the optimal solution with a number of polynomial
    operations with respect to the number of states *n* and actions *m*, against the
    number of exponential operations *m*n* required by methods based on direct search.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划方法相对于状态数量 *n* 和动作数量 *m* 的多项式操作次数收敛到最优解，而基于直接搜索的方法则需要指数操作次数 *m*n*。
- en: The DP methods update the estimates of the values of the states, based on the
    estimates of the values of the successor states; or they update the estimates
    on the basis of past estimates. This represents a special property, which is called
    **bootstrapping**. Several methods of reinforcement learning perform bootstrapping,
    even methods that do not require a perfect model of the environment, as required
    by the DP methods.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划方法基于后续状态的值估计来更新状态的值估计；或者基于过去的估计来更新。这代表了一种特殊属性，称为**自助法**。几种强化学习方法执行自助法，甚至是不需要像动态规划方法那样要求完美环境模型的方法。
- en: Monte Carlo methods
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法
- en: MC methods for estimating the value function and discovering excellent policies
    do not require the presence of a model of the environment. They are able to learn
    through the use of the agent's experience alone or from samples of state sequences,
    actions, and rewards obtained from the interactions between agent and environment.
    The experience can be acquired by the agent in line with the learning process
    or emulated by a previously populated dataset. The possibility of gaining experience
    during learning (online learning) is interesting because it allows obtaining excellent
    behavior even in the absence of a priori knowledge of the dynamics of the environment.
    Even learning through an already populated experience dataset can be interesting,
    because if combined with online learning, it makes automatic policy improvement
    induced by others' experiences possible.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 用于估计值函数和发现优秀策略的蒙特卡洛方法不需要环境模型的存在。它们能够仅通过代理的经验或从代理与环境交互中获得的状态序列、动作和奖励的样本来学习。经验可以通过代理在符合学习过程的情况下获得，或者通过预先填充的数据集来模拟。在学习过程中获得经验（在线学习）的可能性很有趣，因为它即使在缺乏对环境动力学先验知识的情况下，也能获得优秀的行为。即使通过已经填充的经验数据集进行学习也可能很有趣，因为如果与在线学习相结合，它使得通过他人的经验引起的自动策略改进成为可能。
- en: To solve the reinforcement learning problems, MC methods estimate the value
    function on the basis of the total sum of rewards, obtained on average in the
    past episodes. This assumes that the experience is divided into episodes, and
    that all episodes are composed of a finite number of transitions. This is because
    in MC methods, only once an episode is completed takes place the estimate of the
    new values ​​and the modification of the policy. MC methods iteratively estimate
    policy and value function. In this case, however, each iteration cycle is equivalent
    to completing an episode—the new estimates of policy and value function occur
    episode by episode.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决强化学习问题，蒙特卡洛方法基于过去回合中获得的总奖励的平均值来估计值函数。这假设经验被划分为回合，并且所有回合都由有限数量的转换组成。这是因为，在蒙特卡洛方法中，只有当回合完成后，才会进行新值的估计和策略的修改。蒙特卡洛方法迭代地估计策略和值函数。然而，在这种情况下，每个迭代周期相当于完成一个回合——策略和值函数的新估计是逐个回合发生的。
- en: Usually the term MC is used for estimation methods, which operations involve
    random components; in this case, MC refers to reinforcement learning methods based
    on total reward averages. Unlike the DP methods that calculate the values ​​for
    each state, the MC methods calculate the values ​​for each state-action pair,
    because in the absence of a model, only state values ​​are not sufficient to decide
    which action is best performed in a certain state.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，MC术语用于估计方法，其中涉及随机成分的操作；在这种情况下，MC指的是基于总奖励平均值的强化学习方法。与计算每个状态值的DP方法不同，MC方法计算每个状态-动作对的值，因为在没有模型的情况下，只有状态值不足以决定在某个状态下执行哪种动作最好。
- en: Deep Q-Network
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q网络
- en: '**Deep Q-Network** (**DQN**) algorithms combine both the reinforcement learning
    approach and the deep learning approach. DQN learns by itself, learning in an
    empirical way and without a rigid programming aimed at a particular objective,
    such as winning a game of chess.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度Q网络**（**DQN**）算法结合了强化学习和深度学习的方法。DQN通过自我学习，以经验方式学习，而不需要针对特定目标（如赢得棋类游戏）的严格编程。'
- en: DQN represents an application of Q-learning with the use of deep learning for
    the approximation of the evaluation function. The DQN was proposed by Mnih et
    al. through an article published in *Nature* on February 26, 2015\. As a consequence,
    a lot of research institutes joined this field, because deep neural networks can
    empower reinforcement learning algorithms to directly deal with high-dimensional
    states.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: DQN代表了一种使用深度学习来近似评估函数的Q学习的应用。DQN是由Mnih等人于2015年2月26日在《自然》杂志上发表的文章中提出的。因此，许多研究机构加入了这个领域，因为深度神经网络可以使强化学习算法能够直接处理高维状态。
- en: 'The use of deep neural networks is due to the fact that researchers noted the
    following: using a neural network to approximate the **Q-evaluation** function
    in algorithms with reinforcement learning made the system unstable or divergent.
    In fact, it is possible to notice that small updates to *Q* can significantly
    change the policy, distribution of data, and correlations between *Q* and target
    values. These correlations, present in the sequence of observations, are the cause
    of the instability of the algorithms.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络的使用是由于研究人员注意到了以下事实：使用神经网络来近似强化学习算法中的**Q评估**函数使得系统不稳定或发散。实际上，可以注意到对*Q*的小更新可以显著改变策略、数据分布以及*Q*和目标值之间的相关性。这些相关性存在于观察序列中，是算法不稳定的原因。
- en: 'To transform a normal Q-network into a DQN, it is necessary to carry out the
    following precautions:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要将一个普通的Q网络转换为DQN，需要采取以下预防措施：
- en: Replace the single-level neural network with a multi-level convolutional network
    for approximation of the Q-function evaluation
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用多层卷积网络替换单层神经网络来近似Q函数评估
- en: Implement the experience replay
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现经验回放
- en: Use a second network to calculate the target Q-values during your updates
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在更新期间使用第二个网络来计算目标Q值
- en: What is meant by the term **experience replay**? This means that, instead of
    running Q-learning on state/action pairs as they occur during a simulation or
    actual experience, the system stores the data discovered, typically in a large
    table. In this way, our network can train itself using stored memories from its
    experience.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: “经验回放”这个术语是什么意思？这意味着，不是在模拟或实际经验中发生时运行Q学习在状态/动作对上，系统会存储发现的数据，通常在一个大表中。这样，我们的网络可以使用存储的经验记忆来自我训练。
- en: OpenAI Gym
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym
- en: '**OpenAI Gym** is a library that helps us to implement algorithms based on
    reinforcement learning. It includes a growing collection of benchmark issues that
    expose a common interface, and a website where people can share their results
    and compare algorithm performance.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenAI Gym**是一个帮助我们实现基于强化学习算法的库。它包括一个不断增长的基准问题集合，这些问题提供了一个公共接口，以及一个网站，人们可以在那里分享他们的结果并比较算法性能。'
- en: OpenAI Gym focuses on the episodic setting of reinforced learning. In other
    words, the agent's experience is divided into a series of episodes. The initial
    state of the agent is randomly sampled by a distribution, and the interaction
    proceeds until the environment reaches a terminal state. This procedure is repeated
    for each episode, with the aim of maximizing the total reward expectation per
    episode and achieving a high level of performance in the fewest possible episodes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym专注于强化学习的场景设置。换句话说，代理的经验被划分为一系列的场景。代理的初始状态由一个分布随机采样，直到环境达到终端状态，交互过程才继续。这个程序为每个场景重复进行，目的是最大化每个场景的总奖励期望，并在尽可能少的场景中实现高水平的表现。
- en: 'Gym is a toolkit for developing and comparing reinforcement learning algorithms.
    It supports the ability to teach agents everything from walking to playing games
    such as Pong or Pinball. The library is available at the following URL:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Gym是开发比较强化学习算法的工具包。它支持教代理从走路到玩像Pong或弹球这样的游戏的一切。该库可在以下URL获取：
- en: '[https://gym.openai.com/](https://gym.openai.com/)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://gym.openai.com/](https://gym.openai.com/)'
- en: 'The following figure shows the home page of the OpenAI Gym project site:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了OpenAI Gym项目网站的首页：
- en: '![](img/2f87cdac-c992-470e-8e3e-1301f466cbe7.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2f87cdac-c992-470e-8e3e-1301f466cbe7.png)'
- en: 'OpenAI Gym is part of a much more ambitious project: the OpenAI project. OpenAI
    is an **artificial intelligence** (**AI**) research company founded by Elon Musk
    and Sam Altman. It is a non-profit project that aims to promote and develop friendly
    AI in such a way as to benefit humanity as a whole. The organization aims to collaborate
    freely with other institutions and researchers by making their patents and research
    open to the public. The founders decided to undertake this project as they were
    concerned by the existential risk deriving from the indiscriminate use of AI.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym是更大胆的项目——OpenAI项目的一部分。OpenAI是由埃隆·马斯克和山姆·奥特曼共同创立的人工智能研究公司。这是一个非营利项目，旨在以造福全人类的方式促进和开发友好的人工智能。该组织旨在通过使他们的专利和研究对公众开放，自由地与其他机构和研究人员合作。创始人决定承担这个项目，因为他们对来自人工智能无差别使用的存在风险感到担忧。
- en: OpenAI Gym is a library of programs that allow you to develop AIs, measure their
    intellectual abilities ,and enhance their learning abilities. In short, a Gym
    in the form of algorithms that trains the present digital brains to OpenAI Gym
    project them into the future.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym是一个程序库，允许你开发人工智能，衡量它们的智力能力，并增强它们的学习能力。简而言之，这是一个以算法形式存在的Gym，它训练当前的数字大脑，并将它们投射到OpenAI
    Gym项目中。
- en: But there is also another goal. OpenAI wants to stimulate research in the AI
    ​​sector by funding projects that make humanity progress even in those fields
    where there is no economic return. With Gym, on the other hand, it intends to
    standardize the measurement of AI so that researchers can compete on equal terms
    and know where their colleagues have come but, above all, focus on results that
    are really useful for everyone.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 但还有一个目标。OpenAI希望通过资助那些即使在经济上没有回报也能让人类进步的项目来刺激人工智能领域的研究。另一方面，Gym旨在标准化人工智能的测量，以便研究人员可以在平等的基础上竞争，了解他们的同事已经取得了哪些进展，但最重要的是，关注真正对所有人都有用的结果。
- en: The tools available are many. From the ability to play old video games like
    Pong to that of fighting in the GO to control a robot, we just enter our algorithm
    in this digital place to see how it works. The second step is to compare the benchmarks
    obtained with the other ones to see where we stand compared to others, and maybe
    we can collaborate with them to get mutual benefits.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的工具很多。从玩像Pong这样的老式视频游戏到在围棋中战斗，再到控制机器人，我们只需将我们的算法输入这个数字空间，看看它是如何工作的。第二步是将获得的基准与其他基准进行比较，看看我们与其他人的差距，也许我们可以与他们合作，实现互利共赢。
- en: OpenAI Gym makes no assumptions about the structure of our agent and is compatible
    with any numerical computation library, such as TensorFlow or Theano. The Gym
    library is a collection of test problems—environments—that we can use to work
    out our reinforcement learning algorithms. These environments have a shared interface,
    allowing you to write general algorithms.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym对我们的代理结构没有假设，并且与任何数值计算库兼容，例如TensorFlow或Theano。Gym库是我们可以使用来测试我们的强化学习算法的测试问题——环境。这些环境有一个共享的接口，允许你编写通用算法。
- en: 'To install OpenAI Gym, make sure you have previously installed a Python 3.5+
    version; then simply type the following command:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 OpenAI Gym，请确保您之前已安装了 Python 3.5+ 版本；然后只需输入以下命令：
- en: '[PRE0]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once this is done, we will be able to insert the tools made available by the
    library in a simple and immediate way.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，我们将能够以简单直接的方式插入库提供的工具。
- en: Cart-Pole system
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cart-Pole 系统
- en: 'The Cart-Pole system is a classic problem of reinforced learning. The system
    consists of a pole (which acts like an inverted pendulum) attached to a cart via
    a joint, as shown in the following figure:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Cart-Pole 系统 是强化学习的一个经典问题。该系统由一个杆（类似于倒立摆）通过一个关节连接到车上，如图所示：
- en: '![](img/7f3ae859-2e08-4728-813d-8d0e2b489e6c.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f3ae859-2e08-4728-813d-8d0e2b489e6c.png)'
- en: The system is controlled by applying a force of +1 or -1 to the cart. The force
    applied to the cart can be controlled, and the objective is to swing the pole
    upwards and stabilize it. This must be done without the cart falling to the ground.
    At every step, the agent can choose to move the cart left or right, and it receives
    a reward of 1 for every time step that the pole is balanced. If the pole ever
    deviates by more than 15 degrees from upright, then the procedure ends.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统通过向车上施加 +1 或 -1 的力来控制。可以控制施加到车上的力，目标是使杆向上摆动并稳定它。这必须在不让车掉到地面上完成。在每一步，智能体可以选择将车向左或向右移动，并且每当杆平衡时，它都会收到
    1 的奖励。如果杆偏离垂直方向超过 15 度，则程序结束。
- en: 'To run the Cart-Pole example using the OpenAI Gym library, simply type the
    following code:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 OpenAI Gym 库运行 Cart-Pole 示例，只需输入以下代码：
- en: '[PRE1]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As always, we will explain the meaning of each line of code in detail. The
    first line is used to import the `gym` library:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们将详细解释每一行代码的含义。第一行用于导入 `gym` 库：
- en: '[PRE2]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we move on to create the environment by calling the `make` method:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续通过调用 `make` 方法创建环境：
- en: '[PRE3]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This method creates the environment that our agent will run in. An environment
    is a problem with a minimal interface that an agent can interact with. The environments
    in OpenAI Gym are designed in order to allow objective testing and benchmarking
    of an agent's abilities. The Gym library comes with a diverse suite of environments
    that range from easy to difficult and involve many different kinds of data.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法创建我们的智能体将运行的虚拟环境。环境是一个具有最小接口的问题，智能体可以与之交互。OpenAI Gym 中的环境设计是为了允许对智能体能力的客观测试和基准测试。Gym
    库附带了一系列从简单到困难、涉及多种不同类型数据的环境。
- en: 'For a list of the available environments, refer to the following link:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取可用环境的列表，请参阅以下链接：
- en: '[https://gym.openai.com/envs](https://gym.openai.com/envs)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://gym.openai.com/envs](https://gym.openai.com/envs)'
- en: 'The most used environments are listed here:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的环境列在这里：
- en: '**Classic control and toy text**: Complete small-scale tasks, mostly from the
    reinforcement learning literature. They''re here to get you started.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经典控制和玩具文本**：完成小规模任务，主要来自强化学习文献。它们在这里是为了让您开始。'
- en: '**Algorithmic**: Perform computations such as adding multi-digit numbers and
    reversing sequences.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法**：执行加多位数和反转序列等计算。'
- en: '**Atari**: Play classic Atari games.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Atari**：玩经典 Atari 游戏。'
- en: '**2D and 3D robots**: Control a robot in simulation.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2D 和 3D 机器人**：在模拟中控制机器人。'
- en: 'In our case we have called the CartPole-v0 environment. The `make` method returns
    an `env` object that we will use to interact with the game. But let''s go back
    to analyzing the code. Now we have to initialize the system using the `reset()`
    method:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们已将 CartPole-v0 环境命名为。`make` 方法返回一个 `env` 对象，我们将使用它来与游戏交互。但让我们回到分析代码。现在我们必须使用
    `reset()` 方法初始化系统：
- en: '[PRE4]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This method puts the environment into its initial state, returning an array
    that describes it. At this point, we will use a `for` loop to run an instance
    of the CartPole-v0 environment for `1000` time steps, rendering the environment
    at each step:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将环境置于其初始状态，返回描述它的数组。在此阶段，我们将使用 `for` 循环运行 CartPole-v0 环境的实例 `1000` 次时间步，并在每一步渲染环境：
- en: '[PRE5]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Calling the `render()` method will visually display the current state, while
    subsequent calls to `env.step()` will allow us to interact with the environment,
    returning the new states in response to the actions with which we call it.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `render()` 方法将可视化显示当前状态，而后续对 `env.step()` 的调用将允许我们与环境交互，并返回对调用它的动作的响应的新状态。
- en: 'In this way, we have adopted random actions at each step. At this point, it
    is certainly useful to know what actions we are doing on the environment to decide
    future actions. The `step()` method returns exactly this. In effect, this method
    returns the following four values:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们在每一步都采用了随机动作。在这个时候，了解我们对环境所采取的动作以决定未来的动作是非常有用的。`step()` 方法正是返回这个信息。实际上，这个方法返回以下四个值：
- en: '`observation (object)`: An environment-specific object representing your observation
    of the environment.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`observation (object)`: 代表你对环境观察的环境特定对象。'
- en: '`reward (float)`: Amount of reward achieved by the previous action. The scale
    varies between environments, but the goal is always to increase your total reward.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reward (float)`: 上一个动作获得的奖励量。这个量在环境中变化，但目标始终是增加你的总奖励。'
- en: '`done (boolean)`: Whether it''s time to reset the environment again. Most (but
    not all) tasks are divided into well-defined episodes, and `done` being `True`
    indicates the episode has terminated.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`done (boolean)`: 是否是时候重新设置环境了。大多数（但不是所有）任务被划分为定义良好的剧集，`done` 为 `True` 表示剧集已结束。'
- en: '`info (dict)`: Diagnostic information useful for debugging. It can sometimes
    be useful for learning.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info (dict)`: 用于调试的诊断信息。有时它对学习很有用。'
- en: 'To run this simple example, save the code in a file named `cart.py` and type
    the following command at the bash window:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这个简单的示例，将代码保存到名为 `cart.py` 的文件中，并在 bash 窗口中输入以下命令：
- en: '[PRE6]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this way, a window will be displayed containing our system that is not stable
    and will soon go out of the screen. This is because the push to the cart is given
    randomly, without taking into account the position of the pole.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，就会显示一个包含我们系统（不稳定且很快会超出屏幕）的窗口。这是因为对小车施加的推力是随机的，没有考虑到杆的位置。
- en: To solve the problem, that is, to balance the pole, it is therefore necessary
    to set the push in the opposite direction to the inclination of the pole. So,
    we have to set only two actions, -1 or +1, pushing the cart to the left or the
    right. But in order to do so, we need to know at all times the data deriving from
    the observation of the environment. As we have already said, these pieces of data
    are returned by the `step()` method, in particular they are contained in the observation
    object.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决问题，即平衡杆，因此必须将推力设置在杆倾斜的相反方向。所以，我们只需要设置两种动作，-1 或 +1，将小车推向左边或右边。但为了做到这一点，我们需要随时了解来自环境观察的数据。正如我们之前所说的，这些数据是由
    `step()` 方法返回的，特别是它们包含在观察对象中。
- en: 'This object contains the following parameters:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此对象包含以下参数：
- en: Cart position
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小车位置
- en: Cart velocity
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小车速度
- en: Pole angle
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆的角度
- en: Pole velocity at tip
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆尖的速度
- en: 'These four values become the input of our problem. As we have also anticipated,
    the system is balanced by applying a push to the cart. There are two possible
    options:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个值成为我们问题的输入。正如我们之前预料的，通过向小车施加推力来平衡系统。有两种可能的选择：
- en: Push the cart to the left (0)
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向左推小车（0）
- en: Push it to the right (1)
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向右推（1）
- en: 'It is clear that this is a binary classification problem: four inputs and a
    single binary output.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这是一个二元分类问题：四个输入和一个单一的二元输出。
- en: 'Let us first consider how to extract the values to be used as input. To extract
    these parameters, we just have to change the preceding proposed code:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑如何提取作为输入的值。为了提取这些参数，我们只需更改前面提出的代码：
- en: '[PRE7]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By running the code, we can see that the values contained in the observation
    object are now printed on the screen. All this will be useful soon.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行代码，我们可以看到观察对象中包含的值现在被打印在屏幕上。所有这些很快就会变得有用。
- en: 'Using values returned from the environment observations, the agent has to decide
    on one of two possible actions: to move the cart left or right.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用从环境观察返回的值，智能体必须决定采取两种可能动作之一：将小车向左或向右移动。
- en: Learning phase
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习阶段
- en: Now we have to face the most demanding phase, namely the training of our system.
    In the previous section, we said that the Gym library is focused on the episodic
    setting of reinforced learning. The agent's experience is divided into a series
    of episodes. The initial state of the agent is randomly sampled by a distribution
    and the interaction proceeds until the environment reaches a terminal state. This
    procedure is repeated for each episode with the aim of maximizing the total reward
    expectation per episode and achieving a high level of performance in the fewest
    possible episodes.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须面对最具挑战性的阶段，即我们系统的训练。在前一节中，我们提到 Gym 库专注于强化学习的周期性设置。智能体的经验被划分为一系列的周期。智能体的初始状态由一个分布随机采样，交互过程一直进行到环境达到终端状态。这个程序为每个周期重复进行，目的是最大化每个周期的总奖励期望值，并在尽可能少的周期内达到高水平的表现。
- en: In the learning phase, we must estimate an evaluation function. This function
    must be able to evaluate, through the sum of the rewards, the convenience or otherwise
    of a particular policy. In other words, we must approximate the evaluation function.
    How can we do? One solution is to use an artificial neural network as a function
    approximator.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习阶段，我们必须估计一个评估函数。这个函数必须能够通过奖励的总和来评估特定策略的便利性或其他方面。换句话说，我们必须近似评估函数。我们如何做？一个解决方案是使用人工神经网络作为函数近似器。
- en: Recall that the training of a neural network aims to identify the weights of
    the connections between neurons. In this case, we will choose random values with
    weights for each episode. At the end, we will choose the combination of weights
    that will have collected the maximum reward.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，神经网络训练的目的是识别神经元之间连接的权重。在这种情况下，我们将为每个周期选择随机的权重值。最后，我们将选择收集到最大奖励的权重组合。
- en: The state of the system at a given moment is returned to us by the observation
    object. To choose an action from the actual state, we can use a linear combination
    of the weights and the observation. This is one of the most important special
    cases of function approximation, in which the approximate function is a linear
    function of the weight vector *w*. For every state *s*, there is a real-valued
    vector *x(s)* with the same number of components as *w*. Linear methods approximate
    the state-value function by the inner product between *w* and *x(s)*.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在某一时刻的系统状态由观察对象返回给我们。为了从实际状态中选择一个动作，我们可以使用权重和观察的线性组合。这是函数近似的最重要特殊情况之一，其中近似函数是权重向量
    *w* 的线性函数。对于每个状态 *s*，都有一个与 *w* 具有相同数量的分量的实值向量 *x(s)*。线性方法通过 *w* 和 *x(s)* 的内积来近似状态值函数。
- en: 'In this way, we have specified the methodology that we intend to adopt for
    the solution of the problem. Now, to make the whole training phase easily understandable,
    we report the whole code block and then comment on it in detail on a line-by-line
    basis:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，我们已经指定了我们打算采用的方法来解决问题。现在，为了使整个训练阶段易于理解，我们报告整个代码块，然后逐行详细注释：
- en: '[PRE8]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The first part of the code deals with importing the libraries:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第一部分处理导入库：
- en: '[PRE9]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we move on to create the environment by calling the `make` method:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续通过调用 `make` 方法创建环境：
- en: '[PRE10]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This method creates the environment that our agent will run in. Now let''s
    initialize the parameters we will use:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法创建了我们的智能体将运行的 环境。现在让我们初始化我们将使用的参数：
- en: '[PRE11]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`HighReward` will contain the maximum reward obtained up to the current episode;
    this value will be used as a comparison value. `BestWeights` will contain the
    sequence of weights that will have registered the maximum reward. We can now implement
    the best weight sequence search through an iterative procedure for episodes:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`HighReward` 将包含到目前为止获得的最高奖励；这个值将用作比较值。`BestWeights` 将包含将记录最高奖励的权重序列。我们现在可以通过对每个周期的迭代过程实现最佳权重序列搜索：'
- en: '[PRE12]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We decide to execute the procedure `200` times, so we initialize the system
    using the `reset()` method:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定执行该过程 `200` 次，因此我们使用 `reset()` 方法初始化系统：
- en: '[PRE13]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In each episode, we use a sequence of weights equal in number to the observations
    of the environment, which as previously said is four (cart position, cart velocity,
    pole angle, and pole velocity at tip):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个周期中，我们使用与环境的观察数量相等的权重序列，正如之前所说的，这是四个（小车位置、小车速度、杆角度和杆尖端速度）：
- en: '[PRE14]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: To fix the weights, we have used the `np.random.uniform()` function. This function
    draws samples from a uniform distribution. Samples are uniformly distributed over
    the half-open interval (low and high). It includes low but excludes high.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了固定权重，我们使用了`np.random.uniform()`函数。此函数从均匀分布中抽取样本。样本在半开区间（低和高）上均匀分布。它包括低但不包括高。
- en: 'In other words, any value within the given interval is equally likely to be
    drawn by a uniform distribution. Three parameters have been passed: the lower
    boundary of the output interval, its upper boundary, and the output shape. In
    our case we requested four random values in the interval `(-1,1)`. After doing
    this we initialize the sum of the rewards:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在给定的区间内，任何值都有可能被均匀分布抽取。已经传递了三个参数：输出区间的下界，其上界，以及输出形状。在我们的情况下，我们请求在区间`(-1,1)`内生成四个随机值。完成此操作后，我们初始化奖励的总和：
- en: '[PRE15]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'At this point, we implement another iterative cycle to determine the maximum
    reward we can get with these weights:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们实现另一个迭代周期，以确定使用这些权重可以获得的最大奖励：
- en: '[PRE16]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Calling the `render()` method will visually display the current state:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`render()`方法将可视化显示当前状态：
- en: '[PRE17]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we have to decide the `action`:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须决定`action`：
- en: '[PRE18]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As we said, to decide the action we have used a linear combination of two vectors:
    `weights` and `observation`. To perform a linear combination, we have used the
    `np.matmul()` function; it implements matrix product of two arrays. So, if this
    product is `<0`, then `action` is 0 (move left); otherwise, `action` is 1 (move
    right).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说的，为了决定动作，我们使用了两个向量的线性组合：`weights`和`observation`。为了执行线性组合，我们使用了`np.matmul()`函数；它实现了两个数组的矩阵乘积。因此，如果这个乘积是`<0`，则`action`是0（向左移动）；否则，`action`是1（向右移动）。
- en: It should be noted that a negative product means that the pole is tilted to
    the left, so in order to balance this trend, it is necessary to push the cart
    towards the left. A positive product means the pole is tilted to the right, so
    in order to balance this trend, it is necessary to push the cart towards the right.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，负乘积意味着杆倾斜到左边，因此为了平衡这种趋势，有必要将小车推向左边。正乘积意味着杆倾斜到右边，因此为了平衡这种趋势，有必要将小车推向右边。
- en: 'Now we use the `step()` method to return the new states in response to the
    actions with which we call it. Obviously, the action we pass to the method is
    the one we have just decided:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用`step()`方法来返回我们调用它时采取的动作对应的新状态。显然，我们传递给方法的动作是我们刚刚决定的：
- en: '[PRE19]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As we said, this method returns the following four values:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说的，此方法返回以下四个值：
- en: '`observation` (`object`): An environment-specific object representing your
    observation of the environment.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`observation`（`对象`）：一个特定于环境的对象，代表你对环境的观察。'
- en: '`reward` (`float`): The amount of reward achieved by the previous action. The
    scale varies between environments, but the goal is always to increase your total
    reward.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reward`（`浮点数`）：前一个动作获得的奖励量。在不同的环境中，其比例不同，但目标始终是增加你的总奖励。'
- en: '`done` (`boolean`): Whether it''s time to reset the environment again. Most
    (but not all) tasks are divided into well-defined episodes, and `done` being `True`
    indicates that the episode has terminated.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`done`（`布尔值`）：是否是时候再次重置环境了。大多数（但不是所有）任务被划分为定义良好的剧集，`done`为`True`表示剧集已结束。'
- en: '`info` (`dict`): Diagnostic information useful for debugging. It can sometimes
    be useful for learning.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info`（`字典`）：用于调试的诊断信息。有时它对学习很有用。'
- en: 'We can then update the sum of the rewards with the one just obtained. Remember
    that, for every time step where we keep the pole straight, we get +1 `reward`:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用刚刚获得的奖励更新奖励的总和。记住，对于每次我们保持杆直立的时间步，我们都会获得+1的`reward`：
- en: '[PRE20]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We just have to print the values obtained in this step:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需打印出在此步骤中获得的价值：
- en: '[PRE21]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'At the end of the current iteration, we can make a comparison to check whether
    the total reward obtained is the highest one obtained so far:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前迭代的末尾，我们可以进行比较，以检查获得的总奖励是否是迄今为止获得的最高的：
- en: '[PRE22]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If it is the highest reward obtained so far, update the `HighReward` parameter
    with this value:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是迄今为止获得的最大奖励，则使用此值更新`HighReward`参数：
- en: '[PRE23]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once this is done, fix the sequence of `Weights` of the current step as the
    best one:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，将当前步骤的`Weights`序列固定为最佳序列：
- en: '[PRE24]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: With this instruction, the training phase ends, which will give us the sequence
    of weights that best approximate the evaluation function. We can now test the
    system.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这条指令，训练阶段结束，这将给我们提供最佳逼近评估函数的权重序列。我们现在可以测试系统。
- en: Testing phase
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试阶段
- en: When the training phase is achieved, in practice it means that we have found
    the sequence of weights that best approximates this function, that is, the one
    that has returned the best reward achievable. Now we have to test the system with
    these values to check whether the pole is able to stand for at least `100` time
    steps.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练阶段完成时，在实践中这意味着我们已经找到了最佳逼近该函数的权重序列，即返回了可实现的最佳奖励的那个。现在我们必须用这些值测试系统，以检查杆是否能在至少`100`个时间步内保持平衡。
- en: 'Now, as we are already done in the training phase, to make the whole testing
    phase easily understandable, we report the whole code block and then comment on
    it in detail on a line-by-line basis:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，因为我们已经完成了训练阶段，为了使整个测试阶段易于理解，我们报告整个代码块，然后逐行详细注释：
- en: '[PRE25]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'First, we have to initialize the system once again, using the `reset()` method:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须再次使用`reset()`方法初始化系统：
- en: '[PRE26]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, we have to run an iterative cycle to apply the results obtained in the
    training phase:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须运行一个迭代周期来应用训练阶段获得的结果：
- en: '[PRE27]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'For each step, we will call the `render()` method to visually display the current
    state:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一步，我们将调用`render()`方法来可视化显示当前状态：
- en: '[PRE28]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we have to decide the action to perform on the system based on the best
    weights obtained in the training phase and on the observations of the current
    state:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须根据训练阶段获得的最佳权重和当前状态下的观察结果来决定对系统执行的动作：
- en: '[PRE29]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now we use the `step()` method that returning the new states in response to
    the actions with which we call it. The action passed to the method is the one
    we have just decided:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用返回对所调用动作的新状态的`step()`方法。传递给方法的行为是我们刚刚决定的：
- en: '[PRE30]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Finally, we print the step number and the action decided for visual control
    of the flow.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打印出步数和决定执行的动作，以便进行流程的可视化控制。
- en: By running the proposed code, we can verify that after the training phase, the
    system is able to keep the pole in equilibrium for 100 time steps.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行提出的代码，我们可以验证在训练阶段之后，系统能够在100个时间步内保持杆的平衡。
- en: Summary
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Reinforcement learning aims to create algorithms that can learn and adapt to
    environmental changes. This programming technique is based on the concept of receiving
    external stimuli depending on the algorithm choices. A correct choice will involve
    a premium, while an incorrect choice will lead to a penalty. The goal of system
    is to achieve the best possible result, of course. In this chapter, we dealt with
    the basics of reinforcement learning.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习旨在创建能够学习和适应环境变化的算法。这种编程技术基于根据算法选择接收外部刺激的概念。正确的选择将涉及奖励，而错误的选择将导致惩罚。系统的目标是实现最佳可能的结果。在本章中，我们讨论了强化学习的基础。
- en: To begin with, we saw that the goal of learning with reinforcement is to create
    intelligent agents that are able to learn from their experience. So we analyzed
    the steps to follow to correctly apply a reinforcement learning algorithm. Later
    we explored the Agent-Environment interface. The entity that must achieve the
    goal is called an **agent**. The entity with which the agent must interact is
    called the **environment**, which corresponds to everything outside the agent.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们了解到，强化学习的目标是创建能够从经验中学习的智能代理。因此，我们分析了正确应用强化学习算法的步骤。后来，我们探讨了代理-环境接口。必须实现目标的是被称为**代理**的实体。代理必须与之交互的实体被称为**环境**，它对应于代理之外的一切。
- en: To avoid load problems and computational difficulties, the Agent-Environment
    interaction is considered an MDP. An MDP is a stochastic control process. Then
    the discount factor concept was introduced. The discount factor can be modified
    during the learning process to highlight or not highlight particular actions or
    states. An optimal policy can cause the reinforcement obtained in performing a
    single action to be even low (or negative), provided that overall this leads to
    greater reinforcement.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免加载问题和计算困难，将代理-环境交互视为马尔可夫决策过程（MDP）。MDP是一个随机控制过程。然后引入了折扣因子概念。折扣因子可以在学习过程中修改，以突出或忽略特定的动作或状态。一个最优策略可以使执行单个动作获得的强化甚至很低（或负值），只要这总体上导致更大的强化。
- en: In the central part of the chapter, were dedicated to the analysis of the most
    common reinforcement learning techniques. Q-learning, TD learning, and deep Q-learning
    networks were covered. Finally, we explored the OpenAI Gym libraries and tackled
    the analysis of a practical example of reinforcement learning.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的核心部分，我们专注于分析最常用的强化学习技术。涵盖了Q学习、TD学习和深度Q学习网络。最后，我们探讨了OpenAI Gym库，并分析了强化学习的一个实际案例。
