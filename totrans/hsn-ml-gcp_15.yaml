- en: Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, most computers are based on a symbolic elaboration. The problem is
    first encoded in a set of variables and then processed using an explicit algorithm
    that, for each possible input of the problem, offers an adequate output. However,
    there are problems in which resolution by an explicit algorithm is inefficient
    or even unnatural, for example, a speech recognizer; tackling this kind of problem
    with the classic approach is inefficient. This and other similar problems, such
    as autonomous navigation of a robot or voice assistance in performing an operation,
    are part of a very diverse set of problems that can be addressed directly through
    solutions based on reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is based on a psychology theory, elaborated after a series
    of experiments performed on animals. Defining a goal to be achieved, reinforcement
    learning tries to maximize the rewards received for the execution of the action
    or set of actions that allow us to reach the designated goal. Reinforcement learning
    is a very exciting sector of machine learning, used in everything from autonomous
    cars to video games. It aims to create algorithms that can learn and adapt to
    environmental changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Markov Decision Process** (**MDP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temporal difference** (**TD**) learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Q-learning networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the chapter, you will be fully introduced to the power of reinforcement
    learning and will learn the different approaches to this technique. Several reinforcement
    learning methods will be covered.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning aims to create algorithms that can learn and adapt to
    environmental changes. This programming technique is based on the concept of receiving
    external stimuli depending on the algorithm choices. A correct choice will involve
    a premium while an incorrect choice will lead to a penalty. The goal of the system
    is to achieve the best possible result, of course.
  prefs: []
  type: TYPE_NORMAL
- en: In supervised learning, there is a teacher that tells the system which is the
    correct output (learning with a teacher). This is not always possible. Often we
    have only qualitative information (sometimes binary, right/wrong, or success/failure).
    The information available is called **reinforcement signals**. But the system
    does not give any information on how to update the agent's behavior (that is,
    weights). You cannot define a cost function or a gradient. The goal of the system
    is to create the smart agents that are able to learn from their experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a flowchart that displays reinforcement learning interaction
    with the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b492b5e-a4d9-4cf9-b7e5-719a5f97220b.png)'
  prefs: []
  type: TYPE_IMG
- en: Scientific literature has taken an uncertain stance on the classification of
    learning by reinforcement as a paradigm. In fact, in an initial phase it was considered
    as a special case of supervised learning, and then it was fully promoted as the
    third paradigm of machine learning algorithms. It is applied in different contexts
    in which supervised learning is inefficient; the problems of interaction with
    the environment are clear examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following flow shows the steps to follow to correctly apply a reinforcement
    learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparation of the agent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observation of the environment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selection of the optimal strategy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execution of actions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculation of the corresponding reward (or penalty)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Development of updating strategies (if necessary)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeating steps 2-5 iteratively until the agent learns the optimal strategies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reinforcement learning is based on some theory of psychology, elaborated after
    a series of experiments performed on animals. In particular, American psychologist
    Edward Thorndike noted that if a cat is given a reward immediately after the execution
    of a behavior considered correct, it increases the probability that this behavior
    will repeat itself. While in the face of unwanted behavior, the application of
    a punishment decreases the probability of a repetition of error.
  prefs: []
  type: TYPE_NORMAL
- en: On the basis of this theory, and with a goal to be achieved defined, reinforcement
    learning tries to maximize the rewards received for execution of the action or
    set of actions that allow us to reach the designated goal.
  prefs: []
  type: TYPE_NORMAL
- en: Agent-Environment interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning can be seen as a special case of the interaction problem
    for achieving a goal. The entity that must reach the goal is called an **agent**.
    The entity with which the agent must interact is called the **environment**, which
    corresponds to everything that is external to the agent.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have focused more on the term agent, but what does it represent?
    The agent is a software entity that performs services on behalf of another program,
    usually automatically and invisibly. These software are also called **smart agents**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the most important features of an agent:'
  prefs: []
  type: TYPE_NORMAL
- en: It can choose an action on the environment between a continuous and a discrete
    set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Action depends on the situation. The situation is summarized in the system state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent continuously monitors the environment (input) and continuously changes
    the status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of action is not trivial and requires a certain degree of **intelligence**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent has a smart memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent has a goal-directed behavior but acts in an uncertain environment
    not known a priori or partially known. An agent learns by interacting with the
    environment. Planning can be developed while learning about the environment through
    the measurements made by the agent itself. The strategy is close to trial-and-error
    theory.
  prefs: []
  type: TYPE_NORMAL
- en: Trial and error is a fundamental method of problem solving. It is characterized
    by repeated, varied attempts that are continued until success, or until the agent
    stops trying.
  prefs: []
  type: TYPE_NORMAL
- en: The Agent-Environment interaction is continuous; the agent chooses an action
    to be taken, and in response, the environment changes states, presenting a new
    situation to be faced.
  prefs: []
  type: TYPE_NORMAL
- en: In the particular case of reinforcement learning, the environment provides the
    agent with a reward; it is essential that the source of the reward is the environment
    to avoid the formation of a personal reinforcement mechanism within the agent
    that would compromise learning.
  prefs: []
  type: TYPE_NORMAL
- en: The value of the reward is proportional to the influence that the action has
    in reaching the objective; so it is positive or high in the case of a correct
    action, or negative or low action for an incorrect action.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some examples from real life in which there is an interaction
    between the agent and environment to solve the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: A chess player, for each move, has information on the configurations of pieces
    that can create and on the possible countermoves of the opponent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A little giraffe learns to get up and run at 50 km/h in a few hours
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A truly autonomous robot learns to move in a room to get out of it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameters of a refinery (oil pressure, flow, and so on) are set in real
    time so as to obtain the maximum yield or maximum quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All the examples we have analyzed have the following characteristics in common:'
  prefs: []
  type: TYPE_NORMAL
- en: Interaction with the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objective of the agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncertainty or partial knowledge of the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From the analysis of these examples, it is possible to make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent learns from its own experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the actions change the status (the situation), the possibilities of choices
    in the future change (delayed reward).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The effect of an action cannot be completely predicted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent has a global assessment of it behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It must exploit this information to improve his choices. Choices improve with
    experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems can have a finite or infinite time horizon.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markov Decision Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To avoid load problems and computational difficulties, the Agent-Environment
    interaction is considered as a MDP. MDP is a discrete time stochastic control
    process. At each time step, the process is in a state *s*, and the decision maker
    may choose any action *a* that is available in state *s*. The process responds
    at the next time step by randomly moving into a new state *s'* and giving the
    decision maker a corresponding reward, *r(s,s')*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under these hypotheses, the Agent-Environment interaction can be schematized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent and the environment interact at discrete intervals over time, *t =
    0, 1, 2, … n*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At each interval, the agent receives a representation of the state *st* of the
    environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each element *s[t]* of *S*, where *S* is the set of possible states.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the state is recognized, the agent must take an action a[t] of *A(s[t])*,
    where *A(s[t])* is the set of possible actions in the state *s[t]*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of the action to be taken depends on the objective to be achieved
    and is mapped through the policy indicated with the symbol *π* (discounted cumulative
    reward), which associates the action with a[t] of *A(s)* for each state *s*. The
    term *π[t](s,a)* represents the probability that action *a* is carried out in
    the state *s*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the next time interval *t + 1*, as part of the consequence of the action
    at, the agent receives a numerical reward *r[t] + 1* *R* corresponding to the
    action previously taken a[t].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The consequence of the action represents, instead, the new state *s[t]*. At
    this point, the agent must again code the state and make the choice of the action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This iteration repeats itself until the achievement of the objective by the
    agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The definition of the status *s[t] + 1* depends from the previous state and
    the action taken MDP, that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*s[t] + 1 = δ (s[t],a[t])*'
  prefs: []
  type: TYPE_NORMAL
- en: Here *δ* represents the status function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary:'
  prefs: []
  type: TYPE_NORMAL
- en: In an MDP, the agent can perceive the status *s S* in which he is and has an
    *A* set of actions at his disposal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At each discrete interval *t* of time, the agent detects the current status
    *st* and decides to implement an action at *A*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment responds by providing a reward (a reinforcement) *r[t] = r (s[t],
    a[t])* and moving into the state *s[t] + 1 = δ (s[t], a[t])*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *r* and *δ* functions are part of the environment; they depend only on the
    current state and action (not the previous ones) and are not necessarily known
    to the agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of reinforcement learning is to learn a policy that, for each state
    *s* in which the system is located, indicates to the agent an action to maximize
    the total reinforcement received during the entire action sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s go deeper into some of the terms used:'
  prefs: []
  type: TYPE_NORMAL
- en: A **reward function** defines the goal in a reinforcement learning problem.
    It maps the detected states of the environment into a single number, thus defining
    a reward. As already mentioned, the only goal is to maximize the total reward
    it receives in the long term. The reward function then defines what the good and
    bad events are for the agent. The reward function has the need to be correct,
    and it can be used as a basis for changing the policy. For example, if an action
    selected by the policy is followed by a low reward, the policy can be changed
    to select other actions in that situation in the next step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **policy** defines the behavior of the learning agent at a given time. It
    maps both the detected states of the environment and the actions to take when
    they are in those states. Corresponds to what in psychology would be called a
    **set of rules** or associations of stimulus response. Policy is the fundamental
    part of a reinforcing learning agent, in the sense that it alone is enough to
    determine behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **value function** represents how good a state is for an agent. It is equal
    to the total reward expected for an agent from the status *s*. The value function
    depends on the policy with which the agent selects the actions to be performed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discounted cumulative reward
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we said this: the goal of reinforcement learning is
    to learn a policy that, for each state *s* in which the system is located, indicates
    to the agent an action to maximize the total reinforcement received during the
    entire action sequence. But how can we maximize the total reinforcement received
    during the entire sequence of actions?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The total reinforcement derived from the policy is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bce7ad4-a3fe-4ee6-92e6-560dff764b36.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *r[T]* represents the reward of the action that drives the environment
    in the terminal state *s[T]*.
  prefs: []
  type: TYPE_NORMAL
- en: A possible solution to the problem is to associate the action that provides
    the highest reward to each individual state; that is, we must determine an optimal
    policy such that the previous quantity is maximized.
  prefs: []
  type: TYPE_NORMAL
- en: For problems that do not reach the goal or terminal state in a finite number
    of steps (continuing tasks), *R[t]* tends to infinity.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, the sum of the rewards that one wants to maximize diverges at
    infinity, so this approach is not applicable. Then, it is necessary to develop
    an alternative reinforcement technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'The technique that best suits the reinforcement learning paradigm turns out
    to be discounted cumulative reward, which tries to maximize the following quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94c8312c-b46e-461c-9180-85f557269910.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *γ* is called **discount factor** and it represents the importance for
    future rewards. This parameter can take the values *0 ≤ γ ≤ 1*, with the following
    meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: If *γ <1*, the sequence *r[t]* will converge to a finite value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *γ = 0*, the agent will have no interest in future rewards, but will try
    to maximize the reward only for the current state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *γ = 1*, the agent will try to increase future rewards even at the expense
    of immediate ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discount factor can be modified during the learning process to highlight
    or not particular actions or states. An optimal policy can cause the reinforcement
    obtained in performing a single action to be even low (or even negative), provided
    that overall this leads to greater reinforcement.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration versus exploitation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ideally, the agent must associate with each action at the respective reward
    *r* in order to then choose the most rewarded behavior for achieving the goal.
    This approach is therefore impracticable for complex problems, in which the number
    of states is particularly high and consequently the possible associations increase
    exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: This problem is called the **exploration-exploitation dilemma**. Ideally, the
    agent must explore all possible actions for each state, finding the one that is
    actually most rewarded for exploiting it in achieving its goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, decision-making involves a fundamental choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploitation**: Make the best decision given current information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploration**: Collect more information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this process, the best long-term strategy can lead to considerable sacrifices
    in the short term. Therefore, it is necessary to gather enough information to
    make the best decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of adopting this technique for real-life cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Selection of a store**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploitation**: Go to your favorite store'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploration**: Try a new store'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choice of a route**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploitation**: Choose the best route so far'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploration**: Try a new route'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, in very complex problems, convergence to a very good strategy would
    be too slow.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good solution to the problem is to find a balance between exploration and
    exploitation:'
  prefs: []
  type: TYPE_NORMAL
- en: An agent who limits himself to exploring will always act in a casual way in
    every state, and it is evident that convergence to an optimal strategy is impossible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If an agent explores little, it will always use the usual actions, which may
    not be optimal ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen in the previous sections, reinforcement learning is a programming
    philosophy that aims to develop algorithms that can learn and adapt to changes
    in the environment. This programming technique is based on the assumption of being
    able to receive stimuli from the outside according to the choices of the algorithm.
    So, a correct choice will result in a prize while an incorrect choice will lead
    to a penalization of the system. The goal of the system is to achieve the highest
    possible prize and consequently the best possible result. The techniques related
    to learning by reinforcement are divided into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous learning algorithms**: These techniques start from the assumption
    of having a simple mechanism able to evaluate the choices of the algorithm and
    then reward or punish the algorithm depending on the result. These techniques
    can also adapt to substantial changes in the environment. An example is speech
    recognition programs or OCR programs that improve their performance with use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preventive training algorithms**: These algorithms start from the observation
    that constantly evaluating the actions of the algorithm can be a process that
    cannot be automated or very expensive. In this case, a first phase is applied,
    in which the algorithm is taught; when the system is considered reliable, it is
    crystallized and no more editable. Many electronic components use neural networks
    within them, and the synaptic weights of these networks are not changeable since
    they are fixed during the construction of the circuit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be noted that the categories mentioned previously are implementation
    choices rather than conceptual differences in the algorithm. Therefore, an algorithm
    can often be in the first or second category depending on how it is implemented
    by the designer.
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q-learning is one of the most-used reinforcement learning algorithms. This is
    due to its ability to compare the expected utility of the available actions without
    requiring an environment model.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to this technique, it is possible to find an optimal action for every
    given state in a finished MDP.
  prefs: []
  type: TYPE_NORMAL
- en: 'A general solution to the reinforcement learning problem is to estimate, thanks
    to the learning process, an evaluation function. This function must be able to
    evaluate, through the sum of the rewards, the convenience or otherwise of a particular
    policy. In fact, Q-learning tries to maximize the value of the *Q* function (action-value
    function), which represents the maximum discounted future reward when we perform
    actions *a* in state *s*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Q(S[t],a[t]) = max(R[t+1])*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing the *Q* function, the optimal action *a* in a state *s* is the one
    with the highest *Q* value. At this point, we can define a policy *π(s)* that
    provides us with the best action in any state. Recalling that the policy *π* associates
    the pair *(s; a)* with the probability *(s; a)* that action is carried out in
    the state *s*, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bf8dd3d-0c65-4b9a-809a-75214b472d33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The problem is reduced to the evaluation of the *Q* function. We can then estimate
    the *Q* function for a transition point in terms of the *Q* function at the next
    point through a recursive process. The following is the equation used in a single
    step of the process. This equation is known as **Bellman''s equation** and represents
    the transition rule of Q-learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e95528e-5650-490d-8dce-f82aeb359907.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The terms are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Q(s[t],a[t])* is the current policy of action *a* from state *s.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*r* is the reward for the action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*max[t+1](Q(s[t+1],a[t+1]))* defines the maximum future reward. We performed
    the *a[t]* action to state *s[t]* to reach the *s[t+1]* state. From here, we may
    have multiple actions, each corresponding to some rewards. The maximum of that
    reward is computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*γ* is the discount factor. The *γ* value varies from 0 to 1; if the value
    is near 0, an immediate reward is given preference. If it goes near 1, the importance
    of future rewards increases until 1, where it is considered equal to immediate
    rewards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the basis of the previous equation, the evaluation function *Q* is given
    by the sum of the immediate reward and the maximum reward obtainable starting
    from the next state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the previous formula, we are trying to formulate the delayed rewards
    into immediate rewards. We have previously said that the evaluation of the *Q*
    function represents a recursive process. We can then enter the values obtained
    during this process in a table that we will, of course, call table *Q*. In this
    table, the rows are the states and the columns are the actions. As a starting
    table *Q*, we can use a matrix containing all zeros (we have initialized table
    *Q*), as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31e9ee64-1d2c-48d9-a410-2db4756872d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The elements of this table *Q* (cells) are the rewards that are obtained if
    one is in the state given by the row and the action given by the column is executed.
    The best action to take in any state is the one with the highest reward. Our task
    now is to update this table with new values. To do this, we can adopt the following
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: The status *s[t]* is decoded
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An action *a[t]* is selected
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Action *a[t]* is performed and the reward *r* is received
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The element of table *Q(s[t]; a[t])* is updated with the training rule provided
    by Bellman's equation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The execution of the action a moves the environment in the state *s[t+1]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the next state as the current state (*s[t] = s[t+1]*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start again from point 1 and repeat the process until a terminal state is reached
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In more complex and efficient formulations, it is possible to replace the table,
    whose iteration is still inefficient for complex problems, with a neural network
    where the learning process will change the weights of the synaptic connections.
  prefs: []
  type: TYPE_NORMAL
- en: Temporal difference learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TD learning algorithms are based on reducing the differences between estimates
    made by the agent at different times. Q-learning, seen in the previous section,
    is a TD algorithm, but it is based on the difference between states in immediately
    adjacent instants. TD is more generic and may consider moments and states further
    away.
  prefs: []
  type: TYPE_NORMAL
- en: It is a combination of the ideas of the **Monte Carlo** (**MC**) method and
    the **Dynamic Programming** (**DP**).
  prefs: []
  type: TYPE_NORMAL
- en: MC methods allow solving reinforcement learning problems based on the average
    of the results obtained.
  prefs: []
  type: TYPE_NORMAL
- en: DP represents a set of algorithms that can be used to calculate an optimal policy
    given a perfect model of the environment in the form of an MDP.
  prefs: []
  type: TYPE_NORMAL
- en: A TD algorithm can learn directly from raw data, without a model of the dynamics
    of the environment (such as MC). This algorithm updates the estimates based partly
    on previously learned estimates, without waiting for the final result (bootstrap,
    such as DP). It is suitable for learning without a model of dynamic environments.
    Converge using a fixed policy if the time step is sufficiently small, or if it
    reduces over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in the previous section, Q-learning calculates its values according
    to the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23d2a832-5cd5-4ef2-b9d9-de942401f24b.png)'
  prefs: []
  type: TYPE_IMG
- en: By adopting a one-step look-ahead.
  prefs: []
  type: TYPE_NORMAL
- en: Look-ahead is the generic term for a procedure that attempts to foresee the
    effects of choosing a branching variable to evaluate one of its values. The two
    main aims of look-ahead are to choose a variable to evaluate next and the order
    of values to assign to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is clear that a two-step formula can also be used, as shown in the following
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcdee0c9-9322-4a98-89ad-1cadf49d66e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'More generally with n-step look-ahead, we obtain the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad4e7e0d-46c9-493b-b774-b8b5d7b84fe1.png)'
  prefs: []
  type: TYPE_IMG
- en: Dynamic Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DP represents a set of algorithms that can be used to calculate an optimal policy
    given a perfect model of the environment in the form of a MDP. The fundamental
    idea of DP, as well as reinforcement learning in general, is the use of state
    values and actions, to look for good policies.
  prefs: []
  type: TYPE_NORMAL
- en: The DP methods approach the resolution of Markov decision-making processes through
    the iteration of two processes called **policy evaluation** and **policy improvement**.
  prefs: []
  type: TYPE_NORMAL
- en: Policy evaluation algorithm consists in applying an iterative method to the
    resolution of the Bellman equation. Since convergence is guaranteed to us only
    for *k → ∞*, we must be content to have good approximations by imposing a stopping
    condition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy improvement algorithm improves policy based on current values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A disadvantage of the policy iteration algorithm is that we have to evaluate
    a policy at every step. This involves an iterative process whose time of convergence
    we do not know a priori. This will depend on, among other things, how the starting
    policy was chosen.
  prefs: []
  type: TYPE_NORMAL
- en: One way to overcome this drawback is to cut off the evaluation of the policy
    at a specific step. This operation does not change the guarantee of convergence
    to the optimal value. A special case in which the assessment of the policy is
    blocked by a step by state (also called **sweep**) defines the value iteration
    algorithm. In the value iteration algorithm, a single iteration of calculation
    of the values is performed between each step of the policy improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DP algorithms are therefore essentially based on two processes that take
    place in parallel: policy evaluation and policy improvement. The repeated execution
    of these two processes makes the general process converge towards the optimal
    solution. In the policy iteration algorithm the two phases alternate and each
    ends before the other begins.'
  prefs: []
  type: TYPE_NORMAL
- en: DP methods operate through the entire set of states that can be assumed by the
    environment, performing a complete backup for each state at each iteration. Each
    update operation performed by the backup updates the value of a status based on
    the values ​​of all possible successor states, weighted for their probability
    of occurrence and induced by the policy of choice and dynamics of the environment.
    Full backups are closely related to the Bellman equation; they are nothing more
    than the transformation of the equation into assignment instructions.
  prefs: []
  type: TYPE_NORMAL
- en: When a complete backup iteration does not bring any change to the state values,
    convergence is obtained; therefore the final state values ​​fully satisfy the
    Bellman equation. The DP methods are applicable only if there is a perfect model
    of the alternator, which must be equivalent to a MDP.
  prefs: []
  type: TYPE_NORMAL
- en: Precisely for this reason, the DP algorithms are of little use in reinforcement
    learning, both for their assumption of a perfect model of the environment, and
    for the high and expensive computation, but it is still opportune to mention them
    because they represent the theoretical basis of reinforcement learning. In fact,
    all the methods of reinforcement learning try to achieve the same goal of the
    DP methods, only with lower computational cost and without the assumption of a
    perfect model of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: The DP methods converge to the optimal solution with a number of polynomial
    operations with respect to the number of states *n* and actions *m*, against the
    number of exponential operations *m*n* required by methods based on direct search.
  prefs: []
  type: TYPE_NORMAL
- en: The DP methods update the estimates of the values of the states, based on the
    estimates of the values of the successor states; or they update the estimates
    on the basis of past estimates. This represents a special property, which is called
    **bootstrapping**. Several methods of reinforcement learning perform bootstrapping,
    even methods that do not require a perfect model of the environment, as required
    by the DP methods.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MC methods for estimating the value function and discovering excellent policies
    do not require the presence of a model of the environment. They are able to learn
    through the use of the agent's experience alone or from samples of state sequences,
    actions, and rewards obtained from the interactions between agent and environment.
    The experience can be acquired by the agent in line with the learning process
    or emulated by a previously populated dataset. The possibility of gaining experience
    during learning (online learning) is interesting because it allows obtaining excellent
    behavior even in the absence of a priori knowledge of the dynamics of the environment.
    Even learning through an already populated experience dataset can be interesting,
    because if combined with online learning, it makes automatic policy improvement
    induced by others' experiences possible.
  prefs: []
  type: TYPE_NORMAL
- en: To solve the reinforcement learning problems, MC methods estimate the value
    function on the basis of the total sum of rewards, obtained on average in the
    past episodes. This assumes that the experience is divided into episodes, and
    that all episodes are composed of a finite number of transitions. This is because
    in MC methods, only once an episode is completed takes place the estimate of the
    new values ​​and the modification of the policy. MC methods iteratively estimate
    policy and value function. In this case, however, each iteration cycle is equivalent
    to completing an episode—the new estimates of policy and value function occur
    episode by episode.
  prefs: []
  type: TYPE_NORMAL
- en: Usually the term MC is used for estimation methods, which operations involve
    random components; in this case, MC refers to reinforcement learning methods based
    on total reward averages. Unlike the DP methods that calculate the values ​​for
    each state, the MC methods calculate the values ​​for each state-action pair,
    because in the absence of a model, only state values ​​are not sufficient to decide
    which action is best performed in a certain state.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deep Q-Network** (**DQN**) algorithms combine both the reinforcement learning
    approach and the deep learning approach. DQN learns by itself, learning in an
    empirical way and without a rigid programming aimed at a particular objective,
    such as winning a game of chess.'
  prefs: []
  type: TYPE_NORMAL
- en: DQN represents an application of Q-learning with the use of deep learning for
    the approximation of the evaluation function. The DQN was proposed by Mnih et
    al. through an article published in *Nature* on February 26, 2015\. As a consequence,
    a lot of research institutes joined this field, because deep neural networks can
    empower reinforcement learning algorithms to directly deal with high-dimensional
    states.
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of deep neural networks is due to the fact that researchers noted the
    following: using a neural network to approximate the **Q-evaluation** function
    in algorithms with reinforcement learning made the system unstable or divergent.
    In fact, it is possible to notice that small updates to *Q* can significantly
    change the policy, distribution of data, and correlations between *Q* and target
    values. These correlations, present in the sequence of observations, are the cause
    of the instability of the algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To transform a normal Q-network into a DQN, it is necessary to carry out the
    following precautions:'
  prefs: []
  type: TYPE_NORMAL
- en: Replace the single-level neural network with a multi-level convolutional network
    for approximation of the Q-function evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the experience replay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a second network to calculate the target Q-values during your updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is meant by the term **experience replay**? This means that, instead of
    running Q-learning on state/action pairs as they occur during a simulation or
    actual experience, the system stores the data discovered, typically in a large
    table. In this way, our network can train itself using stored memories from its
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**OpenAI Gym** is a library that helps us to implement algorithms based on
    reinforcement learning. It includes a growing collection of benchmark issues that
    expose a common interface, and a website where people can share their results
    and compare algorithm performance.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym focuses on the episodic setting of reinforced learning. In other
    words, the agent's experience is divided into a series of episodes. The initial
    state of the agent is randomly sampled by a distribution, and the interaction
    proceeds until the environment reaches a terminal state. This procedure is repeated
    for each episode, with the aim of maximizing the total reward expectation per
    episode and achieving a high level of performance in the fewest possible episodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gym is a toolkit for developing and comparing reinforcement learning algorithms.
    It supports the ability to teach agents everything from walking to playing games
    such as Pong or Pinball. The library is available at the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://gym.openai.com/](https://gym.openai.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the home page of the OpenAI Gym project site:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f87cdac-c992-470e-8e3e-1301f466cbe7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'OpenAI Gym is part of a much more ambitious project: the OpenAI project. OpenAI
    is an **artificial intelligence** (**AI**) research company founded by Elon Musk
    and Sam Altman. It is a non-profit project that aims to promote and develop friendly
    AI in such a way as to benefit humanity as a whole. The organization aims to collaborate
    freely with other institutions and researchers by making their patents and research
    open to the public. The founders decided to undertake this project as they were
    concerned by the existential risk deriving from the indiscriminate use of AI.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym is a library of programs that allow you to develop AIs, measure their
    intellectual abilities ,and enhance their learning abilities. In short, a Gym
    in the form of algorithms that trains the present digital brains to OpenAI Gym
    project them into the future.
  prefs: []
  type: TYPE_NORMAL
- en: But there is also another goal. OpenAI wants to stimulate research in the AI
    ​​sector by funding projects that make humanity progress even in those fields
    where there is no economic return. With Gym, on the other hand, it intends to
    standardize the measurement of AI so that researchers can compete on equal terms
    and know where their colleagues have come but, above all, focus on results that
    are really useful for everyone.
  prefs: []
  type: TYPE_NORMAL
- en: The tools available are many. From the ability to play old video games like
    Pong to that of fighting in the GO to control a robot, we just enter our algorithm
    in this digital place to see how it works. The second step is to compare the benchmarks
    obtained with the other ones to see where we stand compared to others, and maybe
    we can collaborate with them to get mutual benefits.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym makes no assumptions about the structure of our agent and is compatible
    with any numerical computation library, such as TensorFlow or Theano. The Gym
    library is a collection of test problems—environments—that we can use to work
    out our reinforcement learning algorithms. These environments have a shared interface,
    allowing you to write general algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install OpenAI Gym, make sure you have previously installed a Python 3.5+
    version; then simply type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once this is done, we will be able to insert the tools made available by the
    library in a simple and immediate way.
  prefs: []
  type: TYPE_NORMAL
- en: Cart-Pole system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Cart-Pole system is a classic problem of reinforced learning. The system
    consists of a pole (which acts like an inverted pendulum) attached to a cart via
    a joint, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f3ae859-2e08-4728-813d-8d0e2b489e6c.png)'
  prefs: []
  type: TYPE_IMG
- en: The system is controlled by applying a force of +1 or -1 to the cart. The force
    applied to the cart can be controlled, and the objective is to swing the pole
    upwards and stabilize it. This must be done without the cart falling to the ground.
    At every step, the agent can choose to move the cart left or right, and it receives
    a reward of 1 for every time step that the pole is balanced. If the pole ever
    deviates by more than 15 degrees from upright, then the procedure ends.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the Cart-Pole example using the OpenAI Gym library, simply type the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As always, we will explain the meaning of each line of code in detail. The
    first line is used to import the `gym` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we move on to create the environment by calling the `make` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This method creates the environment that our agent will run in. An environment
    is a problem with a minimal interface that an agent can interact with. The environments
    in OpenAI Gym are designed in order to allow objective testing and benchmarking
    of an agent's abilities. The Gym library comes with a diverse suite of environments
    that range from easy to difficult and involve many different kinds of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a list of the available environments, refer to the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://gym.openai.com/envs](https://gym.openai.com/envs)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most used environments are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classic control and toy text**: Complete small-scale tasks, mostly from the
    reinforcement learning literature. They''re here to get you started.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithmic**: Perform computations such as adding multi-digit numbers and
    reversing sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Atari**: Play classic Atari games.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2D and 3D robots**: Control a robot in simulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our case we have called the CartPole-v0 environment. The `make` method returns
    an `env` object that we will use to interact with the game. But let''s go back
    to analyzing the code. Now we have to initialize the system using the `reset()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This method puts the environment into its initial state, returning an array
    that describes it. At this point, we will use a `for` loop to run an instance
    of the CartPole-v0 environment for `1000` time steps, rendering the environment
    at each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Calling the `render()` method will visually display the current state, while
    subsequent calls to `env.step()` will allow us to interact with the environment,
    returning the new states in response to the actions with which we call it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this way, we have adopted random actions at each step. At this point, it
    is certainly useful to know what actions we are doing on the environment to decide
    future actions. The `step()` method returns exactly this. In effect, this method
    returns the following four values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`observation (object)`: An environment-specific object representing your observation
    of the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward (float)`: Amount of reward achieved by the previous action. The scale
    varies between environments, but the goal is always to increase your total reward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`done (boolean)`: Whether it''s time to reset the environment again. Most (but
    not all) tasks are divided into well-defined episodes, and `done` being `True`
    indicates the episode has terminated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info (dict)`: Diagnostic information useful for debugging. It can sometimes
    be useful for learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To run this simple example, save the code in a file named `cart.py` and type
    the following command at the bash window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this way, a window will be displayed containing our system that is not stable
    and will soon go out of the screen. This is because the push to the cart is given
    randomly, without taking into account the position of the pole.
  prefs: []
  type: TYPE_NORMAL
- en: To solve the problem, that is, to balance the pole, it is therefore necessary
    to set the push in the opposite direction to the inclination of the pole. So,
    we have to set only two actions, -1 or +1, pushing the cart to the left or the
    right. But in order to do so, we need to know at all times the data deriving from
    the observation of the environment. As we have already said, these pieces of data
    are returned by the `step()` method, in particular they are contained in the observation
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'This object contains the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Cart position
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cart velocity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pole angle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pole velocity at tip
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These four values become the input of our problem. As we have also anticipated,
    the system is balanced by applying a push to the cart. There are two possible
    options:'
  prefs: []
  type: TYPE_NORMAL
- en: Push the cart to the left (0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Push it to the right (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is clear that this is a binary classification problem: four inputs and a
    single binary output.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us first consider how to extract the values to be used as input. To extract
    these parameters, we just have to change the preceding proposed code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: By running the code, we can see that the values contained in the observation
    object are now printed on the screen. All this will be useful soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using values returned from the environment observations, the agent has to decide
    on one of two possible actions: to move the cart left or right.'
  prefs: []
  type: TYPE_NORMAL
- en: Learning phase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we have to face the most demanding phase, namely the training of our system.
    In the previous section, we said that the Gym library is focused on the episodic
    setting of reinforced learning. The agent's experience is divided into a series
    of episodes. The initial state of the agent is randomly sampled by a distribution
    and the interaction proceeds until the environment reaches a terminal state. This
    procedure is repeated for each episode with the aim of maximizing the total reward
    expectation per episode and achieving a high level of performance in the fewest
    possible episodes.
  prefs: []
  type: TYPE_NORMAL
- en: In the learning phase, we must estimate an evaluation function. This function
    must be able to evaluate, through the sum of the rewards, the convenience or otherwise
    of a particular policy. In other words, we must approximate the evaluation function.
    How can we do? One solution is to use an artificial neural network as a function
    approximator.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the training of a neural network aims to identify the weights of
    the connections between neurons. In this case, we will choose random values with
    weights for each episode. At the end, we will choose the combination of weights
    that will have collected the maximum reward.
  prefs: []
  type: TYPE_NORMAL
- en: The state of the system at a given moment is returned to us by the observation
    object. To choose an action from the actual state, we can use a linear combination
    of the weights and the observation. This is one of the most important special
    cases of function approximation, in which the approximate function is a linear
    function of the weight vector *w*. For every state *s*, there is a real-valued
    vector *x(s)* with the same number of components as *w*. Linear methods approximate
    the state-value function by the inner product between *w* and *x(s)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this way, we have specified the methodology that we intend to adopt for
    the solution of the problem. Now, to make the whole training phase easily understandable,
    we report the whole code block and then comment on it in detail on a line-by-line
    basis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The first part of the code deals with importing the libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we move on to create the environment by calling the `make` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This method creates the environment that our agent will run in. Now let''s
    initialize the parameters we will use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`HighReward` will contain the maximum reward obtained up to the current episode;
    this value will be used as a comparison value. `BestWeights` will contain the
    sequence of weights that will have registered the maximum reward. We can now implement
    the best weight sequence search through an iterative procedure for episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We decide to execute the procedure `200` times, so we initialize the system
    using the `reset()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In each episode, we use a sequence of weights equal in number to the observations
    of the environment, which as previously said is four (cart position, cart velocity,
    pole angle, and pole velocity at tip):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: To fix the weights, we have used the `np.random.uniform()` function. This function
    draws samples from a uniform distribution. Samples are uniformly distributed over
    the half-open interval (low and high). It includes low but excludes high.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, any value within the given interval is equally likely to be
    drawn by a uniform distribution. Three parameters have been passed: the lower
    boundary of the output interval, its upper boundary, and the output shape. In
    our case we requested four random values in the interval `(-1,1)`. After doing
    this we initialize the sum of the rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we implement another iterative cycle to determine the maximum
    reward we can get with these weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling the `render()` method will visually display the current state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to decide the `action`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As we said, to decide the action we have used a linear combination of two vectors:
    `weights` and `observation`. To perform a linear combination, we have used the
    `np.matmul()` function; it implements matrix product of two arrays. So, if this
    product is `<0`, then `action` is 0 (move left); otherwise, `action` is 1 (move
    right).'
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that a negative product means that the pole is tilted to
    the left, so in order to balance this trend, it is necessary to push the cart
    towards the left. A positive product means the pole is tilted to the right, so
    in order to balance this trend, it is necessary to push the cart towards the right.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we use the `step()` method to return the new states in response to the
    actions with which we call it. Obviously, the action we pass to the method is
    the one we have just decided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As we said, this method returns the following four values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`observation` (`object`): An environment-specific object representing your
    observation of the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward` (`float`): The amount of reward achieved by the previous action. The
    scale varies between environments, but the goal is always to increase your total
    reward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`done` (`boolean`): Whether it''s time to reset the environment again. Most
    (but not all) tasks are divided into well-defined episodes, and `done` being `True`
    indicates that the episode has terminated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info` (`dict`): Diagnostic information useful for debugging. It can sometimes
    be useful for learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can then update the sum of the rewards with the one just obtained. Remember
    that, for every time step where we keep the pole straight, we get +1 `reward`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We just have to print the values obtained in this step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the current iteration, we can make a comparison to check whether
    the total reward obtained is the highest one obtained so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If it is the highest reward obtained so far, update the `HighReward` parameter
    with this value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is done, fix the sequence of `Weights` of the current step as the
    best one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: With this instruction, the training phase ends, which will give us the sequence
    of weights that best approximate the evaluation function. We can now test the
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Testing phase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the training phase is achieved, in practice it means that we have found
    the sequence of weights that best approximates this function, that is, the one
    that has returned the best reward achievable. Now we have to test the system with
    these values to check whether the pole is able to stand for at least `100` time
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, as we are already done in the training phase, to make the whole testing
    phase easily understandable, we report the whole code block and then comment on
    it in detail on a line-by-line basis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we have to initialize the system once again, using the `reset()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have to run an iterative cycle to apply the results obtained in the
    training phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'For each step, we will call the `render()` method to visually display the current
    state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to decide the action to perform on the system based on the best
    weights obtained in the training phase and on the observations of the current
    state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we use the `step()` method that returning the new states in response to
    the actions with which we call it. The action passed to the method is the one
    we have just decided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we print the step number and the action decided for visual control
    of the flow.
  prefs: []
  type: TYPE_NORMAL
- en: By running the proposed code, we can verify that after the training phase, the
    system is able to keep the pole in equilibrium for 100 time steps.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning aims to create algorithms that can learn and adapt to
    environmental changes. This programming technique is based on the concept of receiving
    external stimuli depending on the algorithm choices. A correct choice will involve
    a premium, while an incorrect choice will lead to a penalty. The goal of system
    is to achieve the best possible result, of course. In this chapter, we dealt with
    the basics of reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, we saw that the goal of learning with reinforcement is to create
    intelligent agents that are able to learn from their experience. So we analyzed
    the steps to follow to correctly apply a reinforcement learning algorithm. Later
    we explored the Agent-Environment interface. The entity that must achieve the
    goal is called an **agent**. The entity with which the agent must interact is
    called the **environment**, which corresponds to everything outside the agent.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid load problems and computational difficulties, the Agent-Environment
    interaction is considered an MDP. An MDP is a stochastic control process. Then
    the discount factor concept was introduced. The discount factor can be modified
    during the learning process to highlight or not highlight particular actions or
    states. An optimal policy can cause the reinforcement obtained in performing a
    single action to be even low (or negative), provided that overall this leads to
    greater reinforcement.
  prefs: []
  type: TYPE_NORMAL
- en: In the central part of the chapter, were dedicated to the analysis of the most
    common reinforcement learning techniques. Q-learning, TD learning, and deep Q-learning
    networks were covered. Finally, we explored the OpenAI Gym libraries and tackled
    the analysis of a practical example of reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
