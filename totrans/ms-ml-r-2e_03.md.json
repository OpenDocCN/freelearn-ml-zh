["```py\n    > library(MASS)\n    > data(biopsy)\n    > str(biopsy)\n    'data.frame':   699 obs. of  11 variables:\n     $ ID   : chr  \"1000025\" \"1002945\" \"1015425\" \n       \"1016277\" ...\n     $ V1   : int  5 5 3 6 4 8 1 2 2 4 ...\n     $ V2   : int  1 4 1 8 1 10 1 1 1 2 ...\n     $ V3   : int  1 4 1 8 1 10 1 2 1 1 ...\n     $ V4   : int  1 5 1 1 3 8 1 1 1 1 ...\n     $ V5   : int  2 7 2 3 2 7 2 2 2 2 ...\n     $ V6   : int  1 10 2 4 1 10 10 1 1 1 ...\n     $ V7   : int  3 3 3 3 3 9 3 3 1 2 ...\n     $ V8   : int  1 2 1 7 1 7 1 1 1 1 ...\n     $ V9   : int  1 1 1 1 1 1 1 1 5 1 ...\n     $ class: Factor w/ 2 levels \"benign\",\"malignant\": 1 1 1 1 1 2 1 1 \n     1 1 ...\n\n```", "```py\n > biopsy$ID = NULL\n\n```", "```py\n > names(biopsy) <- c(\"thick\", \"u.size\", \"u.shape\", \n        \"adhsn\", \"s.size\", \"nucl\",    \"chrom\", \"n.nuc\", \n            \"mit\", \"class\")\n > names(biopsy)\n [1] \"thick\"   \"u.size\"  \"u.shape\" \"adhsn\"   \n        \"s.size\"  \"nucl\" \"chrom\"   \"n.nuc\"\n [9] \"mit\"     \"class\"\n\n```", "```py\n    > biopsy.v2 <- na.omit(biopsy)\n\n```", "```py\n > y <- ifelse(biopsy$class == \"malignant\", 1, 0)\n\n```", "```py\n    > library(reshape2)\n    > library(ggplot2)\n\n```", "```py\n    > biop.m <- melt(biopsy.v2, id.var = \"class\")\n\n```", "```py\n    > ggplot(data = biop.m, aes(x = class, y = value)) \n    + geom_boxplot() + facet_wrap(~ variable, ncol = 3)\n\n```", "```py\n    > library(corrplot)\n    > bc <- cor(biopsy.v2[, 1:9]) #create an object of \n       the features\n    > corrplot.mixed(bc)\n\n```", "```py\n    > set.seed(123) #random number generator\n    > ind <- sample(2, nrow(biopsy.v2), replace = TRUE, \n       prob = c(0.7, 0.3))\n    > train <- biopsy.v2[ind==1, ] #the training data \n       set\n    > test <- biopsy.v2[ind==2, ] #the test data set\n    > str(test) #confirm it worked\n    'data.frame':   209 obs. of  10 variables:\n     $ thick  : int  5 6 4 2 1 7 6 7 1 3 ...\n     $ u.size : int  4 8 1 1 1 4 1 3 1 2 ...\n     $ u.shape: int  4 8 1 2 1 6 1 2 1 1 ...\n     $ adhsn  : int  5 1 3 1 1 4 1 10 1 1 ...\n     $ s.size : int  7 3 2 2 1 6 2 5 2 1 ...\n     $ nucl   : int  10 4 1 1 1 1 1 10 1 1 ...\n     $ chrom  : int  3 3 3 3 3 4 3 5 3 2 ...\n     $ n.nuc  : int  2 7 1 1 1 3 1 4 1 1 ...\n     $ mit    : int  1 1 1 1 1 1 1 4 1 1 ...\n     $ class  : Factor w/ 2 levels benign\",\"malignant\": \n       1 1 1 1 1 2 1 \n       2 1 1 ...\n\n```", "```py\n    > table(train$class)\n       benign malignant\n          302       172\n    > table(test$class)\n       benign malignant\n          142        67\n\n```", "```py\n    > full.fit <- glm(class ~ ., family = binomial, \n      data = train)\n    > summary(full.fit)\n    Call:\n    glm(formula = class ~ ., family = binomial, data = \n      train) \n    Deviance Residuals:\n    Min       1Q   Median       3Q      Max \n    -3.3397  -0.1387  -0.0716   0.0321   2.3559 \n    Coefficients:\n    Estimate Std. Error z value Pr(>|z|) \n    (Intercept)  -9.4293     1.2273  -7.683 1.55e-14 \n      ***\n    thick         0.5252     0.1601   3.280 0.001039 **\n    u.size       -0.1045     0.2446  -0.427 0.669165 \n    u.shape       0.2798     0.2526   1.108 0.268044 \n    adhsn         0.3086     0.1738   1.776 0.075722 . \n    s.size        0.2866     0.2074   1.382 0.167021 \n    nucl          0.4057     0.1213   3.344 0.000826 \n      ***\n    chrom         0.2737     0.2174   1.259 0.208006 \n    n.nuc         0.2244     0.1373   1.635 0.102126 \n    mit           0.4296     0.3393   1.266 0.205402 \n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 \n      '.' 0.1 ' ' 1\n    (Dispersion parameter for binomial family taken to \n       be 1)\n        Null deviance: 620.989  on 473  degrees of \n       freedom\n    Residual deviance:  78.373  on 464  degrees of \n       freedom\n    AIC: 98.373\n    Number of Fisher Scoring iterations: 8\n\n```", "```py\n    > confint(full.fit)\n                       2.5 %     97.5 %\n    (Intercept) -12.23786660 -7.3421509\n    thick         0.23250518  0.8712407\n    u.size       -0.56108960  0.4212527\n    u.shape      -0.24551513  0.7725505\n    adhsn        -0.02257952  0.6760586\n    s.size       -0.11769714  0.7024139\n    nucl          0.17687420  0.6582354\n    chrom        -0.13992177  0.7232904\n    n.nuc        -0.03813490  0.5110293\n    mit          -0.14099177  1.0142786\n\n```", "```py\n    > exp(coef(full.fit))\n     (Intercept)        thick       u.size      u.shape        \n     adhsn\n    8.033466e-05 1.690879e+00 9.007478e-01 1.322844e+00 \n     1.361533e+00\n   s.size         nucl        chrom        n.nuc  mit\n   1.331940e+00 1.500309e+00 1.314783e+00 1.251551e+00 \n    1.536709e+00\n\n```", "```py\n    > library(car)\n    > vif(full.fit)\n       thick  u.size  u.shape  adhsn   s.size   nucl   \n      chrom   n.nuc\n     1.2352 3.2488  2.8303   1.3021  1.6356   1.3729 \n       1.5234  1.3431\n       mit\n    1.059707\n\n```", "```py\n    > train.probs <- predict(full.fit, type = \n       \"response\")\n    > train.probs[1:5] #inspect the first 5 predicted \n        probabilities\n    [1] 0.02052820 0.01087838 0.99992668 0.08987453 \n        0.01379266\n\n```", "```py\n > trainY <- y[ind==1]\n > testY <- y[ind==2]\n > confusionMatrix(trainY, train.probs)\n 0    1\n 0 294    7\n 1   8  165 \n\n```", "```py\n > misClassError(trainY, train.probs)\n [1] 0.0316 \n\n```", "```py\n    > test.probs <- predict(full.fit, newdata = test, \n       type = \"response\")\n    > misClassError(testY, test.probs)\n    [1] 0.0239\n    > confusionMatrix(testY, test.probs)\n        0    1\n    0 139    2\n    1   3   65\n\n```", "```py\n    > library(bestglm)\n    Loading required package: leaps\n\n```", "```py\n > X <- train[, 1:9]\n > Xy <- data.frame(cbind(X, trainY)) \n\n```", "```py\n    > bestglm(Xy = biopsy.cv, IC=\"CV\", \n      CVArgs=list(Method=\"HTF\", K=10, \n      REP=1), family=binomial)\n\n```", "```py\n    Morgan-Tatar search since family is non-gaussian.\n    CV(K = 10, REP = 1)\n    BICq equivalent for q in (7.16797006619085e-05, \n    0.273173435514231)\n    Best Model:\n    Estimate Std. Error   z value     Pr(>|z|)\n    (Intercept) -7.8147191 0.90996494 -8.587934 \n     8.854687e-18\n    thick        0.6188466 0.14713075  4.206100 \n     2.598159e-05\n    u.size       0.6582015 0.15295415  4.303260 \n     1.683031e-05\n    nucl         0.5725902 0.09922549  5.770596 \n     7.899178e-09\n\n```", "```py\n    > reduce.fit <- glm(class ~ thick + u.size + nucl, \n       family = binomial, data = train)\n\n```", "```py\n    > test.cv.probs <- predict(reduce.fit, newdata = \n       test, type = \"response\")\n    > misClassError(testY, test.cv.probs)\n    [1] 0.0383\n    > confusionMatrix(testY, test.cv.probs)\n        0    1\n    0 139    5\n    1   3   62\n\n```", "```py\n    > bestglm(Xy = Xy, IC = \"BIC\", family = binomial)\n    Morgan-Tatar search since family is non-gaussian.\n    BIC\n    BICq equivalent for q in (0.273173435514231, \n      0.577036596263757)\n    Best Model:\n     Estimate Std. Error   z value     Pr(>|z|)\n    (Intercept) -8.6169613 1.03155250 -8.353391 \n      6.633065e-17\n    thick        0.7113613 0.14751510  4.822295 \n      1.419160e-06\n    adhsn        0.4537948 0.15034294  3.018398 \n      2.541153e-03\n    nucl         0.5579922 0.09848156  5.665956 \n      1.462068e-08\n    n.nuc        0.4290854 0.11845720  3.622282 \n      2.920152e-04\n\n```", "```py\n    > bic.fit <- glm(class ~ thick + adhsn + nucl + \n       n.nuc, family = binomial, data = train)\n    > test.bic.probs <- predict(bic.fit, newdata = \n       test, type = \"response\")\n    > misClassError(testY, test.bic.probs)\n    [1] 0.0239\n    > confusionMatrix(testY, test.bic.probs)\n        0    1\n    0 138    1\n    1   4   66\n\n```", "```py\n    > lda.fit <- lda(class ~ ., data = train)\n    > lda.fit\n    Call:\n    lda(class ~ ., data = train)\n    Prior probabilities of groups:\n       benign malignant\n    0.6371308 0.3628692\n    Group means:\n thick  u.size u.shape   adhsn  s.size    nucl   \n      chrom\n    benign    2.9205 1.30463 1.41390 1.32450 2.11589 \n      1.39735 2.08278\n    malignant 7.1918 6.69767 6.68604 5.66860 5.50000 \n      7.67441 5.95930\n                n.nuc     mit\n    benign    1.22516 1.09271\n    malignant 5.90697 2.63953\n    Coefficients of linear discriminants:\n                    LD1\n    thick    0.19557291\n    u.size   0.10555201\n    u.shape  0.06327200\n    adhsn    0.04752757\n    s.size   0.10678521\n    nucl     0.26196145\n    chrom    0.08102965\n    n.nuc    0.11691054\n    mit     -0.01665454\n\n```", "```py\n    > plot(lda.fit, type = \"both\")\n\n```", "```py\n    > train.lda.probs <- predict(lda.fit)$posterior[, \n      2]\n    > misClassError(trainY, train.lda.probs)\n [1] 0.0401\n > confusionMatrix(trainY, train.lda.probs)\n 0    1\n 0 296   13\n 1   6  159 \n\n```", "```py\n    > test.lda.probs <- predict(lda.fit, newdata = \n       test)$posterior[, 2]\n    > misClassError(testY, test.lda.probs)\n    [1] 0.0383\n    > confusionMatrix(testY, test.lda.probs)\n        0    1\n    0 140    6\n    1   2   61\n\n```", "```py\n    > qda.fit = qda(class ~ ., data = train) \n    > qda.fit\n    Call:\n    qda(class ~ ., data = train)\n    Prior probabilities of groups:\n       benign malignant\n    0.6371308 0.3628692\n    Group means:\n Thick u.size u.shape  adhsn s.size   nucl  chrom  \n     n.nuc\n    benign    2.9205 1.3046  1.4139 1.3245 2.1158 \n      1.3973 2.0827 1.2251\n    malignant 7.1918 6.6976  6.6860 5.6686 5.5000 \n      7.6744 5.9593 5.9069\n                   mit\n    benign    1.092715\n    malignant 2.639535\n\n```", "```py\n > train.qda.probs <- predict(qda.fit)$posterior[,          \n      2]\n > misClassError(trainY, train.qda.probs)\n [1] 0.0422\n > confusionMatrix(trainY, train.qda.probs)\n 0    1\n 0 287    5\n 1  15  167\n > test.qda.probs <- predict(qda.fit, newdata = \n      test)$posterior[, 2]\n > misClassError(testY, test.qda.probs)\n [1] 0.0526\n > confusionMatrix(testY, test.qda.probs)\n 0    1\n 0 132    1\n 1  10   66 \n\n```", "```py\n > library(earth)\n > set.seed(1)\n > earth.fit <- earth(class ~ ., data = train,\n pmethod = \"cv\",\n nfold = 5,\n ncross = 3,\n degree = 1,\n minspan = -1,\n glm=list(family=binomial)\n ) \n\n```", "```py\n > summary(earth.fit)\n Call: earth(formula=class~., data=train, \n      pmethod=\"cv\",\n glm=list(family=binomial), degree=1, ncross=3, \n      nfold=5, minspan=-1)\n GLM coefficients\n malignant\n (Intercept) -6.5746417\n u.size 0.1502747\n adhsn 0.3058496\n s.size 0.3188098\n nucl 0.4426061\n n.nuc 0.2307595\n h(thick-3) 0.7019053\n h(3-chrom) -0.6927319\n Earth selected 8 of 10 terms, and 7 of 9 predictors \n      using pmethod=\"cv\"\n Termination condition: RSq changed by less than \n      0.001 at 10 terms\n Importance: nucl, u.size, thick, n.nuc, chrom, \n      s.size, adhsn, \n      u.shape-unused,  ...\n Number of terms at each degree of interaction: 1 7 \n      (additive model)\n Earth GRSq 0.8354593 RSq 0.8450554 mean.oof.RSq \n      0.8331308 (sd 0.0295)\n GLM null.deviance 620.9885 (473 dof) deviance \n      81.90976 (466 dof) \n      iters 8\n pmethod=\"backward\" would have selected the same \n      model:\n 8 terms 7 preds, GRSq 0.8354593 RSq 0.8450554 \n      mean.oof.RSq \n      0.8331308\n\n```", "```py\n > plotmo(earth.fit)\n\n```", "```py\n > plotd(earth.fit)\n\n```", "```py\n > evimp(earth.fit)\n nsubsets   gcv   rss\n nucl          7 100.0 100.0\n u.size        6  44.2  44.8\n thick         5  23.8  25.1\n n.nuc         4  15.1  16.8\n chrom         3   8.3  10.7\n s.size        2   6.0   8.1\n adhsn         1   2.3   4.6\n\n```", "```py\n > test.earth.probs <- predict(earth.fit, newdata = \n      test, type = \"response\")\n > misClassError(testY, test.earth.probs)\n [1] 0.0287\n > confusionMatrix(testY, test.earth.probs)\n 0    1\n 0 138    2\n 1   4   65\n\n```", "```py\n    > library(ROCR)\n    > bad.fit <- glm(class ~ thick, family = binomial, \n      data = test)\n    > test.bad.probs = predict(bad.fit, type = \n      \"response\") #save \n      probabilities\n\n```", "```py\n    > pred.full <- prediction(test.probs, test$class)\n\n```", "```py\n    > perf.full <- performance(pred.full, \"tpr\", \"fpr\")\n\n```", "```py\n    > plot(perf.full, main = \"ROC\", col = 1)\n\n```", "```py\n    > pred.bic <- prediction(test.bic.probs, \n      test$class)\n    > perf.bic <- performance(pred.bic, \"tpr\", \"fpr\")\n    > plot(perf.bic, col = 2, add = TRUE)\n\n```", "```py\n    > pred.bad <- prediction(test.bad.probs, \n      test$class)\n    > perf.bad <- performance(pred.bad, \"tpr\", \"fpr\")\n    > plot(perf.bad, col = 3, add = TRUE)\n    > pred.earth <- prediction(test.earth.probs, \n      test$class)\n    > perf.earth <- performance(pred.earth, \"tpr\", \n      \"fpr\")\n    > plot(perf.earth, col = 4, add = TRUE)\n    > legend(0.6, 0.6, c(\"FULL\", \"BIC\", \"BAD\", \n      \"EARTH\"), 1:4)\n\n```", "```py\n > performance(pred.full, \"auc\")@y.values\n [[1]]\n [1] 0.9972672\n > performance(pred.bic, \"auc\")@y.values\n [[1]]\n [1] 0.9944293\n > performance(pred.bad, \"auc\")@y.values\n [[1]]\n [1] 0.8962056\n > performance(pred.earth, \"auc\")@y.values\n [[1]]\n [1] 0.9952701 \n\n```"]