- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment Lexicons and Vector-Space Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have the machinery that we need to make systems that find emotions in
    texts – **natural language processing** (**NLP**) algorithms for converting raw
    texts into feature sets and machine learning algorithms for extracting patterns
    from feature sets. Over the next few chapters, we will develop a series of emotion
    mining algorithms, very simple ones to start with, leading up to sophisticated
    algorithms that use a variety of advanced techniques.
  prefs: []
  type: TYPE_NORMAL
- en: While doing so, we will use a collection of datasets and a variety of measures
    to test each algorithm and compare the effectiveness of the various preprocessing
    steps. So, this chapter will start by considering the datasets and metrics that
    we will be using as we develop the various algorithms. Once we have the datasets
    and metrics in place, we will consider very simple classifiers based purely on
    sentiment lexicons, and we will look at ways of calculating how strongly an individual
    word expresses a sentiment. This will provide us with a baseline for looking at
    the performance of more sophisticated algorithms in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Datasets and metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment lexicons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting a sentiment lexicon from a corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector-space models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets and metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Over the next few chapters, we will look at several emotion-mining algorithms.
    Before we do so, we need to consider exactly what these algorithms are designed
    to do. There are several slightly different tasks for emotion mining algorithms,
    and we need to be clear about which of these tasks a given algorithm is aimed
    at:'
  prefs: []
  type: TYPE_NORMAL
- en: You might just want to know whether the texts you are looking at are positive
    or negative, or you might want a finer-grained classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might assume that each text expresses exactly one emotion, or at most one
    emotion, or that a text can express several (or no) emotions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might want to know how strongly a text expresses an emotion. For example,
    *I’m a bit irritated by that* and *That makes me absolutely furious* both express
    anger, but the second clearly expresses it much more strongly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will concentrate on algorithms that aim to assign multiple (or no) labels
    to each tweet, with the labels drawn from some collection of candidate emotions,
    ranging from just positive and negative to larger sets drawn from Plutchik’s wheel.
    We will refer to datasets where a single tweet can have zero or more labels as
    **multi-label** datasets. This is to be distinguished from **multi-class** datasets,
    where there are several labels available but exactly one is assigned to each tweet.
    Multi-label datasets are significantly more difficult than simple multi-class
    ones, and the task also gets harder as the set of labels gets larger (it may,
    for instance, be difficult to distinguish between anger and disgust, but they
    are both negative); and it also gets harder if we don’t have a preconception about
    how many emotions are expressed. Since most of the learning algorithms depend
    on comparing some score obtained from the text with a threshold, we can usually
    use this score to assess how strongly a text expresses an emotion rather than
    just whether it expresses it or not. We will mainly concentrate on deciding whether
    a text expresses an emotion rather than how strongly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a selection of the datasets listed in [*Chapter 2*](B18714_02.xhtml#_idTextAnchor061),
    *Building and Using a Dataset*, to train and test the various models that we will
    develop:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Workshop on Computational Approaches to Subjectivity, Sentiment & Social
    Media Analysis** (**WASSA**) dataset, which contains 3.9K English tweets, each
    labeled with one of anger, fear, joy, and sadness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Semeval 2018 Task E_c dataset, which contains a moderate number of tweets
    in English, Arabic, and Spanish, where a fairly high percentage of tweets contain
    emojis, with each tweet labeled with 0 or more emotions from a standard set of
    11 emotions. This dataset contains 7.7K English tweets, 2.9K Arabic tweets, and
    4.2K Spanish tweets. We will refer to this as the SEM-11 set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Semeval 2016 Task El-reg and El-oc dataset, where the El-reg dataset has
    tweets labeled with a score from 0 to 1 for each of a set of four emotions, and
    the El-oc dataset has tweets ranked in terms of which emotion they express. The
    combinations of these datasets, which we will refer to as the SEM4 set, contain
    7.6K English tweets, 2.8K Arabic, and 2.6K Spanish.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CARER dataset is large (slightly over 400K tweets) and has labels for six
    emotions (anger, fear, joy, love, sadness, and surprise). Each tweet is assigned
    exactly one emotion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The IMDb dataset contains 50K negative and positive film reviews and provides
    an interesting test of the robustness of the various algorithms since it is split
    into just two categories (positive and negative), which makes the task of learning
    to classify documents easier. The reviews contain anything from 100 to 1,000 words,
    which is much longer than a tweet and poses a different set of problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A collection of Kuwaiti tweets, annotated either by assigning a label if all
    three annotators unanimously assigned that label (KWT.U) or if at least two of
    them did (KWT.M). This set is particularly interesting because, in a large number
    of cases, the annotators agreed that a tweet expressed no emotions and in some,
    it expressed several, which poses a substantial challenge for classifiers that
    assign a single label to every observation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These datasets provide enough variation to help us verify that a given approach
    to the task of finding emotions is robust across different conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: The WASSA, SEM4, and SEM11 datasets contain emojis, which makes the task of
    emotion mining slightly easier because the main (sole?) point of using emojis
    is to express emotions, though they are sometimes used in slightly surprising
    ways.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SEM4 and SEM11 datasets are multilingual, with data supplied in English,
    Arabic, and Spanish. This helps when trying out approaches that are intended to
    be language-independent since the methodology for collecting the three languages
    is the same.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SEM11 set contains tweets with differing numbers of emotions, including
    none, which can make the task of assigning emotions considerably harder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The CARER dataset is very large, though it does not contain any emojis or hashtags:
    this makes it possible to carry out investigations into how performance varies
    with the size of the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The IMDb set has just two labels, but very long texts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The KWT sets have tweets with zero, one, or more emotions, but this time, a
    very large proportion have zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given that these datasets are supplied in different formats, we need, as usual,
    a common format for representing them. We will use two basic classes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A tweet is an object with a sequence of tokens, a term frequency table, and
    possibly a Gold Standard set of labels, along with several bookkeeping properties:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A dataset is a set containing a set of tweets, a list of emotion names, the
    Gold Standard labels for the tweets, and, again, some bookkeeping properties.
    The most useful of these is an index that assigns each word in the dataset a unique
    index. We will frequently use this in later sections, so it is worth looking at
    how we do it here. The basic idea is that we read the dataset word by word. If
    the word we have just read is already in the index, there is nothing to be done.
    If it is not, then we assign it the current length of the index: this ensures
    that every word gets assigned a unique identifier; once we have added the current
    word, the length of the index will be incremented by one, so the next new word
    will get a new index:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate an index, as shown here, where each word has a unique identifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'class DATASET:    def __init__(self, emotions, tweets, idf, ARGS, N=sys.maxsize):'
  prefs: []
  type: TYPE_NORMAL
- en: self.emotions = sorted(emotions)
  prefs: []
  type: TYPE_NORMAL
- en: self.tweets = tweets
  prefs: []
  type: TYPE_NORMAL
- en: self.GS = [tweet.GS for tweet in self.tweets][:N]
  prefs: []
  type: TYPE_NORMAL
- en: self.idf = idf
  prefs: []
  type: TYPE_NORMAL
- en: self.words = [w[0] for w in reversed(sortTable(idf))]
  prefs: []
  type: TYPE_NORMAL
- en: self.makeIndex()
  prefs: []
  type: TYPE_NORMAL
- en: self.ARGS = ARGS
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: ID     Tweet                    anger  disgust  fear21441  Worry is a down payment    0      1        0
  prefs: []
  type: TYPE_NORMAL
- en: '1535   it makes you #happy.       0      0        0'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '{"text":"i feel awful about it","label":0}{"text":"i really do feel proud of
    myself","label":1}'
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: def convert(self):    # extract the labels from dataset_infos.json
  prefs: []
  type: TYPE_NORMAL
- en: with open(os.path.join(self.DOWNLOAD,
  prefs: []
  type: TYPE_NORMAL
- en: '"dataset_infos.json")) as jsfile:'
  prefs: []
  type: TYPE_NORMAL
- en: infos = json.load(jsfile)
  prefs: []
  type: TYPE_NORMAL
- en: self.labels = infos["default"]["features"]\
  prefs: []
  type: TYPE_NORMAL
- en: '["label"]["names"]'
  prefs: []
  type: TYPE_NORMAL
- en: '# read the data line by line from data.jsonl'
  prefs: []
  type: TYPE_NORMAL
- en: with open(os.path.join(self.PATH, "data.jsonl"))\
  prefs: []
  type: TYPE_NORMAL
- en: 'as input:'
  prefs: []
  type: TYPE_NORMAL
- en: d = [json.loads(line) for line in input]
  prefs: []
  type: TYPE_NORMAL
- en: '# initialise the output with a header line'
  prefs: []
  type: TYPE_NORMAL
- en: csv = "ID\ttext\t%s\n"%("\t".join(self.labels))
  prefs: []
  type: TYPE_NORMAL
- en: '# Go through the data writing each line as the ID,'
  prefs: []
  type: TYPE_NORMAL
- en: '# the text itself and an appropriate set of 0s and 1s'
  prefs: []
  type: TYPE_NORMAL
- en: 'for i, x in enumerate(d):'
  prefs: []
  type: TYPE_NORMAL
- en: cols = ["1" if x['label'] == i else "0"\
  prefs: []
  type: TYPE_NORMAL
- en: for i in range(len(self.labels))]
  prefs: []
  type: TYPE_NORMAL
- en: csv += "%s\t%s\t%s\n"%(i, x['text'],"\t".join(cols))
  prefs: []
  type: TYPE_NORMAL
- en: '# Save this as wholething.csv inside CARER/EN'
  prefs: []
  type: TYPE_NORMAL
- en: 'with open(os.path.join(self.PATH, "wholething.csv"), "w") as out:'
  prefs: []
  type: TYPE_NORMAL
- en: out.write(csv)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: def makeDATASET(src, N=sys.maxsize, args=None):    dataset = [line.strip() for
    line in open(src)][:N]
  prefs: []
  type: TYPE_NORMAL
- en: emotions = None
  prefs: []
  type: TYPE_NORMAL
- en: tweets = []
  prefs: []
  type: TYPE_NORMAL
- en: 'for tweet in dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'if emotions is None:'
  prefs: []
  type: TYPE_NORMAL
- en: emotions = tweet.split()[2:]
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: tweets.append(makeTweet(tweet, args=args))
  prefs: []
  type: TYPE_NORMAL
- en: pruned = prune(tweets)
  prefs: []
  type: TYPE_NORMAL
- en: random.seed(0); random.shuffle(pruned)
  prefs: []
  type: TYPE_NORMAL
- en: df = counter()
  prefs: []
  type: TYPE_NORMAL
- en: index = {}
  prefs: []
  type: TYPE_NORMAL
- en: 'for i, tweet in enumerate(tweets):'
  prefs: []
  type: TYPE_NORMAL
- en: 'for w in tweet.tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: df.add(w)
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: remove singletons from the idf count
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: idf = {}
  prefs: []
  type: TYPE_NORMAL
- en: 'for w in list(df.keys()):'
  prefs: []
  type: TYPE_NORMAL
- en: idf[w] = 1.0/float(df[w]+1)
  prefs: []
  type: TYPE_NORMAL
- en: return DATASET(emotions, tweets, df, idf, args=args)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: def makeTweet(tweet, args):    tweet = tweet.strip().split("\t")
  prefs: []
  type: TYPE_NORMAL
- en: scores = numpy.array([int(score) for score in tweet[2:]])
  prefs: []
  type: TYPE_NORMAL
- en: tweet, text tweet[0], tweet[1]
  prefs: []
  type: TYPE_NORMAL
- en: 'if args["language"] == "AR":'
  prefs: []
  type: TYPE_NORMAL
- en: tokens = a2bw.convert(text, a2bw.a2bwtable).split()
  prefs: []
  type: TYPE_NORMAL
- en: 'if args["stemmer"] == "standard":'
  prefs: []
  type: TYPE_NORMAL
- en: tokens = stemmer.stemAll(tokens, stemmer.TWEETGROUPS)
  prefs: []
  type: TYPE_NORMAL
- en: 'elif args["language"] == "ES":'
  prefs: []
  type: TYPE_NORMAL
- en: 'elif args["language"] == "EN":'
  prefs: []
  type: TYPE_NORMAL
- en: tf = counter()
  prefs: []
  type: TYPE_NORMAL
- en: 'for w in tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: tf.add(word)
  prefs: []
  type: TYPE_NORMAL
- en: return TWEET(id=tweet,tf=tf,scores=scores,tokens=tokens,args=args)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'class BASECLASSIFIER():    def applyToTweets(self, dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: return [self.applyToTweet(tweet) for tweet in dataset.tweets]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'EMOLEX="CORPORA/NRC-Emotion-Lexicon/Arabic-NRC-EMOLEX.txt"def readNRC(ifile=EMOLEX,
    targets=None, ARGS=False):'
  prefs: []
  type: TYPE_NORMAL
- en: lines = list(open(ifile))
  prefs: []
  type: TYPE_NORMAL
- en: '# emotions is the list of emotions in the EMOLEX file'
  prefs: []
  type: TYPE_NORMAL
- en: '# targets is the list of emotions in the dataset that'
  prefs: []
  type: TYPE_NORMAL
- en: '# the classifieris going to be applied to.'
  prefs: []
  type: TYPE_NORMAL
- en: emotions = lines[0].strip().split("\t")[1:-1]
  prefs: []
  type: TYPE_NORMAL
- en: emotionIndex = [True if e in targets else False for e in emotions]
  prefs: []
  type: TYPE_NORMAL
- en: targetIndex = [True if e in emotions else False for e in targets]
  prefs: []
  type: TYPE_NORMAL
- en: lex = {}
  prefs: []
  type: TYPE_NORMAL
- en: '# add entries line by line'
  prefs: []
  type: TYPE_NORMAL
- en: 'for line in lines[1:]:'
  prefs: []
  type: TYPE_NORMAL
- en: line = line.split("\t")
  prefs: []
  type: TYPE_NORMAL
- en: '# if we''re doing it for English'
  prefs: []
  type: TYPE_NORMAL
- en: 'if ARGS.Language == "EN":'
  prefs: []
  type: TYPE_NORMAL
- en: form = line[0]
  prefs: []
  type: TYPE_NORMAL
- en: 'if ARGS.Stemmer.startswith("justRoot"):'
  prefs: []
  type: TYPE_NORMAL
- en: form = justroot(form)
  prefs: []
  type: TYPE_NORMAL
- en: 'elif ARGS.Stemmer.startswith("morphyroot"):'
  prefs: []
  type: TYPE_NORMAL
- en: form = morphyroot(form)
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: 'raise Exception("Unknown language: %s"%(ARGS.Language))'
  prefs: []
  type: TYPE_NORMAL
- en: '# The line we just read is a string, so the values'
  prefs: []
  type: TYPE_NORMAL
- en: '# for the emotions are "0" and "1". We want them as'
  prefs: []
  type: TYPE_NORMAL
- en: '# ints, and we also only want the ones that appear'
  prefs: []
  type: TYPE_NORMAL
- en: '# in emotionIndex, i.e. ones that are present in'
  prefs: []
  type: TYPE_NORMAL
- en: '# the lexicon and in the target dataset'
  prefs: []
  type: TYPE_NORMAL
- en: lex[form] \
  prefs: []
  type: TYPE_NORMAL
- en: = [int(x) for (x, y) in zip(line[1:-1], emotionIndex) if y]
  prefs: []
  type: TYPE_NORMAL
- en: return lex, emotionIndex, targetIndex
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: elif ARGS.Language == "AR":            form = line[-1].strip()
  prefs: []
  type: TYPE_NORMAL
- en: form = a2bw.convert(form, a2bw.a2bwtable)
  prefs: []
  type: TYPE_NORMAL
- en: 'if ARGS.Stemmer == "SEM":'
  prefs: []
  type: TYPE_NORMAL
- en: form = stemArabic(form)
  prefs: []
  type: TYPE_NORMAL
- en: 'elif ARGS.Language == "ES":'
  prefs: []
  type: TYPE_NORMAL
- en: form = line[-1].strip()
  prefs: []
  type: TYPE_NORMAL
- en: 'if ARGS.Stemmer.startswith("stemSpanish"):'
  prefs: []
  type: TYPE_NORMAL
- en: form = stemSpanish(form)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'def calculateScores(self):        for word, cols in self.dataset.index.items():'
  prefs: []
  type: TYPE_NORMAL
- en: '# set up a list of zeros to correspond to the'
  prefs: []
  type: TYPE_NORMAL
- en: '# emotions in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: self.scoredict[word] = [0]*len(self.emotions)
  prefs: []
  type: TYPE_NORMAL
- en: '# count the non-zero emotions for this word'
  prefs: []
  type: TYPE_NORMAL
- en: s = sum(len(col) for col in cols.values())
  prefs: []
  type: TYPE_NORMAL
- en: 'if s > 0:'
  prefs: []
  type: TYPE_NORMAL
- en: 'for col in cols:'
  prefs: []
  type: TYPE_NORMAL
- en: '# use s to rebalance the scores for'
  prefs: []
  type: TYPE_NORMAL
- en: '# emotions for this word so they add up'
  prefs: []
  type: TYPE_NORMAL
- en: '# to 1'
  prefs: []
  type: TYPE_NORMAL
- en: self.scoredict[word][self.colindex[col]]
  prefs: []
  type: TYPE_NORMAL
- en: = len(cols[col])/s
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: def applyToTweet(self, tweet):        scores = [0]*len(self.emotions)
  prefs: []
  type: TYPE_NORMAL
- en: 'for token in tweet.tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: 'if token and token in self.scoredict:'
  prefs: []
  type: TYPE_NORMAL
- en: for i, x in enumerate(
  prefs: []
  type: TYPE_NORMAL
- en: 'self.scoredict[token]):'
  prefs: []
  type: TYPE_NORMAL
- en: scores[i] += x
  prefs: []
  type: TYPE_NORMAL
- en: m = max(scores)
  prefs: []
  type: TYPE_NORMAL
- en: return [1 if x >= m*self.threshold else 0 for x in scores]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'def bestThreshold(self, test=None, show=False):        if test is None:'
  prefs: []
  type: TYPE_NORMAL
- en: test = self.test.tweets
  prefs: []
  type: TYPE_NORMAL
- en: '# Apply this classifier to the tweets we are'
  prefs: []
  type: TYPE_NORMAL
- en: '# interested in: setting probs=True forces it to'
  prefs: []
  type: TYPE_NORMAL
- en: '# return the values actually calculated by the'
  prefs: []
  type: TYPE_NORMAL
- en: '# classifier rather than the 0/1 version obtained'
  prefs: []
  type: TYPE_NORMAL
- en: '# by using the threshold'
  prefs: []
  type: TYPE_NORMAL
- en: train = self.train.tweets[:len(test)]
  prefs: []
  type: TYPE_NORMAL
- en: l = self.applyToTweets(train, threshold=0,
  prefs: []
  type: TYPE_NORMAL
- en: probs=True)
  prefs: []
  type: TYPE_NORMAL
- en: '# The optimal threshold must lie somewhere between'
  prefs: []
  type: TYPE_NORMAL
- en: '# the smallest and largest scores for any tweet'
  prefs: []
  type: TYPE_NORMAL
- en: start = threshold = min(min(tweet.predicted) for tweet in train)
  prefs: []
  type: TYPE_NORMAL
- en: end = max(max(tweet.predicted) for tweet in train)
  prefs: []
  type: TYPE_NORMAL
- en: best = []
  prefs: []
  type: TYPE_NORMAL
- en: '# Go from start to end in small steps using'
  prefs: []
  type: TYPE_NORMAL
- en: '# increasing values for threshold'
  prefs: []
  type: TYPE_NORMAL
- en: 'while threshold <= end:'
  prefs: []
  type: TYPE_NORMAL
- en: l = self.applyToTweets(train,
  prefs: []
  type: TYPE_NORMAL
- en: threshold=threshold)
  prefs: []
  type: TYPE_NORMAL
- en: '# getmetrics returns macro F1, true positives,'
  prefs: []
  type: TYPE_NORMAL
- en: '# false positives, false negatives'
  prefs: []
  type: TYPE_NORMAL
- en: (macroF, tp, fp, fn)
  prefs: []
  type: TYPE_NORMAL
- en: = metrics.getmetrics([tweet.GS for tweet in test], l)
  prefs: []
  type: TYPE_NORMAL
- en: '# Jaccard'
  prefs: []
  type: TYPE_NORMAL
- en: j = tp/(tp+fp+fn)
  prefs: []
  type: TYPE_NORMAL
- en: best = max(best, [j, threshold])
  prefs: []
  type: TYPE_NORMAL
- en: threshold += (end-start)/20
  prefs: []
  type: TYPE_NORMAL
- en: return round(best[1], 5)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'def calculateScores(self):        for word, cols in self.dataset.index.items():'
  prefs: []
  type: TYPE_NORMAL
- en: best = False
  prefs: []
  type: TYPE_NORMAL
- en: bestscore = -1
  prefs: []
  type: TYPE_NORMAL
- en: self.scoredict[word] = [0]*len(self.emotions)
  prefs: []
  type: TYPE_NORMAL
- en: 'for col in cols:'
  prefs: []
  type: TYPE_NORMAL
- en: self.scoredict[word][self.colindex[col]]
  prefs: []
  type: TYPE_NORMAL
- en: = len(cols[col])
  prefs: []
  type: TYPE_NORMAL
- en: s = sum(self.scoredict[word])
  prefs: []
  type: TYPE_NORMAL
- en: 'for i, x in enumerate(self.scoredict[word]):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if s > 0:'
  prefs: []
  type: TYPE_NORMAL
- en: x = x/s-1/len(self.emotions))
  prefs: []
  type: TYPE_NORMAL
- en: self.scoredict[word][i] = max(0, x)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '{..., ''days'': 6, ''sober'': 7, ''do'': 8, "n''t": 9, ''wanna'': 10,  …}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: def sentence2vector(sentence, index):    vector = numpy.zeros(len(index))
  prefs: []
  type: TYPE_NORMAL
- en: 'for word in sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: vector[index[word]] += 1
  prefs: []
  type: TYPE_NORMAL
- en: return vector
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '>>> list(sentence2vector("I do n''t want to be sober".split(), index))[0.,
    0., 1., 0., 0., 0., 0., 1., 1., 1., ...]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '>>> # v is the vector we just made; convert it to a sparse matrix>>> s = sparse.csr_matrix(v)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> # it contains seven 1s'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> list(s.data)'
  prefs: []
  type: TYPE_NORMAL
- en: '[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> # These are at positions 2, 7, 8, ...'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> list(s.indices)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2, 7, 8, 9, 119, 227, 321]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'S0: John ate the pasta[63, 2306, 3304, 7616]'
  prefs: []
  type: TYPE_NORMAL
- en: 'S1: John ate some pasta'
  prefs: []
  type: TYPE_NORMAL
- en: '[229, 2306, 3304, 7616]'
  prefs: []
  type: TYPE_NORMAL
- en: 'S2: John ate the potatoes'
  prefs: []
  type: TYPE_NORMAL
- en: '[63, 3304, 4162, 7616]'
  prefs: []
  type: TYPE_NORMAL
- en: 'S3: Mary drank some beer'
  prefs: []
  type: TYPE_NORMAL
- en: '[229, 5040, 5176, 10372]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: S0      S1      S2      S3S0    1.00    0.75    0.75    0.25
  prefs: []
  type: TYPE_NORMAL
- en: S1    0.75    1.00    0.50    0.00
  prefs: []
  type: TYPE_NORMAL
- en: S2    0.75    0.50    1.00    0.25
  prefs: []
  type: TYPE_NORMAL
- en: S3    0.25    0.00    0.25    1.00
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: def getDF(documents, uselog=numpy.log):    # adding something to df either sets
    or increments a counter
  prefs: []
  type: TYPE_NORMAL
- en: df = counter()
  prefs: []
  type: TYPE_NORMAL
- en: 'for document in enumerate(documents):'
  prefs: []
  type: TYPE_NORMAL
- en: '# for each unique word in the document increment df'
  prefs: []
  type: TYPE_NORMAL
- en: 'for w in set(document.split()):'
  prefs: []
  type: TYPE_NORMAL
- en: df.add(w)
  prefs: []
  type: TYPE_NORMAL
- en: idf = {}
  prefs: []
  type: TYPE_NORMAL
- en: 'for w in df:'
  prefs: []
  type: TYPE_NORMAL
- en: idf[w] = 1.0/float(uselog(df[w])+1)
  prefs: []
  type: TYPE_NORMAL
- en: return df, idf
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: DF    IDFa      1521  0.001
  prefs: []
  type: TYPE_NORMAL
- en: the    1842  0.001
  prefs: []
  type: TYPE_NORMAL
- en: cat    5     0.167
  prefs: []
  type: TYPE_NORMAL
- en: loves  11    0.083
  prefs: []
  type: TYPE_NORMAL
- en: man    85    0.012
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: def sentence2vector(sentence, index, idf={}):    vector = numpy.zeros(len(index))
  prefs: []
  type: TYPE_NORMAL
- en: 'for word in sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: inc = idf[word] if word in idf else 1
  prefs: []
  type: TYPE_NORMAL
- en: vector[index[word]] += inc
  prefs: []
  type: TYPE_NORMAL
- en: return vector
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '>>> list(S1.data)[0.008, 0.3333333333333333, 0.1, 0.5]'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> list(S1.indices)'
  prefs: []
  type: TYPE_NORMAL
- en: '[229, 2306, 3304, 7616]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'king  {''king'': 144, ''new'': 88, ''queen'': 84, ''royal'': 69, ''made'':
    68,...}queen  {''mother'': 123, ''speech'': 86, ''king'': 84, ''royal'': 62, ...}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'class TF-IDFMODE():    def __init__(self, uselog=log, corpus=corpora.BNC, N=10000):'
  prefs: []
  type: TYPE_NORMAL
- en: self.pairs0 = getPairs(corpus)
  prefs: []
  type: TYPE_NORMAL
- en: self.df = sortTable(getDF(self.pairs0))[:N]
  prefs: []
  type: TYPE_NORMAL
- en: self.df = {x[0]:x[1] for x in self.df}
  prefs: []
  type: TYPE_NORMAL
- en: self.pairs1 = {}
  prefs: []
  type: TYPE_NORMAL
- en: 'for word in self.pairs0:'
  prefs: []
  type: TYPE_NORMAL
- en: 'if word in self.df:'
  prefs: []
  type: TYPE_NORMAL
- en: self.pairs1[word] = {}
  prefs: []
  type: TYPE_NORMAL
- en: 'for other in self.pairs0[word]:'
  prefs: []
  type: TYPE_NORMAL
- en: 'if other in self.df:'
  prefs: []
  type: TYPE_NORMAL
- en: self.pairs1[word][other]\
  prefs: []
  type: TYPE_NORMAL
- en: = self.pairs0[word][other]
  prefs: []
  type: TYPE_NORMAL
- en: self.pairs2 = applyIDF(self.pairs1, df=self.df, uselog=uselog)
  prefs: []
  type: TYPE_NORMAL
- en: self.dimensions, self.invdimensions, self.matrices\
  prefs: []
  type: TYPE_NORMAL
- en: = pairs2matrix(self.pairs2)
  prefs: []
  type: TYPE_NORMAL
- en: self.similarities = cosine_similarity(
  prefs: []
  type: TYPE_NORMAL
- en: self.matrices)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: cat:  mouse:0.03, litter:0.02, ginger:0.02, stray:0.02, pet:0.02dog:  stray:0.05,
    bark:0.03, pet:0.03, shepherd:0.03, vet:0.02
  prefs: []
  type: TYPE_NORMAL
- en: eat:  sandwiches:0.03, foods:0.03, bite:0.03, meat:0.02, cake:0.02
  prefs: []
  type: TYPE_NORMAL
- en: 'drink: sipped:0.08, alcoholic:0.03, pints:0.03, merry:0.02, relaxing:0.02'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: def nearest(self, word, N=6):        similarwords = self.similarities[self.dimensions[word]]
  prefs: []
  type: TYPE_NORMAL
- en: matches = list(reversed(sorted([x, i]\
  prefs: []
  type: TYPE_NORMAL
- en: for i, x in enumerate(similarwords)))[1:N]
  prefs: []
  type: TYPE_NORMAL
- en: return [(self.invdimensions[i], s) for [s, i] in matches]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Best matches for cat:dog:0.39,cats:0.25,keyboard:0.23,bin:0.23,embryo:0.22
  prefs: []
  type: TYPE_NORMAL
- en: 'Best matches for dog:'
  prefs: []
  type: TYPE_NORMAL
- en: dogs:0.42,cat:0.39,cats:0.35,hairs:0.26,bullet:0.24
  prefs: []
  type: TYPE_NORMAL
- en: 'Best matches for eat:'
  prefs: []
  type: TYPE_NORMAL
- en: ate:0.35,eaten:0.35,cakes:0.28,eating:0.28,buffet:0.27
  prefs: []
  type: TYPE_NORMAL
- en: 'Best matches for drink:'
  prefs: []
  type: TYPE_NORMAL
- en: brandy:0.41,beer:0.41,coffee:0.38,lager:0.38,drinks:0.36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 61.0    26.0    54.0    90.0    9.0    19.034.0    53.0    73.0    21.0    17.0    67.0
  prefs: []
  type: TYPE_NORMAL
- en: 59.0    75.0    33.0    96.0    59.0    24.0
  prefs: []
  type: TYPE_NORMAL
- en: 72.0    90.0    79.0    88.0    48.0    45.0
  prefs: []
  type: TYPE_NORMAL
- en: 77.0    24.0    88.0    65.0    33.0    94.0
  prefs: []
  type: TYPE_NORMAL
- en: 44.0    0.00    55.0    61.0    71.0    92.0
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: -0.4    0.1    0.3    0.3    0.3   -0.8-0.4   -0.5   -0.5    0.6    0.1    0.2
  prefs: []
  type: TYPE_NORMAL
- en: -0.3    0.3    0.5    0.2    0.4    0.6
  prefs: []
  type: TYPE_NORMAL
- en: -0.5    0.7   -0.5   -0.2   -0.1    0.0
  prefs: []
  type: TYPE_NORMAL
- en: -0.4   -0.5    0.1   -0.7    0.2    0.0
  prefs: []
  type: TYPE_NORMAL
- en: -0.4   -0.1    0.3    0.1   -0.8    0.0
  prefs: []
  type: TYPE_NORMAL
- en: 356.95    103.09    90.57    61.44    53.85    14.53
  prefs: []
  type: TYPE_NORMAL
- en: -0.4   -0.4   -0.3   -0.4   -0.3   -0.3   -0.3   -0.4
  prefs: []
  type: TYPE_NORMAL
- en: -0.4   -0.1    0.7   -0.4   -0.0    0.2   -0.2    0.4
  prefs: []
  type: TYPE_NORMAL
- en: 0.2   -0.5    0.0   -0.1    0.0   -0.5    0.4    0.5
  prefs: []
  type: TYPE_NORMAL
- en: 0.1   -0.2   -0.0    0.3   -0.8    0.3   -0.1    0.3
  prefs: []
  type: TYPE_NORMAL
- en: 0.7   -0.4    0.1   -0.3    0.2    0.2   -0.5   -0.1
  prefs: []
  type: TYPE_NORMAL
- en: -0.3   -0.5   -0.4    0.1    0.4    0.6    0.2    0.1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 76.8    42.8    51.1    46.5    35.2    45.4    40.1    78.972.8    76.4     2.0    78.6    10.9    65.3    16.4    19.8
  prefs: []
  type: TYPE_NORMAL
- en: 59.7    13.3    52.3    22.7    27.5    25.6    36.2    79.2
  prefs: []
  type: TYPE_NORMAL
- en: 26.2    98.3    93.2    36.9    60.7    84.6    19.9    69.9
  prefs: []
  type: TYPE_NORMAL
- en: 92.2    74.3    14.2    57.9    85.8    22.6    52.9    35.9
  prefs: []
  type: TYPE_NORMAL
- en: 44.1    64.1    29.1    69.0    31.9    17.9    76.0    78.0
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Best matches for cat:cats:0.66,hairs:0.62,dog:0.61,dogs:0.60,hair:0.54
  prefs: []
  type: TYPE_NORMAL
- en: 'Best matches for dog:'
  prefs: []
  type: TYPE_NORMAL
- en: dogs:0.72,cats:0.68,cat:0.61,pet:0.54,bull:0.46
  prefs: []
  type: TYPE_NORMAL
- en: 'Best matches for eat:'
  prefs: []
  type: TYPE_NORMAL
- en: meat:0.77,sweets:0.75,ate:0.75,chicken:0.73,delicious:0.72
  prefs: []
  type: TYPE_NORMAL
- en: 'Best matches for drink:'
  prefs: []
  type: TYPE_NORMAL
- en: pint:0.84,sherry:0.83,brandy:0.83,beer:0.81,drank:0.79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: def chooseother(self, token):        # If the classifier has a model, use that
    to find
  prefs: []
  type: TYPE_NORMAL
- en: '# the 5 most similar words to the target and go'
  prefs: []
  type: TYPE_NORMAL
- en: '# through these looking for one that is in the'
  prefs: []
  type: TYPE_NORMAL
- en: '# sentiment lexicon'
  prefs: []
  type: TYPE_NORMAL
- en: 'if self.model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'try:'
  prefs: []
  type: TYPE_NORMAL
- en: 'for other in self.model.nearest(token, topn=5):'
  prefs: []
  type: TYPE_NORMAL
- en: other = other[0]
  prefs: []
  type: TYPE_NORMAL
- en: 'if other in self.scoredict:'
  prefs: []
  type: TYPE_NORMAL
- en: return other
  prefs: []
  type: TYPE_NORMAL
- en: 'except:'
  prefs: []
  type: TYPE_NORMAL
- en: pass
  prefs: []
  type: TYPE_NORMAL
- en: return False
  prefs: []
  type: TYPE_NORMAL
- en: 'def applyToTweet(self, tweet):'
  prefs: []
  type: TYPE_NORMAL
- en: scores = [0]*len(self.emotions)
  prefs: []
  type: TYPE_NORMAL
- en: 'for token in tweet.tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: 'if not token in self.scoredict:'
  prefs: []
  type: TYPE_NORMAL
- en: token = self.chooseother(token)
  prefs: []
  type: TYPE_NORMAL
- en: 'if token in self.scoredict:'
  prefs: []
  type: TYPE_NORMAL
- en: 'for i, x in enumerate(self.scoredict[token]):'
  prefs: []
  type: TYPE_NORMAL
- en: scores[i] += x
  prefs: []
  type: TYPE_NORMAL
- en: m = max(scores)
  prefs: []
  type: TYPE_NORMAL
- en: return [1 if x >= m*self.threshold else 0 for x in scores]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '…cat chosen for kitten: anger:0.05, fear:0.10, joy:0.00, sadness:0.00'
  prefs: []
  type: TYPE_NORMAL
- en: 'fall chosen for plummet: anger:0.00, fear:0.04, joy:0.04, sadness:0.02'
  prefs: []
  type: TYPE_NORMAL
- en: 'restrain chosen for evict: anger:0.72, fear:0.00, joy:0.00, sadness:0.00'
  prefs: []
  type: TYPE_NORMAL
- en: 'arrogance chosen for cynicism: anger:0.72, fear:0.00, joy:0.00, sadness:0.00'
  prefs: []
  type: TYPE_NORMAL
- en: 'overweight chosen for obese: anger:0.00, fear:0.72, joy:0.00, sadness:0.00,
    neutral:0.00'
  prefs: []
  type: TYPE_NORMAL
- en: 'greedy chosen for downtrodden: anger:0.72, fear:0.00, joy:0.00, sadness:0.00'
  prefs: []
  type: TYPE_NORMAL
- en: 'sacred chosen for ancient: anger:0.00, fear:0.72, joy:0.00, sadness:0.00'
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: love ['hate', 'kindness', 'joy', 'passion', 'dread']hate ['adore', 'loathe',
    'despise', 'hated', 'dislike']
  prefs: []
  type: TYPE_NORMAL
- en: adore ['loathe', 'hate', 'despise', 'detest', 'daresay']
  prefs: []
  type: TYPE_NORMAL
- en: happy ['pleased', 'nice', 'glad', 'lucky', 'unhappy']
  prefs: []
  type: TYPE_NORMAL
- en: sad ['funny', 'pathetic', 'miserable', 'depressing', 'strange']
  prefs: []
  type: TYPE_NORMAL
- en: furious ['stunned', 'angry', 'annoyed', 'shocked', 'horrified']
  prefs: []
  type: TYPE_NORMAL
- en: happiness ['sorrow', 'joy', 'fulfilment', 'enjoyment', 'dignity']
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
