- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Sentiment Lexicons and Vector-Space Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感词典和向量空间模型
- en: We now have the machinery that we need to make systems that find emotions in
    texts – **natural language processing** (**NLP**) algorithms for converting raw
    texts into feature sets and machine learning algorithms for extracting patterns
    from feature sets. Over the next few chapters, we will develop a series of emotion
    mining algorithms, very simple ones to start with, leading up to sophisticated
    algorithms that use a variety of advanced techniques.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经拥有了构建在文本中寻找情感的系统的工具——将原始文本转换为特征集的自然语言处理（NLP）算法和从特征集中提取模式的机器学习算法。在接下来的几章中，我们将开发一系列情感挖掘算法，从非常简单的算法开始，逐步发展到使用各种高级技术的复杂算法。
- en: While doing so, we will use a collection of datasets and a variety of measures
    to test each algorithm and compare the effectiveness of the various preprocessing
    steps. So, this chapter will start by considering the datasets and metrics that
    we will be using as we develop the various algorithms. Once we have the datasets
    and metrics in place, we will consider very simple classifiers based purely on
    sentiment lexicons, and we will look at ways of calculating how strongly an individual
    word expresses a sentiment. This will provide us with a baseline for looking at
    the performance of more sophisticated algorithms in later chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程中，我们将使用一系列数据集和多种度量来测试每个算法，并比较各种预处理步骤的有效性。因此，本章将首先考虑我们在开发各种算法时将使用的数据集和度量。一旦我们有了数据集和度量，我们将考虑仅基于情感词典的非常简单的分类器，并探讨计算单个单词表达情感强度的方法。这将为我们提供一个基准，以便在后续章节中查看更复杂算法的性能。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Datasets and metrics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集和度量
- en: Sentiment lexicons
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感词典
- en: Extracting a sentiment lexicon from a corpus
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从语料库中提取情感词典
- en: Vector-space models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量空间模型
- en: Datasets and metrics
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集和度量
- en: 'Over the next few chapters, we will look at several emotion-mining algorithms.
    Before we do so, we need to consider exactly what these algorithms are designed
    to do. There are several slightly different tasks for emotion mining algorithms,
    and we need to be clear about which of these tasks a given algorithm is aimed
    at:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们将探讨几种情感挖掘算法。在我们这样做之前，我们需要考虑这些算法的确切设计目标。情感挖掘算法有几个略有不同的任务，我们需要明确了解给定算法旨在完成哪些任务：
- en: You might just want to know whether the texts you are looking at are positive
    or negative, or you might want a finer-grained classification.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能只想知道你正在查看的文本是积极的还是消极的，或者你可能需要一个更细致的分类。
- en: You might assume that each text expresses exactly one emotion, or at most one
    emotion, or that a text can express several (or no) emotions.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能认为每个文本恰好表达一种情感，或者最多表达一种情感，或者一个文本可以表达几种（或没有）情感。
- en: You might want to know how strongly a text expresses an emotion. For example,
    *I’m a bit irritated by that* and *That makes me absolutely furious* both express
    anger, but the second clearly expresses it much more strongly.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能想知道一个文本表达情感的程度有多强。例如，*我对那有点恼火*和*那让我非常愤怒*都表达了愤怒，但第二个显然表达得更加强烈。
- en: We will concentrate on algorithms that aim to assign multiple (or no) labels
    to each tweet, with the labels drawn from some collection of candidate emotions,
    ranging from just positive and negative to larger sets drawn from Plutchik’s wheel.
    We will refer to datasets where a single tweet can have zero or more labels as
    **multi-label** datasets. This is to be distinguished from **multi-class** datasets,
    where there are several labels available but exactly one is assigned to each tweet.
    Multi-label datasets are significantly more difficult than simple multi-class
    ones, and the task also gets harder as the set of labels gets larger (it may,
    for instance, be difficult to distinguish between anger and disgust, but they
    are both negative); and it also gets harder if we don’t have a preconception about
    how many emotions are expressed. Since most of the learning algorithms depend
    on comparing some score obtained from the text with a threshold, we can usually
    use this score to assess how strongly a text expresses an emotion rather than
    just whether it expresses it or not. We will mainly concentrate on deciding whether
    a text expresses an emotion rather than how strongly.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于旨在为每条推文分配多个（或无）标签的算法，这些标签来自一些候选情感集合，从仅正面和负面到来自Plutchik轮的更大集合。我们将称之为**多标签**数据集。这需要与**多类**数据集区分开来，在多类数据集中有多个标签可用，但每条推文只分配一个确切的情感。多标签数据集比简单的多类数据集要复杂得多，随着标签集合的增大（例如，区分愤怒和厌恶可能很困难，但它们都是负面的），任务也变得更加困难；如果我们没有关于表达多少情感的前见之明，这也会变得更加困难。由于大多数学习算法都依赖于将文本获得的某些分数与阈值进行比较，因此我们通常可以使用这个分数来评估文本表达情感的程度，而不仅仅是它是否表达情感。我们将主要关注决定文本是否表达情感，而不是它表达情感的强度。
- en: 'We will use a selection of the datasets listed in [*Chapter 2*](B18714_02.xhtml#_idTextAnchor061),
    *Building and Using a Dataset*, to train and test the various models that we will
    develop:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[*第2章*](B18714_02.xhtml#_idTextAnchor061)中列出的数据集的一部分，即*构建和使用数据集*，来训练和测试我们将开发的各个模型：
- en: The **Workshop on Computational Approaches to Subjectivity, Sentiment & Social
    Media Analysis** (**WASSA**) dataset, which contains 3.9K English tweets, each
    labeled with one of anger, fear, joy, and sadness.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算方法在主观性、情感与社会媒体分析**（**WASSA**）数据集，其中包含3.9K条英文推文，每条推文都被标记为愤怒、恐惧、喜悦或悲伤之一。'
- en: The Semeval 2018 Task E_c dataset, which contains a moderate number of tweets
    in English, Arabic, and Spanish, where a fairly high percentage of tweets contain
    emojis, with each tweet labeled with 0 or more emotions from a standard set of
    11 emotions. This dataset contains 7.7K English tweets, 2.9K Arabic tweets, and
    4.2K Spanish tweets. We will refer to this as the SEM-11 set.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Semeval 2018任务E_c数据集，其中包含一定数量的英文、阿拉伯语和西班牙语推文，其中相当高比例的推文包含表情符号，每条推文都被标记为来自11种标准情感集合中的0个或多个情感。此数据集包含7.7K条英文推文、2.9K条阿拉伯语推文和4.2K条西班牙语推文。我们将称之为SEM-11集合。
- en: The Semeval 2016 Task El-reg and El-oc dataset, where the El-reg dataset has
    tweets labeled with a score from 0 to 1 for each of a set of four emotions, and
    the El-oc dataset has tweets ranked in terms of which emotion they express. The
    combinations of these datasets, which we will refer to as the SEM4 set, contain
    7.6K English tweets, 2.8K Arabic, and 2.6K Spanish.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Semeval 2016任务El-reg和El-oc数据集，其中El-reg数据集的推文被标记为四个情感集合中每个情感的0到1分的评分，而El-oc数据集的推文则按所表达的情感进行排名。这些数据集的组合，我们将称之为SEM4集合，包含7.6K条英文推文、2.8K条阿拉伯语和2.6K条西班牙语。
- en: The CARER dataset is large (slightly over 400K tweets) and has labels for six
    emotions (anger, fear, joy, love, sadness, and surprise). Each tweet is assigned
    exactly one emotion.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CARER数据集很大（略超过400K条推文），并为六种情感（愤怒、恐惧、喜悦、爱情、悲伤和惊讶）提供标签。每条推文被分配一个确切的情感。
- en: The IMDb dataset contains 50K negative and positive film reviews and provides
    an interesting test of the robustness of the various algorithms since it is split
    into just two categories (positive and negative), which makes the task of learning
    to classify documents easier. The reviews contain anything from 100 to 1,000 words,
    which is much longer than a tweet and poses a different set of problems.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IMDb数据集包含5K条正面和负面电影评论，并提供了对各种算法鲁棒性的有趣测试，因为它仅分为两个类别（正面和负面），这使得学习对文档进行分类的任务变得更容易。评论包含从100到1,000个单词，这比推文长得多，并提出了不同的一系列问题。
- en: A collection of Kuwaiti tweets, annotated either by assigning a label if all
    three annotators unanimously assigned that label (KWT.U) or if at least two of
    them did (KWT.M). This set is particularly interesting because, in a large number
    of cases, the annotators agreed that a tweet expressed no emotions and in some,
    it expressed several, which poses a substantial challenge for classifiers that
    assign a single label to every observation.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组科威特推文，这些推文要么由所有三位注释者一致分配标签（KWT.U），要么至少有两位注释者分配标签（KWT.M）进行标注。这个集合特别有趣，因为在大量情况下，注释者一致认为推文没有表达任何情绪，而在某些情况下，推文表达了多种情绪，这对将每个观察结果分配单个标签的分类器构成了重大挑战。
- en: 'These datasets provide enough variation to help us verify that a given approach
    to the task of finding emotions is robust across different conditions:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集提供了足够的多样性，有助于我们验证针对寻找情绪的任务的给定方法在不同条件下是否稳健：
- en: The WASSA, SEM4, and SEM11 datasets contain emojis, which makes the task of
    emotion mining slightly easier because the main (sole?) point of using emojis
    is to express emotions, though they are sometimes used in slightly surprising
    ways.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WASSA、SEM4和SEM11数据集包含表情符号，这使得情感挖掘的任务稍微容易一些，因为使用表情符号的主要（唯一？）目的是表达情绪，尽管它们有时以略微令人惊讶的方式使用。
- en: The SEM4 and SEM11 datasets are multilingual, with data supplied in English,
    Arabic, and Spanish. This helps when trying out approaches that are intended to
    be language-independent since the methodology for collecting the three languages
    is the same.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SEM4和SEM11数据集是多语言的，提供的数据包括英语、阿拉伯语和西班牙语。这有助于尝试那些旨在语言无关的方法，因为收集三种语言的方法是相同的。
- en: The SEM11 set contains tweets with differing numbers of emotions, including
    none, which can make the task of assigning emotions considerably harder.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SEM11集合包含具有不同数量情绪的推文，包括没有情绪的推文，这可能会使分配情绪的任务变得更加困难。
- en: 'The CARER dataset is very large, though it does not contain any emojis or hashtags:
    this makes it possible to carry out investigations into how performance varies
    with the size of the training data.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CARER数据集非常大，尽管它不包含任何表情符号或标签：这使得我们可以研究性能如何随着训练数据大小的变化而变化。
- en: The IMDb set has just two labels, but very long texts.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IMDb集合只有两个标签，但文本非常长。
- en: The KWT sets have tweets with zero, one, or more emotions, but this time, a
    very large proportion have zero.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KWT集合包含具有零个、一个或多个情绪的推文，但这一次，很大比例的推文没有情绪。
- en: 'Given that these datasets are supplied in different formats, we need, as usual,
    a common format for representing them. We will use two basic classes:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些数据集以不同的格式提供，我们需要，像往常一样，一个共同的格式来表示它们。我们将使用两个基本类：
- en: 'A tweet is an object with a sequence of tokens, a term frequency table, and
    possibly a Gold Standard set of labels, along with several bookkeeping properties:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推文是一个具有标记序列、词频表以及可能的黄金标准标签集的对象，以及一些记账属性：
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'A dataset is a set containing a set of tweets, a list of emotion names, the
    Gold Standard labels for the tweets, and, again, some bookkeeping properties.
    The most useful of these is an index that assigns each word in the dataset a unique
    index. We will frequently use this in later sections, so it is worth looking at
    how we do it here. The basic idea is that we read the dataset word by word. If
    the word we have just read is already in the index, there is nothing to be done.
    If it is not, then we assign it the current length of the index: this ensures
    that every word gets assigned a unique identifier; once we have added the current
    word, the length of the index will be incremented by one, so the next new word
    will get a new index:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集是一个包含一组推文、情绪名称列表、推文的黄金标准标签以及一些记账属性集合。其中最有用的是为数据集中的每个单词分配一个唯一索引的索引。我们将在后面的章节中经常使用它，所以值得看看我们在这里是如何做的。基本思想是我们逐个读取数据集中的单词。如果我们刚刚读取的单词已经在索引中，那么就没有什么要做的。如果它不在，那么我们将其分配给当前索引的长度：这确保了每个单词都被分配了一个唯一的标识符；一旦我们添加了当前单词，索引的长度将增加一个，因此下一个新单词将获得一个新的索引：
- en: '[PRE14]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will generate an index, as shown here, where each word has a unique identifier:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个索引，如下所示，其中每个单词都有一个唯一的标识符：
- en: '[PRE21]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'class DATASET:    def __init__(self, emotions, tweets, idf, ARGS, N=sys.maxsize):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'class DATASET:    def __init__(self, emotions, tweets, idf, ARGS, N=sys.maxsize):'
- en: self.emotions = sorted(emotions)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: self.emotions = sorted(emotions)
- en: self.tweets = tweets
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: self.tweets = tweets
- en: self.GS = [tweet.GS for tweet in self.tweets][:N]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: self.GS = [tweet.GS for tweet in self.tweets][:N]
- en: self.idf = idf
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: self.idf = idf
- en: self.words = [w[0] for w in reversed(sortTable(idf))]
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: self.words = [w[0] for w in reversed(sortTable(idf))]
- en: self.makeIndex()
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: self.makeIndex()
- en: self.ARGS = ARGS
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ID     Tweet                    anger  disgust  fear21441  Worry is a down payment    0      1        0
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '1535   it makes you #happy.       0      0        0'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '{"text":"i feel awful about it","label":0}{"text":"i really do feel proud of
    myself","label":1}'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: def convert(self):    # extract the labels from dataset_infos.json
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: with open(os.path.join(self.DOWNLOAD,
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '"dataset_infos.json")) as jsfile:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: infos = json.load(jsfile)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: self.labels = infos["default"]["features"]\
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '["label"]["names"]'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '# read the data line by line from data.jsonl'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: with open(os.path.join(self.PATH, "data.jsonl"))\
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'as input:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: d = [json.loads(line) for line in input]
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '# initialise the output with a header line'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: csv = "ID\ttext\t%s\n"%("\t".join(self.labels))
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '# Go through the data writing each line as the ID,'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '# the text itself and an appropriate set of 0s and 1s'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'for i, x in enumerate(d):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: cols = ["1" if x['label'] == i else "0"\
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: for i in range(len(self.labels))]
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: csv += "%s\t%s\t%s\n"%(i, x['text'],"\t".join(cols))
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '# Save this as wholething.csv inside CARER/EN'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'with open(os.path.join(self.PATH, "wholething.csv"), "w") as out:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: out.write(csv)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: def makeDATASET(src, N=sys.maxsize, args=None):    dataset = [line.strip() for
    line in open(src)][:N]
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: emotions = None
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: tweets = []
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'for tweet in dataset:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'if emotions is None:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: emotions = tweet.split()[2:]
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: tweets.append(makeTweet(tweet, args=args))
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: pruned = prune(tweets)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: random.seed(0); random.shuffle(pruned)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: df = counter()
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: index = {}
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'for i, tweet in enumerate(tweets):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'for w in tweet.tokens:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: df.add(w)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: remove singletons from the idf count
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: idf = {}
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'for w in list(df.keys()):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: idf[w] = 1.0/float(df[w]+1)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: return DATASET(emotions, tweets, df, idf, args=args)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: def makeTweet(tweet, args):    tweet = tweet.strip().split("\t")
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: scores = numpy.array([int(score) for score in tweet[2:]])
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: tweet, text tweet[0], tweet[1]
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'if args["language"] == "AR":'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: tokens = a2bw.convert(text, a2bw.a2bwtable).split()
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'if args["stemmer"] == "standard":'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: tokens = stemmer.stemAll(tokens, stemmer.TWEETGROUPS)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'elif args["language"] == "ES":'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'elif args["language"] == "EN":'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: tf = counter()
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'for w in tokens:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: tf.add(word)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: return TWEET(id=tweet,tf=tf,scores=scores,tokens=tokens,args=args)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'class BASECLASSIFIER():    def applyToTweets(self, dataset):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: return [self.applyToTweet(tweet) for tweet in dataset.tweets]
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'EMOLEX="CORPORA/NRC-Emotion-Lexicon/Arabic-NRC-EMOLEX.txt"def readNRC(ifile=EMOLEX,
    targets=None, ARGS=False):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: lines = list(open(ifile))
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '# emotions is the list of emotions in the EMOLEX file'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '# targets is the list of emotions in the dataset that'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '# the classifieris going to be applied to.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: emotions = lines[0].strip().split("\t")[1:-1]
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: emotionIndex = [True if e in targets else False for e in emotions]
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: targetIndex = [True if e in emotions else False for e in targets]
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: lex = {}
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '# add entries line by line'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'for line in lines[1:]:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: line = line.split("\t")
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '# if we''re doing it for English'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'if ARGS.Language == "EN":'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: form = line[0]
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'if ARGS.Stemmer.startswith("justRoot"):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: form = justroot(form)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'elif ARGS.Stemmer.startswith("morphyroot"):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 'elif ARGS.Stemmer.startswith("morphyroot"):'
- en: form = morphyroot(form)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: form = morphyroot(form)
- en: '...'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: 'else:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: 'raise Exception("Unknown language: %s"%(ARGS.Language))'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 'raise Exception("未知语言: %s"%(ARGS.Language))'
- en: '# The line we just read is a string, so the values'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '# 我们刚刚读取的行是一个字符串，所以值'
- en: '# for the emotions are "0" and "1". We want them as'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '# 对于情绪是"0"和"1"。我们希望它们作为'
- en: '# ints, and we also only want the ones that appear'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '# 整数，我们只想要那些出现的'
- en: '# in emotionIndex, i.e. ones that are present in'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '# 在emotionIndex中，即存在于'
- en: '# the lexicon and in the target dataset'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '# 在词典和目标数据集中'
- en: lex[form] \
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: lex[form] \
- en: = [int(x) for (x, y) in zip(line[1:-1], emotionIndex) if y]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: = [int(x) for (x, y) in zip(line[1:-1], emotionIndex) if y]
- en: return lex, emotionIndex, targetIndex
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: return lex, emotionIndex, targetIndex
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: elif ARGS.Language == "AR":            form = line[-1].strip()
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: elif ARGS.Language == "AR":            form = line[-1].strip()
- en: form = a2bw.convert(form, a2bw.a2bwtable)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: form = a2bw.convert(form, a2bw.a2bwtable)
- en: 'if ARGS.Stemmer == "SEM":'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 'if ARGS.Stemmer == "SEM":'
- en: form = stemArabic(form)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: form = stemArabic(form)
- en: 'elif ARGS.Language == "ES":'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 'elif ARGS.Language == "ES":'
- en: form = line[-1].strip()
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: form = line[-1].strip()
- en: 'if ARGS.Stemmer.startswith("stemSpanish"):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 'if ARGS.Stemmer.startswith("stemSpanish"):'
- en: form = stemSpanish(form)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: form = stemSpanish(form)
- en: '[PRE30]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'def calculateScores(self):        for word, cols in self.dataset.index.items():'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 'def calculateScores(self):        for word, cols in self.dataset.index.items():'
- en: '# set up a list of zeros to correspond to the'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '# 设置一个零列表，以对应于'
- en: '# emotions in the dataset'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '# 数据集中的情绪'
- en: self.scoredict[word] = [0]*len(self.emotions)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: self.scoredict[word] = [0]*len(self.emotions)
- en: '# count the non-zero emotions for this word'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '# 计算该单词的非零情绪数量'
- en: s = sum(len(col) for col in cols.values())
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: s = sum(len(col) for col in cols.values())
- en: 'if s > 0:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 'if s > 0:'
- en: 'for col in cols:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 'for col in cols:'
- en: '# use s to rebalance the scores for'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '# 使用s重新平衡分数'
- en: '# emotions for this word so they add up'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '# 该单词的情绪，以便它们相加'
- en: '# to 1'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '# 转换为1'
- en: self.scoredict[word][self.colindex[col]]
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: self.scoredict[word][self.colindex[col]]
- en: = len(cols[col])/s
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: = len(cols[col])/s
- en: '[PRE31]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: def applyToTweet(self, tweet):        scores = [0]*len(self.emotions)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: def applyToTweet(self, tweet):        scores = [0]*len(self.emotions)
- en: 'for token in tweet.tokens:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 'for token in tweet.tokens:'
- en: 'if token and token in self.scoredict:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 'if token and token in self.scoredict:'
- en: for i, x in enumerate(
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: for i, x in enumerate(
- en: 'self.scoredict[token]):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 'self.scoredict[token]):'
- en: scores[i] += x
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: scores[i] += x
- en: m = max(scores)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: m = max(scores)
- en: return [1 if x >= m*self.threshold else 0 for x in scores]
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: return [1 if x >= m*self.threshold else 0 for x in scores]
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'def bestThreshold(self, test=None, show=False):        if test is None:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 'def bestThreshold(self, test=None, show=False):        if test is None:'
- en: test = self.test.tweets
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: test = self.test.tweets
- en: '# Apply this classifier to the tweets we are'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '# 将此分类器应用于我们感兴趣的推文'
- en: '# interested in: setting probs=True forces it to'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '# interested in: setting probs=True forces it to'
- en: '# return the values actually calculated by the'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '# 返回实际计算出的值'
- en: '# classifier rather than the 0/1 version obtained'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '# 使用分类器而不是0/1版本获得的'
- en: '# by using the threshold'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '# 通过使用阈值'
- en: train = self.train.tweets[:len(test)]
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: train = self.train.tweets[:len(test)]
- en: l = self.applyToTweets(train, threshold=0,
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: l = self.applyToTweets(train, threshold=0,
- en: probs=True)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: probs=True)
- en: '# The optimal threshold must lie somewhere between'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '# 最佳阈值必须位于'
- en: '# the smallest and largest scores for any tweet'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '# 任何推文的最高和最低分数'
- en: start = threshold = min(min(tweet.predicted) for tweet in train)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: start = threshold = min(min(tweet.predicted) for tweet in train)
- en: end = max(max(tweet.predicted) for tweet in train)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: end = max(max(tweet.predicted) for tweet in train)
- en: best = []
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: best = []
- en: '# Go from start to end in small steps using'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '# 使用小步长从开始到结束'
- en: '# increasing values for threshold'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '# 增加阈值的值'
- en: 'while threshold <= end:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 'while threshold <= end:'
- en: l = self.applyToTweets(train,
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: l = self.applyToTweets(train,
- en: threshold=threshold)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: threshold=threshold)
- en: '# getmetrics returns macro F1, true positives,'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取metrics返回宏F1，真阳性，'
- en: '# false positives, false negatives'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '# 假阳性，假阴性'
- en: (macroF, tp, fp, fn)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: (macroF, tp, fp, fn)
- en: = metrics.getmetrics([tweet.GS for tweet in test], l)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: = metrics.getmetrics([tweet.GS for tweet in test], l)
- en: '# Jaccard'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '# Jaccard'
- en: j = tp/(tp+fp+fn)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: j = tp/(tp+fp+fn)
- en: best = max(best, [j, threshold])
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: best = max(best, [j, threshold])
- en: threshold += (end-start)/20
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: threshold += (end-start)/20
- en: return round(best[1], 5)
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: return round(best[1], 5)
- en: '[PRE33]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'def calculateScores(self):        for word, cols in self.dataset.index.items():'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 'def calculateScores(self):        for word, cols in self.dataset.index.items():'
- en: best = False
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: best = False
- en: bestscore = -1
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: bestscore = -1
- en: self.scoredict[word] = [0]*len(self.emotions)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: self.scoredict[word] = [0]*len(self.emotions)
- en: 'for col in cols:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 'for col in cols:'
- en: self.scoredict[word][self.colindex[col]]
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: self.scoredict[word][self.colindex[col]]
- en: = len(cols[col])
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: = len(cols[col])
- en: s = sum(self.scoredict[word])
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: s = sum(self.scoredict[word])
- en: 'for i, x in enumerate(self.scoredict[word]):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i, x in enumerate(self.scoredict[word]):'
- en: 'if s > 0:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 'if s > 0:'
- en: x = x/s-1/len(self.emotions))
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: x = x/s-1/len(self.emotions))
- en: self.scoredict[word][i] = max(0, x)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: self.scoredict[word][i] = max(0, x)
- en: '[PRE34]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '{..., ''days'': 6, ''sober'': 7, ''do'': 8, "n''t": 9, ''wanna'': 10,  …}'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '{..., ''days'': 6, ''sober'': 7, ''do'': 8, "n''t": 9, ''wanna'': 10,  …}'
- en: '[PRE35]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: def sentence2vector(sentence, index):    vector = numpy.zeros(len(index))
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: def sentence2vector(sentence, index):    vector = numpy.zeros(len(index))
- en: 'for word in sentence:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 'for word in sentence:'
- en: vector[index[word]] += 1
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: vector[index[word]] += 1
- en: return vector
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: return vector
- en: '[PRE36]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '>>> list(sentence2vector("I do n''t want to be sober".split(), index))[0.,
    0., 1., 0., 0., 0., 0., 1., 1., 1., ...]'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> list(sentence2vector("I do n''t want to be sober".split(), index))[0.,
    0., 1., 0., 0., 0., 0., 1., 1., 1., ...]'
- en: '[PRE37]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '>>> # v is the vector we just made; convert it to a sparse matrix>>> s = sparse.csr_matrix(v)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> # v 是我们刚刚创建的向量；将其转换为稀疏矩阵>>> s = sparse.csr_matrix(v)'
- en: '>>> # it contains seven 1s'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> # 它包含七个 1'
- en: '>>> list(s.data)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> list(s.data)'
- en: '[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]'
- en: '>>> # These are at positions 2, 7, 8, ...'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> # 这些位置在 2, 7, 8, ...'
- en: '>>> list(s.indices)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> list(s.indices)'
- en: '[2, 7, 8, 9, 119, 227, 321]'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[2, 7, 8, 9, 119, 227, 321]'
- en: '[PRE38]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'S0: John ate the pasta[63, 2306, 3304, 7616]'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 'S0: 约翰吃了意大利面[63, 2306, 3304, 7616]'
- en: 'S1: John ate some pasta'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 'S1: 约翰吃了一些意大利面'
- en: '[229, 2306, 3304, 7616]'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[229, 2306, 3304, 7616]'
- en: 'S2: John ate the potatoes'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 'S2: 约翰吃了土豆'
- en: '[63, 3304, 4162, 7616]'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[229, 2306, 3304, 7616]'
- en: 'S3: Mary drank some beer'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 'S3: 玛丽喝了一些啤酒'
- en: '[229, 5040, 5176, 10372]'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[229, 5040, 5176, 10372]'
- en: '[PRE39]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: S0      S1      S2      S3S0    1.00    0.75    0.75    0.25
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: S0      S1      S2      S3S0    1.00    0.75    0.75    0.25
- en: S1    0.75    1.00    0.50    0.00
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: S1    0.75    1.00    0.50    0.00
- en: S2    0.75    0.50    1.00    0.25
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: S2    0.75    0.50    1.00    0.25
- en: S3    0.25    0.00    0.25    1.00
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: S3    0.25    0.00    0.25    1.00
- en: '[PRE40]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: def getDF(documents, uselog=numpy.log):    # adding something to df either sets
    or increments a counter
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: def getDF(documents, uselog=numpy.log):    # 在 df 中添加一些内容，要么设置计数器，要么增加计数
- en: df = counter()
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: df = counter()
- en: 'for document in enumerate(documents):'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 'for document in enumerate(documents):'
- en: '# for each unique word in the document increment df'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '# 对于文档中的每个唯一单词，增加 df'
- en: 'for w in set(document.split()):'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 'for w in set(document.split()):'
- en: df.add(w)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: df.add(w)
- en: idf = {}
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: idf = {}
- en: 'for w in df:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 'for w in df:'
- en: idf[w] = 1.0/float(uselog(df[w])+1)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: idf[w] = 1.0/float(uselog(df[w])+1)
- en: return df, idf
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: return df, idf
- en: '[PRE41]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: DF    IDFa      1521  0.001
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: DF    IDFa      1521  0.001
- en: the    1842  0.001
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: the    1842  0.001
- en: cat    5     0.167
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: cat    5     0.167
- en: loves  11    0.083
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: loves  11    0.083
- en: man    85    0.012
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: man    85    0.012
- en: '[PRE42]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: def sentence2vector(sentence, index, idf={}):    vector = numpy.zeros(len(index))
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: def sentence2vector(sentence, index, idf={}):    vector = numpy.zeros(len(index))
- en: 'for word in sentence:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 'for word in sentence:'
- en: inc = idf[word] if word in idf else 1
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: inc = idf[word] if word in idf else 1
- en: vector[index[word]] += inc
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: vector[index[word]] += inc
- en: return vector
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: return vector
- en: '[PRE43]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '>>> list(S1.data)[0.008, 0.3333333333333333, 0.1, 0.5]'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> list(S1.data)[0.008, 0.3333333333333333, 0.1, 0.5]'
- en: '>>> list(S1.indices)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> list(S1.indices)'
- en: '[229, 2306, 3304, 7616]'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[229, 2306, 3304, 7616]'
- en: '[PRE44]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'king  {''king'': 144, ''new'': 88, ''queen'': 84, ''royal'': 69, ''made'':
    68,...}queen  {''mother'': 123, ''speech'': 86, ''king'': 84, ''royal'': 62, ...}'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 'king  {''king'': 144, ''new'': 88, ''queen'': 84, ''royal'': 69, ''made'':
    68,...}queen  {''mother'': 123, ''speech'': 86, ''king'': 84, ''royal'': 62, ...}'
- en: '[PRE45]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'class TF-IDFMODE():    def __init__(self, uselog=log, corpus=corpora.BNC, N=10000):'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 'class TF-IDFMODE():    def __init__(self, uselog=log, corpus=corpora.BNC, N=10000):'
- en: self.pairs0 = getPairs(corpus)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: self.pairs0 = getPairs(corpus)
- en: self.df = sortTable(getDF(self.pairs0))[:N]
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: self.df = sortTable(getDF(self.pairs0))[:N]
- en: self.df = {x[0]:x[1] for x in self.df}
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: self.df = {x[0]:x[1] for x in self.df}
- en: self.pairs1 = {}
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: self.pairs1 = {}
- en: 'for word in self.pairs0:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 'for word in self.pairs0:'
- en: 'if word in self.df:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 'if word in self.df:'
- en: self.pairs1[word] = {}
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: self.pairs1[word] = {}
- en: 'for other in self.pairs0[word]:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 'for other in self.pairs0[word]:'
- en: 'if other in self.df:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 'if other in self.df:'
- en: self.pairs1[word][other]\
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: self.pairs1[word][other]\
- en: = self.pairs0[word][other]
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: = self.pairs0[word][other]
- en: self.pairs2 = applyIDF(self.pairs1, df=self.df, uselog=uselog)
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: self.pairs2 = applyIDF(self.pairs1, df=self.df, uselog=uselog)
- en: self.dimensions, self.invdimensions, self.matrices\
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: self.dimensions, self.invdimensions, self.matrices\
- en: = pairs2matrix(self.pairs2)
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: = pairs2matrix(self.pairs2)
- en: self.similarities = cosine_similarity(
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: self.similarities = cosine_similarity(
- en: self.matrices)
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: self.matrices)
- en: '[PRE46]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: cat:  mouse:0.03, litter:0.02, ginger:0.02, stray:0.02, pet:0.02dog:  stray:0.05,
    bark:0.03, pet:0.03, shepherd:0.03, vet:0.02
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: cat:  mouse:0.03, litter:0.02, ginger:0.02, stray:0.02, pet:0.02dog:  stray:0.05,
    bark:0.03, pet:0.03, shepherd:0.03, vet:0.02
- en: eat:  sandwiches:0.03, foods:0.03, bite:0.03, meat:0.02, cake:0.02
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: eat:  sandwiches:0.03, foods:0.03, bite:0.03, meat:0.02, cake:0.02
- en: 'drink: sipped:0.08, alcoholic:0.03, pints:0.03, merry:0.02, relaxing:0.02'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 'drink: sipped:0.08, alcoholic:0.03, pints:0.03, merry:0.02, relaxing:0.02'
- en: '[PRE47]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: def nearest(self, word, N=6):        similarwords = self.similarities[self.dimensions[word]]
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: def nearest(self, word, N=6):        similarwords = self.similarities[self.dimensions[word]]
- en: matches = list(reversed(sorted([x, i]\
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: matches = list(reversed(sorted([x, i]\
- en: for i, x in enumerate(similarwords)))[1:N]
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: for i, x in enumerate(similarwords)))[1:N]
- en: return [(self.invdimensions[i], s) for [s, i] in matches]
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: return [(self.invdimensions[i], s) for [s, i] in matches]
- en: '[PRE48]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Best matches for cat:dog:0.39,cats:0.25,keyboard:0.23,bin:0.23,embryo:0.22
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳匹配结果为 cat:dog:0.39,cats:0.25,keyboard:0.23,bin:0.23,embryo:0.22
- en: 'Best matches for dog:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳匹配结果为 dog:'
- en: dogs:0.42,cat:0.39,cats:0.35,hairs:0.26,bullet:0.24
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: dogs:0.42,cat:0.39,cats:0.35,hairs:0.26,bullet:0.24
- en: 'Best matches for eat:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳匹配结果为 eat:'
- en: ate:0.35,eaten:0.35,cakes:0.28,eating:0.28,buffet:0.27
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'Best matches for drink:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: brandy:0.41,beer:0.41,coffee:0.38,lager:0.38,drinks:0.36
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 61.0    26.0    54.0    90.0    9.0    19.034.0    53.0    73.0    21.0    17.0    67.0
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 59.0    75.0    33.0    96.0    59.0    24.0
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: 72.0    90.0    79.0    88.0    48.0    45.0
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 77.0    24.0    88.0    65.0    33.0    94.0
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 44.0    0.00    55.0    61.0    71.0    92.0
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: -0.4    0.1    0.3    0.3    0.3   -0.8-0.4   -0.5   -0.5    0.6    0.1    0.2
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: -0.3    0.3    0.5    0.2    0.4    0.6
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: -0.5    0.7   -0.5   -0.2   -0.1    0.0
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: -0.4   -0.5    0.1   -0.7    0.2    0.0
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: -0.4   -0.1    0.3    0.1   -0.8    0.0
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 356.95    103.09    90.57    61.44    53.85    14.53
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: -0.4   -0.4   -0.3   -0.4   -0.3   -0.3   -0.3   -0.4
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: -0.4   -0.1    0.7   -0.4   -0.0    0.2   -0.2    0.4
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 0.2   -0.5    0.0   -0.1    0.0   -0.5    0.4    0.5
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 0.1   -0.2   -0.0    0.3   -0.8    0.3   -0.1    0.3
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 0.7   -0.4    0.1   -0.3    0.2    0.2   -0.5   -0.1
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: -0.3   -0.5   -0.4    0.1    0.4    0.6    0.2    0.1
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 76.8    42.8    51.1    46.5    35.2    45.4    40.1    78.972.8    76.4     2.0    78.6    10.9    65.3    16.4    19.8
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 59.7    13.3    52.3    22.7    27.5    25.6    36.2    79.2
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 26.2    98.3    93.2    36.9    60.7    84.6    19.9    69.9
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 92.2    74.3    14.2    57.9    85.8    22.6    52.9    35.9
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 44.1    64.1    29.1    69.0    31.9    17.9    76.0    78.0
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Best matches for cat:cats:0.66,hairs:0.62,dog:0.61,dogs:0.60,hair:0.54
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'Best matches for dog:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: dogs:0.72,cats:0.68,cat:0.61,pet:0.54,bull:0.46
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'Best matches for eat:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: meat:0.77,sweets:0.75,ate:0.75,chicken:0.73,delicious:0.72
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'Best matches for drink:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: pint:0.84,sherry:0.83,brandy:0.83,beer:0.81,drank:0.79
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: def chooseother(self, token):        # If the classifier has a model, use that
    to find
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '# the 5 most similar words to the target and go'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '# through these looking for one that is in the'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '# sentiment lexicon'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'if self.model:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'try:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'for other in self.model.nearest(token, topn=5):'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: other = other[0]
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'if other in self.scoredict:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: return other
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'except:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: pass
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: return False
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: 'def applyToTweet(self, tweet):'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: scores = [0]*len(self.emotions)
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'for token in tweet.tokens:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 'if not token in self.scoredict:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: token = self.chooseother(token)
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'if token in self.scoredict:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'for i, x in enumerate(self.scoredict[token]):'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: scores[i] += x
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: m = max(scores)
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: return [1 if x >= m*self.threshold else 0 for x in scores]
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '…cat chosen for kitten: anger:0.05, fear:0.10, joy:0.00, sadness:0.00'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'fall chosen for plummet: anger:0.00, fear:0.04, joy:0.04, sadness:0.02'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'restrain chosen for evict: anger:0.72, fear:0.00, joy:0.00, sadness:0.00'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'arrogance chosen for cynicism: anger:0.72, fear:0.00, joy:0.00, sadness:0.00'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'overweight chosen for obese: anger:0.00, fear:0.72, joy:0.00, sadness:0.00,
    neutral:0.00'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'greedy chosen for downtrodden: anger:0.72, fear:0.00, joy:0.00, sadness:0.00'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'sacred chosen for ancient: anger:0.00, fear:0.72, joy:0.00, sadness:0.00'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: love ['hate', 'kindness', 'joy', 'passion', 'dread']hate ['adore', 'loathe',
    'despise', 'hated', 'dislike']
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: adore ['loathe', 'hate', 'despise', 'detest', 'daresay']
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 崇拜 ['憎恨', '仇恨', '轻视', '厌恶', '敢说']
- en: happy ['pleased', 'nice', 'glad', 'lucky', 'unhappy']
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 快乐 ['高兴', '和善', '愉快', '幸运', '不快乐']
- en: sad ['funny', 'pathetic', 'miserable', 'depressing', 'strange']
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 悲伤 ['滑稽', '悲惨', '痛苦', '压抑', '奇怪']
- en: furious ['stunned', 'angry', 'annoyed', 'shocked', 'horrified']
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 愤怒 ['震惊', '生气', '烦恼', '震惊', '恐惧']
- en: happiness ['sorrow', 'joy', 'fulfilment', 'enjoyment', 'dignity']
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 幸福 ['悲伤', '快乐', '满足', '享受', '尊严']
- en: '[PRE56]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
