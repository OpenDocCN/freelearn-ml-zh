<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer076">
			<h1 id="_idParaDest-91"><em class="italic"><a id="_idTextAnchor090"/>Chapter 5</em>: Ingesting and Streaming Data from the Edge </h1>
			<p><strong class="bold">Edge computing</strong> can reduce the amount of data transferred to the cloud (or on-premises datacenter), thus saving on network bandwidth costs. Often, high-performance edge applications require local compute, storage, network, data analytics, and machine learning capabilities to process high-fidelity data in low latencies. AWS extends infrastructure to the edge, beyond <strong class="bold">Regions</strong> and <strong class="bold">Availability Zones</strong>, as close to the endpoint as required by your workload. As you will have learned in previous chapters, <strong class="bold">AWS IoT Greengrass</strong> allows you to run sophisticated edge applications on devices and gateways. </p>
			<p>In this chapter, you will learn about the different data design and transformation strategies applicable for edge workloads. We will explain how you can ingest data from different sensors through different workflows based on <strong class="bold">data velocity</strong> (such as hot, warm, and cold), <strong class="bold">data variety</strong> (such as structured and unstructured), and <strong class="bold">data volume</strong> (such as high frequency or low frequency) on the edge. Thereafter, you will learn the approaches of streaming the raw and transformed data from the edge to different cloud services. By the end of this chapter, you should be familiar with data processing using AWS IoT Greengrass.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Defining data models for IoT workloads</li>
				<li>Designing data patterns for the edge </li>
				<li>Getting to know Stream Manager </li>
				<li>Building your first data orchestration workflow on the edge</li>
				<li>Streaming from the edge to a data lake on the cloud</li>
			</ul>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor091"/>Technical requirements </h1>
			<p>The technical requirements for this chapter are the same as those outlined in <a href="B17595_02_Final_SS_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a><em class="italic">, Foundations of Edge Workloads</em>. See the full requirements in that chapter.</p>
			<p>You will find the GitHub code repository here: <a href="https://github.com/PacktPublishing/Intelligent-Workloads-at-the-Edge/tree/main/chapter5">https://github.com/PacktPublishing/Intelligent-Workloads-at-the-Edge/tree/main/chapter5</a></p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/>Defining data models for IoT workloads </h1>
			<p>According to <a id="_idIndexMarker390"/>the IDC, the sum of the world's data <a id="_idIndexMarker391"/>will grow from 33 <strong class="bold">zettabytes</strong> (<strong class="bold">ZB</strong>) in <a id="_idIndexMarker392"/>2018 to 175 ZB by 2025. Additionally, the IDC estimates that there will be 41.6 billion connected IoT devices or <em class="italic">things</em>, generating 79.4 ZB of data in 2025 (<a href="https://www.datanami.com/2018/11/27/global-datasphere-to-hit-175-zettabytes-by-2025-idc-says/">https://www.datanami.com/2018/11/27/global-datasphere-to-hit-175-zettabytes-by-2025-idc-says/</a>). Additionally, many other sources reiterate that data and information are the <em class="italic">currency</em>, the <em class="italic">lifeblood</em>, and even the <em class="italic">new oil</em> of the information industry.</p>
			<p>Therefore, the data-driven economy is here<a id="_idIndexMarker393"/> to stay and the <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) will act as the enabler to ingest data from a huge number of devices (or endpoints), such as sensors and actuators, and generate aggregated insights for achieving business outcomes. Thus, as an IoT practitioner, you should be comfortable with the basic concepts of <strong class="bold">data modeling</strong> and how that enables <strong class="bold">data management</strong> on the edge. </p>
			<p>All organizations across different verticals such as industrial, commercial, consumer, transportation, energy, healthcare, and others are exploring new use cases to improve their top line or bottom line and innovate on behalf of their customers. IoT devices such as a connected hub in a consumer home, a smart parking meter on a road, or a connected car will coexist with customers and will operate even when there is no connectivity to the internet.</p>
			<p>This is a paradigm shift from the centralized solutions that worked for enterprises in the past. For example, a banking employee might have hosted their workloads in datacenters, but now they can monitor customer activities (such as suspicious actions, footfalls, or availability of cash in an ATM) at their branch locations in near real time to serve customers better. Therefore, a new strategy is required to act on the data generated locally and be able to process and stream data from the edge to the cloud. </p>
			<p>In this chapter, we are going to rethink and re-evaluate the applicability of different big data <a id="_idIndexMarker394"/>architectures in the context of IoT and edge computing. The<a id="_idIndexMarker395"/> three areas we will consider are data management, data architecture patterns, and anti-patterns. </p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor093"/>What is data management?</h2>
			<p>As per<a id="_idIndexMarker396"/> the <strong class="bold">Data Management Body of Knowledge</strong> (<strong class="bold">DMBOK2</strong>) from the <strong class="bold">Data Management Association</strong> (<strong class="bold">DAMA</strong>), data <a id="_idIndexMarker397"/>management is <a id="_idIndexMarker398"/>the development, execution, and supervision of plans, policies, programs, and practices that deliver, control, protect, and enhance the value of data and information assets throughout their life cycles (for more information, please <a id="_idIndexMarker399"/>refer to <em class="italic">DAMA-DMBOK2</em> at <a href="https://technicspub.com/dmbok/">https://technicspub.com/dmbok/</a>). </p>
			<p>DAMA covers the data management framework in great detail, as shown in the following diagram: </p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="Images/B17595_05_001.jpg" alt="Figure 5.1 – The data management life cycle&#13;&#10;" width="1488" height="917"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – The data management life cycle</p>
			<p>Here, we recognized an opportunity to augment the framework from DAMA with concepts that are<a id="_idIndexMarker400"/> relevant to edge computing. Therefore, in this section, we will dive deeper into the principles related to data modeling, data <a id="_idIndexMarker401"/>architecture, and <strong class="bold">Data Integration and Interoperability</strong> (<strong class="bold">DII</strong>), which we think are relevant for edge computing and IoT. </p>
			<p>Let's define data in the context of IoT before we discuss how to model it. IoT data is generated from different sensors, actuators, and gateways. Therefore, they can come in different forms such as the following:</p>
			<ul>
				<li><strong class="bold">Structured data</strong>: This refers to a predictable form of data; examples include device metadata and device relationships.</li>
				<li><strong class="bold">Semi-structured data</strong>: This is a form of data with a certain degree of variance and randomness; examples include sensor and actuator feeds.</li>
				<li><strong class="bold">Unstructured data</strong>: This is a form of data with a higher degree of variance and randomness; examples include raw images, audio, or videos.</li>
			</ul>
			<p>Now, let's discuss how the different forms of data can be governed, organized, and stored using<a id="_idIndexMarker402"/> data modeling techniques. </p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor094"/>What is data modeling? </h2>
			<p><strong class="bold">Data modeling</strong> is a common<a id="_idIndexMarker403"/> practice in software engineering, where data requirements are defined and analyzed to support the business processes of different information systems in the scope of the organization. There are three different types of data models: </p>
			<ul>
				<li>Conceptual data models</li>
				<li>Logical data models </li>
				<li>Physical data models </li>
			</ul>
			<p>In the following diagram, the relationships between different modeling approaches are presented:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="Images/B17595_05_002.jpg" alt="Figure 5.2 – Data modeling approaches&#13;&#10;" width="974" height="756"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Data modeling approaches</p>
			<p class="callout-heading">Fun fact </p>
			<p class="callout">The Enigma machine was used by the German military as the primary mode of communication for all secure wireless communications during World War II. Alan Turing cracked the Enigma code roughly 80 years ago when he figured out the text that's placed at the end of every message. This helped to decipher key secret messages from the German military and helped end the world war. Additionally, this mechanism led to the era of unlocking insights by defining a language to decipher data, which was later formalized as data modeling.</p>
			<p>The most common data modeling technique for a database is an <strong class="bold">Entity-Relationship</strong> (<strong class="bold">ER</strong>) model. An ER model <a id="_idIndexMarker404"/>in software engineering is a common way to represent the relationship between different entities such as people, objects, places, events, and more in a graphical <a id="_idIndexMarker405"/>way in order to organize information better to drive the business processes of an organization. In other terms, an ER model is an abstract data model that defines a data structure for the required information and can be independent of any specific database technology. In this section, we will explain the different data models using the ER model approach. First, let's define, with the help of a use case diagram, the relationship between a customer and their devices associated with a connected HBS hub:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="Images/B17595_05_003.jpg" alt="Figure 5.3 – A use case diagram&#13;&#10;" width="912" height="418"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – A use case diagram</p>
			<p>Now, let's build the ER diagram through a series of conceptual, logical, and physical models:</p>
			<ol>
				<li>The <strong class="bold">conceptual data model</strong> defines the entities and their relationships. This is the first <a id="_idIndexMarker406"/>step of the data modeling exercise and is used by personas such as data architects to gather the initial set of requirements from the business stakeholders. For example, <em class="italic">sensor</em>, <em class="italic">device</em>, and <em class="italic">customer</em> are three entities in a relationship, as shown in the following diagram: <div id="_idContainer057" class="IMG---Figure"><img src="Images/B17595_05_004.jpg" alt="Figure 5.4 – A conceptual data model&#13;&#10;" width="797" height="223"/></div><p class="figure-caption">Figure 5.4 – A conceptual data model</p></li>
				<li>The conceptual <a id="_idIndexMarker407"/>model is then rendered into a <strong class="bold">logical data model</strong>. In this<a id="_idIndexMarker408"/> step of data modeling, the data structure along with additional properties are defined using a conceptual model as the foundation. <p>For example, you can define the properties of the different entities in the relationship such as the sensor type or device identifier (generally, a serial number or a MAC address), as shown in the following list. Additionally, there could be different forms of relationships, such as the following:</p><ul><li>Association is the relationship between devices and sensors.</li><li>Ownership is the relationship between customers and devices. </li></ul><p>The preceding points are illustrated in the following diagram:</p><div id="_idContainer058" class="IMG---Figure"><img src="Images/B17595_05_005.jpg" alt="Figure 5.5 – A logical data model&#13;&#10;" width="747" height="154"/></div><p class="figure-caption">Figure 5.5 – A logical data model</p></li>
				<li>The final step in data <a id="_idIndexMarker409"/>modeling is to build a <strong class="bold">physical data model</strong> from the defined logical model. A physical model is often a technology or product-specific implementation of the data model. For example, you define the data types for the different properties of an entity, such as a number or a string, that will be deployed on a<a id="_idIndexMarker410"/> database solution from a specific vendor:</li>
			</ol>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="Images/B17595_05_006.jpg" alt="Figure 5.6 – A physical data model&#13;&#10;" width="769" height="143"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – A physical data model</p>
			<p>Enterprises have used ER modeling for decades to design and govern complex distributed data solutions. All the preceding steps can be visualized as the following workflow, which is not limited to any specific technology, product, subject area, or operating environment (such as a data center, cloud, or edge):</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="Images/B17595_05_007.jpg" alt="Figure 5.7 – The data modeling flow&#13;&#10;" width="1650" height="435"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – The data modeling flow</p>
			<p>Now that you have<a id="_idIndexMarker411"/> understood the foundations of data modeling, in the next section, let's examine how this can be achieved for IoT workloads. </p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor095"/>How do you design data models for IoT?</h2>
			<p>Now, let's take a <a id="_idIndexMarker412"/>look at some examples of <a id="_idIndexMarker413"/>how to apply the preceding data modeling concepts to the realm of structured, unstructured, and semi-structured data that are common with IoT workloads. Generally, when we refer to data modeling for structured data, a <strong class="bold">relational database</strong> (<strong class="bold">RDBMS</strong>) comes<a id="_idIndexMarker414"/> to mind first. However, for most IoT workloads, structured data generally includes hierarchical relationships between a device and other entities. And that is better illustrated using a graph or an ordered key-value database. Similarly, for semi-structured data, when it comes to IoT workloads, it's mostly illustrated as a key-value, time series, or document store.  </p>
			<p>In this section, we will give you a glimpse of data modeling techniques using NoSQL data solutions to continue building additional functionalities for HBS. Modeling an RDBMS is outside the scope of this book. However, if you are interested in learning about them, there are tons of materials available on the internet that you can refer to. </p>
			<p>NoSQL databases are designed to offer freedom to developers to break away from a longer cycle of database schema designs. However, it's a mistake to assume that NoSQL databases lack any sort of data model. Designing a NoSQL solution is quite different from an RDBMS design. For RDBMS, developers generally create a data model that adheres to normalization guidelines, without focusing on access patterns. The data model can be modified later when new business requirements arise, thus leading to a lengthy release cycle. The collected data is organized in different tables with rows, columns, and referential integrities. In contrast, for a NoSQL solution design, developers cannot begin designing the models until they know the questions that are required to be answered. Understanding the business queries working backward from the use case is quintessential. Therefore, the general rule of thumb to remember during data modeling through a relational or NoSQL database is the following:</p>
			<ul>
				<li>Relational modeling primarily cares about the structure of data. The design principle is <em class="italic">What answers do I get?</em> </li>
				<li>NoSQL modeling primarily cares about application-specific access patterns. The design principle is <em class="italic">What questions do I ask?</em> <p class="callout-heading">Fun fact </p><p class="callout">The <a id="_idIndexMarker415"/>common<a id="_idIndexMarker416"/> translation of the NoSQL acronym is <em class="italic">Not only SQL</em>. This highlights the fact that NoSQL doesn't only support NoSQL, but it can handle relational, semi-structured, and unstructured data. Organizations such as Amazon, Facebook, Twitter, LinkedIn, and Google have designed different NoSQL technologies.</p></li>
			</ul>
			<p>Before I show you some examples of data modeling, let's understand the five fundamental properties of our application's (that is, the HBS hub) access patterns that need to be considered <a id="_idIndexMarker417"/>in order to come up with relevant questions:</p>
			<ul>
				<li><strong class="bold">Data type</strong>: What's the type of data in scope? For example, is the data related to telemetry, command-control, or critical events? Let's quickly refresh the use of each of these data types: <p>a) <strong class="bold">Telemetry</strong>: This is a constant stream of data transmitted by sensors/actuators, such as temperature or humidity readings , which can be aggregated on the edge or published as it is to the cloud for further processing. </p><p>b) <strong class="bold">Command and Control</strong>: These are actionable messages, such as turning on/off the lights, which can occur between two devices or between an end user and the device.</p><p>c) <strong class="bold">Events</strong>: These are data patterns that identify more complex scenarios than regular telemetry data, such as network outages in a home or a fire alarm in a building.</p></li>
				<li><strong class="bold">Data size</strong>: What is the quantity of data in scope? Is it necessary to store and retrieve data locally (on the edge), or does the data require transmission to a different data persistence layer (such as a data lake on the cloud)?</li>
				<li><strong class="bold">Data shape</strong>: What's the form of data being generated from different edge devices such as text, blobs, and images? Note that different data forms such as images and videos<a id="_idIndexMarker418"/> might have different computational needs (think of GPUs).</li>
				<li><strong class="bold">Data velocity</strong>: What's the speed of data to process queries based on the required latencies? Do you have a hot, warm, or cold path of data? </li>
				<li><strong class="bold">Data consistency</strong>: How much of this data needs to have strong versus eventual consistency? </li>
			</ul>
			<p>Answering the<a id="_idIndexMarker419"/> preceding questions will help you to<a id="_idIndexMarker420"/> determine whether the solution should be based on one of the following: </p>
			<ul>
				<li><strong class="bold">BASE methodology</strong>: <strong class="bold">Basically Available</strong>,<strong class="bold"> Soft-state</strong>,<strong class="bold"> Eventual consistency</strong>, which are typical characteristics of NoSQL databases</li>
				<li><strong class="bold">ACID methodology</strong>: <strong class="bold">Atomicity</strong>,<strong class="bold"> Consistency</strong>,<strong class="bold"> Isolation</strong>,<strong class="bold"> </strong>and<strong class="bold"> Durability</strong>, which are typical characteristics of relational databases</li>
			</ul>
			<p>We will discuss these concepts in more detail next.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor096"/>Selecting between ACID or BASE for IoT workloads</h2>
			<p>The following <a id="_idIndexMarker421"/>table lists some of the key <a id="_idIndexMarker422"/>differences between the two methodologies. This should enable you to make an informed decision working backward from your use case: </p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="Images/B17595_05_008.jpg" alt="Figure 5.8 – ACID versus BASE summary&#13;&#10;" width="397" height="326"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – ACID versus BASE summary</p>
			<p class="callout-heading">Fun fact </p>
			<p class="callout">ACID and BASE represent opposing sides of the pH spectrum. Jim Grey conceived the idea in 1970 and subsequently published a paper, called <em class="italic">The Transaction Concept: Virtues and Limitations</em>, in June 1981. </p>
			<p>So far, you have understood the fundamentals of data modeling and design approaches. You must be curious about how to relate those concepts to the connected HBS product, which you have been developing in earlier chapters. Let's explore how the rubber meets the road. </p>
			<p>Do you still <a id="_idIndexMarker423"/>remember the <strong class="bold">first phase</strong> of <a id="_idIndexMarker424"/>data modeling? </p>
			<p>Bingo! Conceptual it is.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor097"/>Conceptual modeling of the connected HBS hub </h2>
			<p>The following <a id="_idIndexMarker425"/>diagram is a hypothetical <a id="_idIndexMarker426"/>conceptual model of the HBS hub:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="Images/B17595_05_009.jpg" alt="Figure 5.9 – A conceptual data model for connected HBS&#13;&#10;" width="1064" height="582"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – A conceptual data model for connected HBS</p>
			<p>In the preceding diagram, you can observe how the different devices such as lights, HVAC, and washing machines are installed in different rooms of a house. Now the conceptual model is in place, let's take a look at the logical view. </p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor098"/>The logical modeling of the connected HBS hub </h2>
			<p>To build the<a id="_idIndexMarker427"/> logical model, we need to ask ourselves <a id="_idIndexMarker428"/>the type of questions an end consumer might ask, such as the following:</p>
			<ul>
				<li>Show the status of a device (such as is the washing complete?).</li>
				<li>Turn a device on or off (such as turn off the lights).</li>
				<li>Show the readings of a device (such as what's the temperature now?).</li>
				<li>Take a new reading (such as how much energy is being consumed by the refrigerator?).</li>
				<li>Show the aggregated connectivity status of a device (or devices) for a period.</li>
			</ul>
			<p>To address these questions, let's determine the access patterns for our end application:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="Images/B17595_05_Table_1.jpg" alt="Figure 5.10 – A logical data model for connected HBS&#13;&#10;" width="1650" height="624"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10 – A logical data model for connected HBS</p>
			<p>Now that we have captured the summary of our data modeling requirements, you can observe that the solution needs to ingest data in both structured and semi-structured formats at high frequency. Additionally, it doesn't require strong consistency. Therefore, it makes<a id="_idIndexMarker429"/> sense to <a id="_idIndexMarker430"/>design the data layer using a NoSQL solution that leverages the BASE methodology. </p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor099"/>The physical modeling of the connected HBS hub</h2>
			<p>As the final <a id="_idIndexMarker431"/>step, we need to define the physical <a id="_idIndexMarker432"/>data model from the gathered requirements. To do that, we will define a primary and a secondary key. You are not required to define all of the attributes if they're not known to you, which is a key advantage of a NoSQL solution. </p>
			<h3>Defining the primary key </h3>
			<p>As the name <a id="_idIndexMarker433"/>suggests, this is one of the required attributes in a table, which is often known as a <strong class="bold">Partition key</strong>. In a <a id="_idIndexMarker434"/>table, no two primary keys should have the same value. There is<a id="_idIndexMarker435"/> also a concept of a <strong class="bold">Composite key</strong>. It's composed of two attributes, a <a id="_idIndexMarker436"/>partition key and a <strong class="bold">Sort key</strong>. </p>
			<p>In our scenario, we will create a <strong class="source-inline">Sensor</strong> table with a composite key (as depicted in the following screenshot). The <a id="_idIndexMarker437"/>primary key is a <strong class="bold">device identifier</strong>, and the sort key is a timestamp that enables us to query data in a time range: </p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="Images/B17595_05_011.jpg" alt="Figure 5.11 – Composite keys in a sensor table&#13;&#10;" width="693" height="255"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.11 – Composite keys in a sensor table</p>
			<h3>Defining the secondary indexes </h3>
			<p>A <strong class="bold">secondary index</strong> allows <a id="_idIndexMarker438"/>us to <a id="_idIndexMarker439"/>query the data in the table using a different key, in addition to queries against the primary or composite keys. This gives your applications more flexibility in querying the data for different use cases. Performing a query using a secondary index is pretty similar to querying from the table directly. </p>
			<p>Therefore, for secondary indexes, as shown in the following chart, we have selected the primary key as a sensor identifier (<strong class="source-inline">sensor_id</strong>) along with timestamp as the sort key:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="Images/B17595_05_012.jpg" alt="Figure 5.12 – Secondary indexes in a sensor table&#13;&#10;" width="1186" height="436"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.12 – Secondary indexes in a sensor table</p>
			<h3>Defining the additional attributes </h3>
			<p>The key <a id="_idIndexMarker440"/>advantage of a NoSQL solution is that there is no enforced schema. Therefore, other attributes can be created on the fly as data comes in. That being said, if some of the attributes are already known to the developer, there is no restriction to include those in the data <a id="_idTextAnchor100"/><a id="_idTextAnchor101"/>model:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="Images/B17595_05_013.jpg" alt="Figure 5.13 – Other attributes in a sensor table&#13;&#10;" width="464" height="424"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.13 – Other attributes in a sensor table</p>
			<p>Now that the data layer exists, let's create the interfaces to access this data. </p>
			<h3>Defining the interfaces </h3>
			<p>Now we will <a id="_idIndexMarker441"/>create two different facets for the sensor table. A <strong class="bold">facet</strong> is a <a id="_idIndexMarker442"/>virtual construct that enables different views of the data stored in a table. The facets can be mapped to a functional construct such <a id="_idIndexMarker443"/>as a method or an API for performing various <strong class="bold">Create, Read, Update, Delete</strong> (<strong class="bold">CRUD</strong>) operations on a table: </p>
			<ul>
				<li><strong class="source-inline">putItems</strong>: This facet allows write operations and requires the composite keys at the minimum in the payload.</li>
				<li><strong class="source-inline">getItems</strong>: This facet allows read operations that can query items with all or selective attributes.</li>
			</ul>
			<p>The following screenshot depicts the <strong class="source-inline">getItems</strong> facet def<a id="_idTextAnchor102"/><a id="_idTextAnchor103"/>inition:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="Images/B17595_05_014.jpg" alt="Figure 5.14 – The getItems facet definition&#13;&#10;" width="589" height="344"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.14 – The getItems facet definition</p>
			<p>So, now you <a id="_idIndexMarker444"/>have created the data model along with its interfaces. This enables you to understand the data characteristics that are required to develop edge applications. </p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor104"/>Designing data patterns on the edge </h1>
			<p>As data flows securely <a id="_idIndexMarker445"/>from different sensors/actuators on the edge to the gateway or cloud over different protocols or channels, it is necessary for it to be safely stored, processed, and cataloged for further consumption. Therefore, any IoT data architecture needs to take into consideration the data models (as explained earlier), data storage, data flow patterns, and anti-patterns, which will be covered<a id="_idIndexMarker446"/> in this section. Let's start with data storage. </p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor105"/>Data storage</h2>
			<p>Big data<a id="_idIndexMarker447"/> solutions <a id="_idIndexMarker448"/>on the cloud are designed to reliably store terabytes, petabytes, or exabytes of data and can scale across multiple geographic locations globally to provide high availability and redundancy for businesses to <a id="_idIndexMarker449"/>meet their <strong class="bold">Recovery Time Objective</strong> (<strong class="bold">RTO</strong>) and <strong class="bold">Recovery Point Objective</strong> (<strong class="bold">RPO</strong>). However, edge solutions, such as our very<a id="_idIndexMarker450"/> own connected HBS hub solution, are resource-constrained in terms of compute, storage, and network. Therefore, we need to design the edge solution to cater to different time-sensitive, low-latency use cases and hand off the heavy lifting <a id="_idIndexMarker451"/>to the cloud. A <strong class="bold">data lake</strong> is a well-known pattern on the cloud today, which allows a centralized repository to store data as it arrives, without having to first structure the data. Thereafter, different types of analytics, machine learning, and visualizations can be performed on that data for consumers to achieve better business outcomes. So, what is the equivalent of a data lake for the edge?</p>
			<p>Let's introduce a new pattern, called<a id="_idIndexMarker452"/> a <em class="italic">data pond</em>, for the authoritative source of data (that is, the golden source) that is generated and temporarily persisted on the edge. Certain characteristics of a data pond are listed as follows: </p>
			<ul>
				<li>A data pond enables the quick ingestion and consumption of data in a fast and flexible fashion. A data producer is only required to know where to push the data, that is, the local storage, local stream, or cloud. The choice of the storage layer, schema, ingestion frequency, and quality of the data is left to the data producer.</li>
				<li>A data pond should work with low-cost storage. Generally, IoT devices are low in storage; therefore, only highly valuable data that's relevant for the edge operations can be persisted locally. The rest of the data is pushed to the cloud for additional processing or thrown away (if noisy). </li>
				<li>A data pond supports schema on read. There can be multiple streams supporting multiple schemas in a data pond.</li>
				<li>A data pond should support the data protection mechanisms at rest and in encryption. It's also useful to implement role-based access that allows auditing the data trail as it flows from the edge to the cloud.</li>
			</ul>
			<p>The following diagram shows an edge architecture of how data collected from different sensors/actuators<a id="_idIndexMarker453"/> can be persisted and securely governed in a <a id="_idIndexMarker454"/>data pond: </p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="Images/B17595_05_015.jpg" alt="Figure 5.15 – A data pond architecture at the edge&#13;&#10;" width="839" height="518"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.15 – A data pond architecture at the edge</p>
			<p>The organizational entities involved in the preceding data flow include the following: </p>
			<ul>
				<li><strong class="bold">Data producers</strong>: These are entities that generate data. These include physical (such as sensors, actuators, or associated devices) or logical (such as applications) entities and are configured to store data in the data pond or publish data to the cloud. </li>
				<li><strong class="bold">Data pond team</strong>: Generally, the data operations team defines the data access mechanisms for the data pond (or lake) and the development team supports data management. </li>
				<li><strong class="bold">Data consumers</strong>: Edge and cloud applications retrieve data from the data pond (or lake) using the mechanisms authorized to further iterate on the data and meet business needs. </li>
			</ul>
			<p>The following screenshot shows the organizational entities for the data pond:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="Images/B17595_05_016.jpg" alt="Figure 5.16 – The organizational entities for the data pond&#13;&#10;" width="897" height="325"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.16 – The organizational entities for the data pond</p>
			<p>Now you have understood how data can be stored on a data pond and be managed or governed by <a id="_idIndexMarker455"/>different entities. Next, let's discuss the different flavors of <a id="_idIndexMarker456"/>data and how they can be integrated. </p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor106"/>Data integration concepts</h2>
			<p>DII occurs<a id="_idIndexMarker457"/> through <a id="_idIndexMarker458"/>different <a id="_idIndexMarker459"/>layers, as follows: </p>
			<ul>
				<li><strong class="bold">Batch</strong>: This layer aggregates data that has been generated by data producers. The goal is to increase the accuracy of data insights through the consolidation of data from multiple sources or dimensions.</li>
				<li><strong class="bold">Speed</strong>: This layer streams data generated by data producers. The goal is to allow a near real-time analysis of data with an acceptable level of accuracy.</li>
				<li><strong class="bold">Serving</strong>: This layer merges the data from the batch layer and the speed layer to enable the downstream <a id="_idIndexMarker460"/>consumers or business users with holistic and incremental insights into the business.</li>
			</ul>
			<p>The following diagram is an illustration of DII:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="Images/B17595_05_017.jpg" alt="Figure 5.17 -- Data Integration and Interoperability (DII)&#13;&#10;" width="628" height="245"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.17 -- Data Integration and Interoperability (DII)</p>
			<p>As you can see, there are multiple layers within the data flow that are commonly implemented <a id="_idIndexMarker461"/>using the <strong class="bold">Extract, Transform, and Load</strong> (<strong class="bold">ETL</strong>) methodology <a id="_idIndexMarker462"/>or the <strong class="bold">Extract, Load, and Transform</strong> (<strong class="bold">ELT</strong>) methodology in the big data world. The ETL methodology involves steps to extract data from different sources, implement data quality and consistency standards, transform (or aggregate) the data to conform to a standard format, and load (or deliver) data to downstream applications. </p>
			<p>The ELT process is a variant of ETL with similar steps. The difference is that extracted data is loaded before the transformation. This is common for edge workloads as well, where the local <a id="_idIndexMarker463"/>gateway might not have enough resources to do the <a id="_idIndexMarker464"/>transformation locally; therefore, it publishes the data prior to additional processing. </p>
			<p>But how are these data integration patterns used in the edge? Let's explore this next. </p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor107"/>Data flow patterns</h2>
			<p>An ETL flow <a id="_idIndexMarker465"/>on the edge will include three distinct steps, as follows: </p>
			<ol>
				<li value="1">Data extraction from devices such as sensors/actuators</li>
				<li>Data transformation to clean, filter, or restructure data into an optimized format for further consumption</li>
				<li>Data loading to publish data to the persistence layer such as a data pond, a data lake, or a data warehouse</li>
			</ol>
			<p>For an ELT flow, <em class="italic">steps 2</em> and<em class="italic"> 3</em> will take place in the reverse order. </p>
			<p class="callout-heading">An ETL Scenario for a Connected Home </p>
			<p class="callout">For example, in a connected home scenario, it's common to extract data from different sensors/actuators, followed by a data transformation that might include format changes, structural changes, semantic conversions, or deduplication. Additionally, data transformation allows you to filter out any noisy data from the home (think of a crying baby or a noisy pet), resulting in reduced network charges of publishing all the bits and bytes to the cloud. Based on a use case such as an intrusion alert or replenishing a printer toner, data transformation can be performed in batch or real time, by eitherphysically storing the result in a staging area or virtually storing the transferred data in memory until you are ready to move to the load step. </p>
			<p>These core patterns (ETL or ELT) have evolved, with time, into different data flow architectures, such as event-driven, batch, lambda, and<em class="italic"> </em><strong class="bold">complex event processing</strong><em class="italic"> (</em><strong class="bold">CEP</strong><em class="italic">)</em>. We will<a id="_idIndexMarker466"/> explain each of them in the next section. </p>
			<h3>Event-driven (or streaming)</h3>
			<p>It's very <a id="_idIndexMarker467"/>common for edge applications to generate and<a id="_idIndexMarker468"/> process data in smaller sets throughout the day when an event happens. Near real-time data processing has a lower latency and can be both synchronous and asynchronous. </p>
			<p>In an asynchronous data flow, the devices (such as sensors) do not wait for the receiving system to acknowledge updates before continuing processing. For example, in a connected home, a motion/occupancy sensor can trigger an intruder notification based on a detected event but continue to monitor without waiting for an acknowledgment.</p>
			<p>In comparison, in a real-time synchronous data flow, no time delay or other differences between source and target are acceptable. For example, in a connected home, if there is a fire alarm, it should notify the emergency services in a deterministic way. </p>
			<p>With AWS Greengrass, you can design both <strong class="bold">synchronous</strong> and <strong class="bold">asynchronous</strong> data communications. In addition to this, as we build multi-faceted architectures on the edge, it's quite normal to build multiprocessing or multithreaded polyglot solutions on the edge to<a id="_idIndexMarker469"/> support <a id="_idIndexMarker470"/>different low-latency use cases:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="Images/B17595_05_018.jpg" alt="Figure 5.18 – Event-driven architecture at the edge&#13;&#10;" width="1614" height="672"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.18 – Event-driven architecture at the edge</p>
			<h3>Micro-batch (or aggregated processing)</h3>
			<p>Most <a id="_idIndexMarker471"/>enterprises<a id="_idIndexMarker472"/> perform frequent batch processing to enable end users with business insights. In this mode, data moving will represent either the full set of data at a given point in time, such as the energy meter reading of a connected home at the end of a period (such as the day, week, or month), or data that has changed values since the last time it was sent, such as a hvac reading or a triggered fire alarm. Generally, batch systems are designed to scale and grow proportionally along with the data. However, that's not feasible on the edge due to the lack of horsepower, as explained earlier. </p>
			<p>Therefore, for IoT use cases, leveraging micro-batch processing is more common. Here, the data is stored locally and is processed on a much higher frequency, such as every few seconds, minutes, hours, or days (over weeks or months). This allows data consumers to gather insights from local data sources with reduced latency and cost, even when disconnected from the <a id="_idIndexMarker473"/>internet. The <strong class="bold">Stream Manager</strong> capability of AWS Greengrass allows you to perform aggregated processing on the edge. Stream Manager brings enhanced functionalities regarding how to process data on the edge such as defining a bandwidth and data prioritization for multiple channels, timeout behavior, and direct export mechanisms to different AWS services such as Amazon S3, AWS IoT Analytics, AWS IoT<a id="_idIndexMarker474"/> SiteWise, and<a id="_idIndexMarker475"/> Amazon Kinesis data streams:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="Images/B17595_05_019.jpg" alt="Figure 5.19 – Micro-batch architecture at the edge&#13;&#10;" width="1610" height="978"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.19 – Micro-batch architecture at the edge</p>
			<h3>Lambda architecture </h3>
			<p>Lambda <a id="_idIndexMarker476"/>architecture is an approach that combines both <a id="_idIndexMarker477"/>micro-batch and stream (near real-time) data processing. It makes the consolidated data available for downstream consumption. For example, a refrigeration unit, a humidifier, or any critical piece of machinery on a manufacturing plant can be monitored and fixed before it becomes non-operational. So, for a connected HBS hub solution, micro-batch processing will allow you to detect long-term trends or failure patterns. This capability in turn, will help your fleet operators recommend preventive or predictive maintenance for the machines to end consumers. This workflow is often referred to as the warm or cold path of the data analytics flow. </p>
			<p>On the other hand, stream processing will allow the fleet operators to derive near real-time insights through telemetry data. This will enable consumers to take mission-critical actions such as locking the entire house and calling emergency services if any theft is detected. This is <a id="_idIndexMarker478"/>also referred to as the hot path in lambda<a id="_idIndexMarker479"/> architecture:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="Images/B17595_05_020.jpg" alt="Figure 5.20 – Lambda architecture at the edge&#13;&#10;" width="1609" height="1186"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.20 – Lambda architecture at the edge</p>
			<p class="callout-heading">Fun fact </p>
			<p class="callout">Lambda architecture has nothing to do with the AWS lambda service. The term was coined by Nathan Marz, who worked on big-data-related technologies at <em class="italic">BackType</em> and <em class="italic">Twitter</em>. This is a design<a id="_idIndexMarker480"/> pattern for describing data <a id="_idIndexMarker481"/>processing that is scalable and fault-tolerant. </p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor108"/>Data flow anti-patterns for the edge</h2>
			<p>So far, we have <a id="_idIndexMarker482"/>learned about the common data flow patterns on the edge. Let's also discuss some of the anti-patterns. </p>
			<h3>Complex Event Processing (CEP) </h3>
			<p>Events are data patterns that identify complex circumstances from ingested data, such as network outages in a home or a fire alarm in a building. It might be easier to detect events from a few sensors or devices; however, getting visibility into complex events from disparate sources and being able to capture states or trigger conditional logic to identify and resolve issues quickly requires special treatment. </p>
			<p>That's where the CEP pattern comes into play. CEP can be resource-intensive and needs to scale to all sizes of data and grow proportionally. Therefore, it's still not a very common pattern on the edge. On the cloud, managed services such as AWS IoT events or AWS EventBridge can make it easier for you to perform CEP on the data generated from your IoT devices.</p>
			<h3>Batch</h3>
			<p>Traditionally, in batch <a id="_idIndexMarker483"/>processing, data moves in aggregates as blobs or files either on an ad hoc request from a consumer or automatically on a periodic schedule. Data will either be a full set (referred to as snapshot) or a changed set (delta) from a given point in time. Batch processing requires continuous scaling of the underlying infrastructure to facilitate the data growth and processing requirements. Therefore, it's a pattern that is better suited for big data or data warehousing solutions on the cloud. That being said, for an edge use case, you can still leverage the micro-batch pattern (as explained earlier) to aggregate data that's feasible in the context of a resource-constrained environment.</p>
			<h3>Replication</h3>
			<p>It's a common <a id="_idIndexMarker484"/>practice on the cloud to maintain redundant copies of datasets across different locations to improve business continuity, improve the end user experience, or enhance data resiliency. However, in the context of the <a id="_idIndexMarker485"/>edge, <strong class="bold">data replication</strong> can be expensive, as you might require redundant deployments. For example, with a connected HBS hub solution, if the gateway needs to support redundant storage for replication, it will increase<a id="_idIndexMarker486"/> the <strong class="bold">bill of materials</strong> (<strong class="bold">BOM</strong>) cost of the hardware, and you can lose the competitive edge on the market. </p>
			<h3>Archiving </h3>
			<p>Data that is <a id="_idIndexMarker487"/>used infrequently or not actively used can be moved to an alternate data storage solution that is more cost-effective to the organization. Similar to replication, for archiving data locally on the edge, additional deployment of hardware resource is necessary. This increases the <strong class="bold">bill of materials</strong> (<strong class="bold">BOM</strong>) cost of the device and leads to additional operational overhead. Therefore, it's common to archive the transformed data from the data lake <a id="_idIndexMarker488"/>to a cost-effective storage service on the cloud such as <strong class="bold">Amazon Glacier</strong>. Thereafter, this data can be used for local operations, data recovery, or regulatory needs. </p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor109"/>A hands-on approach with the lab </h1>
			<p>In this <a id="_idIndexMarker489"/>section, you will learn how to build a lambda architecture on the edge using different AWS services. The following diagram shows the lambda architecture:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="Images/B17595_05_021.jpg" alt="Figure 5.21 – The lab architecture&#13;&#10;" width="1227" height="509"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.21 – The lab architecture</p>
			<p>The preceding workflow uses the following services. In this chapter, you will complete <em class="italic">steps 1–6</em> (as shown in <em class="italic">Figure 5.21</em>). This includes designing and deploying the edge components, processing, and transforming data locally, and pushing the data to different cloud <a id="_idIndexMarker490"/>services: </p>
			<p class="figure-caption">\</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="Images/B17595_05_022.jpg" alt="Figure 5.22 – The hands-on lab components&#13;&#10;" width="556" height="149"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.22 – The hands-on lab components</p>
			<p>In this hands-on section, your objective will consist of the following:</p>
			<ol>
				<li value="1">Build the cloud resource (that is, Amazon Kinesis data streams, Amazon S3 bucket, and DynamoDB tables).</li>
				<li>Build and deploy the edge components (that is, artifacts and recipes) locally on Raspberry Pi.</li>
				<li>Validate<a id="_idIndexMarker491"/> that the data is streamed from the edge to the cloud (AWS IoT Core).</li>
			</ol>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor110"/>Building cloud resources </h2>
			<p>Deploy the<a id="_idIndexMarker492"/> CloudFormation template from the <strong class="source-inline">chapter5/cfn</strong> folder <a id="_idIndexMarker493"/>to create cloud resources such as Amazon S3 buckets, Kinesis data streams, and DynamoDB tables. You will need to substitute these respective names from the <em class="italic">Resources</em> section of the deployed stack, when requested, in the following section. </p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor111"/>Building edge components</h2>
			<p>Now, let's <a id="_idIndexMarker494"/>hop on to our device to <a id="_idIndexMarker495"/>build and deploy the required edge components:</p>
			<ol>
				<li value="1">Navigate to the following working directory from the Terminal of your Raspberry Pi device:<p class="source-code"><strong class="bold">cd hbshub/artifacts</strong></p></li>
				<li>Open the Python script using the editor of your choice (such as <em class="italic">nano</em>, <em class="italic">vi</em>, or <em class="italic">emac</em>):<p class="source-code"><strong class="bold">nano com.hbs.hub.Publisher/1.0.0/hbs_sensor.py</strong></p><p>The following code simulates data from the fictional sensors associated with the HBS hub. In the real world, data will be published from the real sensors over serial or GPIO, which needs to be captured. The following function is in a <strong class="source-inline">DummySensor</strong> class that will be referenced by the <strong class="source-inline">Publisher</strong> component in the next step:</p><p class="source-code">    def read_value(self):</p><p class="source-code">        message = {}</p><p class="source-code">        </p><p class="source-code">        device_list = ['hvac', 'refrigerator', 'washingmachine']</p><p class="source-code">        device_name = random.choice(device_list)</p><p class="source-code">        </p><p class="source-code">        if device_name == 'hvac' :</p><p class="source-code">            message['device_id'] = "1"</p><p class="source-code">            message['timestamp'] = float("%.4f" % (time()))</p><p class="source-code">            message['device_name'] = device_name</p><p class="source-code">            message['temperature'] = round(random.uniform(10.0, 99.0), 2)</p><p class="source-code">            message['humidity'] = round(random.uniform(10.0, 99.0), 2)</p><p class="source-code">        elif device_name == 'washingmachine' :</p><p class="source-code">            message['device_id'] = "2"</p><p class="source-code">            message['timestamp'] = float("%.4f" % (time()))</p><p class="source-code">            message['device_name'] = device_name</p><p class="source-code">            message['duty_cycles'] = round(random.uniform(10.0, 99.0), 2)</p><p class="source-code">        else :</p><p class="source-code">            message['device_id'] = "3"</p><p class="source-code">            message['timestamp'] = float("%.4f" % (time()))</p><p class="source-code">            message['device_name'] = device_name</p><p class="source-code">            message['vibration'] = round(random.uniform(100.0, 999.0), 2)</p><p class="source-code">        </p><p class="source-code">        return message</p></li>
				<li>Now, open the following <strong class="source-inline">publisher</strong> script and navigate through the code:<p class="source-code"><strong class="bold">nano com.hbs.hub.Publisher /1.0.0/hbs_publisher.py</strong></p><p>The following publisher code streams the data from the dummy sensors, as explained<a id="_idIndexMarker496"/> in<a id="_idIndexMarker497"/> the previous step, to a <strong class="source-inline">hbs/localtopic</strong> topic every <strong class="source-inline">10</strong> seconds over <strong class="source-inline">ipc</strong>: </p><p class="source-code">TIMEOUT = 10</p><p class="source-code">ipc_client = awsiot.greengrasscoreipc.connect()</p><p class="source-code">sensor = DummySensor()</p><p class="source-code">while True:</p><p class="source-code">    message = sensor.read_value()</p><p class="source-code">    message_json = json.dumps(message).encode('utf-8')</p><p class="source-code">    request = PublishToTopicRequest()</p><p class="source-code">    request.topic = args.pub_topic</p><p class="source-code">    publish_message = PublishMessage()</p><p class="source-code">    publish_message.json_message = JsonMessage()</p><p class="source-code">    publish_message.json_message.message = message</p><p class="source-code">    request.publish_message = publish_message</p><p class="source-code">    operation = ipc_client.new_publish_to_topic()</p><p class="source-code">    operation.activate(request)</p><p class="source-code">    future = operation.get_response()</p><p class="source-code">    future.result(TIMEOUT)</p><p class="source-code">    print("publish")</p><p class="source-code">    time.sleep(5)</p></li>
				<li>Now you have reviewed the code, check the following recipe file to review the access controls and dependencies that are required by the <strong class="source-inline">Publisher</strong> component: <p class="source-code"><strong class="bold">cd ~/hbshub/recipes</strong></p><p class="source-code"><strong class="bold">nano com.hbs.hub.Publisher-1.0.0.yaml</strong></p></li>
				<li>Now we have <a id="_idIndexMarker498"/>the component<a id="_idIndexMarker499"/> and the recipe, let's create a local deployment: <p class="source-code"><strong class="bold">sudo /greengrass/v2/bin/greengrass-cli deployment create   --recipeDir ~/hbshub/recipes --artifactDir ~/hbshub/artifacts --merge "com.hbs.hub.Publisher=1.0.0"</strong></p><p class="source-code">Local deployment submitted! Deployment Id: xxxxxxxxxxxxxx</p></li>
				<li>Verify that the component has successfully been deployed (and is running) using the following command: <p class="source-code"><strong class="bold">sudo /greengrass/v2/bin/greengrass-cli component list</strong></p><p>The following is the output:</p><p class="source-code">Components currently running in Greengrass:</p><p class="source-code">Component Name: com.hbs.hub.Publisher</p><p class="source-code">    Version: 1.0.0</p><p class="source-code">    State: RUNNING</p></li>
				<li>Now that the <strong class="source-inline">Publisher</strong> component is up and running, let's review the code in the <strong class="source-inline">Subscriber</strong> component as well: <p class="source-code"><strong class="bold">cd hbshub/artifacts</strong></p><p class="source-code"><strong class="bold">nano com.hbs.hub.Subscriber/1.0.0/hbs_subscriber.py</strong></p><p>The <strong class="source-inline">Subscriber</strong> component subscribes to the <strong class="source-inline">hbs/localtopic</strong> topic over the <strong class="source-inline">ipc</strong> protocol <a id="_idIndexMarker500"/>and gets triggered by <a id="_idIndexMarker501"/>the published events from the publisher:</p><p class="source-code">def setup_subscription():</p><p class="source-code">    request = SubscribeToTopicRequest()</p><p class="source-code">    request.topic = args.sub_topic</p><p class="source-code">    handler = StreamHandler()</p><p class="source-code">    operation = ipc_client.new_subscribe_to_topic(handler) </p><p class="source-code">    future = operation.activate(request)</p><p class="source-code">    future.result(TIMEOUT)</p><p class="source-code">    return operation</p><p>Then, the subscriber pushes the messages over <strong class="source-inline">mqtt</strong> to the <strong class="source-inline">hbs/cloudtopic</strong> cloud topic on AWS IoT Core:</p><p class="source-code">def send_cloud(message_json):</p><p class="source-code">    message_json_string = json.dumps(message_json)</p><p class="source-code">    request = PublishToIoTCoreRequest()</p><p class="source-code">    request.topic_name = args.pub_topic</p><p class="source-code">    request.qos = QOS.AT_LEAST_ONCE</p><p class="source-code">    request.payload = bytes(message_json_string,"utf-8")</p><p class="source-code">    publish_message = PublishMessage()</p><p class="source-code">    publish_message.json_message = JsonMessage()</p><p class="source-code">    publish_message.json_message.message = bytes(message_json_string, "utf-8")</p><p class="source-code">    request.publish_message = publish_message</p><p class="source-code">    operation = ipc_client.new_publish_to_iot_core()</p><p class="source-code">    operation.activate(request)</p><p class="source-code">    logger.debug(message_json)</p></li>
				<li>Now you have reviewed the code, let's check the following recipe file to review the access <a id="_idIndexMarker502"/>controls and dependencies<a id="_idIndexMarker503"/> required by the <strong class="source-inline">Subscriber</strong> component: <p class="source-code"><strong class="bold">cd ~/hbshub/recipes</strong></p><p class="source-code"><strong class="bold">nano com.hbs.hub.Subscriber-1.0.0.yaml</strong></p></li>
				<li>Now we have the component and the recipe, let's create a local deployment: <p class="source-code"><strong class="bold">sudo /greengrass/v2/bin/greengrass-cli deployment create   --recipeDir ~/hbshub/recipes --artifactDir ~/hbshub/artifacts --merge "com.hbs.hub.Subscriber=1.0.0"</strong></p><p>The following is the output:</p><p class="source-code">Local deployment submitted! Deployment Id: xxxxxxxxxxxxxx</p></li>
				<li>Verify that the component has successfully been deployed (and is running) using the following command. Now you should see both the <strong class="source-inline">Publisher</strong> and <strong class="source-inline">Subscriber</strong> components running locally:<p class="source-code"><strong class="bold">sudo /greengrass/v2/bin/greengrass-cli component list</strong></p><p>The following is the output:</p><p class="source-code">Components currently running in Greengrass:</p><p class="source-code">Component Name: com.hbs.hub.Publisher</p><p class="source-code">    Version: 1.0.0</p><p class="source-code">    State: RUNNING</p><p class="source-code">Component Name: com.hbs.hub.Subscriber</p><p class="source-code">    Version: 1.0.0</p><p class="source-code">    State: RUNNING</p></li>
				<li>As you have observed, in the preceding code, the <strong class="source-inline">Subscriber</strong> component will not only subscribe to the local <strong class="source-inline">mqtt</strong> topics on the Raspberry Pi, but it will also start publishing data to AWS IoT Core (on the cloud). Let's verify that from the AWS IoT console: <p>Please<a id="_idIndexMarker504"/> navigate <a id="_idIndexMarker505"/>to <strong class="bold">AWS IoT console</strong>. | Select <strong class="bold">Test</strong> (on the left pane). | Choose <strong class="bold">MQTT Client</strong>. | Subscribe to <strong class="bold">Topics</strong>. | Type <strong class="source-inline">hbs/cloudtopic</strong>. | Click <strong class="bold">Subscribe</strong>.</p><p class="callout-heading">Tip </p><p class="callout">If you have changed the default topic names in the recipe file, please use that name when you subscribe; otherwise, you won't see the incoming messages.</p></li>
				<li>Now that you have near real-time data flowing from the edge to the cloud, let's work on the micro-batch flow by integrating with Stream Manager. This component will subscribe to the <strong class="source-inline">hbslocal/topic</strong> topic (same as the subscriber). However, it will append the data to a local data stream using the Stream Manager functionality rather than publishing it to the cloud. Stream Manager is a key functionality for you to build a lambda architecture on the edge. We will break down the code into different snippets for you to understand these concepts better. So, let's navigate to the working directory: <p class="source-code"><strong class="bold">nano com.hbs.hub.Aggregator/1.0.0/hbs_aggregator.py</strong></p></li>
				<li>First, we create a local stream with the required properties such as stream name, data size, time to live, persistence, data flushing, data retention strategy, and more. Data within the stream can stay local for further processing or can be exported to the cloud using the export definition parameter. In our case, we are exporting the data to Kinesis, but you can use a similar approach to export the data to other <a id="_idIndexMarker506"/>supported services such <a id="_idIndexMarker507"/>as S3, IoT Analytics, and more:<p class="source-code"> iotclient.create_message_stream(</p><p class="source-code">     MessageStreamDefinition(</p><p class="source-code">       name=stream_name,  </p><p class="source-code">       max_size=268435456,  </p><p class="source-code">       stream_segment_size=16777216,  </p><p class="source-code">       time_to_live_millis=None,</p><p class="source-code">     strategy_on_full=StrategyOnFull.OverwriteOldestData,  </p><p class="source-code">       persistence=Persistence.File,  </p><p class="source-code">       flush_on_write=False,  </p><p class="source-code">       export_definition=ExportDefinition(</p><p class="source-code">            kinesis=[</p><p class="source-code">              KinesisConfig(</p><p class="source-code">                  identifier="KinesisExport",</p><p class="source-code">                  kinesis_stream_name=kinesis_stream,</p><p class="source-code">                  batch_size=1,</p><p class="source-code">                  batch_interval_millis=60000,</p><p class="source-code">                  priority=1</p></li>
				<li>Now the stream is defined, the data is appended through <strong class="source-inline">append_message api</strong>:<p class="source-code">sequence_number = client.append_message(stream_name=stream_name, data= event.json_message.message)</p><p class="callout-heading">Fact Check</p><p class="callout">Stream Manager allows you to deploy a lambda architecture on the edge without having to deploy and manage a separate lightweight database or streaming solution. Therefore, you can reduce the operational overhead or BOM cost of this solution. In addition to this, with Stream Manager as a data pond, you can persist data on the edge using a schema-less approach dynamically (remember BASE?). And finally, you can publish data to the cloud using the native integrations between the Stream Manager and cloud data services, such as IoT Analytics, S3, and Kinesis, without having to write any additional code. Stream Manager can also be beneficial for use cases with larger payloads such as blobs, images, or videos that can be easily transmitted over HTTPS. </p></li>
				<li>Now<a id="_idIndexMarker508"/> that you have reviewed the code, let's<a id="_idIndexMarker509"/> add the required permission for the Stream Manager component to update the Kinesis stream:<p>Please navigate to <strong class="bold">AWS IoT console</strong>. | Select <strong class="bold">Secure</strong> (on the left pane). | Choose <strong class="bold">Role Aliases</strong> and select the appropriate one. | Click on the <strong class="bold">Edit IAM Role</strong>. | Attach policies. | Choose <strong class="bold">Amazon Kinesis Full Access</strong>. | Attach policy.</p><p>Please note that it's not recommended to use a blanket policy similar to this for production workloads. This is used here in order to ease the reader into operating in a test environment. </p></li>
				<li>Let's perform a quick check of the recipe file prior to deploying this component:<p class="source-code"><strong class="bold">cd ~/hbshub/recipes</strong></p><p class="source-code"><strong class="bold">nano com.hbs.hub.Aggregator-1.0.0.yaml</strong></p><p>Please note the <strong class="source-inline">Configuration</strong> section in this recipe file, as it requires the Kinesis stream name to be updated. This can be retrieved from the resources section of the deployed CloudFormation stack. Also, note the dependencies on the Stream Manager component and the reference to <strong class="source-inline">sdk</strong>, which is required by the <a id="_idIndexMarker510"/>component<a id="_idIndexMarker511"/> at runtime: </p><p class="source-code">ComponentConfiguration:</p><p class="source-code">  DefaultConfiguration:</p><p class="source-code">    sub_topic: "hbs/localtopic"</p><p class="source-code">    kinesis_stream: "&lt;replace-with-kinesis-stream-from cfn&gt;"</p><p class="source-code">    accessControl: </p><p class="source-code">      aws.greengrass.ipc.pubsub:</p><p class="source-code">        com.hbs.hub.Aggregator:pubsub:1:</p><p class="source-code">          policyDescription: "Allows access to subscribe to topics"</p><p class="source-code">          operations:</p><p class="source-code">            - aws.greengrass#SubscribeToTopic</p><p class="source-code">            - aws.greengrass#PublishToTopic</p><p class="source-code">          resources: </p><p class="source-code">            - "*"</p><p class="source-code">ComponentDependencies:</p><p class="source-code">  aws.greengrass.StreamManager:</p><p class="source-code">    VersionRequirement: "^2.0.0"</p><p class="source-code">Manifests:</p><p class="source-code">  - Platform:</p><p class="source-code">      os: all</p><p class="source-code">    Lifecycle:</p><p class="source-code">      Install: </p><p class="source-code">        pip3 install awsiotsdk numpy -t .</p><p class="source-code">      Run: |</p><p class="source-code">        export PYTHONPATH=$PYTHONPATH:{artifacts:path}/stream_manager</p><p class="source-code">        PYTHONPATH=$PWD python3 -u {artifacts:path}/hbs_aggregator.py --sub-topic="{configuration:/sub_topic}" --kinesis-stream="{configuration:/kinesis_stream}"</p></li>
				<li>Next, as we have the artifact and the recipe reviewed, let's create a local deployment: <p class="source-code"><strong class="bold">sudo /greengrass/v2/bin/greengrass-cli deployment create   --recipeDir ~/hbshub/recipes --artifactDir ~/hbshub/artifacts --merge "com.hbs.hub.Aggregator=1.0.0"</strong></p><p class="source-code"><strong class="bold">Local deployment submitted! Deployment Id: xxxxxxxxxxxxxx</strong></p></li>
				<li>Verify that<a id="_idIndexMarker512"/> the <a id="_idIndexMarker513"/>component has been successfully deployed (and is running) using the following command. You should observe all the following components running locally:<p class="source-code"><strong class="bold">sudo /greengrass/v2/bin/greengrass-cli component list</strong></p><p>The output is as follows:</p><p class="source-code">Components currently running in Greengrass:</p><p class="source-code">Component Name: com.hbs.hub.Publisher</p><p class="source-code">    Version: 1.0.0</p><p class="source-code">    State: RUNNING</p><p class="source-code">Component Name: com.hbs.hub.Subscriber</p><p class="source-code">    Version: 1.0.0</p><p class="source-code">    State: RUNNING</p><p class="source-code">Component Name: com.hbs.hub.Aggregator</p><p class="source-code">    Version: 1.0.0</p><p class="source-code">    State: RUNNING</p></li>
				<li>The <strong class="source-inline">Aggregator</strong> component will publish the data directly from the local stream to the Kinesis stream on the cloud. Let's navigate to the AWS S3 console to check whether the incoming messages are appearing:<p>Go to the <strong class="bold">Amazon Kinesis console</strong>. | Select <strong class="bold">Data Streams</strong>. | Choose the stream. | Go to the <strong class="bold">Monitoring</strong> tab. | Check the metrics such as <strong class="bold">Incoming data</strong> or <strong class="bold">Get records</strong>.</p></li>
			</ol>
			<p>If you see the metrics showing some data points on the chart, it means the data is successfully reaching the cloud. </p>
			<p class="callout-heading">Note </p>
			<p class="callout">You can always find <a id="_idIndexMarker514"/>the specific resource names required for this lab (such as the preceding <a id="_idIndexMarker515"/>Kinesis stream) in the <em class="italic">Resources</em> or <em class="italic">Output</em> section of the CloudFormation stack deployed earlier. </p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor112"/>Validating the data streamed from the edge to the cloud</h2>
			<p>In this <a id="_idIndexMarker516"/>section, you will perform some final validation to ensure the transactional and batch data streamed from the edge components is successfully persisted on the data lake:</p>
			<ol>
				<li value="1">In <em class="italic">step 19 of the previous section</em>, you validated that the Kinesis stream is getting data through metrics. Now, let's understand how that data is persisted to the data lake from the streaming layer:<p>Go to the <strong class="bold">Amazon Kinesis console</strong><strong class="bold">.</strong> | Select <strong class="bold">Delivery Streams</strong>. | Choose the respective delivery stream. | Click on the <strong class="bold">Configuration</strong> tab. | Scroll down to <strong class="bold">Destination Settings</strong>. | Click on the S3 bucket under the Amazon S3 destination.</p><p>Click on the bucket and drill down to the child buckets that store the batch data in a zipped format to help optimize storage costs. </p></li>
				<li>As the final step, navigate to <strong class="bold">AWS DynamoDB</strong> to review the raw sensor data streaming in near real time through AWS IoT Core:<p>On the <strong class="bold">DynamoDB</strong> console, choose <strong class="source-inline">Tables</strong>. Then, select the table for this lab. Click on <strong class="bold">View Items</strong>.</p><p>Can you view the time series data? Excellent work. </p><p class="callout-heading">Note</p><p class="callout">If you are not able to complete any of the preceding steps, please refer to the <em class="italic">Troubleshooting</em> section in the GitHub repository or create an issue for additional instructions. </p></li>
			</ol>
			<p>Congratulations! You have come a long way to learn how to build a lambda architecture that spans<a id="_idIndexMarker517"/> from the edge to the cloud using different AWS edge and cloud services. Now, let's wrap up with some additional topics before we conclude this chapter.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor113"/>Additional topics for reference</h1>
			<p>Aside from what we have read so far, there are a couple of topics that I wish to mention. Whenever you have the time, please check them out, as they do have lots of benefits and can be found online. </p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor114"/>Time series databases </h2>
			<p>In this<a id="_idIndexMarker518"/> chapter, we learned how to leverage a NoSQL (key-value) data store such as Amazon DynamoDB for persisting time series data. Another common <a id="_idIndexMarker519"/>way<a id="_idIndexMarker520"/> to persist IoT data is to use a <strong class="bold">time series database</strong> (<strong class="bold">TSDB</strong>) such as <strong class="bold">Amazon Timestream</strong> or <strong class="bold">Apache Cassandra</strong>. As you know by now, time series data consists of measurements or events collected from different sources such as sensors and actuators that are indexed over time. Therefore, the fundamentals of modeling a time series database are quite similar to what was explained earlier with NoSQL data solutions. So, the obvious question that remains is <em class="italic">How do you choose between NoSQL and TSDB?</em> Take a look at the following considerations: </p>
			<ul>
				<li><strong class="bold">Consider the data summarization and data precision requirements</strong>: <p>For example, show me the energy utilization on a monthly or yearly basis. This requires going over a series of data points indexed by a time range to calculate a percentile increase of energy over the same period in the last 12 weeks, summarized by weeks. This kind of querying could get expensive with a distributed key-value store. </p></li>
				<li><strong class="bold">Consider purging the data after a period of time</strong>: <p>For example, do consumers really care about the high precision metrics from an hourly basis to calculate their overall energy utilization per month? Probably not. Therefore, it's more efficient to store high-precision data for a short period of time and, thereafter, aggregated and downsampled data for identifying long-term trends. This functionality can partially be achieved with some NoSQL databases as well (such as the DynamoDB item expiry functionality). However, TSDBs are better suited as they can also offer downsampling and aggregation capabilities using different means, such as materialized views. </p></li>
			</ul>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor115"/>Unstructured data </h2>
			<p>You must be<a id="_idIndexMarker521"/> curious that most of our discussion in this chapter was related to structured and semi-structured data. We did not touch upon unstructured data (such as images, audio, and videos) at all. You are spot on. Considering IoT is the bridge between the physical world and the cyber world, there will be a huge amount of unstructured data that will need to be processed for different analytics and machine learning use cases. </p>
			<p>For example, consider a scenario where the security cameras installed in your customer's home detect any infiltration or unexpected movements through the motion sensors and start streaming a video feed of the surroundings. The feed will be available through your smart hub or mobile devices for consumption. Therefore, in this scenario, the security camera is streaming videos that are unstructured data, as a P2P feed that can also be stored (if the user allows) locally on the hub or to an object store on the cloud. In <a href="B17595_07_Final_SS_ePub.xhtml#_idTextAnchor138"><em class="italic">Chapter 7</em></a>, <em class="italic">Machine Learning Workloads at the Edge</em>, you will learn the techniques to ingest, store, and infer unstructured data from the edge. However, we will not delve into the modeling techniques for unstructured data, as it primarily falls under data science<a id="_idIndexMarker522"/> and is not very relevant in the day-to-day life of IoT practitioners. </p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor116"/>Summary</h1>
			<p>In this chapter, you learned about different data modeling techniques, data storage, and data integration patterns that are common with IoT edge workloads. You learned how to build, test, and deploy edge components on Greengrass. Additionally, you implemented a lambda architecture to collect, process, and stream data from disparate sources on the edge. Finally, you validated the workflow by visualizing the incoming data on IoT Core. </p>
			<p>In the next chapter, you will learn how all this data can be served on the cloud to generate valuable insights for different end consumers. </p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor117"/>Knowledge check</h1>
			<p>Before moving on to the next chapter, test your knowledge by answering these questions. The answers can be found at the end of the book: </p>
			<ol>
				<li value="1">True or false: Data modeling is only applicable for relational databases.</li>
				<li>What is the benefit of performing a data modeling exercise? </li>
				<li>Is there any relevance of ETL architectures for edge computing? (Hint: Think lambda.) </li>
				<li>True or false: Lambda architecture is the same as AWS lambda service.</li>
				<li>Can you think of at least one benefit of data processing at the edge?</li>
				<li>Which component of Greengrass is required to be run at the bare minimum for the device to be functional?</li>
				<li>True or false: Managing streams for real-time processing is a cloud-only thing.</li>
				<li>What strategy could you implement to persist data on the edge locally for a longer time?</li>
			</ol>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor118"/>References</h1>
			<p>Take a look at the following resources for additional information on the concepts discussed in this chapter:</p>
			<ul>
				<li><em class="italic">Data Management Body of Knowledge</em>: <a href="https://www.dama.org/cpages/body-of-knowledge">https://www.dama.org/cpages/body-of-knowledge</a></li>
				<li><em class="italic">Amazon's Dynamo</em>: <a href="https://www.allthingsdistributed.com/2007/10/amazons_dynamo.html">https://www.allthingsdistributed.com/2007/10/amazons_dynamo.html</a></li>
				<li><em class="italic">NoSQL Design for DynamoDB</em>: <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html</a></li>
				<li><em class="italic">Lambda Architecture</em>: <a href="http://lambda-architecture.net/">http://lambda-architecture.net/</a></li>
				<li><em class="italic">Managing data streams on the AWS IoT Greengrass Core</em>: <a href="https://docs.aws.amazon.com/greengrass/v2/developerguide/manage-data-streams.html">https://docs.aws.amazon.com/greengrass/v2/developerguide/manage-data-streams.html</a></li>
				<li><em class="italic">Data Lake on AWS</em>: <a href="https://aws.amazon.com/solutions/implementations/data-lake-solution/">https://aws.amazon.com/solutions/implementations/data-lake-solution/</a></li>
			</ul>
		</div>
	</div></body></html>