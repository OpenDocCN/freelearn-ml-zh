["```py\npip install tensorflow\n```", "```py\nimport tensorflow as tf\n```", "```py\n    import tensorflow as tf\n    input1 = tf.constant(2.0, tf.float32, name='input1')\n    input2 = tf.constant(3.0, tf.float32, name='input2')\n    input3 = tf.constant(4.0, tf.float32, name='input3')\n    product12 = tf.multiply(input1, input2)\n    sum = tf.add(product12, input3)\n    ```", "```py\n    with tf.Session() as session:\n        print(session.run(product12))\n        print(session.run(sum))\n    ```", "```py\n    6.0\n    10.0\n    ```", "```py\nimport tensorflow as tf\ninput1 = tf.constant(2.0, tf.float32, name='input1')\ninput2 = tf.placeholder(tf.float32, name='p')\ninput3 = tf.Variable(0.0, tf.float32, name='x')\nproduct12 = tf.multiply(input1, input2)\nsum = tf.add(product12, input3)\nwith tf.Session() as session:\n    initializer = tf.global_variables_initializer()\n    session.run(initializer)\n    print(session.run(sum, feed_dict={input2: 3.0}))\n```", "```py\nrandomMatrix = tf.Variable(tf.random_normal([3, 4]))\nwith tf.Session() as session:\n    initializer = tf.global_variables_initializer()\n    print( session.run(initializer))\n    print( session.run(randomMatrix))\n\nNone\n[[-0.41974232 1.8810892 -1.4549098 -0.73987174]\n[ 2.1072254 1.7968426 -0.38310152 0.98115194]\n[-0.550108 -0.41858754 1.3511614 1.2387075 ]]\n```", "```py\ny = f(x1*w1 + x2*w2 + x3*w3 + x4*w4)\n```", "```py\ny = f(x1*w1 + x2*w2 + x3*w3 + x4*w4)\n```", "```py\ny = f(x1*w1 + x2*w2 + x3*w3 + x4*w4 + b)\ny = f(x â‹… w + b)\n```", "```py\nimport numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.e ** (-x))\n```", "```py\nimport matplotlib.pylab as plt\nx = np.arange(-10, 10, 0.1)\nplt.plot(x, sigmoid(x))\nplt.show()\n```", "```py\ndef tanh(x):\n    return 2 / (1 + np.e ** (-2*x)) - 1\n```", "```py\nx = np.arange(-10, 10, 0.1)\nplt.plot(x, tanh(x))\nplt.show()\n```", "```py\ndef relu(x):\n    return 0 if x < 0 else x\n```", "```py\ndef reluArr(arr):\n   return [relu(x) for x in arr]\nx = np.arange(-10, 10, 0.1)\nplt.plot(x, reluArr(x))\nplt.show()\n```", "```py\ndef softmax(list):\n    return np.exp(list) / np.sum(np.exp(list))\n```", "```py\nsoftmax([1,2,1])\n```", "```py\narray([0.21194156, 0.57611688, 0.21194156])\n```", "```py\n    def get_y( f, x3 ):\n        return f(2*1+0.5*2+1.5*x3)\n    ```", "```py\n    import numpy as np\n    def sigmoid(x):\n        return 1 / (1 + np.e ** (-x))\n    def tanh(x):\n        return 2 / (1 + np.e ** (-2*x)) - 1\n    def relu(x):\n        return 0 if x < 0 else x\n    ```", "```py\n    get_y( sigmoid, -2 )\n    ```", "```py\n    get_y(sigmoid, -1)\n    ```", "```py\n    get_y(sigmoid, 0)\n    ```", "```py\n    get_y(sigmoid, 1)\n    ```", "```py\n    get_y(sigmoid, 2)\n    ```", "```py\n    get_y(tanh, -2)\n    ```", "```py\n    get_y(tanh, -1)\n    ```", "```py\n    get_y(tanh, 0)\n    ```", "```py\n    get_y(tanh, 1)\n    ```", "```py\n    get_y(tanh, 2)\n    ```", "```py\n    get_y(relu,-2)\n    ```", "```py\n    get_y(relu,-1)\n    ```", "```py\n    get_y(relu,0)\n    ```", "```py\n    get_y(relu,1)\n    ```", "```py\n    get_y(relu,2)\n    ```", "```py\npip install keras\n```", "```py\nimport tensorflow.keras.datasets.mnist as mnist\n(features_train, label_train),(features_test, label_test) =\nmnist.load_ data()\n```", "```py\nfrom PIL import Image\nImage.fromarray(features_train[5])\n```", "```py\nlabel_train[5]\n2\n```", "```py\nfeatures_train = features_train / 255.0\nfeatures_test = features_test / 255.0\n```", "```py\ndef flatten(matrix):\n    return [elem for row in matrix for elem in row]\nflatten([[1,2],[3,4]])\n```", "```py\n [1, 2, 3, 4]\n```", "```py\nfeatures_train_vector = [\n    flatten(image) for image in features_train\n]\nfeatures_test_vector = [\n    flatten(image) for image in features_test\n]\n```", "```py\nimport numpy as np\nlabel_train_vector = np.zeros((label_train.size, 10))\nfor i, label in enumerate(label_train_vector):\n    label[label_train[i]] = 1\nlabel_test_vector = np.zeros((label_test.size, 10))\nfor i, label in enumerate(label_test_vector):\n    label[label_test[i]] = 1\n```", "```py\nimport tensorflow as tf\nf = tf.nn.sigmoid\nx = tf.placeholder(tf.float32, [None, 28 * 28])\nW = tf.Variable(tf.random_normal([784, 10]))\nb = tf.Variable(tf.random_normal([10]))\n```", "```py\ndef classify(x):\n    return f(tf.add(tf.matmul(x, W), b))\n```", "```py\ny = classify(x)\ny_true = tf.placeholder(tf.float32, [None, 10])\ncross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n    logits=y,\n    labels=y_true\n)\n```", "```py\ncost = tf.reduce_mean(cross_entropy)\n```", "```py\noptimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.5).minimize(cost)\n```", "```py\ncross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n    logits=y,\n    labels=y_true\n)\n```", "```py\nsession = tf.Session()\n```", "```py\nsession.run(tf.global_variables_initializer())\n```", "```py\niterations = 300\nbatch_size = 200\nfor i in range(iterations):\n    min = i * batch_size\n    max = (i+1) * batch_size\n    dictionary = {\n        x: features_train_vector[min:max],\n        y_true: label_train_vector[min:max]\n    }\n    session.run(optimizer, feed_dict=dictionary)\n    print('iteration: ', i)\n```", "```py\nsession.run(classify(x), feed_dict={\n    x: features_test_vector[:10]\n} )\n```", "```py\nlabel_predicted = session.run(classify(x), feed_dict={\n    x: features_test_vector\n})\n```", "```py\nnp.argmax([0.1, 0.3, 0.5, 0.2, 0, 0, 0, 0.2, 0, 0 ])\n```", "```py\nlabel_predicted = [\n    np.argmax(label) for label in label_predicted\n]\n```", "```py\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nconfusion_matrix(label_test, label_predicted)\naccuracy_score(label_test, label_predicted)\nprecision_score( label_test, label_predicted, average='weighted' )\nrecall_score( label_test, label_predicted, average='weighted' )\nf1_score( label_test, label_predicted, average='weighted' )\n```", "```py\niterations = 300\nbatch_size = 200\nfor i in range(iterations):\n    min = i * batch_size\n    max = (i+1) * batch_size\n    dictionary = {\n        x: features_train_vector[min:max],\n        y_true: label_train_vector[min:max]\n    }\n    session.run(optimizer, feed_dict=dictionary)\n```", "```py\niterations = 6000\nbatch_size = 100\nsample_size = len(features_train_vector)\nfor _ in range(iterations):\n    indices = random.sample(range(sample_size), batchSize)\n    batch_features = [\n        features_train_vector[i] for i in indices\n    ]\n    batch_labels = [\n        label_train_vector[i] for i in indices\n    ]\n    min = i * batch_size\n    max = (i+1) * batch_size\n    dictionary = {\n        x: batch_features,\n        y_true: batch_labels\n    }\n    session.run(optimizer, feed_dict=dictionary)\n```", "```py\nimport random\nrandom.sample(range(1,91), 5)\n```", "```py\n[63, 58, 25, 41, 60]\n```", "```py\nx = tf.placeholder(tf.float32, [None, 28 * 28 ])\nf = tf.nn.softmax\nW1 = tf.Variable(tf.random_normal([784, 200]))\nb1 = tf.Variable(tf.random_normal([200]))\nlayer1_out = f(tf.add( tf.matmul(x, W1), b1))\nW2 = tf.Variable(tf.random_normal([200, 10]))\nb2 = tf.Variable(tf.random_normal([10]))\ny = f(tf.add(tf.matmul(layer1_out, W2), b2))\n```"]