<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Clean Up Your Personal Twitter Timeline by Clustering Tweets</h1>
                </header>
            
            <article>
                
<p class="mce-root">Here's a little bit of gossip for you: The original project for this title had to do with detecting foreign influence on US elections in social media. At about the same time, I was also applying for a visa to the United States, to give a series of talks. It later transpired that I hadn't needed the visa after all; ESTA covered all the things I had wanted to do in the United States. But as I was preparing for the visa, an attorney gave me a very stern talking-to about writing a book on the politics of the United States. The general advice is this—if I don't want trouble with US Customs and Border Patrol, I should not write or say anything on social media about American politics, and especially not write a chapter of a book on it. So, I had to hastily rewrite this chapter. The majority of methods used in this chapter can be used for the original purpose, but the content is a lot milder.</p>
<p class="mce-root">I use Twitter a lot. I mainly tweet and read Twitter in my downtime. I follow many people who share similar interests, among other things, machine learning, artificial intelligence, Go, linguistics, and programming languages. These people not only share interests with me; they also share interests with one another. As such, sometimes, multiple people may be tweeting about the same topic.</p>
<p class="mce-root">As may be obvious from the fact that I use Twitter a lot, I am a novelty junkie. I like new things. Multiple people tweeting about the same topic is nice if I am interested in the differing viewpoints, but I don't use Twitter like that. I use Twitter as a sort of summary of interesting topics. Events X, Y, and Z happened. It's good enough that I know they happened. For most topics, there is no benefit for me to go deep and learn what the finer points are, and 140 characters is not a lot of characters for nuance anyway. Therefore, a shallow overview is enough to keep my general knowledge abreast with the rest of the population.</p>
<p class="mce-root">Thus, when multiple people tweet about the same topic, that's repetition in my newsfeed. That's annoying. What if, instead of that, my feed could just be one instance of each topic?</p>
<p class="mce-root">I think of my Twitter-reading habit as happening in sessions. Each session is typically five minutes. I really only read about 100 tweets each session. If out of 100 tweets I read, 30% of the people I follow overlap on topics, then I really only have read 30 tweets of real content. That's not efficient at all! Efficiency means being able to cover more topics per session.</p>
<p class="mce-root">So, how do you increase efficiency in reading tweets? Well, remove the tweets that cover the same topic of course! There is the secondary matter of choosing the best tweet that summarizes the topic, but that's a subject for another day.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The project </h1>
                </header>
            
            <article>
                
<p class="mce-root">What we're going to do is to cluster tweets on Twitter. We will be using two different clustering techniques, K-means and DBSCAN. For this chapter, we're going to rely on some skills we built up in <a href="12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml">Chapter 2</a>, <em>Linear Regression – House Price Prediction</em>. We will also be using the same libraries used in <a href="">Chapter 2</a>, <em>Linear Regression – House Price Prediction</em>. On top of that, we will also be using the clusters library by mpraski.</p>
<p class="mce-root">By the end of the project, we will be able to clean up any collection of tweets from Twitter, and cluster them into groups. The main body of code that fulfills the objective is very simple, it's only about 150 lines of code in total. The rest of the code is for fetching and preprocessing data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">K-means </h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>K-means</strong> is a method of clustering data. The problem is posed as this—given a dataset of N items, we wish to partition the data into K groups. How do you do so?</p>
<p class="mce-root">Allow me to take a side bar and explore the wonderful world of coordinates. No, no, don't run! It's very visual.</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-283 image-border" src="Images/2b3bd494-5b0d-41b0-a81e-0401099e2b92.png" style="width:14.58em;height:5.67em;" width="288" height="112"/></p>
<p class="mce-root">Which line is longer? How do you know?</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-286 image-border" src="Images/7b12adde-1086-432d-bbcc-5ad5ccd91311.png" style="width:9.08em;height:6.00em;" width="196" height="131"/></p>
<p class="mce-root">You know which line is longer because you can measure each line from points a, b, c, and d. Now, let's try something different:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-284 image-border" src="Images/2c45dafa-dfaa-4e0b-b663-7766b5ef57ec.png" style="width:6.25em;height:6.08em;" width="154" height="147"/></p>
<p class="mce-root">Which dot is closest to X? How do you know?</p>
<p class="mce-root">You know because again, you can measure the distance between the dots. And now, for our final exercise:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-287 image-border" src="Images/6035e6fc-1038-4cbe-9bfa-15c56ba47388.png" style="width:7.08em;height:6.50em;" width="192" height="177"/></p>
<p class="mce-root">Consider the distance between the following:</p>
<ul>
<li class="mce-root"><strong>A</strong> and <strong>X</strong></li>
<li class="mce-root"><strong>A</strong> and <strong>Y</strong></li>
<li class="mce-root"><strong>A</strong> and <strong>Z</strong></li>
<li class="mce-root"><strong>B</strong> and <strong>X</strong></li>
<li class="mce-root"><strong>B</strong> and <strong>Y</strong></li>
<li class="mce-root"><strong>B</strong> and <strong>Z</strong></li>
<li class="mce-root"><strong>C</strong> and <strong>X</strong></li>
<li class="mce-root"><strong>C</strong> and <strong>Y</strong></li>
<li class="mce-root"><strong>C</strong> and <strong>Z</strong></li>
</ul>
<p class="mce-root">What is the average distance between <strong>A</strong> and <strong>X</strong>, <strong>B</strong> and <strong>X</strong>, and <strong>C</strong> and <strong>X</strong>? What is the average distance between <strong>A</strong> and <strong>Y</strong>, <strong>B</strong> and <strong>Y</strong> and <strong>C</strong> and <strong>Y</strong>? What is the average distance between <strong>A</strong> and <strong>Z</strong>, <strong>B</strong> and <strong>Z</strong>, and <strong>C</strong> and <strong>Z</strong>?</p>
<p class="mce-root">If you had to choose one point between <strong>X</strong>, <strong>Y</strong>, and <strong>Z</strong> to represent the <strong>A</strong>, <strong>B</strong>, and <strong>C</strong>, which would you choose?</p>
<p class="mce-root">Congratulations! You just did a very simple and abbreviated version of K-means clustering. Specifically, you did a variant where <em>k = 1</em>. If you had to pick two points between <strong>X</strong>, <strong>Y</strong>, and <strong>Z</strong>, then that's <em>k = 2</em>. A cluster is therefore the set of points that make it such that the average distance of the group is minimal.</p>
<p class="mce-root">That's a mouthful, but think back to what you just did. Now, instead of just three points, <strong>A</strong>, <strong>B</strong>, and <strong>C</strong>, you have many points. And you aren't given <strong>X</strong>, <strong>Y</strong>, or <strong>Z</strong>; you'd have to generate your own <strong>X</strong>, <strong>Y</strong>, and <strong>Z</strong> points. Then, you have to find the groups that minimize the distance to each possible points of <strong>X</strong>, <strong>Y</strong>, and <strong>Z</strong>.</p>
<p class="mce-root">That is, in a nutshell, K-means. It's easy to understand, but hard to implement it well. It turns out K-means is NP-hard; it may not be solved in polynomial time.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">DBSCAN</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>DBSCAN</strong> inherits the idea that data can be represented as multidimensional points. Again, sticking with a two-dimensional example, this is in rough steps how DBSCAN works:</p>
<ol>
<li class="mce-root">Pick a point that has not been visited before.</li>
<li class="mce-root">Draw a circle with the point as the center. The radius of the circle is epsilon.</li>
<li class="mce-root">Count how many other points fall into the circle. If there are more than a specified threshold, we mark all the points as being part of the same cluster.</li>
<li class="mce-root">Recursively do the same for each point in this cluster. Doing so expands the cluster.</li>
<li class="mce-root">Repeat these steps.</li>
</ol>
<p class="mce-root">I highly encourage you to do this on dotted paper and try to draw this out yourself. Start by plotting random points, and use pencils to draw circles on paper. This will give you an intuition of how DBSCAN works. The picture shows my working that enhanced my intuition about how DBSCAN works. I found this intuition to be very useful.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data acquisition</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the earlier exercises, I asked you to look at the dots and figure out the distance. This gives a hint as to how we need to think of our data. We need to think of our data as coordinates in some imaginary coordinate space. Now, our data won't be just two-dimensional, because it's textual. Instead, it'll be multidimensional. This gives us hints as to how our data will look—slices of numbers representing a coordinate in some arbitrarily large N-dimensional space.</p>
<p class="mce-root">But, first, we'll need to get the data.</p>
<p class="mce-root">To acquire the tweets from the feed, we'll be using Aditya Mukherjee's excellent Anaconda library. To install it, simply run <kbd>go get -u github.com/ChimeraCoder/Anaconda</kbd>.</p>
<p class="mce-root">Of course, one can't just grab data from Twitter willy-nilly. We will need to acquire data via the Twitter API. The documentation of Twitter's API is the best source to get started: <a href="https://developer.twitter.com/en/docs/basics/getting-started" target="_blank">https://developer.twitter.com/en/docs/basics/getting-started</a>.</p>
<p class="mce-root">You will need to first apply for a Twitter developer account (if you don't already have it): <a href="https://developer.twitter.com/en/apply/user" target="_blank">https://developer.twitter.com/en/apply/user</a>. The process is rather lengthy and requires human approval for a developer account. Despite this, you don't need developer access to develop this project. I thought I had access to Twitter's API when I started, but it turns out I didn't. The good news is, the Twitter API documentation page does provide enough examples to get started with developing the necessary data structures.</p>
<p class="mce-root">The specific end point that we're interested in is this: <a href="https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline.html" target="_blank">https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline.html</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Exploratory data analysis</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's look at the <kbd>JSON</kbd> acquired from the Twitter API endpoint. A single tweet looks something like this (from the Twitter API documentation example):</p>
<pre class="mce-root"> {<br/> "coordinates": null,<br/> "truncated": false,<br/> "created_at": "Tue Aug 28 19:59:34 +0000 2012",<br/> "favorited": false,<br/> "id_str": "240539141056638977",<br/> "in_reply_to_user_id_str": null,<br/> "entities": {<br/> "urls": [<br/>],<br/> "hashtags": <br/>],<br/> "user_mentions": [<br/>]<br/> },<br/> "text": "You'd be right more often if you thought you were wrong.",<br/> "contributors": null,<br/> "id": 240539141056638977,<br/> "retweet_count": 1,<br/> "in_reply_to_status_id_str": null,<br/> "geo": null,<br/> "retweeted": false,<br/> "in_reply_to_user_id": null,<br/> "place": null,<br/> "source": "web",<br/> "user": {<br/> "name": "Taylor Singletary",<br/> "profile_sidebar_fill_color": "FBFBFB",<br/> "profile_background_tile": true,<br/> "profile_sidebar_border_color": "000000",<br/> "profile_image_url": "http://a0.twimg.com/profile_images/2546730059/f6a8zq58mg1hn0ha8vie_normal.jpeg",<br/> "created_at": "Wed Mar 07 22:23:19 +0000 2007",<br/> "location": "San Francisco, CA",<br/> "follow_request_sent": false,<br/> "id_str": "819797",<br/> "is_translator": false,<br/> "profile_link_color": "c71818",<br/> "entities": {<br/> "url": {<br/> "urls": [<br/> {<br/> "expanded_url": "http://www.rebelmouse.com/episod/",<br/> "url": "http://t.co/Lxw7upbN",<br/> "indices": [<br/> 0,<br/> 20<br/> ],<br/> "display_url": "rebelmouse.com/episod/"<br/> }<br/> ]<br/> },<br/> "description": {<br/> "urls": [<br/>]<br/> }<br/> },<br/> "default_profile": false,<br/> "url": "http://t.co/Lxw7upbN",<br/> "contributors_enabled": false,<br/> "favourites_count": 15990,<br/> "utc_offset": -28800,<br/> "profile_image_url_https": "https://si0.twimg.com/profile_images/2546730059/f6a8zq58mg1hn0ha8vie_normal.jpeg",<br/> "id": 819797,<br/> "listed_count": 340,<br/> "profile_use_background_image": true,<br/> "profile_text_color": "D20909",<br/> "followers_count": 7126,<br/> "lang": "en",<br/> "protected": false,<br/> "geo_enabled": true,<br/> "notifications": false,<br/> "description": "Reality Technician, Twitter API team, synthesizer enthusiast; a most excellent adventure in timelines. I know it's hard to believe in something you can't see.",<br/> "profile_background_color": "000000",<br/> "verified": false,<br/> "time_zone": "Pacific Time (US &amp; Canada)",<br/> "profile_background_image_url_https": "https://si0.twimg.com/profile_background_images/643655842/hzfv12wini4q60zzrthg.png",<br/> "statuses_count": 18076,<br/> "profile_background_image_url": "http://a0.twimg.com/profile_background_images/643655842/hzfv12wini4q60zzrthg.png",<br/> "default_profile_image": false,<br/> "friends_count": 5444,<br/> "following": true,<br/> "show_all_inline_media": true,<br/> "screen_name": "episod"<br/> },<br/> "in_reply_to_screen_name": null,<br/> "in_reply_to_status_id": null<br/> }</pre>
<p class="mce-root">We will be representing each individual tweet in a data structure that looks like this:</p>
<pre class="mce-root"> type processedTweet struct {<br/> anaconda.Tweet<br/>// post processed stuff<br/> ids []int // to implement Document<br/> textVec []float64<br/> normTextVec []float64<br/> location []float64<br/> isRT bool<br/> }</pre>
<p class="mce-root">Note that we embed <kbd>anaconda.Tweet</kbd>, which is given as such in the Anaconda package:</p>
<pre class="mce-root"> type Tweet struct {<br/> Contributors []int64 `json:"contributors"`<br/> Coordinates *Coordinates `json:"coordinates"`<br/> CreatedAt string `json:"created_at"`<br/> DisplayTextRange []int `json:"display_text_range"`<br/> Entities Entities `json:"entities"`<br/> ExtendedEntities Entities `json:"extended_entities"`<br/> ExtendedTweet ExtendedTweet `json:"extended_tweet"`<br/> FavoriteCount int `json:"favorite_count"`<br/> Favorited bool `json:"favorited"`<br/> FilterLevel string `json:"filter_level"`<br/> FullText string `json:"full_text"`<br/> HasExtendedProfile bool `json:"has_extended_profile"`<br/> Id int64 `json:"id"`<br/> IdStr string `json:"id_str"`<br/> InReplyToScreenName string `json:"in_reply_to_screen_name"`<br/> InReplyToStatusID int64 `json:"in_reply_to_status_id"`<br/> InReplyToStatusIdStr string `json:"in_reply_to_status_id_str"`<br/> InReplyToUserID int64 `json:"in_reply_to_user_id"`<br/> InReplyToUserIdStr string `json:"in_reply_to_user_id_str"`<br/> IsTranslationEnabled bool `json:"is_translation_enabled"`<br/> Lang string `json:"lang"`<br/> Place Place `json:"place"`<br/> QuotedStatusID int64 `json:"quoted_status_id"`<br/> QuotedStatusIdStr string `json:"quoted_status_id_str"`<br/> QuotedStatus *Tweet `json:"quoted_status"`<br/> PossiblySensitive bool `json:"possibly_sensitive"`<br/> PossiblySensitiveAppealable bool `json:"possibly_sensitive_appealable"`<br/> RetweetCount int `json:"retweet_count"`<br/> Retweeted bool `json:"retweeted"`<br/> RetweetedStatus *Tweet `json:"retweeted_status"`<br/> Source string `json:"source"`<br/> Scopes map[string]interface{} `json:"scopes"`<br/> Text string `json:"text"`<br/> User User `json:"user"`<br/> WithheldCopyright bool `json:"withheld_copyright"`<br/> WithheldInCountries []string `json:"withheld_in_countries"`<br/> WithheldScope string `json:"withheld_scope"`<br/> }</pre>
<p class="mce-root">In the interest of building the program, we'll use the example tweets supplied by Twitter. I saved the example responses into a file called <kbd>example.json</kbd> and then a <kbd>mock</kbd> function is created to mock calling the API:</p>
<pre class="mce-root"> func mock() []*processedTweet {<br/> f, err := os.Open("example.json")<br/> dieIfErr(err)<br/> return load(f)<br/> }<br/> func load(r io.Reader) (retVal []*processedTweet) {<br/> dec := json.NewDecoder(r)<br/> dieIfErr(dec.Decode(&amp;retVal))<br/> return retVal<br/> }</pre>
<p class="mce-root">The utility function <kbd>dieIfErr</kbd> is defined as usual:</p>
<pre class="mce-root"> func dieIfErr(err error) {<br/> if err != nil {<br/> log.Fatal(err)<br/> }<br/> }</pre>
<p class="mce-root">Note that in <kbd>mock</kbd>, no API calls to Twitter were made. In the future, we will be creating a function with a similar API so we can just replace the mock version of this function with the real one, which acquires the timeline from the API.</p>
<p class="mce-root">For now, we can test that this works by the following program:</p>
<pre class="mce-root"> func main(){<br/> tweets := mock()<br/> for _, tweet := range tweets {<br/> fmt.Printf("%q\n", tweet.FullText)<br/> }<br/> }</pre>
<p class="mce-root">This is the output I got:</p>
<pre class="mce-root"> $ go run *.go<br/> "just another test"<br/> "lecturing at the \"analyzing big data with twitter\" class at @cal with @othman http://t.co/bfj7zkDJ"<br/> "You'd be right more often if you thought you were wrong."</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data massage</h1>
                </header>
            
            <article>
                
<p class="mce-root">When we tested that the data structure made sense, we printed the <kbd>FullText</kbd> field. We wish to cluster based on the content of the tweet. What matters to us is that content. This can be found in the <kbd>FullText</kbd> field of the struct. Later on in the chapter, we will see how we may use the metadata of the tweets, such as location, to help cluster the tweets better.</p>
<p class="mce-root">As mentioned in the previous sections, each individual tweet needs to be represented as a coordinate in some higher-dimensional space. Thus, our goal is to take all the tweets in a timeline and preprocess them in such a way that we can get this output table:</p>
<pre class="mce-root">| Tweet ID | twitter | test | right | wrong |<br/> |:--------:|:------:|:----:|:----:|:---:|<br/> | 1 | 0 | 1 | 0 | 0 |<br/> | 2 | 1 | 0 | 0 | 0 |<br/> | 3 | 0 | 0 | 1 | 1 |</pre>
<p class="mce-root">Each row in the table represents a tweet, indexed by the tweet ID. The columns that follow are words that exist in the tweet, indexed by its header. So, in the first row, <kbd>test</kbd> appears in the tweet, while <kbd>twitter</kbd>, <kbd>right</kbd>, and <kbd>wrong</kbd> do not. The slice of numbers <kbd>[0 1 0 0]</kbd> in the first row is the input we require for the clustering algorithms.</p>
<p class="mce-root">Of course, binary numbers indicating the presence of a word in a tweet isn't the best. It'd be more interesting if the relative importance of the word is used instead. Again, we turn to the familiar TF-IDF, first introduced in <a href="12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml">Chapter 2</a>, <span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label"><em>Linear Regression – House Price Prediction</em>, </span></span>for this. More advanced techniques such as using word embeddings exist. But you'd be surprised how well something as simple as TF-IDF can perform.</p>
<p class="mce-root">By now, the process should be familiar—we want to represent the text as a slice of numbers, not as a slice of bytes. In order to do so, we would have to require some sort of dictionary to convert the words in the text into IDs. From there, we can built the table.</p>
<p class="mce-root">Again, like in <a href="https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&amp;action=edit#post_46">Chapter 2</a>,<span> </span><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label"><em>Linear Regression – House Price</em> Prediction,</span></span> we shall approach this with a simple tokenization strategy. More advanced tokenizers are nice, but not necessary for our purpose. Instead, we'll rely on good old <kbd>strings.Field</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The processor </h1>
                </header>
            
            <article>
                
<p class="mce-root">Having laid out our requirements, we can combine them into a single data structure that contains the things we need. Here's how the processor data structure looks:</p>
<pre class="mce-root"> type processor struct {<br/> tfidf *tfidf.TFIDF<br/> corpus *corpus.Corpus<br/> locations map[string]int<br/> t transform.Transformer<br/> locCount int<br/> }</pre>
<p class="mce-root">For now, ignore the <kbd>locations</kbd> field. We shall look into how metadata might be useful in clustering.</p>
<p class="mce-root">To create a new <kbd>processor</kbd>, the following function is defined:</p>
<pre class="mce-root"> func newProcessor() *processor {<br/> c, err := corpus.Construct(corpus.WithWords([]string{mention, hashtag, retweet, url}))<br/> dieIfErr(err)<br/> return &amp;processor{<br/> tfidf: tfidf.New(),<br/> corpus: c,<br/> locations: make(map[string]int),<br/> }<br/> }</pre>
<p class="mce-root">Here, we see some interesting decisions. The corpus is constructed with a number of special strings—<kbd>mention</kbd>, <kbd>hashtag</kbd>, <kbd>retweet</kbd>, and <kbd>url.</kbd> These are defined as follows:</p>
<pre class="mce-root"> const (<br/> mention = "&lt;@MENTION&gt;"<br/> hashtag = "&lt;#HASHTAG&gt;"<br/> retweet = "&lt;RETWEET&gt;"<br/> url = "&lt;URL&gt;"<br/> )</pre>
<p class="mce-root">Some of the designs of this is for historical reasons. A long time ago, before Twitter supported retweets as an action, people manually retweeted tweets by prepending <kbd>RT</kbd> on to tweets. If we are to analyze data far into the past (which we won't for this chapter), then we'd have to be aware of the historical designs of Twitter as well. So, you must design for that.</p>
<p class="mce-root">But having constructed a corpus with special keywords implies something. It implies that when converting the text of a tweet into a bunch of IDs and numbers, mentions, hashtags, retweets, and URLs are all treated as the same. It implies we don't really want to care what the URL is, or who is mentioned. However, when it comes to hashtags, that's the interesting case.</p>
<p class="mce-root">A hashtag is typically used to denote the topic of the tweet. Think <kbd>#MeToo</kbd> or <kbd>#TimesUp</kbd>. A hashtag contains information. Compressing all hashtags into one single ID may not be useful. This is a point to note when we experiment later on.</p>
<p class="mce-root">Having said all that, here's how a list of <kbd>*processedTweet</kbd> is processed. We will be revisiting and revising the function as the chapter goes on:</p>
<pre class="mce-root"> func (p *processor) process(a []*processedTweet) {<br/> for _, tt := range a {<br/> for _, word := range strings.Fields(tt.FullText) {<br/> wordID, ok := p.single(word)<br/> if ok {<br/> tt.ids = append(tt.ids, wordID)<br/> }<br/>if isRT(word) {<br/> tt.isRT = true<br/> }<br/> }<br/> p.tfidf.Add(tt)<br/> }<br/>p.tfidf.CalculateIDF()<br/> // calculate scores<br/> for _, tt := range a {<br/> tt.textVec = p.tfidf.Score(tt)<br/> }<br/>// normalize text vector<br/> size := p.corpus.Size()<br/> for _, tt := range a {<br/> tt.normTextVec = make([]float64, size)<br/> for i := range tt.ids {<br/> tt.normTextVec[tt.ids[i]] = tt.textVec[i]<br/> }<br/> }<br/> }</pre>
<p class="mce-root">Let's go through this function line by line.</p>
<p class="mce-root">We start by ranging over all the <kbd>*processedTweets</kbd>. <kbd>a</kbd> is <kbd>[]*processedTweet</kbd> for a good reason—we want to modify the structure as we go along. If <kbd>a</kbd> were <kbd>[]processedTweet</kbd>, then we would have to either allocate a lot more, or have complicated modification schemes.</p>
<p class="mce-root">Each tweet is comprised of its <kbd>FullText</kbd>. We want to extract each word from the text, and then give each word its own ID. To do that, this is the loop:</p>
<pre class="mce-root"> for _, word := range strings.Fields(tt.FullText) {<br/> wordID, ok := p.single(word)<br/> if ok {<br/> tt.ids = append(tt.ids, wordID)<br/> }<br/> }</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Preprocessing a single word </h1>
                </header>
            
            <article>
                
<p class="mce-root">The <kbd>p.single</kbd> processes a single word. It returns the ID of the word, and whether to add it to the list of words that make up the tweet. It is defined as follows:</p>
<pre class="mce-root"> func (p *processor) single(a string) (wordID int, ok bool) {<br/> word := strings.ToLower(a)<br/> if _, ok = stopwords[word]; ok {<br/> return -1, false<br/> }<br/> if strings.HasPrefix(word, "#") {<br/> return p.corpus.Add(hashtag), true<br/> }<br/> if strings.HasPrefix(word, "@") {<br/> return p.corpus.Add(mention), true<br/> }<br/> if strings.HasPrefix(word, "http://") {<br/> return p.corpus.Add(url), true<br/> }<br/> if isRT(word) {<br/> return p.corpus.Add(retweet), false<br/> }<br/>return p.corpus.Add(word), true<br/> }</pre>
<p class="mce-root">We start by making the word lowercase. This makes words such as <kbd>café</kbd> and <kbd>Café</kbd> equivalent.</p>
<p class="mce-root">Speaking of <kbd>café</kbd>, what would happen if there are two tweets mentioning a <kbd>café</kbd>, but one user writes <kbd>café</kbd> and the other writes cafe? Assume, of course, they both refer to the same thing. We'd need some form of normalization to tell us that they're the same.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Normalizing a string</h1>
                </header>
            
            <article>
                
<p class="mce-root">First, the word is to be normalized into <kbd>NFKC</kbd> form. In <a href="https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&amp;action=edit#post_46">Chapter 2</a>,<span> </span><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label"><em>Linear Regression–House Price Prediction</em></span></span>, this was introduced, but I then mentioned that LingSpam basically provides normalized datasets. In real-world data, which Twitter is, data is often dirty. Hence, we need to be able to compare them on an apples-to-apples basis.</p>
<p class="mce-root">To show this, let's write a side program:</p>
<pre class="mce-root"> package main<br/>import (<br/> "fmt"<br/> "unicode"<br/>"golang.org/x/text/transform"<br/> "golang.org/x/text/unicode/norm"<br/> )<br/>func isMn(r rune) bool { return unicode.Is(unicode.Mn, r) }<br/>func main() {<br/> str1 := "cafe"<br/> str2 := "café"<br/> str3 := "cafe\u0301"<br/> fmt.Println(str1 == str2)<br/> fmt.Println(str2 == str3) <br/>t := transform.Chain(norm.NFD, transform.RemoveFunc(isMn), norm.NFKC)<br/> str1a, _, _ := transform.String(t, str1)<br/> str2a, _, _ := transform.String(t, str2)<br/> str3a, _, _ := transform.String(t, str3)<br/>fmt.Println(str1a == str2a)<br/> fmt.Println(str2a == str3a)<br/> }</pre>
<p class="mce-root">The first thing to note is that there are at least three ways of writing the word <kbd>café</kbd>, which for the purposes of this demonstration means coffee shop. It's clear from the first two comparisons that the words are not the same. But since they mean the same thing, a comparison should return <kbd>true</kbd>.</p>
<p class="mce-root">To do that, we will need to transform all the text to one form, and then comapare it. To do so, we would need to define a transformer:</p>
<pre class="mce-root">t := transform.Chain(norm.NFD, transform.RemoveFunc(isMn), norm.NFKC)</pre>
<p class="mce-root">This transformer is a chain of text transformers, applied one after another.</p>
<p class="mce-root">First, we convert all the text to its decomposing form, NFD. This would turn <kbd>café</kbd> into <kbd>cafe\u0301</kbd>.</p>
<p class="mce-root">Then, we remove any non-spacing mark. This turns <kbd>cafe\u0301</kbd> into <kbd>cafe</kbd>. This removal function is done with the <kbd>isMn</kbd> function, defined as follows:</p>
<pre class="mce-root">func isMn(r rune) bool { return unicode.Is(unicode.Mn, r) }</pre>
<p class="mce-root">Lastly, convert everything to NKFC form for maximum compatibility and space saving. All three strings are now equal.</p>
<p class="mce-root">Note that this type of comparison is done with one single assumption that belies it all: there is one language that we're doing our comparisons in—English. <strong>Café</strong> in French means <strong>coffee</strong> as well as <strong>coffee shop</strong>. This kind of normalization, where we remove diacritical marks, works so long as removing a diacritic mark does not change the meaning of the word. We'd have to be more careful around normalization when dealing with multiple languages. But for this project, this is a good enough assumption.</p>
<p class="mce-root">With this new knowledge, we will need to update our <kbd>processor</kbd> type:</p>
<pre class="mce-root"> type processor struct {<br/> tfidf *tfidf.TFIDF<br/> corpus *corpus.Corpus<br/> transformer transformer.Transformer<br/> locations map[string]int<br/> locCount int<br/> }<br/>func newProcessor() *processor {<br/> c, err := corpus.Construct(corpus.WithWords([]string{mention, hashtag, retweet, url}))<br/> dieIfErr(err)<br/>t := transform.Chain(norm.NFD, transform.RemoveFunc(isMn), norm.NFKC)<br/> return &amp;processor{<br/> tfidf: tfidf.New(),<br/> corpus: c,<br/> transformer: t,<br/> locations: make(map[string]int),<br/> }<br/> }</pre>
<p class="mce-root">The first line of our <kbd>p.single</kbd> function would have to change too, from this:</p>
<pre class="mce-root"> func (p *processor) single(a string) (wordID int, ok bool) {<br/> word := strings.ToLower(a)</pre>
<p class="mce-root">It will change to this:</p>
<pre class="mce-root"> func (p *processor) single(a string) (wordID int, ok bool) {<br/> word, _, err := transform.String(p.transformer, a)<br/> dieIfErr(err)<br/> word = strings.ToLower(word)</pre>
<p class="mce-root">If you're feeling extra hard-working, try making <kbd>strings.ToLower</kbd> a <kbd>transform.Transformer</kbd>. It is harder than you might expect, but not as hard as it appears.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Preprocessing stopwords</h1>
                </header>
            
            <article>
                
<p class="mce-root">Enough about normalization. We now turn our focus to <kbd>stopwords</kbd>.</p>
<p class="mce-root">Recall from <a href="https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&amp;action=edit#post_46">Chapter 2</a>,<span> </span><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label"><em>Linear Regression–House Price Prediction</em>,</span></span> that <kbd>stopwords</kbd> are words such as <strong>the</strong>, <strong>there</strong>, <strong>from</strong>, and so on. They're connective words, useful in understanding the specific context of sentences, but for a naive statistical analysis, they often add nothing more than noise. So, we have to remove them.</p>
<p class="mce-root">A check for <kbd>stopwords</kbd> is simple. If a word matches a <kbd>stopwords</kbd>, we'll return <kbd>false</kbd> for whether to add the word ID into the sentence:</p>
<pre class="mce-root">if _, ok = stopwords[word]; ok {<br/> return -1, false<br/> }</pre>
<p class="mce-root">Where does the list of <kbd>stopwords</kbd> come from? It's simple enough that I just wrote this in <kbd>stopwords.go</kbd>:</p>
<pre class="mce-root">const sw = `a about above across after afterwards again against all almost alone along already also although always am among amongst amoungst amount an and another any anyhow anyone anything anyway anywhere are around as at back be became because become becomes becoming been before beforehand behind being below beside besides between beyond bill both bottom but by call can cannot can't cant co co. computer con could couldnt couldn't cry de describe detail did didn didn't didnt do does doesn doesn't doesnt doing don done down due during each eg e.g eight either eleven else elsewhere empty enough etc even ever every everyone everything everywhere except few fifteen fify fill find fire first five for former formerly forty found four from front full further get give go had has hasnt hasn't hasn have he hence her here hereafter hereby herein hereupon hers herself him himself his how however hundred i ie i.e. if in inc indeed interest into is it its itself just keep kg km last latter latterly least less ltd made make many may me meanwhile might mill mine more moreover most mostly move much must my myself name namely neither never nevertheless next nine no nobody none noone nor not nothing now nowhere of off often on once one only onto or other others otherwise our ours ourselves out over own part per perhaps please put quite rather re really regarding same say see seem seemed seeming seems serious several she should show side since sincere six sixty so some somehow someone something sometime sometimes somewhere still such system take ten than that the their them themselves then thence there thereafter thereby therefore therein thereupon these they thick thin third this those though three through throughout thru thus to together too top toward towards twelve twenty two un under unless until up upon us used using various very via was we well were what whatever when whence whenever where whereafter whereas whereby wherein whereupon wherever whether which while whither who whoever whole whom whose why will with within without would yet you your yours yourself yourselves`</pre>
<pre class="mce-root">var stopwords = make(map[string]struct{})<br/>func init() {<br/> for _, s := range strings.Split(sw, " ") {<br/> stopwords[s] = struct{}{}<br/> }<br/> }</pre>
<p class="mce-root">And that's it! A tweet with content that looks like this—<em>an apple a day keeps the doctor away</em> would have the IDs for <em>apple</em>, <em>day</em>, <em>doctor</em>, and <em>away</em>.</p>
<p class="mce-root">The list of stopwords is adapted from the list that is used in the <kbd>lingo</kbd> package. The list of stopwords in the <kbd>lingo</kbd> package is meant to be used on lemmatized words. Because we're not lemmatizing, some words were manually added. It's not perfect but works well enough for our purpose.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Preprocessing Twitter entities </h1>
                </header>
            
            <article>
                
<p class="mce-root">After we've removed the stopwords, it's time to process the special Twitter entities:</p>
<pre class="mce-root"> if strings.HasPrefix(word, "#") {<br/> return p.corpus.Add(hashtag), true<br/> }<br/> if strings.HasPrefix(word, "@") {<br/> return p.corpus.Add(mention), true<br/> }<br/> if strings.HasPrefix(word, "http://") {<br/> return p.corpus.Add(url), true<br/> }</pre>
<p class="mce-root">These are straightforwards enough.</p>
<p class="mce-root">If a word begins with <kbd>"#"</kbd>, then it's a hashtag. We might want to come back to this later, so it's good to keep this in mind.</p>
<p class="mce-root">Any word that begins with a <kbd>"@"</kbd> is a mention. This is a little tricky. Sometimes, people tweet things such as <kbd>I am @PlaceName</kbd>, indicating a location, as opposed to mentioning a user (indeed, one may find <kbd>@PlaceName</kbd> does not exist). Or, alternatively, people may tweet something such as <kbd>I am @ PlaceName</kbd>. In this case, the solo <kbd>"@"</kbd> would still be treated as a mention. I found that for the former (<kbd>@PlaceName</kbd>), it doesn't really matter if the word is treated as a mention. Twitter's API does indeed return a list of mentions that you may check against. But for my personal timeline, this was extra work that isn't necessary. So, think of this as an extra credit project—check against the list of mentions from the API.</p>
<p class="mce-root">Of course, we shan't be as lazy as to leave everything to extra credit; simple checks can be made—if <kbd>@</kbd> is solo, then we shouldn't treat it as a mention. It should be treated as <kbd>at</kbd>.</p>
<p class="mce-root">Now, we check for URLs. The line <kbd>if strings.HasPrefix(word, "http://")</kbd>checks for a <kbd>http://</kbd> prefix. This isn't good. This doesn't account for URLs with a <kbd>https</kbd> scheme.</p>
<p class="mce-root">Now we know how to modify this section of the code. It looks like this:</p>
<pre class="mce-root"> switch {<br/> case strings.HasPrefix(word, "#"):<br/> return p.corpus.Add(hashtag), true<br/> case strings.HasPrefix(word, "@"):<br/> if len(word) == 0 {<br/> return p.corpus.Add("at"), true<br/> }<br/> return p.corpus.Add(mention), true<br/> case strings.HasPrefix(word, "http"):<br/> return p.corpus.Add(url), true<br/> }</pre>
<p class="mce-root">Lastly, a final line of code is added to handle historical tweets before retweets were supported by Twitter:</p>
<pre class="mce-root"> if word == "rt" {<br/> return p.corpus.Add(retweet), false<br/> }</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Processing a single tweet </h1>
                </header>
            
            <article>
                
<p class="mce-root">Consider the following snippet of code:</p>
<pre class="mce-root"> for _, tt := range a {<br/> for _, word := range strings.Fields(tt.FullText) {<br/> wordID, ok := p.single(word)<br/> if ok {<br/> tt.ids = append(tt.ids, wordID)<br/> }<br/>if word == "rt" {<br/> tt.isRT = true<br/> }<br/> }<br/> p.tfidf.Add(tt)<br/> }</pre>
<p class="mce-root">What it says is after we've preprocessed every single word, we simply add that word to the TFIDF.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Clustering </h1>
                </header>
            
            <article>
                
<p class="mce-root">The purpose of this project is to clean up the amount of tweets that I have to read. If there is a reading budget of 100 tweets, I don't want to be reading 50 tweets on the same topic; they may well represent different viewpoints, but in general for skimming purposes, are not relevant to my interests. Clustering provides a good solution to this problem.</p>
<p class="mce-root">First, if the tweets are clustered, the 50 tweets on the same topic will be grouped in the same cluster. This allows me to dig in deeper if I wish. Otherwise, I can just skip those tweets and move on.</p>
<p class="mce-root">In this project, we wish to use K-means. To do so, we'll use Marcin Praski's <kbd>clusters</kbd> library. To install it, simply run <kbd>go get -u github.com/mpraski/clusters</kbd>. It's a good library, and it comes built in with multiple clustering algorithms. I introduced K-means before, but we're also going to be using DBSCAN. </p>
<p class="mce-root">Last, we're going to be using the DMMClust algorithm to compare against. The DMMClust algorithm is in a different library. To install it, simply run <kbd>go get -u github.com/go-nlp/dmmclust</kbd>. The purpose of DMMClust is to cluster small texts using an innovative process.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Clustering with K-means </h1>
                </header>
            
            <article>
                
<p class="mce-root">As a recap, here's what we did so far—we processed each tweet in a list of tweets from the home timeline to be a slice of <kbd>float64</kbd>. These represent the coordinates in the higher-dimensional space. Now, all we need to do is the following:</p>
<ol>
<li class="mce-root">Create a clusterer.</li>
<li class="mce-root">Create a <kbd>[][]float64</kbd> representing all the tweets from the timeline.</li>
<li class="mce-root">Train the clusterer.</li>
<li class="mce-root">Predict which tweet belongs in which cluster.</li>
</ol>
<p class="mce-root">It can be done as follows:</p>
<pre class="mce-root"> func main() {<br/> tweets := mock()<br/> p := newProcessor()<br/> p.process(tweets)<br/>// create a clusterer<br/> c, err := clusters.KMeans(10000, 25, clusters.EuclideanDistance)<br/> dieIfErr(err)<br/>data := asMatrix(tweets)<br/> dieIfErr(c.Learn(data))clusters := c.Guesses()<br/> for i, clust := range clusters{<br/> fmt.Printf("%d: %q\n", clust, tweets[i].FullText)<br/> }<br/> }</pre>
<p class="mce-root">Surprised? Let's break it down.</p>
<p class="mce-root">The first few lines are for processing <kbd>tweets</kbd>:</p>
<pre class="mce-root"> tweets := mock()<br/> p := newProcessor()<br/> p.process(tweets)</pre>
<p class="mce-root">We then create a clusterer:</p>
<pre class="mce-root"> // create a clusterer<br/> c, err := clusters.KMeans(10000, 25, clusters.EuclideanDistance)<br/> dieIfErr(err)</pre>
<p class="mce-root">Here, we say we want a K-means clusterer. We'll train on the data 10,000 times, and we want it to find 25 clusters, using the <kbd>EuclideanDistance</kbd> method to calculate distances. The Euclidean distance is your bog standard distance calculation, the same one you'd use to calculate the distance between two points in the exercises in the K-means section before. There are other methods of calculating distances, which are more suited for textual data. Later in this chapter, I'll show you how to create a distance function, the Jacard distance, which is much better than Euclidean distance when used on text.</p>
<p class="mce-root">After we've created a clusterer, we need to convert our list of <kbd>tweets</kbd> into a matrix. We then train the clusterer:</p>
<pre class="mce-root"> data := asMatrix(tweets)<br/> dieIfErr(c.Learn(data))</pre>
<p class="mce-root">And, finally, we display the <kbd>clusters</kbd>:</p>
<pre class="mce-root"> clusters := c.Guesses()<br/> for i, clust := range clusters{<br/> fmt.Printf("%d: %q\n", clust, tweets[i].FullText)<br/> }</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Clustering with DBSCAN </h1>
                </header>
            
            <article>
                
<p class="mce-root">Clustering with DBSCAN using Marcin's package is equally simple. In fact, you would just need to change one single line of code from this:</p>
<pre class="mce-root">c, err := clusters.KMeans(10000, 25, clusters.EuclideanDistance)</pre>
<p class="mce-root">You would change it to this:</p>
<pre class="mce-root">c, err := clusters.DBSCAN(eps, minPts, clusters.EuclideanDistance)</pre>
<p class="mce-root">Now, of course, the question is what values should <kbd>eps</kbd> and <kbd>minPts</kbd> be?</p>
<p class="mce-root"><kbd>eps</kbd> represents the minimum distance required for two points to be considered a neighbor. <kbd>minPts</kbd> is the minimum number of points to form a dense cluster. Let's address <kbd>eps</kbd> first.</p>
<p class="mce-root">How do we know what the best distance is? A good way to figure this out is usually to visualize the data. In fact, this is what the original inventors of the DBSCAN algorithm suggests. But what exactly are we to visualize?</p>
<p class="mce-root">We want to visualize the distance between the tweets. Given a dataset, we can compute a distance matrix that looks something like this:</p>
<pre class="mce-root">| | A | B | C | ... |<br/> |--|--|--|--|--|--|<br/> | A | | | | |<br/> | B | | | | |<br/> | C | | | | |<br/> | ... | | | | |</pre>
<p class="mce-root">To do so, we write the following function:</p>
<pre class="mce-root"> func knn(a [][]float64, k int, distance func(a, b []float64) float64) ([][]float64, []float64) {<br/> var distances [][]float64<br/> for _, row := range a {<br/> var dists []float64<br/> for _, row2 := range a {<br/> dist := distance(row, row2)<br/> dists = append(dists, dist)<br/> }<br/> sort.Sort(sort.Float64Slice(dists))<br/> topK := dists[:k]<br/> distances = append(distances, topK)<br/> }<br/> var lastCol []float64<br/> for _, d := range distances {<br/> l := d[len(d)-1]<br/> lastCol = append(lastCol, l)<br/> }<br/> sort.Sort(sort.Float64Slice(lastCol))<br/> return distances, lastCol<br/> }</pre>
<p class="mce-root">This function takes a matrix of floats; each row represents a tweet, and finds the top k-nearest neighbors. Let's walk through the algorithm. As we walk though the algorithm, bear in mind that each row is a tweet; you can think of each row therefore as a very complicated coordinate.</p>
<p class="mce-root">The first thing we want to do is to find the distance between a tweet and another tweet, hence the following block:</p>
<pre class="mce-root"> var distances [][]float64<br/> for _, row := range a {<br/> var dists []float64<br/> for _, row2 := range a {<br/> dist := distance(row, row2)<br/> dists = append(dists, dist)<br/> }</pre>
<p class="mce-root">Of particular note are the two expressions <kbd>for _, row := range a</kbd> and <kbd>for _, row2 := range a</kbd>. In a normal KNN function, you'd have two matrices, <kbd>a</kbd> and <kbd>b</kbd>, and you'd find the distance between a tweet in <kbd>a</kbd> and a tweet in <kbd>b</kbd>. But for the purposes of drawing this chart, we are going to compare tweets within the same dataset.</p>
<p class="mce-root">Once we acquired all the distances, we want to find the closest neighbors, so we sort the list and then put them in the distance matrix:</p>
<pre class="mce-root"> sort.Sort(sort.Float64Slice(dists))<br/> topK := dists[:k]<br/> distances = append(distances, topK)</pre>
<p class="mce-root">This, in a very quick way, is how to do K-nearest neighbors. Of course, it's not the most efficient. The algorithm I've shown here is <em>O(n^2)</em>. There are better ways of doing things, but for the purpose of this project, this suffices.</p>
<p class="mce-root">After that, we grab the last column of the matrix and sort the last column. This is what we wish to plot. The plotting code is not unlike that seen in previous chapters. I shall provide it here with no further elaboration on how to use it:</p>
<pre class="mce-root"> func plotKNNDist(a []float64) plotter.XYs {<br/> points := make(plotter.XYs, len(a))<br/> for i, val := range a {<br/> points[i].X = float64(i)<br/> points[i].Y = val<br/> }<br/> return points<br/> }</pre>
<p class="mce-root">When I plot the real Twitter data to figure out the ideal <kbd>eps</kbd>, I get the following output:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-468 image-border" src="Images/ee722fa9-fd28-45b5-9f84-2f49e9dc4954.png" style="width:80.67em;height:81.08em;" width="968" height="973"/></p>
<p class="mce-root">What you want to find is an <kbd>elbow</kbd> or <kbd>knee</kbd> in the picture. Unfortunately, as you can tell, there are many of them. This is going to make clustering with the DBSCAN algorithm difficult. What this means is that the data is rather noisy.</p>
<p class="mce-root">One of the things that is of particular importance is the distance function used. I will go into this a little further in following sections on tweaking the program.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Clustering with DMMClust </h1>
                </header>
            
            <article>
                
<p class="mce-root">Having been somewhat discouraged by the distance plot of my Twitter home feed, I looked into another way of clustering tweets. To that end, I used the <kbd>dmmclust</kbd> library (of which I am the primary author). The purpose of the DMMClust algorithm is that it is able to handle small texts quite well. Indeed, it was written to handle the problem of having small text.</p>
<p class="mce-root">What exactly is a small text? Most text clustering research out there is done on texts with large amounts of words. Twitter, up to very recently, only supported 140 characters. As you may imagine, the amount of information that 140 characters to be transmitted as human language is not very much.</p>
<p class="mce-root">The DMMClust algorithm works very much like students joining high school social clubs. Imagine the tweets as a bunch of students. Each student randomly joins a social club. Within each social club, they may like their fellow members of the club, or they may not. If they do not like the people in the group, they are allowed to change social clubs. This happens until all the clubs have people who like each other the most, or until the amount of iterations runs out.</p>
<p class="mce-root">This, in a nutshell, is how the DMMClust algorithm works.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Real data</h1>
                </header>
            
            <article>
                
<p class="mce-root">Up to this point, we've been working on an example <kbd>JSON</kbd> that the Twitter documentation provides. I assume by now you have your Twitter API access. So, let's get real Twitter data!</p>
<p>To get your API keys from the developer portal, click on the <span class="packt_screen">Get Started</span> link. You will come to a page such as this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-469 image-border" src="Images/ba823fed-8ca0-4c68-94b9-4dceb42f62da.png" style="width:162.50em;height:111.83em;" width="1950" height="1342"/></p>
<p>Select <span class="packt_screen">Create an app</span>. You will be brought to a page that looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-470 image-border" src="Images/5bfe0179-c124-46df-b8f7-421c22ec9fea.png" style="width:132.17em;height:29.08em;" width="1586" height="349"/></p>
<p class="CDPAlignLeft CDPAlign"/>
<p class="CDPAlignLeft CDPAlign">I had previously created a Twitter app a long time ago (it had very similar features to the one we're creating in this project); hence, I have an app there already. Click on the blue <span class="packt_screen">Create an app</span> button at the top right. You will be brought to the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-471 image-border" src="Images/241b3ba0-075d-4278-91d6-9cc220c5d281.png" style="width:140.00em;height:135.75em;" width="1680" height="1629"/></p>
<p><span>Fill in the form then click <span class="packt_screen">submit</span>. It might take a few days before you receive an email saying the app has been approved for development. Be sure to be truthful in the description. Lastly, you should then be able to click into your app, and get the following page, which shows your API key and secret:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-472 image-border" src="Images/02da6d9d-162e-4652-a314-42b97726412e.png" style="width:143.50em;height:84.75em;" width="1722" height="1017"/></p>
<p>Click <span class="packt_screen">Create</span> to create your access token and access token secret. You'll be needing them.</p>
<p class="mce-root">Now that we have our API access key, this is how you'd access Twitter using the Anaconda package:</p>
<pre class="mce-root"> const (<br/> ACCESSTOKEN = "_____"<br/> ACCESSTOKENSECRET = "______"<br/> CONSUMERKEY = "_____"<br/> CONSUMERSECRET = "_______"<br/> )<br/>func main() {<br/> twitter := anaconda.NewTwitterApiWithCredentials(ACCESSTOKEN, ACCESSTOKENSECRET, CONSUMERKEY, CONSUMERSECRET)<br/> raw, err := twitter.GetHomeTimeline(nil)<br/>f, err := os.OpenFile("dev.json", os.O_TRUNC|os.O_WRONLY|os.O_CREATE, 0644)<br/> dieIfErr(err)<br/> enc := json.NewEncoder(f)<br/> enc.Encode(raw)<br/> f.Close()<br/> }</pre>
<p class="mce-root">At first glance, this snippet of code is a little weird. Let's go through the code line by line. The first six lines deal with the access tokens and keys. Obviously, they should not be hardcoded in. A good way to handle secrets like these is to put them in environment variables. I'll leave that as an exercise to the reader. We'll move on to the rest of the code:</p>
<pre class="mce-root"> twitter := anaconda.NewTwitterApiWithCredentials(ACCESSTOKEN, ACCESSTOKENSECRET, CONSUMERKEY, CONSUMERSECRET)<br/> raw, err := twitter.GetHomeTimeline(nil)</pre>
<p class="mce-root">These two lines uses the Anaconda library to get the tweets found in the Home timeline. The <kbd>nil</kbd> being passed in may be of interest. Why would one do this? The <kbd>GetHomeTimeline</kbd> method takes a map of <kbd>url.Values</kbd>. The package can be found in the standard library as <kbd>net/url</kbd>. Values is defined thus:</p>
<pre class="mce-root">type Values map[string][]string</pre>
<p class="mce-root">But what do the values represent? It turns out that you may pass some parameters to the Twitter API. The parameters and what they do are enumerated here: <a href="https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline" target="_blank">https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline</a>. I don't wish to limit anything, so passing in <kbd>nil</kbd> is acceptable.</p>
<p class="mce-root">The result is <kbd>[]anaconda.Tweet</kbd>, all neatly packaged up for us to use. The following few lines are therefore quite odd:</p>
<pre class="mce-root"> f, err := os.OpenFile("dev.json", os.O_TRUNC|os.O_WRONLY|os.O_CREATE, 0644)<br/> dieIfErr(err)<br/> enc := json.NewEncoder(f)<br/> enc.Encode(raw)<br/> f.Close()</pre>
<p class="mce-root">Why would I want to save this as a <kbd>JSON</kbd> file? The answer is simple—when using machine learning algorithms, you may need to tune the algorithm. Saving the request as a <kbd>JSON</kbd> file serves two purposes:</p>
<ul>
<li>It allows for consistency. Under active development, you would expect to tweak the algorithm a lot. If the JSON file keeps changing, how do you know if it's the tweaks that are making the improvements, and not because the JSON has changed?</li>
<li>Being a good citizen. Twitter's API is rate limited. This means you cannot request the same thing over and over again too many times. While testing and tuning machine learning algorithms, you are likely to have to repeatedly process your data over and over again. Instead of hammering the Twitter servers, you should be a good citizen and use a locally cached copy.</li>
</ul>
<p class="mce-root">We defined <kbd>load</kbd> earlier. Again, we shall see its usefulness in the context of tweaking the algorithms.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The program </h1>
                </header>
            
            <article>
                
<p class="mce-root">Once we've done that, we may move the previous <kbd>main()</kbd> into a different function, leaving ourselves with a blank canvas for <kbd>main()</kbd> again. We're now ready for the meat of the program. This is a skeleton program. You're encouraged to actually actively change the program while writing this:</p>
<pre class="mce-root">func main() {<br/> f, err := os.Open("dev.json")<br/> dieIfErr(err)<br/> tweets := load(f)<br/> p := newProcessor()<br/> tweets = p.process(tweets)<br/>expC := 20<br/> distances, last := knn(asMatrix(tweets), expC, clusters.EuclideanDistance)<br/> log.Printf("distances %v | %v", distances, last)<br/>// plot for DBSCAN elbows<br/> plt, err := plot.New()<br/> dieIfErr(err)<br/> plotutil.AddLinePoints(plt, "KNN Distance", plotKNNDist(last))<br/> plt.Save(25*vg.Centimeter, 25*vg.Centimeter, "KNNDist.png")<br/>// actually do the clustering<br/> dmmClust := dmm(tweets, expC, p.corpus.Size())<br/> kmeansClust := kmeans(tweets, expC)<br/> dbscanClust, clustCount := dbscan(tweets)<br/>// print output<br/> log.Printf("len(tweets)%d", len(tweets))<br/> var buf bytes.Buffer<br/>bc := byClusters2(dmmClust, expC)<br/> lc, tweetCount := largestCluster2(dmmClust)<br/> fmt.Fprintf(&amp;buf, "Largest Cluster %d - %d tweets\n", lc, tweetCount)<br/> for i, t := range bc {<br/> fmt.Fprintf(&amp;buf, "CLUSTER %d: %d\n", i, len(t))<br/> for _, c := range t {<br/> fmt.Fprintf(&amp;buf, "\t%v\n", tweets[c].clean2)<br/> }<br/> }<br/> fmt.Fprintf(&amp;buf, "==============\n")<br/> bc2 := byClusters(kmeansClust, expC)<br/> for i, t := range bc2 {<br/> fmt.Fprintf(&amp;buf, "CLUSTER %d: %d\n", i, len(t))<br/> for _, c := range t {<br/> fmt.Fprintf(&amp;buf, "\t%v\n", tweets[c].clean2)<br/> }<br/> }<br/> fmt.Fprintf(&amp;buf, "==============\n")<br/> bc3 := byClusters(dbscanClust, clustCount)<br/> for i, t := range bc3 {<br/> fmt.Fprintf(&amp;buf, "CLUSTER %d: %d\n", i, len(t))<br/> for _, c := range t {<br/> fmt.Fprintf(&amp;buf, "\t%v\n", tweets[c].clean2)<br/> }<br/> }<br/>log.Println(buf.String())<br/> }</pre>
<p class="mce-root">There are some utility functions that I have yet to show you. Now it's time to define them:</p>
<pre class="mce-root"> func dmm(a []*processedTweet, expC int, corpusSize int) []dmmclust.Cluster {<br/> conf := dmmclust.Config{<br/> K: expC,<br/> Vocabulary: corpusSize,<br/> Iter: 1000,<br/> Alpha: 0.0,<br/> Beta: 0.01,<br/> Score: dmmclust.Algorithm4,<br/> Sampler: dmmclust.NewGibbs(rand.New(rand.NewSource(1337))),<br/> }<br/> dmmClust, err := dmmclust.FindClusters(toDocs(a), conf)<br/> dieIfErr(err)<br/> return dmmClust<br/> }<br/>func kmeans(a []*processedTweet, expC int) []int {<br/> // create a clusterer<br/> kmeans, err := clusters.KMeans(100000, expC, clusters.EuclideanDistance)<br/> dieIfErr(err)<br/> data := asMatrix(a)<br/> dieIfErr(kmeans.Learn(data))<br/> return kmeans.Guesses()<br/> }<br/>func dbscan(a []*processedTweet) ([]int, int) {<br/> dbscan, err := clusters.DBSCAN(5, 0.965, 8, clusters.EuclideanDistance)<br/> dieIfErr(err)<br/> data := asMatrix(a)<br/> dieIfErr(dbscan.Learn(data))<br/> clust := dbscan.Guesses()<br/>counter := make(map[int]struct{})<br/> for _, c := range clust {<br/> counter[c] = struct{}{}<br/> }<br/> return clust, len(counter)<br/> }<br/>func largestCluster(clusters []int) (int, int) {<br/> cc := make(map[int]int)<br/> for _, c := range clusters {<br/> cc[c]++<br/> }<br/>var retVal, maxVal int<br/>for k, v := range cc {<br/> if v &gt; maxVal {<br/> retVal = k<br/> maxVal = v<br/> }<br/> }<br/> return retVal, cc[retVal]<br/> }<br/>func largestCluster2(clusters []dmmclust.Cluster) (int, int) {<br/> cc := make(map[int]int)<br/> for _, c := range clusters {<br/> cc[c.ID()]++<br/> }<br/>var retVal, maxVal int<br/>for k, v := range cc {<br/> if v &gt; maxVal {<br/> retVal = k<br/> maxVal = v<br/> }<br/> }<br/> return retVal, cc[retVal]<br/> }<br/>func byClusters(a []int, expectedClusters int) (retVal [][]int) {<br/> if expectedClusters == 0 {<br/> return nil<br/> }<br/> retVal = make([][]int, expectedClusters)<br/> var i, v int<br/> defer func() {<br/> if r := recover(); r != nil {<br/> log.Printf("exp %v | %v", expectedClusters, v)<br/> panic(r)<br/> }<br/> }()<br/> for i, v = range a {<br/> if v == -1 {<br/> // retVal[0] = append(retVal[0], i)<br/> continue<br/> }<br/> retVal[v-1] = append(retVal[v-1], i)<br/> }<br/> return retVal<br/> }<br/>func byClusters2(a []dmmclust.Cluster, expectedClusters int) (retVal [][]int) {<br/> retVal = make([][]int, expectedClusters)<br/> for i, v := range a {<br/> retVal[v.ID()] = append(retVal[v.ID()], i)<br/> }<br/> return retVal<br/> }</pre>
<p class="mce-root">These are some of the utility functions that may be found in <kbd>utils.go</kbd>. They mainly help with tweaking the program. Now run the program by typing <kbd>go run *.go</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tweaking the program</h1>
                </header>
            
            <article>
                
<p class="mce-root">If you have been following up to this point, you may get very poor results from all the clustering algorithms. I'd like to remind you that the stated objective of this book in general is to impart an understanding of what it's like to do data science in Go. For the most part, I have advocated a method that can be described as think hard about the problem, then write the answers down. But the reality is that often trial and error are required.</p>
<p class="mce-root">The solution that works for me on my Twitter home timeline may not work for you. For example, this code works well on a friend's Twitter feed. Why is this? He follows a lot of similar people who talk about similar things at the same time. It's a little harder to cluster tweets in my Twitter home feed. I follow a diverse array of people. The people I follow don't have set schedules of tweeting and do not generally interact with other Twitter users. Therefore, the tweets are generally quite diverse already.</p>
<p class="mce-root">It is with this in mind that I encourage you to experiment and tweak your program. In the subsections that follow, I shall outline what worked for me. It may not work for you.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tweaking distances </h1>
                </header>
            
            <article>
                
<p class="mce-root">Up to this point, we had been using Euclidean distance as provided by the <kbd>Marcin</kbd> library. The Euclidean distance is computed as follows:</p>
<pre class="mce-root">$ EuclideanDistance(\mathbf{q},\mathbf{p}) = \sqrt{\sum_{i=1}^n (q_i-p_i)^2}.$</pre>
<p class="mce-root">The <kbd>EuclideanDistance</kbd> is a good metric to use when it comes to coordinates in a Cartesian space. Indeed, earlier I had drawn up an analogy of thinking of a tweet as a bunch of coordinates in space, to explain K-means and DBSCAN. The reality is that text documents aren't really in Cartesian space. You may think of them as being in Cartesian space, but they are not strictly so.</p>
<p class="mce-root">So, allow me to introduce another type of distance, one that is more suited to dealing with textual elements in a bag-of-words-style setting that we're currently doing, the Jaccard distance.</p>
<p class="mce-root">The Jaccard distance is defined as follows:</p>
<pre class="mce-root">$ d_J(A,B) = 1 - J(A,B) = { { |A \cup B| - |A \cap B| } \over |A \cup B| } $</pre>
<p class="mce-root">Here, <kbd>$A$</kbd> and <kbd>$B$</kbd> are sets of words in each tweet. The implementation of the Jaccard distance in Go is rudimentary, but it works:</p>
<pre class="mce-root"> func jaccard(a, b []float64) float64 {<br/> setA, setB := make(map[int]struct{}), make(map[int]struct{})<br/> union := make(map[int]struct{})<br/> for i := range a {<br/> if a[i] != 0 {<br/> union[i] = struct{}{}<br/> setA[i] = struct{}{}<br/> }<br/> }<br/>for i := range b {<br/> if b[i] != 0 {<br/> union[i] = struct{}{}<br/> setB[i] = struct{}{}<br/> }<br/> }<br/>intersection := 0.0<br/> for k := range setA {<br/> if _, ok := setB[k]; ok {<br/> intersection++<br/> }<br/> }<br/>return 1 - (intersection / float64(len(union)))<br/> }</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tweaking the preprocessing step </h1>
                </header>
            
            <article>
                
<p class="mce-root">One thing you may note is that the preprocessing of tweets is very minimal, and some of the rules are odd. For example, all hashtags are treated as one, as are all links and mentions. When this project started, it seemed like a good reason. There is no other justification than it seemed like a good reason; one always needs a springboard from which to jump off in any project. A flimsy excuse at that point is as good as any other.<br/>
<br/>
Nonetheless, I have tweaked my preprocessing steps. These are the functions that I finally settled on. Do observe the difference between this and the original, listed in previous sections:</p>
<pre class="mce-root"> var nl = regexp.MustCompile("\n+")<br/> var ht = regexp.MustCompile("&amp;.+?;")<br/>func (p *processor) single(word string) (wordID int, ok bool) {<br/> if _, ok = stopwords[word]; ok {<br/> return -1, false<br/> }<br/> switch {<br/> case strings.HasPrefix(word, "#"):<br/> word = strings.TrimPrefix(word, "#")<br/> case word == "@":<br/> return -1, false // at is a stop word!<br/> case strings.HasPrefix(word, "http"):<br/> return -1, false<br/> }<br/>if word == "rt" {<br/> return -1, false<br/> }<br/>return p.corpus.Add(word), true<br/> }<br/>func (p *processor) process(a []*processedTweet) []*processedTweet {<br/> // remove things from consideration<br/> i := 0<br/> for _, tt := range a {<br/> if tt.Lang == "en" {<br/> a[i] = tt<br/> i++<br/> }<br/> }<br/> a = a[:i]<br/>var err error<br/> for _, tt := range a {<br/> if tt.RetweetedStatus != nil {<br/> tt.Tweet = *tt.RetweetedStatus<br/> }<br/>tt.clean, _, err = transform.String(p.transformer, tt.FullText)<br/> dieIfErr(err)<br/> tt.clean = strings.ToLower(tt.clean)<br/> tt.clean = nl.ReplaceAllString(tt.clean, "\n")<br/> tt.clean = ht.ReplaceAllString(tt.clean, "")<br/> tt.clean = stripPunct(tt.clean)<br/> log.Printf("%v", tt.clean)<br/> for _, word := range strings.Fields(tt.clean) {<br/> // word = corpus.Singularize(word)<br/> wordID, ok := p.single(word)<br/> if ok {<br/> tt.ids = append(tt.ids, wordID)<br/> tt.clean2 += " "<br/> tt.clean2 += word<br/> }<br/>if word == "rt" {<br/> tt.isRT = true<br/> }<br/> }<br/> p.tfidf.Add(tt)<br/> log.Printf("%v", tt.clean2)<br/> }<br/>p.tfidf.CalculateIDF()<br/> // calculate scores<br/> for _, tt := range a {<br/> tt.textVec = p.tfidf.Score(tt)<br/> }<br/>// normalize text vector<br/> size := p.corpus.Size()<br/> for _, tt := range a {<br/> tt.normTextVec = make([]float64, size)<br/> for i := range tt.ids {<br/> tt.normTextVec[tt.ids[i]] = tt.textVec[i]<br/> }<br/> }<br/> return a<br/> }<br/>func stripPunct(a string) string {<br/> const punct = ",.?;:'\"!’*-“"<br/> return strings.Map(func(r rune) rune {<br/> if strings.IndexRune(punct, r) &lt; 0 {<br/> return r<br/> }<br/> return -1<br/> }, a)<br/> }</pre>
<p class="mce-root">The most notable thing that I have changed is that I now consider a hashtag a word. Mentions are removed. As for URLs, in one of the attempts at clustering, I realized that the clustering algorithms were clustering all the tweets with a URL into the same cluster. That realization made me remove hashtags, mentions, and URLs. Hashtags have the <kbd>#</kbd> removed and are treated as if they were normal words.</p>
<p class="mce-root">Furthermore, you may note that I added some quick and dirty ways to <kbd>clean</kbd> certain things:</p>
<pre class="mce-root"> tt.clean = strings.ToLower(tt.clean)<br/> tt.clean = nl.ReplaceAllString(tt.clean, "\n")<br/> tt.clean = ht.ReplaceAllString(tt.clean, "")<br/> tt.clean = stripPunct(tt.clean)</pre>
<p class="mce-root">Here, I used regular expressions to replace multiple newlines with just one, and to replace all HTML-encoded text with nothing. Lastly, I removed all punctuation.</p>
<p class="mce-root">In a more formal setting, I would use a proper lexer to handle my text. The lexer I'd use would come from Lingo (<kbd>github.com/chewxy/lingo</kbd>). But given that Twitter is a low value environment, there wasn't much point in doing so. A proper lexer like the one in lingo flags text as multiple things, allowing for easy removal.</p>
<p class="mce-root">Another thing you might notice is that I changed the definition of what a tweet is mid-flight:</p>
<pre class="mce-root"> if tt.RetweetedStatus != nil {<br/> tt.Tweet = *tt.RetweetedStatus<br/> }</pre>
<p class="mce-root">This block of code says if a tweet is indeed a retweeted status, replace the tweet with the retweeted tweet. This works for me. But it may not work for you. I personally consider any retweet to be the same as repeating a tweet. So, I do not see why they should be separate. Additionally, Twitter allows for users to comment on a retweet. If you want to include that, you'd have to change the logic a little bit more. Either way, the way I got to this was by manually inspecting the <kbd>JSON</kbd> file I had saved.</p>
<p class="mce-root">It's asking these questions and then making a judgment call what is important in doing data science, either in Go or any other language. It's not about blindly applying algorithms. Rather, it's always driven by what the data tells you.</p>
<p class="mce-root">One last thing that you may note is this curious block of code:</p>
<pre class="mce-root"> // remove things from consideration<br/> i := 0<br/> for _, tt := range a {<br/> if tt.Lang == "en" {<br/> a[i] = tt<br/> i++<br/> }<br/> }<br/> a = a[:i]</pre>
<p class="mce-root">Here, I only consider English tweets. I follow many people who tweet in a variety of languages. At any given time, my home timeline would have about 15% of tweets in French, Chinese, Japanese, or German. Clustering tweets in a different language is a whole different ballgame, so I chose to omit them.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we have learned how to cluster tweets using a variety of clustering methods. Though frequently touted as one of the most robust algorithms, we've shown that DBSCAN has problems with clustering tweets due to the nature of tweets being noisy. Instead, we see that older, more traditional methods, as well as a new method of clustering, would yield better results.</p>
<p>This points to a lesson—<span>there is no one machine-learning algorithm to rule them all; there is no ultimate algorithm. Instead, we need to try more than one thing</span>. In the chapters that follow, this theme will be more apparent, and we shall approach these with more rigor. In the next chapter, we will learn about basics of neural networks and apply them on handwriting to recognize digits.</p>


            </article>

            
        </section>
    </div>



  </body></html>