["```py\nimport os\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport warnings\nimport string\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import mode\n```", "```py\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n```", "```py\nos.chdir(\"/.../Chapter 11/CS - SMS Classification\")\nos.getcwd()\n\ndf_sms = pd.read_csv(\"sms_labeled_data.csv\", encoding = 'utf8')\n```", "```py\ndf_sms.head()\n```", "```py\ndf_sms.shape\n```", "```py\n# Gives the count for ham messages\nprint(df_sms[\"type\"].value_counts()[0])\nno_of_ham_messages = df_sms[\"type\"].value_counts()[0]\n\n# Gives the count for spam messages\nprint(df_sms[\"type\"].value_counts()[1])\nno_of_spam_messages = df_sms[\"type\"].value_counts()[1]\n```", "```py\nsms_count = pd.value_counts(df_sms[\"type\"], sort= True)\nax = sms_count.plot(kind='bar', figsize=(10,10), color= [\"green\", \"orange\"], fontsize=13)\n\nax.set_alpha(0.8)\nax.set_title(\"Percentage Share of Spam and Ham Messages\")\nax.set_ylabel(\"Count of Spam & Ham messages\");\nax.set_yticks([0, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500])\n\ntotals = []\nfor i in ax.patches:\ntotals.append(i.get_height())\n\ntotal = sum(totals)\n\n# set individual bar lables using above list\nfor i in ax.patches:\nstring = str(round((i.get_height()/total)*100, 2))+'%'\n# get_x pulls left or right; get_height pushes up or down\nax.text(i.get_x()+0.16, i.get_height(), string, fontsize=13, color='black')\n```", "```py\nlemmatizer = WordNetLemmatizer()\n\n# Defining a function to remove punctuations, convert text to lowercase and remove stop words\ndef process_text(text):\n    no_punctuations = [char for char in text if char not in string.punctuation]\n    no_punctuations = ''.join(no_punctuations)\n\n    clean_words = [word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    clean_words = [lemmatizer.lemmatize(lem) for lem in clean_words]\n    clean_words = \" \".join(clean_words)\n\n    return clean_words\n```", "```py\ndf_sms['text'] = df_sms['text'].apply(text_processing)\n```", "```py\nX = df_sms.loc[:,'text']\nY = df_sms.loc[:,'type']\nY = Y.astype('int')\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3, random_state=1)\n```", "```py\ncount_vectorizer = CountVectorizer(stop_words='english')\n\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\n```", "```py\ntfidf = TfidfVectorizer(stop_words='english')\n\ntfidf_train = tfidf.fit_transform(X_train)\ntfidf_test = tfidf.transform(X_test)\n```", "```py\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n\nnb.fit(count_train, Y_train)\nnb_pred_train = nb.predict(count_train)\nnb_pred_test = nb.predict(count_test)\nnb_pred_train_proba = nb.predict_proba(count_train)\nnb_pred_test_proba = nb.predict_proba(count_test)\n\nprint('The accuracy for the training data is {}'.format(nb.score(count_train, Y_train)))\nprint('The accuracy for the testing data is {}'.format(nb.score(count_test, Y_test)))\n```", "```py\nprint(classification_report(Y_test, nb_pred_test))\n```", "```py\ntarget_names = ['Spam','Ham']\n\n# Pass actual & predicted values to the confusion matrix()\ncm = confusion_matrix(Y_test, nb_pred_test)\nplt.figure()\nplot_confusion_matrix(cm, classes=target_names)\nplt.show()\n```", "```py\nnb.fit(tfidf_train, Y_train)\nnb_pred_train_tfidf = nb.predict(tfidf_train)\nnb_pred_test_tfidf = nb.predict(tfidf_test)\n\nnb_tfidf_pred_train_proba = nb.predict_proba(tfidf_train)\nnb_tfidf_pred_test_proba = nb.predict_proba(tfidf_test)\n\nprint('The accuracy for the training data is {}'.format(nb.score(count_train, Y_train)))\nprint('The accuracy for the testing data is {}'.format(nb.score(count_test, Y_test)))\n```", "```py\nprint(classification_report(Y_test, nb_pred_test_tfidf))\n\ntarget_names = ['Spam','Ham']\n\n# Pass actual & predicted values to the confusion matrix()\ncm = confusion_matrix(Y_test, nb_pred_test_tfidf)\nplt.figure()\n\nplot_confusion_matrix(cm, classes=target_names)\nplt.show()\n```", "```py\nfrom sklearn.svm import SVC\n\nsvc = SVC(kernel='rbf',probability=True)\nsvc_params = {'C':[0.001, 0.01, 0.1, 1, 10]}\n\nsvc_gcv_rbf_count = GridSearchCV(svc, svc_params, cv=5)\nsvc_gcv_rbf_count.fit(count_train, Y_train)\n\n# We use the grid model to predict the class \nsvc_rbf_train_predicted_values = svc_gcv_rbf_count.predict(count_train)\nsvc_rbf_test_predicted_values = svc_gcv_rbf_count.predict(count_test)\n\n# We use the grid model to predict the class probabilities\nsvc_gcv_train_proba_rbf = svc_gcv_rbf_count.predict_proba(count_train)\nsvc_gcv_test_proba_rbf = svc_gcv_rbf_count.predict_proba(count_test)\n\nprint('The best parameters {}'.format(svc_gcv_rbf_count.best_params_))\nprint('The best score {}'.format(svc_gcv_rbf_count.best_score_))\n```", "```py\nprint(classification_report(Y_test, svc_rbf_test_predicted_values))\n\ntarget_names = ['Spam','Ham']\n\n# Pass actual & predicted values to the confusion matrix()\ncm = confusion_matrix(Y_test, svc_rbf_test_predicted_values)\nplt.figure()\nplot_confusion_matrix(cm,classes=target_names)\nplt.show()\n```", "```py\nsvc = SVC(kernel='rbf',probability=True)\nsvc_params = {'C':[0.001, 0.01, 0.1, 1, 10]}\n\nsvc_gcv = GridSearchCV(svc,svc_params,cv=5)\nsvc_gcv.fit(tfidf_train, Y_train)\n\n# We use the grid model to predict the class \nsvc_tfidf_rbf_train_predicted_values = svc_gcv.predict(tfidf_train)\nsvc_tfidf_rbd_test_predicted_values = svc_gcv.predict(tfidf_test)\n\n# We use the grid model to predict the class probabilities\nsvc_gcv_tfidf_train_proba_rbf = svc_gcv.predict_proba(tfidf_train)\nsvc_gcv_tfidf_test_proba_rbf = svc_gcv.predict_proba(tfidf_test)\n\nprint('The best parameters {}'.format(svc_gcv.best_params_))\nprint('The best score {}'.format(svc_gcv.best_score_))\n```", "```py\n# Set the parameters for grid search\nrf_params = {\"criterion\":[\"gini\",\"entropy\"],\"min_samples_split\":[2,3],\"max_depth\":[None,2,3],\"min_samples_leaf\":[1,5],\"max_leaf_nodes\":[None],\"oob_score\":[True]}\n\n# Create an instance of the Random Forest Classifier()\nrf = RandomForestClassifier()\n\n# Use gridsearchCV(), pass the values you have set for grid search\nrf_gcv = GridSearchCV(rf, rf_params, cv=5)\n\n# Fit the model onto the train data\nrf_gcv.fit(count_train, Y_train)\n\n# We use the grid model to predict the class \nrf_train_predicted_values = rf_gcv.predict(count_train)\nrf_test_predicted_values = rf_gcv.predict(count_test)\n\n# We use the grid model to predict the class probabilities\nrf_gcv_pred_train_proba = rf_gcv.predict_proba(count_train)\nrf_gcv_pred_test_proba = rf_gcv.predict_proba(count_test)\n\nprint('The best parameters {}'.format(rf_gcv.best_params_))\nprint('The best score {}'.format(rf_gcv.best_score_))\n```", "```py\nprint(classification_report(Y_test, rf_test_predicted_values))\n\ntarget_names = ['Spam','Ham']\n\n# Pass actual & predicted values to the confusion matrix()\ncm = confusion_matrix(Y_test, rf_test_predicted_values)\nplt.figure()\nplot_confusion_matrix(cm,classes=target_names)\nplt.show() \n```", "```py\n# Set the parameters for grid search\nrf_params = {\"criterion\":[\"gini\",\"entropy\"],\"min_samples_split\":[2,3],\"max_depth\":[None,2,3],\"min_samples_leaf\":[1,5],\"max_leaf_nodes\":[None],\"oob_score\":[True]}\n\n# Create an instance of the Random Forest Classifier()\nrf = RandomForestClassifier()\n\n# Use gridsearchCV(), pass the values you have set for grid search\nrf_gcv = GridSearchCV(rf, rf_params, cv=5)\n\nrf_gcv.fit(tfidf_train, Y_train)\n\nrf_tfidf_train_predicted_values = rf_gcv.predict(tfidf_train)\nrf_tfidf_test_predicted_values = rf_gcv.predict(tfidf_test)\n\nrf_gcv_tfidf_pred_train_proba = rf_gcv.predict_proba(tfidf_train)\nrf_gcv_tfidf_pred_test_proba = rf_gcv.predict_proba(tfidf_test)\n\nprint('The best parameters {}'.format(rf_gcv.best_params_))\nprint('The best score {}'.format(rf_gcv.best_score_))\n\nprint(classification_report(Y_test, rf_tfidf_test_predicted_values))\n\ntarget_names = ['Spam','Ham']\n# Pass actual & predicted values to the confusion matrix()\ncm = confusion_matrix(Y_test, rf_tfidf_test_predicted_values)\nplt.figure()\nplot_confusion_matrix(cm, classes=target_names)\nplt.show()\n```", "```py\nfpr, tpr, thresholds = roc_curve(Y_test, nb_pred_test_proba[:,1])\nroc_auc = auc(Y_test,nb_pred_test_proba[:,1])\n\nplt.title('ROC Naive Bayes (Count)')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n```", "```py\nplt.subplot(4,3,7)\n\n### Test Count Data\nd = (nb_pred_test_proba + svc_gcv_test_proba_rbf + rf_gcv_pred_test_proba)/4\n\nfpr, tpr, thresholds = roc_curve(Y_test,d[:,1])\nroc_auc = auc(Y_test,d[:,1])\n\nplt.title('ROC Ensemble (Count)')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.subplot(4,3,8)\n\n### Test TF-IDF Data\nd = (nb_tfidf_pred_test_proba + svc_gcv_tfidf_test_proba_rbf + rf_gcv_tfidf_pred_test_proba)/4\n\nfpr, tpr, thresholds = roc_curve(Y_test,d[:,1])\nroc_auc = auc(Y_test,d[:,1])\n\nplt.title('ROC Ensemble (TF-IDF)')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n#plt.show()\n\nplt.tight_layout(pad=1,rect=(0, 0, 3.5, 4))\nplt.show()\n```", "```py\npredicted_array = np.array([nb_pred_test_tfidf, svc_tfidf_rbd_test_predicted_values, rf_tfidf_test_predicted_values])\n\nprint(\"Each array is the prediction of the respective models\")\nprint(predicted_array)\n```", "```py\n# Using mode on the array, we get the max vote for each observation\npredicted_array = mode(predicted_array)\n\n# Check the array\nprint(predicted_array)\n\nprint(\"The accuracy for test\")\naccuracy_score(Y_test, predicted_array[0][0])\n```", "```py\nimport os\nimport glob\nimport pandas as pd\n```", "```py\nos.chdir(\"/.../Chapter 11/CS - IMDB Classification\")\nos.getcwd()\n```", "```py\npath=\"/.../Chapter 11/CS - IMDB Classification/txt_sentoken/pos/*.txt\"\n\nfiles = glob.glob(path)\ntext_pos = []\n\nfor p in files:\n file_read = open(p, \"r\")\n to_append_pos = file_read.read()\n text_pos.append(to_append_pos)\n file_read.close()\n\ndf_pos = pd.DataFrame({'text':text_pos,'label':'positive'})\ndf_pos.head()\n```", "```py\npath=\"/Users/Dippies/CODE PACKT - EML/Chapter 11/CS - IMDB Classification/txt_sentoken/neg/*.txt\"\n\nfiles = glob.glob(path)\ntext_neg = []\n\nfor n in files:\n    file_read = open(n, \"r\")\n    to_append_neg = file_read.read()\n    text_neg.append(to_append_neg)\n    file_read.close()\n\ndf_neg = pd.DataFrame({'text':text_neg,'label':'negative'})\ndf_neg.head()\n```", "```py\ndf_moviereviews=pd.concat([df_pos, df_neg])\n```", "```py\nprint(df_moviereviews.head())\nprint(df_moviereviews.tail())\n```", "```py\nfrom sklearn.utils import shuffle\n\ndf_moviereviews=shuffle(df_moviereviews)\ndf_moviereviews.head(10)\n```", "```py\ndf_moviereviews.shape\n```", "```py\ndf_moviereviews.to_csv(\"/.../Chapter 11/CS - IMDB Classification/Data_IMDB.csv\") \n```", "```py\ndf_moviereviews[\"label\"].value_counts().plot(kind='pie')\nplt.tight_layout(pad=1,rect=(0, 0, 0.7, 1))\n\nplt.text(x=-0.9,y=0.1, \\\n         s=(np.round(((df_moviereviews[\"label\"].\\\n                       value_counts()[0])/(df_moviereviews[\"label\"].value_counts()[0] + \\\n                       df_moviereviews[\"label\"].value_counts()[1])),2)))\n\nplt.text(x=0.4,y=-0.3, \\\n         s=(np.round(((df_moviereviews[\"label\"].\\\n                       value_counts()[1])/(df_moviereviews[\"label\"].value_counts()[0] + \\\n                       df_moviereviews[\"label\"].value_counts()[1])),2)))\n\nplt.title(\"% Share of the Positive and Negative reviews in the dataset\")\n```", "```py\ndf_moviereviews.loc[df_moviereviews[\"label\"]=='positive',\"label\",]=1\ndf_moviereviews.loc[df_moviereviews[\"label\"]=='negative',\"label\",]=0\n```", "```py\nlemmatizer = WordNetLemmatizer()\ndef process_text(text):\n    nopunc = [char for char in text if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n\n    clean_words = [word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    clean_words = [lemmatizer.lemmatize(lem) for lem in clean_words]\n    clean_words = \" \".join(clean_words)\n\n    return clean_words\n```", "```py\ndf_moviereviews['text'] = df_moviereviews['text'].apply(process_text)\n```", "```py\nimport os\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport warnings\nimport string\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import mode\n```", "```py\nX = df_moviereviews.loc[:,'text']\nY = df_moviereviews.loc[:,'label']\nY = Y.astype('int')\n```", "```py\nX_train,X_test,y_train,y_test = train_test_split(X, Y, test_size=.3, random_state=1)\n```", "```py\ncount_vectorizer = CountVectorizer()\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\n```", "```py\ntfidf = TfidfVectorizer()\ntfidf_train = tfidf.fit_transform(X_train)\ntfidf_test = tfidf.transform(X_test)\n```", "```py\n# Set the parameters for grid search\nrf_params = {\"criterion\":[\"gini\",\"entropy\"],\\\n             \"min_samples_split\":[2,3],\\\n             \"max_depth\":[None,2,3],\\\n             \"min_samples_leaf\":[1,5],\\\n             \"max_leaf_nodes\":[None],\\\n             \"oob_score\":[True]}\n\n# Create an instance of the RandomForestClassifier()\nrf = RandomForestClassifier()\nwarnings.filterwarnings(\"ignore\")\n\n# Use gridsearchCV(), pass the values you have set for grid search\nrf_count = GridSearchCV(rf, rf_params, cv=5)\n\nrf_count.fit(count_train, Y_train)\n\n# Predict class predictions & class probabilities with test data\nrf_count_predicted_values = rf_count.predict(count_test)\nrf_count_probabilities = rf_count.predict_proba(count_test)\n\nrf_count_train_accuracy = rf_count.score(count_train, Y_train)\nrf_count_test_accuracy = rf_count.score(count_test, Y_test)\n\nprint('The accuracy for the training data is {}'.\\\n      format(rf_count_train_accuracy))\n\nprint('The accuracy for the testing data is {}'.\\\n      format(rf_count_test_accuracy))\n```", "```py\nprint(classification_report(Y_test, rf_count_predicted_values))\n\n# Pass actual & predicted values to the confusion_matrix()\ncm = confusion_matrix(Y_test, rf_count_predicted_values)\nplt.figure()\nplot_confusion_matrix(cm, classes=target_names,normalize=False)\nplt.show()\n```", "```py\n# Set the parameters for grid search\nrf_params = {\"criterion\":[\"gini\",\"entropy\"],\"min_samples_split\":[2,3],\"max_depth\":[None,2,3],\"min_samples_leaf\":[1,5],\"max_leaf_nodes\":[None],\"oob_score\":[True]}\n\n# Create an instance of the RandomForestClassifier()\nrf = RandomForestClassifier()\nwarnings.filterwarnings(\"ignore\")\n\n# Use gridsearchCV(), pass the values you have set for grid search\nrf_tfidf = GridSearchCV(rf, rf_params, cv=5)\n\nrf_tfidf.fit(tfidf_train, Y_train)\n```", "```py\nrf_tfidf_predicted_values = rf_tfidf.predict(tfidf_test)\nrf_tfidf_probabilities = rf_tfidf.predict_proba(tfidf_test)\n\nrf_train_accuracy = rf_tfidf.score(tfidf_train, Y_train)\nrf_test_accuracy = rf_tfidf.score(tfidf_test, Y_test)\n\nprint('The accuracy for the training data is {}'.format(rf_train_accuracy))\nprint('The accuracy for the testing data is {}'.format(rf_test_accuracy))\n\nprint(classification_report(Y_test, rf_tfidf_predicted_values))\n\n# Pass actual & predicted values to the confusion_matrix()\ncm = confusion_matrix(Y_test, rf_tfidf_predicted_values)\nplt.figure()\nplot_confusion_matrix(cm, classes=target_names,normalize=False)\nplt.show()\n```", "```py\nnb_count = MultinomialNB()\nnb_count.fit(count_train, Y_train)\n\nnb_count_predicted_values = nb_count.predict(count_test)\nnb_count_probabilities = nb_count.predict_proba(count_test)\n\nnb_train_accuracy = nb_count.score(count_train, Y_train)\nnb_test_accuracy = nb_count.score(count_test, Y_test)\n\nprint('The accuracy for the training data is {}'.format(nb_train_accuracy))\nprint('The accuracy for the testing data is {}'.format(nb_test_accuracy))\n```", "```py\nprint(classification_report(Y_test, nb_predicted_values))\n\n# Pass actual & predicted values to the confusion matrix()\ncm = confusion_matrix(Y_test, nb_predicted_values)\nplt.figure()\nplot_confusion_matrix(cm, classes=target_names,normalize=False)\nplt.show()\n```", "```py\nnb_tfidf = MultinomialNB()\nnb_tfidf.fit(count_train, Y_train)\n\nnb_tfidf_predicted_values = nb_tfidf.predict(tfidf_test)\nnb_tfidf_probabilities = nb_tfidf.predict_proba(tfidf_test)\n\nnb_train_accuracy = nb_tfidf.score(tfidf_train, Y_train)\nnb_test_accuracy = nb_tfidf.score(tfidf_test, Y_test)\n\nprint('The accuracy for the training data is {}'.format(nb_train_accuracy))\nprint('The accuracy for the testing data is {}'.format(nb_test_accuracy))\n\nprint(classification_report(Y_test, nb_predicted_values))\n\n#Pass actual & predicted values to the confusion matrix()\ncm = confusion_matrix(Y_test, nb_predicted_values)\nplt.figure()\nplot_confusion_matrix(cm, classes=target_names,normalize=False)\nplt.show()\n```", "```py\nsvc_count = SVC(kernel='linear',probability=True)\nsvc_params = {'C':[0.001, 0.01, 0.1, 1, 10]}\n\nsvc_gcv_count = GridSearchCV(svc_count, svc_params, cv=5)\nsvc_gcv_count.fit(count_train, Y_train)\n\nsvc_count_predicted_values = svc_gcv_count.predict(count_test)\nsvc_count_probabilities = svc_gcv_count.predict_proba(count_test)\n\nsvc_count_train_accuracy = svc_gcv_count.score(count_train, Y_train)\nsvc_count_test_accuracy = svc_gcv_count.score(count_test, Y_test)\n\nprint('The accuracy for the training data is {}'.format(svc_gcv_count.score(count_train, Y_train)))\nprint('The accuracy for the testing data is {}'.format(svc_gcv_count.score(count_test, Y_test)))\n\nprint(classification_report(Y_test, svc_count_predicted_values))\n# Pass actual & predicted values to the confusion_matrix()\ncm = confusion_matrix(Y_test, svc_count_predicted_values)\nplt.figure()\nplot_confusion_matrix(cm, classes=target_names,normalize=False)\nplt.show()\n```", "```py\nsvc_tfidf = SVC(kernel='linear',probability=True)\nsvc_params = {'C':[0.001, 0.01, 0.1, 1, 10]}\n\nsvc_gcv_tfidf = GridSearchCV(svc_tfidf, svc_params, cv=5)\nsvc_gcv_tfidf.fit(tfidf_train, Y_train)\n\nsvc_tfidf_predicted_values = svc_gcv_tfidf.predict(tfidf_test)\nsvc_tfidf_probabilities = svc_gcv_tfidf.predict_proba(tfidf_test)\n\nsvc_tfidf_train_accuracy = svc_gcv_count.score(tfidf_train, Y_train)\nsvc_tfidf_test_accuracy = svc_gcv_count.score(tfidf_test, Y_test)\n\nprint('The accuracy for the training data is {}'.format(svc_gcv_tfidf.score(count_train, Y_train)))\nprint('The accuracy for the testing data is {}'.format(svc_gcv_tfidf.score(count_test, Y_test)))\n\nprint(classification_report(Y_test, svc_tfidf_predicted_values))\n# Pass actual & predicted values to the confusion_matrix()\ncm = confusion_matrix(Y_test, svc_tfidf_predicted_values)\nplt.figure()\nplot_confusion_matrix(cm, classes=target_names)\nplt.show()\n```", "```py\nfpr, tpr, thresholds = roc_curve(Y_test, rf_count_probabilities[:,1])\nroc_auc = auc(Y_test, rf_count_probabilities[:,1])\n\nplt.title('ROC Random Forest Count Data')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n```", "```py\npredicted_values_count = np.array([rf_count_predicted_values, \\\n                                   nb_count_predicted_values, \\\n                                   svc_count_predicted_values])\n\npredicted_values_tfidf = np.array([rf_tfidf_predicted_values, \\\n                                   nb_tfidf_predicted_values, \\\n                                   svc_tfidf_predicted_values])\n\npredicted_values_count = mode(predicted_values_count)\npredicted_values_tfidf = mode(predicted_values_tfidf)\n```", "```py\ncount = np.array([rf_count_test_accuracy,\\\n                  nb_count_test_accuracy,\\\n                  svc_count_test_accuracy,\\\n                  accuracy_score(Y_test, predicted_values_count[0][0])])\n\ntfidf = np.array([rf_tfidf_test_accuracy,\\\n                  nb_tfidf_test_accuracy,\\\n                  svc_tfidf_test_accuracy,\\\n                  accuracy_score(Y_test, predicted_values_tfidf[0][0])])\n\nlabel_list = [\"Random Forest\", \"Naive_Bayes\", \"SVM_Linear\", \"Ensemble\"] \nplt.plot(count)\nplt.plot(tfidf)\nplt.xticks([0,1,2,3],label_list)\n\nfor i in range(4):\n    plt.text(x=i,y=(count[i]+0.001), s=np.round(count[i],4))\n\nfor i in range(4):\n    plt.text(x=i,y=tfidf[i]-0.003, s=np.round(tfidf[i],4))\n\nplt.legend([\"Count\",\"TFIDF\"])\nplt.title(\"Test accuracy\")\n\nplt.tight_layout(pad=1,rect=(0, 0, 2.5, 2))\nplt.show()\n```", "```py\nimport os\nimport pandas as pd\nimport nltk\nfrom nltk.tag import pos_tag\nfrom nltk.corpus import stopwords\n```", "```py\ndf_moviereviews['text'] =df_moviereviews['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndf_moviereviews['text'].head()\n```", "```py\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\nstop = set(stopwords.words('english'))\nexclude = set(string.punctuation) \nlemma = WordNetLemmatizer()\ndef clean(doc):\n    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n    stop_free = ''.join(ch for ch in stop_free if ch not in exclude)\n    normalized = \" \".join(lemma.lemmatize(word) for word in stop_free.split())\n    return normalized\n\ntokenized_sent = [clean(doc).split() for doc in df_moviereviews[\"text\"]]\n```", "```py\ntokenized_sent[0][0:10]\n```", "```py\npostag=[nltk.pos_tag(token) for token in tokenized_sent]\n```", "```py\npostag[0][0:10]\n```"]