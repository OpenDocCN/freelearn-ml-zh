- en: Predicting Failures of Banks - Univariate Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, big data and machine learning have become increasingly popular
    in many areas. It is generally believed that the greater the number of variables
    there are, the more accurate a classifier becomes. However, this is not always
    true.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will reduce the number of variables in the dataset by analyzing
    the individual predictive power of each variable and using different alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrapper method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedded methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this real-world case of predicting the failure of banks, we have a high number
    of variables or financial ratios to train a classifier, so we would expect to
    obtain a great predictive model. With this in mind, why would we want to select
    alternate variables and reduce their number?
  prefs: []
  type: TYPE_NORMAL
- en: Well, in some cases, increasing the dimensionality of the problem by adding
    new features could reduce the performance of our model. This is called the **curse
    of dimensionality** problem.
  prefs: []
  type: TYPE_NORMAL
- en: According to this problem, the fact of adding more features or increasing the
    dimensionality of our feature space will require collecting more data. In this
    sense, the new observations we need to collect have to grow exponentially quickly
    to maintain the learning process and to avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: This problem is commonly observed in cases in which the ratio between the number
    of variables and the observations in our data is not very high.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection is also useful for identifying and removing unneeded, irrelevant,
    and redundant variables from data and for reducing the complexity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Three general classes of feature selection algorithms can be found in many
    machine learning guidelines. These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter methods**: In this method, the variables are selected according to
    the correlation with a target variable. Thus, it is measured by the ability of
    each variable to explain the target of the model. These methods are especially
    useful when the number of variables is high and they help to avoid overfitting.
    As a drawback, it is worth mentioning that, in spite of being non-predictive individually,
    and measured in a univariate way, the variables can become predictive when combined
    with other variables. To summarize, these methods do not consider the relationships
    between variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wrapper methods**: Wrapper methods evaluate subsets of variables to detect
    the possible interactions between variables. In wrapper methods, several combinations
    of variables are used in a predictive model, and a score is given to each combination
    according to the model accuracy. Consequently, it is possible to avoid irrelevant
    combinations. Nevertheless, these methods are very time consuming if the number
    of variables is large, and there is always a risk of overfitting, especially if
    the number of observations is low.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedded methods**: Finally, features that best contribute to improving the
    model accuracy are learned and remembered during the training process by the embedded
    methods. **Regularization** is one such feature selection method. They are also
    known as **penalization methods** because they impose constraints on the optimization
    parameters that usually get models with a fewer number of variables. Lasso, elastic
    net, and Ridge regression are the most common regularization methods. Other examples
    of embedded feature selection algorithms include Lasso, elastic net, and Ridge
    regression algorithms. We will look at these models in more detail later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start with a filter method to reduce the number of variables in a first
    step. For that, we will measure the predictive power or the ability of a variable
    to classify our target variable individually and correctly.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we try to find variables that differentiate correctly between
    solvent and non-solvent banks. To measure the predictive power of a variable,
    we use a metric named **Information Value** (**IV**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, given a grouped variable in *n* groups, each with a certain distribution
    of good banks and bad banks—or in our case, solvent and non-solvent banks—the
    information value for that predictor can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/238254df-559f-4acc-b4cc-5e7c7bfb3b94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The IV statistic is generally interpreted depending on its value:'
  prefs: []
  type: TYPE_NORMAL
- en: '**< 0.02**: The variable of analysis does not accurately separate the classes
    in the target variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**0.02 to 0.1**: The variable has a weak relationship with the target'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**0.1 to 0.3**: The variable displays a medium-strength relationship'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**> 0.3**: The variable is a good predictor of the target'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'According to this value, this variable is itself very predictive. Therefore,
    this variable could be useful to use in our model. Let’s see an example calculation
    of `IV` for a variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/211876f5-808f-43cb-b855-e1ad082758e3.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding table, we can see the calculation of the information value
    of the `UBPRE006` variable, which represents the sum of provisions for loans and
    losses divided by the total assets of a bank.
  prefs: []
  type: TYPE_NORMAL
- en: 'Broadly speaking, when a loan is granted, part of it must be provisioned in
    case the credit becomes delinquent; that is, banks make two types of provisions
    in their income statement to cover the so-called credit risk: the generic ones,
    which are made at the time the loan is granted, and the specific ones, which cover
    the unpaid credits.'
  prefs: []
  type: TYPE_NORMAL
- en: In theory, the higher the ratio, the more likely a bank is to fail, because,
    if the level of provisions is high, this is an indication that the credit quality
    of its loans will be lower.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, in our sample, the percentage of failed banks is 4.70%. In this
    example, the `UBPRE006` variable has been grouped into four categories and an
    additional category measuring the level of missing values. This can be seen in
    the `BadRate` column as the ratio of failed banks that have a value in this ratio
    that is lower than 0.5487%. This is very low, representing only 0.80% of banks
    in this group. As this ratio increases, the ratio of failed banks is higher as
    well. Additionally, there are no banks with missing values in this ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value appearing in the first group of this table is calculated according
    to this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b796ccf0-b1f5-4206-8b47-9a334aa76700.png)'
  prefs: []
  type: TYPE_IMG
- en: The sum of all the values in the `IV` column can be found in row `6` of this
    table in the `3.2803` column.
  prefs: []
  type: TYPE_NORMAL
- en: According to this value, this variable is itself very predictive. Therefore,
    it could be useful to use this variable in our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, **Weight of Evidence** (**WoE**) is a metric that is very
    closely related to the information value. This metric is also included in the
    preceding table. WoE is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01c6b3c7-47fc-41d3-89e9-4d833b89d755.png)'
  prefs: []
  type: TYPE_IMG
- en: In fact, the WoE equation is a part of the IV metric. The value of WoE will
    be 0 if the odds of the *Goods to Bads* ratio is equal to 1.
  prefs: []
  type: TYPE_NORMAL
- en: If the percentage of *Bads* in a group is greater than the percentage of *Goods*,
    the odds ratio will be less than 1 and the WoE will be a negative number; if the
    number of *Goods* is greater than the *Bads* in a group, the WoE value will be
    a positive number.
  prefs: []
  type: TYPE_NORMAL
- en: In general, positive values of WoE indicates that banks placed in the group
    are more solvent than the average of total banks in the sample. On the other hand,
    the higher the negative value, the riskier the banks in this group are.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to calculate the information value for each variable in our training
    set. The `smbinning` package is very useful for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: As already seen, one important step is grouping variables, which is done automatically
    in this package.
  prefs: []
  type: TYPE_NORMAL
- en: We will carry out two different experiments. Thus, the information value will
    be calculated for the training set before and after the missing imputation. We
    will discuss the reason behind these experiments later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'This package assumes that the target variable should take the value of 1 if
    a bank is solvent and 0 otherwise, which is the exact opposite of what we did
    in previous steps. So, the first step consists of reversing the target values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we run the following code (this is a very time-consuming process):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The previous code creates a table where information values will be stored (`table_iv`).
    For each variable in the `train` dataset, the information value is then calculated
    using the `smbinning.sumiv` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the process finishes, a backup of the workspace is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this table, we have the information value of each variable in the training
    sample before and after missing imputation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Behind the information value, a column informs us about the calculation status.
    Different messages could be displayed here as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Numeric binning OK`: The information value has been calculated correctly and
    at least two different groups can be differentiated in the variable to discriminate
    between *good* and *bad* banks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`No significant splits`: The information value is not calculated because the
    variable is not able to discriminate against the future solvency of a bank. Different
    groups of banks are not detected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Uniques values < 5`: A variable has fewer than five different values. In this
    case, the information value is not calculated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Results are different depending on whether the missing values have previously
    been treated or whether the missing values are included in the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `smbinning` package considers the missing values as a new category that,
    in some cases, could contain relevant information or be more related with one
    class in our target variable.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to be aware that imputation of missing variables implies a kind
    of restriction on your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check the impact of this simple decision in terms of predictive power.
    Differences of `IV` existing, or not missing values, are calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c50ea91c-8653-480e-9ada-b7713b9310a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the summary statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: According to the summary statistics, on average, missing imputation reduces
    the predictive power or information value of variables by 5.247%. In some cases,
    variables increase the predictive power as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can classify the predictive power of variables depending on its information
    value. We will consider the thresholds explained previously to define whether
    a variable displays a strong, medium, or weak predictive power:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we can remove variables with a low predictive power. Thus, from
    more than a thousand variables we have in the dataset, we will only select variables
    classified as `Strong` or `Medium`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Despite there being limitations, we will use data where missing values have
    been previously treated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Normally, the use of univariate transformations on the variables that may be
    part of a multivariate model improve its discriminatory power.
  prefs: []
  type: TYPE_NORMAL
- en: Variable transformation is helpful for mitigating the effect that outliers produce
    over the model development and for capturing non-linear relationships between
    the variable and the target.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common practices in credit risk consists of the transformation
    of variables to a different category and then to assign to each category the value
    of its corresponding WoE. Let’s see an example of this. Again, the `smbinning`
    package is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we first need to convert the target to the opposite values we have
    due to package restrictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `smbinning` function to the `UBPRD486` variable or `Tier One Leverage
    Capital`. The `Tier One` ratio represents the core measure of a bank’s financial
    strength from a regulator’s point of view. The higher the ratio, the higher the
    solvency and strength of a bank.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we analyze the distribution of this variable for failed and non-failed
    banks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the tier-one ratio distribution plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4405e573-41f5-4cbe-ad8c-11880830eeea.png)'
  prefs: []
  type: TYPE_IMG
- en: In general, failed banks display lower values in this ratio according to the
    previous screenshot. Applying the `smbinning` function, an object is created and
    variables are categorized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some graphical analysis can be carried out as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot describes the categorization very well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/786c28b5-2a19-492c-8ac8-ccc97482ced3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The 74.6% of banks displays a higher value than 8.16 in this variable . Let’s
    look at the percentage of failed banks by group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot below, shows that high bad rates are observed in banks where
    value of this ratio takes lower or equal values than **5.6**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da3fd1e1-1ee7-4de5-80f8-85dc9d64a8c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As commented, the previous screenshot shows that the lower the value of the
    ratio, the higher the number of failed banks. This variable is very predictive
    because it is easy to find groups by collecting a high number of failed banks.
    Consequently, the 74.8% of banks included in the first group went bankrupt. It
    is also possible to plot the `WoE` value for each group running the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code provides the following screenshot where weight of evidence
    values are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ad480a8-0ebb-4317-b893-8ea6adf70e45.png)'
  prefs: []
  type: TYPE_IMG
- en: For some credit applications, as the development of scoring models, it is a
    very common practice to replace the value of original values of each ratio to
    the corresponding weight of evidence value as shown in the preceding plot. For
    example, values lower or equal than 5.6 will be replaced by the value -4.1\. Thus,
    WoE variables are then used to train the model, using logistic regression, the
    most common approach.
  prefs: []
  type: TYPE_NORMAL
- en: The `smbinning` package also helps to transform original variables to their
    corresponding groups. In my experience, I didn’t find much evidence that WoE transformation
    really improves the performance of a model. So, in this case, we are not going
    to transform our variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Weak variables are removed as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The workspace is then saved as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can continue filtering variables. At this point, the dimensions of our dataset
    are these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'When facing regression or classification problems, some models perform better
    if highly correlated attributes are removed. Correlations can be obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `findCorrelation` function that belongs to the caret package performs a
    search on the correlation matrix and outputs vectors that contain integers. These
    integers correspond to columns which, if removed, can reduce the pairwise correlation.
    This function thus looks for attributes that have a higher level of correlation.
  prefs: []
  type: TYPE_NORMAL
- en: 'It works by considering the absolute values of pairwise correlated columns.
    It removes the variables that have the largest mean absolute, which is calculated
    by comparing variables having a high correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The cut-off option is the threshold for selecting which variables are highly
    correlated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'A total of `262` variables remain in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Wrapper methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As stated at the beginning of this section, **wrapper methods** evaluate subsets
    of variables to detect the possible interactions between variables being a step
    ahead of the filter methods.
  prefs: []
  type: TYPE_NORMAL
- en: In wrapper methods, several combinations of variables are used in a predictive
    model and a score is given to each combination according to the model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In wrapper methods, a classifier is iteratively trained with multiple combinations
    of variables acting as a black box, for which the only output is a ranking of
    important features.
  prefs: []
  type: TYPE_NORMAL
- en: Boruta package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most known wrapper packages in R is called `Boruta`. This package
    is mainly based on the algorithm of **random forests**.
  prefs: []
  type: TYPE_NORMAL
- en: Although this algorithm will be explained in more detail later in the book,
    Boruta, proposed by Breiman in 2001, is a tool that mines data and generates many
    decision trees on the samples and combines them by majority voting. The purpose
    of random forests creating different decision trees is to acquire the best possible
    classifications from different classes of data.
  prefs: []
  type: TYPE_NORMAL
- en: One example of a successful implementation of random forests is in credit card
    fraud detection systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the `Boruta` package, randomized variables are created using multiple combinations
    of other variables in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: New variables are then combined with the original variables and they train a
    different random forest. The importance of the different features is obtained
    by comparing the importance of random variables with the importance of the original
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: Only variables with a higher importance than that of the randomized variables
    are considered important. The `Boruta` package is very time consuming if the number
    of variables is high, especially because the algorithm creates even more variables
    to rank its features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s launch the `Boruta` algorithm in R. First, a `seed` is established to
    make the exercise reproducible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'An auxiliary table is then created from the train dataset and no relevant variables
    are removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `Boruta` algorithm is launched (this is very time consuming and
    can take over an hour):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: When the `wrapper` object is printed, it provides the significance of features
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Boruta` algorithm comes to a conclusion about any of the variables we
    have in our database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Many variables are classified as important and unimportant, but in other cases,
    variables are assigned in a tentative category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Tentative features have an importance that is so close to their best random
    features. In such cases, `Boruta` is not able to make a confident decision with
    regard to the default number of random forest iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make a workspace backup before moving ahead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Through the `TentativeRoughFix` function, it is possible to make a decision
    on the tentative variables. For this, the median feature Z-score with the median
    Z-score of the most important random feature is compared and a decision is made:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Consequently, and according to this package, our train sample will be reduced
    to only `99` variables.
  prefs: []
  type: TYPE_NORMAL
- en: '`Boruta` is not the only wrapper approach. The `caret` package also includes
    a wrapper filter. In this case, the algorithm is called **Recursive Feature Elimination**
    (**RFE**).'
  prefs: []
  type: TYPE_NORMAL
- en: In this algorithm, first a model is trained using all independent variables
    and the importance of features is calculated. The less important variables (*n*)
    are removed from the sample and the model is trained again. This step is repeated
    several times until all variables are used. In each iteration, the performance
    of the model is assessed. Top variables in the model with the best performance
    are determined as the best predictors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this algorithm, apart from random forests (`rfFuncs`), there are many models
    that can be used for training, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression, the `lmFuncs` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes function, `nbFuncs`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagged trees function, `treebagFuncs`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see how this algorithm can be used in R:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, fix a `seed` to obtain the same results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the target variable to a `factor`. Thus, the algorithm is used for
    classification. If not, it is assumed to be a regression problem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, run the algorithm. Random forest is chosen as a classifier with a
    10-fold validation (this execution is also time consuming):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If the `recursive` object is printed, the most important variables are displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output will be obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/346afcb8-0154-4c9e-aff0-7c33ea6ec316.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s get the ranking of all the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we successfully got the rankings of the variables. Nevertheless,
    the user has to select specifically how many final variables will be included
    in the final model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, only the resulting variables after the execution of the `Boruta`
    package are considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Our sample has been reduced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Embedded methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main difference between filter and wrapper approaches is that in filter
    approaches, such as **embedded methods**, you cannot separate the learning and
    feature selection parts.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization methods are the most common type of embedded feature selection
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: In classification problems such as this one, the logistic regression method
    cannot handle the multi-collinearity problem, which occurs when variables are
    very correlated. When the number of observations is not much larger than the number
    of variables of covariates, *p*, then there can be a lot of variability. Consequently,
    this variability could even increase the likelihood by simply adding more parameters,
    resulting in overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: If variables are highly correlated or if collinearity exists, we expect the
    model parameters and variance to be inflated. The high variance is because of
    the wrongly specified model that has redundant predictors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to solve these limitations, some approaches have emerged: Ridge regression,
    Lasso, and elastic net are the most common approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a **Ridge regression**, the size of the regression coefficients is penalized
    based on the L2 norm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89739147-b061-437a-892f-ee9cfe957ab2.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *L(B|y,x)* represents the likelihood of the logistic regression, and λ
    is the tuning parameter that serves to control the relative impact of these two
    terms on the regression coefficient estimates.
  prefs: []
  type: TYPE_NORMAL
- en: A limitation of Ridge regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ridge regression includes all the predictors in the final model. However, it
    usually displays problems in the model interpretation when the number of variable
    *p* is large.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Lasso** represents another alternative of regularization and it overcomes
    the disadvantage of Ridge regression, reducing the number of predictors in the
    final model. This time, it penalizes the size of the regression coefficients using
    an L1 penalty:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b37470cd-3557-47fc-9fca-c1d7116f0779.png)'
  prefs: []
  type: TYPE_IMG
- en: When the λ is sufficiently large, it forces some of the coefficient estimates
    to be exactly equal to zero, obtaining more parsimonious models.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of Lasso
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, Lasso also displays an important weakness: If the number of covariates,
    *p*, is much larger than the number of observations, the number of selected variables
    are bounded by the number of observations.'
  prefs: []
  type: TYPE_NORMAL
- en: Elastic net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Elastic net** tries to overcome Ridge and Lasso models and performs well
    when variables are highly correlated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Elastic net trains a model using all the variables, but it also tries to combine
    the strengths of two previously used approaches (Ridge and Lasso regressions).
    Consequently, elastic net penalizes the size of the regression coefficients based
    on both the L1 norm and the L2 norm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4769f16a-a0f7-40c4-a110-66301c782c53.png)'
  prefs: []
  type: TYPE_IMG
- en: Drawbacks of elastic net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elastic net entails the selection of values λ[1] and λ[2] as being critical
    to a good performance of the model. These parameters are commonly obtained by
    cross-validation techniques. From these methods, Lasso and elastic net are usually
    used for feature selection. At the moment, we have 96 variables in our dataset;
    we decided not to reduce the number of variables.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Dimensionality projection**, or feature projection, consists of converting
    data in a high-dimensional space to a space of fewer dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: High dimensionality increases the computational complexity substantially, and
    could even increase the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality reduction** techniques are useful for featuring selection
    as well. In this case, variables are converted into other new variables through
    different combinations. These combinations extract and summarize the relevant
    information from a complex database with fewer variables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Different algorithms exist, with the following being the most important:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sammon mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular value decomposition** (**SVD**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isomap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local linear embedding** (**LLE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laplacian eigenmaps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t-distributed Stochastic Neighbor Embedding** (**t-SNE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although dimensionality reduction is not very common in cases such as failure
    prediction models or credit risk, we will see an example of this in our data.
  prefs: []
  type: TYPE_NORMAL
- en: We will also see the application of PCA and t-SNE, which are the most used algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a method for extracting important variables on a dataset by making linear
    transformations of variables. Thus, we can define a principal component as a normalized
    linear combination of the original variables.
  prefs: []
  type: TYPE_NORMAL
- en: The first principal component is the linear combination of variables that captures
    the maximum variance in the dataset. The larger the variability captured in the
    first component, the larger the information captured by the component. The first
    component best summarizes the largest information of our data in just one line.
    The second and subsequent principal components are also linear combinations of
    original variables that capture the remaining variance in the data.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is also used when variables are highly correlated. One of the main properties
    of this method is that correlations among different components are zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the implementation in R. For that, we use the `prcomp` function included
    in the `rstat` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Before implementing the PCA approach, variables should be standardized. This
    means that we should see to it that the variables have a mean that should be equal
    to zero and that they should have a standard deviation equal to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done using the `scale` and `center` options as parameters in the
    same function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `center` and `scale` vectors contain the mean and standard deviation of
    the variables we have used.
  prefs: []
  type: TYPE_NORMAL
- en: The rotation measure returns the principal components. We obtain the same number
    of principal components as the variables that we have in the sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s print how these components look. For example, the first rows of the four
    first components are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of these components explains a proportion of the total variance. The proportion
    of variance explained by each component is easy to compute as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first calculate the variance of each component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Then each variance is divided by the sum of the component variances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The first principal component explains about 10% of the variance. The second
    component explains 6% of the variance, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can observe both the total variances and their contribution graphically
    using this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e00eae0-a9c4-46cc-b17e-8b11b1f820fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s run the code to plot variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d21336c7-d632-43f1-aa71-820cb4f64b19.png)'
  prefs: []
  type: TYPE_IMG
- en: The previous screenshots are helpful to determine what the number of variables
    or principal components are that explain an important part of the total variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, these components could be used for modeling instead of using
    the full list of variables. It is interesting to plot the cumulative explained
    variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41c891c7-457f-4e7c-b943-1d69f6347675.png)'
  prefs: []
  type: TYPE_IMG
- en: According to the preceding screenshot, the first 20 components explain about
    60% of the total variance in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We could opt to use these 20 components to create our model. This approach is
    not commonly used in credit risk models, so we will not use these transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, it is important to assess what our dataset looks like. In the
    following screenshot, we show a graphical representation of our data using the
    first two components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we classify each bank in the graph in accordance with its corresponding
    target variable. This time, we use the `ggfortify` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This screenshot shows the classified graph of the failed and non-failed banks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5696ea9d-b8e8-42bf-82fd-b12d5d35a8d6.png)'
  prefs: []
  type: TYPE_IMG
- en: It is quite interesting to see only two components. Although these components
    only explain about 17% of total variance, failed and non-failed banks are to some
    extent differentiated between.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction technique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should consider that a principal component assumes the linear transformations
    of variables, but there are other non-linear dimensionality reduction techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'For me, one of the most interesting techniques is the t-SNE developed by Laurens
    van der Maaten, who says this:'
  prefs: []
  type: TYPE_NORMAL
- en: '"As a sanity check, try running PCA on your data to reduce it to two dimensions.
    If this also gives bad results, then maybe there is not very much nice structure
    in your data in the first place. If PCA works well but t-SNE doesn’t, I am fairly
    sure you did something wrong."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see an example of how t-SNE is applied on our dataset. As usual, it is
    recommended that you fix a `seed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to use the `Rtsne` package. This package contains the `Rtsne`
    function that performs the algorithm. The most important parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pca`: This establishes whether a principal component analysis is carried out
    before running t-SNE .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`perplexity`: This is a measure for information (defined as 2 to the power
    of the Shannon entropy). The `perplexity` parameter establishes the number of
    the nearest neighbors in each observation. This parameter is useful to the algorithm,
    as it enables it to find a balance between local and global relations in the observations
    in your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code to run the algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: It takes a few minutes for this to finish running. Additional information on
    how the algorithm works is also included in the package documentation and its
    references.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the complete dataset is reduced to only two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot our train dataset according to its vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef4fb2f9-bb1f-4913-b9c5-af0e5a1b1a01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s plot it again, giving a color to each target value and failed and
    non-failed banks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/def33c8a-bb4c-40a3-a0fa-adfe8b72603b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that many failed banks are placed in the same part of the resulting
    bi-dimensional map. Nevertheless, one of the main weaknesses of t-SNE is the black-box
    type nature of the algorithm. It is not possible to make inferences on additional
    data based on the results, which does not happen using PCA.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE is mainly used for exploratory data analysis and it is also used as an
    input for clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In this real case, and trying to be accurate with analysis and processes used
    in credit risk, we will ignore the results of PCA and t-SNE, and we will continue
    with our original dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have selected the most predictive variables, we will try to combine
    them using different algorithms. The aim is to develop a model with the highest
    accuracy to predict the future insolvency of banks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before continuing, let’s save the workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how univariate analysis reduced the sample space of
    our problem data and analyzed the data. Consequently, in the next chapter, we
    will see how these variables can be combined to obtain an accurate model, where
    several algorithms will be tested.
  prefs: []
  type: TYPE_NORMAL
