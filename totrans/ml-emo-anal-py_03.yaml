- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labeling Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial Intelligence** (**AI**) models are only as good as the data they
    are trained with. Hence good, high-quality data is vitally important.'
  prefs: []
  type: TYPE_NORMAL
- en: AI algorithms generally start in a basic, simplified, form. In supervised learning,
    accurately labeling (also known as annotating) data is a vitally important step
    to train an algorithm, improve its predictions, and ensure that what it learns
    is right. Numerous studies, reports, and surveys show that data scientists spend
    anywhere between 50-80% of their time doing data preparation and preprocessing
    (see *Figure 3**.1*) – and data labeling is usually a huge part of this.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Distribution of time allocated to machine learning tasks](img/B18714_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Distribution of time allocated to machine learning tasks
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn why it is important to ensure that data is labeled
    correctly; how this can be achieved; how to assess whether it has indeed been
    achieved; and in particular, how to identify annotators who have not carried out
    the task to the required standard.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why labeling must be high quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The labeling process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to label data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining what the best labeling option is for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gold tweets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Competency tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Annotation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding whether to buy or build an annotation solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results from the example scenario
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inter-annotator reliability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krippendorff’s alpha
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debriefing after annotation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we investigate in depth how to do labeling, let us think about why maximum
    effort must be put into ensuring that labels are high quality.
  prefs: []
  type: TYPE_NORMAL
- en: Why labeling must be high quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of labeling data is the process of preparing a dataset such that
    algorithms can learn to recognize patterns that repeat in the data. The idea is
    that once the algorithm has seen enough data labeled in a certain way, it should
    be able to identify such patterns in data that it has not seen previously (known
    as unlabeled data).
  prefs: []
  type: TYPE_NORMAL
- en: Remember, data labeling facilitates the algorithms that power AI to build accurate
    models. With more and more businesses getting interested in AI to help gain insights
    and make predictions, it is no surprise that data labeling is a huge market worth
    billions. It is no exaggeration to say that without data labeling, it is impossible
    to build AI models.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, **Explainable AI** (**XAI**) is more important than ever. Stakeholders
    want to know why a model came to the decision that it did. In turn, this also
    helps build confidence in the model. Part of this is the data labeling stage,
    hence it is vital to ensure that this process is subject to the same rigorous
    checks and balances as other parts of the process; even minor errors can have
    significant consequences upstream that can be problematic for model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to understand that data labeling is not a one-off process.
    Instead, it is a continuous process of building and refinement.
  prefs: []
  type: TYPE_NORMAL
- en: The labeling process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of data labeling usually begins by getting humans to use their domain
    expertise or knowledge, intelligence, sense, and perception to make a decision
    about data that is unlabeled.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, labeled data consists of data that is unlabeled along with a tag,
    label, or name corresponding to some feature found in the data. In **natural language
    processing** (**NLP**), annotators (also known as labelers) will mentally identify
    important aspects of the text and use these to create labels. For example, to
    ascertain the emotion of a text, annotators would perhaps combine an approach
    that looked for key indicators of emotion (e.g., *happy*, *amazed*) with an understanding
    of the context and an exploration of the emotional undertones of the words themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Other types of data labeling include images and audio. When labeling a dataset
    containing images, each image might be labeled as a fruit (e.g., `apple`, `banana`,
    `pear`), or the labeling process may be as simple as asking the annotator to tag
    images as `true` if they contain an apple, or `false` otherwise. When annotators
    are required to consider audio (e.g., speech or other sounds), the annotation
    process can be quite complex, since the labels may have to be attached to segments
    of the input or otherwise time-stamped, which may require sophisticated annotation
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: Although data labeling can be performed by anyone with suitable training, there
    are specialists who are experts in the art of data labeling, ensuring that labeling
    is standardized, robust, and accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'When labeling data, it is a good idea to consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The goal is high-quality data labels. This may entail navigating problems such
    as poorly trained or poorly equipped personnel, inadequate methods and processes,
    or unclear instructions being provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For most AI problems that involve training a model from scratch, non-trivial
    amounts of training data are required. Furthermore, this may not be a one-time
    process. Consequently, keeping scaling in mind when putting together processes,
    training, and other requirements is essential. This then allows the labeling capacity
    to be increased with minimal effort, in a streamlined way, and with minimum disruption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost may also be a factor as labeling can end up being a costly process, and
    to make things worse, this may not always be apparent. For example, paying highly-qualified
    staff to preprocess or clean data using laborious, tedious, and repetitive processes
    is clearly not an efficient use of resources. It might be beneficial to employ
    data labeling specialists under such circumstances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality assurance** (**QA**) must also be considered. The quality and accuracy
    of labeling are directly proportional to the accuracy and care taken when labeling.
    There are various algorithms and techniques that can be used to obtain a measure
    of annotation accuracy. For example, when multiple annotators are used, Fleiss’
    kappa is a statistical measure that can be used to determine the level of agreement
    between the annotators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Typically, the data labeling process consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collection**: Data is collected (e.g., tweets, images, files, etc.), and
    although there is no perfect amount needed, typically, this is a large amount
    as it guarantees to cover the distribution of data better and give better results.
    Note that for fine-tuning the model, smaller amounts may be sufficient.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tagging**: This involves humans identifying objects or aspects of the data,
    typically using a data labeling tool.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**QA**: There is little point in laboriously labeling data if it is not accurate
    and does not inform the model as intended. Consequently, QA processes must be
    in place to assess the accuracy of the labeled data. Without these, it is likely
    the model will not perform adequately.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we finish this section, a word on what to do if there isn’t enough data.
    There are a number of methods that can help here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Augmentation**: This involves creating new data based on the existing data.
    For example, for images, techniques such as horizontal and vertical shifting/flipping,
    random rotation, and random zoom can be used to generate *new* images with different
    resolutions that are cropped, rotated, or zoomed. As an added benefit, this also
    exposes the model to this sort of image and helps it to become more robust.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synthesis**: This involves creating new instances using techniques such as
    **Synthetic Minority Oversampling TEchnique** (**SMOTE**) or **generative adversarial
    networks** (**GANs**) and is typically used when there are not enough examples
    of a minority class for a model to be able to learn the decision boundary. SMOTE
    works by selecting examples that are near to each other in the feature space,
    drawing a line between these, and then creating new samples at points along the
    line. GANs are deep learning models that generate data that impersonates the real
    data by using a method in which two neural networks work against each other: a
    generator, to generate the synthetic data, and a discriminator, which attempts
    to distinguish between real and synthetic data. Both these techniques are risky
    since they rely on assumptions about the data that may not be true, and hence
    they should only be used when very little data is available. Furthermore, any
    biases present in GAN datasets will also be inherited by the model when it generates
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: This involves penalizing data points that are somehow deemed
    less important, hence giving more weight to the more important data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that even with these methods, it is always preferable
    to use well-curated data that covers all cases and is well-distributed.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at some best practices that can make the process run more smoothly
    and with fewer errors.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data labeling is a tedious, but necessary, process. The last thing you want
    to do is have to repeat the process due to some misunderstanding. That is why
    following good practice can improve the effectiveness and accuracy of the process.
    The following are some of the things you can do to ensure that your data is labeled
    efficiently and correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use the right software**: You could just use a spreadsheet, but why not make
    things easier for yourself? There are lots of commercial products available that
    will speed up the process. If you have the right programming skills, it’s also
    worth considering fashioning your own solution. It doesn’t need to be commercial
    -grade – just good enough to do the job. Some of the things you should consider
    are as follows (we talk about these in depth later in this chapter):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide logins for annotators, so they can log back in later and carry on where
    they left off. Labeling is a tedious, mundane task. Logins allow users to log
    off, take a break, and come back (e.g., the next day) refreshed.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Annotators are typically paid to do the task. It is a good idea to intersperse
    **gold standard** data points throughout the task to check that the annotator
    isn’t just randomly selecting an option. If an annotator provides labels that
    fail to match the gold standard points, it is likely that they are not paying
    proper attention to the task. In that case, it may be worth looking in more detail
    at a larger sample of their annotations and, if necessary, eliminating their contributions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If your annotators are volunteers, they may get fed up long before the end.
    In a dataset of 200 items (for example), what you don’t want is that 5 annotators
    (for example) all label data items from 1-100, and no one managed to get to 101-200\.
    To mitigate this probability, you should develop your solution so that annotators
    work on different parts of the dataset. This also helps in the learning process
    because the models won’t learn anything from seeing the same example numerous
    times.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measure model performance**: Research has shown that beyond a certain point,
    throwing more data at a model yields little benefit. Labeling is a time-consuming
    and costly exercise, hence it makes sense to know when to stop, or at least consider
    how to extract further improvements from the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Organize**: Sometimes, you might even want multiple annotators working on
    the same data. It stands to reason that the more annotators you have, the quicker
    you can get the job done. However, you should organize the task so that different
    sections of the dataset are labeled but there should also be some overlap so that
    annotator agreement can be measured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provide clear instructions**: Producing a document or a web page or some
    other written means of getting your instructions across to your annotators in
    a clear, consistent way will help in improving the accuracy of labeling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measure agreement**: Ideally, to ensure high-quality labeling, there should
    be a sensible level of agreement between annotators. This can be ascertained by
    making use of one of the inter-annotator reliability metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following these practices will ensure a streamlined, error-free process that
    provides good-quality labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Where multiple annotators consistently disagree on the label, it is very likely
    that the data point is not suitable for training and should be discarded from
    the dataset. Furthermore, not all data points gathered need to be labeled if the
    requirement is high-quality labels. For example, a noisy or blurry image might
    lead different annotators to see the image differently.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen some best practices, we are ready to think about how to
    actually do the labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Labeling the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many ways to get data labeled, each with its own pros and cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Internal (in-house) labeling**: This is when experts from within an organization
    are used to label data. These are usually people who are domain experts and hence
    are very familiar with the process and requirements. Consequently, this leads
    to better quality control and high-quality labeling. Furthermore, as the data
    doesn’t need to leave the building, there are fewer associated security risks.
    However, internal labeling is not always possible (e.g., the company size is small
    or there is a lot of data to label). Furthermore, domain experts are expensive
    people so asking them to spend inordinate amounts of time on menial annotation
    tasks is probably not the best use of resources!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External (outsourced) labeling**: As the name suggests, this is when the
    job is outsourced to companies that specialize in data labeling. These companies
    are experts at data labeling, and consequently, the process is much smoother,
    quicker, and often, much cheaper as well. However, the danger here is that although
    they may be experts at data labeling, they likely will not be experts in the specific
    domain, and this increases the risk of error. Furthermore, there are also increased
    security risks as data will have to be made available to them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Crowdsourcing**: This is the process of turning to a group of people, sourced
    online, to help with any task. Typically, these people are freelancers and can
    be paid or unpaid. Many of the world’s biggest companies (e.g., McDonald''s) have
    used crowdsourcing for tasks such as research, logo design, and more. It is a
    good option for organizations that are unable to implement internal labeling and
    cannot find a suitable external labeling partner. One of the most popular ways
    to approach crowdsourcing is by using Amazon’s **Mechanical Turk** (**MTurk**).
    **MTurk** is a crowdsourcing marketplace that makes it easier for individuals
    and businesses to connect and hence allows businesses to outsource tasks to a
    workforce that then performs these tasks virtually. This workforce can work in
    collaboration with an in-house team or even alone. It is an extremely efficient
    way to get the job done but does suffer from the fact that quality is not guaranteed,
    there are associated data security risks, and there is a bigger associated management
    overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly, internal labeling is the go-to option, if it is viable. The problem
    here is that some companies just don’t have the resources to do this. Consequently,
    any of the other options can be considered but each has its pros and cons – for
    example, crowdsourcing requires much more quality control than the other methods.
    All things considered, it is probably sensible to either crowdsource (and put
    up with the extra management that will be required) or employ an external labeling
    service, which, in any case, is not as expensive as you might think.
  prefs: []
  type: TYPE_NORMAL
- en: The next sections consider and describe how, for a previous task, we manually
    labeled a dataset by sourcing annotators, building a UI, and including checks
    and balances to ensure that the labeling was fit for purpose and as accurate as
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: Gold tweets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is a sensible idea to intersperse a number of special tweets within the dataset
    that is to be labeled. These should be a mixture of different types of tweets.
    These tweets, referred to as **gold tweets**, were annotated internally beforehand
    (by the authors) and were interspersed within the dataset. They are important
    as they can be used as a mechanism to avoid malicious or incompetent annotations.
    For example, if an annotator’s gold tweet results are below some threshold (e.g.,
    70%), it can be assumed that they either didn’t understand the task, or were demotivated
    to utilize the deeper thinking required on such tasks, and hence their annotations
    should probably be discarded. Essentially, this is a mechanism that avoids malicious
    annotations. Furthermore, to ensure that the results are valid, a metric such
    as Fleiss’ kappa or Krippendorff’s alpha (we will explain these later in this
    chapter) should be used to ensure that there are no problems with rater agreement.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s next look at another way we can determine whether an individual is of
    the required level to undertake our annotation task.
  prefs: []
  type: TYPE_NORMAL
- en: The competency task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When opting for internal, external, or crowdsource approaches, it is always
    a good idea to implement some initial competency tasks, or tests, to assess the
    suitability of the annotators. For example, if you are asking an annotator to
    label Arabic tweets, not only must they understand Arabic but they must also be
    familiar with the nuances of the language and the intricacies of the language
    used on X (formerly known as Twitter).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the labeling exercise should be manageable for the annotators,
    and the level of annotator attention kept high, hence the dataset should be constructed
    with (for example) a mixture of challenging and easy tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We will now describe a robust scenario for labeling. In this example, the objective
    is to label tweets; however, the ideas and methodologies can be adapted for any
    type of data (e.g., images). Consider the following scenario. The task is to label
    1,000 Arabic tweets, hence a request for native Arabic speakers is placed on a
    crowdsourcing website to examine 1,000 tweets and classify them for emotion.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although the task itself was to label Arabic tweets, for ease of reading, the
    screenshots presented in this section have been translated into English.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task itself could be done using Excel, an off-the-shelf software solution,
    or even a custom solution developed in-house. A payment of $100 is offered to
    annotators, but it is made clear that this payment will only be made after the
    annotations are completed. Each of the annotators is asked to perform the same
    task: namely, annotate 1,000 tweets for the same number of emotions.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class or multi-label?
  prefs: []
  type: TYPE_NORMAL
- en: The terms *multi-class* classification and *multi-label* classification describe
    two different problems.
  prefs: []
  type: TYPE_NORMAL
- en: When instances can only be labeled with one from three or more classes, this
    is known as multi-class classification. For example, when classifying images of
    fruits into `[Apple, Banana, Orange]`, each image can belong to one, and only
    one, category.
  prefs: []
  type: TYPE_NORMAL
- en: However, where multiple labels can be predicted for each instance, this is known
    as multi-label classification. For example, when labeling a text with one of `[Education,
    Politics, Religion, Sports]`, the text might be about all of these or none of
    these.
  prefs: []
  type: TYPE_NORMAL
- en: The scenario presented in this chapter addresses the problem of multi-label
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'One tweet at a time was presented to the annotators and two questions were
    asked. The first was a single-answer multiple- choice question:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Q1\. Which of the following options best describes the emotional state of*
    *the tweeter?*'
  prefs: []
  type: TYPE_NORMAL
- en: anger (including annoyance, rage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: anticipation (including interest, vigilance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: disgust (including disinterest, dislike, loathing)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fear (including apprehension, anxiety, terror)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: joy (including serenity, ecstasy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: love (including affection)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: optimism (including hopefulness, confidence)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pessimism (including cynicism, no confidence)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sadness (including pensiveness, grief)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: surprise (including distraction, amazement)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: trust (including acceptance, liking, admiration)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: neutral or no emotion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second question was a checkbox question, where more than one answer could
    be selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Q2\. In addition to your response to Q1, which of the following options further
    describe the emotional state of* *the tweeter?*'
  prefs: []
  type: TYPE_NORMAL
- en: This question included the same choices as Q1, but *neutral or no emotion* was
    replaced with *none of* *the above*.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, it is not unusual to receive a huge number of responses. How
    are you to decide whom to select for the task? It is a good idea to gauge the
    competence levels of the applicants who expressed an interest by asking them to
    complete a short, online, emotion classification exercise consisting of a small
    number of tweets that are representative of the overall task, and ideally sourced
    from the same dataset as the task tweets. To perform the test fairly and consistently,
    applicants should be provided with clear instructions, as seen in *Figure 3**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Instructions for tweet annotators (source: Classification of
    tweets using multiple thresholds with self-correction and weighted conditional
    probabilities, Ahmad and Ramsay, 2020)](img/B18714_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2 – Instructions for tweet annotators (source: Classification of tweets
    using multiple thresholds with self-correction and weighted conditional probabilities,
    Ahmad and Ramsay, 2020)'
  prefs: []
  type: TYPE_NORMAL
- en: It is also sensible to provide the annotators with examples, as seen in *Figure
    3**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Example annotations for annotators (source: Classification of
    tweets using multiple thresholds with self-correction and weighted conditional
    probabilities, Ahmad and Ramsay, 2020)](img/B18714_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3 – Example annotations for annotators (source: Classification of
    tweets using multiple thresholds with self-correction and weighted conditional
    probabilities, Ahmad and Ramsay, 2020)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this scenario, applicants are asked to examine each tweet and select one
    primary emotion and as many secondary emotions as appropriate (we’ll explain why
    we use this setup shortly). It is important to understand that the annotators
    do not possess the same domain knowledge as you; consequently, the instructions
    need to be clear, concise, unambiguous, and easily understood. The instructions
    should clearly describe what you want the annotators to do and contain ample examples.
    For example, in the emotion annotation task, it is a good idea to include the
    different variations of an emotion (e.g., for anger: annoyance and rage). It is
    also usually a good idea to conduct an iterative review of the instructions to
    ensure that any issues are identified before publication. Although instructions
    can be published in any format (Word, PDF, etc.), HTML is a good choice because
    it allows changes to be made without having the overhead of having to redistribute
    copies of the instructions. However, it is a good idea to use version numbering
    and source code management (e.g., Git). Indeed, it may also be sensible to link
    instruction versions with versions of your build.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instructions are also required because labeling tweets is generally a subjective
    task, open to the labeler’s understanding, experiences, background, and interpretation
    of the text with no single point of truth. Indeed, the same annotator may give
    different answers if asked to reannotate the tweet in the future. The following
    tweet is an example where there is no, one, definitive answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*This is good airline service but comes with high tariff. They do not provide
    good service. Best part of this airline is they are* *always available*.'
  prefs: []
  type: TYPE_NORMAL
- en: In this situation, the instructions need to provide clear guidance as to how
    to understand the item, and how to label it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The job of the annotator is not easy. The annotator has to reflect, understand,
    and then select the label, or labels, that will ultimately form the inputs to
    the machine learning model. On the face of it, this is not a difficult task, with
    no specific training or knowledge required, and it can be tackled increasingly
    as an independent resource working from home. However, having to do the same thing
    repeatedly, while at the same time maintaining a level of accuracy and consistency,
    is not straightforward. However, there is no magic wand that can be waved, and
    the expertise of the annotators is as important as any other part of the solution.
    Some of the skills that an annotator is expected to have include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Concentration**: An annotator should have the ability to concentrate on what’s
    on the screen for long periods of time, without becoming side-tracked, and without
    making mistakes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Paying attention to detail**: Getting annotations wrong repeatedly could
    end up in a biased or an underperforming model, which could have other consequences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Working alone**: Annotating can involve long periods of working alone, which
    might be feasible for some, but not for others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understanding the language**: Things such as sarcasm and humor can be hard
    to detect, hence these require a proper level of understanding of the language
    and an advanced level of cognitive process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, the annotator will have to achieve a consistent level of commitment,
    concentration, and care in order to do a good job. However, it must also be understood
    that it is impossible to entirely remove human error, no matter how good the procedures
    and instructions are. It should also be noted that the task is intrinsically subjective,
    and two annotators can sometimes assign different labels to the same tweet without
    either of them being wrong.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Tweet annotation page](img/B18714_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Tweet annotation page
  prefs: []
  type: TYPE_NORMAL
- en: Both the tweets and the emotion answers were sourced from the complete dataset
    that we wish to label. Since these tweets are carefully selected specifically
    for the purpose of the test, the answers are clear, well-defined, and known. Hence
    it is relatively straightforward to select whatever number of annotators is required,
    and who scored the highest.
  prefs: []
  type: TYPE_NORMAL
- en: Having selected the annotators to move forward with, it is now time to move
    on to the task of labeling the proper dataset itself.
  prefs: []
  type: TYPE_NORMAL
- en: The annotation task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The same annotation platform as used for the competence task can also be repurposed
    for the annotation task. However, in the competency task, the tweets were presented
    in sequence, whereas in the annotation task, some clever logic is used. Without
    going into too much detail, it is a good idea to use a database such as Access
    or MySQL. This allows questions to be partitioned, answers to be tracked, and
    the results to be analyzed. This also enables the ability to determine how much
    of the dataset has been labeled, which tweets have been labeled, and, importantly,
    the distribution of those answers (we’ll explain why this is important shortly).
  prefs: []
  type: TYPE_NORMAL
- en: A schema describes how data will be organized and connected and includes tables,
    relationships, and other elements. *Figure 3**.5* shows a simple sample database
    schema.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Database schema diagram](img/B18714_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Database schema diagram
  prefs: []
  type: TYPE_NORMAL
- en: The `USERS` table contains a record for each person who will be labeling. Note
    that the `disabled` column can be used to bar access to the system – for example,
    if a user has failed the competency test. Note also the `LOGINS` table. This simply
    logs the date and time each time a user successfully logs in to the system. This
    can be used to obtain useful information such as the time the user did the labeling,
    and how many sittings were required to do the labeling. In turn, this could be
    useful to pinpoint why (for example) they didn’t do a good job. For example, if
    the `LOGINS` table showed that they were logging in daily late in the evening
    when people are generally tired, this might be a reason for a poor labeling score
    (this is entirely speculative, of course, as in today’s modern world, many people
    prefer working in the afternoons or evenings).
  prefs: []
  type: TYPE_NORMAL
- en: The `DATA_SOURCES` table can be used to group tweets by function (e.g., `Competency`,
    `Live`, and `Test`). The `ITEMS` table then contains items that require labeling
    with a foreign key back to the data source they belong to. Finally, the `ANSWERS`
    table is self-explanatory, linking each item to a user and the answers that were
    supplied.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of labeling 11 emotions, we simply created 11 columns – one
    for each emotion. However, for a truly generic, customizable solution, the labels
    should be stored in a separate table and another table used to link them to the
    items. Furthermore, the number and type of answer expected (single, multiple,
    etc.) should also be configurable.
  prefs: []
  type: TYPE_NORMAL
- en: Although there was payment involved, respondents were free to annotate as many
    tweets as they wished. However, the ideal scenario was that even if each of the
    participants only did some of the task, the overall labeled dataset would be distributed
    across the dataset. In other words, if each of the 5 annotators only labeled 200
    tweets each, the totality should be across the whole dataset (see *Figure 3**.6*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Distributed tweet labeling](img/B18714_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Distributed tweet labeling
  prefs: []
  type: TYPE_NORMAL
- en: 'We can ensure this by making use of some clever programming. Each tweet in
    the database has a unique ID (`item_id`). This is also stored in the `ANSWERS`
    table when an answer is submitted, along with the users ID. In this way, we can
    keep track of which questions have been answered and by whom. Recall that each
    question has to be answered by every annotator. It is then a simple task to fashion
    some SQL, as follows, that finds the question that has been answered the least
    (i.e., has the least number of rows in the `ANSWERS` table):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is done at the beginning when the web page is loading and the system is
    deciding which question to display. Although there are situations where this might
    be useful, this mechanism does not guarantee that each annotator will see every
    tweet; however, it does ensure that by the time every annotator reaches the end
    of the dataset, every item in the dataset will have been annotated. Clearly, this
    is not what is required as it is unhelpful in obtaining measurements on annotator
    agreement. A more useful modification to this technique would be to look for the
    next tweet that needs to be annotated by the user. This then allows users to log
    off and carry on where they left off when they next log in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Validation mechanisms were added to the web page to prevent *lazy* annotation,
    so that it was not possible to move to the next tweet until an answer had been
    selected for *Q1* and at least one answer was selected for *Q2*. In conjunction
    with the *neutral* and *none* options, the idea was that, in theory, this would
    prevent users from simply clicking the **Submit** button without considering the
    tweet (i.e., even if the user didn’t think any of the emotions applied, they were
    still required to register that fact). These options forced the user to proactively
    indicate that there were no emotions in the tweet, the idea being that if they
    were forced to select an option to proceed to the next tweet, then they may as
    well select something relevant as opposed to choosing at random. Users were also
    given four weeks to complete the task, hence there was no requirement to rush
    to complete the task. To facilitate this, the system allowed them to stop and
    come back later to resume from where they left off.
  prefs: []
  type: TYPE_NORMAL
- en: The same interface as the one used for the competency task was used to annotate
    1,000 tweets. One tweet was displayed at a time, as can be seen in *Figure 3**.4*.
    Following the guidelines from Mohammad and Kiritchenko, the annotator was asked
    to answer two questions. The first was a single-answer multiple- choice question
    that asked the user to select one option that best describes the emotional state
    of the tweeter from the emotions. The second was a checkbox question, where multiple
    answers could be selected.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, we described how the applicants were asked to examine each tweet and
    select one primary emotion and as many secondary emotions as appropriate. This
    is the technique described by Mohammad and Kiritchenko, reasoning that they wanted
    to include all emotions, however subtle, that applied in the answer, not just
    the primary emotion. They argue that one of the criticisms of natural language
    annotation is that only the instances with high agreement are kept, and low agreement
    instances are discarded. The high agreement instances typically tend to be simple
    examples of the emotion class, and hence are easier to model. However, in the
    real world, there are hugely more complex and complicated examples and usages
    of language to elicit emotion. If the model has been trained largely on high agreement
    instances, it then performs sub-optimally when it has to process instances that
    it has not seen during training.
  prefs: []
  type: TYPE_NORMAL
- en: Given the amalgamated results from *Q1* and *Q2*, a primary emotion was established
    by a majority vote. Where there was a tie, all tied emotions were deemed as primary
    emotions. The aggregated responses from *Q1* and *Q2* were used to obtain a full
    set of labels for a tweet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two criteria were applied to these aggregated responses:'
  prefs: []
  type: TYPE_NORMAL
- en: If at least half of the annotators indicated that a certain emotion applied,
    then that label was chosen
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no emotion was indicated by at least half of the annotators and more than
    half of the responses indicated that the tweet was neutral, then the tweet was
    marked as neutral
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A typical trend is that the highest-scoring secondary emotions are primary emotions
    such as anger and fear. These usually correspond well with the highest-scoring
    primary emotions. In general, the highest-scoring primary emotions are also the
    highest-scoring secondary emotions, and the lowest-scoring primary emotions are
    the lowest-scoring secondary emotions. However, there may be anomalies in emotions
    such as trust, where it may be a high-ranking primary emotion but a low-ranking
    secondary emotion, indicating that in the presence of other emotions, the usefulness
    of such emotions is limited.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, there are some obvious limitations to this method. The participants
    were self-selected, and no information was available regarding the conditions
    (e.g., whether the participant completed the survey under strict conditions conducive
    to clear thinking) under which the exercise was completed. It is also possible
    that some annotators perhaps did not fully understand the instructions, but declined
    to say so. Hence, even after the competency test, it is likely that there may
    still be invalid annotations. Annotating tweets in this manner is clearly a monotonous,
    time-consuming activity. However, the annotators were paid, hence they should
    be highly motivated to annotate to a high standard. Furthermore, a key advantage
    of datasets constructed as this one was, is that the tweets were not gathered
    by making use of emotion-bearing keywords; hence the dataset should be more representative
    of the types of tweets seen on X (formerly known as Twitter).
  prefs: []
  type: TYPE_NORMAL
- en: Buy or build?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this section, you will need the following prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: Software development tools such as VS Code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database tools such as MySQL Workbench or SQL Server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a web browser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A version control system such as Git or GitHub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is certainly a debate around whether you should develop your own interface,
    or buy a ready-made one off the shelf. Naturally, being programmers at heart,
    our initial inclination is always to go for the former! However, this may not
    always be possible with company size, work priorities, and cost among the factors
    having to be considered. Clearly, for certain situations, it is preferable to
    use one of the ready-made tools. Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tool** | **Data Type** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon SageMaker Ground Truth | Text, image, and video |'
  prefs: []
  type: TYPE_TB
- en: '| Label Studio | Text, image, video, audio, and more |'
  prefs: []
  type: TYPE_TB
- en: '| Sloth | Image and video |'
  prefs: []
  type: TYPE_TB
- en: '| Dataturk | Text, image, and video |'
  prefs: []
  type: TYPE_TB
- en: '| Superannotate | Image |'
  prefs: []
  type: TYPE_TB
- en: '| Audio-annotator | Audio |'
  prefs: []
  type: TYPE_TB
- en: 'Building an annotation interface is not a straightforward task; it requires
    a number of different features, but with a little planning, it can be a smooth
    process. The following are some of the features that should be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: The option to view any associated metadata of the item (e.g., for images, the
    dimensions or the date and time it was taken), as well as the item itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A mobile-friendly interface because not everyone will access it from a desktop
    or a laptop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Administrator login to access stats and view current progress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard reports, and the ability to create custom reports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to easily add new sets of items for labeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to finalize item sets so that no more annotations are allowed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to add gold tweets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to import items for labeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to export results so they can be shared
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, the data that requires labeling will be collected on a different
    platform and also processed via machine learning algorithms on a different platform.
    Consequently, the data will need to be imported and exported from the labeling
    platform in an appropriate format. Ideally, these processes should be part of
    an automated pipeline, so that manual bottlenecks are not created. It is important
    to note that these non-labeling processes will remain the same, regardless of
    whether an in-house or an external solution is used. There is also a possibility,
    if an external solution is used, that carefully-crafted interfaces that work well
    in a pipeline may suddenly stop working and require altering if the external supplier
    changes the way their solution interfaces with the outside world. Furthermore,
    although many external solutions are feature-rich, there may be something niche
    that isn’t available, and although it can be requested, there is no guarantee
    when it will be implemented, if at all. As a final benefit to convince you of
    the merits of developing an in-house solution, if the necessary engineers and
    time are available, bugs can also be fixed, tested, and deployed quicker.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may still be unclear whether to choose from the abundance of annotation
    tools available or to embark on building a solution from scratch. We''ll, therefore,
    finalize this discussion with a summary table to help you decide which route to
    take:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Situation** | **Decision** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| If this is a one-off task, and very unlikely to be repeated again | Buy |'
  prefs: []
  type: TYPE_TB
- en: '| If you need a quick turnaround | Buy |'
  prefs: []
  type: TYPE_TB
- en: '| If cost is not a concern | Buy |'
  prefs: []
  type: TYPE_TB
- en: '| If you have tight security requirements that dictate, for example, that the
    data should not leave the premises | Build/Buy |'
  prefs: []
  type: TYPE_TB
- en: '| If you know that this is a process that will be repeated often and especially
    if costs adding up are a concern | Build/Buy |'
  prefs: []
  type: TYPE_TB
- en: '| If you have a niche requirement that may be useful to others, and hence your
    platform could be sold as a SaaS model, for example | Build |'
  prefs: []
  type: TYPE_TB
- en: '| If your requirements are niche or bespoke and unlikely to be catered for
    by one of the commercial solutions | Build |'
  prefs: []
  type: TYPE_TB
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 3**.7* shows the overall results screen. This can be used to get an
    update on the current status of the labeling and how many each annotator has done,
    and can also be used to access the detailed analysis screen. These details were
    enough for our work, but this screen could be further enhanced with useful information
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How many times the annotator logged in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average response time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the annotator last logged in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly, analyzing such information can lead to further insights that may ultimately
    lead to better annotation and, hence, a better-performing model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Labeling status screen](img/B18714_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Labeling status screen
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3**.8* shows the results summary screen for an individual annotator.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Labeling results](img/B18714_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Labeling results
  prefs: []
  type: TYPE_NORMAL
- en: Each tweet is listed along with the selections made for the tweet and how many
    the annotator got correct. This allows administrators to examine, in detail, each
    tweet and the answers that the annotator supplied. When this screen is used to
    display the competency results (where the actual answers are known), the **Actual
    answersUser answers** column shows the actual answers and the responses, so that
    quick comparisons can be made, and the final **%Correct** column shows how many
    the annotator got correct.
  prefs: []
  type: TYPE_NORMAL
- en: Having collected the labeling results, it is a good idea to understand whether
    the answers are reliable. We look at this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Inter-annotator reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inter-annotator reliability is the widely used term to describe “*the extent
    to which independent coders evaluate the characteristic of a message, or artefact,
    and reach the same conclusion*” (Tinsley and D. J. Weiss). It is an important
    metric because it determines whether the data can be considered valid and is an
    indication of the trustworthiness of the data. Without this reliability, any content
    analysis is useless. From a practical point of view, establishing a high level
    of reliability also has the benefit of allowing the work to be divided among multiple
    annotators. Measuring reliability is actually measuring reproducibility, that
    is, *"*the *likelihood that different coders who receive the same training and
    textual guidance will assign the same value to the same piece of* *content*” (Joyce).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways to measure reliability – two of the most common are Fleiss’
    kappa and Krippendorff’s alpha:'
  prefs: []
  type: TYPE_NORMAL
- en: '`1` and chance agreement would equate to `0`. There is no generally agreed-upon
    measure of significance, although guidelines have been given; the commonly cited
    scale as described can be seen in *Table 3.1*. Fleiss’ kappa can only be used
    with binary (yes/no) or nominal-scale (gender, nationality, etc.) ratings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Kappa** | **Strength** **of agreement** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| <0 | Less than chance agreement |'
  prefs: []
  type: TYPE_TB
- en: '| 0.01–0.20 | Slight agreement |'
  prefs: []
  type: TYPE_TB
- en: '| 0.21– 0.40 | Fair agreement |'
  prefs: []
  type: TYPE_TB
- en: '| 0.41–0.60 | Moderate agreement |'
  prefs: []
  type: TYPE_TB
- en: '| 0.61–0.80 | Substantial agreement |'
  prefs: []
  type: TYPE_TB
- en: '| 0.81–0.99 | Almost perfect agreement |'
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – Interpretation of Fleiss’ kappa
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula to calculate Fleiss’ kappa is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math  display="block"><mrow><mfrac><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>−</mo><msub><mi>P</mi><mi>e</mi></msub></mrow><mrow><mn>1</mn><mo>−</mo><msub><mi>P</mi><mi>e</mi></msub></mrow></mfrac></mrow></math>](img/9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, P0 is the relative observed agreement among raters and Pe is the hypothetical
    probability of chance agreement.
  prefs: []
  type: TYPE_NORMAL
- en: '**Krippendorff’s alpha:** The problem with Fleiss’ kappa is that it requires
    the total number of answers for each item (e.g., a tweet) to be equal. This is
    clearly not possible when annotating tweets with multiple emotions because the
    number of emotions chosen for each tweet can vary. In these situations, Krippendorff’s
    alpha is used because it can cope with various sample sizes, categories, and numbers
    of annotators. It is also able to handle cases where data is missing – for example,
    when an annotator has left an answer blank. Krippendorff’s alpha is also more
    reliable than other measures because, in contrast to other measures that are based
    on agreement, Krippendorff’s alpha is a ratio that is based on the observed and
    the expected *disagreement*. This is one reason why it is believed to be more
    reliable. It has been reported that, in practice, the results from both Krippendorff’s
    alpha and Fleiss’ kappa are similar (Gwet). This is unsurprising since in cases
    where exactly one label is assigned, they compute the same thing, but only Krippendorff’s
    alpha works when a data point can have zero, one, or more labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The interpretation of Krippendorff’s alpha is also more straightforward than
    that of Fleiss’ kappa. The alpha value is a number ranging from `0` to `1`, where
    `0` is perfect disagreement and `1` is perfect agreement. Krippendorff suggests
    that, ideally, the value should be greater than or equal to `0.8`, but tentative
    conclusions are still acceptable when the value is greater than or equal to `0.667`
    (Klaus). However, this would be the lowest acceptable value. It should be noted
    that while Krippendorff’s alpha can measure the overall reliability of the annotation
    process, it does not inform on which annotators or which instances are problematic
    if the alpha is low. For that, further analysis may be conducted – for example,
    comparing the annotations of each pair of annotators, or looking at the instances
    where disagreement occurred.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula to calculate Krippendorff’s alpha is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math  display="block"><mrow><mrow><mn>1</mn><mo>−</mo><mfrac><msub><mi>D</mi><mn>0</mn></msub><msub><mi>D</mi><mi>e</mi></msub></mfrac></mrow></mrow></math>](img/10.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, D0 is the disagreement observed and De is the disagreement expected by
    chance.
  prefs: []
  type: TYPE_NORMAL
- en: That’s enough theory. Let’s work through an example using Krippendorff’s alpha.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating Krippendorff’s alpha
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that Fleiss’ kappa does not support multi-label input, hence Krippendorff’s
    alpha is computed for multi-label classification. In this section, a fictitious
    example, heavily based on Krippendorff’s paper, is set up to work through a simple
    Krippendorf’s alpha calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the scenario where two annotators, A1 and A2, are asked to provide
    10 (N) binary (yes/no) labels to the same set of two data points. The *reliability
    data matrix* then shows the combined responses from each annotator across both
    data points, where the rows are the annotators, and the columns are their labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| **A1** | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **A2** | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Using this matrix, a *coincidence matrix* is created that accounts for all values
    in the reliability data matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: These matrices are slightly different from the typical coincidence matrices
    found in AI and ML in that they are used to record and analyze relationships between
    two or more categorical variables and show items that appear together. The typical
    coincidence matrix used in ML presents a table showing the different outcomes
    of the prediction and results of a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reliability data matrix is hence used to create a 2X2 coincidence matrix
    using the following template:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | 0 | 1 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 000 | 001 | n0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 010 | 011 | n1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | n0 | n1 | 2 x N |'
  prefs: []
  type: TYPE_TB
- en: 'In the coincidence matrix, the values are entered twice – for example, once
    as (`0,1`) and once as (`1,0`). Hence, in the example, `1` is entered as a `0-1`
    pair of values and also as a `1-0` pair of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | 0 | 1 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 10 | 4 | 14 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 4 | 2 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 14 | 5 | 20 |'
  prefs: []
  type: TYPE_TB
- en: 'These values are derived as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0-0` pairs. However, each pair is represented twice, hence `10`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0-1` pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1-0` pairs are the same as the four `0-1` pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1-1` pair, hence `2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, it is a simple matter of using the formula and substituting the values
    to calculate the alpha:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math  display="block"><mrow><mrow><mi>α</mi><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><msub><mi>D</mi><mn>0</mn></msub><msub><mi>D</mi><mi>e</mi></msub></mfrac></mrow></mrow></math>](img/11.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math  display="block"><mrow><mrow><mi>α</mi><mo>=</mo><mn>1</mn><mo>−</mo><mfenced
    open="(" close=")"><mrow><mn>2</mn><mi>N</mi><mo>−</mo><mn>1</mn></mrow></mfenced><mfrac><msub><mn>0</mn><mn>01</mn></msub><mrow><msub><mi>n</mi><mn>0</mn></msub><mo>×</mo><msub><mi>n</mi><mn>1</mn></msub></mrow></mfrac></mrow></mrow></math>](img/12.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math   display="block"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>20</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mfrac><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>14</mml:mn><mml:mo>×</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.095</mml:mn></mml:math>](img/13.png)'
  prefs: []
  type: TYPE_IMG
- en: Luckily, we do not need to code this ourselves since the NLTK metrics package
    has functions to calculate inter-annotator agreement values. The `nltk.metrics.agreement`
    module requires its input to be in the form of a list of triples, where each triple
    contains a label to identify the annotator (e.g., `A1` and `A2`), an item to indicate
    the label (e.g., `1`, `2`, `3`, etc.) and the annotator label (e.g., `0`, `1`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The corresponding Python code for the preceding simple example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the same result as in the worked example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Debrief
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is a good idea to follow up with your annotators after the task to ascertain
    their views about the overall task and whether they experienced any particular
    difficulties with any emotions. For example, tweets may contain several different
    emotions and hence it may be difficult to pinpoint the prevalent ones. There may
    also have been occasions where emotive words were seen but the annotator did not
    feel that this led them to strongly lean toward any particular emotion. It is
    sensible to obtain an understanding of these types of situations as early as possible
    as changes to the procedures for collecting data are likely to change the nature
    of what is collected. Typically, annotators refer to tweets being short and informal
    as a primary reason for being unable to determine a definitive emotion for a tweet.
    It is, therefore, ironic that when tweets are overly lengthy, annotators mention
    that it is hard to restrict themselves to selecting a sensible set of emotions,
    as in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Who is responsible for this tampering? !!! At the University of Nora !! Meeting
    with His Excellency* *the #MinisterofEducation*'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no doubt that data annotation is a challenge, but with the right tools
    and techniques, these problems can be minimized and the process streamlined, resulting
    in a well-labeled dataset that is fit for purpose.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we started by understanding why labeling must be high quality,
    and what the consequences are of even minor errors. The data labeling process
    usually begins by getting humans to use their domain expertise, intelligence,
    sense, and perception to make a decision about data that is unlabeled. We explored
    the process and key considerations and discussed the options when there is not
    enough data available. Data labeling is a tedious but necessary process and is
    prone to errors by the annotators. It is thus important to improve its effectiveness
    and accuracy by identifying and then following good practices. We then discussed
    the various ways to label data and their pros and cons. A common technique is
    to crowdsource, hence we introduced techniques such as gold tweets and competency
    tasks to improve outcomes when following this technique. We also presented a scenario
    for labeling, discussed the labels that might be used and the skills an annotator
    needs, and presented a simple architecture and UI for the data labeling task.
    Finally, we presented the criteria for building or buying a labeling tool, worked
    through the theory behind inter-annotator reliability, and presented the corresponding
    Python code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at competition-based research and evaluation
    strategies, discover how it is not always necessary to create your own dataset,
    and explore how existing datasets can be transformed in ways to make them useful
    for the emotion analysis task.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: T.N. Ahmad and A. Ramsay. *Classification of Tweets using Multiple Thresholds
    with Self-Correction and Weighted Conditional* *Probabilities*. 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'S. M. Mohammad and S. Kiritchenko. *Understanding Emotions: A Dataset of Tweets
    to Study Interactions between Affect Categories*. In Proceedings of the 11th Edition
    of the Language Resources and Evaluation Conference, Miyazaki, Japan, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: H. E. Tinsley and D. J. Weiss. *Interrater reliability and agreement. In Handbook
    of applied multivariate statistics and mathematical modeling*, pages 95–124\.
    Elsevier, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'M. Joyce. *Picking the best intercoder reliability statistic for your digital
    activism content analysis*. In *Digital Activism Research Project: Investigating
    the Global Impact of Comment Forum Speech as a Mirror of Mainstream Discourse*,
    volume 243, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gwet, Kilem. (2015). *On Krippendorff’s* *Alpha Coefficient*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'K. Klaus. *Content analysis: An introduction to its* *methodology*, 1980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K. Krippendorff. *Computing Krippendorff’s* *alpha-reliability*. 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
