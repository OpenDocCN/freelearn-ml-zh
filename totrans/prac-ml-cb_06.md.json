["```py\n> install.packages(\"tree\")\n> install.packages(\"caret\")\n> install.packages(\"e1071\")\n> library(tree)\n> library(caret)\n\n```", "```py\n    > AHD_data <- read.csv(\"d:/Heart.csv\", header = TRUE)\n\n```", "```py\n> str(AHD_data) \n\n```", "```py\n    > head(AHD_data)\n\n```", "```py\n    >dim(AHD_data)\n\n```", "```py\n    > split <- createDataPartition(y=AHD_data$AHD, p = 0.5, list=FALSE)\n\n```", "```py\n    > split\n\n```", "```py\n    > train <- AHD_data[split,]\n\n```", "```py\n    > train\n\n```", "```py\n    > test <- AHD_data[-split,]\n\n```", "```py\n    > test\n\n```", "```py\n    > trees <- tree(AHD ~., train)\n\n```", "```py\n    > plot(trees)\n\n```", "```py\n    > cv.trees <- cv.tree(trees, FUN=prune.misclass)\n\n```", "```py\n    > cv.trees\n\n```", "```py\n    > plot(cv.trees)\n\n```", "```py\n    > prune.trees <- prune.misclass(trees, best=4)\n\n```", "```py\n    > plot(prune.trees)\n\n```", "```py\n    > text(prune.trees, pretty=0)\n\n```", "```py\n    > tree.pred <- predict(prune.trees, test, type='class')\n\n```", "```py\n    > tree.pred\n\n```", "```py\n    > confusionMatrix(tree.pred, test$AHD)\n\n```", "```py\n    > install.packages(\"tree\")\n\n```", "```py\n    > realEstate <- read.table(\"d:/RealEstate.txt\", header=TRUE)\n\n```", "```py\n    > dim(realEstate)\n\n```", "```py\n    > str(realEstate)\n\n```", "```py\n    > head(realEstate)\n\n```", "```py\n    > summary(realEstate)\n\n```", "```py\n> treeModel <- tree(log(MedianHouseValue) ~ Longitude + Latitude, data=realEstate) \n\n```", "```py\n    > summary(treeModel)\n\n```", "```py\n> plot(treeModel) \n\n```", "```py\n    > text(treeModel, cex=.75)\n\n```", "```py\n    > priceDeciles <- quantile(realEstate$MedianHouseValue, 0:10/10)\n\n```", "```py\n    > priceDeciles\n\n```", "```py\n    > summary(priceDeciles)\n\n```", "```py\n    > cutPrices <- cut(realEstate$MedianHouseValue, priceDeciles, include.lowest=TRUE)\n\n```", "```py\n    > head(cutPrices)\n\n```", "```py\n    > summary(cutPrices)\n\n```", "```py\n> plot(realEstate$Longitude, realEstate$Latitude, col=grey(10:2/11)[cutPrices], pch=20, xlab=\"Longitude\",ylab=\"Latitude\") \n\n```", "```py\n    > summary(realEstate$Longitude)\n\n```", "```py\n    > head(realEstate$Longitude)\n\n```", "```py\n    > summary(realEstate$Latitude)\n\n```", "```py\n    > head(realEstate$Latitude)\n\n```", "```py\n    > partition.tree(treeModel, ordvars=c(\"Longitude\",\"Latitude\"), add=TRUE)\n\n```", "```py\n    > treeModel2 <- tree(log(MedianHouseValue) ~ Longitude + Latitude, data=realEstate, mindev=0.001)\n\n```", "```py\n    > summary(treeModel2)\n\n```", "```py\n    > plot(treeModel2)\n\n```", "```py\n    > text(treeModel2, cex=.65)\n\n```", "```py\n    > treeModel3 <- tree(log(MedianHouseValue) ~ ., data=realEstate)\n\n```", "```py\n    > summary(treeModel3)\n\n```", "```py\n    > plot(treeModel3)\n\n```", "```py\n    > text(treeModel3, cex=.75)\n\n```", "```py\n> install.packages(\"quantmod\")\n> install.packages(\"rpart\")\n> install.packages(\"rpart.plot\")\n\n```", "```py\n> library(\"quantmod\")\n> library(\"rpart\")\n> library(\"rpart.plot\")\n\n```", "```py\n    > startDate = as.Date(\"2012-01-01\")\n\n```", "```py\n    > endDate = as.Date(\"2014-01-01\")\n\n```", "```py\n    > getSymbols(\"BAC\", env = .GlobalEnv,  src = \"yahoo\", from = startDate, to = endDate)\n\n```", "```py\n> relativeStrengthIndex3 <- RSI(Op(BAC), n= 3) \n\n```", "```py\n    > relativeStrengthIndex3\n\n```", "```py\n    > exponentialMovingAverage5 <- EMA(Op(BAC),n=5)\n\n```", "```py\n    > exponentialMovingAverage5\n\n```", "```py\n    > dim(exponentialMovingAverage5)\n\n```", "```py\n    > str(exponentialMovingAverage5)\n\n```", "```py\n    > exponentialMovingAverageDiff <- Op(BAC)-exponentialMovingAverage5\n\n```", "```py\n    > MACD <- MACD(Op(BAC),fast = 12, slow = 26, signal = 9)\n\n```", "```py\n    > MACD\n\n```", "```py\n    > head(MACD)\n\n```", "```py\n    > MACDsignal <- MACD[,2]\n\n```", "```py\n    > MACDsignal\n\n```", "```py\n    > stochasticOscillator <- SMI(Op(BAC),n=13,slow=25,fast=2,signal=9)\n\n```", "```py\n    > stochasticOscillator\n\n```", "```py\n    > stochasticOscillatorSignal <- stochasticOscillator[,1]\n\n```", "```py\n    > stochasticOscillatorSignal\n\n```", "```py\n    > PriceChange <- Cl(BAC) - Op(BAC)\n\n```", "```py\n    > PriceChange\n\n```", "```py\n    > binaryClassification <- ifelse(PriceChange>0,\"UP\",\"DOWN\")\n\n```", "```py\n    > binaryClassification\n\n```", "```py\n    > str(binaryClassification)\n\n```", "```py\n> AAPLDataSetNew >-\ndata.frame(weekDays,exponentialMovingAverageDiffRound,\nbinaryClassification) \n\n```", "```py\n    > DataSet\n\n```", "```py\n    > head(DataSet)\n\n```", "```py\n    > str(DataSet)\n\n```", "```py\n    > colnames(DataSet) <- c(\"relativeStrengthIndex3\", \"exponentialMovingAverageDiff\", \"MACDsignal\", \"stochasticOscillator\", \"binaryClassification\")\n\n```", "```py\n    > colnames(DataSet)\n\n```", "```py\n    > DataSet <- DataSet[-c(1:33),]\n\n```", "```py\n    > DataSet\n\n```", "```py\n    > head(DataSet)\n\n```", "```py\n    > str(DataSet)\n\n```", "```py\n    > dim(DataSet)\n\n```", "```py\n    > TrainingDataSet <- DataSet[1:312,]\n\n```", "```py\n    > TrainingDataSet\n\n```", "```py\n    > str(TrainingDataSet)\n\n```", "```py\n    > TestDataSet <- DataSet[313:469,]\n\n```", "```py\n    > TestDataSet\n\n```", "```py\n    > str(TestDataSet)\n\n```", "```py\n    > DecisionTree <- rpart(binaryClassification~relativeStrengthIndex3+exponentialMovingAverageDiff+MACDsignal+stochasticOscillator,data=TrainingDataSet, cp=.001)\n\n```", "```py\n    > prp(DecisionTree,type=2)\n\n```", "```py\n    > printcp(DecisionTree)\n\n```", "```py\n    > plotcp(DecisionTree,upper=\"splits\")\n\n```", "```py\n    > PrunedDecisionTree <- prune(DecisionTree,cp=0.041428)\n\n```", "```py\n    > prp(PrunedDecisionTree, type=4)\n\n```", "```py\n> table(predict(PrunedDecisionTree,TestDataSet), TestDataSet[,5],dnn=list('predicted','actual')) \n\n```", "```py\n    > install.packages(\"quantmod\")\n    > install.packages(\"lubridate\")\n    > install.packages(\"e1071\")\n\n```", "```py\n    > library(\"quantmod\")\n    > library(\"lubridate\")\n    > library(\"e1071\")\n\n```", "```py\n    > startDate = as.Date(\"2012-01-01\")\n\n```", "```py\n    > endDate = as.Date(\"2014-01-01\")\n\n```", "```py\n    > getSymbols(\"AAPL\", env = .GlobalEnv, src = \"yahoo\", from = startDate,  to = endDate)\n\n```", "```py\n    > weekDays <- wday(AAPL, label=TRUE)\n\n```", "```py\n    > head(weekDays)\n\n```", "```py\n    > changeInPrices <- Cl(AAPL) - Op(AAPL)\n\n```", "```py\n    > head(changeInPrices)\n\n```", "```py\n    > summary(changeInPrices)\n\n```", "```py\n    > dim(changeInPrices)\n\n```", "```py\n    > binaryClassification <- ifelse(changeInPrices>0,\"UP\",\"DOWN\")\n\n```", "```py\n    > binaryClassification\n\n```", "```py\n    > summary(binaryClassification)\n\n```", "```py\n    > AAPLDataSet <- data.frame(weekDays,binaryClassification)\n\n```", "```py\n    > AAPLDataSet\n\n```", "```py\n    > head(AAPLDataSet)\n\n```", "```py\n    > dim(AAPLDataSet)\n\n```", "```py\n    > NaiveBayesclassifier <- naiveBayes(AAPLDataSet[,1], AAPLDataSet[,2])\n\n```", "```py\n    > NaiveBayesclassifier\n\n```", "```py\n    > exponentialMovingAverage5 <- EMA(Op(AAPL),n = 5)\n\n```", "```py\n    > exponentialMovingAverage5\n\n```", "```py\n    > summary(exponentialMovingAverage5)\n\n```", "```py\n    > exponentialMovingAverage10 <- EMA(Op(AAPL),n = 10)\n\n```", "```py\n    > exponentialMovingAverage10\n\n```", "```py\n    > summary(exponentialMovingAverage10)\n\n```", "```py\n    > dim(exponentialMovingAverage10)\n\n```", "```py\n    > exponentialMovingAverageDiff <- exponentialMovingAverage5 - exponentialMovingAverage10\n\n```", "```py\n    > exponentialMovingAverageDiff\n\n```", "```py\n    > summary(exponentialMovingAverageDiff)\n\n```", "```py\n    > exponentialMovingAverageDiffRound <- round(exponentialMovingAverageDiff, 2)\n\n```", "```py\n    > summary(exponentialMovingAverageDiffRound)\n\n```", "```py\n> AAPLDataSetNew <- data.frame(weekDays,exponentialMovingAverageDiffRound, binaryClassification) \n\n```", "```py\n> AAPLDataSetNew \n\n```", "```py\n    > summary(AAPLDataSetNew)\n\n```", "```py\n    > AAPLDataSetNew <- AAPLDataSetNew[-c(1:10),]\n\n```", "```py\n> summary(AAPLDataSetNew) \n\n```", "```py\n    > dim(AAPLDataSetNew)\n\n```", "```py\n> trainingDataSet <- AAPLDataSetNew[1:328,] \n\n```", "```py\n    > dim(trainingDataSet)\n\n```", "```py\n    > summary(trainingDataSet)\n\n```", "```py\n    > TestDataSet <- AAPLDataSetNew[329:492,]\n\n```", "```py\n    > dim(TestDataSet)\n\n```", "```py\n    > summary(TestDataSet)\n\n```", "```py\n> exponentialMovingAverageDiffRoundModel <-\nnaiveBayes(trainingDataSet[,1:2],trainingDataSet[,3])\n\n```", "```py\n    > exponentialMovingAverageDiffRoundModel\n\n```", "```py\n    > table(predict(exponentialMovingAverageDiffRoundModel,TestDataSet),\nTestDataSet[,3],dnn=list('Predicted','Actual')) \n\n```", "```py\n> install.packages(\"quantmod\")\n> install.packages(\"randomForest\")\n> install.packages(\"Hmisc\")\n\n```", "```py\n> library(\"quantmod\")\n> library(\"randomForest\")\n> library(\"Hmisc\")\n\n```", "```py\n    > PoundDollar <- read.csv(\"d:/PoundDollar.csv\")\n\n```", "```py\n    > head(PoundDollar)\n\n```", "```py\n    > summary(PoundDollar)\n\n```", "```py\n    > dim(PoundDollar)\n\n```", "```py\n    > DateAndTime <- as.POSIXlt(PoundDollar[,2],format=\"%m/%d/%y %H:%M\")\n\n```", "```py\n    > HighLowClose <- PoundDollar[,4:6]\n\n```", "```py\n    > head(HighLowClose)\n\n```", "```py\n    > summary(HighLowClose)\n\n```", "```py\n    > str(HighLowClose)\n\n```", "```py\n> HighLowClosets <- data.frame(HighLowClose, row.names=DateAndTime) \n\n```", "```py\n    > describe(HighLowClosets)\n\n```", "```py\n    > HighLowClosexts <- as.xts(HighLowClosets)\n\n```", "```py\n    > BollingerBands <- BBands(HighLowClosexts,n=20,SMA,sd=2)\n\n```", "```py\n    > describe(BollingerBands)\n\n```", "```py\n    > Upper <- BollingerBands$up - HighLowClosexts$Close\n\n```", "```py\n    > summary(Upper)\n\n```", "```py\n    > Lower <- BollingerBands$dn - HighLowClosexts$Close\n\n```", "```py\n    > summary(Upper)\n\n```", "```py\n    > Middle <- BollingerBands$mavg - HighLowClosexts$Close\n\n```", "```py\n    > summary(Middle)\n\n```", "```py\n    > PercentageChngpctB <- Delt(BollingerBands$pctB,k=1)\n\n```", "```py\n    > describe(PercentageChngpctB)\n\n```", "```py\n    > PercentageChngUp <- Delt(Upper,k=1)\n\n```", "```py\n    > describe(PercentageChngUp)\n\n```", "```py\n    > PercentageChngLow <- Delt(Lower, k=1)\n\n```", "```py\n    > describe(PercentageChngLow)\n\n```", "```py\n    > PercentageChngMid <- Delt(Middle,k=1)\n\n```", "```py\n    > describe(PercentageChngMid)\n\n```", "```py\n    > Returns <- Delt(HighLowClosexts$Close, k=1)\n\n```", "```py\n> binaryClassification <- ifelse(Returns>0,\"Up\",\"Down\") \n\n```", "```py\n    > summary(binaryClassification)\n\n```", "```py\n    > ClassShifted <- binaryClassification[-1]\n\n```", "```py\n    > FeaturesCombined <- data.frame(Upper, Lower, Middle, BollingerBands$pctB, PercentageChngpctB, PercentageChngUp, PercentageChngLow, PercentageChngMid)\n\n```", "```py\n    > summary(FeaturesCombined)\n\n```", "```py\n    > FeaturesShifted <- FeaturesCombined[-5257,]\n\n```", "```py\n    > FeaturesClassData <- data.frame(FeaturesShifted, ClassShifted)\n\n```", "```py\n    > summary(FeaturesClassData)\n\n```", "```py\n    > FinalModelData <- FeaturesClassData[-c(1:20),]\n\n```", "```py\n    > colnames(FinalModelData) <- c(\"pctB\",\"LowDiff\",\"UpDiff\",\"MidDiff\",\"PercentageChngpctB\",\"PercentageChngUp\",\"PercentageChngLow\",\"PercentageChngMid\",\"binaryClassification\")\n\n```", "```py\n    > str(FinalModelData)\n\n```", "```py\n    > set.seed(1)\n\n```", "```py\n    > FeatureNumber <- tuneRF(FinalModelData[,-9], FinalModelData[,9], ntreeTry=100, stepFactor=1.5, improve=0.01, trace=TRUE, plot=TRUE, dobest=FALSE)\n\n```", "```py\n    > RandomForest <- randomForest(binaryClassification~., data=FinalModelData, mtry=2,  ntree=2000, keep.forest=TRUE, importance=TRUE)\n\n```", "```py\n    > varImpPlot(RandomForest, main = 'Random Forest: Measurement of Importance of Each Feature',pch=16,col='blue' )\n\n```", "```py\n> install.packages(\"quantmod\")\n> install.packages(\"e1071\")\n> install.packages(\"Hmisc\")\n> install.packages(\"ggplot2\")\n\n```", "```py\n> library(\"quantmod\")\n> library(\"e1071\")\n> library(\"Hmisc\")\n> install.packages(\"ggplot2\")\n\n```", "```py\n    > PoundDollar <- read.csv(\"d:/PoundDollar.csv\")\n\n```", "```py\n    > head(PoundDollar)\n\n```", "```py\n    > str(PoundDollar)\n\n```", "```py\n    > relativeStrengthIndex3 <- RSI(Op(PoundDollar), n= 3)\n\n```", "```py\n    > summary(relativeStrengthIndex3)\n\n```", "```py\n    > SeriesMeanAvg50 <- SMA(Op(PoundDollar), n=50)\n\n```", "```py\n    > summary(SeriesMeanAvg50)\n\n```", "```py\n    > describe(SeriesMeanAvg50)\n\n```", "```py\n    > Trend <- Op(PoundDollar) - SeriesMeanAvg50\n\n```", "```py\n    > summary(Trend)\n\n```", "```py\n    > PriceDiff <- Cl(PoundDollar) - Op(PoundDollar)\n\n```", "```py\n    > summary(PriceDiff)\n\n```", "```py\n    > binaryClassification <- ifelse(PriceDiff>0,\"UP\",\"DOWN\")\n\n```", "```py\n    > summary(binaryClassification)\n\n```", "```py\n    > DataSet <- data.frame(relativeStrengthIndex3, Trend, binaryClassification)\n\n```", "```py\n    > summary(DataSet)\n\n```", "```py\n    > str(DataSet)\n\n```", "```py\n    > DataSet <- DataSet[-c(1:49),]\n\n```", "```py\n> dim(DataSet) \n\n```", "```py\n    > TrainingDataSet <- DataSet[1:4528,]\n\n```", "```py\n    > dim(TrainingDataSet)\n\n```", "```py\n    > summary(TrainingDataSet)\n\n```", "```py\n    > TestDataSet <- DataSet[4529:6038,]\n\n```", "```py\n    > dim(TestDataSet)\n\n```", "```py\n    > summary(TestDataSet)\n\n```", "```py\n    > SVM <- svm(binaryClassification~relativeStrengthIndex3+Trend, data=TrainingDataSet, kernel=\"radial\", cost=1, gamma=1/2)\n\n```", "```py\n    > summary(SVM)\n\n```", "```py\n    > TrainingPredictions <- predict(SVM, TrainingDataSet, type=\"class\")\n\n```", "```py\n    > summary(TrainingPredictions)\n\n```", "```py\n    > describe(TrainingPredictions)\n\n```", "```py\n    > TrainingData <- data.frame (TrainingDataSet, TrainingPredictions)\n\n```", "```py\n    > summary(TrainingData)\n\n```", "```py\n    > ggplot(TrainingData,aes(x=Trend,y=relativeStrengthIndex3))    +stat_density2d(geom=\"contour\",aes(color=TrainingPredictions))    +labs(,x=\"Open - SMA50\",y=\"RSI3\",color=\"Training Predictions\")\n\n```", "```py\n> library(\"klar\")\n> library(\"caret\")\n> library (\"stringr\")\n\n```", "```py\n    > labels <- read.csv(\"d:/adult.txt\")\n\n```", "```py\n    > str(allData)\n\n```", "```py\n    > labels <- as.factor(allData[,15])\n\n```", "```py\n    > allFeatures <- allData[,-c(15)]\n\n```", "```py\n    > head(allFeatures)\n\n```", "```py\n    > continuousFeatures <- scale(continuousFeatures)\n\n```", "```py\n    > head(continuousFeatures)\n\n```", "```py\n    > labels.n = rep(0,length(labels))     \n> labels.n[labels==\" <=50K\"] = -1     \n> labels.n[labels==\" >50K\"] = 1     \n> labels = labels.n     \n> rm(labels.n)\n\n```", "```py\n    > trainingData <- createDataPartition(y=labels, p=.8, list=FALSE)\n\n```", "```py\n    > dim(trainingData)\n\n```", "```py\n    > trainingFeatures <- continuousFeatures[trainingData,]     \n> trainingLabels <- labels[trainingData]\n\n```", "```py\n    > remainingLabels <- labels[-trainingData]     \n> remainingFeatures <- continuousFeatures[-trainingData,]\n\n```", "```py\n    > testingData <- createDataPartition(y=remainingLabels, p=.5, list=FALSE)     \n> testingLabels <- remainingLabels[testingData]     \n> testingFeatures <- remainingFeatures[testingData,]\n\n```", "```py\n    > validationLabels <- remainingLabels[-testingData]\n    > validationFeatures <- remainingFeatures[-testingData,]\n\n```", "```py\n> getAccuracy >- function(a,b,features,labels){\n+ estFxn = features %*% a + b;\n+ predictedLabels = rep(0,length(labels));\n+ predictedLabels [estFxn < 0] = -1 ;\n+ predictedLabels [estFxn >= 0] = 1 ;\n+ return(sum(predictedLabels == labels) / length(labels))\n+ }\n\n```", "```py\n> numEpochs = 100\n> numStepsPerEpoch = 500\n> nStepsPerPlot = 30\n> evalidationSetSize = 50\n> c1 = 0.01\n> c2 = 50\n\n```", "```py\n    > lambda_vals = c(0.001, 0.01, 0.1, 1)     \n> bestAccuracy = 0\n\n```", "```py\n    > str(lambda_vals)\n\n```", "```py\n    > accMat <- matrix(NA, nrow = (numStepsPerEpoch/nStepsPerPlot)*numEpochs+1, ncol = length(lambda_vals))\n\n```", "```py\n    > accMatv <- matrix(NA, nrow = (numStepsPerEpoch/nStepsPerPlot)*numEpochs+1, ncol = length(lambda_vals))\n\n```", "```py\nfor(i in 1:4){ \nlambda = lambda_vals[i] \naccMatRow = 1 \naccMatCol = i \na = rep(0,ncol(continuousFeatures)) \nb = 0 \nstepIndex = 0 \n       for (e in 1:numEpochs){\n\n```", "```py\netrainingData <- createDataPartition(y=trainingLabels, p=(1 -   evalidationSetSize/length(trainingLabels)), list=FALSE) \n etrainingFeatures <- trainingFeatures[etrainingData,] \n etrainingLabels <- trainingLabels[etrainingData] \n evalidationFeatures <- trainingFeatures[-etrainingData,] \n evalidationLabels <- trainingLabels[-etrainingData] \n steplength = 1 / (e*c1 + c2) \n for (step in 1:numStepsPerEpoch){ \n stepIndex = stepIndex+1 \n index = sample.int(nrow(etrainingFeatures),1) \n xk = etrainingFeatures[index,] \n yk = etrainingLabels[index] \n costfxn = yk * (a %*% xk + b) \n if(costfxn >= 1){ \n a_dir = lambda * a \n a = a - steplength * a_dir \n } else { \n a_dir = (lambda * a) - (yk * xk) \n a = a - steplength * a_dir \n b_dir = -yk \n b = b - (steplength * b_dir) \n } \n\n```", "```py\nif (stepIndex %% nStepsPerPlot == 1){#30){ \naccMat[accMatRow,accMatCol] = getAccuracy(a,b,evalidationFeatures,evalidationLabels) \naccMatv[accMatRow,accMatCol] = getAccuracy(a,b,validationFeatures,validationLabels) \naccMatRow = accMatRow + 1 \n} \n} \n} \ntempAccuracy = getAccuracy(a,b,validationFeatures,validationLabels) \nprint(str_c(\"tempAcc = \", tempAccuracy,\" and bestAcc = \", bestAccuracy) ) \nif(tempAccuracy > bestAccuracy){ \nbestAccuracy = tempAccuracy \nbest_a = a \nbest_b = b \nbest_lambdaIndex = i \n} \n   }\n\n```", "```py\n   > getAccuracy(best_a,best_b, testingFeatures, testingLabels)\n\n```", "```py\n    > colors = c(\"red\",\"blue\",\"green\",\"black\")\n\n```", "```py\n> xaxislabel = \"Step\"\n> yaxislabels = c(\"Accuracy on Randomized Epoch Validation\nSet\",\"Accuracy on Validation Set\")\n>\n> ylims=c(0,1)\n> stepValues = seq(1,15000,length=500)\n\n```", "```py\n    > mats =  list(accMat,accMatv)\n\n```", "```py\n> for(j in 1:length(mats)){\nmat = mats[[j]]\nfor(i in 1:4){\nif(i == 1){\n\n```", "```py\n plot(stepValues, mat[1:500,i], type = \"l\",xlim=c(0, 15000), ylim=ylims, \n col=colors[i],xlab=xaxislabel,ylab=yaxislabels[j],main=title) \n } else{ \n lines(stepValues, mat[1:500,i], type = \"l\",xlim=c(0, 15000), ylim=ylims, \n col=colors[i],xlab=xaxislabel,ylab=yaxislabels[j],main=title) \n } \n Sys.sleep(1) \n } \n legend(x=10000,y=.5,legend=c(\"lambda=.001\",\"lambda=.01\",\"lambda=.1\",\"lambda=1\"),fill=colors) \n } \n\n```"]