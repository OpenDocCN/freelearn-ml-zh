<html><head></head><body>
		<div id="_idContainer147">
			<h1 id="_idParaDest-157"><em class="italic"><a id="_idTextAnchor159"/>Chapter 9</em>: Production Scoring and the H2O MOJO</h1>
			<p>We spent the entire previous section learning how to build world-class models against data at scale with H2O. In this chapter, we will learn how to deploy these models and make predictions from them. First, we will cover the background on putting models into production scoring systems. We will then learn how H2O makes this easy and flexible. At the center of this story is the H2O <strong class="bold">MOJO</strong> (short for <strong class="bold">Model Object, Optimized</strong>), a ready-to-deploy scoring artifact that you export from your model building environment. We will learn technically what a MOJO is and how to deploy it. We will then code a simple batch file scoring program and embed a MOJO in it. We will finish with some final notes on the MOJO. Altogether, in this chapter, you will develop the knowledge to deploy H2O models in diverse ways and so begin achieving value from live predictions.</p>
			<p>These are the main topics we will cover in this chapter:</p>
			<ul>
				<li>Relating the model building context to the scoring context for H2O models</li>
				<li>Recognizing the diversity of target production systems for H2O models</li>
				<li>Examining the technical design of the H2O deployable artifact, the H2O MOJO</li>
				<li>Writing your own H2O MOJO batch file scorer to show how to embed MOJOs in your own software</li>
			</ul>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor160"/>Technical requirements</h1>
			<p>In this chapter, you will need a Java SE 8 or greater environment. A Java IDE such as Eclipse is optional but useful. You will get a MOJO, a dataset to score and the Java code for the batch file scorer program in the following GitHub repository: <a href="https://github.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O/tree/main/chapt9">https://github.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O/tree/main/chapt9</a>. These artifacts were generated from the model built in <a href="B16721_08_Final_SK_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a>, <em class="italic">Putting It All Together</em>.</p>
			<p>Note that we are done with model building at this point, so you do not need a model building environment pointing to a running H2O cluster. </p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor161"/>The model building and model scoring contexts</h1>
			<p>In <em class="italic">Section 2</em>, <em class="italic">Building State-of-the-Art Models on Large Data Volumes Using H2O</em>, we spent a great amount of focus on building world-class models at scale with H2O. Building highly <a id="_idIndexMarker636"/>accurate and trusted models against massive datasets <a id="_idIndexMarker637"/>can potentially generate millions of dollars for a business, save lives, and define new product areas, but only when the models are deployed to production systems where predictions are made and acted upon. </p>
			<p>This last step of deploying and predicting (or scoring) on a production system can often be time-consuming, problematic, and risky for reasons discussed shortly. H2O makes this transition from a built (trained) model to a deployed model easy. It also provides a wide range of flexibility in regard to where scoring is done (device, web application, database, microservice endpoint, or Kafka queue) and to the velocity of data (real-time, batch, and streaming). And, whatever the production context, the H2O deployed model scores lightning fast.</p>
			<p>At the center of this ease, flexibility, and low-latency production scoring is the H2O MOJO. An H2O MOJO is a ready-to-deploy scoring artifact that is generated by a simple export command at the <a id="_idIndexMarker638"/>end of your model-building code. H2O MOJOs are similar regardless of the model-building algorithm that generated them. As a result, all H2O models are deployed similarly. Before diving into the MOJO and learning how to deploy it, let's first take a look in general at the process of moving from model training to model scoring.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor162"/>Model training to production model scoring</h2>
			<p>We'll first take a general view of how models transition from model training to production scoring and then see how this is done with H2O. </p>
			<h3>Generic training-to-scoring pipeline</h3>
			<p>A generic pipeline of a <a id="_idIndexMarker639"/>trained to a deployed model can be represented as follows:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/Figure_9.1_B16721.jpg" alt="Figure 9.1 – Generalized pipeline from model training to scoring &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Generalized pipeline from model training to scoring </p>
			<p>Do note that this <a id="_idIndexMarker640"/>pipeline is more formally represented and elaborated by the practice called <strong class="bold">Machine Learning Operations</strong> (<strong class="bold">MLOps</strong>), which involves a larger area of concern, but for the <a id="_idIndexMarker641"/>focus of deploying a model to production, the representation here should work for us. </p>
			<p>Each step is summarized as follows: </p>
			<ol>
				<li><strong class="bold">Trained model</strong>: The trained model is converted into a format or software object that can be deployed to a software system. This conversion can occur in many ways. The riskiest <a id="_idIndexMarker642"/>and most time-consuming way is to determine the mathematical logic of the trained model (for example, the branching logic of a tree-based model) and then rewrite that into <strong class="source-inline">if-else</strong> logic in software code. This is time-consuming because the logic of the trained model must be accurately communicated by the data scientist to the software developer, who must implement the logic correctly and then have it tested thoroughly to validate its accuracy. This is also error-prone and, therefore, risky, as well as time-consuming. </li>
			</ol>
			<p>The best-case scenario is when a conversion tool translates the trained model into a deployable artifact. This can be either a format (for example, XML for PMML, PFA, or ONNX) that declares the logic for a production system that is ready to compute against the declarative constructs, or it can be a runnable software artifact (for example, a Python wheel or Java JAR file) that can be embedded into a software program or framework. </p>
			<ol>
				<li value="2"><strong class="bold">Deployable model</strong>: The converted model is deployed to a production system. This written code or <a id="_idIndexMarker643"/>converted artifact is integrated into a software application or framework that, at some point, inputs data into the scoring logic it holds and outputs the scoring result. For example, a customer's data goes in and the probability of churn comes out. </li>
			</ol>
			<p>Model deployment should be performed in TEST and <strong class="bold">production</strong> (<strong class="bold">PROD</strong>) environments, with <a id="_idIndexMarker644"/>deployment and promotion done through <a id="_idIndexMarker645"/>a formal governance process using a <strong class="bold">continuous integration and continuous deployment</strong> (<strong class="bold">CI/CD</strong>) pipeline, as with the deployment of software in general. Deployable artifacts <a id="_idIndexMarker646"/>that are recognizable and standardized across all models built (for example, among different ML algorithms) are easier to automate during deployment than those that are not.</p>
			<ol>
				<li value="3"><strong class="bold">Production system</strong>: Scoring live in production. Production scoring needs can be diverse. Scoring may be needed, for example, against entire database tables in <a id="_idIndexMarker647"/>one batch, against each live ATM transaction sent over the network, inside a web application for every web page click of a customer, or on streams of sensor data sent from edge devices. Scoring may be on a device or on a large server in the cloud. Typically, the faster the score, the better (demands of less than 50 microseconds per score or faster are not uncommon), and the smaller the scorer size and resource consumption footprint, the closer to the edge it can be deployed.</li>
				<li><strong class="bold">Predictions</strong>: Scoring the output. Models output predictions during scoring. Note that predictions need a business context and action to achieve purpose or value. For example, customers who are predicted to churn are given phone calls or special offers <a id="_idIndexMarker648"/>to help ensure they remain customers. Often, scoring outputs require not just predictions, but also explanations in the form of reason codes for those predictions. How did the model weigh each input to the scorer when generating the prediction for a particular customer? In other words, which factors were most important in a specific prediction. These decision weights are represented as reason <a id="_idIndexMarker649"/>codes and they can help personalize a phone call or special offer in the churn case.</li>
			</ol>
			<p>Let's see how the training-to-scoring pipeline is realized with H2O.</p>
			<h3>The H2O pipeline and its advantages</h3>
			<p>Trained H2O models participate in a similar pipeline as discussed, but with important attributes that make <a id="_idIndexMarker650"/>them easy to deploy to a diverse target of software systems and are also very fast when they score there. The deployable artifact for H2O is called a MOJO and it bridges the gap between model training and model scoring, and so is the central character in the story. Attributes of the H2O pipeline are summarized as follows:</p>
			<p class="figure-caption">  </p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/Figure_9.2_B16721.jpg" alt="Figure 9.2 – H2O's model training-to-scoring pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – H2O's model training-to-scoring pipeline</p>
			<p>Let's elaborate <a id="_idIndexMarker651"/>on H2O's advantages of deploying models:</p>
			<ol>
				<li value="1"><strong class="bold">H2O trained model</strong>: H2O MOJOs exported from the model-building IDE as ready to deploy. The data scientist converts the trained model into an exported and ready-to-deploy MOJO by writing a single line of code in the IDE. </li>
				<li><strong class="bold">H2O MOJO</strong>: H2O MOJOs are standardized low-latency scoring artifacts and ready to deploy. The MOJO construct is standardized and shared by all model types and has its own runtime that embeds in any Java runtime. This means that all MOJOS (models) are <a id="_idIndexMarker652"/>identically embedded in any <strong class="bold">Java virtual machine</strong> (<strong class="bold">JVM</strong>) independent of the larger software and hardware context. MOJOs are also lightweight and can be deployed to nearly all infrastructure (except the smallest of edge devices). MOJOs are super fast at scoring and can handle any data velocity (real-time scoring, batch scoring, and streaming scoring).</li>
				<li><strong class="bold">Production system</strong>: H2O MOJOs flexibly deploy to a diversity of production systems. MOJOs deploy to a wide range of production systems. An overview of these systems and details of how MOJOS are deployed to them are given a bit later in this chapter.</li>
				<li><strong class="bold">Predictions</strong>: MOJOs can output a lot of information in their scoring. Inputs to MOJO return predictions in the form of class probabilities for classification, predicted numeric values for regression, and model-specific outcomes for unsupervised <a id="_idIndexMarker653"/>problems. Additionally, and optionally, MOJOs may return reason codes in the form of Shapley or K-LIME values, or other attributes such as leaf node assignments for a prediction.</li>
			</ol>
			<p>Let's focus more on H2O production scoring specifically in the next section.</p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor163"/>H2O production scoring</h1>
			<p>Models achieve their <a id="_idIndexMarker654"/>business value when they are put into production to make predictions (or generate unsupervised results for an unsupervised class of problems). We discuss, in this section, a more detailed view of the H2O pipeline from model building to production scoring.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor164"/>End-to-end production scoring pipeline with H2O</h2>
			<p>Take a look <a id="_idIndexMarker655"/>at the following diagram showing an end-to-end H2O pipeline from model training to model deployment and production scoring:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/Figure_9.3_B16721.jpg" alt="Figure 9.3 – High-level view of full scoring pipeline with H2O &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – High-level view of full scoring pipeline with H2O </p>
			<p>Typically, model building is considered a <strong class="bold">development</strong> (<strong class="bold">DEV</strong>) environment, and model scoring is a PROD environment with source data from each respective environment. </p>
			<p>For DEV, we have treated feature engineering and model training (and many associated steps such as <a id="_idIndexMarker656"/>model explainability and evaluation) extensively in <em class="italic">Section 2</em>, <em class="italic">Building State-of-the-Art Models on Large Data Volumes Using H2O</em>. We also briefly discussed the exportable ready-to-deploy H2O MOJO scoring artifact and deploying it to PROD systems earlier in this chapter.</p>
			<p>Let's identify some key points to keep in mind during this pipeline:</p>
			<ul>
				<li><em class="italic">You need feature engineering parity between DEV and PROD</em>: This means that any feature engineering done to create the training dataset must be matched by the scoring input in TEST/PROD. In other words, the features in the training dataset must be the same as those fed into the model scoring. If there were multiple steps of feature engineering (for example, <strong class="bold">extract, transform, and load</strong> (<strong class="bold">ETL</strong>) from a data source and feature engineering in H2O Sparkling Water) before <a id="_idIndexMarker657"/>constructing the training dataset in DEV, the input to scoring in TEST/PROD must have those same engineered features. </li>
			</ul>
			<p>Having said that, depending on how the MOJO is deployed (H2O Scorer, third-party integration, or your own scorer), you likely will have to input to TEST/PROD only a subset of the features from those in the training dataset. This reflects the fact that the trained model typically selects only a subset of data features that contribute to the final model. This subsetting is not required, however; MOJOs can accept full or subsets of features (compared to the training dataset) depending on how you design it. This flexibility will become clearer later in the chapter when we take a closer look at deploying MOJOs. </p>
			<ul>
				<li><em class="italic">You may need a wrapper around your MOJO (but not with H2O Scorers and most third-party integrations)</em>: MOJOs are ready to deploy to a Java environment. This means the MOJO is ready to convert input data to a prediction output using the mathematical logic derived from model training and held in the MOJO, and that the MOJO itself does not need compiling or modification <a id="_idIndexMarker658"/>in any way. But, you must still make sure the input (for example, CSV, JSON, batch, and so on) feeds into the MOJO in a way that the MOJO can accept. On the other side, you may want to extract more from the MOJO scoring result than only predictions, and you will need to convert the MOJO output to a format expected downstream in the application. You do this by writing a simple Java wrapper class and using the MOJO API called <strong class="source-inline">h2o-genmodel</strong> API to interact with the MOJO. These wrapper classes are not complicated. We will learn more about wrapping MOJOs with an example later in this chapter.<p class="callout-heading">Important Note </p><p class="callout">H2O Scorers and many third-party integrations for MOJOs do not require wrappers because they handle this internally. All you need is the exported MOJO in these cases. Additionally, many integrations occur by way of REST APIs to endpoints of MOJOs deployed on REST servers.</p></li>
				<li><em class="italic">You may want to return reason codes or other information with your predictions</em>: MOJOs return predictions for supervised models and model-specific output for unsupervised models (for example, an anomaly score is returned for a model trained with <strong class="source-inline">H2OIsolationForestEstimator</strong>. But, there is more to retrieve from the MOJO; you can also return reason codes as K-LIME or Shapley values, the decision path taken through tree-based models, or class labels for the prediction of classification problems. These additional outputs are implemented in wrapper code using the <strong class="source-inline">h2o-genmodel</strong> API for scorers you build. They may or may not be built into the functionality of H2O Scorers or out-of-the-box third-party integrations. You will need to check the specifications for these scorers.</li>
				<li><em class="italic">You need a formalized process to deploy and govern your model</em>: Putting models into production involves risks: generally, the risk of failure or delay from errors during <a id="_idIndexMarker659"/>deployment, and the risk to revenue or reputation from adverse consequences from model decisions by deployed models. We will look at this topic more closely in <a href="B16721_14_Final_SK_ePub.xhtml#_idTextAnchor256"><em class="italic">Chapter 14</em></a>, <em class="italic">H2O at Scale in a Larger Platform Context</em>. </li>
				<li><em class="italic">You need MLOps to monitor your model</em>: Models in PROD typically need to be monitored to see whether values of input data are changing over time compared to those in the training data (this result is called data drift). In this case, the model may need to be retrained since the signal it was trained against has changed, possibly causing the predictive accuracy of the model to degrade. Bias, prediction distributions, and other aspects of scoring may also be monitored. </li>
			</ul>
			<p>Model monitoring is outside the capability of MOJOs. MOJOs are concerned with single scores. Monitoring fundamentally tracks aggregate trends from MOJO inputs and outputs and is a separate area of technology and concern that will not be treated here. Do note, however, that H2O has an MLOps platform that performs model monitoring and governance. It is overviewed in <em class="italic">Chapter 16</em>, <em class="italic">The Machine Learning Life Cycle, AI Apps, and H2O AI Hybrid Cloud</em>.</p>
			<p>We have just overviewed the full pipeline from H2O model building to production scoring and identified key points regarding this pipeline. One part of this pipeline is quite variable depending on your needs: the target system on which to deploy your MOJO. Let's explore this in greater detail.</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor165"/>Target production systems for H2O MOJOs</h2>
			<p>One large <a id="_idIndexMarker660"/>advantage of MOJOs is that they can be deployed <a id="_idIndexMarker661"/>to a wide range of production systems. Let's dig deeper using the following diagram to summarize:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/Figure_9.4_B16721.jpg" alt="Figure 9.4 – Taxonomy of production systems for MOJO scoring&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – Taxonomy of production systems for MOJO scoring</p>
			<p>Business requirements mostly determine whether scoring needs to be real time, batch, or streaming and MOJOs can handle the full range of these data velocities. </p>
			<p>It is useful to articulate production target systems into the following three categories for MOJO deployments:</p>
			<ul>
				<li><strong class="bold">H2O scoring system</strong>: This represents the H2O scoring software that is available from H2O. These scorers include a REST server with MLOps and rich model monitoring and governance capabilities (and a lively roadmap that includes batch scoring, champion/challenger testing, A/B testing, and more), a database scorer for batch database table scoring that outputs to a table or file, a file batch scorer, and AMQ and Kafka Scorers for streaming events. H2O is actively adding more scorers, so visit their website to keep up to date. The MLOps scorer specifically is discussed in more detail in <em class="italic">Chapter 16</em>, <em class="italic">The Machine Learning Lifecycle, AI Apps, and H2O AI Hybrid Cloud</em>. </li>
				<li><strong class="bold">Third-party integrations</strong>: Many third parties integrate out-of-the-box with MOJOs for scoring on their framework or software. Others require some glue to be built to create a custom integration.</li>
				<li><strong class="bold">Your own DIY system</strong>: You can embed MOJOs in your software or framework integrations that run a Java environment. Integrations will require a simple Java <a id="_idIndexMarker662"/>wrapper class to interface your <a id="_idIndexMarker663"/>application or framework to the MOJO data input and output capabilities (for example, your REST server will need to convert JSON to a MOJO data object). H2O makes this easy with its <strong class="bold">MOJO API</strong>. Wrapping with the MOJO API is discussed in greater detail with code <a id="_idIndexMarker664"/>examples later in the chapter.</li>
			</ul>
			<p>Note that this chapter provides an introduction to deploying MOJOs to target systems. The entire <a href="B16721_10_Final_SK_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 10</em></a>, <em class="italic">H2O Model Deployment Patterns,</em> will be devoted to walking through multiple examples of MOJO deployments to target systems.</p>
			<p>Now that we understand the end-to-end H2O pipeline from model building to live scoring on diverse production systems, let's take a closer look at its central player: the MOJO.</p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor166"/>H2O MOJO deep dive</h1>
			<p>All MOJOs are fundamentally<a id="_idIndexMarker665"/> similar from a deployment and scoring standpoint. This is true regardless of the MOJO's origin from an upstream model-building standpoint, that is, regardless of which of H2O's wide diversity of model-building algorithms (for example, Generalized Linear Model, and XGBoost) and techniques (for example, Stacked Ensembles and AutoML) and training dataset sizes (from GBs to TBs) were used to build the final model. </p>
			<p>Let's get to know the MOJO in greater detail.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor167"/>What is a MOJO?</h2>
			<p>A <strong class="bold">MOJO</strong> stands for Model Object, Optimized. It is exported from your model-building IDE by running <a id="_idIndexMarker666"/>the following line of code:</p>
			<pre class="source-code">model.download_mojo(path="path/for/my/mojo")</pre>
			<p>This downloads a uniquely-named <strong class="source-inline">.zip</strong> file onto the filesystem of your IDE, to the path you specified. This <strong class="source-inline">.zip</strong> file is the MOJO and this is what is deployed. You do not unzip it, but if you are curious, it contains a <strong class="source-inline">model.ini</strong> file that describes the MOJO as well as multiple <strong class="source-inline">.bin</strong> files, all of which are used by the <strong class="bold">MOJO runtime</strong>.</p>
			<p>What is a MOJO runtime? This is a Java <strong class="source-inline">.jar</strong> file called <strong class="source-inline">h2o-genmodel.jar</strong> and is a generic runtime for <a id="_idIndexMarker667"/>all H2O Core MOJOs. In other words, MOJOs are specific to the trained models they <a id="_idIndexMarker668"/>are derived from, and all MOJOs are loaded identically into the MOJO runtime. The MOJO runtime integrates with a Java runtime (in H2O software, third-party software, or your own software). The following diagram relates MOJOs to the MOJO runtime.</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/Figure_9.5_B16721.jpg" alt="Figure 9.5 – MOJOs and the MOJO runtime&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – MOJOs and the MOJO runtime</p>
			<p>As mentioned previously, MOJOs are deployed to a Java runtime, more formally known as a <strong class="bold">Java virtual machine</strong> (<strong class="bold">JVM</strong>). The software that embeds the MOJO uses <strong class="source-inline">h2o-genmodel.jar</strong> as a <a id="_idIndexMarker669"/>dependent library to do so. The software loads the model-specific MOJO into the generic <strong class="source-inline">h2o-genmodel.jar</strong> runtime <a id="_idIndexMarker670"/>using the <strong class="source-inline">h2o-genmodel</strong> API. The actual scoring logic in the application code also uses <strong class="source-inline">h2o-genmodel.jar</strong> and its API to implement the scoring and extraction of results from the embedded MOJO.</p>
			<p>Let's dig down and elaborate in the next section.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor168"/>Deploying a MOJO</h2>
			<p>You need only the MOJO if you deploy a MOJO to an H2O Scorer or to a third-party software that <a id="_idIndexMarker671"/>integrates MOJOs out-of-the-box. You do not need to consider the MOJO runtime and API in these cases. This is because these software systems have already implemented <strong class="source-inline">h2o-genmodel.jar</strong> (using the <strong class="source-inline">h2o-genmodel</strong> API) behind the scenes, in other words, in the H2O Scorer or third-party software that is deployed and operating.</p>
			<p>In other cases, you need to write the code that embeds the MOJO and extracts its scoring results. This code, typically, is a single Java wrapper class that uses the <strong class="source-inline">h2o-genmodel</strong> API. We will visit this a bit later using a code example. </p>
			<p>This distinction is important and deserves a larger callout.</p>
			<p class="callout-heading">Key Distinction in MOJO Deployment</p>
			<p class="callout">You need only the MOJO when deploying to H2O scoring software or third-party software that integrates MOJOs out of the box (configuration-based).</p>
			<p class="callout">You need to write a simple Java wrapper class using the <strong class="source-inline">h2o-genmodel</strong> API when integrating the MOJO into your own software or third-party software that does not integrate MOJO out of the box. This wrapper requires <strong class="source-inline">h2o-genmodel.jar</strong>, which is the library that the <strong class="source-inline">h2o-genmodel</strong> API represents.</p>
			<p class="callout">(If you are consuming MOJO predictions in third-party software or your own software from a REST server, you do not, of course, need the MOJO or the MOJO runtime. You simply need to conform to the REST endpoint API for the MOJO.)</p>
			<p>Let's look at the case when you need to write a wrapper.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor169"/>Wrapping MOJOs using the H2O MOJO API</h1>
			<p>Let's first <a id="_idIndexMarker672"/>touch upon a few <a id="_idIndexMarker673"/>precursors before learning how to wrap MOJOs inside larger software programs.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor170"/>Obtaining the MOJO runtime</h2>
			<p>You can download <strong class="source-inline">h2o-genmodel.jar</strong> when you download your MOJO from the IDE after model <a id="_idIndexMarker674"/>building. This is simply a matter of adding a new argument to your download statement, as follows:</p>
			<pre class="source-code">Model.download_mojo(path="path/for/my/mojo", </pre>
			<pre class="source-code">                    get_genmodel_jar=True)</pre>
			<p>This method of obtaining <strong class="source-inline">h2o-genmodel.jar</strong> generally is not done in a governed production deployment. This is because <strong class="source-inline">h2o-genmodel.jar</strong> is generic to all MOJOs and is a concern of the software developer and not the data scientists. </p>
			<p>Software developers can <a id="_idIndexMarker675"/>download the MOJO runtime from the Maven repository at <a href="https://mvnrepository.com/artifact/ai.h2o/h2o-genmodel">https://mvnrepository.com/artifact/ai.h2o/h2o-genmodel</a>. The <strong class="source-inline">h2o-genmodel.jar</strong> is backward-compatible; it should work for a MOJO generated from an H2O-3 (or Sparkling Water) version equal to or less than the <strong class="source-inline">h2o-genmodel.jar</strong> version.</p>
			<p class="callout-heading">A Tip for Obtaining the MOJO Runtime (h2o-genmodel.jar)</p>
			<p class="callout">Data scientists do not have to download the MOJO runtime each time they download their MOJO from their model-building IDEs. This is because the MOJO runtime is generic to all MOJOs. A best practice is to let your developers (not the data scientists) concern themselves with obtaining and using the MOJO runtime for production deployments when needed. This can be done through the Maven repository referenced earlier.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor171"/>The h2o-genmodel API</h2>
			<p>Javadocs for <a id="_idIndexMarker676"/>the <strong class="source-inline">h2o-genmodel</strong> API are located at <a href="https://docs.h2o.ai/h2o/latest-stable/h2o-genmodel/javadoc/index.html">https://docs.h2o.ai/h2o/latest-stable/h2o-genmodel/javadoc/index.html</a>. Note that <a id="_idIndexMarker677"/>this is for the latest H2O-3 (or Sparkling Water). To get a <a id="_idIndexMarker678"/>different version, go to <a href="https://docs.h2o.ai/prior_h2o/index.html">https://docs.h2o.ai/prior_h2o/index.html</a>.</p>
			<p>In summary, the <strong class="source-inline">h2o-genmodel</strong> API is used to build wrappers around the MOJO so your application can feed data into the MOJO, extract prediction and decision information from it, and convert these results to the code in your wrapper. The wrapper is typically part of your <a id="_idIndexMarker679"/>larger application and can be seen as the glue between your app and the MOJO.</p>
			<p>Let's dive in.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor172"/>A generalized approach to wrapping your MOJO </h2>
			<p>It will be useful <a id="_idIndexMarker680"/>before writing code to first look at the logical flow of application code for the MOJO wrapper you develop. This can be seen in the following diagram:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/Figure_9.6_B16721.jpg" alt="Figure 9.6 – Logical view of wrapping a MOJO&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Logical view of wrapping a MOJO</p>
			<p>The Java wrapper typically is its own class (or part of a class) and imports <strong class="source-inline">h2o-genmodel.jar</strong> and follows these general logical steps:</p>
			<ol>
				<li value="1">Load <strong class="source-inline">yourMOJO.zip</strong> into the MOJO runtime. Recall that <strong class="source-inline">h2o-genmodel.jar</strong> is the runtime that holds the generic logic to work on model-specific MOJOs. This runtime is now ready to operate on your specific model. </li>
				<li>Feed data into the MOJO. To do so, convert the Java data structure of your input into a MOJO data structure using the <strong class="source-inline">h2o-genmodel</strong> code.</li>
				<li>Score the MOJO. This is a single line of <strong class="source-inline">h2o-genmodel</strong> code.</li>
				<li>Extract the subset of information that you need from the MOJO scoring results. Recall that prediction results (or unsupervised results) represent aspects of the prediction (labels and predictions) as well as aspects of scoring decisions (reason codes, decision path to leaf node results, and other).</li>
				<li>Convert the extracted <a id="_idIndexMarker681"/>results into a data structure needed downstream by the application.</li>
			</ol>
			<p>Let's write a wrapper.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor173"/>Wrapping example – Build a batch file scorer in Java</h2>
			<p>The goal of the wrapper we are writing is to batch score new data from a file. The output of the scoring <a id="_idIndexMarker682"/>will be the input record, the prediction, and the reason codes all formatted <a id="_idIndexMarker683"/>as a line of CSV. The reason codes will be <a id="_idIndexMarker684"/>a single CSV field but the reason codes will be pipe-delimited. </p>
			<p>We will compile this wrapper class as a runnable program that accepts three input parameters:</p>
			<ul>
				<li>Input param 1: <strong class="source-inline">path/of/batch/file/to/score</strong> </li>
				<li>Input param 2: <strong class="source-inline">path/to/yourMOJO.zip</strong></li>
				<li>Input param 3 (optional): The <strong class="source-inline">—shap</strong> flag to trigger the return of Shapley reason codes in addition to the scoring prediction for each row in the file<p class="callout-heading">Shapley Values Add Latency</p><p class="callout">Keep in mind that returning Shapley values adds additional computation and, therefore, latency to each scoring. You might want to benchmark latencies with and without Shapley reason codes in your results to evaluate whether to include them in scoring or not if latency is critical. </p></li>
			</ul>
			<p>We will use the MOJO that you exported at the end of your model building exercise in <a href="B16721_08_Final_SK_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a>, <em class="italic">Putting It All Together</em>. </p>
			<h3>The code</h3>
			<p>Our batch file scorer program will involve a single Java class and will not include error handling and other <a id="_idIndexMarker685"/>production quality software design. Our purpose here is to show the fundamentals of integrating a MOJO into your software. </p>
			<p>Note that the code samples below are elaborated step by step. To access the entire Java code from beginning to end, go to the GitHub repository at <a href="https://github.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O/tree/main/chapt9">https://github.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O/tree/main/chapt9</a>.</p>
			<p>Let's get started:</p>
			<ol>
				<li value="1"><strong class="bold">Create an empty wrapper class</strong>: First, create a class called <strong class="source-inline">BatchFileScorer</strong>. Since this is also an executable program, we will create a <strong class="source-inline">main</strong> method to start the code execution. Note the <strong class="source-inline">import</strong> statements for the <strong class="source-inline">h2o-genmodel</strong> library packages: <p class="source-code">Import java.io.*;</p><p class="source-code">import hex.genmodel.easy.RowData;</p><p class="source-code">import hex.genmodel.easy.EasyPredictModelWrapper;</p><p class="source-code">import hex.genmodel.easy.prediction.*;</p><p class="source-code">import hex.genmodel.MojoModel; </p><p class="source-code">public class BatchFileScorer {</p><p class="source-code">  public static void main(String[] args) throws Exception{</p><p class="source-code">  // we will fill with steps 2 to 4 that follows</p><p class="source-code">  }</p><p class="source-code">}</p></li>
			</ol>
			<p>Now, let's fill the <strong class="source-inline">main</strong> method with code, as shown in the following steps.</p>
			<ol>
				<li value="2"><strong class="bold">Retrieve the input parameters</strong>: We retrieve the input parameters from the program's arguments:<p class="source-code">// get input parameters</p><p class="source-code">File fileToScore = new File(args[0]);</p><p class="source-code">String pathToMojo = args[1];</p><p class="source-code">boolean doShapley = args.length == 3</p><p class="source-code">  &amp;&amp; args[2].equals("--shap"); </p></li>
				<li><strong class="bold">Load the MOJO and configure it to optionally return reason codes</strong>: We load the <a id="_idIndexMarker686"/>MOJO into the MOJO runtime and configure it to return Shapley values:<p class="source-code">// Load the mojo (only once) and configure</p><p class="source-code">EasyPredictModelWrapper.Config config = </p><p class="source-code">  new EasyPredictModelWrapper.Config();</p><p class="source-code">config.setModel(MojoModel.load(pathToMojo);</p><p class="source-code">if (doShapley) config.setEnableContributions(true);</p><p class="source-code">EasyPredictModelWrapper model = </p><p class="source-code">  new EasyPredictModelWrapper(config);</p></li>
			</ol>
			<p>The MOJO is loaded only once here before all scoring later in the code.</p>
			<p class="callout-heading">Important Design Point – Load Your MOJO Once</p>
			<p class="callout">Loading the MOJO can take a few seconds, but it only needs to be loaded into your program once.</p>
			<p class="callout">Load the MOJO once in your wrapper class (for example, when it initializes) before making all scoring requests. You do not want your sub-hundred or sub-ten millisecond scores each preceded by multiple seconds of loading.</p>
			<p>Now to the magic: generating predictions.</p>
			<ol>
				<li value="4"><strong class="bold">Score</strong>: Next, we open the file and iterate and score each line using the MOJO API provided <a id="_idIndexMarker687"/>by the <strong class="source-inline">import</strong> statements shown in <em class="italic">step 1</em>:<p class="source-code">// get each record from the file</p><p class="source-code">BufferedReader br = new BufferedReader(new</p><p class="source-code">  FileReader(fileToScore));</p><p class="source-code">// we are skipping the first line (header line)</p><p class="source-code">br.readLine();</p><p class="source-code">String record = null;</p><p class="source-code">while ((record = br.readLine()) != null) {</p><p class="source-code">  // Convert input record to type required by mojo api</p><p class="source-code">  RowData mojoRow = convertInput(record);</p><p class="source-code">  // make the prediction</p><p class="source-code">  BinomialModelPrediction p = model.predictBinomial(mojoRow);</p><p class="source-code">  // get results from p and format it to your needs</p><p class="source-code">  // in this case, format is csv to write to file</p><p class="source-code">  String outputString = formatOutput(record, p, doShapley);</p><p class="source-code">  // can write this to file </p><p class="source-code">  // but printing to screen for ease of code explanation</p><p class="source-code">  System.out.println(outputString);</p><p class="source-code">  }</p></li>
			</ol>
			<p>That is it! You have loaded the MOJO and configured the scoring, and scored each line of a file. To score, you have converted each record from its application representation (CSV string) to the <strong class="source-inline">h2o-genmodel</strong> representation (the <strong class="source-inline">DataRow</strong> object). You have written one line <a id="_idIndexMarker688"/>of code to score the record. And, you have retrieved the prediction and, optionally, Shapley reason codes from the scoring result. You then formatted this to a representation used by your application.</p>
			<h3>Drill-downs to the code</h3>
			<p>Let's drill down into <a id="_idIndexMarker689"/>methods from the previous code.</p>
			<h4>Method drilldown – Converting your application data object to an h2o-genmodel data object</h4>
			<p>Note that <strong class="source-inline">RowData mojoRow</strong> is where the program code is converted into the <strong class="source-inline">h2o-genmodel</strong> API data <a id="_idIndexMarker690"/>object. In the example here, it is done through the <strong class="source-inline">convertInput(record)</strong> method as shown:</p>
			<pre class="source-code">private static RowData convertInput(String record) {</pre>
			<pre class="source-code">  String[] featureValues = record.split(",");</pre>
			<pre class="source-code">  RowData row = new RowData();</pre>
			<pre class="source-code">  row.put("purpose_te", featureValues[0]);</pre>
			<pre class="source-code">  row.put("addr_state_te", featureValues[1]);</pre>
			<pre class="source-code">  row.put("loan_amnt", featureValues[2]);</pre>
			<pre class="source-code">  row.put("term", featureValues[3]);</pre>
			<pre class="source-code">  row.put("installment", featureValues[4]);</pre>
			<pre class="source-code">  row.put("grade", featureValues[5]);</pre>
			<pre class="source-code">  // omitting features 6 to 24, see code in github repo </pre>
			<pre class="source-code">  row.put("emp_length_missing", featureValues[25]);</pre>
			<pre class="source-code">  return row;</pre>
			<pre class="source-code">}</pre>
			<p>We have simply split the input using a comma as a separator and assigned each value to the H2O <strong class="source-inline">RowData</strong> object, which essentially is a map of key-value pairs with the keys representing <a id="_idIndexMarker691"/>feature names (that is, column headings). There are alternatives to using <strong class="source-inline">RowData</strong>.</p>
			<p class="callout-heading">Design Decision – Choices for converting Your Data Object to the MOJO API Data Object</p>
			<p class="callout">Using the <strong class="source-inline">h2o-genmodel</strong> API's <strong class="source-inline">RowData</strong> class, as we did here, is just one way to convert your application data object into an <strong class="source-inline">h2o-genmodel</strong> object to feed to the MOJO for scoring. Check the API for additional ways that may offer better code design for your implementation.</p>
			<h4>Method drilldown – The single line to score</h4>
			<p>Only a single <a id="_idIndexMarker692"/>line of code was needed to score the MOJO and retrieve results:</p>
			<pre class="source-code">BinomialModelPrediction p = model.predictBinomial(mojoRow); </pre>
			<p>Note that you may need a different class than <strong class="source-inline">BinomialModelPrediction</strong> depending on which <a id="_idIndexMarker693"/>type of model you build. Check the <strong class="source-inline">h2o-genmodel</strong> Javadocs for details on which Java class to use and what scoring information is returned.</p>
			<h4>Method drilldown – Collecting results and formatting as output</h4>
			<p>We <a id="_idIndexMarker694"/>ultimately constructed <a id="_idIndexMarker695"/>a string from the scoring results using the <strong class="source-inline">formatOutput(record, p, doShapley)</strong> method. Here is how that method was implemented:</p>
			<pre class="source-code">private static String formatOutput(String record,</pre>
			<pre class="source-code">  BinomialModelPrediction p, boolean doShapley) {</pre>
			<pre class="source-code">  // start the ouput string with the record being scored</pre>
			<pre class="source-code">  String outputString = record;</pre>
			<pre class="source-code">  // add prediction to output string</pre>
			<pre class="source-code">  outputString += "   PREDICTION (good=0, bad=1): " + p.label</pre>
			<pre class="source-code">  + " " + p.classProbabilities[0];</pre>
			<pre class="source-code">  // add Shapley values (bar-delimited) to output string</pre>
			<pre class="source-code">  if(doShapley) {</pre>
			<pre class="source-code">    outputString += "  SHAP VALUES &gt; 0.01: ";</pre>
			<pre class="source-code">    for (int i=0; i &lt; p.contributions.length; i++) {</pre>
			<pre class="source-code">        // retrieving only Shap values over 0.01</pre>
			<pre class="source-code">        if (p.contributions[i] &lt;  0.01) continue;</pre>
			<pre class="source-code">        outputString += model.getContributionNames()[i] + ": "</pre>
			<pre class="source-code">        + p.contributions[i] + "|" ;</pre>
			<pre class="source-code">    }</pre>
			<pre class="source-code">    return outputString;</pre>
			<pre class="source-code">}</pre>
			<p>The main point here is that the prediction results are held in the <strong class="source-inline">h2o-genmodel</strong> API's <strong class="source-inline">BinomialModelPrediction p</strong> object that was returned from scoring. We can retrieve a lot of information from this object. In our case, we retrieved the predicted class, identified by <strong class="source-inline">p.label</strong> , and its probability, <strong class="source-inline">p.classProbabilities[0]</strong>. Since this is a <strong class="source-inline">BinomialModelPrediction</strong>, the probability of the other class would be retrieved by <strong class="source-inline">p.classProbabilities[1]</strong>.</p>
			<p>We then iterated through an array of the Shapley reason contribution names (<strong class="source-inline">model.getContributionNames()[i]</strong>) and values (<strong class="source-inline">p.contributions[i]</strong>). In our case, we are retrieving only reason codes with values over <strong class="source-inline">0.01</strong>. Alternatively, for <a id="_idIndexMarker696"/>example, we could have sorted <a id="_idIndexMarker697"/>the reasons by value and returned the top five. When returning all reasons, a bias is returned as the last in the array, and the sum of all features and the bias will equal the raw prediction of the model. </p>
			<p>Altogether, we used a bunch of code to format all of this into a CSV string starting with the original record and then appending the predicted class and its probability, and then a bar-delimited list of reason codes. </p>
			<h3>Running the code</h3>
			<p>To run the <a id="_idIndexMarker698"/>application, compile <strong class="source-inline">BatchFileScorer.java</strong> with <strong class="source-inline">h2o-genmodel.jar</strong> as an executable JAR file called <strong class="source-inline">BatchFileScorer.jar</strong>. Then, run the following command in the same directory as <strong class="source-inline">BatchFileScorer.jar</strong>:</p>
			<p class="source-code">java -jar BatchFileScorer.jar \  </p>
			<p class="source-code">path/to/file/to/score \</p>
			<p class="source-code">path/to/mojo</p>
			<p>To retrieve Shapley reason codes, append <strong class="source-inline">--shap</strong> to the statement.</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor174"/>Other things to know about MOJOs</h1>
			<p>You are now ready to deploy MOJOs, with or without required wrappers, as articulated in the previous section. Let's round up our knowledge of MOJOs by addressing the following secondary topics.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor175"/>Inspecting MOJO decision logic</h2>
			<p>For tree-based models, you can use a utility built into <strong class="source-inline">h2o-genmodel.jar</strong> to generate a <a id="_idIndexMarker699"/>graphical representation of the tree logic in the MOJO. Here is how.</p>
			<p>Let's use the same MOJO we used in the previous coding example of building a wrapper class. On the command line where your <strong class="source-inline">h2o-genmodel.jar</strong> is located, run the following:</p>
			<p class="source-code">java -cp h2o-genmodel.jar hex.genmodel.tools.PrintMojo \ </p>
			<p class="source-code">-i "path/to/mojo" \</p>
			<p class="source-code">-o tree.png \</p>
			<p class="source-code">--format png \</p>
			<p class="source-code">--tree 0</p>
			<p>This will create a <strong class="source-inline">.png</strong> file that looks like this:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/Figure_9.7_B16721.jpg" alt="Figure 9.7 – Output of the PrintMojo utility&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Output of the PrintMojo utility</p>
			<p>Note that if you omitted <strong class="source-inline">--tree 0</strong>, you would have generated a folder holding a forest of all trees. We have specified to return only the first one. </p>
			<p>You can also <a id="_idIndexMarker700"/>use <strong class="source-inline">dot</strong> for <strong class="source-inline">--format</strong>. This produces a format that can be consumed by the third-party <strong class="bold">Graphviz</strong> utility to make the graphical representation more prettified than that shown in <em class="italic">Figure 9.7</em>. </p>
			<p>Alternatively, if you wish to include this output for programmatic use, for <strong class="source-inline">–format</strong>, state <strong class="source-inline">.json</strong>, which outputs the file to JSON format. </p>
			<p>See the H2O documentation for more details and configuration alternatives: <a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html#viewing-a-mojo-model">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html#viewing-a-mojo-model</a>.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor176"/>MOJO and POJO</h2>
			<p>OK, let's say it: MOJOs are <a id="_idIndexMarker701"/>not the only H2O deployable artifact. Before <a id="_idIndexMarker702"/>MOJOs, there were only <strong class="bold">Plain Old Java Objects</strong> (<strong class="bold">POJOs</strong>). MOJOs and POJOs are similar from a deployment <a id="_idIndexMarker703"/>viewpoint; they are generated from model building, are ready to deploy, and use the <strong class="source-inline">h2o-genmodel</strong> API to build wrapper <a id="_idIndexMarker704"/>classes, as we discussed before. They are also a bit different. Let's compare, contrast, and conclude.</p>
			<h3>MOJO and POJO similarities</h3>
			<p>The following are <a id="_idIndexMarker705"/>the similarities between <a id="_idIndexMarker706"/>MOJOs and POJOs: </p>
			<ul>
				<li>They are both exported from the IDE after your model is built (or from the H2O Flow UI).</li>
				<li>They are both deployed in the same way: they both run in a JVM, there is the wrapper or no wrapper distinction depending on the target scoring system (H2O Scorers, third-party, or your own software program), and they both use the MOJO runtime (<strong class="source-inline">h2o-genmodel.jar</strong>) and the same API and Javadoc.</li>
			</ul>
			<h3>MOJO and POJO differences</h3>
			<p>These are the <a id="_idIndexMarker707"/>differences between MOJOs <a id="_idIndexMarker708"/>and POJOs:</p>
			<ul>
				<li>A POJO is exported as a single <strong class="source-inline">.java</strong> file that needs to be compiled, whereas the MOJO exports as a single <strong class="source-inline">.zip</strong> file, as described earlier.</li>
				<li>POJOs contain entire trees to navigate the model, whereas MOJOs contain tree metadata and use generic tree-walker code in <strong class="source-inline">h2o-genmodel.zip</strong> to navigate the model. The larger the tree structure, the larger the POJO.</li>
				<li>POJOs are significantly larger than MOJOs (typically 20-25 times larger) and slower than MOJOs when scoring (2-3 times slower). In general, the larger the POJO, the slower it is compared to any MOJO built from the same model.</li>
				<li>Large POJOs may have trouble compiling. POJOs over 1 GB are not supported by H2O.</li>
			</ul>
			<h3>When to use either a MOJO or POJO</h3>
			<p>You should view POJOs as deprecated but still supported (except &gt; 1GB) and sometimes needed in <a id="_idIndexMarker709"/>edge cases. Know that MOJOs are not fully <a id="_idIndexMarker710"/>supported across all algorithms so, in these cases, you are forced to use POJOs. Therefore, use MOJOs when you can and resort to POJOs in infrequent cases when you cannot.</p>
			<p class="callout-heading">Deployment Decision – MOJO or POJO?</p>
			<p class="callout">See MOJOs as your go-to current technology and POJOs as similar to deploy but deprecated yet supported (except &gt; 1 GB). MOJOs have advantages primarily in scoring speed and size footprint.</p>
			<p class="callout">MOJOs are not supported for some algorithms. Check the H2O documentation for current support considerations for MOJOs and POJOs.</p>
			<p>We are now ready to summarize.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor177"/>Summary</h1>
			<p>We began this chapter by taking a high-level view of the transition from model building to model deployment. We saw that this transition is bridged for H2O by the MOJO, a deployable representation of the trained model that is easy to generate from model building and easy to deploy for fast model scoring. </p>
			<p>We then took a closer look at the range of target systems MOJOs can be deployed on, and saw that these must run in a Java runtime but, otherwise, are quite diverse. MOJOs can be scored on real-time, batch, and streaming systems, usefully categorized as H2O Scorers (scoring software provided and supported by H2O), third-party integrations (software provided and supported by companies other than H2O), and your software integrations (software that you build and maintain).</p>
			<p>This categorization of target systems helps us determine whether you can deploy the exported MOJO directly, or whether you need to wrap it in a Java class using the <strong class="source-inline">h2o-genmodel</strong> API to embed it into the scoring software. H2O Scorers and some third-party scorers require only the exported MOJO and no wrapper to be implemented.</p>
			<p>We then took a detailed look at the MOJO and the MOJO runtime, and how these relate to deployments with and without the need for wrappers. We described the general structure of a MOJO wrapper and coded a wrapper to batch score records from a file. Our coding gave us a better understanding of the MOJO API that is used to interact with the MOJO in your application. This understanding included how to use the API to load the MOJO, structure data to a type that can be used by the MOJO, score with the MOJO, and retrieve predictions and reason codes from the scoring results.</p>
			<p>We then learned how to use a handy tool in the MOJO API to obtain a visual, JSON, or dot representation of the decision logic in the MOJO for your model.</p>
			<p>Finally, we introduced the predecessor of the MOJO, the POJO, and characterized it as similar to the MOJO in terms of deployment and use of the MOJO API but deprecated yet supported, and so to be used for a minority of cases when MOJOs cannot. </p>
			<p>Now, we understand in great detail the MOJO and how it is flexibly deployed to a diversity of production scoring systems. Let's move to the next chapter where we will exhibit this flexibility and diversity by describing concrete MOJO deployments on a handful of these systems.</p>
		</div>
	</body></html>