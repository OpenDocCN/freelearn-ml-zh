["```py\ndata <- read.csv('customer_data.csv')\nmodel <- glm(purchased ~ age + income, data = data, family = binomial())\nnew_customer <- data.frame(age = 35, income = 50000)\nprob_purchase <- predict(model, new_customer, type = \"response\")\nif (prob_purchase >= 0.5) {\n  print(\"The customer is predicted to purchase the product.\")\n} else {\n  print(\"The customer is predicted not to purchase the product.\")\n}\n```", "```py\nimport pandas as pd\ndata = pd.read_csv(\"customer_data.csv\")\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(data[['age', 'income']], data['purchased'])\nnew_customer = pd.DataFrame({'age': [35], 'income': [50000]})\nprob_purchase = model.predict_proba(new_customer)[:, 1]\nif prob_purchase >= 0.5:\n    print(\"The customer is predicted to purchase the product.\")\nelse:\n    print(\"The customer is predicted not to purchase the product.\")\n```", "```py\nurl <- \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\"\nhousing <- read.csv(url)\nhousing <- na.omit(housing)\nset.seed(123)\ntrain_index <- sample(nrow(housing), nrow(housing) * 0.8)\ntrain <- housing[train_index, ]\ntest <- housing[-train_index, ]\nlibrary(glmnet)\nx <- model.matrix(median_house_value ~ ., train)[,-1]\ny <- train$median_house_value\nmodel <- cv.glmnet(x, y, alpha = 1)\nx_test <- model.matrix(median_house_value ~ ., test)[,-1]\ny_test <- test$median_house_value\npredictions <- predict(model, newx = x_test)\nrmse <- sqrt(mean((predictions - y_test)^2))\nprint(paste0(\"RMSE: \", rmse))\n```", "```py\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\"\nhousing = pd.read_csv(url)\nhousing = pd.get_dummies(housing, columns=['ocean_proximity'])\nhousing.dropna(inplace=True)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(housing.drop(columns=['median_house_value']), housing['median_house_value'], test_size=0.2, random_state=123)\nfrom sklearn.linear_model import LassoCV\nmodel = LassoCV(alphas=[0.001, 0.01, 0.1, 1], cv=5)\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error\npredictions = model.predict(X_test)\nrmse = mean_squared_error(y_test, predictions, squared=False)\nprint(f\"RMSE: {rmse}\")\n```", "```py\ndata(iris)\niris_cluster <- iris[, c(\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\")]\nset.seed(123)\nkmeans_results <- kmeans(iris_cluster, centers = 3)\nkmeans_results$cluster\n```", "```py\nfrom sklearn.datasets import load_iris\niris = load_iris()\niris_cluster = iris.data[:, [0, 1, 2, 3]]\nfrom sklearn.cluster import KMeans\nkmeans_results = KMeans(n_clusters=3, random_state=123).fit(iris_cluster)\nprint(kmeans_results.labels_)\n```", "```py\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0 0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0 0 2]\n```", "```py\ndata <- data.frame(\n  Animal = c(\"Dog\", \"Cat\", \"Parrot\", \"Eagle\", \"Snake\"),\n  Has_fur = c(\"Yes\", \"Yes\", \"No\", \"No\", \"No\"),\n  Has_feathers = c(\"No\", \"No\", \"Yes\", \"Yes\", \"No\"),\n  Eats_meat = c(\"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\"),\n  Classification = c(\"Mammal\", \"Mammal\", \"Bird\", \"Bird\", \"Reptile\")\n)\nlibrary(rpart)\nlibrary(rpart.plot)\ntree <- rpart(Classification ~ Has_fur + Has_feathers + Eats_meat, data = data, method = \"class\", control = rpart.control(minsplit = 1))\nrpart.plot(tree, type=5)\nnew_data <- data.frame(\n  Has_fur = \"Yes\",\n  Has_feathers = \"No\",\n  Eats_meat = \"Yes\"\n)\npredicted <- predict(tree, new_data, type = \"class\")\nprint(predicted)\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nimport matplotlib.pyplot as plt\ndata = pd.DataFrame({\n    'Animal': ['Dog', 'Cat', 'Parrot', 'Eagle', 'Snake'],\n    'Has_fur': ['Yes', 'Yes', 'No', 'No', 'No'],\n    'Has_feathers': ['No', 'No', 'Yes', 'Yes', 'No'],\n    'Eats_meat': ['Yes', 'Yes', 'No', 'Yes', 'Yes'],\n    'Classification': ['Mammal', 'Mammal', 'Bird', 'Bird', 'Reptile']\n})\ndata_encoded = pd.get_dummies(data[['Has_fur', 'Has_feathers', 'Eats_meat']])\nclf = DecisionTreeClassifier(criterion='entropy', min_samples_split=2)\nclf.fit(data_encoded, data['Classification'])\nplt.figure(figsize=(8, 6))\nplot_tree(clf, feature_names=data_encoded.columns, class_names=np.unique(data['Classification']), filled=True)\nplt.show()\nnew_data = pd.DataFrame({\n    'Has_fur_No': [0],\n    'Has_fur_Yes': [1],\n    'Has_feathers_No': [1],\n    'Has_feathers_Yes': [0],\n    'Eats_meat_No': [0],\n    'Eats_meat_Yes': [1]\n})\npredicted = clf.predict(new_data)\nprint(predicted)\n```", "```py\nlibrary(xgboost)\nlibrary(caret)\ndata(iris)\nset.seed(123)\ntrainIndex <- createDataPartition(iris$Species, p = 0.8, list = FALSE)\ntrain <- iris[trainIndex, ]\ntest <- iris[-trainIndex, ]\ntrain$Species <- as.factor(train$Species)\ntest$Species <- as.factor(test$Species)\ntrain$label <- as.integer(train$Species) - 1\ntest$label <- as.integer(test$Species) - 1\nxgb_model <- xgboost(data = as.matrix(train[, 1:4]),\n                     label = train$label,\n                     nrounds = 10,\n                     objective = \"multi:softmax\",\n                     num_class = 3,\n                     eval_metric = \"mlogloss\")\npredictions <- predict(xgb_model, as.matrix(test[, 1:4]))\npredictions <- factor(predictions, levels = 0:2, labels = levels(iris$Species))\nconfusionMatrix(predictions, test$Species)\n```", "```py\nimport xgboost as xgb\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=123)\nxgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", \n      n_estimators=10, seed=123)\nxgb_model.fit(X_train, y_train)\npredictions = xgb_model.predict(X_test)\nprint(classification_report(y_test, predictions))\n```", "```py\nlibrary(e1071)\ndata(iris)\nset.seed(123)\ntrainIndex <- sample(nrow(iris), 0.7 * nrow(iris))\ntrain <- iris[trainIndex, ]\ntest <- iris[-trainIndex, ]\nmodel <- naiveBayes(Species ~ ., data = train)\npredictions <- predict(model, test)\ncfm <- table(predictions, test$Species)\nprint(cfm)\n```", "```py\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, \n      iris.target, test_size=0.3, random_state=123)\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\nprint(classification_report(y_test, predictions))\n```"]