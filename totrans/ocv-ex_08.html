<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Video Surveillance, Background Modeling, and Morphological Operations"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Video Surveillance, Background Modeling, and Morphological Operations</h1></div></div></div><p>In this chapter, we will learn how to detect a moving object in a video that is taken from a static camera. This is used extensively in video surveillance systems. We will discuss the different characteristics that can be used to build this system. We will learn about background modeling and see how we can use it to build a model of the background in a live video. Once we do this, we will combine all the blocks to detect the objects of interest in the video.</p><p>By the end of this chapter, you should be able to answer the following questions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What is naive background subtraction?</li><li class="listitem" style="list-style-type: disc">What is frame differencing?</li><li class="listitem" style="list-style-type: disc">How to build a background model?</li><li class="listitem" style="list-style-type: disc">How to identify a new object in a static video?</li><li class="listitem" style="list-style-type: disc">What is morphological image processing and how is it related to background modeling?</li><li class="listitem" style="list-style-type: disc">How to achieve different effects using morphological operators?</li></ul></div><div class="section" title="Understanding background subtraction"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec54"/>Understanding background subtraction</h1></div></div></div><p>Background subtraction<a id="id318" class="indexterm"/> is very useful in video surveillance. Basically, the background subtraction technique performs really well in cases where we need to detect moving objects in a static scene. Now, how is this useful for video surveillance? The process of video surveillance involves dealing with a constant data flow. The data stream keeps coming in at all times, and we need to analyze it to identify any suspicious activities. Let's consider the example of a hotel lobby. All the walls and furniture have a fixed location. Now, if we build a background model, we can use it to identify suspicious activities in the lobby. We can take advantage of the fact that the background scene remains static (which happens to be true in this case). This helps us avoid any unnecessary computation overheads.</p><p>As the name suggests, this <a id="id319" class="indexterm"/>algorithm works by detecting the background and assigning each pixel of an image to two classes: either the background (assuming that it's static and stable) or the foreground. It then subtracts the background from the current frame to obtain the foreground. By the static assumption, foreground objects will naturally correspond to objects or people moving in front of the background.</p><p>In order to detect moving objects, we first need to build a model of the background. This is not the same as direct frame differencing because we are actually modeling the background and using this model to detect moving objects. When we say that we are <span class="emphasis"><em>modeling the background</em></span>, we are basically building a mathematical formulation that can be used to represent the background. So, this performs in a much better way than the simple frame differencing technique. This technique tries to detect static parts of the scene and then updates the background model. This background model is then used to detect background pixels. So, it's an adaptive technique that can adjust according to the scene.</p></div></div>
<div class="section" title="Naive background subtraction"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec55"/>Naive background subtraction</h1></div></div></div><p>Let's <a id="id320" class="indexterm"/>start the background subtraction discussion<a id="id321" class="indexterm"/> from the beginning. What does a background subtraction process look like? Consider the following image:</p><div class="mediaobject"><img src="graphics/B04283_08_01.jpg" alt="Naive background subtraction"/></div><p>The preceding<a id="id322" class="indexterm"/> image represents the background scene. Now, let's<a id="id323" class="indexterm"/> introduce a new object into this scene:</p><div class="mediaobject"><img src="graphics/B04283_08_02.jpg" alt="Naive background subtraction"/></div><p>As shown in <a id="id324" class="indexterm"/>the preceding image, there is a new object in the scene. So, if we compute the difference <a id="id325" class="indexterm"/>between this image and our background model, you should be able to identify the location of the TV remote:</p><div class="mediaobject"><img src="graphics/B04283_08_03.jpg" alt="Naive background subtraction"/></div><p>The overall process looks like this:</p><div class="mediaobject"><img src="graphics/B04283_08_04.jpg" alt="Naive background subtraction"/></div><div class="section" title="Does it work well?"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec42"/>Does it work well?</h2></div></div></div><p>There's a <a id="id326" class="indexterm"/>reason why we call it the <span class="strong"><strong>naive</strong></span> approach. It works under ideal conditions, and as we know, nothing is ideal in the real world. It does a reasonably good job of computing the shape of the given object, but it does so under some constraints. One of the main requirements of this approach is that the color and intensity of the object should be sufficiently different from that of the background. Some of the factors that affect these kinds of algorithms are image noise, lighting conditions, autofocus in cameras, and so on.</p><p>Once a new object enters our scene and stays there, it will be difficult to detect new objects that are in front of it. This is because we don't update our background model, and the new object is now part of our background. Consider the following image:</p><div class="mediaobject"><img src="graphics/B04283_08_05.jpg" alt="Does it work well?"/></div><p>Now, let's say a<a id="id327" class="indexterm"/> new object enters our scene:</p><div class="mediaobject"><img src="graphics/B04283_08_06.jpg" alt="Does it work well?"/></div><p>We identify this<a id="id328" class="indexterm"/> to be a new object, which is fine. Let's say another object comes into the scene:</p><div class="mediaobject"><img src="graphics/B04283_08_07.jpg" alt="Does it work well?"/></div><p>It will be difficult <a id="id329" class="indexterm"/>to identify the location of these two different objects because their locations overlap. Here's what we get after subtracting the background and applying the threshold:</p><div class="mediaobject"><img src="graphics/B04283_08_08.jpg" alt="Does it work well?"/></div><p>In this approach, we assume that the background is static. If some parts of our background start moving, then those parts will start getting detected as new objects. So, even if the movements are minor, say a waving flag, it will cause problems in our detection algorithm. This approach is also sensitive to changes in illumination, and it cannot handle any camera movement. Needless to say, it's a delicate approach! We need something that can handle all these things in the real world.</p></div></div>
<div class="section" title="Frame differencing"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec56"/>Frame differencing</h1></div></div></div><p>We <a id="id330" class="indexterm"/>know that we cannot keep a static background image that can be used to detect objects. So, one of the ways to fix this would be to use frame differencing. It is one of the simplest techniques that we can use to see what parts of the video are moving. When we consider a live video stream, the difference between successive frames gives a lot of information. The concept is fairly straightforward. We just take the difference between successive frames and display the difference.</p><p>If I move my laptop rapidly, we can see something like this:</p><div class="mediaobject"><img src="graphics/B04283_08_09.jpg" alt="Frame differencing"/></div><p>Instead of the <a id="id331" class="indexterm"/>laptop, let's move the object and see what happens. If I rapidly shake my head, it will look something like this:</p><div class="mediaobject"><img src="graphics/B04283_08_10.jpg" alt="Frame differencing"/></div><p>As you can see in the preceding images, only the moving parts of the video get highlighted. This <a id="id332" class="indexterm"/>gives us a good starting point to see the areas that are moving in the video. Let's take a look at the function to compute the frame difference:</p><div class="informalexample"><pre class="programlisting">Mat frameDiff(Mat prevFrame, Mat curFrame, Mat nextFrame)
{
    Mat diffFrames1, diffFrames2, output;
    
    // Compute absolute difference between current frame and the next frame
    absdiff(nextFrame, curFrame, diffFrames1);
    
    // Compute absolute difference between current frame and the previous frame
    absdiff(curFrame, prevFrame, diffFrames2);
    
    // Bitwise "AND" operation between the above two diff images
    bitwise_and(diffFrames1, diffFrames2, output);
    
    return output;
}</pre></div><p>Frame differencing is fairly straightforward. You compute the absolute difference between the current frame and previous frame and between the current frame and next frame. We then take these frame differences and apply a bitwise AND operator. This will highlight the moving parts in the image. If you just compute the difference between the current frame and previous frame, it tends to be noisy. Hence, we need to use the bitwise AND operator between successive frame differences to get some stability when we see the moving objects.</p><p>Let's take a look at the function that can extract and return a frame from the webcam:</p><div class="informalexample"><pre class="programlisting">Mat getFrame(VideoCapture cap, float scalingFactor)
{
    //float scalingFactor = 0.5;
    Mat frame, output;

    // Capture the current frame
    cap &gt;&gt; frame;
    
    // Resize the frame
    resize(frame, frame, Size(), scalingFactor, scalingFactor, INTER_AREA);
    
    // Convert to grayscale
    cvtColor(frame, output, CV_BGR2GRAY);
    
    return output;
}</pre></div><p>As we can see, it's <a id="id333" class="indexterm"/>pretty straightforward. We just need to resize the frame and convert it to grayscale. Now that we have the helper functions ready, let's take a look at the <code class="literal">main</code> function and see how it all comes together:</p><div class="informalexample"><pre class="programlisting">int main(int argc, char* argv[])
{
    Mat frame, prevFrame, curFrame, nextFrame;
    char ch;
 
    // Create the capture object
    // 0 -&gt; input arg that specifies it should take the input from the webcam
    VideoCapture cap(0);
    
    // If you cannot open the webcam, stop the execution!
    if( !cap.isOpened() )
        return -1;
    
    //create GUI windows
    namedWindow("Frame");
    
    // Scaling factor to resize the input frames from the webcam
    float scalingFactor = 0.75;
    
    prevFrame = getFrame(cap, scalingFactor);
    curFrame = getFrame(cap, scalingFactor);
    nextFrame = getFrame(cap, scalingFactor);
    
    // Iterate until the user presses the Esc key
    while(true)
    {
        // Show the object movement
        imshow("Object Movement", frameDiff(prevFrame, curFrame, nextFrame));
        
        // Update the variables and grab the next frame
        prevFrame = curFrame;
        curFrame = nextFrame;
        nextFrame = getFrame(cap, scalingFactor);
        
        // Get the keyboard input and check if it's 'Esc'
        // 27 -&gt; ASCII value of 'Esc' key
        ch = waitKey( 30 );
        if (ch == 27) {
            break;
        }
    }
    
    // Release the video capture object
    cap.release();
    
    // Close all windows
    destroyAllWindows();
    
    return 1;
}</pre></div><div class="section" title="How well does it work?"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec43"/>How well does it work?</h2></div></div></div><p>As we can see, frame differencing<a id="id334" class="indexterm"/> addresses a couple of important problems that we faced earlier. It can quickly adapt to lighting changes or camera movements. If an object comes in the frame and stays there, it will not be detected in the future frames. One of the main concerns of this approach is about detecting uniformly colored objects. It can only detect the edges of a uniformly colored object. This is because a large portion of this object will result in very low pixel differences, as shown in the following image:</p><div class="mediaobject"><img src="graphics/B04283_08_11.jpg" alt="How well does it work?"/></div><p>Let's say this <a id="id335" class="indexterm"/>object moved slightly. If we compare this with the previous frame, it will look like this:</p><div class="mediaobject"><img src="graphics/B04283_08_12.jpg" alt="How well does it work?"/></div><p>Hence, we have very few pixels that are labeled on that object. Another concern is that it is difficult to detect <a id="id336" class="indexterm"/>whether an object is moving toward the camera or away from it.</p></div></div>
<div class="section" title="The Mixture of Gaussians approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec57"/>The Mixture of Gaussians approach</h1></div></div></div><p>Before we <a id="id337" class="indexterm"/>talk about <span class="strong"><strong>Mixture of Gaussians</strong></span> (<span class="strong"><strong>MOG</strong></span>), let's see what a <span class="emphasis"><em>mixture model</em></span> is. A mixture model is just a statistical model that can be used to represent the presence of subpopulations within our data. We don't really care about what category each data point belongs to. All we need to do is identify whether the data has multiple groups inside it. Now, if we represent each subpopulation using the Gaussian function, then it's called Mixture of Gaussians. Let's consider the following image:</p><div class="mediaobject"><img src="graphics/B04283_08_13.jpg" alt="The Mixture of Gaussians approach"/></div><p>Now, as we gather more frames in this scene, every part of the image will gradually become part of the background model. This is what we discussed earlier as well. If a scene is static, the model adapts itself to make sure that the background model is updated. The foreground mask, which is supposed to represent the foreground object, looks like a black image at this point because every pixel is part of the background model.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note26"/>Note</h3><p>OpenCV has multiple algorithms implemented for the Mixture of Gaussians approach. One of them is <a id="id338" class="indexterm"/>called <span class="strong"><strong>MOG</strong></span> and the other is called<a id="id339" class="indexterm"/> <span class="strong"><strong>MOG2</strong></span>. To get a detailed explanation, you can refer to <a class="ulink" href="http://docs.opencv.org/master/db/d5c/tutorial_py_bg_subtraction.html#gsc.tab=0">http://docs.opencv.org/master/db/d5c/tutorial_py_bg_subtraction.html#gsc.tab=0</a>. You will also be able check out the original research papers that were used to implement these algorithms.</p></div></div><p>Let's introduce a <a id="id340" class="indexterm"/>new object into this scene and see what the foreground mask looks like using the MOG approach:</p><div class="mediaobject"><img src="graphics/B04283_08_14.jpg" alt="The Mixture of Gaussians approach"/></div><p>Let's wait for some time and introduce a new object into the scene. Let's take a look at what the new foreground mask looks like using the MOG2 approach:</p><div class="mediaobject"><img src="graphics/B04283_08_15.jpg" alt="The Mixture of Gaussians approach"/></div><p>As you can see in the <a id="id341" class="indexterm"/>preceding images, the new objects are being identified correctly. Let's take a look at the interesting part of the code (you can get the complete code in the <code class="literal">.cpp</code> files):</p><div class="informalexample"><pre class="programlisting">int main(int argc, char* argv[])
{
    // Variable declarations and initializations
    
    // Iterate until the user presses the Esc key
    while(true)
    {
        // Capture the current frame
        cap &gt;&gt; frame;
        
        // Resize the frame
        resize(frame, frame, Size(), scalingFactor, scalingFactor, INTER_AREA);
        
        // Update the MOG background model based on the current frame
        pMOG-&gt;operator()(frame, fgMaskMOG);
        
        // Update the MOG2 background model based on the current frame
        pMOG2-&gt;operator()(frame, fgMaskMOG2);
        
        // Show the current frame
        //imshow("Frame", frame);
        
        // Show the MOG foreground mask
        imshow("FG Mask MOG", fgMaskMOG);

        // Show the MOG2 foreground mask
        imshow("FG Mask MOG 2", fgMaskMOG2);
        
        // Get the keyboard input and check if it's 'Esc'
        // 27 -&gt; ASCII value of 'Esc' key
        ch = waitKey( 30 );
        if (ch == 27) {
            break;
        }
    }
    
    // Release the video capture object
    cap.release();
    
    // Close all windows
    destroyAllWindows();
    
    return 1;
}</pre></div><div class="section" title="What happened in the code?"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec44"/>What happened in the code?</h2></div></div></div><p>Let's <a id="id342" class="indexterm"/>quickly go through the code and see what's happening there. We use the Mixture of Gaussians model to create a background subtractor object. This object represents the model that will be updated as and when we encounter new frames from the webcam. As we can see in the code, we initialize two background subtraction models: <code class="literal">BackgroundSubtractorMOG</code> and <code class="literal">BackgroundSubtractorMOG2</code>. They represent two different algorithms that are used for background subtraction. The first one refers to the paper by <span class="emphasis"><em>P. KadewTraKuPong</em></span> and <span class="emphasis"><em>R. Bowden titled</em></span>, <span class="emphasis"><em>An improved adaptive background mixture model for real-time tracking with shadow detection</em></span>. You can check it out at <a class="ulink" href="http://personal.ee.surrey.ac.uk/Personal/R.Bowden/publications/avbs01/avbs01.pdf">http://personal.ee.surrey.ac.uk/Personal/R.Bowden/publications/avbs01/avbs01.pdf</a>. The second one refers to the paper by <span class="emphasis"><em>Z.Zivkovic</em></span> titled, <span class="emphasis"><em>Improved adaptive Gausian Mixture Model for background subtraction</em></span>. You can check it out at <a class="ulink" href="http://www.zoranz.net/Publications/zivkovic2004ICPR.pdf">http://www.zoranz.net/Publications/zivkovic2004ICPR.pdf</a>. We start an infinite <code class="literal">while</code> loop and continuously read the input frames from the webcam. With each frame, we update the background model, as shown in the following lines:</p><div class="informalexample"><pre class="programlisting">pMOG-&gt;operator()(frame, fgMaskMOG);
pMOG2-&gt;operator()(frame, fgMaskMOG2);</pre></div><p>The background <a id="id343" class="indexterm"/>model gets updated in these steps. Now, if a new object enters the scene and stays there, it will become part of the background model. This helps us overcome one of the biggest shortcomings of the naïve background subtraction model.</p></div></div>
<div class="section" title="Morphological image processing"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec58"/>Morphological image processing</h1></div></div></div><p>As discussed <a id="id344" class="indexterm"/>earlier, background subtraction methods are affected by many factors. Their accuracy depends on how we capture the data and how it's processed. One of the biggest factors that tend to affect these algorithms is the noise level. When we say <span class="emphasis"><em>noise</em></span>, we are talking about things, such as graininess in an image, isolated black/white pixels, and so on. These issues tend to affect the quality of our algorithms. This is where morphological image processing comes into picture. Morphological image processing is used extensively in a lot of real-time systems to ensure the quality of the output.</p><p>Morphological image processing refers to processing the shapes of features in the image. For example, you can make a shape thicker or thinner. Morphological operators rely on how the pixels are ordered in an image, but on their values. This is the reason why they are really well suited to manipulate shapes in binary images. Morphological image processing can be applied to grayscale images as well, but the pixel values will not matter much.</p><div class="section" title="What's the underlying principle?"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec45"/>What's the underlying principle?</h2></div></div></div><p>Morphological operators <a id="id345" class="indexterm"/>use a structuring element to modify an image. What is a structuring element? A structuring element is basically a small shape that can be used to inspect a small region in the image. It is positioned at all the pixel locations in the image so that it can inspect that neighborhood. We basically take a small window and overlay it on top of a pixel. Depending on the response, we take an appropriate action at that pixel location.</p><p>Let's consider the following input image:</p><div class="mediaobject"><img src="graphics/B04283_08_16.jpg" alt="What's the underlying principle?"/></div><p>We will apply <a id="id346" class="indexterm"/>a bunch of morphological operations to this image to see how the shape changes.</p></div></div>
<div class="section" title="Slimming the shapes"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec59"/>Slimming the shapes</h1></div></div></div><p>We can achieve this <a id="id347" class="indexterm"/>effect using an operation called <span class="strong"><strong>erosion</strong></span>. This is an operation that makes a shape thinner by peeling the boundary layers of all the shapes in the image:</p><div class="mediaobject"><img src="graphics/B04283_08_17.jpg" alt="Slimming the shapes"/></div><p>Let's take a look at the function that performs morphological erosion:</p><div class="informalexample"><pre class="programlisting">Mat performErosion(Mat inputImage, int erosionElement, int erosionSize)
{
    Mat outputImage;
    int erosionType;
    
    if(erosionElement == 0)
        erosionType = MORPH_RECT;
    
    else if(erosionElement == 1)
        erosionType = MORPH_CROSS;
    
    else if(erosionElement == 2)
        erosionType = MORPH_ELLIPSE;
    
    // Create the structuring element for erosion
    Mat element = getStructuringElement(erosionType, Size(2*erosionSize + 1, 2*erosionSize + 1), Point(erosionSize, erosionSize));
    
    // Erode the image using the structuring element
    erode(inputImage, outputImage, element);
    
    // Return the output image
    return outputImage;
}</pre></div><p>You can <a id="id348" class="indexterm"/>check out the complete code in the <code class="literal">.cpp</code> files to understand how to use this function. Basically, we build a structuring element using an built-in OpenCV function. This object is used as a probe to modify each pixel based on certain conditions. These <span class="emphasis"><em>conditions</em></span> refer to what's happening around that particular pixel in the image. For example, is it surrounded by white pixels? Or is it surrounded by black pixels? Once we have an answer, we can take an appropriate action.</p></div>
<div class="section" title="Thickening the shapes"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec60"/>Thickening the shapes</h1></div></div></div><p>We <a id="id349" class="indexterm"/>use an operation called <a id="id350" class="indexterm"/>
<span class="strong"><strong>dilation</strong></span> to achieve thickening. This is an operation that makes a shape thicker by adding boundary layers to all the shapes in the image:</p><div class="mediaobject"><img src="graphics/B04283_08_18.jpg" alt="Thickening the shapes"/></div><p>Here is the code to do this:</p><div class="informalexample"><pre class="programlisting">Mat performDilation(Mat inputImage, int dilationElement, int dilationSize)
{
    Mat outputImage;
    int dilationType;
    
    if(dilationElement == 0)
        dilationType = MORPH_RECT;
    
    else if(dilationElement == 1)
        dilationType = MORPH_CROSS;
    
    else if(dilationElement == 2)
        dilationType = MORPH_ELLIPSE;
    
    // Create the structuring element for dilation
    Mat element = getStructuringElement(dilationType, Size(2*dilationSize + 1, 2*dilationSize + 1), Point(dilationSize, dilationSize));
    
    // Dilate the image using the structuring element
    dilate(inputImage, outputImage, element);
    
    // Return the output image
    return outputImage;
}</pre></div></div>
<div class="section" title="Other morphological operators"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec61"/>Other morphological operators</h1></div></div></div><p>Here are some<a id="id351" class="indexterm"/> other morphological operators that are interesting. Let's first take a look at the output image. We can take a look at the code at the end of this section.</p><div class="section" title="Morphological opening"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec46"/>Morphological opening</h2></div></div></div><p>This is an <a id="id352" class="indexterm"/>operation that <span class="emphasis"><em>opens</em></span> a shape. This operator is frequently used for noise removal in an image. We can achieve morphological opening by applying erosion followed by dilation to an image. The morphological opening process basically removes small objects from the foreground in the image by placing them in the background:</p><div class="mediaobject"><img src="graphics/B04283_08_19.jpg" alt="Morphological opening"/></div><p>Here is the<a id="id353" class="indexterm"/> function to the perform morphological opening:</p><div class="informalexample"><pre class="programlisting">Mat performOpening(Mat inputImage, int morphologyElement, int morphologySize)
{
    Mat outputImage, tempImage;
    int morphologyType;
    
    if(morphologyElement == 0)
        morphologyType = MORPH_RECT;
    
    else if(morphologyElement == 1)
        morphologyType = MORPH_CROSS;
    
    else if(morphologyElement == 2)
        morphologyType = MORPH_ELLIPSE;
    
    // Create the structuring element for erosion
    Mat element = getStructuringElement(morphologyType, Size(2*morphologySize + 1, 2*morphologySize + 1), Point(morphologySize, morphologySize));
    
    // Apply morphological opening to the image using the structuring element
    erode(inputImage, tempImage, element);
    dilate(tempImage, outputImage, element);
    
    // Return the output image
    return outputImage;
}</pre></div><p>As we can see here, we apply erosion and dilation to the image to perform the morphological opening.</p></div><div class="section" title="Morphological closing"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec47"/>Morphological closing</h2></div></div></div><p>This is an <a id="id354" class="indexterm"/>operation that <span class="emphasis"><em>closes</em></span> a shape by filling the gaps. This operation is also used for noise removal. We achieve morphological closing by applying dilation followed by erosion to an image. This operation removes tiny holes in the foreground by changing small objects in the background into the foreground.</p><div class="mediaobject"><img src="graphics/B04283_08_20.jpg" alt="Morphological closing"/></div><p>Let's quickly take a look at the function to perform the morphological closing:</p><div class="informalexample"><pre class="programlisting">Mat performClosing(Mat inputImage, int morphologyElement, int morphologySize)
{
    Mat outputImage, tempImage;
    int morphologyType;
    
    if(morphologyElement == 0)
        morphologyType = MORPH_RECT;
    
    else if(morphologyElement == 1)
        morphologyType = MORPH_CROSS;
    
    else if(morphologyElement == 2)
        morphologyType = MORPH_ELLIPSE;
    
    // Create the structuring element for erosion
    Mat element = getStructuringElement(morphologyType, Size(2*morphologySize + 1, 2*morphologySize + 1), Point(morphologySize, morphologySize));
    
    // Apply morphological opening to the image using the structuring element
    dilate(inputImage, tempImage, element);
    erode(tempImage, outputImage, element);
    
    // Return the output image
    return outputImage;
}</pre></div></div><div class="section" title="Drawing the boundary"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec48"/>Drawing the boundary</h2></div></div></div><p>We achieve<a id="id355" class="indexterm"/> this using the morphological gradient. This is an operation that draws the boundary around a shape by taking the difference between dilation and erosion of an image:</p><div class="mediaobject"><img src="graphics/B04283_08_21.jpg" alt="Drawing the boundary"/></div><p>Let's take a look at the function to perform the morphological gradient:</p><div class="informalexample"><pre class="programlisting">Mat performMorphologicalGradient(Mat inputImage, int morphologyElement, int morphologySize)
{
    Mat outputImage, tempImage1, tempImage2;
    int morphologyType;
    
    if(morphologyElement == 0)
        morphologyType = MORPH_RECT;
    
    else if(morphologyElement == 1)
        morphologyType = MORPH_CROSS;
    
    else if(morphologyElement == 2)
        morphologyType = MORPH_ELLIPSE;
    
    // Create the structuring element for erosion
    Mat element = getStructuringElement(morphologyType, Size(2*morphologySize + 1, 2*morphologySize + 1), Point(morphologySize, morphologySize));
    
    // Apply morphological gradient to the image using the structuring element
    dilate(inputImage, tempImage1, element);
    erode(inputImage, tempImage2, element);
    
    // Return the output image
    return tempImage1 - tempImage2;
}</pre></div></div><div class="section" title="White Top-Hat transform"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec49"/>White Top-Hat transform</h2></div></div></div><p>While Top-Hat <a id="id356" class="indexterm"/>transform, also simply called Top-Hat transform, extracts finer details from the images. We can apply white top-hat transform <a id="id357" class="indexterm"/>by computing the difference between the input image and its morphological opening. This gives us the objects in the image that are smaller than the structuring elements and are brighter than the surroundings. So, depending on the size of the structuring element, we can extract various objects in the given image:</p><div class="mediaobject"><img src="graphics/B04283_08_22.jpg" alt="White Top-Hat transform"/></div><p>If you look carefully at the output image, you can see those black rectangles. This means that the structuring element was able to fit in there, and so those regions are blackened out. Here is the function to do this:</p><div class="informalexample"><pre class="programlisting">Mat performTopHat(Mat inputImage, int morphologyElement, int morphologySize)
{
    Mat outputImage;
    int morphologyType;
    
    if(morphologyElement == 0)
        morphologyType = MORPH_RECT;
    
    else if(morphologyElement == 1)
        morphologyType = MORPH_CROSS;
    
    else if(morphologyElement == 2)
        morphologyType = MORPH_ELLIPSE;
    
    // Create the structuring element for erosion
    Mat element = getStructuringElement(morphologyType, Size(2*morphologySize + 1, 2*morphologySize + 1), Point(morphologySize, morphologySize));
    
    // Apply top hat operation to the image using the structuring element
    outputImage = inputImage - performOpening(inputImage, morphologyElement, morphologySize);
    
    // Return the output image
    return outputImage;
}</pre></div></div><div class="section" title="Black Top-Hat transform"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec50"/>Black Top-Hat transform</h2></div></div></div><p>Black <a id="id358" class="indexterm"/>Top-Hat transform, also simply called Black Hat transform, extracts finer details from the <a id="id359" class="indexterm"/>image as well. We can apply black top-hat transform by computing the difference between the morphological closing of an image and the image itself. This gives us the objects in the image that are smaller than the structuring element and are darker than the surroundings.</p><div class="mediaobject"><img src="graphics/B04283_08_23.jpg" alt="Black Top-Hat transform"/></div><p>Let's take a look at the function to perform the black hat transform:</p><div class="informalexample"><pre class="programlisting">Mat performBlackHat(Mat inputImage, int morphologyElement, int morphologySize)
{
    Mat outputImage;
    int morphologyType;
    
    if(morphologyElement == 0)
        morphologyType = MORPH_RECT;
    
    else if(morphologyElement == 1)
        morphologyType = MORPH_CROSS;
    
    else if(morphologyElement == 2)
        morphologyType = MORPH_ELLIPSE;
    
    // Create the structuring element for erosion
    Mat element = getStructuringElement(morphologyType, Size(2*morphologySize + 1, 2*morphologySize + 1), Point(morphologySize, morphologySize));
    
    // Apply black hat operation to the image using the structuring element
    outputImage = performClosing(inputImage, morphologyElement, morphologySize) - inputImage;
    
    // Return the output image
    return outputImage;
}</pre></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec62"/>Summary</h1></div></div></div><p>In this chapter, we learned about the algorithms that are used for background modeling and morphological image processing. We discussed naïve background subtraction and its limitations. We learned how to get motion information using frame differencing and how it can be constrain us when we want to track different types of objects. We also discussed Mixture of Gaussians, along with its formulation and implementation details. We then discussed morphological image processing. We learned how it can be used for various purposes and different operations were demonstrated to show the use cases.</p><p>In the next chapter, we will discuss how to track an object and the various techniques that can be used to do it.</p></div></body></html>