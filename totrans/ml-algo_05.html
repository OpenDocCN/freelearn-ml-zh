<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Logistic Regression</h1>
                </header>
            
            <article>
                
<p>This chapter begins by analyzing linear classification problems, with particular focus on logistic regression (despite its name, it's a classification algorithm) and stochastic gradient descent approaches. Even if these strategies appear too simple, they're still the main choices in many classification tasks. Speaking of which, it's useful to remember a very important philosophical principle: <strong>Occam's razor</strong>. In our context, it states that the first choice must always be the simplest and only if it doesn't fit, it's necessary to move on to more complex models. In the second part of the chapter, we're going to discuss some common metrics useful to evaluate a classification task. They are not limited to linear models, so we use them when talking about different strategies as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear classification</h1>
                </header>
            
            <article>
                
<p>Let's consider a generic linear classification problem with two classes. In the following figure, there's an example:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/1e3a5f30-f40e-4a6a-b4f8-dac180fb9041.png"/></div>
<p>Our goal is to find an optimal hyperplane, which separates the two classes. In multi-class problems, the strategy one-vs-all is normally adopted, so the discussion can be focused only on binary classifications. Suppose we have the following dataset:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="50" width="280" src="assets/c16cc001-8b7e-43d2-95c8-346bf6027dfc.png"/></div>
<p>This dataset is associated with the following target set:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="36" width="248" src="assets/389065d9-fc7c-4077-ae7c-1dd305531b84.png"/></div>
<p>We can now define a weight vector made of <em>m</em> continuous components:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="41" width="262" src="assets/c623901b-3744-48c9-9c46-5cff7b1365bf.png"/></p>
<p><span>We can also define the </span>quantity <em>z</em>:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="34" width="226" src="assets/19cfa97f-4466-4d4c-a13d-48476b14bed3.png"/></p>
<p>If <em>x</em> is a variable, <em>z</em> is the value determined by the hyperplane equation. Therefore, if the set of coefficients <em>w</em> that has been determined is correct, it happens that:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="42" width="201" src="assets/3a1dc317-1b6b-47d3-9317-eefa134e7787.png"/></p>
<p>Now we must find a way to optimize <em>w</em>, in order to reduce the classification error. If such a combination exists (with a certain error threshold), we say that our problem is <strong>linearly separable</strong>. On the other hand, when it's impossible to find a linear classifier, the problem is called <strong>non-linearly separable</strong>. A very simple but famous example is given by the logical operator <kbd>XOR</kbd>:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="378" width="402" class="image-border" src="assets/1f827264-813e-45ec-99a0-1e4de9acffcd.png"/></div>
<p>As you can see, any line will always include a wrong sample. Hence, in order to solve this problem, it is necessary to involve non-linear techniques. However, in many real-life cases, we use linear techniques (which are often simpler and faster) for non-linear problems too, accepting a tolerable misclassification error.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression</h1>
                </header>
            
            <article>
                
<p>Even if called regression, this is a classification method which is based on the probability for a sample to belong to a class. As our probabilities must be continuous in <em>R</em> and bounded between (0, 1), it's necessary to introduce a threshold function to filter the term <em>z</em>. The name logistic comes from the decision to use the sigmoid (or logistic) function:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="63" width="339" src="assets/b1d6d740-629e-4fac-a5a1-aae77e2a4178.png"/></div>
<p class="CDPAlignLeft CDPAlign">A partial plot of this function is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="321" width="516" class="image-border" src="assets/8de1bdb9-9e69-498f-af71-cf00358d5b99.png"/></div>
<p>As you can see, the function intersects <em>x=0</em> in the ordinate 0.5, and <em>y&lt;0.5</em> for <em>x&lt;0</em> and <em>y&gt;0.5</em> for <em>x&gt;0</em>. Moreover, its domain is <em>R</em> and it has two asymptotes at 0 and 1. So, we can define the probability for a sample to belong to a class (from now on, we'll call them 0 and 1) as:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="36" width="117" src="assets/2746315b-8631-46d3-bd76-3c5c679ada8b.png"/></p>
<p>At this point, finding the optimal parameters is equivalent to maximizing the log-likelihood given the output class:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="50" width="274" src="assets/730cc66f-36d3-4675-93a3-4b72d0b4d8bb.png"/></p>
<p>Therefore, the optimization problem can be expressed, using the indicator notation, as the minimization of the loss function:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="57" width="469" src="assets/5bb4b7d5-2946-434e-a49f-6ed10a250b80.png"/></p>
<p>If <em>y=0</em>, the first term becomes null and the second one becomes <em>log(1-x)</em>, which is the log-probability of the class 0. On the other hand, if <em>y=1</em>, the second term is 0 and the first one represents the log-probability of <em>x</em>. In this way, both cases are embedded in a single expression. In terms of information theory, it means minimizing the cross-entropy between a target distribution and an approximated one:</p>
<div class="CDPAlignCenter CDPAlign"><img height="48" width="177" src="assets/884e7e28-7dc7-41c4-8679-a09be940169d.png"/></div>
<p>In particular, if <em>log<sub>2</sub></em> is adopted, the functional expresses the number of extra bits requested to encode the original distribution with the predicted one. It's obvious that when <em>J(w) = 0</em>, the two distributions are equal. Therefore, minimizing the cross-entropy is an elegant way to optimize the prediction error when the target distributions are categorical.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation and optimizations</h1>
                </header>
            
            <article>
                
<p>scikit-learn implements the <kbd>LogisticRegression</kbd> class, which can solve this problem using optimized algorithms. Let's consider a toy dataset made of 500 samples:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/19f5a3f5-26a1-4176-8967-bb277683369a.png"/></div>
<p>The dots belong to the class 0, while the triangles belong to the class 1. In order to immediately test the accuracy of our classification, it's useful to split the dataset into training and test sets:</p>
<pre><strong>from sklearn.model_selection import train_test_split</strong><br/><br/><strong>&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)</strong></pre>
<p>Now we can train the model using the default parameters:</p>
<pre><strong>from sklearn.linear_model import LogisticRegression</strong><br/><br/><strong>&gt;&gt;&gt; lr = LogisticRegression()</strong><br/><strong>&gt;&gt;&gt; lr.fit(X_train, Y_train)</strong><br/><strong>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,</strong><br/><strong>          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,</strong><br/><strong>          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,</strong><br/><strong>          verbose=0, warm_start=False)</strong><br/><br/><strong>&gt;&gt;&gt; lr.score(X_test, Y_test)</strong><br/><strong>0.95199999999999996</strong></pre>
<p>It's also possible to check the quality through a cross-validation (like for linear regression):</p>
<pre><strong>from sklearn.model_selection import cross_val_score</strong><br/><br/><strong>&gt;&gt;&gt; cross_val_score(lr, X, Y, scoring='accuracy', cv=10)</strong><br/><strong>array([ 0.96078431,  0.92156863,  0.96      ,  0.98      ,  0.96      ,</strong><br/><strong>        0.98      ,  0.96      ,  0.96      ,  0.91836735,  0.97959184])</strong><br/><br/></pre>
<p>The classification task has been successful without any further action (confirmed also by the cross-validation) and it's also possible to check the resulting hyperplane parameters:</p>
<pre><strong>&gt;&gt;&gt; lr.intercept_</strong><br/><strong>array([-0.64154943])</strong><br/><br/><strong>&gt;&gt;&gt; lr.coef_</strong><br/><strong>array([[ 0.34417875,  3.89362924]])</strong></pre>
<p>In the following figure, there's a representation of this hyperplane (a line), where it's possible to see how the classification works and what samples are misclassified. Considering the local density of the two blocks, it's easy to see that the misclassifications happened for outliers and for some borderline samples. The latter can be controlled by adjusting the hyperparameters, even if a trade-off is often necessary.  For example, if we want to include the four right dots on the separation line, this could exclude some elements in the right part. Later on, we're going to see how to find the optimal solution. However, when a linear classifier can easily find a separating hyperplane (even with a few outliers), we can say that the problem is linearly modelable; otherwise, more sophisticated non-linear techniques must be taken into account.</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="image-border" src="assets/739a41ee-7b3b-47fe-85a9-f56ccae642a0.png"/></div>
<p>Just like for linear regression, it's possible to impose norm conditions on the weights. In particular, the actual functional becomes: </p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="58" width="181" src="assets/0a3158bb-aa68-46db-a804-22115f4b5b4e.png"/></p>
<p>The behavior is the same as explained in the previous chapter. Both produce a shrinkage, but <em>L1</em> forces sparsity. This can be controlled using the parameters penalty (whose values can be <em>L1</em> or <em>L2</em>) and <em>C</em>, which is the inverse regularization factor (1/alpha), so bigger values reduce the strength, while smaller ones (in particular less than 1) force the weights to move closer to the origin. Moreover, <em>L1</em> will prefer vertexes (where all but one components are null), so it's a good idea to apply <kbd>SelectFromModel</kbd> in order to optimize the actual features after shrinkage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stochastic gradient descent algorithms</h1>
                </header>
            
            <article>
                
<p>After discussing the basics of logistic regression, it's useful to introduce the <kbd>SGDClassifier</kbd> class , which implements a very famous algorithm that can be applied to several different loss functions. The idea behind stochastic gradient descent is iterating a weight update based on the gradient of loss function:</p>
<div class="CDPAlignCenter CDPAlign"><img height="31" width="193" src="assets/04ef7d5e-cb6d-4237-8ff5-47b22ec9da89.png"/></div>
<p class="CDPAlignLeft CDPAlign">However, instead of considering the whole dataset, the update procedure is applied on batches randomly extracted from it. In the preceding formula, <em>L</em> is the loss function we want to minimize (as discussed in <a href="c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml" target="_blank">Chapter 2</a>, <em>Important Elements in Machine Learning</em>) and gamma (<kbd>eta0</kbd> in scikit-learn) is the learning rate, a parameter that can be constant or decayed while the learning process proceeds. The  <kbd>learning_rate</kbd> parameter can be also left with its default value (<kbd>optimal</kbd>), which is computed internally according to the regularization factor.</p>
<p class="CDPAlignLeft CDPAlign">The process should end when the weights stop modifying or their variation keeps itself under a selected threshold. The scikit-learn implementation uses the <kbd>n_iter</kbd> parameter to define the number of desired iterations.</p>
<p class="CDPAlignLeft CDPAlign">There are many possible loss functions, but in this chapter, we consider only <kbd>log</kbd> and <kbd>perceptron</kbd>. Some of the other ones will be discussed in the next chapters. The former implements a logistic regression, while the latter (which is also available as the autonomous class <kbd>Perceptron</kbd>) is the simplest neural network, composed of a single layer of weights <em>w</em>, a fixed constant called bias, and a binary output function:</p>
<div class="CDPAlignCenter CDPAlign"><img height="39" width="115" src="assets/e7494786-d0a8-4f97-be29-c494e584a643.png"/></div>
<p class="CDPAlignLeft CDPAlign">The output function (which classifies in two classes) is:</p>
<div class="CDPAlignCenter CDPAlign"><img height="50" width="125" src="assets/e96697b4-9004-4ba9-b7b3-eb2160b6758c.png"/></div>
<p class="CDPAlignLeft CDPAlign">The differences between a <kbd>Perceptron</kbd> and a <kbd>LogisticRegression</kbd> are the output function (sign versus sigmoid) and the training model (with the loss function). A perceptron, in fact, is normally trained by minimizing the mean square distance between the actual value and prediction:</p>
<div class="CDPAlignCenter CDPAlign"><img height="55" width="148" src="assets/f26cedfe-5233-4554-a7fa-4aad05d3f1ed.png"/></div>
<p class="CDPAlignLeft CDPAlign">Just like any other linear classifier, a perceptron is not able to solve nonlinear problems; hence, our example will be generated using the built-in function <kbd>make_classification</kbd>:</p>
<pre><strong>from sklearn.datasets import make_classification</strong><br/><br/><strong>&gt;&gt;&gt; nb_samples = 500</strong><br/><strong>&gt;&gt;&gt; X, Y = make_classification(n_samples=nb_samples, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1)</strong></pre>
<p>In this way, we can generate 500 samples split into two classes:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/87b1ca33-88b8-421b-92c2-0f6588b7566c.png"/></div>
<p>This problem, under a determined precision threshold, can be linearly solved, so our expectations are equivalent for both <kbd>Perceptron</kbd> and <kbd>LogisticRegression.</kbd> In the latter case, the training strategy is focused on maximizing the likelihood of a probability distribution. Considering the dataset, the probability of a red sample to belong to class 0 must be greater than 0.5 (it's equal to 0.5 when <em>z = 0</em>, so when the point lays on the separating hyperplane) and vice versa. On the other hand, a perceptron will adjust the hyperplane so that the dot product between a sample and the weights would be positive or negative, according to the class. In the following figure, there's a geometrical representation of a perceptron (where the bias is 0):</p>
<div class="packt_figref CDPAlignCenter CDPAlign"> <img class="image-border" src="assets/2d6b379f-a8a7-4bb2-acf5-22ce86e63b47.png"/></div>
<p>The weight vector is orthogonal to the separating hyperplane, so that the discrimination can happen only considering the sign of the dot product. An example of stochastic gradient descent with perceptron loss (without <em>L1</em>/<em>L2</em> constraints) is shown as follows:</p>
<pre><strong>from sklearn.linear_model import SGDClassifier</strong><br/><br/><strong>&gt;&gt;&gt; sgd = SGDClassifier(loss='perceptron', learning_rate='optimal', n_iter=10)</strong><br/><strong>&gt;&gt;&gt; cross_val_score(sgd, X, Y, scoring='accuracy', cv=10).mean()</strong><br/><strong>0.98595918367346935</strong></pre>
<p>The same result can be obtained by directly using the <kbd>Perceptron</kbd> class:</p>
<pre><strong>from sklearn.linear_model import Perceptron</strong><br/><br/><strong>&gt;&gt;&gt; perc = Perceptron(n_iter=10)</strong><br/><strong>&gt;&gt;&gt; cross_val_score(perc, X, Y, scoring='accuracy', cv=10).mean()</strong><br/><strong>0.98195918367346935</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding the optimal hyperparameters through grid search</h1>
                </header>
            
            <article>
                
<p>Finding the best hyperparameters (called this because they influence the parameters learned during the training phase) is not always easy and there are seldom good methods to start from. The personal experience (a fundamental element) must be aided by an efficient tool such as <kbd>GridSearchCV</kbd>, which automates the training process of different models and provides the user with optimal values using cross-validation.</p>
<p>As an example, we show how to use it to find the best penalty and strength factors for a linear regression with the Iris toy dataset:</p>
<pre><strong>import multiprocessing</strong><br/><br/><strong>from sklearn.datasets import load_iris</strong><br/><strong>from sklearn.model_selection import GridSearchCV</strong><br/><br/><strong>&gt;&gt;&gt; iris = load_iris()</strong><br/><br/><strong>&gt;&gt;&gt; param_grid = [</strong><br/><strong>   { </strong><br/><strong>      'penalty': [ 'l1', 'l2' ],</strong><br/><strong>      'C': [ 0.5, 1.0, 1.5, 1.8, 2.0, 2.5]</strong><br/><strong>   }</strong><br/><strong>]</strong><br/><br/><strong>&gt;&gt;&gt; gs = GridSearchCV(estimator=LogisticRegression(), param_grid=param_grid,</strong><br/><strong>   scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count())</strong><br/><br/><strong>&gt;&gt;&gt; gs.fit(iris.data, iris.target)</strong><br/><strong>GridSearchCV(cv=10, error_score='raise',</strong><br/><strong>       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,</strong><br/><strong>          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,</strong><br/><strong>          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,</strong><br/><strong>          verbose=0, warm_start=False),</strong><br/><strong>       fit_params={}, iid=True, n_jobs=8,</strong><br/><strong>       param_grid=[{'penalty': ['l1', 'l2'], 'C': [0.1, 0.2, 0.4, 0.5, 1.0, 1.5, 1.8, 2.0, 2.5]}],</strong><br/><strong>       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,</strong><br/><strong>       scoring='accuracy', verbose=0)</strong><br/><br/><strong>&gt;&gt;&gt; gs.best_estimator_</strong><br/><strong>LogisticRegression(C=1.5, class_weight=None, dual=False, fit_intercept=True,</strong><br/><strong>          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,</strong><br/><strong>          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,</strong><br/><strong>          verbose=0, warm_start=False)</strong><br/><br/><strong>&gt;&gt;&gt; cross_val_score(gs.best_estimator_, iris.data, iris.target, scoring='accuracy', cv=10).mean()</strong><br/><strong>0.96666666666666679</strong></pre>
<p>It's possible to insert into the <kbd>param</kbd> dictionary any parameter supported by the model with a list of values. <kbd>GridSearchCV</kbd> will process in parallel and return the best estimator (through the instance variable <kbd>best_estimator_</kbd>, which is an instance of the same classifier specified through the parameter <kbd>estimator</kbd>).</p>
<div class="packt_tip"><span>When working with parallel algorithms, scikit-learn provides the <kbd>n_jobs</kbd> parameter, which allows us to specify how many threads must be used. Setting</span> <kbd>n_jobs=multiprocessing.cpu_count()</kbd> is useful to exploit<span> all CPU cores available on the current machine.</span></div>
<p>In the next example, we're going to find the best parameters of an <kbd>SGDClassifier</kbd> trained with perceptron loss. The dataset is plotted in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/b087a9da-4a5f-45c5-8f08-b261f8ca2522.png"/></div>
<pre><strong>import multiprocessing</strong><br/><br/><strong>from sklearn.model_selection import GridSearchCV</strong><br/><br/><strong>&gt;&gt;&gt; param_grid = [</strong><br/><strong>   { </strong><br/><strong>       'penalty': [ 'l1', 'l2', 'elasticnet' ],</strong><br/><strong>       'alpha': [ 1e-5, 1e-4, 5e-4, 1e-3, 2.3e-3, 5e-3, 1e-2],</strong><br/><strong>       'l1_ratio': [0.01, 0.05, 0.1, 0.15, 0.25, 0.35, 0.5, 0.75, 0.8]</strong><br/><strong>   }</strong><br/><strong>]</strong><br/><br/><strong>&gt;&gt;&gt; sgd = SGDClassifier(loss='perceptron', learning_rate='optimal')</strong><br/><strong>&gt;&gt;&gt; gs = GridSearchCV(estimator=sgd, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count())</strong><br/><br/><strong>&gt;&gt;&gt; gs.fit(X, Y)</strong><br/><strong>GridSearchCV(cv=10, error_score='raise',</strong><br/><strong>       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,</strong><br/><strong>       eta0=0.0, fit_intercept=True, l1_ratio=0.15,</strong><br/><strong>       learning_rate='optimal', loss='perceptron', n_iter=5, n_jobs=1,</strong><br/><strong>       penalty='l2', power_t=0.5, random_state=None, shuffle=True,</strong><br/><strong>       verbose=0, warm_start=False),</strong><br/><strong>       fit_params={}, iid=True, n_jobs=8,</strong><br/><strong>       param_grid=[{'penalty': ['l1', 'l2', 'elasticnet'], 'alpha': [1e-05, 0.0001, 0.0005, 0.001, 0.0023, 0.005, 0.01], 'l1_ratio': [0.01, 0.05, 0.1, 0.15, 0.25, 0.35, 0.5, 0.75, 0.8]}],</strong><br/><strong>       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,</strong><br/><strong>       scoring='accuracy', verbose=0)</strong><br/><br/><strong>&gt;&gt;&gt; gs.best_score_</strong><br/><strong>0.89400000000000002</strong><br/><br/><strong>&gt;&gt;&gt; gs.best_estimator_</strong><br/><strong>SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,</strong><br/><strong>       eta0=0.0, fit_intercept=True, l1_ratio=0.1, learning_rate='optimal',</strong><br/><strong>       loss='perceptron', n_iter=5, n_jobs=1, penalty='elasticnet',</strong><br/><strong>       power_t=0.5, random_state=None, shuffle=True, verbose=0,</strong><br/><strong>       warm_start=False)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification metrics</h1>
                </header>
            
            <article>
                
<p>A classification task can be evaluated in many different ways to achieve specific objectives. Of course, the most important metric is the accuracy, often expressed as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="42" width="372" src="assets/d5f7e0c0-6444-481e-9354-56b0a8870ebb.png"/></div>
<p class="CDPAlignLeft CDPAlign">In scikit-learn, it can be assessed using the built-in <kbd>accuracy_score()</kbd> function:</p>
<pre><strong>from sklearn.metrics import accuracy_score</strong><br/><br/><strong>&gt;&gt;&gt; accuracy_score(Y_test, lr.predict(X_test))</strong><br/><strong>0.94399999999999995</strong></pre>
<p class="CDPAlignLeft CDPAlign">Another very common approach is based on zero-one loss function, which we saw in <a href="c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml" target="_blank">Chapter 2</a>, <em>Important Elements in Machine Learning</em>, which is defined as the normalized average of <em>L<sub>0/1</sub></em> (where <em>1</em> is assigned to misclassifications) over all samples. In the following example, we show a normalized score (if it's close to 0, it's better) and then the same unnormalized value (which is the actual number of misclassifications):</p>
<pre><strong>from sklearn.metrics import zero_one_loss</strong><br/><br/><strong>&gt;&gt;&gt; zero_one_loss(Y_test, lr.predict(X_test))</strong><br/><strong>0.05600000000000005</strong><br/><br/><strong>&gt;&gt;&gt; zero_one_loss(Y_test, lr.predict(X_test), normalize=False)</strong><br/><strong>7L</strong></pre>
<p class="CDPAlignLeft CDPAlign">A similar but opposite metric is the <strong>Jaccard similarity coefficient</strong>, defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="134" width="240" src="assets/a3ca948b-c1fd-4bb1-80bb-74c2f00d8227.png"/></div>
<p class="CDPAlignLeft CDPAlign">This index measures the similarity and is bounded between 0 (worst performances) and 1 (best performances). In the former case, the intersection is null, while in the latter, the intersection and union are equal because there are no misclassifications. In scikit-learn, the implementation is:</p>
<pre><strong>from sklearn.metrics import jaccard_similarity_score</strong><br/><br/><strong>&gt;&gt;&gt; jaccard_similarity_score(Y_test, lr.predict(X_test))</strong><br/><strong>0.94399999999999995</strong></pre>
<p class="CDPAlignLeft CDPAlign">These measures provide a good insight into our classification algorithms. However, in many cases, it's necessary to be able to differentiate between different kinds of misclassifications (we're considering the binary case with the conventional notation: 0-negative, 1-positive), because the relative weight is quite different. For this reason, we introduce the following definitions:</p>
<ul>
<li><strong>True positive</strong>: A positive sample correctly classified</li>
<li><strong>False positive</strong>: A negative sample classified as positive</li>
<li><strong>True negative</strong>: A negative sample correctly classified</li>
<li><strong>False negative</strong>: A positive sample classified as negative</li>
</ul>
<p>At a glance, false positive and false negative can be considered as similar errors, but think about a medical prediction: while a false positive can be easily discovered with further tests, a false negative is often neglected, with repercussions following the consequences of this action. For this reason, it's useful to introduce the concept of a confusion matrix:</p>
<div class="CDPAlignCenter CDPAlign"><img height="301" width="304" class="image-border" src="assets/43c6e2e9-0130-465f-a6fe-9f6061f95593.png"/></div>
<p>In scikit-learn, it's possible to build a confusion matrix using a built-in function. Let's consider a generic logistic regression on a dataset <em>X</em> with labels <em>Y</em>:</p>
<pre><strong>&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)</strong><br/><strong>&gt;&gt;&gt; lr = LogisticRegression()</strong><br/><strong>&gt;&gt;&gt; lr.fit(X_train, Y_train)</strong><br/><strong>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,</strong><br/><strong>          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,</strong><br/><strong>          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,</strong><br/><strong>          verbose=0, warm_start=False)</strong></pre>
<p>Now we can compute our confusion matrix and immediately see how the classifier is working:</p>
<pre><strong>from sklearn.metrics import confusion_matrix</strong><br/><br/><strong>&gt;&gt;&gt; cm = confusion_matrix(y_true=Y_test, y_pred=lr.predict(X_test))</strong><br/><strong>cm[::-1, ::-1]</strong><br/><strong>[[50  5]</strong><br/><strong> [ 2 68]]</strong></pre>
<p>The last operation is needed because scikit-learn adopts an inverse axle. However, in many books, the confusion matrix has true values on the main diagonal, so I preferred to invert the axle.</p>
<div class="packt_infobox">In order to avoid mistakes, I suggest you visit the page at <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html,</a> and check for true/false positive/negative position.</div>
<p>So we have five false negatives and two false positives. If needed, a further analysis can allow for the detection of the misclassifications to decide how to treat them (for example, if their variance overcomes a predefined threshold, it's possible to consider them as outliers and remove them).</p>
<p>Another useful direct measure is:</p>
<div class="CDPAlignCenter CDPAlign"> <img height="51" width="287" src="assets/4a213466-5665-4ee9-82b9-9045d6e38f8b.png"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">This is directly connected with the ability to capture the features that determine the positiveness of a sample, to avoid the misclassification as negative. In scikit-learn, the implementation is:</p>
<pre><strong>from sklearn.metrics import precision_score</strong><br/><br/><strong>&gt;&gt;&gt; precision_score(Y_test, lr.predict(X_test))</strong><br/><strong>0.96153846153846156</strong></pre>
<div class="packt_infobox">If you don't flip the confusion matrix, but want to get the same measures, it's necessary to add the <kbd>pos_label=0</kbd> parameter to all metric score functions.</div>
<p>The ability to detect true positive samples among all the potential positives can be assessed using another measure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="48" width="273" src="assets/e49c8eee-edf6-4a47-ba07-0bc279d0e30e.png"/></div>
<p class="CDPAlignLeft CDPAlign">The scikit-learn implementation is:</p>
<pre><strong>from sklearn.metrics import recall_score</strong><br/><br/><strong>&gt;&gt;&gt; recall_score(Y_test, lr.predict(X_test))</strong><br/><strong>0.90909090909090906</strong></pre>
<p>It's not surprising that we have a 90 percent recall with 96 percent precision, because the number of false negatives (which impact recall) is proportionally higher than the number of false positives (which impact precision). A weighted harmonic mean between precision and recall is provided by:</p>
<div class="CDPAlignCenter CDPAlign"><img height="56" width="269" src="assets/19659e54-38b5-474e-bad7-e3691a70c37e.png"/></div>
<p class="CDPAlignLeft CDPAlign">A beta value equal to 1 determines the so-called <em>F<sub>1</sub></em> score, which is a perfect balance between the two measures. A beta less than 1 gives more importance to <em>precision</em> and a value greater than 1 gives more importance to <em>recall</em>. The following snippet shows how to implement it with scikit-learn:</p>
<pre><strong>from sklearn.metrics import fbeta_score</strong><br/><br/><strong>&gt;&gt;&gt; fbeta_score(Y_test, lr.predict(X_test), beta=1)</strong><br/><strong>0.93457943925233655</strong><br/><br/><strong>&gt;&gt;&gt; fbeta_score(Y_test, lr.predict(X_test), beta=0.75)</strong><br/><strong>0.94197437829691033</strong><br/><br/><strong>&gt;&gt;&gt; fbeta_score(Y_test, lr.predict(X_test), beta=1.25)</strong><br/><strong>0.92886270956048933</strong></pre>
<div class="packt_infobox">For <em>F<sub>1</sub></em> score, scikit-learn provides the function <kbd>f1_score()</kbd>, which is equivalent to <kbd>fbeta_score()</kbd> with <kbd>beta=1</kbd>.</div>
<p>The highest score is achieved by giving more importance to precision (which is higher), while the least one corresponds to a recall predominance. <em>F<sub>Beta</sub></em> is hence useful to have a compact picture of the accuracy as a trade-off between high precision and a limited number of false negatives.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ROC curve</h1>
                </header>
            
            <article>
                
<p>The <strong>ROC curve</strong> (or receiver operating characteristics) is a valuable tool to compare different classifiers that can assign a score to their predictions. In general, this score can be interpreted as a probability, so it's bounded between 0 and 1. The plane is structured like in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="299" width="298" class="image-border" src="assets/9b0ea115-e411-434f-8ba3-5b13729c6527.png"/></div>
<p>The <em>x </em>axis represents the increasing false positive rate (also known as <strong>specificity</strong>), while the <em>y </em>axis represents the true positive rate (also known as <strong>sensitivity</strong>). The dashed oblique line represents a perfectly random classifier, so all the curves below this threshold perform worse than a random choice, while the ones above it show better performances. Of course, the best classifier has an ROC curve split into the segments [0, 0] - [0, 1] and [0, 1] - [1, 1], and our goal is to find algorithms whose performances should be as close as possible to this limit. To show how to create a ROC curve with scikit-learn, we're going to train a model to determine the scores for the predictions (this can be achieved using the <kbd>decision_function()</kbd> or <kbd>predict_proba()</kbd> methods):</p>
<pre><strong>&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)</strong><br/><br/><strong>&gt;&gt;&gt; lr = LogisticRegression()</strong><br/><strong>&gt;&gt;&gt; lr.fit(X_train, Y_train)</strong><br/><strong>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,</strong><br/><strong>          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,</strong><br/><strong>          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,</strong><br/><strong>          verbose=0, warm_start=False)</strong><br/><br/><strong>&gt;&gt;&gt; Y_scores = lr.decision_function(X_test)</strong></pre>
<p>Now we can compute the ROC curve:</p>
<pre><strong>from sklearn.metrics import roc_curve</strong><br/><br/><strong>&gt;&gt;&gt; fpr, tpr, thresholds = roc_curve(Y_test, Y_scores)</strong></pre>
<p>The output is made up of the increasing true and false positive rates and the decreasing thresholds (which isn't normally used for plotting the curve). Before proceeding, it's also useful to compute the <strong>area under the curve</strong> (<strong>AUC)</strong>, whose value is bounded between 0 (worst performances) and 1 (best performances), with a perfectly random value corresponding to 0.5:</p>
<pre><strong>from sklearn.metrics import auc</strong><br/><br/><strong>&gt;&gt;&gt; auc(fpr, tpr)</strong><br/><strong>0.96961038961038959</strong></pre>
<p>We already know that our performances are rather good because the AUC is close to 1. Now we can plot the ROC curve using matplotlib. As this book is not dedicated to this powerful framework, I'm going to use a snippet that can be found in several examples:</p>
<pre><strong>import matplotlib.pyplot as plt</strong><br/><br/><strong>&gt;&gt;&gt; plt.figure(figsize=(8, 8))</strong><br/><strong>&gt;&gt;&gt; plt.plot(fpr, tpr, color='red', label='Logistic regression (AUC: %.2f)' % auc(fpr, tpr))</strong><br/><strong>&gt;&gt;&gt; plt.plot([0, 1], [0, 1], color='blue', linestyle='--')</strong><br/><strong>&gt;&gt;&gt; plt.xlim([0.0, 1.0])</strong><br/><strong>&gt;&gt;&gt; plt.ylim([0.0, 1.01])</strong><br/><strong>&gt;&gt;&gt; plt.title('ROC Curve')</strong><br/><strong>&gt;&gt;&gt; plt.xlabel('False Positive Rate')</strong><br/><strong>&gt;&gt;&gt; plt.ylabel('True Positive Rate')</strong><br/><strong>&gt;&gt;&gt; plt.legend(loc="lower right")</strong><br/><strong>&gt;&gt;&gt; plt.show()</strong></pre>
<p>The resulting ROC curve is the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img height="449" width="438" class="image-border" src="assets/ecd852a4-f726-422e-b401-97340022d9d9.png"/></div>
<p>As confirmed by the AUC, our ROC curve shows very good performance. In later chapters, we're going to use the ROC curve to visually compare different algorithms. As an exercise, you can try different parameters of the same model and plot all the ROC curves, to immediately understand which setting is preferable.</p>
<div class="packt_infobox">I suggest visiting <a href="http://matplotlib.org">http://matplotlib.org</a>, for further information and tutorials. Moreover, an extraordinary tool is Jupyter (<a href="http://jupyter.org">http://jupyter.org</a>), which allows working with interactive notebooks, where you can immediately try your code and visualize in-line plots.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>A linear model classifies samples using separating hyperplanes; hence, a problem is linearly separable if it's possible to find a linear model whose accuracy overcomes a predetermined threshold. Logistic regression is one of most famous linear classifiers, based on the principle of maximizing the probability of a sample belonging to the right class. Stochastic gradient descent classifiers are a more generic family of algorithms, determined by the different loss function that is adopted. SGD allows partial fitting, particularly when the amount of data is too huge to be loaded in memory. A perceptron is a particular instance of SGD, representing a linear neural network that cannot solve the <kbd>XOR</kbd> problem (for this reason, multi-layer perceptrons became the first choice for non-linear classification). However, in general, its performance is comparable to a logistic regression model.</p>
<p>All classifier performances must be measured using different approaches, in order to be able to optimize their parameters or to change them when the results don't meet our requirements. We discussed different metrics and, in particular, the ROC curve, which graphically shows how the different classifiers are performing.</p>
<p>In the next chapter, we're going to discuss naive Bayes classifiers, which are another very famous and powerful family of algorithms. Thanks to this simple approach, it's possible to build spam filtering systems and solve apparently complex problems using only probabilities and the quality of results. Even after decades, it's still superior or comparable to much more complex solutions.</p>


            </article>

            
        </section>
    </body></html>