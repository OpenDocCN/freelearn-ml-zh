<html><head></head><body><div><h1 class="header-title">Discovering Hidden Structures with Unsupervised Learning</h1>
                
            
            
                
<p>So far, we have focused our attention exclusively on supervised learning problems, where every data point in the dataset had a known label or target value. However, what do we do when there is no known output or no teacher to supervise the learning algorithm?</p>
<p>This is what <strong>unsupervised learning</strong> is all about. In unsupervised learning, the learning process is shown only in the input data and is asked to extract knowledge from this data without further instruction. We have already talked about one of the many forms that unsupervised learning comes in—<strong>dimensionality reduction</strong>. Another popular domain is <strong>cluster analysis</strong>, which aims to partition data into distinct groups of similar items.</p>
<p>Some of the problems where clustering techniques can be useful are document analysis, image retrieval, finding spam emails, identifying fake news, identifying criminal activities, and so on.</p>
<p>In this chapter, we want to understand how different clustering algorithms can be used to extract hidden structures in simple, unlabeled datasets. These hidden structures have many benefits, whether they are used in feature extraction, image processing, or even as a preprocessing step for supervised learning tasks. As a concrete example, we will learn how to apply clustering to images to reduce their color spaces to 16 bits. </p>
<p>More specifically, we will cover the following topics:</p>
<ul>
<li><strong>k-means clustering</strong> and <strong>expectation-maximization</strong> and implementing these in OpenCV</li>
<li>Arranging clustering algorithms in hierarchical trees and what are the benefits that come from that</li>
<li>Using unsupervised learning for preprocessing, image processing, and classification</li>
</ul>
<p>Let's get started!</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Using TF-IDF to improve the result</h1>
                
            
            
                
<p>It was called the <strong>Term Frequency-Inverse Document Frequency</strong> (<strong>TF</strong><strong>-IDF</strong>), and we encountered it in <a href="142fec63-a847-4cde-9de9-c34805d2bb84.xhtml" target="_blank">Chapter 4</a>, <em>Representing Data and Engineering Features</em>. If you recall, what TF-IDF does is basically weigh the word count by a measure of how often the words appear in the entire dataset. A useful side effect of this method is the IDF part—the inverse frequency of words. This makes sure that frequent words, such as <em>and</em>, <em>the</em>, and <em>but</em>, carry only a small weight in the classification.</p>
<p>We apply TF-IDF to the feature matrix by calling <kbd>fit_transform</kbd> on our existing feature matrix, <kbd>X</kbd>:</p>
<pre>In [24]: tfidf = feature_extraction.text.TfidfTransformer()In [25]: X_new = tfidf.fit_transform(X)</pre>
<p>Don't forget to split the data; also, ...</p></div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we took our first look at probability theory, learning about random variables and conditional probabilities, which allowed us to get a glimpse of Bayes' theorem—the underpinning of a Naive Bayes classifier. We talked about the difference between discrete and continuous random variables, likelihoods and probabilities, priors and evidence, and normal and Naive Bayes classifiers.</p>
<p>Finally, our theoretical knowledge would be of no use if we didn't apply it to practical examples. We obtained a dataset of raw email messages, parsed it, and trained Bayesian classifiers on it to classify emails as either spam or ham (not spam) using a variety of feature extraction approaches.</p>
<p>In the next chapter, we will switch gears and, for once, discuss what to do if we have to deal with unlabeled data.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>You can refer to the code for this chapter from the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter08">https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter08</a>.</p>
<p>Here is a summary of the software and hardware requirements:</p>
<ul>
<li>You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).</li>
<li>You will need Python version 3.6 (any Python version 3.x will be fine).</li>
<li>You will need Anaconda Python 3 for installing Python and the required modules.</li>
<li>You can use any operating system—macOS, Windows, and Linux-based OSes—along with this book. We recommend you have at least 4 GB RAM in your system.</li>
<li>You don't need to have a GPU to run the code provided along with this book.</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding unsupervised learning</h1>
                
            
            
                
<p>Unsupervised learning might come in many shapes and forms, but the goal is always to convert original data into a richer, more meaningful representation, whether that means making it easier for humans to understand or easier for machine learning algorithms to parse.</p>
<p>Some common applications of unsupervised learning include the following:</p>
<ul>
<li><strong>Dimensionality reduction</strong>: This takes a high-dimensional representation of data consisting of many features and tries to compress the data so that its main characteristics can be explained with a small number of highly informative features. For example, when applied to housing prices in the neighborhoods of Boston, dimensionality reduction might be able to tell us that the indicators we should pay most attention to are the property tax and the neighborhood's crime rate.</li>
<li><strong>Factor analysis</strong>: This tries to find the hidden causes or unobserved components that gave rise to the observed data. For example, when applied to all of the episodes of the 1970s TV show, <em>Scooby-Doo, Where Are You!</em>, factor analysis might be able to tell us that (spoiler alert!) every ghost or monster on the show is essentially some disgruntled count playing an elaborate hoax on the town.</li>
</ul>
<ul>
<li><strong>Cluster analysis</strong>: This tries to partition the data into distinct groups of similar items. This is the type of unsupervised learning we will focus on in this chapter. For example, when applied to all of the movies on Netflix, cluster analysis might be able to automatically group them into genres.</li>
</ul>
<p>To make things more complicated, these analyses have to be performed on unlabeled data, where we do not know beforehand what the right answer should be. Consequently, a major challenge in unsupervised learning is to determine whether an algorithm did well or learned anything useful. Often, the only way to evaluate the result of an unsupervised learning algorithm is to inspect it manually and determine by hand whether the result makes sense.</p>
<p>That being said, unsupervised learning can be immensely helpful, for example, as a preprocessing or feature extraction step. You can think of unsupervised learning as a <strong>data transformation</strong>—a way to transform data from its original representation into a more informative form. Learning a new representation might give us deeper insights into our data, and sometimes, it might even improve the accuracy of supervised learning algorithms.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding k-means clustering</h1>
                
            
            
                
<p>The essential clustering algorithm that OpenCV provides is <em>k</em>-means clustering, which searches for a predestined number of <em>k-</em>clusters (or groups) from an unlabeled multi-dimensional data.</p>
<p>It achieves this by using two simple hypotheses about what optimal clustering should look like:</p>
<ul>
<li>The center of each cluster is basically the mean of all of the points belonging to that cluster, also known as the centroid.</li>
<li>Each data point in that cluster is closer to its center than to all other cluster centers.</li>
</ul>
<p>It's easiest to understand the algorithm by looking at a concrete example.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Implementing our first k-means example</h1>
                
            
            
                
<p>First, let's generate a 2D dataset containing four distinct blobs. To emphasize that this is an unsupervised approach, we will leave the labels out of the visualization:</p>
<ol>
<li>We will continue using <kbd>matplotlib</kbd> for all of our visualization purposes:</li>
</ol>
<pre style="padding-left: 60px">In [1]: import matplotlib.pyplot as plt<br/>...     %matplotlib inline<br/>...     plt.style.use('ggplot')</pre>
<ol start="2">
<li>Following the same recipe from earlier chapters, we will create a total of 300 blobs (<kbd>n_samples=300</kbd>) belonging to four distinct clusters (<kbd>centers=4</kbd>):</li>
</ol>
<pre style="padding-left: 60px">In [2]: from sklearn.datasets.samples_generator import make_blobs<br/>...     X, y_true = make_blobs(n_samples=300, centers=4,<br/>...                            cluster_std=1.0, random_state=10)<br/>...     plt.scatter(X[:, 0], X[:, 1], s=100);</pre>
<p style="padding-left: 60px">This will generate the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1038 image-border" src="img/1479c696-e19b-4992-bbfe-1d4e38a58124.png" style="width:45.67em;height:28.83em;" width="857" height="540"/></p>
<p style="padding-left: 60px">The preceding diagram shows an example dataset of 300 unlabeled points organized into four distinct clusters. Even without assigning target labels to the data, it is straightforward to pick out the four clusters by eye. The <em>k</em>-means algorithm can do this, too, without having any information about target labels or underlying data distributions.</p>
<ol start="3">
<li>Although <em>k</em>-means is, of course, a statistical model, in OpenCV, it does not come via the <kbd>ml</kbd> module and the common <kbd>train</kbd> and <kbd>predict</kbd> API calls. Instead, it is directly available as <kbd>cv2.kmeans</kbd>. To use the model, we have to specify some arguments, such as the termination criteria and some initialization flags. Here, we will tell the algorithm to terminate whenever the error is smaller than 1.0 (<kbd>cv2.TERM_CRITERIA_EPS</kbd>) or when ten iterations have been executed (<kbd>cv2.TERM_CRITERIA_MAX_ITER</kbd>):</li>
</ol>
<pre style="padding-left: 60px">In [3]: import cv2<br/>...     criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,<br/>...                 10, 1.0)<br/>...     flags = cv2.KMEANS_RANDOM_CENTERS</pre>
<ol start="4">
<li>Then, we can pass the preceding data matrix (<kbd>X</kbd>) to <kbd>cv2.means</kbd>. We also specify the number of clusters (<kbd>4</kbd>) and the number of attempts the algorithm should make with different random initial guesses (<kbd>10</kbd>), as shown in the following snippet:</li>
</ol>
<pre style="padding-left: 60px">In [4]: import numpy as np<br/>...     compactness, labels, centers = cv2.kmeans(X.astype(np.float32),<br/>...                                               4, None, criteria,<br/>...                                               10, flags)</pre>
<p style="padding-left: 60px">Three different variables are returned.</p>
<ol start="5">
<li>The first one, <kbd>compactness</kbd>, returns the sum of squared distances from each point to their corresponding cluster centers. A high compactness score indicates that all points are close to their cluster centers, whereas a low compactness score indicates that the different clusters might not be well separated:</li>
</ol>
<pre style="padding-left: 60px">In [5]: compactness<br/>Out[5]: 527.01581170992</pre>
<ol start="6">
<li>Of course, this number strongly depends on the actual values in <kbd>X</kbd>. If the distances between points were large, to begin with, we could not expect an arbitrarily small compactness score. Hence, it is more informative to plot the data points, colored to their assigned cluster labels:</li>
</ol>
<pre style="padding-left: 60px">In [6]: plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')<br/>...     plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200,<br/>...                 alpha=0.5);</pre>
<ol start="7">
<li>This produces a scatter plot of all of the data points colored according to whichever cluster they belong to, with the corresponding cluster centers indicated with a blob of a darker shade in the center of each cluster:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1039 image-border" src="img/9305c4e4-b92e-45a9-865b-ffd047e3ca5e.png" style="width:48.42em;height:29.33em;" width="886" height="537"/></p>
<p>The preceding diagram shows the result of <em>k</em>-means clustering for <em>k=4</em>. The good news here is that the <em>k</em>-means algorithm (at least, in this simple case) assigns the points to clusters very similarly to how we might have, had we done the job by eye. But how did the algorithm find these different clusters so quickly? After all, the number of possible combinations of cluster assignments is exponential to the number of data points! By hand, trying all possible combinations would have certainly taken forever.</p>
<p>Fortunately, an exhaustive search is not necessary. Instead, the typical approach that <em>k</em>-means takes is to use an iterative algorithm, also known as <strong>expectation-maximization</strong>.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding expectation-maximization</h1>
                
            
            
                
<p><em>k</em>-means clustering is but one concrete application of a more general algorithm known as expectation-maximization. In short, the algorithm works as follows:</p>
<ol>
<li>Start with some random cluster centers.</li>
<li>Repeat until convergence:
<ul>
<li><strong>Expectation step</strong>: Assign all data points to their nearest cluster center.</li>
<li><strong>Maximization step</strong>: Update the cluster centers by taking the mean of all of the points in the cluster.</li>
</ul>
</li>
</ol>
<p>Here, the expectation step is so named because it involves updating our expectation of which cluster each point in the dataset belongs to. The maximization step is so named because it involves maximizing a fitness function that defines the location of the cluster centers. In the case of <em>k</em>-means, maximization ...</p></div>



  
<div><h1 class="header-title">Implementing our expectation-maximization solution</h1>
                
            
            
                
<p>The expectation-maximization algorithm is simple enough for us to code it ourselves. To do so, we will define a function, <kbd>find_clusters(X, n_clusters, rseed=5)</kbd>, that takes as input a data matrix (<kbd>X</kbd>), the number of clusters we want to discover (<kbd>n_clusters</kbd>), and a random seed (optional, <kbd>rseed</kbd>). As will become clear in a second, scikit-learn's <kbd>pairwise_distances_argmin</kbd> function will come in handy:</p>
<pre>In [7]: from sklearn.metrics import pairwise_distances_argmin<br/>...     def find_clusters(X, n_clusters, rseed=5):</pre>
<p>We can implement expectation-maximization for <em>k</em>-means in five essential steps:</p>
<ol>
<li><strong>Initialization</strong>: Randomly choose a number of cluster centers, <kbd>n_clusters</kbd>. We don't just pick any random number but instead pick actual data points to be the cluster centers. We do this by permuting <kbd>X</kbd> along its first axis and picking the first <kbd>n_clusters</kbd> points in this random permutation:</li>
</ol>
<pre>        ...         rng = np.random.RandomState(rseed)<br/>        ...         i = rng.permutation(X.shape[0])[:n_clusters]<br/>        ...         centers = X[i]</pre>
<ol start="2">
<li><strong><kbd>while</kbd> looping forever</strong>: Assign labels based on the closest cluster centers. Here, scikit-learn's <kbd>pairwise_distance_argmin</kbd> function does exactly what we want. It computes, for each data point in <kbd>X</kbd>, the index of the closest cluster center in <kbd>centers</kbd>:</li>
</ol>
<pre>        ...         while True:<br/>        ...         labels = pairwise_distances_argmin(X, centers)</pre>
<ol start="3">
<li><strong>Find the new cluster centers</strong>: In this step, we have to take the arithmetic mean of all data points in <kbd>X</kbd> that belong to a specific cluster (<kbd>X[labels == i]</kbd>):</li>
</ol>
<pre>        ...          new_centers = np.array([X[labels ==<br/>                     i].mean(axis=0)</pre>
<ol start="4">
<li><strong>Check for convergence and break the</strong> <kbd>while</kbd> <strong>loop if necessary</strong>: This is the last step to make sure that we stop the execution of the algorithm once the job is done. We determine whether the job is done by checking whether all of the new cluster centers are equal to the old cluster centers. If this is true, we exit the loop; otherwise, we keep looping:</li>
</ol>
<pre>        ...             for i in range(n_clusters)])<br/>        ...             if np.all(centers == new_centers):<br/>        ...                break<br/>        ...             centers = new_centers</pre>
<ol start="5">
<li>Exit the function and return the result:</li>
</ol>
<pre>        ...             return centers, labels</pre>
<p>We can apply our function to the preceding data matrix, <kbd>X</kbd>, we created. Since we know what the data looks like, we know that we are looking for four clusters:</p>
<pre>In [8]: centers, labels = find_clusters(X, 4)<br/>...     plt.scatter(X[:, 0], X[:, 1], c=labels, s=100, cmap='viridis');</pre>
<p>This will generate the following plot. The vital point to observe from the following diagram is that, before applying <em>k</em>-means clustering, all data points were categorized to the same one color; however, after using <em>k</em>-means clustering, each color is a different cluster (similar data points are clustered or grouped in one color) :</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1041 image-border" src="img/9b09ee68-3da7-4eaf-b90b-324b62fa1266.png" style="width:43.25em;height:27.17em;" width="866" height="544"/></p>
<p>The preceding diagram shows the outcome of our home-made <em>k</em>-means using expectation-maximization. As we can see, our home-made algorithm got the job done! Granted, this particular clustering example was fairly easy, and most real-life implementations of <em>k</em>-means clustering will do a bit more under the hood. But for now, we are happy.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Knowing the limitations of expectation-maximization</h1>
                
            
            
                
<p>For all its simplicity, expectation-maximization performs incredibly well in a range of scenarios. That being said, there are some potential limitations that we need to be aware of:</p>
<ul>
<li>Expectation-maximization does not guarantee that we will find the globally best solution.</li>
<li>We must know the number of desired clusters beforehand.</li>
<li>The decision boundaries of the algorithms are linear.</li>
<li>The algorithm is slow for large datasets.</li>
</ul>
<p>Let's quickly discuss these potential caveats in a little more detail.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">The first caveat – no guarantee of finding the global optimum</h1>
                
            
            
                
<p>Although mathematicians have proved that the expectation-maximization step improves the result in each step, there is still no guarantee that, in the end, we will find the global best solution. For example, if we use a different random seed in our simple example (such as using seed <kbd>10</kbd> instead of <kbd>5</kbd>), we suddenly get very poor results:</p>
<pre>In [9]: centers, labels = find_clusters(X, 4, rseed=10)<br/>...     plt.scatter(X[:, 0], X[:, 1], c=labels, s=100, cmap='viridis');</pre>
<p>This will generate the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1042 image-border" src="img/ec702b4a-9401-4088-b676-9f5b5149ee5d.png" style="width:47.17em;height:29.33em;" width="857" height="533"/></p>
<p>The preceding diagram shows an example of <em>k</em>-means missing the global optimum. What happened?</p>
<p>The short answer is that the random initialization of cluster centers was unfortunate. It led to the center of the yellow cluster migrating in-between the two top blobs, essentially combining them into one. As a result, the other clusters got confused because they suddenly had to split two visually distinct blobs into three clusters.</p>
<p>For this reason, it is common for the algorithm to be run for multiple initial states. Indeed, OpenCV does this by default (set by the optional <kbd>attempts</kbd> parameter).</p>


            

            
        
    </div>



  
<div><h1 class="header-title">The second caveat – we must select the number of clusters beforehand</h1>
                
            
            
                
<p>Another potential limitation is that <em>k</em>-means cannot learn the number of clusters from the data. Instead, we must tell it how many clusters we expect beforehand. You can see how this could be problematic for complicated real-world data that you don't fully understand yet.</p>
<p>From the viewpoint of <em>k</em>-means, there is no wrong or nonsensical number of clusters. For example, if we ask the algorithm to identify six clusters in the dataset generated in the preceding section, it will happily proceed and find the best six clusters:</p>
<pre>In [10]: criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,...                  10, 1.0)...      flags = cv2.KMEANS_RANDOM_CENTERS... compactness, labels, centers ...</pre></div>



  
<div><h1 class="header-title">The third caveat – cluster boundaries are linear</h1>
                
            
            
                
<p>The <em>k</em>-means algorithm is based on a simple assumption, which is that points will be closer to their own cluster center than to others. Consequently, <em>k</em>-means always assumes linear boundaries between clusters, meaning that it will fail whenever the geometry of the clusters is more complicated than that.</p>
<p>We see this limitation for ourselves by generating a slightly more complicated dataset. Instead of generating data points from Gaussian blobs, we want to organize the data into two overlapping half circles. We can do this using scikit-learn's <kbd>make_moons</kbd>. Here, we choose 200 data points belonging to two half circles, in combination with some Gaussian noise:</p>
<pre>In [14]: from sklearn.datasets import make_moons<br/>...      X, y = make_moons(200, noise=.05, random_state=12)</pre>
<p>This time, we tell <em>k</em>-means to look for two clusters:</p>
<pre>In [15]: criteria = (cv2.TERM_CRITERIA_EPS +<br/>...                  cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)<br/>...      flags = cv2.KMEANS_RANDOM_CENTERS<br/>...      compactness, labels, centers = cv2.kmeans(X.astype(np.float32),<br/>...                                                2, None, criteria,<br/>...                                                10, flags)<br/>...      plt.scatter(X[:, 0], X[:, 1], c=labels, s=100, cmap='viridis');</pre>
<p>The resulting scatter plot looks like this diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1045 image-border" src="img/305edf9a-f7c3-491e-8a1e-10c6427d1917.png" style="width:44.50em;height:27.42em;" width="880" height="542"/></p>
<p>The preceding diagram shows an example of <em>k</em>-means finding linear boundaries in nonlinear data. As is evident from the plot, <em>k</em>-means failed to identify the two half circles and instead split the data with what looks like a diagonal straight line (from bottom-left to top-right).</p>
<p>This scenario should ring a bell. We had the same problem when we talked about linear SVMs in <a href="419719a8-3340-483a-86be-1d9b94f4a682.xhtml" target="_blank">Chapter 6</a>, <em>Detecting Pedestrians with Support Vector Machines.</em> The idea there was to use the kernel trick to transform the data into a higher-dimensional feature space. Can we do the same here?</p>
<p>We most certainly can. There is a form of kernelized <em>k</em>-means that works akin to the kernel trick for SVMs, called <strong>spectral clustering</strong>. Unfortunately, OpenCV does not provide an implementation of spectral clustering. Fortunately, scikit-learn does:</p>
<pre>In [16]: from sklearn.cluster import SpectralClustering</pre>
<p>The algorithm uses the same API as all other statistical models: we set optional arguments in the constructor and then call <kbd>fit_predict</kbd> on the data. Here, we want to use the graph of the nearest neighbors to compute a higher-dimensional representation of the data and then assign labels using <em>k</em>-means:</p>
<pre>In [17]: model = SpectralClustering(n_clusters=2,<br/>...                                 affinity='nearest_neighbors',<br/>...                                 assign_labels='kmeans')<br/>...      labels = model.fit_predict(X)<br/>...      plt.scatter(X[:, 0], X[:, 1], c=labels, s=100, cmap='viridis');</pre>
<p>The output of spectral clustering looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1046 image-border" src="img/3c51c11b-66f9-408d-b404-ae809e59bb5b.png" style="width:46.75em;height:28.67em;" width="873" height="535"/></p>
<p>We see that spectral clustering gets the job done. Alternatively, we could have transformed the data into a more suitable representation ourselves and then applied OpenCV's linear <em>k</em>-means to it. The lesson of all of this is that, perhaps, again, feature engineering saved the day.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">The fourth caveat – k-means is slow for a large number of samples</h1>
                
            
            
                
<p>The final limitation of <em>k</em>-means is that it is relatively slow for large datasets. You can imagine that quite a lot of algorithms might suffer from this problem. However, <em>k</em>-means is affected especially badly: each iteration of <em>k</em>-means must access every single data point in the dataset and compare it to all of the cluster centers.</p>
<p>You might wonder whether the requirement to access all data points during each iteration is really necessary. For example, you might just use a subset of the data to update the cluster centers at each step. Indeed, this is the exact idea that underlies a variation of the algorithm called <strong>batch-based <em>k</em>-means</strong>. Unfortunately, this algorithm is not implemented ...</p></div>



  
<div><h1 class="header-title">Compressing color spaces using k-means</h1>
                
            
            
                
<p>One interesting use case of <em>k</em>-means is the compression of image color spaces. For example, a standard <strong>color image</strong> comes with 24-bit color depth, providing for a total of 16,777,216 color varieties. However, in most images, a large number of colors will be unused, and many of the pixels in the image will have similar values. The compressed image can then be sent over the internet at a faster speed, and at the receiver end, it can be decompressed to get back the original image. Hence, reducing the storage and transmission cost. However, the image color space compression will be <strong>lossy</strong> and you might not notice fine details in the image after compression.<br/></p>
<div><p>Alternatively, we can use <em>k</em>-means to reduce the color palette too. The idea here is to think of the cluster centers as the decreased color palette. Then, <em>k</em>-means will organize the millions of colors in the original image into the appropriate number of colors.</p>
</div>


            

            
        
    </div>



  
<div><h1 class="header-title">Visualizing the true-color palette</h1>
                
            
            
                
<p>By performing the following steps, you will be able to visualize the true-color palette of a color image:</p>
<ol>
<li>Let's have a look at a particular image:</li>
</ol>
<pre style="padding-left: 60px">In [1]: import cv2...     import numpy as np...     lena = cv2.imread('data/lena.jpg', cv2.IMREAD_COLOR)</pre>
<ol start="2">
<li>By now, we know how to start Matplotlib in our sleep:</li>
</ol>
<pre style="padding-left: 60px">In [2]: import matplotlib.pyplot as plt...     %matplotlib inline...     plt.style.use('ggplot')</pre>
<ol start="3">
<li>However, this time, we want to disable the grid lines that the <kbd>ggplot</kbd> option typically displays over images:</li>
</ol>
<pre style="padding-left: 60px">In [3]: plt.rc('axes', **{'grid': False})</pre>
<ol start="4">
<li>Then, we can visualize Lena with the following command (don't forget to switch the BGR ordering of the color channels to RGB):</li>
</ol>
<pre style="padding-left: 60px">In [4]: plt.imshow(cv2.cvtColor(lena, cv2.COLOR_BGR2RGB)) ...</pre></div>



  
<div><h1 class="header-title">Reducing the color palette using k-means</h1>
                
            
            
                
<p>By referring to the following steps, you will be able to project a color image into a reduced color palette using <em>k</em>-means clustering:</p>
<ol>
<li>Now, let's reduce the 16 million colors to a mere 16 by indicating <em>k</em>-means to cluster all of the 16 million color variations into 16 distinct clusters. We will use the previously mentioned procedure, but now define 16 as the number of clusters:</li>
</ol>
<pre style="padding-left: 60px">In [9]: criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,<br/>...                 10, 1.0)<br/>...     flags = cv2.KMEANS_RANDOM_CENTERS<br/>...     img_data = img_data.astype(np.float32)<br/>...     compactness, labels, centers = cv2.kmeans(img_data,<br/>...                                               16, None, criteria,<br/>...                                               10, flags)</pre>
<ol start="2">
<li>The 16 different colors of your reduced color palette correspond to the resultant clusters. The output from the <kbd>centers</kbd> array reveals that all colors have three entries—<kbd>B</kbd>, <kbd>G</kbd>, and <kbd>R</kbd>—with values between 0 and 1:</li>
</ol>
<pre style="padding-left: 60px">In [10]: centers<br/>Out[10]: array([[ 0.29973754,  0.31500012,  0.48251548],<br/>                [ 0.27192295,  0.35615689,  0.64276862],<br/>                [ 0.17865284,  0.20933454,  0.41286203],<br/>                [ 0.39422086,  0.62827665,  0.94220853],<br/>                [ 0.34117648,  0.58823532,  0.90196079],<br/>                [ 0.42996961,  0.62061119,  0.91163337],<br/>                [ 0.06039202,  0.07102439,  0.1840712 ],<br/>                [ 0.5589878 ,  0.6313886 ,  0.83993536],<br/>                [ 0.37320262,  0.54575169,  0.88888896],<br/>                [ 0.35686275,  0.57385623,  0.88954246],<br/>                [ 0.47058824,  0.48235294,  0.59215689],<br/>                [ 0.34346411,  0.57483661,  0.88627452],<br/>                [ 0.13815609,  0.12984112,  0.21053818],<br/>                [ 0.3752504 ,  0.47029912,  0.75687987],<br/>                [ 0.31909946,  0.54829341,  0.87378371],<br/>                [ 0.40409693,  0.58062142,  0.8547557 ]], dtype=float32)</pre>
<ol start="3">
<li>The <kbd>labels</kbd> vector contains the 16 colors corresponding to the 16 cluster <kbd>labels</kbd>. So, all of the data points with the label 0 will be colored according to row 0 in the <kbd>centers</kbd> array; similarly, all data points with the label 1 will be colored according to row 1 in the <kbd>centers</kbd> array and so on. Hence, we want to use <kbd>labels</kbd> as an index in the <kbd>centers</kbd> array—these are our new colors:</li>
</ol>
<pre style="padding-left: 60px">In [11]: new_colors = centers[labels].reshape((-1, 3))</pre>
<ol start="4">
<li>We can plot the data again, but this time, we will use <kbd>new_colors</kbd> to color the data points accordingly:</li>
</ol>
<pre style="padding-left: 60px">In [12]: plot_pixels(img_data, colors=new_colors, <br/>...      title="Reduce color space: 16 colors")   <strong>                                        </strong></pre>
<p style="padding-left: 60px">The result is the recoloring of the original pixels, where each pixel is assigned the color of its closest cluster center:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1049 image-border" src="img/f3b6e0d8-5b3b-4c4f-98fc-c75abe2aefe1.png" style="width:50.83em;height:22.00em;" width="1355" height="587"/></p>
<ol start="5">
<li>To observe the effect of recoloring, we have to plot <kbd>new_colors</kbd> as an image. We flattened the earlier image to get from the image to the data matrix. To get back to the image now, we need to do the inverse, which is reshape <kbd>new_colors</kbd> according to the shape of the Lena image:</li>
</ol>
<pre style="padding-left: 60px">In [13]: lena_recolored = new_colors.reshape(lena.shape)</pre>
<ol start="6">
<li>Then, we can visualize the recolored Lena image like any other image:</li>
</ol>
<pre style="padding-left: 60px">In [14]: plt.figure(figsize=(10, 6))<br/>...      plt.imshow(cv2.cvtColor(lena_recolored, cv2.COLOR_BGR2RGB));<br/>...      plt.title('16-color image')</pre>
<p>The result looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1050 image-border" src="img/db7c60af-4276-454d-9f22-a32cda3bdb5e.png" style="width:24.25em;height:25.00em;" width="558" height="575"/></p>
<p>It's pretty awesome, right?</p>
<p>Overall, the preceding screenshot is quite clearly recognizable although some details are arguably lost. Given that you compressed the image by a factor of around 1 million, this is pretty remarkable.</p>
<p>You can repeat this procedure for any desired number of colors.</p>
<p>Another way to reduce the color palette of images involves the use of <strong>bilateral filters</strong>. The resulting images often look like cartoon versions of the original image. You can find an example of this in the book, <em>OpenCV with Python Blueprints</em>, by M. Beyeler, Packt Publishing.</p>
<p>Another potential application of <em>k</em>-means is something you might not expect: using it for image classification.</p>
<p class="mce-root"/>


            

            
        
    </div>



  
<div><h1 class="header-title">Classifying handwritten digits using k-means</h1>
                
            
            
                
<p>Although the last application was a pretty creative use of <em>k</em>-means, we can do better still. We have previously discussed <em>k</em>-means in the context of unsupervised learning, where we tried to discover some hidden structure in the data.</p>
<p>However, doesn't the same concept apply to most classification tasks? Let's say our task was to classify handwritten digits. Don't most zeros look similar, if not the same? And don't all zeros look categorically different from all possible ones? Isn't this exactly the kind of <em>hidden structure</em> we set out to discover with unsupervised learning? Doesn't this mean we could use clustering for classification as well?</p>
<p>Let's find out together. In this section, we will attempt ...</p></div>



  
<div><h1 class="header-title">Loading the dataset</h1>
                
            
            
                
<p>From the earlier chapters, you might recall that scikit-learn provides a whole range of handwritten digits via its <kbd>load_digits</kbd> utility function. The dataset consists of 1,797 samples with 64 features each, where each of the features has the brightness of one pixel in an <em>8 x 8</em> image:</p>
<pre>In [1]: from sklearn.datasets import load_digits<br/>...     digits = load_digits()<br/>...     digits.data.shape<br/>Out[1]: (1797, 64)</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Running k-means</h1>
                
            
            
                
<p>Setting up <em>k</em>-means works exactly the same as in the previous examples. We tell the algorithm to perform at most 10 iterations and stop the process if our prediction of the cluster centers does not improve within a distance of <kbd>1.0</kbd>:</p>
<pre>In [2]: import cv2...     criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,...                 10, 1.0)...     flags = cv2.KMEANS_RANDOM_CENTERS</pre>

<p>Then, we apply <em>k</em>-means to the data as we did before. Since there are 10 different digits (0-9), we tell the algorithm to look for 10 distinct clusters:</p>
<pre>In [3]: import numpy as np...     digits.data = digits.data.astype(np.float32)...     compactness, clusters, centers = cv2.kmeans(digits.data, 10, None,...                                                 criteria, 10, flags)</pre>
<p>And we're done!</p>
<p>Similar to the <em>N x 3</em> matrices ...</p></div>



  
<div><h1 class="header-title">Organizing clusters as a hierarchical tree</h1>
                
            
            
                
<p>An alternative to <em>k</em>-means is <strong>hierarchical clustering</strong>. One advantage of hierarchical clustering is that it allows us to organize the different clusters in a hierarchy (also known as a <strong>dendrogram</strong>), which can make it easier to interpret the results. Another useful advantage is that we do not need to specify the number of clusters upfront.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding hierarchical clustering</h1>
                
            
            
                
<p>There are two approaches to hierarchical clustering:</p>
<ul>
<li>In <strong>agglomerative hierarchical clustering</strong>, we start with each data point potentially being its own cluster, and we subsequently merge the closest pair of clusters until only one cluster remains.</li>
<li>In <strong>divisive hierarchical clustering</strong>, it's the other way around; we start by assigning all of the data points to the same cluster, and we subsequently split the cluster into smaller clusters until each cluster only contains one sample.</li>
</ul>
<p>Of course, we can specify the number of desired clusters if we wish to. In the following screenshot, we asked the algorithm to find a total of three clusters:</p>



<p>The preceding screenshot shows a step-by-step example of agglomerative ...</p></div>



  
<div><h1 class="header-title">Implementing agglomerative hierarchical clustering</h1>
                
            
            
                
<p>Although OpenCV does not provide an implementation of agglomerative hierarchical clustering, it is a popular algorithm that should, by all means, belong to our machine learning skill set:</p>
<ol>
<li>We start by generating 10 random data points, just like in the previous screenshot:</li>
</ol>
<pre style="padding-left: 60px">In [1]: from sklearn.datasets import make_blobs<br/>...     X, y = make_blobs(random_state=100, n_samples=10)</pre>
<ol start="2">
<li>Using the familiar statistical modeling API, we import the <kbd>AgglomerativeClustering</kbd> algorithm and specify the desired number of clusters:</li>
</ol>
<pre style="padding-left: 60px">In [2]: from sklearn import cluster<br/>...     agg = cluster.AgglomerativeClustering(n_clusters=3)</pre>
<ol start="3">
<li>Fitting the model to the data works, as usual, via the <kbd>fit_predict</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">In [3]: labels = agg.fit_predict(X)</pre>
<ol start="4">
<li>We can generate a scatter plot where every data point is colored according to the predicted label:</li>
</ol>
<pre style="padding-left: 60px">In [4]: import matplotlib.pyplot as plt<br/>... %matplotlib inline<br/>... plt.style.use('ggplot')<br/>... plt.scatter(X[:, 0], X[:, 1], c=labels, s=100)</pre>
<p>The resulting clustering is equivalent to the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1053 image-border" src="img/1c9d3311-d0f0-4f90-89aa-f93c45af9b60.png" style="width:38.00em;height:24.00em;" width="861" height="544"/></p>
<p>Finally, before we end this chapter, let's look at how to compare clustering algorithms and choose the correct clustering algorithm for the data you have!</p>
<p class="mce-root"/>


            

            
        
    </div>



  
<div><h1 class="header-title">Comparing clustering algorithms</h1>
                
            
            
                
<p>There are around thirteen different clustering algorithms in the <kbd>sklearn</kbd> library. Having thirteen different sets of choices, the question is: what clustering algorithms should you use? The answer is your data. What type of data you have and which clustering you would like to apply on it is how you will select the algorithm. Having said that, there can be many possible algorithms that could be useful for the kind of problem and data you have. Each of the thirteen classes in <kbd>sklearn</kbd> is specialized for specific tasks (such as co-clustering and bi-clustering or clustering features instead of data points). An algorithm specializing in text clustering would be the right choice for clustering text data. Hence, if ...</p></div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we talked about some unsupervised learning algorithms, including <em>k</em>-means, spherical clustering, and agglomerative hierarchical clustering. We saw that <em>k</em>-means is just a specific application of the more general expectation-maximization algorithm, and we discussed its potential limitations. Furthermore, we applied <em>k</em>-means to two specific applications, which were reducing the color palette of images and classifying handwritten digits.</p>
<p>In the next chapter, we will move back into the world of supervised learning and talk about some of the most powerful current machine learning algorithms: neural networks and deep learning.</p>


            

            
        
    </div>



  </body></html>