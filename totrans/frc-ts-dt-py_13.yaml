- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating Performance Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No model of a real-world phenomenon is perfect. There are countless statistical
    assumptions made about the underlying data, there is noise in the measurements,
    and there are unknown and unmodeled factors that contribute to the output. But
    even though it is not perfect, a good model is still informative and valuable.
    So, how do you know whether you have such a good model? How can you be sure your
    predictions for the future can be trusted? **Cross-validation** got us part of
    the way there, by providing a technique to compare unbiased predictions to actual
    values. This chapter is all about how to compare different models.
  prefs: []
  type: TYPE_NORMAL
- en: Prophet features several different metrics that are used for comparing your
    actual values with your predicted values, so you can quantify the performance
    of your model. This tells you how good your model actually is and whether you
    can trust the predictions, and helps you compare the performance of different
    models so you can choose which one is best.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will teach you about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Prophet’s metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Prophet performance metrics DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling irregular cut-offs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning hyperparameters with grid search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data files and code for examples in this chapter can be found at [https://github.com/PacktPublishing/Forecasting-Time-Series-Data-with-Prophet-Second-Edition](https://github.com/PacktPublishing/Forecasting-Time-Series-Data-with-Prophet-Second-Edition).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Prophet’s metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prophet’s `diagnostics` package provides six different metrics you can use to
    evaluate your model. Those metrics are mean squared error, root mean squared error,
    mean absolute error, mean absolute percent error, median absolute percent error,
    and coverage. We’ll discuss each of these in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Mean squared error** (**MSE**) is the sum of the squared difference between
    each predicted value and the actual value, as can be seen in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ![](img/B19630_13_F01.jpg) | (1) |'
  prefs: []
  type: TYPE_TB
- en: The number of samples is represented in the preceding equation by ![](img/B19630_13_F02.png),
    where ![](img/B19630_13_F03.png) is an actual value and ![](img/B19630_13_F14.png)
    is a forecasted value.
  prefs: []
  type: TYPE_NORMAL
- en: MSE may be the most used performance metric, but it does have its downside.
    Because it is not scaled to the data, its value is not easy to interpret – the
    unit of MSE is the square of your `y` unit. It is also sensitive to outliers,
    although this may be either desirable or undesirable, depending upon your data
    and interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: However, it remains popular because it can be proven that MSE is equal to the
    bias squared plus the variance, so minimizing this metric can reduce both bias
    and variance. MSE is never negative and the closer it is to zero, the better the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Root mean squared error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you scale MSE to the same units as that of your data by taking the square
    root, you arrive at the **root mean squared** **error** (**RMSE**):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ![](img/B19630_13_F05.png) | (2) |'
  prefs: []
  type: TYPE_TB
- en: RMSE shares the same advantages and disadvantages as MSE, although its units
    are more interpretable. As with MSE, it places more importance on points with
    large errors than those with small errors.
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Mean absolute error** (**MAE**) is similar to MSE except that it takes the
    absolute value of the error, not the square:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ![](img/B19630_13_F06.png) | (3) |'
  prefs: []
  type: TYPE_TB
- en: MAE, in contrast with MSE and RMSE, weighs each error equally; it does not place
    more importance on outliers or points with uncommonly high errors. Like MSE though,
    MAE is not scaled to the data. So, if you find that your model reports an MAE
    of, say, 10, is this good or bad? If the average value in your dataset is 1,000,
    then an error of 10 would be just 1%. If the average of your data is 1, though,
    then an MAE of 10 would mean your predictions are off by 1,000%!
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to scale MAE to the data, it will often be divided by the data’s mean
    value, to arrive at a percentage:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ![](img/B19630_13_F07.png) | (4) |'
  prefs: []
  type: TYPE_TB
- en: This format for MAE is not supported in Prophet, although you can create it
    yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute percent error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Mean absolute percent error** (**MAPE**) is another very common metric despite
    its poor ability to represent the performance of a model. Not to be confused with
    the total MAE divided by the mean value, MAPE divides each error by the value
    of the data point at that error:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ![](img/B19630_13_F08.png) | (5) |'
  prefs: []
  type: TYPE_TB
- en: This makes the metric skewed to overly represent errors that occur when the
    data values are low. For this reason, MAPE is considered asymmetric—it puts a
    heavier penalty on negative errors (when the forecast is higher than the actual
    result) than on positive errors. Optimizing for MAPE will often leave your model
    undershooting the values it is targeting. Furthermore, because you are dividing
    by each ![](img/B19630_13_F09.png) value, if any of them are zero, then the calculation
    will produce a division-by-zero error. Very small values of ![](img/B19630_13_F10.png)
    will also cause floating-point calculation problems. Prophet will detect whether
    any ![](img/B19630_13_F11.png) values are at or near zero and if found, it will
    simply skip MAPE calculations and proceed to the other metrics called for. The
    upside to MAPE, though, is that it has natural interpretability – it is easy to
    intuitively understand.
  prefs: []
  type: TYPE_NORMAL
- en: Median absolute percent error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Median absolute percent error** (**MdAPE**) is the same as MAPE, except it
    uses the median instead of the mean. It can be useful with noisy data when MAPE
    may be the preferred metric but too many outliers are swaying it. For example,
    significant holidays can create large spikes in data and the median is able to
    smooth out predictions if MAPE experiences issues.'
  prefs: []
  type: TYPE_NORMAL
- en: Symmetric mean absolute percent error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **symmetric mean absolute percent error** (**SMAPE**) attempts to overcome
    the asymmetric deficiency of MAPE described previously.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ![](img/B19630_13_F12.png) | (6) |'
  prefs: []
  type: TYPE_TB
- en: SMAPE is expressed as a percentage, which allows it to compare performance between
    datasets of varying magnitude. A shortcoming of SMAPE, though, is that it becomes
    unstable when both the actual value and forecast value are close to zero. The
    upper limit of the equation is 200%, which can feel a bit bizarre intuitively.
    For this reason, some formulations of the equation leave out the division by 2
    in the denominator.
  prefs: []
  type: TYPE_NORMAL
- en: Coverage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final Prophet metric is **coverage**. Coverage is simply the percentage
    of actual values that lie between the predicted upper and lower uncertainty bounds.
    By default, the uncertainty limits cover 80% of the data, so your coverage value
    should be 0.8.
  prefs: []
  type: TYPE_NORMAL
- en: If you find a coverage value that does not equal the `interval_width` set during
    model instantiation, it means your model is not well calibrated to the uncertainty.
    In practice, this simply means that you probably cannot trust the stated uncertainty
    intervals in the future portions of your forecast and may want to adjust them
    based on the coverage value.
  prefs: []
  type: TYPE_NORMAL
- en: And of course, the cross-validation DataFrame contains all of your actual ![](img/B19630_13_F13.png)
    values and your model’s predicted ![](img/B19630_13_F14.png) values, so any other
    metric you can come up with to compare those values, you can calculate and use
    yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the best metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deciding which performance metric to optimize your model with is not a trivial
    choice. It can have a significant impact on your final model, depending on the
    characteristics of the data. When worked out mathematically, it can be shown that
    optimizing your model for MSE will create a model predicting values close to the
    mean of your data, and optimizing for MAE will create predictions close to the
    median value. Optimizing for MAPE will tend to produce abnormally low forecasts
    because it applies such a high weight to errors occurring at low points in the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: So, between MSE (or RMSE) and MAE, which is better? RMSE aims to be correct
    to the average data point and MAE aims to overshoot the actual value as often
    as it undershoots. This difference will only materialize when the mean and median
    of your data are different – in highly skewed data. As the median will be further
    from the tail in skewed data than the mean will be, the MAE will introduce bias
    toward the bulk of data and away from the tail. A biased model is the greatest
    disadvantage of MAE.
  prefs: []
  type: TYPE_NORMAL
- en: MSE’s disadvantage is its sensitivity to outliers. Imagine a time series that
    is generally flat except for a couple of extreme outliers. MSE will really focus
    on the forecast errors of those outliers, so it will tend to miss the mark more
    often than MAE will. In general, the median is more robust to outliers than the
    mean.
  prefs: []
  type: TYPE_NORMAL
- en: So, should we consider robustness to outliers a good thing? Not necessarily.
    If your time series is intermittent – that is, if most dates have a ![](img/B19630_03_F02.png)
    value of 0 – you don’t want to target the median value but the mean. The median
    will be 0! In this case, you would desire MSE precisely because it is sensitive
    to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there is no easy answer to which is the best metric to use. The
    analyst must pay attention to bias, skewness, and outliers to determine which
    metric will work best. And there is no reason you can’t try multiple metrics and
    see which forecast seems the most reasonable to you!
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Prophet performance metrics DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you’ve learned what the different options are for performance metrics
    in Prophet, let’s start coding and see how to access them. We’ll use the same
    online retail sales data we used in [*Chapter 12*](B19630_12.xhtml#_idTextAnchor794),
    *Performing Cross-Validation*. Along with our usual imports, we are going to add
    the `performance_metrics` function from Prophet’s `diagnostics` package and the
    `plot_cross_validation_metric` function from the `plot` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s load the data, create our forecast, and plot the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we’re not interested in any future predictions, we don’t need to create
    the `future` DataFrame. We’ll just focus on the 3 years of data we’ve got:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Online retail sales forecast](img/Fig_13.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Online retail sales forecast
  prefs: []
  type: TYPE_NORMAL
- en: 'The `performance_metrics` function requires a cross-validation DataFrame as
    input, so we’ll create one in the same manner as you learned in [*Chapter 12*](B19630_12.xhtml#_idTextAnchor794),
    *Performing Cross-Validation*. We’ll set `horizon` to `90 days`, so each fold
    in the cross-validation will be `90 days`. The `period` of `30 days` is how often
    to begin a new fold and `initial` being set to `730 days` is our first 2-year
    training period, untouched by validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll send `df_cv` to the `performance_metrics` function. By default,
    this function will calculate each of the five available metrics. You can specify
    a subset of these by passing a list of metric names to the `metrics` argument.
    Let’s include all five and display the first few rows of the resulting DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output DataFrame is indexed by days in the `horizon`, so each row represents
    the values of those metrics when the model is asked to forecast that many days
    out. This is just the first five rows (your results may vary slightly due to randomness
    in the optimization algorithm):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Performance metrics DataFrame](img/Fig_13.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 – Performance metrics DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering why the first row in the `horizon` column is `9 days`.
    Each metric value in the DataFrame is the rolling average of its calculation up
    to the day specified. The `performance_metrics` function takes a `rolling_window`
    argument where you can change the window size, but the default is `0.1`. This
    number is the fraction of `horizon` to include in the window. With 10% of our
    90-day `horizon` being 9 days, this is the first row of the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use this DataFrame on its own or you can visualize it with Prophet’s
    `plot_cross_validation_metric` function. This function actually calls the `performance_metrics`
    function itself, so you do not need to create a `df_p` first, just a `df_cv`.
    Here, we’ll plot the MAE by passing `''mae''` to the `metric` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot shows each MAE measurement along the horizon and the rolling
    average value of those measurements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Cross-validation plot](img/Fig_13.3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – Cross-validation plot
  prefs: []
  type: TYPE_NORMAL
- en: Our cross-validation settings were `horizon='90 days', period='30 days', initial='730
    days'`, which, for the 1 year of data remaining after the initial training period,
    resulted in a total of ten 90-day forecasts. So, for each day in our horizon,
    the preceding plot will have 10 MAE measurements. If you counted up all the dots
    on that plot, it should be 900\. The solid line is the rolling average value,
    with the window size being the same default, `0.1`, as the `performance_metrics`
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify this by using the same `rolling_window` argument in the `plot_cross_validation_metric`
    function. Just to make it very clear how this window size affects the plot, let’s
    compare two RMSE plots, one with a 1% window size and one with a 10% size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `ax` argument to plot both lines on the same chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Comparing different window sizes](img/Fig_13.4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 – Comparing different window sizes
  prefs: []
  type: TYPE_NORMAL
- en: The smoother line is the one with a wider window size, the default window size.
    Because the window is not centered but set to the right edge, the first 8 days
    do not show the rolling average line when using 10% of the horizon. Setting the
    window to 1% will include all data at the cost of being noisier.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve learned how to use the cross-validation plot, let’s use it to
    see a problem that can arise when letting Prophet automatically select the cut-off
    dates to begin each cross-validation fold.
  prefs: []
  type: TYPE_NORMAL
- en: Handling irregular cut-offs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll be using a new dataset for this example. The **World Food Programme**
    (**WFP**) is the branch of the United Nations focused on hunger and food security.
    One of the greatest contributing factors to food security issues in developing
    countries that the WFP tracks is rainfall because it can affect agricultural production.
    Thus, predicting rainfall is of critical importance in planning aid delivery.
  prefs: []
  type: TYPE_NORMAL
- en: This data represents the rainfall received over 30 years in one of the regions
    the WFP monitors. What makes this dataset unique is that the WFP recorded the
    amount of rain that accumulated three times per month, on the 1st, the 11th, and
    the 21st. The accumulation from the 1st to the 11th is a 10-day period. It’s the
    same from the 11th to the 21st. But the period from the 21st of one month to the
    1st of the next varies depending upon the month. In a normal February, it will
    be 8 days. In a leap year, 9 days. Months of 30 and 31 days will see a period
    of 10 and 11 days respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s perform cross-validation as you’ve learned so far and see what effect
    this will have. First, we need to train a Prophet model on the data. You should
    have everything already imported if you’re continuing from the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you remember, cross-validation is not concerned with any future, unknown
    periods. Therefore, it’s unnecessary to build a `future` DataFrame and predict
    on it. I did so in this example merely to remind you of the first potential pitfall
    you learned about in [*Chapter 4*](B19630_04.xhtml#_idTextAnchor197), *Handling
    Non-Daily Data*, when we used data with regular gaps. We needed to adjust our
    `future` DataFrame to avoid unconstrained predictions, and we’ve done that again
    here by restricting future dates only to those on the 1st, 11th, and 21st of each
    month. Here’s what the forecast looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – Rainfall forecast](img/Fig_13.5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 – Rainfall forecast
  prefs: []
  type: TYPE_NORMAL
- en: It has a nearly flat trend, rising slightly until 2010 and then turning downward.
    As you may have expected, the model is dominated by yearly seasonality, with rainfall
    in December (summer in the Southern Hemisphere) at almost zero and rainfall at
    its maximum in June.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s build a cross-validation plot. We’ll forecast `90 days` (the horizon)
    and create a new fold every `30 days` (the period). Our initial training period
    will be `1826 days`, or 5 years. Finally, let’s plot the RMSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Prophet uses `horizon`, `period`, and `initial` to calculate a set of evenly
    spaced cut-offs. `horizon` is then used again to set the length of each fold’s
    forecast but `period` and `initial` are not needed after choosing the cut-offs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The effect of letting Prophet automatically set the cut-offs is that they are
    inconveniently located compared to our data. We only have data for 3 days per
    month, and those 3 days are not consistently spaced. This means that each fold
    in our cross-validation starts effectively randomly somewhere in the data, producing
    a plot that seems to suggest each day in the horizon has data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.6 – Cross-validation with automatic cut-offs](img/Fig_13.6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 – Cross-validation with automatic cut-offs
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cross_validation` function will accept a `cutoffs` argument that takes
    a list of user-specified cut-off dates to use. This also means that `initial`
    and `period` are no longer necessary. This code block will use a list comprehension
    to iterate over each year, then each month, then each day of either the 1st, 11th,
    or 21st, and create a list of pandas `Timestamp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we replot our cross-validation but send this list of cut-off dates,
    we’ll see something dramatically different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, each fold is begun on a day for which we have data. The next day for which
    data exists will be either 8, 9, 10, or 11 days later. Hence, the plot shows 4
    discrete days in `horizon` where a forecast took place:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7 – Cross-validation with custom cut-offs](img/Fig_13.7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 – Cross-validation with custom cut-offs
  prefs: []
  type: TYPE_NORMAL
- en: Both *Figure 13**.6* and *Figure 13**.7* show an average RMSE of just above
    20, so the results are very similar. The difference is simply the ease of interpretation
    and consistency. You may encounter this situation often if your data is recorded
    monthly or in any increment of months because they have inconsistent durations.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning hyperparameters with grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the final section of this chapter, we’ll look at grid search and work through
    an example, continuing with this rainfall data. If you’re not familiar with the
    concept of grid search, it’s a way to exhaustively check all reasonable combinations
    of hyperparameters against a performance indicator and choose the best combination
    to train your final model. With Prophet, you might decide to select the following
    hyperparameters and values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8 – Prophet grid search parameters](img/Fig_13.8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 – Prophet grid search parameters
  prefs: []
  type: TYPE_NORMAL
- en: With these parameters, a grid search will iterate through each unique combination,
    use cross-validation to calculate and save a performance metric, and then output
    the set of parameter values that resulted in the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prophet does not have a grid search method the way, for example, `sklearn`
    does. One is easy enough to build yourself in Python though, so let’s see how
    to set it up. The first step is to define our parameter grid. We’ll use the grid
    shown in *Figure 13**.8*, but we’re not including holidays in our model (the weather
    doesn’t regularly check its calendar and adjust rainfall if it finds a holiday!),
    so we’ll leave that out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll use Python’s `itertools` package to iterate over that grid and
    create a list of each unique combination. We’ll need to import `itertools` first;
    and while we’re at it, let’s import `numpy` as well, because we’ll be using it
    later. We’ll also create an empty list to hold all of the RMSE values, assuming
    that’s our chosen performance metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We could allow Prophet to define our cut-off periods, but because we’re using
    this rainfall data, let’s set `cutoffs` ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The final step in running our grid search, before we evaluate the results, is
    to iterate over each combination we saved in the `all_params` list, and build
    a model, a cross-validation DataFrame, and a performance metrics DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say that we know we want `yearly_seasonality=4` to keep the curve smooth,
    and we’ll complete model instantiation with the parameter combination for that
    iteration. In the `performance_metrics` function, we are using `rolling_window=1`.
    This means that we are averaging 100% of the data in that fold to calculate the
    metric, so instead of a series of values, we only get one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'That code is going to take a long time to run. The length of our `all_params`
    list, after all, is 32, which means you’ll be training and cross-validating 32
    total models. I did say grid search was exhaustive! (On a typical laptop, you
    can expect it will take around 8-12 hours to complete; to speed up the example,
    you may consider reducing the number of parameters in the `param_grid` dictionary,
    such as, for example, `param_grid = {''changepoint_prior_scale'': [0.1, 0.01]`,
    `''seasonality_prior_scale'': [1.0, 0.1]}`, which will train and cross-validate
    only four total models. Be sure to recreate your `all_params` dictionary after
    changing `param_grid`.) To inspect the results, we’ll build a DataFrame with the
    parameter combinations and their associated RMSEs, and then display a portion
    of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The full DataFrame has 32 rows, one for each combination of parameters, but
    here we see the first five rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9 – Grid search DataFrame](img/Fig_13.9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 – Grid search DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s use NumPy to find the parameters with the lowest RMSE value
    and then print them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Printing `best_params` should display this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The biggest difference between the best parameters found with grid search and
    those we’ve used so far is that the changepoint regularization would be better
    set to a much stronger level. With a lower prior scale, the magnitudes of changepoints
    would be less, and the trend curve would be even flatter. Intuitively, this seems
    appropriate; especially for longer forecasts, where allowing larger trend changes
    would create unrealistic rainfall forecasts far into the future.
  prefs: []
  type: TYPE_NORMAL
- en: Probably the most critical parameter to tune is `changepoint_prior_scale`. If
    this value is too small, the trend will underfit the variance. Variance that should
    be modeled with the trend will instead be modeled in the noise term. If the prior
    scale is too large, the trend will exhibit too much flexibility and may start
    to capture some of the yearly seasonality. A range between `0.5` and `0.001` will
    work in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: The `seasonality_prior_scale` parameter is probably the second most impactful
    parameter. A typical range is usually `10`, with essentially no regularization,
    down to `0.01`. Anything smaller and the seasonality is likely to be regularized
    to a negligible effect. You also have the option of setting each seasonality to
    `False` and using `add_seasonality` to choose prior scales individually, but this
    causes your grid search to increase in computing time exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: You may also want to add `fourier_order` to your grid search, but I’ve found
    it works great to build a quick model with defaults, inspect the components, and
    choose Fourier orders myself that fit my intuition. In a fully automated setup,
    keeping Fourier orders at their defaults will probably be fine.
  prefs: []
  type: TYPE_NORMAL
- en: '`holidays_prior_scale` is also a tunable parameter, with many of the same characteristics
    as `seasonality_prior_scale`. Just keep in mind that many models won’t have holidays
    and so there will be no need to include this parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: The last of the critical parameters that should always be considered is `seasonality_mode`.
    In this book, you have learned a few rules of thumb to help decide which mode
    to use, but more often than not, it isn’t clear. The best thing to do is simply
    inspect a plot of your time series and see whether the magnitude of seasonal fluctuations
    grows with the trend or stays constant. If you can’t tell, go ahead and add `seasonality_mode`
    to the grid.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the default value of 80% for `changepoint_range` will be good. It provides
    a nice balance of allowing the trend to change where appropriate but not allowing
    it to overfit in the last 20% of data where errors cannot be corrected. If you’re
    the analyst and paying close attention, it’s easy to see if the default range
    is not appropriate. But in a fully automated setting, it’s probably better to
    be conservative and leave it at 80%.
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining parameters are best left out of your grid search. For `''growth''`,
    it is either `''linear''`, `''logistic''`, or `''flat''`, and you as the analyst
    should choose. Setting it to `''logistic''` will require setting `''cap''` and
    `''floor''` as well. Many of the remaining parameters, such as `n_changepoints`
    and the yearly, weekly, and daily seasonalities, are better controlled with parameters
    already included in the search: `changepoint_prior_scale` in the case of changepoints
    and `seasonality_prior_scale` with seasonalities.'
  prefs: []
  type: TYPE_NORMAL
- en: The final parameters, `mcmc_samples`, `interval_width`, and `uncertainty_samples`,
    don’t affect your `yhat` in any way and therefore have no effect on your performance
    metric. They only control the uncertainty intervals.
  prefs: []
  type: TYPE_NORMAL
- en: Use common sense with grid search – it is a very long process, so don’t include
    every parameter and every possible value in your hyperparameter grid. Often the
    best approach an analyst can take is to provide intuition and human touch to the
    process and let the computer do the number-crunching.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use Prophet’s performance metrics to extend
    the usefulness of cross-validation. You learned about the six metrics Prophet
    has out of the box, namely, MSE, RMSE, MAE, MAPE, MdAPE, and coverage. You learned
    about many of the advantages and disadvantages of these metrics, and situations
    where you may want to use or avoid any one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you learned how to create Prophet’s performance metrics DataFrame and
    use it to create a plot of your preferred cross-validation metric so as to be
    able to evaluate the performance of your model on unseen data across a range of
    forecast horizons. You then used this plot with the WFP’s rainfall data to see
    a situation where Prophet’s automatic cut-off date selection is not ideal, and
    how to create custom cut-off dates.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you brought all of this together in an exhaustive grid search of Prophet
    hyperparameters. This process enabled you to use a data-driven technique to fine-tune
    your model and optimize it for a metric of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, the final chapter of this book, you will learn about a
    few more tricks in Prophet’s bag to help put your models into a production environment.
  prefs: []
  type: TYPE_NORMAL
