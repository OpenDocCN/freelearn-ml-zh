- en: Analyzing Visitor Patterns to Make Recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will focus on the problem of finding similar visitors based on
    the theme park attractions they attend, to make improved marketing recommendations.
    Collaborative filtering methods will be introduced with examples showing how to
    train and obtain custom recommendations both in Apache Spark (EMR) and through
    the AWS SageMaker built-in algorithms. Many companies leverage the kinds of algorithms
    we describe in this chapter to improve the engagement of their customers by recommending
    products that have a proven record of being relevant to similar customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Making theme park attraction recommendations through Flickr data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding recommendations through Apache Spark's Alternating Least Squares method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommending attractions through SageMaker Factorization Machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making theme park attraction recommendations through Flickr data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we will make use of the dataset from [https://sites.google.com/site/limkwanhui/datacode](https://sites.google.com/site/limkwanhui/datacode),
    which consists of Flickr data from users who take photos at different locations,
    these photos are then mapped to known theme park attractions. Flickr is an image-hosting
    service. Let's assume Flickr wants to create a plug-in on their mobile app that,
    as users take photos on the different attractions, identifies user preferences
    and provides recommendations on other attractions that might be of interest to
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Let's also suppose that the number of photos a user takes on a particular attraction
    is an indicator of their interest in the attraction. Our goal is to analyze a
    dataset with triples of the *user ID, attraction, number of photos taken* form
    so that given an arbitrary set of attractions visited by a user, the model is
    able to recommend new attractions that similar users found interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Collaborative filtering is a process for providing recommendations to users
    based on their behavior by analyzing the behaviors of a lot of users. We observe
    the effects of this algorithm in our day-to-day life in a large number of applications.
    For example, when you are using streaming services, such as Netflix or YouTube,
    it recommends videos that you may be interested in based on your streaming history.
    Social networks, such as Twitter and LinkedIn, suggest people for you to follow
    or connect with based on your current contacts. Services such as Instagram and
    Facebook curate posts from your friends and tailor your timeline based on the
    posts that you read or like. As a data scientist, collaborative filtering algorithms
    are really useful when you are building recommendation systems based on a large
    amount of user data.
  prefs: []
  type: TYPE_NORMAL
- en: There are various ways in which collaborative filtering can be implemented on
    a dataset. In this chapter, we will be discussing the memory-based approach and
    the model-based approach.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-based approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the memory-based approach, we generate recommendations in two phases. Consider
    a situation where we are trying to generate recommendations for a given user based
    on their interests. In the first phase, we discover users who are similar to the
    given user based on their interests. We rank all the users based on how similar
    they are to a given user. In the second phase, we discover the top interests among
    the group of users that are most similar to a given user. The top interests are
    ranked based on their similarity to the set of top-ranked users. This ranked list
    of interests is then presented to the original user as recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the process of movie recommendations, we look at the movies
    a user is interested in or has watched recently and discover other users who have
    watched similar movies. Based on the top-ranked list of similar users, we look
    at the movies they have watched recently and rank them based on the similarity
    to the list of ranked users. Then, the top-ranked movies are then presented as
    recommendations to the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find a similarity between users, we use functions called similarity measures.
    Similarity measures are popularly used in search engines to rank a similarity
    between query terms and documents. In this section, we discuss the cosine similarity
    measure, which is commonly used in collaborative filtering. We treat each user''s
    interest as a vector. To discover users with similar interests, we calculate the
    cosine of the angle between two users'' interest vector. It can be represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e44e8858-e65a-442f-86e8-cc65325e8e62.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on the similarity between a given user and all users in the dataset, we
    select the top k users. We then aggregate the interest vectors of all users to
    discover the top-ranked interests and recommend it to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Note that memory-based models do not use any modeling algorithms that were discussed
    in the previous chapters. They only rely on simple arithmetic to generate recommendations
    for users based on their interests.
  prefs: []
  type: TYPE_NORMAL
- en: Model-based approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the model-based approach, we use machine learning techniques to train a
    model that can predict the probability of each interest being relevant to a given
    user. Various algorithms, such as Bayesian models or clustering models, can be
    applied for model-based collaborative filtering. However, in this chapter, we
    focus on the matrix-factorization-based approach.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix factorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The matrix factorization approach works by decomposing a matrix of users and
    the interests of the users. In this methodology, we map the user data and the
    information about the interests to a set of factors. The score of a user to interest
    is calculated by taking a dot product of the vector scores for the user and the
    interest.  These factors can be inferred from the user ratings or from the external
    information about the interests in the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider a matrix where one dimension represents the users and
    the other dimension represents the movies the users have rated:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Avengers** | **Jumanji** | **Spiderman** |'
  prefs: []
  type: TYPE_TB
- en: '| User 1 | 4 |  | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| User 2 | 1 | 5 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| User n | 5 | 2 | 4 |'
  prefs: []
  type: TYPE_TB
- en: The values in the matrix are the ratings provided by the users. The matrix factorization
    methodology maps the movies into shorter vectors that represent concepts, such
    as the genre of the movie. These vectors are known as latent factors. We map the
    movies to genres and also map what genres users are interested in based on how
    they rate movies in each genre. Using this information, we can calculate the similarity
    between a user and movie based on the dot product (multiplying the interest of
    the user in genres by the likelihood of the movie to belong to genres) between
    both the vectors. Thus, the unknown ratings in the matrix can be predicted using
    the knowledge of known ratings by consolidating the users and interests to less
    granular items (that is, genres). In our previous example, we assume that we already
    have a known mapping of movies to genre. However, we cannot make an assumption
    that we will always have explicit data to generate such mappings to latent factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we explore methodologies that can help us generate such mappings automatically
    based on the data.  Matrix factorization models therefore need to be able to generate
    a map between users and interests through a latent factors vector. To ensure we
    can generate a dot product between the latent factors of a user and the item,
    the length of the latent factors is set to a fixed value. Each interest item, ![](img/b1751de6-aac4-4c85-b8e3-cda4ce8defca.png),
    is represented by a vector, ![](img/d82d8f27-75f4-420f-82a4-670c39aff92a.png),
    and each user, ![](img/0e7f0f13-8c2c-4560-9ff1-528999b61e2a.png), is represented
    by a vector, ![](img/7a61b9cd-ac9f-4912-b6f4-68d4ea25bac9.png). The ![](img/f16968da-4c4d-4caf-bfbe-beffe6d887e2.png)
    and ![](img/760e2953-72f9-4f46-915e-7da0de7cd402.png) vectors are both latent
    factors that are derived from the data. The rating for an item, ![](img/bb19f614-4e7e-4d27-95ad-737235fd23e8.png),
    for a user, ![](img/2cf02bfe-0617-47cc-865a-32914451a6bb.png), is represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef81c489-1e91-4258-9b3f-b17578757613.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In general, if we already have a partial set of ratings from users to interests,
    we can use that to model ratings between other users and interests. We use optimization
    techniques to calculate this. Our objective is to predict the values of ![](img/0b81e02c-349c-4ee0-be51-a825132cce01.png)
    and ![](img/edfabc37-59ef-44c3-88bb-61f64d4263e8.png). Hence, we do that by minimizing
    the regularized error when predicting these vectors by using the known ratings.
    This is represented in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08e98314-a4eb-49c1-bd23-0cb4916391f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *k* is a set of ![](img/b89e6751-a35a-462f-9921-9fbbd6021e15.png) where
    the rating, ![](img/9e086582-82c0-4fda-b029-dee387107679.png), is known. Now,
    let's look at study two approaches for minimizing the preceding equation.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We studied the stochastic gradient-descent algorithm in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms* regarding linear regression. A similar
    methodology is used to minimize the function to predict the correct latent factors
    for each user and interest. We use an iterative approach, where during each iteration,
    we calculate the error of predicting ![](img/a7ba6505-3b19-432d-a187-5f83f41d271c.png)
    and ![](img/695327bb-3516-4b2f-8d6d-0511dc04cd6d.png) based on all the known ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55123d63-6338-4689-a390-47ed3a1e378b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on the magnitude of error, we update the values of  ![](img/629216ac-47e5-43be-b576-bb9812831018.png)
    and ![](img/bb88a91e-797e-45b5-bd34-99e1820eb0a2.png) in the opposite direction
    of the error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4231d69-9439-4118-af42-1eeb63226ff3.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/3653b777-1627-4d63-98e9-6983a00d2316.png)'
  prefs: []
  type: TYPE_IMG
- en: We stop the iterations after the values of ![](img/a7ba6505-3b19-432d-a187-5f83f41d271c.png)
    and ![](img/695327bb-3516-4b2f-8d6d-0511dc04cd6d.png) converge. Stochastic gradient
    descent is also used in algorithms such as **Factorization Machines **(**FMs**),
    which uses it to compute values of vectors. FMs are a variant of **support vector
    machine** (**SVM**) models that can be applied in a collaborative filtering framework.
    We do not explain support vector machines or FMs in detail in this book, but encourage
    you to understand how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Alternating Least Squares
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the challenges of minimizing the optimization function to predict the
    values of both ![](img/c5e3c13e-f99f-4f26-90be-89c5eb603d61.png) and ![](img/39b18f7c-0914-4b9a-8e92-464d3b246e6d.png)
    is that the equation is not convex. This is because we are trying to optimize
    two values at the same time. However, if we used a constant for one of the values,
    or ![](img/c5e3c13e-f99f-4f26-90be-89c5eb603d61.png) or ![](img/39b18f7c-0914-4b9a-8e92-464d3b246e6d.png),
    we can solve the equation optimally for the other variable. Hence, in the Alternating
    Least Squares technique, we alternatively set the values of ![](img/c5e3c13e-f99f-4f26-90be-89c5eb603d61.png)
    and ![](img/39b18f7c-0914-4b9a-8e92-464d3b246e6d.png) as constant while optimizing
    for the other vector.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, in the first step, we set base values for both the vectors. Assuming
    that one of the values is constant, we use linear programming to optimize the
    other vector. In the next step, we set the value of the optimized vector as constant
    and optimize for the other variable. We will not explain how linear programming
    is used to optimize for quadratic questions as it is an entire field of study
    and not in the scope of this book. This methodology optimizes each vector until
    convergence.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of stochastic gradient descent is that it is faster than the ALS
    method, as it depends on predicting the values of both the vectors in each step
    while modifying the vectors based on the proportion of errors. However, in the
    ALS methodology, the system calculates the values of each vector independently,
    and hence leads to better optimization. Moreover, when the matrix is dense, the
    gradient descent methodology has to learn from each set of data, making it less
    efficient than the ALS methodology.
  prefs: []
  type: TYPE_NORMAL
- en: Finding recommendations through Apache Spark's ALS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will go through the process of creating recommendations
    in Apache Spark using **Alternating Least Squares** (**ALS**).
  prefs: []
  type: TYPE_NORMAL
- en: Data gathering and exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step is to download the data from [https://sites.google.com/site/limkwanhui/datacode](https://sites.google.com/site/limkwanhui/datacode)
    . We will be using the `poiList-sigir17` dataset with photos taken by users at
    different theme park attractions (identified as points of interest by Flickr).
    There are following two datasets we''re interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of points of interests, which captures the names and other properties
    of each attraction:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the first few lines of the `poi_df` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf8ca937-b9e2-4f10-919c-8acb409538fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The photos taken by Flickr users at different points of interest:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows a sample of the `visits_df` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb169bd1-9b0d-4330-bb02-2deae5faff7e.png)'
  prefs: []
  type: TYPE_IMG
- en: In this dataset, we will be using the `nsid` field (indicating the user taking
    the photo) and `poiID`, which indicates the actual point of interest or attraction
    visited while taking the photo. For our purposes, we will ignore the rest of the
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do some basic inspection on our dataset. The dataset has about 300,000
    rows of data. By taking a sample of 1,000 entries, we can see that there are 36
    unique Flickr users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding `describe()` command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is important, as we need to have enough entries per user to ensure we have
    enough information about users to make predictions. Furthermore, it's actually
    more relevant to know whether users visit different attractions. One the nice
    things about Apache Spark is that one can work on datasets using SQL. Finding
    the number of distinct attractions users see on average can easily be done with
    SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to work with SQL, we first need to give a table name to the dataset.
    This is done by registering a temp table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we register the tables, we can do queries, such as finding the number
    of unique attractions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Or we can combine SQL with other dataset operations, such as `.describe()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot contains the result of the output of the `show()`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6020f4a-0a8b-4555-afe9-877e3e6886b5.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding SQL command finds the number of distinct attractions each user
    visits. The describe dataset operation finds statistics on these users, which
    tells us that, on average, users visit about five different locations. This is
    important as we need to have enough attractions per user to be able to correctly
    identify user patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we should look at the number of photos users take at each location,
    to validate that in fact we can use the number of photos taken as an indicator
    of the user''s interest. We do that through the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is shown by the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92832e20-1d60-4f6d-8293-1c2682b54201.png)'
  prefs: []
  type: TYPE_IMG
- en: The SQL command counts the number of entries for each user and attraction, and
    then we find a statistical summary using the describe. We can conclude therefore
    that on average, each user takes about eight pictures at every location they visit.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train our model, we will construct a dataset that computes the number of
    photos taken by each user at each location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the first few lines of the `train_df` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdcd6859-698d-47f8-bf86-8cec71cca1b0.png)'
  prefs: []
  type: TYPE_IMG
- en: We hash the user because the ALS trainer just supports numerical values as features.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the model, we simply need to construct an instance of ALS and provide
    the user column, item column (in this case the attraction IDs), and the rating
    column (in this case, `pictures_takes` is used as a proxy for rating). `coldStartStrategy`
    is set to drop as we''re not interested in making predictions for users or attractions
    not present in the dataset (that is, predictions for such entries will be dropped
    rather than returning NaN):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Getting recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we build a model, we can generate predictions for all users in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will pick the top 10 recommendations for each user. Note
    that because of how ALS works, it might actually recommend attractions already
    visited by the user, so we need to discard that for our purposes, as we will see
    later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recommendations look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/debd84dc-dfbb-44cb-bce9-2a1bef310250.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each user gets a list of tuples with the recommended attraction as well as
    the score for the recommendation. In this case, the score represents the estimated
    number of photos we would expect each user to take at the recommended location.
    Even though the model just provides the IDs of the attractions, we would like
    to inspect a few of these recommendations to make sure they are good. In order
    to do that, we will construct a dictionary of IDs to attraction names (point of
    interest names) by collecting the result of a query that finds the name of each
    attraction in the points table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The map contains the following entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For each user, we want to remove the recommendations for already-visited sites
    and output the recommendations. To do that, we need to process the list of tuples
    on each row. Apache Spark provides a convenient way to do this by allowing users
    to create custom SQL functions, or **user-defined functions** (**UDFs**). We will
    define and register a UDF that is capable of extracting the names of each recommended
    attraction through the use of the preceding map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `poi_names` function receives the recommendations tuple for a user as well
    as the attractions visited and then returns a string that contains all recommended
    attraction names that were not in the set of visited, as well as an enumeration
    of the visited attractions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then register the recommendations as a table so it can be used in our next
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding query joins the user recommendations table with the visits table
    and joins by user, collecting all points of interest visited by each user, and
    through the UDF it outputs the recommended attractions as well as the names of
    the already-visited attractions. We sample and collect a few instances of the
    table to inspect. In the companion notebook, we can observe the entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can observe that this user visited a number of adventure-like attractions
    and the model recommended a few more. Here, the reader can inspect a couple more
    recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Recommending attractions through SageMaker Factorization Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FMs are one of the most widely used algorithms for making recommendations when
    it comes to very sparse input. It is similar to the **stochastic gradient descent** (**SGD**)
    algorithm we discussed under the model-based matrix factorization methodology.
    In this section, we will show how to use AWS' built-in algorithm implementation
    of FMs to get recommendations for our theme park visitors.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the dataset for learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to use such an algorithm, we need to prepare our dataset in a different
    way. We will pose the recommendation problem as a regression problem in which
    the input are a pair of user and attraction, and the output is the expected level
    of interest this user will have toward the attraction. The training dataset must
    have the actual empirical interest (measured by the number of photos taken) for
    each pair of user and attraction. With this data, the FM model will then be able
    to predict the interest of an arbitrary attraction for any user. Hence, to obtain
    recommendations for a user, we just need to find the list of attractions that
    yields the highest predicted level of interest.
  prefs: []
  type: TYPE_NORMAL
- en: '**So then how do we encode the user and the attractions in a dataset?**'
  prefs: []
  type: TYPE_NORMAL
- en: Given that FMs are extremely good at dealing with high-dimensional features,
    we can one-hot encode our input. Since there are 8,903 users and 31 attractions,
    our input vector will be of length 8,934 where the first 31 vector components
    will correspond to the 31 different attractions, and the remaining positions correspond
    to each user. The vector will always have zeros except for the positions corresponding
    to the user and attraction, which will have a value of 1\. The target feature
    (label) used in our model will be the level of interest, which we will discretize
    to a value of 1 to 5 by normalizing the number of pictures taken according to
    their corresponding quantile.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows how such a training dataset could look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/351fd038-4fe8-4e44-adfc-5d92549a6256.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can imagine, this matrix is extremely sparse, therefore we need to encode
    our rows using a sparse representation. Like most SageMaker algorithms, we must
    drop our data in S3 to allow SageMaker to train the data. In past chapters, we
    used CSV as an input. However, CSV is not a good representation for our dataset;
    given its sparse nature, it would occupy too much space (with a lot of repeated
    zeros!). In fact, at the time of writing, SageMaker doesn''t even support CSV
    as an input format. In a sparse representation, each vector must indicate the
    following three values:'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The positions in which we have a value other than 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The values at each of these non-zero positions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, the sparse representation for the first row in the preceding figure
    would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Vector size = 8934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-zero positions = [1, 33]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values at non-sero positions = [1, 1]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only input format FMs currently supports is called protobuf recordIO. Protobuf,
    short for **Protocol buffers**, is a language-neutral, platform-neutral extensible
    mechanism for serializing structured data initially developed by Google. In our
    case, the structure will be the sparse representation of our matrix. Each record
    in the protobuf file we store in S3 will have all three items necessary for sparse
    representation, as well as the target feature (label).
  prefs: []
  type: TYPE_NORMAL
- en: Following, we will go through the process of preparing the dataset and uploading
    it to S3.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with the Spark dataframe that we used for training in the previous
    section (`train_df`) and apply a `Pipeline` that does the one-hot encoding as
    well as normalizing the photos-taken target feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The pipeline is similar to pipelines we've built in the previous chapters, the
    difference being that we have not included a machine learning algorithm as a final
    step (since this stage will run through SageMaker's FMs once the dataset is in
    S3). We first string index the user and attraction (point of interest) features,
    and then chain them into a one-hot encoder. The quantile discretizer will reduce
    the photos taken feature into five buckets according to their percentile. We will
    name this feature `interest_level`. Additionally, we will assemble a vector with
    these encoded attractions and user vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we transform the training dataset by applying the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/771a08f9-086d-423d-8188-f2bcca0be7b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Note how the encoded fields (`user_hash_id_encoded`, `poi_id_encoded`, and features)
    show the sparse representation of the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have this encoded dataset, we can split them into testing and training.
    SageMaker will use the training dataset for fitting and the test dataset for finding
    the validation errors at each epoch upon training. We need to convert each of
    these datasets into recordio format and upload them to s3.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were working in Scala (the native programming language used by Spark),
    we could do something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, `pyspark` does not support writing a dataframe directly into
    recordio format at the time of this writing. Instead we will collect all our spark
    dataframes in memory and convert each row to a sparse vector, and then upload
    it to S3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `spark_vector_to_sparse_matrix` function does exactly that. It
    takes a Spark dataframe row and converts it into a sparse `csr_matrix` (from `scipy`,
    a Python library with scientific utilities). The `upload_matrices_to_s3` function
    receives a Spark dataset (either training or testing), collects each row, builds
    a sparse vector with the features, and stacks them into a matrix. Additionally,
    it builds a target feature vector with all the interest levels. Given this matrix
    and label vector, we use the utility function `write_spmatrix_to_sparse_tensor`,
    of the `sagemaker` library to write the data in recordio format. Finally, we upload
    that object to S3\. To do this, let''s first import all the necessary dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s define two auxiliary functions: `spark_vector_to_sparse_matrix`,
    which will take a row and produce a `scipy` sparse matrix, and `upload_matrices_to_s3`,
    which is responsible for uploading the test or training dataset to s3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to upload the training and testing dataset by calling the `upload_matrices_to_s3`
    method on both variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the data in S3 in the right format for learning, we can start
    training our model to get recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will instantiate the SageMaker session and define the paths where to read
    and write the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'With the session, we can instantiate the SageMaker estimator by setting the
    number and type of computers to use. We also specify the hyperparameters. Two
    important parameters to consider are the feature dim (which is the length of our
    training vectors) and the predictor type. Since our problem is posed as a regression,
    we will use regressor. If instead of interest level, we had modeled it as a presence/no
    presence of interest, we would have used the `binary_classifier` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The logs will show some validation stats and a confirmation for when the model
    has completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Getting recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the model is fitted, we can launch a predictor web service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This will launch the web service endpoint that hosts the trained model and
    is now ready to receive requests with predictions. Let''s take one user from our
    recommendations made with Spark''s ALS and compare it to the predictions made
    by SageMaker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can collect the features of that user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `build_request` is a convenient function to create a JSON request compatible
    with how SageMaker expects the sparse-encoded requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As we know, the user ID position in the vector is `3297` and the attraction
    position is `4`. We can call the service to get a prediction for the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: More details about the formats of the JSON requests and responses can be found
    here: [https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we can ask the predictor for the score for an arbitrary pair of (user,
    attraction), we''ll find the scores of all 31 attractions for the user in question
    and then sort by score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Given those scores, we can find the names of the highest-ranking attractions,
    excluding those already visited:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compare this with the recommendations made by Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As the reader might notice, there are many overlapping recommendations. For
    a more thorough analysis regarding the quality of the model and its predictive
    power, we can use the evaluation methods discussed in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms,* as this problem is posed as a regression.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we studied a new type of machine learning algorithm called
    collaborative filtering. This algorithm is used in recommendation systems. We
    looked at memory-based approaches that use similarity measures to find users similar
    to a given user and discover recommendations based on the collective interests
    of the top-ranked similar users. We also studied a model-based approach called
    matrix factorization, that maps users and interests to latent factors and generate
    recommendations based on these factors. We also studied the implementations of
    various collaborative filtering approaches in Apache Spark and SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will focus on a very popular topic: deep learning.
    We will cover the theory behind this advanced field as well as a few modern applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Find an example of a recommendation system that is not described in this chapter.
    Evaluate which approach of collaborative filtering would fit that approach.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a movie-recommendation engine, explore how the issue of sparsity of data
    affects each algorithm listed in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
