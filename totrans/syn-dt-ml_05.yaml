- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Synthetic Data as a Solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter highlights the main advantages of synthetic data. You will learn
    why synthetic data is a promising solution for privacy issues. At the same time,
    you will understand how synthetic data generation approaches can be configured
    to cover rare scenarios that are extremely difficult and expensive to capture
    in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data generation methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main advantages of synthetic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthetic data as a revolutionary solution for privacy issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthetic data as a revolutionary solution for cost and time efficiency issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthetic data as a revolutionary solution for rare data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main advantages of synthetic data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen so far, synthetic data has a wide set of applications because
    of its enormous advantages. Let’s highlight some of these advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Unbiased
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diversity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data controllability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic data generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic data labeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Annotation quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low cost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 5**.1* highlights some of the key benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – The main advantages of synthetic data](img/Figure_05_01_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – The main advantages of synthetic data
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will delve into each of these advantages. We will see the limitations
    of real data and how synthetic data is a solution.
  prefs: []
  type: TYPE_NORMAL
- en: Unbiased
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Real data is curated and annotated by human annotators. In practice, it is easy
    for humans, intentionally or accidentally, to neglect or overemphasize certain
    groups in the population based on some attributes, such as ethnicity, skin color,
    gender, age, or political views. This creates a biased dataset that negatively
    affects both training and testing ML models since biased training data gives a
    corrupted representation of the studied phenomenon or processes that occur in
    the real world. As a consequence, the ML model will be biased in its decisions.
    This bias may lead to race, sex, or age discrimination, which causes tremendous
    unwanted consequences on companies’ reputations, customers, and revenue.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss one example in NLP. Researchers in the paper titled *The risk
    of racial bias in hate speech detection* ([https://aclanthology.org/P19-1163.pdf](https://aclanthology.org/P19-1163.pdf))
    demonstrated that tweets by African Americans are two times more likely to be
    flagged as offensive by automatic hate speech detection ML models. They link the
    reason to biased training data because of annotators’ bias.
  prefs: []
  type: TYPE_NORMAL
- en: Another example comes from computer vision with racial discrimination in face
    recognition ML algorithms. Face recognition ML models developed by Microsoft,
    IBM, and Amazon were shown to be less accurate at predicting certain genders and
    skin colors. These ML models were specifically identified to be less accurate
    at predicting darker female faces. Some of the largest tech companies in the world,
    such as Microsoft and IBM, proposed immediate actions to improve their data collection
    process to mitigate the bias problem in their ML models. Please refer to *Racial
    Discrimination in Face Recognition Technology* ([https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology](https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology))
    for a thorough discussion and more details.
  prefs: []
  type: TYPE_NORMAL
- en: As you might expect, synthetic data can be automatically generated and annotated.
    Thus, the error by human element factor can be removed or minimized in data generation
    and annotation processes. Therefore, fewer human errors are expected with synthetic
    data. At the same time, data can be generated so that it’s evenly distributed
    over the population. Thus, unbiased training and testing data can be generated
    to support various applications and scenarios. Additionally, in the case of any
    issues with dataset bias, synthetic data generation approaches can easily be reconfigured
    to address these problems, which is much faster compared to real data.
  prefs: []
  type: TYPE_NORMAL
- en: Diverse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of generating synthetic data can be customized to cover rare cases
    and scenarios that are not frequent, not easy to capture, or too expensive to
    annotate. Although it is possible to curate and annotate a diverse real dataset,
    it is extremely hard to achieve and requires more effort, time, and budget. For
    instance, capturing real data under adverse weather conditions is harder compared
    to normal weather. At the same time, capturing data is extremely hard during natural
    disasters and catastrophes, such as earthquakes, wildfires, hurricanes, tornados,
    and volcanoes. This limits the usability of ML models under similar scenarios.
    Therefore, **Life-Critical Systems** or **Safety-Critical Systems** (**SCSs**)
    that are based on ML models may fail or malfunction under these scenarios. This
    failure may cause death, injuries, damage to equipment, and harm to the environment.
    In addition to this, there are instances where certain events occur frequently,
    yet they are challenging to capture, such as burglary, sexual exploitation of
    children, domestic abuse, scams and fraud, street harassment, and terrorism.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data generation techniques such as statistical models or simulators
    can be designed or configured to cover all these scenarios. Thus, it makes the
    ML models more robust against similar scenarios, which saves lives and properties
    and also protects societies. Additionally, it opens the door for researchers to
    focus specifically on rare conditions, events, and scenarios. As a result, diverse
    and balanced synthetic datasets can be generated to advance research in various
    fields. At the same time, synthetic data can augment the available real datasets
    so that they become more diverse and well-balanced.
  prefs: []
  type: TYPE_NORMAL
- en: Controllable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Nature and its entangled processes and phenomena generate real data. For example,
    you can collect images of human faces to train a face recognition algorithm, but
    you will have no control over the data generation process itself. You may choose
    to consider or neglect certain attributes but you cannot perturb the process of
    how a human face may look in the real world! You can use filters or any image
    processing technique you want but still, you are only changing how you perceive
    these images. In other words, you do not make the world desaturated when you wear
    sunglasses: you only perceive the world as being darker with reduced color saturation.
    These are two completely different ideas!'
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data allows you to change the world itself as you want. For instance,
    you can create a city where all people wear the same clothes, walk the same way,
    and hold the same items! A good question you may ask here is, *why would ML researchers
    or practitioners want to do this in the* *first place?*
  prefs: []
  type: TYPE_NORMAL
- en: Being able to control the process of synthetic data generation is extremely
    important in the field of ML. It allows you to train and test your model on extremely
    rare conditions, analyze the algorithm’s robustness and accuracy, and reiterate
    the assumptions, the ML model’s design, and the training data. Synthetic data
    gives you the ability to control the parameters of the environment, the elements,
    and their interactions with each other. Consequently, there is no need to generate
    irrelevant or redundant data that does not help your ML model. Thus, with synthetic
    data, you can manage, control, and guide the data generation and annotation processes
    to achieve your objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Scalable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ML models are integral to a tremendous number of industries, including the
    automotive, healthcare, financial, and entertainment industries. The need for
    more accurate and robust ML models drives researchers to propose deeper and more
    complex ML models. These deep models are usually composed of more layers and thus
    more neurons. This means a huge number of parameters to tune in the training process.
    Consequently, ML models need more training data as these industries evolve (see
    *Figure 5**.2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – To increase profitability, companies require precise and complex
    ML models, which necessitates larger amounts of training data](img/Figure_05_02_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – To increase profitability, companies require precise and complex
    ML models, which necessitates larger amounts of training data
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the process of collecting, cleaning, and annotating real data
    is extremely slow and expensive. Companies need to respond to market changes swiftly;
    otherwise, they may lose customers, reputation, and opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Synthetic data is perfectly scalable: once the data generation and annotation
    pipelines have been configured, it is easy to generate large-scale datasets under
    new conditions as necessary. For example, you can generate an unlimited number
    of training images using **Generative Adversarial Networks** (**GANs**), **Variational
    Autoencoders** (**VAEs**), or simulators such as **CARLA** ([https://carla.org/](https://carla.org/)).'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic data labeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main advantages of synthetic data is automatic data labeling. Since
    the data generation process is controlled, automatic data labeling is possible
    with synthetic data. For example, if you utilize a game engine such as **Unreal**
    or **Unity** to generate and label synthetic data, it is possible to tag objects
    and thus to know exactly which objects are seen by your camera at a given frame.
  prefs: []
  type: TYPE_NORMAL
- en: This is the benefit of synthetic data! It saves you extensive time, effort,
    and money that you need to spend on annotating real data. Moreover, you can annotate
    private or confidential data without being worried about annotators disclosing
    sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: Annotation quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Synthetic data is automatically annotated, unlike real data. Human factor errors
    are minimized and limited. Thus, annotating high-quality and large-scale synthetic
    datasets is possible. For example, synthetic data algorithms can provide, to the
    pixel level, accurate ground truth for semantic segmentation, which is impossible
    to achieve with real data due to the limitations of human annotators. In many
    situations, there is a trade-off between quality and quantity when working with
    real data. Using synthetic data, you can achieve both objectives with less time
    and a lower budget.
  prefs: []
  type: TYPE_NORMAL
- en: Low cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Synthetic data, as we have mentioned, does not require you to capture data from
    the real world and it does not need annotators to annotate it. Synthetic data
    can be generated and automatically annotated using the appropriate algorithms.
    Thus, after developing the generation and annotation pipelines, it is possible
    to generate an unlimited number of training examples. For example, generating
    a training dataset of a thousand images and generating a training dataset of a
    million images would cost almost the same amount!
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at a particular benefit of synthetic data
    – that is, using it to solve privacy issues with sensitive data and applications.
  prefs: []
  type: TYPE_NORMAL
- en: Solving privacy issues with synthetic data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In certain fields, such as healthcare and finance, a lot of data is available,
    but the main obstacle is annotating and sharing the data. Even if we have a large-scale
    real dataset that is “perfectly” annotated, sometimes, we cannot share it with
    ML practitioners because it contains sensitive information that could be used
    by a third party to identify individuals or reveal critical information about
    businesses and organizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we know, ML models cannot work without data, so what is the solution? A
    simple solution is to use the *real* data to generate *synthetic* data that we
    can share with others without any privacy issues while still representing the
    real data. We can utilize some synthetic data generation approaches to leverage
    the real dataset to generate a synthetic dataset that still represents the relationship
    between variables, hidden patterns, and associations in the real data while not
    revealing sensitive information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Some synthetic dataset generation approaches disentangle sensitive
    information from data patterns](img/Figure_05_03_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Some synthetic dataset generation approaches disentangle sensitive
    information from data patterns
  prefs: []
  type: TYPE_NORMAL
- en: In this scope, we can understand that synthetic data generation approaches disentangle
    sensitive information from the associations and relationships between the variables
    (see *Figure 5**.3*). Thus, ML models can still be trained on the synthetic data
    and learn the hidden patterns in the original real data without being directly
    trained on it.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data does not contain information about real individuals, so no harm
    is caused. For example, let’s assume you have a synthetic dataset of human faces.
    It is fine to use this data as you want. Additionally, you would not be restricted
    by regulations as you would if you were using real human faces. This would allow
    you to explore novel creative ideas. However, if you were going to use sensitive
    information from real humans, you would be limited to the main purpose you used
    to collect the data for. Thus, you cannot investigate new ideas without permission
    from the people who participated. Additionally, data should not be kept longer
    than necessary. All of these regulations limit the usability of sensitive real
    datasets, which makes synthetic ones a perfect alternative. For more information,
    please check out *The Data Protection Act 2018*, which is the UK’s implementation
    of the **General Data Protection Regulation** (**GDPR**)([https://www.gov.uk/data-protection](https://www.gov.uk/data-protection)).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss why synthetic data is the solution for
    cost and time-efficiency issues in data generation and annotation.
  prefs: []
  type: TYPE_NORMAL
- en: Using synthetic data to solve time and efficiency issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic data generation of synthetic data removes many unnecessary elements
    in the real data curation and annotation pipeline. Collecting real data often
    requires special equipment, such as high-resolution cameras, microphones, or LiDAR.
    At the same time, you need engineers and technicians who are trained to use such
    equipment. You lose time and money training engineers and buying or renting this
    equipment. Often, data curators need to travel and visit various locations to
    collect suitable data, meaning that you would have to pay for transportation,
    accommodation, insurance, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data is an effective solution for these issues (see *Figure 5**.4*).
    In addition to the preceding issues, it is easy to conclude that synthetic data
    has a lower carbon footprint than real data. Thus, it is even better for the environment!
  prefs: []
  type: TYPE_NORMAL
- en: Data annotation is one of the main issues that makes real datasets cumbersome.
    Annotating large-scale datasets is an extremely expensive and time-consuming process.
    Annotating a large-scale dataset can take weeks, months, or even years. The huge
    amount of time it takes to fulfill the annotation process may make companies fall
    behind their competitors, causing them to lose market share and customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'It becomes even worse if you are working with real sensitive data. By law,
    it is mandatory to take extra care when it comes to storing, processing, annotating,
    or transferring this type of data. This means more budget, effort, and time to
    spend. However, using synthetic data removes all of this extra work and eases
    your workload:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Real Data | Synthetic Data |'
  prefs: []
  type: TYPE_TB
- en: '| Recording/Capturing Equipment | ↑ | ↓ |'
  prefs: []
  type: TYPE_TB
- en: '| Transportation/Accommodation | ↑ | ↓ |'
  prefs: []
  type: TYPE_TB
- en: '| Training | ↑ | ↓ |'
  prefs: []
  type: TYPE_TB
- en: '| Insurance | ↑ | ↓ |'
  prefs: []
  type: TYPE_TB
- en: '| Time | ↑ | ↓ |'
  prefs: []
  type: TYPE_TB
- en: '| Regulations | ↑ | ↓ |'
  prefs: []
  type: TYPE_TB
- en: '| ↑ You need more and ↓ you need less |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.4 – Comparison between synthetic and real data
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will understand why synthetic data can cover rare and
    special scenarios compared to real data.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data as a revolutionary solution for rare data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rare data occurs in the real world because of infrequent events or phenomena.
    In other words, these events occur but with low frequency. We can broadly classify
    these events into these categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural catastrophes**: This category includes events such as floods, asteroid
    impacts, earthquakes, and tsunamis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anthropogenic**: This category includes events such as industrial accidents,
    financial crises, and violent conflicts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These events create many major changes in the environment, which may cause state-of-the-art
    ML models to fail. For example, a face recognition system may not work well in
    the case of the evacuation of a building because as the building becomes more
    crowded, movement patterns may change. While these events are rare, their impacts
    on societies are tremendous. ML models that function inappropriately may greatly
    increase the number of deaths and injuries.
  prefs: []
  type: TYPE_NORMAL
- en: 'For ML models to be robust and accurate, these models need to be trained and
    tested on both standard and rare conditions. Capturing real data of rare events
    is extremely hard and expensive. Most ML models assume that they will work under
    standard conditions and scenarios. Unfortunately, these ML models generally fail
    or struggle under any scenarios that deviate from the standard ones. For instance,
    in *Semantic Segmentation under Adverse Conditions: A Weather and Nighttime-aware
    Synthetic Data-based Approach* ([https://bmvc2022.mpi-inf.mpg.de/0977.pdf](https://bmvc2022.mpi-inf.mpg.de/0977.pdf)),
    researchers demonstrated that state-of-the-art semantic segmentation methods perform
    well under standard conditions, such as normal weather conditions and sufficient
    illumination. However, these methods struggle or fail under adverse conditions,
    such as foggy, rainy, and snowy weather conditions or at nighttime.'
  prefs: []
  type: TYPE_NORMAL
- en: The key reason for neglecting rare scenarios is the fact that collecting training
    data under these circumstances may take a long time, a lot of training and effort
    is required to capture these rare events, and it can be a dangerous process. Finally,
    we should note that rare data is not just useful for training purposes – it is
    essential for understanding the limitations of ML models in practice.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, real data is not balanced in the real world. Thus,
    the data that we collect from the real world will reflect this imbalance. Unfortunately,
    ML models are sensitive to imbalanced datasets. Thus, imbalanced training datasets
    cause ML models to develop a corrupted understanding of the problem. For example,
    if we train a cats-dogs classifier on a dataset that includes 30 cats and 70 dogs,
    the model will tend to predict dogs twice as often as it predicts cats. Thus,
    balanced training datasets make the models train better and converge faster.
  prefs: []
  type: TYPE_NORMAL
- en: Standard conditions, events, and attributes are more likely to occur in the
    real world than rare events. Thus, you are more likely to have a dataset that
    specifically focuses on normal conditions and neglects rare ones.
  prefs: []
  type: TYPE_NORMAL
- en: As you might expect, synthetic data can be used to simulate these rare events.
    Thus, generating a perfectly balanced large-scale dataset is easy to achieve with
    synthetic data. Synthetic data generation methods may be used to generate a full
    training dataset from scratch. At the same time, synthetic data can be utilized
    to complement real datasets. Thus, you can make your ML models more robust against
    rare events and conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data generation methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are different methods to generate synthetic data: some of them are based
    on statistical models and others rely on game engines and simulators. *Statistical
    models* are non-deterministic mathematical models that include variables represented
    as probability distributions. Based on the problem, these models are usually trained
    using real data to understand the hidden patterns and correlations in the data.
    Then, the trained ML model can be used to generate new samples automatically,
    such as images, text, tables, and more. These new samples can be utilized by other
    ML models for training or testing purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data can also be generated using *game engines and simulators*. These
    tools are utilized to create 3D virtual worlds. These 3D worlds can be generated
    using **Procedural Content Generation** (**PCG**) techniques to control scene
    attributes, the interaction between scene elements, and the diversity and quality
    of the generated data.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the main challenge for most synthetic data generation
    approaches is building a data generation and annotation pipeline, which requires
    careful design and engineering. However, once the pipeline is ready, it is usually
    simple to use and can be utilized for an enormous range of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the main advantages of using synthetic data.
    We discussed that synthetic data is easy to generate, manage, and annotate. When
    it comes to privacy issues that we have with sensitive real data, utilizing synthetic
    data is an ideal solution.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to utilize simulators and rendering engines
    to generate synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: Part 3:Synthetic Data Generation Approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will be introduced to the main synthetic data generation approaches.
    You will learn how to leverage simulators and rendering engines, **Generative
    Adversarial Networks** (**GANs**), video games, and diffusion models to generate
    synthetic data. You will explore the potential of these approaches in ML. Moreover,
    you will understand the challenges and pros and cons of each method. This part
    will be supported with hands-on practical examples to learn how to generate and
    utilize synthetic data in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B18494_06.xhtml#_idTextAnchor099), *Leveraging Simulators and
    Rendering Engines to Generate Synthetic Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B18494_07.xhtml#_idTextAnchor120), *Exploring Generative Adversarial
    Networks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B18494_08.xhtml#_idTextAnchor138), *Video Games as a Source of
    Synthetic Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18494_09.xhtml#_idTextAnchor154), *Exploring Diffusion Models
    for Synthetic Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
