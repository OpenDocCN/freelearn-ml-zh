- en: Chapter 4. Traffic Stops – Barking Up the Wrong Tree?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the prior two chapters, you were a software developer who was injecting machine
    learning into their existing line of business application. In this chapter, we
    are going to put on our research analyst hat and see if we can discover some hidden
    insights from an existing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The scientific process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The research analyst historically followed this pattern of discovery and analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The scientific process](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'With the rise of the data scientist, that workflow has changed to something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The scientific process](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Notice how the work does not end after reporting the results of a model. Rather,
    the data scientist is often responsible for moving the working models from their
    desktop and into a production application. With this new responsibility, comes
    new power, to reverse-paraphrase Spider-Man. The data scientist's skillset becomes
    broader because they have to understand software engineering techniques to go
    along with their traditional skill set.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing that the data scientist knows by heart is this following workflow.
    Inside the Test With Experiment block, there is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The scientific process](img/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In terms of time spent, the **clean data** block is very large compared to the
    other blocks. This is because most of the work effort is spent with data acquisition
    and preparation. Historically, much of this **data munging** was dealing with
    missing, malformed, and illogical data. The traditional way to try to minimize
    the work effort on this step was to create data warehouses and clean the data
    as it transferred from the source system into the warehouse (sometimes called
    the Extract, Transform, Load, or ETL process). While this had some limited success,
    it was a fairly expensive endeavor and a fixed schema meant that changes became
    particularly difficult. More recent efforts in this space have surrounded gathering
    data in its native format, dumping it into **data lakes** and then building jobs
    on top of the lake that are specific to the data's native format and structure.
    Sometimes, this is called *putting the data in a rectangle* because you may be
    taking unstructured data, cleaning and aggregating it, and then outputting it
    in a two-dimensional data frame. The power of this data frame is that you can
    then combine it with other data frames to do some more interesting analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Open data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most exciting civic movements that align with Big Data and Machine
    Learning is **Open Data**. Although there is not as much buzz around it, it's
    a very exciting and important transformation in data science. The premise of open
    data is that local, state, and national governments will become much more accountable
    and efficient if they expose the public data that they currently maintain in a
    RESTful format. Currently, most government agencies might have paper records,
    charge a significant sum to output an ad-hoc query, or occasionally have an FTP
    site with some `.xls` or `.pdf` files that get refreshed from time to time. Open
    Data is a movement that takes the same, if not more, data and places it on a web
    service that can be consumed by applications and/or research analysts. The key
    thing is security and privacy of the data. Historically, some government agencies
    have practised security by obscurity (we have the records online but the only
    way to get to it is by our custom web frontend) and open data makes that kind
    of defense obsolete. Truth be told, security by obscurity has never really worked
    (how hard is it to write a screen scraper?) and all it has really done is made
    it harder for well-intentioned people to accomplish their goals.
  prefs: []
  type: TYPE_NORMAL
- en: The rise of open data also coincides with the formulation of groups of people
    who are hacking for civic good. Sometimes these are ad hoc meetup groups that
    center on a single technology stack and other groups are much more formal. For
    example, Code for America has *brigades* in many cities across the world. If you
    are interested in helping a local chapter, you can find information on their website
    [http://www.codeforamerica.org/](http://www.codeforamerica.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Hack-4-Good
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's pretend we are a member of a local chapter of a fictional organization
    civic hacking called "Hack-4-Good". At the latest meeting, the leader announces,
    "Through a public record request, we have obtained all of the traffic stop information
    in our town. Does anyone know what to do with this dataset?" You immediately throw
    your hand in the air and say, "Heck yeah, Baby!" Okay, maybe you don't use those
    exact words but your enthusiasm is undeniable.
  prefs: []
  type: TYPE_NORMAL
- en: Since you are a research analyst by training, the first thing you want to do
    is load the data into your IDE and start exploring the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up Visual Studio and create a new Solution called `Hack4Good.TrafficStop.Solution`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hack-4-Good](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Add a new F# Library project to the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hack-4-Good](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: FsLab and type providers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that the project skeleton has been set up, open up the `Script.fsx` file
    and remove all of its contents. Next, let''s take a look at a really neat library
    called FsLab ([http://fslab.org/](http://fslab.org/)). Go to the NuGet package
    manager console and enter this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will install SqlClient, so we can access our data. Go to the NuGet
    package manager and enter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With the ceremony out of the way, let''s get to coding. Let''s first bring
    the traffic ticket dataset into our script. Go into `Script.fsx` and enter this
    at the top:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get a series of dialog boxes from Visual Studio that looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![FsLab and type providers](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Click on **Enable**. As a general point, whenever you get a dialog box from
    Visual Studio like this, click on **Enable**. For example, depending on our machine's
    configuration, you might get these dialog boxes when you run the following `open`
    statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back in our script, enter this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, enter this into the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The first line should look familiar; it's a connection string just like we used
    in the last chapter. The only difference is the database. But what's with the
    next line of code?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is an example of a type provider. Type providers are one of the best features
    of F# and it is unique to the language. I like to think of type providers as **object
    relational mapping** (**ORM**) on steroids. This type provider is inspecting the
    database and generating F# types for me to use in the REPL—which we will see in
    action in a second. In fact, the type provider is sitting on top of **Entity Framework**
    (**EF**), which is sitting on top of ADO.NET. If you got excited about how much
    more productive you were when you went from hand-rolling ADO.NET code to EF, you
    should be equally excited about how much more productive you can be working with
    type providers; it is really the next generation of data access. Another cool
    thing is that type providers are not just for relational database management systems—there
    is a JSON type provider, a `.csv` type provider, and others. You can read more
    about type providers at [https://msdn.microsoft.com/en-us/library/hh156509.aspx](https://msdn.microsoft.com/en-us/library/hh156509.aspx)
    and once you see them in action, you will find them indispensable in your coding
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back to the code. The next line is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It creates the actual instance of the type to be used. The next line is where
    the rubber meets the road:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this line, we are traversing the `TrafficStop` table and printing out the
    street address. If you highlight all of the code in the script so far and send
    it to the REPL, you will see the last part of 30,000 addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Before we press on, I want to mention how cool type providers are. With three
    lines of code, I defined a database schema, connected to it, and then pulled down
    records. Not only that, but the result set from the database is `IEnumerable`.
    So everything I have done with `Seq` in prior chapters to transform and shape
    the data, I can do here.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With this new found power, let''s start exploring. Enter this into the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending to the REPL, we will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We see our data frame has some interesting elements we can analyze: the date
    and time of the traffic stop, the geocoordinate of the traffic stop, and the final
    disposition of the stop. We also have some data that does not seem useful for
    analysis: the `CadCallId` which is probably the primary key of the source system.
    This might be useful for later auditing. We also have `StreetAddress`, which is
    the same as the geocoordinate, but in a less analyzable form. Finally, we have
    some fields thrown in by Entity Framework (`EntityKey`, `EntityState`, and `Id`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make a data frame with only the fields we care about. Enter this into
    the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And sending it to the REPL, we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: It is interesting that although F# really, really tries to prevent you from
    using null, it does support it. In fact, all four of our fields are nullable.
    I'll show you how to deal with nulls a bit further on in the chapter as they are
    often a major headache when coding.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one more data frame we should create before getting too far down the
    analysis. As a general rule, the machine learning models that we use, prefer primitive
    types such as ints, floats, and bools. They have a much harder time with strings,
    especially strings that represent categorical data. You probably noticed that
    I brought in `DispositionId` into the `trafficStops` data frame and not `DispositionDesc`.
    However, we still don''t want to lose that description because we might want to
    refer to it later. Let''s create a separate data frame for this lookup data. In
    the script, enter this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And then send it to the REPL to get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the code, we have a couple of new things. First, we are using the
    high order function `Seq.distinctBy`, which you can probably guess return records
    with the distinct values specified in the argument. Interestingly, the entire
    traffic stop record is being returned, not just the values in the lambda. If you
    are wondering which record gets picked by F# to represent the distinct disposition,
    you have to chalk it up to magic. Okay, maybe not. As it was traversing the data
    frame, F# picked the first record where there was a new unique value for `DispositionID`
    and `DispositionDesc`. In any event, since we only care about the `DispositionId`
    and `DispositionDesc`, we then mapped the traffic stop record into a tuple on
    this line of code: `Seq.map (fun d -> d.DispositionId, d.DispositionDesc`. That
    should look familiar to you by now.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With our data frames set up, let''s start digging into the data. One of the
    nice things about having a `DateTime` value is that it represents many different
    factors that might be worth exploring. For example, how many traffic stops are
    performed by month? What about the day of the week? Is there a time factor in
    the stops? Does more happen at night or in the day? Let''s start writing some
    code. Go to the script and enter this code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And sending it to the REPL, you should see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Just a quick glance tells you that there are a whole lot of traffic stops being
    performed in September, and December looks like a light month. Digging into the
    code, there is a new high-order function that I used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`groupBy` is a very powerful function, but it can be a bit confusing the first
    time you use it (it was for me, at least). I came to a better understanding of
    `groupBy` by working backwards and looking at the output of a simple array. Go
    into the script file and enter this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending that to the REPL gives this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice that the output is a tuple. The first item of the tuple is
    the value on which the `groupBy` grouped the data. The next item is a subarray
    with only the values from the original array that match the tuple''s first item.
    Diving into the ones, `(1, [|1; 1; 1|])`, we can see that the number 1 was the
    `groupBy` value and that there were three 1s in the original array. `groupBy`
    can be applied to record types too. Consider this data frame. From left to right,
    the columns are `USState`, `Gender`, `YearOfBirth`, `NameGiven`, and `NumberOfInstances`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| USState | Gender | YearOfBirth | NameGiven | NumberOfInstances |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AK | F | 1910 | Annie | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| AK | F | 1910 | Anna | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| AK | F | 1910 | Margaret | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| AL | F | 1910 | Annie | 90 |'
  prefs: []
  type: TYPE_TB
- en: '| AL | F | 1910 | Anna | 88 |'
  prefs: []
  type: TYPE_TB
- en: '| AL | F | 1910 | Margaret | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| AZ | F | 1910 | Annie | 46 |'
  prefs: []
  type: TYPE_TB
- en: '| AZ | F | 1910 | Anna | 34 |'
  prefs: []
  type: TYPE_TB
- en: '| AZ | F | 1910 | Margaret | 12 |'
  prefs: []
  type: TYPE_TB
- en: 'Applying a `groupBy` on `NameGiven` to this data frame gives the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '| fst | snd |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Annie | AK | F | 1910 | Annie | 12 |'
  prefs: []
  type: TYPE_TB
- en: '|   | AL | F | 1910 | Annie | 90 |'
  prefs: []
  type: TYPE_TB
- en: '|   | AZ | F | 1910 | Annie | 46 |'
  prefs: []
  type: TYPE_TB
- en: '| Anna | AK | F | 1910 | Anna | 10 |'
  prefs: []
  type: TYPE_TB
- en: '|   | AL | F | 1910 | Anna | 88 |'
  prefs: []
  type: TYPE_TB
- en: '|   | AZ | F | 1910 | Anna | 34 |'
  prefs: []
  type: TYPE_TB
- en: '| Margaret | AK | F | 1910 | Margaret | 8 |'
  prefs: []
  type: TYPE_TB
- en: '|   | AL | F | 1910 | Margaret | 86 |'
  prefs: []
  type: TYPE_TB
- en: '|   | AZ | F | 1910 | Margaret | 12 |'
  prefs: []
  type: TYPE_TB
- en: With the `fst` of the tuple the `NameGiven`, and the `snd` being a data frame
    with only the records that match the `fst`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's continue with the next line of code `|> Seq.map (fun (m, ts) -> m, ts
    |> Seq.length)`.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that we are mapping the original tuple of month and `trafficStops`
    to a new tuple of month and the length of the array that was `snd` of the original
    tuple. This effectively reduces our data into a sequence of length 12 (one for
    each month). The `fst` is the month and the `snd` is the number of stops that
    occurred. Next we sort it by month and then push it to an array.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this pattern set, let''s do a couple of more `groupBy`. Let''s do `Day`
    and `DayOfWeek`. Go into the script and enter this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You will notice one subtle change from the month analysis that we just did—`|>
    Seq.map (fun (dow, ts) -> dow, Seq.length ts)` has a different syntax for getting
    the length of the `snd`. Instead of writing `ts |> Seq.length`, I wrote `Seq.length
    ts`. Both styles are perfectly valid F#, but the latter is considered more idiomatic.
    I will begin using this style more frequently in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'So once we send this to the REPL, we can see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the results, it should be pretty obvious what we are doing. The 25th
    of every month looks like the day where most of the traffic stops occur and Thursday
    sure has a lot of stops. I wonder what would happen if the 25th fell on a Thursday
    for a given month?
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive deeper into the data, I want to point out that the last three
    blocks of code are very similar. They all follow this pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of having three chunks of code that are almost identical, is there
    a way we can consolidate them into a single function? Yes there is. What if we
    wrote a function like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we called it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Would that work? You bet your bippy. Sending it to the REPL, we can see we
    are getting the same results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Those of you coming from C# and VB.NET might have gotten very uncomfortable
    with the transform''s interface. You probably would have been much more comfortable
    with this syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `()` and the commas make it look much more like C# and VB.NET. Although
    both are perfectly valid F#, this is another place where it is considered more
    idiomatic to remove the parenthesis and the commas. I will begin using this style
    more frequently in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Also, notice that I am passing two functions into the transform function. This
    is very different from imperative C#/VB.NET where we usually pass data into a
    method. I have noticed that functional programming is more about bringing the
    operations to the data than bringing the data to the operations, which has profound
    implications once we start applying machine learning to big data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to our transform function, we can see that the `mapper` function
    is pretty the same in the three times we invoked it: `(fun (dow, ts) -> dow, Seq.length
    ts)`. The only difference is the name we gave the first part of the tuple. This
    seems like another great place where we can consolidate some code. Let''s rewrite
    transform like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'And sending that to the REPL, we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Pretty cool, huh? We will be doing this kind of programming more and more in
    this book. Once you get the hang of it, you will start seeing patterns in your
    code that you never had seen before, and you have yet another powerful arrow in
    your toolbox (as I mix metaphors).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since you are now up to speed on `groupBy`, I want to rewrite our transform
    function to ditch it. Instead of using the `groupBy` and `map` functions, let''s
    rewrite it using the `countBy` high order function. While we are at it, let''s
    rename our function to something that is a bit more intention revealing. Type
    this into the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the REPL, we get the same values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Looking at the data in the REPL is a good start, but pictures are a much more
    powerful and effective way to communicate information. For example, is there some
    kind of monthly seasonality for traffic stops? Let''s put the data into a chart
    to find out. In your script, enter this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Your REPL should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Your default browser should be trying to open and should show you this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So we see the spike in September and the drop off in December that had already
    caught our eye. If the date/time has some curious patterns, what about the geolocation?
    Enter this into the script file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![Visualization](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Not very helpful. The problem is that the FsLab geomap covers Google''s `geoMap`
    API, and that API only goes to the country level. Instead of using Fslab then,
    we can roll our own. This is a fairly complex process using Bing maps, WPF dependency
    properties, and the like, so I will not explain it in this book. The code is available
    for you to review on the download section of our site. So, close your eyes and
    pretend the last 3 hours I spent working on this went by in 2 seconds and we have
    this map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So what can we tell off the bat? There are traffic stops everywhere, though
    they seem to concentrate on main streets. Based on the initial analysis, the term
    "speed trap" might be less about location and more about month, day, and time.
    Also, we can't draw too much from this map because we don't know the traffic patterns—more
    stops might be on more traveled streets or might be an indicator of key areas
    where there are traffic stops. To help us dig into the data more, let's move away
    from simple descriptive statistics and apply a common machine learning technique
    called a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The principle of decision tree is this: you can use a tree-like structure to
    make predictions. Here is a cantonal example of whether we will play tennis today:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision trees](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Each decision is called a node and the final result (in our case, the **Yes**/**No**
    boxes) is called the leaf. The analogy to a tree is somewhat appropriate. In fact,
    I would call it a decision branch with each decision called a crook and the final
    results being called leaves. However, J.R. Quinlan didn't ask me in 1986 when
    he invented this methodology. In any event, the number of levels of the tree is
    called the height of the tree. In our prior example, the tree has a maximum height
    of two. The nodes that are possible for a given point is called the features.
    In our prior example, Outlook has three features (sunny, overcast, and rain) and
    Strong Wind has two features (yes and no).
  prefs: []
  type: TYPE_NORMAL
- en: One of the real benefits of the decision tree is its simplicity of conveying
    information. Humans often do mental decision trees as long as the number of nodes
    are small and the calculation to move to the next node is simple. (Should I order
    decaf or regular? Do I need to study tonight?) Computers come in handy when there
    are many nodes and the calculations are complex. For example, we can hand the
    computer a whole bunch of historical data from people who decide to play tennis
    and it can determine that, for sunny days, the actual decision point is not 30°,
    but 31.2°. Something to keep in mind though is that decision trees often become
    less meaningful as the number of features increase and the depth gets too large.
    We'll look at ways to handle this a little bit later.
  prefs: []
  type: TYPE_NORMAL
- en: Accord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s make a decision tree with our traffic stop data. Go back to Visual Studio,
    open up **Solution Explorer**, and add a new script called `Accord.fsx`. Enter
    this into the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is the same that you used in `Script.fsx` . Send it to the REPL to
    make sure you copy-pasted it correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, open up the NuGet package manager and enter in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Go back to the script and enter in this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: With that out of the way, let's create a data structure that we can pass to
    Accord. As I mentioned earlier, decision trees often have a problem with a large
    number of features. A common technique to mitigate this is to bin the data. When
    you bin the data, you take your original data and put them into large groups.
    For example, we can take all of the times for the traffic stops and bin them into
    AM or PM, depending on whether they occurred before or after noon. Binning is
    a commonly used technique in data science—sometimes justifiably and sometimes
    just as a way to make the model conform to a desired output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to our script, create the following record type for our decision
    tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: You will see that I created two bins of data. The first is called `AMPM` and
    it is for the time of the stop. The second is called `ReceviedTicket` as a Boolean.
    If you remember, there are 18 different values for the disposition. We only care
    whether the person received a ticket (called citation), so we are binning citations
    to true and noncitations to false. There is one more thing that you probably noticed—`ReceivedTicket`
    isn't simply a bool, it is a bool option. As you might remember, F# really doesn't
    like nulls. Although it can support null, F# instead encourages you to use something
    called an option type in its place.
  prefs: []
  type: TYPE_NORMAL
- en: 'An option type can have two values: `Some<T>` or `None`. If you are not familiar
    with the syntax of `Some<T>`, it means that `Some` is limited to only one type.
    Therefore, you can write `Some<bool>`, `Some<int>`, or `Some<string>`. With an
    option type, you can verify if a field has a value that you care about: `Some`
    or `None`. Not only that, the compiler forces you to be explicit with the choice.
    This compiler checking forces you to be explicit with the value and is an extremely
    powerful construct. Indeed, it is one of the reasons that F# code often has fewer
    bugs than other languages because it forces the developer to confront problems
    sooner and prevents them from sweeping them under the rug into a null where they
    can be accidentally ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to our code, let''s write two functions that will bin our original
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending that to the REPL, we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that `ReceivedTicket` returns three possibilities with the option type:
    `Some true`, `Some false`, and `None`. The reason I did not include the other
    disposition values into `Some false` versus `None` was because we are only concentrating
    on traffic violations, not all of the reasons why an officer might stop a car.
    This kind of filtering is often used in data science to help make the dataset
    align with what we are trying to prove. We are not getting into more detail here
    about filtering as there are entire books written about the best way to deal with
    outliers and non-conforming data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Back to our code. Let''s take the data out of the database and put it into
    our `TrafficStop` record type. Go into the script and enter this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the REPL, we see the last bit of all of our records in the
    data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'With the data shaped somewhat, let''s get it ready for Accord. As I mentioned
    earlier, Accord wants the input data for the decision tree to be in a `int[][]`
    and the output to be in a `int[]`. However, it also needs the inputs to be tagged
    to make the model work. We achieve that by passing in an array of attributes.
    Back in the script file, add this code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the REPL, we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Some of the sharp-eyed people reading this book might have noticed that the
    month decision variable has a range of 13 and not 12\. This is because the values
    for month are 1-12 and Accord needs 13 to account for the possibility of any feature
    value up to 13 (like 12.99—we know that won't exist but Accord does not). Day
    of week is 0 to 6, so it gets a `7`.
  prefs: []
  type: TYPE_NORMAL
- en: 'So going back to our script, add in the following blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the REPL, we get the end of our data frame being converted
    into `int` arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'With everything setup, let''s go ahead and run our tree. Enter this to the
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the REPL gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Just like all the other models we have seen so far, we need both an output from
    the model and some information about how good our model is at predicting based
    on the data that we provided. In this case, the model is off by 28%, which is
    pretty high for a decision tree. With the model created, we can now ask the tree
    to predict if we will get a ticket or a warning on a Saturday in October in the
    evening.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter this script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending it to the REPL, we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: It looks like we will get a warning and not a ticket.
  prefs: []
  type: TYPE_NORMAL
- en: 'As I mentioned, 28% is high for a decision tree. Is there a way to get that
    number down? Perhaps binning will help. Go back to the REPL and type in this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the REPL, we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Perhaps we can bin the months of the year in quarters? Let''s create a function
    that does that. Go into the script file and enter this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the REPL, we see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: This did not improve our model. Perhaps we can keep working with the data, or
    perhaps there is not a correlation between a ticket/warning based on the data
    that we have. Walking away from a model is often one of the hardest things you
    have to do in data science, especially if you have spent a considerable amount
    of time on it, but it is often the right thing to do.
  prefs: []
  type: TYPE_NORMAL
- en: numl
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we leave decision trees, I want to look at another way of calculating
    them. Instead of using Accord.Net, I want to introduce another .Net machine learning
    library called **numl**. numl is a new kid on the block and can offer a lower
    barrier entry to machine learning. Although not as expansive as Accord, it does
    offer many common models, including a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to **Solution Explorer** and add another script called `numl.fsx`. Then
    go into the NuGet package manager and pull down numl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Go back to the numl script and enter in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: This is the same code as the `Accord.fsx` script, so you can copy and paste
    it from there. Send it to the REPL to make sure you copy-pasted it correctly.
    Next, add in this block to reference numl.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, enter this block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending that to the REPL, returns this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: There are two things to notice here. First, just like Accord, numl wants the
    input to its modeling engine to be in a certain format. In this case, it is not
    arrays of ints. Rather, it wants object types (as of the time of writing). In
    order to know what to make of each object, it needs to have attributes associated
    with each element, hence the `TrafficStop'` type that has either `[Feature]` or
    `[Label]` added. As you can guess, features are for input and labels are for outputs.
    The second thing to notice is that we call `|> Seq.map box`. This converts our
    types such as int, string, and bool to object, which is what numl wants.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that out of the way, we can see what numl comes up with. Enter this into
    the script window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending that to the REPL, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: One of the nice things about numl is that the `ToString()` overload prints out
    a graphical representation of our tree. This is a great way to quickly visually
    inspect to see what we have. You can also see that the accuracy of the model is
    pretty much the same as Accord. If you run this script several times, you will
    get slightly different answers because of the way numl splits the data. Taking
    another look at the tree, let's see if we can interpret it in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The modeling engine found that the best feature to start splitting on was AM/PM.
    If the traffic stop was in the afternoon, you will get a warning and not a ticket.
    If it was in the morning, we moved down to the next decision point on the tree.
    What we can see is that, if the traffic stop occurred in the AM between July and
    December, we would not get a ticket. If the AM traffic stop was between Jan and
    June, we would have to go to the next level, which is the day of the week. In
    this case, the model split between Sunday-Tuesday and Wednesday-Saturday. You
    will notice that both terminal nodes are false too. Where is the true? Can the
    model predict that I will get a ticket? No, this model cannot reasonably predict
    when you will get a ticket. As before, we will have to leave this model behind.
    However, this exercise was not a waste because we will use this data with some
    more data and a different model to create something that has some practical value.
  prefs: []
  type: TYPE_NORMAL
- en: One last question before we leave this chapter, "What machine learning is going
    on here?" We can say that numl is using machine learning because it is doing several
    iterations with the data. But what does that mean? If you look at the last line
    of code we wrote, `let model = Learner.Learn(dataFrame', 0.80, 25, generator)`,
    you can see that the third argument is 25\. This is the number of times the model
    is run and then numl picks the best model. In effect, then, the machine is "learning"
    but evaluating several possible models and selecting one for us. I don't really
    consider this machine learning because we are not introducing new data to the
    learning to make it smarter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at using testing and training sets to accomplish
    some of that, but we still have the problem that this is a point-in-time analysis.
    How would you make this model self-teaching? Point of fact, I would not bother
    with the model in its current state because the model has been demonstrated to
    be useless. However, if the model was useful, I can imagine a scenario where we
    constantly update the dataset and run the model based on more datasets that our
    open data friends can get their hands on. With that, we can have an application
    running that might remind drivers before they leave the house in the morning that
    based on the date/time/weather/other factors, they should be taking it easier
    than normal. Perhaps a simple text or tweet to the driver? In any event, once
    we have a real model, we can see an application like this in action.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we put on our data scientist hat and took a look at using F#
    to do data exploration and analysis. We were exposed to open data and the awesomeness
    of type providers. We then implemented a decision tree, though ultimately we concluded
    that the data did not show a significant relationship.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will address some of the issues that we have been glossing
    over so far and take a deep dive into obtaining, cleaning, and organizing our
    data.
  prefs: []
  type: TYPE_NORMAL
