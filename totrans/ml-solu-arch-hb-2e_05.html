<html><head></head><body>
<div class="Basic-Text-Frame" id="_idContainer091">
<h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 class="chapterTitle" id="_idParaDest-146"><span class="koboSpan" id="kobo.2.1">Exploring Open-Source ML Libraries</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">There is a wide range of </span><strong class="keyWord"><span class="koboSpan" id="kobo.4.1">machine learning</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.6.1">ML</span></strong><span class="koboSpan" id="kobo.7.1">) and data science technologies available, encompassing both open-source and commercial products. </span><span class="koboSpan" id="kobo.7.2">Different organizations have adopted different approaches when it comes to building their ML platforms. </span><span class="koboSpan" id="kobo.7.3">Some have opted for in-house teams that leverage open-source technology stacks, allowing for greater flexibility and customization. </span><span class="koboSpan" id="kobo.7.4">Others have chosen commercial products to focus on addressing specific business and data challenges. </span><span class="koboSpan" id="kobo.7.5">Additionally, some organizations have adopted a hybrid architecture, combining open-source and commercial tools to harness the benefits of both. </span><span class="koboSpan" id="kobo.7.6">As a practitioner in ML solutions architecture, it is crucial to be knowledgeable about the available open-source ML technologies and their applications in building robust ML solutions.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.8.1">In the upcoming chapters, our focus will be on exploring different open-source technologies for experimentation, model building, and the development of ML platforms. </span><span class="koboSpan" id="kobo.8.2">In this chapter specifically, we will delve into popular ML libraries including scikit-learn, Spark, TensorFlow, and PyTorch. </span><span class="koboSpan" id="kobo.8.3">We will examine the core capabilities of these libraries and demonstrate how they can be effectively utilized throughout the various stages of an ML project lifecycle, encompassing tasks such as data processing, model development, and model evaluation. </span><span class="koboSpan" id="kobo.8.4">Moreover, you will have the opportunity to engage in hands-on exercises, gaining practical experience with these ML libraries and their application in training models.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.9.1">Specifically, we will be covering the following main topics:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.10.1">Core features of open-source ML libraries</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.11.1">Understanding the scikit-learn ML library</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.12.1">Understanding the Apache Spark ML library</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.13.1">Understanding the TensorFlow ML library and hands-on lab</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.14.1">Understanding the PyTorch ML library and hands-on lab</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.15.1">How to choose between TensorFlow and PyTorch</span></li>
</ul>
<h1 class="heading-1" id="_idParaDest-147"><span class="koboSpan" id="kobo.16.1">Technical requirements</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.17.1">In this chapter, you will need access to your local machine where you installed the </span><strong class="keyWord"><span class="koboSpan" id="kobo.18.1">Jupyter</span></strong><span class="koboSpan" id="kobo.19.1"> environment from </span><em class="chapterRef"><span class="koboSpan" id="kobo.20.1">Chapter 3</span></em><span class="koboSpan" id="kobo.21.1">, </span><em class="italic"><span class="koboSpan" id="kobo.22.1">Exploring ML Algorithms</span></em><span class="koboSpan" id="kobo.23.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.24.1">You can find the code samples used in this chapter at </span><a href="https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter05"><span class="url"><span class="koboSpan" id="kobo.25.1">https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter05</span></span></a><span class="koboSpan" id="kobo.26.1">.</span></p>
<h1 class="heading-1" id="_idParaDest-148"><span class="koboSpan" id="kobo.27.1">Core features of open-source ML libraries</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.28.1">ML libraries </span><a id="_idIndexMarker443"/><span class="koboSpan" id="kobo.29.1">are software libraries designed to facilitate the implementation of ML algorithms and techniques. </span><span class="koboSpan" id="kobo.29.2">While they share similarities with other software libraries, what sets them apart is their specialized support for various ML functionalities. </span><span class="koboSpan" id="kobo.29.3">These libraries typically offer a range of features through</span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.30.1"> different sub-packages, including:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.31.1">Data manipulation and processing</span></strong><span class="koboSpan" id="kobo.32.1">: This includes support for different data tasks such as loading data of different formats, data manipulation, data analysis, data visualization, data transformation, and feature extraction.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.33.1">Model building and training</span></strong><span class="koboSpan" id="kobo.34.1">: This includes support for built-in ML algorithms as well as capabilities for building custom algorithms for a wide range of ML tasks. </span><span class="koboSpan" id="kobo.34.2">Most ML libraries also have built-in support for the commonly used loss functions (such as mean squared error or cross-entropy) and a list of optimizers (such as gradient descent, Adam, etc.) to choose from. </span><span class="koboSpan" id="kobo.34.3">Some libraries also provide advanced support for distributed model training across multiple CPU/GPU devices or compute nodes.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.35.1">Model evaluation and validation</span></strong><span class="koboSpan" id="kobo.36.1">: This includes packages for evaluating the performance of trained models, such as model accuracy, precision, recall, or error rates.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.37.1">Model saving and loading</span></strong><span class="koboSpan" id="kobo.38.1">: This includes support for saving the models to various formats for persistence and support for loading saved models into memory for predictions.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.39.1">Model serving</span></strong><span class="koboSpan" id="kobo.40.1">: This includes model serving features to expose trained ML models behind an API, usually a RESTful API web service.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.41.1">Interpretation</span></strong><span class="koboSpan" id="kobo.42.1">: This includes functionality for interpreting model predictions and feature importance.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.43.1">ML libraries </span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.44.1">typically offer support for multiple programming languages, including popular options such as Python, Java, and Scala, catering to diverse user requirements. </span><span class="koboSpan" id="kobo.44.2">Python, in particular, has emerged as a prominent language in the field of ML, and many libraries provide extensive support for its interface. </span><span class="koboSpan" id="kobo.44.3">While the user-facing interface is often implemented in Python, the backend and underlying algorithms of these libraries are primarily written in compiled languages like C++ and Cython. </span><span class="koboSpan" id="kobo.44.4">This combination allows for efficient and optimized performance during model training and inference. </span><span class="koboSpan" id="kobo.44.5">In the following sections, we will delve into some widely used ML libraries to gain a deeper understanding of their features and capabilities, starting with scikit-learn, a widely used ML library for building ML models.</span></p>
<h1 class="heading-1" id="_idParaDest-149"><span class="koboSpan" id="kobo.45.1">Understanding the scikit-learn ML library</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.46.1">scikit-learn (</span><a href="https://scikit-learn.org/"><span class="url"><span class="koboSpan" id="kobo.47.1">https://scikit-learn.org/</span></span></a><span class="koboSpan" id="kobo.48.1">) is an</span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.49.1"> open-source ML library</span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.50.1"> for Python. </span><span class="koboSpan" id="kobo.50.2">Initially released in 2007, it is one of the most popular ML libraries for solving many ML tasks, such as classification, regression, clustering, and dimensionality reduction. </span><span class="koboSpan" id="kobo.50.3">scikit-learn is widely used by companies in different industries and academics for solving real-world business cases such as churn prediction, customer segmentation, recommendations, and fraud detection.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.51.1">scikit-learn is built mainly on top of three foundational libraries: </span><strong class="keyWord"><span class="koboSpan" id="kobo.52.1">NumPy</span></strong><span class="koboSpan" id="kobo.53.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.54.1">SciPy</span></strong><span class="koboSpan" id="kobo.55.1">, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.56.1">Matplotlib</span></strong><span class="koboSpan" id="kobo.57.1">: </span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.58.1">NumPy is</span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.59.1"> a Python-based library for managing large, multidimensional arrays and matrices, with additional mathematical functions to operate on the arrays and matrices.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.60.1">SciPy provides </span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.61.1">scientific computing functionality, such as optimization, linear algebra, and Fourier transform.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.62.1">Matplotlib</span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.63.1"> is used for plotting data for data visualization.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.64.1">In all, scikit-learn is a sufficient and effective tool for a range of common data processing and model-building tasks.</span></p>
<h2 class="heading-2" id="_idParaDest-150"><span class="koboSpan" id="kobo.65.1">Installing scikit-learn</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.66.1">You can easily </span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.67.1">install the scikit-learn package on different operating systems such as macOS, Windows, and Linux. </span><span class="koboSpan" id="kobo.67.2">The scikit-learn library package is hosted on</span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.68.1"> the </span><strong class="keyWord"><span class="koboSpan" id="kobo.69.1">Python Package Index</span></strong><span class="koboSpan" id="kobo.70.1"> site (</span><a href="https://pypi.org/"><span class="url"><span class="koboSpan" id="kobo.71.1">https://pypi.org/</span></span></a><span class="koboSpan" id="kobo.72.1">) and the </span><strong class="keyWord"><span class="koboSpan" id="kobo.73.1">Anaconda</span></strong><span class="koboSpan" id="kobo.74.1"> package repository (</span><a href="https://anaconda.org/anaconda/repo"><span class="url"><span class="koboSpan" id="kobo.75.1">https://anaconda.org/anaconda/repo</span></span></a><span class="koboSpan" id="kobo.76.1">). </span><span class="koboSpan" id="kobo.76.2">To </span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.77.1">install it in your environment, you can use either</span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.78.1"> the </span><code class="inlineCode"><span class="koboSpan" id="kobo.79.1">pip</span></code><span class="koboSpan" id="kobo.80.1"> package manager or the </span><strong class="keyWord"><span class="koboSpan" id="kobo.81.1">Conda</span></strong><span class="koboSpan" id="kobo.82.1"> package manager. </span><span class="koboSpan" id="kobo.82.2">A package manager allows you to install and manage the </span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.83.1">installation of library packages in your operating system.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.84.1">To install the </span><code class="inlineCode"><span class="koboSpan" id="kobo.85.1">scikit-learn</span></code><span class="koboSpan" id="kobo.86.1"> library using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.87.1">pip</span></code><span class="koboSpan" id="kobo.88.1"> or Conda package manager, you can simply run </span><code class="inlineCode"><span class="koboSpan" id="kobo.89.1">pip install -U scikit-learn</span></code><span class="koboSpan" id="kobo.90.1"> to install it from the PyPI index or run </span><code class="inlineCode"><span class="koboSpan" id="kobo.91.1">conda install scikit-learn</span></code><span class="koboSpan" id="kobo.92.1"> if you want to use a Conda environment. </span><span class="koboSpan" id="kobo.92.2">You can learn more about </span><code class="inlineCode"><span class="koboSpan" id="kobo.93.1">pip</span></code><span class="koboSpan" id="kobo.94.1"> at </span><a href="https://pip.pypa.io/"><span class="url"><span class="koboSpan" id="kobo.95.1">https://pip.pypa.io/</span></span></a><span class="koboSpan" id="kobo.96.1"> and Conda at </span><a href="http://docs.conda.io"><span class="url"><span class="koboSpan" id="kobo.97.1">http://docs.conda.io</span></span></a><span class="koboSpan" id="kobo.98.1">.</span></p>
<h2 class="heading-2" id="_idParaDest-151"><span class="koboSpan" id="kobo.99.1">Core components of scikit-learn</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.100.1">The scikit-learn library</span><a id="_idIndexMarker456"/><span class="koboSpan" id="kobo.101.1"> provides a wide range of Python classes and functionalities for the various stages of the ML lifecycle. </span><span class="koboSpan" id="kobo.101.2">It consists of several main components, as depicted in the following diagram. </span><span class="koboSpan" id="kobo.101.3">By utilizing these components, you can construct ML pipelines and perform tasks such as classification, regression, and clustering.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.102.1"><img alt="Figure 5.1 – scikit-learn components " src="../Images/B20836_05_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.103.1">Figure 5.1: scikit-learn components</span></p>
<p class="normal"><span class="koboSpan" id="kobo.104.1">Now, let’s delve deeper into how these components support the different stages of the ML lifecycle:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.105.1">Preparing data</span></strong><span class="koboSpan" id="kobo.106.1">: For data manipulation and processing, the </span><code class="inlineCode"><span class="koboSpan" id="kobo.107.1">pandas</span></code><span class="koboSpan" id="kobo.108.1"> library is commonly used. </span><span class="koboSpan" id="kobo.108.2">It provides core data loading and saving functions, as well as utilities for data manipulations such as data selection, data arrangement, and data statistical summaries. </span><code class="inlineCode"><span class="koboSpan" id="kobo.109.1">pandas</span></code><span class="koboSpan" id="kobo.110.1"> is built on top of NumPy. </span><span class="koboSpan" id="kobo.110.2">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.111.1">pandas</span></code><span class="koboSpan" id="kobo.112.1"> library also comes with some visualization features such as pie charts, scatter plots, and box plots.
    </span><p class="normal"><span class="koboSpan" id="kobo.113.1">scikit-learn</span><a id="_idIndexMarker457"/><span class="koboSpan" id="kobo.114.1"> provides a list of transformers for data processing and transformation, such as imputing missing values, encoding categorical values, normalization, and feature extraction for text and images. </span><span class="koboSpan" id="kobo.114.2">You can find the full list of transformers at </span><a href="https://scikit-learn.org/stable/data_transforms.html"><span class="url"><span class="koboSpan" id="kobo.115.1">https://scikit-learn.org/stable/data_transforms.html</span></span></a><span class="koboSpan" id="kobo.116.1">. </span><span class="koboSpan" id="kobo.116.2">Furthermore, you have the flexibility to create custom transformers.</span></p></li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.117.1">Model training</span></strong><span class="koboSpan" id="kobo.118.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.119.1">scikit-learn</span></code><span class="koboSpan" id="kobo.120.1"> provides a long list of ML algorithms (also known as estimators) for classification and regression (for example, logistic regression, k-nearest neighbors, and random forest), as well as clustering (for example, k-means). </span><span class="koboSpan" id="kobo.120.2">You can find the full list of algorithms at </span><a href="https://scikit-learn.org/stable/index.html"><span class="url"><span class="koboSpan" id="kobo.121.1">https://scikit-learn.org/stable/index.html</span></span></a><span class="koboSpan" id="kobo.122.1">. </span><span class="koboSpan" id="kobo.122.2">The following sample code shows the syntax for using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.123.1">RandomForestClassifier</span></code><span class="koboSpan" id="kobo.124.1"> algorithm to train a model using a labeled training dataset:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.125.1">from</span></span><span class="koboSpan" id="kobo.126.1"> sklearn.ensemble </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.127.1">import</span></span><span class="koboSpan" id="kobo.128.1"> RandomForestClassifier
model = RandomForestClassifier (
  max_depth, max_features, n_estimators
)
model.fit(train_X, train_y)
</span></code></pre>
</li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.129.1">Model evaluation</span></strong><span class="koboSpan" id="kobo.130.1">: scikit-learn has utilities for hyperparameter tuning and cross-validation, as well as </span><code class="inlineCode"><span class="koboSpan" id="kobo.131.1">metrics</span></code><span class="koboSpan" id="kobo.132.1"> classes for model evaluations. </span><span class="koboSpan" id="kobo.132.2">You can find the full list of model selection and evaluation utilities at </span><a href="https://scikit-learn.org/stable/model_selection.html"><span class="url"><span class="koboSpan" id="kobo.133.1">https://scikit-learn.org/stable/model_selection.html</span></span></a><span class="koboSpan" id="kobo.134.1">. </span><span class="koboSpan" id="kobo.134.2">The following sample code shows the </span><code class="inlineCode"><span class="koboSpan" id="kobo.135.1">accuracy_score</span></code><span class="koboSpan" id="kobo.136.1"> class for evaluating the accuracy of classification models:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.137.1">from</span></span><span class="koboSpan" id="kobo.138.1"> sklearn.metrics </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.139.1">import</span></span><span class="koboSpan" id="kobo.140.1"> accuracy_score
acc = accuracy_score (true_label, predicted_label)
</span></code></pre>
<div class="note">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.141.1">Hyperparameter tuning</span></strong><span class="koboSpan" id="kobo.142.1"> involves </span><a id="_idIndexMarker458"/><span class="koboSpan" id="kobo.143.1">optimizing the configuration settings (hyperparameters) of an ML model to enhance its performance and achieve better results on a given task or dataset. </span><span class="koboSpan" id="kobo.143.2">Cross-validation is a statistical technique used to assess the performance and generalizability of an ML model by dividing the dataset into multiple subsets, training the model on different combinations, and evaluating its performance across each subset.</span></p>
</div></li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.144.1">Model saving</span></strong><span class="koboSpan" id="kobo.145.1">: scikit-learn can save model artifacts using Python object serialization (</span><code class="inlineCode"><span class="koboSpan" id="kobo.146.1">pickle</span></code><span class="koboSpan" id="kobo.147.1"> or </span><code class="inlineCode"><span class="koboSpan" id="kobo.148.1">joblib</span></code><span class="koboSpan" id="kobo.149.1">). </span><span class="koboSpan" id="kobo.149.2">The serialized </span><code class="inlineCode"><span class="koboSpan" id="kobo.150.1">pickle</span></code><span class="koboSpan" id="kobo.151.1"> file can be loaded into memory for predictions. </span><span class="koboSpan" id="kobo.151.2">The following sample code shows the syntax for saving a model using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.152.1">joblib</span></code><span class="koboSpan" id="kobo.153.1"> class:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.154.1">import</span></span><span class="koboSpan" id="kobo.155.1"> joblib
joblib.dump(model, </span><span class="hljs-string"><span class="koboSpan" id="kobo.156.1">"saved_model_name.joblib"</span></span><span class="koboSpan" id="kobo.157.1">)
</span></code></pre>
</li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.158.1">Pipeline</span></strong><span class="koboSpan" id="kobo.159.1">: scikit-learn also </span><a id="_idIndexMarker459"/><span class="koboSpan" id="kobo.160.1">provides a pipeline utility for stringing together different transformers and estimators as </span><a id="_idIndexMarker460"/><span class="koboSpan" id="kobo.161.1">a single processing pipeline, and it can be reused as a single unit. </span><span class="koboSpan" id="kobo.161.2">This is especially useful when you need to preprocess data for modeling training and model prediction, as both require the data to be processed in the same way:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.162.1">from</span></span><span class="koboSpan" id="kobo.163.1"> sklearn.pipeline </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.164.1">import</span></span><span class="koboSpan" id="kobo.165.1"> Pipeline
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.166.1">from</span></span><span class="koboSpan" id="kobo.167.1"> sklearn.preprocessing </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.168.1">import</span></span><span class="koboSpan" id="kobo.169.1"> StandardScaler
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.170.1">from</span></span><span class="koboSpan" id="kobo.171.1"> sklearn.ensemble </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.172.1">import</span></span><span class="koboSpan" id="kobo.173.1"> RandomForestClassifier
pipe = Pipeline([(</span><span class="hljs-string"><span class="koboSpan" id="kobo.174.1">'scaler'</span></span><span class="koboSpan" id="kobo.175.1">, StandardScaler()), (RF, RandomForestClassifier())])
pipe.fit(X_train, y_train)
</span></code></pre>
</li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.176.1">As demonstrated, getting started with scikit-learn for experimenting with and constructing ML models is straightforward. </span><span class="koboSpan" id="kobo.176.2">scikit-learn is particularly suitable for typical regression, classification, and clustering tasks performed on a single machine. </span><span class="koboSpan" id="kobo.176.3">However, if you’re working with extensive datasets or require distributed training across multiple machines, scikit-learn may not be the optimal choice unless the algorithm supports incremental training, such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.177.1">SGDRegressor</span></code><span class="koboSpan" id="kobo.178.1">. </span><span class="koboSpan" id="kobo.178.2">Therefore, moving on, let’s explore alternative ML libraries that excel in large-scale model training scenarios.</span></p>
<div class="note">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.179.1">Incremental training</span></strong><span class="koboSpan" id="kobo.180.1"> is an</span><a id="_idIndexMarker461"/><span class="koboSpan" id="kobo.181.1"> ML approach where a model is updated and refined continuously as new data becomes available, allowing the model to adapt to evolving patterns and improve its performance over time.</span></p>
</div>
<h1 class="heading-1" id="_idParaDest-152"><span class="koboSpan" id="kobo.182.1">Understanding the Apache Spark ML library</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.183.1">Apache Spark </span><a id="_idIndexMarker462"/><span class="koboSpan" id="kobo.184.1">is an</span><a id="_idIndexMarker463"/><span class="koboSpan" id="kobo.185.1"> advanced framework for distributed data processing, designed to handle large-scale data processing tasks. </span><span class="koboSpan" id="kobo.185.2">With its distributed computing capabilities, Spark enables applications to efficiently load and process data across a cluster of machines by leveraging in-memory computing, thereby significantly reducing processing times.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.186.1">Architecturally, a Spark cluster </span><a id="_idIndexMarker464"/><span class="koboSpan" id="kobo.187.1">consists of a master node and worker nodes for running different Spark applications. </span><span class="koboSpan" id="kobo.187.2">Each application that runs in a Spark cluster has a driver program and its own set of processes, which are coordinated by </span><a id="_idIndexMarker465"/><span class="koboSpan" id="kobo.188.1">the </span><strong class="keyWord"><span class="koboSpan" id="kobo.189.1">SparkSession</span></strong><span class="koboSpan" id="kobo.190.1"> object in the driver program. </span><span class="koboSpan" id="kobo.190.2">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.191.1">SparkSession</span></code><span class="koboSpan" id="kobo.192.1"> object in the driver program connects to a cluster manager (for example, Mesos, Yarn, Kubernetes, or Spark’s standalone cluster manager), which is responsible for allocating resources in the cluster for the Spark application. </span><span class="koboSpan" id="kobo.192.2">Specifically, the cluster manager acquires resources on worker nodes </span><a id="_idIndexMarker466"/><span class="koboSpan" id="kobo.193.1">called </span><strong class="keyWord"><span class="koboSpan" id="kobo.194.1">executors</span></strong><span class="koboSpan" id="kobo.195.1"> to run computations and store data for the Spark application. </span><span class="koboSpan" id="kobo.195.2">Executors are configured with resources such as the number of CPU cores and memory to meet task processing needs. </span><span class="koboSpan" id="kobo.195.3">Once the executors have been allocated, the cluster manager sends the application code (Java JAR or Python files) to the executors. </span><span class="koboSpan" id="kobo.195.4">Finally, </span><code class="inlineCode"><span class="koboSpan" id="kobo.196.1">SparkContext</span></code><span class="koboSpan" id="kobo.197.1"> sends the tasks to the executors to run. </span><span class="koboSpan" id="kobo.197.2">The following diagram shows how a driver program interacts with a cluster manager and executor to run a task:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.198.1"><img alt="Figure 5.2 – Running a Spark application on a Spark cluster " src="../Images/B20836_05_02.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.199.1">Figure 5.2: Running a Spark application on a Spark cluster</span></p>
<p class="normal"><span class="koboSpan" id="kobo.200.1">Each Spark </span><a id="_idIndexMarker467"/><span class="koboSpan" id="kobo.201.1">application gets its own set of executors, which remain active for the duration of the application. </span><span class="koboSpan" id="kobo.201.2">The executors for different applications are isolated from each other, and they can only share data through external data storage.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.202.1">The ML package for Spark is called MLlib, which runs on top of the distributed Spark architecture. </span><span class="koboSpan" id="kobo.202.2">It is capable of processing and training models with a large dataset that does not fit into the memory of a single machine. </span><span class="koboSpan" id="kobo.202.3">It provides APIs in different programming languages, including Python, Java, Scala, and R. </span><span class="koboSpan" id="kobo.202.4">From a structural perspective, it is very similar to that of the scikit-learn library in terms of core components and model development flow.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.203.1">Spark is highly popular and adopted by companies of all sizes across different industries. </span><span class="koboSpan" id="kobo.203.2">Large</span><a id="_idIndexMarker468"/><span class="koboSpan" id="kobo.204.1"> companies such</span><a id="_idIndexMarker469"/><span class="koboSpan" id="kobo.205.1"> as </span><strong class="keyWord"><span class="koboSpan" id="kobo.206.1">Netflix</span></strong><span class="koboSpan" id="kobo.207.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.208.1">Uber</span></strong><span class="koboSpan" id="kobo.209.1">, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.210.1">Pinterest</span></strong><span class="koboSpan" id="kobo.211.1"> use Spark for large-scale</span><a id="_idIndexMarker470"/><span class="koboSpan" id="kobo.212.1"> data processing and transformation, as well as running ML models.</span></p>
<h2 class="heading-2" id="_idParaDest-153"><span class="koboSpan" id="kobo.213.1">Installing Spark ML</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.214.1">Spark ML libraries</span><a id="_idIndexMarker471"/><span class="koboSpan" id="kobo.215.1"> are included as part of the Spark installation. </span><span class="koboSpan" id="kobo.215.2">PySpark is the Python API for Spark, and it can be installed like a regular Python package using </span><code class="inlineCode"><span class="koboSpan" id="kobo.216.1">pip</span></code><span class="koboSpan" id="kobo.217.1"> (</span><code class="inlineCode"><span class="koboSpan" id="kobo.218.1">pip install pyspark</span></code><span class="koboSpan" id="kobo.219.1">). </span><span class="koboSpan" id="kobo.219.2">Note that PySpark requires Java and Python to be installed on the machine before it can be installed. </span><span class="koboSpan" id="kobo.219.3">You can find Spark’s installation instructions at </span><a href="https://spark.apache.org/docs/latest/"><span class="url"><span class="koboSpan" id="kobo.220.1">https://spark.apache.org/docs/latest/</span></span></a><span class="koboSpan" id="kobo.221.1">.</span></p>
<h2 class="heading-2" id="_idParaDest-154"><span class="koboSpan" id="kobo.222.1">Core components of the Spark ML library</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.223.1">Similar to </span><a id="_idIndexMarker472"/><span class="koboSpan" id="kobo.224.1">the scikit-learn library, Spark and Spark ML provide a full range of functionality for building ML models, from data preparation to model evaluation and model persistence. </span><span class="koboSpan" id="kobo.224.2">The following diagram shows the core components that are available in Spark for building ML models:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.225.1"><img alt="Figure 5.3 – Core components of Spark ML " src="../Images/B20836_05_03.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.226.1">Figure 5.3: Core components of Spark ML</span></p>
<p class="normal"><span class="koboSpan" id="kobo.227.1">Let’s take a closer look at the core functions supported by the Spark and Spark ML library packages:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.228.1">Preparing data</span></strong><span class="koboSpan" id="kobo.229.1">: Spark supports Spark DataFrames, distributed data collections that can be used for tasks such as data joining, aggregation, filtering, and other data manipulation needs. </span><span class="koboSpan" id="kobo.229.2">Conceptually, a Spark DataFrame is equivalent to a table in a relational database. </span><span class="koboSpan" id="kobo.229.3">A Spark DataFrame can be distributed (that is, partitioned) across many machines, which allows fast data processing in parallel. </span><span class="koboSpan" id="kobo.229.4">Spark DataFrames also operate on a model called the</span><a id="_idIndexMarker473"/><span class="koboSpan" id="kobo.230.1"> lazy execution model. </span><strong class="keyWord"><span class="koboSpan" id="kobo.231.1">Lazy execution</span></strong><span class="koboSpan" id="kobo.232.1"> defines</span><a id="_idIndexMarker474"/><span class="koboSpan" id="kobo.233.1"> a set of transformations (for example, adding a column or filtering column) and the transformations are only executed when an action (such as calculating the min/max of a column) is needed. </span><span class="koboSpan" id="kobo.233.2">This allows an execution plan for the different transformations and actions to be generated to optimize the execution’s performance.
    </span><p class="normal"><span class="koboSpan" id="kobo.234.1">To start using the Spark functionality, you need to create a Spark session. </span><span class="koboSpan" id="kobo.234.2">A Spark session creates a </span><code class="inlineCode"><span class="koboSpan" id="kobo.235.1">SparkContext</span></code><span class="koboSpan" id="kobo.236.1"> object, which is the entry point to the Spark functionality. </span><span class="koboSpan" id="kobo.236.2">The following sample code shows how you can create a Spark session:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.237.1">from</span></span><span class="koboSpan" id="kobo.238.1"> pyspark.sql </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.239.1">import</span></span><span class="koboSpan" id="kobo.240.1"> SparkSession
spark = SparkSession.builder.appName(</span><span class="hljs-string"><span class="koboSpan" id="kobo.241.1">'appname'</span></span><span class="koboSpan" id="kobo.242.1">).getOrCreate()
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.243.1">A Spark DataFrame </span><a id="_idIndexMarker475"/><span class="koboSpan" id="kobo.244.1">can be constructed from many different sources, such as structured </span><a id="_idIndexMarker476"/><span class="koboSpan" id="kobo.245.1">data files (for example, CSV or JSON) and external databases. </span><span class="koboSpan" id="kobo.245.2">The following code sample reads a CSV file into a Spark DataFrame:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.246.1">dataFrame = spark.read.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.247.1">format</span></span><span class="koboSpan" id="kobo.248.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.249.1">'csv'</span></span><span class="koboSpan" id="kobo.250.1">).load(file_path)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.251.1">There are many transformers for data processing and transformation in Spark based on different data processing needs, such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.252.1">Tokenizer</span></code><span class="koboSpan" id="kobo.253.1"> (which breaks text down into individual words) and </span><code class="inlineCode"><span class="koboSpan" id="kobo.254.1">StandardScalar</span></code><span class="koboSpan" id="kobo.255.1"> (which normalizes a feature into unit deviation and/or zero mean). </span><span class="koboSpan" id="kobo.255.2">You can find a list of supported transformers at </span><a href="https://spark.apache.org/docs/2.1.0/ml-features.html"><span class="url"><span class="koboSpan" id="kobo.256.1">https://spark.apache.org/docs/2.1.0/ml-features.html</span></span></a><span class="koboSpan" id="kobo.257.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.258.1">To use a transformer, first, you must initiate it with function parameters, such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.259.1">inputCol</span></code><span class="koboSpan" id="kobo.260.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.261.1">outputCol</span></code><span class="koboSpan" id="kobo.262.1">, then call the </span><code class="inlineCode"><span class="koboSpan" id="kobo.263.1">fit()</span></code><span class="koboSpan" id="kobo.264.1"> function on the DataFrame that contains the data, and finally, call the </span><code class="inlineCode"><span class="koboSpan" id="kobo.265.1">transform()</span></code><span class="koboSpan" id="kobo.266.1"> function to transfer the features in the DataFrame:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.267.1">from</span></span><span class="koboSpan" id="kobo.268.1"> pyspark.ml.feature </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.269.1">import</span></span><span class="koboSpan" id="kobo.270.1"> StandardScaler
scaler = StandardScaler(inputCol=</span><span class="hljs-string"><span class="koboSpan" id="kobo.271.1">"features"</span></span><span class="koboSpan" id="kobo.272.1">,  outputCol=</span><span class="hljs-string"><span class="koboSpan" id="kobo.273.1">"scaledFeatures"</span></span><span class="koboSpan" id="kobo.274.1">, withStd=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.275.1">True</span></span><span class="koboSpan" id="kobo.276.1">, withMean=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.277.1">False</span></span><span class="koboSpan" id="kobo.278.1">)
scalerModel = scaler.fit(dataFrame)
scaledData = scalerModel.transform(dataFrame)
</span></code></pre>
</li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.279.1">Model training</span></strong><span class="koboSpan" id="kobo.280.1">: Spark ML supports a wide range of ML algorithms for classification, regression, clustering, recommendation, and topic modeling. </span><span class="koboSpan" id="kobo.280.2">You can find a list of Spark ML algorithms at </span><a href="https://spark.apache.org/docs/1.4.1/mllib-guide.html"><span class="url"><span class="koboSpan" id="kobo.281.1">https://spark.apache.org/docs/1.4.1/mllib-guide.html</span></span></a><span class="koboSpan" id="kobo.282.1">. </span><span class="koboSpan" id="kobo.282.2">The following code sample shows how you can train a logistic regression model:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.283.1">from</span></span><span class="koboSpan" id="kobo.284.1"> pyspark.ml.classification </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.285.1">import</span></span><span class="koboSpan" id="kobo.286.1"> LogisticRegression
lr_algo = LogisticRegression(
  maxIter regParam, elasticNetParam
)
lr_model = lr_algo.fit(dataFrame)
</span></code></pre>
</li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.287.1">Model evaluation</span></strong><span class="koboSpan" id="kobo.288.1">: For model selection and evaluation, Spark ML provides utilities for cross-validation, hyperparameter tuning, and model evaluation metrics. </span><span class="koboSpan" id="kobo.288.2">You can find the list of evaluators at </span><a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html"><span class="url"><span class="koboSpan" id="kobo.289.1">https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html</span></span></a><span class="koboSpan" id="kobo.290.1">. </span><span class="koboSpan" id="kobo.290.2">The following</span><a id="_idIndexMarker477"/><span class="koboSpan" id="kobo.291.1"> code block illustrates using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.292.1">BinaryClassificationEvaluator</span></code><span class="koboSpan" id="kobo.293.1"> to evaluate a model with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.294.1">areaUnderPR</span></code><span class="koboSpan" id="kobo.295.1"> metric:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.296.1">From pyspark.ml.evaluation </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.297.1">import</span></span><span class="koboSpan" id="kobo.298.1"> BinaryClassificationEvaluator
dataset = spark.createDataFrame(scoreAndLabels, [</span><span class="hljs-string"><span class="koboSpan" id="kobo.299.1">"raw"</span></span><span class="koboSpan" id="kobo.300.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.301.1">"label"</span></span><span class="koboSpan" id="kobo.302.1">])
evaluator = BinaryClassificationEvaluator()
evaluator.setRawPredictionCol(</span><span class="hljs-string"><span class="koboSpan" id="kobo.303.1">"raw"</span></span><span class="koboSpan" id="kobo.304.1">)
evaluator.evaluate(dataset)
evaluator.evaluate(dataset, {evaluator.metricName: </span><span class="hljs-string"><span class="koboSpan" id="kobo.305.1">"areaUnderPR"</span></span><span class="koboSpan" id="kobo.306.1">})
</span></code></pre>
</li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.307.1">Pipeline</span></strong><span class="koboSpan" id="kobo.308.1">: Spark ML also has support for the pipeline concept, similar to that of scikit-learn. </span><span class="koboSpan" id="kobo.308.2">With the pipeline concept, you can sequence a series of transformation and model training steps as a unified repeatable step:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.309.1">from</span></span><span class="koboSpan" id="kobo.310.1"> pyspark.ml </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.311.1">import</span></span><span class="koboSpan" id="kobo.312.1"> Pipeline
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.313.1">from</span></span><span class="koboSpan" id="kobo.314.1"> pyspark.ml.classification </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.315.1">import</span></span><span class="koboSpan" id="kobo.316.1"> LogisticRegression
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.317.1">from</span></span><span class="koboSpan" id="kobo.318.1"> pyspark.ml.feature </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.319.1">import</span></span><span class="koboSpan" id="kobo.320.1"> HashingTF, Tokenizer
lr_tokenizer = Tokenizer(inputCol, outputCol)
lr_hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol)
lr_algo = LogisticRegression(maxIter, regParam)
lr_pipeline = Pipeline(stages=[lr_tokenizer, lr_hashingTF, lr_algo])
lr_model = lr_pipeline.fit(training)
</span></code></pre>
</li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.321.1">Model saving</span></strong><span class="koboSpan" id="kobo.322.1">: The Spark ML pipeline can be serialized into a serialization format called an Mleap bundle, which is an external library from Spark. </span><span class="koboSpan" id="kobo.322.2">A serialized Mleap bundle can be deserialized back into Spark for batch scoring or an Mleap runtime to run real-time APIs. </span><span class="koboSpan" id="kobo.322.3">You can find more details about Mleap at </span><a href="https://combust.github.io/mleap-docs/"><span class="url"><span class="koboSpan" id="kobo.323.1">https://combust.github.io/mleap-docs/</span></span></a><span class="koboSpan" id="kobo.324.1">. </span><span class="koboSpan" id="kobo.324.2">The following code shows the syntax for serializing a Spark model into the Mleap format:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.325.1">import</span></span><span class="koboSpan" id="kobo.326.1"> mleap.pyspark
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.327.1">from</span></span><span class="koboSpan" id="kobo.328.1"> pyspark.ml </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.329.1">import</span></span><span class="koboSpan" id="kobo.330.1"> Pipeline, PipelineModel
lr_model.serializeToBundle(</span><span class="hljs-string"><span class="koboSpan" id="kobo.331.1">"saved_file_path"</span></span><span class="koboSpan" id="kobo.332.1">, lr_model.transform(dataframe))
</span></code></pre>
</li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.333.1">Spark is a </span><a id="_idIndexMarker478"/><span class="koboSpan" id="kobo.334.1">versatile framework that enables large-scale data processing and ML. </span><span class="koboSpan" id="kobo.334.2">While it excels in traditional ML tasks, it also offers limited support for neural network training, including the multilayer perceptron algorithm. </span><span class="koboSpan" id="kobo.334.3">However, for more comprehensive deep learning capabilities, we will explore dedicated ML libraries including TensorFlow and PyTorch in the upcoming sections.</span></p>
<h1 class="heading-1" id="_idParaDest-155"><span class="koboSpan" id="kobo.335.1">Understanding the TensorFlow deep learning library</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.336.1">Initially </span><a id="_idIndexMarker479"/><span class="koboSpan" id="kobo.337.1">released in 2015, TensorFlow is a popular open-source ML library, primarily backed up by Google, which is mainly designed for deep learning. </span><span class="koboSpan" id="kobo.337.2">TensorFlow has been used by companies of all sizes for training and building state-of-the-art deep learning models for a range of use cases, including computer vision, speech recognition, question-answering, text summarization, forecasting, and robotics.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.338.1">TensorFlow works based on the concept of the computational graph, where data flows through nodes that represent mathematical operations. </span><span class="koboSpan" id="kobo.338.2">The core idea is to construct a graph of operations and tensors, with tensors being </span><em class="italic"><span class="koboSpan" id="kobo.339.1">n</span></em><span class="koboSpan" id="kobo.340.1">-dimensional arrays that carry data. </span><span class="koboSpan" id="kobo.340.2">An example of a tensor could be a scalar value (for example, </span><code class="inlineCode"><span class="koboSpan" id="kobo.341.1">1.0</span></code><span class="koboSpan" id="kobo.342.1">), a one-dimensional vector (for example, </span><code class="inlineCode"><span class="koboSpan" id="kobo.343.1">[1.0, 2.0, 3.0]</span></code><span class="koboSpan" id="kobo.344.1">), a two-dimensional matrix (for example, </span><code class="inlineCode"><span class="koboSpan" id="kobo.345.1">[[1.0, 2.0, 3.0]</span></code><span class="koboSpan" id="kobo.346.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.347.1">[4.0, 5.0, 6.0]]</span></code><span class="koboSpan" id="kobo.348.1">), or even higher dimensional matrices. </span><span class="koboSpan" id="kobo.348.2">Operations are performed on these tensors, allowing for mathematical computations like addition or matrix multiplication. </span><span class="koboSpan" id="kobo.348.3">The following diagram shows a sample computational graph for performing a sequence of mathematical operations on tensors:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.349.1"><img alt="Figure 5.4 – Data flow diagram " src="../Images/B20836_05_04.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.350.1">Figure 5.4: Data flow diagram</span></p>
<p class="normal"><span class="koboSpan" id="kobo.351.1">In the preceding </span><a id="_idIndexMarker480"/><span class="koboSpan" id="kobo.352.1">computational diagram, the rectangular nodes are mathematical operations, while the circles represent tensors. </span><span class="koboSpan" id="kobo.352.2">This particular diagram shows a computational graph for performing an artificial neuron tensor operation, which is to perform a matrix multiplication of </span><em class="italic"><span class="koboSpan" id="kobo.353.1">W</span></em><span class="koboSpan" id="kobo.354.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.355.1">X</span></em><span class="koboSpan" id="kobo.356.1">, followed by the addition of </span><em class="italic"><span class="koboSpan" id="kobo.357.1">b</span></em><span class="koboSpan" id="kobo.358.1">, and, lastly, apply a </span><em class="italic"><span class="koboSpan" id="kobo.359.1">ReLU</span></em><span class="koboSpan" id="kobo.360.1"> action function. </span><span class="koboSpan" id="kobo.360.2">The equivalent mathematical formula is as follows:</span></p>
<p class="center"><span class="koboSpan" id="kobo.361.1"><img alt="" role="presentation" src="../Images/B20836_05_001.png"/></span></p>
<p class="normal"><span class="koboSpan" id="kobo.362.1">TensorFlow allows users to define and manipulate the computation graph using its high-level API or by directly working with its lower-level components. </span><span class="koboSpan" id="kobo.362.2">This flexibility allows researchers and developers to create complex models and algorithms. </span><span class="koboSpan" id="kobo.362.3">Furthermore, TensorFlow supports distributed computing, allowing the graph to be executed across multiple devices or machines, which is crucial for handling large-scale ML tasks. </span><span class="koboSpan" id="kobo.362.4">This distributed architecture enables TensorFlow to leverage the power of clusters or GPUs to accelerate the training and inference of deep learning models.</span></p>
<h2 class="heading-2" id="_idParaDest-156"><span class="koboSpan" id="kobo.363.1">Installing TensorFlow</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.364.1">TensorFlow</span><a id="_idIndexMarker481"/><span class="koboSpan" id="kobo.365.1"> can be installed using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.366.1">pip install --upgrade tensorflow</span></code><span class="koboSpan" id="kobo.367.1"> command in a Python-based environment. </span><span class="koboSpan" id="kobo.367.2">After installation, TensorFlow can be used just like any other Python library package.</span></p>
<h2 class="heading-2" id="_idParaDest-157"><span class="koboSpan" id="kobo.368.1">Core components of TensorFlow</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.369.1">The TensorFlow library provides a rich set of features for different ML steps, from data preparation to model serving. </span><span class="koboSpan" id="kobo.369.2">The</span><a id="_idIndexMarker482"/><span class="koboSpan" id="kobo.370.1"> following diagram illustrates the core building blocks of the TensorFlow library:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.371.1"><img alt="Figure 5.5 – TensorFlow components " src="../Images/B20836_05_05.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.372.1">Figure 5.5: TensorFlow components</span></p>
<p class="normal"><span class="koboSpan" id="kobo.373.1">Training an ML model using TensorFlow 2.x involves the following main steps:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><strong class="keyWord"><span class="koboSpan" id="kobo.374.1">Preparing the dataset</span></strong><span class="koboSpan" id="kobo.375.1">: TensorFlow 2.x provides a </span><code class="inlineCode"><span class="koboSpan" id="kobo.376.1">tf.data</span></code><span class="koboSpan" id="kobo.377.1"> library for efficiently loading data from sources (such as files), transforming data (such as changing the values of the dataset), and setting up the dataset for training (such as configuring batch size or data prefetching). </span><span class="koboSpan" id="kobo.377.2">These data classes provide efficient ways to pass data to the training algorithms for optimized model training. </span><span class="koboSpan" id="kobo.377.3">The</span><a id="_idIndexMarker483"/><span class="koboSpan" id="kobo.378.1"> TensorFlow </span><strong class="keyWord"><span class="koboSpan" id="kobo.379.1">Keras</span></strong><span class="koboSpan" id="kobo.380.1"> API also provides a list of built-in classes (MNIST, CIFAR, IMDB, MNIST Fashion, and Reuters Newswire) for building simple deep learning models. </span><span class="koboSpan" id="kobo.380.2">You can also feed a NumPy array or Python generator (a function that behaves like an iterator) to a model in TensorFlow for model training, but </span><code class="inlineCode"><span class="koboSpan" id="kobo.381.1">tf.data</span></code><span class="koboSpan" id="kobo.382.1"> is the recommended approach.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.383.1">Defining the neural network</span></strong><span class="koboSpan" id="kobo.384.1">: TensorFlow 2.x provides multiple ways to use or build a neural network for model training. </span><span class="koboSpan" id="kobo.384.2">You can use the premade estimators (the </span><code class="inlineCode"><span class="koboSpan" id="kobo.385.1">tf.estimator</span></code><span class="koboSpan" id="kobo.386.1"> class) such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.387.1">DNNRegressor</span></code><span class="koboSpan" id="kobo.388.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.389.1">DNNClassifier</span></code><span class="koboSpan" id="kobo.390.1"> to train models. </span><span class="koboSpan" id="kobo.390.2">Or, you can create custom neural networks using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.391.1">tf.keras</span></code><span class="koboSpan" id="kobo.392.1"> class, which provides a list of primitives such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.393.1">tf.keras.layers</span></code><span class="koboSpan" id="kobo.394.1"> for constructing neural network layers and </span><code class="inlineCode"><span class="koboSpan" id="kobo.395.1">tf.keras.activation</span></code><span class="koboSpan" id="kobo.396.1"> such as</span><a id="_idIndexMarker484"/><span class="koboSpan" id="kobo.397.1"> ReLU, </span><strong class="keyWord"><span class="koboSpan" id="kobo.398.1">Sigmoid</span></strong><span class="koboSpan" id="kobo.399.1">, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.400.1">Softmax</span></strong><span class="koboSpan" id="kobo.401.1"> for building neural networks. </span><span class="koboSpan" id="kobo.401.2">Softmax is </span><a id="_idIndexMarker485"/><span class="koboSpan" id="kobo.402.1">usually used as the last output of a neural network for a multiclass problem. </span><span class="koboSpan" id="kobo.402.2">It takes a vector of real numbers (positive and negative) as input and normalizes the vector as a probability distribution to represent the probabilities of different class labels, such as the different types of hand-written digits. </span><span class="koboSpan" id="kobo.402.3">For binary classification problems, Sigmoid is normally used and it returns a value between 0 and 1.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.403.1">Defining the loss function</span></strong><span class="koboSpan" id="kobo.404.1">: TensorFlow 2.x provides a list of built-in loss functions </span><a id="_idIndexMarker486"/><span class="koboSpan" id="kobo.405.1">such as </span><strong class="keyWord"><span class="koboSpan" id="kobo.406.1">mean squared error</span></strong><span class="koboSpan" id="kobo.407.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.408.1">MSE</span></strong><span class="koboSpan" id="kobo.409.1">) and </span><strong class="keyWord"><span class="koboSpan" id="kobo.410.1">mean absolute error</span></strong><span class="koboSpan" id="kobo.411.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.412.1">MAE</span></strong><span class="koboSpan" id="kobo.413.1">) for regression tasks and cross-entropy loss </span><a id="_idIndexMarker487"/><span class="koboSpan" id="kobo.414.1">for classification tasks. </span><span class="koboSpan" id="kobo.414.2">You can find more details about MSE and MAE at </span><a href="https://en.wikipedia.org/wiki/Mean_squared_error"><span class="url"><span class="koboSpan" id="kobo.415.1">https://en.wikipedia.org/wiki/Mean_squared_error</span></span></a><span class="koboSpan" id="kobo.416.1"> and </span><a href="https://en.wikipedia.org/wiki/Mean_absolute_error"><span class="url"><span class="koboSpan" id="kobo.417.1">https://en.wikipedia.org/wiki/Mean_absolute_error</span></span></a><span class="koboSpan" id="kobo.418.1">. </span><span class="koboSpan" id="kobo.418.2">You can find a list of supported loss functions in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.419.1">tf.keras.losses</span></code><span class="koboSpan" id="kobo.420.1"> class. </span><span class="koboSpan" id="kobo.420.2">For more details about the </span><a id="_idIndexMarker488"/><span class="koboSpan" id="kobo.421.1">different losses, refer to </span><a href="https://keras.io/api/losses/"><span class="url"><span class="koboSpan" id="kobo.422.1">https://keras.io/api/losses/</span></span></a><span class="koboSpan" id="kobo.423.1">. </span><span class="koboSpan" id="kobo.423.2">There is also the flexibility to define custom loss functions if the built-in loss functions do not meet the needs.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.424.1">Selecting the optimizer</span></strong><span class="koboSpan" id="kobo.425.1">: TensorFlow 2.x provides a list of built-in optimizers for model training, such</span><a id="_idIndexMarker489"/><span class="koboSpan" id="kobo.426.1"> as the </span><strong class="keyWord"><span class="koboSpan" id="kobo.427.1">Adam</span></strong><span class="koboSpan" id="kobo.428.1"> optimizer and the </span><strong class="keyWord"><span class="koboSpan" id="kobo.429.1">stochastic gradient descent</span></strong><span class="koboSpan" id="kobo.430.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.431.1">SGD</span></strong><span class="koboSpan" id="kobo.432.1">) optimizer</span><a id="_idIndexMarker490"/><span class="koboSpan" id="kobo.433.1"> for parameters optimization, with its </span><code class="inlineCode"><span class="koboSpan" id="kobo.434.1">tf.keras.optimizers</span></code><span class="koboSpan" id="kobo.435.1"> class. </span><span class="koboSpan" id="kobo.435.2">You can find more details about the different supported optimizers at </span><a href="https://keras.io/api/optimizers/"><span class="url"><span class="koboSpan" id="kobo.436.1">https://keras.io/api/optimizers/</span></span></a><span class="koboSpan" id="kobo.437.1">. </span><span class="koboSpan" id="kobo.437.2">Adam and SGD are two of the most commonly used optimizers.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.438.1">Selecting the evaluation metrics</span></strong><span class="koboSpan" id="kobo.439.1">: TensorFlow 2.x has a list of built-in model evaluation metrics (for example, accuracy and cross-entropy) for model training evaluations with its </span><code class="inlineCode"><span class="koboSpan" id="kobo.440.1">tf.keras.metrics</span></code><span class="koboSpan" id="kobo.441.1"> class. </span><span class="koboSpan" id="kobo.441.2">You can also define custom metrics for model evaluation during training.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.442.1">Compiling the network into a model</span></strong><span class="koboSpan" id="kobo.443.1">: This step compiles the defined network, along with the defined loss function, optimizer, and evaluation metrics, into a computational graph that’s ready for model training.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.444.1">Fitting the model</span></strong><span class="koboSpan" id="kobo.445.1">: This step kicks off the model training process by feeding the data to the computational graph through batches and multiple epochs to optimize the model parameters.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.446.1">Evaluating the trained model</span></strong><span class="koboSpan" id="kobo.447.1">: Once the model has been trained, you can evaluate the model using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.448.1">evaluate()</span></code><span class="koboSpan" id="kobo.449.1"> function against the test data.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.450.1">Saving the model</span></strong><span class="koboSpan" id="kobo.451.1">: The model </span><a id="_idIndexMarker491"/><span class="koboSpan" id="kobo.452.1">can be saved in the TensorFlow </span><strong class="keyWord"><span class="koboSpan" id="kobo.453.1">SavedModel</span></strong><span class="koboSpan" id="kobo.454.1"> serialization format </span><a id="_idIndexMarker492"/><span class="koboSpan" id="kobo.455.1">or </span><strong class="keyWord"><span class="koboSpan" id="kobo.456.1">Hierarchical Data Format</span></strong><span class="koboSpan" id="kobo.457.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.458.1">HDF5</span></strong><span class="koboSpan" id="kobo.459.1">) format.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.460.1">Model serving</span></strong><span class="koboSpan" id="kobo.461.1">: TensorFlow comes with a model serving framework called TensorFlow Serving, which we will cover in greater detail in </span><em class="chapterRef"><span class="koboSpan" id="kobo.462.1">Chapter 7</span></em><span class="koboSpan" id="kobo.463.1">, </span><em class="italic"><span class="koboSpan" id="kobo.464.1">Open-Source ML Platform</span></em><span class="koboSpan" id="kobo.465.1">.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.466.1">The TensorFlow library </span><a id="_idIndexMarker493"/><span class="koboSpan" id="kobo.467.1">is designed for large-scale production-grade data processing and model training. </span><span class="koboSpan" id="kobo.467.2">As such, it provides capabilities for large-scale distributed data processing and model training on a cluster of servers against a large dataset. </span><span class="koboSpan" id="kobo.467.3">We will cover large-scale distributed data processing and model training in greater detail in </span><em class="chapterRef"><span class="koboSpan" id="kobo.468.1">Chapter 10</span></em><span class="koboSpan" id="kobo.469.1">, </span><em class="italic"><span class="koboSpan" id="kobo.470.1">Advanced ML Engineering</span></em><span class="koboSpan" id="kobo.471.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.472.1">To support the </span><a id="_idIndexMarker494"/><span class="koboSpan" id="kobo.473.1">complete process of building and deploying ML pipelines, TensorFlow provides </span><strong class="keyWord"><span class="koboSpan" id="kobo.474.1">TensorFlow Extended</span></strong><span class="koboSpan" id="kobo.475.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.476.1">TFX</span></strong><span class="koboSpan" id="kobo.477.1">). </span><span class="koboSpan" id="kobo.477.2">TFX integrates multiple components and libraries from the TensorFlow ecosystem, creating a cohesive platform for tasks such as data ingestion, data validation, preprocessing, model training, model evaluation, and model deployment. </span><span class="koboSpan" id="kobo.477.3">Its architecture is designed to be modular and scalable, enabling users to tailor and expand the pipeline to meet their specific requirements. </span><span class="koboSpan" id="kobo.477.4">You can get more </span><a id="_idIndexMarker495"/><span class="koboSpan" id="kobo.478.1">details about TFX at </span><a href="https://www.tensorflow.org/tfx"><span class="url"><span class="koboSpan" id="kobo.479.1">https://www.tensorflow.org/tfx</span></span></a><span class="koboSpan" id="kobo.480.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.481.1">TensorFlow offers an expanded ecosystem of libraries and extensions for solving a wide range of advanced ML problems, including federated learning (training a model using decentralized data), model optimization (optimizing a model for deployment and execution), and probabilistic reasoning (reasoning under uncertainty using probability theory). </span><span class="koboSpan" id="kobo.481.2">It also provides support for mobile and edge devices with this TensorFlow Lite component, and support for browsers through the TensorFlow.js library.</span></p>
<h2 class="heading-2" id="_idParaDest-158"><span class="koboSpan" id="kobo.482.1">Hands-on exercise – training a TensorFlow model</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.483.1">With deep learning </span><a id="_idIndexMarker496"/><span class="koboSpan" id="kobo.484.1">dominating the recent ML advancement, it is important to have some hands-on experience with deep learning frameworks. </span><span class="koboSpan" id="kobo.484.2">In this exercise, you will learn how to install the TensorFlow library in your local Jupyter environment and build and train a simple neural network model. </span><span class="koboSpan" id="kobo.484.3">Launch a Jupyter notebook that you have previously installed on your machine. </span><span class="koboSpan" id="kobo.484.4">If you don’t remember how to do this, revisit the </span><em class="italic"><span class="koboSpan" id="kobo.485.1">Hands-on lab</span></em><span class="koboSpan" id="kobo.486.1"> section of </span><em class="chapterRef"><span class="koboSpan" id="kobo.487.1">Chapter 3</span></em><span class="koboSpan" id="kobo.488.1">, </span><em class="italic"><span class="koboSpan" id="kobo.489.1">Exploring ML Algorithms</span></em><span class="koboSpan" id="kobo.490.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.491.1">Once the Jupyter notebook is running, create a new folder by selecting the </span><strong class="screenText"><span class="koboSpan" id="kobo.492.1">New</span></strong><span class="koboSpan" id="kobo.493.1"> dropdown and then </span><strong class="screenText"><span class="koboSpan" id="kobo.494.1">Folder</span></strong><span class="koboSpan" id="kobo.495.1">. </span><span class="koboSpan" id="kobo.495.2">Rename the folder </span><code class="inlineCode"><span class="koboSpan" id="kobo.496.1">TensorFlowLab</span></code><span class="koboSpan" id="kobo.497.1">. </span><span class="koboSpan" id="kobo.497.2">Open the </span><code class="inlineCode"><span class="koboSpan" id="kobo.498.1">TensorFlowLab</span></code><span class="koboSpan" id="kobo.499.1"> folder, create </span><a id="_idIndexMarker497"/><span class="koboSpan" id="kobo.500.1">a new notebook inside this folder, and rename the notebook </span><code class="inlineCode"><span class="koboSpan" id="kobo.501.1">Tensorflow-lab1.ipynb</span></code><span class="koboSpan" id="kobo.502.1">. </span><span class="koboSpan" id="kobo.502.2">Now, let’s get started:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.503.1">Inside the first cell, run the following code to install TensorFlow. </span><span class="koboSpan" id="kobo.503.2">As mentioned in </span><em class="italic"><span class="koboSpan" id="kobo.504.1">Chapter 3</span></em><span class="koboSpan" id="kobo.505.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.506.1">pip</span></code><span class="koboSpan" id="kobo.507.1"> is the Python package installation utility:
        </span><pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.508.1">! </span><span class="koboSpan" id="kobo.508.2">pip3 install --upgrade tensorflow
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.509.1">Now, we must import the library and load the sample training data. </span><span class="koboSpan" id="kobo.509.2">We will use the built-in </span><code class="inlineCode"><span class="koboSpan" id="kobo.510.1">fashion_mnist</span></code><span class="koboSpan" id="kobo.511.1"> dataset that comes with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.512.1">keras</span></code><span class="koboSpan" id="kobo.513.1"> library to do so. </span><span class="koboSpan" id="kobo.513.2">Next, we must load the data into a </span><code class="inlineCode"><span class="koboSpan" id="kobo.514.1">tf.data.Dataset</span></code><span class="koboSpan" id="kobo.515.1"> class and then call its </span><code class="inlineCode"><span class="koboSpan" id="kobo.516.1">batch()</span></code><span class="koboSpan" id="kobo.517.1"> function to set up a batch size. </span><span class="koboSpan" id="kobo.517.2">Run the following code block in a new cell to load the data and configure the dataset:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.518.1">import</span></span><span class="koboSpan" id="kobo.519.1"> numpy </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.520.1">as</span></span><span class="koboSpan" id="kobo.521.1"> np
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.522.1">import</span></span><span class="koboSpan" id="kobo.523.1"> tensorflow </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.524.1">as</span></span><span class="koboSpan" id="kobo.525.1"> tf
train, test = tf.keras.datasets.fashion_mnist.load_data()
images, labels = train
labels = labels.astype(np.int32)
images = images/</span><span class="hljs-number"><span class="koboSpan" id="kobo.526.1">256</span></span><span class="koboSpan" id="kobo.527.1">  
train_ds = tf.data.Dataset.from_tensor_slices((images, labels))
train_ds = train_ds.batch(</span><span class="hljs-number"><span class="koboSpan" id="kobo.528.1">32</span></span><span class="koboSpan" id="kobo.529.1">)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.530.1">Let’s see what the data looks like. </span><span class="koboSpan" id="kobo.530.2">Run the following code block in a new cell to view the sample data. </span><strong class="keyWord"><span class="koboSpan" id="kobo.531.1">Matplotlib</span></strong><span class="koboSpan" id="kobo.532.1"> is a </span><a id="_idIndexMarker498"/><span class="koboSpan" id="kobo.533.1">Python visualization library, used to display an image:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.534.1">from</span></span><span class="koboSpan" id="kobo.535.1"> matplotlib </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.536.1">import</span></span><span class="koboSpan" id="kobo.537.1"> pyplot </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.538.1">as</span></span><span class="koboSpan" id="kobo.539.1"> plt
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.540.1">print</span></span><span class="koboSpan" id="kobo.541.1"> (</span><span class="hljs-string"><span class="koboSpan" id="kobo.542.1">"label:"</span></span><span class="koboSpan" id="kobo.543.1"> + </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.544.1">str</span></span><span class="koboSpan" id="kobo.545.1">(labels[</span><span class="hljs-number"><span class="koboSpan" id="kobo.546.1">0</span></span><span class="koboSpan" id="kobo.547.1">]))
pixels = images[</span><span class="hljs-number"><span class="koboSpan" id="kobo.548.1">0</span></span><span class="koboSpan" id="kobo.549.1">]
plt.imshow(pixels, cmap=</span><span class="hljs-string"><span class="koboSpan" id="kobo.550.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.551.1">gray'</span></span><span class="koboSpan" id="kobo.552.1">)
plt.show()
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.553.1">Next, we build a simple </span><strong class="keyWord"><span class="koboSpan" id="kobo.554.1">Multilayer Perception</span></strong><span class="koboSpan" id="kobo.555.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.556.1">MLP</span></strong><span class="koboSpan" id="kobo.557.1">) network with two hidden layers (one with </span><code class="inlineCode"><span class="koboSpan" id="kobo.558.1">100</span></code><span class="koboSpan" id="kobo.559.1"> nodes and one with </span><code class="inlineCode"><span class="koboSpan" id="kobo.560.1">50</span></code><span class="koboSpan" id="kobo.561.1"> nodes) and an output layer with </span><code class="inlineCode"><span class="koboSpan" id="kobo.562.1">10</span></code><span class="koboSpan" id="kobo.563.1"> nodes (each node represents a class label). </span><span class="koboSpan" id="kobo.563.2">Then, we must compile the network using the </span><strong class="keyWord"><span class="koboSpan" id="kobo.564.1">Adam</span></strong><span class="koboSpan" id="kobo.565.1"> optimizer, use</span><a id="_idIndexMarker499"/><span class="koboSpan" id="kobo.566.1"> the cross-entropy loss as the optimization objective, and use accuracy as the measuring metric. 
    </span><p class="normal"><span class="koboSpan" id="kobo.567.1">The Adam optimizer is a variation of </span><strong class="keyWord"><span class="koboSpan" id="kobo.568.1">gradient descent</span></strong><span class="koboSpan" id="kobo.569.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.570.1">GD</span></strong><span class="koboSpan" id="kobo.571.1">), and it improves upon GD mainly in</span><a id="_idIndexMarker500"/><span class="koboSpan" id="kobo.572.1"> the area of the adaptive learning rate for updating the parameters to improve model convergence, whereas GD uses a constant learning rate for parameter updating. </span><span class="koboSpan" id="kobo.572.2">Cross-entropy measures the performance of a classification model, where the output is the probability distribution for the different classes adding up to 1. </span><span class="koboSpan" id="kobo.572.3">The cross-entropy error increases when the predicted distribution diverges from the actual class label. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.573.1">To kick </span><a id="_idIndexMarker501"/><span class="koboSpan" id="kobo.574.1">off the training process, we must call the </span><code class="inlineCode"><span class="koboSpan" id="kobo.575.1">fit()</span></code><span class="koboSpan" id="kobo.576.1"> function, which is a required step in this case. </span><span class="koboSpan" id="kobo.576.2">We will run the training for </span><code class="inlineCode"><span class="koboSpan" id="kobo.577.1">10</span></code><span class="koboSpan" id="kobo.578.1"> epochs. </span><span class="koboSpan" id="kobo.578.2">One epoch is one pass of the entire training dataset. </span><span class="koboSpan" id="kobo.578.3">Note that we are running 10 epochs here for illustration purposes only. </span><span class="koboSpan" id="kobo.578.4">The actual number will be based on the specific training job and the desired model performance:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.579.1">model = tf.keras.Sequential([
   tf.keras.layers.Flatten(),
   tf.keras.layers.Dense(</span><span class="hljs-number"><span class="koboSpan" id="kobo.580.1">100</span></span><span class="koboSpan" id="kobo.581.1">, activation=</span><span class="hljs-string"><span class="koboSpan" id="kobo.582.1">"relu"</span></span><span class="koboSpan" id="kobo.583.1">),
   tf.keras.layers.Dense(</span><span class="hljs-number"><span class="koboSpan" id="kobo.584.1">50</span></span><span class="koboSpan" id="kobo.585.1">, activation=</span><span class="hljs-string"><span class="koboSpan" id="kobo.586.1">"relu"</span></span><span class="koboSpan" id="kobo.587.1">),
   tf.keras.layers.Dense(</span><span class="hljs-number"><span class="koboSpan" id="kobo.588.1">10</span></span><span class="koboSpan" id="kobo.589.1">),
   tf.keras.layers.Softmax()
])
model.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.590.1">compile</span></span><span class="koboSpan" id="kobo.591.1">(optimizer=</span><span class="hljs-string"><span class="koboSpan" id="kobo.592.1">'adam'</span></span><span class="koboSpan" id="kobo.593.1">,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
model.fit(train_ds, epochs=</span><span class="hljs-number"><span class="koboSpan" id="kobo.594.1">10</span></span><span class="koboSpan" id="kobo.595.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.596.1">When the model is training, you should see a loss metric and accuracy metric being reported for each epoch to help understand the progress of the training job.</span></p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="5"><span class="koboSpan" id="kobo.597.1">Now that the model has been trained, we need to validate its performance using the test dataset. </span><span class="koboSpan" id="kobo.597.2">In the following code, we are creating a </span><code class="inlineCode"><span class="koboSpan" id="kobo.598.1">test_ds</span></code><span class="koboSpan" id="kobo.599.1"> for the test data:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.600.1">images_test, labels_test = test
labels_test = labels_test.astype(np.int32)
images_test = images_test/</span><span class="hljs-number"><span class="koboSpan" id="kobo.601.1">256</span></span><span class="koboSpan" id="kobo.602.1">  
    
test_ds = tf.data.Dataset.from_tensor_slices((images_test, labels_test))
test_ds = train_ds.batch(</span><span class="hljs-number"><span class="koboSpan" id="kobo.603.1">32</span></span><span class="koboSpan" id="kobo.604.1">)
test_ds = train_ds.shuffle(</span><span class="hljs-number"><span class="koboSpan" id="kobo.605.1">30</span></span><span class="koboSpan" id="kobo.606.1">)
results = model.evaluate(test_ds)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.607.1">print</span></span><span class="koboSpan" id="kobo.608.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.609.1">"test loss, test acc:"</span></span><span class="koboSpan" id="kobo.610.1">, results)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.611.1">You can also use the standalone </span><code class="inlineCode"><span class="koboSpan" id="kobo.612.1">keras.metrics</span></code><span class="koboSpan" id="kobo.613.1"> to evaluate the model. </span><span class="koboSpan" id="kobo.613.2">Here, we are getting the prediction results and using </span><code class="inlineCode"><span class="koboSpan" id="kobo.614.1">tf.keras.metrics.Accuracy</span></code><span class="koboSpan" id="kobo.615.1"> to calculate the accuracy of predictions against the true values in </span><code class="inlineCode"><span class="koboSpan" id="kobo.616.1">test[1]</span></code><span class="koboSpan" id="kobo.617.1">:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.618.1">predictions = model.predict(test[</span><span class="hljs-number"><span class="koboSpan" id="kobo.619.1">0</span></span><span class="koboSpan" id="kobo.620.1">])
predicted_labels = np.argmax(predictions, axis=</span><span class="hljs-number"><span class="koboSpan" id="kobo.621.1">1</span></span><span class="koboSpan" id="kobo.622.1">)
m = tf.keras.metrics.Accuracy()
m.update_state(predicted_labels, test[</span><span class="hljs-number"><span class="koboSpan" id="kobo.623.1">1</span></span><span class="koboSpan" id="kobo.624.1">])
m.result().numpy()
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.625.1">You may</span><a id="_idIndexMarker502"/><span class="koboSpan" id="kobo.626.1"> notice the accuracy metrics in the previous step and this step are slightly different. </span><span class="koboSpan" id="kobo.626.2">That’s because the dataset samples used for evaluation are not exactly the same.</span></p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="7"><span class="koboSpan" id="kobo.627.1">To save the model, run the following code in a new cell. </span><span class="koboSpan" id="kobo.627.2">It will save the model in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.628.1">SavedModel</span></code><span class="koboSpan" id="kobo.629.1"> serialization format:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.630.1">model.save(</span><span class="hljs-string"><span class="koboSpan" id="kobo.631.1">"my_model.keras"</span></span><span class="koboSpan" id="kobo.632.1">)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.633.1">Open the </span><code class="inlineCode"><span class="koboSpan" id="kobo.634.1">model</span></code><span class="koboSpan" id="kobo.635.1"> directory. </span><span class="koboSpan" id="kobo.635.2">You should see that several files have been generated, such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.636.1">saved_model.pb</span></code><span class="koboSpan" id="kobo.637.1">, and several files under the </span><code class="inlineCode"><span class="koboSpan" id="kobo.638.1">variables</span></code><span class="koboSpan" id="kobo.639.1"> subdirectory.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.640.1">Great job! </span><span class="koboSpan" id="kobo.640.2">You have successfully installed the TensorFlow package in your local Jupyter environment and completed the training of a deep learning model. </span><span class="koboSpan" id="kobo.640.3">Through this process, you now possess the basic knowledge about TensorFlow and its capabilities for training deep learning models. </span><span class="koboSpan" id="kobo.640.4">Let’s shift our focus to PyTorch, another widely used and highly regarded deep learning library that excels in both experimental and production-grade ML model training.</span></p>
<h1 class="heading-1" id="_idParaDest-159"><span class="koboSpan" id="kobo.641.1">Understanding the PyTorch deep learning library</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.642.1">PyTorch </span><a id="_idIndexMarker503"/><span class="koboSpan" id="kobo.643.1">is an open-source ML library that was designed for deep learning using GPUs and CPUs. </span><span class="koboSpan" id="kobo.643.2">Initially released in 2016, it is a highly popular ML framework with a large following and many adoptions. </span><span class="koboSpan" id="kobo.643.3">Many technology companies, including</span><a id="_idIndexMarker504"/><span class="koboSpan" id="kobo.644.1"> tech giants</span><a id="_idIndexMarker505"/><span class="koboSpan" id="kobo.645.1"> such as </span><strong class="keyWord"><span class="koboSpan" id="kobo.646.1">Facebook</span></strong><span class="koboSpan" id="kobo.647.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.648.1">Microsoft</span></strong><span class="koboSpan" id="kobo.649.1">, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.650.1">Airbnb</span></strong><span class="koboSpan" id="kobo.651.1">, all </span><a id="_idIndexMarker506"/><span class="koboSpan" id="kobo.652.1">use PyTorch heavily for a wide range of deep learning use cases, such as computer vision </span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.653.1">and </span><strong class="keyWord"><span class="koboSpan" id="kobo.654.1">natural language processing</span></strong><span class="koboSpan" id="kobo.655.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.656.1">NLP</span></strong><span class="koboSpan" id="kobo.657.1">).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.658.1">PyTorch strikes a good balance of performance (using a C++ backend) with ease of use with default support for dynamic computational graphs and interoperability with the rest of the Python ecosystem. </span><span class="koboSpan" id="kobo.658.2">For example, with PyTorch, you can easily convert between NumPy arrays and PyTorch tensors. </span><span class="koboSpan" id="kobo.658.3">To allow for easy backward propagation, PyTorch has built-in support for automatically computing gradients, a vital requirement for gradient-based model optimization.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.659.1">The PyTorch library consists of several key modules, including </span><a id="_idIndexMarker508"/><span class="koboSpan" id="kobo.660.1">tensors, </span><strong class="keyWord"><span class="koboSpan" id="kobo.661.1">autograd</span></strong><span class="koboSpan" id="kobo.662.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.663.1">optimizer</span></strong><span class="koboSpan" id="kobo.664.1">, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.665.1">neural network</span></strong><span class="koboSpan" id="kobo.666.1">. </span><span class="koboSpan" id="kobo.666.2">Tensors are used to store and operate multidimensional arrays of numbers. </span><span class="koboSpan" id="kobo.666.3">You can perform various operations on tensors such as matrix multiplication, transpose, returning the max number, and dimensionality manipulation. </span><span class="koboSpan" id="kobo.666.4">PyTorch supports automatic gradient calculation with its Autograd module. </span><span class="koboSpan" id="kobo.666.5">When performing a forward pass, the Autograd module </span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.667.1">simultaneously builds up a function that computes the gradient. </span><span class="koboSpan" id="kobo.667.2">The Optimizer module</span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.668.1"> provides various algorithms such as SGD and Adam for updating model parameters. </span><span class="koboSpan" id="kobo.668.2">The Neural Network module</span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.669.1"> provides modules that represent different layers of a neural network such as the linear layer, embedding layer, and dropout layer. </span><span class="koboSpan" id="kobo.669.2">It also provides a list of loss functions that are commonly used for training deep learning models.</span></p>
<h2 class="heading-2" id="_idParaDest-160"><span class="koboSpan" id="kobo.670.1">Installing PyTorch</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.671.1">PyTorch can run </span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.672.1">on different operating systems, including Linux, Mac, and Windows. </span><span class="koboSpan" id="kobo.672.2">You can follow the instructions at </span><a href="https://pytorch.org/"><span class="url"><span class="koboSpan" id="kobo.673.1">https://pytorch.org/</span></span></a><span class="koboSpan" id="kobo.674.1"> to install</span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.675.1"> it in your environment. </span><span class="koboSpan" id="kobo.675.2">For example, you can use the </span><code class="inlineCode"><span class="koboSpan" id="kobo.676.1">pip install torch</span></code><span class="koboSpan" id="kobo.677.1"> command to install it in a Python-based environment.</span></p>
<h2 class="heading-2" id="_idParaDest-161"><span class="koboSpan" id="kobo.678.1">Core components of PyTorch</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.679.1">Similar to </span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.680.1">TensorFlow, PyTorch also supports the end-to-end ML workflow, from data preparation to model serving. </span><span class="koboSpan" id="kobo.680.2">The following diagram shows what different PyTorch modules are used to train and serve a PyTorch model:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.681.1"><img alt="Figure 5.6 – PyTorch modules for model training and serving " src="../Images/B20836_05_06.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.682.1">Figure 5.6: PyTorch modules for model training and serving</span></p>
<p class="normal"><span class="koboSpan" id="kobo.683.1">The steps involved in training a deep learning model are very similar to that of TensorFlow model training. </span><span class="koboSpan" id="kobo.683.2">We’ll look at the PyTorch-specific details in the following steps:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><strong class="keyWord"><span class="koboSpan" id="kobo.684.1">Preparing the dataset</span></strong><span class="koboSpan" id="kobo.685.1">: PyTorch provides two primitives for dataset and data loading management: </span><code class="inlineCode"><span class="koboSpan" id="kobo.686.1">torch.utils.data.Dataset</span></code><span class="koboSpan" id="kobo.687.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.688.1">torch.utils.data.Dataloader</span></code><span class="koboSpan" id="kobo.689.1">. </span><code class="inlineCode"><span class="koboSpan" id="kobo.690.1">Dataset</span></code><span class="koboSpan" id="kobo.691.1"> stores data samples and their corresponding labels, while </span><code class="inlineCode"><span class="koboSpan" id="kobo.692.1">Dataloader</span></code><span class="koboSpan" id="kobo.693.1"> wraps around the dataset and provides easy and efficient access to the data for model training. </span><code class="inlineCode"><span class="koboSpan" id="kobo.694.1">Dataloader</span></code><span class="koboSpan" id="kobo.695.1"> provides functions such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.696.1">shuffle</span></code><span class="koboSpan" id="kobo.697.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.698.1">batch_size</span></code><span class="koboSpan" id="kobo.699.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.700.1">prefetch_factor</span></code><span class="koboSpan" id="kobo.701.1"> to control how the data is loaded and fed to the training algorithm. </span><span class="koboSpan" id="kobo.701.2">Additionally, as the data in the dataset might need to be transformed before training is performed, </span><code class="inlineCode"><span class="koboSpan" id="kobo.702.1">Dataset</span></code><span class="koboSpan" id="kobo.703.1"> allows you to use a user-defined function to transform the data.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.704.1">Defining the neural network</span></strong><span class="koboSpan" id="kobo.705.1">: PyTorch provides a high-level abstraction for building neural networks with its </span><code class="inlineCode"><span class="koboSpan" id="kobo.706.1">torch.nn</span></code><span class="koboSpan" id="kobo.707.1"> class, which provides built-in support for different neural network layers such as linear layers and convolutional layers, as well as activation layers such as Sigmoid and ReLU. </span><span class="koboSpan" id="kobo.707.2">It also has container classes such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.708.1">nn.Sequential</span></code><span class="koboSpan" id="kobo.709.1"> for packaging different layers into a complete network. </span><span class="koboSpan" id="kobo.709.2">Existing neural networks can also be loaded into PyTorch for training.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.710.1">Defining the loss function</span></strong><span class="koboSpan" id="kobo.711.1">: PyTorch provides several built-in loss functions in its </span><code class="inlineCode"><span class="koboSpan" id="kobo.712.1">torch.nn</span></code><span class="koboSpan" id="kobo.713.1"> class, such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.714.1">nn.MSELoss</span></code><span class="koboSpan" id="kobo.715.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.716.1">nn.CrossEntropyLoss</span></code><span class="koboSpan" id="kobo.717.1">.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.718.1">Selecting the optimizer</span></strong><span class="koboSpan" id="kobo.719.1">: PyTorch provides several optimizers with its </span><code class="inlineCode"><span class="koboSpan" id="kobo.720.1">nn.optim</span></code><span class="koboSpan" id="kobo.721.1"> classes. </span><span class="koboSpan" id="kobo.721.2">Examples </span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.722.1">of optimizers include </span><code class="inlineCode"><span class="koboSpan" id="kobo.723.1">optim.SGD</span></code><span class="koboSpan" id="kobo.724.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.725.1">optim.Adam</span></code><span class="koboSpan" id="kobo.726.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.727.1">optim.RMSProp</span></code><span class="koboSpan" id="kobo.728.1">. </span><span class="koboSpan" id="kobo.728.2">All the optimizers have a </span><code class="inlineCode"><span class="koboSpan" id="kobo.729.1">step()</span></code><span class="koboSpan" id="kobo.730.1"> function that updates model parameters with each forward pass. </span><span class="koboSpan" id="kobo.730.2">There’s also a backward pass that calculates the gradients.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.731.1">Selecting the evaluation metrics</span></strong><span class="koboSpan" id="kobo.732.1">: The PyTorch </span><code class="inlineCode"><span class="koboSpan" id="kobo.733.1">ignite.metrics</span></code><span class="koboSpan" id="kobo.734.1"> class provides several evaluation metrics such as precision, recall, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.735.1">RootMeanSquaredError</span></code><span class="koboSpan" id="kobo.736.1"> for evaluating model performances. </span><span class="koboSpan" id="kobo.736.2">You can learn more about precision and recall at </span><a href="https://en.wikipedia.org/wiki/Precision_and_recall"><span class="url"><span class="koboSpan" id="kobo.737.1">https://en.wikipedia.org/wiki/Precision_and_recall</span></span></a><span class="koboSpan" id="kobo.738.1">. </span><span class="koboSpan" id="kobo.738.2">You can also use the scikit-learn metrics libraries to help evaluate models.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.739.1">Training the model</span></strong><span class="koboSpan" id="kobo.740.1">: Training a model in PyTorch involves three main steps in each training loop: forward pass the training data, backward pass the training data to calculate the gradient, and perform the optimizer step to update the gradient.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.741.1">Saving/loading the model</span></strong><span class="koboSpan" id="kobo.742.1">: The </span><code class="inlineCode"><span class="koboSpan" id="kobo.743.1">torch.save()</span></code><span class="koboSpan" id="kobo.744.1"> function saves a model in a serialized </span><code class="inlineCode"><span class="koboSpan" id="kobo.745.1">pickle</span></code><span class="koboSpan" id="kobo.746.1"> format. </span><span class="koboSpan" id="kobo.746.2">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.747.1">torch.load()</span></code><span class="koboSpan" id="kobo.748.1"> function loads a serialized model into memory for inference. </span><span class="koboSpan" id="kobo.748.2">A common convention is to save the files with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.749.1">.pth</span></code><span class="koboSpan" id="kobo.750.1"> or </span><code class="inlineCode"><span class="koboSpan" id="kobo.751.1">.pt</span></code><span class="koboSpan" id="kobo.752.1"> extension. </span><span class="koboSpan" id="kobo.752.2">You can also save multiple models into a single file.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.753.1">Model serving</span></strong><span class="koboSpan" id="kobo.754.1">: PyTorch comes with a model serving library called TorchServe, which we will cover in more detail in </span><em class="chapterRef"><span class="koboSpan" id="kobo.755.1">Chapter 7</span></em><span class="koboSpan" id="kobo.756.1">, </span><em class="italic"><span class="koboSpan" id="kobo.757.1">Open-Source ML Platforms</span></em><span class="koboSpan" id="kobo.758.1">.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.759.1">The PyTorch library supports large-scale distributed data processing and model training, which we will cover in more detail in </span><em class="chapterRef"><span class="koboSpan" id="kobo.760.1">Chapter 10</span></em><span class="koboSpan" id="kobo.761.1">, </span><em class="italic"><span class="koboSpan" id="kobo.762.1">Advanced ML Engineering</span></em><span class="koboSpan" id="kobo.763.1">. </span><span class="koboSpan" id="kobo.763.2">Like TensorFlow, PyTorch also offers an ecosystem of library packages for a wide range of ML problems, including ML privacy, adversarial robustness, video understanding, and drug discovery.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.764.1">Now that you have learned about the fundamentals of PyTorch, let’s get hands-on through a simple exercise.</span></p>
<h2 class="heading-2" id="_idParaDest-162"><span class="koboSpan" id="kobo.765.1">Hands-on exercise – building and training a PyTorch model</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.766.1">In this hands-on exercise, you will </span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.767.1">learn how to install the PyTorch library on your local machine and train a simple deep learning model using PyTorch. </span><span class="koboSpan" id="kobo.767.2">Launch a Jupyter notebook that you have previously installed on your machine. </span><span class="koboSpan" id="kobo.767.3">If you don’t remember how to do this, visit the </span><em class="italic"><span class="koboSpan" id="kobo.768.1">Hands-on lab</span></em><span class="koboSpan" id="kobo.769.1"> section of </span><em class="chapterRef"><span class="koboSpan" id="kobo.770.1">Chapter 3</span></em><span class="koboSpan" id="kobo.771.1">, </span><em class="italic"><span class="koboSpan" id="kobo.772.1">Exploring ML Algorithms</span></em><span class="koboSpan" id="kobo.773.1">. </span><span class="koboSpan" id="kobo.773.2">Now, let’s get started:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.774.1">Create a new folder called </span><code class="inlineCode"><span class="koboSpan" id="kobo.775.1">pytorch-lab</span></code><span class="koboSpan" id="kobo.776.1"> in your Jupyter Notebook environment and create a new notebook file called </span><code class="inlineCode"><span class="koboSpan" id="kobo.777.1">pytorch-lab1.ipynb</span></code><span class="koboSpan" id="kobo.778.1">. </span><span class="koboSpan" id="kobo.778.2">Run the following command in a cell to install PyTorch and the </span><code class="inlineCode"><span class="koboSpan" id="kobo.779.1">torchvision</span></code><span class="koboSpan" id="kobo.780.1"> package. </span><code class="inlineCode"><span class="koboSpan" id="kobo.781.1">torchvision</span></code><span class="koboSpan" id="kobo.782.1"> contains a set of computer vision models and datasets. </span><span class="koboSpan" id="kobo.782.2">We will use the pre-built MNIST dataset in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.783.1">torchvision</span></code><span class="koboSpan" id="kobo.784.1"> package for this exercise:
        </span><pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.785.1">!pip3 install torch
!pip3 install torchvision
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.786.1">The</span><a id="_idIndexMarker517"/><span class="koboSpan" id="kobo.787.1"> following sample code shows the previously mentioned main components. </span><span class="koboSpan" id="kobo.787.2">Be sure to run each code block in a separate Jupyter notebook cell for optimal readability.
    </span><p class="normal"><span class="koboSpan" id="kobo.788.1">First, we must import the necessary library packages and load the MNIST dataset from the </span><code class="inlineCode"><span class="koboSpan" id="kobo.789.1">torchvision</span></code><span class="koboSpan" id="kobo.790.1"> dataset class:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.791.1">import</span></span><span class="koboSpan" id="kobo.792.1"> numpy </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.793.1">as</span></span><span class="koboSpan" id="kobo.794.1"> np
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.795.1">import</span></span><span class="koboSpan" id="kobo.796.1"> matplotlib.pyplot </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.797.1">as</span></span><span class="koboSpan" id="kobo.798.1"> plt
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.799.1">import</span></span><span class="koboSpan" id="kobo.800.1"> torch
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.801.1">from</span></span><span class="koboSpan" id="kobo.802.1"> torchvision </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.803.1">import</span></span><span class="koboSpan" id="kobo.804.1"> datasets, transforms
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.805.1">from</span></span><span class="koboSpan" id="kobo.806.1"> torch </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.807.1">import</span></span><span class="koboSpan" id="kobo.808.1"> nn, optim
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((</span><span class="hljs-number"><span class="koboSpan" id="kobo.809.1">0.5</span></span><span class="koboSpan" id="kobo.810.1">,), (</span><span class="hljs-number"><span class="koboSpan" id="kobo.811.1">0.5</span></span><span class="koboSpan" id="kobo.812.1">,),)])
trainset = datasets.MNIST(</span><span class="hljs-string"><span class="koboSpan" id="kobo.813.1">'pytorch_data/train/'</span></span><span class="koboSpan" id="kobo.814.1">, download=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.815.1">True</span></span><span class="koboSpan" id="kobo.816.1">, train=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.817.1">True</span></span><span class="koboSpan" id="kobo.818.1">, transform=transform)
valset = datasets.MNIST(</span><span class="hljs-string"><span class="koboSpan" id="kobo.819.1">'pytorch_data/test/'</span></span><span class="koboSpan" id="kobo.820.1">, download=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.821.1">True</span></span><span class="koboSpan" id="kobo.822.1">, train=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.823.1">False</span></span><span class="koboSpan" id="kobo.824.1">, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=</span><span class="hljs-number"><span class="koboSpan" id="kobo.825.1">64</span></span><span class="koboSpan" id="kobo.826.1">, shuffle=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.827.1">True</span></span><span class="koboSpan" id="kobo.828.1">)
</span></code></pre>
</li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="3"><span class="koboSpan" id="kobo.829.1">Next, we must construct an MLP neural network for classification. </span><span class="koboSpan" id="kobo.829.2">This MLP network has two hidden layers with ReLU activation for the first and second layers. </span><span class="koboSpan" id="kobo.829.3">The MLP model takes an input size of </span><code class="inlineCode"><span class="koboSpan" id="kobo.830.1">784</span></code><span class="koboSpan" id="kobo.831.1">, which is the flattened dimension of a 28x28 image. </span><span class="koboSpan" id="kobo.831.2">The first hidden layer has </span><code class="inlineCode"><span class="koboSpan" id="kobo.832.1">128</span></code><span class="koboSpan" id="kobo.833.1"> nodes (neurons), while the second layer has </span><code class="inlineCode"><span class="koboSpan" id="kobo.834.1">64</span></code><span class="koboSpan" id="kobo.835.1"> nodes (neurons). </span><span class="koboSpan" id="kobo.835.2">The final layer has </span><code class="inlineCode"><span class="koboSpan" id="kobo.836.1">10</span></code><span class="koboSpan" id="kobo.837.1"> nodes because we have 10 class labels:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.838.1">model = nn.Sequential(nn.Linear(</span><span class="hljs-number"><span class="koboSpan" id="kobo.839.1">784</span></span><span class="koboSpan" id="kobo.840.1">, </span><span class="hljs-number"><span class="koboSpan" id="kobo.841.1">128</span></span><span class="koboSpan" id="kobo.842.1">),
                      nn.ReLU(),
                      nn.Linear(</span><span class="hljs-number"><span class="koboSpan" id="kobo.843.1">128</span></span><span class="koboSpan" id="kobo.844.1">, </span><span class="hljs-number"><span class="koboSpan" id="kobo.845.1">64</span></span><span class="koboSpan" id="kobo.846.1">),
                      nn.ReLU(),
                      nn.Linear(</span><span class="hljs-number"><span class="koboSpan" id="kobo.847.1">64</span></span><span class="koboSpan" id="kobo.848.1">, </span><span class="hljs-number"><span class="koboSpan" id="kobo.849.1">10</span></span><span class="koboSpan" id="kobo.850.1">))
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.851.1">Here’s a sample of the image data:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.852.1">images, labels = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.853.1">next</span></span><span class="koboSpan" id="kobo.854.1">(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.855.1">iter</span></span><span class="koboSpan" id="kobo.856.1">(trainloader))
pixels = images[</span><span class="hljs-number"><span class="koboSpan" id="kobo.857.1">0</span></span><span class="koboSpan" id="kobo.858.1">][</span><span class="hljs-number"><span class="koboSpan" id="kobo.859.1">0</span></span><span class="koboSpan" id="kobo.860.1">]
plt.imshow(pixels, cmap=</span><span class="hljs-string"><span class="koboSpan" id="kobo.861.1">'gray'</span></span><span class="koboSpan" id="kobo.862.1">)
plt.show()
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.863.1">Now, we </span><a id="_idIndexMarker518"/><span class="koboSpan" id="kobo.864.1">must define a </span><strong class="keyWord"><span class="koboSpan" id="kobo.865.1">cross-entropy loss function</span></strong><span class="koboSpan" id="kobo.866.1"> for the</span><a id="_idIndexMarker519"/><span class="koboSpan" id="kobo.867.1"> training process since we want to measure the error in the probability distribution for all the labels. </span><span class="koboSpan" id="kobo.867.2">Internally, PyTorch’s </span><code class="inlineCode"><span class="koboSpan" id="kobo.868.1">CrossEntropyLoss</span></code><span class="koboSpan" id="kobo.869.1"> automatically applies a </span><code class="inlineCode"><span class="koboSpan" id="kobo.870.1">softmax</span></code><span class="koboSpan" id="kobo.871.1"> to the network output to calculate the probability distributions for the different classes. </span><span class="koboSpan" id="kobo.871.2">For the optimizer, we have chosen the Adam optimizer with a learning rate of </span><code class="inlineCode"><span class="koboSpan" id="kobo.872.1">0.003</span></code><span class="koboSpan" id="kobo.873.1">. </span><span class="koboSpan" id="kobo.873.2">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.874.1">view()</span></code><span class="koboSpan" id="kobo.875.1"> function flattens the two-dimensional input array (28x28) into a one-dimensional vector since our neural network takes a one-dimensional vector input:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.876.1">criterion = nn.CrossEntropyLoss()
images = images.view(images.shape[</span><span class="hljs-number"><span class="koboSpan" id="kobo.877.1">0</span></span><span class="koboSpan" id="kobo.878.1">], -</span><span class="hljs-number"><span class="koboSpan" id="kobo.879.1">1</span></span><span class="koboSpan" id="kobo.880.1">)
output = model(images)
loss = criterion(output, labels)
optimizer = optim.Adam(model.parameters(), lr=</span><span class="hljs-number"><span class="koboSpan" id="kobo.881.1">0.003</span></span><span class="koboSpan" id="kobo.882.1">)
</span></code></pre>
<div class="note">
<p class="normal"><span class="koboSpan" id="kobo.883.1">Learning rate is a hyperparameter that determines the size of the steps taken during the optimization process.</span></p>
</div></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="6"><span class="koboSpan" id="kobo.884.1">Now, let’s </span><a id="_idIndexMarker520"/><span class="koboSpan" id="kobo.885.1">start the training process. </span><span class="koboSpan" id="kobo.885.2">We are going to run </span><code class="inlineCode"><span class="koboSpan" id="kobo.886.1">15 </span></code><span class="koboSpan" id="kobo.887.1">epochs. </span><span class="koboSpan" id="kobo.887.2">Unlike the TensorFlow Keras API, where you just call a </span><code class="inlineCode"><span class="koboSpan" id="kobo.888.1">fit()</span></code><span class="koboSpan" id="kobo.889.1"> function to start the training, PyTorch requires you to build a training loop and specifically run the forward pass (</span><code class="inlineCode"><span class="koboSpan" id="kobo.890.1">model (images)</span></code><span class="koboSpan" id="kobo.891.1">), run the backward pass to learn (</span><code class="inlineCode"><span class="koboSpan" id="kobo.892.1">loss.backward()</span></code><span class="koboSpan" id="kobo.893.1">), update the model weights (</span><code class="inlineCode"><span class="koboSpan" id="kobo.894.1">optimizer.step()</span></code><span class="koboSpan" id="kobo.895.1">), and then calculate the total loss and the average loss. </span><span class="koboSpan" id="kobo.895.2">For each training step, </span><code class="inlineCode"><span class="koboSpan" id="kobo.896.1">trainloader</span></code><span class="koboSpan" id="kobo.897.1"> returns one batch (a batch size of </span><code class="inlineCode"><span class="koboSpan" id="kobo.898.1">64</span></code><span class="koboSpan" id="kobo.899.1">) of training data samples. </span><span class="koboSpan" id="kobo.899.2">Each training sample is flattened into a 784-long vector. </span><span class="koboSpan" id="kobo.899.3">The optimizer is reset with zeros for each training step:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.900.1">epochs = </span><span class="hljs-number"><span class="koboSpan" id="kobo.901.1">15</span></span>
<span class="hljs-key ord"><span class="koboSpan" id="kobo.902.1">for</span></span><span class="koboSpan" id="kobo.903.1"> e </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.904.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.905.1">range</span></span><span class="koboSpan" id="kobo.906.1">(epochs):
    running_loss = </span><span class="hljs-number"><span class="koboSpan" id="kobo.907.1">0</span></span>
<span class="hljs-key ord"><span class="koboSpan" id="kobo.908.1">for</span></span><span class="koboSpan" id="kobo.909.1"> images, labels </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.910.1">in</span></span><span class="koboSpan" id="kobo.911.1"> trainloader:
        images = images.view(images.shape[</span><span class="hljs-number"><span class="koboSpan" id="kobo.912.1">0</span></span><span class="koboSpan" id="kobo.913.1">], -</span><span class="hljs-number"><span class="koboSpan" id="kobo.914.1">1</span></span><span class="koboSpan" id="kobo.915.1">)
        optimizer.zero_grad()
        output = model(images)
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.916.1">else</span></span><span class="koboSpan" id="kobo.917.1">:
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.918.1">print</span></span><span class="koboSpan" id="kobo.919.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.920.1">"Epoch {} - Training loss: {}"</span></span><span class="koboSpan" id="kobo.921.1">.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.922.1">format</span></span><span class="koboSpan" id="kobo.923.1">(e, running_loss/</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.924.1">len</span></span><span class="koboSpan" id="kobo.925.1">(trainloader)))
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.926.1">When the training code runs, it should print out the average loss for each epoch.</span></p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="7"><span class="koboSpan" id="kobo.927.1">To test the</span><a id="_idIndexMarker521"/><span class="koboSpan" id="kobo.928.1"> accuracy using the validation data, we must run the validation dataset through the trained model and use scikit-learn</span><code class="inlineCode"><span class="koboSpan" id="kobo.929.1">.metrics.accuracy_score()</span></code><span class="koboSpan" id="kobo.930.1"> to calculate the model’s accuracy:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.931.1">valloader = torch.utils.data.DataLoader(valset, batch_size=valset.data.shape[</span><span class="hljs-number"><span class="koboSpan" id="kobo.932.1">0</span></span><span class="koboSpan" id="kobo.933.1">], shuffle=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.934.1">True</span></span><span class="koboSpan" id="kobo.935.1">)
val_images, val_labels = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.936.1">next</span></span><span class="koboSpan" id="kobo.937.1">(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.938.1">iter</span></span><span class="koboSpan" id="kobo.939.1">(valloader))
val_images = val_images.view(val_images.shape[</span><span class="hljs-number"><span class="koboSpan" id="kobo.940.1">0</span></span><span class="koboSpan" id="kobo.941.1">], -</span><span class="hljs-number"><span class="koboSpan" id="kobo.942.1">1</span></span><span class="koboSpan" id="kobo.943.1">)
predictions = model (val_images)
predicted_labels = np.argmax(predictions.detach().numpy(), axis=</span><span class="hljs-number"><span class="koboSpan" id="kobo.944.1">1</span></span><span class="koboSpan" id="kobo.945.1">)
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.946.1">from</span></span><span class="koboSpan" id="kobo.947.1"> sklearn.metrics </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.948.1">import</span></span><span class="koboSpan" id="kobo.949.1"> accuracy_score
accuracy_score(val_labels.detach().numpy(), predicted_labels)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.950.1">Finally, we must save the model to a file:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.951.1">torch.save(model, </span><span class="hljs-string"><span class="koboSpan" id="kobo.952.1">'./model/my_mnist_model.pt'</span></span><span class="koboSpan" id="kobo.953.1">)
</span></code></pre>
</li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.954.1">Congratulations! </span><span class="koboSpan" id="kobo.954.2">You have successfully installed PyTorch in your local Jupyter environment and trained a deep learning PyTorch model.</span></p>
<h1 class="heading-1" id="_idParaDest-163"><span class="koboSpan" id="kobo.955.1">How to choose between TensorFlow and PyTorch</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.956.1">TensorFlow </span><a id="_idIndexMarker522"/><span class="koboSpan" id="kobo.957.1">and PyTorch are the two most popular frameworks in the domain of deep learning. </span><span class="koboSpan" id="kobo.957.2">So, a pertinent question</span><a id="_idIndexMarker523"/><span class="koboSpan" id="kobo.958.1"> arises: How does one make an informed choice between the two? </span><span class="koboSpan" id="kobo.958.2">To help answer this question, let’s do a quick comparative analysis of these frameworks:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.959.1">Ease of use</span></strong><span class="koboSpan" id="kobo.960.1">: PyTorch is generally considered more user-friendly and Pythonic. </span><span class="koboSpan" id="kobo.960.2">The control flow feels closer to native Python and the dynamic computation graphs of PyTorch make it easier to debug and iterate compared to TensorFlow’s static graphs. </span><span class="koboSpan" id="kobo.960.3">However, eager execution support in TensorFlow 2.0 helps close this gap. </span><span class="koboSpan" id="kobo.960.4">PyTorch is also considered more object-oriented than TensorFlow.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.961.1">Community popularity</span></strong><span class="koboSpan" id="kobo.962.1">: Both frameworks enjoy robust community support and are highly popular. </span><span class="koboSpan" id="kobo.962.2">TensorFlow initially had a lead; however, PyTorch has caught up in popularity in recent years, according to the Google Trends report. </span><span class="koboSpan" id="kobo.962.3">PyTorch is more widely adopted within the research community and dominates the implementation of research papers.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.963.1">Model availability</span></strong><span class="koboSpan" id="kobo.964.1">: TensorFlow has the TensorFlow Model Garden, which hosts a collection of models utilizing TensorFlow APIs, covering various ML tasks such as computer vision, NLP, and recommendation. </span><span class="koboSpan" id="kobo.964.2">It also features TensorFlow Hub, offering a collection of pre-trained models ready for deployment or fine-tuning across a wide range of ML tasks. </span><span class="koboSpan" id="kobo.964.3">Similarly, PyTorch has PyTorch Hub, a library integrated into PyTorch that provides easy access to a broad array of pre-trained models for computer vision, NLP, and more.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.965.1">Deployment</span></strong><span class="koboSpan" id="kobo.966.1">: Both frameworks are suitable for the production deployment of ML models. </span><span class="koboSpan" id="kobo.966.2">TensorFlow is considered to have a more comprehensive model deployment stack with TensorFlow Serving, TensorFlow Lite for mobile and edge devices, and TensorFlow.js for browser deployment. </span><span class="koboSpan" id="kobo.966.3">TensorFlow Extended is an end-to-end model deployment platform encompassing model validation, monitoring, and explanation. </span><span class="koboSpan" id="kobo.966.4">PyTorch offers TorchServe, a model-serving framework for PyTorch models, and PyTorch Mobile for deploying models on iOS and Android devices. </span><span class="koboSpan" id="kobo.966.5">PyTorch relies more on third-party solutions for end-to-end integration in the deployment process.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.967.1">To conclude, both frameworks provide comparable capabilities throughout the entire ML lifecycle, accommodating similar use cases. </span><span class="koboSpan" id="kobo.967.2">If your organization has already committed to</span><a id="_idIndexMarker524"/><span class="koboSpan" id="kobo.968.1"> either TensorFlow</span><a id="_idIndexMarker525"/><span class="koboSpan" id="kobo.969.1"> or PyTorch, it is advisable to proceed with that decision. </span><span class="koboSpan" id="kobo.969.2">However, for those embarking on the initial stages, PyTorch might offer a more accessible starting point owing to its ease of use.</span></p>
<h1 class="heading-1" id="_idParaDest-164"><span class="koboSpan" id="kobo.970.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.971.1">In this chapter, we have explored several popular open-source ML library packages, including scikit-learn, Spark ML, TensorFlow, and PyTorch. </span><span class="koboSpan" id="kobo.971.2">By now, you should have a good understanding of the fundamental components of these libraries and how they can be leveraged to train ML models. </span><span class="koboSpan" id="kobo.971.3">Additionally, we have delved into the TensorFlow and PyTorch frameworks to construct artificial neural networks, train deep learning models, and save these models to files. </span><span class="koboSpan" id="kobo.971.4">These model files can then be utilized in model-serving environments to make predictions.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.972.1">In the next chapter, we will delve into Kubernetes and its role as a foundational infrastructure for constructing open-source ML solutions.</span></p>
<h1 class="heading-1"><span class="koboSpan" id="kobo.973.1">Join our community on Discord</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.974.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
<p class="normal"><a href="https://packt.link/mlsah "><span class="url"><span class="koboSpan" id="kobo.975.1">https://packt.link/mlsah</span></span></a></p>
<p class="normal"><span class="koboSpan" id="kobo.976.1"><img alt="" role="presentation" src="../Images/QR_Code70205728346636561.png"/></span></p>
</div>
</body></html>