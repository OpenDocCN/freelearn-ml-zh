- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing Good Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a Kaggle competition, in the heat of modeling and submitting results, it
    may seem enough to take at face value the results you get back from the leaderboard.
    In the end, you may think that what counts in a competition is your ranking. This
    is a common error that is made repeatedly in competitions. In actual fact, you
    won’t know what the actual leaderboard (the private one) looks like until after
    the competition has closed, and trusting the public part of it is not advisable
    because it is quite often misleading.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will introduce you to the importance of **validation**
    in data competitions. You will learn about:'
  prefs: []
  type: TYPE_NORMAL
- en: What overfitting is and how a public leaderboard can be misleading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dreadful shake-ups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The different kinds of validation strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to spot and leverage leakages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What your strategies should be when choosing your final submissions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring your performances when modeling and distinguishing when overfitting
    happens is a key competency not only in data science competitions but in all data
    science projects. Validating your models properly is one of the most important
    skills that you can learn from a Kaggle competition and that you can resell in
    the professional world.
  prefs: []
  type: TYPE_NORMAL
- en: Snooping on the leaderboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we previously described, in each competition, Kaggle divides the test set
    into a **public part**, which is visualized on the ongoing leaderboard, and a
    **private part**, which will be used to calculate the final scores. These test
    parts are usually randomly determined (although in time series competitions, they
    are determined based on time) and the entire test set is released without any
    distinction made between public and private.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, in order to avoid the scrutinizing of test data in certain competitions,
    Kaggle has even held back the test data, providing only some examples of it and
    replacing them with the real test set when the submission is made. These are called
    **Code** competitions because you are not actually providing the predictions themselves,
    but a Notebook containing the code to generate them.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a submission derived from a model will cover the entire test set,
    but only the public part will immediately be scored, leaving the scoring of the
    private part until after the competition has closed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given this, three considerations arise:'
  prefs: []
  type: TYPE_NORMAL
- en: In order for a competition to work properly, training data and test data should
    be from **the same distribution**. Moreover, the private and public parts of the
    test data should resemble each other in terms of distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if the training and test data are apparently from the same distribution,
    the **lack of sufficient examples** in either set could make it difficult to obtain
    aligned results between the training data and the public and private test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The public test data should be regarded as a holdout test in a data science
    project: to be used only for final validation. Hence, it should not be queried
    much in order to avoid what is called **adaptive overfitting**, which implies
    a model that works well on a specific test set but underperforms on others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping in mind these three considerations is paramount to understanding the
    dynamics of a competition. In most competitions, there are always quite a few
    questions in the discussion forums about how the training, public, and private
    test data relate to each other, and it is quite common to see submissions of hundreds
    of solutions that have only been evaluated based on their efficacy on the public
    leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also common to hear discussions about **shake-ups** that revolutionize
    the rankings. They are, in fact, a rearranging of the final rankings that can
    disappoint many who previously held better positions on the public leaderboard.
    Anecdotally, shake-ups are commonly attributed to differences between the training
    and test set or between the private and public parts of the test data. They are
    measured *ex ante* based on how competitors have seen their expected local scores
    correlate with the leaderboard feedback and *ex post* by a series of analyses
    based on two figures:'
  prefs: []
  type: TYPE_NORMAL
- en: A general shake-up figure based on `mean(abs(private_rank-public_rank)/number_of_teams)`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A top leaderboard shake-up figure, taking into account only the top 10% of public
    ranks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These *ex post* figures were first devised by *Steve Donoho* ([https://www.kaggle.com/breakfastpirate](https://www.kaggle.com/breakfastpirate))
    who compiled a ranking of the worst Kaggle shake-ups (see [https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49106#278831](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49106#278831)).
    They are nowadays easily available, recreated by many Notebooks based on the Meta
    Kaggle dataset we discussed in *Chapter 5*, *Competition Tasks and Metrics* (see
    [https://www.kaggle.com/jtrotman/meta-kaggle-competition-shake-up](https://www.kaggle.com/jtrotman/meta-kaggle-competition-shake-up)).
    For instance, by consulting these figures, you may find out how dreadful the *RSNA
    Intracranial Hemorrhage Detection* competition was for many because of its shake-ups,
    especially in the top positions.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, aside from an *ex post* evaluation, there are quite a few lessons
    that we can get from previous shake-ups that can help you in your Kaggle competitions.
    A few researchers from UC Berkeley think so too. In their paper presented at NIPS
    2019, Roelofs, Fridovich-Keil et al. study in detail a few thousand Kaggle competitions
    to gain insight into the public-private leaderboard dynamics in Kaggle competitions.
    Although they focus on a limited subset of competitions (120, above a certain
    number of participants, focused on binary classification), they obtained some
    interesting findings:'
  prefs: []
  type: TYPE_NORMAL
- en: There is little adaptive overfitting; in other words, public standings usually
    do hold in the unveiled private leaderboard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most shake-ups are due to random fluctuations and overcrowded rankings where
    competitors are too near to each other, and any slight change in the performance
    in the private test sets causes major changes in the rankings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shake-ups happen when the training set is very small or the training data is
    not **independent and identically distributed** (**i.i.d.**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The full paper, Roelofs, R., Fridovich-Keil, S. et al. *A meta-analysis of
    overfitting in machine learning*. Proceedings of the 33^(rd) International Conference
    on Neural Information Processing Systems. 2019, can be found at this link: [https://papers.nips.cc/paper/2019/file/ee39e503b6bedf0c98c388b7e8589aca-Paper.pdf](https://papers.nips.cc/paper/2019/file/ee39e503b6bedf0c98c388b7e8589aca-Paper.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our long experience of Kaggle competitions, however, we have seen quite
    a lot of problems with adaptive overfitting since the beginning. For instance,
    you can read *Greg Park*’s analysis of one of the first competitions we ever took
    part in: [http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/](http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/).
    Since this is quite a common and persistent problem for many Kagglers, we suggest
    a strategy that is a bit more sophisticated than simply following what happens
    on the public leaderboard:'
  prefs: []
  type: TYPE_NORMAL
- en: Always build reliable cross-validation systems for local scoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always try to control non-i.i.d distributions using the best validation scheme
    dictated by the situation. Unless clearly stated in the description of the competition,
    it is not an easy task to spot non-i.i.d. distributions, but you can get hints
    from discussion or by experimenting using stratified validation schemes (when
    stratifying according to a certain feature, the results improve decisively, for
    instance).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlate local scoring with the public leaderboard in order to figure out whether
    or not they go in the same direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test using adversarial validation, revealing whether or not the test distribution
    is similar to the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make your solutions more robust using ensembling, especially if you are working
    with small datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we are going to explore each of these ideas (except
    for ensembling, which is the topic of a future chapter) and provide you with all
    the best tools and strategies to obtain the best results, especially on the private
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of validation in competitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you think about a competition carefully, you can imagine it as a huge system
    of experiments. Whoever can create the most systematic and efficient way to run
    these experiments wins.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, in spite of all your theoretical knowledge, you will be in competition
    with the hundreds or thousands of data professionals who have more or less the
    same competencies as you.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, they will be using exactly the same data as you and roughly the
    same tools for learning from the data (TensorFlow, PyTorch, Scikit-learn, and
    so on). Some will surely have better access to computational resources, although
    the availability of Kaggle Notebooks and generally decreasing cloud computing
    prices mean the gap is no longer so wide. Consequently, if you look at differences
    in knowledge, data, models, and available computers, you won’t find many discriminating
    factors between you and the other competitors that could explain huge performance
    differences in a competition. Yet, some participants consistently outperform others,
    implying there is some underlying success factor.
  prefs: []
  type: TYPE_NORMAL
- en: In interviews and meet-ups, some Kagglers describe this success factor as “grit,”
    some others as “trying everything,” some others again as a “willingness to put
    everything you have into a competition.” These may sound a bit obscure and magic.
    Instead, we call it **systematic experimentation**. In our opinion, the key to
    successful participation resides in the number of experiments you conduct and
    the way you run all of them. The more experiments you undertake, the more chances
    you will have to crack the problem better than other participants. This number
    certainly depends on a few factors, such as the time you have available, your
    computing resources (the faster the better, but as we previously mentioned, this
    is not such a strong differentiator *per se*), your team size, and their involvement
    in the task. This aligns with the commonly reported grit and engagement as keys
    for success.
  prefs: []
  type: TYPE_NORMAL
- en: However, these are not the only factors affecting the result. You have to take
    into account that the way you run your experiments also has an impact. *Fail fast
    and learn from it* is an important factor in a competition. Of course, you need
    to reflect carefully both when you fail and when you succeed in order to learn
    something from your experiences, or your competition will just turn into a random
    sequence of attempts in the hope of picking the right solution.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, *ceteris paribus*, having a proper **validation strategy** is the
    great discriminator between successful Kaggle competitors and those who just overfit
    the leaderboard and end up in lower-than-expected rankings after a competition.
  prefs: []
  type: TYPE_NORMAL
- en: '**Validation** is the method you use to correctly evaluate the errors that
    your model produces and to measure how its performance improves or decreases based
    on your experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: Generally, the impact of choosing proper validation is too often overlooked
    in favor of more quantitative factors, such as having the latest, most powerful
    GPU or a larger team producing submissions.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, if you count only on the firepower of experiments and their results
    on the leaderboard, it will be like “throwing mud at the wall and hoping something
    will stick” (see [http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/](http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/)).
    Sometimes such a strategy will work, but most often it won’t, because you will
    miss important opportunities to experiment in the right direction, and you won’t
    even be able to see the shining gem you managed to produce in the middle of all
    that mud. For instance, if you concentrate too much on trying your luck on the
    public leaderboard using a random, unsystematic strategy, even if you produce
    great solutions, you may end up not choosing your final submission correctly and
    missing the best scoring one on the private leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: Having a proper validation strategy can help you decide which of your models
    should be submitted for ranking on the private test set. Though the temptation
    to submit your top public leaderboard models may be high, *always consider your
    own validation scores*. For your final submissions, depending on the situation
    and whether or not you trust the leaderboard, choose your best model based on
    the leaderboard and your best based on your local validation results. If you don’t
    trust the leaderboard (especially when the training sample is small or the examples
    are non-i.i.d.), submit models that have two of the best validation scores, picking
    two very different models or ensembles. In this way, you will reduce the risk
    of choosing solutions that won’t perform on the private test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having pointed out the importance of having a method of experimenting, what
    is left is all a matter of the practicalities of validation. In fact, when you
    model a solution, you take a series of interrelated decisions:'
  prefs: []
  type: TYPE_NORMAL
- en: How to process your data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What model to apply
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to change the model’s architecture (especially true for deep learning models)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to set the model’s hyperparameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to post-process the predictions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even if the public leaderboard is perfectly correlated with the private one,
    the limited number of daily submissions (a limitation present in all competitions)
    prevents you from even scratching the surface of possible tests that you could
    do in all the aforementioned areas. Having a proper validation system tells you
    beforehand if what you are doing could work on the leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Dmitry_Larko.png)'
  prefs: []
  type: TYPE_IMG
- en: Dmitry Larko
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/dmitrylarko](https://www.kaggle.com/dmitrylarko)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dmitry Larko** is a Kaggle Competition Grandmaster and the chief data scientist
    at H2O.ai. He has over a decade of experience in ML and data science. He discovered
    Kaggle in December 2012 and participated in his first competition a few months
    later. He is a strong advocate of validation in Kaggle competitions, as he told
    us in his interview.'
  prefs: []
  type: TYPE_NORMAL
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  prefs: []
  type: TYPE_NORMAL
- en: '*I have mostly participated in competitions for tabular datasets but also enjoy
    competitions for computer vision.*'
  prefs: []
  type: TYPE_NORMAL
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  prefs: []
  type: TYPE_NORMAL
- en: '*I always try to start simple and build a submission pipeline for smaller/simpler
    models first. A major step here is to create a proper validation scheme so you
    can validate your ideas in a robust way. Also, it is always a good idea to spend
    as much time as you can looking at the data and analyzing it.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*In my day-to-day work, I am building an AutoML platform, so a lot of things
    I try on Kaggle end up being implemented as a part of this platform.*'
  prefs: []
  type: TYPE_NORMAL
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  prefs: []
  type: TYPE_NORMAL
- en: '*Nothing comes to my mind, and it doesn’t matter, because what is technically
    challenging for me could be a piece of cake for somebody else. Technical challenges
    are not that important; what’s important is to remember that a competition is
    somewhat like a marathon, not a sprint. Or you can see it as a marathon of sprints
    if you like. So, it is important not to get exhausted, sleep well, exercise, and
    take a walk in a park to regenerate your brain for new ideas. To win a Kaggle
    competition, you will need all your creativity and expertise and sometimes even
    a bit of luck.*'
  prefs: []
  type: TYPE_NORMAL
- en: Has Kaggle helped you in your career? If so, how?
  prefs: []
  type: TYPE_NORMAL
- en: '*I got my current job thanks to the fact I was a Kaggle Competition Grandmaster.
    For my current employer, this fact was evidence enough of my expertise in the
    field.*'
  prefs: []
  type: TYPE_NORMAL
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  prefs: []
  type: TYPE_NORMAL
- en: '*Mostly they overlook the right validation scheme and follow the feedback from
    the public leaderboard. That ends badly in most cases, leading to something known
    as a “shake-up” on Kaggle.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, they rush to skip exploratory data analysis and build models right away,
    which leads to simplistic solutions and mediocre leaderboard scores.*'
  prefs: []
  type: TYPE_NORMAL
- en: What mistakes have you made in competitions in the past?
  prefs: []
  type: TYPE_NORMAL
- en: '*My main mistake is really the same that an inexperienced person will make
    – following the leaderboard score and not my internal validation. Every time I
    decided to do so, it cost me several places on the leaderboard.*'
  prefs: []
  type: TYPE_NORMAL
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: '*That would be the usual suspects. For tabular data: LightGBM, XGBoost, CatBoost;
    for deep learning: PyTorch, PyTorch-Lightning, timm; and Scikit-learn for everyone.*'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  prefs: []
  type: TYPE_NORMAL
- en: '*Start simple, always validate; believe in your validation score and not the
    leaderboard score.*'
  prefs: []
  type: TYPE_NORMAL
- en: Bias and variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A good validation system helps you with metrics that are more reliable than
    the error measures you get from your training set. In fact, metrics obtained on
    the training set are affected by the capacity and complexity of each model. You
    can think of the **capacity** of a model as its memory that it can use to learn
    from data.
  prefs: []
  type: TYPE_NORMAL
- en: Each model has a set of internal parameters that help the model to record the
    patterns taken from the data. Every model has its own skills for acquiring patterns,
    and some models will spot certain rules or associations whereas others will spot
    others. As a model extracts patterns from data, it records them in its “memory.”
  prefs: []
  type: TYPE_NORMAL
- en: 'You also hear about the capacity or expressiveness of a model as a matter of
    **bias and variance**. In this case, the bias and variance of a model refer to
    the predictions, but the underlying principle is strictly related to the expressiveness
    of a model. Models can be reduced to mathematical functions that map an input
    (the observed data) to a result (the predictions). Some mathematical functions
    are more complex than others, in the number of internal parameters they have and
    in the ways they use them:'
  prefs: []
  type: TYPE_NORMAL
- en: If the mathematical function of a model is not complex or expressive enough
    to capture the complexity of the problem you are trying to solve, we talk of **bias**,
    because your predictions will be limited (“biased”) by the limits of the model
    itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the mathematical function at the core of a model is too complex for the problem
    at hand, we have a **variance** problem, because the model will record more details
    and noise in the training data than needed and its predictions will be deeply
    influenced by them and become erratic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nowadays, given the advances in machine learning and the available computation
    resources, the problem is always due to variance, since deep neural networks and
    gradient boosting, the most commonly used solutions, often have a mathematical
    expressiveness that exceeds what most of the problems you will face need in order
    to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: When all the useful patterns that a certain model can extract have been captured,
    if the model has not exhausted its capacity, it will then start memorizing data
    characteristics and signals that are unrelated to the prediction (usually referred
    to as **noise**). While the initially extracted patterns will help the model to
    generalize to a test dataset and predict more correctly, not everything that it
    learns specifically about the training set will help; instead, it may damage its
    performance. The process of learning elements of the training set that have no
    generalization value is commonly called **overfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: The core purpose of validation is to explicitly define a score or loss value
    that separates the generalizable part of that value from that due to overfitting
    the training set characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the **validation loss**. You can see the situation visualized in the
    following figure of learning curves:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Learning more from the training data does not always mean learning
    to predict'
  prefs: []
  type: TYPE_NORMAL
- en: If you graph the loss measure on the *y*-axis against some measure of learning
    effort of the model (this could be epochs for neural networks, or rounds for gradient
    boosting) on the *x*-axis, you will notice that learning always seems to happen
    on the training dataset, but this is not always true on other data.
  prefs: []
  type: TYPE_NORMAL
- en: The same thing happens even if you change the hyperparameters, process the data,
    or decide on a different model altogether. The curves will change shape, but you’ll
    always have a sweet point where overfitting starts. That point can be different
    across models and between the various choices that you make in your modeling efforts.
    If you have properly computed the point when overfitting starts thanks to a correct
    validation strategy, your model’s performance will surely correlate with the leaderboard
    results (both public and private), and your validation metrics will provide you
    with a proxy to evaluate your work without making any submissions.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can hear about overfitting at various levels:'
  prefs: []
  type: TYPE_NORMAL
- en: At the level of the training data, when you use a model that is too complex
    for the problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the level of the validation set itself, when you tune your model too much
    with respect to a specific validation set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the level of the public leaderboard, when your results are far from what
    you would expect from your training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the level of the private leaderboard, when in spite of the good results on
    the public leaderboard, your private scores will be disappointing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though slightly different in meaning, they all equally imply that your model
    is not generalizable, as we have described in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Trying different splitting strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As previously discussed, the validation loss is based on a data sample that
    is not part of the training set. It is an empirical measure that tells you how
    good your model is at predicting, and a more correct one than the score you get
    from your training, which will tell you mostly how much your model has memorized
    the training data patterns. Correctly choosing the data sample you use for validation
    constitutes your validation strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize the strategies for validating your model and measuring its performance
    correctly, you have a couple of choices:'
  prefs: []
  type: TYPE_NORMAL
- en: The first choice is to **work with a holdout system**, incurring the risk of
    not properly choosing a representative sample of the data or overfitting to your
    validation holdout.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second option is to **use a probabilistic approach** and rely on a series
    of samples to draw your conclusions on your models. Among the probabilistic approaches,
    you have cross-validation, **leave-one-out** (**LOO**), and bootstrap. Among the
    cross-validation strategies, there are different nuances depending on the sampling
    strategies you take based on the characteristic of your data (simple random sampling,
    stratified sampling, sampling by groups, time sampling).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What all these strategies have in common is that they are **sampling strategies**.
    It means that they help you to infer a general measure (the performance of your
    model) based on a small part of your data, randomly selected. Sampling is at the
    root of statistics and it is not an exact procedure because, based on your sampling
    method, your available data, and the randomness of picking up certain cases as
    part of your sample, you will experience a certain degree of error.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you rely on a biased sample, your evaluation metric may be
    estimated incorrectly (over- or under-estimated). However, if properly designed
    and implemented, sampling strategies generally provide you with a good estimate
    of your general measure.
  prefs: []
  type: TYPE_NORMAL
- en: The other aspect that all these strategies have in common is that they are **partitions**,
    which divide cases in an exclusive way as either part of the training or part
    of the validation. In fact, as we discussed, since most models have a certain
    memorization capability, using the same cases in both training and validation
    leads to inflated estimates because it allows the model to demonstrate its memorization
    abilities; instead, we want it to be evaluated on its ability to derive patterns
    and functions that work on *unseen* examples.
  prefs: []
  type: TYPE_NORMAL
- en: The basic train-test split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first strategy that we will analyze is the **train-test split**. In this
    strategy, you sample a portion of your training set (also known as the **holdout**)
    and you use it as a test set for all the models that you train using the remaining
    part of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The great advantage of this strategy is that it is very simple: you pick up
    a part of your data and you check your work on that part. You usually split the
    data 80/20 in favor of the training partition. In Scikit-learn, it is implemented
    in the `train_test_split` function. We’ll draw your attention to a couple of aspects
    of the method:'
  prefs: []
  type: TYPE_NORMAL
- en: When you have large amounts of data, you can expect that the test data you extract
    is similar to (representative of) the original distribution on the entire dataset.
    However, since the extraction process is based on randomness, you always have
    the chance of extracting a non-representative sample. In particular, the chance
    increases if the training sample you start from is small. Comparing the extracted
    holdout partition using **adversarial validation** (more about this in a few sections)
    can help you to make sure you are evaluating your efforts in a correct way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, to ensure that your test sampling is representative, especially
    with regard to how the training data relates to the target variable, you can use
    **stratification**, which ensures that the proportions of certain features are
    respected in the sampled data. You can use the `stratify` parameter in the `train_test_split`
    function and provide an array containing the class distribution to preserve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have to remark that, even if you have a representative holdout available,
    sometimes a simple train-test split is not enough for ensuring a correct tracking
    of your efforts in a competition.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, as you keep checking on this test set, you may drive your choices to
    some kind of adaptation overfitting (in other words, erroneously picking up the
    noise of the training set as signals), as happens when you frequently evaluate
    on the public leaderboard. For this reason, a probabilistic evaluation, though
    more computationally expensive, is more suited for a competition.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic evaluation methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Probabilistic evaluation of the performance of a machine learning model is based
    on the statistical properties of a sample from a distribution. By sampling, you
    create a smaller set of your original data that is expected to have the same characteristics.
    In addition, what is left untouched from the sampling constitutes a sample in
    itself, and it is also expected to have the same characteristics as the original
    data. By training and testing your model on this sampled data and repeating this
    procedure a large number of times, you are basically creating a statistical estimator
    measuring the performance of your model. Every sample may have some “error” in
    it; that is, it may not be fully representative of the true distribution of the
    original data. However, as you sample more, the mean of your estimators on these
    multiple samples will converge to the true mean of the measure you are estimating
    (this is an observed outcome that, in probability, is explained by a theorem called
    the *Law of Large Numbers*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Probabilistic estimators naturally require more computations than a simple
    train-test split, but they offer more confidence that you are correctly estimating
    the right measure: the general performance of your model.'
  prefs: []
  type: TYPE_NORMAL
- en: k-fold cross-validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most used probabilistic validation method is **k****-fold cross-validation**,
    which is recognized as having the ability to correctly estimate the performance
    of your model on unseen test data drawn from the same distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is clearly explained in the paper Bates, S., Hastie, T., and Tibshirani,
    R.; *Cross-validation: what does it estimate and how well does it do it?* arXiv
    preprint arX­iv:2104.00673, 2021 ([https://arxiv.org/pdf/2104.00673.pdf](https://arxiv.org/pdf/2104.00673.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-fold cross-validation can be successfully used to compare predictive models,
    as well as when selecting the hyperparameters for your model that will perform
    the best on the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few different variations of *k*-fold cross-validation, but
    the simplest one, which is implemented in the `KFold` function in Scikit-learn,
    is based on the splitting of your available training data into *k* partitions.
    After that, for *k* iterations, one of the *k* partitions is taken as a test set
    while the others are used for the training of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *k* validation scores are then averaged and that averaged score value is
    the *k*-fold validation score, which will tell you the estimated average model
    performance on any unseen data. The standard deviation of the scores will inform
    you about the uncertainty of the estimate. *Figure 6.2* demonstrates how 5-fold
    cross-validation is structured:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_06_02.png)Figure 6.2: How a 5-fold validation scheme is structured'
  prefs: []
  type: TYPE_NORMAL
- en: One important aspect of the *k*-fold cross-validation score you have to keep
    in mind is that it estimates the average score of a model trained on the same
    quantity of data as *k - 1* folds. If, afterward, you train your model on all
    your data, the previous validation estimate no longer holds. As *k* approaches
    the number *n* of examples, you have an increasingly correct estimate of the model
    derived on the full training set, yet, due to the growing correlation between
    the estimates you obtain from each fold, you will lose all the probabilistic estimates
    of the validation. In this case, you’ll end up having a number showing you the
    performance of your model on your training data (which is still a useful estimate
    for comparison reasons, but it won’t help you in correctly estimating the generalization
    power of your model).
  prefs: []
  type: TYPE_NORMAL
- en: When you reach *k = n*, you have the LOO validation method, which is useful
    when you have a few cases available. The method is mostly an unbiased fitting
    measure since it uses almost all the available data for training and just one
    example for testing. Yet it is not a good estimate of the expected performance
    on unseen data. Its repeated tests over the whole dataset are highly correlated
    with each other and the resulting LOO metric represents more the performance of
    the model on the dataset itself than the performance the model would have on unknown
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The correct *k* number of partitions to choose is decided based on a few aspects
    relative to the data you have available:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The smaller the *k* (the minimum is 2), the smaller each fold will be, and
    consequently, the more bias in learning there will be for a model trained on *k
    - 1* folds: your model validated on a smaller *k* will be less well-performing
    with respect to a model trained on a larger *k*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The higher the *k*, the more the data, yet the more correlated your validation
    estimates: you will lose the interesting properties of *k*-fold cross-validation
    in estimating the performance on unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commonly, *k* is set to 5, 7, or 10, more seldom to 20 folds. We usually regard
    *k* = 5 or *k* = 10 as a good choice for a competition, with the latter using
    more data for each training (90% of the available data), and hence being more
    suitable for figuring out the performance of your model when you retrain on the
    full dataset.
  prefs: []
  type: TYPE_NORMAL
- en: When deciding upon what *k* to choose for a specific dataset in a competition,
    we find it useful to reflect on two perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, the choice of the number of folds should reflect your goals:'
  prefs: []
  type: TYPE_NORMAL
- en: If your purpose is performance estimation, you need models with low bias estimates
    (which means no systematic distortion of estimates). You can achieve this by using
    a higher number of folds, usually between 10 and 20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your aim is parameter tuning, you need a mix of bias and variance, so it
    is advisable to use a medium number of folds, usually between 5 and 7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, if your purpose is just to apply variable selection and simplify your
    dataset, you need models with low variance estimates (or you will have disagreement).
    Hence, a lower number of folds will suffice, usually between 3 and 5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the size of the available data is quite large, you can safely stay on the
    lower side of the suggested bands.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, if you are just aiming for performance estimation, consider that the
    more folds you use, the fewer cases you will have in your validation set, so the
    more the estimates of each fold will be correlated. Beyond a certain point, increasing
    *k* renders your cross-validation estimates less predictive of unseen test sets
    and more representative of an estimate of how well-performing your model is on
    your training set. This also means that, with more folds, you can get the perfect
    out-of-fold prediction for stacking purposes, as we will explain in detail in
    *Chapter 9*, *Ensembling with Blending and Stacking Solutions*.
  prefs: []
  type: TYPE_NORMAL
- en: In Kaggle competitions, *k*-fold cross-validation is often applied not only
    for validating your solution approach and figuring out the performance of your
    model, but to produce your prediction. When you cross-validate, you are subsampling,
    and averaging the results of multiple models built on subsamples of the data is
    an effective strategy for fighting against variance, and often more effective
    than training on all the data available (we will discuss this more in *Chapter
    9*). Hence, many Kaggle competitors use the models built during cross-validation
    to provide a series of predictions on the test set that, averaged, will provide
    them with the solution.
  prefs: []
  type: TYPE_NORMAL
- en: k-fold variations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since it is based on random sampling, *k*-fold can provide unsuitable splits
    when:'
  prefs: []
  type: TYPE_NORMAL
- en: You have to preserve the proportion of small classes, both at a target level
    and at the level of features. This is typical when your target is highly imbalanced.
    Typical examples are spam datasets (because spam is a small fraction of the normal
    email volume) or any credit risk dataset where you have to predict the not-so-frequent
    event of a defaulted loan.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have to preserve the distribution of a numeric variable, both at a target
    level and at the level of features. This is typical of regression problems where
    the distribution is quite skewed or you have heavy, long tails. A common example
    is house price prediction, where you have a consistent small portion of houses
    on sale that will cost much more than the average house.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your cases are non-i.i.d, in particular when dealing with time series forecasting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first two scenarios, the solution is the **stratified** **k****-fold**,
    where the sampling is done in a controlled way that preserves the distribution
    you want to preserve. If you need to preserve the distribution of a single class,
    you can use `StratifiedKFold` from Scikit-learn, using a stratification variable,
    usually your target variable but also any other feature whose distribution you
    need to preserve. The function will produce a set of indexes that will help you
    to partition your data accordingly. You can also obtain the same result with a
    numeric variable, after having discretized it, using `pandas.cut` or Scikit-learn’s
    `KBinsDiscretizer`.
  prefs: []
  type: TYPE_NORMAL
- en: It is a bit more complicated when you have to stratify based on multiple variables
    or overlapping labels, such as in multi-label classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a solution in the **Scikit-multilearn** package ([http://scikit.ml/](http://scikit.ml/)),
    in particular, the `IterativeStratification` command that helps you to control
    the order (the number of combined proportions of multiple variables) that you
    want to preserve ([http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html](http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html)).
    It implements the algorithm explained by the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: Sechidis, K., Tsoumakas, G., and Vlahavas, I. (2011). *On the stratification
    of multi-label data*. *Machine Learning and Knowledge Discovery in Databases,
    145-158*. [http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf](http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Szymański, P. and Kajdanowicz, T.; *Proceedings of the First International
    Workshop on Learning with Imbalanced Domains*: *Theory and Applications*, PMLR
    74:22-35, 2017\. [http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html](http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can actually make good use of stratification even when your problem is not
    a classification, but a regression. Using stratification in regression problems
    helps your regressor to fit during cross-validation on a similar distribution
    of the target (or of the predictors) to the one found in the entire sample. In
    these cases, in order to have `StratifiedKFold` working correctly, you have to
    use a discrete proxy for your target instead of your continuous target.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first, simplest way of achieving this is to use the pandas `cut` function
    and divide your target into a large enough number of bins, such as 10 or 20:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to determine the number of bins to be used, *Abhishek Thakur* prefers
    to use **Sturges’ rule** based on the number of examples available, and provide
    that number to the pandas `cut` function (see [https://www.kaggle.com/abhishek/step-1-create-folds](https://www.kaggle.com/abhishek/step-1-create-folds)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: An alternative approach is to focus on the distributions of the features in
    the training set and aim to reproduce them. This requires the use of **cluster
    analysis** (an unsupervised approach) on the features of the training set, thus
    excluding the target variable and any identifiers, and then using the predicted
    clusters as strata. You can see an example in this Notebook ([https://www.kaggle.com/lucamassaron/are-you-doing-cross-validation-the-best-way](https://www.kaggle.com/lucamassaron/are-you-doing-cross-validation-the-best-way)),
    where first a PCA (principal component analysis) is performed to remove correlations,
    and then a *k*-means cluster analysis is performed. You can decide on the number
    of clusters to use by running empirical tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Proceeding with our discussion of the cases where *k*-fold can provide unsuitable
    splits, things get tricky in the third scenario, when you have non-i.i.d. data,
    such as in the case of some grouping happening among examples. The problem with
    non-i.i.d. examples is that the features and target are correlated between the
    examples (hence it is easier to predict all the examples if you know just one
    example among them). In fact, if you happen to have the same group divided between
    training and testing, your model may learn to distinguish the groups and not the
    target itself, producing a good validation score but very bad results on the leaderboard.
    The solution here is to use `GroupKFold`: by providing a grouping variable, you
    will have the assurance that each group will be placed either in the training
    folds or in the validation ones, but never split between the two.'
  prefs: []
  type: TYPE_NORMAL
- en: Discovering groupings in the data that render your data non-i.i.d. is actually
    not an easy task to accomplish. Unless stated by the competition problem, you
    will have to rely on your ability to investigate the data (using unsupervised
    learning techniques, such as cluster analysis) and the domain of the problem.
    For instance, if your data is about mobile telephone usage, you may realize that
    some examples are from the same user by noticing sequences of similar values in
    the features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time series analysis presents the same problem, and since data is non-i.i.d.,
    you cannot validate by random sampling because you will mix different time frames
    and later time frames could bear traces of the previous ones (a characteristic
    called **auto-correlation** in statistics). In the most basic approach to validation
    in time series, you can use a training and validation split based on time, as
    illustrated by *Figure 6.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Training and validation splits are based on time'
  prefs: []
  type: TYPE_NORMAL
- en: Your validation capabilities will be limited, however, since your validation
    will be anchored to a specific time. For a more complex approach, you can use
    time split validation, `TimeSeriesSplit`, as provided by the Scikit-learn package
    (`sklearn.model_selection.TimeSeriesSplit`). `TimeSeriesSplit` can help you set
    the timeframe of your training and testing portions of the time series.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the training timeframe, the `TimeSeriesSplit` function can help
    you to set your training data so it involves all the past data before the test
    timeframe, or limit it to a fixed period lookback (for instance, always using
    the data from three months before the test timeframe for training).
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6.4*, you can see the structure of a time-based validation strategy
    involving a growing training set and a moving validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: The training set is growing over time'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6.5*, you can instead see how the strategy changes if you stipulate
    that the training set has a fixed lookback:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_06_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Training and validation splits are moving over time'
  prefs: []
  type: TYPE_NORMAL
- en: In our experience, going by a fixed lookback helps to provide a fairer evaluation
    of time series models since you are always counting on the same training set size.
  prefs: []
  type: TYPE_NORMAL
- en: By instead using a growing training set size over time, you confuse the effects
    of your model performance across time slices with the decreasing bias in your
    model (since more examples mean less bias).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, remember that `TimeSeriesSplit` can be set to keep a pre-defined gap
    between your training and test time. This is extremely useful when you are told
    that the test set is a certain amount of time in the future (for instance, a month
    after the training data) and you want to test if your model is robust enough to
    predict that far into the future.
  prefs: []
  type: TYPE_NORMAL
- en: Nested cross-validation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At this point, it is important to introduce **nested cross-validation**. Up
    to now, we have only discussed testing models with respect to their final performance,
    but often you also need to test their intermediate performance when tuning their
    hyperparameters. In fact, you cannot test how certain model parameters work on
    your test set and then use the same data in order to evaluate the final performance.
    Since you have specifically found the best parameters that work on the test set,
    your evaluation measure on the same test set will be too optimistic; on a different
    test set, you will probably not obtain the exact same result. In this case, you
    have to distinguish between a **validation set**, which is used to evaluate the
    performance of various models and hyperparameters, and a **test set**, which will
    help you to estimate the final performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a test-train split, this is achieved by splitting the test
    part into two new parts. The usual split is 70/20/10 for training, validation,
    and testing, respectively (but you can decide differently). If you are using cross-validation,
    you need nested cross-validation; that is, you do cross-validation based on the
    split of another cross-validation. Essentially, you run your usual cross-validation,
    but when you have to evaluate different models or different parameters, you run
    cross-validation based on the fold split.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example in *Figure 6.6* demonstrates this internal and external cross-validation
    structure. Within the external part, you determine the portion of the data used
    to test your evaluation metric. Within the internal part, which is fed by the
    training data from the external part, you arrange training/validation splits in
    order to evaluate and optimize specific model choices, such as deciding which
    model or hyperparameter values to pick:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_06_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: How nested cross-validation is structured in an external and an
    internal loop'
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach has the advantage of making your test and parameter search fully
    reliable, but in doing so you incur a couple of problems:'
  prefs: []
  type: TYPE_NORMAL
- en: A reduced training set, since you first split by cross-validation, and then
    you split again
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More importantly, it requires a huge amount of model building: if you run two
    nested 10-fold cross-validations, you’ll need to run 100 models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Especially for this last reason, some Kagglers tend to ignore nested cross-validation
    and risk some adaptive fitting by using the same cross-validation for both model/parameter
    search and performance evaluation, or using a fixed test sample for the final
    evaluation. In our experience, this approach can work as well, though it may result
    in overestimating model performance and overfitting if you are generating out-of-fold
    predictions to be used for successive modeling (something we are going to discuss
    in the next section). We always suggest you try the most suitable methodology
    for testing your models. If your aim is to correctly estimate your model’s performance
    and reuse its predictions in other models, remember that using nested cross-validation,
    whenever possible, can provide you with a less overfitting solution and could
    make the difference in certain competitions.
  prefs: []
  type: TYPE_NORMAL
- en: Producing out-of-fold predictions (OOF)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An interesting application of cross-validation, besides estimating your evaluation
    metric performance, is producing test predictions and out-of-fold predictions.
    In fact, as you train on portions of your training data and predict on the remaining
    ones, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predict on the test set**: The average of all the predictions is often more
    effective than re-training the same model on all the data: this is an ensembling
    technique related to blending, which will be dealt with in *Chapter 9*, *Ensembling
    with Blending and Stacking Solutions*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predict on the validation set**: In the end, you will have predictions for
    the entire training set and can re-order them in the same order as the original
    training data. These predictions are commonly referred to as **out-of-fold** (**OOF**)
    **predictions** and they can be extremely useful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first use of OOF predictions is to estimate your performance since you can
    compute your evaluation metric directly on the OOF predictions. The performance
    obtained is different from the cross-validated estimates (based on sampling);
    it doesn’t have the same probabilistic characteristics, so it is not a valid way
    to measure generalization performance, but it can inform you about the performance
    of your model on the specific set you are training on.
  prefs: []
  type: TYPE_NORMAL
- en: A second use is to produce a plot and visualize the predictions against the
    ground truth values or against other predictions obtained from different models.
    This will help you in understanding how each model works and if their predictions
    are correlated.
  prefs: []
  type: TYPE_NORMAL
- en: The last use is to create meta-features or meta-predictors. This will also be
    fully explored in *Chapter 9*, but it is important to remark on now, as OOF predictions
    are a byproduct of cross-validation and they work because, during cross-validation,
    your model is always predicting on examples that it has not seen during training
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Since every prediction in your OOF predictions has been generated by a model
    trained on different data, these predictions are unbiased and you can use them
    without any fear of overfitting (though there are some caveats that will be discussed
    in the next chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating OOF predictions can be done in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: By coding a procedure that stores the validation predictions into a prediction
    vector, taking care to arrange them in the same index position as the examples
    in the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using the Scikit-learn function `cross_val_predict`, which will automatically
    generate the OOF predictions for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be seeing this second technique in action when we look at adversarial
    validation later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Subsampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are other validation strategies aside from *k*-fold cross-validation,
    but they do not have the same generalization properties. We have already discussed
    LOO, which is the case when *k = n* (where *n* is the number of examples). Another
    choice is **subsampling**. Subsampling is similar to *k*-fold, but you do not
    have fixed folds; you use as many as you think are necessary (in other words,
    take an educated guess). You repetitively subsample your data, each time using
    the data that you sampled as training data and the data that has been left unsampled
    for your validation. By averaging the evaluation metrics of all the subsamples,
    you will get a validation estimate of the performances of your model.
  prefs: []
  type: TYPE_NORMAL
- en: Since you are systematically testing all your examples, as in *k*-fold, you
    actually need quite a lot of trials to have a good chance of testing all of them.
    For the same reason, some cases may be tested more than others if you do not apply
    enough subsamples. You can run this sort of validation using `ShuffleSplit` from
    Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: The bootstrap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, another option is to try the **bootstrap**, which has been devised
    in statistics to conclude the error distribution of an estimate; for the same
    reasons, it can be used for performance estimation. The bootstrap requires you
    to draw a sample, *with replacement*, that is the same size as the available data.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you can use the bootstrap in two different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: As in statistics, you can bootstrap multiple times, train your model on the
    samples, and compute your evaluation metric on the training data itself. The average
    of the bootstraps will provide your final evaluation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, as in subsampling, you can use the bootstrapped sample for your training
    and what is left not sampled from the data as your test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our experience, the first method of calculating the evaluation metric on
    the bootstrapped training data, often used in statistics for linear models in
    order to estimate the value of the model’s coefficients and their error distributions,
    is much less useful in machine learning. This is because many machine learning
    algorithms tend to overfit the training data, hence you can never have a valid
    metric evaluation on your training data, even if you bootstrap it. For this reason,
    Efron and Tibshirani (see *Efron*, *B. and Tibshirani, R*. *Improvements on cross-validation:
    the 632+ bootstrap method.* Journal of the American Statistical Association 92.438
    (1997): 548-560.) proposed **the 632+ estimator** as a final validation metric.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, they proposed a simple version, called the 632 bootstrap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_06_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this formula, given your evaluation metric *err*, *err*[fit] is your metric
    computed on the training data and *err*[bootstrap] is the metric computed on the
    bootstrapped data. However, in the case of an overfitted training model, *err*[fit]
    would tend to zero, rendering the estimator not very useful. Therefore, they developed
    a second version of the 632+ bootstrap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_06_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *w* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_06_003.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17574_06_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Here you have a new parameter, ![](img/B17574_06_005.png), which is the **no-information
    error rate**, estimated by evaluating the prediction model on all possible combinations
    of targets and predictors. Calculating ![](img/B17574_06_005.png) is indeed intractable,
    as discussed by the developers of Scikit-learn ([https://github.com/scikit-learn/scikit-learn/issues/9153](https://github.com/scikit-learn/scikit-learn/issues/9153)).
  prefs: []
  type: TYPE_NORMAL
- en: Given the limits and intractability of using the bootstrap as in classical statistics
    for machine learning applications, you can instead use the second method, getting
    your evaluation from the examples left not sampled by the bootstrap.
  prefs: []
  type: TYPE_NORMAL
- en: In this form, the bootstrap is an alternative to cross-validation, but as with
    subsampling, it requires building many more models and testing them than for cross-validation.
    However, it makes sense to know about such alternatives in case your cross-validation
    is showing too high a variance in the evaluation metric and you need more intensive
    checking through testing and re-testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, this method has been implemented in Scikit-learn ([https://github.com/scikit-learn/scikit-learn/blob/0.16.X/sklearn/cross_validation.py#L613](https://github.com/scikit-learn/scikit-learn/blob/0.16.X/sklearn/cross_validation.py#L613))
    but was then removed. Since you cannot find the bootstrap anymore on Scikit-learn
    and it bootstrapped even the test data, you can use our own implementation. Here
    is our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In conclusion, the bootstrap is indeed an alternative to cross-validation. It
    is certainly more widely used in statistics and finance. In machine learning,
    the golden rule is to use the *k*-fold cross-validation approach. However, we
    suggest not forgetting about the bootstrap in all those situations where, due
    to outliers or a few examples that are too heterogeneous, you have a large standard
    error of the evaluation metric in cross-validation. In these cases, the bootstrap
    will prove much more useful in validating your models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Ryan_Chesler.png)'
  prefs: []
  type: TYPE_IMG
- en: Ryan Chesler
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/ryches](https://www.kaggle.com/ryches)'
  prefs: []
  type: TYPE_NORMAL
- en: Our second interview of the chapter is with Ryan Chesler, a Discussions Grandmaster
    and Notebooks and Competitions Master. He is a Data Scientist at H2O.ai and one
    of the organizers of the San Diego Machine Learning group on Meetup ([https://www.meetup.com/San-Diego-Machine-Learning/](https://www.meetup.com/San-Diego-Machine-Learning/)).
    The importance of validation came up in a few of his answers.
  prefs: []
  type: TYPE_NORMAL
- en: What’s your favourite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  prefs: []
  type: TYPE_NORMAL
- en: '*I tend to dabble in all kinds of competitions. It is more interesting to sample
    varied problems than specialize in a specific niche like computer vision or natural
    language processing. The ones I find most interesting are the ones where there
    are deep insights that can be derived from the data and error of predictions.
    For me, error analysis is one of the most illuminating processes; understanding
    where the model is failing and trying to find some way to improve the model or
    input data representation to address the weakness.*'
  prefs: []
  type: TYPE_NORMAL
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  prefs: []
  type: TYPE_NORMAL
- en: '*My approach is similar in both cases. Many people seem to favor exploratory
    data analysis before any modeling efforts, but I find that the process of preparing
    the data for modeling is usually sufficient. My typical approach is to manually
    view the data and make some preliminary decisions about how I think I can best
    model the data and different options to explore. After this, I build the model
    and evaluate performance, and then focus on analysing errors and reason about
    the next modeling steps based on where I see the model making errors.*'
  prefs: []
  type: TYPE_NORMAL
- en: Has Kaggle helped you in your career? If so, how?
  prefs: []
  type: TYPE_NORMAL
- en: '*Yes, it helped me get my current job. I work at H2O and they greatly value
    Kaggle achievements. My previous job also liked that I performed well in competitions.*'
  prefs: []
  type: TYPE_NORMAL
- en: You are also the organizer of a meetup in San Diego with over two thousand participants.
    Is this related to your experience with Kaggle?
  prefs: []
  type: TYPE_NORMAL
- en: '*Yes, it is absolutely related. I started from very little knowledge and tried
    out a Kaggle competition without much success at first. I went to a local meetup
    and found people to team up with and learn from. At the time, I got to work with
    people of a much higher skill level than me and we did really well in a competition,
    3rd/4500+ teams.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*After this, the group stopped being as consistent and I wanted to keep the
    community going, so I made my own group and started organizing my own events.
    I’ve been doing that for almost 4 years and I get to be on the opposite side of
    the table teaching people and helping them get started. We originally just focused
    on Kaggle competitions and trying to form teams, but have slowly started branching
    off to doing book clubs and lectures on various topics of interest. I attribute
    a lot of my success to having this dedicated weekly time to study and think about
    machine learning.*'
  prefs: []
  type: TYPE_NORMAL
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  prefs: []
  type: TYPE_NORMAL
- en: '*In my experience, a lot of people overstate the importance of bias-variance
    trade-off and overfitting. This is something I have seen people consistently worry
    about too much. The focus should not be making training and validation performance
    close, but make validation performance as good as possible.*'
  prefs: []
  type: TYPE_NORMAL
- en: What mistakes have you made in competitions in the past?
  prefs: []
  type: TYPE_NORMAL
- en: '*My consistent mistake is not exploring enough. Sometimes I have ideas that
    I discount too early and turn out to be important for improving performance. Very
    often I can get close to competitive performance on the first try, but iterating
    and continuing to improve as I try new things takes a slightly different skill
    that I am still working on mastering.*'
  prefs: []
  type: TYPE_NORMAL
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: '*I use a lot of the standard tools: XGBoost, LightGBM, Pytorch, TensorFlow,
    Scikit-learn. I don’t have any strong affinity for a specific tool or library,
    just whatever is relevant to the problem.*'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  prefs: []
  type: TYPE_NORMAL
- en: '*I think the most important thing people have to keep in mind is good validation.
    Very often I see people fooling themselves thinking their performance is improving
    but then submitting to the leaderboard and realizing it didn’t actually go how
    they expected. It is an important skill to understand how to match assumptions
    with your new unseen data and build a model that is robust to new conditions.*'
  prefs: []
  type: TYPE_NORMAL
- en: Tuning your model validation system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, you should have a complete overview of all possible validation
    strategies. When you approach a competition, you devise your validation strategy
    and you implement it. Then, you test if the strategy you have chosen is correct.
  prefs: []
  type: TYPE_NORMAL
- en: As a golden rule, be guided in devising your validation strategy by the idea
    that you have to replicate the same approach used by the organizers of the competition
    to split the data into training, private, and public test sets. Ask yourself how
    the organizers have arranged those splits. Did they draw a random sample? Did
    they try to preserve some specific distribution in the data? Are the test sets
    actually drawn from the same distribution as the training data?
  prefs: []
  type: TYPE_NORMAL
- en: These are not the questions you would ask yourself in a real-world project.
    Contrary to a real-world project where you have to generalize at all costs, a
    competition has a much narrower focus on having a model that performs on the given
    test set (especially the private one). If you focus on this idea from the beginning,
    you will have more of a chance of finding out the best validation strategy, which
    will help you rank more highly in the competition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this is a trial-and-error process, as you try to find the best validation
    strategy for the competition, you can systematically apply the following two consistency
    checks in order to figure out if you are on the right path:'
  prefs: []
  type: TYPE_NORMAL
- en: First, you have to check if your local tests are consistent, that is, that the
    single cross-validation fold errors are not so different from each other or, when
    you opt for a simple train-test split, that the same results are reproducible
    using different train-test splits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, you have to check if your local validation error is consistent with the
    results on the public leaderboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you’re failing the first check, you have a few options depending on the
    following possible origins of the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: You don’t have much training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data is too diverse and every training partition is very different from
    every other (for instance, if you have too many **high cardinality** features,
    that is, features with too many levels – like zip codes – or if you have multivariate
    outliers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both cases, the point is that you lack data with respect to the model you
    want to implement. Even when the problem just appears to be that the data is too
    diverse, plotting learning curves will make it evident to you that your model
    needs more data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, unless you find out that moving to a simpler algorithm works
    on the evaluation metric (in which case trading variance for bias may worsen your
    model’s performance, but not always), your best choice is to use an extensive
    validation approach. This can be implemented by:'
  prefs: []
  type: TYPE_NORMAL
- en: Using larger *k* values (thus approaching LOO where *k = n*). Your validation
    results will be less about the capability of your model to perform on unseen data,
    but by using larger training portions, you will have the advantage of more stable
    evaluations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Averaging the results of multiple *k*-fold validations (based on different data
    partitions picked by different random seed initializations).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using repetitive bootstrapping.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that when you find unstable local validation results, you won’t
    be the only one to suffer from the problem. Usually, this is a common problem
    due to the data’s origin and characteristics. By keeping tuned in to the discussion
    forums, you may get hints at possible solutions. For instance, a good solution
    for high cardinality features is target encoding; stratification can help with
    outliers; and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The situation is different when you’ve passed the first check but failed the
    second; your local cross-validation is consistent but you find that it doesn’t
    hold on the leaderboard. In order to realize this problem exists, you have to
    keep diligent note of all your experiments, validation test types, random seeds
    used, and leaderboard results if you submitted the resulting predictions. In this
    way, you can draw a simple scatterplot and try fitting a linear regression or,
    even simpler, compute a correlation between your local results and the associated
    public leaderboard scores. It costs some time and patience to annotate and analyze
    all of these, but it is the most important meta-analysis of your competition performances
    that you can keep track of.
  prefs: []
  type: TYPE_NORMAL
- en: When the mismatch is because your validation score is systematically lower or
    higher than the leaderboard score, you actually have a strong signal that something
    is missing from your validation strategy, but this problem does not prevent you
    from improving your model. In fact, you can keep on working on your model and
    expect improvements to be reflected on the leaderboard, though not in a proportional
    way. However, systematic differences are always a red flag, implying something
    is different between what you are doing and what the organizers have arranged
    for testing the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'An even worse scenario occurs when your local cross-validation scores do not
    correlate at all with the leaderboard feedback. This is really a red flag. When
    you realize this is the case, you should immediately run a series of tests and
    investigations in order to figure out why, because, regardless of whether it is
    a common problem or not, the situation poses a serious threat to your final rankings.
    There are a few possibilities in such a scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: You figure out that the test set is drawn from a different distribution to the
    training set. The adversarial validation test (that we will discuss in the next
    section) is the method that can enlighten you in such a situation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data is non-i.i.d. but this is not explicit. For instance, in *The Nature
    Conservancy Fisheries Monitoring* competition ([https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring)),
    images in the training set were taken from similar situations (fishing boats).
    You had to figure out by yourself how to arrange them in order to avoid the model
    learning to identify the target rather than the context of the images (see, for
    instance, this work by *Anokas*: [https://www.kaggle.com/anokas/finding-boatids](https://www.kaggle.com/anokas/finding-boatids)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The multivariate distribution of the features is the same, but some groups are
    distributed differently in the test set. If you can figure out the differences,
    you can set your training set and your validation accordingly and gain an edge.
    You need to probe the public leaderboard to work this out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The test data is drifted or trended, which is usually the case in time series
    predictions. Again, you need to probe the public leaderboard to get some insight
    about some possible post-processing that could help your score, for instance,
    applying a multiplier to your predictions, thus mimicking a decreasing or increasing
    trend in the test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we’ve discussed before, probing the leaderboard is the act of making specifically
    devised submissions in order to get insights about the composition of the public
    test set. It works particularly well if the private test set is similar to the
    public one. There are no general methods for probing, so you have to devise a
    probing methodology according to the type of competition and problem.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the paper *Climbing the Kaggle Leaderboard by Exploiting the
    Log-Loss Oracle* ([https://export.arxiv.org/pdf/1707.01825](https://export.arxiv.org/pdf/1707.01825)),
    Jacob explains how to get fourth position in a competition without even downloading
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'With regard to regression problems, in the recent *30 Days of ML* organized
    by Kaggle, *Hung Khoi* explained how probing the leaderboard helped him to understand
    the differences in the mean and standard deviation of the target column between
    the training dataset and the public test data (see: [https://www.kaggle.com/c/30-days-of-ml/discussion/269541](https://www.kaggle.com/c/30-days-of-ml/discussion/269541)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'He used the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_06_007.png)'
  prefs: []
  type: TYPE_IMG
- en: Essentially, you need just two submissions to solve for the mean and variance
    of the test target, since there are two unknown terms – variance and mean.
  prefs: []
  type: TYPE_NORMAL
- en: You can also get some other ideas about leaderboard probing from *Chris Deotte*
    ([https://www.kaggle.com/cdeotte](https://www.kaggle.com/cdeotte)) from this post,
    [https://www.kaggle.com/cdeotte/lb-probing-strategies-0-890-2nd-place](https://www.kaggle.com/cdeotte/lb-probing-strategies-0-890-2nd-place),
    relevant to the *Don’t Overfit II competition* ([https://www.kaggle.com/c/dont-overfit-ii](https://www.kaggle.com/c/dont-overfit-ii)).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to get a feeling about how probing information from the leaderboard
    is a double-edged sword, you can read about how *Zahar Chikishev* managed to probe
    information from the *LANL Earthquake Prediction* competition, ending up in 87^(th)
    place in the private leaderboard after leading in the public one: [https://towardsdatascience.com/how-to-lb-probe-on-kaggle-c0aa21458bfe](https://towardsdatascience.com/how-to-lb-probe-on-kaggle-c0aa21458bfe)'
  prefs: []
  type: TYPE_NORMAL
- en: Using adversarial validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have discussed, cross-validation allows you to test your model’s ability
    to generalize to unseen datasets coming from the same distribution as your training
    data. Hopefully, since in a Kaggle competition you are asked to create a model
    that can predict on the public and private datasets, you should expect that such
    test data is from the same distribution as the training data. In reality, this
    is not always the case.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you do not overfit to the test data because you have based your decision
    not only on the leaderboard results but also considered your cross-validation,
    you may still be surprised by the results. This could happen in the event that
    the test set is even slightly different from the training set on which you have
    based your model. In fact, the target probability and its distribution, as well
    as how the predictive variables relate to it, inform your model during training
    about certain expectations that cannot be satisfied if the test data is different
    from the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, it is not enough to avoid overfitting to the leaderboard as we have discussed
    up to now, but, in the first place, it is also advisable to find out if your test
    data is comparable to the training data. Then, if they differ, you have to figure
    out if there is any chance that you can mitigate the different distributions between
    training and test data and build a model that performs on that test set.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adversarial validation** has been developed just for this purpose. It is
    a technique allowing you to easily estimate the degree of difference between your
    training and test data. This technique was long rumored among Kaggle participants
    and transmitted from team to team until it emerged publicly thanks to a post by
    *Zygmunt Zając* ([https://www.kaggle.com/zygmunt](https://www.kaggle.com/zygmunt))
    on his FastML blog.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is simple: take your training data, remove the target, assemble your
    training data together with your test data, and create a new binary classification
    target where the positive label is assigned to the test data. At this point, run
    a machine learning classifier and evaluate for the ROC-AUC evaluation metric (we
    discussed this metric in the previous chapter on *Detailing Competition Tasks
    and Metrics*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If your ROC-AUC is around 0.5, it means that the training and test data are
    not easily distinguishable and are apparently from the same distribution. ROC-AUC
    values higher than 0.5 and nearing 1.0 signal that it is easy for the algorithm
    to figure out what is from the training set and what is from the test set: in
    such a case, don’t expect to be able to easily generalize to the test set because
    it clearly comes from a different distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find an example Notebook written for the *Sberbank Russian Housing
    Market* competition ([https://www.kaggle.com/c/sberbank-russian-housing-market](https://www.kaggle.com/c/sberbank-russian-housing-market))
    that demonstrates a practical example of adversarial validation and its usage
    in a competition here: [https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms](https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since your data may be of different types (numeric or string labels) and you
    may have missing cases, you’ll need some data processing before being able to
    successfully run the classifier. Our suggestion is to use the random forest classifier
    because:'
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t output true probabilities but its results are intended as simply
    ordinal, which is a perfect fit for an ROC-AUC score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The random forest is a flexible algorithm based on decision trees that can do
    feature selection by itself and operate on different types of features without
    any pre-processing, while rendering all the data numeric. It is also quite robust
    to overfitting and you don’t have to think too much about fixing its hyperparameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don’t need much data processing because of its tree-based nature. For missing
    data, you can simply replace the values with an improbable negative value such
    as -999, and you can deal with string variables by converting their strings into
    numbers (for instance, using the Scikit-learn label encoder, `sklearn.preprocessing.LabelEncoder`).
    As a solution, it performs less well than one-hot encoding, but it is very speedy
    and it will work properly for the problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although building a classification model is the most direct way to adversarially
    validate your test set, you can also use other approaches. One approach is to
    map both training and test data into a lower-dimensional space, as in this post
    ([https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data](https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data))
    by *NanoMathias* ([https://www.kaggle.com/nanomathias](https://www.kaggle.com/nanomathias)).
    Although requiring more tuning work, such an approach based on t-SNE and PCA has
    the great advantage of being graphically representable in an appealing and understandable
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t forget that our brains are more adept at spotting patterns in visual representations
    than numeric ones (for an articulate discussion about our visual abilities, see
    [https://onlinelibrary.wiley.com/doi/full/10.1002/qua.24480](https://onlinelibrary.wiley.com/doi/full/10.1002/qua.24480)).
  prefs: []
  type: TYPE_NORMAL
- en: PCA and t-SNE are not the only tools that can help you to reduce the dimensionality
    of your data and allow you to visualize it. UMAP ([https://github.com/lmcinnes/umap](https://github.com/lmcinnes/umap))
    can often provide a faster low dimensionality solution with clear and distinct
    data clusters. Variational auto-encoders (discussed in *Chapter 7*, *Modeling
    for Tabular Competitions*) can instead deal with non-linear dimensionality reduction
    and offer a more useful representation than PCA; they are more complicated to
    set up and tune, however.
  prefs: []
  type: TYPE_NORMAL
- en: Example implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While you can find examples of adversarial validation in the original article
    by Zygmunt and the Notebook we linked, we have created a fresh example for you,
    based on the Playground competition *Tabular Playground Series – Jan 2021* ([https://www.kaggle.com/c/tabular-playground-series-jan-2021](https://www.kaggle.com/c/tabular-playground-series-jan-2021)).
  prefs: []
  type: TYPE_NORMAL
- en: 'You start by importing some Python packages and getting the training and test
    data from the competition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Data preparation is short and to the point. Since all features are numeric,
    you won’t need any label encoding, but you do have to fill any missing values
    with a negative number (-1 usually works fine), and drop the target and also any
    identifiers; when the identifier is progressive, the adversarial validation may
    return a high ROC-AUC score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you just need to generate `RandomForestClassifier` predictions
    for your data using the `cross_val_predict` function, which automatically creates
    a cross-validation scheme and stores the predictions on the validation fold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As a result, you obtain predictions that are unbiased (they are not overfit
    as you did not predict on what you trained) and that can be used for error estimation.
    Please note that `cross_val_predict` won’t fit your instantiated model, so you
    won’t get any information from it, such as what the important features used by
    the model are. If you need such information, you just need to fit it first by
    calling `model.fit(X, y)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you can query the ROC-AUC score for your predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You should obtain a value of around 0.49-0.50 (`cross_val_predict` won’t be
    deterministic unless you use cross-validation with a fixed `random_seed`). This
    means that you cannot easily distinguish training from test data. Hence, they
    come from the same distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Handling different distributions of training and test data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ROC-AUC scores of 0.8 or more would alert you that the test set is peculiar
    and quite distinguishable from the training data. In these cases, what can you
    do? You actually have a few strategies at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: Suppression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training on cases most similar to the test set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating by mimicking the test set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With **suppression**, you remove the variables that most influence the result
    in the adversarial test set until the distributions are the same again. To do
    so, you require an iterative approach. This time, you fit your model to all your
    data, and then you check the importance measures (provided, for instance, by the
    `feature_importances_` method in the Scikit-learn `RandomForest` classifier) and
    the ROC-AUC fit score. At this point, you remove the most important variable for
    the model from your data and run everything again. You repeat this cycle where
    you train, measure the ROC-AUC fit, and drop the most important variable from
    your data until the fitted ROC-AUC score decreases to around 0.5\.
  prefs: []
  type: TYPE_NORMAL
- en: The only problem with this method is that you may actually be forced to remove
    the majority of important variables from your data, and any model you then build
    on such variable censored data won’t be able to predict sufficiently correctly
    due to the lack of informative features.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you **train on the examples most similar to the test set**, you instead
    take a different approach, focusing not on the *variables* but on the *samples*
    you are using for training. In this case, you pick up from the training set only
    the samples that fit the test distribution. Any trained model then suits the testing
    distribution (but it won’t be generalizable to anything else), which should allow
    you to test the best on the competition problem. The limitation of this approach
    is that you are cutting down the size of your dataset and, depending on the number
    of samples that fit the test distribution, you may suffer from a very biased resulting
    model due to the lack of training examples. In our previous example, picking up
    just the adversarial predictions on the training data that exceed a probability
    of 0.5 and summing them results in picking only 1,495 cases (the number is so
    small because the test set is not very different from the training set):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Finally, with the strategy of **validating by mimicking the test set**, you
    keep on training on all the data, but for validation purposes, you pick your examples
    only from the adversarial predictions on the training set that exceed a probability
    of 0.5 (or an even higher threshold such as 0.9).
  prefs: []
  type: TYPE_NORMAL
- en: Having a validation set tuned to the test set will allow you to pick all the
    possible hyperparameters and model choices that will favor a better result on
    the leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we can figure out that `feature_19` and `feature_54` appear
    the most different between the training/test split from the output of the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To conclude, we have a few more remarks on adversarial validation. First, using
    it will generally help you to perform better in competitions, but not always.
    Kaggle’s Code competitions, and other competitions where you cannot fully access
    the test set, cannot be inspected by adversarial validation. In addition, adversarial
    validation can inform you about the test data as a whole, but it cannot advise
    you on the split between the private and the public test data, which is the cause
    of the most common form of public leaderboard overfitting and consequent shake-up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, adversarial validation, though a very specific method devised for
    competitions, has quite a few practical use cases in the real world: how often
    have you picked the wrong test set to validate your models? The method we have
    presented here can enlighten you about whether you are using the test data, and
    any validation data, in your projects properly. Moreover, data changes and models
    in production may be affected by such changes and produce bad predictions if you
    don’t retrain them. This is called **concept drift**, and by using adversarial
    validation, you can immediately understand if you have to retrain new models to
    put into production or if you can leave the previous ones in operation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Giuliano_Janson.png)'
  prefs: []
  type: TYPE_IMG
- en: Giuliano Janson
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/adjgiulio](https://www.kaggle.com/adjgiulio)'
  prefs: []
  type: TYPE_NORMAL
- en: Giuliano Janson is a Competitions Grandmaster and senior applied scientist for
    ML and NLP at Zillow Group. He spoke to us about his competition wins, the importance
    of cross-validation, and data leakages, the subject of the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  prefs: []
  type: TYPE_NORMAL
- en: '*My perfect competition is made up of a) an interesting problem to solve, b)
    a mid-size dataset that is small enough to fit in memory but not too small to
    become an overfitting headache, and c) an opportunity to be creative from a feature
    engineering perspective. The combination of those three dimensions is where I’m
    at my best in competitive ML because I feel I have the means to use rigor and
    creativity without having to worry about engineering constraints.*'
  prefs: []
  type: TYPE_NORMAL
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  prefs: []
  type: TYPE_NORMAL
- en: '*A Kaggle competition is a marathon. Going into a competition, I know I can
    get 90 to 95% of my best final score with a couple of days of work. The rest is
    a slow grind. The only success metric is your score; nothing else matters.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*My daily work looks more like a series of sprints. Model performance is only
    a small portion of what I need to consider. A go-live date might be just as important,
    or other aspects such as interpretability, scalability, and maintainability could
    tip the scale in a totally different direction. After each sprint, priorities
    are reassessed and the end product might look totally different from what was
    originally envisioned. Also, modeling is a small part of my day. I spend far more
    time talking to people, managing priorities, building use cases, scrubbing data,
    and thinking about everything that it takes to make a prototype model a successful
    production solution.*'
  prefs: []
  type: TYPE_NORMAL
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  prefs: []
  type: TYPE_NORMAL
- en: '*One of the two competitions I won, the Genentech Cancer competition, was a
    Masters-only competition. The data provided was raw transactional data. There
    was no nice tabular dataset to start from. This is the type of work I love because
    feature engineering is actually one of my favorite parts of ML. Since I had worked
    in healthcare for a decade at the time of the competition, I had business and
    clinical insights on the data, but most of all, I had engineering insights on
    the complexity of correctly handling this type of data and about all the things
    that can go wrong when this type of transactional raw data is not handled carefully.
    That turned out to be key to winning, as one of the initial hypotheses regarding
    a possible source of leakage turned out to be true, and provided a “golden feature”
    that gave the final boost to our model. The insight from the competition is to
    always be extra careful when doing feature engineering or setting up validation
    approaches. Leakage can be very hard to detect and the usual train/validation/test
    approach to model validation will provide no help in identifying leakage in most
    cases, thus putting a model at risk of underperforming in production.*'
  prefs: []
  type: TYPE_NORMAL
- en: Has Kaggle helped you in your career? If so, how?
  prefs: []
  type: TYPE_NORMAL
- en: '*Kaggle has helped me in two ways. First, it provided a low barrier entry point
    to modern ML, a ton of exposure to cutting-edge modeling techniques, and forced
    me to truly understand the art and science of professional-grade model validation
    techniques. Second, Kaggle provided access to some of the brightest minds in applied
    ML. What I learned teaming up with some of the top Kaggle participants are lessons
    I cherish and try to share with my teammates every day.*'
  prefs: []
  type: TYPE_NORMAL
- en: How have you built up your portfolio thanks to Kaggle?
  prefs: []
  type: TYPE_NORMAL
- en: '*My professional career hasn’t been directly impacted much by my Kaggle résumé.
    By that, I mean I haven’t got job offers or interviews as a result of my Kaggle
    standings. I started Kaggle when I was already in a senior data science role,
    albeit with not much of an ML focus. Thanks to what I learned on Kaggle, I was
    able to better advocate a change in my career to move into an ML-focused job.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*To this date, many folks I work with enjoy chatting about competitive ML and
    are curious about tips and tricks from my Kaggle experience, but it is also true
    that a large portion of the ML community might not even know what Kaggle is.*'
  prefs: []
  type: TYPE_NORMAL
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  prefs: []
  type: TYPE_NORMAL
- en: '*The importance of proper cross-validation is easily overlooked by participants
    new to competitive ML. A solid cross-validation framework allows you to measure
    improvement reliably and objectively. And in a competition that might be as long
    as six months, the best models do not usually come from those who have the best
    initial ideas, but from those who are willing to iterate and adjust based on empirical
    feedback from the data. A great validation framework is at the foundation of it
    all.*'
  prefs: []
  type: TYPE_NORMAL
- en: What mistakes have you made in competitions in the past?
  prefs: []
  type: TYPE_NORMAL
- en: '*One of the lessons learned that I always share with people new to ML is to
    “never get over-enamored with overly complex ideas.” When facing a new complex
    problem, it is easy to be tempted to build complex solutions. Complex solutions
    usually require time to develop. But the main issue is that complex solutions
    are often of marginal value, conditional on robust baselines. For example, imagine
    you want to model the outcome of an election and start thinking about a series
    of features to capture complex conditional relationships among observable and
    latent geographic, socio-economic, and temporal features. You could spend weeks
    developing these features, under the assumption that because they are so well
    thought out, they will be impactful.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The mistake is that while often those complex features could be very powerful
    on their own, conditional on a series of simple features and on a model that can
    already build highly optimized, data-driven deep interaction, all of a sudden,
    the complex features we built with time and effort may lead to little to no marginal
    improvement. My advice is to stick to Occam’s razor and try easy things before
    being tempted by more complex approaches.*'
  prefs: []
  type: TYPE_NORMAL
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: '*I’m a pandas and Scikit-learn person. I love how pandas enables easy data
    manipulation and exploration and how I can quickly prototype models using Scikit-learn
    in a matter of minutes. Most of my prototype work is done using these two libraries.
    That said, my final models are often based on XGBoost. For deep learning, I love
    using Keras.*'
  prefs: []
  type: TYPE_NORMAL
- en: Handling leakage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common issue in Kaggle competitions that can affect the outcome of the challenge
    is data leakage. **Data leakage**, often mentioned simply as **leakage** or with
    other fancy names (such as *golden features*), involves information in the training
    phase that won’t be available at prediction time. The presence of such information
    (leakage) will make your model over-perform in training and testing, allowing
    you to rank highly in the competition, but will render unusable or at best suboptimal
    any solution based on it from the sponsor’s point of view.
  prefs: []
  type: TYPE_NORMAL
- en: We can define leakage as “when information concerning the ground truth is artificially
    and unintentionally introduced within the training feature data, or training metadata”
    as stated by *Michael Kim* ([https://www.kaggle.com/mikeskim](https://www.kaggle.com/mikeskim))
    in his presentation at *Kaggle Days San Francisco* in 2019.
  prefs: []
  type: TYPE_NORMAL
- en: Leakage is often found in Kaggle competitions, despite careful checking from
    both the sponsor and the Kaggle team. Such situations are due to the subtle and
    sneaky nature of leakage, which can unexpectedly appear due to the intense searching
    undertaken by Kagglers, who are always looking for any way to score better in
    a competition.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t confuse data leakage with a leaky validation strategy. In a leaky validation
    strategy, the problem is that you have arranged your validation strategy in a
    way that favors better validation scores because some information leaks from the
    training data. It has nothing to do with the competition itself, but it relates
    to how you are handling your validation. It occurs if you run any pre-processing
    modifying your data (normalization, dimensionality reduction, missing value imputation)
    before separating training and validation or test data.
  prefs: []
  type: TYPE_NORMAL
- en: In order to prevent leaky validation, if you are using Scikit-learn to manipulate
    and process your data, you absolutely have to exclude your validation data from
    any fitting operation. Fitting operations tend to create leakage if applied to
    any data you use for validation. The best way to avoid this is to use Scikit-learn
    pipelines ([https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)),
    which will enclose both your data processing and model together, thereby avoiding
    any risk of inadvertently applying any leaking transformation to your data.
  prefs: []
  type: TYPE_NORMAL
- en: Data leakage instead is therefore something that is not strictly related to
    validation operations, though it affects them deeply. Even though this chapter
    is principally devoted to validation strategies, at this point we consider it
    necessary to discuss data leakage, since this issue can profoundly affect how
    you evaluate your models and their ability to generalize beyond the competition
    test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking, leakage can originate at a feature or example level. **Feature
    leakage** is by far the most common. It can be caused by the existence of a proxy
    for the target, or by a feature that is posterior to the target itself. A target
    proxy could be anything derived from processing the label itself or from the test
    split process; for instance, when defining identifiers, specific identifiers (a
    numeration arc, for instance) may be associated with certain target responses,
    making it easier for a model to guess if properly fed with the information processed
    in the right way. A more subtle way in which data processing can cause leakage
    is when the competition organizers have processed the training and test set together
    before splitting it. Historically, leakages in Kaggle competitions have been found
    in:'
  prefs: []
  type: TYPE_NORMAL
- en: Mishandled data preparation from organizers, especially when they operate on
    a combination of training and test data (for example, in *Loan Default Prediction*
    ([https://www.kaggle.com/c/loan-default-prediction](https://www.kaggle.com/c/loan-default-prediction)),
    organizers initially used features with aggregated historical data that leaked
    future information).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Row order when it is connected to a time index or to specific data groups (for
    instance, in *Telstra Network Disruptions* ([https://www.kaggle.com/c/telstra-recruiting-network](https://www.kaggle.com/c/telstra-recruiting-network)),
    the order of records in a feature hinted at proxy information, the location, which
    was not present in the data and which was very predictive).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Column order when it is connected to a time index (you get hints by using the
    columns as rows).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature duplication in consecutive rows because it can hint at examples with
    correlated responses, such as in *Bosch Production Line Performance* (see the
    first-place solution by *Beluga* at [https://www.kaggle.com/c/bosch-production-line-performance/discussion/25434](https://www.kaggle.com/c/bosch-production-line-performance/discussion/25434)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Image metadata (as in *Two Sigma Connect: Rental Listing Inquiries* ([https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries))).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hashes or other easily crackable anonymization practices of encodings and identifiers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The trouble with posterior information originates from the way we deal with
    information when we do not consider the effects of time and of the sequence of
    cause and effect that spans across time. Since we are looking back at the past,
    we often forget that certain variables that make sense at the present moment do
    not have value in the past. For instance, if you have to calculate a credit score
    for a loan to a new company, knowing that payments of the borrowed money are often
    late is a great indicator of the lower reliability and higher risk represented
    by the debtor, but you cannot know this before you have lent out the money. This
    is also a problem that you will commonly find when analyzing company databases
    in your projects: your query data will represent present situations, not past
    ones. Reconstructing past information can also be a difficult task if you cannot
    specify that you wish to retrieve only the information that was present at a certain
    time. For this reason, great effort has to be spent on finding these leaking features
    and excluding or adjusting them before building any model.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar problems are also common in Kaggle competitions based on the same kind
    of data (banking or insurance, for instance), though, since much care is put into
    the preparation of the data for the competition, they appear in more subtle ways
    and forms. In general, it is easy to spot these leaking features since they strongly
    correlate with the target, and a domain expert can figure out why (for instance,
    knowing at what stage the data is recorded in the databases). Therefore, in competitions,
    you never find such obvious features, but derivatives of them, often transformed
    or processed features that have slipped away from the control of the sponsor.
    Since the features are often anonymized to preserve the sponsor’s business, they
    end up lurking among the others. This has given rise to a series of hunts for
    the golden/magic features, a search to combine existing features in the dataset
    in order to have the leakage emerge.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read an enlightening post by *Corey Levison* here: [https://www.linkedin.com/pulse/winning-13th-place-kaggles-magic-competition-corey-levinson/](https://www.linkedin.com/pulse/winning-13th-place-kaggles-magic-competition-corey-levinson/).
    It tells the story of how the *Santander Customer Transaction Prediction* competition
    turned into a hunt for magic features for his team.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another good example is provided by *dune_dweller* here: [https://www.kaggle.com/c/telstra-recruiting-network/discussion/19239#109766](https://www.kaggle.com/c/telstra-recruiting-network/discussion/19239#109766).
    By looking at how the data was ordered, dune_dweller found out that the data was
    likely in time order. Putting this information in a new feature increased the
    score.'
  prefs: []
  type: TYPE_NORMAL
- en: The other way in which leakage can occur is by **training example leakage**.
    This happens especially with non-i.i.d. data. This means that some cases correlate
    between themselves because they are from the same period (or from contiguous ones)
    or the same group. If such cases are not all together either in the training or
    test data, but separated between them, there is a high chance that the machine
    learning algorithm will learn how to spot the cases (and derive the predictions)
    rather than using general rules. An often-cited example of such a situation involves
    the team of *Prof. Andrew Ng* (see [https://twitter.com/nizkroberts/status/931121395748270080](https://twitter.com/nizkroberts/status/931121395748270080)).
    In 2017, they wrote a paper using a dataset of 100,000 x-rays from 30,000 patients.
    They used a random split in order to separate training and test data, not realizing
    that the x-rays of the same patient could end up partly in the training set and
    partly in the test set. Practitioners such as Nick Roberts spotted this fact,
    pointing out a possible leakage that could have inflated the performances of the
    model and that led to a substantial revision of the paper itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'What happens when there is a data leakage in a Kaggle competition? Kaggle has
    clear policies about it and will either:'
  prefs: []
  type: TYPE_NORMAL
- en: Let the competition continue as is (especially if the leakage only has a small
    impact)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove the leakage from the set and relaunch the competition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate a new test set that does not have the leakage present
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In particular, Kaggle recommends making any leakage found public, though this
    is not compulsory or sanctioned if it doesn’t happen. However, in our experience,
    if there is any leakage in a competition, it will soon become very apparent and
    the discussion forums will start lighting up with a discussion about magic stuff
    and the like. You will soon know, if you are attentive to what is being said in
    the forums and able to put together all the hints provided by different Kagglers.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, please beware that some players may even use discussions about magic
    features to distract other competitors from serious modeling. For instance, in
    *Santander Customer Transaction Prediction*, there was a famous situation involving
    some Kagglers who fueled in other participants an interest in magic features that
    weren’t actually so magic, directing their efforts in the wrong direction (see
    the discussion here: [https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87057#502362](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87057#502362)).'
  prefs: []
  type: TYPE_NORMAL
- en: Our suggestion is to carefully read the discussions around leakage and magic
    features that arise in the competition’s forum, and decide whether to pursue the
    research and use any leakage found based on your own interest and motivations
    for participating in the competition.
  prefs: []
  type: TYPE_NORMAL
- en: Not exploiting any leakage may really damage your final rankings, though it
    will surely spoil your learning experience (because leakage is a distortion and
    you cannot claim anything about the models using it). If you are not participating
    in a competition in order to gain a reputation or to later approach the sponsor
    for an opportunity to be hired, it is perfectly fine to use any leakage you come
    across. Otherwise, just ignore it and keep on working hard on your models (who
    knows; maybe Kaggle will reset or fix the competition by the end, rendering the
    leakage ineffective to the great disappointment of the many who used it).
  prefs: []
  type: TYPE_NORMAL
- en: 'Leakages are very different from competition to competition. If you want to
    get an idea of a few real leakages that have happened in Kaggle competitions,
    you can have a look at these three memorable ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/c/predicting-red-hat-business-value/discussion/22807](https://www.kaggle.com/c/predicting-red-hat-business-value/discussion/22807)
    from *Predicting Red Hat Business Value* ([https://www.kaggle.com/c/predicting-red-hat-business-value](https://www.kaggle.com/c/predicting-red-hat-business-value))
    where the problem arose because of an imperfect train/test split methodology of
    the competition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/c/talkingdata-mobile-user-demographics /discussion/23403](https://www.kaggle.com/c/talkingdata-mobile-user-demographics/discussion/23403)
    from *TalkingData Mobile User Demographics* ([https://www.kaggle.com/c/talkingdata-mobile-user-demographics](https://www.kaggle.com/c/talkingdata-mobile-user-demographics))
    where a series of problems and non-i.i.d cases affected the correct train/test
    split of the competition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870)
    from *Two Sigma Connect: Rental Listing Inquiries* ([https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries))
    where metadata (the creation time of each folder) did the trick.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having arrived at the end of the chapter, we will summarize the advice we have
    discussed along the way so you can organize your validation strategy and reach
    the end of a competition with a few suitable models to submit.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we first analyzed the dynamics of the public leaderboard, exploring
    problems such as adaptive overfitting and shake-ups. We then discussed the importance
    of validation in a data science competition, building a reliable system, tuning
    it to the leaderboard, and then keeping track of your efforts.
  prefs: []
  type: TYPE_NORMAL
- en: Having discussed the various validation strategies, we also saw the best way
    of tuning your hyperparameters and checking your test data or validation partitions
    by using adversarial validation. We concluded by discussing some of the various
    leakages that have been experienced in Kaggle competitions and we provided advice
    about how to deal with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are our closing suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: Always spend the first part of the competition building a reliable validation
    scheme, favoring more a *k*-fold over a train-test split, given its probabilistic
    nature and ability to generalize to unseen data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your validation scheme is unstable, use more folds or run it multiple times
    with different data partitions. Always check your test set using adversarial validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep track of results based on both your validation scheme and the leaderboard.
    For the exploration of possible optimizations and breakthroughs (such as magic
    features or leakages), trust your validation score more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we explained at the beginning of the chapter, use your validation scores
    when deciding your final submissions to the competition. For your final submissions,
    depending on the situation and whether or not you trust the leaderboard, choose
    among your best local cross-validated models and good-scoring submissions on the
    leaderboard, favoring the first over the second.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point of our journey, we are ready to discuss how to tackle competitions
    using tabular data, which is numeric or categorical data arranged in matrices
    (with rows representing the examples and columns the features). In the next chapter,
    we discuss the Tabular Playground Series, a monthly contest organized by Kaggle
    using tabular data (organized by *Inversion*: [https://www.kaggle.com/inversion](https://www.kaggle.com/inversion)).'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we will introduce you to some specific techniques to help you shine
    in these competitions, such as feature engineering, target encoding, denoising
    autoencoders, and some neural networks for tabular data, as an alternative to
    the recognized state-of-the-art learning algorithms in tabular data problems (the
    gradient boosting algorithms such as XGBoost, LightGBM, or CatBoost).
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code40480600921811704671.png)'
  prefs: []
  type: TYPE_IMG
