- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding the ML Model Development Life Cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore the different steps that exist in a typical
    AI/ML project. This information is an important foundation for an AI/ML solutions
    architect role because you will need to advise companies on how to implement these
    steps efficiently. It is also a foundation for the rest of the contents of this
    book, as in later chapters, you will create your own machine learning projects,
    and it’s important that you understand the steps in the process. We will also
    explore the concept of MLOps in this book and how the ML model development life
    cycle serves as the basis for the MLOps paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the ML model development life cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common challenges encountered in the ML model development life cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for overcoming common challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of the ML model development life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may be familiar with the concept of the **software development life cycle**
    (**SDLC**), which is taught in computer science classes in schools all over the
    world. The SDLC concept began to be formulated in the 1960s and early 1970s, and
    by now it is a well-established and well-understood process that is used in various
    formats by pretty much every company that develops software. Without formalized
    processes for people to follow when developing software, it would be difficult
    for companies to efficiently produce high-quality software and the software development
    industry would be quite chaotic. In fact, that’s how the software development
    industry was in its early years, and that’s how the machine learning industry
    currently is for most companies. Only in the past couple of years has the industry
    started to establish some structure around how companies should develop ML models
    and their related applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we provide a high-level overview of the ML model development
    life cycle, outlining each of the steps that you will encounter in most machine
    learning projects. Let’s begin with a quick recap of the SDLC. We will make references
    to this more well-established set of processes where relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The term **MDLC**, representing **model development life cycle**, was originally
    coined by a friend and colleague of mine named Fei Yuan. He and I worked on building
    an MLOps process at Amazon before the term “MLOps” began to be used in the industry.
    That’s not to say that we were the only people trying to automate the steps in
    data science projects. For example, the technical reviewer for this book shared
    with me that he and some colleagues had implemented MLOps-type workloads in the
    early 2000’s with SAS and a process model called CRISP-DM, which stands for CRoss
    Industry Standard Process for Data Mining. You can learn more about CRISP-DM at
    the following URL: [https://www.datascience-pm.com/crisp-dm-2/](https://www.datascience-pm.com/crisp-dm-2/).
    Fortunately, in recent years, MLOps has become an important and popular concept
    in ML model development, and now there are lots of MLOps tools available, which
    we will cover later in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: SDLC – a quick recap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first and simplest versions of the SDLC is known as the Waterfall
    model because the flow of activities in the process is sequential, where the deliverables
    from each activity serve as dependencies for the next activity in the flow. *Figure
    2**.1* shows the original Waterfall diagram from a paper titled *Managing the
    development of large software systems* by Winston Royce in 1970.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: SDLC Waterfall model](img/B18143_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: SDLC Waterfall model'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the process starts by gathering and analyzing the requirements
    that the system needs to satisfy and then designing, coding, and testing the software
    before deploying it for use. After the resulting software has been deployed, you
    then need to manage it via ongoing operations activities. This model was later
    updated to include feedback loops between the various stages. For example, the
    feedback from testing could result in an updated coding step, which in turn could
    result in an updated program design step, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: A well-known limitation of the Waterfall model is that the process doesn’t facilitate
    the rapid innovation or flexibility that is required in today’s fast-paced software
    development industry, whereby new requirements often come to light during the
    development, testing, and deployment phases. The software design needs to be updated
    frequently, and the various stages in the process are more cyclical in nature
    to allow updates to occur more flexibly (see *Figure 2**.2*). As a result, newer
    development methodologies such as Agile have emerged. Nevertheless, the methodical
    sequence of events from gathering requirements through to designing, coding, testing,
    deploying, and monitoring software still exists in various forms in system design
    projects, and this extends to ML systems and projects.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: A cyclical approach to software development, often referred to
    as DevOps (source: https://openclipart.org/download/313185/1546764098.svg)](img/B18143_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: A cyclical approach to software development, often referred to
    as DevOps (source: https://openclipart.org/download/313185/1546764098.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: Typical ML project stages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Interestingly, it took a while for the lessons from the traditional software
    development industry to be applied to ML model development. When ML development
    suddenly experienced a huge uptick in popularity during the past few years, many
    companies dived into the race without formalized processes in place, and as a
    result, companies ran into their own random unexpected issues without much ability
    to standardize across the industry. Fortunately, lessons have been learned from
    the early pioneers in this process, and standardized project activities have emerged.
    The following are the kinds of steps that you can expect to take in most ML development
    projects:'
  prefs: []
  type: TYPE_NORMAL
- en: Gather, analyze, and understand the business requirements for which the model
    will be developed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find and gather relevant data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore and understand the contents of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform or manipulate the data for ML model training, which may include feature
    engineering and storing features for use in later steps. This step is often also
    closely linked to *step 6* because the selected algorithm may have specific requirements
    regarding how the data needs to be presented.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For supervised learning models, label the data if the required labels are not
    already present in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick an algorithm that suits the requirements of the business case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure and tune hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monitor the model after deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 2**.3* shows a visual representation of these steps, and we will dive
    into these steps in detail in the coming sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: Typical ML project stages](img/B18143_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: Typical ML project stages'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are some similarities between the ML model development
    process and the traditional SDLC, but there are also some differences that are
    unique to ML model development. Most notable is the inclusion of data in the process.
    The fact that we now need to include the manipulation of data in our overall process
    adds a lot of complexity, as you will see when we go through each of the process
    steps in more detail. It should be noted that each step in the overall life cycle
    is often cyclical in nature, whereby the data science team may need to perform
    each task—or combinations of tasks—multiple times, with different inputs and outputs,
    using a trial-and-error methodology until they find the optimal approach to use
    for each step.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering, analyzing, and understanding the business requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This step is often omitted from ML life cycle diagrams because such diagrams
    usually focus on the technical steps, which follow later in our project. This
    can be considered as step zero because this generally needs to happen before any
    of the technical steps in our project begin. Just like in traditional software
    development, the overall process must begin with gathering and understanding the
    business requirements that the model will be built to address. For example, will
    the models produced by our project be used to forecast sales revenue for the next
    year or are we setting out to build an application that will monitor people’s
    health data and provide them with health-related recommendations based on that
    data? The business requirements influence the decisions we make in the later steps
    of our project, such as what kinds of data we need to gather, what ML algorithms
    we will use to train our models, and what kinds of metrics we will measure in
    relation to our models’ performance.
  prefs: []
  type: TYPE_NORMAL
- en: In this part of an AI/ML project, the solutions architect will work with business
    leaders to understand what they want to achieve from a business perspective and
    will then work with technical personnel to translate the business requirements
    into technical requirements. Defining the technical requirements is one of the
    first steps in defining the overall strategy to meet the business objectives outlined
    by the business leadership. This includes identifying any constraints that may
    exist, such as working with data scientists to determine what kinds of data would
    be required to address the business goals and whether that data can be gathered,
    generated, or procured from somewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Finding and gathering relevant data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We briefly touched on this topic in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015).
    Data is what ML models learn from, so without data, there is no machine learning.
    If the project team—including the data scientists and data engineers (we will
    explain these roles in more detail later)—cannot figure out how to get the data
    that would be required to meet the business objective, then the project could
    be a non-starter right from the beginning, so this is a critical step in the process.
    Sources of data vary based on the type of project, but the following are some
    examples of data that could be used for various AI/ML use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Historical data that contains the details of customer credit card transactions
    and/or banking transactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data relating to what customers are purchasing online
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Housing sales data in a particular region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log entry data that contains details of a technical system’s operational events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Health data that is tracked by wearable devices such as watches or fitness trackers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data gathered from people filling in forms or surveys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data streamed from **Internet of Things** (**IoT**) devices such as factory
    conveyor belts or a fleet of construction vehicles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, there are different types of data that can be used for many
    different purposes. The data science team’s first major task will be to define
    and locate the data that needs to be used for the project. This is not an atomic
    activity in that it does not need to happen all at once at the beginning of a
    project. Often, the science team begins with some idea of the data they need,
    and then they may refine the data requirements based on testing and feedback in
    later steps of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and understanding the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the data science team has gathered data that they believe could be used
    to address the business requirements, they usually don’t just dive into training
    ML models on that data. Instead, they usually need to inspect the data to assess
    whether it really can be used adequately for the purposes of the project. Raw
    data is often not in an optimal state to be used by some ML algorithms. Let’s
    take a couple of examples from our list of potential data sources. If we’re using
    data gathered from people filling in forms or surveys, people may input the details
    incorrectly. They may leave some fields blank or misspell some of the details
    during input. As another example, if we’re using data from sensors such as wearable
    health trackers or other IoT devices such as mechanical machinery sensors, those
    sensors may malfunction and record corrupted data. As such, data scientists often
    need to inspect the data and look for errors, anomalies, or potentially corrupted
    data. In [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015), we also mentioned that
    data scientists may want to get statistical details regarding the data, such as
    the range of values that are generally seen for particular variables in the data
    or other statistical distribution details. In the hands-on activities later in
    this book, we will be using data visualization tools and other data inspection
    tools to explore and understand the contents of our datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming or manipulating the data for ML model training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Missing or corrupted data can cause problems when training ML models. Some algorithms
    that need to operate on numeric data will produce errors if they encounter non-numeric
    values, including null and garbled/corrupted characters. Even for algorithms that
    can handle such values gracefully, the values may skew the learning process in
    unexpected ways, thus affecting the performance of the resulting models.
  prefs: []
  type: TYPE_NORMAL
- en: When data scientists find that a dataset isn’t perfectly ready for use in training
    ML models, they don’t usually just give up but rather try to make changes to the
    data to bring it closer to the desired state. We call this process **feature engineering**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Some literature publications use the term “feature engineering” only to refer
    to the process of creating new features from existing features (such as our price
    per square foot example), while other literature uses the same term to describe
    all activities related to manipulating features in our dataset, including replacing
    missing values.
  prefs: []
  type: TYPE_NORMAL
- en: This could include data cleaning (or cleansing) techniques, such as replacing
    missing data with something more meaningful. As an example, let’s imagine that
    some medical condition is more likely to occur as a person gets older and we want
    to build a model that predicts the likelihood of this condition occurring. In
    this case, a person’s age would be an important input feature in our dataset.
    During our data exploration activities, if we discover that some of the records
    in our dataset are missing the age value, we could compute the average age of
    all of the people in our dataset and replace each missing age value with the average
    age value. Alternatively, we could replace each value with the modal (i.e., the
    most frequently occurring) age value. Either of these cases would at least be
    better than having missing or corrupted values in our dataset during the training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the optimal variables and values for addressing specific business requirements
    may not be readily available in whatever raw data we can access. Instead, data
    scientists often need to combine data from different sources and come up with
    clever ways to derive new data from the available data. A very simple example
    would be if we specifically need a person’s age as an input variable but the dataset
    only contains their date of birth. In that case, the data scientist could add
    another feature, age, to the dataset, and subtract the date of birth from the
    current date in order to calculate the person’s age. A slightly more complex example
    would be if we wanted to predict housing prices and we determined that price per
    square foot would be an important input feature for our model but our dataset
    only contains the total price for each house and the total area of each house
    in square feet. In that case, to create our price per square foot input feature
    for each house, we could divide the total cost of each house by the total area
    of that house in square feet and then add that as a feature in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to understand that when a data scientist has created the features
    that are important for training their model, they will often want to store those
    features somewhere for later use, rather than needing to re-create them again
    and again. Later in this book, we will explore tools that have been developed
    for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Data labeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015), supervised
    learning algorithms rely on labels in the dataset during the training process,
    which tell the models what the correct answers are for the types of data relationships
    that the model is trying to learn. *Figure 2**.4* shows our example of a labeled
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: An example of labels (highlighted in green) in a dataset](img/B18143_02_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: An example of labels (highlighted in green) in a dataset'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re lucky, you will find a dataset that can be used to address your business
    requirements and already contains the requisite labels or “correct answers” for
    the variables that you want to predict. If not, you will need to add the labels
    to the dataset. Again, considering that your dataset could contain millions of
    data points, this could be a significantly complex and time-consuming task to
    take on. And, just like any of the other features in your dataset, the quality
    of your labels directly impacts how reliable your model’s predictions will be.
    With this in mind, you will need access to a labor force that can accurately label
    your dataset and other tools that facilitate the labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Another data preparation step that is used for supervised learning algorithms
    is to split the dataset into three different subsets, which are used for the training,
    validation, and testing of the model, respectively. We will describe the use of
    these subsets in the model training section later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: An important thing to call out at this point is the concept of **data leakage**,
    which refers to a scenario in which information from outside the training dataset
    is used to create the model. This can cause the model to perform well on the training
    data (because it has information it wouldn’t have in a real-world scenario) but
    perform poorly in production due to these unintentional hints.
  prefs: []
  type: TYPE_NORMAL
- en: There are various causes that can lead to data leakage, such as how and when
    we split our datasets during our data science project, or how we label our data.
    For example, during data preparation activities such as labeling or feature engineering,
    we could accidentally include knowledge that would not be available to the model
    in a real-world application when the model needs to make predictions. Consider
    a scenario in which we are using historical data to train our model. We may accidentally
    include information that only became available after the events that are represented
    in the dataset actually occurred. While this data may be relevant and may help
    influence the outcome, it will harm our model performance if that information
    would not be available to our model in a real world scenario at the time when
    our model needs to make a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Picking an algorithm and model architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are lots of different types of ML algorithms that can be used for various
    purposes, with new algorithms and model architecture patterns emerging regularly.
    In some cases, choosing your approach is an easy decision because there are some
    algorithms and model architectures that are particularly suited to specific use
    cases. For example, if you want to implement a computer vision use case, then
    something such as a convolutional neural network architecture would be a good
    starting point. On the other hand, choosing what kind of ML algorithm and implementation
    to use for a specific problem can be a difficult task that often depends on the
    experience of the data science team. For example, an experienced team of data
    scientists will have worked on many different projects and developed a working
    understanding of what kinds of algorithms work best for various circumstances,
    whereas a less experienced data science team may need to perform a lot more experimentation
    with various algorithms and model architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to direct business requirements, such as “we need a computer vision
    model to identify manufacturing defects”, the chosen algorithm can also depend
    on less-tangible business requirements, such as “the model needs to run on limited
    computing resources” or “the explainability of the model is extremely important
    in this use case.” Each of the aforementioned requirements puts different constraints
    on the types of algorithms the data science team could select for the given use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: As with most of the steps in the overall AI/ML project life cycle, selecting
    the best algorithm and model architecture to use can require the data science
    team to implement a cyclical trial-and-error approach, whereby they may experiment
    with different algorithms, architectures, and inputs/outputs until they find the
    optimal implementation. We will be exploring various algorithms and their unique
    characteristics in the hands-on activities later in this book, but overall, it
    is best to start with a simple baseline model, so that we have a starting point
    to compare metrics and understand the base dataset. Then, we can test out more
    complex models and assess whether they perform better.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is probably the most well-known activity in the AI/ML project life cycle.
    It’s where the model actually learns from the data. For unsupervised algorithms,
    this is where they may form the clusters that we talked about in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    for example. For supervised algorithms, this is where our training, validation,
    and testing datasets come into the picture. In [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    we briefly talked about how linear algebra and calculus can be used in machine
    learning. If we take our linear regression example, this is exactly where those
    concepts would come into play. Our model would first try to find the relationships
    between the features and the labeled target outputs. That is, it would try to
    find the coefficients for each of our features, which, when used in combination
    (e.g., by adding them all together), would produce the labeled target output.
    It tries to calculate the coefficients that would work for all data points in
    the dataset, and to do this, it needs to scan through all of the items in the
    training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model usually starts this process with a random guess, so it inevitably
    is incorrect on the first try. However, it then calculates the errors and makes
    adjustments to minimize those errors in future iterations through the dataset.
    There are a number of different methods and algorithms that can be used to minimize
    errors, but a very popular method is called gradient descent. We briefly mentioned
    gradient descent in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015), but we’ll
    talk about it in more detail here. In gradient descent, the algorithm works on
    finding the minimum point of what we call the loss function, which is a representation
    of the errors that are produced when our model tries to guess the coefficients
    of the features that produce the labeled outputs for each data point in our dataset.
    *Equation 2.1* shows the equation for calculating the **mean squared error** (**MSE**)
    as an example for a loss function for linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 2.1: Mean squared error formula'
  prefs: []
  type: TYPE_NORMAL
- en: In *Equation 2.1*, *n* represents the number of data points in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what this formula is saying, let’s start with the section in
    parenthesis: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math>](img/2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/3.png)
    represents our model’s predicted target variable for each datapoint and ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/4.png)
    represents the true target variable’s value for each datapoint.'
  prefs: []
  type: TYPE_NORMAL
- en: Within the parentheses in *Equation 2.1*, we subtract the true value from our
    model’s predicted value in order to calculate the error of our model’s prediction,
    similar to what we described in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    and we then square the result. In this case, we are calculating what’s referred
    to as the **Euclidean distance**, which is the distance between the predicted
    value and the true value in two-dimensional space. Squaring the result also has
    the effect of removing negative values from the results of our subtractions.
  prefs: []
  type: TYPE_NORMAL
- en: The summation symbol, *Σ* (sigma), in the equation represents adding up all
    of the calculated errors for all data points in our training dataset. Then, we
    divide the final result—i.e., the total error for all predictions—by the number
    of data points in our dataset in order to calculate the average (or mean) error
    for all of our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that we want to minimize the error in each training iteration, by
    finding the minimum point of this loss function (also referred to as the **objective
    function**). To get an understanding of what it means to find the minimum point
    in a loss function, it helps if we can graph the function. *Figure 2**.5* shows
    an example of a two-dimensional loss function graph for the MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: MSE loss function graph showing minimum point](img/B18143_02_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: MSE loss function graph showing minimum point'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each time the algorithm calculates the loss from each training iteration, that
    loss value can be represented as a point on the graph. Considering that we want
    to move toward the minimum point in order to minimize the loss, we want to take
    a step downward on the graph. Whenever we want to move from one point to another
    (even when moving our bodies around in real life), there are two aspects of the
    movement that we need to determine: direction and magnitude, i.e., in which direction
    do we want to move and how far? This is where gradient descent comes into the
    picture. It helps us to determine the direction in which we should move in order
    to progress towards the minimum point. Let’s take a look at how it works in more
    detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine the graph in *Figure 2**.5* is a valley between two mountains and we
    are standing at a point that represents the loss calculated in our most recent
    training iteration. That point is somewhere on the side of the valley, such as
    the location shown in *Figure 2**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6: MSE loss function graph showing current location](img/B18143_02_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: MSE loss function graph showing current location'
  prefs: []
  type: TYPE_NORMAL
- en: For a human, walking downhill is somewhat instinctive because we have sensory
    input that tells us which way is downhill. For example, our feet can feel the
    slope of the hill at our current location, we can feel the pull of gravity downwards,
    and we may also be able to see our surroundings and therefore see which way is
    downhill. However, our gradient descent algorithm does not have these sensory
    inputs and it can only use mathematics to find out which way is downhill. Then,
    it needs to use programmatic methods to define what it means to take a step in
    that direction. Our algorithm knows the current location on the graph and it can
    calculate the derivative of the function (a concept from differential calculus)
    to determine the slope of the graph at the current location. *Figure 2**.7* shows
    an example of the slope of a line at a point on the graph, which represents the
    derivative of the function at that point.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7: Derivative of the loss function at a particular point](img/B18143_02_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Derivative of the loss function at a particular point'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the derivative has been calculated, this information can be used by our
    algorithm to take a step toward the minimum point. *Equation 2.2* shows how each
    next step is calculated for gradient descent in the context of linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>θ</mi><mi>j</mi></msub><mo>=</mo><msub><mi>θ</mi><mi>j</mi></msub><mo>−</mo><mi>α</mi><mfrac><mn>1</mn><mi>m</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo>(</mo><msub><mi>h</mi><mi>θ</mi></msub><mfenced
    open="(" close=")"><msup><mi>x</mi><mfenced open="(" close=")"><mi>i</mi></mfenced></msup></mfenced><mo>−</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow></mrow></mrow></mrow></math>](img/5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 2.2: Gradient descent for linear regression'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Equation 2.2*, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>θ</mi><mi>j</mi></msub></mrow></math>](img/6.png)represents
    the location on the graph, and Ɵ represents the vector of coefficients for the
    features of each data point in our dataset. Remember that we are trying to find
    the set of coefficients for our features that results in the least error between
    our model’s predictions and the true values of our data points’ target variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>h</mi><mi>θ</mi></msub><mfenced
    open="(" close=")"><msup><mi>x</mi><mfenced open="(" close=")"><mi>i</mi></mfenced></msup></mfenced></mrow></mrow></math>](img/7.png)then
    represents the predicted target variable for each data point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></math>](img/8.png)represents
    the true target value for each data point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see, in the broader set of parentheses, we are again subtracting the
    true target value from the predicted target variable value for each data point.
    This is because *Equation 2.2* is derived from *Equation 2.1* (the mathematical
    proof of this derivation is omitted here for simplicity).
  prefs: []
  type: TYPE_NORMAL
- en: '*m* represents the number of features we have for each data point in our dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/9.png)
    is what we refer to as the **learning rate**. It’s one of the hyperparameters
    of our algorithm and it determines the size of the step we should take; i.e.,
    the magnitude by which we should move in the selected direction.'
  prefs: []
  type: TYPE_NORMAL
- en: Altogether, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mstyle
    scriptlevel="+1"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mrow><mo>(</mo><msub><mi>h</mi><mi>θ</mi></msub><mfenced
    open="(" close=")"><msup><mi>x</mi><mfenced open="(" close=")"><mi>i</mi></mfenced></msup></mfenced><mo>−</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow></mrow></mrow></mrow></math>](img/10.png)represents
    the derivative of the loss function at our current location on the graph. Therefore,
    *Equation 2.2* says that our next location will be equal to our current location
    minus the derivative of the loss function at our current location on the graph
    multiplied by the learning rate. This then takes a step toward the minimum point,
    whereby the derivative of the loss function determines the direction and the combination
    with the learning rate defines the magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: It’s very important to note that *Equation 2.2* represents just one step in
    the gradient descent process. We iterate over this process many times, in each
    iteration scanning through the dataset and inputting our estimated coefficients,
    calculating the error/loss, and then reducing the loss by using gradient descent
    to step toward the loss function’s minimum point. We may never exactly reach the
    minimum point but even if we can get quite close, our model’s estimates and predictions
    could be acceptably accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should note that there are different configurations for gradient descent.
    In batch gradient descent, we would go through the entire training set in each
    iteration. Alternatively, we could implement mini-batch gradient descent, in which
    case each iteration would process subsets of the training dataset. This approach
    is less thorough but can be more efficient. A popular implementation is known
    as stochastic gradient descent (the word “stochastic” means “random”). In stochastic
    gradient descent, we take a subset of random samples from our dataset in each
    iteration, which could be as little as one data point in each sample. The key
    here is that because we take a random subset in each iteration, we start from
    a different point in our feature space each time. At first, this seems somewhat
    erratic, as we jump around to different points in our feature space. However,
    this approach has been shown to be quite effective in minimizing the overall loss
    function. It can also help to avoid something referred to as **local minima**,
    which refers to the fact that some loss functions are not as simple as the one
    we showed in *Figure 2**.5*. They may have multiple peaks and valleys, in which
    case the bottom of any of the valleys could be considered a type of minimum point,
    but a local minimum may not be the overall minimum of the function, which is referred
    to as the **global minimum**. *Figure 2**.8* shows an example of multiple minima
    and maxima:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8: Local and global minima and maxima](img/B18143_02_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Local and global minima and maxima'
  prefs: []
  type: TYPE_NORMAL
- en: Although we focused on two-dimensional loss functions in this section, loss
    functions can have more than two dimensions, and the same concepts also apply
    in higher-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we chose a specific algorithm (linear regression) and type
    of dataset (tabular) to illustrate an example of the model training process. There
    are, of course, other algorithms and types of data for different use cases, and
    the training processes for those use cases would have their own unique implementations.
    However, the overall model training process generally involves processing the
    input data to try to find some kind of useful pattern or relationship and then
    incrementally honing the accuracy of that pattern or relationship in a repetitive
    way until some determined threshold of accuracy is met or until the training is
    deemed to be ineffective (if the model is failing to effectively learn any useful
    patterns or relationships) and therefore is terminated.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and tuning hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hyperparameters are parameters that define aspects of how your model training
    jobs run. They are not the parameters in your dataset from which your models learn
    but rather external configuration options related to how the model training process
    is executed. To start with a simple example: we’ve discussed that training jobs
    often need to cycle through the training dataset multiple times in order to learn
    patterns in the data. One of the hyperparameters you may configure for a model
    training job would be to specify how many times it should cycle over the dataset.
    This is often referred to as the number of **epochs**, where an epoch represents
    one iteration through the training dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the optimal values for your hyperparameters is another activity that
    usually requires a lot of trial-and-error attempts, in which you may need to experiment
    with different combinations of hyperparameter values in order to find the optimal
    settings to maximize your model training performance. Let’s continue our simple
    example of configuring the number of times the training job should process through
    the training dataset. Considering that the model could potentially learn more
    information each time it runs through the dataset, we may at first think that
    deciding on the number of epochs would be a simple choice, i.e., that we should
    just set this value to be very high so that the model learns more from the data.
    However, in reality, this is often not the best option because it’s not always
    true that a model learns more useful information every time it processes through
    the dataset. There are concepts in machine learning called **underfitting** and
    **overfitting**, which we will explore in the *challenges* section of this chapter.
    They relate to problems in which continued training on the existing dataset will
    fail to yield desired results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even in cases when the model does learn useful information each time it processes
    the dataset, it often reaches a point where the rate at which it is learning new
    information will taper off or reach a plateau. When this happens, it would be
    inefficient to continue running through the dataset again and again. Bear in mind
    that it can be very expensive to train a model on a large dataset, so you do not
    want to keep processing the data when the model is reaching a plateau in its learning.
    We can measure the rate of learning by generating a **learning curve** graph,
    which graphs the training errors during the training process. *Figure 2**.9* shows
    an example of a learning curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9: Learning curve graph](img/B18143_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: Learning curve graph'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2**.9*, the gap between the blue and red lines represents the error
    between our model’s predictions on the training dataset and the testing dataset.
    When the graph shows that the gap between the blue and red lines is not significantly
    decreasing, then we know that continued training will not make our model significantly
    more accurate, and it may be a good point at which to stop the training process.
  prefs: []
  type: TYPE_NORMAL
- en: An analogy would be if we had a student who had read a book so many times that
    they memorized every word and thoroughly understood every concept in the book.
    At that point, it would be unnecessary to continue instructing the student to
    read the book again and again because they would no longer learn anything new
    from that book. Now let’s also imagine that we needed to pay the student every
    time they read the book, which is analogous to paying for the computing resources
    required to train the model on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the number of epochs is just one example of trying to find the best
    configuration for a particular hyperparameter. Different types of algorithms have
    different types of hyperparameters that pertain to them, and trying to test all
    of the various combinations of hyperparameter values can be a painstaking and
    time-consuming task for data scientists. Fortunately, Google Cloud has a tool
    that performs this task for us in an automated fashion, which we will explore
    later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally! We have found the right combinations of data, algorithms, and hyperparameter
    values and we’ve gotten to a point where our model is ready to be used in the
    real world, which we refer to as **hosting** or **serving** our model. Getting
    to this point has required a lot of work. We may have trained hundreds of different
    versions of our model to get it to a point at which it’s ready to be deployed
    in production. Our data science team may have had to experiment with lots of different
    dataset versions and transformations and lots of different algorithms and hyperparameter
    values in order to finally get some meaningful results or insights. Up until only
    around five years ago, performing all of those steps and tracking their results
    would have been a very slow, manual, and painstaking process. It still can be
    a lot of work, but at least now there are tools that automate a lot of these steps
    and they can be completed much more quickly, perhaps taking weeks instead of months.
    Later in this book, we’ll talk about something called AutoML, which can reduce
    this entire process down to a few minutes or hours in just a few short commands!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying our model can be as simple as packaging it into a Docker container
    and deploying the container on a server, although we will usually want to create
    some kind of web-based API to facilitate access to the model by applications.
    We will do this in a hands-on activity later. Also, when we cover MLOps, we’ll
    see how it may make sense for us to create pipelines to automate the deployment
    of our model, in much the same way as we use CI/CD pipelines to build and deploy
    regular software applications.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring the model after deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You might think that once you’ve successfully tested and deployed the model
    to production then your job is done. However, the fun doesn’t stop there! Just
    like regular software, you need to monitor the performance of your model on an
    ongoing basis. This includes traditional monitoring, such as keeping track of
    how many requests your model is serving in a given timeframe (per second, for
    example), how long it takes for your model to respond to a request (latency),
    and whether these metrics are changing over time. However, ML models also have
    additional requirements that need to be monitored, such as ML-specific metrics
    such as those mentioned in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015) (MAE,
    MSE, accuracy, precision, etc.). These metrics help us to understand how our models
    are performing from an inference perspective, so we need to monitor them and ensure
    they continue to meet our business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The stages in the ML model development life cycle that you’ve learned about
    in this section form the basis for understanding MLOps and AutoML. We dedicate
    an entire chapter of this book to ML engineering and MLOps on Google Cloud, but
    for now, at a high level, you can consider the goals of MLOps and AutoML to be
    the automation of all of the steps in the ML model development life cycle. You’ll
    see in the chapter on MLOps that there are tools we can use to create pipelines
    that automate all of the outlined steps. We can have complex combinations of pipelines
    within pipelines, which would automate everything from preparing and transforming
    the input data to training and deploying the models, monitoring the models in
    production, and automatically kicking off the entire process all over again if
    we detect that our model has stopped providing desirable results and we want to
    retrain the models on updated data. This would provide a self-healing model ecosystem,
    which helps to keep our models up to date on an ongoing basis.
  prefs: []
  type: TYPE_NORMAL
- en: Roles and personas in AI/ML projects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Throughout this book, we mention various roles such as data scientist, data
    engineer, and ML engineer. We also mention more traditional roles such as software
    engineer, project manager, stakeholder, and business leader. The traditional roles
    have been defined in the industry for decades now, so we will not define them
    here, but there is often confusion about the newer roles that are specific to
    AI/ML projects, so we’ll take some time to briefly describe them here. In small
    teams, it should be noted that a single person may perform all of these roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data engineer**: Data engineers are usually involved in the earlier stages
    of the data science project—specifically the data gathering, exploration, and
    transformation stages. A data engineer will often be tasked with finding relevant
    data and cleaning it up to be used in later stages of the project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data scientist**: A data scientist is usually the role we associate with
    actually training ML models. They will usually also perform data gathering, exploration,
    and transformation activities as they iterate through various model training experiments.
    In some cases, data scientists will be the more senior members working on the
    project and they may provide direction to the data engineers and ML engineers.
    They will often be responsible for the resulting ML models that are created, although
    those models are created and deployed with help from the data engineers and ML
    engineers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML engineer**: The ML engineer role usually refers to a software engineer
    who has ML or data science expertise. They understand ML concepts and they are
    experts in the stages of the model development life cycle. They are usually the
    bridge that brings DevOps expertise to an ML project in order to create an MLOps
    workload. When a data scientist creates a model that they believe is ready to
    be used in production, they may work with an ML engineer to put all of the mechanisms
    in place to actually deploy the model into production via an MLOps pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve covered the major steps and concepts found in a typical AI/ML
    project, let’s take a look at the kinds of pitfalls that companies often run into
    when trying to implement such projects.
  prefs: []
  type: TYPE_NORMAL
- en: Common challenges encountered in the ML model development life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For some of the stages in the ML model development life cycle, we’ve already
    discussed various challenges that you are likely to encounter in those stages.
    However, in this section, we specifically call out major challenges that you need
    to be aware of as an AI/ML solutions architect interacting with companies who
    are implementing AI/ML workloads. In the *Best practices for overcoming common
    challenges* section later in this chapter, we’ll look at ways to overcome many
    of these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Finding and gathering relevant data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of our first major challenges is finding relevant data for the business
    problem our models are being built to address. We presented some examples of potential
    data sources in the previous section, and in some cases, the data you need may
    be readily available to you, but finding the relevant data is not always straightforward
    for data scientists and data engineers. The following are some common challenges
    with regard to finding and accessing the right data:'
  prefs: []
  type: TYPE_NORMAL
- en: If you work in a large company, the data may exist somewhere in your company
    owned by another team or organization, but you may not know about it or you may
    not know how to find it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data might need to be created from a combination of different data sources
    that are dispersed throughout your company, all owned by disjointed organizations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data could contain sensitive information, and therefore be subject to regulations
    and restrictions regarding how it needs to be stored and accessed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may need to consult with experts in order to find, validate, and understand
    the data, e.g., financial data, medical data, atmospheric data, or other data
    related to fields of specific expertise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data may be stored in databases that are restricted for use only in production
    transaction operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may not know whether you can trust the contents of the data. For example,
    is it accurate?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data could contain inherent biases or other unknown challenges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Picking an algorithm and model architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to picking an algorithm or model architecture to use, one of the
    biggest challenges initially can be just figuring out where to start. You don’t
    want to spend months just experimenting with different options before finding
    a useful implementation or never finding a useful implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Data labeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data labeling can be a very manual task, requiring humans to go through enormous
    amounts of data and add the labels for each data point. Tools have been built
    in recent years that automate some labeling tasks or make the tasks easier for
    humans to perform, but there is still a need to have humans involved in labeling
    datasets. So, a major challenge that can exist for companies is finding and hiring
    a strong data-labeling workforce. Bear in mind that some data labeling tasks may
    require specific expertise. For example, consider a dataset consisting of medical
    images. A specific characteristic in the image could indicate the presence of
    a particular medical condition. It often requires special training to be able
    to read the medical images and identify the specific characteristics in question,
    so this is not a task for which you could hire any random person. As we’ve discussed
    previously, if your labels are not accurate, then your models will not be accurate,
    and in the example of medical imaging described here, this could have critical
    implications for a medical facility that is tasked with diagnosing life-threatening
    medical conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Training models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Two classic challenges with regard to training models are the problems of **underfitting**
    and **overfitting**. These challenges relate to how well your model learns a relationship
    or pattern in your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of supervised learning, we generally split our dataset into the
    three subsets mentioned earlier: training, validation, and testing. The validation
    set is usually used in hyperparameter tuning, the training dataset is what the
    model is trained on, and the testing dataset is how we evaluate the trained model.
    We evaluate the model based on the metrics that we’ve defined for that model,
    such as accuracy, precision, or MSE. In this context, the test dataset is new
    data that the model did not see during the model training process and we want
    to determine if the model’s predictions are accurate when it sees this new data—i.e.,
    we want to determine how well the model **generalizes** to new data. If a model
    provides very accurate predictions on the training dataset but inaccurate or less
    accurate predictions on the testing dataset then we say that the model is overfitting.
    This means it fits too closely to the training data, and it cannot perform well
    when it is exposed to new data.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if we find that the model is not performing well on either
    dataset (training or testing), then we say it is underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.10* shows an example of fitting, overfitting, and underfitting
    for a classification model that is trying to determine the difference between
    the blue and red data points. The black boundary line represents overfitting because
    it fits much too precisely to the dataset, and the purple line represents underfitting
    because it does not do a good job of capturing the differences between the blue
    and red dots. The green line does a pretty good job of separating the blue and
    red dots; it’s not completely perfect, but it could be an acceptable model that
    generalizes well to the characteristics of the data points.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10: Example of fitting, overfitting, and underfitting](img/B18143_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Example of fitting, overfitting, and underfitting'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to classic training challenges such as those mentioned, there are
    other challenges that relate to the overall process of how training is performed
    as part of a broader AI/ML project, such as lineage tracking, as we mentioned
    in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015). In large AI/ML projects, there
    may be multiple teams of data scientists performing experiments and training hundreds
    of models. Keeping track of their results and sharing them with each other can
    be very challenging in large projects.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and tuning hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finding the optimal set of hyperparameter values can be almost as challenging
    as curating a usable dataset or choosing the correct algorithm to use. Considering
    that hyperparameters affect how our model training jobs operate, each job can
    take a long time to run, and there can be thousands of combinations of hyperparameter
    values to explore, doing this manually can be very challenging and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although we generally perform some validation and testing during the training
    and hyperparameter tuning processes, we should thoroughly evaluate our model before
    deploying it in production. In the section titled “Common challenges in developing
    machine learning applications” in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    we discussed the challenge of delivering business value from data science projects.
    We called out the need for data scientists to work with business stakeholders
    to thoroughly understand the business requirements that the target AI/ML system
    is intended to address. The evaluation step in our data science project is where
    we check to ensure that the models and solutions we’ve created adequately address
    those business requirements, based on the metrics we defined for measuring success.
    In addition to the data science team evaluating the models, it may also be relevant
    at this point to review the results with the business stakeholders to ensure they
    align with expectations. If we find that the results are not satisfactory, we
    usually need to retry the process from earlier in the data science lifecycle,
    perhaps with new or different data, different algorithms, and/or different hyperparameter
    values. We may need to repeat these steps in an interactive process until we appropriately
    satisfy the business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When deploying our model, we’ll need to select the kinds of computing resources
    that are required to serve our model adequately. Depending on the model architecture,
    we may need to include GPUs/TPUs, usually also in combination with CPUs, and,
    of course, RAM. A really important activity at this point in the project is to
    “right-size” these components. To do this, we’ll need to estimate how much of
    each type of component will be required to serve our model as accurately as possible
    given the traffic we expect to receive in terms of requests per second to our
    model. Why is this so important? Well, model hosting is often reported by companies
    to be by far their number one cost when it comes to their AI/ML. It can account
    for up to 90% of a company’s AI/ML costs. So, if we configure our servers with
    more resources than we need, it will increase those costs. On the other hand,
    if we don’t configure enough resources, we will not be able to handle the number
    of requests coming from our clients, resulting in disruption of service from our
    models. Fortunately, cloud AI/ML services such as Vertex will auto-scale our model-hosting
    infrastructure to meet increased demand, but we still need to get the sizing as
    accurate as possible for each server in order to control our costs.
  prefs: []
  type: TYPE_NORMAL
- en: Something we also need to keep in mind when deploying our models is how quickly
    our applications need to get responses for the inference requests. We will need
    to keep this in mind as we architect our model hosting infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring models after deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to monitoring the various metrics associated with our models, an
    important concept to call out at this point is something referred to as **drift**,
    which can be represented in various formats, such as model drift, data drift,
    or concept drift. To explain the concept of drift, we will first dive a bit deeper
    into the relationship between the model training process and how models operate
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: Note that during model training, the model was exposed to data in a specific
    format and with specific constraints, and that’s how it learned. Let’s refer to
    the state of the input data as its **shape**, which refers to the format and constraints
    of that data. When we deploy our model to production and we expose it to new data
    in order to get predictions from the model, we want to ensure that the shape of
    the new data matches the shape of the training data as much as possible. We are
    not referring to the contents of the data but rather how the data is represented
    to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'During our training process, we may have performed transformations on raw data
    to make it more suitable for training the model. If so, we need to perform the
    same kinds of transformations on any new data that we send to the model to make
    predictions. In the real world, however, data can change over time, and therefore
    the shape of the data can change over time. We refer to this as drift, which is
    when the raw data out in the real world has fundamentally changed in some way
    since we trained our model. Let’s look at a couple of examples of drift in order
    to clarify the point:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example one**:'
  prefs: []
  type: TYPE_NORMAL
- en: We trained our model on data that was gathered from customers filling in forms
    online. Our model tries to predict whether those customers would likely respond
    well to a specific marketing campaign in which we would send them targeted emails
    with discounts on shoes. Recently, an administrator decided that they wanted to
    capture some additional information from customers and that some of the previously
    captured data is no longer relevant, so they added some fields to the form and
    removed some other fields. Now, when new customers fill in the form and that data
    is sent to our model, there will be extra fields in the input that the model has
    never seen before and the other fields that it expects to see will no longer be
    there. This could affect our model’s ability to interpret and use the input data
    effectively, causing errors or incorrect predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example two**:'
  prefs: []
  type: TYPE_NORMAL
- en: We have built a model that estimates how quickly we can deliver products to
    customers. The model uses inputs from a number of different sources. One of the
    sources is a dataset that contains historical data on how long it took to deliver
    products to customers in past deliveries. We receive an update of that dataset
    every day, which contains the details of all of the orders that were delivered
    the previous day. A critical feature in that dataset is the delivery time, which
    is measured in days. This dataset is created by a system that is owned by a different
    organization in our company, so we do not have control over that dataset. Our
    delivery process has become a lot more efficient recently, and some products are
    being delivered the same day, so the delivery time has now been updated to be
    measured in hours instead of days. However, nobody informed our data science team
    about this change. Now, our model looks at the delivery time feature, and because
    the unit of measurement has changed, its predictions for new delivery times are
    incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting example is something with which we all are somewhat familiar.
    Many large retailers use AI/ML models to forecast what to stock in their inventory
    based on data related to what customers are purchasing and they try to look for
    emerging trends to identify changes in consumer behavior. In the first few weeks
    of the COVID-19 pandemic, there was an enormous and sudden shift in what everybody
    wanted to buy. The models were probably surprised to find that everybody was suddenly
    very interested in one thing in particular, which up until then, was generally
    sold at a very predictable rate. What was this thing that the models predicted
    everybody was suddenly interested in? Toilet paper!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11: Empty toilet paper shelves in a store (source: https://commons.wikimedia.org/wiki/File:COVID-19_-_Toilet_Paper_Shortage_%2849740588227%29.jpg)](img/B18143_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.11: Empty toilet paper shelves in a store (source: https://commons.wikimedia.org/wiki/File:COVID-19_-_Toilet_Paper_Shortage_%2849740588227%29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: There are also much more subtle changes that can happen to our data over time.
    We’ve already discussed how data scientists often want to inspect data before
    training a model, and one of the aspects they often want to inspect is the statistical
    distribution of the values of each of the features (mean, mode, maximum, minimum,
    etc.). These are important details because they give us an understanding of what
    kinds of values our features are generally expected to contain. During inspection,
    this can help us to identify outliers that could indicate erroneous data or that
    may inform us of some other characteristic of the data that we were previously
    unaware of. We can also apply this knowledge to make predictions during production.
    We can analyze the data that is sent to our models for inference purposes, and
    if we see that the statistical distributions have changed in a consistent way,
    then it could alert us to potential data corruption or to the fact that the data
    has genuinely changed, which could indicate a need to update our models by training
    them on new data that matches the updated shape that we’re observing in the real
    world.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, drift can lead to our models becoming less accurate or providing
    erroneous results, and we therefore need to monitor specifically for this by inspecting
    the data that we’re observing in production, as well as monitoring the expected
    metrics for our models. If we see that our model’s metrics are declining over
    time—or suddenly—this could be an indication of drift between the kind of data
    the model was trained on and the kind of data it observes in production.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for overcoming common challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section contains pointers and best practices that companies have developed
    over time to address many of the challenges discussed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Finding and gathering relevant data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed data silos being a common challenge in large companies, as well
    as restrictions with regard to how data is stored and accessed, especially sensitive
    data that could be subject to various regulations and compliance requirements.
    A key to overcoming these challenges is to break down the silos by creating centralized
    data lakes and data discovery mechanisms, such as a searchable data catalog that
    contains metadata describing the various datasets in our data lake. To ensure
    that our data is stored and accessed securely, we need to implement robust encryption
    and permission-based access mechanisms. We will explore these topics in more detail
    in later chapters, and we will also perform some hands-on activities regarding
    detecting and addressing bias and other problems in our datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In cases where our data exists in databases that are restricted specifically
    for transactional business operations, we could implement a **change data capture**
    (**CDC**) solution that replicates our data to a data lake that can then be used
    for data analytics and AI/ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Considering that the data-gathering process happens at the very beginning of
    our AI/ML workload, it’s critical that we implement data quality checks at this
    point in order to prevent issues later in our workload. For example, we know that
    training our models on corrupt data will result in errors or less accurate model
    outputs. Bear in mind that in most ML use cases, we are periodically training
    our models on updated data that’s coming from some source, perhaps from a system
    that is owned and operated by another team or organization. Therefore, if we create
    data quality checks as this data is coming into our workload and we detect data
    quality issues, we should implement mechanisms that prevent further steps in our
    process from proceeding. Otherwise, performing the downstream steps in our process,
    such as data transformations and model training, would be a waste of time and
    money and could lead to worse consequences in production, such as malfunctioning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Data labeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your company is having difficulties finding a workforce to perform your labeling
    tasks, Google Cloud’s data labeling service can help you to get your data labeled
    appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Picking an algorithm and model architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Where should we start when picking an algorithm? This is a common challenge,
    and as a result, data scientists have been constructing solutions to try to make
    this easier. In this section, we’ll describe a tiered framework for approaching
    a new AI/ML project in this context:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tier 1**: You can see if a packaged solution already exists for your business
    problem. For example, Google Cloud has created packaged solutions for lots of
    different types of use cases such as computer vision, NLP, and forecasting. We’ll
    be covering these solutions in more detail in the coming chapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tier 2**: If you want to create and deploy your own model without doing any
    of the work required to do so, check out Google Cloud’s AutoML functionality to
    see if it meets your needs. We’ll also explore this in a hands-on activity in
    a later chapter in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tier 3**: If you want to get started with a model that has been trained by
    somebody else, there are numerous hubs and “model zoos” that exist for data scientists
    to share models they’ve created. Analogous to software libraries in traditional
    software development, these are assets that have been created by other data scientists
    for specific purposes, which you can reuse rather than starting from scratch to
    implement the same functionality. For example, you can find pre-trained models
    for various use cases in Google Cloud’s AI Hub (https://cloud.google.com/ai-hub/docs/introduction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tier 4**: If the previous options do not meet your specific needs, you can
    create your own custom models. In this case, Google Cloud Vertex AI provides built-in
    algorithms for common use cases such as linear regression, image classification,
    object detection, and many more, or you can install your own custom model to run
    on Vertex AI. Vertex AI provides many tools for each step in the AI/ML project
    life cycle, and we will explore most of them in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are some established methods to address overfitting and underfitting.
    One cause for overfitting can be that the model did not get access to a sufficient
    amount of different data points from which to learn an appropriate pattern. Looking
    at a very extreme case of this, let’s imagine that we have just one data point
    in our dataset and our model processes this data point over and over again until
    it finds a set of coefficients that it can use to relate the input features accurately
    to the target output for that data point. Now, whenever it sees that same data
    point, it can easily and accurately predict the target variable. However, if we
    show it a new datapoint with a similar structure—i.e., the same number and types
    of features—but different values for those features, it’s unlikely that our model
    will accurately predict the output for the new datapoint because it has only learned
    the specific characteristics of a single data point during training. This is a
    case of overfitting, whereby the model works really well on the specific data
    point on which it was trained but does not make accurate predictions for other
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: One method that can help to address overfitting in this regard is to provide
    more data points when training our model. If our algorithm has seen thousands
    or millions of data points during training, it’s much more likely to have built
    a more generalized model that has a broader understanding of the feature space
    and the relationship between the features and the target variables. Therefore,
    when it sees a new data point, it may be able to make more accurate predictions
    for the target variable of that new data point.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a trade-off that we need to keep in mind in this context: although
    our training process is likely to build a more generalized model as we provide
    more and more data points, we need to keep in mind that training models on large
    datasets can be expensive. We may find that after the model has seen millions
    of data points, each new training iteration is only increasing the model’s generalization
    metrics by very small amounts. For example, if our model currently has a 99.67%
    accuracy rate and each training iteration increases its accuracy by 0.0001% but
    it costs thousands of dollars to do so, it may not make sense from a financial
    perspective to keep training the model on more and more data points, especially
    if our business considers 99.5% to be accurate enough to meet the business needs.
    This last point is important—the trade-off in training costs versus increases
    in accuracy is dependent on the business requirements. If we’re building a model
    for a medical diagnosis use case or a model whose forecasting accuracy can cost
    our company millions of dollars if it is incorrect by 0.001%, then it may be worth
    it to keep training the model on more data points. In any case, what you will
    generally need to do is define a threshold at which the business considers the
    model’s metrics to be sufficient and measure the increase in that metric as the
    model is trained on more data points. If you see that the metric begins to plateau
    after a certain amount of data points, it may be time to stop the training process.'
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that adding more data points is not always an option because
    you may only have a limited dataset to begin with and it may be difficult to gather
    more real-world data for your specific use case. In these scenarios, you may be
    able to generate synthetic data with similar characteristics as your real-world
    data or use mechanisms to optimize the use of your existing dataset during the
    training process, such as maximizing the training dataset by using cross-validation,
    which we’ll explore in a hands-on activity later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Another potential cause of overfitting is if the model is too “complex”, by
    which we mean that too many features may be used for each data point in the training
    dataset. Again, taking an extreme example, if each data point has thousands of
    features, the model will learn very specific relationships between the features
    and the training dataset, which may not generalize well to other data points.
    In this case, a solution could be to remove features that are not deemed to be
    critical to figuring out the optimal relationship between the features and the
    target variables. Selecting the relevant features can be a challenge in itself,
    and we will explore mechanisms such as **principal component analysis** (**PCA**)
    to help select the most relevant features.
  prefs: []
  type: TYPE_NORMAL
- en: The opposite of overfitting, then, is underfitting, and one potential cause
    of underfitting is if the model is too simple, in which case there may not be
    a sufficient number of features for each data point in the dataset for the model
    to determine a meaningful relationship between the features and the target variables.
    Of course, in this case, we would want to find or generate additional features
    than can help our model to learn more meaningful relationships between those features
    and the target variables.
  prefs: []
  type: TYPE_NORMAL
- en: To address the challenge of keeping track of experiments and their results in
    large-scale ML projects, we will use Vertex ML Metadata, which tracks all of our
    experiments and their inputs and outputs for us (i.e., the lineage of our data
    and ML model artifacts).
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and tuning hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are some methodical ways in which to explore what we call the “hyperparameter
    space”, which means all of the different possible values for our hyperparameters.
    The following are some popular methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random search**: The random search approach uses a subsampling technique
    in which hyperparameter values are selected at random for each training job experiment.
    This will not result in all possible values of every hyperparameter being tested,
    but it can often be quite an efficient method for finding an effective set of
    hyperparameter values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grid search**: The grid search approach to hyperparameter tuning is the most
    exhaustive because it will try out every possible combination of values of each
    hyperparameter. This means that it will generally take a lot more time than a
    random search approach. Also, bear in mind that each training job costs money,
    so if you have a large hyperparameter space, this can be very expensive or even
    infeasible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian optimization**: Earlier in this chapter, we talked about using gradient
    descent to optimize a function by finding its minimum point. Bayesian optimization
    is another type of optimization technique. It’s quite a complex process, and it
    is usually more efficient than the other approaches mentioned previously. Fortunately,
    Google Cloud’s Vertex AI Vizier service will perform Bayesian optimization for
    you, so if you use that tool, you won’t need to implement it yourself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are interested in diving into the inner workings of Bayesian optimization,
    I recommend referring to the following paper: [https://arxiv.org/abs/1807.02811](https://arxiv.org/abs/1807.02811)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Google Cloud’s Vertex AI Vizier service will run lots of training job experiments
    for you, trying out many different combinations of hyperparameter values for each
    experiment, and will find the optimal hyperparameter values to run your ML training
    jobs efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Key tip
  prefs: []
  type: TYPE_NORMAL
- en: To save yourself a lot of painstaking work and time when doing hyperparameter
    optimization, use a cloud service that has been built for that purpose, such as
    Google Cloud’s Vertex AI Vizier service.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Latency is often a key factor in the deployment of our models, in that we need
    to ensure our model hosting infrastructure meets the latency requirements expected
    by our client applications.
  prefs: []
  type: TYPE_NORMAL
- en: One decision point that comes into view in this context is whether we have a
    batch or online use case. In an online use case, a client sends a piece of input
    data to our model and it waits to receive an inference response. This usually
    happens when our client application needs an answer quickly, such as when a customer
    is performing a transaction on our website and we want to see if the transaction
    seems fraudulent. This is a real-time use case, and therefore the latency generally
    needs to be very low; perhaps a few milliseconds. You will usually need to work
    with business leaders to define the acceptable latency.
  prefs: []
  type: TYPE_NORMAL
- en: In a batch use case, our model can process large amounts of data at a time.
    For example, as input to our model at inference time, we may provide a large file
    containing thousands or millions of data points for which we want our model to
    make predictions, and our model could work for hours on processing those inputs
    and save all of the inference results as outputs in another file, which we could
    reference later.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Batch use cases are usually associated with scenarios in which we do not require
    low latency. However, ironically, there is also a scenario in which batch use
    cases can actually help provide lower latency at inference time. Consider a scenario
    in which we’re running a retail website and we want to get insights from our users’
    purchasing histories in order to recommend products that customers may be interested
    in buying when they visit our website. Depending on the amount of historical data
    we have, it could take a long time to process that data. Therefore, we don’t want
    to do this in real time when a customer visits our website. Instead, we could
    periodically run a batch inference job every night and store our results in a
    file or a key–value database. Then, when customers visit our site, we can fetch
    the pre-computed inferences from our file or database. In this scenario, fetching
    a value from a file or database will usually be much quicker than performing an
    online inference in real time. Note that this would only be suitable for certain
    use cases. For example, it would not work for a transactional fraud evaluation
    use case because we would need real-time characteristics from the ongoing transaction
    for that scenario. Consequently, as a data scientist or AI/ML solutions architect,
    you will need to determine what kind of inferencing approach works best for each
    of your use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring models after deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you detect that drift is occurring in production, this is a sign that you
    may need to update your models. We recommend putting mechanisms in place to automate
    the retraining of your models with updated data if you detect drift, especially
    if you’re managing large numbers of models. I’ve worked with organizations that
    are running hundreds of models simultaneously in production, and it’s not feasible
    to manually monitor, curate, and retrain all of those models on an ongoing basis.
    In that scenario, we’ve implemented MLOps frameworks that would retrain the models
    on updated data whenever a model’s metrics consistently dropped below a preconfigured
    threshold that we considered to be acceptable. The MLOps framework would then
    test the new model, and if the new model’s metrics outperformed the current model’s
    metrics in production, it would replace the production model with the new model.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to defining and monitoring the model’s operational metrics in
    production, you can use Google Cloud Monitoring for that purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored a quick recap of the traditional SDLC and introduced
    the concept of the ML model development life cycle. We discussed each of the steps
    that we usually encounter in most AI/ML projects, and then we dived into specific
    challenges that generally exist in each step. Finally, we covered approaches and
    best practices that companies have learned over time to help them address some
    of those common challenges.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll begin to explore the various different services in
    Google Cloud that can be used to implement AI/ML workloads.
  prefs: []
  type: TYPE_NORMAL
