- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Understanding the ML Model Development Life Cycle
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习模型开发生命周期
- en: In this chapter, we will explore the different steps that exist in a typical
    AI/ML project. This information is an important foundation for an AI/ML solutions
    architect role because you will need to advise companies on how to implement these
    steps efficiently. It is also a foundation for the rest of the contents of this
    book, as in later chapters, you will create your own machine learning projects,
    and it’s important that you understand the steps in the process. We will also
    explore the concept of MLOps in this book and how the ML model development life
    cycle serves as the basis for the MLOps paradigm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨典型AI/ML项目中存在的不同步骤。这些信息是AI/ML解决方案架构师角色的重要基础，因为您需要向公司建议如何高效地实施这些步骤。它也是本书其余内容的基础，因为在后面的章节中，您将创建自己的机器学习项目，了解过程中的步骤非常重要。我们还将探讨本书中的MLOps概念以及ML模型开发生命周期如何作为MLOps范式的基石。
- en: 'This chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: An overview of the ML model development life cycle
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型开发生命周期概述
- en: Common challenges encountered in the ML model development life cycle
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型开发生命周期中遇到的常见挑战
- en: Best practices for overcoming common challenges
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克服常见挑战的最佳实践
- en: An overview of the ML model development life cycle
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型开发生命周期概述
- en: You may be familiar with the concept of the **software development life cycle**
    (**SDLC**), which is taught in computer science classes in schools all over the
    world. The SDLC concept began to be formulated in the 1960s and early 1970s, and
    by now it is a well-established and well-understood process that is used in various
    formats by pretty much every company that develops software. Without formalized
    processes for people to follow when developing software, it would be difficult
    for companies to efficiently produce high-quality software and the software development
    industry would be quite chaotic. In fact, that’s how the software development
    industry was in its early years, and that’s how the machine learning industry
    currently is for most companies. Only in the past couple of years has the industry
    started to establish some structure around how companies should develop ML models
    and their related applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能熟悉**软件开发生命周期**（**SDLC**）的概念，这是在世界各地的计算机科学课程中教授的内容。SDLC概念始于20世纪60年代和70年代初，到目前为止，它已经成为一个确立且被广泛理解的过程，被几乎每家开发软件的公司以各种格式使用。如果没有正式化的流程供人们在开发软件时遵循，公司就难以高效地生产高质量的软件，软件开发行业将会非常混乱。事实上，这就是软件开发行业在早期的情况，目前对于大多数公司来说，机器学习行业也是如此。只有在过去几年里，行业才开始围绕公司如何开发ML模型及其相关应用建立一些结构。
- en: In this section, we provide a high-level overview of the ML model development
    life cycle, outlining each of the steps that you will encounter in most machine
    learning projects. Let’s begin with a quick recap of the SDLC. We will make references
    to this more well-established set of processes where relevant.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了一个ML模型开发生命周期的概述，概述了您在大多数机器学习项目中会遇到的所有步骤。让我们先快速回顾一下SDLC。在相关的地方，我们将引用这个更成熟的过程集。
- en: Note
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The term **MDLC**, representing **model development life cycle**, was originally
    coined by a friend and colleague of mine named Fei Yuan. He and I worked on building
    an MLOps process at Amazon before the term “MLOps” began to be used in the industry.
    That’s not to say that we were the only people trying to automate the steps in
    data science projects. For example, the technical reviewer for this book shared
    with me that he and some colleagues had implemented MLOps-type workloads in the
    early 2000’s with SAS and a process model called CRISP-DM, which stands for CRoss
    Industry Standard Process for Data Mining. You can learn more about CRISP-DM at
    the following URL: [https://www.datascience-pm.com/crisp-dm-2/](https://www.datascience-pm.com/crisp-dm-2/).
    Fortunately, in recent years, MLOps has become an important and popular concept
    in ML model development, and now there are lots of MLOps tools available, which
    we will cover later in this book.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 术语**MDLC**，代表**模型开发生命周期**，最初是由我的一个朋友和同事费元提出的。在“MLOps”一词在业界开始使用之前，他和我在亚马逊构建了一个MLOps流程。这并不是说我们是我们唯一试图自动化数据科学项目步骤的人。例如，这本书的技术审稿人告诉我，他和一些同事在2000年代初使用SAS和一个名为CRISP-DM的流程模型实现了MLOps类型的工作负载，CRISP-DM代表跨行业标准数据挖掘流程。您可以在以下网址了解更多关于CRISP-DM的信息：[https://www.datascience-pm.com/crisp-dm-2/](https://www.datascience-pm.com/crisp-dm-2/)。幸运的是，近年来，MLOps已成为机器学习模型开发中的一个重要且流行的概念，现在有许多MLOps工具可供使用，我们将在本书的后续部分介绍。
- en: SDLC – a quick recap
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SDLC——快速回顾
- en: One of the first and simplest versions of the SDLC is known as the Waterfall
    model because the flow of activities in the process is sequential, where the deliverables
    from each activity serve as dependencies for the next activity in the flow. *Figure
    2**.1* shows the original Waterfall diagram from a paper titled *Managing the
    development of large software systems* by Winston Royce in 1970.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: SDLC的一个最早和最简单的版本被称为瀑布模型，因为该流程中的活动流程是顺序的，其中每个活动的交付成果作为下一个活动在流程中的依赖项。*图2.1*显示了1970年温斯顿·罗伊斯（Winston
    Royce）在论文《管理大型软件开发》中展示的原始瀑布图。
- en: '![Figure 2.1: SDLC Waterfall model](img/B18143_02_1.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1：SDLC瀑布模型](img/B18143_02_1.jpg)'
- en: 'Figure 2.1: SDLC Waterfall model'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：SDLC瀑布模型
- en: As we can see, the process starts by gathering and analyzing the requirements
    that the system needs to satisfy and then designing, coding, and testing the software
    before deploying it for use. After the resulting software has been deployed, you
    then need to manage it via ongoing operations activities. This model was later
    updated to include feedback loops between the various stages. For example, the
    feedback from testing could result in an updated coding step, which in turn could
    result in an updated program design step, and so on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，该流程从收集和分析系统需要满足的需求开始，然后设计、编码和测试软件，最后部署使用。在部署后的软件之后，您需要通过持续运营活动来管理它。该模型后来更新，包括各个阶段之间的反馈循环。例如，测试的反馈可能导致更新的编码步骤，这反过来又可能导致更新的程序设计步骤，依此类推。
- en: A well-known limitation of the Waterfall model is that the process doesn’t facilitate
    the rapid innovation or flexibility that is required in today’s fast-paced software
    development industry, whereby new requirements often come to light during the
    development, testing, and deployment phases. The software design needs to be updated
    frequently, and the various stages in the process are more cyclical in nature
    to allow updates to occur more flexibly (see *Figure 2**.2*). As a result, newer
    development methodologies such as Agile have emerged. Nevertheless, the methodical
    sequence of events from gathering requirements through to designing, coding, testing,
    deploying, and monitoring software still exists in various forms in system design
    projects, and this extends to ML systems and projects.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 瀑布模型的一个众所周知的问题是，该流程不促进今天快速发展的软件开发行业所需的快速创新或灵活性，因为在开发、测试和部署阶段，新需求经常出现。软件设计需要频繁更新，流程的各个阶段更具有循环性，以便更灵活地进行更新（参见*图2.2*）。因此，像敏捷这样的新开发方法出现了。尽管如此，从收集需求到设计、编码、测试、部署和监控软件的方法论事件序列仍然以各种形式存在于系统设计项目中，这扩展到了机器学习和项目。
- en: '![Figure 2.2: A cyclical approach to software development, often referred to
    as DevOps (source: https://openclipart.org/download/313185/1546764098.svg)](img/B18143_02_2.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2：软件开发循环方法，通常称为DevOps（来源：https://openclipart.org/download/313185/1546764098.svg）](img/B18143_02_2.jpg)'
- en: 'Figure 2.2: A cyclical approach to software development, often referred to
    as DevOps (source: https://openclipart.org/download/313185/1546764098.svg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：软件开发的一种循环方法，通常被称为DevOps（来源：https://openclipart.org/download/313185/1546764098.svg）
- en: Typical ML project stages
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 典型的机器学习项目阶段
- en: 'Interestingly, it took a while for the lessons from the traditional software
    development industry to be applied to ML model development. When ML development
    suddenly experienced a huge uptick in popularity during the past few years, many
    companies dived into the race without formalized processes in place, and as a
    result, companies ran into their own random unexpected issues without much ability
    to standardize across the industry. Fortunately, lessons have been learned from
    the early pioneers in this process, and standardized project activities have emerged.
    The following are the kinds of steps that you can expect to take in most ML development
    projects:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，将传统软件开发行业的经验应用到机器学习模型开发上花费了一些时间。在过去几年中，随着机器学习开发突然受到巨大的人气提升，许多公司在没有正式流程的情况下跳入了这场竞赛，结果，公司在没有太多能力在整个行业标准化的情况下遇到了自己随机意外的问题。幸运的是，从这一过程中的早期先驱者那里学到了经验教训，并出现了标准化的项目活动。以下是在大多数机器学习开发项目中可以预期采取的步骤：
- en: Gather, analyze, and understand the business requirements for which the model
    will be developed.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集、分析和理解模型将开发的业务需求。
- en: Find and gather relevant data.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 寻找并收集相关数据。
- en: Explore and understand the contents of the data.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索和理解数据的内容。
- en: Transform or manipulate the data for ML model training, which may include feature
    engineering and storing features for use in later steps. This step is often also
    closely linked to *step 6* because the selected algorithm may have specific requirements
    regarding how the data needs to be presented.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行转换或操作以用于机器学习模型训练，这可能包括特征工程和存储特征以供后续步骤使用。这一步骤通常也与*步骤6*紧密相关，因为选定的算法可能对数据如何呈现有特定的要求。
- en: For supervised learning models, label the data if the required labels are not
    already present in the dataset.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于监督学习模型，如果数据集中尚未存在所需的标签，则对数据进行标记。
- en: Pick an algorithm that suits the requirements of the business case.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个适合业务案例要求的算法。
- en: Train a model.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个模型。
- en: Configure and tune hyperparameters.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置和调整超参数。
- en: Deploy the model.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署模型。
- en: Monitor the model after deployment.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署模型后监控模型。
- en: '*Figure 2**.3* shows a visual representation of these steps, and we will dive
    into these steps in detail in the coming sections:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2*.*3*展示了这些步骤的视觉表示，我们将在接下来的章节中详细探讨这些步骤：'
- en: '![Figure 2.3: Typical ML project stages](img/B18143_02_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3：典型的机器学习项目阶段](img/B18143_02_03.jpg)'
- en: 'Figure 2.3: Typical ML project stages'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：典型的机器学习项目阶段
- en: As you can see, there are some similarities between the ML model development
    process and the traditional SDLC, but there are also some differences that are
    unique to ML model development. Most notable is the inclusion of data in the process.
    The fact that we now need to include the manipulation of data in our overall process
    adds a lot of complexity, as you will see when we go through each of the process
    steps in more detail. It should be noted that each step in the overall life cycle
    is often cyclical in nature, whereby the data science team may need to perform
    each task—or combinations of tasks—multiple times, with different inputs and outputs,
    using a trial-and-error methodology until they find the optimal approach to use
    for each step.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，机器学习模型开发过程与传统软件开发生命周期（SDLC）之间有一些相似之处，但也存在一些独特的差异。最值得注意的是，数据被纳入了过程。我们现在需要将数据的操作纳入整体过程，这增加了许多复杂性，正如我们在更详细地通过每个过程步骤时将会看到的。需要注意的是，生命周期中的每个步骤通常是循环性质的，其中数据科学团队可能需要多次执行每个任务或任务组合，使用试错法，直到找到每个步骤的最佳使用方法。
- en: Gathering, analyzing, and understanding the business requirements
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收集、分析和理解业务需求
- en: This step is often omitted from ML life cycle diagrams because such diagrams
    usually focus on the technical steps, which follow later in our project. This
    can be considered as step zero because this generally needs to happen before any
    of the technical steps in our project begin. Just like in traditional software
    development, the overall process must begin with gathering and understanding the
    business requirements that the model will be built to address. For example, will
    the models produced by our project be used to forecast sales revenue for the next
    year or are we setting out to build an application that will monitor people’s
    health data and provide them with health-related recommendations based on that
    data? The business requirements influence the decisions we make in the later steps
    of our project, such as what kinds of data we need to gather, what ML algorithms
    we will use to train our models, and what kinds of metrics we will measure in
    relation to our models’ performance.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤通常被省略在机器学习生命周期图中，因为这样的图通常关注后续技术步骤，而这些步骤在我们项目中的后续阶段。这可以被视为零步骤，因为通常需要在我们的项目中的任何技术步骤开始之前发生。就像在传统的软件开发中一样，整个过程必须从收集和理解模型将要解决的业务需求开始。例如，我们项目产生的模型将用于预测下一年的销售收入，还是我们正在着手构建一个将监控人们的健康数据并根据这些数据提供健康相关建议的应用程序？业务需求会影响我们在项目后续步骤中做出的决策，例如我们需要收集哪些类型的数据，我们将使用哪些机器学习算法来训练我们的模型，以及我们将如何衡量与模型性能相关的指标。
- en: In this part of an AI/ML project, the solutions architect will work with business
    leaders to understand what they want to achieve from a business perspective and
    will then work with technical personnel to translate the business requirements
    into technical requirements. Defining the technical requirements is one of the
    first steps in defining the overall strategy to meet the business objectives outlined
    by the business leadership. This includes identifying any constraints that may
    exist, such as working with data scientists to determine what kinds of data would
    be required to address the business goals and whether that data can be gathered,
    generated, or procured from somewhere.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能/机器学习项目的这个部分，解决方案架构师将与业务领导合作，了解他们从业务角度想要实现的目标，然后将与技术人员合作，将业务需求转化为技术需求。定义技术需求是定义满足业务领导概述的业务目标的整体策略的第一步之一。这包括确定可能存在的任何限制，例如与数据科学家合作确定需要哪些类型的数据来解决业务目标，以及这些数据是否可以收集、生成或从某处采购。
- en: Finding and gathering relevant data
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找和收集相关数据
- en: 'We briefly touched on this topic in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015).
    Data is what ML models learn from, so without data, there is no machine learning.
    If the project team—including the data scientists and data engineers (we will
    explain these roles in more detail later)—cannot figure out how to get the data
    that would be required to meet the business objective, then the project could
    be a non-starter right from the beginning, so this is a critical step in the process.
    Sources of data vary based on the type of project, but the following are some
    examples of data that could be used for various AI/ML use cases:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中简要提到了这个话题。数据是机器学习模型学习的基础，所以没有数据就没有机器学习。如果项目团队——包括数据科学家和数据工程师（我们将在后面更详细地解释这些角色）——无法想出如何获取满足业务目标所需的数据，那么项目可能从一开始就难以启动，因此这是过程中的一个关键步骤。数据来源根据项目类型而异，但以下是一些可用于各种人工智能/机器学习用例的数据示例：
- en: Historical data that contains the details of customer credit card transactions
    and/or banking transactions
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含客户信用卡交易和/或银行交易详细信息的歷史数据
- en: Data relating to what customers are purchasing online
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与客户在线购买相关的数据
- en: Housing sales data in a particular region
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定地区的住房销售数据
- en: Log entry data that contains details of a technical system’s operational events
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含技术系统操作事件详细信息的日志条目数据
- en: Health data that is tracked by wearable devices such as watches or fitness trackers
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由可穿戴设备（如手表或健身追踪器）追踪的健康数据
- en: Data gathered from people filling in forms or surveys
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从填写表格或调查问卷的人那里收集的数据
- en: Data streamed from **Internet of Things** (**IoT**) devices such as factory
    conveyor belts or a fleet of construction vehicles
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自**物联网（IoT**）设备（如工厂输送带或建筑车辆车队）的数据流
- en: As you can see, there are different types of data that can be used for many
    different purposes. The data science team’s first major task will be to define
    and locate the data that needs to be used for the project. This is not an atomic
    activity in that it does not need to happen all at once at the beginning of a
    project. Often, the science team begins with some idea of the data they need,
    and then they may refine the data requirements based on testing and feedback in
    later steps of the project.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，有不同类型的数据可以用于许多不同的目的。数据科学团队的首要任务是定义和定位项目所需使用的数据。这不是一个原子活动，因为它不需要在项目开始时一次性完成。通常，科学团队会先对所需数据有一个大致的想法，然后根据项目后期测试和反馈来细化数据需求。
- en: Exploring and understanding the data
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索和理解数据
- en: When the data science team has gathered data that they believe could be used
    to address the business requirements, they usually don’t just dive into training
    ML models on that data. Instead, they usually need to inspect the data to assess
    whether it really can be used adequately for the purposes of the project. Raw
    data is often not in an optimal state to be used by some ML algorithms. Let’s
    take a couple of examples from our list of potential data sources. If we’re using
    data gathered from people filling in forms or surveys, people may input the details
    incorrectly. They may leave some fields blank or misspell some of the details
    during input. As another example, if we’re using data from sensors such as wearable
    health trackers or other IoT devices such as mechanical machinery sensors, those
    sensors may malfunction and record corrupted data. As such, data scientists often
    need to inspect the data and look for errors, anomalies, or potentially corrupted
    data. In [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015), we also mentioned that
    data scientists may want to get statistical details regarding the data, such as
    the range of values that are generally seen for particular variables in the data
    or other statistical distribution details. In the hands-on activities later in
    this book, we will be using data visualization tools and other data inspection
    tools to explore and understand the contents of our datasets.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据科学团队收集了他们认为可以用于满足业务需求的数据时，他们通常不会直接在该数据上训练机器学习模型。相反，他们通常需要检查数据，以评估其是否真的能够满足项目的需求。原始数据往往不适合某些机器学习算法使用。让我们从我们的潜在数据源列表中举几个例子。如果我们使用的是人们填写表格或调查表收集的数据，人们可能会输入错误的信息。他们可能会留空某些字段或在输入时拼写错误。作为另一个例子，如果我们使用的是来自可穿戴健康追踪器或其他物联网设备（如机械设备传感器）的数据，这些传感器可能会出现故障并记录损坏的数据。因此，数据科学家通常需要检查数据，寻找错误、异常或潜在损坏的数据。在[第1章](B18143_01.xhtml#_idTextAnchor015)中，我们也提到数据科学家可能希望获取有关数据的统计细节，例如数据中特定变量的值范围或其他统计分布细节。在本书的后续实践活动中，我们将使用数据可视化工具和其他数据检查工具来探索和理解数据集的内容。
- en: Transforming or manipulating the data for ML model training
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对数据进行转换或操作以用于机器学习模型训练
- en: Missing or corrupted data can cause problems when training ML models. Some algorithms
    that need to operate on numeric data will produce errors if they encounter non-numeric
    values, including null and garbled/corrupted characters. Even for algorithms that
    can handle such values gracefully, the values may skew the learning process in
    unexpected ways, thus affecting the performance of the resulting models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失或损坏的数据在训练机器学习模型时可能会引起问题。一些需要操作数值数据的算法在遇到非数值值（包括空值和乱码/损坏的字符）时会产生错误。即使对于可以优雅处理这些值的算法，这些值也可能以意想不到的方式扭曲学习过程，从而影响最终模型的表现。
- en: When data scientists find that a dataset isn’t perfectly ready for use in training
    ML models, they don’t usually just give up but rather try to make changes to the
    data to bring it closer to the desired state. We call this process **feature engineering**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据科学家发现某个数据集不适合用于训练机器学习模型时，他们通常不会放弃，而是尝试对数据进行修改，使其更接近理想状态。我们称这个过程为**特征工程**。
- en: Note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Some literature publications use the term “feature engineering” only to refer
    to the process of creating new features from existing features (such as our price
    per square foot example), while other literature uses the same term to describe
    all activities related to manipulating features in our dataset, including replacing
    missing values.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一些文献出版物仅用“特征工程”一词来指代从现有特征（例如我们每平方英尺的价格示例）创建新特征的过程，而其他文献则用同一术语来描述与操纵我们数据集中特征相关的所有活动，包括替换缺失值。
- en: This could include data cleaning (or cleansing) techniques, such as replacing
    missing data with something more meaningful. As an example, let’s imagine that
    some medical condition is more likely to occur as a person gets older and we want
    to build a model that predicts the likelihood of this condition occurring. In
    this case, a person’s age would be an important input feature in our dataset.
    During our data exploration activities, if we discover that some of the records
    in our dataset are missing the age value, we could compute the average age of
    all of the people in our dataset and replace each missing age value with the average
    age value. Alternatively, we could replace each value with the modal (i.e., the
    most frequently occurring) age value. Either of these cases would at least be
    better than having missing or corrupted values in our dataset during the training
    process.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能包括数据清洗（或净化）技术，例如用更有意义的东西替换缺失数据。例如，让我们假设某种医疗状况更可能在一个人变老时发生，我们希望构建一个预测这种状况发生可能性的模型。在这种情况下，一个人的年龄将是我们数据集中的重要输入特征。在我们的数据探索活动中，如果我们发现数据集中的一些记录缺失年龄值，我们可以计算数据集中所有人的平均年龄，并用平均年龄值替换每个缺失的年龄值。或者，我们可以用众数（即最频繁出现的值）替换每个值。这两种情况至少比在训练过程中数据集中有缺失或损坏的值要好。
- en: Also, the optimal variables and values for addressing specific business requirements
    may not be readily available in whatever raw data we can access. Instead, data
    scientists often need to combine data from different sources and come up with
    clever ways to derive new data from the available data. A very simple example
    would be if we specifically need a person’s age as an input variable but the dataset
    only contains their date of birth. In that case, the data scientist could add
    another feature, age, to the dataset, and subtract the date of birth from the
    current date in order to calculate the person’s age. A slightly more complex example
    would be if we wanted to predict housing prices and we determined that price per
    square foot would be an important input feature for our model but our dataset
    only contains the total price for each house and the total area of each house
    in square feet. In that case, to create our price per square foot input feature
    for each house, we could divide the total cost of each house by the total area
    of that house in square feet and then add that as a feature in our dataset.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，针对特定业务需求的最佳变量和值可能在我们能访问的任何原始数据中都不易获得。相反，数据科学家通常需要结合来自不同来源的数据，并想出巧妙的方法从现有数据中推导出新的数据。一个非常简单的例子是，如果我们特别需要一个年龄作为输入变量，但数据集只包含他们的出生日期。在这种情况下，数据科学家可以在数据集中添加另一个特征，即年龄，并从当前日期中减去出生日期以计算该人的年龄。一个稍微复杂一点的例子是，如果我们想预测房价，并确定每平方英尺的价格将是我们模型的重要输入特征，但我们的数据集只包含每所房子的总价格和每所房子的总面积（以平方英尺为单位）。在这种情况下，为了创建每所房子的每平方英尺输入特征，我们可以将每所房子的总成本除以该房子的总面积，然后将这个值作为数据集中的特征添加。
- en: It’s important to understand that when a data scientist has created the features
    that are important for training their model, they will often want to store those
    features somewhere for later use, rather than needing to re-create them again
    and again. Later in this book, we will explore tools that have been developed
    for this purpose.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，当数据科学家创建了训练模型所需的重要特征后，他们通常会希望将这些特征存储在某个地方以备后用，而不是需要一次又一次地重新创建它们。在本书的后面部分，我们将探讨为这一目的开发的工具。
- en: Data labeling
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据标注
- en: As we discussed in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015), supervised
    learning algorithms rely on labels in the dataset during the training process,
    which tell the models what the correct answers are for the types of data relationships
    that the model is trying to learn. *Figure 2**.4* shows our example of a labeled
    dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中讨论的那样，监督学习算法在训练过程中依赖于数据集中的标签，这些标签告诉模型模型试图学习的各种数据关系类型的正确答案。*图2.4*展示了我们的标记数据集示例。
- en: '![Figure 2.4: An example of labels (highlighted in green) in a dataset](img/B18143_02_4.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4：数据集中标签的示例（绿色突出显示）](img/B18143_02_4.jpg)'
- en: 'Figure 2.4: An example of labels (highlighted in green) in a dataset'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：数据集中标签的示例（绿色突出显示）
- en: If you’re lucky, you will find a dataset that can be used to address your business
    requirements and already contains the requisite labels or “correct answers” for
    the variables that you want to predict. If not, you will need to add the labels
    to the dataset. Again, considering that your dataset could contain millions of
    data points, this could be a significantly complex and time-consuming task to
    take on. And, just like any of the other features in your dataset, the quality
    of your labels directly impacts how reliable your model’s predictions will be.
    With this in mind, you will need access to a labor force that can accurately label
    your dataset and other tools that facilitate the labeling.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你很幸运，你将找到一个可以用来解决你的业务需求并且已经包含你想要预测的变量的必要标签或“正确答案”的数据集。如果没有，你将需要将标签添加到数据集中。再次强调，考虑到你的数据集可能包含数百万个数据点，这可能是一项非常复杂且耗时的任务。而且，就像你数据集中的其他任何特征一样，你标签的质量直接影响到你模型预测的可靠性。因此，你需要能够准确标记你的数据集的劳动力，以及其他有助于标记的工具。
- en: Another data preparation step that is used for supervised learning algorithms
    is to split the dataset into three different subsets, which are used for the training,
    validation, and testing of the model, respectively. We will describe the use of
    these subsets in the model training section later in this chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用于监督学习算法的数据准备步骤是将数据集分成三个不同的子集，分别用于模型的训练、验证和测试。我们将在本章后面的模型训练部分描述这些子集的使用。
- en: An important thing to call out at this point is the concept of **data leakage**,
    which refers to a scenario in which information from outside the training dataset
    is used to create the model. This can cause the model to perform well on the training
    data (because it has information it wouldn’t have in a real-world scenario) but
    perform poorly in production due to these unintentional hints.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要强调的一个重要概念是**数据泄露**，它指的是使用训练数据集之外的信息来创建模型的情况。这可能导致模型在训练数据上表现良好（因为它拥有在现实世界场景中不会拥有的信息），但在生产中由于这些无意中的提示而表现不佳。
- en: There are various causes that can lead to data leakage, such as how and when
    we split our datasets during our data science project, or how we label our data.
    For example, during data preparation activities such as labeling or feature engineering,
    we could accidentally include knowledge that would not be available to the model
    in a real-world application when the model needs to make predictions. Consider
    a scenario in which we are using historical data to train our model. We may accidentally
    include information that only became available after the events that are represented
    in the dataset actually occurred. While this data may be relevant and may help
    influence the outcome, it will harm our model performance if that information
    would not be available to our model in a real world scenario at the time when
    our model needs to make a prediction.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 数据泄露的原因有很多，比如在我们进行数据科学项目时如何以及何时分割我们的数据集，或者我们如何标记我们的数据。例如，在数据准备活动，如标记或特征工程中，我们可能会意外地包含在现实世界应用中模型无法获得的知识。考虑这样一个场景，我们正在使用历史数据来训练我们的模型。我们可能会意外地包含在数据集中表示的事件实际发生之后才变得可用的信息。虽然这些数据可能相关并且可能有助于影响结果，但如果这些信息在模型需要做出预测的现实世界场景中不可用，那么它将损害我们的模型性能。
- en: Picking an algorithm and model architecture
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择算法和模型架构
- en: There are lots of different types of ML algorithms that can be used for various
    purposes, with new algorithms and model architecture patterns emerging regularly.
    In some cases, choosing your approach is an easy decision because there are some
    algorithms and model architectures that are particularly suited to specific use
    cases. For example, if you want to implement a computer vision use case, then
    something such as a convolutional neural network architecture would be a good
    starting point. On the other hand, choosing what kind of ML algorithm and implementation
    to use for a specific problem can be a difficult task that often depends on the
    experience of the data science team. For example, an experienced team of data
    scientists will have worked on many different projects and developed a working
    understanding of what kinds of algorithms work best for various circumstances,
    whereas a less experienced data science team may need to perform a lot more experimentation
    with various algorithms and model architectures.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同类型的机器学习算法，可以用于各种目的，并且新的算法和模型架构模式经常出现。在某些情况下，选择你的方法是一个简单的决定，因为有一些算法和模型架构特别适合特定的用例。例如，如果你想实现一个计算机视觉用例，那么像卷积神经网络架构这样的东西将是一个好的起点。另一方面，选择用于特定问题的机器学习算法和实现可能是一项困难的任务，这通常取决于数据科学团队的经验。例如，经验丰富的数据科学家团队可能已经参与了多个不同的项目，并形成了对不同情况下哪些算法效果最好的实际理解，而经验较少的数据科学团队可能需要对各种算法和模型架构进行更多的实验。
- en: In addition to direct business requirements, such as “we need a computer vision
    model to identify manufacturing defects”, the chosen algorithm can also depend
    on less-tangible business requirements, such as “the model needs to run on limited
    computing resources” or “the explainability of the model is extremely important
    in this use case.” Each of the aforementioned requirements puts different constraints
    on the types of algorithms the data science team could select for the given use
    case.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直接的业务需求，例如“我们需要一个计算机视觉模型来识别制造缺陷”，所选择的算法还可以依赖于不那么具体的企业需求，例如“模型需要在有限的计算资源上运行”或“在这个用例中，模型的可解释性非常重要。”上述每个要求都对数据科学团队为特定用例选择算法的类型施加了不同的约束。
- en: As with most of the steps in the overall AI/ML project life cycle, selecting
    the best algorithm and model architecture to use can require the data science
    team to implement a cyclical trial-and-error approach, whereby they may experiment
    with different algorithms, architectures, and inputs/outputs until they find the
    optimal implementation. We will be exploring various algorithms and their unique
    characteristics in the hands-on activities later in this book, but overall, it
    is best to start with a simple baseline model, so that we have a starting point
    to compare metrics and understand the base dataset. Then, we can test out more
    complex models and assess whether they perform better.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与整体AI/ML项目生命周期中的大多数步骤一样，选择最佳的算法和模型架构可能需要数据科学团队实施一个循环的试错方法，他们可能会尝试不同的算法、架构、输入/输出，直到找到最佳的实施方案。我们将在本书后面的实践活动中探讨各种算法及其独特的特性，但总体来说，最好从一个简单的基线模型开始，这样我们就有了一个比较指标和了解基础数据集的起点。然后，我们可以测试更复杂的模型，并评估它们是否表现更好。
- en: Training a model
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: This is probably the most well-known activity in the AI/ML project life cycle.
    It’s where the model actually learns from the data. For unsupervised algorithms,
    this is where they may form the clusters that we talked about in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    for example. For supervised algorithms, this is where our training, validation,
    and testing datasets come into the picture. In [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    we briefly talked about how linear algebra and calculus can be used in machine
    learning. If we take our linear regression example, this is exactly where those
    concepts would come into play. Our model would first try to find the relationships
    between the features and the labeled target outputs. That is, it would try to
    find the coefficients for each of our features, which, when used in combination
    (e.g., by adding them all together), would produce the labeled target output.
    It tries to calculate the coefficients that would work for all data points in
    the dataset, and to do this, it needs to scan through all of the items in the
    training dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是AI/ML项目生命周期中最知名的活动。这是模型真正从数据中学习的地方。对于无监督算法，这可能就是它们形成我们在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中提到的那些聚类的时刻，例如。对于监督算法，这是我们的训练、验证和测试数据集进入场景的地方。在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中，我们简要地讨论了线性代数和微积分在机器学习中的应用。如果我们以线性回归为例，这正是那些概念会发挥作用的地方。我们的模型首先会尝试找到特征和标记的目标输出之间的关系。也就是说，它会尝试找到每个特征的系数，当这些系数组合使用（例如，通过将它们全部相加）时，会产生标记的目标输出。它试图计算适用于数据集中所有数据点的系数，为此，它需要扫描训练数据集中的所有项目。
- en: 'The model usually starts this process with a random guess, so it inevitably
    is incorrect on the first try. However, it then calculates the errors and makes
    adjustments to minimize those errors in future iterations through the dataset.
    There are a number of different methods and algorithms that can be used to minimize
    errors, but a very popular method is called gradient descent. We briefly mentioned
    gradient descent in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015), but we’ll
    talk about it in more detail here. In gradient descent, the algorithm works on
    finding the minimum point of what we call the loss function, which is a representation
    of the errors that are produced when our model tries to guess the coefficients
    of the features that produce the labeled outputs for each data point in our dataset.
    *Equation 2.1* shows the equation for calculating the **mean squared error** (**MSE**)
    as an example for a loss function for linear regression:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通常从这个过程开始时进行随机猜测，因此它不可避免地在第一次尝试时是错误的。然而，然后它会计算错误并调整以通过数据集的后续迭代来最小化这些错误。有几种不同的方法和算法可以用来最小化错误，但一个非常流行的方法是称为梯度下降。我们在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中简要提到了梯度下降，但在这里我们将更详细地讨论它。在梯度下降中，算法致力于找到我们所说的损失函数的最小值，损失函数是我们模型试图猜测数据集中每个数据点产生标记输出的特征系数时产生的错误的表示。"方程式2.1"展示了计算线性回归损失函数的**均方误差**（MSE）的方程式示例：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/1.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>](img/1.png)'
- en: 'Equation 2.1: Mean squared error formula'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 2.1：均方误差公式
- en: In *Equation 2.1*, *n* represents the number of data points in our dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *方程式 2.1* 中，*n* 代表数据集中数据点的数量。
- en: 'To understand what this formula is saying, let’s start with the section in
    parenthesis: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math>](img/2.png)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这个公式所表达的意思，让我们从括号内的部分开始：![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math>](img/2.png)
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/3.png)
    represents our model’s predicted target variable for each datapoint and ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/4.png)
    represents the true target variable’s value for each datapoint.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/3.png)
    代表每个数据点的模型预测目标变量，而 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/4.png)
    代表每个数据点的真实目标变量的值。'
- en: Within the parentheses in *Equation 2.1*, we subtract the true value from our
    model’s predicted value in order to calculate the error of our model’s prediction,
    similar to what we described in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    and we then square the result. In this case, we are calculating what’s referred
    to as the **Euclidean distance**, which is the distance between the predicted
    value and the true value in two-dimensional space. Squaring the result also has
    the effect of removing negative values from the results of our subtractions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *方程式 2.1* 的括号内，我们通过从模型预测值中减去真实值来计算模型预测的错误，这与我们在 [*第一章*](B18143_01.xhtml#_idTextAnchor015)
    中描述的类似，然后对结果进行平方。在这种情况下，我们计算的是所谓的 **欧几里得距离**，即在二维空间中预测值与真实值之间的距离。对结果进行平方也起到了消除减法结果中负值的作用。
- en: The summation symbol, *Σ* (sigma), in the equation represents adding up all
    of the calculated errors for all data points in our training dataset. Then, we
    divide the final result—i.e., the total error for all predictions—by the number
    of data points in our dataset in order to calculate the average (or mean) error
    for all of our predictions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 方程中的求和符号*Σ*（西格玛）代表将训练数据集中所有数据点的计算误差加起来。然后，我们将最终结果——即所有预测的总误差——除以数据集中的数据点数量，以计算所有预测的平均误差（或均值）。
- en: 'Remember that we want to minimize the error in each training iteration, by
    finding the minimum point of this loss function (also referred to as the **objective
    function**). To get an understanding of what it means to find the minimum point
    in a loss function, it helps if we can graph the function. *Figure 2**.5* shows
    an example of a two-dimensional loss function graph for the MSE:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们希望通过找到这个损失函数（也称为**目标函数**）的最小点来最小化每次训练迭代中的误差。为了理解在损失函数中找到最小点意味着什么，如果我们能够绘制出函数的图形，那会有所帮助。图2.5展示了均方误差（MSE）的二维损失函数图的例子：
- en: '![Figure 2.5: MSE loss function graph showing minimum point](img/B18143_02_5.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5：展示最小点的均方误差损失函数图](img/B18143_02_5.jpg)'
- en: 'Figure 2.5: MSE loss function graph showing minimum point'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5：展示最小点的均方误差损失函数图
- en: 'Each time the algorithm calculates the loss from each training iteration, that
    loss value can be represented as a point on the graph. Considering that we want
    to move toward the minimum point in order to minimize the loss, we want to take
    a step downward on the graph. Whenever we want to move from one point to another
    (even when moving our bodies around in real life), there are two aspects of the
    movement that we need to determine: direction and magnitude, i.e., in which direction
    do we want to move and how far? This is where gradient descent comes into the
    picture. It helps us to determine the direction in which we should move in order
    to progress towards the minimum point. Let’s take a look at how it works in more
    detail.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每次算法从每次训练迭代中计算出损失时，那个损失值可以表示为图上的一个点。考虑到我们想要移动到最小点以最小化损失，我们希望在图上向下迈一步。无论何时我们想要从一个点移动到另一个点（即使在我们现实生活中移动身体），我们需要确定运动的两个方面：方向和大小，即我们想要朝哪个方向移动以及移动多远？这就是梯度下降发挥作用的地方。它帮助我们确定应该朝哪个方向移动以向最小点前进。让我们更详细地看看它是如何工作的。
- en: Imagine the graph in *Figure 2**.5* is a valley between two mountains and we
    are standing at a point that represents the loss calculated in our most recent
    training iteration. That point is somewhere on the side of the valley, such as
    the location shown in *Figure 2**.6*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下图2.5是一个山谷，我们站在代表最近一次训练迭代中计算出的损失的那个点上。那个点位于山谷的侧面，例如图2.6中所示的位置。
- en: '![Figure 2.6: MSE loss function graph showing current location](img/B18143_02_6.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图2.6：展示当前位置的均方误差损失函数图](img/B18143_02_6.jpg)'
- en: 'Figure 2.6: MSE loss function graph showing current location'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：展示当前位置的均方误差损失函数图
- en: For a human, walking downhill is somewhat instinctive because we have sensory
    input that tells us which way is downhill. For example, our feet can feel the
    slope of the hill at our current location, we can feel the pull of gravity downwards,
    and we may also be able to see our surroundings and therefore see which way is
    downhill. However, our gradient descent algorithm does not have these sensory
    inputs and it can only use mathematics to find out which way is downhill. Then,
    it needs to use programmatic methods to define what it means to take a step in
    that direction. Our algorithm knows the current location on the graph and it can
    calculate the derivative of the function (a concept from differential calculus)
    to determine the slope of the graph at the current location. *Figure 2**.7* shows
    an example of the slope of a line at a point on the graph, which represents the
    derivative of the function at that point.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类来说，下山行走有一定的本能，因为我们有感觉输入告诉我们哪个方向是下坡。例如，我们的脚可以感觉到当前位置的山坡斜度，我们可以感觉到向下的重力拉扯，我们也许还能够看到我们的周围环境，因此可以看到哪个方向是下坡。然而，我们的梯度下降算法没有这些感觉输入，它只能通过数学来找出哪个方向是下坡。然后，它需要使用程序方法来定义在那个方向上迈步的含义。我们的算法知道图上的当前位置，并且可以计算函数的导数（来自微分学的概念）以确定当前位置的图形斜率。图2.7展示了图上某一点处直线斜率的例子，这代表了该点处函数的导数。
- en: '![Figure 2.7: Derivative of the loss function at a particular point](img/B18143_02_7.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图2.7：特定点的损失函数导数](img/B18143_02_7.jpg)'
- en: 'Figure 2.7: Derivative of the loss function at a particular point'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：特定点的损失函数导数
- en: 'When the derivative has been calculated, this information can be used by our
    algorithm to take a step toward the minimum point. *Equation 2.2* shows how each
    next step is calculated for gradient descent in the context of linear regression:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当导数被计算出来后，这个信息可以被我们的算法用来朝着最小点迈出一步。*方程2.2*展示了在线性回归的背景下，如何计算每个后续步骤的梯度下降：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>θ</mi><mi>j</mi></msub><mo>=</mo><msub><mi>θ</mi><mi>j</mi></msub><mo>−</mo><mi>α</mi><mfrac><mn>1</mn><mi>m</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo>(</mo><msub><mi>h</mi><mi>θ</mi></msub><mfenced
    open="(" close=")"><msup><mi>x</mi><mfenced open="(" close=")"><mi>i</mi></mfenced></msup></mfenced><mo>−</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow></mrow></mrow></mrow></math>](img/5.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>θ</mi><mi>j</mi></msub><mo>=</mo><msub><mi>θ</mi><mi>j</mi></msub><mo>−</mo><mi>α</mi><mfrac><mn>1</mn><mi>m</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo>(</mo><msub><mi>h</mi><mi>θ</mi></msub><mfenced
    open="(" close=")"><msup><mi>x</mi><mfenced open="(" close=")"><mi>i</mi></mfenced></msup></mfenced><mo>−</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow></mrow></mrow></mrow></math>](img/5.png)'
- en: 'Equation 2.2: Gradient descent for linear regression'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 方程2.2：线性回归的梯度下降
- en: 'In *Equation 2.2*, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>θ</mi><mi>j</mi></msub></mrow></math>](img/6.png)represents
    the location on the graph, and Ɵ represents the vector of coefficients for the
    features of each data point in our dataset. Remember that we are trying to find
    the set of coefficients for our features that results in the least error between
    our model’s predictions and the true values of our data points’ target variables:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在*方程2.2*中，![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>θ</mi><mi>j</mi></msub></mrow></math>](img/6.png)代表图上的位置，而θ代表我们数据集中每个数据点特征的系数向量。记住，我们正在尝试找到一组系数，使得我们的模型预测与数据点目标变量的真实值之间的误差最小：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>h</mi><mi>θ</mi></msub><mfenced
    open="(" close=")"><msup><mi>x</mi><mfenced open="(" close=")"><mi>i</mi></mfenced></msup></mfenced></mrow></mrow></math>](img/7.png)then
    represents the predicted target variable for each data point'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>h</mi><mi>θ</mi></msub><mfenced
    open="(" close=")"><msup><mi>x</mi><mfenced open="(" close=")"><mi>i</mi></mfenced></msup></mfenced></mrow></mrow></math>](img/7.png)表示每个数据点的预测目标变量'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></math>](img/8.png)represents
    the true target value for each data point'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></math>](img/8.png)表示每个数据点的真实目标值'
- en: As we can see, in the broader set of parentheses, we are again subtracting the
    true target value from the predicted target variable value for each data point.
    This is because *Equation 2.2* is derived from *Equation 2.1* (the mathematical
    proof of this derivation is omitted here for simplicity).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在更广泛的一组括号中，我们再次从每个数据点的预测目标变量值中减去真实目标值。这是因为*方程2.2*是从*方程2.1*（这里省略了该推导的数学证明以简化）推导出来的。
- en: '*m* represents the number of features we have for each data point in our dataset.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*m*代表我们数据集中每个数据点的特征数量。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/9.png)
    is what we refer to as the **learning rate**. It’s one of the hyperparameters
    of our algorithm and it determines the size of the step we should take; i.e.,
    the magnitude by which we should move in the selected direction.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/9.png)就是我们所说的**学习率**。它是我们算法的其中一个超参数，它决定了我们应该采取的步长大小；即，我们在所选方向上移动的幅度。'
- en: Altogether, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mstyle
    scriptlevel="+1"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mrow><mo>(</mo><msub><mi>h</mi><mi>θ</mi></msub><mfenced
    open="(" close=")"><msup><mi>x</mi><mfenced open="(" close=")"><mi>i</mi></mfenced></msup></mfenced><mo>−</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow></mrow></mrow></mrow></math>](img/10.png)represents
    the derivative of the loss function at our current location on the graph. Therefore,
    *Equation 2.2* says that our next location will be equal to our current location
    minus the derivative of the loss function at our current location on the graph
    multiplied by the learning rate. This then takes a step toward the minimum point,
    whereby the derivative of the loss function determines the direction and the combination
    with the learning rate defines the magnitude.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mstyle
    scriptlevel="+1"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mrow><mo>(</mo><msub><mi>h</mi><mi>θ</mi></msub><mfenced
    open="(" close=")"><msup><mi>x</mi><mfenced open="(" close=")"><mi>i</mi></mfenced></msup></mfenced><mo>−</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow></mrow></mrow></mrow></math>](img/10.png)代表了在图上当前位置损失函数的导数。因此，*公式2.2*表明我们的下一个位置将等于当前位置减去在图上当前位置损失函数的导数乘以学习率。这样就会朝着最小值点迈出一步，其中损失函数的导数决定了方向，而与学习率的组合则定义了幅度。
- en: It’s very important to note that *Equation 2.2* represents just one step in
    the gradient descent process. We iterate over this process many times, in each
    iteration scanning through the dataset and inputting our estimated coefficients,
    calculating the error/loss, and then reducing the loss by using gradient descent
    to step toward the loss function’s minimum point. We may never exactly reach the
    minimum point but even if we can get quite close, our model’s estimates and predictions
    could be acceptably accurate.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 非常重要的是要注意，*公式2.2*仅仅代表了梯度下降过程中的一个步骤。我们多次迭代这个过程，在每次迭代中，我们遍历数据集，输入我们的估计系数，计算误差/损失，然后通过梯度下降朝着损失函数的最小值点迈进来减少损失。我们可能永远无法精确地达到最小值点，但即使我们能够非常接近，我们的模型估计和预测也可能是可接受的准确。
- en: 'We should note that there are different configurations for gradient descent.
    In batch gradient descent, we would go through the entire training set in each
    iteration. Alternatively, we could implement mini-batch gradient descent, in which
    case each iteration would process subsets of the training dataset. This approach
    is less thorough but can be more efficient. A popular implementation is known
    as stochastic gradient descent (the word “stochastic” means “random”). In stochastic
    gradient descent, we take a subset of random samples from our dataset in each
    iteration, which could be as little as one data point in each sample. The key
    here is that because we take a random subset in each iteration, we start from
    a different point in our feature space each time. At first, this seems somewhat
    erratic, as we jump around to different points in our feature space. However,
    this approach has been shown to be quite effective in minimizing the overall loss
    function. It can also help to avoid something referred to as **local minima**,
    which refers to the fact that some loss functions are not as simple as the one
    we showed in *Figure 2**.5*. They may have multiple peaks and valleys, in which
    case the bottom of any of the valleys could be considered a type of minimum point,
    but a local minimum may not be the overall minimum of the function, which is referred
    to as the **global minimum**. *Figure 2**.8* shows an example of multiple minima
    and maxima:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，梯度下降有不同的配置。在批量梯度下降中，我们会在每次迭代中遍历整个训练集。或者，我们可以实现小批量梯度下降，在这种情况下，每次迭代会处理训练数据集的子集。这种方法不如全面，但可能更有效率。一种流行的实现方式被称为随机梯度下降（“随机”一词意味着“随机”）。在随机梯度下降中，我们每次迭代从数据集中抽取一个随机样本子集，这可能是每个样本中只有一个数据点。关键在于，因为我们每次迭代都抽取一个随机子集，所以我们每次都是从特征空间的不同点开始。起初，这似乎有些无序，因为我们会在特征空间的不同点跳跃。然而，这种方法已被证明在最小化整体损失函数方面非常有效。它还可以帮助避免所谓的**局部最小值**，这指的是某些损失函数并不像我们在*图2.5*中展示的那样简单。它们可能有多个峰值和谷底，在这种情况下，任何谷底的底部都可以被视为一种最小点，但局部最小值可能不是函数的整体最小值，这被称为**全局最小值**。*图2.8*展示了多个最小值和最大值的例子：
- en: '![Figure 2.8: Local and global minima and maxima](img/B18143_02_8.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图2.8：局部和全局最小值及最大值](img/B18143_02_8.jpg)'
- en: 'Figure 2.8: Local and global minima and maxima'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：局部和全局最小值及最大值
- en: Although we focused on two-dimensional loss functions in this section, loss
    functions can have more than two dimensions, and the same concepts also apply
    in higher-dimensional space.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在这个部分关注了二维损失函数，但损失函数可以超过两个维度，同样的概念也适用于更高维的空间。
- en: Note
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this section, we chose a specific algorithm (linear regression) and type
    of dataset (tabular) to illustrate an example of the model training process. There
    are, of course, other algorithms and types of data for different use cases, and
    the training processes for those use cases would have their own unique implementations.
    However, the overall model training process generally involves processing the
    input data to try to find some kind of useful pattern or relationship and then
    incrementally honing the accuracy of that pattern or relationship in a repetitive
    way until some determined threshold of accuracy is met or until the training is
    deemed to be ineffective (if the model is failing to effectively learn any useful
    patterns or relationships) and therefore is terminated.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们选择了一个特定的算法（线性回归）和一种数据集类型（表格）来展示模型训练过程的例子。当然，还有其他算法和数据类型适用于不同的用例，那些用例的训练过程会有它们自己独特的实现。然而，整体模型训练过程通常涉及处理输入数据，试图找到某种有用的模式或关系，然后以重复的方式逐步提高该模式或关系的准确性，直到达到某个确定的准确度阈值或直到训练被认为无效（如果模型未能有效地学习任何有用的模式或关系）并因此终止。
- en: Configuring and tuning hyperparameters
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置和调整超参数
- en: 'Hyperparameters are parameters that define aspects of how your model training
    jobs run. They are not the parameters in your dataset from which your models learn
    but rather external configuration options related to how the model training process
    is executed. To start with a simple example: we’ve discussed that training jobs
    often need to cycle through the training dataset multiple times in order to learn
    patterns in the data. One of the hyperparameters you may configure for a model
    training job would be to specify how many times it should cycle over the dataset.
    This is often referred to as the number of **epochs**, where an epoch represents
    one iteration through the training dataset.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是定义模型训练作业运行方面的参数。它们不是模型从数据集中学习的参数，而是与模型训练过程执行相关的外部配置选项。以一个简单的例子开始：我们讨论过，训练作业通常需要多次遍历训练数据集，以便学习数据中的模式。您可能为模型训练作业配置的一个超参数可能是指定它应该遍历数据集的次数。这通常被称为**epoch**的数量，其中epoch代表一次遍历训练数据集。
- en: Choosing the optimal values for your hyperparameters is another activity that
    usually requires a lot of trial-and-error attempts, in which you may need to experiment
    with different combinations of hyperparameter values in order to find the optimal
    settings to maximize your model training performance. Let’s continue our simple
    example of configuring the number of times the training job should process through
    the training dataset. Considering that the model could potentially learn more
    information each time it runs through the dataset, we may at first think that
    deciding on the number of epochs would be a simple choice, i.e., that we should
    just set this value to be very high so that the model learns more from the data.
    However, in reality, this is often not the best option because it’s not always
    true that a model learns more useful information every time it processes through
    the dataset. There are concepts in machine learning called **underfitting** and
    **overfitting**, which we will explore in the *challenges* section of this chapter.
    They relate to problems in which continued training on the existing dataset will
    fail to yield desired results.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为您的超参数选择最佳值是另一种通常需要大量试错尝试的活动，在这个过程中，您可能需要尝试不同的超参数值组合，以找到最大化模型训练性能的最佳设置。让我们继续我们的简单例子，即配置训练作业应该处理训练数据集的次数。考虑到模型每次遍历数据集时可能学习到更多信息，我们最初可能会认为决定epoch的数量将是一个简单的选择，即我们只需将此值设置得非常高，以便模型从数据中学习到更多信息。然而，在现实中，这通常不是最佳选择，因为模型每次处理数据集时并不总是能学习到更多有用的信息。机器学习中存在一些称为**欠拟合**和**过拟合**的概念，我们将在本章的*挑战*部分进行探讨。它们与继续在现有数据集上训练将无法产生预期结果的问题相关。
- en: 'Even in cases when the model does learn useful information each time it processes
    the dataset, it often reaches a point where the rate at which it is learning new
    information will taper off or reach a plateau. When this happens, it would be
    inefficient to continue running through the dataset again and again. Bear in mind
    that it can be very expensive to train a model on a large dataset, so you do not
    want to keep processing the data when the model is reaching a plateau in its learning.
    We can measure the rate of learning by generating a **learning curve** graph,
    which graphs the training errors during the training process. *Figure 2**.9* shows
    an example of a learning curve:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在模型每次处理数据集时都能学习到有用信息的情况下，它通常也会达到一个点，即它学习新信息的速率会放缓或达到平台期。当这种情况发生时，再次反复遍历数据集将是不高效的。请记住，在大型数据集上训练模型可能非常昂贵，因此当模型的学习达到平台期时，您不希望继续处理数据。我们可以通过生成**学习曲线**图来衡量学习速率，该图显示了训练过程中的训练错误。*图2.9*显示了学习曲线的一个示例：
- en: '![Figure 2.9: Learning curve graph](img/B18143_02_09.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图2.9：学习曲线图](img/B18143_02_09.jpg)'
- en: 'Figure 2.9: Learning curve graph'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：学习曲线图
- en: In *Figure 2**.9*, the gap between the blue and red lines represents the error
    between our model’s predictions on the training dataset and the testing dataset.
    When the graph shows that the gap between the blue and red lines is not significantly
    decreasing, then we know that continued training will not make our model significantly
    more accurate, and it may be a good point at which to stop the training process.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图2.9**中，蓝色和红色线之间的差距代表我们的模型在训练数据集和测试数据集上的预测误差。当图表显示蓝色和红色线之间的差距没有显著减少时，我们知道继续训练不会使我们的模型显著更准确，这可能是一个停止训练过程的良好时机。
- en: An analogy would be if we had a student who had read a book so many times that
    they memorized every word and thoroughly understood every concept in the book.
    At that point, it would be unnecessary to continue instructing the student to
    read the book again and again because they would no longer learn anything new
    from that book. Now let’s also imagine that we needed to pay the student every
    time they read the book, which is analogous to paying for the computing resources
    required to train the model on the dataset.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一个类比是，如果我们有一个学生已经读过一本书很多次，以至于记住了每个单词并彻底理解了书中的每个概念。在那个时刻，继续指导学生反复阅读这本书就变得没有必要了，因为他们将不再从这本书中学到任何新东西。现在让我们也想象一下，我们需要每次学生阅读这本书时都付给他们报酬，这类似于为在数据集上训练模型所需的计算资源付费。
- en: Configuring the number of epochs is just one example of trying to find the best
    configuration for a particular hyperparameter. Different types of algorithms have
    different types of hyperparameters that pertain to them, and trying to test all
    of the various combinations of hyperparameter values can be a painstaking and
    time-consuming task for data scientists. Fortunately, Google Cloud has a tool
    that performs this task for us in an automated fashion, which we will explore
    later in this book.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 配置训练轮数（epochs）是尝试为特定超参数找到最佳配置的一个例子。不同类型的算法有不同的超参数类型，尝试测试所有超参数值的组合对于数据科学家来说可能是一个痛苦且耗时的工作。幸运的是，Google
    Cloud有一个工具可以自动执行这项任务，我们将在本书的后面部分探讨。
- en: Deploying the model
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署模型
- en: Finally! We have found the right combinations of data, algorithms, and hyperparameter
    values and we’ve gotten to a point where our model is ready to be used in the
    real world, which we refer to as **hosting** or **serving** our model. Getting
    to this point has required a lot of work. We may have trained hundreds of different
    versions of our model to get it to a point at which it’s ready to be deployed
    in production. Our data science team may have had to experiment with lots of different
    dataset versions and transformations and lots of different algorithms and hyperparameter
    values in order to finally get some meaningful results or insights. Up until only
    around five years ago, performing all of those steps and tracking their results
    would have been a very slow, manual, and painstaking process. It still can be
    a lot of work, but at least now there are tools that automate a lot of these steps
    and they can be completed much more quickly, perhaps taking weeks instead of months.
    Later in this book, we’ll talk about something called AutoML, which can reduce
    this entire process down to a few minutes or hours in just a few short commands!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 终于！我们已经找到了合适的数据、算法和超参数值的组合，并且我们的模型已经准备好在现实世界中使用了，我们称之为**托管**或**服务**我们的模型。达到这个阶段需要大量的工作。我们可能已经训练了数百个不同版本的模型，以便将其部署到生产环境中。我们的数据科学团队可能不得不尝试许多不同的数据集版本和转换，以及许多不同的算法和超参数值，才能最终获得一些有意义的成果或见解。直到大约五年前，执行所有这些步骤并跟踪其结果仍然是一个非常缓慢、手动且痛苦的过程。尽管如此，现在至少有工具可以自动化许多这些步骤，并且可以更快地完成，可能只需要几周而不是几个月。在这本书的后面部分，我们将讨论一种称为AutoML的技术，它可以将整个流程简化为几个简短的命令，只需几分钟或几小时即可完成！
- en: Deploying our model can be as simple as packaging it into a Docker container
    and deploying the container on a server, although we will usually want to create
    some kind of web-based API to facilitate access to the model by applications.
    We will do this in a hands-on activity later. Also, when we cover MLOps, we’ll
    see how it may make sense for us to create pipelines to automate the deployment
    of our model, in much the same way as we use CI/CD pipelines to build and deploy
    regular software applications.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 部署我们的模型可能就像将其打包成Docker容器并在服务器上部署容器一样简单，尽管我们通常会希望创建某种基于Web的API，以方便应用程序访问模型。我们将在后续的动手活动中这样做。此外，当我们介绍MLOps时，我们将看到为什么对我们来说创建管道来自动化模型的部署是有意义的，这与我们使用CI/CD管道构建和部署常规软件应用的方式非常相似。
- en: Monitoring the model after deployment
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署后监控模型
- en: You might think that once you’ve successfully tested and deployed the model
    to production then your job is done. However, the fun doesn’t stop there! Just
    like regular software, you need to monitor the performance of your model on an
    ongoing basis. This includes traditional monitoring, such as keeping track of
    how many requests your model is serving in a given timeframe (per second, for
    example), how long it takes for your model to respond to a request (latency),
    and whether these metrics are changing over time. However, ML models also have
    additional requirements that need to be monitored, such as ML-specific metrics
    such as those mentioned in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015) (MAE,
    MSE, accuracy, precision, etc.). These metrics help us to understand how our models
    are performing from an inference perspective, so we need to monitor them and ensure
    they continue to meet our business requirements.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为一旦你成功测试并部署了模型到生产环境，你的工作就完成了。然而，乐趣并没有停止！就像常规软件一样，你需要持续监控你的模型性能。这包括传统的监控，例如跟踪你的模型在给定时间段内（例如每秒）服务了多少请求，模型响应请求需要多长时间（延迟），以及这些指标是否随时间变化。然而，机器学习模型还有额外的监控需求，例如像在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中提到的机器学习特定指标（MAE、MSE、准确率、精确度等）。这些指标帮助我们了解我们的模型从推理角度的表现，因此我们需要监控它们，并确保它们继续满足我们的业务需求。
- en: Note
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The stages in the ML model development life cycle that you’ve learned about
    in this section form the basis for understanding MLOps and AutoML. We dedicate
    an entire chapter of this book to ML engineering and MLOps on Google Cloud, but
    for now, at a high level, you can consider the goals of MLOps and AutoML to be
    the automation of all of the steps in the ML model development life cycle. You’ll
    see in the chapter on MLOps that there are tools we can use to create pipelines
    that automate all of the outlined steps. We can have complex combinations of pipelines
    within pipelines, which would automate everything from preparing and transforming
    the input data to training and deploying the models, monitoring the models in
    production, and automatically kicking off the entire process all over again if
    we detect that our model has stopped providing desirable results and we want to
    retrain the models on updated data. This would provide a self-healing model ecosystem,
    which helps to keep our models up to date on an ongoing basis.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中你学到的机器学习模型开发生命周期中的各个阶段是理解MLOps和AutoML的基础。我们在这本书中用整整一章来介绍Google Cloud上的机器学习工程和MLOps，但到目前为止，从高层次来看，你可以认为MLOps和AutoML的目标是自动化机器学习模型开发生命周期的所有步骤。你将在MLOps章节中看到，我们可以使用工具来创建管道，自动化所有概述的步骤。我们可以在管道中拥有复杂的管道组合，这将自动化从准备和转换输入数据到训练和部署模型，监控生产中的模型，以及如果我们检测到我们的模型停止提供期望的结果，我们希望重新训练模型在更新的数据上，自动重新启动整个过程的全部工作。这将提供一个自我修复的模型生态系统，有助于持续保持我们的模型更新。
- en: Roles and personas in AI/ML projects
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI/ML项目中的角色和人物
- en: 'Throughout this book, we mention various roles such as data scientist, data
    engineer, and ML engineer. We also mention more traditional roles such as software
    engineer, project manager, stakeholder, and business leader. The traditional roles
    have been defined in the industry for decades now, so we will not define them
    here, but there is often confusion about the newer roles that are specific to
    AI/ML projects, so we’ll take some time to briefly describe them here. In small
    teams, it should be noted that a single person may perform all of these roles:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们提到了各种角色，如数据科学家、数据工程师和机器学习工程师。我们还提到了更多传统的角色，如软件工程师、项目经理、利益相关者和业务领导者。这些传统角色在行业中已经定义了几十年，所以我们在这里不会定义它们，但对于特定于AI/ML项目的较新角色，常常存在混淆，因此我们将在这里简要描述它们。在小型团队中，需要注意的是，一个人可能需要执行所有这些角色：
- en: '**Data engineer**: Data engineers are usually involved in the earlier stages
    of the data science project—specifically the data gathering, exploration, and
    transformation stages. A data engineer will often be tasked with finding relevant
    data and cleaning it up to be used in later stages of the project.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据工程师**：数据工程师通常参与数据科学项目的早期阶段——具体来说是数据收集、探索和转换阶段。数据工程师通常负责找到相关数据并将其清理干净，以便在项目的后期阶段使用。'
- en: '**Data scientist**: A data scientist is usually the role we associate with
    actually training ML models. They will usually also perform data gathering, exploration,
    and transformation activities as they iterate through various model training experiments.
    In some cases, data scientists will be the more senior members working on the
    project and they may provide direction to the data engineers and ML engineers.
    They will often be responsible for the resulting ML models that are created, although
    those models are created and deployed with help from the data engineers and ML
    engineers.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据科学家**：数据科学家通常是实际训练机器学习模型的角色。他们在迭代各种模型训练实验的过程中，通常会执行数据收集、探索和转换等活动。在某些情况下，数据科学家将是项目中的资深成员，他们可能会为数据工程师和机器学习工程师提供指导。他们通常负责创建的机器学习模型，尽管这些模型是在数据工程师和机器学习工程师的帮助下创建和部署的。'
- en: '**ML engineer**: The ML engineer role usually refers to a software engineer
    who has ML or data science expertise. They understand ML concepts and they are
    experts in the stages of the model development life cycle. They are usually the
    bridge that brings DevOps expertise to an ML project in order to create an MLOps
    workload. When a data scientist creates a model that they believe is ready to
    be used in production, they may work with an ML engineer to put all of the mechanisms
    in place to actually deploy the model into production via an MLOps pipeline.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习工程师**：机器学习工程师角色通常指的是具有机器学习或数据科学专长的软件工程师。他们理解机器学习概念，并且是模型开发生命周期各个阶段的专家。他们通常是连接DevOps专长到机器学习项目的桥梁，以便创建MLOps工作负载。当数据科学家创建他们认为可以用于生产的模型时，他们可能会与机器学习工程师合作，在MLOps管道中部署模型到生产环境中所需的全部机制。'
- en: Now that we’ve covered the major steps and concepts found in a typical AI/ML
    project, let’s take a look at the kinds of pitfalls that companies often run into
    when trying to implement such projects.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了典型AI/ML项目中发现的重大步骤和概念，让我们来看看公司在尝试实施此类项目时通常会遇到的陷阱。
- en: Common challenges encountered in the ML model development life cycle
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在机器学习模型开发生命周期中遇到的常见挑战
- en: For some of the stages in the ML model development life cycle, we’ve already
    discussed various challenges that you are likely to encounter in those stages.
    However, in this section, we specifically call out major challenges that you need
    to be aware of as an AI/ML solutions architect interacting with companies who
    are implementing AI/ML workloads. In the *Best practices for overcoming common
    challenges* section later in this chapter, we’ll look at ways to overcome many
    of these challenges.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习模型开发生命周期中的某些阶段，我们已经讨论了你在这些阶段可能会遇到的各种挑战。然而，在本节中，我们特别指出了一些作为与实施AI/ML工作负载的公司互动的AI/ML解决方案架构师需要了解的主要挑战。在本章后面的“克服常见挑战的最佳实践”部分，我们将探讨克服许多这些挑战的方法。
- en: Finding and gathering relevant data
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找和收集相关数据
- en: 'One of our first major challenges is finding relevant data for the business
    problem our models are being built to address. We presented some examples of potential
    data sources in the previous section, and in some cases, the data you need may
    be readily available to you, but finding the relevant data is not always straightforward
    for data scientists and data engineers. The following are some common challenges
    with regard to finding and accessing the right data:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们面临的第一大挑战之一是找到为解决我们的模型构建的业务问题所需的相关数据。我们在上一节中提供了一些潜在数据源的示例，在某些情况下，您可能已经可以轻松获得所需的数据，但找到相关数据对于数据科学家和数据工程师来说并不总是直接的。以下是在寻找和访问正确数据方面的一些常见挑战：
- en: If you work in a large company, the data may exist somewhere in your company
    owned by another team or organization, but you may not know about it or you may
    not know how to find it.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您在一家大公司工作，数据可能存在于您公司内的另一个团队或组织中，但您可能不知道它或不知道如何找到它。
- en: The data might need to be created from a combination of different data sources
    that are dispersed throughout your company, all owned by disjointed organizations.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可能需要从散布在公司各个部门的不同数据源中创建，这些数据源由不同的组织拥有。
- en: The data could contain sensitive information, and therefore be subject to regulations
    and restrictions regarding how it needs to be stored and accessed.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可能包含敏感信息，因此可能受到有关其存储和访问方式的法规和限制。
- en: You may need to consult with experts in order to find, validate, and understand
    the data, e.g., financial data, medical data, atmospheric data, or other data
    related to fields of specific expertise.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能需要咨询专家以找到、验证和理解数据，例如，财务数据、医疗数据、大气数据或与特定专业领域相关的其他数据。
- en: The data may be stored in databases that are restricted for use only in production
    transaction operations.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可能存储在仅限于生产事务操作使用的数据库中。
- en: You may not know whether you can trust the contents of the data. For example,
    is it accurate?
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能不知道您是否可以信任数据的内含内容。例如，它是否准确？
- en: The data could contain inherent biases or other unknown challenges.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可能包含固有的偏差或其他未知挑战。
- en: Picking an algorithm and model architecture
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择算法和模型架构
- en: When it comes to picking an algorithm or model architecture to use, one of the
    biggest challenges initially can be just figuring out where to start. You don’t
    want to spend months just experimenting with different options before finding
    a useful implementation or never finding a useful implementation.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到选择要使用的算法或模型架构时，最初的最大的挑战之一可能是仅仅确定从哪里开始。您不希望花费数月时间仅在不同选项上进行实验，直到找到有用的实现或永远找不到有用的实现。
- en: Data labeling
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标注
- en: Data labeling can be a very manual task, requiring humans to go through enormous
    amounts of data and add the labels for each data point. Tools have been built
    in recent years that automate some labeling tasks or make the tasks easier for
    humans to perform, but there is still a need to have humans involved in labeling
    datasets. So, a major challenge that can exist for companies is finding and hiring
    a strong data-labeling workforce. Bear in mind that some data labeling tasks may
    require specific expertise. For example, consider a dataset consisting of medical
    images. A specific characteristic in the image could indicate the presence of
    a particular medical condition. It often requires special training to be able
    to read the medical images and identify the specific characteristics in question,
    so this is not a task for which you could hire any random person. As we’ve discussed
    previously, if your labels are not accurate, then your models will not be accurate,
    and in the example of medical imaging described here, this could have critical
    implications for a medical facility that is tasked with diagnosing life-threatening
    medical conditions.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标注可能是一项非常手动的工作，需要人类通过大量数据并为每个数据点添加标签。近年来已经开发了一些工具来自动化一些标注任务或使任务对人类来说更容易执行，但仍然需要在标注数据集时有人类参与。因此，公司可能面临的一个主要挑战是寻找和雇佣一支强大的数据标注团队。请记住，某些数据标注任务可能需要特定的专业知识。例如，考虑一个由医学图像组成的数据集。图像中的特定特征可能表明存在某种特定的医疗状况。通常需要特殊培训才能阅读医学图像并识别所讨论的具体特征，因此这不是可以雇佣任何随机人员的任务。正如我们之前讨论的，如果您的标签不准确，那么您的模型也不会准确，在本例中，对于被委托诊断危及生命医疗状况的医疗设施来说，这可能具有重大影响。
- en: Training models
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: Two classic challenges with regard to training models are the problems of **underfitting**
    and **overfitting**. These challenges relate to how well your model learns a relationship
    or pattern in your data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模型训练的两个经典挑战是**欠拟合**和**过拟合**问题。这些挑战与你的模型如何学习数据中的关系或模式有关。
- en: 'In the case of supervised learning, we generally split our dataset into the
    three subsets mentioned earlier: training, validation, and testing. The validation
    set is usually used in hyperparameter tuning, the training dataset is what the
    model is trained on, and the testing dataset is how we evaluate the trained model.
    We evaluate the model based on the metrics that we’ve defined for that model,
    such as accuracy, precision, or MSE. In this context, the test dataset is new
    data that the model did not see during the model training process and we want
    to determine if the model’s predictions are accurate when it sees this new data—i.e.,
    we want to determine how well the model **generalizes** to new data. If a model
    provides very accurate predictions on the training dataset but inaccurate or less
    accurate predictions on the testing dataset then we say that the model is overfitting.
    This means it fits too closely to the training data, and it cannot perform well
    when it is exposed to new data.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习的情况下，我们通常将数据集分为前面提到的三个子集：训练集、验证集和测试集。验证集通常用于超参数调整，训练数据集是模型训练所用的数据，测试数据集是我们评估训练模型的方式。我们根据为该模型定义的指标来评估模型，例如准确率、精确度或均方误差。在这种情况下，测试数据集是模型在训练过程中未见过的新的数据，我们希望确定当模型看到这些新数据时，其预测是否准确——即，我们希望确定模型对新数据的**泛化能力**。如果一个模型在训练数据集上提供非常准确的预测，但在测试数据集上提供不准确或不太准确的预测，那么我们就说该模型是过拟合的。这意味着它过于紧密地拟合了训练数据，当它接触到新数据时无法表现良好。
- en: On the other hand, if we find that the model is not performing well on either
    dataset (training or testing), then we say it is underfitting.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们发现模型在任一数据集（训练或测试）上表现不佳，那么我们就说它是欠拟合的。
- en: '*Figure 2**.10* shows an example of fitting, overfitting, and underfitting
    for a classification model that is trying to determine the difference between
    the blue and red data points. The black boundary line represents overfitting because
    it fits much too precisely to the dataset, and the purple line represents underfitting
    because it does not do a good job of capturing the differences between the blue
    and red dots. The green line does a pretty good job of separating the blue and
    red dots; it’s not completely perfect, but it could be an acceptable model that
    generalizes well to the characteristics of the data points.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.10*展示了试图确定蓝色和红色数据点之间差异的分类模型的拟合、过拟合和欠拟合的示例。黑色边界线代表过拟合，因为它与数据集拟合得过于精确，紫色线代表欠拟合，因为它没有很好地捕捉蓝色和红色点之间的差异。绿色线很好地分离了蓝色和红色点；它并不完全完美，但可能是一个泛化能力良好的可接受的模型。'
- en: '![Figure 2.10: Example of fitting, overfitting, and underfitting](img/B18143_02_10.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图2.10：拟合、过拟合和欠拟合的示例](img/B18143_02_10.jpg)'
- en: 'Figure 2.10: Example of fitting, overfitting, and underfitting'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10：拟合、过拟合和欠拟合的示例
- en: In addition to classic training challenges such as those mentioned, there are
    other challenges that relate to the overall process of how training is performed
    as part of a broader AI/ML project, such as lineage tracking, as we mentioned
    in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015). In large AI/ML projects, there
    may be multiple teams of data scientists performing experiments and training hundreds
    of models. Keeping track of their results and sharing them with each other can
    be very challenging in large projects.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的经典训练挑战之外，还有其他挑战与作为更广泛AI/ML项目一部分的训练整体过程有关，例如我们提到的[*第一章*](B18143_01.xhtml#_idTextAnchor015)中的谱系跟踪。在大型的AI/ML项目中，可能有多个数据科学家团队进行实验和训练数百个模型。在大型项目中跟踪他们的结果并与他们分享可能非常具有挑战性。
- en: Configuring and tuning hyperparameters
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置和调整超参数
- en: Finding the optimal set of hyperparameter values can be almost as challenging
    as curating a usable dataset or choosing the correct algorithm to use. Considering
    that hyperparameters affect how our model training jobs operate, each job can
    take a long time to run, and there can be thousands of combinations of hyperparameter
    values to explore, doing this manually can be very challenging and time-consuming.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最佳的超参数值集合可能几乎与制作一个可用的数据集或选择正确的算法一样具有挑战性。考虑到超参数会影响我们的模型训练作业的运行方式，每个作业可能需要很长时间才能运行，并且可能存在数千种超参数值的组合需要探索，手动进行这项工作可能非常具有挑战性和耗时。
- en: Evaluating models
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: Although we generally perform some validation and testing during the training
    and hyperparameter tuning processes, we should thoroughly evaluate our model before
    deploying it in production. In the section titled “Common challenges in developing
    machine learning applications” in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    we discussed the challenge of delivering business value from data science projects.
    We called out the need for data scientists to work with business stakeholders
    to thoroughly understand the business requirements that the target AI/ML system
    is intended to address. The evaluation step in our data science project is where
    we check to ensure that the models and solutions we’ve created adequately address
    those business requirements, based on the metrics we defined for measuring success.
    In addition to the data science team evaluating the models, it may also be relevant
    at this point to review the results with the business stakeholders to ensure they
    align with expectations. If we find that the results are not satisfactory, we
    usually need to retry the process from earlier in the data science lifecycle,
    perhaps with new or different data, different algorithms, and/or different hyperparameter
    values. We may need to repeat these steps in an interactive process until we appropriately
    satisfy the business requirements.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在训练和超参数调整过程中通常进行一些验证和测试，但在将模型部署到生产环境中之前，我们应该彻底评估我们的模型。在第一章“开发机器学习应用中的常见挑战”部分中，我们讨论了从数据科学项目中实现商业价值所面临的挑战。[*第一章*](B18143_01.xhtml#_idTextAnchor015)中，我们强调了数据科学家与业务利益相关者合作，彻底理解目标人工智能/机器学习系统旨在解决的业务需求的重要性。在我们数据科学项目中，评估步骤就是检查我们创建的模型和解决方案是否充分满足了这些业务需求，这是基于我们定义的成功度量标准。除了数据科学团队评估模型外，在此阶段与业务利益相关者审查结果，以确保它们符合预期，也可能是相关的。如果我们发现结果不满意，我们通常需要从数据科学生命周期的早期重新尝试这个过程，可能使用新的或不同的数据、不同的算法和/或不同的超参数值。我们可能需要在交互式过程中重复这些步骤，直到我们适当地满足业务需求。
- en: Deploying models
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署模型
- en: When deploying our model, we’ll need to select the kinds of computing resources
    that are required to serve our model adequately. Depending on the model architecture,
    we may need to include GPUs/TPUs, usually also in combination with CPUs, and,
    of course, RAM. A really important activity at this point in the project is to
    “right-size” these components. To do this, we’ll need to estimate how much of
    each type of component will be required to serve our model as accurately as possible
    given the traffic we expect to receive in terms of requests per second to our
    model. Why is this so important? Well, model hosting is often reported by companies
    to be by far their number one cost when it comes to their AI/ML. It can account
    for up to 90% of a company’s AI/ML costs. So, if we configure our servers with
    more resources than we need, it will increase those costs. On the other hand,
    if we don’t configure enough resources, we will not be able to handle the number
    of requests coming from our clients, resulting in disruption of service from our
    models. Fortunately, cloud AI/ML services such as Vertex will auto-scale our model-hosting
    infrastructure to meet increased demand, but we still need to get the sizing as
    accurate as possible for each server in order to control our costs.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署我们的模型时，我们需要选择足够的计算资源来充分服务我们的模型。根据模型架构，我们可能需要包括GPU/TPU，通常也结合CPU，当然还有RAM。在这个项目阶段的一个非常重要的活动是“正确配置”这些组件。为此，我们需要估计每种类型的组件需要多少才能尽可能准确地服务我们的模型，考虑到我们期望接收的每秒请求数量。为什么这如此重要呢？嗯，模型托管通常被公司报告为它们在AI/ML方面的最大成本。它可能占公司AI/ML成本的90%。因此，如果我们配置的服务器比我们需要的资源更多，这将增加成本。另一方面，如果我们没有配置足够的资源，我们将无法处理来自客户的请求数量，从而导致我们的模型服务中断。幸运的是，像Vertex这样的云AI/ML服务可以自动扩展我们的模型托管基础设施以满足增加的需求，但我们需要尽可能准确地确定每台服务器的规模，以便控制成本。
- en: Something we also need to keep in mind when deploying our models is how quickly
    our applications need to get responses for the inference requests. We will need
    to keep this in mind as we architect our model hosting infrastructure.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署我们的模型时，我们还需要考虑我们的应用程序需要多快地对推理请求做出响应。在构建我们的模型托管基础设施时，我们需要记住这一点。
- en: Monitoring models after deployment
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署后监控模型
- en: In addition to monitoring the various metrics associated with our models, an
    important concept to call out at this point is something referred to as **drift**,
    which can be represented in various formats, such as model drift, data drift,
    or concept drift. To explain the concept of drift, we will first dive a bit deeper
    into the relationship between the model training process and how models operate
    in production.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 除了监控与我们的模型相关的各种指标外，在此阶段需要强调的一个重要概念是所谓的**漂移**，它可以以各种格式表示，如模型漂移、数据漂移或概念漂移。为了解释漂移的概念，我们首先需要深入探讨模型训练过程与模型在生产中运行之间的关系。
- en: Note that during model training, the model was exposed to data in a specific
    format and with specific constraints, and that’s how it learned. Let’s refer to
    the state of the input data as its **shape**, which refers to the format and constraints
    of that data. When we deploy our model to production and we expose it to new data
    in order to get predictions from the model, we want to ensure that the shape of
    the new data matches the shape of the training data as much as possible. We are
    not referring to the contents of the data but rather how the data is represented
    to the model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在模型训练期间，模型接触到了特定格式和特定约束的数据，这就是它学习的方式。让我们将输入数据的状态称为其**形状**，它指的是数据的格式和约束。当我们将我们的模型部署到生产环境中，并暴露给新数据以从模型中获得预测时，我们希望确保新数据的形状尽可能匹配训练数据的形状。我们不是指数据的内涵，而是数据如何表示给模型。
- en: 'During our training process, we may have performed transformations on raw data
    to make it more suitable for training the model. If so, we need to perform the
    same kinds of transformations on any new data that we send to the model to make
    predictions. In the real world, however, data can change over time, and therefore
    the shape of the data can change over time. We refer to this as drift, which is
    when the raw data out in the real world has fundamentally changed in some way
    since we trained our model. Let’s look at a couple of examples of drift in order
    to clarify the point:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的训练过程中，我们可能对原始数据进行了转换，使其更适合模型训练。如果是这样，我们需要对任何我们发送给模型进行预测的新数据进行相同的转换。然而，在现实世界中，数据会随着时间的推移而变化，因此数据的形状也会随着时间的推移而变化。我们称这种现象为漂移，即自我们训练模型以来，现实世界中的原始数据在某种程度上发生了根本性的变化。让我们看看几个漂移的例子，以便阐明这一点：
- en: '**Example one**:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例一**：'
- en: We trained our model on data that was gathered from customers filling in forms
    online. Our model tries to predict whether those customers would likely respond
    well to a specific marketing campaign in which we would send them targeted emails
    with discounts on shoes. Recently, an administrator decided that they wanted to
    capture some additional information from customers and that some of the previously
    captured data is no longer relevant, so they added some fields to the form and
    removed some other fields. Now, when new customers fill in the form and that data
    is sent to our model, there will be extra fields in the input that the model has
    never seen before and the other fields that it expects to see will no longer be
    there. This could affect our model’s ability to interpret and use the input data
    effectively, causing errors or incorrect predictions.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用从在线填写表格的客户那里收集的数据来训练我们的模型。我们的模型试图预测这些客户是否可能对一项特定的营销活动做出良好反应，该活动将向他们发送带有鞋类折扣的目标电子邮件。最近，一位管理员决定他们想要从客户那里获取一些额外的信息，并且一些之前收集的数据不再相关，因此他们在表格中添加了一些字段并删除了一些其他字段。现在，当新客户填写表格并将这些数据发送到我们的模型时，输入中将有模型以前从未见过的额外字段，而模型期望看到的其他字段将不再存在。这可能会影响模型有效解释和使用输入数据的能力，导致错误或预测不正确。
- en: '**Example two**:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例二**：'
- en: We have built a model that estimates how quickly we can deliver products to
    customers. The model uses inputs from a number of different sources. One of the
    sources is a dataset that contains historical data on how long it took to deliver
    products to customers in past deliveries. We receive an update of that dataset
    every day, which contains the details of all of the orders that were delivered
    the previous day. A critical feature in that dataset is the delivery time, which
    is measured in days. This dataset is created by a system that is owned by a different
    organization in our company, so we do not have control over that dataset. Our
    delivery process has become a lot more efficient recently, and some products are
    being delivered the same day, so the delivery time has now been updated to be
    measured in hours instead of days. However, nobody informed our data science team
    about this change. Now, our model looks at the delivery time feature, and because
    the unit of measurement has changed, its predictions for new delivery times are
    incorrect.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个模型，用于估算我们向客户交付产品所需的速度。该模型使用来自多个不同来源的输入。其中一个来源是一个包含过去交付中向客户交付产品所需时间的历史数据的数据库。我们每天都会收到该数据库的更新，其中包含前一天交付的所有订单的详细信息。该数据库中的一个关键特征是交付时间，以天为单位衡量。这个数据库是由我们公司中另一个组织拥有的系统创建的，因此我们无法控制该数据库。最近，我们的交付流程变得更加高效，一些产品现在可以在同一天交付，因此交付时间现在已更新为以小时为单位而不是以天为单位。然而，没有人通知我们的数据科学团队关于这一变化。现在，我们的模型查看交付时间特征，由于度量单位已更改，其对新交付时间的预测是不正确的。
- en: Another interesting example is something with which we all are somewhat familiar.
    Many large retailers use AI/ML models to forecast what to stock in their inventory
    based on data related to what customers are purchasing and they try to look for
    emerging trends to identify changes in consumer behavior. In the first few weeks
    of the COVID-19 pandemic, there was an enormous and sudden shift in what everybody
    wanted to buy. The models were probably surprised to find that everybody was suddenly
    very interested in one thing in particular, which up until then, was generally
    sold at a very predictable rate. What was this thing that the models predicted
    everybody was suddenly interested in? Toilet paper!
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的例子是我们都多少有些熟悉的。许多大型零售商使用AI/ML模型根据与客户购买相关的数据来预测他们库存中应该存放什么，他们试图寻找新兴趋势以识别消费者行为的改变。在COVID-19大流行的前几周，人们对购买什么的需求发生了巨大而突然的变化。模型可能惊讶地发现，突然之间，每个人都对某一特定事物非常感兴趣，在此之前，这一事物通常以非常可预测的速度销售。模型预测的，突然之间每个人都对什么特别感兴趣的东西是什么？卫生纸！
- en: '![Figure 2.11: Empty toilet paper shelves in a store (source: https://commons.wikimedia.org/wiki/File:COVID-19_-_Toilet_Paper_Shortage_%2849740588227%29.jpg)](img/B18143_02_11.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图2.11：商店中空空的卫生纸货架（来源：https://commons.wikimedia.org/wiki/File:COVID-19_-_Toilet_Paper_Shortage_%2849740588227%29.jpg)](img/B18143_02_11.jpg)'
- en: 'Figure 2.11: Empty toilet paper shelves in a store (source: https://commons.wikimedia.org/wiki/File:COVID-19_-_Toilet_Paper_Shortage_%2849740588227%29.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11：商店中空空的卫生纸货架（来源：https://commons.wikimedia.org/wiki/File:COVID-19_-_Toilet_Paper_Shortage_%2849740588227%29.jpg）
- en: There are also much more subtle changes that can happen to our data over time.
    We’ve already discussed how data scientists often want to inspect data before
    training a model, and one of the aspects they often want to inspect is the statistical
    distribution of the values of each of the features (mean, mode, maximum, minimum,
    etc.). These are important details because they give us an understanding of what
    kinds of values our features are generally expected to contain. During inspection,
    this can help us to identify outliers that could indicate erroneous data or that
    may inform us of some other characteristic of the data that we were previously
    unaware of. We can also apply this knowledge to make predictions during production.
    We can analyze the data that is sent to our models for inference purposes, and
    if we see that the statistical distributions have changed in a consistent way,
    then it could alert us to potential data corruption or to the fact that the data
    has genuinely changed, which could indicate a need to update our models by training
    them on new data that matches the updated shape that we’re observing in the real
    world.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们的数据也可能发生许多更微妙的变化。我们之前已经讨论了数据科学家在训练模型之前通常想要检查数据，他们经常想要检查的一个方面是每个特征（均值、众数、最大值、最小值等）的统计分布。这些细节很重要，因为它们让我们了解我们的特征通常预期包含哪些类型的值。在检查过程中，这可以帮助我们识别出可能表明错误数据或可能让我们了解我们之前未意识到的数据其他特征的异常值。我们还可以将这种知识应用于生产中的预测。我们可以分析发送到我们的模型进行推理的数据，如果我们看到统计分布以一致的方式发生变化，那么这可能会提醒我们潜在的数据损坏或数据确实发生了变化，这可能会表明我们需要通过在新数据上训练来更新我们的模型，这些新数据与我们在现实世界中观察到的更新形状相匹配。
- en: As we can see, drift can lead to our models becoming less accurate or providing
    erroneous results, and we therefore need to monitor specifically for this by inspecting
    the data that we’re observing in production, as well as monitoring the expected
    metrics for our models. If we see that our model’s metrics are declining over
    time—or suddenly—this could be an indication of drift between the kind of data
    the model was trained on and the kind of data it observes in production.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，漂移可能导致我们的模型变得不准确或提供错误的结果，因此我们需要通过检查我们在生产中观察到的数据以及监控我们模型的预期指标来特别关注这一点。如果我们看到我们模型的指标随着时间的推移或突然下降，这可能是模型训练的数据类型与它在生产中观察到的数据类型之间漂移的迹象。
- en: Best practices for overcoming common challenges
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 克服常见挑战的最佳实践
- en: This section contains pointers and best practices that companies have developed
    over time to address many of the challenges discussed in the previous section.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含公司随着时间的推移开发出的指针和最佳实践，以解决上一节中讨论的许多挑战。
- en: Finding and gathering relevant data
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找和收集相关数据
- en: We discussed data silos being a common challenge in large companies, as well
    as restrictions with regard to how data is stored and accessed, especially sensitive
    data that could be subject to various regulations and compliance requirements.
    A key to overcoming these challenges is to break down the silos by creating centralized
    data lakes and data discovery mechanisms, such as a searchable data catalog that
    contains metadata describing the various datasets in our data lake. To ensure
    that our data is stored and accessed securely, we need to implement robust encryption
    and permission-based access mechanisms. We will explore these topics in more detail
    in later chapters, and we will also perform some hands-on activities regarding
    detecting and addressing bias and other problems in our datasets.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了数据孤岛是大型公司中常见的挑战，以及关于数据存储和访问的限制，特别是可能受到各种法规和合规要求约束的敏感数据。克服这些挑战的关键是通过创建集中的数据湖和数据发现机制来打破孤岛，例如包含描述我们数据湖中各种数据集元数据的可搜索数据目录。为了确保我们的数据存储和访问安全，我们需要实施强大的加密和基于权限的访问机制。我们将在后面的章节中更详细地探讨这些主题，并执行一些关于检测和解决数据集中偏差和其他问题的动手活动。
- en: In cases where our data exists in databases that are restricted specifically
    for transactional business operations, we could implement a **change data capture**
    (**CDC**) solution that replicates our data to a data lake that can then be used
    for data analytics and AI/ML workloads.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据存在于专门为交易性业务操作限制的数据库中时，我们可以实施一个**变更数据捕获**（**CDC**）解决方案，将我们的数据复制到可以用于数据分析和AI/ML工作负载的数据湖。
- en: Considering that the data-gathering process happens at the very beginning of
    our AI/ML workload, it’s critical that we implement data quality checks at this
    point in order to prevent issues later in our workload. For example, we know that
    training our models on corrupt data will result in errors or less accurate model
    outputs. Bear in mind that in most ML use cases, we are periodically training
    our models on updated data that’s coming from some source, perhaps from a system
    that is owned and operated by another team or organization. Therefore, if we create
    data quality checks as this data is coming into our workload and we detect data
    quality issues, we should implement mechanisms that prevent further steps in our
    process from proceeding. Otherwise, performing the downstream steps in our process,
    such as data transformations and model training, would be a waste of time and
    money and could lead to worse consequences in production, such as malfunctioning
    models.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到数据收集过程发生在我们的AI/ML工作负载的最初阶段，我们必须在这个阶段实施数据质量检查，以防止工作负载后期出现问题。例如，我们知道在损坏的数据上训练我们的模型会导致错误或模型输出不准确。请记住，在大多数ML用例中，我们定期在来自某个来源的更新数据上训练我们的模型，这个来源可能是由另一个团队或组织拥有和运营的系统。因此，如果我们在这个数据进入工作负载时创建数据质量检查，并且检测到数据质量问题，我们应该实施机制来防止我们的流程中的后续步骤继续进行。否则，执行我们流程中的下游步骤，如数据转换和模型训练，将是时间和金钱的浪费，并可能导致生产中出现更糟糕的后果，如模型故障。
- en: Data labeling
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标注
- en: If your company is having difficulties finding a workforce to perform your labeling
    tasks, Google Cloud’s data labeling service can help you to get your data labeled
    appropriately.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的公司在寻找执行标注任务的工作团队方面遇到困难，谷歌云的数据标注服务可以帮助您适当地标注数据。
- en: Picking an algorithm and model architecture
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择算法和模型架构
- en: 'Where should we start when picking an algorithm? This is a common challenge,
    and as a result, data scientists have been constructing solutions to try to make
    this easier. In this section, we’ll describe a tiered framework for approaching
    a new AI/ML project in this context:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择算法时，我们应该从哪里开始？这是一个常见的挑战，因此数据科学家一直在构建解决方案，试图使这个过程更容易。在本节中，我们将描述一个分层框架，用于在此背景下处理新的AI/ML项目：
- en: '**Tier 1**: You can see if a packaged solution already exists for your business
    problem. For example, Google Cloud has created packaged solutions for lots of
    different types of use cases such as computer vision, NLP, and forecasting. We’ll
    be covering these solutions in more detail in the coming chapters.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一级**：您可以查看是否已经存在针对您业务问题的打包解决方案。例如，谷歌云已经为许多不同类型的用例创建了打包解决方案，如计算机视觉、自然语言处理和预测。我们将在接下来的章节中更详细地介绍这些解决方案。'
- en: '**Tier 2**: If you want to create and deploy your own model without doing any
    of the work required to do so, check out Google Cloud’s AutoML functionality to
    see if it meets your needs. We’ll also explore this in a hands-on activity in
    a later chapter in this book.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tier 3**: If you want to get started with a model that has been trained by
    somebody else, there are numerous hubs and “model zoos” that exist for data scientists
    to share models they’ve created. Analogous to software libraries in traditional
    software development, these are assets that have been created by other data scientists
    for specific purposes, which you can reuse rather than starting from scratch to
    implement the same functionality. For example, you can find pre-trained models
    for various use cases in Google Cloud’s AI Hub (https://cloud.google.com/ai-hub/docs/introduction).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tier 4**: If the previous options do not meet your specific needs, you can
    create your own custom models. In this case, Google Cloud Vertex AI provides built-in
    algorithms for common use cases such as linear regression, image classification,
    object detection, and many more, or you can install your own custom model to run
    on Vertex AI. Vertex AI provides many tools for each step in the AI/ML project
    life cycle, and we will explore most of them in this book.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training models
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are some established methods to address overfitting and underfitting.
    One cause for overfitting can be that the model did not get access to a sufficient
    amount of different data points from which to learn an appropriate pattern. Looking
    at a very extreme case of this, let’s imagine that we have just one data point
    in our dataset and our model processes this data point over and over again until
    it finds a set of coefficients that it can use to relate the input features accurately
    to the target output for that data point. Now, whenever it sees that same data
    point, it can easily and accurately predict the target variable. However, if we
    show it a new datapoint with a similar structure—i.e., the same number and types
    of features—but different values for those features, it’s unlikely that our model
    will accurately predict the output for the new datapoint because it has only learned
    the specific characteristics of a single data point during training. This is a
    case of overfitting, whereby the model works really well on the specific data
    point on which it was trained but does not make accurate predictions for other
    data points.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: One method that can help to address overfitting in this regard is to provide
    more data points when training our model. If our algorithm has seen thousands
    or millions of data points during training, it’s much more likely to have built
    a more generalized model that has a broader understanding of the feature space
    and the relationship between the features and the target variables. Therefore,
    when it sees a new data point, it may be able to make more accurate predictions
    for the target variable of that new data point.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a trade-off that we need to keep in mind in this context: although
    our training process is likely to build a more generalized model as we provide
    more and more data points, we need to keep in mind that training models on large
    datasets can be expensive. We may find that after the model has seen millions
    of data points, each new training iteration is only increasing the model’s generalization
    metrics by very small amounts. For example, if our model currently has a 99.67%
    accuracy rate and each training iteration increases its accuracy by 0.0001% but
    it costs thousands of dollars to do so, it may not make sense from a financial
    perspective to keep training the model on more and more data points, especially
    if our business considers 99.5% to be accurate enough to meet the business needs.
    This last point is important—the trade-off in training costs versus increases
    in accuracy is dependent on the business requirements. If we’re building a model
    for a medical diagnosis use case or a model whose forecasting accuracy can cost
    our company millions of dollars if it is incorrect by 0.001%, then it may be worth
    it to keep training the model on more data points. In any case, what you will
    generally need to do is define a threshold at which the business considers the
    model’s metrics to be sufficient and measure the increase in that metric as the
    model is trained on more data points. If you see that the metric begins to plateau
    after a certain amount of data points, it may be time to stop the training process.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，我们需要注意一个权衡：虽然我们的训练过程很可能会随着我们提供越来越多的数据点而构建一个更通用的模型，但我们还需要记住，在大数据集上训练模型可能会很昂贵。我们可能会发现，当模型已经看到了数百万个数据点后，每次新的训练迭代只使模型的泛化指标略有增加。例如，如果我们的模型目前的准确率为99.67%，每次训练迭代只将其准确率提高0.0001%，但这样做可能需要花费数千美元，那么从财务角度来看，继续在越来越多的数据点上训练模型可能就没有意义了，尤其是如果我们认为99.5%的准确率已经足够满足业务需求的话。这一点很重要——训练成本与准确率增加之间的权衡取决于业务需求。如果我们正在构建一个用于医疗诊断用例的模型或一个预测准确率错误可能导致公司损失数百万美元的模型，那么继续在更多数据点上训练模型可能是值得的。在任何情况下，你通常需要做的是定义一个阈值，在这个阈值下，业务认为模型的指标已经足够，并测量随着模型在更多数据点上训练而该指标的增量。如果你看到该指标在达到一定数量的数据点后开始趋于平稳，那么可能就是停止训练过程的时候了。
- en: It should be noted that adding more data points is not always an option because
    you may only have a limited dataset to begin with and it may be difficult to gather
    more real-world data for your specific use case. In these scenarios, you may be
    able to generate synthetic data with similar characteristics as your real-world
    data or use mechanisms to optimize the use of your existing dataset during the
    training process, such as maximizing the training dataset by using cross-validation,
    which we’ll explore in a hands-on activity later in this book.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，增加更多数据点并不总是可行的，因为你可能一开始就只有有限的数据集，并且可能很难为你的特定用例收集更多真实世界的数据。在这些情况下，你可能能够生成具有与你的真实世界数据相似特征的人工合成数据，或者使用机制在训练过程中优化现有数据集的使用，例如通过交叉验证来最大化训练数据集，我们将在本书后面的动手活动中探讨这一点。
- en: Another potential cause of overfitting is if the model is too “complex”, by
    which we mean that too many features may be used for each data point in the training
    dataset. Again, taking an extreme example, if each data point has thousands of
    features, the model will learn very specific relationships between the features
    and the training dataset, which may not generalize well to other data points.
    In this case, a solution could be to remove features that are not deemed to be
    critical to figuring out the optimal relationship between the features and the
    target variables. Selecting the relevant features can be a challenge in itself,
    and we will explore mechanisms such as **principal component analysis** (**PCA**)
    to help select the most relevant features.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能导致过度拟合的潜在原因是模型过于“复杂”，这里的“复杂”意味着训练数据集中每个数据点可能使用了过多的特征。再次以极端的例子来说明，如果每个数据点都有数千个特征，模型将学习到特征与训练数据集之间非常具体的关系，这些关系可能无法很好地推广到其他数据点。在这种情况下，一个解决方案可能是删除那些被认为对于确定特征与目标变量之间最佳关系的非关键特征。选择相关特征本身就是一个挑战，我们将探讨如**主成分分析**（**PCA**）等机制来帮助选择最相关的特征。
- en: The opposite of overfitting, then, is underfitting, and one potential cause
    of underfitting is if the model is too simple, in which case there may not be
    a sufficient number of features for each data point in the dataset for the model
    to determine a meaningful relationship between the features and the target variables.
    Of course, in this case, we would want to find or generate additional features
    than can help our model to learn more meaningful relationships between those features
    and the target variables.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: To address the challenge of keeping track of experiments and their results in
    large-scale ML projects, we will use Vertex ML Metadata, which tracks all of our
    experiments and their inputs and outputs for us (i.e., the lineage of our data
    and ML model artifacts).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and tuning hyperparameters
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are some methodical ways in which to explore what we call the “hyperparameter
    space”, which means all of the different possible values for our hyperparameters.
    The following are some popular methods:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '**Random search**: The random search approach uses a subsampling technique
    in which hyperparameter values are selected at random for each training job experiment.
    This will not result in all possible values of every hyperparameter being tested,
    but it can often be quite an efficient method for finding an effective set of
    hyperparameter values.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grid search**: The grid search approach to hyperparameter tuning is the most
    exhaustive because it will try out every possible combination of values of each
    hyperparameter. This means that it will generally take a lot more time than a
    random search approach. Also, bear in mind that each training job costs money,
    so if you have a large hyperparameter space, this can be very expensive or even
    infeasible.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian optimization**: Earlier in this chapter, we talked about using gradient
    descent to optimize a function by finding its minimum point. Bayesian optimization
    is another type of optimization technique. It’s quite a complex process, and it
    is usually more efficient than the other approaches mentioned previously. Fortunately,
    Google Cloud’s Vertex AI Vizier service will perform Bayesian optimization for
    you, so if you use that tool, you won’t need to implement it yourself.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are interested in diving into the inner workings of Bayesian optimization,
    I recommend referring to the following paper: [https://arxiv.org/abs/1807.02811](https://arxiv.org/abs/1807.02811)'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Google Cloud’s Vertex AI Vizier service will run lots of training job experiments
    for you, trying out many different combinations of hyperparameter values for each
    experiment, and will find the optimal hyperparameter values to run your ML training
    jobs efficiently.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Key tip
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: To save yourself a lot of painstaking work and time when doing hyperparameter
    optimization, use a cloud service that has been built for that purpose, such as
    Google Cloud’s Vertex AI Vizier service.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Deploying models
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Latency is often a key factor in the deployment of our models, in that we need
    to ensure our model hosting infrastructure meets the latency requirements expected
    by our client applications.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟通常是我们的模型部署中的一个关键因素，我们需要确保我们的模型托管基础设施满足客户端应用程序预期的延迟要求。
- en: One decision point that comes into view in this context is whether we have a
    batch or online use case. In an online use case, a client sends a piece of input
    data to our model and it waits to receive an inference response. This usually
    happens when our client application needs an answer quickly, such as when a customer
    is performing a transaction on our website and we want to see if the transaction
    seems fraudulent. This is a real-time use case, and therefore the latency generally
    needs to be very low; perhaps a few milliseconds. You will usually need to work
    with business leaders to define the acceptable latency.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，一个需要考虑的决定点是，我们是否有批量或在线使用场景。在在线使用场景中，客户端将一块输入数据发送到我们的模型，并等待接收推理响应。这通常发生在我们的客户端应用程序需要快速得到答案时，例如当客户在我们的网站上执行交易时，我们想查看交易是否看起来是欺诈性的。这是一个实时使用场景，因此延迟通常需要非常低；可能只有几毫秒。你通常需要与业务领导者合作，以定义可接受的延迟。
- en: In a batch use case, our model can process large amounts of data at a time.
    For example, as input to our model at inference time, we may provide a large file
    containing thousands or millions of data points for which we want our model to
    make predictions, and our model could work for hours on processing those inputs
    and save all of the inference results as outputs in another file, which we could
    reference later.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在批量使用场景中，我们的模型可以一次处理大量数据。例如，在推理时间作为我们模型的输入，我们可能提供一个包含数千或数百万数据点的文件，我们希望我们的模型对这些数据进行预测，我们的模型可能需要数小时来处理这些输入，并将所有推理结果保存为另一个文件中的输出，我们可以在以后参考。
- en: Tip
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Batch use cases are usually associated with scenarios in which we do not require
    low latency. However, ironically, there is also a scenario in which batch use
    cases can actually help provide lower latency at inference time. Consider a scenario
    in which we’re running a retail website and we want to get insights from our users’
    purchasing histories in order to recommend products that customers may be interested
    in buying when they visit our website. Depending on the amount of historical data
    we have, it could take a long time to process that data. Therefore, we don’t want
    to do this in real time when a customer visits our website. Instead, we could
    periodically run a batch inference job every night and store our results in a
    file or a key–value database. Then, when customers visit our site, we can fetch
    the pre-computed inferences from our file or database. In this scenario, fetching
    a value from a file or database will usually be much quicker than performing an
    online inference in real time. Note that this would only be suitable for certain
    use cases. For example, it would not work for a transactional fraud evaluation
    use case because we would need real-time characteristics from the ongoing transaction
    for that scenario. Consequently, as a data scientist or AI/ML solutions architect,
    you will need to determine what kind of inferencing approach works best for each
    of your use cases.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 批量使用场景通常与不需要低延迟的场景相关联。然而，具有讽刺意味的是，也存在一种场景，批量使用场景实际上可以帮助在推理时间提供更低的延迟。考虑这样一个场景，我们正在运行一个零售网站，我们想从用户的购买历史中获取洞察，以便向客户推荐他们可能在访问我们的网站时感兴趣购买的产品。根据我们拥有的历史数据量，处理这些数据可能需要很长时间。因此，我们不想在客户访问我们的网站时实时进行这项操作。相反，我们可以在每晚定期运行一个批量推理作业，并将我们的结果存储在文件或键值数据库中。然后，当客户访问我们的网站时，我们可以从我们的文件或数据库中检索预先计算的推理结果。在这种情况下，从文件或数据库中检索一个值通常会比实时在线推理快得多。请注意，这仅适用于某些用例。例如，它不适用于交易性欺诈评估用例，因为我们需要从该场景的持续交易中获得实时特征。因此，作为数据科学家或AI/ML解决方案架构师，您需要确定哪种推理方法最适合您的每个用例。
- en: Monitoring models after deployment
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署后监控模型
- en: If you detect that drift is occurring in production, this is a sign that you
    may need to update your models. We recommend putting mechanisms in place to automate
    the retraining of your models with updated data if you detect drift, especially
    if you’re managing large numbers of models. I’ve worked with organizations that
    are running hundreds of models simultaneously in production, and it’s not feasible
    to manually monitor, curate, and retrain all of those models on an ongoing basis.
    In that scenario, we’ve implemented MLOps frameworks that would retrain the models
    on updated data whenever a model’s metrics consistently dropped below a preconfigured
    threshold that we considered to be acceptable. The MLOps framework would then
    test the new model, and if the new model’s metrics outperformed the current model’s
    metrics in production, it would replace the production model with the new model.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检测到生产中出现了漂移，这表明你可能需要更新你的模型。我们建议在检测到漂移时，建立机制来自动化使用更新数据重新训练你的模型，尤其是如果你正在管理大量模型时。我曾与那些在生产中同时运行数百个模型的组织合作过，手动监控、整理和持续重新训练所有这些模型是不可行的。在这种情况下，我们实施了MLOps框架，该框架会在模型的指标持续低于我们认为是可接受的预配置阈值时，在更新数据上重新训练模型。然后，MLOps框架会对新模型进行测试，如果新模型的指标在生产中优于当前模型的指标，它就会用新模型替换生产模型。
- en: When it comes to defining and monitoring the model’s operational metrics in
    production, you can use Google Cloud Monitoring for that purpose.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到定义和监控生产中模型的操作指标时，你可以使用Google Cloud Monitoring来完成这个目的。
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored a quick recap of the traditional SDLC and introduced
    the concept of the ML model development life cycle. We discussed each of the steps
    that we usually encounter in most AI/ML projects, and then we dived into specific
    challenges that generally exist in each step. Finally, we covered approaches and
    best practices that companies have learned over time to help them address some
    of those common challenges.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们快速回顾了传统的SDLC（软件开发生命周期），并介绍了ML模型开发生命周期的概念。我们讨论了在大多数AI/ML项目中通常遇到的每个步骤，然后深入探讨了每个步骤中普遍存在的具体挑战。最后，我们介绍了公司随着时间的推移学到的方法和最佳实践，以帮助他们解决一些常见的挑战。
- en: In the next chapter, we’ll begin to explore the various different services in
    Google Cloud that can be used to implement AI/ML workloads.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始探索Google Cloud中可以用来实现AI/ML工作负载的各种不同服务。
