- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adversarial Robustness
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning interpretation has many concerns, ranging from knowledge discovery
    to high-stakes ones with tangible ethical implications, like the fairness issues
    examined in the last two chapters. In this chapter, we will direct our attention
    to concerns involving reliability, safety, and security.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: As we realized using the **contrastive explanation method** in *Chapter 7*,
    *Visualizing Convolutional Neural Networks*, we can easily trick an image classifier
    into making embarrassingly false predictions. This ability can have serious ramifications.
    For instance, a perpetrator can place a black sticker on a yield sign, and while
    most drivers would still recognize this as a yield sign, a self-driving car may
    no longer recognize it and, as a result, crash. A bank robber could wear a cooling
    suit designed to trick the bank vault’s thermal imaging system, and while any
    human would notice it, the imaging system would fail to do so.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The risk is not limited to sophisticated image classifiers. Other models can
    be tricked! The **counterfactual examples** produced in *Chapter 6**, Anchors
    and Counterfactual Explanations,* are like adversarial examples except with the
    goal of deception. An attacker could leverage any misclassification example, straddling
    the decision boundary adversarially. For instance, a spammer could realize that
    adjusting some email attributes increases the likelihood of circumventing spam
    filters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'Complex models are more vulnerable to adversarial attacks. So why would we
    trust them?! We can certainly make them more foolproof, and that’s what adversarial
    robustness entails. An adversary can purposely thwart a model in many ways, but
    we will focus on evasion attacks and briefly explain other forms of attacks. Then
    we will explain two defense methods: spatial smoothing preprocessing and adversarial
    training. Lastly, we will demonstrate one robustness evaluation method.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the main topics we will cover:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Learning about evasion attacks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defending against targeted attacks with preprocessing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shielding against any evasion attack through adversarial training of a robust
    classifier
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter’s example uses the `mldatasets`, `numpy`, `sklearn`, `tensorflow`,
    `keras`, `adversarial-robustness-toolbox`, `matplotlib`, and `seaborn` libraries.
    Instructions on how to install all of these libraries are in the *Preface*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter is located here: [https://packt.link/1MNrL](https://packt.link/1MNrL)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: The mission
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The privately contracted security services industry market worldwide is valued
    at over USD 250 billion and is growing at around 5% annually. However, it faces
    many challenges, such as shortages of adequately trained guards and specialized
    security experts in many jurisdictions, and a whole host of unexpected security
    threats. These threats include widespread coordinated cybersecurity attacks, massive
    riots, social upheaval, and, last but not least, health risks brought on by pandemics.
    Indeed, 2020 tested the industry with a wave of ransomware, misinformation attacks,
    protests, and COVID-19 to boot.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 全球私人签约保安服务行业市场规模超过2500亿美元，年增长率为约5%。然而，它面临着许多挑战，例如在许多司法管辖区缺乏足够培训的保安和专业的安全专家，以及一系列意外的安全威胁。这些威胁包括广泛的协调一致的网络安全攻击、大规模暴乱、社会动荡，以及最后但同样重要的是，由大流行带来的健康风险。确实，2020年通过勒索软件、虚假信息攻击、抗议活动和COVID-19等一系列事件考验了该行业。
- en: In the wake of this, one of the largest hospital networks in the United States
    asked their contracted security company to monitor the correct use of masks by
    both visitors and personnel throughout the hospitals. The security company has
    struggled with this request because it diverts security personnel from tackling
    other threats, such as intruders, combative patients, and belligerent visitors.
    It has video surveillance in every hallway, operating room, waiting room, and
    hospital entrance. It’s impossible to have eyes on every camera feed every time,
    so the security company thought they could assist guards with deep learning models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，美国最大的医院网络之一要求他们的签约保安公司监控医院内访客和员工佩戴口罩的正确性。保安公司因为这项请求而感到困扰，因为它分散了保安人员应对其他威胁（如入侵者、斗殴患者和挑衅访客）的精力。该公司在每个走廊、手术室、候诊室和医院入口都有视频监控。每次都不可能监控到每个摄像头的画面，因此保安公司认为他们可以用深度学习模型来协助保安：
- en: 'These models already alert unusual activities, such as running in the hallways
    and brandishing weapons anywhere on the premises. They have proposed to the hospital
    network that they would like to add a new model that detects masks’ correct usage.
    Before COVID-19, there were policies in place for mandatory mask usage in certain
    areas of each hospital, and during COVID-19, it was required everywhere. The hospital
    administrators would like to turn on and off this monitoring feature, depending
    on pandemic risk levels moving forward. They realize that personnel get fatigued
    and forget to put masks back on, or they partially slip off at times. Many visitors
    are also hostile toward using masks and may wear one when entering the hospital
    but take it off when no guard is around. This isn’t always intentional, so they
    wouldn’t want to dispatch guards on every alert, unlike other threats. Instead,
    they’d rather use awareness and a little bit of shame to modify behavior and only
    intervene with repeat offenders:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型已经能够检测到异常活动，例如在走廊里奔跑和在物业任何地方挥舞武器。他们向医院网络提出建议，希望添加一个新模型来检测口罩的正确使用。在COVID-19之前，医院各区域已经实施了强制佩戴口罩的政策，而在COVID-19期间，则要求在所有地方佩戴口罩。医院管理员希望根据未来的大流行风险水平来开启和关闭这一监控功能。他们意识到，人员会感到疲惫并忘记戴上口罩，或者有时口罩会部分滑落。许多访客也对佩戴口罩持敌对态度，他们可能会在进入医院时戴上口罩，但如果没有保安在场，就会摘下。这并不总是故意的，因此他们不希望像对其他威胁一样，在每次警报时都派遣保安：
- en: '![A yellow sign on a pole  Description automatically generated with low confidence](img/B18406_13_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![杆上的黄色标志  描述由低置信度自动生成](img/B18406_13_01.png)'
- en: 'Figure 13.1: Radar speed signs like this one help curb speeding'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1：像这样的雷达速度标志有助于遏制超速
- en: Awareness is a very effective method with radar speed signs (see *Figure 13.1*),
    which make roads safer by only making drivers aware that they are driving too
    fast. Likewise, having a screen at the end of heavily trafficked hallways showing
    snapshots of those who have recently either mistakenly or purposely not complied
    with mandatory mask usage potentially creates some embarrassment for offenders.
    The system will log repeat offenders so that security guards can look for them
    and either make them comply or ask them to vacate the premises.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 意识是雷达速度标志（见*图13.1*）的一种非常有效的方法，它通过仅让驾驶员意识到他们开得太快，从而使道路更安全。同样，在繁忙走廊的尽头设置屏幕，显示最近错误或故意未遵守强制佩戴口罩规定的人的快照，可能会让违规者感到尴尬。系统将记录反复违规者，以便保安可以找到他们，要么让他们遵守规定，要么要求他们离开现场。
- en: There’s some concern with visitors trying to trick the model into evading compliance,
    so the security company has hired you to ensure that the model is robust in the
    face of this kind of adversarial attack. Security officers have noticed some low-tech
    trickery before, such as people momentarily covering their faces with their hands
    or a part of their sweater when they realize cameras monitor them. Also, in one
    disturbing incident, a visitor dimmed the lights and sprayed some gel on a camera,
    and in another, an individual painted their mouth. However, there are concerns
    about higher-tech attacks, such as jamming the camera’s wireless signal or shining
    high-powered lasers directly into cameras. Devices that perform these attacks
    are increasingly easier to obtain and could impact other surveillance functions
    on a larger scale, like preventing theft. The security company hopes this robustness
    exercise can inform their efforts to improve every surveillance system and model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于试图欺骗模型规避合规性的访客，存在一些担忧，因此安全公司雇佣你来确保模型在面对这种对抗性攻击时具有鲁棒性。安全官员在之前注意到一些低技术含量的诡计，例如人们在意识到摄像头正在监控他们时，会暂时用手或毛衣的一部分遮住他们的脸。在一个令人不安的事件中，访客降低了灯光，并在摄像头上喷了一些凝胶，在另一个事件中，有人涂鸦了他们的嘴巴。然而，人们对更高技术攻击的担忧，例如干扰摄像头的无线信号或直接向摄像头照射高功率激光。执行这些攻击的设备越来越容易获得，可能会对更大规模的监控功能，如防止盗窃，产生影响。安全公司希望这种鲁棒性练习能够为改善每个监控系统和模型的努力提供信息。
- en: Eventually, the security company would like to produce its own dataset with
    face images from the hospitals they monitor. Meanwhile, synthetically masked faces
    from external sources are the best they can do to productionize a model in the
    short term. To this end, you have been provided a large dataset of synthetically
    correctly and incorrectly masked faces and their unmasked counterparts. The two
    datasets were combined into a single one, and the original dimensions of 1,024
    × 1,024 were reduced to the thumbnail size of 124 × 124\. Also, for efficiency’s
    sake, 21,000 images were sampled from roughly 210,000 in these datasets.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，安全公司希望使用他们监控的医院中的面部图像来生成自己的数据集。同时，从外部来源合成的面具面部图像是他们短期内将模型投入生产的最佳选择。为此，你被提供了一组合成的正确和错误面具面部图像及其未面具对应图像的大型数据集。这两个数据集被合并成一个，原始的
    1,024 × 1,024 尺寸被减少到缩略图的 124 × 124 尺寸。此外，为了提高效率，从这些数据集中采样了大约 21,000 张图像。
- en: The approach
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: 'You’ve decided to take a four-fold approach:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经决定采取四步方法：
- en: Exploring several possible evasion attacks to understand how vulnerable the
    model is to them and how credible they are as threats
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索几种可能的规避攻击，以了解模型对这些攻击的脆弱性以及它们作为威胁的可靠性
- en: Using a preprocessing method to protect a model against these attacks
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预处理方法来保护模型免受这些攻击
- en: Leveraging adversarial retraining to produce a robust classifier that is intrinsically
    less prone to many of these attacks
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用对抗性再训练来生成一个本质上对许多此类攻击更不易受影响的鲁棒分类器
- en: Evaluating robustness with state-of-the-art methods to assure hospital administrators
    that the model is adversarially robust
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最先进的方法评估鲁棒性，以确保医院管理员相信该模型具有对抗性鲁棒性
- en: Let’s get started!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: The preparations
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You will find the code for this example here: [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/13/Masks.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/13/Masks.ipynb)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下位置找到这个示例的代码：[https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/13/Masks.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/13/Masks.ipynb)
- en: Loading the libraries
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载库
- en: 'To run this example, you need to install the following libraries:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，你需要安装以下库：
- en: '`mldatasets` to load the dataset'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mldatasets` 用于加载数据集'
- en: '`numpy` and `sklearn` (scikit-learn) to manipulate it'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy` 和 `sklearn` (scikit-learn) 用于操作它'
- en: '`tensorflow` to fit the models'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow` 用于拟合模型'
- en: '`matplotlib` and `seaborn` to visualize the interpretations'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib` 和 `seaborn` 用于可视化解释'
- en: 'You should load all of them first:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该首先加载所有这些：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s check that TensorFlow has loaded the right version with `print(tf.__version__)`.
    The version should be 2.0 and above.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 `print(tf.__version__)` 检查 TensorFlow 是否加载了正确的版本。版本应该是 2.0 及以上。
- en: 'We should also disable eager execution and verify that it worked with the following
    command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该禁用即时执行，并验证它是否已通过以下命令完成：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output should say that it’s `False`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该显示为 `False`。
- en: In TensorFlow, turning eager execution mode on means that it doesn’t require
    a computational graph or a session. It’s the default for TensorFlow 2.x and later
    but not in prior versions, so you need to disable it to avoid incompatibilities
    with code that was optimized for prior versions of TensorFlow.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，开启急切执行模式意味着它不需要计算图或会话。这是TensorFlow 2.x及以后版本的默认设置，但在之前的版本中不是，所以你需要禁用它以避免与为TensorFlow早期版本优化的代码不兼容。
- en: Understanding and preparing the data
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和准备数据
- en: 'We will load the data into four NumPy arrays, corresponding to the training/test
    datasets. While we are at it, we will divide `X` face images by 255 because, that
    way, they will be of values between zero and one, which is better for deep learning
    models. We call this feature scaling. We will need to record the `min_` and `max_`
    for the training data because we will need these later:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据加载到四个NumPy数组中，对应于训练/测试数据集。在此过程中，我们将`X`面部图像除以255，因为这样它们的值将在零和一之间，这对深度学习模型更好。我们称这种特征缩放。我们需要记录训练数据的`min_`和`max_`，因为我们稍后会需要这些信息：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It’s always important to verify our data when we load it to make sure it didn’t
    get corrupted:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们加载数据时，始终验证数据非常重要，以确保数据没有被损坏：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Therefore, a preprocessing step we will need to perform is to **One-Hot Encode**
    (**OHE**) the `y` labels because we will need the OHE form to evaluate the model’s
    predictive performance. Once we initialize the `OneHotEncoder`, we will need to
    `fit` it into the training data (`y_train`). We can also extract the categories
    from the encoder into a list (`labels_l`) to verify that it has all three:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要执行的一个预处理步骤是将`y`标签**独热编码**（**OHE**），因为我们需要OHE形式来评估模型的预测性能。一旦我们初始化`OneHotEncoder`，我们就需要将其`fit`到训练数据（`y_train`）中。我们还可以将编码器中的类别提取到一个列表（`labels_l`）中，以验证它包含所有三个类别：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For reproducibility’s sake, always initialize your random seeds like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保可复现性，始终以这种方式初始化你的随机种子：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Making machine learning truly reproducible means also making it deterministic,
    which means that training with the same data will produce a model with the same
    parameters. Determinism is very difficult with deep learning and is often session-,
    platform-, and architecture-dependent. If you use an NVIDIA GPU, you can install
    a library called `framework-reproducibility`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使机器学习真正可复现意味着也要使其确定性，这意味着使用相同的数据进行训练将产生具有相同参数的模型。在深度学习中实现确定性非常困难，并且通常依赖于会话、平台和架构。如果你使用NVIDIA
    GPU，你可以安装一个名为`framework-reproducibility`的库。
- en: 'Many of the adversarial attack, defense, and evaluation methods we will study
    in this chapter are very resource-intensive, so if we used the entire test dataset
    with them, they could likely take many hours on a single method! For efficiency,
    it is strongly suggested to use samples of the test dataset. Therefore, we will
    create a medium 200-image sample (`X_test_mdsample`, `y_test_mdsample`) and a
    small 20-image sample (`X_test_smsample`, `y_test_smsample`) using `np.random.choice`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将要学习的许多对抗攻击、防御和评估方法都非常资源密集，所以如果我们用整个测试数据集来使用它们，它们可能需要数小时才能完成单个方法！为了提高效率，强烈建议使用测试数据集的样本。因此，我们将使用`np.random.choice`创建一个中等大小的200张图像样本（`X_test_mdsample`,
    `y_test_mdsample`）和一个小型20张图像样本（`X_test_smsample`, `y_test_smsample`）：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We have two sample sizes because some methods could take too long with a larger
    sample size. Now, let’s take a peek at what images are in our datasets. In the
    preceding code, we have taken a medium and small sample of our test dataset. We
    will place each image of our small sample in a 4 × 5 grid with the class label
    above it, with the following code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个样本大小，因为某些方法在较大的样本大小下可能需要太长时间。现在，让我们看看我们的数据集中有哪些图像。在先前的代码中，我们已经从我们的测试数据集中取了一个中等和一个小样本。我们将使用以下代码将我们小样本中的每张图像放置在一个4
    × 5的网格中，类别标签位于其上方：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code plots the grid of images in *Figure 13.2*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码在*图13.2*中绘制了图像网格：
- en: '![A collage of a person  Description automatically generated with medium confidence](img/B18406_13_02.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![一个人物的拼贴画  描述由中等置信度自动生成](img/B18406_13_02.png)'
- en: 'Figure 13.2: A small test dataset sample of masked and unmasked faces'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2：一个带有遮挡和未遮挡面部的小型测试数据集样本
- en: '*Figure 13.2* depicts a variety of correctly and incorrectly masked and unmasked
    faces of all ages, genders, and ethnicities. Despite the variety, one thing to
    note about this dataset is that it only has light blue surgical masks represented,
    and images are mostly at a front-facing angle. Ideally, we would generate an even
    larger dataset with all colors and types of masks and augment it further with
    random rotations, shears, and brightness adjustments, either before or during
    training. These augmentations would make for a much more robust model. Nevertheless,
    we must differentiate between this general type of robustness and adversarial
    robustness.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.2* 展示了各种年龄、性别和种族的正面和反面、带口罩和不带口罩的面部图像。尽管种类繁多，但关于这个数据集的一个需要注意的事项是，它只展示了浅蓝色的外科口罩，且图像大多是正面角度。理想情况下，我们会生成一个包含所有颜色和类型口罩的更大数据集，并在训练前或训练期间对其进行随机旋转、剪切和亮度调整，以进一步增强模型的鲁棒性。这些增强将使模型更加鲁棒。尽管如此，我们必须区分这种一般类型的鲁棒性和对抗鲁棒性。'
- en: Loading the CNN base model
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载 CNN 基础模型
- en: 'You don’t have to train the CNN base model, but the code to do so is provided
    nonetheless in the GitHub repository. The pretrained model has also been stored
    there. We can quickly load the model and output its summary like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您不必训练 CNN 基础模型，但相关的代码已提供在 GitHub 仓库中。预训练模型也已存储在那里。我们可以快速加载模型并输出其摘要，如下所示：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding snippet outputs the following summary:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段输出了以下摘要：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The summary has pretty much everything we need to know about the model. It has
    four convolutional layers (`Conv2D`), each followed by a max pool layer (`MaxPooling2D`).
    It then has a `Flatten` layer and a fully connected layer (`Dense`). Then, there’s
    more `Dropout` before the second `Dense` layer. Naturally, three neurons are in
    this final layer, corresponding to each class.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要几乎包含了我们需要了解的所有关于模型的信息。它有四个卷积层 (`Conv2D`)，每个卷积层后面都跟着一个最大池化层 (`MaxPooling2D`)。然后是一个
    `Flatten` 层和一个全连接层 (`Dense`)。接着，在第二个 `Dense` 层之前还有更多的 `Dropout`。自然地，这个最终层有三个神经元，对应于每个类别。
- en: Assessing the CNN base classifier
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估 CNN 基础分类器
- en: 'We can evaluate the model using the test dataset with the `evaluate_multiclass_mdl`
    function. The arguments include the model (`base_model`), our test data (`X_test`),
    and the corresponding labels (`y_test`), as well as the class names (`labels_l`)
    and the encoder (`ohe`). Lastly, we don’t need to plot the ROC curves since, given
    the high accuracy, they won’t be very informative (`plot_roc=False`). This function
    returns the predicted labels and probabilities, which we can store as variables
    for later use:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `evaluate_multiclass_mdl` 函数和测试数据集来评估模型。参数包括模型 (`base_model`)、我们的测试数据
    (`X_test`) 和相应的标签 (`y_test`)，以及类名 (`labels_l`) 和编码器 (`ohe`)。最后，由于准确率很高，我们不需要绘制
    ROC 曲线（`plot_roc=False`）。此函数返回预测标签和概率，我们可以将这些变量存储起来以供以后使用：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code generates *Figure 13.3*, with a confusion matrix and performance
    metrics for each class:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了 *图 13.3*，其中包含每个类别的混淆矩阵和性能指标：
- en: '![](img/B18406_13_03.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_13_03.png)'
- en: 'Figure 13.3: The confusion matrix and predictive performance metrics for the
    base classifier, evaluated on the test dataset'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3：在测试数据集上评估的基础分类器的混淆矩阵和预测性能指标
- en: Even though the confusion matrix in *Figure 13.3* seems to suggest a perfect
    classification, pay attention to the circled areas. We can tell the model had
    some issues with misclassifying incorrectly masked faces once we see the recall
    (99.5%) breakdown.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图 13.3 中的混淆矩阵似乎表明分类完美，但请注意圈出的区域。一旦我们看到召回率（99.5%）的分解，我们就可以知道模型在错误地分类带口罩的面部图像时存在一些问题。
- en: Now, we can start attacking this model to assess how robust it actually is!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始攻击这个模型，以评估它的实际鲁棒性！
- en: Learning about evasion attacks
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解逃避攻击
- en: 'There are six broad categories of adversarial attacks:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有六种广泛的对抗攻击类别：
- en: '**Evasion**: designing an input that can cause a model to make an incorrect
    prediction, especially when it wouldn’t fool a human observer. It can either be
    targeted or untargeted, depending on the attacker’s intention to fool the model
    into misclassifying a specific class (targeted) or, rather, misclassifying any
    class (untargeted). The attack methods can be white-box if the attacker has full
    access to the model and its training dataset, or black-box with only inference
    access. Gray-box sits in the middle. Black-box is always model-agnostic, whereas
    white and gray-box methods might be.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规避攻击**：设计一个输入，使其能够导致模型做出错误的预测，尤其是当它不会欺骗人类观察者时。它可以是定向的或非定向的，这取决于攻击者意图欺骗模型将特定类别（定向）或任何类别（非定向）误分类。攻击方法可以是白盒攻击，如果攻击者可以完全访问模型及其训练数据集，或者黑盒攻击，只有推理访问。灰盒攻击位于中间。黑盒攻击总是模型无关的，而白盒和灰盒方法可能不是。'
- en: '**Poisoning**: injecting faulty training data or parameters into a model can
    come in many forms, depending on the attacker’s capabilities and access. For instance,
    for systems with user-generated data, the attacker may be capable of adding faulty
    data or labels. If they have more access, perhaps they can modify large amounts
    of data. They can also adjust the learning algorithm, hyperparameters, or data
    augmentation schemes. Like evasion, poisoning can also be targeted and untargeted.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投毒攻击**：将错误的训练数据或参数注入模型，其形式取决于攻击者的能力和访问权限。例如，对于用户生成数据的系统，攻击者可能能够添加错误的数据或标签。如果他们有更多的访问权限，也许他们可以修改大量数据。他们还可以调整学习算法、超参数或数据增强方案。像规避攻击一样，投毒攻击也可以是定向的或非定向的。'
- en: '**Inference**: extracting the training dataset through model inference. Inference
    attacks also come in many forms and can be used for espionage (privacy attacks)
    through membership inference, which confirms if one example (for instance, a specific
    person) was in the training dataset. Attribute inference ascertains if an example
    category (for instance, an ethnicity) was represented in the training data. Input
    inference (also known as model inversion) has attack methods to extract a training
    dataset from a model rather than guessing and confirming. These have broad privacy
    and regulatory implications, especially in medical and legal applications.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理攻击**：通过模型推理提取训练数据集。推理攻击也以多种形式出现，可以通过成员推理进行间谍活动（隐私攻击），以确认一个示例（例如，一个特定的人）是否在训练数据集中。属性推理确定一个示例类别（例如，种族）是否在训练数据中表示。输入推理（也称为模型反演）有攻击方法可以从模型中提取训练数据集，而不是猜测和确认。这些具有广泛的隐私和监管影响，尤其是在医疗和法律应用中。'
- en: '**Trojaning**: this implants malicious functionality activated with a trigger
    during inference but requires retraining the model.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特洛伊木马攻击**：这会在推理期间通过触发器激活恶意功能，但需要重新训练模型。'
- en: '**Backdooring**: similar to trojans but a backdoor remains, even when a model
    is retrained from scratch.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后门攻击**：类似于特洛伊木马，但即使模型从头开始重新训练，后门仍然存在。'
- en: '**Reprogramming**: remote sabotaging of a model during training by sneaking
    in examples that are specifically designed to produce specific outputs. For instance,
    if you provide enough examples labeled as tiger shark where four small black squares
    are always in the same place, the model will learn that that is a tiger shark,
    regardless of what it is, thus intentionally forcing the model to overfit.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重编程**：在训练过程中通过悄悄引入专门设计以产生特定输出的示例来远程破坏模型。例如，如果你提供了足够多的标记为虎鲨的示例，其中四个小黑方块总是出现在相同的位置，模型就会学习到那是一个虎鲨，无论它是什么，从而故意迫使模型过度拟合。'
- en: 'The first three are the most studied forms of adversarial attacks. Attacks
    can be further subcategorized once we split them by stage and goal (see *Figure
    13.4*). The stage is when the attack is perpetrated because it can impact the
    model training or its inference, and the goal is what the attacker hopes to gain
    from it. This chapter will only deal with evasion sabotage attacks because we
    expect hospital visitors, patients, and personnel to occasionally sabotage the
    production model:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 前三种是最受研究的对抗攻击形式。一旦我们根据阶段和目标将它们分开，攻击可以进一步细分（见*图13.4*）。阶段是指攻击实施时，因为它可以影响模型训练或其推理，而目标是攻击者希望从中获得什么。本章将仅处理规避破坏攻击，因为我们预计医院访客、患者和工作人员偶尔会破坏生产模型：
- en: '![](img/B18406_13_04.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_13_04.png)'
- en: 'Figure 13.4: Table of adversarial attack category methods by stage and goal'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4：按阶段和目标分类的对抗攻击方法表
- en: Even though we use white-box methods to attack, defend, and evaluate a model’s
    robustness, we don’t expect attackers to have this level of access. We will only
    use white-box methods because we have full access to the model. In other circumstances,
    such as a bank surveillance system with a thermal imaging system and a corresponding
    model to detect perpetrators, we could expect professional attackers to use black-box
    methods to find vulnerabilities! So, as defenders of this system, we would be
    wise to try the very same attack methods.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用白盒方法来攻击、防御和评估模型的鲁棒性，但我们并不期望攻击者拥有这种级别的访问权限。我们只会使用白盒方法，因为我们完全访问了模型。在其他情况下，例如带有热成像系统和相应模型以检测犯罪者的银行监控系统，我们可能会预期专业攻击者使用黑盒方法来寻找漏洞！因此，作为该系统的防御者，我们明智的做法是尝试相同的攻击方法。
- en: 'The library we will use for adversarial robustness is called the **Adversarial
    Robustness Toolbox** (**ART**), and it’s supported by the **LF AI & Data Foundation**
    – the same folks that support other open-source projects such as AIX360 and AIF360,
    explored in *Chapter 11**, Bias Mitigation and Causal Inference Methods*. ART
    requires that attacked models are abstracted in an estimator or classifier, even
    if it’s a black-box one. We will use `KerasClassifier` for most of this chapter
    except for the last section, in which we will use `TensorFlowV2Classifier`. Initializing
    an ART classifier is simple. You must specify the `model`, and sometimes there
    are other required attributes. For `KerasClassifier`, all remaining attributes
    are optional, but it is recommended you use `clip_values` to specify the range
    of the features. Many attacks are input permutations, so knowing what input values
    are allowed or feasible is essential:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于对抗鲁棒性的库称为**对抗鲁棒性工具箱**（**ART**），它由**LF AI & 数据基金会**支持——这些人还支持其他开源项目，如 AIX360
    和 AIF360，这些项目在**第11章**中进行了探讨，即偏差缓解和因果推断方法。ART 要求攻击模型被抽象为估计器或分类器，即使它是黑盒的。在本章的大部分内容中，我们将使用
    `KerasClassifier`，但在最后一节中，我们将使用 `TensorFlowV2Classifier`。初始化ART分类器很简单。你必须指定 `model`，有时还有其他必需的属性。对于
    `KerasClassifier`，所有剩余的属性都是可选的，但建议你使用 `clip_values` 来指定特征的取值范围。许多攻击是输入排列，因此了解允许或可行的输入值是什么至关重要：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code, we also prepare two arrays with probabilities for the
    predicted class of the medium and small samples. It is entirely optional, but
    these assist in placing the predicted probability next to the predicted label
    when plotting some examples.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们还准备了两个数组，用于预测中等和较小样本的类别概率。这完全是可选的，但这些有助于在绘制一些示例时将预测概率放置在预测标签旁边。
- en: Fast gradient sign method attack
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速梯度符号法攻击
- en: One of the most popular attack methods is the **Fast Gradient Sign Method**
    (**FSGM** or **FGM**). As the name implies, it leverages a deep learning model’s
    gradient to find adversarial examples. It performs small perturbations on the
    pixels of the input image, either additions or subtractions. Which one to use
    depends on the gradient’s sign, which indicates what direction would increase
    or decrease the loss according to the pixel’s intensity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的攻击方法之一是**快速梯度符号法**（**FSGM** 或 **FGM**）。正如其名所示，它利用深度学习模型的梯度来寻找对抗性示例。它对输入图像的像素进行小的扰动，无论是加法还是减法。使用哪种方法取决于梯度的符号，这表明根据像素的强度，哪个方向会增加或减少损失。
- en: 'As with all ART attack methods, you first initialize it by providing the ART
    estimator or classifier. `FastGradientMethod` also requires an attack step size
    `eps`, which will condition the attack strength. Incidentally, `eps` stands for
    epsilon (![](img/B18406_13_001.png)), which represents error margins or infinitesimal
    approximation errors. A small step size will cause pixel intensity changes to
    be less visible, but it will also misclassify fewer examples. A larger step size
    will cause more examples to be misclassified with more visible changes:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有ART攻击方法一样，你首先通过提供ART估计器或分类器来初始化它。`FastGradientMethod` 还需要一个攻击步长 `eps`，这将决定攻击强度。顺便提一下，`eps`
    代表 epsilon (![](img/B18406_13_001.png))，它代表误差范围或无穷小近似误差。小的步长会导致像素强度变化不太明显，但它也会错误分类较少的示例。较大的步长会导致更多示例被错误分类，并且变化更明显：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After initializing, the next step is to `generate` the adversarial examples.
    The only required attribute is original examples (`X_test_mdsample`). Please note
    that FSGM can be targeted, so there’s an optional `targeted` attribute in the
    initialization, but you would also need to provide corresponding labels in the
    generation. This attack is untargeted because the attacker’s intent is to sabotage
    the model:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化后，下一步是`generate`对抗示例。唯一必需的属性是原始示例（`X_test_mdsample`）。请注意，FSGM可以是针对特定目标的，因此在初始化中有一个可选的`targeted`属性，但你还需要在生成时提供相应的标签。这种攻击是非针对特定目标的，因为攻击者的意图是破坏模型：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Generating the adversarial examples with FSGM is quick, unlike other methods,
    hence the “Fast”!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他方法相比，使用FSGM生成对抗示例非常快，因此称之为“快速”！
- en: 'Now, we will do two things in one swoop. First, evaluate the adversarial examples
    (`X_test_fgsm`) against our base classifier’s model (`base_classifier.model`)
    with `evaluate_multiclass_mdl`. Then we can employ `compare_image_predictions`
    to plot a grid of images, contrasting the randomly selected adversarial examples
    (`X_test_fgsm`) against the original ones (`X_test_mdsample`) and their corresponding
    predicted labels (`y_test_fgsm_pred`, `y_test_mdsample`) and probabilities (`y_test_fgsm_prob`,
    `y_test_mdsample_prob`). We customize the titles and limit the grid to four examples
    (`num_samples`). By default, `compare_image_predictions` only compares misclassifications
    but an optional attribute, `use_misclass`, can be set to `False` to compare correct
    classifications:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将一举两得。首先，使用`evaluate_multiclass_mdl`评估对抗示例（`X_test_fgsm`）对我们基础分类器模型（`base_classifier.model`）的模型。然后我们可以使用`compare_image_predictions`来绘制图像网格，对比随机选择的对抗示例（`X_test_fgsm`）与原始示例（`X_test_mdsample`）及其相应的预测标签（`y_test_fgsm_pred`，`y_test_mdsample`）和概率（`y_test_fgsm_prob`，`y_test_mdsample_prob`）。我们自定义标题并限制网格为四个示例（`num_samples`）。默认情况下，`compare_image_predictions`仅比较误分类，但可以通过将可选属性`use_misclass`设置为`False`来比较正确分类：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The preceding code outputs a table first, which shows that the model has only
    44% accuracy with FSGM-attacked examples! And even though it wasn’t a targeted
    attack, it was most effective toward correctly masked faces. So hypothetically,
    if perpetrators managed to cause this level of signal distortion or interference,
    they would severely undermine the security companies’ ability to monitor mask
    compliance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码首先输出一个表格，显示模型在FSGM攻击示例上的准确率仅为44%！尽管这不是针对特定目标的攻击，但它对正确遮挡的面部效果最为显著。所以假设，如果肇事者能够造成这种程度的信号扭曲或干扰，他们将严重削弱公司监控口罩合规性的能力。
- en: 'The code also outputs *Figure 13.5*, which shows some misclassifications caused
    by the FSGM attack. The attack pretty much evenly distributed noise throughout
    the images. It also shows that the image was only modified by a mean absolute
    error of 0.092, and since pixel values range between 0 and 1, this means 9.2%.
    If you were to calibrate attacks so that they are less detectable but still impactful,
    you must note that an `eps` of 0.1 causes a 9.2% mean absolute perturbation, which
    reduces accuracy to 44%:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 代码还输出了*图13.5*，该图显示了由FSGM攻击引起的一些误分类。攻击在图像中几乎均匀地分布了噪声。它还显示图像仅通过均方误差0.092进行了修改，由于像素值介于0和1之间，这意味着9.2%。如果你要校准攻击以使其更难检测但仍然具有影响力，你必须注意，`eps`为0.1会导致9.2%的平均绝对扰动，这会将准确性降低到44%：
- en: '![A collage of a person  Description automatically generated with low confidence](img/B18406_13_05.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![一个人物的拼贴画  描述由低置信度自动生成](img/B18406_13_05.png)'
- en: 'Figure 13.5: Plot comparing FSGM-attacked versus the original images for the
    base classifier'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5：比较基础分类器FSGM攻击前后图像的图表
- en: Speaking of less detectable attacks, we will now learn about Carlini and Wagner
    attacks.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 说到更难检测的攻击，我们现在将了解Carlini和Wagner攻击。
- en: Carlini and Wagner infinity norm attack
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Carlini和Wagner无穷范攻击
- en: 'In 2017, **Carlini and Wagner** (**C&W**) employed three **norm-based distance
    metrics**: ![](img/B18406_13_002.png), ![](img/B18406_13_003.png), and ![](img/B18406_13_004.png),
    measuring the difference between the original and adversarial example. In other
    papers, these metrics had already been discussed, including the FSGM one. The
    innovation introduced by C&W was how these metrics were leveraged, using a gradient
    descent-based optimization algorithm designed to approximate a loss function minima.
    Specifically, to avoid getting stuck at a local minimum, they use multiple starting
    points in the gradient descent. And so that the process “yields a valid image,”
    it evaluates three methods to box-constrain the optimization problem. In this
    case, we want to find an adversarial example where the distances between that
    example and the original image are minimal, while also remaining realistic.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'All three C&W attacks (![](img/B18406_13_002.png), ![](img/B18406_13_003.png),
    and ![](img/B18406_13_004.png)) use the Adam optimizer to quickly converge. Their
    main difference is the distance metric, of which ![](img/B18406_13_004.png) is
    arguably the best one. It’s defined as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18406_13_009.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: And because it’s the maximum distance to any coordinate, you make sure that
    the adversarial example is not just “on average” minimally different but also
    not too different anywhere in the feature space. That’s what would make an attack
    less detectable!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Initializing C&W infinity norm attacks and generating adversarial examples
    with them is similar to FSGM. To initialize `CarliniLInfMethod`, we define optionally
    a `batch_size` (the default is `128`). Then, to `generate` an untargeted adversarial
    attack, the same applies as with FSGM. Only `X` is needed when untargeted, and
    `y` when targeted:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will now evaluate the C&W adversarial examples (`X_test_cw`) just as we
    did with FSGM. It’s exactly the same code, only with `fsgm` replaced with `cw`
    and different titles in `compare_image_predictions`. Just as with FSGM, the following
    code will yield a classification report and grid of images (*Figure 13.6*):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As outputted by the preceding code, the C&W adversarial examples have a 92%
    accuracy with our base model. The drop is sufficient to render the model useless
    for its intended purpose. If the attacker disturbed a camera’s signal just enough,
    they could achieve the same results. And, as you can tell by *Figure 13.6*, the
    perturbation of 0.3% is tiny compared to FSGM, but it was sufficient to misclassify
    8%, including the four in the grid that seem apparent to the naked eye:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of a child  Description automatically generated with low confidence](img/B18406_13_06.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.6: Plot comparing C&W infinity norm-attacked versus the original
    images for the base classifier'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes it doesn’t matter if an attack goes undetected or not. The point of
    it is to make a statement, and that’s what adversarial patches can do.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Targeted adversarial patch attack
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Adversarial Patches** (**APs**) are a robust, universal, and targeted method.
    You generate a patch you can either superimpose on an image, or print and physically
    place in a scene to trick a classifier into ignoring everything else in the scene.
    It is designed to work under a wide variety of conditions and transformations.
    Unlike other adversarial example generation approaches, there’s no intention of
    camouflaging the attack because, essentially, you replace a detectable portion
    of the scene with the patch. The method works by leveraging a variant of **Expectation
    Over Transformation** (**EOT**), which trains images over transformations of a
    given patch on different locations of an image. What it learns is the patch that
    fools the classifier the most, given the training examples.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'This method requires more parameters and steps than FSGM and C&W. For starters,
    we will use `AdversarialPatchNumpy`, which is the variant that works with any
    neural network image or video classifier. There’s also one for TensorFlow v2,
    but our base classifier is a `KerasClassifier`. The first argument is the classifier
    (`base_classifier`), and the other ones we will define are optional but highly
    recommended. The scaling ranges `scale_min` and `scale_max` are particularly important
    because they define how big can patches be in relation to the images – in this
    case, we want to test no smaller than 40% and no larger than 70%. Besides that,
    it makes sense to define a target class (`target`). In this case, we want the
    patch to target the “Correct” class. For the `learning_rate` and max iterations
    (`max_iter`), we use the defaults but note that these can be tuned to improve
    patch adversarial effectiveness:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We don’t want the patch generation algorithm to waste time testing patches
    everywhere in images, so we can direct this effort by using a Boolean mask. This
    mask tells it where it can center the patch. To make the mask, we start by creating
    an array of zeros, 128 × 128\. Then we place ones in the rectangular area between
    pixels 80–93 and 45–84, which loosely corresponds to cover the center of the mouth
    area in most of the images. Lastly, we expand the array’s dimensions so that it’s
    `(1, W, H)` and convert it to a Boolean. Then we can proceed to `generate` patches
    using the small-size test dataset samples and the mask:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can now plot the patch with the following snippet:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code produced the image in *Figure 13.7*. As expected, it has
    plenty of the shades of blue found in masks. It also has bright red and yellow
    hues, mostly missing from training examples, which confuse the classifier:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated with low confidence](img/B18406_13_07.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.7: AP generated image to misclassify as correctly masked'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike other methods, `generate` didn’t produce adversarial examples but a
    single patch, which is an image we can then place on top of images to create adversarial
    examples. This task is performed with `apply_patch`, which takes the original
    examples `X_test_smsample` and a scale; we will use 55%. It is also recommended
    to use a `mask` that will make sure the patch is applied where it makes more sense
    – in this case, in the area around the mouth:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now it’s time to evaluate our attack and examine some misclassifications. We
    will do exactly as before and reuse the code that produced *Figure 13.5* and *Figure
    13.7*, except we replace the variables so that they have `ap` and a corresponding
    title:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code yields the accuracy result of our attack at 65%, which is
    quite good considering how few examples it was trained on. AP needs more data
    than other methods. Targeted attacks, in general, need more examples to understand
    how to best target one class. The preceding code also produced the grid of images
    in *Figure 13.8*, which demonstrates how, hypothetically, if people walked around
    holding a cardboard patch in front of their face, they could easily fool the model:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing person, posing, different, same  Description automatically
    generated](img/B18406_13_08.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.8: Plot comparing AP-attacked versus the original images for base
    classifier'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have studied three attack methods but haven’t yet tackled how to
    defend against these attacks. We will explore a couple of solutions next.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Defending against targeted attacks with preprocessing
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are five broad categories of adversarial defenses:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing**: changing the model’s inputs so that they are harder to attack.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training**: training a new robust model that is designed to overcome attacks.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detection**: detecting attacks. For instance, you can train a model to detect
    adversarial examples.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer**: modifying model architecture and training so that it’s more
    robust – this may include techniques such as distillation, input filters, neuron
    pruning, and unlearning.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Postprocessing**: changing model outputs to overcome production inference
    or model extraction attacks.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Only the first four defenses work with evasion attacks, and in this chapter,
    we will only cover the first two: **preprocessing** and **adversarial training**.
    FGSM and C&W can be defended easily with either of these, but an AP is tougher
    to defend against, so it might require a stronger **detection** or **transformer**
    method.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Before we defend, we must create a targeted attack. We will employ **Projected
    Gradient Descent** (**PGD**), which is a strong attack very similar in output
    to FSGM – that is, it produces noisy images. We won’t explain PGD in detail here
    but what is important to note is, like FSGM, it is regarded as a **first-order
    adversary** because it leverages first-order information about a network (due
    to gradient descent). Also, experiments prove that robustness against PGD ensures
    robustness against any first-order adversary. Specifically, PGD is a strong attack,
    so it makes for conclusive benchmarks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a targeted attack against the correctly masked class, it’s best that
    we only select examples that aren’t correctly masked (`x_test_notmasked`) and
    their corresponding labels (`y_test_notmasked`) and predicted probabilities (`y_test_notmasked_prob`).
    Then, we want to create an array with the class (`Correct`) that we want to generate
    adversarial examples for (`y_test_masked`):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We initialize `ProjectedGradientDescent` as we did with FSGM, except we will
    set the maximum perturbation (`eps`), attack step size (`eps_step`), maximum iterations
    (`max_iter`), and `targeted=True`. Precisely because it is targeted, we will set
    both `X` and `y`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now let’s evaluate the PGD attack as we did before, but this time, let’s plot
    the confusion matrix (`plot_conf_matrix=True`):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Chart  Description automatically generated](img/B18406_13_09.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.9: Confusion matrix for PGD attacked examples evaluated against the
    base classifier'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s run `compare_image_prediction` to see some random misclassifications:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding code plots the grid of images in *Figure 13.10*. The mean absolute
    perturbation is the highest we’ve seen so far at 14.7%, and all unmasked faces
    in the grid are classified as correctly masked:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of a person  Description automatically generated with medium confidence](img/B18406_13_10.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.10: Plot comparing PGD-attacked versus original images for the base
    classifier'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy cannot get worse, and the images are grainy beyond repair. So how
    can we combat noise? If you recall, we have dealt with this problem before. In
    *Chapter 7*, *Visualizing Convolutional Neural Networks*, **SmoothGrad** improved
    saliency maps by averaging the gradients. It’s a different application but the
    same principle – just as with a human, a noisy saliency map is more challenging
    to interpret than a smooth one, and a grainy image is much more challenging for
    a model to interpret than a smooth one.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '**Spatial smoothing** is just a fancy way of saying blur! However, what’s novel
    about it being introduced as an adversarial defense method is that the proposed
    implementation (`SpatialSmoothing`) calls for using the median and not the mean
    in a sliding window. The `window_size` is configurable, and it is recommended
    to adjust it where it is most useful as a defense. Once the defense has been initialized,
    you plug in the adversarial examples (`X_test_pgd`). It will output spatially
    smoothed adversarial examples (`X_test_pgd_ss`):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we can take the blurred adversarial examples produced and evaluate them
    as we did before – first, with `evaluate_multiclass_mdl` to get predicted labels
    (`y_test_pgd_ss_pred`) and probabilities (`y_test_pgd_ss_prob`) and the output
    of some predictive performance metrics. With `compare_image_predictions` to plot
    a grid of images, let’s use `use_misclass=False` to compare properly classified
    images – in other words, the adversarial examples that were defended successfully:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The preceding code yields an accuracy of 54%, which is much better than 0% before
    the spatial smoothing defense. It also produces *Figure 13.11*, which demonstrates
    how blur effectively thwarted the PGD attack. It even halved the mean absolute
    perturbation!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of people  Description automatically generated with low confidence](img/B18406_13_11.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.11: Plot comparing spatially smoothed PGD-attacked images versus
    the original images for the base classifier'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will try another defense method in our toolbox: adversarial training!'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Shielding against any evasion attack by adversarial training of a robust classifier
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 7*, *Visualizing Convolutional Neural Networks*, we identified a
    garbage image classifier that would likely perform poorly in the intended environment
    of a municipal recycling plant. The abysmal performance on out-of-sample data
    was due to the classifier being trained on a large variety of publicly available
    images that don’t match the expected conditions, or the characteristics of materials
    that are processed by a recycling plant. The chapter’s conclusion called for training
    a network with images that represent their intended environment to make for a
    more robust model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: For model robustness, training data variety is critical, but only if it represents
    the intended environment. In statistical terms, it’s a question of using samples
    for training that accurately depict the population so that a model learns to classify
    them correctly. For adversarial robustness, the same principles apply. If you
    augment data to include plausible examples of adversarial attacks, the model will
    learn to classify them. That’s what adversarial training is in a nutshell.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning researchers in the adversarial robustness field suggest this
    form of defense is very effective against any kind of evasion attack, essentially
    shielding it. That being said, it’s not impervious. Its effectiveness is contingent
    on using the right kind of adversarial examples in training, the optimal hyperparameters,
    and so forth. There are some guidelines outlined by researchers, such as increasing
    the number of neurons in the hidden layers and using PGD or BIM to produce adversarial
    examples for the training. **BIM** stands for **Basic Iterative Method**. It’s
    like FSGM but not fast because it iterates to approximate the best adversarial
    example within a ![](img/B18406_13_010.png)-neighborhood for the original image.
    The `eps` attribute bounds this neighborhood.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Training a robust model can be very resource-intensive. It is not required
    because we can download one already trained for us, but it’s important to understand
    how to perform this with ART. We will explain these steps to give the option of
    completing the model training with ART. Otherwise, just skip the steps and download
    the trained model. The `robust_model` is very much like the `base_model` except
    we use equal-sized filters in the four convolutional (`Conv2D`) layers. We do
    this to decrease complexity to counter the complexity we add by quadrupling the
    neurons in the first hidden (`Dense`) layer, as suggested by the machine learning
    researchers:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `summary()` in the preceding code outputs the following. You can see that
    trainable parameters total around 3.6 million – similar to the base model:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we can adversarially train the model by first initializing a new `KerasClassifier`
    with the `robust_model`. Then, we initialize a `BasicIterativeMethod` attack on
    this classifier. Lastly, we initialize `AdversarialTrainer` with the `robust_classifier`
    and the BIM attack and `fit` it. Please note that we saved the BIM attack into
    a variable called `attacks` because this could be a list of ART attacks instead
    of a single one. Also, note that `AdversarialTrainer` has an attribute called
    `ratio`. This attribute determines what percentage of the training examples are
    adversarial examples. This percentage dramatically impacts the effectiveness of
    adversarial attacks. If it’s too low, it might not perform well with adversarial
    examples, and if it’s too high, it might perform less effectively with non-adversarial
    examples. If we run the `trainer`, it will likely take many hours to complete,
    so don’t get alarmed:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If you didn’t train the `robust_classifier`, you can download a pretrained
    `robust_model` and initialize the `robust_classifier` with it like this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let’s evaluate the `robust_classifier` against the original test dataset
    using `evaluate_multiclass_mdl`. We set `plot_conf_matrix=True` to see the confusion
    matrix:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code outputs the confusion matrix and performance metrics in
    *Figure 13.12*. It’s 1.8% less accurate than the base classifier. Most of the
    misclassifications are with correctly masked faces getting classified as incorrectly
    masked. There’s certainly a trade-off when choosing a 50% adversarial example
    ratio, or perhaps we can tune the hyperparameters or the model architecture to
    improve this:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, treemap chart  Description automatically generated](img/Image15514.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.12: Robust classifier confusion metrics and performance metrics'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how the robust model fares against adversarial attacks. Let’s use
    `FastGradientMethod` again, but this time, replace `base_classifier` with `robust_classifier`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we can employ `evaluate_multiclass_mdl` and `compare_image_predictions`
    to measure and observe the effectiveness of our attack, but this time against
    the `robust_classifier`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![A collage of a baby  Description automatically generated with low confidence](img/B18406_13_13.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.13: Plot comparing FSGM-attacked versus the original images for the
    robust classifier'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have evaluated the robustness of models but only against one attack
    strength, without factoring in possible defenses, thus evaluating its robustness.
    In the next section, we will study a method that does this.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating adversarial robustness
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s necessary to test your systems in any engineering endeavor to see how vulnerable
    they are to attacks or accidental failures. However, security is a domain where
    you must stress-test your solutions to ascertain what level of attacks are needed
    to make your system break down beyond an acceptable threshold. Furthermore, figuring
    out what level of defense is needed to curtail an attack is useful information
    too.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Comparing model robustness with attack strength
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now have two classifiers we can compare against an equally strengthened attack,
    and we try different attack strengths to see how they fare across all of them.
    We will use FSGM because it’s fast, but you could use any method!
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'The first attack strength we can assess is no attack strength. In other words,
    what is the classification accuracy against the test dataset with no attack? We
    already had stored the predicted labels for both the base (`y_test_pred`) and
    robust (`y_test_robust_pred`) models, so this is easy to obtain with scikit-learn’s
    `accuracy_score` metric:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can now iterate across a range of attack strengths (`eps_range`) between
    0.01 and 0.9\. Using `linspace`, we can generate 9 values between 0.01 and 0.09
    and 9 values between 0.1 and 0.9, and `concatenate` them into a single array.
    We will test attacks for these 18 `eps` values by `for`-looping through all of
    them, attacking each model, and retrieving the post-attack accuracies with `evaluate`.
    The respective accuracies are appended to two lists (`accuracy_base`, `accuracy_robust`).
    And after the `for` loop, we prepend 0 to the `eps_range` to account for the accuracies
    prior to any attacks:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, we can plot the accuracies for both classifiers across all attack strengths
    with the following code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code generates *Figure 13.14*, which demonstrates that the robust
    model performs better between attack strengths of 0.02 and 0.3 but then does consistently
    about 10% worse:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/Image15613.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.14: Accuracy measured for the robust and base classifiers at different
    FSGM attack strengths'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: One thing that *Figure 13.14* fails to account for is defenses. For example,
    if hospital cameras were constantly jammed or tampered with, the security company
    would be remiss not to defend their models. The easiest way to do so for this
    kind of attack is with some sort of smoothing.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial training also produces an empirically robust classifier that you
    cannot guarantee will work under certain pre-defined circumstances, which is why
    there’s a need for certifiable defenses.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Mission accomplished
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The mission was to perform some adversarial robustness tests on their face mask
    model to determine if hospital visitors and staff can evade mandatory mask compliance.
    The base model performed very poorly on many evasion attacks, from the most aggressive
    to the most subtle.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: You also looked at possible defenses to these attacks, such as spatial smoothing
    and adversarial retraining. And then, you explored ways to evaluate the robustness
    of your proposed defenses. You can now provide an end-to-end framework to defend
    against this kind of attack. That being said, what you did was only a proof of
    concept.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Now, you can propose training a certifiably robust model against the attacks
    the hospitals expect to encounter the most. But first, you need the ingredients
    for a generally robust model. To this end, you will need to take all 210,000 images
    in the original dataset, make many variations on mask colors and types with them,
    and augment them even further with reasonable brightness, shear, and rotation
    transformations. Lastly, the robust model needs to be trained with several kinds
    of attacks, including several kinds of APs. These are important because they mimic
    the most common compliance evasion behavior of concealing faces with body parts
    or clothing items.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading this chapter, you should understand how attacks can be perpetrated
    on machine learning models and evasion attacks in particular. You should know
    how to perform FSGM, BIM, PGD, C&W, and AP attacks, as well as how to defend against
    them with spatial smoothing and adversarial training. Last but not least, you
    know how to evaluate adversarial robustness.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is the last one, and it outlines some ideas on what’s next
    for machine learning interpretation.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Dataset sources
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adnane Cabani, Karim Hammoudi, Halim Benhabiles, and Mahmoud Melkemi, 2020,
    *MaskedFace-Net - A dataset of correctly/incorrectly masked face images in the
    context of COVID-19*, Smart Health, ISSN 2352–6483, Elsevier: [https://doi.org/10.1016/j.smhl.2020.100144](https://doi.org/10.1016/j.smhl.2020.100144)
    (Creative Commons BY-NC-SA 4.0 license by NVIDIA Corporation)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karras, T., Laine, S., and Aila, T., 2019, *A Style-Based Generator Architecture
    for Generative Adversarial Networks*. 2019 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), 4396–4405: [https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948)
    (Creative Commons BY-NC-SA 4.0 license by NVIDIA Corporation)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Polyakov, A., 2019, Aug 6, *How to attack Machine Learning (Evasion, Poisoning,
    Inference, Trojans, Backdoors)* [blog post]: [https://towardsdatascience.com/how-to-attack-machine-learning-evasion-poisoning-inference-trojans-backdoors-a7cb5832595c](https://towardsdatascience.com/how-to-attack-machine-learning-evasion-poisoning-inference-trojans-backdoors-a7cb5832595c)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carlini, N., & Wagner, D., 2017, *Towards Evaluating the Robustness of Neural
    Networks*. 2017 IEEE Symposium on Security and Privacy (SP), 39–57: [https://arxiv.org/abs/1608.04644](https://arxiv.org/abs/1608.04644)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown, T., Mané, D., Roy, A., Abadi, M., and Gilmer, J., 2017, *Adversarial
    Patch*. ArXiv: [https://arxiv.org/abs/1712.09665](https://arxiv.org/abs/1712.09665)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn more on Discord
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/inml](Chapter_13.xhtml)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code107161072033138125.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
