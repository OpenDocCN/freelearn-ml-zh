- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adversarial Robustness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning interpretation has many concerns, ranging from knowledge discovery
    to high-stakes ones with tangible ethical implications, like the fairness issues
    examined in the last two chapters. In this chapter, we will direct our attention
    to concerns involving reliability, safety, and security.
  prefs: []
  type: TYPE_NORMAL
- en: As we realized using the **contrastive explanation method** in *Chapter 7*,
    *Visualizing Convolutional Neural Networks*, we can easily trick an image classifier
    into making embarrassingly false predictions. This ability can have serious ramifications.
    For instance, a perpetrator can place a black sticker on a yield sign, and while
    most drivers would still recognize this as a yield sign, a self-driving car may
    no longer recognize it and, as a result, crash. A bank robber could wear a cooling
    suit designed to trick the bank vault’s thermal imaging system, and while any
    human would notice it, the imaging system would fail to do so.
  prefs: []
  type: TYPE_NORMAL
- en: The risk is not limited to sophisticated image classifiers. Other models can
    be tricked! The **counterfactual examples** produced in *Chapter 6**, Anchors
    and Counterfactual Explanations,* are like adversarial examples except with the
    goal of deception. An attacker could leverage any misclassification example, straddling
    the decision boundary adversarially. For instance, a spammer could realize that
    adjusting some email attributes increases the likelihood of circumventing spam
    filters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Complex models are more vulnerable to adversarial attacks. So why would we
    trust them?! We can certainly make them more foolproof, and that’s what adversarial
    robustness entails. An adversary can purposely thwart a model in many ways, but
    we will focus on evasion attacks and briefly explain other forms of attacks. Then
    we will explain two defense methods: spatial smoothing preprocessing and adversarial
    training. Lastly, we will demonstrate one robustness evaluation method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the main topics we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning about evasion attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defending against targeted attacks with preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shielding against any evasion attack through adversarial training of a robust
    classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter’s example uses the `mldatasets`, `numpy`, `sklearn`, `tensorflow`,
    `keras`, `adversarial-robustness-toolbox`, `matplotlib`, and `seaborn` libraries.
    Instructions on how to install all of these libraries are in the *Preface*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter is located here: [https://packt.link/1MNrL](https://packt.link/1MNrL)'
  prefs: []
  type: TYPE_NORMAL
- en: The mission
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The privately contracted security services industry market worldwide is valued
    at over USD 250 billion and is growing at around 5% annually. However, it faces
    many challenges, such as shortages of adequately trained guards and specialized
    security experts in many jurisdictions, and a whole host of unexpected security
    threats. These threats include widespread coordinated cybersecurity attacks, massive
    riots, social upheaval, and, last but not least, health risks brought on by pandemics.
    Indeed, 2020 tested the industry with a wave of ransomware, misinformation attacks,
    protests, and COVID-19 to boot.
  prefs: []
  type: TYPE_NORMAL
- en: In the wake of this, one of the largest hospital networks in the United States
    asked their contracted security company to monitor the correct use of masks by
    both visitors and personnel throughout the hospitals. The security company has
    struggled with this request because it diverts security personnel from tackling
    other threats, such as intruders, combative patients, and belligerent visitors.
    It has video surveillance in every hallway, operating room, waiting room, and
    hospital entrance. It’s impossible to have eyes on every camera feed every time,
    so the security company thought they could assist guards with deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'These models already alert unusual activities, such as running in the hallways
    and brandishing weapons anywhere on the premises. They have proposed to the hospital
    network that they would like to add a new model that detects masks’ correct usage.
    Before COVID-19, there were policies in place for mandatory mask usage in certain
    areas of each hospital, and during COVID-19, it was required everywhere. The hospital
    administrators would like to turn on and off this monitoring feature, depending
    on pandemic risk levels moving forward. They realize that personnel get fatigued
    and forget to put masks back on, or they partially slip off at times. Many visitors
    are also hostile toward using masks and may wear one when entering the hospital
    but take it off when no guard is around. This isn’t always intentional, so they
    wouldn’t want to dispatch guards on every alert, unlike other threats. Instead,
    they’d rather use awareness and a little bit of shame to modify behavior and only
    intervene with repeat offenders:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A yellow sign on a pole  Description automatically generated with low confidence](img/B18406_13_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1: Radar speed signs like this one help curb speeding'
  prefs: []
  type: TYPE_NORMAL
- en: Awareness is a very effective method with radar speed signs (see *Figure 13.1*),
    which make roads safer by only making drivers aware that they are driving too
    fast. Likewise, having a screen at the end of heavily trafficked hallways showing
    snapshots of those who have recently either mistakenly or purposely not complied
    with mandatory mask usage potentially creates some embarrassment for offenders.
    The system will log repeat offenders so that security guards can look for them
    and either make them comply or ask them to vacate the premises.
  prefs: []
  type: TYPE_NORMAL
- en: There’s some concern with visitors trying to trick the model into evading compliance,
    so the security company has hired you to ensure that the model is robust in the
    face of this kind of adversarial attack. Security officers have noticed some low-tech
    trickery before, such as people momentarily covering their faces with their hands
    or a part of their sweater when they realize cameras monitor them. Also, in one
    disturbing incident, a visitor dimmed the lights and sprayed some gel on a camera,
    and in another, an individual painted their mouth. However, there are concerns
    about higher-tech attacks, such as jamming the camera’s wireless signal or shining
    high-powered lasers directly into cameras. Devices that perform these attacks
    are increasingly easier to obtain and could impact other surveillance functions
    on a larger scale, like preventing theft. The security company hopes this robustness
    exercise can inform their efforts to improve every surveillance system and model.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, the security company would like to produce its own dataset with
    face images from the hospitals they monitor. Meanwhile, synthetically masked faces
    from external sources are the best they can do to productionize a model in the
    short term. To this end, you have been provided a large dataset of synthetically
    correctly and incorrectly masked faces and their unmasked counterparts. The two
    datasets were combined into a single one, and the original dimensions of 1,024
    × 1,024 were reduced to the thumbnail size of 124 × 124\. Also, for efficiency’s
    sake, 21,000 images were sampled from roughly 210,000 in these datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You’ve decided to take a four-fold approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring several possible evasion attacks to understand how vulnerable the
    model is to them and how credible they are as threats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a preprocessing method to protect a model against these attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging adversarial retraining to produce a robust classifier that is intrinsically
    less prone to many of these attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating robustness with state-of-the-art methods to assure hospital administrators
    that the model is adversarially robust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: The preparations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will find the code for this example here: [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/13/Masks.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/13/Masks.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run this example, you need to install the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mldatasets` to load the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` and `sklearn` (scikit-learn) to manipulate it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensorflow` to fit the models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` and `seaborn` to visualize the interpretations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You should load all of them first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s check that TensorFlow has loaded the right version with `print(tf.__version__)`.
    The version should be 2.0 and above.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should also disable eager execution and verify that it worked with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The output should say that it’s `False`.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow, turning eager execution mode on means that it doesn’t require
    a computational graph or a session. It’s the default for TensorFlow 2.x and later
    but not in prior versions, so you need to disable it to avoid incompatibilities
    with code that was optimized for prior versions of TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will load the data into four NumPy arrays, corresponding to the training/test
    datasets. While we are at it, we will divide `X` face images by 255 because, that
    way, they will be of values between zero and one, which is better for deep learning
    models. We call this feature scaling. We will need to record the `min_` and `max_`
    for the training data because we will need these later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s always important to verify our data when we load it to make sure it didn’t
    get corrupted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, a preprocessing step we will need to perform is to **One-Hot Encode**
    (**OHE**) the `y` labels because we will need the OHE form to evaluate the model’s
    predictive performance. Once we initialize the `OneHotEncoder`, we will need to
    `fit` it into the training data (`y_train`). We can also extract the categories
    from the encoder into a list (`labels_l`) to verify that it has all three:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For reproducibility’s sake, always initialize your random seeds like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Making machine learning truly reproducible means also making it deterministic,
    which means that training with the same data will produce a model with the same
    parameters. Determinism is very difficult with deep learning and is often session-,
    platform-, and architecture-dependent. If you use an NVIDIA GPU, you can install
    a library called `framework-reproducibility`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the adversarial attack, defense, and evaluation methods we will study
    in this chapter are very resource-intensive, so if we used the entire test dataset
    with them, they could likely take many hours on a single method! For efficiency,
    it is strongly suggested to use samples of the test dataset. Therefore, we will
    create a medium 200-image sample (`X_test_mdsample`, `y_test_mdsample`) and a
    small 20-image sample (`X_test_smsample`, `y_test_smsample`) using `np.random.choice`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We have two sample sizes because some methods could take too long with a larger
    sample size. Now, let’s take a peek at what images are in our datasets. In the
    preceding code, we have taken a medium and small sample of our test dataset. We
    will place each image of our small sample in a 4 × 5 grid with the class label
    above it, with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code plots the grid of images in *Figure 13.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of a person  Description automatically generated with medium confidence](img/B18406_13_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2: A small test dataset sample of masked and unmasked faces'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13.2* depicts a variety of correctly and incorrectly masked and unmasked
    faces of all ages, genders, and ethnicities. Despite the variety, one thing to
    note about this dataset is that it only has light blue surgical masks represented,
    and images are mostly at a front-facing angle. Ideally, we would generate an even
    larger dataset with all colors and types of masks and augment it further with
    random rotations, shears, and brightness adjustments, either before or during
    training. These augmentations would make for a much more robust model. Nevertheless,
    we must differentiate between this general type of robustness and adversarial
    robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the CNN base model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You don’t have to train the CNN base model, but the code to do so is provided
    nonetheless in the GitHub repository. The pretrained model has also been stored
    there. We can quickly load the model and output its summary like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding snippet outputs the following summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The summary has pretty much everything we need to know about the model. It has
    four convolutional layers (`Conv2D`), each followed by a max pool layer (`MaxPooling2D`).
    It then has a `Flatten` layer and a fully connected layer (`Dense`). Then, there’s
    more `Dropout` before the second `Dense` layer. Naturally, three neurons are in
    this final layer, corresponding to each class.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing the CNN base classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can evaluate the model using the test dataset with the `evaluate_multiclass_mdl`
    function. The arguments include the model (`base_model`), our test data (`X_test`),
    and the corresponding labels (`y_test`), as well as the class names (`labels_l`)
    and the encoder (`ohe`). Lastly, we don’t need to plot the ROC curves since, given
    the high accuracy, they won’t be very informative (`plot_roc=False`). This function
    returns the predicted labels and probabilities, which we can store as variables
    for later use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates *Figure 13.3*, with a confusion matrix and performance
    metrics for each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18406_13_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.3: The confusion matrix and predictive performance metrics for the
    base classifier, evaluated on the test dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Even though the confusion matrix in *Figure 13.3* seems to suggest a perfect
    classification, pay attention to the circled areas. We can tell the model had
    some issues with misclassifying incorrectly masked faces once we see the recall
    (99.5%) breakdown.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can start attacking this model to assess how robust it actually is!
  prefs: []
  type: TYPE_NORMAL
- en: Learning about evasion attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are six broad categories of adversarial attacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evasion**: designing an input that can cause a model to make an incorrect
    prediction, especially when it wouldn’t fool a human observer. It can either be
    targeted or untargeted, depending on the attacker’s intention to fool the model
    into misclassifying a specific class (targeted) or, rather, misclassifying any
    class (untargeted). The attack methods can be white-box if the attacker has full
    access to the model and its training dataset, or black-box with only inference
    access. Gray-box sits in the middle. Black-box is always model-agnostic, whereas
    white and gray-box methods might be.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Poisoning**: injecting faulty training data or parameters into a model can
    come in many forms, depending on the attacker’s capabilities and access. For instance,
    for systems with user-generated data, the attacker may be capable of adding faulty
    data or labels. If they have more access, perhaps they can modify large amounts
    of data. They can also adjust the learning algorithm, hyperparameters, or data
    augmentation schemes. Like evasion, poisoning can also be targeted and untargeted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference**: extracting the training dataset through model inference. Inference
    attacks also come in many forms and can be used for espionage (privacy attacks)
    through membership inference, which confirms if one example (for instance, a specific
    person) was in the training dataset. Attribute inference ascertains if an example
    category (for instance, an ethnicity) was represented in the training data. Input
    inference (also known as model inversion) has attack methods to extract a training
    dataset from a model rather than guessing and confirming. These have broad privacy
    and regulatory implications, especially in medical and legal applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trojaning**: this implants malicious functionality activated with a trigger
    during inference but requires retraining the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backdooring**: similar to trojans but a backdoor remains, even when a model
    is retrained from scratch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reprogramming**: remote sabotaging of a model during training by sneaking
    in examples that are specifically designed to produce specific outputs. For instance,
    if you provide enough examples labeled as tiger shark where four small black squares
    are always in the same place, the model will learn that that is a tiger shark,
    regardless of what it is, thus intentionally forcing the model to overfit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first three are the most studied forms of adversarial attacks. Attacks
    can be further subcategorized once we split them by stage and goal (see *Figure
    13.4*). The stage is when the attack is perpetrated because it can impact the
    model training or its inference, and the goal is what the attacker hopes to gain
    from it. This chapter will only deal with evasion sabotage attacks because we
    expect hospital visitors, patients, and personnel to occasionally sabotage the
    production model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18406_13_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.4: Table of adversarial attack category methods by stage and goal'
  prefs: []
  type: TYPE_NORMAL
- en: Even though we use white-box methods to attack, defend, and evaluate a model’s
    robustness, we don’t expect attackers to have this level of access. We will only
    use white-box methods because we have full access to the model. In other circumstances,
    such as a bank surveillance system with a thermal imaging system and a corresponding
    model to detect perpetrators, we could expect professional attackers to use black-box
    methods to find vulnerabilities! So, as defenders of this system, we would be
    wise to try the very same attack methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The library we will use for adversarial robustness is called the **Adversarial
    Robustness Toolbox** (**ART**), and it’s supported by the **LF AI & Data Foundation**
    – the same folks that support other open-source projects such as AIX360 and AIF360,
    explored in *Chapter 11**, Bias Mitigation and Causal Inference Methods*. ART
    requires that attacked models are abstracted in an estimator or classifier, even
    if it’s a black-box one. We will use `KerasClassifier` for most of this chapter
    except for the last section, in which we will use `TensorFlowV2Classifier`. Initializing
    an ART classifier is simple. You must specify the `model`, and sometimes there
    are other required attributes. For `KerasClassifier`, all remaining attributes
    are optional, but it is recommended you use `clip_values` to specify the range
    of the features. Many attacks are input permutations, so knowing what input values
    are allowed or feasible is essential:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we also prepare two arrays with probabilities for the
    predicted class of the medium and small samples. It is entirely optional, but
    these assist in placing the predicted probability next to the predicted label
    when plotting some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Fast gradient sign method attack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most popular attack methods is the **Fast Gradient Sign Method**
    (**FSGM** or **FGM**). As the name implies, it leverages a deep learning model’s
    gradient to find adversarial examples. It performs small perturbations on the
    pixels of the input image, either additions or subtractions. Which one to use
    depends on the gradient’s sign, which indicates what direction would increase
    or decrease the loss according to the pixel’s intensity.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with all ART attack methods, you first initialize it by providing the ART
    estimator or classifier. `FastGradientMethod` also requires an attack step size
    `eps`, which will condition the attack strength. Incidentally, `eps` stands for
    epsilon (![](img/B18406_13_001.png)), which represents error margins or infinitesimal
    approximation errors. A small step size will cause pixel intensity changes to
    be less visible, but it will also misclassify fewer examples. A larger step size
    will cause more examples to be misclassified with more visible changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After initializing, the next step is to `generate` the adversarial examples.
    The only required attribute is original examples (`X_test_mdsample`). Please note
    that FSGM can be targeted, so there’s an optional `targeted` attribute in the
    initialization, but you would also need to provide corresponding labels in the
    generation. This attack is untargeted because the attacker’s intent is to sabotage
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Generating the adversarial examples with FSGM is quick, unlike other methods,
    hence the “Fast”!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will do two things in one swoop. First, evaluate the adversarial examples
    (`X_test_fgsm`) against our base classifier’s model (`base_classifier.model`)
    with `evaluate_multiclass_mdl`. Then we can employ `compare_image_predictions`
    to plot a grid of images, contrasting the randomly selected adversarial examples
    (`X_test_fgsm`) against the original ones (`X_test_mdsample`) and their corresponding
    predicted labels (`y_test_fgsm_pred`, `y_test_mdsample`) and probabilities (`y_test_fgsm_prob`,
    `y_test_mdsample_prob`). We customize the titles and limit the grid to four examples
    (`num_samples`). By default, `compare_image_predictions` only compares misclassifications
    but an optional attribute, `use_misclass`, can be set to `False` to compare correct
    classifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code outputs a table first, which shows that the model has only
    44% accuracy with FSGM-attacked examples! And even though it wasn’t a targeted
    attack, it was most effective toward correctly masked faces. So hypothetically,
    if perpetrators managed to cause this level of signal distortion or interference,
    they would severely undermine the security companies’ ability to monitor mask
    compliance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code also outputs *Figure 13.5*, which shows some misclassifications caused
    by the FSGM attack. The attack pretty much evenly distributed noise throughout
    the images. It also shows that the image was only modified by a mean absolute
    error of 0.092, and since pixel values range between 0 and 1, this means 9.2%.
    If you were to calibrate attacks so that they are less detectable but still impactful,
    you must note that an `eps` of 0.1 causes a 9.2% mean absolute perturbation, which
    reduces accuracy to 44%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of a person  Description automatically generated with low confidence](img/B18406_13_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.5: Plot comparing FSGM-attacked versus the original images for the
    base classifier'
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of less detectable attacks, we will now learn about Carlini and Wagner
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Carlini and Wagner infinity norm attack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In 2017, **Carlini and Wagner** (**C&W**) employed three **norm-based distance
    metrics**: ![](img/B18406_13_002.png), ![](img/B18406_13_003.png), and ![](img/B18406_13_004.png),
    measuring the difference between the original and adversarial example. In other
    papers, these metrics had already been discussed, including the FSGM one. The
    innovation introduced by C&W was how these metrics were leveraged, using a gradient
    descent-based optimization algorithm designed to approximate a loss function minima.
    Specifically, to avoid getting stuck at a local minimum, they use multiple starting
    points in the gradient descent. And so that the process “yields a valid image,”
    it evaluates three methods to box-constrain the optimization problem. In this
    case, we want to find an adversarial example where the distances between that
    example and the original image are minimal, while also remaining realistic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All three C&W attacks (![](img/B18406_13_002.png), ![](img/B18406_13_003.png),
    and ![](img/B18406_13_004.png)) use the Adam optimizer to quickly converge. Their
    main difference is the distance metric, of which ![](img/B18406_13_004.png) is
    arguably the best one. It’s defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18406_13_009.png)'
  prefs: []
  type: TYPE_IMG
- en: And because it’s the maximum distance to any coordinate, you make sure that
    the adversarial example is not just “on average” minimally different but also
    not too different anywhere in the feature space. That’s what would make an attack
    less detectable!
  prefs: []
  type: TYPE_NORMAL
- en: 'Initializing C&W infinity norm attacks and generating adversarial examples
    with them is similar to FSGM. To initialize `CarliniLInfMethod`, we define optionally
    a `batch_size` (the default is `128`). Then, to `generate` an untargeted adversarial
    attack, the same applies as with FSGM. Only `X` is needed when untargeted, and
    `y` when targeted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now evaluate the C&W adversarial examples (`X_test_cw`) just as we
    did with FSGM. It’s exactly the same code, only with `fsgm` replaced with `cw`
    and different titles in `compare_image_predictions`. Just as with FSGM, the following
    code will yield a classification report and grid of images (*Figure 13.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As outputted by the preceding code, the C&W adversarial examples have a 92%
    accuracy with our base model. The drop is sufficient to render the model useless
    for its intended purpose. If the attacker disturbed a camera’s signal just enough,
    they could achieve the same results. And, as you can tell by *Figure 13.6*, the
    perturbation of 0.3% is tiny compared to FSGM, but it was sufficient to misclassify
    8%, including the four in the grid that seem apparent to the naked eye:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of a child  Description automatically generated with low confidence](img/B18406_13_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.6: Plot comparing C&W infinity norm-attacked versus the original
    images for the base classifier'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes it doesn’t matter if an attack goes undetected or not. The point of
    it is to make a statement, and that’s what adversarial patches can do.
  prefs: []
  type: TYPE_NORMAL
- en: Targeted adversarial patch attack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Adversarial Patches** (**APs**) are a robust, universal, and targeted method.
    You generate a patch you can either superimpose on an image, or print and physically
    place in a scene to trick a classifier into ignoring everything else in the scene.
    It is designed to work under a wide variety of conditions and transformations.
    Unlike other adversarial example generation approaches, there’s no intention of
    camouflaging the attack because, essentially, you replace a detectable portion
    of the scene with the patch. The method works by leveraging a variant of **Expectation
    Over Transformation** (**EOT**), which trains images over transformations of a
    given patch on different locations of an image. What it learns is the patch that
    fools the classifier the most, given the training examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This method requires more parameters and steps than FSGM and C&W. For starters,
    we will use `AdversarialPatchNumpy`, which is the variant that works with any
    neural network image or video classifier. There’s also one for TensorFlow v2,
    but our base classifier is a `KerasClassifier`. The first argument is the classifier
    (`base_classifier`), and the other ones we will define are optional but highly
    recommended. The scaling ranges `scale_min` and `scale_max` are particularly important
    because they define how big can patches be in relation to the images – in this
    case, we want to test no smaller than 40% and no larger than 70%. Besides that,
    it makes sense to define a target class (`target`). In this case, we want the
    patch to target the “Correct” class. For the `learning_rate` and max iterations
    (`max_iter`), we use the defaults but note that these can be tuned to improve
    patch adversarial effectiveness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We don’t want the patch generation algorithm to waste time testing patches
    everywhere in images, so we can direct this effort by using a Boolean mask. This
    mask tells it where it can center the patch. To make the mask, we start by creating
    an array of zeros, 128 × 128\. Then we place ones in the rectangular area between
    pixels 80–93 and 45–84, which loosely corresponds to cover the center of the mouth
    area in most of the images. Lastly, we expand the array’s dimensions so that it’s
    `(1, W, H)` and convert it to a Boolean. Then we can proceed to `generate` patches
    using the small-size test dataset samples and the mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now plot the patch with the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produced the image in *Figure 13.7*. As expected, it has
    plenty of the shades of blue found in masks. It also has bright red and yellow
    hues, mostly missing from training examples, which confuse the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated with low confidence](img/B18406_13_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.7: AP generated image to misclassify as correctly masked'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike other methods, `generate` didn’t produce adversarial examples but a
    single patch, which is an image we can then place on top of images to create adversarial
    examples. This task is performed with `apply_patch`, which takes the original
    examples `X_test_smsample` and a scale; we will use 55%. It is also recommended
    to use a `mask` that will make sure the patch is applied where it makes more sense
    – in this case, in the area around the mouth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it’s time to evaluate our attack and examine some misclassifications. We
    will do exactly as before and reuse the code that produced *Figure 13.5* and *Figure
    13.7*, except we replace the variables so that they have `ap` and a corresponding
    title:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code yields the accuracy result of our attack at 65%, which is
    quite good considering how few examples it was trained on. AP needs more data
    than other methods. Targeted attacks, in general, need more examples to understand
    how to best target one class. The preceding code also produced the grid of images
    in *Figure 13.8*, which demonstrates how, hypothetically, if people walked around
    holding a cardboard patch in front of their face, they could easily fool the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing person, posing, different, same  Description automatically
    generated](img/B18406_13_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.8: Plot comparing AP-attacked versus the original images for base
    classifier'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have studied three attack methods but haven’t yet tackled how to
    defend against these attacks. We will explore a couple of solutions next.
  prefs: []
  type: TYPE_NORMAL
- en: Defending against targeted attacks with preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are five broad categories of adversarial defenses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing**: changing the model’s inputs so that they are harder to attack.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training**: training a new robust model that is designed to overcome attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detection**: detecting attacks. For instance, you can train a model to detect
    adversarial examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer**: modifying model architecture and training so that it’s more
    robust – this may include techniques such as distillation, input filters, neuron
    pruning, and unlearning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Postprocessing**: changing model outputs to overcome production inference
    or model extraction attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Only the first four defenses work with evasion attacks, and in this chapter,
    we will only cover the first two: **preprocessing** and **adversarial training**.
    FGSM and C&W can be defended easily with either of these, but an AP is tougher
    to defend against, so it might require a stronger **detection** or **transformer**
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we defend, we must create a targeted attack. We will employ **Projected
    Gradient Descent** (**PGD**), which is a strong attack very similar in output
    to FSGM – that is, it produces noisy images. We won’t explain PGD in detail here
    but what is important to note is, like FSGM, it is regarded as a **first-order
    adversary** because it leverages first-order information about a network (due
    to gradient descent). Also, experiments prove that robustness against PGD ensures
    robustness against any first-order adversary. Specifically, PGD is a strong attack,
    so it makes for conclusive benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a targeted attack against the correctly masked class, it’s best that
    we only select examples that aren’t correctly masked (`x_test_notmasked`) and
    their corresponding labels (`y_test_notmasked`) and predicted probabilities (`y_test_notmasked_prob`).
    Then, we want to create an array with the class (`Correct`) that we want to generate
    adversarial examples for (`y_test_masked`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We initialize `ProjectedGradientDescent` as we did with FSGM, except we will
    set the maximum perturbation (`eps`), attack step size (`eps_step`), maximum iterations
    (`max_iter`), and `targeted=True`. Precisely because it is targeted, we will set
    both `X` and `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s evaluate the PGD attack as we did before, but this time, let’s plot
    the confusion matrix (`plot_conf_matrix=True`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![Chart  Description automatically generated](img/B18406_13_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.9: Confusion matrix for PGD attacked examples evaluated against the
    base classifier'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s run `compare_image_prediction` to see some random misclassifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code plots the grid of images in *Figure 13.10*. The mean absolute
    perturbation is the highest we’ve seen so far at 14.7%, and all unmasked faces
    in the grid are classified as correctly masked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of a person  Description automatically generated with medium confidence](img/B18406_13_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.10: Plot comparing PGD-attacked versus original images for the base
    classifier'
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy cannot get worse, and the images are grainy beyond repair. So how
    can we combat noise? If you recall, we have dealt with this problem before. In
    *Chapter 7*, *Visualizing Convolutional Neural Networks*, **SmoothGrad** improved
    saliency maps by averaging the gradients. It’s a different application but the
    same principle – just as with a human, a noisy saliency map is more challenging
    to interpret than a smooth one, and a grainy image is much more challenging for
    a model to interpret than a smooth one.
  prefs: []
  type: TYPE_NORMAL
- en: '**Spatial smoothing** is just a fancy way of saying blur! However, what’s novel
    about it being introduced as an adversarial defense method is that the proposed
    implementation (`SpatialSmoothing`) calls for using the median and not the mean
    in a sliding window. The `window_size` is configurable, and it is recommended
    to adjust it where it is most useful as a defense. Once the defense has been initialized,
    you plug in the adversarial examples (`X_test_pgd`). It will output spatially
    smoothed adversarial examples (`X_test_pgd_ss`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can take the blurred adversarial examples produced and evaluate them
    as we did before – first, with `evaluate_multiclass_mdl` to get predicted labels
    (`y_test_pgd_ss_pred`) and probabilities (`y_test_pgd_ss_prob`) and the output
    of some predictive performance metrics. With `compare_image_predictions` to plot
    a grid of images, let’s use `use_misclass=False` to compare properly classified
    images – in other words, the adversarial examples that were defended successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code yields an accuracy of 54%, which is much better than 0% before
    the spatial smoothing defense. It also produces *Figure 13.11*, which demonstrates
    how blur effectively thwarted the PGD attack. It even halved the mean absolute
    perturbation!
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of people  Description automatically generated with low confidence](img/B18406_13_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.11: Plot comparing spatially smoothed PGD-attacked images versus
    the original images for the base classifier'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will try another defense method in our toolbox: adversarial training!'
  prefs: []
  type: TYPE_NORMAL
- en: Shielding against any evasion attack by adversarial training of a robust classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 7*, *Visualizing Convolutional Neural Networks*, we identified a
    garbage image classifier that would likely perform poorly in the intended environment
    of a municipal recycling plant. The abysmal performance on out-of-sample data
    was due to the classifier being trained on a large variety of publicly available
    images that don’t match the expected conditions, or the characteristics of materials
    that are processed by a recycling plant. The chapter’s conclusion called for training
    a network with images that represent their intended environment to make for a
    more robust model.
  prefs: []
  type: TYPE_NORMAL
- en: For model robustness, training data variety is critical, but only if it represents
    the intended environment. In statistical terms, it’s a question of using samples
    for training that accurately depict the population so that a model learns to classify
    them correctly. For adversarial robustness, the same principles apply. If you
    augment data to include plausible examples of adversarial attacks, the model will
    learn to classify them. That’s what adversarial training is in a nutshell.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning researchers in the adversarial robustness field suggest this
    form of defense is very effective against any kind of evasion attack, essentially
    shielding it. That being said, it’s not impervious. Its effectiveness is contingent
    on using the right kind of adversarial examples in training, the optimal hyperparameters,
    and so forth. There are some guidelines outlined by researchers, such as increasing
    the number of neurons in the hidden layers and using PGD or BIM to produce adversarial
    examples for the training. **BIM** stands for **Basic Iterative Method**. It’s
    like FSGM but not fast because it iterates to approximate the best adversarial
    example within a ![](img/B18406_13_010.png)-neighborhood for the original image.
    The `eps` attribute bounds this neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training a robust model can be very resource-intensive. It is not required
    because we can download one already trained for us, but it’s important to understand
    how to perform this with ART. We will explain these steps to give the option of
    completing the model training with ART. Otherwise, just skip the steps and download
    the trained model. The `robust_model` is very much like the `base_model` except
    we use equal-sized filters in the four convolutional (`Conv2D`) layers. We do
    this to decrease complexity to counter the complexity we add by quadrupling the
    neurons in the first hidden (`Dense`) layer, as suggested by the machine learning
    researchers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `summary()` in the preceding code outputs the following. You can see that
    trainable parameters total around 3.6 million – similar to the base model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can adversarially train the model by first initializing a new `KerasClassifier`
    with the `robust_model`. Then, we initialize a `BasicIterativeMethod` attack on
    this classifier. Lastly, we initialize `AdversarialTrainer` with the `robust_classifier`
    and the BIM attack and `fit` it. Please note that we saved the BIM attack into
    a variable called `attacks` because this could be a list of ART attacks instead
    of a single one. Also, note that `AdversarialTrainer` has an attribute called
    `ratio`. This attribute determines what percentage of the training examples are
    adversarial examples. This percentage dramatically impacts the effectiveness of
    adversarial attacks. If it’s too low, it might not perform well with adversarial
    examples, and if it’s too high, it might perform less effectively with non-adversarial
    examples. If we run the `trainer`, it will likely take many hours to complete,
    so don’t get alarmed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If you didn’t train the `robust_classifier`, you can download a pretrained
    `robust_model` and initialize the `robust_classifier` with it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s evaluate the `robust_classifier` against the original test dataset
    using `evaluate_multiclass_mdl`. We set `plot_conf_matrix=True` to see the confusion
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code outputs the confusion matrix and performance metrics in
    *Figure 13.12*. It’s 1.8% less accurate than the base classifier. Most of the
    misclassifications are with correctly masked faces getting classified as incorrectly
    masked. There’s certainly a trade-off when choosing a 50% adversarial example
    ratio, or perhaps we can tune the hyperparameters or the model architecture to
    improve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, treemap chart  Description automatically generated](img/Image15514.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.12: Robust classifier confusion metrics and performance metrics'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how the robust model fares against adversarial attacks. Let’s use
    `FastGradientMethod` again, but this time, replace `base_classifier` with `robust_classifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can employ `evaluate_multiclass_mdl` and `compare_image_predictions`
    to measure and observe the effectiveness of our attack, but this time against
    the `robust_classifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![A collage of a baby  Description automatically generated with low confidence](img/B18406_13_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.13: Plot comparing FSGM-attacked versus the original images for the
    robust classifier'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have evaluated the robustness of models but only against one attack
    strength, without factoring in possible defenses, thus evaluating its robustness.
    In the next section, we will study a method that does this.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating adversarial robustness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s necessary to test your systems in any engineering endeavor to see how vulnerable
    they are to attacks or accidental failures. However, security is a domain where
    you must stress-test your solutions to ascertain what level of attacks are needed
    to make your system break down beyond an acceptable threshold. Furthermore, figuring
    out what level of defense is needed to curtail an attack is useful information
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing model robustness with attack strength
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now have two classifiers we can compare against an equally strengthened attack,
    and we try different attack strengths to see how they fare across all of them.
    We will use FSGM because it’s fast, but you could use any method!
  prefs: []
  type: TYPE_NORMAL
- en: 'The first attack strength we can assess is no attack strength. In other words,
    what is the classification accuracy against the test dataset with no attack? We
    already had stored the predicted labels for both the base (`y_test_pred`) and
    robust (`y_test_robust_pred`) models, so this is easy to obtain with scikit-learn’s
    `accuracy_score` metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now iterate across a range of attack strengths (`eps_range`) between
    0.01 and 0.9\. Using `linspace`, we can generate 9 values between 0.01 and 0.09
    and 9 values between 0.1 and 0.9, and `concatenate` them into a single array.
    We will test attacks for these 18 `eps` values by `for`-looping through all of
    them, attacking each model, and retrieving the post-attack accuracies with `evaluate`.
    The respective accuracies are appended to two lists (`accuracy_base`, `accuracy_robust`).
    And after the `for` loop, we prepend 0 to the `eps_range` to account for the accuracies
    prior to any attacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can plot the accuracies for both classifiers across all attack strengths
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates *Figure 13.14*, which demonstrates that the robust
    model performs better between attack strengths of 0.02 and 0.3 but then does consistently
    about 10% worse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/Image15613.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.14: Accuracy measured for the robust and base classifiers at different
    FSGM attack strengths'
  prefs: []
  type: TYPE_NORMAL
- en: One thing that *Figure 13.14* fails to account for is defenses. For example,
    if hospital cameras were constantly jammed or tampered with, the security company
    would be remiss not to defend their models. The easiest way to do so for this
    kind of attack is with some sort of smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial training also produces an empirically robust classifier that you
    cannot guarantee will work under certain pre-defined circumstances, which is why
    there’s a need for certifiable defenses.
  prefs: []
  type: TYPE_NORMAL
- en: Mission accomplished
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The mission was to perform some adversarial robustness tests on their face mask
    model to determine if hospital visitors and staff can evade mandatory mask compliance.
    The base model performed very poorly on many evasion attacks, from the most aggressive
    to the most subtle.
  prefs: []
  type: TYPE_NORMAL
- en: You also looked at possible defenses to these attacks, such as spatial smoothing
    and adversarial retraining. And then, you explored ways to evaluate the robustness
    of your proposed defenses. You can now provide an end-to-end framework to defend
    against this kind of attack. That being said, what you did was only a proof of
    concept.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you can propose training a certifiably robust model against the attacks
    the hospitals expect to encounter the most. But first, you need the ingredients
    for a generally robust model. To this end, you will need to take all 210,000 images
    in the original dataset, make many variations on mask colors and types with them,
    and augment them even further with reasonable brightness, shear, and rotation
    transformations. Lastly, the robust model needs to be trained with several kinds
    of attacks, including several kinds of APs. These are important because they mimic
    the most common compliance evasion behavior of concealing faces with body parts
    or clothing items.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading this chapter, you should understand how attacks can be perpetrated
    on machine learning models and evasion attacks in particular. You should know
    how to perform FSGM, BIM, PGD, C&W, and AP attacks, as well as how to defend against
    them with spatial smoothing and adversarial training. Last but not least, you
    know how to evaluate adversarial robustness.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is the last one, and it outlines some ideas on what’s next
    for machine learning interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adnane Cabani, Karim Hammoudi, Halim Benhabiles, and Mahmoud Melkemi, 2020,
    *MaskedFace-Net - A dataset of correctly/incorrectly masked face images in the
    context of COVID-19*, Smart Health, ISSN 2352–6483, Elsevier: [https://doi.org/10.1016/j.smhl.2020.100144](https://doi.org/10.1016/j.smhl.2020.100144)
    (Creative Commons BY-NC-SA 4.0 license by NVIDIA Corporation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karras, T., Laine, S., and Aila, T., 2019, *A Style-Based Generator Architecture
    for Generative Adversarial Networks*. 2019 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), 4396–4405: [https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948)
    (Creative Commons BY-NC-SA 4.0 license by NVIDIA Corporation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Polyakov, A., 2019, Aug 6, *How to attack Machine Learning (Evasion, Poisoning,
    Inference, Trojans, Backdoors)* [blog post]: [https://towardsdatascience.com/how-to-attack-machine-learning-evasion-poisoning-inference-trojans-backdoors-a7cb5832595c](https://towardsdatascience.com/how-to-attack-machine-learning-evasion-poisoning-inference-trojans-backdoors-a7cb5832595c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carlini, N., & Wagner, D., 2017, *Towards Evaluating the Robustness of Neural
    Networks*. 2017 IEEE Symposium on Security and Privacy (SP), 39–57: [https://arxiv.org/abs/1608.04644](https://arxiv.org/abs/1608.04644)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown, T., Mané, D., Roy, A., Abadi, M., and Gilmer, J., 2017, *Adversarial
    Patch*. ArXiv: [https://arxiv.org/abs/1712.09665](https://arxiv.org/abs/1712.09665)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/inml](Chapter_13.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code107161072033138125.png)'
  prefs: []
  type: TYPE_IMG
