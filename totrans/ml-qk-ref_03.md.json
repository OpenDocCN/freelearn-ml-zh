["```py\n1 - (60% * 50% * 35% * 45%)                                                                                                                                                                              1- 0.04725 = 0.95275\n\n# Accuracy is close to 95%.\n```", "```py\n#importing Libraries\nfrom sklearn.utils import resample\n```", "```py\ndataset=[10,2\n```", "```py\n0,30,40,50,60,70,80,90,100]\n```", "```py\n#using \"resample\" function generate a bootstrap sample\nboot_samp = resample(dataset, replace=True, n_samples=5, random_state=1)\n```", "```py\n#extracting OOB sample\nOOB=[x for x in dataset if x not in boot_samp]\n```", "```py\nprint(boot_samp)\n```", "```py\n[60, 90, 100, 60, 10]\n```", "```py\nprint(OOB)\n```", "```py\n[20, 30, 40, 50, 70, 80]\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n#importing our parameter tuning dependencies\nfrom sklearn.model_selection import (cross_val_score, GridSearchCV,StratifiedKFold, ShuffleSplit )\n#importing our dependencies for Feature Selection\nfrom sklearn.feature_selection import (SelectKBest, RFE, RFECV)\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.cross_validation import ShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom collections import defaultdict\n# Importing our sklearn dependencies for the modeling\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import KFold\nfrom sklearn import metrics\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, \n classification_report, roc_curve, auc)\n```", "```py\ndata= pd.read_csv(\"breastcancer.csv\")\n```", "```py\ndata.info()\n```", "```py\ndata.head()\n```", "```py\ndata.diagnosis.unique()\n```", "```py\ndata.describe()\n```", "```py\ndata['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})\n\ndatas = pd.DataFrame(preprocessing.scale(data.iloc[:,1:32]))\ndatas.columns = list(data.iloc[:,1:32].columns)\ndatas['diagnosis'] = data['diagnosis']\n\ndatas.diagnosis.value_counts().plot(kind='bar', alpha = 0.5, facecolor = 'b', figsize=(12,6))\nplt.title(\"Diagnosis (M=1, B=0)\", fontsize = '18')\nplt.ylabel(\"Total Number of Patients\")\nplt.grid(b=True)\n```", "```py\ndata_mean = data[['diagnosis','radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean', 'compactness_mean', 'concavity_mean','concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n\nplt.figure(figsize=(10,10))\nfoo = sns.heatmap(data_mean.corr(), vmax=1, square=True, annot=True)\n```", "```py\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn import metrics\npredictors = data_mean.columns[2:11]\ntarget = \"diagnosis\"\nX = data_mean.loc[:,predictors]\ny = np.ravel(data.loc[:,[target]])\n# Split the dataset in train and test:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint ('Shape of training set : %i & Shape of test set : %i' % (X_train.shape[0],X_test.shape[0]) )\nprint ('There are very few data points so 10-fold cross validation should give us a better estimate')\n```", "```py\nparam_grid = {\n 'n_estimators': [ 25, 50, 100, 150, 300, 500], \n \"max_depth\": [ 5, 8, 15, 25],\n \"max_features\": ['auto', 'sqrt', 'log2'] \n } \n#use OOB samples (\"oob_score= True\") to estimate the generalization accuracy.\nrfc = RandomForestClassifier(bootstrap= True, n_jobs= 1, oob_score= True)\n#let's use cv=10 in the GridSearchCV call\n#performance estimation\n#initiate the grid \ngrid = GridSearchCV(rfc, param_grid = param_grid, cv=10, scoring ='accuracy')\n#fit your data before you can get the best parameter combination.\ngrid.fit(X,y)\ngrid.cv_results_\n```", "```py\n# Let's find out the best scores, parameter and the estimator from the gridsearchCV\nprint(\"GridSearhCV best model:\\n \")\nprint('The best score: ', grid.best_score_)\nprint('The best parameter:', grid.best_params_)\nprint('The best model estimator:', grid.best_estimator_)\n```", "```py\n# model = RandomForestClassifier() with optimal values\nmodel = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n max_depth=8, max_features='sqrt', max_leaf_nodes=None,\n min_impurity_decrease=0.0, min_impurity_split=None,\n min_samples_leaf=1, min_samples_split=2,\n min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=1,\n oob_score=True, random_state=None, verbose=0, warm_start=False)\nmodel.fit(X_train, y_train)\n```", "```py\nprint(\"Performance Accuracy on the Testing data:\", round(model.score(X_test, y_test) *100))\n```", "```py\n#Getting the predictions for X\ny_pred = model.predict(X_test)\nprint('Total Predictions {}'.format(len(y_pred)))\n```", "```py\ntruth = pd.DataFrame(y_test, columns= ['Truth'])\npredictions = pd.DataFrame(y_pred, columns= ['Predictions'])\nframes = [truth, predictions]\n_result = pd.concat(frames, axis=1)\nprint(_result.shape)\n_result.head()\n```", "```py\n# 10 fold cross-validation with a Tree classifier on the training dataset# 10 fold \n#splitting the data, fitting a model and computing the score 10 consecutive times\ncv_scores = []\nscores = cross_val_score(rfc, X_train, y_train, cv=10, scoring='accuracy')\ncv_scores.append(scores.mean())\ncv_scores.append(scores.std())\n\n#cross validation mean score\nprint(\"10 k-fold cross validation mean score: \", scores.mean() *100)\n```", "```py\n# printing classification accuracy score rounded\nprint(\"Classification accuracy: \", round(accuracy_score(y_test, y_pred, normalize=True) * 100))\n```", "```py\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred) \nplt.figure(figsize=(12,6))\nax = plt.axes()\nax.set_title('Confusion Matrix for both classes\\n', size=21)\nsns.heatmap(cm, cmap= 'plasma',annot=True, fmt='g') # cmap\nplt.show()\n```", "```py\n# The classification Report\ntarget_names = ['Benign [Class 0]', 'Malignant[Class 1]']\nprint(classification_report(y_test, y_pred, target_names=target_names))\n```", "```py\ny_pred_proba = model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"curve, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n```", "```py\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n```", "```py\nparam_grid = {\n 'n_estimators': [ 25, 50, 100, 150, 300, 500], # the more parameters, the more computational expensive\n \"max_depth\": [ 5, 8, 15, 25],\n \"max_features\": ['auto', 'sqrt', 'log2'] \n }\ngbm = GradientBoostingClassifier(learning_rate=0.1,random_state=10,subsample=0.8)\n#performance estimation\n#initiate the grid \ngrid = GridSearchCV(gbm, param_grid = param_grid, cv=10, scoring ='accuracy')\n#fit your data before you can get the best parameter combination.\ngrid.fit(X,y)\ngrid.cv_results_     \n```", "```py\n#Let's find out the best scores, parameter and the estimator from the gridsearchCV\nprint(\"GridSearhCV best model:\\n \")\nprint('The best score: ', grid.best_score_)\nprint('The best parameter:', grid.best_params_)\nprint('The best model estimator:', grid.best_estimator_)\n```", "```py\nmodel2 = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n learning_rate=0.1, loss='deviance', max_depth=5,\n max_features='sqrt', max_leaf_nodes=None,\n min_impurity_decrease=0.0, min_impurity_split=None,\n min_samples_leaf=1, min_samples_split=2,\n min_weight_fraction_leaf=0.0, n_estimators=150,\n presort='auto', random_state=10, subsample=0.8, verbose=0,\n warm_start=False)\nmodel2.fit(X_train, y_train) \n\nprint(\"Performance Accuracy on the Testing data:\", round(model2.score(X_test, y_test) *100))\n```", "```py\n#getting the predictions for X\ny_pred2 = model2.predict(X_test)\nprint('Total Predictions {}'.format(len(y_pred2)))\n```", "```py\ntruth = pd.DataFrame(y_test, columns= ['Truth'])\npredictions = pd.DataFrame(y_pred, columns= ['Predictions'])\nframes = [truth, predictions]\n_result = pd.concat(frames, axis=1)\nprint(_result.shape)\n_result.head()\n```", "```py\ncv_scores = []\n\nscores2 = cross_val_score(gbm, X_train, y_train, cv=10, scoring='accuracy')\ncv_scores.append(scores2.mean())\ncv_scores.append(scores2.std())\n\n#cross validation mean score \nprint(\"10 k-fold cross validation mean score: \", scores2.mean() *100)\n```", "```py\n#printing classification accuracy score rounded\nprint(\"Classification accuracy: \", round(accuracy_score(y_test, y_pred2, normalize=True) * 100))\n```", "```py\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred2)\nplt.figure(figsize=(12,6))\nax = plt.axes()\nax.set_title('Confusion Matrix for both classes\\n', size=21)\nsns.heatmap(cm, cmap= 'plasma',annot=True, fmt='g') # cmap\nplt.show()\n```"]