<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Machine Learning in Computer Vision</h1>
                
            
            <article>
                
<p class="calibre2"> In the previous chapters, we learned about a number of algorithms for object detection and tracking. We learned how to use color-based algorithms, such as Mean Shift and CAM Shift, in conjunction with histograms and back-projection images to locate an object in an image with incredible speed. We also learned about template matching and how it can be used to find objects with a known template of pixels in an image. All of these algorithms rely in one way or another on image properties, such as brightness or color, that are easily affected by a change in lighting of the environment. Based on these facts, we moved on to learn about algorithms that are based on knowledge about significant areas in an image, called <strong class="calibre4">keypoints</strong> or <strong class="calibre4">features</strong>. We learned about many edge- and keypoint-detection algorithms and how to extract descriptors for those keypoints. We also learned about descriptor matchers and how to detect an object in an image using good matches of descriptors extracted from an image of the object of interest and the scene where we're looking for that object.</p>
<p class="calibre2">In this chapter, we're going to take one big step forward and learn about algorithms that can be used to extract a model from a large number of images of an object, and later use that model to detect an object in an image or simply classify an image. Such algorithms are the meeting point of machine learning algorithms and computer vision algorithms. Anyone familiar with artificial intelligence and machine learning algorithms in general will have an easy time proceeding with this chapter, even if they are not fluent in the exact algorithms and examples presented in this chapter. However, those who are totally new to such concepts will probably need to grab another book, preferably about machine learning, to familiarize themselves with algorithms, such as <strong class="calibre4">support vector machines</strong> (<strong class="calibre4"><span class="calibre12">SVM</span></strong>)<span class="calibre12">,</span> <strong class="calibre4">artificial neural networks</strong> (<strong class="calibre4"><span class="calibre12">ANN</span></strong>)<span class="calibre12">, cascade classification, and deep learning, which we'll be learning about in this chapter.</span></p>
<p class="calibre2"/>
<p class="calibre2">In this chapter, we'll look at the following:</p>
<ul class="calibre10">
<li class="calibre11">How to train and use SVM for classification</li>
<li class="calibre11">Using HOG and SVM for image classification</li>
<li class="calibre11">How to train and use ANN for prediction</li>
<li class="calibre11">How to train and use Haar or LBP cascade classifiers for real-time object detection</li>
<li class="calibre11">How to use pre-trained models from third-party deep learning frameworks</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Technical requirements</h1>
                
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">An IDE to develop C++ or Python applications</li>
<li class="calibre11">The OpenCV library</li>
</ul>
<p class="calibre2">Refer to <span class="calibre12"><a target="_blank" href="part0030.html#SJGS0-15c05657f8254d318ea883ef10fc67f4" class="calibre9">Chapter 2</a>, <em class="calibre7">Getting Started with OpenCV</em></span> for more information about how to set up a personal computer and make it ready for developing computer vision applications using the OpenCV library.</p>
<p class="calibre2">You can use this URL to download the source codes and examples for this chapter: <a href="https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter08" class="calibre9"><span>https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter08</span></a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Support vector machines</h1>
                
            
            <article>
                
<p class="calibre2">To put it as simply as possible, SVMs are used for creating a model from a labeled set of training samples that can be used to predict the label of new samples. For instance, assume we have a set of sample data belonging to two different groups. Each sample in our training dataset is a vector of floating-point numbers that can correspond to anything, such as a simple point in 2D or 3D space, and each sample is labeled with a number, such as 1, 2, or 3. Having such data, we can train an SVM model that can be used to predict the label of new 2D or 3D points. Let's think about another problem. Imagine we have the data of temperatures for 365 days in cities from all the continents in the world, and each vector of the 365 temperature values is labeled with 1 for Asia, 2 for Europe, 3 for Africa, and so on. We can use this data to train an SVM model that can be used to predict the continent of new vectors of temperature values (for 365 days) and associate them with a label. Even though these examples might not be useful in practice, they describe the concept of SVMs.</p>
<p class="calibre2"/>
<p class="calibre2">We can use the <kbd class="calibre13">SVM</kbd> class in OpenCV to train and use SVM models. Let's go through the usage of the <kbd class="calibre13">SVM</kbd> class in detail with a complete example:</p>
<ol class="calibre14">
<li value="1" class="calibre11">Since machine learning algorithms in OpenCV are included under the <kbd class="calibre13">ml</kbd> namespace, we need to make sure we include those namespaces in our code, so that the classes within them are easily accessible, using the following code:</li>
</ol>
<pre class="calibre30">using namespace cv; 
using namespace ml; </pre>
<ol start="2" class="calibre14">
<li value="2" class="calibre11">Create the training dataset. As we mentioned before, the training dataset is a set of vectors (samples) of floating-point numbers, and each vector is labeled with the class ID or category of that vector. Let's start with samples first:</li>
</ol>
<pre class="calibre30">const int SAMPLE_COUNT = 8; 
float samplesA[SAMPLE_COUNT][2] 
        = { {250, 50}, 
            {125, 100}, 
            {50, 50}, 
            {150, 150}, 
            {100, 250}, 
            {250, 250}, 
            {150, 50}, 
            {50, 250} }; 
Mat samples(SAMPLE_COUNT, 2, CV_32F, samplesA); </pre>
<p class="calibre31">In this example, each sample in our dataset of eight samples contains two floating-point values that can be demonstrated using a point on an image with an <em class="calibre7">x</em> and <em class="calibre7">y</em> value.</p>
<ol start="3" class="calibre14">
<li value="3" class="calibre11">We also need to create the label (or response) data, which obviously must be the same length as the samples. Here it is:</li>
</ol>
<pre class="calibre30">int responsesA[SAMPLE_COUNT] 
        = {2, 2, 2, 2, 1, 2, 2, 1}; 
Mat responses(SAMPLE_COUNT, 1, CV_32S, responsesA); </pre>
<p class="calibre31">As you can see, our samples are labeled with the <kbd class="calibre13">1</kbd> and <kbd class="calibre13">2</kbd> values, so we're expecting our model to be able to differentiate new samples between the given two groups of samples.</p>
<p class="calibre2"/>
<ol start="4" class="calibre14">
<li value="4" class="calibre11">OpenCV uses the <kbd class="calibre13">TrainData</kbd> class to simplify the preparation and usage of the training dataset. Here's how it's used:</li>
</ol>
<pre class="calibre30">Ptr&lt;TrainData&gt; data; 
SampleTypes layout = ROW_SAMPLE; 
data = TrainData::create(samples, 
                         layout, 
                         responses); </pre>
<p class="calibre31"><kbd class="calibre13">layout</kbd> in the preceding code is set to <kbd class="calibre13">ROW_SAMPLE</kbd> because each row in our dataset contains one sample. If the layout of the dataset was vertical, in other words, if each sample in the dataset was a column in the <kbd class="calibre13">samples</kbd> matrix, we'd need to set <kbd class="calibre13">layout</kbd> to <kbd class="calibre13">COL_SAMPLE</kbd>.</p>
<ol start="5" class="calibre14">
<li value="5" class="calibre11">Create the actual <kbd class="calibre13">SVM</kbd> class instance. This class in OpenCV implements various types of SVM classification algorithms, and they can be used by setting the correct parameters. In this example, we're going to use the most basic (and common) set of parameters for the <kbd class="calibre13">SVM</kbd> class, but to be able to use all possible features of this algorithm, make sure to go through the OpenCV <kbd class="calibre13">SVM</kbd> class documentation pages. Here's an example that shows how we can use SVM to perform a linear <em class="calibre26">n</em>-class classification:</li>
</ol>
<pre class="calibre30">Ptr&lt;SVM&gt; svm = SVM::create(); 
svm-&gt;setType(SVM::C_SVC); 
svm-&gt;setKernel(SVM::LINEAR); 
svm-&gt;setTermCriteria( 
            TermCriteria(TermCriteria::MAX_ITER + 
                         TermCriteria::EPS, 
                         100, 
                         1e-6)); </pre>
<ol start="6" class="calibre14">
<li value="6" class="calibre11">Train the SVM model using the <kbd class="calibre13">train</kbd> (or <kbd class="calibre13">trainAuto</kbd>) method, as seen here:</li>
</ol>
<pre class="calibre30">if(!svm-&gt;train(data)) 
{ 
    cout &lt;&lt; "training failed" &lt;&lt; endl; 
    return -1; 
} </pre>
<p class="calibre31">Based on the amount of data in our training samples dataset, the training process might take some time. In our case, it should be fast enough though, since we just used a handful of samples to train the model.</p>
<p class="calibre2"/>
<ol start="7" class="calibre14">
<li value="7" class="calibre11">We're going to use the SVM model to actually predict the label of new samples. Remember that each sample in our training set was a 2D point in an image. We're going to find the label of each 2D point in an image with a width and height of <kbd class="calibre13">300</kbd> pixels, and then color each pixel as green or blue, based on whether its predicted label is <kbd class="calibre13">1</kbd> or <kbd class="calibre13">2</kbd>. Here's how:</li>
</ol>
<pre class="calibre30">Mat image = Mat::zeros(300, 
                       300, 
                       CV_8UC3); 
Vec3b blue(255,0,0), green(0,255,0); 
for (int i=0; i&lt;image.rows; ++i) 
{ 
    for (int j=0; j&lt;image.cols; ++j) 
    { 
        Mat_&lt;float&gt; sampleMat(1,2); 
        sampleMat &lt;&lt; j, i; 
        float response = svm-&gt;predict(sampleMat); 
 
        if (response == 1) 
            image.at&lt;Vec3b&gt;(i, j)  = green; 
        else if (response == 2) 
            image.at&lt;Vec3b&gt;(i, j)  = blue; 
    } 
} </pre>
<ol start="8" class="calibre14">
<li value="8" class="calibre11">Go ahead and display the result of predictions, but, to be able to perfectly visualize the classification result of the SVM algorithm, it's better to draw the training samples we used to create the SVM model. Let's do it using the following code:</li>
</ol>
<pre class="calibre30">Vec3b black(0,0,0), white(255,255,255), color; 
for(int i=0; i&lt;SAMPLE_COUNT; i++) 
{ 
    Point p(samplesA[i][0], 
            samplesA[i][1]); 
    if (responsesA[i] == 1) 
        color = black; 
    else if (responsesA[i] == 2) 
        color = white; 
    circle(image, 
           p, 
           5, 
           color, 
           CV_FILLED); 
}</pre>
<p class="calibre2">The two types of samples (<kbd class="calibre13">1</kbd> and <kbd class="calibre13">2</kbd>) are drawn as black and white circles over the resultant image. The following diagram depicts the result of the complete SVM classification we just performed:</p>
<div class="cdpaligncenter"><img src="../images/00100.gif" class="calibre111"/></div>
<p class="calibre2">This demonstration is quite simple and, in reality, SVM can be used for much more complex classification problems, however, it literally shows the most essential aspect of SVM, which is the separation of various groups of data that are labeled the same. As you can see in the preceding image, the line separating the blue region from the green region is the best single line that can most efficiently separate the black dots and white dots on the image.</p>
<p class="calibre2">You can experiment with this phenomenon by updating the labels, or, in other words, the responses in the preceding example, as seen here:</p>
<pre class="calibre15">int responsesA[SAMPLE_COUNT] 
        = {2, 2, 2, 2, 1, 1, 2, 1}; </pre>
<p class="calibre2">Trying to visualize the results now will produce something similar to the following, which again depicts the most efficient line for separating the two groups of dots:</p>
<div class="cdpaligncenter"><img src="../images/00101.gif" class="calibre112"/></div>
<p class="calibre2">You can very easily add more classes to your data, or, in other words, have more labels for your training sample set. Here's an example:</p>
<pre class="calibre15">int responsesA[SAMPLE_COUNT] 
        = {2, 2, 3, 2, 1, 1, 2, 1};</pre>
<p class="calibre2">We can try visualizing the results again by adding a yellow color, for instance, for the third class region, and a gray dot for training samples that belong to that class. Here's the result of the same SVM example when used with three classes instead of two:</p>
<div class="cdpaligncenter"><img src="../images/00102.gif" class="calibre113"/></div>
<p class="calibre2">If you recall the example of 365 days from before, it is quite obvious that we can also add more dimensionality to the SVM model and not just classes, but it wouldn't be visually possible to display the results with a simple image such as the one in the preceding example.</p>
<p class="calibre2">Before continuing with the usage of the SVM algorithm for actual object detection and image classification, it's worth noting that, just like any other machine learning algorithm, having more samples in your dataset will result in a much better classification and higher accuracy, but it will also take more time to train the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Classifying images using SVM and HOG</h1>
                
            
            <article>
                
<p class="calibre2"><strong class="calibre4"><span class="calibre12">Histogram of Oriented Gradients</span></strong> (<strong class="calibre4"><span class="calibre12">HOG</span></strong>) is an algorithm that can be used to describe an image using a vector of floating-point descriptors that correspond to the oriented gradient values extracted from that image. The HOG algorithm is very popular and certainly worth reading about in detail to understand how it is implemented in OpenCV, but, for the purposes of this book and especially this section, we'll just mention that the number of the floating-point descriptors will always be the same when they are extracted from images that have exactly the same size with the same HOG parameters. To better understand this, recall that descriptors extracted from an image using the feature detection algorithms we learned about in the previous chapter can have different numbers of elements in them. The HOG algorithm, though, will always produce a vector of the same length if the parameters are unchanged across a set of images of the same size.</p>
<p class="calibre2"/>
<p class="calibre2">This makes the HOG algorithm ideal for being used in conjunction with SVM, to train a model that can be used to classify images. Let's see how it's done with an example. Imagine we have a set of images that contain images of a traffic sign in one folder, and anything but that specific traffic sign in another folder. The following pictures depicts the images in our samples dataset, separated by a black line in between:</p>
<div class="cdpaligncenter"><img src="../images/00103.jpeg" class="calibre114"/></div>
<p class="calibre2">Using images similar to the preceding samples, we're going to train the SVM model to detect whether an image is the traffic sign we're looking for or not. Let's start:</p>
<ol class="calibre14">
<li value="1" class="calibre11">Create an <kbd class="calibre13">HOGDescriptor</kbd> object. <kbd class="calibre13">HOGDescriptor</kbd>, or the HOG algorithm, is a special type of descriptor algorithm that relies on a given window size, block size, and various other parameters; for the sake of simplicity, we'll avoid all but the window size. The HOG algorithm's window size in our example is <kbd class="calibre13">128</kbd> by <kbd class="calibre13">128</kbd> pixels, which is set as seen here:</li>
</ol>
<pre class="calibre30">HOGDescriptor hog; 
hog.winSize = Size(128, 128); </pre>
<p class="calibre31">Sample images should have the same size as the window size, otherwise we need to use the <kbd class="calibre13">resize</kbd> function to make sure they are resized to the HOG window size later on. This guarantees the same descriptor size every time the HOG algorithm is used.</p>
<p class="calibre2"/>
<ol start="2" class="calibre14">
<li value="2" class="calibre11">As we just mentioned, the vector length of the descriptor extracted using <kbd class="calibre13">HOGDescriptor</kbd> will be constant if the image size is constant, and, assuming that image has the same size as <kbd class="calibre13">winSize,</kbd> you can get the descriptor length using the following code:</li>
</ol>
<pre class="calibre30">vector&lt;float&gt; tempDesc; 
hog.compute(Mat(hog.winSize, CV_8UC3), 
            tempDesc); 
int descriptorSize = tempDesc.size(); </pre>
<p class="calibre31">We'll use <kbd class="calibre13">descriptorSize</kbd> later on when we read the sample images.</p>
<ol start="3" class="calibre14">
<li value="3" class="calibre11">Assuming the images of the traffic sign are inside a folder called <kbd class="calibre13">pos</kbd> (for positive) and the rest inside a folder called <kbd class="calibre13">neg</kbd> (for negative), we can use the <kbd class="calibre13">glob</kbd> function to get the list of image files in those folders, as seen here:</li>
</ol>
<pre class="calibre30">vector&lt;String&gt; posFiles; 
glob("/pos", posFiles); 
 
vector&lt;String&gt; negFiles; 
glob("/neg", negFiles); </pre>
<ol start="4" class="calibre14">
<li value="4" class="calibre11">Create buffers to store the HOG descriptors for negative and positive sample images (from <kbd class="calibre13">pos</kbd> and <kbd class="calibre13">neg</kbd> folders). We also need an additional buffer for the labels (or responses), as seen in the following example:</li>
</ol>
<pre class="calibre30">int scount = posFiles.size() + negFiles.size(); 
 
Mat samples(scount, 
            descriptorSize, 
            CV_32F); 
 
Mat responses(scount, 
              1, 
              CV_32S); </pre>
<ol start="5" class="calibre14">
<li value="5" class="calibre11">We need to use the <kbd class="calibre13">HOGDescriptor</kbd> class to extract the HOG descriptors from positive images and store them in <kbd class="calibre13">samples</kbd>, as seen here:</li>
</ol>
<pre class="calibre30">for(int i=0; i&lt;posFiles.size(); i++) 
{ 
    Mat image = imread(posFiles.at(i)); 
    if(image.empty()) 
        continue; 
    vector&lt;float&gt; descriptors; 
    if((image.cols != hog.winSize.width) 
            || 
            (image.rows != hog.winSize.height)) 
    { 
        resize(image, image, hog.winSize); 
    } 
    hog.compute(image, descriptors); 
    Mat(1, descriptorSize, CV_32F, descriptors.data()) 
            .copyTo(samples.row(i)); 
    responses.at&lt;int&gt;(i) = +1; // positive 
} </pre>
<p class="calibre31">It needs to be noted that we have added <kbd class="calibre13">+1</kbd> for the labels (responses) of the positive samples. We'll need to use a different number, such as <kbd class="calibre13">-1</kbd>, when we label the negative samples.</p>
<ol start="6" class="calibre14">
<li value="6" class="calibre11">After the positive samples, we add the negative samples and their responses to the designated buffers:</li>
</ol>
<pre class="calibre30">for(int i=0; i&lt;negFiles.size(); i++) 
{ 
    Mat image = imread(negFiles.at(i)); 
    if(image.empty()) 
        continue; 
    vector&lt;float&gt; descriptors; 
    if((image.cols != hog.winSize.width) 
            || 
            (image.rows != hog.winSize.height)) 
    { 
        resize(image, image, hog.winSize); 
    } 
    hog.compute(image, descriptors); 
    Mat(1, descriptorSize, CV_32F, descriptors.data()) 
            .copyTo(samples.row(i + posFiles.size())); 
    responses.at&lt;int&gt;(i + posFiles.size()) = -1;
} </pre>
<ol start="7" class="calibre14">
<li value="7" class="calibre11">Similar to the example from the previous section, we need to form a <kbd class="calibre13">TrainData</kbd> object using <kbd class="calibre13">samples</kbd> and <kbd class="calibre13">responses</kbd> to be used with the <kbd class="calibre13">train</kbd> function. Here's how it's done:</li>
</ol>
<pre class="calibre30">Ptr&lt;TrainData&gt; tdata = TrainData::create(samples, 
                                         ROW_SAMPLE, 
                                         responses);</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<ol start="8" class="calibre14">
<li value="8" class="calibre11">Now, we need to train the SVM model as seen in the following example code:</li>
</ol>
<pre class="calibre30">Ptr&lt;SVM&gt; svm = SVM::create(); 
svm-&gt;setType(SVM::C_SVC); 
svm-&gt;setKernel(SVM::LINEAR); 
svm-&gt;setTermCriteria( 
            TermCriteria(TermCriteria::MAX_ITER + 
                         TermCriteria::EPS, 
                         10000, 
                         1e-6)); 
 
svm-&gt;train(tdata); </pre>
<p class="calibre2">After the training is completed, the SVM model is ready to be used for classifying images with the same size as the HOG window size (in this case, <kbd class="calibre13">128</kbd> by <kbd class="calibre13">128</kbd> pixels) using the <kbd class="calibre13">predict</kbd> method of the  <kbd class="calibre13">SVM</kbd> class. Here is how:</p>
<pre class="calibre15">Mat image = imread("image.jpg"); 
 
if((image.cols != hog.winSize.width) 
        || 
        (image.rows != hog.winSize.height)) 
{ 
    resize(image, image, hog.winSize); 
} 
 
vector&lt;float&gt; descs; 
hog.compute(image, descs); 
int result = svm-&gt;predict(descs); 
if(result == +1) 
{ 
    cout &lt;&lt; "Image contains a traffic sign." &lt;&lt; endl; 
} 
else if(result == -1) 
{ 
    cout &lt;&lt; "Image does not contain a traffic sign." &lt;&lt; endl; 
} </pre>
<p class="calibre2">In the preceding code, we simply read an image and resize it to the HOG window size. Then we use the <kbd class="calibre13">compute</kbd> method of the <kbd class="calibre13">HOGDescriptor</kbd> class, just like when we were training the model. Except, this time, we use the <kbd class="calibre13">predict</kbd> method to find the label of this new image. If the <kbd class="calibre13">result</kbd> equals <kbd class="calibre13">+1</kbd>, which was the label we assigned for traffic sign images when we trained the SVM model, then we know that the image is the image of a traffic sign, otherwise it's not.</p>
<p class="calibre2"/>
<p class="calibre2">The accuracy of the result completely depends on the quantity and quality of the data you have used to train your SVM model. This, in fact, is the case for each and every machine learning algorithm. The more you train your model, the more accurate it becomes.</p>
<div class="packt_infobox">This method of classification assumes that the input image is of the same characteristics as the trained images. Meaning, if the image contains a traffic sign, it is cropped similarly to the images we used to train the model. For instance, if you use an image that contains the traffic sign image we're looking for, but also contain much more, then the result will probably be incorrect.</div>
<p class="calibre2">As the amount of data in your training set increases, it will take more time to train your model. So, it's important to avoid retraining your model every time you want to use it. The <kbd class="calibre13">SVM</kbd> class allows you to save and load SVM models using the <kbd class="calibre13">save</kbd> and <kbd class="calibre13">load</kbd> methods. Here is how you can save a trained SVM model for later use and to avoid retraining it:</p>
<pre class="calibre15">svm-&gt;save("trained_svm_model.xml"); </pre>
<p class="calibre2">The file will be saved using the provided filename and extension (XML or any other  file type supported by OpenCV). Later, using the static <kbd class="calibre13">load</kbd> function, you can create an SVM object that contains the exact parameters and trained model. Here's an example:</p>
<pre class="calibre15">Ptr&lt;SVM&gt; svm = SVM::load("trained_svm_model.xml "); </pre>
<p class="calibre2">Try using the <kbd class="calibre13">SVM</kbd> class along with <kbd class="calibre13">HOGDescriptor</kbd> to train models that can detect and classify more types using images of various objects stored in different folders.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Training models with artificial neural networks</h1>
                
            
            <article>
                
<p class="calibre2">ANN can be used to train a model using a set of sample input and output vectors. ANN is a highly popular machine learning algorithm and the basis of many modern artificial intelligence algorithms that are used to train models for classification and correlation. Especially in computer vision, the ANN algorithm can be used along with a wide range of feature-description algorithms to learn about images of objects, or even faces of different people, and then used to detect them in images.</p>
<p class="calibre2"/>
<p class="calibre2">You can use the <kbd class="calibre13">ANN_MLP</kbd> class (which stands for <strong class="calibre4">artificial neural networks—</strong><strong class="calibre4">multi-layer</strong> <strong class="calibre4">perceptron</strong>) in OpenCV to implement ANN in your applications. The usage of this class is quite similar to that of the <kbd class="calibre13">SVM</kbd> class, so we're going to give a simple example to learn the differences and how it's used in practice, and we'll leave the rest for you to discover by yourself.</p>
<p class="calibre2">Creating the training samples dataset is exactly the same for all machine learning algorithms in OpenCV, or, to be precise, for all subclasses of the <kbd class="calibre13">StatsModel</kbd> class. The <kbd class="calibre13">ANN_MLP</kbd> class is no exception to this, so, just like with the <kbd class="calibre13">SVM</kbd> class, first we need to create a <kbd class="calibre13">TrainData</kbd> object that contains all the sample and response data that we need to use when training our ANN model, as seen here:</p>
<pre class="calibre15">SampleTypes layout = ROW_SAMPLE; 
data = TrainData::create(samples, 
                         layout, 
                         responses); </pre>
<p class="calibre2"><kbd class="calibre13">samples</kbd> and <kbd class="calibre13">responses</kbd>, in the preceding code, are both <kbd class="calibre13">Mat</kbd> objects that contain a number of rows that equals the number of all the training data we have in our dataset. As for the number of columns in them, let's recall that the ANN algorithm can be used to learn the relationship between vectors of input and output data. This means that the number of columns in the training input data, or <kbd class="calibre13">samples</kbd>, can be different from the number of columns in the training output data, or <kbd class="calibre13">responses</kbd>. We'll refer to the number of columns in <kbd class="calibre13">samples</kbd> as the number of features, and to the number of columns in <kbd class="calibre13">responses</kbd> as the number of classes. Simply put, we're going to learn the relationship of features to classes using a training dataset.</p>
<p class="calibre2">After taking care of the training dataset, we need to create an <kbd class="calibre13">ANN_MLP</kbd> object using the following code:</p>
<pre class="calibre15">Ptr&lt;ANN_MLP&gt; ann = ANN_MLP::create(); </pre>
<p class="calibre2">We have skipped all the customizations and used the default set of parameters. In the case that you need to use a fully customized <kbd class="calibre13">ANN_MLP</kbd> object, you need to set the activation function, termination criteria, and various other parameters in the <kbd class="calibre13">ANN_MLP</kbd> class. To learn more about this, make sure to refer to the OpenCV documentation and online resources about artificial neural networks.</p>
<p class="calibre2"/>
<p class="calibre2">Setting the correct layer sizes in the ANN algorithm requires experience and depends on the use case, but it can also be set using a few trial-and-error sessions. Here's how you can set the number and size of each layer in the ANN algorithm, and the <kbd class="calibre13">ANN_MLP</kbd> class to be specific:</p>
<pre class="calibre15">Mat_&lt;int&gt; layers(4,1); 
layers(0) = featureCount;    // input layer 
layers(1) = classCount * 4;  // hidden layer 1 
layers(2) = classCount * 2;  // hidden layer 2 
layers(3) = classCount;      // output layer 
ann-&gt;setLayerSizes(layers); </pre>
<p class="calibre2">In the preceding code, the number of rows in the <kbd class="calibre13">layers</kbd> object refers to the number of layers we want to have in our ANN. The first element in the <kbd class="calibre13">layers</kbd> object should contain the number of features in our dataset, and the last element in the <kbd class="calibre13">layers</kbd> object should contain the number of classes. Recall that the number of features equals the column count of <kbd class="calibre13">samples</kbd>, and the number of classes equals the column count of <kbd class="calibre13">responses</kbd>. The rest of the elements in the <kbd class="calibre13">layers</kbd> object contain the sizes of hidden layers.</p>
<p class="calibre2">Training the ANN model is done by using the <kbd class="calibre13">train</kbd> method, as seen in the following example:</p>
<pre class="calibre15">if(!ann-&gt;train(data)) 
{ 
    cout &lt;&lt; "training failed" &lt;&lt; endl; 
    return -1; 
} </pre>
<p class="calibre2">After the training is completed, we can use the <kbd class="calibre13">save</kbd> and <kbd class="calibre13">load</kbd> methods in exactly the same way as we saw before, to save the model for later use, or reload it from a saved file.</p>
<p class="calibre2">Using the model with the <kbd class="calibre13">ANN_MLP</kbd> class is also quite similar to the <kbd class="calibre13">SVM</kbd> class. Here's an example:</p>
<pre class="calibre15">Mat_&lt;float&gt; input(1, featureCount); 
Mat_&lt;float&gt; output(1, classCount); 
// fill the input Mat 
ann-&gt;predict(input, output); </pre>
<p class="calibre2">Choosing the right machine learning algorithm for each problem requires experience and knowledge about where the project is going to be used. SVM is quite simple, and suitable when we need to work with the classification of data and in the segmentation of groups of similar data, whereas ANN can be easily used to approximate a function between sets of input and output vectors (regression). Make sure to try out different machine learning problems to better understand where and when to use a specific algorithm.</p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The cascading classification algorithm</h1>
                
            
            <article>
                
<p class="calibre2">Cascading classification is another machine learning algorithm that can be used to train a model from many (hundreds, or even thousands) positive and negative image samples. As we explained earlier, a positive image refers to the image in an object of interest (such as a face, a car, or a traffic signal) that we want our model to learn and later classify or detect. On the other hand, a negative image corresponds to any arbitrary image that does not contain our object of interest. The model trained using this algorithm is referred to as a cascade classifier.</p>
<p class="calibre2">The most important aspect of a cascade classifier, as can be guessed from its name, is its cascading nature of learning and detecting an object using the extracted features. The most widely used features in cascade classifiers, and consequently cascade classifier types, are Haar and <strong class="calibre4">local binary pattern</strong> (<strong class="calibre4">LBP</strong>). In this section, we're going to learn how to use existing OpenCV Haar and LBP cascade classifiers to detect faces, eyes, and more in real-time, and then learn how to train our own cascade classifiers to detect any other objects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Object detection using cascade classifiers</h1>
                
            
            <article>
                
<p class="calibre2">To be able to use previously trained cascade classifiers in OpenCV, you can use the <kbd class="calibre13">CascadeClassifier</kbd> class and the simple methods it provides for loading a classifier from file or performing scale-invariant detection in images. OpenCV contains a number of trained classifiers to detect faces, eyes, and so on in real-time. If we browse to the OpenCV installation (or build) folder, it usually contains a folder called <kbd class="calibre13">etc</kbd>, which contains the following subfolders:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre13">haarcascades</kbd></li>
<li class="calibre11"><kbd class="calibre13">lbpcascades</kbd></li>
</ul>
<p class="calibre2"><kbd class="calibre13">haarcascades</kbd> contains pre-trained Haar cascade classifiers. <kbd class="calibre13">lbpcascades</kbd>, on the other hand, contains pre-trained LBP cascade classifiers. Haar cascade classifiers are usually slower than LBP cascade classifiers, but they also provide much better accuracy in most cases. To learn about the details of Haar and LBP cascade classifies, make sure to refer to the OpenCV documentation as well as online resources about Haar wavelets, Haar-like features, and local binary patterns. As we'll learn in the next section, LBP cascade classifiers are also a lot faster to train than Haar classifiers; with enough training data samples, you can reach a similar accuracy for both of the classifier types.</p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Under each one of the classifier folders we just mentioned, you can find a number of pre-trained cascade classifiers. You can load these classifiers and prepare them for object detection in real-time using the <kbd class="calibre13">load</kbd> method of the <kbd class="calibre13">CascadeClassifier</kbd> class, as seen in the following example:</p>
<pre class="calibre15">CascadeClassifier detector; 
if(!detector.load("classifier.xml")) 
{ 
    cout &lt;&lt; "Can't load the provided cascade classifier." &lt;&lt; endl; 
    return -1; 
} </pre>
<p class="calibre2">After a cascade classifier is successfully loaded, you can use the <kbd class="calibre13">detectMultiScale</kbd> method to detect objects in an image and return a vector containing the bounding rectangles of the detected objects, as seen in the following example:</p>
<pre class="calibre15">vector&lt;Rect&gt; objects; 
detector.detectMultiScale(frame, 
                          objects); 
 
 
for(int i=0; i&lt; objects.size(); i++) 
{ 
    rectangle(frame, 
              objects [i], 
              color, 
              thickness); 
} </pre>
<p class="calibre2"><kbd class="calibre13">color</kbd> and <kbd class="calibre13">thickness</kbd> are previously defined to affect the rectangle drawn for each detected object, as seen here:</p>
<pre class="calibre15">Scalar color = Scalar(0,0,255); 
int thickness = 2; </pre>
<p class="calibre2">Try loading the <kbd class="calibre13">haarcascade_frontalface_default.xml</kbd> classifier in the <kbd class="calibre13">haarcascades</kbd> folder, which comes preinstalled with OpenCV, to test the preceding example. Trying to run the preceding code with an image that contains a face would result in something similar to this:</p>
<div class="cdpaligncenter"><img src="../images/00104.gif" class="calibre115"/></div>
<p class="calibre2">The accuracy of the cascade classifier, as with any other machine learning model, depends completely on the quality and quantity of the training samples dataset. As it was mentioned before, cascade classifiers are widely popular, especially for real-time object detection. To be able to view the performance of cascade classifiers on any computer, you can use the following code:</p>
<pre class="calibre15">double t = (double)getTickCount(); 
 
detector.detectMultiScale(image, 
                          objects); 
 
t = ((double)getTickCount() - t)/getTickFrequency(); 
t *= 1000; // convert to ms </pre>
<p class="calibre2">The last line in the preceding code is used to convert the unit of the time measurement from seconds to milliseconds. You can use the following code to print out the result over the output image, in the lower-left corner for example:</p>
<pre class="calibre15">Scalar green = Scalar(0,255,0); 
int thickness = 2; 
double scale = 0.75; 
putText(frame, 
        "Took " + to_string(int(t)) + "ms to detect", 
        Point(0, frame.rows-1), 
        FONT_HERSHEY_SIMPLEX, 
        scale, 
        green, 
        thickness);</pre>
<p class="calibre2">This will produce an output image that contains text similar to what is seen in the following example:</p>
<div class="cdpaligncenter"><img src="../images/00105.jpeg" class="calibre116"/></div>
<p class="calibre2">Try different pre-trained cascade classifiers shipped with OpenCV and check their performance against each other. One very obvious observation will be the significantly faster detection speed of LBP cascade classifiers.</p>
<p class="calibre2">In the previous examples, we used only the default set of parameters needed for the <kbd class="calibre13">detectMultiScale</kbd> method of the <kbd class="calibre13">CascadeClassifier</kbd> class, however, to modify its behavior and, in some cases, to significantly improve its performance, you'll need to adjust a few more parameters, as seen in the following example:</p>
<pre class="calibre15">double scaleFactor = 1.1; 
int minNeighbors = 3; 
int flags = 0; // not used 
Size minSize(50,50); 
Size maxSize(500, 500); 
vector&lt;Rect&gt; objects; 
detector.detectMultiScale(image, 
                          objects, 
                          scaleFactor, 
                          minNeighbors, 
                          flags, 
                          minSize, 
                          maxSize); </pre>
<p class="calibre2">The <kbd class="calibre13">scaleFactor</kbd> parameter is used to specify the scaling of the image after each detection. This means resizing the image and performing the detection internally. This is in fact how multi-scale detection algorithms work. An image is searched for in an object, its size is reduced by the given <kbd class="calibre13">scaleFactor</kbd>, and the search is performed again. Size reduction is performed repeatedly until the image size is smaller than the classifier size. The results from all detections in all scales are then returned. The <kbd class="calibre13">scaleFactor</kbd> parameter must always contain a value greater than 1.0 (not equal to and not lower than). For higher sensitivity in multi-scale detection, you can set a value such as 1.01 or 1.05, which will lead to much longer detection times, and vice versa. The <kbd class="calibre13">minNeighbors</kbd> parameter refers to the grouping of detections that are near or similar to each other to retain a detected <kbd class="calibre13">object</kbd>.</p>
<p class="calibre2"/>
<p class="calibre2">The <kbd class="calibre13">flags</kbd> parameter is simply ignored in recent versions of OpenCV. As for the <kbd class="calibre13">minSize</kbd> and <kbd class="calibre13">maxSize</kbd> parameters, they are used to specify the minimum and maximum possible sizes of an object in an image. This can significantly increase the accuracy and speed of the <kbd class="calibre13">detectMultiScale</kbd> function, since detected objects that do not fall into the given size range are simply ignored and rescaling is done only until <kbd class="calibre13">minSize</kbd> is reached.</p>
<p class="calibre2"><kbd class="calibre13">detectMultiScale</kbd> has two other variations that we skipped for the sake of simplifying the examples, but you should check them out for yourself to learn more about cascade classifiers and multi-scale detection in general. Make sure to also search online for pre-trained classifiers by fellow computer vision developers and try using them in your applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Training cascade classifiers</h1>
                
            
            <article>
                
<p class="calibre2">As we mentioned previously, you can also create your own cascade classifiers to detect any other object if you have enough positive and negative sample images. Training a classifier using OpenCV involves taking a number of steps and using a number of special OpenCV applications, which we'll go through in this section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Creating samples</h1>
                
            
            <article>
                
<p class="calibre2">First things first, you need a tool called <kbd class="calibre13">opencv_createsamples</kbd> to prepare the positive image sample set. The negative image samples, on the other hand, are extracted automatically during the training from a provided folder containing arbitrary images that do NOT include the object of interest. The <kbd class="calibre13">opencv_createsamples</kbd> application can be found inside the <kbd class="calibre13">bin</kbd> folder of the OpenCV installation. It can be used to create the positive samples dataset, either by using a single image of the object of interest and applying distortions and transformations to it, or by using previously cropped or annotated images of the object of interest. Let's learn about the former case first.</p>
<p class="calibre2"/>
<p class="calibre2">Imagine you have the following image of a traffic sign (or any other object, for that matter) and you want to create a positive sample dataset using it:</p>
<div class="cdpaligncenter"><img src="../images/00106.jpeg" class="calibre117"/></div>
<p class="calibre2">You should also have a folder containing the source of the negative samples. As we mentioned previously, you need to have a folder containing arbitrary images that do not contain the object of interest. Let's assume we have some images similar to the following that we'll be using to create negative samples from:</p>
<div class="cdpaligncenter"><img src="../images/00107.jpeg" class="calibre118"/></div>
<p class="calibre2">Note that the size and aspect ratio of negative images, or to use the correct terminology, the background images, is not at all important. However, they must be at least as big as the minimum-detectable object (classifier size) and they must never contain images of the object of interest.</p>
<p class="calibre2"/>
<p class="calibre2">To train a proper cascade classifier, sometimes you need hundreds or even thousands of sample images that are distorted in different ways, which is not easy to create. In fact, gathering training data is one of the most time-consuming steps in creating a cascade classifier. The <kbd class="calibre13">opencv_createsamples</kbd> application can help with this problem by taking in the previous image of the object we're creating a classifier for and producing a positive samples dataset by applying distortions and using the background images. Here's an example of how it's used:</p>
<pre class="calibre15">opencv_createsamples -vec samples.vec -img sign.png -bg bg.txt  
    -num 250 -bgcolor 0 -bgthresh 10 -maxidev 50 
    -maxxangle 0.7 -maxyangle 0.7 -maxzangle 0.5 
    -w 32 -h 32 </pre>
<p class="calibre2">Here is a description of the parameters used in the preceding command:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre13">vec</kbd> is used to specify the positive samples file that will be created. In this case, it is the <kbd class="calibre13">samples.vec</kbd> file.</li>
<li class="calibre11"><kbd class="calibre13">img</kbd> is used to specify the input image that will be used to generate the samples. In our case, it's <kbd class="calibre13">sign.png</kbd>.</li>
<li class="calibre11"><kbd class="calibre13">bg</kbd> is used to specify the background's description file. A background's description file is a simple text file that contains the paths to all background images (each line in the background's description file contains the path to one background image). We have created a file named <kbd class="calibre13">bg.txt</kbd> and provided it to the <kbd class="calibre13">bg</kbd> parameter.</li>
<li class="calibre11">The <kbd class="calibre13">num</kbd> parameter determines the number of positive samples you want to generate using the given input image and backgrounds; 250, in our case. You can, but of course, use a higher or lower number, depending on the accuracy and duration of training that you require.</li>
<li class="calibre11"><kbd class="calibre13">bgcolor</kbd> can be used to define the background color in terms of its grayscale intensity. As you can see in our input image (the traffic sign image), the background color is black, thus the value of this parameter is zero.</li>
<li class="calibre11">The <kbd class="calibre13">bgthresh</kbd> parameter specifies the threshold of the accepted <kbd class="calibre13">bgcolor</kbd> parameter. This is especially useful in the case of compression artifacts that are common to some image formats and might cause slightly different pixel values for the same color. We have used 10 for the value of this parameter to allow a slight level of tolerance for background pixels.</li>
<li class="calibre11"><kbd class="calibre13">maxidev</kbd> can be used to set the maximum intensity deviation of the foreground pixel values while generating the samples. A value of 50 means the intensity of the foreground pixels can vary between their original values +/- 50.</li>
<li class="calibre11"><kbd class="calibre13">maxxangle</kbd>, <kbd class="calibre13">maxyangle</kbd>, and <kbd class="calibre13">maxzangle</kbd> correspond to the maximum possible rotation allowed in the <em class="calibre26">x</em>, <em class="calibre26">y</em>, and <em class="calibre26">z</em> directions when creating new samples. These values are in radians, for which we have provided 0.7, 0.7, and 0.5.</li>
<li class="calibre11">The <kbd class="calibre13">w</kbd> and <kbd class="calibre13">h</kbd> parameters define the width and height of the samples. We have used 32 for both of them since the object we're looking to train a classifier for fits in a square shape. These same values will be used later on, when training the classifier. Also note that this will be the minimum detectable size in your trained classifier later on.</li>
</ul>
<div class="packt_infobox">Besides the parameters in the preceding list, the <kbd class="calibre29">opencv_createsamples</kbd> application also accepts a <kbd class="calibre29">show</kbd> parameter that can be used to display the created samples, an <kbd class="calibre29">inv</kbd> parameter that can be used to invert the colors of samples, and a <kbd class="calibre29">randinv</kbd> parameter that can be used to set or unset the random inversion of pixels in samples.</div>
<p class="calibre2">Running the preceding command will produce the given number of samples by performing rotations and intensity changes to the foreground pixels. Here are some of the resultant samples:</p>
<div class="cdpaligncenter"><img src="../images/00108.gif" class="calibre20"/></div>
<p class="calibre2">Now that we have a positive samples vector file, produced by <kbd class="calibre13">opencv_createsamples</kbd>, and a folder that contains the background images along with a background's description file (<kbd class="calibre13">bg.txt</kbd> from the previous example), we can start the training of our cascade classifier, but before that, let's also learn about the second method of creating our positive samples vector, which is by extracting them from various annotated images that contain our object of interest.</p>
<p class="calibre2">This second method involves using another official OpenCV tool which is used for annotating positive samples in images. This tool is called <kbd class="calibre13">opencv_annotation</kbd> and it can be used to conveniently mark the areas in a number of images that contain our positive samples, or in other words, the objects, we're going to train a cascade classifier for them. The <kbd class="calibre13">opencv_annotation</kbd> tool produces an annotation text file (after a manual annotation of the objects) that can be used with the <kbd class="calibre13">opencv_createsamples</kbd> tool to produce a positive samples vector suitable for use with the OpenCV cascade training tool that we'll learn about in the next section.</p>
<p class="calibre2"/>
<p class="calibre2">Let's assume we have a folder containing images similar to the following:</p>
<div class="cdpaligncenter"><img src="../images/00109.jpeg" class="calibre20"/></div>
<p class="calibre2">All of these images are located in a single folder and all of them contain one or more samples of the traffic sign (the object of interest) that we're looking for. We can use the following command to start the <kbd class="calibre13">opencv_annotation</kbd> tool and manually annotate the samples:</p>
<pre class="calibre15">opencv_annotation --images=imgpath --annotations=anno.txt </pre>
<p class="calibre2">In the preceding command, <kbd class="calibre13">imgpath</kbd> must be replaced with the path (preferably the absolute path and with forward slashes) to the folder containing the images. <kbd class="calibre13">anno.txt</kbd>, or any other file name provided instead, will be filled with the annotation results, which can be used with <kbd class="calibre13">opencv_createsamples</kbd> to create a positive samples vector. Executing the preceding command will start the <kbd class="calibre13">opencv_annotation</kbd> tool and output the following text, which describes how to use the tool and its shortcut keys:</p>
<pre class="calibre15">* mark rectangles with the left mouse button, 
* press 'c' to accept a selection, 
* press 'd' to delete the latest selection, 
* press 'n' to proceed with next image, 
* press 'esc' to stop.</pre>
<p class="calibre2">Immediately after the preceding output, a window similar to the following will be displayed:</p>
<div class="cdpaligncenter"><img src="../images/00110.jpeg" class="calibre119"/></div>
<p class="calibre2">You can highlight an object using your mouse's left button, which will cause a red rectangle to be drawn. Pressing the <em class="calibre7">C</em> key will finalize the annotation and it will become red. Continue this process for the rest of the samples (if any) in the same image and press <em class="calibre7">N</em> to go to the next image. After all images are annotated, you can exit the application by pressing the <em class="calibre7">Esc</em> key.</p>
<div class="packt_infobox">In addition to the <kbd class="calibre29">-images</kbd> and <kbd class="calibre29">-annotations</kbd> parameters, the <kbd class="calibre29">opencv_annotation</kbd> tool also includes an optional parameter, called <kbd class="calibre29">-maxWindowHeight</kbd>, that can be used to resize images that are bigger than a given size. The resize factor in this case can be specified with another optional parameter called <kbd class="calibre29">-resizeFactor</kbd>.</div>
<p class="calibre2">The annotation file created by the <kbd class="calibre13">opencv_annotation</kbd> tool will look like the following:</p>
<pre class="calibre15">signs01.jpg 2 145 439 105 125 1469 335 185 180 
signs02.jpg 1 862 468 906 818 
signs03.jpg 1 1450 680 530 626 
signs04.jpg 1 426 326 302 298 
signs05.jpg 0 
signs06.jpg 1 1074 401 127 147 
signs07.jpg 1 1190 540 182 194 
signs08.jpg 1 794 460 470 488</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Each line in the annotations file contains the path to an image, followed by the number of objects of interest in that image followed by the <em class="calibre7">x</em>, <em class="calibre7">y</em>, width, and height values of the bounding rectangles of those objects. You can use the following command to produce a samples vector using this annotation text file:</p>
<pre class="calibre15">opencv_createsamples -info anno.txt -vec samples.vec -w 32 -h 32 </pre>
<p class="calibre2">Note that this time we used the <kbd class="calibre13">opencv_createsamples</kbd> tool with the <kbd class="calibre13">-info</kbd> parameter, which wasn't present when we used this tool to generate samples from an image and arbitrary backgrounds. We are now ready to train a cascade classifier that is capable of detecting the traffic sign we created the samples for.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Creating the classifier</h1>
                
            
            <article>
                
<p class="calibre2">The last tool we're going to learn about is called <kbd class="calibre13">opencv_traincascade</kbd>, which, as you can guess, is used to train cascade classifiers. If you have enough samples and background images, and if you have already taken care of the samples vector as it was described in the preceding section, then the only thing you need to do is to run the <kbd class="calibre13">opencv_traincascade</kbd> tool and wait for the training to be completed. Let's see an example training command and then go through the parameters in detail:</p>
<pre class="calibre15">opencv_traincascade -data classifier -vec samples.vec 
    -bg bg.txt -numPos 200 -numNeg 200 -w 32 -h 32 </pre>
<p class="calibre2">This is the simplest way of starting the training process, and only uses the mandatory parameters. All parameters used in this command are self-explanatory, except the <kbd class="calibre13">-data</kbd> parameter, which must be an existing folder that will be used to create the files required during the training process and the final trained classifier (called <kbd class="calibre13">cascade.xml</kbd>) will be created in this folder.</p>
<div class="packt_infobox"><kbd class="calibre29">numPos</kbd> cannot contain a number higher than the number of positive samples in your <kbd class="calibre29">samples.vec</kbd> file, however, <kbd class="calibre29">numNeg</kbd> can contain basically any number since the training process, will simply try to create random negative samples by extracting portions of the provided background images.</div>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">The <kbd class="calibre13">opencv_traincascade</kbd> tool will create a number of XML files in the folder set as the <kbd class="calibre13">-data</kbd> parameter, which must not be modified until the training process is completed. Here is a short description for each one of them:</p>
<ul class="calibre10">
<li class="calibre11">The <kbd class="calibre13">params.xml</kbd> file will contain the parameters used for training the classifier.</li>
<li class="calibre11"><kbd class="calibre13">stage#.xml</kbd> files are checkpoints that are created after each training stage is completed. They then can be used to resume the training later on if the training process was terminated for an unexpected reason.</li>
<li class="calibre11">The <kbd class="calibre13">cascade.xml</kbd> file is the trained classifier and the last file that will be created by the training tool. You can copy this file, rename it to something convenient (such as <kbd class="calibre13">trsign_classifier.xml</kbd> or something like that), and use it with the <kbd class="calibre13">CascadeClassifier</kbd> class, as we learned in the previous sections, to perform multi-scale object detection.</li>
</ul>
<p class="calibre2"><kbd class="calibre13">opencv_traincascade</kbd> is an extremely customizable and flexible tool, and you can easily modify its many optional parameters to make sure the trained classifier fits your needs. Here is a description of some of its most used parameters:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre13">numStages</kbd> can be used to set the number of stages used to train the cascade classifier. By default, <kbd class="calibre13">numStages</kbd> equals 20, but you can decrease this value to shorten the training time while sacrificing the accuracy or vice versa.</li>
<li class="calibre11">The <kbd class="calibre13">precalcValBufSize</kbd> and <kbd class="calibre13">precalcIdxBufSize</kbd> parameters can be used to increase or decrease the amount of memory used for various calculations during the training of the cascade classifier. You can modify these parameters to make sure the training process is performed with more efficiency.</li>
<li class="calibre11"><kbd class="calibre13">featureType</kbd> is one of the most important parameters of the training tool, and it can be used to set the type of the trained classifier to <kbd class="calibre13">HAAR</kbd> (default if ignored) or <kbd class="calibre13">LBP</kbd>. As mentioned before, LBP classifiers are trained much faster than Haar classifiers and their detection is also significantly faster, but they lack the accuracy of the Haar cascade classifiers. With a proper amount of training samples, you might be able to train an LBP classifier that can compete with a Haar classifier in terms of accuracy.</li>
</ul>
<p class="calibre2">For a complete list of parameters and their descriptions, make sure to refer to the OpenCV online documentation.</p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Using deep learning models</h1>
                
            
            <article>
                
<p class="calibre2">In recent years, there has been a huge improvement in the field of deep learning, or, to be precise, <strong class="calibre4">deep neural networks</strong> (<strong class="calibre4">DNN</strong>), and more and more libraries and frameworks are being introduced that use deep learning algorithms and models, especially for computer vision purposes such as object detection in real-time. You can use the most recent versions of the OpenCV library to read pre-trained models for the most popular DNN frameworks, such as Caffe, Torch, and TensorFlow, and use them for object detection and prediction tasks.</p>
<p class="calibre2">DNN-related algorithms and classes in OpenCV are all located under the <kbd class="calibre13">dnn</kbd> namespace, so, to be able to use them, you need to make sure to include the following in your code:</p>
<pre class="calibre15">using namespace cv; 
using namespace dnn; </pre>
<p class="calibre2">We're going to walk through the loading and use of a pre-trained model from the TensorFlow library in OpenCV for real-time object detection. This example demonstrates the basics of how to use deep neural networ models trained by a third-party library (TensorFlow in this case). So, let's start:</p>
<ol class="calibre14">
<li value="1" class="calibre11">Download a pre-trained TensorFlow model that can be used for object detection. For our example, make sure you download the latest version of <kbd class="calibre13">ssd_mobilenet_v1_coco</kbd> from the search, for official TensorFlow models online instead.</li>
</ol>
<p class="calibre31">Note that this link can possibly change in the future (maybe not soon, but it's worth mentioning), so, in case this happens, you need to simply search online for the <kbd class="calibre13">TensorFlow</kbd> model zoo, which, in <kbd class="calibre13">TensorFlow</kbd> terms, is a zoo containing the pre-trained object detection models.</p>
<ol start="2" class="calibre14">
<li value="2" class="calibre11">After downloading the <kbd class="calibre13">ssd_mobilenet_v1_coco</kbd> model package file, you need to extract it to a folder of your choice. You'll end up with the <kbd class="calibre13">frozen_inference_graph.pb</kbd> file in the folder where you extracted the model package, along with a few more files. You need to extract a text graph file from this model file before it can be used for real-time object detection in OpenCV. This extraction can be performed by using a script called <kbd class="calibre13">tf_text_graph_ssd.py</kbd>, which is a Python script that is included in the OpenCV installation by default and can be found in the following path:</li>
</ol>
<pre class="calibre30">opencv-source-files/samples/dnntf_text_graph_ssd.py</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre31">You can execute this script using the following command:</p>
<pre class="calibre30"><strong class="calibre1">tf_text_graph_ssd.py --input frozen_inference_graph.pb 
            --output frozen_inference_graph.pbtxt</strong> </pre>
<div class="packt_infobox">Note that the correct execution of this script totally depends on whether you have a correct TensorFlow installation on your computer or not.</div>
<ol start="3" class="calibre14">
<li value="3" class="calibre11">You should have the <kbd class="calibre13">frozen_inference_graph.pb</kbd> and <kbd class="calibre13">frozen_inference_graph.pbtxt</kbd> files, so we can start using them in OpenCV to detect objects. For this reason, we need to create a DNN <kbd class="calibre13">Network</kbd> object and read the model files into it, as seen in the following example:</li>
</ol>
<pre class="calibre30">Net network = readNetFromTensorflow( 
        "frozen_inference_graph.pb", 
        "frozen_inference_graph.pbtxt"); 
if(network.empty()) 
    { 
        cout &lt;&lt; "Can't load TensorFlow model." &lt;&lt; endl; 
        return -1; 
    } </pre>
<ol start="4" class="calibre14">
<li value="4" class="calibre11">After making sure the model is correctly loaded, you can use the following code to perform a real-time object detection in a frame read from the camera, an image, or a video file:</li>
</ol>
<pre class="calibre30">const int inWidth = 300; 
const int inHeight = 300; 
const float meanVal = 127.5; // 255 divided by 2 
const float inScaleFactor = 1.0f / meanVal; 
bool swapRB = true; 
bool crop = false; 
Mat inputBlob = blobFromImage(frame, 
                      inScaleFactor, 
                      Size(inWidth, inHeight), 
                      Scalar(meanVal, meanVal, meanVal), 
                      swapRB, 
                      crop); 
 
network.setInput(inputBlob); 
 
Mat result = network.forward();</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre31">It's worth noting that the values passed to the <kbd class="calibre13">blobFromImage</kbd> function completely depend on the model, and you should use the exact same values if you're using the same model from this example. The <kbd class="calibre13">blobFromImage</kbd> function will create a BLOB that is suitable for use with the deep neural network's prediction function, or, to be precise, the <kbd class="calibre13">forward</kbd> function.</p>
<ol start="5" class="calibre14">
<li value="5" class="calibre11">After the detection is complete, you can use the following code to extract the detected objects and their bounding rectangles, all into a single <kbd class="calibre13">Mat</kbd> object:</li>
</ol>
<pre class="calibre33">Mat detections(result.size[2], 
               result.size[3], 
               CV_32F, 
               result.ptr&lt;float&gt;()); </pre>
<ol start="6" class="calibre14">
<li value="6" class="calibre11">The <kbd class="calibre13">detections</kbd> object can be looped through to extract the individual detections that have an acceptable detection confidence level and draw the results on the input image. Here's an example:</li>
</ol>
<pre class="calibre30">const float confidenceThreshold = 0.5f; 
for(int i=0; i&lt;detections.rows; i++) 
{ 
    float confidence = detections.at&lt;float&gt;(i, 2); 
    if(confidence &gt; confidenceThreshold) 
    { 
        // passed the confidence threshold 
    } 
} </pre>
<p class="calibre31">The confidence level, which is the third element in each row of the <kbd class="calibre13">detections</kbd> object, can be adjusted to get more accurate results, but <kbd class="calibre13">0.5</kbd> should be a reasonable value for most cases, or at least for a start.</p>
<ol start="7" class="calibre14">
<li value="7" class="calibre11">After a detection passes the confidence criteria, we can extract the detected object ID and bounding rectangle and draw it on the input image, as seen here:</li>
</ol>
<pre class="calibre30">int objectClass = (int)(detections.at&lt;float&gt;(i, 1)) - 1; 
int left = static_cast&lt;int&gt;( 
            detections.at&lt;float&gt;(i, 3) * frame.cols); 
int top = static_cast&lt;int&gt;( 
            detections.at&lt;float&gt;(i, 4) * frame.rows); 
int right = static_cast&lt;int&gt;( 
            detections.at&lt;float&gt;(i, 5) * frame.cols); 
int bottom = static_cast&lt;int&gt;( 
            detections.at&lt;float&gt;(i, 6) * frame.rows); 
rectangle(frame, Point(left, top), 
          Point(right, bottom), Scalar(0, 255, 0)); 
String label = "ID = " + to_string(objectClass); 
if(objectClass &lt; labels.size()) 
    label = labels[objectClass]; 
int baseLine = 0; 
Size labelSize = getTextSize(label, FONT_HERSHEY_SIMPLEX, 
                             0.5, 2, &amp;baseLine); 
top = max(top, labelSize.height); 
 
rectangle(frame, 
          Point(left, top - labelSize.height), 
          Point(left + labelSize.width, top + baseLine), 
          white, 
                 CV_FILLED); 
 
putText(frame, label, Point(left, top), 
        FONT_HERSHEY_SIMPLEX, 0.5, red); </pre>
<p class="calibre2">In the preceding example, <kbd class="calibre13">objectClass</kbd> refers to the ID of the detected object, which is the second element in each row of the detection's object. The third, fourth, fifth, and sixth elements, on the other hand, correspond to the left, top, right, and bottom values of the bounding rectangle of each detected object. The rest of the code is simply drawing the results, which leaves the <kbd class="calibre13">labels</kbd> object. <kbd class="calibre13">labels</kbd> is a <kbd class="calibre13">vector</kbd> of <kbd class="calibre13">string</kbd> values that can be used to retrieve the human-readable text of each object ID. These labels, similar to the rest of the parameters we used in this example, are model-dependent. For instance, in our example case, the labels can be found here:</p>
<p class="calibre2"><a href="https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt" class="calibre9"><span>https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt</span></a></p>
<p class="calibre2">We have converted this into the following labels vector used in the preceding example:</p>
<pre class="calibre15">const vector&lt;string&gt; labels = { "person", "bicycle" ...}; </pre>
<p class="calibre2">The following image demonstrates the result of the object detection using a pre-trained TensorFlow model in OpenCV:</p>
<p class="calibre2"/>
<div class="cdpaligncenter"><img src="../images/00111.jpeg" class="calibre20"/></div>
<p class="calibre2">Using deep learning has proven to be highly efficient, especially when we need to train and detect multiple objects in real-time. Make sure to refer to the <kbd class="calibre13">TensorFlow</kbd> and OpenCV documentation for more about how to use pre-trained models, or how to train and retrain DNN models for an object that doesn't have an already trained model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">We started the final chapter of this book by learning about SVM models and how to train them to classify groups of similar data. We learned how SVM can be used in conjunction with the HOG descriptor to learn about one or more specific objects and then detect and classify them in new images. After learning about SVM models, we moved on to using ANN models, which offer much more power in cases where we have multiple columns in both the input and output of the training samples. This chapter also included a complete guide on how to train and use Haar and LBP cascade classifiers. We are now familiar with the usage of official OpenCV tools that can be used to prepare a training dataset from scratch and then train a cascade classifier using that dataset. Finally, we ended this chapter and this book by learning about the usage of pre-trained deep learning object detection models in OpenCV.</p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Questions</h1>
                
            
            <article>
                
<ol class="calibre14">
<li value="1" class="calibre11">What is the difference between the <kbd class="calibre13">train</kbd> and <kbd class="calibre13">trainAuto</kbd> methods in the <kbd class="calibre13">SVM</kbd> class?</li>
<li value="2" class="calibre11">Demonstrate the difference between the linear and histogram intersection.</li>
<li value="3" class="calibre11">How do you calculate the HOG descriptor size for a HOG window size of 128 x 96 pixels (the rest of the HOG parameters are untouched)?</li>
<li value="4" class="calibre11">How do you update an existing trained <kbd class="calibre13">ANN_MLP</kbd>, instead of training from scratch?</li>
<li value="5" class="calibre11">What is the required command (by using <kbd class="calibre13">opencv_createsamples</kbd>) to create a positive samples vector from a single image of a company logo? Assume we want to have 1,000 samples with a width of 24 and a height of 32, and by using default parameters for rotations and inversions.</li>
<li value="6" class="calibre11">What is the required command to train an LBP cascade classifier for the company logo from the previous question?</li>
<li value="7" class="calibre11">What is the default number of stages for training a cascade classifier in <kbd class="calibre13">opencv_traincascade</kbd>? How can we change it? What is the downside of increasing and decreasing the number of stages far beyond its default value?</li>
</ol>


            </article>

            
        </section>
    </body></html>