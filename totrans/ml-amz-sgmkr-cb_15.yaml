- en: '*Chapter 12*: DataRobot Python API'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第12章*：DataRobot Python API'
- en: Users can access DataRobot's capabilities using DataRobot's Python client package.
    This lets us ingest data, create machine learning projects, make predictions from
    models, and manage models programmatically. It is easy to see the advantages that
    **Application Programming Interfaces** (**APIs**) offer users. The integrated
    use of Python and DataRobot lets us leverage the AutoML capabilities DataRobot
    presents, all while exploiting the programmatic flexibility and potential that
    Python possesses.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以使用DataRobot的Python客户端包访问DataRobot的功能。这使得我们可以导入数据，创建机器学习项目，从模型中进行预测，并编程管理模型。API为用户提供的优势很容易看出。Python和DataRobot的集成使用使我们能够利用DataRobot提供的AutoML能力，同时利用Python具有的程序灵活性和潜力。
- en: 'In this chapter, we will use the DataRobot Python API to ingest data, create
    a project with models, evaluate the models, and make predictions against them.
    At a high level, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用DataRobot Python API来导入数据，创建包含模型的工程，评估模型，并对它们进行预测。从高层次上讲，我们将涵盖以下主题：
- en: Accessing the DataRobot API
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问DataRobot API
- en: Understanding the DataRobot Python client
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解DataRobot Python客户端
- en: Building models programmatically
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程构建模型
- en: Making predictions programmatically
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程进行预测
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For the analysis and modeling that will be carried out in this chapter, you
    will need access to the DataRobot software. Jupyter Notebook is crucial for this
    chapter as most of the interactions with DataRobot will be carried out from the
    console. Your Python version should be 2.7 or 3.4+. Now, let's look at the dataset
    that will be utilized in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章将要进行的分析和建模中，您将需要访问DataRobot软件。Jupyter Notebook对于本章至关重要，因为与DataRobot的大部分交互都将从控制台进行。您的Python版本应为2.7或3.4以上。现在，让我们看看本章将使用的数据库集。
- en: Check out the following video to see the Code in Action at [https://bit.ly/3wV4qx5](https://bit.ly/3wV4qx5).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，了解代码在[https://bit.ly/3wV4qx5](https://bit.ly/3wV4qx5)上的实际应用。
- en: Automobile Dataset
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 汽车数据集
- en: The automobile dataset can be accessed at the UCI Machine Learning Repository
    ( [https://archive.ics.uci.edu/ml/datasets/Automobile](https://archive.ics.uci.edu/ml/datasets/Automobile)).
    Each row in this dataset represents a specific automobile. The features (columns)
    describe its characteristics, risk rating, and associated normalized losses. Even
    though it is a small dataset, it has many features that are numerical as well
    as categorical. Its features are described on its web page and the data is provided
    in`.csv` format.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在UCI机器学习仓库（[https://archive.ics.uci.edu/ml/datasets/Automobile](https://archive.ics.uci.edu/ml/datasets/Automobile)）访问汽车数据集。该数据集中的每一行代表一辆特定的汽车。特征（列）描述了其特性、风险评级和相关的归一化损失。尽管这是一个小型数据集，但它具有许多既是数值型又是分类型的特征。其特征在网页上有所描述，数据以`.csv`格式提供。
- en: Dataset Citation
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集引用
- en: 'Dua, D. and Graff, C. (2019). UCI Machine Learning Repository ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)).
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Dua, D. 和 Graff, C. (2019). UCI机器学习仓库 ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml))。加州大学欧文分校，信息与计算机科学学院，加州，欧文。
- en: Accessing the DataRobot API
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问DataRobot API
- en: The programmatic use of DataRobot enables data experts to leverage the platform's
    efficacies while having the flexibility associated with typical programming. With
    the API access of DataRobot, data from numerous sources can be integrated for
    analytic or modeling purposes. This capability is not only limited to the data
    that's ingested, but also the output of the outcome. For instance, API access
    makes it possible for a customer risk profiling model to get data from differing
    sources, such as Google BigQuery, local files, as well as AWS S3 buckets. And
    in a few lines of codes, the outcomes can update records on Salesforce, as well
    as those surfaced on PowerBI via a BigQuery table. The strength of this multiple
    data source integration capability is furthered as this enables the automated,
    scheduled, end-to-end periodic refresh of model outcomes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot 的程序化使用使得数据专家能够在保持典型编程灵活性的同时，利用平台的高效性。通过 DataRobot 的 API 访问，可以集成来自多个来源的数据，用于分析或建模目的。这种能力不仅限于摄入的数据，还包括结果的输出。例如，API
    访问使得客户风险分析模型能够从不同的来源获取数据，例如 Google BigQuery、本地文件以及 AWS S3 存储桶。通过几行代码，结果可以更新 Salesforce
    上的记录，以及通过 BigQuery 表显示在 PowerBI 上的记录。这种多数据源集成能力的优势进一步体现在它能够实现模型结果的自动化、定期、端到端的周期性刷新。
- en: In this preceding case, it becomes possible for the client base to be rescored
    periodically. Regarding scoring data, the DataRobot platform can only score datasets
    that are less than 1 GB in size. When problems require huge datasets, the **Batch
    Prediction API** normally chunks up the data and scores them concurrently. For
    a dataset with hundreds of millions of rows, it is possible to set up an iterative
    job to chunk up the data and score it iteratively using the Batch Prediction API.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在此先前的案例中，客户基础可以定期重新评分。关于评分数据，DataRobot 平台只能评分小于 1 GB 的数据集。当问题需要大量数据集时，**批量预测
    API** 通常会将数据分块并并发评分。对于包含数亿行的数据集，可以使用批量预测 API 设置迭代作业，分块数据并迭代评分。
- en: In addition, the API access to DataRobot allows users to develop user-defined
    features that make commercial sense before analysis and those based on scored
    model outcomes. This makes the modeling process more robust as it allows human
    intelligence to be applied to outcomes. In the preceding client risk profiling
    case, it becomes possible to classify customers into risk categories for easier
    business decision making. Also, based on the explanations given, the next best
    actions could be developed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DataRobot 的 API 访问允许用户在分析之前开发具有商业意义的用户定义特征，以及基于评分模型结果的特征。这使得建模过程更加稳健，因为它允许将人类智能应用于结果。在前面的客户风险分析案例中，可以将客户分类到风险类别，以便于更轻松的商业决策。此外，根据给出的解释，还可以开发下一步的最佳行动方案。
- en: Furthermore, programmatic use of DataRobot allows users to configure differing
    visualizations as they deem fit. This also offers analysts a broader range of
    visual outcome types. The Seaborn and Matplotlib Python libraries offer a huge
    range of visualization types with differing configurations. This also allows certain
    data subgroups or splits to be visualized. Among other benefits, it becomes possible
    to even select certain aspects of the data to be visualized.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DataRobot 的程序化使用允许用户根据需要配置不同的可视化。这也为分析师提供了更广泛的视觉结果类型。Seaborn 和 Matplotlib
    Python 库提供了大量不同配置的可视化类型。这也允许可视化某些数据子组或分割。在其他好处中，甚至可以选择要可视化的数据的一些方面。
- en: One of the big advantages of accessing DataRobot using its API is the ability
    to create multiple projects iteratively. Two easy examples come to mind here.
    One approach to improving the outcomes of multi-class modeling is to use the one
    versus all modeling paradigm. This involves creating models for each of the classes.
    When scoring, all the models are used to score the data and for each row, the
    class with the highest score is attributed to the row. To bring this to life,
    let's assume we are building models to predict wheel drive types based on other
    characteristics. First, models are created for the three main types of wheel drives;
    that is, **front-wheel drive** (**FWD**), **four-wheel drive** (**4WD**), and
    **rear-wheel drive** (**RWD**). Data is then scored against all three models,
    and the model that presents each row with the highest prediction is assumed as
    the class the row belongs to.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DataRobot的API访问其的一大优点是能够迭代创建多个项目。这里有两个简单的例子。提高多类建模结果的一种方法是用“一对多”建模范式。这涉及到为每个类别创建模型。在评分时，所有模型都用于评分数据，并且对于每一行，具有最高评分的类别被分配给该行。为了使这一点更加生动，让我们假设我们正在构建基于其他特征的预测车轮驱动类型的模型。首先，为三种主要的车轮驱动类型创建模型；即，**前轮驱动**（**FWD**）、**全轮驱动**（**4WD**）和**后轮驱动**（**RWD**）。然后，数据将对所有三个模型进行评分，并且假设呈现每一行最高预测的模型是该行所属的类别。
- en: The model factory is another example where multiple model projects are integrated
    into a system so that each project builds models for a subgroup in the data. In
    some problems, data tends to be nested in that certain variables tend to govern
    the way models behave generally. A point in case is modeling the performance of
    students nested in class. These features, such as the class teacher for schools,
    tend to control the effect other exogenous variables have on the dependent variable.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型工厂是另一个例子，其中多个模型项目被集成到一个系统中，以便每个项目为数据中的子组构建模型。在某些问题中，数据往往嵌套在某种变量倾向于控制模型行为方式。一个例子是建模嵌套在班级中的学生表现。这些特征，如学校的班级教师，往往控制其他外生变量对因变量的影响。
- en: In the case of cars, their brands typically drive their prices. For instance,
    irrespective of how similar a Skoda is to an Audi, the Audi will most likely be
    more expensive. As such, when developing models for such a case, it is ideal to
    create models for each of the car brands. In the context of programmatically accessing
    DataRobot, the idea would be to run an iteration of the project for each of the
    car brands.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在汽车的情况下，它们的品牌通常驱动价格。例如，无论斯柯达与奥迪有多相似，奥迪很可能更贵。因此，在开发此类案例的模型时，为每个汽车品牌创建模型是理想的。在以编程方式访问DataRobot的背景下，这个想法将是为每个汽车品牌运行项目的一个迭代。
- en: In addition to creating and scoring DataRobot models programmatically, we will
    use Jupyter Notebook's **Integrated Development Environment** (**IDE**) to build
    projects for a case of one versus all and a model factory. However, before we
    can create projects with DataRobot using an API, certain identification processes
    must be covered. Let's have a look.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了以编程方式创建和评分DataRobot模型外，我们还将使用Jupyter Notebook的**集成开发环境**（**IDE**）来构建一个“一对多”和一个模型工厂的项目。然而，在我们能够使用API使用DataRobot创建项目之前，必须覆盖某些识别过程。让我们看看。
- en: 'To programmatically access DataRobot, users need to create an API key. This
    key is then used to access the platform from a client. To create an API key, open
    the **Account** menu at the top right-hand corner of the home page (see *Figure
    12.1*). From there, access the **Developer Tools** window (see *Figure 12.1*):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要以编程方式访问DataRobot，用户需要创建一个API密钥。这个密钥随后用于从客户端访问平台。要创建API密钥，请打开主页右上角的**账户**菜单（见*图12.1*）。从那里，访问**开发者工具**窗口（见*图12.1*）：
- en: '![Figure 12.1 – Accessing Developer Tools'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 12.1 – Accessing Developer Tools'
- en: '](img/B17159_12_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17159_12_01.jpg]'
- en: Figure 12.1 – Accessing Developer Tools
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – 访问开发者工具
- en: 'After opening the **Developer Tools** window, click on **Create New Key** and
    enter the name of the new key. On saving the new key''s name, the API key will
    be generated (see *Figure 12.2*). After this, the generated key is copied and
    secured. The API key, along with the endpoint, is necessary to establish a connection
    between the local machine and the DataRobot instance:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 打开**开发者工具**窗口后，点击**创建新密钥**并输入新密钥的名称。在保存新密钥的名称后，API密钥将被生成（见*图12.2*）。之后，生成的密钥将被复制并安全存储。API密钥以及端点对于在本地机器和数据Robot实例之间建立连接是必要的：
- en: '![Figure 12.2 – Creating an API key'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 12.2 – Creating an API key'
- en: '](img/B17159_12_02.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17159_12_02.jpg)'
- en: Figure 12.2 – Creating an API key
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – 创建API密钥
- en: The endpoint parameter is the URL of the DataRobot endpoint. [https://app.datarobot.com/api/v2](https://app.datarobot.com/api/v2)
    is the default endpoint for the US cloud-based endpoint for its US and Japanese
    users. The EU-managed cloud endpoint is [https://app.eu.datarobot.com/api/v2](https://app.eu.datarobot.com/api/v2).
    VPC, on-premises, hybrid, or private users usually have their deployment endpoint
    as their DataRobot GUI root. To enhance security, these credentials are sometimes
    stored and accessed as `.yaml` files. These two credentials enable a connection
    between a computer and a DataRobot instance to use the DataRobot Python client.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 端点参数是DataRobot端点的URL。[https://app.datarobot.com/api/v2](https://app.datarobot.com/api/v2)是针对美国云端的默认端点，适用于其美国和日本用户。欧盟管理的云端点是[https://app.eu.datarobot.com/api/v2](https://app.eu.datarobot.com/api/v2)。VPC、本地、混合或私有用户通常将他们的部署端点作为DataRobot
    GUI根。为了增强安全性，这些凭证有时以`.yaml`文件的形式存储和访问。这两个凭证使得计算机与DataRobot实例之间能够建立连接，以使用DataRobot
    Python客户端。
- en: Using the DataRobot Python client
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DataRobot Python客户端
- en: The Python programming language is one of the most popular programming languages
    used by data scientists. It is flexible yet powerful. Being able to integrate
    the AutoML capabilities of DataRobot and utilize the flexibility of Python offers
    data scientists various benefits, as we mentioned earlier.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Python编程语言是数据科学家使用最流行的编程语言之一。它既灵活又强大。能够集成DataRobot的AutoML功能并利用Python的灵活性，为数据科学家提供了各种好处，正如我们之前提到的。
- en: Programming in Python using the Jupyter IDE.
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Jupyter IDE进行Python编程。
- en: Now, let's explore the DataRobot Python client.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索DataRobot Python客户端。
- en: To use the DataRobot Python client, Python must be version 2.7 or 3.4+. The
    most up-to-date version of DataRobot must be installed. For the cloud version,
    the `pip` command will install the most recent version of the `DataRobot` package.
    On Python, running `!pip install datarobot` should install the `DataRobot` package.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用DataRobot Python客户端，Python版本必须是2.7或3.4以上。必须安装最新版本的DataRobot。对于云版本，`pip`命令将安装`DataRobot`包的最新版本。在Python上运行`!pip
    install datarobot`应该会安装`DataRobot`包。
- en: 'Having installed the `DataRobot` package, the package has been imported. The
    `Client` method of the `DataRobot` package provides the much-needed connection
    to the DataRobot instance. As shown in *Figure 12.3*, the basic format for the
    `Client` method is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了`DataRobot`包后，该包已被导入。`DataRobot`包的`Client`方法提供了连接到DataRobot实例所需的重要连接。如图12.3所示，`Client`方法的基本格式如下：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In terms of data ingestion, data can be imported from different sources. This
    process is identical to normal data imports with Python. The local file installation
    is quite straightforward. Here, all you need is the API key and the file path.
    *Figure 12.3* presents the code for ingesting the automobile dataset. For the
    JDBC connection, to get data from platforms such as BigQuery and Snowflake, in
    addition to the API key, the identity of the data source object is required, as
    well as the user database's credentials – their usernames and passwords. The user
    database's credentials are provided by their organization's database administrators.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据摄取方面，数据可以从不同的来源导入。这个过程与使用Python的正常数据导入相同。本地文件安装相当简单。在这里，你需要的是API密钥和文件路径。*图12.3*展示了导入汽车数据集的代码。对于JDBC连接，要从BigQuery和Snowflake等平台获取数据，除了API密钥外，还需要数据源对象的身份以及用户数据库的凭证——它们的用户名和密码。用户数据库的凭证由其组织的数据库管理员提供。
- en: In this section, we established how to access the credentials necessary to programmatically
    use DataRobot. We have also imported data programmatically. Naturally, conducting
    some analysis and modeling comes after ingesting data. In the next section, we
    will create machine learning models using the Python API.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了如何访问编程使用DataRobot所需的凭证。我们已通过编程方式导入数据。自然地，在摄取数据之后，进行一些分析和建模是接下来的步骤。在下一节中，我们将使用Python
    API创建机器学习模型。
- en: Building models programmatically
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编程构建模型
- en: Now that we have imported the data, we will start building models programmatically.
    We will look at building the most basic models, then explore how to extract and
    visualize feature impact, before evaluating the performance of our models. Then,
    we will create more complex projects. Specifically, we will build one versus all
    **multiclass** classification models and **model factories**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经导入了数据，我们将开始以编程方式构建模型。我们将查看如何构建最基础的模型，然后探索如何提取和可视化特征影响，最后评估我们模型的性能。然后，我们将创建更复杂的项目。具体来说，我们将构建一对多**多分类**分类模型和**模型工厂**。
- en: 'To create a DataRobot project, we must use the DataRobot `Project.start` method.
    The basic format for this is importing the necessary libraries (DataRobot, in
    the following case). Thereafter, the access credentials are presented, as described
    in the previous section. It is at the point that the `Project` method is called.
    `project_name`, `sourcedata`, and `target` are the minimal parameters that are
    required by the `Project` method for projects to be created. The `project_name`
    parameter tells DataRobot the name to give the created project. `sourcedata` provides
    information regarding the location of the data that''s required to create models.
    This could be a location or a Python object. Finally, `target` specifies the target
    variable for the models to be built, as shown here:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建DataRobot项目，我们必须使用DataRobot的`Project.start`方法。这个基本格式是导入必要的库（在下面的例子中是DataRobot）。之后，展示访问凭证，如前所述。正是在这一点上调用`Project`方法。`project_name`、`sourcedata`和`target`是`Project`方法创建项目所需的最小参数。`project_name`参数告诉DataRobot为创建的项目命名。`sourcedata`提供有关创建模型所需数据位置的信息。这可能是一个位置或一个Python对象。最后，`target`指定要构建的模型的目标变量，如图所示：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The basic format for creating projects was shown in the preceding section and
    illustrated in *Figure 12.3*. Once the models have been created, we can use the
    `project.get_models` method to get a list of them. This list of models is presented
    in order by their validation scores by default. For this example, we will be using
    the automobile dataset, which we used to build models in [*Chapter 6*](B17159_06_Final_NM_ePub.xhtml#_idTextAnchor104),
    *Model Building with DataRobot*. The project''s name is `autoproject_1`. Here,
    the file''s location is specifically stored in a pandas object called `data`.
    The target variable is `price`. Note that these parameters are case-sensitive:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 创建项目的格式在前面章节中已展示，并在*图12.3*中说明。一旦创建了模型，我们可以使用`project.get_models`方法获取它们的列表。默认情况下，这些模型按其验证分数排序。对于这个例子，我们将使用汽车数据集，我们在[*第6章*](B17159_06_Final_NM_ePub.xhtml#_idTextAnchor104)中用它来构建模型，*使用DataRobot进行模型构建*。项目的名称是`autoproject_1`。在这里，文件的位置被特别存储在一个名为`data`的pandas对象中。目标变量是`price`。请注意，这些参数是区分大小写的：
- en: '![Figure 12.3 – Programmatically creating DataRobot models and extracting their
    lists'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.3 – 以编程方式创建DataRobot模型并提取其列表'
- en: '](img/B17159_12_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_03.jpg)'
- en: Figure 12.3 – Programmatically creating DataRobot models and extracting their
    lists
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – 以编程方式创建DataRobot模型和提取其列表
- en: 'Once you''ve created the model, the `get_models` method is called to list the
    models. We can see that the best performing model is `Gradient Boosted Greedy
    Trees Regressor (Least-Square Loss)`. To evaluate this model, we need to extract
    its ID. To do so, we must create an object, `best_model_01`, to store the best-performing
    model. This metrics method is then called for this model. As shown in the following
    screenshot, the cross-validation RMSE for this model is `2107.40`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了模型，就需要调用`get_models`方法来列出模型。我们可以看到表现最好的模型是`Gradient Boosted Greedy Trees
    Regressor (Least-Square Loss)`。为了评估这个模型，我们需要提取其ID。为此，我们必须创建一个对象，`best_model_01`，来存储表现最好的模型。然后对这个模型调用此指标方法。如图所示，该模型的交叉验证RMSE为`2107.40`：
- en: '![Figure 12.4 – Programmatically evaluating DataRobot models'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.4 – 以编程方式评估DataRobot模型'
- en: '](img/B17159_12_04.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_04.jpg)'
- en: Figure 12.4 – Programmatically evaluating DataRobot models
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 – 以编程方式评估DataRobot模型
- en: 'To provide some insight into the price drivers, we need the feature impacts.
    These can be retrieved through the DataRobot API using the `get_or_feature_impact`
    method. To visualize the feature impacts for projects, we must define a function
    called `plot_FI` that takes in the model''s name and chart title as parameters,
    gets the feature impacts, and then normalizes and plots them using Seaborn''s
    bar plot method. Regarding the `autoproject_1` project, the following screenshot
    shows how to retrieve and present the feature impacts using the `plot_FI` function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一些关于价格驱动因素的了解，我们需要特征影响。这些可以通过DataRobot API使用`get_or_feature_impact`方法检索。为了可视化项目的特征影响，我们必须定义一个名为`plot_FI`的函数，该函数接受模型名称和图表标题作为参数，获取特征影响，然后使用Seaborn的条形图方法进行归一化和绘图。关于`autoproject_1`项目，以下截图显示了如何使用`plot_FI`函数检索和展示特征影响：
- en: '![Figure 12.5 – Defining a function and extracting the feature impacts'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.5 – 定义函数并提取特征影响'
- en: '](img/B17159_12_05.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_05.jpg)'
- en: Figure 12.5 – Defining a function and extracting the feature impacts
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 – 定义函数并提取特征影响
- en: 'Programmatic access to DataRobot furthers the benefits the platform offers.
    With programmatic access, you can take advantage of the iterative process within
    Python, and users can create multiple projects for the same dataset. Now, let''s
    look at two ways to create multiple projects from the same dataset: **multi-class**
    classification and **model factory**.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过程序访问DataRobot可以进一步发挥该平台的优势。通过程序访问，您可以利用Python中的迭代过程，用户可以为同一数据集创建多个项目。现在，让我们看看从同一数据集创建多个项目的两种方法：**多类别**分类和**模型工厂**。
- en: Multi-class classification involves classifying instances into more than two
    classes. It is possible to create a single project that classifies rows into either
    of these classes. Essentially, this is a model that classes rows into one of all
    the available classes. Another way to approach this problem involves building
    different models for the different classes. Within this approach, a model is built
    for each of the classes as a target. You can see how this can be executed using
    Python's iterative process; that is, by looping through all the target levels.
    The one versus all method is better for performing classification problems with
    more than two classes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 多类别分类涉及将实例分类到两个以上的类别。可以创建一个单一的项目，将行分类到这些类别中的任何一个。本质上，这是一个将行分类到所有可用类别之一的模型。另一种处理此问题的方法是为不同的类别构建不同的模型。在此方法中，为每个类别作为目标构建一个模型。您可以看到如何使用Python的迭代过程执行此操作；即通过遍历所有目标级别。一对多方法更适合执行具有两个以上类别的分类问题。
- en: 'Now, let''s demonstrate how to use the one versus all method on the auto pricing
    project. Here, we will create price classes using the pandas `qcut`. `qcut` helps
    divide data into similarly sized bins. Using this function, we can divide our
    data into price classes – low to high. The following screenshot shows this price
    discretizing process and checking the distribution of cases across the classes:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们演示如何在汽车定价项目中使用一对多方法。在这里，我们将使用pandas的`qcut`创建价格类别。`qcut`有助于将数据划分为大小相似的区间。使用此函数，我们可以将我们的数据划分为价格类别——从低到高。以下截图显示了此价格离散化过程和检查案例在类别中的分布：
- en: '![Figure 12.6 – Price discretization'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.6 – 定价离散化'
- en: '](img/B17159_12_06.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_06.jpg)'
- en: Figure 12.6 – Price discretization
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6 – 定价离散化
- en: 'Having created the classes, to allow for data **leakages**, we will drop the
    initial price variable. We will write a loop that builds models for each of the
    price classes. Perform the following steps:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了类别后，为了允许数据**泄露**，我们将删除初始的价格变量。我们将编写一个循环，为每个价格类别构建模型。执行以下步骤：
- en: Turn the `price_class` variable into dummy variables.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`price_class`变量转换为哑变量。
- en: For each iteration, create a DataRobot project after a dummified price class
    name.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每次迭代，在创建一个经过哑变量处理的定价类别名称后创建一个DataRobot项目。
- en: For each iteration, we drop the `price_class` dummy level being modeled. This
    ensures that there are no leakages.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每次迭代，我们删除正在建模的`price_class`哑变量级别。这确保了没有泄露。
- en: For each iteration, we must build the models for a target variable dummy.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每次迭代，我们必须为目标变量哑变量构建模型。
- en: 'After creating the projects, the top-performing model for each project is selected
    and stored in a dictionary:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建项目后，每个项目的最佳模型被选中并存储在字典中：
- en: '![Figure 12.7 – Creating a one versus all classification suite of projects'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.7 – 创建一个一对多分类项目套件'
- en: '](img/B17159_12_07.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_07.jpg)'
- en: Figure 12.7 – Creating a one versus all classification suite of projects
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7 – 创建一个单对多分类项目套件
- en: This process involves creating projects with a suite of models with targets
    iterating through all the price classes. After creating the projects, the best
    model for each target class is selected using an iteration of all the projects
    with names starting with `Auto`, and then the top-performing model for each project.
    These best models are placed in a dictionary.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程涉及创建具有一系列模型的项目，目标遍历所有价格类别。创建项目后，使用以`Auto`开头的所有项目的迭代选择每个目标类别的最佳模型，然后选择每个项目的最佳性能模型。这些最佳模型被放置在一个字典中。
- en: It is sometimes recommended, if not ideal, to create different projects with
    a subset of the data. After selecting all the cases for the target variable, you
    must create a random subset of the data for each project creation iteration. In
    the auto pricing case, however, we were unable to explore this as the out-sample
    size was limiting.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有时建议（如果不是理想的情况），使用数据子集创建不同的项目。在选择了目标变量的所有案例后，必须为每个项目创建迭代创建随机数据子集。然而，在自动定价的情况下，我们无法探索这一点，因为外样本大小有限制。
- en: 'A `fuel-type`):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'A `燃料类型`):'
- en: First, create and store a project.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建并存储一个项目。
- en: Select cases for the target variable (the influencer of interest). In this case,
    the variable is `fuel-type`. Here, this variable is selected, and differing levels
    of this variable are used to create DataRobot projects. In simple terms, this
    step involves, for instance, selecting all the rows with `fuel-type` set to `gas`
    as a subgroup.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择目标变量的案例（感兴趣的变量）。在这种情况下，变量是`燃料类型`。在这里，选择了这个变量，并使用这个变量的不同级别来创建DataRobot项目。简单来说，这一步涉及，例如，选择所有将`燃料类型`设置为`汽油`的行作为一个子组。
- en: If necessary, define the evaluation metric. Here, we can alter aspects of the
    advanced options we encountered in [*Chapter 6*](B17159_06_Final_NM_ePub.xhtml#_idTextAnchor104),
    *Model Building with DataRobot*. Other advance options can be selected and altered.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果需要，定义评估指标。在这里，我们可以修改我们在[*第6章*](B17159_06_Final_NM_ePub.xhtml#_idTextAnchor104)中遇到的先进选项，即使用DataRobot进行模型构建。还可以选择并修改其他先进选项。
- en: If necessary, set a data limit that a class will be deselected for (for instance,
    if the number of rows is less than 20 for that class). The importance of this
    step lies in the fact that some variable levels could have very low occurrences,
    so the sample size within the subgroup is small. Therefore, creating models out
    of these becomes a challenge. This step becomes the best place to drop such variable
    levels using the count of cases within the subgroup.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果需要，为班级设置一个数据限制，使得该班级将取消选择（例如，如果该班级的行数少于20）。这一步骤的重要性在于，某些变量级别可能具有非常低的频率，因此子组内的样本量很小。因此，从这些变量中创建模型成为了一个挑战。这一步骤是使用子组内案例计数来删除此类变量级别的最佳位置。
- en: All the models from all the projects are selected and stored in a dictionary.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有项目中的所有模型都被选中并存储在一个字典中。
- en: 'Some of these steps are evident in creating a model factor for the auto pricing
    problem (see *Figure 12.8*). Here, `fuel-type` is selected as the feature that
    projects are created on. In this case, only two projects are created: one for
    gas automobiles and another for diesel ones. Now that we''ve created the models,
    the next step is to collect the best-performing models for each `fuel-type`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建用于自动定价问题的模型因子时，一些步骤是显而易见的（参见*图12.8*）。在这里，`燃料类型`被选为创建项目的基础特征。在这种情况下，只创建了两个项目：一个用于汽油汽车，另一个用于柴油汽车。现在我们已经创建了模型，下一步是收集每个`燃料类型`的最佳性能模型：
- en: '![Figure 12.8 – Creating model factories'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.8 – 创建模型工厂'
- en: '](img/B17159_12_08.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_08.jpg)'
- en: Figure 12.8 – Creating model factories
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8 – 创建模型工厂
- en: The efficacy of using one versus all multiclass classification models and model
    factories lies in their ability to fit models to each level of the target variable.
    This happens in an automated fashion and considers the sample validation, all
    the preprocessing steps, and the model training process. When data cardinality
    and volume are high, these approaches would mostly outperform typical modeling.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单对多类别分类模型和模型工厂相比，其有效性在于它们能够将模型拟合到目标变量的每个级别。这是自动完成的，并考虑了样本验证、所有预处理步骤以及模型训练过程。当数据基数和体积较高时，这些方法通常会优于典型的建模方法。
- en: 'For the model factory, multiple projects are created for the different levels
    of the feature of interest. To evaluate this, the best-performing model for each
    project is selected from the dictionary for all projects. This set of best models
    from all the projects is stored in another dictionary object. A `for` loop is
    then run across all the models of the dictionary to extract the performance of
    the model, as shown in the following screenshot:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型工厂，为感兴趣特征的各个级别创建了多个项目。为了评估这一点，从所有项目的字典中选择了每个项目的最佳性能模型。这个来自所有项目的最佳模型集存储在另一个字典对象中。然后，通过一个`for`循环遍历字典中的所有模型以提取模型的性能，如下面的截图所示：
- en: '![Figure 12.9 – Evaluating the performance of models with a model factory'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.9 – 使用模型工厂评估模型的性能'
- en: '](img/B17159_12_09.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_09.jpg)'
- en: Figure 12.9 – Evaluating the performance of models with a model factory
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9 – 使用模型工厂评估模型的性能
- en: 'Improved model performance is only one of the reasons you should use the one
    versus all multiclass classification models, as well as model factors. Sometimes,
    understanding the drivers is equally as important. Visualizing the feature importance
    for the different fuel types could present an interesting contrast in drivers.
    This means that different factors affect the prices of different fuel types. This
    could have a bearing on strategic decisions. As shown in the following screenshot,
    the Python API can be used to plot the feature impacts by leveraging chart functions
    from Seaborn and Matplotlib:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 改善模型性能只是您应该使用一对多多类分类模型以及模型因素的原因之一。有时，了解驱动因素同样重要。可视化不同燃料类型的特征重要性可以展示驱动因素之间的有趣对比。这意味着不同的因素会影响不同燃料类型的定价。这可能会影响战略决策。如下面的截图所示，可以使用Python
    API通过利用Seaborn和Matplotlib的图表函数来绘制特征影响：
- en: '![Figure 12.10 – Feature impacts for the differing diesel and gas automobiles'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.10 – 不同柴油和汽油汽车的特性影响'
- en: '](img/B17159_12_10.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_10.jpg)'
- en: Figure 12.10 – Feature impacts for the differing diesel and gas automobiles
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10 – 不同柴油和汽油汽车的特性影响
- en: As we can see, there are some differences in the feature impacts for the automobile
    fuel types. While `curb-weight` seems to be an important driver, its effect is
    relatively more important for diesel vehicles. Similarly, for gas cars, the power
    that's generated by these automobiles, as typified by the `engine_size` and `horsepower`
    features, carries more importance in determining price than those of diesel cars.
    You can already see the effect such preliminary findings could have on decisions
    and how this could be applied to other commercial cases. Using feature importance
    to examine multiple models can also be applied in the case of one versus all classification
    problems.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，汽车燃料类型的特性影响存在一些差异。虽然` curb-weight`似乎是一个重要的驱动因素，但其影响对柴油车辆来说相对更重要。同样，对于汽油车来说，这些汽车产生的动力，如`engine_size`和`horsepower`特征所典型化的，在确定价格方面比柴油车更重要。您已经可以看到这些初步发现对决策的影响以及如何将其应用于其他商业案例。使用特征重要性来检查多个模型也可以应用于一对多分类问题。
- en: In this section, we created basic DataRobot projects using the Python API. After,
    we solved more complex problems by using multiple projects within a system. There,
    we created one versus all projects to solve multiclass classification problems
    and model factories to solve multi-level problems involving subgroups. We also
    explored feature impact and model evaluation. Having programmatically created
    models, we will now learn how to make predictions using these models. Specifically,
    we will learn how to deploy models, make predictions, extract explanations from
    models, and score large datasets through parallelization.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用Python API创建了基本的DataRobot项目。之后，我们通过在一个系统中使用多个项目来解决更复杂的问题。在那里，我们创建了一对多项目来解决涉及子组的多层问题，并使用模型工厂来解决多级问题。我们还探讨了特征影响和模型评估。在程序化创建模型后，我们现在将学习如何使用这些模型进行预测。具体来说，我们将学习如何部署模型、进行预测、从模型中提取解释，并通过并行化对大型数据集进行评分。
- en: Making predictions programmatically
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 程序化进行预测
- en: The possibilities that programmatically using DataRobot presents are enormous.
    By using its API, models can be deployed and predictions can be made against them.
    Before making programmatical predictions within the production environment, models
    need to be deployed. DataRobot models are deployed using Portable Prediction Servers.
    These are Docker containers that can host machine learning models, which serve
    predictions and prediction explanations through a REST API.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DataRobot程序化使用所提供的可能性是巨大的。通过使用其API，可以将模型部署并对它们进行预测。在制作生产环境中的程序化预测之前，需要部署模型。DataRobot模型通过便携式预测服务器进行部署。这些是Docker容器，可以托管机器学习模型，并通过REST
    API提供预测和预测解释。
- en: 'To deploy models, we can use the DataRobot package''s `deployment` method.
    Here, we must provide a description, the DataRobot model''s ID, as well as its
    label to create the deployments. A typical Python deployment script follows this
    format:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署模型，我们可以使用DataRobot包的`deployment`方法。在这里，我们必须提供一个描述、DataRobot模型的ID以及其标签来创建部署。一个典型的Python部署脚本遵循以下格式：
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As per this approach, the following screenshot shows how `autoproject_1`, which
    we created in the *Building models programmatically* section, can be deployed.
    Here, the model ID is `best_model_1`. We will label `AutoBase Deployment` with
    a description of `Base Automobile Price Deployment`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这种方法，以下截图显示了在*程序化构建模型*部分创建的`autoproject_1`如何部署。在这里，模型ID是`best_model_1`。我们将`AutoBase
    Deployment`标记为`Base Automobile Price Deployment`的描述：
- en: '![Figure 12.11 – Deploying a model programmatically'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.11 – 程序化部署模型'
- en: '](img/B17159_12_11.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_11.jpg)'
- en: Figure 12.11 – Deploying a model programmatically
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11 – 程序化部署模型
- en: 'The deployment process can be iterated to enable those of more complex projects.
    For instance, with model factories, irrespective of the number of levels the differentiating
    variable has, with a single `for` loop, all the best models can be deployed to
    DataRobot. For each of the best models, a deployment is created, which is then
    used to score new data. The script for deploying the model factory for the automobile
    project, along with the fuel type as its differentiating variable, is shown in
    the following screenshot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 部署过程可以迭代，以使更复杂的项目能够实现。例如，使用模型工厂，无论区分变量有多少级别，只需一个`for`循环，就可以将所有最佳模型部署到DataRobot。对于每个最佳模型，都会创建一个部署，然后用于评分新数据。以下截图显示了用于汽车项目的模型工厂部署脚本，其中燃料类型是其区分变量：
- en: '![Figure 12.12 – Deploying models from a model factory'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.12 – 从模型工厂部署模型'
- en: '](img/B17159_12_12.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_12.jpg)'
- en: Figure 12.12 – Deploying models from a model factory
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12 – 从模型工厂部署模型
- en: 'Having deployed the models, predictions can be made against them. To make simple
    predictions within the development environment, we can use the `DataRobot BatchPredictionJob.score_to_file`
    method. To make predictions, this method requires the model ID, prediction data,
    and the location where the scored data will be stored. Here, we will use `best_model_1`
    to score the same model we used to develop the model, the `df` data object, and
    the location path, which specifies the prediction file path as `./pred.csv`. The
    `passthrough_columns_set` parameter specifies the columns from the original dataset
    that will be included in the predictions. Since this is set to `''all''`, all
    the columns are returned, as shown here:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 部署了模型后，可以对它们进行预测。要在开发环境中进行简单预测，我们可以使用`DataRobot BatchPredictionJob.score_to_file`方法。要进行预测，此方法需要模型ID、预测数据和评分数据将存储的位置。在这里，我们将使用`best_model_1`对用于开发模型的同一模型进行评分，即`df`数据对象，以及位置路径，指定预测文件路径为`./pred.csv`。`passthrough_columns_set`参数指定将包含在预测中的原始数据集的列。由于此设置为`'all'`，因此返回所有列，如下所示：
- en: '![Figure 12.13 – Simple programmatic prediction'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.13 – 简单程序化预测'
- en: '](img/B17159_12_13.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_13.jpg)'
- en: Figure 12.13 – Simple programmatic prediction
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13 – 简单程序化预测
- en: These predictions comprise all the columns from the initial dataset, in addition
    to the predicted prices. There are cases where it is ideal to include rationales
    behind predictions. In such cases, the `max_explanations` parameter should be
    included in the job's configuration. This parameter sets the highest number of
    explanations to be provided for every data row.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测包括初始数据集的所有列，以及预测价格。在某些情况下，包括预测背后的理由是理想的。在这种情况下，应在作业配置中包含`max_explanations`参数。此参数设置每行数据应提供的最高解释数量。
- en: Summary
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: DataRobot provides us with a unique capability to rapidly develop models. With
    this platform, data scientists can combine the benefits of DataRobot and the flexibilities
    of open programming. In this chapter, we explored ways to access the credentials
    needed to programmatically use DataRobot. Using the Python client, we demonstrated
    ways in which data can be ingested and how basic projects can be created. We started
    building models for more complex problems. We created model factories as well
    as one versus all models. Finally, we demonstrated how models can be deployed
    and used to score data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot 为我们提供了一种独特的快速开发模型的能力。借助这个平台，数据科学家可以结合 DataRobot 的优势以及开放编程的灵活性。在本章中，我们探讨了获取用于程序化使用
    DataRobot 所需凭证的方法。使用 Python 客户端，我们演示了数据摄取的方式以及如何创建基本项目。我们开始构建用于更复杂问题的模型。我们创建了模型工厂以及一对多模型。最后，我们展示了如何部署模型并用于评分数据。
- en: One of the key advantages of programmatically using DataRobot is the ability
    to ingest data from numerous sources, score them, and store them in the relevant
    sources. This makes it possible to carry out end-to-end dataset scoring. It becomes
    possible for a system to be set up to score models periodically. With this comes
    numerous data quality and model monitoring concerns. The next chapter will focus
    on how to control the quality of the models and data on the DataRobot platform,
    as well as using the Python API.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DataRobot 进行程序化操作的一个关键优势是能够从众多来源摄取数据，对其进行评分，并将它们存储在相关来源中。这使得进行端到端数据集评分成为可能。系统可以设置为定期进行模型评分。随之而来的是许多数据质量和模型监控问题。下一章将重点介绍如何在
    DataRobot 平台上控制模型和数据的质量，以及如何使用 Python API。
