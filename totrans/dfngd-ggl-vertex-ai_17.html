<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer166">
			<h1 id="_idParaDest-256" class="chapter-number"><a id="_idTextAnchor282"/>17</h1>
			<h1 id="_idParaDest-257"><a id="_idTextAnchor283"/>Natural Language Models – Detecting Fake News Articles!</h1>
			<p>A significant amount of content on the internet is in textual format. Almost every organization stores lots of internal data and resources as text documents. <strong class="bold">Natural language processing</strong> (<strong class="bold">NLP</strong>) is a subfield of machine learning that’s concerned with organizing, understanding, and making decisions based on textual input data. Over the past decade, NLP has become the utmost important aspect of transforming business processes and making informed decisions. For example, a sentiment analysis model can help a business understand the high-level sentiments of their customers toward their products and services. A topic modeling algorithm combined with sentiment analysis can figure out the key pain points of the customers and thus it can inform the business decisions to make customer satisfaction <span class="No-Break">a priority.</span></p>
			<p>In this chapter, we will develop an ML system that can recognize fake news articles. Such systems can help in keeping the information and news on the internet more accurate and safer. We will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Detecting fake news <span class="No-Break">using NLP</span></li>
				<li>Launching model training on <span class="No-Break">Vertex AI</span></li>
				<li>BERT-based fake <span class="No-Break">news classification</span></li>
			</ul>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor284"/>Technical requirements</h1>
			<p>The code samples used in this chapter can be found in this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter17"><span class="No-Break">https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter17</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-259"><a id="_idTextAnchor285"/>Detecting fake news using NLP</h1>
			<p>Nowadays, due to the increase in the <a id="_idIndexMarker1128"/>use of the internet, it has become really easy to spread fake news. A large number of users are consuming and posting content on the internet via their social media accounts daily. It has become difficult to distinguish the real news from the fake news. Fake news, however, can do significant damage to a person, society, organization, or political party. Looking at the scale, it is impossible to skim through every article manually or using a human reviewer. Thus, there is a need to develop smart algorithms that can automatically detect fake news articles and stop the spread of dangerous news as soon as it <span class="No-Break">is generated.</span></p>
			<p>ML-based classification algorithms can be used to detect fake news. First, we need a good training dataset to train the classification model on so that it can learn the common patterns of fake news and thus automatically distinguish it from real news. In this section, we will train an ML model to classify articles as “fake” <span class="No-Break">versus “real.”</span></p>
			<h2 id="_idParaDest-260"><a id="_idTextAnchor286"/>Fake news classification with random forest</h2>
			<p>In this section, we will use<a id="_idIndexMarker1129"/> a tree-based classification algorithm known as random forest to detect fake news articles. In the last section of this chapter, we will also train a complex deep learning-based classifier and compare the accuracy of both models. Let’s dive into the experiment. All the code related to these experiments can be found in this book’s GitHub repository, as mentioned in the Technical <span class="No-Break">requirements section<a id="_idTextAnchor287"/><a id="_idTextAnchor288"/>.</span></p>
			<h2 id="_idParaDest-261">About the datase<a id="_idTextAnchor289"/>t</h2>
			<p>We have downloaded the<a id="_idIndexMarker1130"/> dataset from Kaggle, which has an open-to-use license. The dataset contains about 72k news articles with titles, texts, and labels. Almost 50% of the articles are “fake,” while the remainder are “real.” We will utilize this dataset to train an NLP-based classification model that can detect fake news. We will keep some parts of this dataset as unseen data so that we can test the model results after training. The link for downloading the data can be found in the Jupyter Notebook in this book’s <span class="No-Break">GitHub repository.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">We have already downloaded and decompressed the data in the same directory as the <span class="No-Break">Jupyter Noteboo<a id="_idTextAnchor290"/><a id="_idTextAnchor291"/>k.</span></p>
			<p>Now, let’s jump into the<a id="_idIndexMarker1131"/> implementation part. We will start by importing useful <span class="No-Break">Python libraries.</span></p>
			<h2 id="_idParaDest-262"><a id="_idTextAnchor292"/>Importing useful libraries</h2>
			<p>The first step is to load<a id="_idIndexMarker1132"/> some useful Python libraries in a <span class="No-Break">notebook c<a id="_idTextAnchor293"/>ell:</span></p>
			<pre class="source-code">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm_notebook
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve
%matplotlib in<a id="_idTextAnchor294"/><a id="_idTextAnchor295"/>line</pre>			<p>Next, we will load and verify the <span class="No-Break">input dataset.</span></p>
			<h2 id="_idParaDest-263"><a id="_idTextAnchor296"/>Reading and verifying the data</h2>
			<p>Here, we will read the data <a id="_idIndexMarker1133"/>from a CSV file into a pandas DataFrame called <strong class="source-inline">news_df</strong>. We will print the shape of the DataFrame and a few <span class="No-Break">top entri<a id="_idTextAnchor297"/>es:</span></p>
			<pre class="source-code">
news_df = pd.read_csv("WELFake_Dataset.csv")
print(news_df.shape)
news_df.h<a id="_idTextAnchor298"/><a id="_idTextAnchor299"/>ead()</pre>			<p>The output of this cell is shown in <span class="No-Break"><em class="italic">Figure 17</em></span><em class="italic">.1</em>. As we can see, there are 72,134 news articles in this table, each with a title, body, <span class="No-Break">and label:</span></p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/B17792_17_1.jpg" alt="Figure 17.1 – Fake news article detection dataset overview" width="1197" height="266"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.1 – Fake news article detection dataset overview</p>
			<p>Now, let’s see if there are <a id="_idIndexMarker1134"/>any missing values present in this <span class="No-Break">data table.</span></p>
			<h2 id="_idParaDest-264"><a id="_idTextAnchor300"/>NULL value check</h2>
			<p>We need to check if there <a id="_idIndexMarker1135"/>are any NULL values present in the dataset. There are different ways of handling NULL values. If the percentage of NULL values is very low, we can choose to drop those rows from the table; otherwise, we can fill those entries with some value. In our case, we will fill the NULL fields using an empty <span class="No-Break">string v<a id="_idTextAnchor301"/>alue:</span></p>
			<pre class="source-code">
news_df.isnull().sum()</pre>			<p>Here’s the output of <span class="No-Break">this cell:</span></p>
			<pre class="source-code">
Unnamed: 0      0
title         558
text           39
label           0
dtype:<a id="_idTextAnchor302"/> int64</pre>			<p>As we can see, there’s a very small number of entries with NULL values. Let’s fill them with <span class="No-Break">empty str<a id="_idTextAnchor303"/>ings:</span></p>
			<pre class="source-code">
news_df.fillna('', inplace<a id="_idTextAnchor304"/><a id="_idTextAnchor305"/>=True)</pre>			<p>We can now move on to data cleaning <span class="No-Break">and pre-processing.</span></p>
			<h2 id="_idParaDest-265"><a id="_idTextAnchor306"/>Combining title and text into a single column</h2>
			<p>Let’s create a new <a id="_idIndexMarker1136"/>column called <strong class="source-inline">content</strong> with combined <strong class="source-inline">title</strong> and <strong class="source-inline">text</strong> elements so that it contains all the textual information available related to the news article. Once we’ve done this, we will be able to use this column for model training and <span class="No-Break">classification pur<a id="_idTextAnchor307"/>poses:</span></p>
			<pre class="source-code">
news_df['content'] = [x + ' ' + y for x,y in zip(news_df.title, news_df<a id="_idTextAnchor308"/><a id="_idTextAnchor309"/>.text)]</pre>			<p>Now that our textual<a id="_idIndexMarker1137"/> content is in a single column, we can start cleaning and preparing it for <span class="No-Break">the model.</span></p>
			<h2 id="_idParaDest-266"><a id="_idTextAnchor310"/>Cleaning and pre-processing data</h2>
			<p>ML algorithms are very sensitive to noisy data. So, it is of utmost importance to clean and process the data before passing it into the model for training; this will allow the model to learn useful information from it. As we are using a classical ML algorithm here, we will need to do some aggressive <a id="_idIndexMarker1138"/>cleaning and pre-processing. In the case of deep learning, data processing is not required (as shown in the last section of this chapter). When we solve NLP problems using classical ML algorithms, we often use feature extraction methods such as TF and TF-IDF. As we know, these feature extraction methods are sensitive to the count of words, so it becomes important to remove less meaningful words (such as stopwords) and characters from <span class="No-Break">the text.</span></p>
			<p>In this experiment, we will follow these steps to clean and pre-process <span class="No-Break">the data:</span></p>
			<ol>
				<li>Remove special characters and numbers from <span class="No-Break">the text.</span></li>
				<li>Convert the text into lowercase (so that “HELLO” and “hello” are the same for the <span class="No-Break">classification algorithm).</span></li>
				<li>Split the content by space to get a list <span class="No-Break">of words.</span></li>
				<li>Remove stopwords. These are common English words and are often meaningless in a sentence. Examples include they, the, and, he, <span class="No-Break">and him.</span></li>
				<li>Apply stemming. This involves reducing the words to their root forms (for example, “happiness” should be reduced to “happy” so that different variations of the same word are equal for <span class="No-Break">the model).</span></li>
			</ol>
			<pre class="source-code">
Join words with spaces in between to create text.
We will require some NLP-specific libraries that will help in preparing the data. Here, we will utilize the <strong class="source-inline">nltk</strong> (Natural Language Toolkit) library to remove stopwords and apply the stemming operation. To convert our text data into a numerical format, we will utilize the <strong class="source-inline">TfidfVectorizer</strong> method from the <strong class="source-inline">sklearn<a id="_idTextAnchor311"/></strong> library:
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer</pre>			<p class="callout-heading">Note</p>
			<p class="callout">When we install the <strong class="source-inline">nltk</strong> library, it doesn’t automatically download all the required resources related to it. In our case, we will have to explicitly download the English language stopwords. We can do that by running the following command in <span class="No-Break">a terminal:</span></p>
			<pre class="console">
nltk.download("st<a id="_idTextAnchor312"/>opwords")</pre>			<p>Here is our data cleaning and <a id="_idIndexMarker1139"/>pre-processing function. As this function runs on the entire dataset, it takes some time <span class="No-Break">to complete:</span></p>
			<pre class="source-code">
def clean_and_prepare_content(text):
    text = re.sub('[^a-zA-Z]',' ', text)
    text = text.lower()
    text_words = text.split()
    imp_text_words = [word for word in text_words if not word in stopwords.words('english')]
    stemmed_words = [porter_stemmer.stem(word) for word in imp_text_words]
    processed_text = ' '.join(stemmed_words)
    return processed_text
porter_stemmer = PorterStemmer()
news_df['processed_content'] = news_df.content.apply(lambda content: clean_and_prepare_content<a id="_idTextAnchor313"/><a id="_idTextAnchor314"/>(content))</pre>			<p>Now, let’s separate our <a id="_idIndexMarker1140"/>content and labels into arrays for <span class="No-Break">modeling purposes.</span></p>
			<h2 id="_idParaDest-267"><a id="_idTextAnchor315"/>Separating the data and labels</h2>
			<p>Here, we are separating the <a id="_idIndexMarker1141"/>data and labels and putting them into two <span class="No-Break">sep<a id="_idTextAnchor316"/>arate lists:</span></p>
			<pre class="source-code">
X = news_df.processed_content.values
y = news_df.label.values
print(X.shape, y.shape)
Here's the output:
(7213<a id="_idTextAnchor317"/><a id="_idTextAnchor318"/>4,) (72134,)</pre>			<p>Now, let’s convert the text into <span class="No-Break">numeric values.</span></p>
			<h2 id="_idParaDest-268"><a id="_idTextAnchor319"/>Converting text into numeric data</h2>
			<p>As ML algorithms only <a id="_idIndexMarker1142"/>understand numbers, we will need to convert the textual data into numeric format. In our experiment, we will be creating <span class="No-Break">TF-<a id="_idTextAnchor320"/>IDF features:</span></p>
			<pre class="source-code">
vectorizer = TfidfVectorizer()
vectorizer.fit(X)
X = vectorizer<a id="_idTextAnchor321"/>.transform(X)
print(X.shape)
Here's the output:
(7<a id="_idTextAnchor322"/><a id="_idTextAnchor323"/>2134, 162203)</pre>			<p>We can now split the data into training and test partitions so that we can test the results of our model <span class="No-Break">after training.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">In a real NLP project, We must split the dataset into training and test sets before applying numerical transformations. The transformation function (such as <strong class="source-inline">TfidfVectorizer</strong>) should only be fit to the training data and then applied to test data. This is because, in a real-world setting, we might get some unknown words in the dataset and our model is not supposed to see those words during training. Another issue with this setting is that it causes data leakage as the statistics that are calculated over the entire dataset also<a id="_idIndexMarker1143"/> belong to the test partition. In this example, we have done this transformation before splitting the dataset just <span class="No-Break">for simplicity.</span></p>
			<h2 id="_idParaDest-269"><a id="_idTextAnchor324"/>Splitting the data</h2>
			<p>Next, we must split the data<a id="_idIndexMarker1144"/> into training and test partitions. We will use about 80% of the data for training and the remaining 2<a id="_idTextAnchor325"/>0% <span class="No-Break">for testing:</span></p>
			<pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, r<a id="_idTextAnchor326"/><a id="_idTextAnchor327"/>andom_state=42)</pre>			<p>Our training and test data partitions are now ready to be fed to the model. Next, we’ll define <span class="No-Break">the model.</span></p>
			<h2 id="_idParaDest-270"><a id="_idTextAnchor328"/>Defining the random forest classifier</h2>
			<p>For our simple experiment, we <a id="_idIndexMarker1145"/>are using default hyperparameter values for the random forest model. However, in a real-world use case, we can experiment with different sets of hyperparameter values to get the best results. Alternatively, we can utilize hyperparameter tuning to find the best hyperparameter<a id="_idTextAnchor329"/>s for <span class="No-Break">our model:</span></p>
			<pre class="source-code">
rf_model = RandomFo<a id="_idTextAnchor330"/><a id="_idTextAnchor331"/>restClassifier()</pre>			<p>Let’s go ahead and train the model on the <span class="No-Break">training partition.</span></p>
			<h2 id="_idParaDest-271"><a id="_idTextAnchor332"/>Training the model</h2>
			<p>Let’s fit our model to <a id="_idIndexMarker1146"/>the <a id="_idTextAnchor333"/><span class="No-Break">training dataset:</span></p>
			<pre class="source-code">
rf_model.fit(X_train, y_train)</pre>			<p>Here’s <span class="No-Break">the output:</span></p>
			<pre class="source-code">
RandomF<a id="_idTextAnchor334"/><a id="_idTextAnchor335"/>orestClassifier()</pre>			<p>Our model training is now<a id="_idIndexMarker1147"/> complete, which means we can start predicting the test data to check the <span class="No-Break">model’s results.</span></p>
			<h2 id="_idParaDest-272"><a id="_idTextAnchor336"/>Predicting the test data</h2>
			<p>Here, we’re using our trained<a id="_idIndexMarker1148"/> random forest classifier to make predictions on the test partition. The <strong class="source-inline">predict</strong> function gives the class-level output, while the <strong class="source-inline">predict_proba</strong> function gives the <span class="No-Break">pro<a id="_idTextAnchor337"/>babilistic outputs:</span></p>
			<pre class="source-code">
y_pred = rf_model.predict(X_test)
y_proba = rf_model.pr<a id="_idTextAnchor338"/><a id="_idTextAnchor339"/>edict_proba(X_test)</pre>			<p>Here, we have made predictions on the entire set. Let’s check how well our <span class="No-Break">model did.</span></p>
			<h2 id="_idParaDest-273"><a id="_idTextAnchor340"/>Checking the results/metrics on the test dataset</h2>
			<p>The next important step is to<a id="_idIndexMarker1149"/> check and verify the performance of our model on the test dataset. Here, we will use sklearn’s classification report method to get the precision, recall, and F1 score for each class. Check out the fol<a id="_idTextAnchor341"/><a id="_idTextAnchor342"/>lowing <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
# c<a id="_idTextAnchor343"/>lassification report
print(
    classification_report(
        y_test,
        y_pred,
        target_names=['Real', 'Fake'],
    )
)</pre>			<p>Here is the output of the <span class="No-Break">classification report:</span></p>
			<pre class="source-code">
              precision    recall  f1-score   support
        Real       0.94      0.92      0.93      7006
        Fake       0.93      0.94      0.94      7421
    accuracy                           0.93     14427
   macro avg       0.93      0.93      0.93     14427
weighted avg       0.93      0.93      0.93     14427</pre>			<p>As we can see, our model has about 93% precision and recall for both classes. The overall accuracy is also <a id="_idIndexMarker1150"/>about 93%. So, we can say that our model is good enough to identify about 93% of the fake <span class="No-Break">news articles.</span></p>
			<p>Next, let’s plot the ROC curve. The <a id="_idIndexMarker1151"/>ROC curve is a<a id="_idIndexMarker1152"/> graph between the <strong class="bold">false positive rate</strong> (<strong class="bold">FPR</strong>) and <strong class="bold">true positive rate</strong> (<strong class="bold">TPR</strong>) of a <span class="No-Break">classification model:</span></p>
			<pre class="source-code">
def plot_roc_curve(y_true, y_prob):
    """
    plots the roc curve based of the probabilities
    """
    fpr, tpr, thresholds = roc_curve(y_true, y_prob)
    plt.plot(fpr, tpr)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
plot_roc_curve(y_test, y_proba[:,1])</pre>			<p>Check out <span class="No-Break"><em class="italic">Figure 17</em></span><em class="italic">.2</em> to see the ROC curve for our experiment. In a typical ROC curve, the X-axis represents the FPR and the Y-axis represents the TPR of the model. The <strong class="bold">area under the ROC curve</strong>, also known<a id="_idIndexMarker1153"/> as <strong class="bold">ROC-AUC</strong>, indicates the quality of a classification model. Having a higher area value signifies a <span class="No-Break">better model:</span></p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/B17792_17_2.jpg" alt="Figure 17.2 – The ROC curve for the fake news classification model" width="567" height="432"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.2 – The ROC curve for the fake ne<a id="_idTextAnchor344"/><a id="_idTextAnchor345"/>ws classification model</p>
			<p>Let’s also check out the <a id="_idIndexMarker1154"/>confusion matrix to see where our model is <span class="No-Break">making mistakes.</span></p>
			<h2 id="_idParaDest-274"><a id="_idTextAnchor346"/>Confusion matrix</h2>
			<p>Finally, let’s also print the <a id="_idIndexMarker1155"/>confusion matrix for our classification. A confusion matrix shows the number of correct and incorrect classifications of each class. It also shows which other classes were predicted as mistakes if the classification is wrong (false positi<a id="_idTextAnchor347"/>ves and <span class="No-Break">false negatives):</span></p>
			<pre class="source-code">
confusion_matrix(y_test, y_pred,)</pre>			<p>Here’s <span class="No-Break">the output:</span></p>
			<pre class="source-code">
array([[6455,  5<a id="_idTextAnchor348"/><a id="_idTextAnchor349"/>51],
       [ 409, 7012]])</pre>			<p>Our experiment is now complete. If the results are satisfactory, we can go ahead and deploy this model as an API. If the results are still not acceptable, we can do more experiments with <span class="No-Break">different settings.</span></p>
			<p>If we want to do a lot of experiments in parallel, we can launch many parallel experiments via Vertex AI training jobs without needing to monitor them constantly and check back later when<a id="_idIndexMarker1156"/> training is complete. In the next section, we will see how Vertex AI training jobs can <span class="No-Break">be configured.</span></p>
			<h1 id="_idParaDest-275"><a id="_idTextAnchor350"/>Launching model training on Vertex AI</h1>
			<p>In this section, we will launch <a id="_idIndexMarker1157"/>our training experiment as a Vertex AI training job. There are multiple advantages of launching training jobs on Vertex AI instead of doing it in a <span class="No-Break">Juypter Notebook:</span></p>
			<ul>
				<li>The flexibility to launch any number of <span class="No-Break">parallel experiments</span></li>
				<li>We can choose the best hardware for model training, which is very important when accelerators are needed to train deep <span class="No-Break">learning models.</span></li>
				<li>We don’t need active monitoring regarding <span class="No-Break">training progress</span></li>
				<li>There’s no fear of the Jupyter <span class="No-Break">Notebook crashing</span></li>
				<li>Vertex AI training jobs can be configured to log metadata and experiments in the Google Cloud <span class="No-Break">Console UI</span></li>
				<li>In this section, we will create and launch a Vertex AI training job for our experiment. There are two main things we need to do to launch a Vertex AI training job. First, we need to put the dataset in a location that will be accessible to the Vertex AI job (such as GCS or BigQuery). Second, we need to put the model training code together into a single <strong class="source-inline">task.py</strong> file so that it can be packaged into a training container with all the <span class="No-Break">necessary dependencies.</span></li>
			</ul>
			<p>Here are the steps we need to follow to create and launch <a id="_idTextAnchor351"/><a id="_idTextAnchor352"/>our Vertex AI <span class="No-Break">training job:</span></p>
			<ol>
				<li>Upload the dataset to GCS or BigQuery (we will <span class="No-Break">use GCS).</span></li>
				<li>Create a <strong class="source-inline">task.py</strong> file that does <span class="No-Break">the following:</span><ul><li>Reads data <span class="No-Break">from GCS</span></li><li>Does the necessary <span class="No-Break">data preparation</span></li><li>Trains the <span class="No-Break">RF model</span></li><li>Saves the trained model <span class="No-Break">into GCS</span></li><li>Does prediction on the <span class="No-Break">test set</span></li><li>(Optionally) Saves predictions <span class="No-Break">to GCS</span></li><li>Prints <span class="No-Break">some results/metrics</span></li></ul></li>
				<li>Use a prebuilt <span class="No-Break">training image.</span></li>
				<li>Launch Vertex <span class="No-Break">AI training.</span></li>
				<li>Monitor the progress on the Google Cloud <span class="No-Break">Console UI.</span></li>
			</ol>
			<p>Considering these steps, we<a id="_idIndexMarker1158"/> have already created a <strong class="source-inline">task.py</strong> file for our experiment; it can be found in this book’s GitHub repository. Next, we will learn how to launch the job using this <span class="No-Break"><strong class="source-inline">task.py</strong></span><span class="No-Break"> file.</span></p>
			<h2 id="_idParaDest-276"><a id="_idTextAnchor353"/>Setting configurations</h2>
			<p>Here, we will define the <a id="_idIndexMarker1159"/>configurations related to the project and data locations that will be necessary while launching the training job on Vertex AI. The following snippet shows some configurations related to <span class="No-Break">our experiment:</span></p>
			<pre class="source-code">
PROJECT_ID='417xxxxxxx97'
REGION='us-west2'
BUCKET_URI='gs://my-training-artifacts'
DATA_LOCATION='gs://my-training-artifacts/WELFake_Dataset.csv'
# prebuilt training containers
TRAIN_VERSION = "tf-cpu.2-9"
TRAIN_IMAGE = "us-docker.pkg.dev/vertex-ai/training/{}:latest".format(TRAIN_VERSION)</pre>			<p>Let’s initialize the Vertex<a id="_idIndexMarker1160"/> AI SDK with <span class="No-Break">appropriate variables.</span></p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor354"/>Initializing the Vertex AI SDK</h2>
			<p>Here, we’re initializing<a id="_idIndexMarker1161"/> the Vertex AI SDK to <a id="_idIndexMarker1162"/>set the project, location, and staging bucket for <span class="No-Break">our jobs:</span></p>
			<pre class="source-code">
from google.cloud import aiplatform
aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)</pre>			<p>Now that our configurations have been set, we can start defining the Vertex AI <span class="No-Break">training job.</span></p>
			<h2 id="_idParaDest-278"><a id="_idTextAnchor355"/>Defining the Vertex AI training job</h2>
			<p>The following code block <a id="_idIndexMarker1163"/>defines a Vertex AI training job for our experiment. Here, we pass <strong class="source-inline">display_name</strong>, which will help us locate our job within the console UI. Note that we are passing our <strong class="source-inline">task.py</strong> file as the script path variable. <strong class="source-inline">container_uri</strong> is the prebuilt container that will be used to launch the job. Finally, we can specify any additional Python packages that are required to run our training code. In our case, we need to install the <strong class="source-inline">nltk</strong> package for some <span class="No-Break">NLP-related functionalities:</span></p>
			<pre class="source-code">
job = aiplatform.CustomTrainingJob(
    display_name="fake_news_detection",
    script_path="task.py",
    container_uri=TRAIN_IMAGE,
    requirements=["nltk"],
)</pre>			<p>Our Vertex AI-based custom training job is now ready. Let’s <span class="No-Break">run it.</span></p>
			<h2 id="_idParaDest-279"><a id="_idTextAnchor356"/>Running the Vertex AI job</h2>
			<p>We are all set to launch<a id="_idIndexMarker1164"/> our training job. We are using an <strong class="source-inline">n1-standard-16</strong> type of machine for our experiment that can be modified as per our needs. Check out the following snippet, which launches our training job on <span class="No-Break">Vertex AI:</span></p>
			<pre class="source-code">
# Start the training job
model = job.run(
    machine_type = "n1-standard-16",
    replica_count=1,
)</pre>			<p>After launching the job, we should see a URL in the output pointing to the job within the Cloud Console UI. The output should look something <span class="No-Break">like this:</span></p>
			<pre class="source-code">
Training script copied to:
gs://my-training-artifacts/aiplatform-2023-09-04-04:41:36.367-aiplatform_custom_trainer_script-0.1.tar.gz.
Training Output directory:
gs://my-training-artifacts/aiplatform-custom-training-2023-09-04-04:41:36.625
View Training:
<a href="https://console.cloud.google.com/ai/platform/locations/us-west2/training/8404xxxxxxxxxx898?project=417xxxxxxxx7">https://console.cloud.google.com/ai/platform/locations/us-west2/training/8404xxxxxxxxxx898?project=417xxxxxxxx7</a>
CustomTrainingJob projects/417xxxxxxxx7/locations/us-west2/trainingPipelines/840xxxxxxxxxx92 current state:
PipelineState.PIPELINE_STATE_RUNNING
View backing custom job:
<a href="https://console.cloud.google.com/ai/platform/locations/us-west2/training/678xxxxxxxxxxx48?project=417xxxxxxxx7">https://console.cloud.google.com/ai/platform/locations/us-west2/training/678xxxxxxxxxxx48?project=417xxxxxxxx7</a>
CustomTrainingJob projects/417xxxxxxxx7/locations/us-west2/trainingPipelines/840xxxxxxxxxx92 current state:
PipelineState.PIPELINE_STATE_RUNNING</pre>			<p>With that, we have<a id="_idIndexMarker1165"/> successfully launched our experiment as a training job on Vertex. We can now monitor the progress of our job using the Cloud Console UI. Next, we’ll solve this problem using a deep learning approach so that we hopefully get <span class="No-Break">better results.</span></p>
			<h1 id="_idParaDest-280"><a id="_idTextAnchor357"/>BERT-based fake news classification</h1>
			<p>In our first experiment, we trained a classical random forest classifier on TF-IDF features to detect fake versus real news<a id="_idIndexMarker1166"/> articles and got an accuracy score of about 93%. In this section, we will train a deep learning model for the same task and see if we get any accuracy gains over the classical tree-based approach. Deep learning has changed the way we used to solve NLP problems. Classical approaches required hand-crafted features, most of which were related to the frequency of words appearing in a document. Looking at the complexity of languages, just knowing the count of words in a paragraph is not enough. The order in which words occur also has a significant impact on the <a id="_idIndexMarker1167"/>overall meaning of the paragraph or sentence. Deep learning approaches such as <strong class="bold">Long-Short-Term-Memory</strong> (<strong class="bold">LSTM</strong>) also consider the sequential dependency of words in sentences or paragraphs to get a more meaningful feature representation. LSTM has achieved great success in many NLP tasks but there have been some limitations. As these models are trained sequentially, it becomes really difficult to scale these models. Secondly, when we work with very long sequences, LSTMs suffer from context loss and thus they are not ideal for understanding the context of longer sequences. Due to some limitations, including the ones discussed here, new ways of learning context from sequential inputs <span class="No-Break">were invented.</span></p>
			<p>The advent of transformer-based models was groundbreaking for the field of NLP, as well as Vision AI. Transformer-based models heavily rely on attention mechanisms to capture the context and inter-sequence patterns, and they are also able to handle very long sequences of inputs. <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>) is<a id="_idIndexMarker1168"/> a family of NLP models that are based on some parts of the transformer architecture. BERT-based models have achieved great success in tons of NLP tasks, some of which seemed close to impossible in <span class="No-Break">past decades.</span></p>
			<p>Another advantage of working with deep learning models is that we don’t have to train them from scratch every time. We always utilize pre-trained models and then fine-tune them on our domain-specific data to get great results faster and without requiring a lot of domain-specific training data. This <a id="_idIndexMarker1169"/>approach is termed <strong class="bold">transfer learning</strong> and is where large deep learning models are pre-trained with huge amounts of data, after which they can be utilized for many downstream domain-specific tasks as they can be fine-tuned <a id="_idIndexMarker1170"/>with a small amount of domain-specific <span class="No-Break">training data.</span></p>
			<h2 id="_idParaDest-281"><a id="_idTextAnchor358"/>BERT for fake news classification</h2>
			<p>In this experiment, we<a id="_idIndexMarker1171"/> will utilize a pre-trained BERT model and fine-tune it slightly on our news article training dataset. Let’s <span class="No-Break">get started.</span></p>
			<h2 id="_idParaDest-282"><a id="_idTextAnchor359"/>Importing useful libraries</h2>
			<p>In this experiment, we <a id="_idIndexMarker1172"/>will utilize PyTorch as a framework for fine-tuning the BERT model. We also utilize the <strong class="source-inline">transformers</strong> library from Hugging Face to load the pre-trained weights of the BERT-based model with some other tooling that is useful for setting <span class="No-Break">up fine-tuning:</span></p>
			<pre class="source-code">
import torch
from transformers import BertTokenizer
from transformers import BertForSequenceClassification
from transformers import AdamW
from transformers import get_linear_schedule_with_warmup
from torch.utils.data import TensorDataset
from torch.utils.data import random_split, DataLoader</pre>			<p>Now, let’s start preparing <span class="No-Break">the dataset.</span></p>
			<h2 id="_idParaDest-283"><a id="_idTextAnchor360"/>The dataset</h2>
			<p>We will work with the same dataset that we used in the first experiment. So, we will follow the same steps here<a id="_idIndexMarker1173"/> as well – we will load the data, treat NULL values, and create a content column with all the <span class="No-Break">necessary text:</span></p>
			<pre class="source-code">
news_df = pd.read_csv("WELFake_Dataset.csv")
news_df.fillna('', inplace=True)
news_df['content'] = [x + ' ' + y for x,y in zip(news_df.title, news_df.text)]</pre>			<p>Next, we will convert t<a id="_idTextAnchor361"/>he text in the content column <span class="No-Break">to lowercase:</span></p>
			<pre class="source-code">
news_df['content'] = news_df['content'].apply(lambda text: text.lower())</pre>			<p>Now, we will separate text from labels and store them <span class="No-Break">as lists:</span></p>
			<pre class="source-code">
texts = news_df.content.values
labels = news_df.label.values
print(len(texts), len(labels))</pre>			<p>Here’s <span class="No-Break">the output:</span></p>
			<p><span class="No-Break">72134 72134</span></p>
			<p>Now, let’s prepare our dataset as per the requirements of BERT <span class="No-Break">model inputs.</span></p>
			<h2 id="_idParaDest-284"><a id="_idTextAnchor362"/>Data preparation</h2>
			<p>As we are working with the <a id="_idIndexMarker1174"/>BERT model now, we don’t need to perform lots of data cleanups, such as removing numbers, removing stopwords, stemming, and so on. Each BERT model has a tokenizer that is utilized to convert textual data into numeric IDs. So, we will need to find the appropriate BERT tokenizer (which can be loaded by the <strong class="source-inline">transformers</strong> library from Hugging Face), do tokenization, and also create attention masks for <span class="No-Break">training p<a id="_idTextAnchor363"/>urposes.</span></p>
			<p>Let’s create the <span class="No-Break">tokenizer object:</span></p>
			<pre class="source-code">
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')</pre>			<p>Here, we’re defining a function that will create toke<a id="_idTextAnchor364"/>nized text and attention masks <span class="No-Break">for training:</span></p>
			<pre class="source-code">
def prepare_tokenized_data(texts, labs='None'):
    global labels
    input_id_list = []
    attention_masks = []</pre>			<p>Next, we must generate encodings and attention masks for each <span class="No-Break">input text:</span></p>
			<pre class="source-code">
    for text in tqdm_notebook(texts):
        encoded_dict = tokenizer.encode_plus(
            text,
            add_special_tokens = True,
            truncation = 'longest_first',
            max_length = 100,
            pad_to_max_length = True,
            return_attention_mask = True,
            return_tensors = 'pt'
        )
        input_id_list.append(encoded_dict['input_ids'])
       attention_masks.append(encoded_dict['attention_mask'])</pre>			<p>Now, let’s convert the<a id="_idIndexMarker1175"/> lists into <span class="No-Break">PyTorch tensors:</span></p>
			<pre class="source-code">
    input_id_list = torch.cat(input_id_list, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    if labs != 'None':
        labels = torch.tensor(labels)
        return input_id_list, attention_masks, labels
    else:
        return input_id_list, attention_masks</pre>			<p>Here, we’re calling the necessary method, to prepare <span class="No-Break">our data:</span></p>
			<pre class="source-code">
input_id_list, attention_masks, labels = (
    prepare_tokenized_data(texts, labels)
)</pre>			<p>Now, let’s split our <a id="_idIndexMarker1176"/>data into training and <span class="No-Break">test partitions.</span></p>
			<h2 id="_idParaDest-285"><a id="_idTextAnchor365"/>Splitting the data</h2>
			<p>Here, we will create a<a id="_idIndexMarker1177"/> tensor dataset out of our input IDs, attention masks, and labels and split it into training and test sets. Similar to the first experiment, we will be using about 80% of the data for training (or fine-tuning) purposes and the remaining 20% to test the results and metrics. The following snippet shows how to create and split the <span class="No-Break">tensor dataset:</span></p>
			<pre class="source-code">
tensor_dataset = TensorDataset(input_id_list, attention_masks, labels)
# lets keep 80% articles for training and 20% for test
train_size = int(0.8 * len(tensor_dataset))
test_size = len(tensor_dataset) - train_size
train_data, test_data = random_split(tensor_dataset, [train_size, test_size])
print(len(train_data.indices), len(test_data.indices))</pre>			<p>Here’s <span class="No-Break">the output:</span></p>
			<pre class="source-code">
(57707, 14427)</pre>			<p>Now, let’s define data loader objects with the required <span class="No-Break">batch size.</span></p>
			<h2 id="_idParaDest-286"><a id="_idTextAnchor366"/>Creating data loader objects for batching</h2>
			<p>The next step is to <a id="_idIndexMarker1178"/>create data loader objects for both the training and test partitions. We will have a batch size of 32 for the traini<a id="_idTextAnchor367"/>ng data and a batch size of 1 for the <span class="No-Break">test data:</span></p>
			<pre class="source-code">
batch_size = 32
num_workers = 4
train_data_loader = DataLoader(
    dataset=train_data,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
)
# test data loader with batch size of 1
test_data_loader = DataLoader(
    dataset=test_data,
    batch_size=1,
    shuffle=False,
)</pre>			<p>Our data is now<a id="_idIndexMarker1179"/> ready for the model. This means we can load the model and <span class="No-Break">start training.</span></p>
			<h2 id="_idParaDest-287"><a id="_idTextAnchor368"/>Loading the pre-trained BERT model</h2>
			<p>Here, we will load the<a id="_idIndexMarker1180"/> pre-trained weights of the BERT-based model so that we can fine-tune it further on our custom dataset. The pre-trained weights of many BERT variants are available on Hugging Face and can be loaded through the <strong class="source-inline">transformers</strong> library, as shown in the <span class="No-Break">following snippet:</span></p>
			<pre class="source-code">
device = 'cpu'
bert_model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2,
    output_attentions=False,
    output_hidden_states=False,
)
bert_model.to(device)</pre>			<p>Using the <strong class="source-inline">device</strong> variable, we can choose to load our model on an accelerator, such as a GPU. This snippet downloads the pre-trained weights of the <strong class="source-inline">bert-base-uncased</strong> model with a classification layer of two labels. Executing this snippet also prints the BERT architecture summary, which will look something similar to <span class="No-Break">the following:</span></p>
			<pre class="source-code">
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
   . . . . . . . . . . .
   . . . . . . . . . . .</pre>			<p>Now that our model <a id="_idIndexMarker1181"/>has been loaded, let’s define the <span class="No-Break">optimization settings.</span></p>
			<p><span class="No-Break">Optimizer</span></p>
			<p>Here, we’re defining the <a id="_idTextAnchor369"/><strong class="source-inline">AdamW</strong> optimizer and setting a custom <span class="No-Break">learning rate:</span></p>
			<pre class="source-code">
optimizer = torch.optim.AdamW(
    bert_model.parameters(),
    lr=6e-6,
    eps=1e-8,
)</pre>			<p>Let’s also define a scheduler for our <span class="No-Break">model training.</span></p>
			<h2 id="_idParaDest-288"><a id="_idTextAnchor370"/>Scheduler</h2>
			<p>Here, we’re setting up training <a id="_idIndexMarker1182"/>steps and a training scheduler. We’re planning to fine-tune our model for just <strong class="source-inline">3</strong> epochs on our training partition, af<a id="_idTextAnchor371"/>ter which we will check the results on our <span class="No-Break">test set:</span></p>
			<pre class="source-code">
num_epochs = 3
steps_per_epoch = len(train_data_loader)
total_steps = steps_per_epoch * num_epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps = 0,
    num_training_steps = total_steps,
)</pre>			<p>Now, we are all set to start <a id="_idIndexMarker1183"/>training <span class="No-Break">the model.</span></p>
			<h2 id="_idParaDest-289"><a id="_idTextAnchor372"/>Training BERT</h2>
			<p>Here, we will<a id="_idIndexMarker1184"/> fine-tune the BERT model on our training data for <strong class="source-inline">3</strong> epochs, as defined in the <span class="No-Break">previous sub-section:</span></p>
			<pre class="source-code">
bert_model.train()
for epoch in range(num_epochs):
    total_loss = 0
    for i, (ids, masks, labels) in enumerate(train_data_loader):
        ids = ids.to(device)
        masks = masks.to(device)
        labels = labels.to(device)
        loss = bert_model(ids, token_type_ids=None, attention_mask=masks, labels=labels)[0]
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()
    print('Epoch: {}, Loss: {:.4f}'.format(epoch+1, total_loss / steps_per_epoch))</pre>			<p>This training snippet prints the model loss after each epoch of training is completed. The loss output from our experiment is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
Epoch: 1, Loss: 0.0803
Epoch: 2, Loss: 0.0229
Epoch: 3, Loss: 0.0112</pre>			<p>Now that our model training (or fi<a id="_idTextAnchor373"/>ne-tuning) is complete, we can save the <span class="No-Break">model weights:</span></p>
			<pre class="source-code">
# save trained model locally
torch.save(bert_model.state_dict(), 'BERT.ckpt')</pre>			<p>With that, we<a id="_idIndexMarker1185"/> have successfully trained and saved the model. Now, let’s move on to <span class="No-Break">model evaluation.</span></p>
			<h2 id="_idParaDest-290"><a id="_idTextAnchor374"/>Loading model weights for evaluation</h2>
			<p>Here, we’re loading our<a id="_idIndexMarker1186"/> trained model weights for <span class="No-Break">evaluation purposes:</span></p>
			<pre class="source-code">
bert_model.eval()
bert_model.load_state_dict(
    torch.load('BERT.ckpt'),
)
Output:
&lt;All keys matched successfully&gt;</pre>			<p>Let’s check the accuracy of the trained model on the <span class="No-Break">test dataset.</span></p>
			<h2 id="_idParaDest-291"><a id="_idTextAnchor375"/>Calculating the accuracy of the test dataset</h2>
			<p>Now that our <a id="_idIndexMarker1187"/>model has been trained and loaded for evaluation, we can make predictions on the test dataset and check <span class="No-Break">its accuracy.</span></p>
			<p>We will store the predictions in lists and also count the number of correct predictions in the <span class="No-Break">following variables:</span></p>
			<pre class="source-code">
correct_predictions = 0
predictions = []
reals = []</pre>			<p>Here, we’re running model predictions on the <span class="No-Break">test data:</span></p>
			<pre class="source-code">
for i, (ids, masks, labels) in enumerate(test_data_loader):
    ids = ids.to(device)
    masks = masks.to(device)
    labels = labels.to(device)
    bert_out = bert_model(ids, token_type_ids=None, attention_mask=masks, labels=labels)[1]
    prediction = torch.max(bert_out, 1)[1][0].item()
    true_label = labels[0].item()
    correct_predictions += int(prediction == true_label)
    predictions.append(prediction)
    reals.append(true_label)</pre>			<p>We can calculate<a id="_idIndexMarker1188"/> the accuracy by dividing the correct predictions by the <span class="No-Break">total predictions:</span></p>
			<pre class="source-code">
avg_correct_predictions = correct_predictions / len(test_data)
print('Accuracy: {:.4f}\n'.format(avg_correct_predictions))</pre>			<p>The output of this cell shows that our model has about 99% accuracy on the test dataset. This is a huge improvement over the classic random <span class="No-Break">forest model:</span></p>
			<pre class="source-code">
Accuracy: 0.9902</pre>			<p>Finally, let’s print the <span class="No-Break">confusion matrix:</span></p>
			<pre class="source-code">
print(confusion_matrix(reals, predictions,))</pre>			<p>Here is <span class="No-Break">the output:</span></p>
			<pre class="source-code">
[[7025   53]
 [  88 7261]]</pre>			<p>Now, we can generate a classification report for <span class="No-Break">our model.</span></p>
			<h2 id="_idParaDest-292"><a id="_idTextAnchor376"/>Classification report</h2>
			<p>Finally, we <a id="_idIndexMarker1189"/>will print the classification report of our experiment to understand the precision, recall, and F1 score of <span class="No-Break">each class:</span></p>
			<pre class="source-code">
print(
    classification_report(
        reals,
        predictions,
        target_names=['Real', 'Fake'],
    )
)</pre>			<p>Here’s <span class="No-Break">the output:</span></p>
			<pre class="source-code">
              precision    recall  f1-score   support
        Real       0.99      0.99      0.99      7078
        Fake       0.99      0.99      0.99      7349
    accuracy                           0.99     14427
   macro avg       0.99      0.99      0.99     14427
weighted avg       0.99      0.99      0.99     14427</pre>			<p>The preceding <a id="_idIndexMarker1190"/>output shows that our BERT-based classification model is extremely accurate with an accuracy of about 99%. Similarly, for each class, we have precision and recall scores of about 99%. This experiment showed that using a pre-trained deep learning model can enhance the accuracy of classification by a <span class="No-Break">great margin.</span></p>
			<h1 id="_idParaDest-293"><a id="_idTextAnchor377"/>Summary</h1>
			<p>This chapter was about a real-world NLP use case for detecting fake news. In the current era of the internet, spreading fake news has become quite easy and it can be dangerous for the reputation of a person, society, organization, or political party. As we have seen in our experiments, ML classification can be used as a powerful tool for detecting fake news articles. Deep learning-based approaches can further improve the results of text classification use cases without requiring much <span class="No-Break">fine-tuning data.</span></p>
			<p>After reading this chapter, you should be confident about training and applying classification models on text classification use cases, similar to fake news detection. You should also have a good understanding of the cleaning and pre-processing steps that are needed to apply classical models, such as random forest, on text data. At this point, you should be able to launch large-scale ML experiments as Vertex AI training jobs. Finally, you should have a good understanding of how deep learning-based BERT models can be applied and fine-tuned for text classification <span class="No-Break">use cases.</span></p>
		</div>
	</div>
</div>
</body></html>