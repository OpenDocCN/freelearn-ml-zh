- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bringing Your Own Models for Database Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we’ve covered the process of training models natively using **Redshift
    Machine Learning** (**Redshift ML**). However, there may be instances where you
    need to utilize models built outside of Redshift. To address this, Redshift ML
    offers the **Bring Your Own Model** (**BYOM**) feature, allowing users to integrate
    their Amazon SageMaker machine learning models with Amazon Redshift. This feature
    facilitates making predictions and performing other machine learning tasks on
    data stored in the warehouse, without requiring data movement.
  prefs: []
  type: TYPE_NORMAL
- en: 'BYOM offers two approaches: **local inference** and **remote inference**. In
    this chapter, we’ll delve into the workings of BYOM and explore the various options
    available for creating and integrating BYOM. You’ll be guided through the process
    of building a machine learning model in Amazon SageMaker, and subsequently, employing
    Redshift ML’s BYOM feature to bring that model to Redshift. Moreover, you’ll learn
    how to apply these models to the data stored in Redshift’s data warehouse to make
    predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be proficient in bringing Amazon SageMaker-created
    models and executing predictions within Amazon Redshift. Utilizing BYOM, you can
    deploy models such as **XGBoost** and a **multilayer perceptron** (**MLP**) to
    Redshift ML. Once a pre-trained model is deployed on Redshift ML, you can run
    inferences locally on Redshift without relying on a SageMaker endpoint or SageMaker
    Studio. This simplicity empowers data analysts to conduct inference on new data
    using models created externally to Redshift, eliminating concerns about accessing
    SageMaker’s services.
  prefs: []
  type: TYPE_NORMAL
- en: This method significantly speeds up the delivery of machine learning models
    created outside of Redshift to the data team. Furthermore, since Redshift ML interacts
    with native Redshift SQL, the user experience for the data team remains consistent
    with other data analysis work performed on the data warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will go through the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of BYOM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supported model types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BYOM for local inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BYOM for remote inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires a web browser and access to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An AWS account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Amazon Redshift Serverless endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Amazon SageMaker notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Redshift Query Editor v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completing the *Getting started with Amazon Redshift Serverless* section in
    [*Chapter 1*](B19071_01.xhtml#_idTextAnchor015)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the code used in this chapter here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data files required for this chapter are located in a public S3 bucket:
    `s3://packt-serverless-ml-redshift/`.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of BYOM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With Amazon Redshift ML, you can use an existing ML model built in Amazon SageMaker
    and use it in Redshift without having to retrain it. To use BYOM, you need to
    provide model artifacts or a SageMaker endpoint, which takes a batch of data and
    returns predictions. BYOM is useful in cases where a machine learning model is
    not yet available in Redshift ML, for example, at the time of writing this book,
    a Random Cut Forest model is not yet available in Redshift ML, so you can build
    this model in SageMaker and easily bring it to Redshift and then use it against
    the data stored in Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some specific benefits of using Redshift ML with your own ML model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved efficiency**: By using an existing ML model, you can save time and
    resources that would otherwise be spent on training a new model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easy integration**: Redshift ML makes it easy to integrate your ML model
    into your data pipeline, allowing you to use it for real-time predictions or batch
    predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Redshift ML is built on top of the highly scalable and performant
    Amazon Redshift data warehouse, so you can use your ML model to make predictions
    on large datasets without worrying about performance issues'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supported model types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Amazon Redshift ML supports a wide range of machine learning models through
    the BYOM feature. Some common types of models that can be used with BYOM include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear regression models**: These models are like number predictors. They
    take into account several factors or features and use them to guess a specific
    numerical outcome. For example, if you want to predict the price of a house, a
    linear regression model would consider factors such as the size of the house,
    the number of rooms, and the location to estimate the house’s price.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logistic regression models**: These models are binary outcome predictors.
    Instead of guessing numbers, they answer *yes* or *no* questions or make *0*/*1*
    predictions. For instance, if you want to predict whether a student will pass
    or fail an exam, a logistic regression model would consider factors such as the
    student’s study hours, previous test scores, and attendance to determine the likelihood
    of passing the exam.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision tree models**: These are used to make predictions based on a tree-like
    structure. Think of it like a decision-making tree for predictions. You start
    at the top and follow branches based on known features. At each branch, you make
    a decision based on a feature and keep going until you reach a final prediction
    at the leaves. It’s a step-by-step process to find the most likely outcome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forest models**: These are ensembles of decision trees. Groups of
    decision trees work together. Each tree is trained on a different part of the
    data. To make a prediction, all the trees give their answers, and their predictions
    are averaged to get the final result. It’s like taking the opinions of multiple
    trees to make a more accurate guess.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient boosting models**: These are also ensembles of decision trees, These
    are groups of decision trees that work together, but here, unlike in a random
    forest model, the trees are trained one after the other, and each tree tries to
    fix the mistakes of the previous one. They learn from each other’s errors and
    become better as a team. It’s like a learning process where they keep improving
    until they make good predictions together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural network models**: These are complex, multi-layered models that are
    able to learn complex patterns in data. These models are capable of learning intricate
    patterns in data. They operate using a process of information analysis, discovering
    underlying correlations similar to the functioning of interconnected neurons in
    the human brain. Through extensive training and exposure to diverse datasets,
    the model refines its ability to decipher complex patterns, making it proficient
    in uncovering intricate relationships within new data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support vector machines** (**SVMs**): SVMs are powerful classifiers, acting
    like incredibly intelligent dividers. Imagine a 3D space with points representing
    different things. SVMs determine the most optimal way to draw a line or plane,
    called a hyperplane, that perfectly separates two distinct groups of points. It’s
    as if they possess an extraordinary ability to find the perfect boundary, ensuring
    the two groups are kept as far apart as possible, such as drawing an invisible
    but flawless line that keeps everything perfectly organized on each side.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of the types of models that can be used with BYOM
    in Amazon Redshift. In general, any model that can be represented as a set of
    model artifacts and a prediction function can be used with BYOM in Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: We have learned what Redshift ML BYOM is and its benefits. In the next section,
    you will create a BYOM local inference model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the BYOM local inference model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With BYOM local inference, the machine learning model and its dependencies are
    packaged into a group of files and deployed to Amazon Redshift where the data
    is stored, allowing users to make predictions on the stored data. Model artifacts
    and their dependencies are created when a model is trained and created on the
    Amazon SageMaker platform. By deploying the model directly onto the Redshift service,
    you are not moving the data over the network to another service. Local inference
    can be useful for scenarios where the data is sensitive or requires low latency
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start working on creating the BYOM local inference model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a local inference model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create the BYOM local inference model, the first step involves training
    and validating an Amazon SageMaker model. For this purpose, we will train and
    validate an XGBoost linear regression machine learning model on Amazon SageMaker.
    Follow the instructions found here to create the Amazon SageMaker model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: After you have followed the instructions given at the preceding URL, validate
    the model by running prediction functions. Now, let’s move on to the next steps.
    After successfully generating the predictions, we will create the Redshift ML
    model. Using the same notebook, let’s run a few commands to set some parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the model and running predictions on Redshift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, validate the model by running prediction functions.
  prefs: []
  type: TYPE_NORMAL
- en: With the model trained and validated in SageMaker, it’s time to import it into
    Redshift. In the next section, using the same SageMaker notebook, we will set
    up the required parameters to build the Redshift `CREATE MODEL` statement. You
    will use this statement in Query Editor v2 to create your model in Redshift ML,
    enabling you to perform local inference on the data stored in the Redshift cluster
    with the integrated SageMaker model.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before setting up the parameters, run the following command in Query Editor
    v2 to create the schema for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step of this process is setting up the following parameter values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`S3_BUCKET` is used to store Redshift ML artifacts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MODEL_PATH` is the S3 location of the model artifact of the Amazon SageMaker
    model. Optionally, you can print `model_data` using the `print` function in Python
    and look at the artifact location.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`REDSHIFT_IAM_ROLE` is the cluster role:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will generate the `CREATE MODEL` statement that you are going to run
    on Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the CREATE MODEL statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Execute the code provided here in a Jupyter notebook to automatically generate
    the `CREATE` `MODEL` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding statement is the `CREATE MODEL` statement that you
    are going to run in Query Editor v2\. Please copy the statement and head over
    to Query Editor v2 to perform the remaining steps.
  prefs: []
  type: TYPE_NORMAL
- en: Running local inference on Redshift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is the `CREATE MODEL` statement. You should have a similar one
    generated, where `FROM`, `IAM_ROLE`, and `S3_BUCKET` have different values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding command, the `FROM` clause takes `model_data` as input, which
    contains the SageMaker model artifacts. When this command is run, Amazon Redshift
    ML compiles the model, deploys it to Redshift, and creates a `predict_abalone_age`
    prediction function, which is used in an SQL command to generate predictions natively
    in Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the `CREATE MODEL` statement is completed, you can use the `show model`
    command to see the model’s status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Local inference model metadata](img/B19071_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Local inference model metadata
  prefs: []
  type: TYPE_NORMAL
- en: Notice that **Model State** is **READY** and **S3 Model Path** is the one we
    gave when creating the model. **Inference Type** is **Local**, which means the
    model type is local inference.
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully created the local inference model; now, let’s prepare a
    test dataset to test whether the local inference is working without any issues.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Load the test data from the S3 bucket to a Redshift table to test our local
    inference model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please update `IAM_ROLE`. Do not change the S3 bucket location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to create the table and load the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample the test table to make sure the data is loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the sample dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Showing sample records from the test dataset](img/B19071_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Showing sample records from the test dataset
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have loaded the test data, let’s run the `SELECT` command, which
    invokes the `predict_abalone_age` function.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, call the prediction function that was created as part of the `CREATE`
    `MODEL` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output of the predictions generated using local inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Showing actual versus predicted values](img/B19071_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Showing actual versus predicted values
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully trained and validated a SageMaker model and then deployed
    it to Redshift ML. We also generated predictions using the local inference function.
    This demonstrates Redshift’s BYOM local inference feature.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you are going to learn about the BYOM remote inference
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: BYOM using a SageMaker endpoint for remote inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore how to create a BYOM remote inference for an
    Amazon SageMaker Random Cut Forest model. This means you are bringing your own
    machine learning model, which is trained on data outside of Redshift, and using
    it to make predictions on data stored in a Redshift cluster using an endpoint.
    In this method, to use BYOM for remote inference, a machine learning model is
    trained, an endpoint is created in Amazon SageMaker, and then the endpoint is
    accessed from within a Redshift query using SQL functions provided by the Amazon
    Redshift ML extension.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method is useful when Redshift ML does not natively support models, for
    example, a Random Cut Forest model. You can read more about Random Cut Forest
    here: [https://tinyurl.com/348v8nnw](https://tinyurl.com/348v8nnw).'
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate this feature, you will first need to follow the instructions
    found in this notebook ([https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb))
    to create a Random Cut Forest machine learning model using Amazon SageMaker to
    detect anomalies. Please complete the Amazon SageMaker model training and validate
    the model to make sure the endpoint is working and then proceed to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating BYOM remote inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have validated that the SageMaker endpoint is deployed and working
    properly, let’s define a `CREATE MODEL` reference point inside Redshift by specifying
    the SageMaker endpoint. Using the same notebook, let’s build the `CREATE MODEL`
    statement in Jupyter and run it in Query Editor v2.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start by setting up the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`S3_Bucket` is used to store Redshift ML artifacts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SAGEMAKER_ENDPOINT` is the model endpoint on the SageMaker side to run inferences
    against'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`REDSHIFT_IAM_ROLE` is the cluster role:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please update `REDSHIFT_IAM_ROLE` with your Redshift cluster role.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the BYOM remote inference command
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s generate the `CREATE MODEL` statement by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You have finished the work with the Jupyter notebook. Now you have a pre-trained
    model in Amazon SageMaker and the next step is to bring it into Redshift ML. To
    do so, access Query Editor v2, connect to the Serverless endpoint, and run the
    commands outlined next.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Query Editor v2, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Retrieve the model metadata by running the `show` `model` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Remote inference model metadata](img/B19071_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Remote inference model metadata
  prefs: []
  type: TYPE_NORMAL
- en: Notice that in the model metadata, the **Model State** parameter is set to **READY**,
    indicating that the model is deployed. The **Endpoint** name is **randomcutforest-2022-12-31-03-48-13-259**.
    **Inference Type** is set to **Remote** inference. When this model is run, Redshift
    ML sends data stored in Redshift in batches to SageMaker, where inferences are
    generated. Generated predicted values are then sent back to Redshift, which are
    eventually presented to the user.
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully deployed the model. In the next section, let’s run predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The data preparation script
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following code snippet shows the data preparation script that you will
    need to run on Redshift. We will create the table that will be used to run inference
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please update the `IAM_ROLE` parameter with your Redshift cluster attached role.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample the data to make sure data is loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Showing sample records from the test dataset](img/B19071_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Showing sample records from the test dataset
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the remote inference endpoint and test dataset, let’s invoke
    the prediction function.
  prefs: []
  type: TYPE_NORMAL
- en: Computing anomaly scores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let’s compute the anomaly scores from the entire taxi dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the remote inference predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Showing remote function prediction values](img/B19071_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Showing remote function prediction values
  prefs: []
  type: TYPE_NORMAL
- en: The preceding output shows the anomalous score for different days and the number
    of passengers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we will print any data points with scores greater
    than 3 and standard deviations (approximately the 99.9th percentile) from the
    mean score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Showing unacceptable anomaly scores](img/B19071_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Showing unacceptable anomaly scores
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding results, we see that some days’ ridership is way higher and
    our remote inference function is flagging them as anomalous. This concludes the
    section on bringing remote inference models into Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the benefits and use cases of Amazon Redshift
    ML BYOM for local and remote inference. We created two SageMaker models and then
    imported them into Redshift ML as local inference and remote inference model types.
    We loaded test datasets in Redshift and then we ran the prediction functions and
    validated both types. This demonstrates how Redshift simplifies and empowers the
    business community to perform inference on new data using models created outside.
    This method speeds up the delivery of machine learning models created outside
    of Redshift to the data warehouse team.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you are going to learn about Amazon Forecast, which enables
    you to perform forecasting using Redshift ML.
  prefs: []
  type: TYPE_NORMAL
