<html><head></head><body>
		<div id="_idContainer092">
			<h1 id="_idParaDest-139"><em class="italic"><a id="_idTextAnchor146"/>Chapter 7</em>: Online Regression</h1>
			<p>After looking at online anomaly detection and online classification throughout the previous chapters, there is one large category of online machine learning that remains to be seen. <strong class="bold">Regression</strong> is the family of supervised machine learning models that applies to use cases in which the target variable is numerical.</p>
			<p>In anomaly detection and classification, you have seen how to build models to predict categorical targets (yes/no and iris species), but you have not yet seen how to work with a target that is numerical. Working with numerical data requires having methods that work differently, both in the deeper layers of model training and model definition and also in our use of metrics. </p>
			<p>Imagine being a weather forecaster trying to forecast the temperature (Celsius) for tomorrow. Maybe you expect a sunny day, and you have a model that you use to predict a temperature of 25 degrees Celsius. Imagine if the next day, you observe that it is cold and only 18 degrees; you were clearly wrong. </p>
			<p>Now, imagine that you predicted 24 degrees. In a classification use case, you may tend to say that 25 is not 24, so the result is wrong. However, the result of 24 is <em class="italic">less wrong</em> than the result of 18.</p>
			<p>In regression, one single prediction can be more or less wrong. In practice, you will rarely be entirely right. In classification, you are either wrong or right, so this is different. This introduces a need for new metrics and a change in the model benchmarking process.</p>
			<p>In this chapter, you will first get a deeper introduction to regression models, focusing on online regression models in River. After that, you'll be working on a regression model benchmark.</p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>Defining regression</li>
				<li>Use cases of regression</li>
				<li>Overview of regression algorithms in River</li>
			</ul>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor147"/>Technical requirements</h1>
			<p>You can find all the code for this book on GitHub at the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python">https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python</a>. If you are not yet familiar with Git and GitHub, the easiest way to download the notebooks and code samples is the following:</p>
			<ol>
				<li>Go to the link of the repository.</li>
				<li>Go to the green <strong class="bold">Code</strong> button.</li>
				<li>Select <strong class="bold">Download ZIP</strong>.</li>
			</ol>
			<p>When you download the ZIP file, unzip it in your local environment, and you will be able to access the code through your preferred Python editor.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor148"/>Python environment</h2>
			<p>To follow along with this book, you can download the code in the repository and execute it using your preferred Python editor.</p>
			<p>If you are not yet familiar with Python environments, I would advise you to check out Anaconda (<a href="https://www.anaconda.com/products/individual">https://www.anaconda.com/products/individual</a>), which comes with Jupyter Notebook and JupyterLab, which are both great for executing notebooks. It also comes with Spyder and VS Code for editing scripts and programs.</p>
			<p>If you have difficulty installing Python or the associated programs on your machine, you can check out Google Colab (<a href="https://colab.research.google.com/">https://colab.research.google.com/</a>) or Kaggle Notebooks (<a href="https://www.kaggle.com/code">https://www.kaggle.com/code</a>), which both allow you to run Python code in online notebooks for free, without any setup required.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor149"/>Defining regression</h1>
			<p>In this chapter, you will discover regression. Regression is a supervised machine learning task in which a model is constructed that predicts or estimates a numerical target variable based on numerical or categorical independent variables.</p>
			<p>The simplest type of regression model is <a id="_idIndexMarker368"/><strong class="bold">linear regression</strong>. Let's consider a super simple example of how a linear regression could be used for regression.</p>
			<p>Imagine that we have a dataset in which we have observations of 10 people. Based on the number of hours they study per week, we have to estimate their average grade (on a 1 to 10 scale). Of course, this is a strongly oversimplified problem.</p>
			<p>The data looks as follows:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-1</p>
			<pre class="source-code">import pandas as pd</pre>
			<pre class="source-code">nb_hrs_studies = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</pre>
			<pre class="source-code">avg_grade = [5.5, 5.8, 6.8, 7.2, 7.4, 7.8, 8.2, 8.8, 9.3, 9.4]</pre>
			<pre class="source-code">data = pd.DataFrame({'nb_hrs_studies': nb_hrs_studies, 'avg_grade': avg_grade})</pre>
			<pre class="source-code">data</pre>
			<p>You will obtain the following data frame:</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B18335_07_1.jpg" alt="Figure 7.1 – The dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – The dataset</p>
			<p>Let's plot the data to see how this can be made into a <a id="_idIndexMarker369"/>regression problem:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-2</p>
			<pre class="source-code">import matplotlib.pyplot as plt</pre>
			<pre class="source-code">plt.scatter(data['nb_hrs_studies'], data['avg_grade'])</pre>
			<pre class="source-code">plt.xlabel('nb_hrs_studies')</pre>
			<pre class="source-code">plt.ylabel('avg_grades')</pre>
			<p>This results in the following output:</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B18335_07_2.jpg" alt="Figure 7.2 – A scatter plot of the data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – A scatter plot of the data</p>
			<p>Now, the goal of linear regression is to fit the line (or hyperplane) that best goes through these points and is able to predict an estimated <strong class="source-inline">avg_grades</strong> for any <strong class="source-inline">nb_hrs_studies</strong>. Other regression models each have their specific way to construct the prediction function, but eventually have the same goal: creating the best fitting formula to predict a numerical target variable using one or more independent variables.</p>
			<p>In the next section, you'll discover some example use cases in which regression can be used.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor150"/>Use cases of regression</h1>
			<p>The use cases of <a id="_idIndexMarker370"/>regression are huge: it is a very commonly used method in many projects. Still, let's see some examples to get a better idea of the different types of use cases that can benefit from regression models.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor151"/>Use case 1 – Forecasting</h2>
			<p>A very common use case for <a id="_idIndexMarker371"/>regression algorithms is forecasting. In forecasting, the goal is to predict future values of a variable that is measured over time. Such variables are called <a id="_idIndexMarker372"/><strong class="bold">time series</strong>. Although a number of specific methods exist for time series modeling, regression models are also great contenders for obtaining good performance on future prediction performance.</p>
			<p>In some forecasting use cases, real-time responses are very important. An example is stock trading, in which the datapoints of stock prices arrive at a huge velocity and forecasts have to be adapted straight away to use the best possible information for stock trades. Even automated stock trading algorithms exist, and they need to react fast in order to make the most profit on their trades as possible.</p>
			<p>For further reading on this topic, you could start by checking out the following links:</p>
			<ul>
				<li><a href="https://www.investopedia.com/articles/financial-theory/09/regression-analysis-basics-business.asp">https://www.investopedia.com/articles/financial-theory/09/regression-analysis-basics-business.asp</a></li>
				<li><a href="https://www.mathworks.com/help/econ/time-series-regression-vii-forecasting.html">https://www.mathworks.com/help/econ/time-series-regression-vii-forecasting.html</a></li>
			</ul>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor152"/>Use case 2 – Predicting the number of faulty products in manufacturing</h2>
			<p>The second example of real-time and <a id="_idIndexMarker373"/>streaming regression models being used in practice is the application of predictive maintenance models in manufacturing. For example, you could use a real-time prediction of the number of faulty products per hour in a production line. This would be a regression model as well, as the outcome is a number rather than a categorical variable.</p>
			<p>The production line could use this prediction for a real-time alerting system, for example, once a threshold of faulty products is predicted to be reached. Real-time data integration is important for this, as having the wrong products being produced is a large waste of resources.</p>
			<p>The following two resources will allow you to read more about this use case:</p>
			<ul>
				<li><a href="https://www.sciencedirect.com/science/article/pii/S2405896316308084">https://www.sciencedirect.com/science/article/pii/S2405896316308084</a></li>
				<li><a href="https://www.researchgate.net/publication/315855789_Regression_Models_for_Lean_Production">https://www.researchgate.net/publication/315855789_Regression_Models_for_Lean_Production</a></li>
			</ul>
			<p>Now that we have explored some use cases of <a id="_idIndexMarker374"/>regression, let's get started with the various algorithms that we have for regression.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor153"/>Overview of regression algorithms in River</h1>
			<p>There is a large number of online regression models available in the River online machine learning package.</p>
			<p>A selection of relevant ones are as follows:</p>
			<ul>
				<li><strong class="source-inline">LinearRegression</strong></li>
				<li><strong class="source-inline">HoeffdingAdaptiveTreeRegressor</strong></li>
				<li><strong class="source-inline">SGTRegressor</strong></li>
				<li><strong class="source-inline">SRPRegressor</strong></li>
			</ul>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor154"/>Regression algorithm 1 – LinearRegression</h2>
			<p>Linear regression is one of the most basic regression models. A simple linear regression is a regression model that fits a straight line through the datapoints. The following graph illustrates this:</p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B18335_07_3.jpg" alt="Figure 7.3 – A linear model in a scatter plot&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – A linear model in a scatter plot</p>
			<p>This orange line is a result of the following formula:</p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/Formula_07_001.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <em class="italic">y</em> represents <strong class="source-inline">avg_grades</strong> and <em class="italic">x</em> represents <strong class="source-inline">nb_hrs_studies</strong>. When fitting the model, the <em class="italic">a</em> and <em class="italic">b</em> coefficients are estimates. The <em class="italic">b</em> coefficient in this formula is called the<a id="_idIndexMarker375"/> intercept. It indicates<a id="_idIndexMarker376"/> the value of <em class="italic">y</em> when <em class="italic">x</em> equals <strong class="source-inline">0</strong>. The <em class="italic">a</em> coefficient represents the slope of the line. For each additional step in <em class="italic">x</em>, <em class="italic">a</em> indicates the amount that is added to <em class="italic">y</em>.</p>
			<p>This is a version of linear regression, but there is also a version called <strong class="bold">multiple linear regression</strong>, in which there are<a id="_idIndexMarker377"/> multiple <em class="italic">x</em> variables. In this case, the model does not represent a line but rather a hyperplane, in which a slope coefficient is added for each additional <em class="italic">x</em> variable.</p>
			<h3>Linear regression in River</h3>
			<p>Let's now move on to build an<a id="_idIndexMarker378"/> example of online linear regression using River ML in Python:</p>
			<ol>
				<li value="1">If you remember from the <a id="_idIndexMarker379"/>previous example, we used a function called <strong class="source-inline">make_classification</strong> from <strong class="source-inline">scikit-learn</strong>. The same can be<a id="_idIndexMarker380"/> done for regression problems using <strong class="source-inline">make_regression</strong>:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-3</p>
			<p class="source-code">from sklearn.datasets import make_regression</p>
			<p class="source-code">X,y = make_regression(n_samples=1000,n_features=5,n_informative=5,noise=100)</p>
			<ol>
				<li value="2">To get a better idea of what has resulted from this <strong class="source-inline">make_regression</strong> function, let's inspect <strong class="source-inline">X</strong> of this dataset. You can use the following code to get a quick overview of the data:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-4</p>
			<p class="source-code">pd.DataFrame(X).describe()</p>
			<p>The <strong class="source-inline">describe()</strong> method will put out a data frame with descriptive statistics of the variables, as follows:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B18335_07_4.jpg" alt="Figure 7.4 – Descriptive statistics&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Descriptive statistics</p>
			<p>There are five<a id="_idIndexMarker381"/> columns in the <strong class="source-inline">X</strong> data, and there are 1,000 observations.</p>
			<ol>
				<li value="3">Now, to look at the <strong class="source-inline">y</strong> variable, also called<a id="_idIndexMarker382"/> the <strong class="source-inline">target</strong> variable, we <a id="_idIndexMarker383"/>can make a histogram as follows:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-5</p>
			<p class="source-code">pd.Series(y).hist()</p>
			<p>The resulting histogram can be seen in the following figure:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B18335_07_5.jpg" alt="Figure 7.5 – The resulting histogram&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – The resulting histogram</p>
			<p>There is much more exploratory data analysis that could be done here, but that would be out of <a id="_idIndexMarker384"/>scope for this book. </p>
			<ol>
				<li value="4">Let's now move on <a id="_idIndexMarker385"/>to the creation of a train and test set to create a fair model validation approach. In the following code, you can see how to create the <strong class="source-inline">train_test_split</strong> function from <strong class="source-inline">scikit-learn</strong> to create<a id="_idIndexMarker386"/> a train-test split:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-6</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</p>
			<ol>
				<li value="5">You can create the linear regression in River using the following code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-7</p>
			<p class="source-code">!pip install river</p>
			<p class="source-code">from river.linear_model import LinearRegression</p>
			<p class="source-code">model = LinearRegression()</p>
			<ol>
				<li value="6">This model then has to be fitted to the training data. We use the same loop as you have seen earlier on in the book. This loop goes through the individual datapoints (<strong class="source-inline">X</strong> and <strong class="source-inline">y</strong>) and converts the <strong class="source-inline">X</strong> values into a dictionary, as required by River. The model is then <a id="_idIndexMarker387"/>updated datapoint by datapoint using the <strong class="source-inline">learn_one</strong> method:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-8</p>
			<p class="source-code"># fit the model</p>
			<p class="source-code">for x_i,y_i in zip(X_train,y_train):</p>
			<p class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">    model.learn_one(x_json,y_i)</p>
			<ol>
				<li value="7">Once the model has learned from the training data, it needs to be evaluated on the test set. This can be done<a id="_idIndexMarker388"/> by looping through the test data and making a prediction for the <strong class="source-inline">X</strong> values of each datapoint. The <strong class="source-inline">y</strong> values are stored in a list for evaluation against the actual <strong class="source-inline">y</strong> values of the test dataset:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-9</p>
			<p class="source-code"># predict on the test set</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">preds = []</p>
			<p class="source-code">for x_i in X_test:</p>
			<p class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">    preds.append(model.predict_one(x_json))</p>
			<ol>
				<li value="8">We can now compute the metric of our choice for this regression model, for example, the <strong class="source-inline">r2</strong> score. This can be done using the following code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-10</p>
			<p class="source-code"># compute accuracy</p>
			<p class="source-code">from sklearn.metrics import r2_score</p>
			<p class="source-code">r2_score(y_test, preds)</p>
			<p>The obtained result is <strong class="source-inline">0.478</strong>. </p>
			<p>Let's find out whether other <a id="_idIndexMarker389"/>models are more performant at this task<a id="_idIndexMarker390"/> in the next section.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor155"/>Regression algorithm 2 – HoeffdingAdaptiveTreeRegressor</h2>
			<p>The second online<a id="_idIndexMarker391"/> regression model that we'll cover is a much more specific model for online regression. Whereas the <strong class="source-inline">LinearRegression</strong> model, just like many other models, is an online adaptation of an essentially offline model, many other models are developed specifically for online models. <strong class="source-inline">HoeffdingAdaptiveTreeRegressor</strong> is one of those.</p>
			<p>The <strong class="bold">Hoeffding Adaptive Tree regressor</strong> (<strong class="bold">HATR</strong>) is a regression <a id="_idIndexMarker392"/>model that is based on the <strong class="bold">Hoeffding Adaptive Tree Classifier</strong> (<strong class="bold">HATC</strong>). HATC is a tree-based model<a id="_idIndexMarker393"/> that uses the <strong class="bold">adaptive windowing</strong> (<strong class="bold">ADWIN</strong>) methodology to <a id="_idIndexMarker394"/>monitor the performance of the different branches of a tree. The HATC methodology replaces the branches with new branches when their time is due. This is determined by observing the better performance of the new branches by the old branches. HATC is also available in River.</p>
			<p>The HATR regression version is based on the HATC approach and uses an ADWIN concept-drift detector at each decision node. This allows the method to detect possible changes in the underlying<a id="_idIndexMarker395"/> data, which is called <strong class="bold">drift</strong>. Drift detection will be covered in more detail in a further chapter.</p>
			<h3>HoeffdingAdaptiveTreeRegressor in River</h3>
			<p>We will check out an example as follows:</p>
			<ol>
				<li value="1">Let's get started with fitting the model on the same data as we used in the previous <a id="_idIndexMarker396"/>model:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-11</p>
			<p class="source-code">from river.tree import HoeffdingAdaptiveTreeRegressor</p>
			<p class="source-code">model = HoeffdingAdaptiveTreeRegressor(seed=42)</p>
			<p class="source-code"># fit the model</p>
			<p class="source-code">for x_i,y_i in zip(X_train,y_train):</p>
			<p class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">    model.learn_one(x_json,y_i)</p>
			<p class="source-code"># predict on the test set</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">preds = []</p>
			<p class="source-code">for x_i in X_test:</p>
			<p class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">    preds.append(model.predict_one(x_json))</p>
			<p class="source-code"># compute accuracy</p>
			<p class="source-code">from sklearn.metrics import r2_score</p>
			<p class="source-code">r2_score(y_test, preds)</p>
			<ol>
				<li value="2">This model obtains an <strong class="source-inline">r2</strong> score that is <a id="_idIndexMarker397"/>a little worse than the linear regression: <strong class="source-inline">0.437</strong>. Let's see if we can do something to make it work better. Let's write a grid search to see whether a number of hyperparameters can help to improve the model.</li>
			</ol>
			<p>For this, let's write<a id="_idIndexMarker398"/> the model as a function that takes values for the hyperparameters <a id="_idIndexMarker399"/>and that returns the <strong class="source-inline">r2</strong> score:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-12</p>
			<p class="source-code">def evaluate_HATR(grace_period, leaf_prediction, model_selector_decay):</p>
			<p class="source-code">    # model pipeline</p>
			<p class="source-code">    model = (</p>
			<p class="source-code">        HoeffdingAdaptiveTreeRegressor(</p>
			<p class="source-code">            grace_period=grace_period,</p>
			<p class="source-code">            leaf_prediction=leaf_prediction,</p>
			<p class="source-code">            model_selector_decay=model_selector_decay,</p>
			<p class="source-code">            seed=42)</p>
			<p class="source-code">    )</p>
			<p class="source-code">    # fit the model</p>
			<p class="source-code">    for x_i,y_i in zip(X_train,y_train):</p>
			<p class="source-code">        x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">        model.learn_one(x_json,y_i)</p>
			<p class="source-code">    # predict on the test set</p>
			<p class="source-code">    preds = []</p>
			<p class="source-code">    for x_i in X_test:</p>
			<p class="source-code">        x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">        preds.append(model.predict_one(x_json))</p>
			<p class="source-code">    # compute accuracy</p>
			<p class="source-code">    return r2_score(y_test, preds)</p>
			<ol>
				<li value="3">Let's specify the <a id="_idIndexMarker400"/>hyperparameters to tune as follows:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-13</p>
			<p class="source-code">grace_periods=[0,5,10,]</p>
			<p class="source-code">leaf_predictions=['mean','adaptive']</p>
			<p class="source-code">model_selector_decays=[ 0.3, 0.8,  0.95]</p>
			<ol>
				<li value="4">We then loop through<a id="_idIndexMarker401"/> the data as follows:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-14</p>
			<p class="source-code">results = []</p>
			<p class="source-code">i = 0</p>
			<p class="source-code">for grace_period in grace_periods:</p>
			<p class="source-code">    for leaf_prediction in leaf_predictions:</p>
			<p class="source-code">        for model_selector_decay in model_selector_decays:</p>
			<p class="source-code">            print(i)</p>
			<p class="source-code">            i = i+1</p>
			<p class="source-code">            results.append([grace_period, leaf_prediction, model_selector_decay,evaluate_HATR(grace_period, leaf_prediction, model_selector_decay)])</p>
			<ol>
				<li value="5">The results can then be obtained as follows:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-15</p>
			<p class="source-code">pd.DataFrame(results, columns=['grace_period', 'leaf_prediction', 'model_selector_decay', 'r2_score' ]).sort_values('r2_score', ascending=False)</p>
			<p>The obtained result is slightly disappointing, as none of the tested values were able to generate a better result. Unfortunately, this is part of data science, as not all models work<a id="_idIndexMarker402"/> well on each use <a id="_idIndexMarker403"/>case. </p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B18335_07_6.jpg" alt="Figure 7.6 – The resulting output of Code Block 7-15&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – The resulting output of Code Block 7-15</p>
			<p>Let's move on to the next model and see whether it fits better.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor156"/>Regression algorithm 3 – SGTRegressor</h2>
			<p><strong class="source-inline">SGTRegressor</strong> is a stochastic <a id="_idIndexMarker404"/>gradient tree for regression. It is another decision tree-based model that can learn with new data arriving. It is an incremental decision tree that minimizes the mean squared error by minimizing the<a id="_idIndexMarker405"/> loss function.</p>
			<h3>SGTRegressor in River</h3>
			<p>We'll check this out using the <a id="_idIndexMarker406"/>following example:</p>
			<ol>
				<li value="1">Let's test <a id="_idIndexMarker407"/>whether this model can improve the performance of this regression task:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-16</p>
			<p class="source-code">from river.tree import SGTRegressor</p>
			<p class="source-code"># model pipeline</p>
			<p class="source-code">model = SGTRegressor()</p>
			<p class="source-code"># fit the model</p>
			<p class="source-code">for x_i,y_i in zip(X_train,y_train):</p>
			<p class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">    model.learn_one(x_json,y_i)</p>
			<p class="source-code"># predict on the test set</p>
			<p class="source-code">preds = []</p>
			<p class="source-code">for x_i in X_test:</p>
			<p class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">    preds.append(model.predict_one(x_json))</p>
			<p class="source-code"># compute accuracy</p>
			<p class="source-code">r2_score(y_test, preds)</p>
			<ol>
				<li value="2">The result is worse than the previous models, as it is <strong class="source-inline">0.07</strong>. Let's again see whether it can be <a id="_idIndexMarker408"/>optimized using <a id="_idIndexMarker409"/>hyperparameter tuning:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-17</p>
			<p class="source-code">from river.tree import SGTRegressor</p>
			<p class="source-code">def evaluate_SGT(delta, lambda_value, grace_period):</p>
			<p class="source-code">    # model pipeline </p>
			<p class="source-code">    model = SGTRegressor(delta=delta,</p>
			<p class="source-code">                        lambda_value=lambda_value,</p>
			<p class="source-code">                        grace_period=grace_period,)</p>
			<p class="source-code">    # fit the model</p>
			<p class="source-code">    for x_i,y_i in zip(X_train,y_train):</p>
			<p class="source-code">        x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">        model.learn_one(x_json,y_i)</p>
			<p class="source-code">    # predict on the test set</p>
			<p class="source-code">    preds = []</p>
			<p class="source-code">    for x_i in X_test:</p>
			<p class="source-code">        x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">        preds.append(model.predict_one(x_json))</p>
			<p class="source-code">    # compute accuracy</p>
			<p class="source-code">    return r2_score(y_test, preds)</p>
			<ol>
				<li value="3">For <a id="_idIndexMarker410"/>this trial, we'll optimize<a id="_idIndexMarker411"/> the <strong class="source-inline">grace_period</strong>, <strong class="source-inline">lambda_value</strong>, and <strong class="source-inline">delta</strong> hyperparameters:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-18</p>
			<p class="source-code">grace_periods=[0,10,25]</p>
			<p class="source-code">lambda_values=[0.5, 0.8, 1.]</p>
			<p class="source-code">deltas=[0.0001, 0.001, 0.01, 0.1]</p>
			<ol>
				<li value="4">You can run the <a id="_idIndexMarker412"/>optimization loop using the following<a id="_idIndexMarker413"/> code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-19</p>
			<p class="source-code">results = []</p>
			<p class="source-code">i = 0</p>
			<p class="source-code">for grace_period in grace_periods:</p>
			<p class="source-code">    for lambda_value in lambda_values:</p>
			<p class="source-code">        for delta in deltas:</p>
			<p class="source-code">            print(i)</p>
			<p class="source-code">            i = i+1</p>
			<p class="source-code">            result = evaluate_SGT(delta, lambda_value, grace_period)</p>
			<p class="source-code">            print(result)</p>
			<p class="source-code">            results.append([delta, lambda_value, grace_period,result])</p>
			<ol>
				<li value="5">The best results can <a id="_idIndexMarker414"/>be shown using the<a id="_idIndexMarker415"/> following line of code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-20</p>
			<p class="source-code">pd.DataFrame(results, columns=['delta', 'lambda_value', 'grace_period', 'r2_score' ]).sort_values('r2_score', ascending=False)</p>
			<p>The result is shown in the following:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B18335_07_7.jpg" alt="Figure 7.7 – The resulting output of Code Block 7-20&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – The resulting output of Code Block 7-20</p>
			<p>The result is<a id="_idIndexMarker416"/> better than the non-tuned <strong class="source-inline">SGTRegressor</strong>, but much worse than the previous two models. The <a id="_idIndexMarker417"/>model could be optimized<a id="_idIndexMarker418"/> further, but it does not seem the best go-to for the <a id="_idIndexMarker419"/>current data.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor157"/>Regression algorithm 4 – SRPRegressor</h2>
			<p><strong class="source-inline">SRPRegressor</strong>, or <strong class="bold">Streaming Random Patches regressor</strong>, is an ensemble method that trains an ensemble of base<a id="_idIndexMarker420"/> learners on subsets <a id="_idIndexMarker421"/>of the input data. These subsets are called <strong class="bold">patches</strong> and are both subsets of features and subsets of<a id="_idIndexMarker422"/> observations. This is the same approach as<a id="_idIndexMarker423"/> the <strong class="bold">random forest</strong> that was seen in the previous chapter.</p>
			<h3>SRPRegressor in River</h3>
			<p>We will check this out using the following example:</p>
			<ol>
				<li value="1">In this example, let's use linear regression as a base learner, as this model has had the<a id="_idIndexMarker424"/> best performance <a id="_idIndexMarker425"/>compared to the other models tested<a id="_idIndexMarker426"/> in this chapter:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-21</p>
			<p class="source-code">from river.ensemble import SRPRegressor</p>
			<p class="source-code"># model pipeline </p>
			<p class="source-code">base_model = LinearRegression()</p>
			<p class="source-code">model = SRPRegressor(</p>
			<p class="source-code">    model=base_model,</p>
			<p class="source-code">    n_models=3,</p>
			<p class="source-code">    seed=42</p>
			<p class="source-code">)</p>
			<p class="source-code"># fit the model</p>
			<p class="source-code">for x_i,y_i in zip(X_train,y_train):</p>
			<p class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">    model.learn_one(x_json,y_i)</p>
			<p class="source-code"># predict on the test set</p>
			<p class="source-code">preds = []</p>
			<p class="source-code">for x_i in X_test:</p>
			<p class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">    preds.append(model.predict_one(x_json))</p>
			<p class="source-code"># compute accuracy</p>
			<p class="source-code">r2_score(y_test, preds)</p>
			<ol>
				<li value="2">The<a id="_idIndexMarker427"/> resulting score is <strong class="source-inline">0.34</strong>. Let's try and tune the number of models used to see whether this<a id="_idIndexMarker428"/> can improve <a id="_idIndexMarker429"/>performance:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-22</p>
			<p class="source-code">def evaluate_SRP(n_models):</p>
			<p class="source-code">    # model pipeline </p>
			<p class="source-code">    base_model = LinearRegression()</p>
			<p class="source-code">    model = SRPRegressor(</p>
			<p class="source-code">        model=base_model,</p>
			<p class="source-code">        n_models=n_models,</p>
			<p class="source-code">        seed=42</p>
			<p class="source-code">    )</p>
			<p class="source-code">    # fit the model</p>
			<p class="source-code">    for x_i,y_i in zip(X_train,y_train):</p>
			<p class="source-code">        x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">        model.learn_one(x_json,y_i)</p>
			<p class="source-code">    # predict on the test set</p>
			<p class="source-code">    preds = []</p>
			<p class="source-code">    for x_i in X_test:</p>
			<p class="source-code">        x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">        preds.append(model.predict_one(x_json))</p>
			<p class="source-code">    # compute accuracy</p>
			<p class="source-code">    return r2_score(y_test, preds)</p>
			<ol>
				<li value="3">You can execute the tuning loop with the following code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-23</p>
			<p class="source-code">results = []</p>
			<p class="source-code">for n_models in range(1, 50):</p>
			<p class="source-code">    results.append([n_models, evaluate_SRP(n_models)])</p>
			<ol>
				<li value="4">The following line <a id="_idIndexMarker430"/>shows the results for <a id="_idIndexMarker431"/>each value of <strong class="source-inline">n_models</strong>:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 7-24</p>
			<p class="source-code">pd.DataFrame(results,columns=['n_models', 'r2_score']).sort_values('r2_score', ascending=False)</p>
			<p>The result is <a id="_idIndexMarker432"/>shown in the following:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B18335_07_8.jpg" alt="Figure 7.8 – The resulting output of Code Block 7-24&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – The resulting output of Code Block 7-24</p>
			<p>Apparently, the<a id="_idIndexMarker433"/> result at 12 models has found a sweet spot at which the performance is <strong class="source-inline">0.457</strong>. Compared to the simple <strong class="source-inline">LinearRegression</strong> model with a score of <strong class="source-inline">0.478</strong>, this is a worse result. This indicates that the <strong class="source-inline">LinearRegression</strong> model has the best score of the four <a id="_idIndexMarker434"/>models tested in this dataset. </p>
			<p>Of course, this result is <a id="_idIndexMarker435"/>strongly related to the data-generating process that is behind the <strong class="source-inline">make_regression</strong> function. If the <strong class="source-inline">make_regression</strong> function were to add anything such as time trends, the adaptive models would probably have been more performant than the simple linear model.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor158"/>Summary</h1>
			<p>In this chapter, you have seen the basics of regression modeling. You have learned that there are some similarities between classification and anomaly detection models, but that there are also some fundamental differences.</p>
			<p>The main difference in regression is that the target variables are numeric, whereas they are categorical in classification. This introduces a difference in metrics, but also in the model definition and the way the models work deep down.</p>
			<p>You have seen several traditional, offline regression models and their adaptation to working in an online training manner. You have also seen some online regression models that are made specifically for online training and streaming.</p>
			<p>As in the previous chapters, you have seen how to implement a modeling benchmark using a train-test set. The field of ML does not stop evolving, and newer and better models are published regularly. This introduces the need for practitioners to be solid in their skills to evaluate models. </p>
			<p>Mastering model evaluation is often even more important than knowing the largest list of models. You need to know a large number of models to start modeling, but it is the evaluation that will allow you to avoid pushing erroneous or overfitted models into production.</p>
			<p>Although this is generally true for ML, the next chapter will introduce a category of models that has a fundamentally different take on this. Reinforcement learning is a category of online ML in which the focus is on model updating. Online models have the capacity to learn on each piece of data that gets into the system as well, but reinforcement learning is focused even more on having almost autonomous learning. This will be the scope of the next chapter.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor159"/>Further reading</h1>
			<ul>
				<li><em class="italic">LinearRegression</em>: <a href="https://riverml.xyz/latest/api/linear-model/LinearRegression/">https://riverml.xyz/latest/api/linear-model/LinearRegression/</a></li>
				<li><em class="italic">Make_regression</em>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html</a></li>
				<li><em class="italic">HoeffdingAdaptiveTreeRegressor</em>: <a href="https://riverml.xyz/latest/api/tree/HoeffdingAdaptiveTreeRegressor/">https://riverml.xyz/latest/api/tree/HoeffdingAdaptiveTreeRegressor/</a></li>
				<li><em class="italic">HoeffdingAdaptiveTreeClassifier</em>: <a href="https://riverml.xyz/latest/api/tree/HoeffdingAdaptiveTreeClassifier/">https://riverml.xyz/latest/api/tree/HoeffdingAdaptiveTreeClassifier/</a></li>
				<li><em class="italic">Adaptive learning and mining for data streams and frequent patterns</em>: <a href="https://dl.acm.org/doi/abs/10.1145/1656274.1656287">https://dl.acm.org/doi/abs/10.1145/1656274.1656287</a></li>
				<li><em class="italic">SGTRegressor</em>: <a href="https://riverml.xyz/latest/api/tree/SGTRegressor/">https://riverml.xyz/latest/api/tree/SGTRegressor/</a></li>
				<li><em class="italic">SRPRegressor</em>: <a href="https://riverml.xyz/latest/api/ensemble/SRPRegressor/">https://riverml.xyz/latest/api/ensemble/SRPRegressor/</a></li>
			</ul>
		</div>
	</body></html>