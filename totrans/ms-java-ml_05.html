<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Real-Time Stream Machine Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Real-Time Stream Machine Learning</h1></div></div></div><p>In <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>, <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>, and <a class="link" href="ch04.html" title="Chapter 4. Semi-Supervised and Active Learning">Chapter 4</a>, <span class="emphasis"><em>Semi-Supervised and Active Learning</em></span>, we discussed various techniques of classification, clustering, outlier detection, semi-supervised, and active learning. The form of learning done from existing or historic data is traditionally known as batch learning. </p><p>All of these algorithms or techniques assume three things, namely:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Finite training data is available to build different models.</li><li class="listitem" style="list-style-type: disc">The learned model will be static; that is, patterns won't change.</li><li class="listitem" style="list-style-type: disc">The data distribution also will remain the same.</li></ul></div><p>In many real-world data scenarios, there is either no training data available a priori or the data is dynamic in nature; that is, changes continuously with respect to time. Many real-world applications may also have data which has a transient nature to it and comes in high velocity or volume such as IoT sensor information, network monitoring, and Twitter feeds. The requirement here is to learn from the instance immediately and then update the learning. </p><p>The nature of dynamic data and potentially changing distribution renders existing batch-based algorithms and techniques generally unsuitable for such tasks. This gave rise to adaptable or updatable or incremental learning algorithms in machine learning. These techniques can be applied to continuously learn from the data streams. In many cases,  the disadvantage of learning from Big Data due to size and the need to fit the entire data into memory can also be overcome by converting the Big Data learning problem into an incremental learning problem and inspecting one example at a time. </p><p>In this chapter, we will discuss the assumptions and discuss different techniques in supervised and unsupervised learning that facilitate real-time or stream machine learning. We will use the open source library <span class="strong"><strong>Massive Online Analysis</strong></span> (<span class="strong"><strong>MOA</strong></span>) for performing a real-world <a id="id905" class="indexterm"/>case study. </p><p>The major sections of this chapter are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Assumptions and mathematical notation.</li><li class="listitem" style="list-style-type: disc">Basic stream processing and computational techniques. A discussion of stream computations, sliding windows including the ADWIN algorithm, and sampling.</li><li class="listitem" style="list-style-type: disc">Concept drift and drift detection: Introduces learning evolving systems and data management, detection methods, and implicit and explicit adaptation.</li><li class="listitem" style="list-style-type: disc">Incremental supervised learning: A discussion of learning from labeled stream data, modeling techniques including linear, non-linear, and ensemble algorithms. This is followed by validation, evaluation, and model comparison methods.</li><li class="listitem" style="list-style-type: disc">Incremental unsupervised learning: Clustering techniques similar to those discussed in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>, including validation and evaluation techniques.</li><li class="listitem" style="list-style-type: disc">Unsupervised learning using outlier detection: Partition-based and distance-based, and the validation and evaluation techniques used.</li><li class="listitem" style="list-style-type: disc">Case study for stream-based learning: Introduces the MOA framework, presents the business problem, feature analysis, mapping to machine learning blueprint; describes the experiments, and concludes with the presentation and analysis of the results.</li></ul></div><div class="section" title="Assumptions and mathematical notations"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec42"/>Assumptions and mathematical notations</h1></div></div></div><p>There <a id="id906" class="indexterm"/>are some key assumptions made by many stream <a id="id907" class="indexterm"/>machine learning techniques and we will <a id="id908" class="indexterm"/>state them explicitly here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <a id="id909" class="indexterm"/>number of features in the data is fixed. </li><li class="listitem" style="list-style-type: disc">Data has small to medium dimensions, or number of features, typically in the hundreds.</li><li class="listitem" style="list-style-type: disc">The number of examples or training data can be infinite or very large, typically in the millions or billions.</li><li class="listitem" style="list-style-type: disc">The number of class labels in supervised learning or clusters are small and finite, typically less than 10.</li><li class="listitem" style="list-style-type: disc">Normally, there is an upper bound on memory; that is, we cannot fit all the data in <a id="id910" class="indexterm"/>memory, so learning <a id="id911" class="indexterm"/>from data must take this into <a id="id912" class="indexterm"/>account, especially lazy learners such as K-Nearest-Neighbors.</li><li class="listitem" style="list-style-type: disc">Normally, <a id="id913" class="indexterm"/>there is an upper bound on the time taken to process the event or the data, typically a few milliseconds.</li><li class="listitem" style="list-style-type: disc">The patterns or the distributions in the data can be evolving over time. </li><li class="listitem" style="list-style-type: disc">Learning algorithms must converge to a solution in finite time. </li></ul></div><p>Let <span class="emphasis"><em>D</em></span><sub>t</sub> = {<span class="strong"><strong>x</strong></span><sub>i</sub>, y<sub>i</sub> : <span class="emphasis"><em>y = f(x)</em></span>} be the given data available at time <span class="emphasis"><em>t</em></span> ∈ {1, 2, … <span class="emphasis"><em>i</em></span>}.</p><p>An incremental learning algorithm produces sequences of models/hypotheses {.., <span class="emphasis"><em>G</em></span><sub>j-1</sub>, G<sub>j</sub>, <span class="emphasis"><em>G</em></span><sub>j+1</sub>..} for the sequence of data {.., <span class="emphasis"><em>D</em></span><sub>j-1</sub>, <span class="emphasis"><em>D</em></span><sub>j</sub>, <span class="emphasis"><em>D</em></span><sub>j+1</sub>..} and model/hypothesis <span class="emphasis"><em>G</em></span><sub>i</sub> depends only on the previous hypothesis <span class="emphasis"><em>G</em></span><sub>i-1</sub> and current data <span class="emphasis"><em>D</em></span><sub>i</sub>.</p></div></div>
<div class="section" title="Basic stream processing and computational techniques"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec43"/>Basic stream processing and computational techniques</h1></div></div></div><p>We <a id="id914" class="indexterm"/>will now describe some basic computations that can be <a id="id915" class="indexterm"/>performed on the stream of data. If we must run summary operations such as aggregations or histograms with limits on memory and speed, we can be sure that some kind of trade-off will be needed. Two well-known types of approximations in these situations are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>ϵ</em></span> Approximation: The computation is close to the exact value within the fraction <span class="emphasis"><em>ϵ</em></span> of error.</li><li class="listitem" style="list-style-type: disc">(<span class="emphasis"><em>ϵ</em></span><span class="emphasis"><em>, δ</em></span>) Approximation: The computation is close to the exact value within 1 ± <span class="emphasis"><em>ϵ</em></span> with probability within 1 – <span class="emphasis"><em>δ</em></span>. </li></ul></div><div class="section" title="Stream computations"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec68"/>Stream computations</h2></div></div></div><p>We will <a id="id916" class="indexterm"/>illustrate some basic computations and aggregations to highlight the difference between batch and stream-based calculations when we must compute basic operations with constraints on memory and yet consider the entire data:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Frequency count or point queries</strong></span>: The generic technique of Count-Min Sketch <a id="id917" class="indexterm"/>has been successfully <a id="id918" class="indexterm"/>applied to perform various summarizations on the data streams. The primary technique is creating a window of size <span class="emphasis"><em>w</em></span> x <span class="emphasis"><em>d</em></span>. Then, given a desired probability (δ) and admissible error (<span class="emphasis"><em>ϵ</em></span>), the size of data in memory can be created using <span class="emphasis"><em>w</em></span> = 2/ <span class="emphasis"><em>ϵ</em></span> and <span class="inlinemediaobject"><img src="graphics/B05137_05_019.jpg" alt="Stream computations"/></span>. Associated with each row is a hash <a id="id919" class="indexterm"/>function: <span class="emphasis"><em>h</em></span>(.). This <a id="id920" class="indexterm"/>uniformly transforms a value <span class="emphasis"><em>x</em></span> to a value in the interval [1, 2 … <span class="emphasis"><em>w</em></span>]. This method of lookup and updates can be used for performing point queries of values or dot products or frequency counts. 
 </li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Distinct count</strong></span>: The generic <a id="id921" class="indexterm"/>technique of Hash-Sketch can be used to perform "distinct values" queries or counts. Given the domain of incoming stream values x ∈ [0,1,2….N-1], the hash function <span class="emphasis"><em>h</em></span>(<span class="emphasis"><em>x</em></span>) maps the values uniformly across [0,1,….2L-1], where <span class="emphasis"><em>L=O(log N)</em></span>. </li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Mean</strong></span>: Computing the mean without the need for storing all the values is very useful <a id="id922" class="indexterm"/>and is normally employed using a recursive method where only the number of observations <span class="emphasis"><em>(n)</em></span> and sum of values seen so far (∑<span class="emphasis"><em>x</em></span><sub>n</sub>) is needed:<div class="mediaobject"><img src="graphics/B05137_05_029.jpg" alt="Stream computations"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Standard deviation</strong></span>: Like the mean, standard deviation can be computed using <a id="id923" class="indexterm"/>the memoryless option with only the number of observations (<span class="emphasis"><em>n</em></span>), sum of values seen so far (∑<span class="emphasis"><em>x</em></span><sub>n</sub>), and sum of squares of the values (∑<span class="emphasis"><em>x</em></span><sub>n</sub><sup>2</sup>):<div class="mediaobject"><img src="graphics/B05137_05_031.jpg" alt="Stream computations"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Correlation coefficient</strong></span>: Given a stream of two different values, many algorithms <a id="id924" class="indexterm"/>need to compute the correlation coefficient between the two which can be done by maintaining the running sum of each stream (∑<span class="emphasis"><em>x</em></span><sub>n</sub> and ∑<span class="emphasis"><em>y</em></span><sub>n</sub>), the sum of squared values (∑<span class="emphasis"><em>x</em></span><sub>n</sub><sup>2</sup> and ∑<span class="emphasis"><em>y</em></span><sub>n</sub><sup>2</sup>), and the cross-product (∑<span class="emphasis"><em>x</em></span><sub>n</sub>x <span class="emphasis"><em>y</em></span><sub>n</sub>). The correlation is given by:<div class="mediaobject"><img src="graphics/B05137_05_036.jpg" alt="Stream computations"/></div></li></ul></div></div><div class="section" title="Sliding windows"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec69"/>Sliding windows</h2></div></div></div><p>Often, you <a id="id925" class="indexterm"/>don't need the entire <a id="id926" class="indexterm"/>data for computing statistics or summarizations but only the "recent past". In such cases, sliding window techniques are used to calculate summary statistics by keeping the window size either fixed or adaptable and moving it over the recent past. </p><p>
<span class="strong"><strong>ADaptable sliding WINdow</strong></span> (<span class="strong"><strong>ADWIN</strong></span>) is one well-known technique used to detect change as <a id="id927" class="indexterm"/>well as estimating values needed in the computation. The idea behind ADWIN is to keep a variable-length window of last seen values with the characteristic that the window has a maximum length statistically consistent with the fact that there has been no change in the average value within the window. In other words, the older values are dropped if and only if a new incoming value would change the average. This has the two-fold advantage of recording change and maintaining the dynamic value, such as aggregate, over the recent streams. The determination of the subjective notion "large enough" for dropping items can be determined using the well-known Hoeffding bound as:</p><div class="mediaobject"><img src="graphics/B05137_05_037.jpg" alt="Sliding windows"/></div><p>Here <span class="inlinemediaobject"><img src="graphics/B05137_05_038.jpg" alt="Sliding windows"/></span> is the harmonic mean between two windows <span class="emphasis"><em>W</em></span><sub>0</sub> and <span class="emphasis"><em>W</em></span><sub>1</sub> of size |<span class="emphasis"><em>W</em></span><sub>0</sub>| and |<span class="emphasis"><em>W</em></span><sub>1</sub>| respectively, with <span class="emphasis"><em>W</em></span><sub>1</sub> containing the more recent elements. Further, let <span class="inlinemediaobject"><img src="graphics/B05137_05_043.jpg" alt="Sliding windows"/></span> and <span class="inlinemediaobject"><img src="graphics/B05137_05_044.jpg" alt="Sliding windows"/></span> be the respective calculated averages.</p><p>The algorithm can be generalized as: </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">ADWIN (<span class="emphasis"><em>x: inputstream, δ: confidence</em></span>)</li><li class="listitem">init (<span class="emphasis"><em>W</em></span>) //Initialize Window <span class="emphasis"><em>W</em></span></li><li class="listitem">while (<span class="emphasis"><em>x</em></span>){<p>W ← W ∪ {<span class="emphasis"><em>x</em></span><sub>t</sub>} //add new instance <span class="emphasis"><em>x</em></span><sub>t</sub> to the head of Window <span class="emphasis"><em>W</em></span>
</p></li><li class="listitem">repeat
W ← W – <span class="emphasis"><em>x</em></span>old //drop elements from tail of the window</li><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B05137_05_056.jpg" alt="Sliding windows"/></span>
 &lt; <span class="inlinemediaobject"><img src="graphics/B05137_05_057.jpg" alt="Sliding windows"/></span> holds for every split of <span class="emphasis"><em>W</em></span></li><li class="listitem">
output <span class="inlinemediaobject"><img src="graphics/B05137_05_058.jpg" alt="Sliding windows"/></span></li><li class="listitem">}</li></ol></div><p>ADWIN has <a id="id928" class="indexterm"/>also shown that it provides theoretical bounds <a id="id929" class="indexterm"/>on false positives and false negatives, which makes it a very promising technique to use. </p></div><div class="section" title="Sampling"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec70"/>Sampling</h2></div></div></div><p>In many <a id="id930" class="indexterm"/>stream-based algorithms, there is <a id="id931" class="indexterm"/>a need to reduce the data or select a subset of data for analysis. The normal methodology of sampling on the whole data must be augmented for stream-based data. </p><p>The key concerns in sampling that must be addressed are how unbiased the samples are and how representative they are of the population from which streams are being generated. In a non-streaming environment, this depends completely on the sample size and the sampling method. Uniform random sampling (<a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>) is one of the most well-known techniques employed to reduce the data in the batch data world. The reservoir sampling technique is considered to be a very effective way of reducing the data given the memory constraints. </p><p>The basic idea of reservoir sampling is to keep a reservoir or sample of fixed size, say <span class="emphasis"><em>k</em></span>, and every element that enters the stream has a probability <span class="emphasis"><em>k/n</em></span> of replacing an older element in the reservoir. The detailed algorithm is shown here:</p><div class="informalexample"><pre class="programlisting">ReservoirSampling(x:inputstream, k:sizeOfReservoir)
//add first k elements to reservoir
for(i = 0; i &lt; k; i++)
  addToReservoir(x)
  while (x){
    for(i = 0; i &lt; k; i++)
    //flip a coin to get random integer
    r = randomInteger[1..n]
    if(r ≤ k){
      //move it inside the reservoir
      addToReservoir(x)
      //delete an instance randomly from reservoir
      position = randomInteger[1..k]
      removeInstance(position)
    }
}</pre></div><p>There <a id="id932" class="indexterm"/>are extensions to these such as the Min-wise Sampling and Load Shedding that overcome <a id="id933" class="indexterm"/>some issues associated with the base method. </p></div></div>
<div class="section" title="Concept drift and drift detection"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec44"/>Concept drift and drift detection</h1></div></div></div><p>As <a id="id934" class="indexterm"/>discussed in the introduction of the chapter, the dynamic nature of <a id="id935" class="indexterm"/>infinite streams stands in direct opposition to the basic principles of stationary learning; that is, that the distribution of the data or patterns remain constant. Although there can be changes that are <span class="emphasis"><em>swift</em></span> or <span class="emphasis"><em>abrupt</em></span>, the discussion here is around slow, gradual changes. These slow, gradual changes are fairly hard to detect and separating the changes from the noise becomes tougher still: </p><div class="mediaobject"><img src="graphics/B05137_05_071.jpg" alt="Concept drift and drift detection"/><div class="caption"><p>Figure 1 Concept drift illustrated by the gradual change in color from yellow to blue in the bottom panel. Sampled data reflects underlying change in data distribution, which must be detected and a new model learned.</p></div></div><p>There have been several techniques described in various studies in the last two decades that can be categorized as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B05137_05_072.jpg" alt="Concept drift and drift detection"/><div class="caption"><p>Figure 2 Categories of drift detection techniques</p></div></div><div class="section" title="Data management"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec71"/>Data management</h2></div></div></div><p>The <a id="id936" class="indexterm"/>main idea is to manage a model in memory that is <a id="id937" class="indexterm"/>consistent with the dynamic nature of the data.</p></div><div class="section" title="Partial memory"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec72"/>Partial memory</h2></div></div></div><p>These <a id="id938" class="indexterm"/>techniques use the most recently used data in a <a id="id939" class="indexterm"/>memory buffer to learn or derive summary information. The key question as discussed previously is: what is the right window size to be effective in detecting the change and learning effectively? In Fixed Window Size based techniques, we use the idea of a queue where a new instance with a recent timestamp comes in and the one with the oldest is evicted. The window thus contains all the recent enough examples and the size is generally chosen based on physical availability of memory and size of data elements in the queue. In Adaptive Window Size, the queue is used in conjunction with a detection algorithm. When the detection algorithm indicates signs of drifts based on performance evaluation, the window size can be reduced to effectively remove old examples which no longer help the model.</p><div class="section" title="Full memory"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec74"/>Full memory</h3></div></div></div><p>The <a id="id940" class="indexterm"/>idea is to store sufficient statistics over all the examples or data seen. One way to do this is to put weights on the data and weights decay over time. Exponential weighting using the rate factor given by λ can be very effective: </p><p>w<sub>λ</sub> (<span class="emphasis"><em>x</em></span>) = <span class="emphasis"><em>exp</em></span>(– λ * i)</p></div><div class="section" title="Detection methods"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec75"/>Detection methods</h3></div></div></div><p>Given <a id="id941" class="indexterm"/>the probability <span class="emphasis"><em>P(X) </em></span>that the given data is observed, the probability of patterns/class <span class="emphasis"><em>P(C)</em></span>, and the probability of data given class <span class="emphasis"><em>P(X|C)</em></span>—which is the model—the detection method can be divided into two categories, at a high level:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Monitoring evolution or performance of the model, classifier, or <span class="emphasis"><em>P(C|X)</em></span></li><li class="listitem" style="list-style-type: disc">Monitoring distributions from the environment or observing <span class="emphasis"><em>P(X)</em></span>, <span class="emphasis"><em>P(C)</em></span>, and <span class="emphasis"><em>P(X|C)</em></span></li></ul></div><div class="section" title="Monitoring model evolution"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec132"/>Monitoring model evolution</h4></div></div></div><p>Although <a id="id942" class="indexterm"/>this method is based on the <a id="id943" class="indexterm"/>assumption that all the learning of models <a id="id944" class="indexterm"/>is stationary and data is coming from <span class="strong"><strong>independent, identical distributions</strong></span> (<span class="strong"><strong>i.i.d.</strong></span>), which doesn't hold true in many applications, it has nevertheless been shown to be effective. Some of the well-known techniques are described next.</p><div class="section" title="Widmer and Kubat"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec16"/>Widmer and Kubat</h5></div></div></div><p>This <a id="id945" class="indexterm"/>is one of the earliest methods which observed the <a id="id946" class="indexterm"/>error rates or misclassification rates and the changes <a id="id947" class="indexterm"/>to the model such as tree structures due to new branches, for instance. Using <a id="id948" class="indexterm"/>these and known thresholds, the learning window size is increased or decreased.</p></div><div class="section" title="Drift Detection Method or DDM"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec17"/>Drift Detection Method or DDM</h5></div></div></div><p>This <a id="id949" class="indexterm"/>method assumes that the parameter <a id="id950" class="indexterm"/>being observed, such as the classifier labeling things correctly or incorrectly, is a binary random variable which follows a binomial distribution. It assumes probability of misclassification at probability pi with standard deviation of <span class="inlinemediaobject"><img src="graphics/B05137_05_083.jpg" alt="Drift Detection Method or DDM"/></span> where the values are computed at the <span class="emphasis"><em>i</em></span><sup>th</sup> point in the sequence. The method then uses two levels:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Warning level: When <span class="emphasis"><em>p</em></span><sub>i</sub> + <span class="emphasis"><em>s</em></span><sub>i</sub> ≥ <span class="emphasis"><em>p</em></span><sub>min</sub><span class="emphasis"><em> </em></span>+ 2 * <span class="emphasis"><em>s</em></span><sub>min</sub></li><li class="listitem" style="list-style-type: disc">Detection level: When <span class="emphasis"><em>p</em></span><sub>i</sub> + <span class="emphasis"><em>s</em></span><sub>i</sub> ≥ <span class="emphasis"><em>p</em></span><sub>min</sub><span class="emphasis"><em> </em></span>+ 3 * <span class="emphasis"><em>s</em></span><sub>min</sub></li></ul></div><p>All the examples between the "warning" and "detection" levels are used to train a new classifier that will replace the "non-performing" classifier when the "detection" level is reached.</p></div><div class="section" title="Early Drift Detection Method or EDDM"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec18"/>Early Drift Detection Method or EDDM</h5></div></div></div><p>EDDM uses <a id="id951" class="indexterm"/>the same technique <a id="id952" class="indexterm"/>as DDM but with a slight modification. It uses classification rate (that is, recall) rather than error rate (1 – accuracy) and uses the distance between the number of right predictions and two wrong predictions to change the levels. </p><p>EDDM <a id="id953" class="indexterm"/>computes the mean <a id="id954" class="indexterm"/>distance between the two errors <span class="emphasis"><em>p</em></span><sub>i</sub><span class="emphasis"><em><sup>'</sup></em></span> and standard deviation between the two <span class="emphasis"><em>s</em></span><sub>i</sub><span class="emphasis"><em>'</em></span>. The levels are then:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Warning level: (<span class="emphasis"><em>p</em></span><sub>i</sub><span class="emphasis"><em><sup>'</sup></em></span> + 2 * <span class="emphasis"><em>s</em></span><sub>i</sub><span class="emphasis"><em><sup>'</sup></em></span>) ⁄ (<span class="emphasis"><em>p<sup>'</sup></em></span><sub>max</sub> + 2 * <span class="emphasis"><em>s<sup>'</sup></em></span><sub>max</sub>) &lt; <span class="emphasis"><em>α</em></span></li><li class="listitem" style="list-style-type: disc">Detection level: (<span class="emphasis"><em>p</em></span><sub>i</sub><span class="emphasis"><em><sup>'</sup></em></span> + 2 * <span class="emphasis"><em>s</em></span><sub>i</sub><span class="emphasis"><em>'</em></span>) ⁄ (<span class="emphasis"><em>p<sup>'</sup></em></span><sub>max</sub> + 2 * <span class="emphasis"><em>s<sup>'</sup></em></span><sub>max</sub>) &lt; <span class="emphasis"><em>β</em></span></li></ul></div><p>The parameters <span class="emphasis"><em>α</em></span> and <span class="emphasis"><em>β</em></span> are normally tuned by the user to something around 90% and 95% respectively. </p></div></div><div class="section" title="Monitoring distribution changes"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec133"/>Monitoring distribution changes</h4></div></div></div><p>When <a id="id955" class="indexterm"/>there are no models or classifiers <a id="id956" class="indexterm"/>to detect changes, we apply techniques that use some form of statistical tests for monitoring distribution changes. These tests are used to identify the distribution changes. Owing to assumptions, whether parametric or non-parametric, and different biases, it is difficult to say concretely which works best. Here we provide some of the well-known statistical tests.</p><div class="section" title="Welch's t test"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec19"/>Welch's t test</h5></div></div></div><p>This <a id="id957" class="indexterm"/>is an adaptation of the Student <span class="emphasis"><em>t</em></span> test with <a id="id958" class="indexterm"/>two samples. The test is adapted to take two windows of size <span class="emphasis"><em>N</em></span><sub>1</sub> and <span class="emphasis"><em>N</em></span><sub>2</sub> with means <span class="inlinemediaobject"><img src="graphics/B05137_05_096.jpg" alt="Welch's t test"/></span>and <span class="inlinemediaobject"><img src="graphics/B05137_05_097.jpg" alt="Welch's t test"/></span> and variances <span class="inlinemediaobject"><img src="graphics/B05137_05_348.jpg" alt="Welch's t test"/></span> and <span class="inlinemediaobject"><img src="graphics/B05137_05_349.jpg" alt="Welch's t test"/></span> to compute the <span class="emphasis"><em>p</em></span> value and use that to reject or accept the null hypothesis:</p><div class="mediaobject"><img src="graphics/B05137_05_100.jpg" alt="Welch's t test"/></div><div class="section" title="Kolmogorov-Smirnov's test"><div class="titlepage"><div><div><h6 class="title"><a id="ch05lvl6sec01"/>Kolmogorov-Smirnov's test</h6></div></div></div><p>This <a id="id959" class="indexterm"/>statistical test is normally <a id="id960" class="indexterm"/>used to compare distances between two distributions and validate if they are below certain thresholds or not. This can be adapted for change detection by using windows of two different sample sizes, <span class="emphasis"><em>N</em></span><sub>1</sub> and <span class="emphasis"><em>N</em></span><sub>2</sub>, with different cumulative distribution functions, <span class="emphasis"><em>F</em></span><sub>1</sub>(<span class="emphasis"><em>x</em></span>) and F<sub>2</sub>(<span class="emphasis"><em>x</em></span>), <span class="emphasis"><em>KS</em></span> distance:</p><div class="mediaobject"><img src="graphics/B05137_05_104.jpg" alt="Kolmogorov-Smirnov's test"/></div><p>The null hypothesis, which assumes the two distributions are similar, is rejected with confidence of <span class="emphasis"><em>α</em></span> if and only if <span class="inlinemediaobject"><img src="graphics/B05137_05_105.jpg" alt="Kolmogorov-Smirnov's test"/></span>, which is obtained by a lookup in the Kolmogorov-Smirnov's table.</p></div><div class="section" title="CUSUM and Page-Hinckley test"><div class="titlepage"><div><div><h6 class="title"><a id="ch05lvl6sec02"/>CUSUM and Page-Hinckley test</h6></div></div></div><p>The <span class="strong"><strong>cumulative sum</strong></span> (<span class="strong"><strong>CUSUM</strong></span>) is designed <a id="id961" class="indexterm"/>to indicate when the mean of the input is significantly <a id="id962" class="indexterm"/>different from zero: </p><p>
<span class="emphasis"><em>g</em></span><sub>0</sub> = 0 , <span class="emphasis"><em>g</em></span><sub>t</sub> = <span class="emphasis"><em>max</em></span>(0, <span class="emphasis"><em>g</em></span><sub>t–1</sub>) + <span class="emphasis"><em>ϵ</em></span><sub>t</sub> – <span class="emphasis"><em>v</em></span>)</p><p>We <a id="id963" class="indexterm"/>raise change detection when <span class="emphasis"><em>g</em></span><sub>t</sub> &gt; <span class="emphasis"><em>h</em></span>, where (<span class="emphasis"><em>h, v</em></span>) are user-defined <a id="id964" class="indexterm"/>parameters. Note that the CUSUM test is memoryless and is one-sided or asymmetric, detecting only the increase.</p><p>The Page Hinckley test is similar to CUSUM with a small variation as shown here:</p><p>
<span class="emphasis"><em>g</em></span>0 = 0 , <span class="emphasis"><em>g</em></span><sub>t</sub> = <span class="emphasis"><em>g</em></span><sub>t–1</sub> + <span class="emphasis"><em>ϵ</em></span><sub>t</sub> – <span class="emphasis"><em>v</em></span>)</p><p>For increasing and decreasing the values, we use <span class="emphasis"><em>G</em></span><sub>t</sub><span class="emphasis"><em> = min(g</em></span><sub>t</sub>, <span class="emphasis"><em>G</em></span><sub>t–1</sub><span class="emphasis"><em>) or G</em></span><sub>t</sub><span class="emphasis"><em> = max(g</em></span><sub>t</sub>, <span class="emphasis"><em>G</em></span><sub>t–1</sub><span class="emphasis"><em>)</em></span>, and <span class="emphasis"><em>Gt – gt &gt; h</em></span> for change detection.</p></div></div></div></div><div class="section" title="Adaptation methods"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec76"/>Adaptation methods</h3></div></div></div><p>Explicit <a id="id965" class="indexterm"/>and implicit adaptation are the two well-known <a id="id966" class="indexterm"/>techniques for adapting to environment changes when a change is detected.</p><div class="section" title="Explicit adaptation"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec134"/>Explicit adaptation</h4></div></div></div><p>In <a id="id967" class="indexterm"/>explicit adaptation, an additional technique from among the following is used:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Retrain the model from scratch with new data so the previous model or data does not impact the new model</li><li class="listitem" style="list-style-type: disc">Update the model with the changes or new data such that the transition is smooth—assumes changes are gradual and not drastic</li><li class="listitem" style="list-style-type: disc">Create a sequence or ensemble of models that are learned over time—when a collaborative approach is better than any single model</li></ul></div></div><div class="section" title="Implicit adaptation"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec135"/>Implicit adaptation</h4></div></div></div><p>In <a id="id968" class="indexterm"/>implicit adaptation, we generally use ensemble algorithms/models to adapt to the concept change. This can mean using different combinations ranging from a single classifier, to predicting in the ensemble, to using ADWIN for adaptive window-based with the classifier—all fall within the choices for implicit <a id="id969" class="indexterm"/>adaptation.</p></div></div></div></div>
<div class="section" title="Incremental supervised learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec45"/>Incremental supervised learning</h1></div></div></div><p>This <a id="id970" class="indexterm"/>section introduces several techniques used to learn from stream data when the true label for each instance is available. In particular, we present linear, non-linear, and ensemble-based algorithms adapted to incremental learning, as well as methods required in the evaluation and validation of these models, keeping in mind that learning is constrained by limits on memory and CPU time.</p><div class="section" title="Modeling techniques"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec73"/>Modeling techniques</h2></div></div></div><p>The <a id="id971" class="indexterm"/>modeling techniques are divided into linear algorithms, non-linear algorithms, and ensemble methods.</p><div class="section" title="Linear algorithms"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec77"/>Linear algorithms</h3></div></div></div><p>The <a id="id972" class="indexterm"/>linear methods described here require little to no adaptation to handle stream data.</p><div class="section" title="Online linear models with loss functions"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec136"/>Online linear models with loss functions</h4></div></div></div><p>Different <a id="id973" class="indexterm"/>loss functions such as hinge, logistic, and squared error can be used in this algorithm.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec20"/>Inputs and outputs</h5></div></div></div><p>Only numeric <a id="id974" class="indexterm"/>features are used in these <a id="id975" class="indexterm"/>methods. The choice of loss function <span class="emphasis"><em>l</em></span> and learning rate λ at which to apply the weight updates are taken as input parameters. The output is typically updatable models that give predictions accompanied by confidence values.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec21"/>How does it work?</h5></div></div></div><p>The <a id="id976" class="indexterm"/>basic algorithm assumes linear weight combinations similar to linear/logistic regression explained in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>. The stream or online learning algorithm can be summed up as: </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">for(t=1,2,…T) do<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>x</strong></span><sub>t</sub> = <span class="emphasis"><em>receive()</em></span>; // receive the data</li><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B05137_05_116.jpg" alt="How does it work?"/></span>
; //predict the label
</li><li class="listitem"><span class="emphasis"><em>y</em></span><sub>t</sub> = <span class="emphasis"><em>obtainTrueLabel()</em></span>; // get the true label </li><li class="listitem"><span class="emphasis"><em>loss</em></span> = <span class="emphasis"><em>l</em></span>(<span class="strong"><strong>w</strong></span><sub>t</sub>, (<span class="strong"><strong>x</strong></span><sub>t</sub>, <span class="strong"><strong>w</strong></span><sub>t</sub>)); // calculate the loss</li><li class="listitem">if(<span class="emphasis"><em>l</em></span>(<span class="strong"><strong>w</strong></span>t,(<span class="strong"><strong>x</strong></span>t, <span class="strong"><strong>w</strong></span>t )) &gt;  0 then</li><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B05137_05_120.jpg" alt="How does it work?"/></span>
; //update the weights
</li><li class="listitem">end</li></ol></div></li><li class="listitem">end</li></ol></div><p>Different <a id="id977" class="indexterm"/>loss functions can be plugged in based on types of problems; some of the well-known types are shown here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Classification:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Hinge loss: <span class="emphasis"><em>l</em></span>(<span class="strong"><strong>w</strong></span><sub>t</sub>, (<span class="strong"><strong>x</strong></span><sub>t</sub>, <span class="strong"><strong>w</strong></span><sub>t</sub>)) = max(0, 1 – <span class="emphasis"><em>yf</em></span>(<span class="strong"><strong>x</strong></span><sub>t</sub>, <span class="strong"><strong>w</strong></span><sub>t</sub>))</li><li class="listitem" style="list-style-type: disc">
Logistic loss: <span class="inlinemediaobject"><img src="graphics/B05137_05_123.jpg" alt="How does it work?"/></span></li></ul></div></li><li class="listitem" style="list-style-type: disc">Regression:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">
Squared loss: <span class="inlinemediaobject"><img src="graphics/B05137_05_124.jpg" alt="How does it work?"/></span></li></ul></div></li></ul></div><p>
<span class="strong"><strong>Stochastic Gradient Descent</strong></span> (<span class="strong"><strong>SGD</strong></span>) can be thought of as changing the weights to minimize <a id="id978" class="indexterm"/>the squared loss as in the preceding loss functions but going in the direction of the gradient with each example. The update of weights can be described as:</p><div class="mediaobject"><img src="graphics/B05137_05_125.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_05_126.jpg" alt="How does it work?"/></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec22"/>Advantages and limitations</h5></div></div></div><p>Online <a id="id979" class="indexterm"/>linear models have similar <a id="id980" class="indexterm"/>advantages and disadvantages as the linear models described in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Interpretable to some level as the weights of each features give insights on the impact of each feature</li><li class="listitem" style="list-style-type: disc">Assumes linear relationship, additive and uncorrelated features, and hence doesn't model complex non-linear real-world data </li><li class="listitem" style="list-style-type: disc">Very susceptible to outliers in the data</li><li class="listitem" style="list-style-type: disc">Very fast and normally one of the first algorithms to try or baseline</li></ul></div></div></div><div class="section" title="Online Naïve Bayes"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec137"/>Online Naïve Bayes</h4></div></div></div><p>Bayes theorem <a id="id981" class="indexterm"/>is applied to get predictions <a id="id982" class="indexterm"/>as the posterior probability, given for an <span class="emphasis"><em>m</em></span> dimensional input:</p><div class="mediaobject"><img src="graphics/B05137_05_128.jpg" alt="Online Naïve Bayes"/></div><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec23"/>Inputs and outputs</h5></div></div></div><p>Online <a id="id983" class="indexterm"/>Naïve Bayes can accept both categorical and continuous <a id="id984" class="indexterm"/>inputs. The categorical features are easier, as the algorithm must maintain counts for each class while computing the <span class="emphasis"><em>P</em></span>(<span class="strong"><strong>X</strong></span><sub>j</sub>|<span class="emphasis"><em>Y</em></span>) probability for each feature given the class. For continuous features, we must either assume a distribution, such as Gaussian, or to compute online Kernel Density estimates in an incremental way or discretize the numeric features incrementally. The outputs are updatable models and can predict the class accompanied by confidence value. Being probabilistic models, they have better confidence scores distributed between 0 and 1.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec24"/>How does it work?</h5></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">for(t = 1,2,…T) do<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>x</strong></span><sub>t</sub> = <span class="emphasis"><em>receive()</em></span>; // receive <a id="id985" class="indexterm"/>the data</li><li class="listitem">incrementCounters(<span class="strong"><strong>x</strong></span><sub>t</sub>); //update the <span class="emphasis"><em>P(</em></span><span class="strong"><strong>X</strong></span>j<span class="emphasis"><em>|Y)</em></span></li><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B05137_05_132.jpg" alt="How does it work?"/></span>
 //posterior probability
</li></ol></div></li><li class="listitem">end</li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec25"/>Advantages and limitations</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">This <a id="id986" class="indexterm"/>is the fastest algorithm and has a low memory <a id="id987" class="indexterm"/>footprint as well as computation cost. It is very popular among online or fast learners.</li><li class="listitem" style="list-style-type: disc">Assumes distribution or some biases on the numeric features that can impact the predictive quality.</li></ul></div></div></div></div><div class="section" title="Non-linear algorithms"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec78"/>Non-linear algorithms</h3></div></div></div><p>One <a id="id988" class="indexterm"/>of the most popular non-linear stream learning <a id="id989" class="indexterm"/>classifiers in use is the Hoeffding Tree. In the following subsections, the notion of the Hoeffding bound is introduced, followed by the algorithm itself.</p><div class="section" title="Hoeffding trees or very fast decision trees (VFDT)"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec138"/>Hoeffding trees or very fast decision trees (VFDT)</h4></div></div></div><p>The <a id="id990" class="indexterm"/>key idea behind <span class="strong"><strong>Hoeffding Trees</strong></span> (<span class="strong"><strong>HT</strong></span>) is the concept of the Hoeffding bound. Given a real-valued random variable <span class="strong"><strong>x</strong></span> whose range <a id="id991" class="indexterm"/>of values has size <span class="strong"><strong>R</strong></span>, suppose <a id="id992" class="indexterm"/>we have <span class="strong"><strong>n</strong></span> independent observations of <a id="id993" class="indexterm"/>
<span class="strong"><strong>x</strong></span> and compute the mean as <span class="inlinemediaobject"><img src="graphics/B05137_05_136.jpg" alt="Hoeffding trees or very fast decision trees (VFDT)"/></span>. </p><p>The Hoeffding bound states that, with a probability of 1 – δ, the actual mean of the variable <span class="strong"><strong>x</strong></span> is at least <span class="inlinemediaobject"><img src="graphics/B05137_05_138.jpg" alt="Hoeffding trees or very fast decision trees (VFDT)"/></span> where <span class="inlinemediaobject"><img src="graphics/B05137_05_139.jpg" alt="Hoeffding trees or very fast decision trees (VFDT)"/></span>
</p><p>The Hoeffding bound is independent of the probability distribution generating the samples and gives a good approximation with just <span class="strong"><strong>n</strong></span> examples. </p><p>The idea of the Hoeffding bound is used in the leaf expansion. If <span class="emphasis"><em>x</em></span><sub>1</sub> is the most informative feature and <span class="emphasis"><em>x</em></span><sub>2</sub> ranks second, then split using a user-defined split function <span class="emphasis"><em>G</em></span>(.) in a way such that:</p><div class="mediaobject"><img src="graphics/B05137_05_144.jpg" alt="Hoeffding trees or very fast decision trees (VFDT)"/></div><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec26"/>Inputs and outputs</h5></div></div></div><p>Both <a id="id994" class="indexterm"/>categorical and continuous data can be part of the <a id="id995" class="indexterm"/>data input. Continuous features are discretized in many <a id="id996" class="indexterm"/>implementations. The desired probability parameter 1 – δ and the split function common to Decision Trees <span class="emphasis"><em>G</em></span>(.) becomes part of the input. The output is the interpretable Decision Tree model and can predict/learn with class and confidence values.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec27"/>How does it work?</h5></div></div></div><p>HoeffdingTree(x:inputstream,G(.):splitFunction,δ:probabilityBound) </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let HT be a tree with single leaf(root)</li><li class="listitem">InitCounts(<span class="emphasis"><em>n</em></span><sub>ijk</sub>, <span class="emphasis"><em>root</em></span>)</li><li class="listitem">for(t=1,2,…T) do //all data from stream<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>x</strong></span><span class="emphasis"><em><sub>t</sub> = receive();</em></span> //receive the data</li><li class="listitem"><span class="emphasis"><em>y<sub>t</sub> = obtainTrueLabel();</em></span> //get the true label </li><li class="listitem">HTGrow((<span class="strong"><strong>x</strong></span><sub>t</sub>, <span class="emphasis"><em>y</em></span><sub>t</sub>), <span class="strong"><strong>HT</strong></span>, δ)</li><li class="listitem">end</li></ol></div></li></ol></div><p>
<span class="strong"><strong>HTGrow</strong></span>((<span class="strong"><strong>x</strong></span><sub>t</sub>, <span class="emphasis"><em>y</em></span><sub>t</sub>), <span class="strong"><strong>HT</strong></span>, <span class="emphasis"><em>G</em></span>(.), δ)</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="emphasis"><em>l = sort((</em></span><span class="strong"><strong>x</strong></span><sub>t</sub>, <span class="emphasis"><em>y</em></span><sub>t</sub>), <span class="strong"><strong>HT</strong></span>); //sort the data to leaf l using HT</li><li class="listitem"><span class="emphasis"><em>updateCounts(n</em></span><sub>ijk</sub>,<span class="emphasis"><em> l);</em></span> // update the counts at leaf l</li><li class="listitem"><span class="emphasis"><em>if(examplesSoFarNotOfSameClass();</em></span>// check if there are multiple classes</li><li class="listitem"><span class="emphasis"><em>computeForEachFeature(,G(.))</em></span><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B05137_ch5.jpg" alt="How does it work?"/></span></li></ol></div></li></ol></div><p>Hoeffding Trees have interesting properties, such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">They are a robust low variance model</li><li class="listitem" style="list-style-type: disc">They exhibit lower overfitting</li><li class="listitem" style="list-style-type: disc">Theoretical guarantees with high probability on the error rate exist due to Hoeffding bounds</li></ul></div><p>There are variations to Hoeffding Trees that can adapt concept drift, known as Concept-Adapting VFDT. They use the sliding window concept on the stream. Each node in the decision tree keeps sufficient statistics; based on the Hoeffding test, an alternate subtree is grown and swapped in when the accuracy is better.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec28"/>Advantages and limitations</h5></div></div></div><p>The advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The basic <a id="id997" class="indexterm"/>HT has issues with attributes <a id="id998" class="indexterm"/>being close to the chosen <span class="emphasis"><em>ϵ</em></span> and <a id="id999" class="indexterm"/>breaks the ties. Deciding the number of attributes <a id="id1000" class="indexterm"/>at any node is again an issue. Some of it is resolved in VFDT.</li><li class="listitem" style="list-style-type: disc">Memory constraints on expansion of the trees as well as time spent on an instance becomes an issue as the tree changes.</li><li class="listitem" style="list-style-type: disc">VFDT has issues with changes in patterns and CVFDT tries to overcome these as discussed previously. It is one of the most elegant, fast, interpretable algorithms for real-time and big data.</li></ul></div></div></div></div><div class="section" title="Ensemble algorithms"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec79"/>Ensemble algorithms</h3></div></div></div><p>The <a id="id1001" class="indexterm"/>idea behind ensemble learning is similar <a id="id1002" class="indexterm"/>to batch supervised learning where multiple algorithms are trained and combined in some form to predict unseen data. The same benefits accrue even in the online setting from different approaches to ensembles; for example, using multiple algorithms of different types, using models of similar type but with different parameters or sampled data, all so that different search spaces or patterns are found and the total error is reduced.</p><div class="section" title="Weighted majority algorithm"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec139"/>Weighted majority algorithm</h4></div></div></div><p>The <span class="strong"><strong>weighted majority algorithm</strong></span> (<span class="strong"><strong>WMA</strong></span>) trains <a id="id1003" class="indexterm"/>a set of base classifiers and combines their votes, weighted in some way, and <a id="id1004" class="indexterm"/>makes predictions based on the majority. </p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec29"/>Inputs and outputs</h5></div></div></div><p>The <a id="id1005" class="indexterm"/>constraint on types of inputs (categorical only, continuous only, or mixed) depends on the chosen base classifiers. The interpretability <a id="id1006" class="indexterm"/>of the model depends on the base model(s) selected but it is difficult to interpret the outputs of a combination of models. The weights for each model get updated by a factor (<span class="emphasis"><em>β</em></span>) per example/instance when the prediction is incorrect. The combination of weights and models can give some idea of interpretability. </p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec30"/>How does it work?</h5></div></div></div><p>
<span class="emphasis"><em>WeightedMajorityAlgorithm(x: inputstream, hm: m learner models)</em></span>
</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="emphasis"><em>initializeWeights(w</em></span><sub>i</sub><span class="emphasis"><em>)</em></span></li><li class="listitem">for(t=1, 2,…T)  do<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="emphasis"><em>x</em></span><sub>t</sub><span class="emphasis"><em> = receive();</em></span></li><li class="listitem"><span class="emphasis"><em>foreach model hk </em></span><span class="emphasis"><em>∈</em></span><span class="emphasis"><em> h</em></span></li><li class="listitem"><span class="emphasis"><em>y</em></span><sub>i</sub> ← <span class="emphasis"><em>h</em></span><sub>k</sub>(<span class="strong"><strong>x</strong></span><sub>t</sub>);</li></ol></div></li><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B05137_05_172.jpg" alt="How does it work?"/></span><span class="inlinemediaobject"><img src="graphics/B05137_05_173.jpg" alt="How does it work?"/></span></li><li class="listitem">
else
<span class="inlinemediaobject"><img src="graphics/B05137_05_173a.jpg" alt="How does it work?"/></span></li><li class="listitem">if <span class="emphasis"><em>y</em></span> is known then<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">for <span class="emphasis"><em>i</em></span> = 1 to m do</li><li class="listitem">if <span class="emphasis"><em>y</em></span>i ≠ <span class="emphasis"><em>y</em></span> then</li><li class="listitem"><span class="emphasis"><em>wi</em></span> ← <span class="emphasis"><em>w</em></span>i<span class="emphasis"><em> * β</em></span><p>end if</p><p>end for</p></li></ol></div></li><li class="listitem">end</li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec31"/>Advantages and limitations</h5></div></div></div><p>The advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">WMA has <a id="id1007" class="indexterm"/>simple implementation and <a id="id1008" class="indexterm"/>theoretic bounds on ensemble <a id="id1009" class="indexterm"/>errors</li><li class="listitem" style="list-style-type: disc">The difficulty is to choose the right base algorithm as the model and the number of models in the pool</li></ul></div></div></div><div class="section" title="Online Bagging algorithm"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec140"/>Online Bagging algorithm</h4></div></div></div><p>As <a id="id1010" class="indexterm"/>we saw in the chapter on supervised <a id="id1011" class="indexterm"/>learning, the bagging algorithm, which creates different samples from the training sets and uses multiple algorithms to learn and predict, reduces the variance and is very effective in learning.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec32"/>Inputs and outputs</h5></div></div></div><p>The <a id="id1012" class="indexterm"/>constraint on the types of inputs (categorical only, continuous only, or mixed) depends on the chosen base classifiers. The base <a id="id1013" class="indexterm"/>classifier algorithm with parameter choices corresponding to the algorithm are also the inputs. The output is the learned model that can predict the class/confidence based on the classifier chosen.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec33"/>How does it work?</h5></div></div></div><p>The <a id="id1014" class="indexterm"/>basic batch bagging algorithm requires the entire data to be available to create different samples and provide these samples to different classifiers. Oza's Online Bagging algorithm changes this constraint and makes it possible to learn from unbounded data streams. Based on sampling, each training instance in the original algorithm gets replicated many times and each base model is trained with <span class="emphasis"><em>k</em></span> copies of the original instances where: </p><p>
<span class="emphasis"><em>P(k) = exp(–1)/k!</em></span>
</p><p>This is equivalent to taking one training example and choosing for each classifier <span class="emphasis"><em>k~Poisson(1)</em></span> and updating the base classifier <span class="emphasis"><em>k</em></span> times. Thus, the dependency on the number of examples is removed and the algorithm can run on an infinite stream:</p><p>
<span class="emphasis"><em>OnlineBagging(x: inputstream, h</em></span><sub>m</sub><span class="emphasis"><em>: m learner models)</em></span>
</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">initialize base models <span class="emphasis"><em>h</em></span><sub>m</sub> for all <span class="emphasis"><em>m</em></span> ∈ {1,2,..<span class="emphasis"><em>M</em></span>}</li><li class="listitem">for(t=1,2,…T)  do<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="emphasis"><em>x</em></span><sub>t</sub><span class="emphasis"><em>=receive();</em></span></li><li class="listitem">foreach model <span class="emphasis"><em>m</em></span> = {1,2,..<span class="emphasis"><em>M</em></span>}<p>
<span class="emphasis"><em>w = Poisson</em></span>(1)</p><p>
<span class="emphasis"><em>updateModel(h</em></span><sub>m</sub><span class="emphasis"><em>, w, x</em></span><sub>t</sub><span class="emphasis"><em>)</em></span>
</p></li><li class="listitem">end</li></ol></div></li><li class="listitem">return</li><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B05137_05_191.jpg" alt="How does it work?"/></span></li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec34"/>Advantages and limitations</h5></div></div></div><p>The advantages and limitaions are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It <a id="id1015" class="indexterm"/>has been empirically shown to be one of the most <a id="id1016" class="indexterm"/>successful online or stream algorithms.</li><li class="listitem" style="list-style-type: disc">The weight must be given to the data instance without looking at the other instances; this reduces the choices of different weighting schemes which are available in batch and are good in model performance.</li></ul></div><p>The performance is entirely determined by the choice of the <span class="emphasis"><em>M</em></span> learners—the type of learner used for the problem domain. We can only decide on this choice by adopting different validation techniques described in the section on model validation techniques.</p></div></div><div class="section" title="Online Boosting algorithm"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec141"/>Online Boosting algorithm</h4></div></div></div><p>The <a id="id1017" class="indexterm"/>supervised boosting algorithm <a id="id1018" class="indexterm"/>takes many <span class="emphasis"><em>weak learners</em></span> whose accuracy is slightly greater than random and combines them to produce a strong learner by iteratively sampling the misclassified examples. The concept is identical in Oza's Online Boosting algorithm with modification done for a continuous data stream.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec35"/>Inputs and outputs</h5></div></div></div><p>The <a id="id1019" class="indexterm"/>constraint on types of inputs (categorical only, continuous only, or mixed) depends on the chosen base classifiers. The base classifier <a id="id1020" class="indexterm"/>algorithms and their respective parameters are inputs. The output is the learned model that can predict the class/confidence based on the classifier chosen.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec36"/>How does it work?</h5></div></div></div><p>The <a id="id1021" class="indexterm"/>modification of batch boosting to online boosting is done as follows: </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Keep two sets of weights for <span class="emphasis"><em>M</em></span> base models, <span class="emphasis"><em>λ</em></span><sup>c</sup> is a vector of dimension <span class="emphasis"><em>M</em></span> which carries the sum of weights of correctly classified instances, and <span class="emphasis"><em>λ</em></span><sup>w</sup> is a vector of dimension <span class="emphasis"><em>M</em></span>, which carries the sum of weights of incorrectly classified instances. </li><li class="listitem">The weights are initialized to 1.</li><li class="listitem">Given a new instance (<span class="strong"><strong>x</strong></span><sub>t</sub>, <span class="emphasis"><em>y</em></span><sub>t</sub>), the algorithm goes through the iterations of updating the base models.</li><li class="listitem">For each base model, the following steps are repeated:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">For the first iteration, <span class="emphasis"><em>k = Poisson(λ)</em></span> is set and the learning classifier updates the algorithm (denoted here by <span class="emphasis"><em>h</em></span><sub>1</sub>) <span class="emphasis"><em>k</em></span> times using (<span class="strong"><strong>x</strong></span><sub>t</sub>, <span class="emphasis"><em>y</em></span><sub>t</sub>):</li><li class="listitem">If <span class="emphasis"><em>h</em></span><sub>1</sub> incorrectly classifies the instance, the <span class="emphasis"><em>λ</em></span><sup>w1</sup> is incremented, <span class="emphasis"><em>ϵ</em></span><sub>1</sub>, the weighted fraction, incorrectly classified by <span class="emphasis"><em>h</em></span><sub>1</sub>, is computed and the weight of the example is multiplied by 1/2 <span class="emphasis"><em>ϵ</em></span><sub>1</sub>.</li></ol></div></li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec37"/>Advantages and limitations</h5></div></div></div><p>The advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Again, the <a id="id1022" class="indexterm"/>performance is determined by the choice <a id="id1023" class="indexterm"/>of the multiple learners, their types and the particular domain of the problem. The different methods described in the section on model validation techniques help us in choosing the learners.</li><li class="listitem" style="list-style-type: disc">Oza's Online Boosting has been shown theoretically and empirically not to be "lossless"; that is, the model is different compared to its batch version. Thus, it suffers from performance issues and different extensions have been studied in recent years to improve performance.</li></ul></div></div></div></div></div><div class="section" title="Validation, evaluation, and comparisons in online setting"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec74"/>Validation, evaluation, and comparisons in online setting</h2></div></div></div><p>In <a id="id1024" class="indexterm"/>contrast to the modes of machine learning <a id="id1025" class="indexterm"/>we saw in the previous chapters, stream <a id="id1026" class="indexterm"/>learning presents unique challenges to performing the core steps of validation and evaluation. The fact that we are no longer dealing with batch data means the standard techniques for validation evaluation and model comparison must be adapted for incremental learning.</p><div class="section" title="Model validation techniques"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec80"/>Model validation techniques</h3></div></div></div><p>In the<a id="id1027" class="indexterm"/> off-line or the <a id="id1028" class="indexterm"/>batch setting, we discussed various methods of tuning the parameters of the algorithm or testing the generalization capability of the algorithms as a counter-measure against overfitting. Some of the techniques in the batch labeled data, such as cross-validation, are not directly applicable in the online or stream settings. The most common techniques used in online or stream settings are given next.</p><div class="section" title="Prequential evaluation"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec142"/>Prequential evaluation</h4></div></div></div><p>The <a id="id1029" class="indexterm"/>prequential evaluation method is a method where instances are provided to the algorithm and the output prediction of the algorithm is then measured in comparison with the actual label using a loss function. Thus, the algorithm is always tested on the unseen data and needs no "holdout" data to estimate the generalization. The prequential error is computed based on the sum of the accumulated loss function between actual values and predicted values, given by: </p><div class="mediaobject"><img src="graphics/B05137_05_206.jpg" alt="Prequential evaluation"/></div><p>Three variations of basic prequential evaluation are done for better estimation on changing data, which are: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Using Landmark Window (basic)</li><li class="listitem" style="list-style-type: disc">Using Sliding Window</li><li class="listitem" style="list-style-type: disc">Using forgetting mechanism</li></ul></div><p>The last two methods are extensions of previously described techniques where you put weights or fading factors on the predictions that reduce over time.</p></div><div class="section" title="Holdout evaluation"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec143"/>Holdout evaluation</h4></div></div></div><p>This <a id="id1030" class="indexterm"/>is the extension of the holdout mechanism or "independent test set" methodology of the batch learning. Here the total labeled set or stream data is separated into training and testing sets, either based on some fixed intervals or the number of examples/instances the algorithm has seen. Imagine a continuous stream of data and we place well-known intervals at <span class="inlinemediaobject"><img src="graphics/B05137_05_207.jpg" alt="Holdout evaluation"/></span> and <span class="inlinemediaobject"><img src="graphics/B05137_05_208.jpg" alt="Holdout evaluation"/></span> to compare the evaluation metrics, as discussed in next section, for performance.</p></div><div class="section" title="Controlled permutations"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec144"/>Controlled permutations</h4></div></div></div><p>The issue with the aforementioned mechanisms is that they provide "average" behavior over <a id="id1031" class="indexterm"/>time and can mask some basic issues such as the algorithm doing well at the start and very poorly at the end due to drift, for example. The advantage of the preceding methods is that they can be applied to real incoming streams to get estimates. One way to overcome the disadvantage is to create different random sets of the data where the order is shuffled a bit while maintaining the proximity in time and the evaluation is done over a number of these random sets.</p></div><div class="section" title="Evaluation criteria"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec145"/>Evaluation criteria</h4></div></div></div><p>Most of the <a id="id1032" class="indexterm"/>evaluation criteria are the same as described in the chapter on supervised learning and should be chosen based on the business problem, the mapping of the business problem to the machine learning techniques, and on the benefits derived. In this section, the most commonly used online supervised learning evaluation criteria are summarized for the reader:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Accuracy</strong></span>: A measure of <a id="id1033" class="indexterm"/>getting the true positives and true negatives correctly classified by the learning algorithm:<div class="mediaobject"><img src="graphics/B05137_05_209.jpg" alt="Evaluation criteria"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Balanced accuracy</strong></span>: When the classes are imbalanced, balanced accuracy is often <a id="id1034" class="indexterm"/>used as a measure. Balanced accuracy is an arithmetic mean of specificity and sensitivity. It can be also thought of as accuracy when positive and negative instances are drawn from the same probability in a binary classification problem. </li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Area under the ROC curve</strong></span> (<span class="strong"><strong>AUC</strong></span>): Area under the ROC curve gives a good measure <a id="id1035" class="indexterm"/>of generalization of the algorithm. Closer to 1.0 means the algorithm has good generalization capability while close to 0.5 means it is closer to a random guess.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Kappa statistic</strong></span> (<span class="strong"><strong>K</strong></span>): The Kappa statistic is used to measure the observed accuracy <a id="id1036" class="indexterm"/>with the expected accuracy of random guessing in the classification. In online learning, the Kappa statistic is used by computing the prequential accuracy (<span class="emphasis"><em>p</em></span>o) and the random classifier accuracy (<span class="emphasis"><em>p</em></span>c) and is given by:<div class="mediaobject"><img src="graphics/B05137_05_213.jpg" alt="Evaluation criteria"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Kappa Plus statistic</strong></span>: The Kappa Plus statistic is a modification to the Kappa statistic <a id="id1037" class="indexterm"/>obtained by replacing the random classifier by the persistent classifier. The persistent classifier is a classifier which predicts the next instance based on the label or outcome of the previous instance.</li></ul></div><p>When considering "drift" or change in the concept as discussed earlier, in addition to these standard measures, some well-known measures given are used to give a quantitative measure:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Probability of true change detection</strong></span>: Usually, measured with synthetic data or data where the changes are known. It gives the ability of the learning algorithm to detect the change.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Probability of false alarm</strong></span>: Instead of using the False Positive Rate in the off-line setting, the online setting uses the inverse of <span class="emphasis"><em>time to detection or the average run</em></span> length which is computed using the expected time between false positive detections.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Delay of detection</strong></span>: This is measured as the time required, terms of instances, to identify the drift.</li></ul></div></div><div class="section" title="Comparing algorithms and metrics"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec146"/>Comparing algorithms and metrics</h4></div></div></div><p>When <a id="id1038" class="indexterm"/>comparing two classifiers or learners in online settings, the usual mechanism is the method of taking a performance metric, such as the error rate, and using a statistical test adapted to online learning. Two widely used methods are described next:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>McNemar test</strong></span>: McNemar's test is a non-parametric statistical test normally employed to <a id="id1039" class="indexterm"/>compare two classifiers' evaluation metrics, such as "error rate", by storing simple statistics about the two classifiers. By computing statistic <span class="emphasis"><em>a</em></span>, the number of correctly classified points by one algorithm that are incorrectly classified by the other, and statistic <span class="emphasis"><em>b</em></span>, which is the inverse, we obtain the McNemar's Test as:<div class="mediaobject"><img src="graphics/B05137_05_217.jpg" alt="Comparing algorithms and metrics"/></div><p>The test follows a χ2 distribution and the p-value can be used to check for statistical significance.</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Nemenyi test</strong></span>: When there are multiple algorithms and multiple datasets, we use the <a id="id1040" class="indexterm"/>Nemenyi test for statistical significance, which is based on average ranks across all. Two algorithms are considered to be performing differently in a statistically significant way if the ranks differ by a critical difference given by:<div class="mediaobject"><img src="graphics/B05137_05_218.jpg" alt="Comparing algorithms and metrics"/></div><p>Here,  K=number of algorithms, N=number of datasets.</p></li></ul></div><p>The critical <a id="id1041" class="indexterm"/>difference values are assumed to follow a Student-T distribution.</p></div></div></div></div>
<div class="section" title="Incremental unsupervised learning using clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec46"/>Incremental unsupervised learning using clustering</h1></div></div></div><p>The concept <a id="id1042" class="indexterm"/>behind clustering <a id="id1043" class="indexterm"/>in a data stream remains the same as in batch or offline modes; that is, finding interesting clusters or patterns which group together in the data while keeping the limits on finite memory and time required to process as constraints. Doing single-pass modifications to existing algorithms or keeping a small memory buffer to do mini-batch versions of existing algorithms, constitute the basic changes done in all the algorithms to make them suitable for stream or real-time unsupervised learning. </p><div class="section" title="Modeling techniques"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec75"/>Modeling techniques</h2></div></div></div><p>The <a id="id1044" class="indexterm"/>clustering modeling techniques for online learning are divided into partition-based, hierarchical-based, density-based, and grid-based, similar to the case of batch-based clustering. </p><div class="section" title="Partition based"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec81"/>Partition based</h3></div></div></div><p>The <a id="id1045" class="indexterm"/>concept of partition-based algorithms is similar to batch-based clustering where <span class="strong"><strong>k</strong></span> clusters are formed to optimize certain objective functions such as minimizing the inter-cluster distance, maximizing the intra-cluster distance, and so on. </p><div class="section" title="Online k-Means"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec147"/>Online k-Means</h4></div></div></div><p>k-Means <a id="id1046" class="indexterm"/>is the most popular clustering algorithm, which partitions the data into user-specified <span class="emphasis"><em>k</em></span> clusters, mostly to minimize the squared error or distance between centroids and cluster assigned points. We will illustrate a very basic online adaptation of k-Means, of which several variants exist. </p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec38"/>Inputs and outputs</h5></div></div></div><p>Mainly, numeric <a id="id1047" class="indexterm"/>features are considered as inputs; a few tools take <a id="id1048" class="indexterm"/>categorical features and convert them into some form of numeric representation. The algorithm itself takes the parameters' number of clusters <span class="emphasis"><em>k</em></span> and number of max iterations <span class="emphasis"><em>n</em></span> as inputs.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec39"/>How does it work?</h5></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The input data stream is considered to be infinite but of constant block size.</li><li class="listitem">A memory <a id="id1049" class="indexterm"/>buffer of the block size is kept reserved to store the data or a compressed representation of the data.</li><li class="listitem">Initially, the first stream of data of block size is used to find the <span class="emphasis"><em>k</em></span> centroids of the clusters, the centroid information is stored and the buffer is cleared.</li><li class="listitem">For the next data after it reaches the block size: <div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">For either max number of iterations or until there is no change in the centroids:</li><li class="listitem">Execute k-Means with buffer data and the present centroids.</li><li class="listitem">Minimize the squared sum error between centroids and data assigned to the cluster.</li><li class="listitem">After the iterations, the buffer is cleared and new centroids are obtained.</li></ol></div></li><li class="listitem">Repeat step 4 until the data is no longer available.</li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec40"/>Advantages and limitations</h5></div></div></div><p>The advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Similar <a id="id1050" class="indexterm"/>to batch-based, the shape of the detected cluster <a id="id1051" class="indexterm"/>depends on the distance measure and is not appropriate in problem domains with irregular shapes.</li><li class="listitem" style="list-style-type: disc">The choice of parameter <span class="strong"><strong>k</strong></span>, as in batch-based, can limit the performance in datasets with many distinct patterns or clusters. </li><li class="listitem" style="list-style-type: disc">Outliers and missing data can pose lots of irregularities in clustering behavior of online k-Means.</li><li class="listitem" style="list-style-type: disc">If the selected buffer size or the block size of the stream on which iterative k-Means runs is small, it will not find the right clusters. If the chosen block size is large, it can result in slowdown or <a id="id1052" class="indexterm"/>missed changes in the data. Extensions such as <span class="strong"><strong>Very Fast k-Means Algorithm</strong></span> (<span class="strong"><strong>VFKM</strong></span>), which uses the Hoeffding bound to determine the buffer size, overcome this limitation to a large extent. </li></ul></div></div></div></div><div class="section" title="Hierarchical based and micro clustering"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec82"/>Hierarchical based and micro clustering</h3></div></div></div><p>Hierarchical <a id="id1053" class="indexterm"/>methods are <a id="id1054" class="indexterm"/>normally based on <span class="strong"><strong>Clustering Features</strong></span> (<span class="strong"><strong>CF</strong></span>) and <span class="strong"><strong>Clustering Trees</strong></span> (<span class="strong"><strong>CT</strong></span>). We will describe the <a id="id1055" class="indexterm"/>basics and elements of hierarchical <a id="id1056" class="indexterm"/>clustering and the BIRCH algorithm, the extension of which the CluStream algorithm is based on.</p><p>The Clustering Feature is a way to compute and preserve a summarization statistic about the cluster in a compressed way rather than holding on to the whole data belonging to the cluster. In a <span class="strong"><strong>d</strong></span> dimensional dataset, with <span class="strong"><strong>N</strong></span> points in the cluster, two aggregates in the form of total sum <span class="strong"><strong>LS</strong></span> for each dimensions and total squared sum of data <span class="strong"><strong>SS</strong></span> again for each dimension, are computed and the vector representing this triplet form the Clustering Feature:</p><p>
<span class="emphasis"><em>CF</em></span><sub>j</sub><span class="emphasis"><em> = &lt; N, LS</em></span><sub>j</sub><span class="emphasis"><em>, SS</em></span><sub>j</sub><span class="emphasis"><em> &gt;</em></span>
</p><p>These statistics <a id="id1057" class="indexterm"/>are useful in summarizing the <a id="id1058" class="indexterm"/>entire cluster information. The centroid of the cluster can be easily computed using:</p><p>
<span class="emphasis"><em>centroid</em></span><sub>j</sub><span class="emphasis"><em> = LS</em></span><sub>j</sub><span class="emphasis"><em>/N</em></span>
</p><p>The radius of the cluster can be estimated using:</p><div class="mediaobject"><img src="graphics/B05137_05_231.jpg" alt="Hierarchical based and micro clustering"/></div><p>The diameter of the cluster can be estimated using:</p><div class="mediaobject"><img src="graphics/B05137_05_232.jpg" alt="Hierarchical based and micro clustering"/></div><p>CF vectors have great incremental and additive properties which becomes useful in stream or incremental updates.</p><p>For an incremental update, when we must update the CF vector, the following holds true: </p><div class="mediaobject"><img src="graphics/B05137_05_233.jpg" alt="Hierarchical based and micro clustering"/></div><div class="mediaobject"><img src="graphics/B05137_05_234.jpg" alt="Hierarchical based and micro clustering"/></div><div class="mediaobject"><img src="graphics/B05137_05_235.jpg" alt="Hierarchical based and micro clustering"/></div><p>When two CFs have to be merged, the following holds true:</p><div class="mediaobject"><img src="graphics/B05137_05_236.jpg" alt="Hierarchical based and micro clustering"/></div><div class="mediaobject"><img src="graphics/B05137_05_237.jpg" alt="Hierarchical based and micro clustering"/></div><div class="mediaobject"><img src="graphics/B05137_05_238.jpg" alt="Hierarchical based and micro clustering"/></div><p>The <span class="strong"><strong>Clustering Feature Tree</strong></span> (<span class="strong"><strong>CF Tree</strong></span>) represents <a id="id1059" class="indexterm"/>an hierarchical tree structure. The construction of the CF tree requires two user defined parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Branching factor <span class="strong"><strong>b</strong></span> which is the maximum number of sub-clusters or non-leaf nodes any node can have</li><li class="listitem" style="list-style-type: disc">Maximum diameter (or radius) <span class="strong"><strong>T</strong></span>, the number of examples that can be absorbed by the leaf node for a CF parent node</li></ul></div><p>CF Tree operations <a id="id1060" class="indexterm"/>such as insertion are done by recursively <a id="id1061" class="indexterm"/>traversing the CF Tree and using the CF vector for finding the closest node based on distance metrics. If a leaf node has already absorbed the maximum elements given by parameter <span class="emphasis"><em>T</em></span>, the node is split. At the end of the operation, the CF vector is appropriately updated for its statistic:</p><div class="mediaobject"><img src="graphics/B05137_05_245.jpg" alt="Hierarchical based and micro clustering"/><div class="caption"><p>Figure 3 An example Clustering Feature Tree illustrating hierarchical structure.</p></div></div><p>We will discuss <span class="strong"><strong>BIRCH</strong></span> (<span class="strong"><strong>Balanced Iterative Reducing and Clustering Hierarchies</strong></span>) following <a id="id1062" class="indexterm"/>this concept.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl5sec41"/>Inputs and outputs</h4></div></div></div><p>BIRCH only <a id="id1063" class="indexterm"/>accepts numeric features. CF and CF tree parameters, such as branching factor <span class="emphasis"><em>b</em></span> and maximum <a id="id1064" class="indexterm"/>diameter (or radius) <span class="emphasis"><em>T</em></span> for leaf are user-defined inputs. </p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl5sec42"/>How does it work?</h4></div></div></div><p>BIRCH, designed <a id="id1065" class="indexterm"/>for very large databases, was meant to be a <span class="emphasis"><em>two-pass</em></span> algorithm; that is, scan the entire data once and re-scan it again, thus being an <span class="emphasis"><em>O(N)</em></span> algorithm. It can be modified easily enough for online as a single pass algorithm preserving the same properties: </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In the first phase or scan, it goes over the data and creates an in-memory CF Tree structure by sequentially visiting the points and carrying out CF Tree operations as discussed previously.</li><li class="listitem">In the second phase, an optional phase, we remove outliers and merge sub-clusters.</li><li class="listitem">Phase three is to overcome the issue of order of data in phase one. We use agglomerative hierarchical clustering to refactor the CF Tree.</li><li class="listitem">Phase four is the last phase which is an optional phase to compute statistics such as centroids, assign data to closest centroids, and so on, for more effectiveness.</li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl5sec43"/>Advantages and limitations</h4></div></div></div><p>The advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It is <a id="id1066" class="indexterm"/>one of the most popular algorithms that scales linearly on a large database or stream of data.</li><li class="listitem" style="list-style-type: disc">It <a id="id1067" class="indexterm"/>has compact memory representation in the form of the CF and CF Tree for statistics and operations on incoming data.</li><li class="listitem" style="list-style-type: disc">It handles outliers better than most algorithms.</li><li class="listitem" style="list-style-type: disc">One of the major limitations is that it has been shown not to perform well when the shape of the clusters is not spherical.</li><li class="listitem" style="list-style-type: disc">The concepts of CF vector and clustering in BIRCH were extended for efficient stream mining requirements by Aggarwal <span class="emphasis"><em>et al</em></span> and named <span class="emphasis"><em>micro-cluster and CluStream</em></span>. </li></ul></div></div><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl5sec44"/>Inputs and outputs</h4></div></div></div><p>CluStream only <a id="id1068" class="indexterm"/>accepts numeric features. Among the user-defined <a id="id1069" class="indexterm"/>parameters are the number of micro-clusters in memory (<span class="emphasis"><em>q</em></span>) and the threshold (<span class="emphasis"><em>δ</em></span>) in time after which they can be deleted. Additionally, included in the input are time-sensitive parameters for storing the micro-clusters information, given by <span class="emphasis"><em>α</em></span> and <span class="emphasis"><em>l</em></span>.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl5sec45"/>How does it work?</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The micro-cluster <a id="id1070" class="indexterm"/>extends the CF vector and keeps two additional measures. They are the sum of the timestamps and sum of the squares of timestamps: <p>
<span class="emphasis"><em>microCluster</em></span><sub>j</sub><span class="emphasis"><em> = &lt; N, LS</em></span><sub>j</sub><span class="emphasis"><em>, SS</em></span><sub>j</sub><span class="emphasis"><em>, ST, SST&gt;</em></span>
</p></li><li class="listitem">The algorithm stores <span class="emphasis"><em>q</em></span> micro-clusters in memory and each micro-cluster has a <span class="emphasis"><em>maximum boundary</em></span> that can be computed based on means and standard deviations between centroid and cluster instance distances. The measures are multiplied by a factor which decreases exponentially with time.</li><li class="listitem">For each new instance, we select the closest micro-cluster based on Euclidean distance and decide whether it should be absorbed:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">If the distance between the new instance and the centroid of the closest micro-cluster falls within the maximum boundary, it is absorbed and the micro-cluster statistics are updated.</li><li class="listitem">If none of the micro-clusters can absorb, a new micro-cluster is created with the instance and based on the timestamp and threshold (<span class="emphasis"><em>δ</em></span>), the oldest micro-cluster is deleted. </li><li class="listitem">Assuming normal distribution of timestamps, if the relevance time—the time of arrival of instance found by CluStream—is below the user-specified threshold, it is considered an outlier and removed. Otherwise, the two closest micro-clusters are merged.</li></ol></div></li><li class="listitem">The micro-cluster information is stored in secondary storage from time to time by using a pyramidal time window concept. Each micro-cluster has time intervals decrease exponentially using <span class="emphasis"><em>α</em></span>l to create snapshots. These help in efficient search in both time and space.</li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl5sec46"/>Advantages and limitations</h4></div></div></div><p>The advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">CluStream <a id="id1071" class="indexterm"/>has been shown to be very effective in finding clusters in real time</li><li class="listitem" style="list-style-type: disc">The CluStream <a id="id1072" class="indexterm"/>algorithm, through effective storage using a pyramidal timestamp, has efficient time and space usage. CluStream, like BIRCH, can find only spherical shaped clusters </li></ul></div></div></div><div class="section" title="Density based"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec83"/>Density based</h3></div></div></div><p>Similar to <a id="id1073" class="indexterm"/>batch clustering, density-based <a id="id1074" class="indexterm"/>techniques overcome the "shape" issue faced by distance-based algorithms. Here we will present a well-known density-based algorithm, DenStream, which is based on the concepts of CF and CF Trees discussed previously.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl5sec47"/>Inputs and outputs</h4></div></div></div><p>The <a id="id1075" class="indexterm"/>extent of the neighborhood of a core micro-cluster <a id="id1076" class="indexterm"/>is the user-defined radius <span class="emphasis"><em>ϵ</em></span>. A second input value is the minimum total weight <span class="emphasis"><em>µ</em></span> of the micro-cluster which is the sum over the weighted function of the arrival time of each instance in the object, where the weight decays with a time constant proportional to another user-defined parameter, <span class="emphasis"><em>λ</em></span>. Finally, an input factor <span class="emphasis"><em>β</em></span> ∈ (0,1) is used to distinguish potential core micro-clusters from outlier micro-clusters.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl5sec48"/>How does it work?</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Based on <a id="id1077" class="indexterm"/>the micro-cluster concepts of CluStream, DenStream holds two data structures: <span class="emphasis"><em>p-micro-cluster</em></span> for potential clusters and <span class="emphasis"><em>o-micro-clusters</em></span> for outlier detection.</li><li class="listitem">Each <span class="emphasis"><em>p-micro-cluster</em></span> structure has: <div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
A weight associated with it which decreases exponentially with the timestamps it has been updated with. If there are <span class="emphasis"><em>j</em></span> objects in the micro-cluster:
<span class="inlinemediaobject"><img src="graphics/B05137_05_260.jpg" alt="How does it work?"/></span>
 where <span class="emphasis"><em>f(t) = </em></span>2<sup>-λt</sup></li><li class="listitem">  <span class="strong"><strong>Weighted linear sum</strong></span> (<span class="strong"><strong>WLS</strong></span>) and <span class="strong"><strong>weighted linear sum of squares</strong></span> (<span class="strong"><strong>WSS</strong></span>) are stored in micro-clusters similar to linear sum and sum of squares:<div class="mediaobject"><img src="graphics/B05137_05_262.jpg" alt="How does it work?"/></div><p>
</p><div class="mediaobject"><img src="graphics/B05137_05_263.jpg" alt="How does it work?"/></div><p>
</p></li><li class="listitem">The <a id="id1078" class="indexterm"/>mean, radius, and diameter of the clusters are <a id="id1079" class="indexterm"/>then computed using the weighted measures defined previously, exactly like in CF. For example, the radius can be given as: </li></ol></div><div class="mediaobject"><img src="graphics/B05137_05_264.jpg" alt="How does it work?"/></div></li><li class="listitem">Each <span class="emphasis"><em>o-micro-cluster</em></span> has the same structure as <span class="emphasis"><em>p-micro-cluster</em></span> and timestamps associated with it. </li><li class="listitem">When a new instance arrives:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">A closest <span class="emphasis"><em>p-micro-cluster</em></span> is found and the instance is inserted if the new radius is within the user-defined boundary <span class="emphasis"><em>ϵ</em></span>. If inserted, the <span class="emphasis"><em>p-micro-cluster</em></span> statistics are updated accordingly.</li><li class="listitem">Otherwise, an <span class="emphasis"><em>o-micro-cluster</em></span> is found and the instance is inserted if the new radius is again within the boundary. The boundary is defined by <span class="emphasis"><em>β</em></span> × <span class="emphasis"><em>μ</em></span>, the product of the user-defined parameters, and if the radius grows beyond this value, the <span class="emphasis"><em>o-micro-cluster</em></span> is moved to the <span class="emphasis"><em>p-micro-cluster</em></span>.</li><li class="listitem">If the instance <a id="id1080" class="indexterm"/>cannot be absorbed by an <span class="emphasis"><em>o-micro-cluster</em></span>, then a new micro-cluster is added to the <span class="emphasis"><em>o-micro-clusters</em></span>.</li></ol></div></li><li class="listitem">At the time interval <span class="emphasis"><em>t</em></span> based on weights, the <span class="emphasis"><em>o-micro-cluster</em></span> can become the <span class="emphasis"><em>p-micro-cluster</em></span> or vice versa. The time interval is defined in terms of <span class="emphasis"><em>λ</em></span>, <span class="emphasis"><em>β</em></span>, and <span class="emphasis"><em>µ</em></span> as: <div class="mediaobject"><img src="graphics/B05137_05_269.jpg" alt="How does it work?"/></div></li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl5sec49"/>Advantages and limitations</h4></div></div></div><p>The advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Based <a id="id1081" class="indexterm"/>on the parameters, DenStream can find effective <a id="id1082" class="indexterm"/>clusters and outliers for real-time data.</li><li class="listitem" style="list-style-type: disc">It has the advantage of finding clusters and outliers of any shape or size. </li><li class="listitem" style="list-style-type: disc">The house keeping job of updating the <span class="emphasis"><em>o-micro-cluster</em></span> and <span class="emphasis"><em>p-micro-cluster</em></span> can be computationally expensive if not selected properly.</li></ul></div></div></div><div class="section" title="Grid based"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec84"/>Grid based</h3></div></div></div><p>This <a id="id1083" class="indexterm"/>technique is based on discretizing <a id="id1084" class="indexterm"/>the multi-dimensional continuous space into a multi-dimensional discretized version with grids. The mapping of the incoming instance to grid online and maintaining the grid offline results in an efficient and effective way of finding clusters in real-time. </p><p>Here we present D-Stream, which is a grid-based online stream clustering algorithm. </p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl5sec50"/>Inputs and outputs</h4></div></div></div><p>As in <a id="id1085" class="indexterm"/>density-based algorithms, the idea of decaying weight of <a id="id1086" class="indexterm"/>instances is used in D-Stream. Additionally, as described next, cells in the grid formed from the input space may be deemed sparse, dense, or sporadic, distinctions that are central to the computational and space efficiency of the algorithm. The inputs to the grid-based algorithm, then, are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>λ</em></span>: The decay factor </li><li class="listitem" style="list-style-type: disc">0 &lt; <span class="emphasis"><em>C</em></span><sub>l</sub> &lt; 1 and <span class="emphasis"><em>C</em></span><sub>m</sub> &gt; 1: Parameters <a id="id1087" class="indexterm"/>that control the <a id="id1088" class="indexterm"/>boundary between dense and sparse cells in the grid</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>β</em></span> &gt; 0: A constant that controls one of the conditions when a sparse cell is to be considered sporadic.</li></ul></div></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl5sec51"/>How does it work?</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Each<a id="id1089" class="indexterm"/> instance arriving at time <span class="emphasis"><em>t</em></span> has a density coefficient that decreases exponentially over time:<div class="mediaobject"><img src="graphics/B05137_05_273.jpg" alt="How does it work?"/></div></li><li class="listitem">Density of the grid cell <span class="emphasis"><em>g</em></span> at any given time <span class="emphasis"><em>t</em></span> is given by <span class="emphasis"><em>D(g, t)</em></span> and is the sum of the adjusted density of all instances given by <span class="emphasis"><em>E(g, t)</em></span> that are mapped to grid cell <span class="emphasis"><em>g</em></span>:<div class="mediaobject"><img src="graphics/B05137_05_277.jpg" alt="How does it work?"/></div></li><li class="listitem">Each cell in the grid captures the statistics as a characterization vector given by:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>CV(g) = </em></span>&lt;<span class="emphasis"><em>t</em></span><sub>g</sub>, <span class="emphasis"><em>t</em></span>&gt;<sub>m</sub>, <span class="emphasis"><em>D</em></span>, <span class="emphasis"><em>label</em></span>, <span class="emphasis"><em>status</em></span>&gt; where:</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>t</em></span><sub>g</sub> = last time grid cell was updated</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>t</em></span><sub>m</sub><span class="emphasis"><em> </em></span>= last time grid cell was removed due to sparseness</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>D</em></span> = density of the grid cell when last updated</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>label</em></span> = class label of the grid cell</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>status</em></span> = {NORMAL or SPORADIC}</li></ul></div></li><li class="listitem">When the new instance arrives, it gets mapped to a cell <span class="emphasis"><em>g</em></span> and the characteristic vector is updated. If <span class="emphasis"><em>g</em></span> is not available, it is created and the list of grids is updated.</li><li class="listitem">Grid cells with empty instances are removed. Also, cells that have not been updated over a long time can become sparse, and conversely, when many instances are mapped, they become dense.</li><li class="listitem">At a regular time interval known as a gap, the grid cells are inspected for status and the cells with fewer instances than a number—determined by a density threshold function—are treated as outliers and removed. </li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl5sec52"/>Advantages and limitations</h4></div></div></div><p>The advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">D-Streams <a id="id1090" class="indexterm"/>have theoretically and empirically been shown to <a id="id1091" class="indexterm"/>find sporadic and normal clusters with very high efficiency in space and time.</li><li class="listitem" style="list-style-type: disc">It can find clusters of any shape or size effectively.</li></ul></div></div></div><div class="section" title="Validation and evaluation techniques"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec85"/>Validation and evaluation techniques</h3></div></div></div><p>Many of <a id="id1092" class="indexterm"/>the static clustering evaluation measures discussed in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>, have an assumption <a id="id1093" class="indexterm"/>of static and non-evolving patterns. Some of these internal and external measures are used even in streaming based cluster detection. Our goal in this section is to first highlight problems inherent to cluster evaluation in stream learning, then describe different internal and external measures that address these, and finally, present some existing measures—both internal and external—that are still valid. </p><div class="section" title="Key issues in stream cluster evaluation"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec151"/>Key issues in stream cluster evaluation</h4></div></div></div><p>It is <a id="id1094" class="indexterm"/>important to understand some of the important issues that are specific to streaming and clustering, as the measures need to address them:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Aging</strong></span>: The property of points being not relevant to the clustering measure after a given time.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Missed points</strong></span>: The property of a point not only being missed as belonging to the cluster but the amount by which it was missed being in the cluster.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Misplaced points</strong></span>: Changes in clusters caused by evolving new clusters. Merging existing or deleting clusters results in ever-misplaced points with time. The impact of these changes with respect to time must be taken into account.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Cluster noise</strong></span>: Choosing data that should not belong to the cluster or forming clusters around noise and its impact over time must be taken into account.</li></ul></div></div><div class="section" title="Evaluation measures"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec152"/>Evaluation measures</h4></div></div></div><p>Evaluation <a id="id1095" class="indexterm"/>measures for clustering in the context of streaming data must provide a useful index of the quality of clustering, taking into consideration the effect of evolving and noisy data streams, overlapping and merging clusters, and so on. Here we present some external measures used in stream clustering. Many internal measures encountered in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>, such as the Silhouette coefficient, Dunn's Index, and R-Squared, are also used and are not repeated here.</p><div class="section" title="Cluster Mapping Measures (CMM)"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec53"/>Cluster Mapping Measures (CMM)</h5></div></div></div><p>The <a id="id1096" class="indexterm"/>idea behind <a id="id1097" class="indexterm"/>CMM is to quantify the connectivity of the points to clusters given the ground truth. It works in three phases:</p><p>
<span class="strong"><strong>Mapping phase</strong></span>: In this phase, clusters assigned by the stream learning algorithm are mapped <a id="id1098" class="indexterm"/>to the ground truth clusters. Based on these, various statistics of distance and point connectivity are measured using the concepts of k-Nearest Neighbors.</p><p>The average distance of point <span class="emphasis"><em>p</em></span> to its closest <span class="emphasis"><em>k</em></span> neighbors in a cluster <span class="emphasis"><em>C</em></span>i is given by:</p><div class="mediaobject"><img src="graphics/B05137_05_287.jpg" alt="Cluster Mapping Measures (CMM)"/></div><p>The average distance for a cluster <span class="emphasis"><em>C</em></span><sub>i</sub> is given by:</p><div class="mediaobject"><img src="graphics/B05137_05_288.jpg" alt="Cluster Mapping Measures (CMM)"/></div><p>The point connectivity of a point <span class="emphasis"><em>p</em></span> in a cluster <span class="emphasis"><em>C</em></span><sub>i</sub> is given by:</p><div class="mediaobject"><img src="graphics/B05137_05_289.jpg" alt="Cluster Mapping Measures (CMM)"/></div><p>Class frequencies are counted for each cluster and the mapping of the cluster to the ground truth is performed by calculating the histograms and similarity in the clustering.</p><p>Specifically, a cluster <span class="emphasis"><em>C</em></span><sub>i</sub> is mapped to the ground truth class, and <span class="emphasis"><em>Cl</em></span><sub>j</sub> is mapped to a ground truth cluster <span class="inlinemediaobject"><img src="graphics/B05137_05_350.jpg" alt="Cluster Mapping Measures (CMM)"/></span>, which covers the majority of class frequencies of <span class="emphasis"><em>C</em></span><sub>i</sub>. The surplus is defined as the number of instances from class <span class="emphasis"><em>Cl</em></span><sub>i</sub> not covered by the ground truth cluster <span class="inlinemediaobject"><img src="graphics/B05137_05_350.jpg" alt="Cluster Mapping Measures (CMM)"/></span> and total surplus for instances in classes <span class="emphasis"><em>Cl</em></span><sub>1</sub>, <span class="emphasis"><em>Cl</em></span><sub>2</sub>, <span class="emphasis"><em>Cl</em></span><sub>3</sub> … <span class="emphasis"><em>Cl</em></span><sub>1</sub> in cluster <span class="emphasis"><em>C</em></span><sub>i</sub> compared to <span class="inlinemediaobject"><img src="graphics/B05137_05_350.jpg" alt="Cluster Mapping Measures (CMM)"/></span> is given by:</p><div class="mediaobject"><img src="graphics/B05137_05_294.jpg" alt="Cluster Mapping Measures (CMM)"/></div><p>Cluster <span class="emphasis"><em>C</em></span><sub>i</sub> is mapped using:</p><div class="mediaobject"><img src="graphics/B05137_05_295.jpg" alt="Cluster Mapping Measures (CMM)"/></div><p>
<span class="strong"><strong>Penalty phase</strong></span>: The penalty for every instance that is mapped incorrectly is calculated in this <a id="id1099" class="indexterm"/>step using computations of fault objects; that is, objects which are not noise and yet incorrectly placed, using: </p><div class="mediaobject"><img src="graphics/B05137_05_296.jpg" alt="Cluster Mapping Measures (CMM)"/></div><p>The overall penalty of point <span class="emphasis"><em>o</em></span> with respect to all clusters found is given by:</p><div class="mediaobject"><img src="graphics/B05137_05_297.jpg" alt="Cluster Mapping Measures (CMM)"/></div><p>
<span class="strong"><strong>CMM calculation</strong></span>: Using all the penalties weighted over the lifespan is given by:</p><div class="mediaobject"><img src="graphics/B05137_05_298.jpg" alt="Cluster Mapping Measures (CMM)"/></div><p>Here, <span class="emphasis"><em>C</em></span> is found clusters, <span class="emphasis"><em>Cl</em></span> is ground truth clusters, <span class="emphasis"><em>F</em></span> is the fault objects and <span class="emphasis"><em>w(o)</em></span> is the weight of instance.</p></div><div class="section" title="V-Measure"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec54"/>V-Measure</h5></div></div></div><p>Validity <a id="id1100" class="indexterm"/>or V-Measure is an external measure <a id="id1101" class="indexterm"/>which is computed based on two properties that are of interest <a id="id1102" class="indexterm"/>in stream clustering, namely, <span class="strong"><strong>Homogeneity</strong></span> and <span class="strong"><strong>Completeness</strong></span>. If there are <span class="emphasis"><em>n</em></span> classes as set <span class="emphasis"><em>C</em></span> = {<span class="emphasis"><em>c</em></span><sub>1</sub>, <span class="emphasis"><em>c</em></span><sub>2</sub> …, <span class="emphasis"><em>c</em></span><sub>n</sub>} and <span class="emphasis"><em>k</em></span> clusters <span class="emphasis"><em>K</em></span> = {<span class="emphasis"><em>k</em></span><sub>1</sub>, <span class="emphasis"><em>k</em></span><sub>2..</sub><span class="emphasis"><em>k</em></span><sub>m</sub>}, contingency <a id="id1103" class="indexterm"/>tables are created such that <span class="emphasis"><em>A</em></span> = {<span class="emphasis"><em>a</em></span><sub>ij</sub>} corresponds to the count of instances in class <span class="emphasis"><em>c</em></span><sub>i</sub> and cluster <span class="emphasis"><em>k</em></span><sub>j</sub>.</p><p>
<span class="strong"><strong>Homogeneity</strong></span>: Homogeneity is defined as a property of a cluster that reflects the extent to which all the data in the cluster belongs to the same class.</p><p>Conditional entropy and class entropy:</p><div class="mediaobject"><img src="graphics/B05137_05_309.jpg" alt="V-Measure"/></div><div class="mediaobject"><img src="graphics/B05137_05_310.jpg" alt="V-Measure"/></div><p>Homogeneity is defined as:</p><div class="mediaobject"><img src="graphics/B05137_05_311.jpg" alt="V-Measure"/></div><p>A higher value of homogeneity is more desirable.</p><p>
<span class="strong"><strong>Completeness</strong></span>: Completeness <a id="id1104" class="indexterm"/>is defined as the mirror property of Homogeneity, that is, having all instances of a single class belong to the same cluster. </p><p>Similar to Homogeneity, conditional entropies and cluster entropy are defined as: </p><div class="mediaobject"><img src="graphics/B05137_05_312.jpg" alt="V-Measure"/></div><div class="mediaobject"><img src="graphics/B05137_05_313.jpg" alt="V-Measure"/></div><p>Completeness is defined as:</p><div class="mediaobject"><img src="graphics/B05137_05_314.jpg" alt="V-Measure"/></div><p>V-Measure is defined as the harmonic mean of homogeneity and completeness using a weight factor <span class="emphasis"><em>β</em></span>:</p><div class="mediaobject"><img src="graphics/B05137_05_316.jpg" alt="V-Measure"/></div><p>A higher value of completeness or V-measure is better.</p></div><div class="section" title="Other external measures"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec55"/>Other external measures</h5></div></div></div><p>Some <a id="id1105" class="indexterm"/>external measures which are quite popular in comparing the clustering algorithms or measuring the effectiveness of clustering when the classes are known are given next:</p><p>
<span class="strong"><strong>Purity</strong></span> and <span class="strong"><strong>Entropy</strong></span>: They <a id="id1106" class="indexterm"/>are similar to homogeneity and completeness defined previously.</p><p>Purity <a id="id1107" class="indexterm"/>is defined as:</p><div class="mediaobject"><img src="graphics/B05137_05_317.jpg" alt="Other external measures"/></div><p>Entropy is defined as:</p><div class="mediaobject"><img src="graphics/B05137_05_318.jpg" alt="Other external measures"/></div><p>Here, <span class="emphasis"><em>q</em></span> = number of classes, <span class="emphasis"><em>k</em></span> = number of clusters, <span class="emphasis"><em>n</em></span>r = size of cluster <span class="emphasis"><em>r</em></span> and <span class="inlinemediaobject"><img src="graphics/B05137_05_320.jpg" alt="Other external measures"/></span>.</p><p>
<span class="strong"><strong>Precision</strong></span>, <span class="strong"><strong>Recall</strong></span>, and <span class="strong"><strong>F-Measure</strong></span>: Information <a id="id1108" class="indexterm"/>retrieval measures modified for clustering algorithms <a id="id1109" class="indexterm"/>are as follows:</p><p>Given, <span class="inlinemediaobject"><img src="graphics/B05137_05_321.jpg" alt="Other external measures"/></span> and <span class="inlinemediaobject"><img src="graphics/B05137_05_322.jpg" alt="Other external measures"/></span>.</p><p>Precision <a id="id1110" class="indexterm"/>is defined as:</p><div class="mediaobject"><img src="graphics/B05137_05_323.jpg" alt="Other external measures"/></div><p>Recall is defined as:</p><div class="mediaobject"><img src="graphics/B05137_05_324.jpg" alt="Other external measures"/></div><p>F-measures is defined as:</p><div class="mediaobject"><img src="graphics/B05137_05_325.jpg" alt="Other external measures"/></div></div></div></div></div></div>
<div class="section" title="Unsupervised learning using outlier detection"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec47"/>Unsupervised learning using outlier detection</h1></div></div></div><p>The <a id="id1111" class="indexterm"/>subject of finding outliers or anomalies <a id="id1112" class="indexterm"/>in the data streams is one of the emerging fields in machine learning. This area has not been explored by researchers as much as classification and clustering-based problems have. However, there have been some very interesting ideas extending the concepts of clustering to find outliers from data streams. We will provide some of the research that has been proved to be very effective in stream outlier detection. </p><div class="section" title="Partition-based clustering for outlier detection"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec76"/>Partition-based clustering for outlier detection</h2></div></div></div><p>The central idea here is to use an online partition-based clustering algorithm and based on either <a id="id1113" class="indexterm"/>cluster size ranking or inter-cluster distance ranking, label the clusters as outliers. </p><p>Here we present one such algorithm proposed by Koupaie <span class="emphasis"><em>et al</em></span>., using incremental k-Means.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec86"/>Inputs and outputs</h3></div></div></div><p>Only <a id="id1114" class="indexterm"/>numeric features are used, as in most k-Means algorithms. The <a id="id1115" class="indexterm"/>number of clusters <span class="emphasis"><em>k</em></span> and the number of windows of outliers <span class="emphasis"><em>n</em></span>, on which offline clustering happens, are input parameters. The output is constant outliers (local and global) and an updatable model that detects these.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec87"/>How does it work?</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">This algorithm <a id="id1116" class="indexterm"/>works by having the k-Means algorithm in two modes, an offline mode and an online mode, both working in parallel.</li><li class="listitem">For the online mode:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Apply k-Means on the given window <span class="emphasis"><em>w</em></span> and find clusters and partitions of the data with clusters.</li><li class="listitem">Rank the clusters based on the cluster distances and cluster size. The clusters which are farthest apart and small in size are considered outliers.</li><li class="listitem">Store the outliers in memory for the window as a set <span class="emphasis"><em>O</em></span><sub>w</sub> = {<span class="strong"><strong>x</strong></span><sub>1</sub>, <span class="strong"><strong>x</strong></span><sub>2..</sub><span class="strong"><strong>x</strong></span><sub>n</sub>} and regard them as local outliers.</li><li class="listitem">The window is cleared and the process is repeated.</li></ol></div></li><li class="listitem">For the offline mode:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Get outliers from <span class="emphasis"><em>n</em></span>, previous windows, and create a set: <span class="inlinemediaobject"><img src="graphics/B05137_05_328.jpg" alt="How does it work?"/></span></li><li class="listitem">Cluster this window with set <span class="emphasis"><em>S</em></span> using k-Means and find clusters which are farthest away and small in size.</li><li class="listitem">These clusters are global outliers. </li><li class="listitem">The window is cleared and the process is repeated.</li></ol></div></li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec88"/>Advantages and limitations</h3></div></div></div><p>The advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It is <a id="id1117" class="indexterm"/>very sensitive to the two parameters <span class="emphasis"><em>k</em></span> and <span class="emphasis"><em>n</em></span> and can <a id="id1118" class="indexterm"/>generate lots of noise.</li><li class="listitem" style="list-style-type: disc">Only spherical clusters/outliers are found and outliers with different shapes get missed.</li></ul></div></div></div><div class="section" title="Distance-based clustering for outlier detection"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec77"/>Distance-based clustering for outlier detection</h2></div></div></div><p>Distance-based <a id="id1119" class="indexterm"/>outlier detection is the most studied, researched, and implemented method in the area of stream learning. There are many variants of the distance-based methods, based on sliding windows, the number of nearest neighbors, radius and thresholds, and other measures for considering outliers in the data. We will try to give a sampling of the most important algorithms in this section.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec89"/>Inputs and outputs</h3></div></div></div><p>Most <a id="id1120" class="indexterm"/>algorithms take the following parameters as inputs:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Window size <span class="emphasis"><em>w</em></span>, corresponding to the fixed size on which the algorithm looks for outlier patterns</li><li class="listitem" style="list-style-type: disc">Sliding size <span class="emphasis"><em>s</em></span>, corresponds to the number of new instances that will be added to the window, and old ones removed</li><li class="listitem" style="list-style-type: disc">The <a id="id1121" class="indexterm"/>count threshold <span class="emphasis"><em>k</em></span> of instances when using nearest neighbor computation</li><li class="listitem" style="list-style-type: disc">The distance threshold <span class="emphasis"><em>R</em></span> used to define the outlier threshold in distances</li></ul></div><p>Outliers as labels or scores (based on neighbors and distance) are outputs.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec90"/>How does it work?</h3></div></div></div><p>We <a id="id1122" class="indexterm"/>present different variants of distance-based stream outlier algorithms, giving insights into what they do differently or uniquely. The unique elements in each algorithm define what happens when the slide expires, how a new slide is processed, and how outliers are reported.</p><div class="section" title="Exact Storm"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec153"/>Exact Storm</h4></div></div></div><p>Exact Storm<a id="id1123" class="indexterm"/> stores the data in the current window <span class="emphasis"><em>w</em></span> in a well-known <a id="id1124" class="indexterm"/>index structure, so that the range query search or query to find neighbors within the distance <span class="emphasis"><em>R</em></span> for a given point is done efficiently. It also stores <span class="emphasis"><em>k</em></span> preceding and succeeding neighbors of all data points:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Expired Slide</strong></span>: Instances in expired slides are removed from the index structure that affects range queries but are preserved in the preceding list of neighbors.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>New Slide</strong></span>: For each data point in the new slide, range query <span class="emphasis"><em>R</em></span> is executed, results are used to update the preceding and succeeding list for the instance, and the instance is stored in the index structure.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Outlier Reporting</strong></span>: In any window, after the processing of expired and new slide elements is complete, any instance with at least <span class="emphasis"><em>k</em></span> elements from the succeeding list and non-expired preceding list is reported as an outlier.</li></ul></div></div><div class="section" title="Abstract-C"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec154"/>Abstract-C</h4></div></div></div><p>Abstract-C keeps <a id="id1125" class="indexterm"/>the index structure similar to Exact Storm but <a id="id1126" class="indexterm"/>instead of preceding and succeeding lists for every object it just maintains a list of counts of neighbors for the windows the instance is participating in:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Expired Slide</strong></span>: Instances in expired slides are removed from the index structure that affects range queries and the first element from the list of counts is removed corresponding to the last window.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>New Slide</strong></span>: For each data point in the new slide, range query <span class="emphasis"><em>R</em></span> is executed and results are used to update the list count. For existing instances, the count gets updated with new neighbors and instances are added to the index structure.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Outlier Reporting</strong></span>: In any window, after the processing of expired and new slide elements is complete, all instances with a neighbors count less than <span class="emphasis"><em>k</em></span> in the current window are considered outliers.</li></ul></div></div><div class="section" title="Direct Update of Events (DUE)"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec155"/>Direct Update of Events (DUE)</h4></div></div></div><p>DUE keeps <a id="id1127" class="indexterm"/>the index structure for efficient <a id="id1128" class="indexterm"/>range queries exactly like the other algorithms but has a different assumption, that when an expired slide occurs, not every instance is affected in the same way. It maintains two priority queues: the unsafe inlier queue and the outlier list. The unsafe inlier queue has sorted instances based on the increasing order of smallest expiration time of their preceding neighbors. The outlier list has all the outliers in the current window:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Expired Slide</strong></span>: Instances in expired slides are removed from the index structure that affects range queries and the unsafe inlier queue is updated for expired neighbors. Those unsafe inliers which become outliers are removed from the priority queue and moved to the outlier list.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>New Slide</strong></span>: For each data point in the new slide, range query <span class="emphasis"><em>R</em></span> is executed, results are used to update the succeeding neighbors of the point, and only the most recent preceding points are updated for the instance. Based on the updates, the point is added to the unsafe inlier priority queue or removed from the queue and added to the outlier list.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Outlier Reporting</strong></span>: In any window, after the processing of expired and new slide elements is complete, all instances in the outlier list are reported as outliers.</li></ul></div></div><div class="section" title="Micro Clustering based Algorithm (MCOD)"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec156"/>Micro Clustering based Algorithm (MCOD)</h4></div></div></div><p>Micro-clustering <a id="id1129" class="indexterm"/>based outlier <a id="id1130" class="indexterm"/>detection overcomes the computational issues of performing range queries for every data point. The micro-cluster data structure is used instead of range queries in these algorithms. A micro-cluster is centered <a id="id1131" class="indexterm"/>around an instance and has a radius of <span class="emphasis"><em>R</em></span>. All the points belonging to the micro-clusters become inliers. The points that are outside can be outliers or inliers and stored in a separate list. It also has a data structure similar to DUE to keep a priority queue of unsafe inliers:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Expired Slide</strong></span>: Instances in expired slides are removed from both micro-clusters and the data structure with outliers and inliers. The unsafe inlier queue is <a id="id1132" class="indexterm"/>updated for expired neighbors as in the DUE algorithm. Micro-clusters are also updated for non-expired data points.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>New Slide</strong></span>: For each data point in the new slide, the instance either becomes a center of a micro-cluster, or part of a micro-cluster or added to the event queue and the data structure of the outliers. If the point is within the distance <span class="emphasis"><em>R</em></span>, it gets assigned to an existing micro-cluster; otherwise, if there are <span class="emphasis"><em>k</em></span> points within <span class="emphasis"><em>R</em></span>, it becomes the center of the new micro cluster; if not, it goes into the two structures of the event queue and possible outliers.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Outlier Reporting</strong></span>: In any window, after the processing of expired and new slide elements is complete, any instance in the outlier structure with less than <span class="emphasis"><em>k</em></span> neighboring instances is reported as an outlier.</li></ul></div></div><div class="section" title="Approx Storm"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec157"/>Approx Storm</h4></div></div></div><p>Approx <a id="id1133" class="indexterm"/>Storm, as the name suggests, is an approximation of <a id="id1134" class="indexterm"/>Exact Storm. The two approximations are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Reducing the number of data points in the window by adding a factor <span class="emphasis"><em>ρ</em></span> and changing the window to <span class="emphasis"><em>ρW</em></span>.</li><li class="listitem" style="list-style-type: disc">Storing the number instead of the data structure of preceding neighbors by using the fraction of the number of neighbors which are safe inliers in the preceding list to the number in the current window.</li></ul></div><p>The processing of expired and new slides and how outliers are determined based on these steps follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>Expired Slide</strong></span>: Same as Exact Storm—instances in expired slides are removed from the index structure that affects range queries but preserved in the preceding list of neighbors.</li><li class="listitem"><span class="strong"><strong>New Slide</strong></span>: For each data point in the new slide, range query <span class="emphasis"><em>R</em></span> is executed, results are used to compute the fraction discussed previously, and the index structure is updated. The number of safe inliers are constrained to <span class="emphasis"><em>ρW</em></span> by removing random inliers if the size exceeds that value. The assumption is that most of the points in safe inliers are safe. </li><li class="listitem"><span class="strong"><strong>Outlier Reporting</strong></span>: In any window, after the processing of expired and new slide elements has been completed, when an approximation (see <span class="emphasis"><em>References</em></span> [17]) of the number <a id="id1135" class="indexterm"/>of neighbors of an instance based on <a id="id1136" class="indexterm"/>the fraction, window size, and preceding list is a value less than <span class="emphasis"><em>k</em></span>, it is considered as an outlier.</li></ol></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec56"/>Advantages and limitations</h5></div></div></div><p>The advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Exact Storm is <a id="id1137" class="indexterm"/>demanding in storage and CPU for storing <a id="id1138" class="indexterm"/>lists and retrieving neighbors. Also, it introduces delays; even though they are implemented in efficient data structures, range queries can be slow.</li><li class="listitem" style="list-style-type: disc">Abstract-C has a small advantage over Exact Storm, as no time is spent on finding active neighbors for each instance in the window. The storage and time spent is still very much dependent on the window and slide chosen.</li><li class="listitem" style="list-style-type: disc">DUE has some advantage over Exact Storm and Abstract-C as it can efficiently re-evaluate the "inlierness" of points (that is, whether unsafe inliers remain inliers or become outliers) but sorting the structure impacts both CPU and memory.</li><li class="listitem" style="list-style-type: disc">MCOD has distinct advantages in memory and CPU owing to the use of the micro-cluster structure and removing the pair-wise distance computation. Storing the neighborhood information in micro-clusters helps memory too.</li><li class="listitem" style="list-style-type: disc">Approx Storm has an advantage of time over the others as it doesn't process the expired data points over the previous window.</li></ul></div></div></div></div><div class="section" title="Validation and evaluation techniques"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec91"/>Validation and evaluation techniques</h3></div></div></div><p>Validation <a id="id1139" class="indexterm"/>and evaluation of stream-based outliers <a id="id1140" class="indexterm"/>is still an open research area. In many research comparisons, we see various metrics being used, such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Time to evaluate in terms of CPU times per object</li><li class="listitem" style="list-style-type: disc">Number of outliers detected in the streams</li><li class="listitem" style="list-style-type: disc">Number of outliers that correlate to existing labels, TP/Precision/Recall/Area under PRC curve, and so on</li></ul></div><p>By varying parameters such as window-size, neighbors within radius, and so on, we determine the sensitivity to the performance metrics mentioned previously and determine the robustness.</p></div></div></div>
<div class="section" title="Case study in stream learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec48"/>Case study in stream learning</h1></div></div></div><p>The <a id="id1141" class="indexterm"/>case study in this chapter consists of several experiments that illustrate different methods of stream-based machine learning. A well-studied dataset was chosen as the stream data source and supervised tree based methods such as Naïve Bayes, Hoeffding Tree, as well as ensemble methods, were used. Among unsupervised methods, clustering algorithms used include k-Means, DBSCAN, CluStream, and CluTree. Outlier detection techniques include MCOD and SimpleCOD, among others. We also show results from classification experiments that demonstrate handling concept drift. The ADWIN algorithm for calculating statistics in a sliding window, as described earlier in this chapter, is employed in several algorithms used in the classification experiments.</p><div class="section" title="Tools and software"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec78"/>Tools and software</h2></div></div></div><p>One of the <a id="id1142" class="indexterm"/>most popular and arguably the most comprehensive <a id="id1143" class="indexterm"/>Java-based frameworks for data stream mining is the open source <span class="strong"><strong>Massive Online Analysis</strong></span> (<span class="strong"><strong>MOA</strong></span>) software created by the <a id="id1144" class="indexterm"/>University of Waikato. The framework <a id="id1145" class="indexterm"/>is a collection of stream classification, clustering, and outlier detection algorithms and has support for change detection and concept drift. It also includes data generators and several evaluation tools. The framework can be extended with new stream data generators, algorithms, and evaluators. In this case study, we employ several stream data learning methods using a file-based data stream.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>Product homepage: <a class="ulink" href="http://moa.cms.waikato.ac.nz/">http://moa.cms.waikato.ac.nz/</a>
</p><p>GitHub: <a class="ulink" href="https://github.com/Waikato/moa">https://github.com/Waikato/moa</a>
</p></div></div><p>As shown in the series of screenshots from the MOA tool shown in <span class="emphasis"><em>Figure 4</em></span> and <span class="emphasis"><em>Figure 5</em></span>, the top-level menu lets you choose the type of learning to be done. For the classification experiments, for example, configuration of the tools consists of selecting the task to run (selected to be prequential evaluation here), and then configuring which learner and evaluator we want to use, and finally, the source of the data stream. A window width parameter shown in the <span class="strong"><strong>Configure Task</strong></span> dialog can affect the accuracy of the model chosen, as we will see in the experiment results. Other than choosing different values for the window width, all base learner parameters were left as default values. Once the task is configured it is run by clicking the <span class="strong"><strong>Run</strong></span> button:</p><div class="mediaobject"><img src="graphics/B05137_05_339.jpg" alt="Tools and software"/></div><div class="mediaobject"><img src="graphics/B05137_05_340.jpg" alt="Tools and software"/><div class="caption"><p>Figure 4. MOA graphical interface for configuring prequential evaluation for classification which includes setting the window width</p></div></div><div class="mediaobject"><img src="graphics/B05137_05_341.jpg" alt="Tools and software"/></div><div class="mediaobject"><img src="graphics/B05137_05_342.jpg" alt="Tools and software"/><div class="caption"><p>Figure 5. MOA graphical interface for prequential classification task. Within the Configure task, you must choose a learner, locate the data stream (details not shown), and select an evaluator </p></div></div><p>After the task has completed running, model evaluation results can be exported to a CSV file.</p></div><div class="section" title="Business problem"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec79"/>Business problem</h2></div></div></div><p>The <a id="id1146" class="indexterm"/>problem for this case study is to continuously learn from a stream of electricity market data and predict the direction of movement of the market price. We compare the accuracy and average cost of different classification methods including concept drift as well as the performance of clustering and outlier detection. </p></div><div class="section" title="Machine learning mapping"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec80"/>Machine learning mapping</h2></div></div></div><p>The <a id="id1147" class="indexterm"/>dataset used in this case study can be used to illustrate classical batch-based supervised and unsupervised learning techniques. However, here we treat it as a stream-based data source to show how we can employ the techniques described in this chapter to perform classification, clustering, and outlier detection tasks using the MOA framework. Within this context, we demonstrate how incremental learning can be achieved under assumptions of a stationary as well as an evolving data stream exhibiting concept drift.</p></div><div class="section" title="Data collection"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec81"/>Data collection</h2></div></div></div><p>The <a id="id1148" class="indexterm"/>dataset is known as the Electricity <a id="id1149" class="indexterm"/>or ELEC dataset, which was collected by the New South Wales Electricity Market. The prices in this market are variable, and are adjusted every 5 minutes based on supply and demand. This dataset consists of 45,312 such data points obtained every half-hour between May 1996 and December 1998. The target is an indication of the movement of the price, whether up or down, relative to the 24-hour moving average.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>The data file is a publicly available file in the ARRF format at <a class="ulink" href="http://downloads.sourceforge.net/project/moa-datastream/Datasets/Classification/elecNormNew.arff.zip?r=http%3A%2F%2Fmoa.cms.waikato.ac.nz%2Fdatasets%2F&amp;ts=1483128450&amp;use_mirror=cytranet">http://downloads.sourceforge.net/project/moa-datastream/Datasets/Classification/elecNormNew.arff.zip?r=http%3A%2F%2Fmoa.cms.waikato.ac.nz%2Fdatasets%2F&amp;ts=1483128450&amp;use_mirror=cytranet</a>.</p></div></div></div><div class="section" title="Data sampling and transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec82"/>Data sampling and transformation</h2></div></div></div><p>In the <a id="id1150" class="indexterm"/>experiments conducted here, no data <a id="id1151" class="indexterm"/>sampling is done; each example in the dataset is processed individually and no example is excluded. All numeric data elements have been normalized to a value between 0 and 1.</p><div class="section" title="Feature analysis and dimensionality reduction"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec92"/>Feature analysis and dimensionality reduction</h3></div></div></div><p>The ELEC <a id="id1152" class="indexterm"/>dataset has 45,312 records <a id="id1153" class="indexterm"/>with nine features, including the target class. The features class and day are nominal (categorical), all others are numeric (continuous). The features are listed in <span class="emphasis"><em>Table 1</em></span> and <span class="emphasis"><em>Table 2</em></span> and give descriptive statistics for the ELEC dataset:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Name</p>
</th><th style="text-align: left" valign="bottom">
<p>Data Type</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>class</p>
</td><td style="text-align: left" valign="top">
<p>nominal</p>
</td><td style="text-align: left" valign="top">
<p>UP, DOWN—direction of price movement relative to 24-hour moving average</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>date</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top">
<p>Date price recorded</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>day</p>
</td><td style="text-align: left" valign="top">
<p>nominal</p>
</td><td style="text-align: left" valign="top">
<p>Day of the week (1-7)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>period</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>nswprice</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top">
<p>Electricity price in NSW</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>nswdemand</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top">
<p>Electricity demand in NSW</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>vicprice</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top">
<p>Electricity price in Victoria</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>vicdemand</p>
</td><td style="text-align: left" valign="top">
<p>continuous</p>
</td><td style="text-align: left" valign="top">
<p>Electricity demand in Victoria</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>transfer</p>
</td><td style="text-align: left" valign="top">
<p>integer</p>
</td><td style="text-align: left" valign="top"> </td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 1. ELEC dataset features </em></span></p></blockquote></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>count</p>
</th><th style="text-align: left" valign="bottom">
<p>mean</p>
</th><th style="text-align: left" valign="bottom">
<p>std</p>
</th><th style="text-align: left" valign="bottom">
<p>25%</p>
</th><th style="text-align: left" valign="bottom">
<p>50%</p>
</th><th style="text-align: left" valign="bottom">
<p>75%</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>date</p>
</td><td style="text-align: left" valign="top">
<p>45312</p>
</td><td style="text-align: left" valign="top">
<p>0.49908</p>
</td><td style="text-align: left" valign="top">
<p>0.340308</p>
</td><td style="text-align: left" valign="top">
<p>0.031934</p>
</td><td style="text-align: left" valign="top">
<p>0.456329</p>
</td><td style="text-align: left" valign="top">
<p>0.880547</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>period</p>
</td><td style="text-align: left" valign="top">
<p>45312</p>
</td><td style="text-align: left" valign="top">
<p>0.5</p>
</td><td style="text-align: left" valign="top">
<p>0.294756</p>
</td><td style="text-align: left" valign="top">
<p>0.25</p>
</td><td style="text-align: left" valign="top">
<p>0.5</p>
</td><td style="text-align: left" valign="top">
<p>0.75</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>nswprice</p>
</td><td style="text-align: left" valign="top">
<p>45312</p>
</td><td style="text-align: left" valign="top">
<p>0.057868</p>
</td><td style="text-align: left" valign="top">
<p>0.039991</p>
</td><td style="text-align: left" valign="top">
<p>0.035127</p>
</td><td style="text-align: left" valign="top">
<p>0.048652</p>
</td><td style="text-align: left" valign="top">
<p>0.074336</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>nswdemand</p>
</td><td style="text-align: left" valign="top">
<p>45312</p>
</td><td style="text-align: left" valign="top">
<p>0.425418</p>
</td><td style="text-align: left" valign="top">
<p>0.163323</p>
</td><td style="text-align: left" valign="top">
<p>0.309134</p>
</td><td style="text-align: left" valign="top">
<p>0.443693</p>
</td><td style="text-align: left" valign="top">
<p>0.536001</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>vicprice</p>
</td><td style="text-align: left" valign="top">
<p>45312</p>
</td><td style="text-align: left" valign="top">
<p>0.003467</p>
</td><td style="text-align: left" valign="top">
<p>0.010213</p>
</td><td style="text-align: left" valign="top">
<p>0.002277</p>
</td><td style="text-align: left" valign="top">
<p>0.003467</p>
</td><td style="text-align: left" valign="top">
<p>0.003467</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>vicdemand</p>
</td><td style="text-align: left" valign="top">
<p>45312</p>
</td><td style="text-align: left" valign="top">
<p>0.422915</p>
</td><td style="text-align: left" valign="top">
<p>0.120965</p>
</td><td style="text-align: left" valign="top">
<p>0.372346</p>
</td><td style="text-align: left" valign="top">
<p>0.422915</p>
</td><td style="text-align: left" valign="top">
<p>0.469252</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>transfer</p>
</td><td style="text-align: left" valign="top">
<p>45312</p>
</td><td style="text-align: left" valign="top">
<p>0.500526</p>
</td><td style="text-align: left" valign="top">
<p>0.153373</p>
</td><td style="text-align: left" valign="top">
<p>0.414912</p>
</td><td style="text-align: left" valign="top">
<p>0.414912</p>
</td><td style="text-align: left" valign="top">
<p>0.605702</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>Table 2. Descriptive statistics of ELEC dataset features</em></span></p></blockquote></div><p>The feature reduction step is omitted here as it is in most stream-based learning.</p></div></div><div class="section" title="Models, results, and evaluation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec83"/>Models, results, and evaluation</h2></div></div></div><p>The<a id="id1154" class="indexterm"/> experiments are divided into classification, concept <a id="id1155" class="indexterm"/>drift, clustering, and outlier <a id="id1156" class="indexterm"/>detection. Details of the learning process for each set of experiments and the results of the experiments are given here.</p><div class="section" title="Supervised learning experiments"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec93"/>Supervised learning experiments</h3></div></div></div><p>For <a id="id1157" class="indexterm"/>this set of experiments, a choice of linear, non-linear, and ensemble learners were chosen in order to illustrate the <a id="id1158" class="indexterm"/>behavior of a variety of classifiers. <span class="strong"><strong>Stochastic Gradient Descent</strong></span> (<span class="strong"><strong>SGD</strong></span>), which uses a linear SVM, and Naïve Bayes are the linear classifiers, while Lazy k-NN is the non-linear classifier.  For ensemble learning, we use two meta-learners, <span class="strong"><strong>Leveraging Bagging</strong></span> (<span class="strong"><strong>LB</strong></span>) and OxaBag, with different <a id="id1159" class="indexterm"/>linear and non-linear base learners such as SGD, Naïve Bayes, and Hoeffding Trees. The algorithm used in OxaBag is described in the section on ensemble algorithms. In LB, the weight factor used for resampling is variable (the default value of 6 is used here) whereas the weight in OxaBag is fixed at 1.</p><p>Prequential evaluation is chosen for all the classification methods, so each example is first tested against the prediction with the existing model, and then used for training the model. This requires the selection of a window width, and the performance of the various models for different values of the window width are listed in <span class="emphasis"><em>Table 3</em></span>. Widths of 100, 500, 1,000, and 5,000 elements were used:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>Window width</p>
</th><th style="text-align: left" valign="bottom">
<p>Evaluation time (CPU seconds)</p>
</th><th style="text-align: left" valign="bottom">
<p>Model cost (RAM-Hours)</p>
</th><th style="text-align: left" valign="bottom">
<p>Classifications correct (percent)</p>
</th><th style="text-align: left" valign="bottom">
<p>Kappa Statistic (percent)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>SGD</p>
</td><td style="text-align: left" valign="top">
<p>100</p>
</td><td style="text-align: left" valign="top">
<p>0.5781</p>
</td><td style="text-align: left" valign="top">
<p>3.76E-10</p>
</td><td style="text-align: left" valign="top">
<p>67</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>SGD</p>
</td><td style="text-align: left" valign="top">
<p>500</p>
</td><td style="text-align: left" valign="top">
<p>0.5781</p>
</td><td style="text-align: left" valign="top">
<p>3.76E-10</p>
</td><td style="text-align: left" valign="top">
<p>55.6</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>SGD</p>
</td><td style="text-align: left" valign="top">
<p>1000</p>
</td><td style="text-align: left" valign="top">
<p>0.5469</p>
</td><td style="text-align: left" valign="top">
<p>3.55E-10</p>
</td><td style="text-align: left" valign="top">
<p>53.3</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>SGD</p>
</td><td style="text-align: left" valign="top">
<p>5000</p>
</td><td style="text-align: left" valign="top">
<p>0.5469</p>
</td><td style="text-align: left" valign="top">
<p>3.55E-10</p>
</td><td style="text-align: left" valign="top">
<p>53.78</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>NaiveBayes</p>
</td><td style="text-align: left" valign="top">
<p>100</p>
</td><td style="text-align: left" valign="top">
<p>0.7656</p>
</td><td style="text-align: left" valign="top">
<p>8.78E-10</p>
</td><td style="text-align: left" valign="top">
<p>86</p>
</td><td style="text-align: left" valign="top">
<p>65.7030</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>NaiveBayes</p>
</td><td style="text-align: left" valign="top">
<p>500</p>
</td><td style="text-align: left" valign="top">
<p>0.6094</p>
</td><td style="text-align: left" valign="top">
<p>8.00E-10</p>
</td><td style="text-align: left" valign="top">
<p>82.2</p>
</td><td style="text-align: left" valign="top">
<p>62.6778</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>NaiveBayes</p>
</td><td style="text-align: left" valign="top">
<p>1000</p>
</td><td style="text-align: left" valign="top">
<p>0.6719</p>
</td><td style="text-align: left" valign="top">
<p>7.77E-10</p>
</td><td style="text-align: left" valign="top">
<p>75.3</p>
</td><td style="text-align: left" valign="top">
<p>48.8583</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>NaiveBayes</p>
</td><td style="text-align: left" valign="top">
<p>5000</p>
</td><td style="text-align: left" valign="top">
<p>0.6406</p>
</td><td style="text-align: left" valign="top">
<p>7.35E-10</p>
</td><td style="text-align: left" valign="top">
<p>77.84</p>
</td><td style="text-align: left" valign="top">
<p>54.1966</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>kNN</p>
</td><td style="text-align: left" valign="top">
<p>100</p>
</td><td style="text-align: left" valign="top">
<p>34.6406</p>
</td><td style="text-align: left" valign="top">
<p>4.66E-06</p>
</td><td style="text-align: left" valign="top">
<p>74</p>
</td><td style="text-align: left" valign="top">
<p>36.3057</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>kNN</p>
</td><td style="text-align: left" valign="top">
<p>500</p>
</td><td style="text-align: left" valign="top">
<p>34.5469</p>
</td><td style="text-align: left" valign="top">
<p>4.65E-06</p>
</td><td style="text-align: left" valign="top">
<p>79.8</p>
</td><td style="text-align: left" valign="top">
<p>59.1424</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>kNN</p>
</td><td style="text-align: left" valign="top">
<p>1000</p>
</td><td style="text-align: left" valign="top">
<p>35.8750</p>
</td><td style="text-align: left" valign="top">
<p>4.83E-06</p>
</td><td style="text-align: left" valign="top">
<p>82.5</p>
</td><td style="text-align: left" valign="top">
<p>64.8049</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>kNN</p>
</td><td style="text-align: left" valign="top">
<p>5000</p>
</td><td style="text-align: left" valign="top">
<p>35.0312</p>
</td><td style="text-align: left" valign="top">
<p>4.71E-06</p>
</td><td style="text-align: left" valign="top">
<p>80.32</p>
</td><td style="text-align: left" valign="top">
<p>60.4594</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-kNN</p>
</td><td style="text-align: left" valign="top">
<p>100</p>
</td><td style="text-align: left" valign="top">
<p>637.8125</p>
</td><td style="text-align: left" valign="top">
<p>2.88E-04</p>
</td><td style="text-align: left" valign="top">
<p>74</p>
</td><td style="text-align: left" valign="top">
<p>36.3057</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-kNN</p>
</td><td style="text-align: left" valign="top">
<p>500</p>
</td><td style="text-align: left" valign="top">
<p>638.9687</p>
</td><td style="text-align: left" valign="top">
<p>2.89E-04</p>
</td><td style="text-align: left" valign="top">
<p>79.8</p>
</td><td style="text-align: left" valign="top">
<p>59.1424</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-kNN</p>
</td><td style="text-align: left" valign="top">
<p>1000</p>
</td><td style="text-align: left" valign="top">
<p>655.8125</p>
</td><td style="text-align: left" valign="top">
<p>2.96E-04</p>
</td><td style="text-align: left" valign="top">
<p>82.4</p>
</td><td style="text-align: left" valign="top">
<p>64.5802</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-kNN</p>
</td><td style="text-align: left" valign="top">
<p>5000</p>
</td><td style="text-align: left" valign="top">
<p>667.6094</p>
</td><td style="text-align: left" valign="top">
<p>3.02E-04</p>
</td><td style="text-align: left" valign="top">
<p>80.66</p>
</td><td style="text-align: left" valign="top">
<p>61.0965</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-HoeffdingTree</p>
</td><td style="text-align: left" valign="top">
<p>100</p>
</td><td style="text-align: left" valign="top">
<p>13.6875</p>
</td><td style="text-align: left" valign="top">
<p>2.98E-06</p>
</td><td style="text-align: left" valign="top">
<p>91</p>
</td><td style="text-align: left" valign="top">
<p>79.1667</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-HoeffdingTree</p>
</td><td style="text-align: left" valign="top">
<p>500</p>
</td><td style="text-align: left" valign="top">
<p>13.5781</p>
</td><td style="text-align: left" valign="top">
<p>2.96E-06</p>
</td><td style="text-align: left" valign="top">
<p>93</p>
</td><td style="text-align: left" valign="top">
<p>85.8925</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-HoeffdingTree</p>
</td><td style="text-align: left" valign="top">
<p>1000</p>
</td><td style="text-align: left" valign="top">
<p>12.5625</p>
</td><td style="text-align: left" valign="top">
<p>2.74E-06</p>
</td><td style="text-align: left" valign="top">
<p>92.1</p>
</td><td style="text-align: left" valign="top">
<p>84.1665</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-HoeffdingTree</p>
</td><td style="text-align: left" valign="top">
<p>5000</p>
</td><td style="text-align: left" valign="top">
<p>12.7656</p>
</td><td style="text-align: left" valign="top">
<p>2.78E-06</p>
</td><td style="text-align: left" valign="top">
<p>90.74</p>
</td><td style="text-align: left" valign="top">
<p>81.3184</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>Table 3. Classifier performance for different window sizes</em></span></p></blockquote></div><p>For the algorithms in <span class="emphasis"><em>Table 4</em></span>, the performance was the same for each value of the window width used:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>Evaluation time (CPU seconds)</p>
</th><th style="text-align: left" valign="bottom">
<p>Model cost (RAM-Hours)</p>
</th><th style="text-align: left" valign="bottom">
<p>Classifications correct (percent)</p>
</th><th style="text-align: left" valign="bottom">
<p>Kappa Statistic (percent)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>HoeffdingTree</p>
</td><td style="text-align: left" valign="top">
<p>1.1562</p>
</td><td style="text-align: left" valign="top">
<p>3.85E-08</p>
</td><td style="text-align: left" valign="top">
<p>79.1953</p>
</td><td style="text-align: left" valign="top">
<p>57.2266</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>HoeffdingAdaptiveTree</p>
</td><td style="text-align: left" valign="top">
<p>2.0469</p>
</td><td style="text-align: left" valign="top">
<p>2.84E-09</p>
</td><td style="text-align: left" valign="top">
<p>83.3863</p>
</td><td style="text-align: left" valign="top">
<p>65.5569</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>OzaBag-NaiveBayes</p>
</td><td style="text-align: left" valign="top">
<p>2.01562</p>
</td><td style="text-align: left" valign="top">
<p>1.57E-08</p>
</td><td style="text-align: left" valign="top">
<p>73.4794</p>
</td><td style="text-align: left" valign="top">
<p>42.7636</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>OzaBagAdwin-HoeffdingTree</p>
</td><td style="text-align: left" valign="top">
<p>5.7812</p>
</td><td style="text-align: left" valign="top">
<p>2.26E-07</p>
</td><td style="text-align: left" valign="top">
<p>84.3485</p>
</td><td style="text-align: left" valign="top">
<p>67.5221</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-SGD</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>1.67E-08</p>
</td><td style="text-align: left" valign="top">
<p>57.6977</p>
</td><td style="text-align: left" valign="top">
<p>3.0887</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-NaiveBayes</p>
</td><td style="text-align: left" valign="top">
<p>3.5937</p>
</td><td style="text-align: left" valign="top">
<p>3.99E-08</p>
</td><td style="text-align: left" valign="top">
<p>78.8753</p>
</td><td style="text-align: left" valign="top">
<p>55.7639</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>Table 4. Performance of classifiers (same for all window widths used)</em></span></p></blockquote></div><div class="section" title="Concept drift experiments"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec158"/>Concept drift experiments</h4></div></div></div><p>In <a id="id1160" class="indexterm"/>this experiment, we continue using EvaluatePrequential when configuring the classification task. This time we select the <code class="literal">DriftDetectionMethodClassifier</code> as the learner and DDM as the drift detection method. This demonstrates adapting to an evolving data stream. Base learners used and the results obtained are shown in <span class="emphasis"><em>Table 5</em></span>:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>Evaluation time (CPU seconds)</p>
</th><th style="text-align: left" valign="bottom">
<p>Model cost (RAM-Hours)</p>
</th><th style="text-align: left" valign="bottom">
<p>Classifications correct (percent)</p>
</th><th style="text-align: left" valign="bottom">
<p>Kappa Statistic (percent)</p>
</th><th style="text-align: left" valign="bottom">
<p>Change detected</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>SGD</p>
</td><td style="text-align: left" valign="top">
<p>0.307368829</p>
</td><td style="text-align: left" valign="top">
<p>1.61E-09</p>
</td><td style="text-align: left" valign="top">
<p>53.3</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>132</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Naïve-Bayes</p>
</td><td style="text-align: left" valign="top">
<p>0.298290727</p>
</td><td style="text-align: left" valign="top">
<p>1.58E-09</p>
</td><td style="text-align: left" valign="top">
<p>86.6</p>
</td><td style="text-align: left" valign="top">
<p>73.03986</p>
</td><td style="text-align: left" valign="top">
<p>143</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Lazy-kNN</p>
</td><td style="text-align: left" valign="top">
<p>10.34161893</p>
</td><td style="text-align: left" valign="top">
<p>1.74E-06</p>
</td><td style="text-align: left" valign="top">
<p>87.4</p>
</td><td style="text-align: left" valign="top">
<p>74.8498</p>
</td><td style="text-align: left" valign="top">
<p>12</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>HoeffdingTree</p>
</td><td style="text-align: left" valign="top">
<p>0.472981754</p>
</td><td style="text-align: left" valign="top">
<p>5.49E-09</p>
</td><td style="text-align: left" valign="top">
<p>86.2</p>
</td><td style="text-align: left" valign="top">
<p>72.19816</p>
</td><td style="text-align: left" valign="top">
<p>169</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>HoeffdingAdaptiveTree</p>
</td><td style="text-align: left" valign="top">
<p>0.598665043</p>
</td><td style="text-align: left" valign="top">
<p>7.19E-09</p>
</td><td style="text-align: left" valign="top">
<p>84</p>
</td><td style="text-align: left" valign="top">
<p>67.80878</p>
</td><td style="text-align: left" valign="top">
<p>155</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-SGD</p>
</td><td style="text-align: left" valign="top">
<p>0.912737325</p>
</td><td style="text-align: left" valign="top">
<p>2.33E-08</p>
</td><td style="text-align: left" valign="top">
<p>53.3</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>132</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-NaiveBayes</p>
</td><td style="text-align: left" valign="top">
<p>1.990137758</p>
</td><td style="text-align: left" valign="top">
<p>3.61E-08</p>
</td><td style="text-align: left" valign="top">
<p>85.7</p>
</td><td style="text-align: left" valign="top">
<p>71.24056</p>
</td><td style="text-align: left" valign="top">
<p>205</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>OzaBag-NaiveBayes</p>
</td><td style="text-align: left" valign="top">
<p>1.342189725</p>
</td><td style="text-align: left" valign="top">
<p>2.29E-08</p>
</td><td style="text-align: left" valign="top">
<p>77.4</p>
</td><td style="text-align: left" valign="top">
<p>54.017</p>
</td><td style="text-align: left" valign="top">
<p>211</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-kNN</p>
</td><td style="text-align: left" valign="top">
<p>173.3624715</p>
</td><td style="text-align: left" valign="top">
<p>1.14E-04</p>
</td><td style="text-align: left" valign="top">
<p>87.5</p>
</td><td style="text-align: left" valign="top">
<p>75.03296</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LB-HoeffdingTree</p>
</td><td style="text-align: left" valign="top">
<p>5.660440101</p>
</td><td style="text-align: left" valign="top">
<p>1.61E-06</p>
</td><td style="text-align: left" valign="top">
<p>91.3</p>
</td><td style="text-align: left" valign="top">
<p>82.56317</p>
</td><td style="text-align: left" valign="top">
<p>59</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>OzaBag-HoeffdingTree</p>
</td><td style="text-align: left" valign="top">
<p>4.306455545</p>
</td><td style="text-align: left" valign="top">
<p>3.48E-07</p>
</td><td style="text-align: left" valign="top">
<p>85.4</p>
</td><td style="text-align: left" valign="top">
<p>70.60209</p>
</td><td style="text-align: left" valign="top">
<p>125</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 5. Performance of classifiers with concept drift detection </em></span></p></blockquote></div></div></div><div class="section" title="Clustering experiments"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec95"/>Clustering experiments</h3></div></div></div><p>Almost <a id="id1161" class="indexterm"/>all of the clustering algorithms implemented in the MOA tool were used in this experiment. Both extrinsic and intrinsic evaluation results were collected and are tabulated in <span class="emphasis"><em>Table 6</em></span>. CMM, homogeneity, and completeness were defined earlier in this chapter. We have encountered Purity and Silhouette coefficients before, from the discussion in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>. SSQ is the sum of squared distances of instances from their respective cluster centers; the lower the value of SSQ, the better. The use of micro-clustering is indicated by <span class="emphasis"><em>m = 1</em></span> in the table. How often the macro-clusters are calculated is determined by the selected time horizon <span class="emphasis"><em>h</em></span>, in instances:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>CMM</p>
</th><th style="text-align: left" valign="bottom">
<p>Homogeneity</p>
</th><th style="text-align: left" valign="bottom">
<p>Completeness</p>
</th><th style="text-align: left" valign="bottom">
<p>Purity</p>
</th><th style="text-align: left" valign="bottom">
<p>SSQ</p>
</th><th style="text-align: left" valign="bottom">
<p>Silhouette Coefficient</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Clustream With k-Means (h = 5000; k = 2; m = 1)</p>
</td><td style="text-align: left" valign="top">
<p>0.7168</p>
</td><td style="text-align: left" valign="top">
<p>-1.0000</p>
</td><td style="text-align: left" valign="top">
<p>0.1737</p>
</td><td style="text-align: left" valign="top">
<p>0.9504</p>
</td><td style="text-align: left" valign="top">
<p>9.1975</p>
</td><td style="text-align: left" valign="top">
<p>0.5687</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Clustream With k-Means </p>
<p>(h = 1000; k = 5)</p>
</td><td style="text-align: left" valign="top">
<p>0.5391</p>
</td><td style="text-align: left" valign="top">
<p>-1.0000</p>
</td><td style="text-align: left" valign="top">
<p>0.8377</p>
</td><td style="text-align: left" valign="top">
<p>0.7238</p>
</td><td style="text-align: left" valign="top">
<p>283.6543</p>
</td><td style="text-align: left" valign="top">
<p>0.8264</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Clustream (h = 1000; m = 1)</p>
</td><td style="text-align: left" valign="top">
<p>0.6241</p>
</td><td style="text-align: left" valign="top">
<p>-1.0000</p>
</td><td style="text-align: left" valign="top">
<p>0.4363</p>
</td><td style="text-align: left" valign="top">
<p>0.9932</p>
</td><td style="text-align: left" valign="top">
<p>7.2734</p>
</td><td style="text-align: left" valign="top">
<p>0.4936</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Denstream With DBSCAN (h = 1000)</p>
</td><td style="text-align: left" valign="top">
<p>0.4455</p>
</td><td style="text-align: left" valign="top">
<p>-1.0000</p>
</td><td style="text-align: left" valign="top">
<p>0.7586</p>
</td><td style="text-align: left" valign="top">
<p>0.9167</p>
</td><td style="text-align: left" valign="top">
<p>428.7604</p>
</td><td style="text-align: left" valign="top">
<p>0.6682</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ClusTree (h = 5000; m = 1)</p>
</td><td style="text-align: left" valign="top">
<p>0.7984</p>
</td><td style="text-align: left" valign="top">
<p>0.4874</p>
</td><td style="text-align: left" valign="top">
<p>-0.4815</p>
</td><td style="text-align: left" valign="top">
<p>0.9489</p>
</td><td style="text-align: left" valign="top">
<p>11.7789</p>
</td><td style="text-align: left" valign="top">
<p>0.6879</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ClusTree (h = 1000; m = 1)</p>
</td><td style="text-align: left" valign="top">
<p>0.7090</p>
</td><td style="text-align: left" valign="top">
<p>-1.0000</p>
</td><td style="text-align: left" valign="top">
<p>0.3979</p>
</td><td style="text-align: left" valign="top">
<p>0.9072</p>
</td><td style="text-align: left" valign="top">
<p>13.4190</p>
</td><td style="text-align: left" valign="top">
<p>0.5385</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>AbstractC</p>
</td><td style="text-align: left" valign="top">
<p>1.0000</p>
</td><td style="text-align: left" valign="top">
<p>1.0000</p>
</td><td style="text-align: left" valign="top">
<p>-8.1354</p>
</td><td style="text-align: left" valign="top">
<p>1.0000</p>
</td><td style="text-align: left" valign="top">
<p>0.0000</p>
</td><td style="text-align: left" valign="top">
<p>0.0000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>MCOD (w = 1000)</p>
</td><td style="text-align: left" valign="top">
<p>1.0000</p>
</td><td style="text-align: left" valign="top">
<p>1.0000</p>
</td><td style="text-align: left" valign="top">
<p>-8.1354</p>
</td><td style="text-align: left" valign="top">
<p>1.0000</p>
</td><td style="text-align: left" valign="top">
<p>0.0000</p>
</td><td style="text-align: left" valign="top">
<p>0.0000</p>
</td></tr></tbody></table></div></div><div class="section" title="Outlier detection experiments"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec96"/>Outlier detection experiments</h3></div></div></div><p>In the<a id="id1162" class="indexterm"/> final set of experiments, five outlier detection algorithms were used to process the ELEC dataset. Results are given in <span class="emphasis"><em>Table 7</em></span>:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>Nodes always inlier</p>
</th><th style="text-align: left" valign="bottom">
<p>Nodes always outlier</p>
</th><th style="text-align: left" valign="bottom">
<p>Nodes both inlier and outlier</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>MCOD</p>
</td><td style="text-align: left" valign="top">
<p>42449 (93.7%)</p>
</td><td style="text-align: left" valign="top">
<p>302 (0.7%)</p>
</td><td style="text-align: left" valign="top">
<p>2561 (5.7%)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ApproxSTORM</p>
</td><td style="text-align: left" valign="top">
<p>41080 (90.7%)</p>
</td><td style="text-align: left" valign="top">
<p>358 (0.8%)</p>
</td><td style="text-align: left" valign="top">
<p>3874 (8.5%)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>SimpleCOD</p>
</td><td style="text-align: left" valign="top">
<p>42449 (93.7%)</p>
</td><td style="text-align: left" valign="top">
<p>302 (0.7%)</p>
</td><td style="text-align: left" valign="top">
<p>2561 (5.7%)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>AbstractC</p>
</td><td style="text-align: left" valign="top">
<p>42449 (93.7%)</p>
</td><td style="text-align: left" valign="top">
<p>302 (0.7%)</p>
</td><td style="text-align: left" valign="top">
<p>2561 (5.7%)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ExactSTORM</p>
</td><td style="text-align: left" valign="top">
<p>42449 (93.7%)</p>
</td><td style="text-align: left" valign="top">
<p>302 (0.7%)</p>
</td><td style="text-align: left" valign="top">
<p>2561 (5.7%)</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>Table 7. Evaluation of outlier detection</em></span></p></blockquote></div><p>The following plots (<span class="emphasis"><em>Figure 6</em></span>) show results for three pairs of features after running algorithm Abstract-C on the entire dataset. In each of the plots, it is easy to see the outliers identified by the circles surrounding the data points. Although it is difficult to visualize the outliers spatially in multiple dimensions simultaneously, the set bi-variate plots give some indication of the result of outlier detection methods applied in a stream-based setting:</p><div class="mediaobject"><img src="graphics/B05137_05_343.jpg" alt="Outlier detection experiments"/></div><div class="mediaobject"><img src="graphics/B05137_05_344.jpg" alt="Outlier detection experiments"/></div><div class="mediaobject"><img src="graphics/B05137_05_345.jpg" alt="Outlier detection experiments"/><div class="caption"><p>Figure 6. Outlier detection using Abstract-C, for three pairs of features, after processing all 45,300 instances</p></div></div><p>The image in <span class="emphasis"><em>Figure 7</em></span> shows a screenshot from MOA when two algorithms, <code class="literal">Angiulli.ExactSTORM</code> and <code class="literal">Angiulli.ApproxSTORM</code>, were run simultaneously; a bivariate <a id="id1163" class="indexterm"/>scatter-plot for each algorithm is shown side-by-side, accompanied by a comparison of the per-object processing time:</p><div class="mediaobject"><img src="graphics/B05137_05_346.jpg" alt="Outlier detection experiments"/><div class="caption"><p>Figure 7. Visualization of outlier detection in MOA</p></div></div></div></div><div class="section" title="Analysis of stream learning results"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec85"/>Analysis of stream learning results</h2></div></div></div><p>Based <a id="id1164" class="indexterm"/>on the evaluation of the learned models from the classification, clustering, and outlier detection experiments, analysis reveals several interesting observations.</p><p>Classification experiments:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Performance improves from linear to non-linear algorithms in quite a significant way as shown in Table 3. Linear SGD has the best performance using an accuracy metric of 67%, while KNN and Hoeffding Tree show 82.4 to 93%. This <a id="id1165" class="indexterm"/>clearly shows that the problem is non-linear and using a non-linear algorithm will give better performance.</li><li class="listitem" style="list-style-type: disc">K-NNs give good performance but come at the cost of evaluation time, as shown in Table 3. Evaluation time as well as memory are significantly higher—about two orders of magnitude—than the linear methods. When the model has to perform in tighter evaluation cycles, extreme caution in choosing algorithms such as KNNs must be observed.</li><li class="listitem" style="list-style-type: disc">Hoeffding Trees gives the best classification rate and Kappa statistic. The evaluation time is also not as high as KNNs but is still in the order of seconds, which may or may not be acceptable in many real-time stream-based applications.</li><li class="listitem" style="list-style-type: disc">The evaluation time of Naive Bayes is the lowest—though not much different from SGD—and with the right choice of window width can give performance second only to Hoeffding Trees. For example, at width 100, we have a classification rate of 86 with Naïve Bayes, which is next best to 93 of Hoeffding Trees but compared to over 13 seconds, Naïve Bayes takes only 0.76 seconds, as shown in <span class="emphasis"><em>Table 3</em></span>.</li><li class="listitem" style="list-style-type: disc">Keeping the window width constant, there is a clear pattern of improvement from linear (SGD, Naïve Bayes) to non-linear (Hoeffding Trees) to ensemble based (OzaBag, Adwin, Hoeffding Tree) shown in Table 4. This clearly shows that in theory, the choice of ensembles can help reduce the errors but comes at the cost of foregoing interpretability in the models.</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Table 5</em></span>, when compared to <span class="emphasis"><em>Table 3</em></span> and <span class="emphasis"><em>Table 4</em></span>, shows why having drift protection and learning with automated drift detection increases robustness. Ensemble-based learning of OzaBag-NaiveBayes, OzaBag-HoeffdingTrees, and OzaBag-HoeffdingAdaptiveTree all show improvements over the non- drift protected runs as an example.</li></ul></div><p>Clustering experiments:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">From the first two models in <span class="emphasis"><em>Table 6</em></span>, we see that k-Means, with a horizon of 5,000 instances and <span class="emphasis"><em>k</em></span> of 2, exhibits higher purity, higher CMM, and lower SSQ compared to the model with a smaller horizon and <span class="emphasis"><em>k</em></span> of 5. In the complete set of results (available on this book's website, see link below), one can see that the effect of the larger horizon is the predominant factor responsible for the differences. </li><li class="listitem" style="list-style-type: disc">In the clustering models using micro-clustering, the SSQ is typically significantly smaller than when no micro-clustering is used. This is understandable, as there are far more clusters and fewer instances per cluster, and SSQ is measured with respect to the cluster center.</li><li class="listitem" style="list-style-type: disc">DBSCAN was found to be insensitive to micro-clustering, and size of the horizon. Compared to all other models, it ranks high on both intrinsic (Silhouette coefficient) as well as extrinsic measures (Completeness, Purity).</li><li class="listitem" style="list-style-type: disc">The two ClusTree <a id="id1166" class="indexterm"/>models have among the best CMM and Purity scores, with low SSQ due to micro-clusters.  </li><li class="listitem" style="list-style-type: disc">The final two outlier-based clustering algorithms have perfect CMM and Purity scores. The metrics are not significantly affected by the window size (although this impacts the evaluation time), or the value of k, the neighbor count threshold.</li></ul></div><p>Outlier detection experiments:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">All the techniques in this set of experiments performed equally well, with the exception of ApproxSTORM, which is to be expected considering the reduced window used in this method compared to the exact version.</li><li class="listitem" style="list-style-type: disc">The ratio of instances, always inlier versus those always outlier, is close to 140 for the majority of the models. Whether this implies adequate discriminatory power for a given dataset depends on the goals of the real-time learning <a id="id1167" class="indexterm"/>problem.</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/>Note</h3><p>All MOA configuration files and results from the experiments are available at: <a class="ulink" href="https://github.com/mjmlbook/mastering-java-machine-learning/Chapter5">https://github.com/mjmlbook/mastering-java-machine-learning/Chapter5</a>.</p></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec49"/>Summary</h1></div></div></div><p>The assumptions in stream-based learning are different from batch-based learning, chief among them being upper bounds on operating memory and computation times. Running statistics using sliding windows or sampling must be computed in order to scale to a potentially infinite stream of data. We make the distinction between learning from stationary data, where it is assumed the generating data distribution is constant, and dynamic or evolving data, where concept drift must be accounted for. This is accomplished by techniques involving the monitoring of model performance changes or the monitoring of data distribution changes. Explicit and implicit adaptation methods are ways to adjust to the concept change.</p><p>Several supervised and unsupervised learning methods have been adapted for incremental online learning. Supervised methods include linear, non-linear, and ensemble techniques, The HoeffdingTree is introduced which is particularly interesting due largely in part to its guarantees on upper bounds on error rates. Model validation techniques such as prequential evaluation are adaptations unique to incremental learning. For stationary supervised learning, evaluation measures are similar to those used in batch-based learning. Other measures are used in the case of evolving data streams.</p><p>Clustering algorithms operating under fixed memory and time constraints typically use small memory buffers with standard techniques in a single pass. Issues specific to streaming must be considered during evaluations of clustering, such as aging, noise, and missed or misplaced points. Outlier detection in data streams is a relatively new and growing field. Extending ideas in clustering to anomaly detection has proved very effective.</p><p>The experiments in the case study in this chapter use the Java framework MOA, illustrating various stream-based learning techniques for supervised, clustering, and outlier detection.</p><p>In the next chapter, we embark on a tour of the probabilistic graph modelling techniques that are useful in representing, eliciting knowledge, and learning in various domains.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec50"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">G. Cormode and S. Muthukrishnan (2010). <span class="emphasis"><em>An improved data stream summary: The Count-Min sketch and its applications</em></span>. Journal of Algorithms, 55(1):58–75, 2005.</li><li class="listitem">João Gama (2010). <span class="emphasis"><em>Knowledge Discovery from Data Streams, Chapman and Hall / CRC Data Mining and Knowledge Discovery Series</em></span>, CRC Press 2010, ISBN 978-1-4398-2611-9, pp. I-XIX, 1-237.</li><li class="listitem">B. Babcock, M. Datar, R. Motwani (2002). <span class="emphasis"><em>Sampling from a moving window over streaming data</em></span>, in Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms, pp.633–634, 2002.</li><li class="listitem">Bifet, A. and Gavalda, R. (2007). <span class="emphasis"><em>Learning from time-changing data with adaptive windowing</em></span>. In Proceedings of SIAM int. conf. on Data Mining. SDM. 443–448.</li><li class="listitem">Vitter, J. (1985). <span class="emphasis"><em>Random sampling with a reservoir. ACM Trans. Math. Softw</em></span>. 11, 1, 37–57.</li><li class="listitem">Gama, J., Medas, P., Castillo, G., and Rodrigues, P. (2004). <span class="emphasis"><em>Learning with drift detection</em></span>. In Proceedings of the 17th Brazilian symp. on Artif. Intell. SBIA. 286–295. </li><li class="listitem">Gama, J., Sebastiao, R., and Rodrigues, P. 2013. <span class="emphasis"><em>On evaluating stream learning algorithms</em></span>. Machine Learning 90, 3, 317–346.</li><li class="listitem">Domingos, P. and Hulten, G. (2000). <span class="emphasis"><em>Mining high-speed data streams</em></span>. In Proceedings. of the 6th ACM SIGKDD int. conference on Knowledge discovery and data mining. KDD. 71–80.</li><li class="listitem">Oza, N. (2001). <span class="emphasis"><em>Online ensemble learning</em></span>. Ph.D. thesis, University of California Berkeley.</li><li class="listitem">Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M., Bouchachia, A. (2014). <span class="emphasis"><em>A Survey on Concept Drift Adaptation.ACM Computing Surveys</em></span> 46(4), Article No. 44.</li><li class="listitem">Farnstrom, F., Lewis, J., and Elkan, C. (2000). <span class="emphasis"><em>Scalability for clustering algorithms revisited</em></span>. SIGKDD Exploration, 51–57.</li><li class="listitem">Zhang, T., Ramakrishnan, R., and Livny, M. (1996). <span class="emphasis"><em>BIRCH: An Efficient Data Clustering Method for Very Large Databases</em></span>. In Proceedings of the ACM SIGMOD International Conference on Management of Data. ACM Press, 103–114.</li><li class="listitem">Aggarwal, C. (2003). <span class="emphasis"><em>A Framework for Diagnosing Changes in Evolving Data Streams</em></span>. In ACM SIGMOD Conference. 575–586.</li><li class="listitem">Chen, Y. and Tu, L. (2007). <span class="emphasis"><em>Density-based clustering for real-time stream data</em></span>. In KDD '07: Proceedings of the 13th ACM SIGKDD international conference on knowledge discovery and data mining. ACM Press, 133–142.</li><li class="listitem">Kremer, H., Kranen, P., Jansen, T., Seidl, T., Bifet, A., Holmes, G., and Pfahringer, B. (2011). <span class="emphasis"><em>An effective evaluation measure for clustering on evolving data streams</em></span>. In proceedings of the 17th ACM SIGKDD international conference on knowledge discovery and data mining. KDD '11. ACM, New York, NY, USA, 868–876.</li><li class="listitem">Mahdiraji, A. R. (2009). <span class="emphasis"><em>Clustering data stream: A survey of algorithms</em></span>. International Journal of Knowledge-Based and Intelligent Engineering Systems, 39–44.</li><li class="listitem">F. Angiulli and F. Fassetti (2007). <span class="emphasis"><em>Detecting distance-based outliers in streams of data</em></span>. In proceedings of the Sixteenth ACM Conference on Information and Knowledge Management, CIKM '07, pages 811–820, New York, NY, USA, 2007. ACM.</li><li class="listitem">D. Yang, E. A. Rundensteiner, and M. O. Ward (2009). <span class="emphasis"><em>Neighbor-based pattern detection for windows over streaming data</em></span>. In Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology, EDBT '09, pages 529–540, New York, NY, USA, 2009. ACM.</li><li class="listitem">M. Kontaki, A. Gounaris, A. Papadopoulos, K. Tsichlas, and Y. Manolopoulos (2011). <span class="emphasis"><em>Continuous monitoring of distance-based outliers over data streams</em></span>. In Data Engineering (ICDE), 2011 IEEE 27th International Conference on, pages 135–146, April 2011.</li></ol></div></div></body></html>