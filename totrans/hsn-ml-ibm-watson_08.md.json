["```py\ndf_data_2.createOrReplaceTempView(\"station\")\nsqlDF = spark.sql(\"SELECT * FROM station where VALUE > 200\")\nsqlDF.show()\n```", "```py\nsplitted_data=df_data.randomSplit([0.8,0.18,0.02],24)\ntrain_data=splitted_data[0]\ntest_data=splitted_data[1]\npredict_data=splitted_data[2]\nprint(\"Number of training records: \" + str(train_data.count())) print(\"Number of testing records : \" + str(test_data.count())) print(\"Number of prediction records : \" + str(predict_data.count()))\n```", "```py\nfrompyspark.ml.featureimportOneHotEncoder,StringIndexer,IndexToString,VectorAssembler\nfrompyspark.ml.classification importRandomForestClassifier\nfrompyspark.ml.evaluationimportMulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model\n```", "```py\nstringIndexer_label=StringIndexer(inputCol=\"PRODUCT_LINE\",outputCol=\"label\").fit(df_data)\nstringIndexer_prof=StringIndexer(inputCol=\"PROFESSION\",outputCol=\"PROFESSION_IX\")\nstringIndexer_gend=StringIndexer(inputCol=\"GENDER\",outputCol=\"GENDER_IX\")\nstringIndexer_mar = StringIndexer(inputCol=\"MARITAL_STATUS\", outputCol=\"MARITAL_STATUS_IX\")\n```", "```py\nvectorAssembler_features = VectorAssembler(inputCols=[\"GENDER_IX\", \"AGE\", \"MARITAL_STATUS_IX\", \"PROFESSION_IX\"], outputCol=\"features\")\n```", "```py\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n```", "```py\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=stringIndexer_label.labels)\n```", "```py\npipeline_rf = Pipeline(stages=[stringIndexer_label, stringIndexer_prof, stringIndexer_gend, stringIndexer_mar, vectorAssembler_features, rf, labelConverter])\n```", "```py\ndf_data_1 = pd.read_csv(body, sep=',',names = ['STATION', 'DATE', 'METRIC', 'VALUE', 'C5', 'C6', 'C7', 'C8'])\n```", "```py\nimport ibmos2spark\n# @hidden_cell\ncredentials = {\n   'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'service_id': 'iam-ServiceId-f9f1f892-3a72-4bdd-9d12-32b5a616dbfa',\n   'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token',\n   'api_key': 'D2NjbuA02Ra3Pq6OueNW0JZZU6S3MKXOookVfQsKfH3L'\n}\nconfiguration_name = 'os_f20250362df648648ee81858c2a341b5_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf_data_2 = spark.read\\\n .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n .option('header', 'true')\\\n .load(cos.url('2015.CSV', 'chapter6-donotdelete-pr-qy3imqdyi8jv3w'))\ndf_data_2.take(5)\n```", "```py\ndf_data_2.printSchema()\n```", "```py\nfor row in df_data_2.take(2):\n    print(row)\n    print( \"*\" * 104)\n```", "```py\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\ndf_data_2.registerTempTable(\"MyWeather\")\n```", "```py\ntemp_df =  sqlContext.sql(\"select * from MyWeather\")\nprint (type(temp_df))\nprint (\"*\" * 104)\nprint (temp_df)\n```", "```py\nimport pandas as pd\nsqlContext.sql(\"select STATION, METRIC from MyWeather limit 2\").toPandas()\n```", "```py\nquery = \"\"\"\nselect\n    STATION ,\n    count(*) as metric_count\nfrom MyWeather\ngroup by STATION\norder by count(*) desc\n\"\"\"\nsqlContext.sql(query).toPandas()\n```", "```py\ndf_data_2.groupBy(\"DATE\").count().show()\ndf_data_2.groupBy(\"DATE\").count().collect()\n```", "```py\ncount = [item[1] for item in df_data_2.groupBy(\"DATE\").count().collect()]\nyear = [item[0] for item in df_data_2.groupBy(\"DATE\").count().collect()]\nnumber_of_metrics_per_year = {\"count\":count, \"DATE\" : year}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nnumber_of_metrics_per_year = pd.DataFrame(number_of_metrics_per_year )\nnumber_of_metrics_per_year .head()\n```", "```py\nnumber_of_metrics_per_year = number_of_metrics_per_year.sort_values(by = \"DATE\")\nnumber_of_metrics_per_year.plot(figsize = (20,10), kind = \"bar\", color = \"red\", x = \"DATE\", y = \"count\", legend = False)\nplt.xlabel(\"\", fontsize = 18)\nplt.ylabel(\"Number of Metrics\", fontsize = 18)\nplt.title(\"Number of Metrics Per Date\", fontsize = 28)\nplt.xticks(size = 18)\nplt.yticks(size = 18)\nplt.show()\n```", "```py\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\ndf_data_2.registerTempTable(\"MyWeather\")\n```", "```py\ntemp_df =  sqlContext.sql(\"select * from MyWeather where METRIC = 'PRCP' and VALUE>500\")\nprint (temp_df)\ntemp_df.count()\n```", "```py\ntemp_df.groupBy(\"DATE\").count().show()\ntemp_df.groupBy(\"DATE\").count().collect()\ncount = [item[1] for item in temp_df.groupBy(\"DATE\").count().collect()]\nyear = [item[0] for item in temp_df.groupBy(\"DATE\").count().collect()]\nnumber_of_metrics_per_year = {\"count\":count, \"DATE\" : year}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nnumber_of_metrics_per_year = pd.DataFrame(number_of_metrics_per_year )\nnumber_of_metrics_per_year .head()\nnumber_of_metrics_per_year = number_of_metrics_per_year.sort_values(by = \"DATE\")\nnumber_of_metrics_per_year.plot(figsize = (20,10), kind = \"bar\", color = \"red\", x = \"DATE\", y = \"count\", legend = False)\nplt.xlabel(\"\", fontsize = 18)\nplt.ylabel(\"Number of Metrics\", fontsize = 18)\nplt.title(\"Number of Metrics Per Date\", fontsize = 28)\nplt.xticks(size = 18)\nplt.yticks(size = 18)\nplt.show()\n```", "```py\ntemp_df =  sqlContext.sql(\"select * from MyWeather where METRIC = 'PRCP' and VALUE > 2999\")\nprint (temp_df)\ntemp_df.count()\n```"]