- en: '*Chapter 2*: Enabling and Operationalization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have just learned the basics of what Elastic ML is doing to accomplish both
    unsupervised automated anomaly detection and supervised data frame analysis. Now
    it is time to get detailed about how Elastic ML works inside the Elastic Stack
    (Elasticsearch and Kibana).
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will focus on both the installation (really, the enablement) of
    Elastic ML features and a detailed discussion of the logistics of the operation,
    especially with respect to anomaly detection. Specifically, we will cover the
    following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Elastic ML features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding operationalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The information in this chapter will use the Elastic Stack as it exists in v7.10
    and the workflow of the Elasticsearch Service of Elastic Cloud as of November
    2020.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Elastic ML features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process for enabling Elastic ML features inside the Elastic Stack is slightly
    different if you are doing so within a self-managed cluster versus using the **Elasticsearch**
    **Service** (**ESS**) of Elastic Cloud. In short, on a self-managed cluster, the
    features of ML are enabled via a license key (either a commercial key or a trial
    key). In ESS, a dedicated ML node needs to be provisioned within the cluster in
    order to utilize Elastic ML. In the following sections, we will explain the details
    of how this is accomplished in both scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling ML on a self-managed cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have a self-managed cluster that was created from the downloading of
    Elastic's default distributions of Elasticsearch and Kibana (available at [elastic.co/downloads/](http://elastic.co/downloads/)),
    enabling Elastic ML features via a license key is very simple. Be sure to not
    use the Apache 2.0 licensed open source distributions that do not contain the
    X-Pack code base.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic ML, unlike the bulk of the capabilities of the Elastic Stack, is not
    free – it requires a commercial (specifically, a **Platinum** level) license.
    It is, however, open source in that the source code is out in the open on GitHub
    ([github.com/elastic/ml-cpp](http://github.com/elastic/ml-cpp)) and that users
    can look at the code, file issues, make comments, or even execute pull requests.
    However, the usage of Elastic ML is governed by a commercial agreement with Elastic,
    the company.
  prefs: []
  type: TYPE_NORMAL
- en: When Elastic ML was first released (back in the v5.x days), it was part of the
    closed source features known as **X-Pack** that required a separate installation
    step. However, as of version 6.3, the code of X-Pack was "opened" ([elastic.co/what-is/open-x-pack](http://elastic.co/what-is/open-x-pack))
    and folded into the default distribution of Elasticsearch and Kibana. Therefore,
    a separate X-Pack installation step was no longer necessary, just the "enablement"
    of the features via a commercial license (or a trial license).
  prefs: []
  type: TYPE_NORMAL
- en: The installation procedure for Elasticsearch and Kibana itself is beyond the
    scope of this book, but it is easily accomplished by following the online documentation
    on the Elastic website (available at [elastic.co/guide/](http://elastic.co/guide/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once Elasticsearch and Kibana are running, navigate to the **Stack** option
    from the left-side navigation menu and select **License Management**. You will
    see a screen like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – The License management screen in Kibana'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_02_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 – The License management screen in Kibana
  prefs: []
  type: TYPE_NORMAL
- en: Notice that, by default, the license level applied is the free **Basic** tier.
    This enables you to use some of the advanced features not found in the Apache
    2.0 licensed open source distribution, or on third-party services (such as the
    Amazon Elasticsearch Service). A handy guide for comparing the features that exist
    at the different license levels can be found on the Elastic website at [elastic.co/subscriptions](http://elastic.co/subscriptions).
  prefs: []
  type: TYPE_NORMAL
- en: 'As previously stated, Elastic ML requires a Platinum tier license. If you have
    purchased a Platinum license from Elastic, you can apply that license by clicking
    on the **Update license** button, as shown on the screen in *Figure 2.1*. If you
    do not have a Platinum license, you can start a free 30-day trial by clicking
    the **Start my trial** button to enable Elastic ML and the other Platinum features
    (assuming you agree to the license terms and conditions):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Starting a free 30-day trial'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_02_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 – Starting a free 30-day trial
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this is complete, the licensing screen will indicate that you are now
    in an active trial of the Platinum features of the Elastic Stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Trial license activated'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_02_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 – Trial license activated
  prefs: []
  type: TYPE_NORMAL
- en: Once this is done, you can start to use Elastic ML right away. Additional configuration
    steps are needed to take advantage of the other Platinum features, but those steps
    are outside the scope of this book. Consult the Elastic documentation for further
    assistance on configuring those features.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling ML in the cloud – Elasticsearch Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If downloading, installing, and self-managing the Elastic Stack is less interesting
    than just getting the Elastic Stack platform offered as a service, then head on
    over to Elastic Cloud ([cloud.elastic.co](http://cloud.elastic.co)) and sign up
    for a free trial, using only your email:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Elastic Cloud welcome screen'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_02_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 – Elastic Cloud welcome screen
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Once inside the Elastic Cloud interface after logging in, you will have the
    ability to start a free trial by clicking the **Start your free trial** button:![Figure
    2.5 – Elastic Cloud home screen
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_02_005.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.5 – Elastic Cloud home screen
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the button is clicked, you will see that your 14-day free trial of ESS
    has started:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Elasticsearch Service trial enabled'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17040_02_006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.6 – Elasticsearch Service trial enabled
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Of course, in order to try out Elastic ML, you first need an Elastic Stack cluster
    provisioned. There are a few options to create what ESS refers to as **deployments**,
    with some that are tailored to specific use cases. For this example, we will use
    the **Elastic Stack** template on the left of *Figure 2.6* and choose the **I/O
    Optimized** hardware profile, but feel free to experiment with the other options
    during your trial:![Figure 2.7 – Creating an ESS deployment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_02_007.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.7 – Creating an ESS deployment
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also choose what cloud provider and which region to start your cluster
    in, but most importantly, if you want to use ML features, you must enable an ML
    node by first clicking on the **Customize** button near the bottom-right corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After clicking the **Customize** button, you will see a new screen that allows
    you to add an ML node:![Figure 2.8 – Customizing deployment to add an ML node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_02_008.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.8 – Customizing deployment to add an ML node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Near the bottom of *Figure 2.8* is a link to **Add Machine Learning nodes**
    to your cluster. Clicking on this will reveal the ML node configuration:![Figure
    2.9 – Adding ML node(s)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_02_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.9 – Adding ML node(s)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: During the free 14-day trial period of ESS, you can only add one 1 GB ML node
    (in one or two availability zones). If you move from a free trial to a paid subscription,
    you can obviously create more or larger ML nodes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the ML node is added to the configuration, click on the **Create Deployment**
    button to initiate the process for ESS to create your cluster for you, which will
    take a few minutes. In the meantime, you will be shown the default credentials
    that you will use to access the cluster:![Figure 2.10 – Default assigned credentials
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_02_010.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.10 – Default assigned credentials
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can download these credentials for use later. Don't worry if you forgot
    to download them – you can always reset the password later if needed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the cluster is up and running as shown in *Figure 2.11* (usually only
    after a few minutes), you will see the following view of your deployment, with
    an **Open Kibana** button that will allow you to launch into your deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Deployment successfully created'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_02_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 – Deployment successfully created
  prefs: []
  type: TYPE_NORMAL
- en: Once the **Open Kibana** button is clicked, you will be automatically authenticated
    into Kibana, where you will be ready to use ML straight away – no additional configuration
    steps are necessary.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, from the perspective of the user who wants to use Elastic ML,
    there is little difference between the self-managed configuration shown earlier
    and the setup created in ESS. The one major difference, however, is that the configuration
    here in ESS has Elastic ML always isolated to a `data`, `ingest`, and `ml` roles
    all on the same node). We will discuss this concept later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a functioning Elastic Stack with ML enabled, we are getting
    closer to being able to start analyzing data, which will begin in [*Chapter 3*](B17040_03_Epub_AM.xhtml#_idTextAnchor049),
    *Anomaly Detection*. But first, let's understand the operationalization of Elastic
    ML.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding operationalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At some point on your journey with using Elastic ML, it will be helpful to understand
    a number of key concepts regarding how Elastic ML is operationalized within the
    Elastic Stack. This includes information about how the analytics run on the cluster
    nodes and how data that is to be analyzed by ML is retrieved and processed.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Some concepts in this section may not be intuitive until you actually start
    using Elastic ML on some real examples. Don't worry if you feel like you prefer
    to skim (or even skip) this section now and return to it later following some
    genuine experience of using Elastic ML.
  prefs: []
  type: TYPE_NORMAL
- en: ML nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First and foremost, since Elasticsearch is, by nature, a distributed multi-node
    solution, it is only natural that the ML feature of the Elastic Stack works as
    a native plugin that obeys many of the same operational concepts. As described
    in the documentation ([elastic.co/guide/en/elasticsearch/reference/current/ml-settings.html](http://elastic.co/guide/en/elasticsearch/reference/current/ml-settings.html)),
    ML can be enabled on any or all nodes, but it is a best practice in a production
    system to have dedicated ML nodes. We saw this best practice forced on the user
    in Elastic Cloud ESS – the user must create dedicated ML nodes if ML is desired
    to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Having dedicated ML nodes is also helpful in optimizing the types of resources
    specifically required by ML. Unlike data nodes that are involved in a fair amount
    of disk I/O loads due to indexing and searching, ML nodes are more compute- and
    memory-intensive. With this knowledge, you can size the hardware appropriately
    for dedicated ML nodes.
  prefs: []
  type: TYPE_NORMAL
- en: One key thing to note—the ML algorithms do not run in the `autodetect` for anomaly
    detection and `data_frame_analyzer` for data frame analytics) can be seen in the
    process list (if you were to run the `ps` command on Linux, for example). There
    will be one process for every actively running ML job. In multi-node setups, ML
    will distribute the jobs to each of the ML-enabled nodes to balance the load of
    the work.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic ML obeys a setting called `xpack.ml.max_machine_memory_percent`, which
    governs how much system memory can be used by ML jobs. The default value of this
    setting is 30%. The limit is based on the total memory of the machine, not memory
    that is currently free. Don't forget that the Elasticsearch JVM may take up to
    around 50% of the available machine memory, so leaving 30% to ML and the remaining
    20% for the operating system and other ancillary processes is prudent, albeit
    conservative. Jobs are not allocated to a node if doing so would cause the estimated
    memory use of ML jobs to exceed the limit defined by this setting.
  prefs: []
  type: TYPE_NORMAL
- en: 'While there is no empirical formula to determine the size and number of dedicated
    ML nodes, some good rules of thumb are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Have one dedicated ML node (two for high availability/fault tolerance if a single
    node becomes unavailable) for cluster sizes of up to 10 data nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have at least two ML nodes for clusters of up to 20 nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add an additional ML node for every additional 10 data nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This general approach of reserving about 10-20% of your cluster capacity to
    dedicated ML nodes is certainly a reasonable suggestion, but it does not obviate
    the need to do your own sizing, characterization testing, and resource monitoring.
    As we will see in several later chapters, the resource demands on your ML tasks
    will greatly depend on what kind(s) of analyses are being invoked, as well as
    the density and volume of the data being analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: Jobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Elastic ML, the job is the unit of work. There are both **anomaly detection**
    jobs and **data frame analytics** jobs. Both take some kind of data as input and
    produce new information as output. Jobs can be created using the ML UI in Kibana,
    or programmatically via the API. They also require ML-enabled nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In general, anomaly detection jobs can be run as a single-shot batch analysis
    (over a swath of historical data) or continuously run in real time on time series
    data – data that is constantly being indexed by your Elastic Stack (or both, really).
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, data frame analytics jobs are not continuous – they are single-shot
    executions that produce output results and/or an output model that is used for
    subsequent **inferencing**, discussed in more depth in chapters 9 to 13\.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, from an operationalization standpoint, anomaly detection jobs are
    a bit more complex – as multiple can be running simultaneously, doing independent
    things and analyzing data from different indices. In other words, anomaly detection
    jobs are likely to be continuously busy within a typical cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we will see in more depth later, the main configuration elements of an anomaly
    detection job are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Job name/ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis bucketization window (the **bucket span**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The definition and settings for the query to obtain the raw data to be analyzed
    (the **datafeed**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The anomaly detection configuration recipe (the **detector**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the notion of jobs understood, we'll next focus on how the bucketing of
    time series data is an important concept in the analysis of real-time data.
  prefs: []
  type: TYPE_NORMAL
- en: Bucketing data in a time series analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bucketing input data is an important concept to understand in Elastic ML's anomaly
    detection. Set with a key parameter at the job level called `bucket_span`, the
    input data from the datafeed (described next) is collected into mini batches for
    processing. Think of the bucket span as a pre-analysis aggregation interval—the
    window of time in which a portion of the data is aggregated over for the purposes
    of analysis. The shorter the duration of `bucket_span`, the more granular the
    analysis, but also the higher the potential for noisy artifacts in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, the following graph shows the same dataset aggregated over three
    different intervals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Aggregations of the same data over different time intervals'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_02_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12 – Aggregations of the same data over different time intervals
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the prominent anomalous spike seen in the version aggregated over
    the 5- minute interval becomes all but lost if the data is aggregated over a 60-minute
    interval due to the fact of the spike's short (<2 minute) duration. In fact, at
    this 60-minute interval, the spike doesn't even seem that anomalous any more.
  prefs: []
  type: TYPE_NORMAL
- en: This is a practical consideration behind the choice of `bucket_span`. On the
    one hand, having a shorter aggregation period is helpful because it will increase
    the frequency of the analysis (and thus reduce the interval of notification if
    there is something anomalous), but making it too short may highlight features
    in the data that you don't really care about. If the brief spike that's shown
    in the preceding data is a meaningful anomaly for you, then the 5-minute view
    of the data is sufficient. If, however, a perturbation of the data that is very
    brief seems like an unnecessary distraction, then avoid a low value of `bucket_span`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Some additional practical considerations can be found on Elastic''s blog: [elastic.co/blog/explaining-the-bucket-span-in-machine-learning-for-elasticsearch](http://elastic.co/blog/explaining-the-bucket-span-in-machine-learning-for-elasticsearch).'
  prefs: []
  type: TYPE_NORMAL
- en: Feeding data to Elastic ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anomaly detection jobs obviously need data to analyze (and use to build and
    mature the statistical models). This data comes from your time series indices
    in Elasticsearch. The datafeed is the mechanism by which this data is retrieved
    (searched) on a routine basis and presented to the ML algorithms. Its configuration
    is mostly obscured from the user, except in the case of the creation of an advanced
    job in the UI (or by using the anomaly detection API). However, it is important
    to understand what the datafeed is doing behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the concept of a **Watch** input in **Watcher**, the datafeed will
    routinely query for data against an index pattern (or **saved search**) that contains
    the data to be analyzed. How often (and how much data at a time) the datafeed
    queries the data depends on a number of factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`query`: The actual query (expressed in Elasticsearch DSL) that will be used
    to retrieve data from the source index for analysis. The user can choose to query
    all documents in the source index or to selectively filter and/or aggregate the
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bucket_span`: We have already established that `bucket_span` controls the
    width of the ongoing analysis window. Therefore, the job of the datafeed is to
    make sure that the buckets are full of chronologically ordered data. You can therefore
    see that the datafeed will make a date range query to Elasticsearch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frequency`: A parameter that controls how often the raw data is physically
    queried. If this is between 2 and 20 minutes, `frequency` will equal `bucket_span`
    (as in, query every 5 minutes for the last 5 minutes'' worth of data). If `bucket_span`
    is longer, `frequency`, by default, will be a smaller number (more frequent) so
    that the overall long interval is not expected to be queried all at once. This
    is helpful if the dataset is rather voluminous. In other words, the interval of
    a long `bucket_span` will be chopped up into smaller intervals simply for the
    purposes of querying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query_delay`: This controls the amount of time "behind now" that the datafeed
    should query for a bucket span''s worth of data. The default is 60 seconds when
    the job is configured via the API, or a randomized value between 60 seconds and
    120 seconds if the job is configured via the UI. Therefore, with a `bucket_span`
    value of 5 minutes and a `query_delay` value of 60 seconds at 12:01 P.M., the
    datafeed will request data in the range of 11:55 A.M. to midnight. This extra
    little delay allows for delays in the ingest pipeline to ensure that no data is
    excluded from the analysis if its ingestion is delayed for any reason. If the
    system detects that the anomaly detection job is missing data due to possible
    ingest delays, a system-generated `query_delay` might need to be increased to
    remedy it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scroll_size`: In most cases, the type of search that the datafeed executes
    to Elasticsearch uses the scroll API ([elastic.co/guide/en/elasticsearch/reference/current/scroll-api.html](http://elastic.co/guide/en/elasticsearch/reference/current/scroll-api.html)).
    The scroll size defines how much the datafeed queries to Elasticsearch at a time.
    For example, if the datafeed is set to query for log data every 5 minutes, but
    in a typical 5-minute window there are 1 million events, the idea of scrolling
    that data means that not all 1 million events will be expected to be fetched with
    one giant query. Rather, it will do it with many queries in increments of `scroll_size`.
    By default, this scroll size is set conservatively to 1,000\. So, to get 1 million
    records returned to ML, the datafeed will ask Elasticsearch for 1,000 rows, 1,000
    times. Increasing `scroll_size` to 10,000 will reduce the number of scrolls to
    100\. In general, beefier clusters should be able to handle a larger `scroll_size`
    and thus be more efficient in the overall process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is an exception, however, in the case of a single metric job. The single
    metric job (described more in [*Chapter 3*](B17040_03_Epub_AM.xhtml#_idTextAnchor049),
    *Anomaly Detection*) is a simple ML job that allows only one-time series metrics
    to be analyzed. In this case, the scroll API is not used to obtain the raw data—rather,
    the datafeed will automatically create a query aggregation (using the `date_histogram`
    aggregation). This aggregation technique can also be used for any anomaly detection
    job, but it currently requires direct editing of the job's JSON configuration
    and should be reserved for expert users.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of feeding data to Elastic ML for data frame analytics jobs, the paradigm
    is different from anomaly detection because data isn't being fed to the analytics
    continuously, in real time. The specifics on how to feed data to a data frame
    analytics job will be covered in chapters 9-13.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a deeper understanding of how data flows into Elastic ML for
    analysis, let's now look at some of the indices that are used to support Elastic
    ML's operation.
  prefs: []
  type: TYPE_NORMAL
- en: The supporting indices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For Elastic ML to function, there are several supporting indices that exist
    and serve specific purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.ml-config`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.ml-state-*`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.ml-notifications-*`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.ml-annotations-*`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.ml-stats-*`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.ml-anomalies-*`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these indices are **system indices** (and most are **hidden indices**),
    meaning that they are not intended to be written to or manipulated by the end
    user. However, it is often helpful to understand their function/role, so let's
    take each one in turn.
  prefs: []
  type: TYPE_NORMAL
- en: .ml-config
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `.ml-config` index contains configuration information about all of the ML
    jobs that are currently defined in the system. The information contained in this
    index is readable and interpretable by the average user.
  prefs: []
  type: TYPE_NORMAL
- en: .ml-state-*
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `.ml-state` index is the place where Elastic ML keeps the internal information
    about the progress of data frame analytics jobs and anomaly detection statistical
    models that have been learned for a specific dataset, plus additional logistical
    information. This index is *not* meant to be understandable by a user—it is the
    backend algorithms of ML that will read and write entries in this index.
  prefs: []
  type: TYPE_NORMAL
- en: .ml-notifications-*
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This index is the place where Elastic ML stores the audit messages that appear
    in the `elasticsearch.log` file.
  prefs: []
  type: TYPE_NORMAL
- en: .ml-annotations-*
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This index stores records of annotations associated with anomaly detection jobs.
    This includes user-created annotations that can be defined with the anomaly detection
    UI, but also system-created annotations, such as ingest delay warnings and model
    snapshot notifications.
  prefs: []
  type: TYPE_NORMAL
- en: .ml-stats-*
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This index retains information about the progress and performance of data frame
    analytics jobs.
  prefs: []
  type: TYPE_NORMAL
- en: .ml-anomalies-*
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The .`ml-anomalies-*` indices contain the detailed results of ML jobs. These
    indices are instrumental in leveraging the output of the ML algorithms. All information
    displayed in the ML UI will be driven from this result data. Additionally, proactive
    alerting on anomalies will be accomplished by having queries configured against
    these indices. More information on this will be presented in [*Chapter 6*](B17040_06_Epub_AM.xhtml#_idTextAnchor117),
    *Alerting on ML Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the names and roles of the system indices owned and managed
    by Elastic ML, let's next look specifically at `.ml-state` and `.ml-anomalies`
    and how they contribute to the runtime orchestration of the anomaly detection
    jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection orchestration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because anomaly detection jobs can be run continuously on live, time series
    data, a rather complex orchestration occurs. A simplified diagram of this process
    is shown in *Figure 2.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Simplified sequence of an anomaly detection job''s operation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_02_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13 – Simplified sequence of an anomaly detection job's operation
  prefs: []
  type: TYPE_NORMAL
- en: The `autodetect` process, which is the physical manifestation of the anomaly
    detection job, is what is represented by the analyze versus model step in *Figure
    2.13*. The `.ml-state` index is read and written to by the `autodetect` process
    occasionally (as described in the next section). The output of the `autodetect`
    process (the results of the analysis) is stored in the `.ml-anomalies-*` indices.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the preceding procedures are done once per `bucket_span` (except
    for the actual read/write from `.ml-state`). The key takeaway is that this orchestration
    enables the anomaly detection job to be online (that is, not offline/batch) and
    constantly learning on newly ingested data. This process is also handled automatically
    by Elastic ML, so that the user doesn't have to worry about the complex logistics
    required to make it all happen.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection model snapshots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned in the previous section, the "state" of the anomaly detection
    model is stored in the `.ml-state` index. However, it is not actually read or
    written with every bucket span. Instead, the model state is mostly kept in the
    memory of the `autodetect` process and is only periodically serialized to `.ml-state`.
    If the anomaly detection job is asked to run over a large swath of historical
    data, or is running in real time, then the model is serialized in the following
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Periodically, on a schedule of about every 3 to 4 hours (or at an interval defined
    by `background_persist_interval`, if explicitly set)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the anomaly detection job is put in the **closed** state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of this periodic serialization of the model, older snapshots are automatically
    deleted with a nightly system maintenance job. By default, if there are snapshots
    over 1 day older than the newest snapshot in the `.ml-state` index, they are deleted
    except for the first snapshot each day. Additionally, all snapshots over 10 days
    older than the newest snapshot are deleted. If you want to exempt a specific snapshot
    from this cleanup and keep it around indefinitely, use the UI in Kibana or the
    updated model snapshots API to set the value of the `retain` setting to `true`.
  prefs: []
  type: TYPE_NORMAL
- en: It may also be apparent that having saved snapshots now allows the user to revert
    the job to use one of these previously taken snapshots of the model in the event
    of something going wrong operationally, or an unexpected situation arising. In
    one of the *Tips and tricks* sections of the [*Appendix*](B17040_14_Epub_AM.xhtml#_idTextAnchor248),
    we will work through an example that demonstrates how to ignore time periods and
    revert a job to use a model snapshot.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To summarize, in this chapter, we covered the procedures around the enabling
    of Elastic ML's features in both a self-managed on-premises Elastic Stack and
    within the Elasticsearch Service of Elastic Cloud. Additionally, we looked under
    the hood to see the deep integration points with the rest of the Elastic Stack
    and how Elastic ML works from an operational perspective.
  prefs: []
  type: TYPE_NORMAL
- en: As we look ahead to future chapters, the focus will now shift away from the
    conceptual and background information into the realm of practical usage. Starting
    with the next chapter, we will jump right into the comprehensive capabilities
    of Elastic ML's anomaly detection and we will learn how to configure jobs to solve
    some practical use cases in log analytics, metric analysis, and user behavior
    analytics.
  prefs: []
  type: TYPE_NORMAL
