["```py\nimport numpy as np\n# Model's probabilities of samples 1 and 2 for the 3 classes\nprobs_sample_1 = np.array([0.05, 0.85, 0.10])\nprobs_sample_2 = np.array([0.35, 0.15, 0.50])\ndef least_confident_score(predicted_probs):\n    return 1 - predicted_probs[np.argmax(predicted_probs)]\nLC_samples_scores = np.array(\n    [least_confident_score(probs_sample_1),\n    least_confident_score(probs_sample_2)])\nprint(f'Least confident score for sample 1 is: \n    {LC_samples_scores[0]}')\nprint(f'Least confident score for sample 2 is: \n    {LC_samples_scores[1]}')\nmost_informative_sample = np.argmax(LC_samples_scores)\nprint(f'The most informative sample is sample \n    {most_informative_sample+1}')\n```", "```py\nLeast confident score for sample 1 is: 0.15000000000000002\nLeast confident score for sample 2 is: 0.5\n```", "```py\nimport numpy as np\n# Model's probabilities of sample 1 and 2 for the 3 classes\nprobs_sample_1 = np.array([0.05, 0.85, 0.10])\nprobs_sample_2 = np.array([0.35, 0.15, 0.50])\ndef margin_score(predicted_probs):\n    predicted_probs_max_1 = np.sort(predicted_probs)[-1]\n    predicted_probs_max_2 = np.sort(predicted_probs)[-2]\n    margin_score = predicted_probs_max_1 - predicted_probs_max_2\n    return margin_score\n# For sample 1\nmargin_score_sample_1 = margin_score(probs_sample_1)\nprint(f'The margin score of sample 1 is: {margin_score_sample_1}')\n# For sample 2\nmargin_score_sample_2 = margin_score(probs_sample_2)\nprint(f'The margin score of sample 2 is: {margin_score_sample_2}')\nmargin_scores = np.array([margin_score_sample_1, \n    margin_score_sample_2])\nmost_informative_sample = np.argmin(margin_scores)\nprint(f'The most informative sample is sample \n    {most_informative_sample+1}')\n```", "```py\nThe margin score of sample 1 is: 0.75\nThe margin score of sample 2 is: 0.15000000000000002\nThe most informative sample is sample 2\n```", "```py\nimport numpy as np\n# Model's probabilities of sample 1 and 2 for the 3 classes\nprobs_sample_1 = np.array([0.05, 0.85, 0.10])\nprobs_sample_2 = np.array([0.35, 0.15, 0.50])\ndef ratio_score(predicted_probs):\n    predicted_probs_max_1 = np.sort(predicted_probs)[-1]\n    predicted_probs_max_2 = np.sort(predicted_probs)[-2]\n    margin_score = predicted_probs_max_1 / predicted_probs_max_2\n    return margin_score\n# For sample 1\nratio_score_sample_1 = ratio_score(probs_sample_1)\nprint(f'The ratio score of sample 1 is: {ratio_score_sample_1}')\n# For sample 2\nratio_score_sample_2 = ratio_score(probs_sample_2)\nprint(f'The ratio score of sample 2 is: {ratio_score_sample_2}')\nmargin_scores = np.array([ratio_score_sample_1, ratio_score_sample_2])\nmost_informative_sample = np.argmin(margin_scores)\nprint(f'The most informative sample is sample \n    {most_informative_sample+1}')\n```", "```py\nThe ratio score of sample 1 is: 8.5\nThe ratio score of sample 2 is: 1.4285714285714286\nThe most informative sample is sample 2\n```", "```py\nimport numpy as np\n# Model's probabilities of sample 1 and 2 for the 3 classes\nprobs_sample_1 = np.array([0.05, 0.85, 0.10])\nprobs_sample_2 = np.array([0.35, 0.15, 0.50])\ndef entropy_score(predicted_probs):\n    return -np.multiply(predicted_probs, \\\n        np.nan_to_num(np.log2(predicted_probs))).sum()\n# For sample 1\nentropy_score_sample_1 = entropy_score(probs_sample_1)\nprint(f'The margin score of sample 1 is: {entropy_score_sample_1}')\n# For sample 2\nentropy_score_sample_2 = entropy_score(probs_sample_2)\nprint(f'The margin score of sample 2 is: {entropy_score_sample_2}')\nentropy_scores = np.array([entropy_score_sample_1, \\\n    entropy_score_sample_2])\nmost_informative_sample = np.argmax(entropy_scores)\nprint(f'The most informative sample is sample \n    {most_informative_sample+1}')\n```", "```py\nThe margin score of sample 1 is: 0.747584679824574\nThe margin score of sample 2 is: 1.4406454496153462\nThe most informative sample is sample 2.\n```", "```py\nimport numpy as np\n# Predicted labels from 2 committee members\ny1 = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\ny2 = np.array([1, 1, 0, 0, 1, 1, 0, 0, 1, 1])\n# Calculate disagreement\ndisagreement = np.abs(y1 - y2)\n# Find index of point with max disagreement\nquery_index = np.argmax(disagreement)\nprint(f\"Data point {query_index+1} selected with maximum disagreement\")\n```", "```py\nData point 1 selected with maximum disagreement\n```", "```py\nimport numpy as np\np1 = np.array([[0.6, 0.4], [0.2, 0.8], [0.8, 0.2], [0.4, 0.6], [0.7, 0.3],\n               [0.2, 0.8], [0.9, 0.1], [0.5, 0.5], [0.3, 0.7], [0.1, 0.9]])\np2 = np.array([[0.3, 0.7], [0.1, 0.9], [0.9, 0.1], [0.7, 0.3], [0.4, 0.6],\n               [0.1, 0.9], [0.8, 0.2], [0.4, 0.6], [0.6, 0.4], [0.2, 0.8]])\n# Average probabilities per class\np_class0 = (p1[:, 0] + p2[:, 0]) / 2\np_class1 = (p1[:, 1] + p2[:, 1]) / 2\np_avg = np.concatenate((p_class0.reshape(-1, 1), \\\n    p_class1.reshape(-1, 1)), axis=1)\n# Calculate entropy\nH = -np.sum(p_avg * np.log2(p_avg), axis=1)\nquery_index = np.argmax(H)\nprint(f\"Data point {query_index+1} selected with maximum entropy of {H[query_index]}\")\n```", "```py\nData point 1 selected with maximum entropy of 0.9927744539878083\n```", "```py\nimport numpy as np\np1 = np.array([[0.6, 0.4], [0.2, 0.8], [0.8, 0.2], [0.4, 0.6], [0.7, 0.3],\n               [0.2, 0.8], [0.9, 0.1], [0.5, 0.5], [0.3, 0.7], [0.1, 0.9]])\np2 = np.array([[0.3, 0.7], [0.1, 0.9], [0.9, 0.1], [0.7, 0.3], [0.4, 0.6],\n               [0.1, 0.9], [0.8, 0.2], [0.4, 0.6], [0.6, 0.4], [0.2, 0.8]])\nKL1 = np.sum(p1 * np.log(p1 / p2), axis=1)\nKL2 = np.sum(p2 * np.log(p2 / p1), axis=1)\navg_KL = (KL1 + KL2) / 2\nprint(\"KL(M1||M2):\", KL1)\nprint(\"KL(M2||M1):\", KL2)\nprint(\"Average KL:\", avg_KL)\nquery_index = np.argmax(avg_KL)\nprint(\"\\nData point\", query_index+1, \"selected with max average KL of\", avg_KL[query_index])\n```", "```py\nKL(M1||M2): [0.19204199 0.04440301 0.04440301 0.19204199 0.1837869  0.04440301\n0.03669001 0.020411   0.1837869  0.03669001]\nKL(M2||M1): [0.1837869  0.03669001 0.03669001 0.1837869  0.19204199 0.03669001\n0.04440301 0.02013551 0.19204199 0.04440301]\nAverage KL: [0.18791445 0.04054651 0.04054651 0.18791445 0.18791445 0.04054651\n0.04054651 0.02027326 0.18791445 0.04054651]\nData point 1 selected with max average KL of 0.18791444527430518\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics.pairwise import pairwise_distances\n# Generate sample 2D data\nnp.random.seed(1)\nX = np.concatenate((np.random.randn(100, 2) + [2, 2],\n                    np.random.randn(50, 2)))\n```", "```py\n    # kNN density\n    knn = NearestNeighbors(n_neighbors=5).fit(X)\n    distances, _ = knn.kneighbors(X)\n    knn_density = 1 / distances.sum(axis=1)\n    ```", "```py\n    # Kernel density estimation\n    kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)\n    kde_dens = np.exp(kde.score_samples(X))\n    ```", "```py\n    # K-means clustering\n    km = KMeans(n_clusters=5).fit(X)\n    km_density = 1 / pairwise_distances(X, \n        km.cluster_centers_).sum(axis=1)\n    ```", "```py\n    # Maximum Mean Discrepancy\n    mmd = pairwise_distances(X).mean()\n    mmd_density = 1 / pairwise_distances(X).sum(axis=1)\n    ```", "```py\n# Plot the density estimations\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs[0, 0].scatter(X[:, 0], X[:, 1], c=knn_density)\naxs[0, 0].set_title('kNN Density')\naxs[0, 1].scatter(X[:, 0], X[:, 1], c=kde_dens)\naxs[0, 1].set_title('Kernel Density')\naxs[1, 0].scatter(X[:, 0], X[:, 1], c=km_density)\naxs[1, 0].set_title('K-Means Density')\naxs[1, 1].scatter(X[:, 0], X[:, 1], c=mmd_density)\naxs[1, 1].set_title('Maximum mean discrepancy (MMD) Density')\nfig.suptitle('Density-Weighted Sampling methods')\nplt.show()\n```"]