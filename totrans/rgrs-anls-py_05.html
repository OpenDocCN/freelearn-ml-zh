<html><head></head><body>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05" class="calibre1"/>Chapter 5. Data Preparation</h1></div></div></div><p class="calibre8">After providing solid foundations for an understanding of the two basic linear models for regression and classification, we devote this chapter to a discussion about the data feeding the model. In the next pages, we will describe what can routinely be done to prepare the data in the best way and how to deal with more challenging situations, such as when data is missing or outliers are present.</p><p class="calibre8">Real-world experiments produce real data, which, in contrast to synthetic or simulated data, is often very varied. Real data is also quite messy, and frequently it proves wrong in ways that are obvious and some that are, initially, quite subtle. As a data practitioner, you will almost never find your data already prepared in the right form to be immediately analyzed for your purposes.</p><p class="calibre8">Writing a compendium of bad data and its remedies is outside the scope of this book, but our intention is to provide you with the basics to help you manage the majority of common data problems and correctly feed your algorithm. After all, the commonly known <a id="id369" class="calibre1"/>acronym <strong class="calibre2">garbage in, garbage out</strong> (<strong class="calibre2">GIGO</strong>) is a truth that we have to face and accept.</p><p class="calibre8">Throughout this chapter, we will therefore discover a variety of topics, Python classes, and functions that will allow you to:</p><div><ul class="itemizedlist"><li class="listitem">Properly scale numeric features and have an easier time not just comparing and interpreting coefficients but also when dealing with unusual or missing values or with very sparse matrices (very common in textual data processing)</li><li class="listitem">Turn qualitative features into numeric values that can be accepted by a regression model and correctly transformed into predictions</li><li class="listitem">Transform numeric features in the smartest possible way to convert non-linear relationships in your data into linear ones</li><li class="listitem">Determine what to do when important data is missing to estimate a replacement or even just let the regression manage the best solution by itself</li><li class="listitem">Repair any unusual or strange value in your data and make your regression model always work properly</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch05lvl1sec29" class="calibre1"/>Numeric feature scaling</h1></div></div></div><p class="calibre8">In <a class="calibre1" title="Chapter 3. Multiple Regression in Action" href="part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6">Chapter 3</a>, <em class="calibre9">Multiple Regression in Action</em>, inside the feature scaling section, we discussed how changing your original variables to a similar scale could help better interpret the resulting <a id="id370" class="calibre1"/>regression coefficients. Moreover, scaling is essential when using gradient descent-based algorithms because it facilitates quicker converging to a solution. For gradient descent, we will introduce other techniques that can only work using scaled features. However, apart for the technical requirements of certain algorithms, now our intention is to draw your attention to how feature scaling can be helpful when working with data that can sometimes be missing or faulty.</p><p class="calibre8">Missing or wrong data can happen not just during training but also during the production phase. Now, if a missing value is encountered, you have two design options to create a model sufficiently robust to cope with such a problem:</p><div><ul class="itemizedlist"><li class="listitem">Actively deal with the missing values (there is a paragraph in this chapter devoted to this)</li><li class="listitem">Passively deal with it and:<div><ul class="itemizedlist1"><li class="listitem">Your system throws an error and everything goes down (and remains down till the problem is solved)</li><li class="listitem">Your system ignores the missing data and computes the values that are not missing</li></ul></div></li></ul></div><p class="calibre8">It is certainly worrying that your prediction system can get struck and could stop, but ignoring it and summing the present values could produce highly biased results. If your regression equation has been designed to work with all its variables, then it cannot work properly when some data is missing. Anyway, let's again recall the linear regression formula:</p><div><img src="img/00089.jpeg" alt="Numeric feature scaling" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As you may guess, the bias coefficient is actually there to stay; it will always appear, whatever the situation with your predictors is. Consequently, even in an extreme case, such as when all the <em class="calibre9">X</em> are missing, if you standardize your variables thus they have zero mean.</p><p class="calibre8">Let's see this in practice and discover how properly rescaling your predictors can help to fix missing values, allow advanced optimization techniques such as gradient descent, regularization, and stochastic learning (more about the latter two in later chapters), and easily detect outlying and anomalous values.</p><p class="calibre8">First, let's upload our basic packages and functions for the analysis:</p><div><pre class="programlisting">
<strong class="calibre2">In: import numpy as np</strong>
<strong class="calibre2">  import pandas as pd</strong>
<strong class="calibre2">  import matplotlib.pyplot as plt</strong>
<strong class="calibre2">  from sklearn.datasets import load_boston</strong>
<strong class="calibre2">  from sklearn import linear_model</strong>
<strong class="calibre2">  %matplotlib inline</strong>
<strong class="calibre2">  #To set float output to 5 decimals and to suppress printing of \small floating point values using scientific notation</strong>
<strong class="calibre2">  np.set_printoptions(precision=5, suppress=True)</strong>
</pre></div><p class="calibre8">Please notice that <a id="id371" class="calibre1"/>the Boston dataset is also reloaded. We refer to <em class="calibre9">y</em> as the target variable and to <em class="calibre9">X</em> as the predictors' array.</p><div><pre class="programlisting">
<strong class="calibre2">In:	boston = load_boston()</strong>
<strong class="calibre2">  dataset = pd.DataFrame(boston.data, \columns=boston.feature_names)</strong>
<strong class="calibre2">  dataset['target'] = boston.target</strong>
<strong class="calibre2">  observations = len(dataset)</strong>
<strong class="calibre2">  variables = dataset.columns[:-1]</strong>
<strong class="calibre2">  X = dataset.ix[:,:-1]</strong>
<strong class="calibre2">  y = dataset['target'].values</strong>
</pre></div><p class="calibre8">Since we would also like to have a test on the logistic regression, we now transform the target variable into a binary response by putting all values above the score of 25 at level "1".</p><div><pre class="programlisting">
<strong class="calibre2">In: yq = np.array(y&gt;25, dtype=int)</strong>
</pre></div><p class="calibre8">After this operation, our qualitative response variable is named <code class="email">yq</code>.</p></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec52" class="calibre1"/>Mean centering</h2></div></div></div><p class="calibre8">For all rescaling operations, we suggest the functions to be found in the <code class="email">preprocessing</code> module of the Scikit-learn package. In particular, we will be using <code class="email">StandardScaler</code> and <code class="email">MinMaxScaler</code>. Like all classes in Scikit-learn, they both have a <code class="email">fit</code> method that will record and store the parameters that allow correct scaling. They also feature a <code class="email">transform</code> method<a id="id372" class="calibre1"/> that could be immediately applied on the same data (the <code class="email">fit_transform</code> method will also do the trick) or on any other data, for example data used for validation, testing, or even, later on, production.</p><p class="calibre8">The <code class="email">StandardScaler</code> class will <a id="id373" class="calibre1"/>rescale your variables by removing the mean, an action also called centering. In fact, in your training set the rescaled variables will all have zero mean and the features will be forced to the unit variance. After fitting, the class will contain the <code class="email">mean_</code> and <code class="email">std_</code> vectors, granting you access to the means and standard deviations that made the scaling possible. Therefore, in any following set for testing purpose or predictions in production, you will be able to apply the exact same transformations, thus maintaining the data consistency necessary for the algorithm to work exactly.</p><p class="calibre8">The <code class="email">MinMaxScaler</code> class will <a id="id374" class="calibre1"/>rescale your variables setting a new minimum and maximum value in the range pointed out by you. After fitting, <code class="email">min_</code> and <code class="email">scale_</code> will report the minimum values (subtracted from the original variables) and the scale used for dividing your<a id="id375" class="calibre1"/> variables to have the intended maximum values, respectively.</p><div><h3 class="title2"><a id="tip20" class="calibre1"/>Tip</h3><p class="calibre8">If you reuse one of the two classes, after being trained, on other new data, the new variables might have different maximum and minimum values, causing the resulting transformed variables to be off-scale (above maximum or below minimum, or with an anomalous value). When this happens, it is important to check if the new data has anomalous values and question whether we used the correct data for the training phase when we defined the transformations and the coefficients.</p></div><p class="calibre8">Let's now upload both the scaling classes and get a remainder of the coefficients and intercept value when fitting <a id="id376" class="calibre1"/>a linear regression on the Boston dataset:</p><div><pre class="programlisting">
<strong class="calibre2">In: from sklearn.preprocessing import StandardScaler</strong>
<strong class="calibre2">  from sklearn.preprocessing import MinMaxScaler</strong>
<strong class="calibre2">  linear_regression = linear_model.LinearRegression(normalize=False,\fit_intercept=True)</strong>
<strong class="calibre2">  linear_regression.fit(X,y)</strong>
<strong class="calibre2">  print ("coefficients: %s\nintercept: %0.3f" % \(linear_regression.coef_,linear_regression.intercept_))</strong>

<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00090.jpeg" alt="Mean centering" class="calibre10"/></div><p class="calibre11"> </p><div><h3 class="title2"><a id="tip21" class="calibre1"/>Tip</h3><p class="calibre8">If you get your results from your Jupyter Notebook in scientific notation, it could be helpful to first use <code class="email">import numpy as np</code> and then <code class="email">np.set_printoptions(precision=5, suppress=True)</code>.</p></div><p class="calibre8">In particular, let's notice the intercept. Given the linear regression formula, we can expect that to be the regression output when all predictors are zero. Let's also have a look at the minimum values to check if they are negative, zero, or positive.</p><div><pre class="programlisting">
<strong class="calibre2">In: dataset.min()</strong>

<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00091.jpeg" alt="Mean centering" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Given the range in our variables, there could never be a situation when all the predictors are zero, implying that the intercept, though still functional and essential for the proper working of our <a id="id377" class="calibre1"/>model, is not representing any really expected value.</p><p class="calibre8">Now, as a first scaling operation, let's just center the variables, that is, remove the mean, and see if this action changes something in our linear regression.</p><div><pre class="programlisting">
<strong class="calibre2">In: centering = StandardScaler(with_mean=True, with_std=False)</strong>
<strong class="calibre2">  linear_regression.fit(centering.fit_transform(X),y)</strong>
<strong class="calibre2">  print ("coefficients: %s\nintercept: %s" % \(linear_regression.coef_,linear_regression.intercept_))</strong>

<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00092.jpeg" alt="Mean centering" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Though the coefficients have remained the same, now the intercept is <strong class="calibre2">22.533</strong>, a value that has a particular meaning in our Boston Housing prices problem:</p><div><pre class="programlisting">
<strong class="calibre2">In: print ('mean: %0.3f' % np.mean(y))</strong>

<strong class="calibre2">Out:mean: 22.533</strong>
</pre></div><p class="calibre8">Having the <a id="id378" class="calibre1"/>intercept valued as the average target value means that when one or more values are missing we expect them to automatically get a zero value if we centered the variable, and our regression will naturally tend to output the average value of the target variable.</p></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec53" class="calibre1"/>Standardization</h2></div></div></div><p class="calibre8">At this point, we<a id="id379" class="calibre1"/> can also try scaling everything to unit variance and <a id="id380" class="calibre1"/>check the results:</p><div><pre class="programlisting">
<strong class="calibre2">In: standardization = StandardScaler(with_mean=True, with_std=True)</strong>
<strong class="calibre2">  linear_regression.fit(standardization.fit_transform(X),y)</strong>
<strong class="calibre2">  print ("coefficients: %s\nintercept: %0.3f" % \(linear_regression.coef_,linear_regression.intercept_))</strong>

<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00093.jpeg" alt="Standardization" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As expected, now the coefficients are different, and each one now represents the unit change in the target after a modification in the predictors' equivalent to a standard deviation. However, the scales are not fully comparable if the distributions of our predictors are not normal (standardization implies a normal bell-shaped distribution), yet we can now compare the impact of each predictor and allow both the automatic handling of missing values and the correct functioning of advanced algorithms.</p></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec54" class="calibre1"/>Normalization</h2></div></div></div><p class="calibre8">Normalization<a id="id381" class="calibre1"/> rescales as standardization, by acting on ranges of the<a id="id382" class="calibre1"/> predictors, but it has different properties. In fact, when using normalization the zero value is the minimum value in the range of values of each predictor. That means that zero doesn't represent the mean anymore. Moreover, rescaling between the maximum and the minimum can become misleading if there are anomalous values at either one of the extremities (most of your values will get squeezed around a certain region in <code class="email">[0,1]</code>, usually in the center of the range).</p><div><pre class="programlisting">
<strong class="calibre2">In: scaling  = MinMaxScaler(feature_range=(0, 1))</strong>
<strong class="calibre2">  linear_regression.fit(scaling.fit_transform(X),y)</strong>
<strong class="calibre2">  print ("coefficients: %s\nintercept: %0.3f" % \(linear_regression.coef_,linear_regression.intercept_))</strong>

<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00094.jpeg" alt="Normalization" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Applying the <code class="email">MinMaxScaler</code> in a range of 0 and 1 drastically changes both the coefficients and the intercept, but this could be acceptable under certain circumstances. In fact, when working with big data derived from textual data or logs, we sometimes realize that the matrices<a id="id383" class="calibre1"/> we are working on are not especially populated with <a id="id384" class="calibre1"/>values, zero often being the most frequent value to be encountered. To speed up the calculations and allow huge matrices to be kept in memory, matrices are stored in a sparse format.</p><p class="calibre8">Sparse matrices do not occupy all the memory necessary for their size, they just store coordinates and non-zero values. In such situations, standardizing the variables would change zero to the mean and a large number of previously zero cells would have to be defined, causing the matrix to occupy much more memory. Scaling between zero and one allows taking values in a comparable order and keeping all previously zero entries, thus not modifying the matrix dimensions in memory.</p></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_5"><a id="ch05lvl2sec55" class="calibre1"/>The logistic regression case</h2></div></div></div><p class="calibre8">A special discussion has to be devoted to logistic regression. As we illustrated in the previous chapter, in logistic regression we model the odds ratio of the probability of response. We<a id="id385" class="calibre1"/> can use the standardized coefficients as a trick to face missing data, as seen with linear regression, but things then turn out to be a bit different from when we try to guess a target numeric value in linear regression analysis.</p><p class="calibre8">Let's explore an example that could clarify the matter. We will be using the Boston dataset to demonstrate the logistic regression case and we will use the <code class="email">yq</code> vector, previously defined, as a response variable. For the logistic regression, we won't be using the Scikit-learn implementation this time but rather the Statsmodels package, so we can easily show some insights about the coefficients in the model:</p><div><pre class="programlisting">
<strong class="calibre2">In: import statsmodels.api as sm</strong>
<strong class="calibre2">  Xq = sm.add_constant(standardization.fit_transform(X))</strong>
<strong class="calibre2">  logit = sm.Logit(yq, Xq)</strong>
<strong class="calibre2">  result = logit.fit()</strong>
<strong class="calibre2">  print (result.summary())</strong>

<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00095.jpeg" alt="The logistic regression case" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Using the <a id="id386" class="calibre1"/>standardized predictors, as in a linear regression, we can interpret the coefficients in terms of the same scale and consider the intercept as the response when all predictors have an average value. Contrary to linear regression, in logistic regression a unit change in predictors changes the odds ratio of the response of a quantity equivalent to the exponentiation of the coefficients themselves:</p><div><pre class="programlisting">
<strong class="calibre2">In: print ('odd ratios of coefficients: %s' % np.exp(result.params))</strong>

<strong class="calibre2">Out: odd ratios of coefficients: [  0.04717   0.90948   1.2896    0.46908   1.2779    0.45277   3.75996   1.10314   0.28966  15.9012    0.16158   0.46602   0.81363   0.07275]</strong>
</pre></div><p class="calibre8">We recall how odds ratios are calculated: given a certain probability <em class="calibre9">p</em> of an event, an odds ratio is the ratio between <em class="calibre9">p</em> and its complement to 1, odds <em class="calibre9">ratio = p / (1−p)</em>. When the odds ratio is equivalent to 1, our probability is exactly 0.5. When the probability is above 0.5 the odds ratio is above 1; when, on the contrary, our probability is less than 0.5 then the <a id="id387" class="calibre1"/>odds ratio is below 1. By applying the natural logarithm (as logistic regression does), the values are distributed around the zero value (50% probability). Clearly working with probabilities is more intuitive<a id="id388" class="calibre1"/> therefore, a simple transformation, the sigmoid transformation, will convert the coefficients to more understandable probabilities:</p><div><pre class="programlisting">
<strong class="calibre2">In: def sigmoid(p):</strong>
<strong class="calibre2">  return 1 / (1 + np.exp(-p))</strong>

<strong class="calibre2">  print ('intercept: %0.3f' % result.params[0])</strong>
<strong class="calibre2">  print ('probability of value above 25 when all predictors are \average: %0.3f' % 	sigmoid(result.params[0]))</strong>

<strong class="calibre2">Out: intercept: -3.054</strong>
<strong class="calibre2">    probability of value above 25 when all predictors</strong>
<strong class="calibre2">    are average: 0.045</strong>
</pre></div><p class="calibre8">Transforming the intercept into a probability using the sigmoid function, we obtain <code class="email">0.045</code>, which is the probability of a house value above <code class="email">25</code> when all predictors bear the mean value. Please notice that such a probability is different from the average probability in the sample:</p><div><pre class="programlisting">
<strong class="calibre2">In: print ('average likelihood of positive response: %0.3f' % </strong>
<strong class="calibre2">       (sum(yq) /float(len(yq))))</strong>

<strong class="calibre2">Out: average likelihood of positive response: 0.245</strong>
</pre></div><p class="calibre8">In fact, that's the baseline probability of having a house value above <code class="email">25</code>, taking account of any possible value of the predictors. What we extracted from logistic regression is instead a particular probability, not a general one. You can actually get a comparable likelihood when you model a logistic regression with only an intercept (the so-called null model), thus allowing the predictors to vary freely:</p><div><pre class="programlisting">
<strong class="calibre2">In: C = np.ones(len(X))</strong>
<strong class="calibre2">  logit = sm.Logit(yq, C)</strong>
<strong class="calibre2">  result = logit.fit()</strong>
<strong class="calibre2">  print (result.summary())</strong>
<strong class="calibre2">  print ('\nprobability of value above 25 using just a constant: %0.3f' % 	sigmoid(result.params[0]))</strong>

<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00096.jpeg" alt="The logistic regression case" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec30" class="calibre1"/>Qualitative feature encoding</h1></div></div></div><p class="calibre8">Beyond numeric features, which have been the main topic of this section so far, a great part of your data<a id="id389" class="calibre1"/> will also comprise qualitative variables. Databases especially tend to record data readable and understandable by human beings; consequently, they are quite crowded by qualitative data, which can appear in data fields in the form of text or just single labels explicating information, such as telling you the class of an observation or some of its characteristics.</p><p class="calibre8">For a better understanding of qualitative variables, a working example could be a weather dataset. Such a dataset describes conditions under which you would want to play tennis because of weather information such as outlook, temperature, humidity, and wind, which are all kinds of information that can be rendered by numeric measurements. However, you will easily find such data online and recorded in datasets with their qualitative translations such as <code class="email">sunny</code> or <code class="email">overcast</code>, rather than numeric satellite/weather-station measurements. We will work on this kind of data to demonstrate how it is still possible to transform it in such a way that it can be included effectively into a linear model:</p><div><pre class="programlisting">
<strong class="calibre2">In: outlook   = ['sunny', 'overcast', 'rainy']</strong>
<strong class="calibre2">  temperature = ['hot', 'mild', 'cool']</strong>
<strong class="calibre2">  humidity    = ['high', 'normal']</strong>
<strong class="calibre2">  windy       = ['TRUE', 'FALSE']</strong>

<strong class="calibre2">  weather_dataset = list()</strong>

<strong class="calibre2">  for o in outlook:</strong>
<strong class="calibre2">    for t in temperature:</strong>
<strong class="calibre2">      for h in humidity:</strong>
<strong class="calibre2">        for w in windy:</strong>
<strong class="calibre2">          weather_dataset.append([o,t,h,w])</strong>

<strong class="calibre2">  play = [0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]</strong>
</pre></div><p class="calibre8">A linear regressor can analyze qualitative data only after having been properly transformed into a numeric value. A common type of qualitative data comprises nominal variables, which are expressed by a limited set of textual labels. For instance, a nominal variable could be the color of a product or the outlook of the weather (as in our weather example). The textual values that a variable can assume are called levels; in our example, outlook has three levels: <code class="email">sunny</code>, <code class="email">overcast</code> and <code class="email">rainy</code>, all of them represented as strings.</p><p class="calibre8">If we think that <a id="id390" class="calibre1"/>any of these can be present or not (each label excludes the other), we can easily transform each nominal variable with n levels into n distinct variables, each one telling us if a certain characteristic is present or not. If we use the value of 1 for denoting the presence of the level and 0 for its absence (like binary coding, such as in computers), we will have a working transformation of qualitative information into a numeric one (technically it is a Boolean, but for practical purposes we model it as a 0 – 1 numeric integer). Such transformed variables are called indicator or binary variables in machine learning terminology, whereas in statistics they are described<a id="id391" class="calibre1"/> as <strong class="calibre2">dichotomies</strong> (a more technical term) or dummies variables. They act in a regression formula as modifiers of the intercept when the level is present.</p><p class="calibre8">When the levels of a variable are ordered, there's another possible transformation. For instance, they can be qualitative labels such as good, average, acceptable, and bad. In such an occurrence, the labels can also be converted into growing or decreasing numbers following the ordering of the meaning of the labels. Therefore, in our example, good could be 3, average 2, acceptable 1, and bad 0. Such encoding directly translates a qualitative variable into a numeric one, but it works only with labels that can be ordered (that is, where you can define <em class="calibre9">greater than</em> and <em class="calibre9">lower than</em> relations). The transformation implies, since a single coefficient will be calculated in the regression model for all the levels, that the difference in the outcome passing from good to average is the same as passing from acceptable to bad. In reality, this is often not true due to non-linearity. In such a case, binary encoding is still the best solution.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec56" class="calibre1"/>Dummy coding with Pandas</h2></div></div></div><p class="calibre8">The fastest <a id="id392" class="calibre1"/>way to transform a set of qualitative <a id="id393" class="calibre1"/>variables into binary ones is using a Pandas function, <code class="email">get_dummies</code>:</p><div><pre class="programlisting">
<strong class="calibre2">In: import pandas as pd</strong>
<strong class="calibre2">  df = pd.DataFrame(weather_dataset, columns=['outlook', \'temperature', 'humidity', 'windy'])</strong>
</pre></div><p class="calibre8">After transforming all your data into a Pandas DataFrame, it is quite easy to call single variables and single cases to be transformed into binary variables:</p><div><pre class="programlisting">
<strong class="calibre2">In: print (pd.get_dummies(df.humidity).ix[:5,:])</strong>
<strong class="calibre2">Out:    high  normal</strong>
<strong class="calibre2">    0     1       0</strong>
<strong class="calibre2">    1     1       0</strong>
<strong class="calibre2">    2     0       1</strong>
<strong class="calibre2">    3     0       1</strong>
<strong class="calibre2">    4     1       0</strong>
<strong class="calibre2">    5     1       0</strong>
</pre></div><p class="calibre8">Pandas can really transform all your variables in a breeze; all you need is to point out the DataFrame you <a id="id394" class="calibre1"/>want to entirely transform or specify just the variables to be converted:</p><div><pre class="programlisting">
<strong class="calibre2">In: dummy_encoding = pd.get_dummies(df)</strong>
</pre></div><p class="calibre8">After the transformation, a regression model can immediately analyze the resulting new DataFrame:</p><div><pre class="programlisting">
<strong class="calibre2">In: import statsmodels.api as sm</strong>
<strong class="calibre2">  X = sm.add_constant(dummy_encoding)</strong>
<strong class="calibre2">  logit = sm.Logit(play, X)</strong>
<strong class="calibre2">  result = logit.fit()</strong>
<strong class="calibre2">  print (result.summary())</strong>

<strong class="calibre2">Out: </strong>
</pre></div><div><img src="img/00097.jpeg" alt="Dummy coding with Pandas" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Some regression methods<a id="id395" class="calibre1"/> do not really like you to have all the <a id="id396" class="calibre1"/>binary variables expressing a qualitative variable (but this is not so in our case). Certain optimization methods do not love perfect collinearity, such as in the case of a complete binarization (in fact, if you know all the other dichotomies, then the remaining ones can be perfectly guessed by summing the others—it has value 1 when the sum of the others is zero). In such a case, you just need to drop a level of your choice from each set of binary variables. By doing so, the omitted coefficient will be incorporated into the intercept and the regression model will just work as before, though with a different set of variables and coefficients:</p><div><pre class="programlisting">
<strong class="calibre2">In: X.drop(['outlook_sunny', 'temperature_mild', 'humidity_normal', 'windy_FALSE'], inplace=True, axis=1)</strong>
<strong class="calibre2">  logit = sm.Logit(play, X)</strong>
<strong class="calibre2">  result = logit.fit()</strong>
<strong class="calibre2">  print (result.summary())</strong>

<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00098.jpeg" alt="Dummy coding with Pandas" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8"><code class="email">get_dummies</code> presents only one drawback: it constructs the binary variables directly, reading the<a id="id397" class="calibre1"/> levels from the dataset you are<a id="id398" class="calibre1"/> converting. Consequently, if you first build a set of binary variables from a sample and then another one from another sample of your same data, it can produce different transformed datasets because of rare levels not appearing in one of the samples.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec57" class="calibre1"/>DictVectorizer and one-hot encoding</h2></div></div></div><p class="calibre8">The Scikit-learn<a id="id399" class="calibre1"/> package offers a way, though a bit less direct, to<a id="id400" class="calibre1"/> consistently transform your qualitative<a id="id401" class="calibre1"/> variables into numeric ones.</p><p class="calibre8">The <code class="email">DictVectorizer</code> class <a id="id402" class="calibre1"/>can read datasets made of a list of dictionaries, properly transform the string label data into a set of binaries, and leave the numeric data untouched. If instead you already have qualitative variables coded as numeric types in your dataset, all you need is to transform them to string values before having them processed by <a id="id403" class="calibre1"/>
<code class="email">DictVectorizer</code>.</p><p class="calibre8">All you have to do is first create a dictionary representation of your dataset, as in the following example:</p><div><pre class="programlisting">
<strong class="calibre2">In: from sklearn.feature_extraction import DictVectorizer</strong>
<strong class="calibre2">  vectorizer = DictVectorizer(sparse = False)</strong>
<strong class="calibre2">  dict_representation = [{varname:var for var, varname in \zip(row,['outlook', 'temperature', 'humidity', 'windy'])}</strong>
<strong class="calibre2">  for row in weather_dataset]</strong>
<strong class="calibre2">  print (dict_representation[0])</strong>
<strong class="calibre2">  print (vectorizer.fit_transform(dict_representation))</strong>

<strong class="calibre2">Out: {'windy': 'TRUE', 'humidity': 'high', 'temperature': 'hot', 'outlook': 'sunny'}</strong>
<strong class="calibre2">  [[ 1.  0.  0.  0.  1.  0.  1.  0.  0.  1.]</strong>
<strong class="calibre2">  [ 1.  0.  0.  0.  1.  0.  1.  0.  1.  0.]</strong>
<strong class="calibre2">  [ 0.  1.  0.  0.  1.  0.  1.  0.  0.  1.]</strong>
<strong class="calibre2">...</strong>
</pre></div><p class="calibre8">A dictionary<a id="id404" class="calibre1"/> representation is in the form of a list of dictionaries <a id="id405" class="calibre1"/>whose keys are the variables' names<a id="id406" class="calibre1"/> and whose values are their numeric or label value. To obtain <a id="id407" class="calibre1"/>such a representation, you will need to duplicate your dataset, and that could represent a big limitation if you are working with little memory available.</p><p class="calibre8">On the other hand, the class keeps memory of the transformations and thus everything can be exactly replicated on any other data sample using the <code class="email">transform</code> method, overcoming the limitation we've seen with the Pandas <code class="email">get_dummies</code> method.</p><p class="calibre8">You can also easily visualize the transformations by calling the <code class="email">features_names_</code> method.</p><div><pre class="programlisting">
<strong class="calibre2">In: print (vectorizer.feature_names_)</strong>

<strong class="calibre2">Out: ['humidity=high', 'humidity=normal', 'outlook=overcast', \
      'outlook=rainy', 'outlook=sunny', 'temperature=cool', \
      'temperature=hot', 'temperature=mild', 'windy=FALSE', \
      'windy=TRUE']</strong>
</pre></div><p class="calibre8">If the limit number of binarizations does not justify the entire conversion of the dataset into a dictionary representation, by using the <code class="email">LabelEncoder</code> and <code class="email">LabelBinarizer</code> class, available in the <code class="email">preprocessing</code> package in Scikit-learn, you can encode and transform a single variable at a time.</p><p class="calibre8"><code class="email">LabelEncoder</code> turns the labels into numbers and <code class="email">LabelBinarizer</code> transforms the numbers into dichotomies. The consistency of all such operations across different samples is guaranteed by the <code class="email">fit</code> and <code class="email">transforms</code> methods that are characteristic of all the classes present in Scikit-learn, where <code class="email">fit</code> picks and records the parameters from data and the <code class="email">transform</code> method applies it to new data afterwards.</p><p class="calibre8">Let's test a transformation on the outlook variable. We first convert the text labels into numbers:</p><div><pre class="programlisting">
<strong class="calibre2">In: from sklearn.preprocessing import LabelEncoder, LabelBinarizer</strong>
<strong class="calibre2">label_encoder = LabelEncoder()</strong>
<strong class="calibre2">print (label_encoder.fit_transform(df.outlook))</strong>

<strong class="calibre2">Out: [2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]</strong>
</pre></div><p class="calibre8">The assigned <a id="id408" class="calibre1"/>numbers are given by the position of the <a id="id409" class="calibre1"/>label in the list that you get using <a id="id410" class="calibre1"/>an <code class="email">inverse_transform</code> method:</p><div><pre class="programlisting">
<strong class="calibre2">In: label_encoder.inverse_transform([0,1,2])</strong>

<strong class="calibre2">Out: array(['overcast', 'rainy', 'sunny'], dtype=object)</strong>
</pre></div><p class="calibre8">Or by just requiring the recorded classes, glancing at the <code class="email">classes_</code> internal variable:</p><div><pre class="programlisting">
<strong class="calibre2">In: print (label_encoder.classes_)</strong>

<strong class="calibre2">Out: ['overcast' 'rainy' 'sunny']</strong>
</pre></div><p class="calibre8">Once numerically <a id="id411" class="calibre1"/>encoded, <code class="email">LabelBinarizer</code> can transform everything into indicator variables, allowing you to decide what values should be placed in the dichotomy.</p><p class="calibre8">In fact, if you worry about missing values, you can encode the negative value as <code class="email">−1</code>, leaving the missing case at <code class="email">0</code> (in that case, the missing value will be passively taken in charge by the intercept as seen before).</p><div><pre class="programlisting">
<strong class="calibre2">In: label_binarizer = LabelBinarizer(neg_label=0, pos_label=1, \sparse_output=False)</strong>
<strong class="calibre2">  print (label_binarizer.fit_transform( \label_encoder.fit_transform(df.outlook)))</strong>

<strong class="calibre2">Out: [[0 0 1]</strong>
<strong class="calibre2">      [0 0 1]</strong>
<strong class="calibre2">      [0 0 1]</strong>
<strong class="calibre2">...</strong>
</pre></div><p class="calibre8">Another great advantage of this method is that it allows sparse representations, thus saving memory when working with large datasets.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec58" class="calibre1"/>Feature hasher</h2></div></div></div><p class="calibre8">One-hot encoding<a id="id412" class="calibre1"/> is a powerful transformation that allows <a id="id413" class="calibre1"/>any kind of data to be represented just using a binary variable. Using the same approach, you can even transform text into variables that can be analyzed by a linear regression model.</p><p class="calibre8">The idea is to transform any occurrence of a certain word in the text into a specific binary variable, so the model will assign a binary value connected to the presence of a word in the text. For instance, if you want to analyze the Latin motto <em class="calibre9">Nomina sunt consequentia rerum</em> (that means "names follow things"), you can force all the text to lowercase and enumerate all the distinct words present in the text by tokenizing them. By doing so, you intend to <a id="id414" class="calibre1"/>separate them (in our case, the tokenization is <a id="id415" class="calibre1"/>quite simple, we just split by spaces) in a way that it is often <a id="id416" class="calibre1"/>referred to as a <strong class="calibre2">bag of words</strong> (<strong class="calibre2">BoW</strong>) representation:</p><div><pre class="programlisting">
<strong class="calibre2">In: your_text = 'Nomina sunt consequentia rerum'</strong>
<strong class="calibre2">mapping_words_in_text = {word:position for position, word in enumerate(set(your_text.lower().split(' ')))}</strong>
<strong class="calibre2">print (mapping_words_in_text)</strong>

<strong class="calibre2">Out: {'rerum': 0, 'sunt': 1, 'consequentia': 2, 'nomina': 3}</strong>
</pre></div><p class="calibre8">The preceding code just transforms all your textual data into a dictionary containing the lowercase words and their positional index in a vector of binary variables.</p><p class="calibre8">The length of this vector is the length of the dictionary, and each binary flag has unit value when the corresponding word is present in the analyzed text. Therefore, the vector for all our phrase is <code class="email">[1,1,1,1]</code> and the vector for a phrase just containing the word <code class="email">'rerum'</code> should be <code class="email">[1,0,0,0]</code> because the positional index of the word is <code class="email">0</code>.</p><p class="calibre8">Figuratively, you can imagine our vector as a line of lamps; each time, you turn on only those whose corresponding word is present in the text you are analyzing.</p><div><h3 class="title2"><a id="tip22" class="calibre1"/>Tip</h3><p class="calibre8">Transforming a word into an indicator is just a starting point. You can also count how many times a word appears in a text and normalize that count by considering the length of the text you are transforming. In fact, in longer text, certain words have a higher chance of appearing multiple times than in shorter ones. By normalizing the word count, for instance, in such a way that the sum of word counts cannot be over a certain number, it will appear like reducing all texts to the same length. These are just some of the possible transformations that are part <a id="id417" class="calibre1"/>of <strong class="calibre2">natural language processing</strong> (<strong class="calibre2">NLP</strong>), and they are viable for a linear regression model.</p></div><p class="calibre8">The Scikit-learn package offers a specialized class for automatically transforming text into vectors of binary variables; this is the <code class="email">CountVectorizer class</code>. It allows the transformation of a list or array of textual data into a sparse matrix. Setting the <code class="email">binary</code> parameter to <code class="email">True</code>, when transforming the data with just binary encoding, represents the sparse matrix as a set of unit values in correspondence to the texts where a word is present. As a simple example, we can encode this series of texts:</p><div><pre class="programlisting">
<strong class="calibre2">In: corpus = ['The quick fox jumped over the lazy dog', 'I sought a dog wondering around with a bird', 'My dog is named Fido']</strong>
</pre></div><p class="calibre8">The only common word in the corpus (the term for a collection of documents subject to a linguistic <a id="id418" class="calibre1"/>analysis, so it is common to have a bilingual corpus or <a id="id419" class="calibre1"/>even a more heterogeneous one) is <code class="email">'dog'</code>. This should be reflected in our matrix; in fact, just a single column always has the unit value:</p><div><pre class="programlisting">
<strong class="calibre2">In: from sklearn.feature_extraction.text import CountVectorizer</strong>
<strong class="calibre2">textual_one_hot_encoder = CountVectorizer(binary=True)</strong>
<strong class="calibre2">textual_one_hot_encoder.fit(corpus)</strong>
<strong class="calibre2">vectorized_text = textual_one_hot_encoder.transform(corpus)</strong>
<strong class="calibre2">print(vectorized_text.todense())</strong>

<strong class="calibre2">Out: [[0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0]</strong>
<strong class="calibre2">      [1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1]</strong>
<strong class="calibre2">      [0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0]]</strong>
</pre></div><p class="calibre8">To visualize the resulting matrix as an output, which would otherwise just be made up of the coordinates where the unit values are in the matrix, we need to make the resulting sparse matrix dense using the <code class="email">.todense()</code> method.</p><div><h3 class="title2"><a id="tip23" class="calibre1"/>Tip</h3><p class="calibre8">Being a toy dataset, such transformation won't imply much in our example. Beware of doing the same when your corpus is large because that could cause an out-of-memory error on your system.</p></div><p class="calibre8">We notice that the third column has three units, so we imagine that it could represent the word <code class="email">'dog'</code>. We can verify that by requiring a list representing the dictionary and the positional arrangement of words using the <code class="email">.get_feature_names()</code> method:</p><div><pre class="programlisting">
<strong class="calibre2">In: print (textual_one_hot_encoder.get_feature_names())</strong>

<strong class="calibre2">Out: ['around', 'bird', 'dog', 'fido', 'fox', 'is', 'jumped', 'lazy', 'my', 'named', 'over', 'quick', 'sought', 'the', 'with', 'wondering']</strong>
</pre></div><p class="calibre8">Leveraging the ability to quickly build dictionaries of words, you can transform and use it even for the prediction of text.</p><p class="calibre8">The only trouble you can incur using such a representation is when you encounter a word never seen before. Let's see what happens:</p><div><pre class="programlisting">
<strong class="calibre2">In: print (textual_one_hot_encoder.transform(['John went home today']).todense())</strong>

<strong class="calibre2">Out: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]</strong>
</pre></div><p class="calibre8">Such behavior should actually have been expected. Since no word of the phrase has been encountered before, it doesn't have any space to fit in the vectorized text.</p><p class="calibre8">A quick and working solution would be to define a very large sparse vector (which until filled with data won't occupy much space, no matter the dimensions) and to use the specific <a id="id420" class="calibre1"/>characteristics of hash functions to deterministically<a id="id421" class="calibre1"/> assign a position to every word in the vector, without having the need to observe the word itself before the assignment. This is also called the <a id="id422" class="calibre1"/>
<strong class="calibre2">hashing trick</strong> and can be applied using the Scikit-learn <code class="email">HashingVectorizers</code>.</p><div><pre class="programlisting">
<strong class="calibre2">In: from sklearn.feature_extraction.text import HashingVectorizer</strong>
<strong class="calibre2">    hashing_trick = HashingVectorizer(n_features=11, binary=True, \norm=None, non_negative=True)</strong>
<strong class="calibre2">    M = hashing_trick.transform(corpus)</strong>
<strong class="calibre2">    print (M.todense())</strong>

<strong class="calibre2">Out: [[ 1.  0.  0.  1.  1.  0.  0.  1.  0.  0.  0.]</strong>
<strong class="calibre2">      [ 0.  0.  0.  1.  0.  1.  0.  1.  1.  0.  0.]</strong>
<strong class="calibre2">      [ 0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.]]</strong>
</pre></div><p class="calibre8">The <code class="email">HashingVectorizer</code> class has quite a few options for you to explore, especially for text treatment, such as allowing more sophisticated tokenizing (even custom ones), the removal of common words, stripping accents, and the parsing of different encodings.</p><p class="calibre8">In a replica of what we have done before with <code class="email">CountVectorizer</code>, we fixed an output vector of 11 elements. By doing so, we can notice and discuss two relevant characteristics of the preceding output.</p><p class="calibre8">First, clearly the position of the words is different (it depends on the hash function), and we cannot get back a dictionary of what word is in what position (but we can be confident that the hashing function has done its job properly). Now, we have no fear of vectorizing any previously unseen new text:</p><div><pre class="programlisting">
<strong class="calibre2">In: print (hashing_trick.transform(['John is the owner of that dog']).todense())</strong>

<strong class="calibre2">Out: [[1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]</strong>
</pre></div><p class="calibre8">The second thing is that, by observing well the distributions of unit values in the matrix, you have values concentrating on certain positions, whereas others are left empty. This is due to <em class="calibre9">the collision problem</em> with hash functions when bounded in a limited number of positions (actually we set the <code class="email">n_features</code> parameter to 11 for ease of understanding, though in a real analysis it is good practice to set it to higher figures).</p><div><h3 class="title2"><a id="tip24" class="calibre1"/>Tip</h3><p class="calibre8">To avoid any<a id="id423" class="calibre1"/> unwanted collision, a good <code class="email">n_features</code> value<a id="id424" class="calibre1"/> is between <em class="calibre9">2**21</em> and <em class="calibre9">2**24</em>, depending on the expected variety of text (the more variety, the larger the vector should be).</p></div></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec31" class="calibre1"/>Numeric feature transformation</h1></div></div></div><p class="calibre8">Numeric features <a id="id425" class="calibre1"/>can be transformed, regardless of the target variable. This is often a prerequisite for better performance of certain classifiers, particularly distance-based. We usually avoid ( besides specific cases such as when modeling a percentage or distributions with long queues) transforming the target, since we will make any pre-existent linear relationship between the target and other features non-linear.</p><p class="calibre8">We will keep on working on the Boston Housing dataset:</p><div><pre class="programlisting">
<strong class="calibre2">In: import numpy as np</strong>
<strong class="calibre2">  boston = load_boston()</strong>
<strong class="calibre2">  labels = boston.feature_names</strong>
<strong class="calibre2">  X = boston.data</strong>
<strong class="calibre2">  y = boston.target</strong>
<strong class="calibre2">  print (boston.feature_names)</strong>

<strong class="calibre2">Out: ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' \'RAD' 'TAX' 'PTRATIO' 'B' 'LSTAT']</strong>
</pre></div><p class="calibre8">As before, we fit the model using <code class="email">LinearRegression</code> from Scikit-learn, this time measuring its R-squared value using the <code class="email">r2_score</code> function from the <code class="email">metrics</code> module:</p><div><pre class="programlisting">
<strong class="calibre2">In: linear_regression = \linear_model.LinearRegression(fit_intercept=True)</strong>
<strong class="calibre2">  linear_regression.fit(X, y)</strong>

<strong class="calibre2">  from sklearn.metrics import r2_score</strong>
<strong class="calibre2">  print ("R-squared: %0.3f" % r2_score(y, \linear_regression.predict(X)))</strong>

<strong class="calibre2">Out: R-squared: 0.741</strong>
</pre></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec59" class="calibre1"/>Observing residuals</h2></div></div></div><p class="calibre8">Residuals are <a id="id426" class="calibre1"/>what's left from the original response when the<a id="id427" class="calibre1"/> predicted value is removed. It is numeric information telling us what the linear model wasn't able to grasp and predict by its set of coefficients and intercepts.</p><p class="calibre8">Obtaining residuals when working with Scikit-learn requires just one operation:</p><div><pre class="programlisting">
<strong class="calibre2">In: residuals = y - linear_regression.predict(X)</strong>
<strong class="calibre2">  print ("Head of residual %s" % residuals[:5])</strong>
<strong class="calibre2">  print ("Mean of residuals: %0.3f" % np.mean(residuals))</strong>
<strong class="calibre2">  print ("Standard deviation of residuals: %0.3f" \% np.std(residuals))</strong>

<strong class="calibre2">Out: Head of residual [-6.00821 -3.42986  4.12977  4.79186  8.25712]</strong>
<strong class="calibre2">    Mean of residuals: 0.000</strong>
<strong class="calibre2">    Standard deviation of residuals: 4.680</strong>
</pre></div><p class="calibre8">The residuals <a id="id428" class="calibre1"/>of a linear regression always have mean zero and their <a id="id429" class="calibre1"/>standard deviation depends on the size of the error produced. Residuals can provide insight on an unusual observation and non-linearity because, after telling us about what's left, they can direct us to specific troublesome data points or puzzling patterns in data.</p><p class="calibre8">For the specific problem of detecting non-linearity, we are going to use a plot based on residuals called<a id="id430" class="calibre1"/> the <strong class="calibre2">partial residual plot</strong>. In this plot, we compare the regression residuals summed with the values derived from the modeled coefficient of a variable against the original values of the variable itself:</p><div><pre class="programlisting">
<strong class="calibre2">In: var = 7 # the variable in position 7 is DIS</strong>
<strong class="calibre2">  partial_residual = residuals + X[:,var] * \linear_regression.coef_[var]</strong>
<strong class="calibre2">  plt.plot(X[:,var], partial_residual, 'wo')</strong>
<strong class="calibre2">  plt.xlabel(boston.feature_names[var])</strong>
<strong class="calibre2">  plt.ylabel('partial residuals')</strong>
<strong class="calibre2">  plt.show()</strong>
<strong class="calibre2">Out: </strong>
</pre></div><div><img src="img/00099.jpeg" alt="Observing residuals" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">After having calculated the residual of the regression, we decide to inspect one variable at a time. After picking up our selected variable, we create a partial residual by summing the residuals of the regression with the multiplication of the variable values multiplied by its coefficient. In such a way, we <em class="calibre9">extract</em> the variable from the regression line and we put it in the residuals. Now, as partial residuals, we have both the errors and the coefficient-weighted variable. If we plot it against the variable itself, we can notice whether there is any non-linear <a id="id431" class="calibre1"/>pattern. If there is one, we know that we should<a id="id432" class="calibre1"/> try some modification.</p><p class="calibre8">In our case, there is some sign that the points bend after the value 2 of our variable, a clear non-linearity sign such as any bend or pattern different from an elongated, straight cloud of points. Square, inverse, logarithmic transformations can often solve such problems without adding new terms, such as when using the polynomial expansion:</p><div><pre class="programlisting">
<strong class="calibre2">In: X_t = X.copy()</strong>
<strong class="calibre2">  X_t[:,var] = 1./np.sqrt(X_t[:,var])</strong>
<strong class="calibre2">  linear_regression.fit(X_t, y)</strong>
<strong class="calibre2">  partial_residual = residuals + X_t[:,var] * \linear_regression.coef_[var]</strong>
<strong class="calibre2">  plt.plot(X_t[:,var], partial_residual, 'wo')</strong>
<strong class="calibre2">  plt.xlabel(boston.feature_names[var])</strong>
<strong class="calibre2">  plt.ylabel('partial residuals')</strong>
<strong class="calibre2">  plt.show()</strong>
<strong class="calibre2">  print ("R-squared: %0.3f" % r2_score(y, \linear_regression.predict(X_t)))</strong>

<strong class="calibre2">Out: R-squared: 0.769</strong>
</pre></div><div><img src="img/00100.jpeg" alt="Observing residuals" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Just notice how an inverse square transformation rendered the partial residual plot straighter, something that is reflected in a higher R-squared value, indicating an increased capacity of the model to capture the data distribution.</p><p class="calibre8">As a rule, the <a id="id433" class="calibre1"/>following transformations should always<a id="id434" class="calibre1"/> be tried (singularly or in combination) to find a fix for a non-linearity:</p><div><table border="1" class="blockquote1"><colgroup class="calibre23"><col class="calibre24"/><col class="calibre24"/></colgroup><thead class="calibre25"><tr class="calibre26"><th valign="bottom" class="calibre27">
<p class="calibre14">Function names</p>
</th><th valign="bottom" class="calibre27">
<p class="calibre14">Functions</p>
</th></tr></thead><tbody class="calibre28"><tr class="calibre26"><td class="calibre13">
<p class="calibre14">Logarithmic</p>
</td><td class="calibre13">
<p class="calibre14"><code class="literal">np.log(x)</code></p>
</td></tr><tr class="calibre26"><td class="calibre13">
<p class="calibre14">Exponential</p>
</td><td class="calibre13">
<p class="calibre14"><code class="literal">np.exp(x)</code></p>
</td></tr><tr class="calibre26"><td class="calibre13">
<p class="calibre14">Squared</p>
</td><td class="calibre13">
<p class="calibre14"><code class="literal">x**2</code></p>
</td></tr><tr class="calibre26"><td class="calibre13">
<p class="calibre14">Cubed</p>
</td><td class="calibre13">
<p class="calibre14"><code class="literal">x**3</code></p>
</td></tr><tr class="calibre26"><td class="calibre13">
<p class="calibre14">Square root</p>
</td><td class="calibre13">
<p class="calibre14"><code class="literal">np.sqrt(x)</code></p>
</td></tr><tr class="calibre26"><td class="calibre13">
<p class="calibre14">Cube root</p>
</td><td class="calibre13">
<p class="calibre14"><code class="literal">x**(1./3.)</code></p>
</td></tr><tr class="calibre26"><td class="calibre13">
<p class="calibre14">Inverse</p>
</td><td class="calibre13">
<p class="calibre14"><code class="literal">1. / x</code></p>
</td></tr></tbody></table></div><div><h3 class="title2"><a id="tip25" class="calibre1"/>Tip</h3><p class="calibre8">Some of the transformations suggested in the preceding table won't work properly after normalization or otherwise in the presence of zero and negative values: logarithmic transformation needs positive values above zero, square root won't work with negative values, and inverse transformation won't operate with zero values. Sometimes adding a constant may help (like in the case of <code class="email">np.log(x+1)</code>). Generally, just try the possible transformations, according to your data values.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec60" class="calibre1"/>Summarizations by binning</h2></div></div></div><p class="calibre8">When it is not <a id="id435" class="calibre1"/>easy to figure out the exact transformation, a <a id="id436" class="calibre1"/>quick solution could be to transform the<a id="id437" class="calibre1"/> continuous numeric variable into a series of binary variables, thus allowing the estimation of a coefficient for each single part of the numeric range of the variable.</p><p class="calibre8">Though fast and convenient, this solution will increase the size of your dataset (unless you use a sparse representation of the matrix) and it will risk too much overfitting on your data.</p><p class="calibre8">First, you divide your values into equally spaced bins and you notice the edges of the bins using the <code class="email">histogram</code> function from Numpy. After that, using the <code class="email">digitize</code> function, you convert the value in their bin number, based on the bin boundaries provided before. Finally, you can transform all the bin numbers into binary variables using the previously present <code class="email">LabelBinarizer</code> from Scikit-learn.</p><p class="calibre8">At this point, all <a id="id438" class="calibre1"/>you have to do is replace the <a id="id439" class="calibre1"/>previous variable with this new set of binary indicators <a id="id440" class="calibre1"/>and refit the model for checking the improvement:</p><div><pre class="programlisting">
<strong class="calibre2">In: import numpy as np</strong>
<strong class="calibre2">  from sklearn.preprocessing import LabelBinarizer</strong>
<strong class="calibre2">  LB = LabelBinarizer()</strong>
<strong class="calibre2">  X_t = X.copy()</strong>
<strong class="calibre2">  edges = np.histogram(X_t[:,var], bins=20)[1]</strong>
<strong class="calibre2">  binning = np.digitize(X_t[:,var], edges)</strong>
<strong class="calibre2">  X_t = np.column_stack((np.delete(X_t, var, \axis=1),LB.fit_transform(binning)))</strong>
<strong class="calibre2">  linear_regression.fit(X_t, y)</strong>
<strong class="calibre2">  print ("R-squared: %0.3f" % r2_score(y, \linear_regression.predict(X_t)))</strong>

<strong class="calibre2">Out: R-squared: 0.768</strong>
</pre></div></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec32" class="calibre1"/>Missing data</h1></div></div></div><p class="calibre8">Missing data appears often in real-life data, sometimes randomly in random occurrences, more often because of some bias in its recording and treatment. All linear models work on complete numeric matrices and cannot deal directly with such problems; consequently, it is up to you to take <a id="id441" class="calibre1"/>care of feeding suitable data for the algorithm to process.</p><p class="calibre8">Even if your initial dataset does not present any missing data, it is still possible to encounter missing values in the production phase. In such a case, the best strategy is surely that of dealing with them passively, as presented at the beginning of the chapter, by standardizing all the numeric variables.</p><div><h3 class="title2"><a id="tip26" class="calibre1"/>Tip</h3><p class="calibre8">As for as indicator variables, in order to passively intercept missing values, a possible strategy is instead to encode the presence of the label as <code class="email">1</code> and its absence as <code class="email">-1</code>, leaving the zero value for missing values.</p></div><p class="calibre8">When missing values are present from the beginning of the project, it is certainly better to deal with them explicitly, trying to figure out if there is any systematic pattern behind missing values. In Python arrays, upon which both the Pandas and Scikit-learn packages are built, missing values are marked by a <a id="id442" class="calibre1"/>special value, <strong class="calibre2">Not a Number</strong> (<strong class="calibre2">NaN</strong>), which is repeatable using the value available from the NumPy constant <code class="email">nan</code>.</p><p class="calibre8">Creating a toy array with a missing value is easy:</p><div><pre class="programlisting">
<strong class="calibre2">In: import Numpy as np</strong>
<strong class="calibre2">  example = np.array([1,2,np.nan,4,5])</strong>
<strong class="calibre2">  print (example)</strong>

<strong class="calibre2">Out: [  1.   2.  nan   4.   5.]</strong>
</pre></div><p class="calibre8">Also discovering<a id="id443" class="calibre1"/> where missing values are in a vector (the result is a vector of Booleans):</p><div><pre class="programlisting">
<strong class="calibre2">In: print (np.isnan(example))</strong>

<strong class="calibre2">Out: [False False  True False False]</strong>
</pre></div><p class="calibre8">Replacing all missing elements can be done easily by slicing or by the function <code class="email">nan_to_num</code>, which turns every <code class="email">nan</code> to zero:</p><div><pre class="programlisting">
<strong class="calibre2">In: print (np.nan_to_num(example))</strong>

<strong class="calibre2">Out: [ 1.  2.  0.  4.  5.]</strong>
</pre></div><p class="calibre8">Using slicing, you could decide to use something more sophisticated than a constant, such as the mean of valid elements in the vector:</p><div><pre class="programlisting">
<strong class="calibre2">In: missing = np.isnan(example)</strong>
<strong class="calibre2">  replacing_value = np.mean(example[~missing])</strong>
<strong class="calibre2">  example[missing] = replacing_value</strong>
<strong class="calibre2">  print (example)</strong>

<strong class="calibre2">Out: [ 1.  2.  3.  4.  5.]</strong>
</pre></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec61" class="calibre1"/>Missing data imputation</h2></div></div></div><p class="calibre8">Consistency <a id="id444" class="calibre1"/>of treatment between samples of data is essential when working with predictive models. If you replace the missing values with a certain constant or a particular mean, that should be consistent during both the training and the production phase. The Scikit-learn package offers the <code class="email">Imputer</code> class in the <code class="email">preprocessing</code> module which can learn a solution by the <code class="email">fit</code> method and then consistently apply it by the <code class="email">transform</code> one.</p><p class="calibre8">Let's start demonstrating it after putting some missing values in the Boston dataset:</p><div><pre class="programlisting">
<strong class="calibre2">In: from random import sample, seed</strong>
<strong class="calibre2">  import numpy as np</strong>
<strong class="calibre2">  seed(19)</strong>
<strong class="calibre2">  Xm = X.copy()</strong>
<strong class="calibre2">  missing = sample(range(len(y)), len(y)//4)</strong>
<strong class="calibre2">  Xm[missing,5] = np.nan</strong>
<strong class="calibre2">  print ("Header of Xm[:,5] : %s" % Xm[:10,5])</strong>

<strong class="calibre2">Out: Header of Xm[:,5] : [ 6.575    nan  7.185    nan  7.147  6.43   6.012  6.172    nan  6.004]</strong>
</pre></div><div><h3 class="title2"><a id="tip27" class="calibre1"/>Tip</h3><p class="calibre8">It is quite unlikely that you will get the same result due to the random nature of the sampling process. Please notice that the exercise sets a seed so you can count on the same results on your PC.</p></div><p class="calibre8">Now about a<a id="id445" class="calibre1"/> quarter of observations in the variable should be missing. Let's use <code class="email">Imputer</code> to replace them using a mean:</p><div><pre class="programlisting">
<strong class="calibre2">In: from sklearn.preprocessing import Imputer</strong>
<strong class="calibre2">  impute = Imputer(missing_values = 'NaN', strategy='mean', axis=1)</strong>
<strong class="calibre2">  print ("Header of imputed Xm[:,5] : %s" % \impute.fit_transform(Xm[:,5])[0][:10])</strong>

<strong class="calibre2">Out: Header of imputed Xm[:,5] : [ 6.575    6.25446  7.185    6.25446  7.147    6.43     6.012    6.172  6.25446  6.004  ]</strong>
</pre></div><p class="calibre8">Imputer allows you to define any value as missing (sometimes in a re-elaborated dataset missing values could be encoded with negative values or other extreme values) and to choose alternative strategies<a id="id446" class="calibre1"/> rather than the mean. Other alternatives<a id="id447" class="calibre1"/> are <strong class="calibre2">median</strong> and <strong class="calibre2">mode</strong>. The median is useful if you suspect that outlying values are influencing and biasing the average (in house prices, some very expensive and exclusive houses or areas could be the reason). Mode, the most frequent value, is instead the optimal choice if you are working with discrete values (for instance a sequence of integer values with a limited range).</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec62" class="calibre1"/>Keeping track of missing values</h2></div></div></div><p class="calibre8">If you suspect that there is some bias in the missing value pattern, by imputing them you will lose <a id="id448" class="calibre1"/>any trace of it. Before imputing, a good practice is to create a binary variable recording where all missing values were and to add it as a feature to the dataset. As seen before, it is quite easy using NumPy to create such a new feature, transforming the Boolean vector created by <code class="email">isnan</code> into a vector of integers:</p><div><pre class="programlisting">
<strong class="calibre2">In: missing_indicator = np.isnan(Xm[:,5]).astype(int)</strong>
<strong class="calibre2">  print ("Header of missing indicator : %s" \% missing_indicator[:10])</strong>

<strong class="calibre2">Out: Header of missing indicator : [0 1 1 0 0 0 0 0 1 1]</strong>
</pre></div><p class="calibre8">The linear<a id="id449" class="calibre1"/> regression model will create a coefficient for this indicator of missing values and, if any pattern exists, its informative value will be captured by a coefficient.</p></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec33" class="calibre1"/>Outliers</h1></div></div></div><p class="calibre8">After properly transforming all the quantitative and qualitative variables and fixing any missing data, what's left is just to detect any possible outlier and to deal with it by removing it from<a id="id450" class="calibre1"/> the data or by imputing it as if it were a missing case.</p><p class="calibre8">An outlier, sometimes also referred to as an anomaly, is an observation that is very different from all the others you have observed so far. It can be viewed as an unusual case that stands out, and it could pop up due to a mistake (an erroneous value completely out of scale) or simply a value that occurred (rarely, but it occurred). Though understanding the origin of an outlier could help to fix the problem in the most appropriate way (an error could be legitimately removed; a rare case could be kept or capped or even imputed as a missing case), what is of utmost concern is the effect of one or more outliers on your regression analysis results. Any anomalous data in a regression analysis means a distortion of the regression's coefficients and a limit on the ability of the model to correctly predict usual cases.</p><div><h3 class="title2"><a id="tip28" class="calibre1"/>Tip</h3><p class="calibre8">Despite the importance of controlling outliers, unfortunately practitioners often overlook this activity because, in contrast to the other preparations illustrated throughout the chapter, omitting to detect outliers won't stop the analysis you are working on and you will get your regression coefficients and results (both probably quite inexact). However, having an analysis run smoothly to the end doesn't mean that everything is fine with the analysis itself. An outlier can distort an analysis in two ways depending on whether the anomalous value is on the target variable or on the predictors.</p></div><p class="calibre8">In order to detect outliers, there are a few approaches, some based on the observation of variables taken singularly (the single-variable, or univariate, approach), and some based on reworking all the variables together into a synthetic measure (the multivariate approach).</p><p class="calibre8">The best single variable approach is based on the observation of standardized variables and on the plotting of box plots:</p><div><ul class="itemizedlist"><li class="listitem">Using standardized variables, everything scoring further than the absolute value of three standard deviations from the mean is suspect, though such a rule of thumb doesn't generalize well if the distribution is not normal</li><li class="listitem">Using boxplots, the <strong class="calibre2">interquartile range</strong> (shortened to <strong class="calibre2">IQR</strong>; it is the difference<a id="id451" class="calibre1"/> between the values at the 75<sup class="calibre21">th</sup> and the 25<sup class="calibre21">th</sup> percentile) is used to detect suspect outliers beyond the 75<sup class="calibre21">th</sup> and 25<sup class="calibre21">th</sup> percentiles. If there are examples whose values are outside the IQR, they can be considered suspicious, especially if their value is beyond 1.5 times the IQR's boundary value. If they exceed 3 times the IQR's limit, they are almost certainly outliers.</li></ul></div><div><h3 class="title2"><a id="tip29" class="calibre1"/>Tip</h3><p class="calibre8">The Scikit-learn package offers a couple of classes for automatically detecting outliers using sophisticated approaches: <code class="email">EllipticEnvelope</code> and <code class="email">OneClassSVM</code>. Though a treatise of both these complex algorithms is out of scope here, if outliers or unusual data is the main problem with your data, we suggest having a look at this web page for some quick recipes you can adopt in your scripts: <a class="calibre1" href="http://scikit-learn.org/stable/modules/outlier_detection.html">http://scikit-learn.org/stable/modules/outlier_detection.html</a>. Otherwise, you could always read our previous book <em class="calibre9">Python Data Science Essentials</em>, <em class="calibre9">Alberto Boschetti</em> and <em class="calibre9">Luca Massaron</em>, <em class="calibre9">Packt Publishing</em>.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec63" class="calibre1"/>Outliers on the response</h2></div></div></div><p class="calibre8">The first <a id="id452" class="calibre1"/>step in looking for outliers is to check the response variable. In observing plots of the variable distribution and of the residuals of the regression, it is important to check if there are values that, because of a too high or too low value, are out of the main distribution.</p><p class="calibre8">Usually, unless accompanied by outlying predictors, outliers in the response have little impact on the estimated coefficients; however, from a statistical point of view, since they affect the amount of the root-squared error, they reduce the explained variance (the squared r) and inflate the standard errors of the estimate. Both such effects represent a problem when your approach is a statistical one, whereas they are of little concern for data science purposes.</p><p class="calibre8">To figure out which responses are outliers, we should first monitor the target distribution. We start by recalling the Boston dataset:</p><div><pre class="programlisting">
<strong class="calibre2">In: boston = load_boston()</strong>
<strong class="calibre2">  dataset = pd.DataFrame(boston.data, columns=boston.feature_names)</strong>
<strong class="calibre2">  labels = boston.feature_names</strong>
<strong class="calibre2">  X = dataset</strong>
<strong class="calibre2">  y = boston.target</strong>
</pre></div><p class="calibre8">A <code class="email">boxplot</code> function can hint at any outlying values in the target variable:</p><div><pre class="programlisting">
<strong class="calibre2">In: plt.boxplot(y,labels=('y'))</strong>
<strong class="calibre2">plt.show()</strong>
</pre></div><div><img src="img/00101.jpeg" alt="Outliers on the response" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The box display and its whiskers tell us that quite a few values are out of the IQR, so they are suspect<a id="id453" class="calibre1"/> ones. We also notice a certain concentration at the value 50; in fact the values are capped at 50.</p><p class="calibre8">At this point, we can try to build our regression model and inspect the resulting residuals. We will standardize them using the Root Mean Squared error. An easy approach to implement though it is not the most precise, it is still enough good to reveal any significant problem:</p><div><pre class="programlisting">
<strong class="calibre2">In: scatter = plt.plot(linear_regression.predict(X), \standardized_residuals, 'wo')</strong>
<strong class="calibre2">  plt.plot([-10,50],[0,0], "r-")</strong>
<strong class="calibre2">  plt.plot([-10,50],[3,3], "r--")</strong>
<strong class="calibre2">  plt.plot([-10,50],[-3,-3], "r--")</strong>
<strong class="calibre2">  plt.xlabel('fitted values')</strong>
<strong class="calibre2">  plt.ylabel('standardized residuals')</strong>
<strong class="calibre2">  plt.show()</strong>
</pre></div><div><img src="img/00102.jpeg" alt="Outliers on the response" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Making a<a id="id454" class="calibre1"/> scatterplot of the values fitted by the regression against the standardized residuals, we notice there are a few outlying cases over three standard deviations from the zero mean. The capped values especially, clearly visible in the graph as a line of points, seem problematic.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec64" class="calibre1"/>Outliers among the predictors</h2></div></div></div><p class="calibre8">As we inspected the<a id="id455" class="calibre1"/> target variable, it is now time to have a look also at the predictors. If unusual observations were outliers in the target variable, similar cases in the predictors are instead named influential or high leverage observations<a id="id456" class="calibre1"/> because they can really make an impact on more than the <strong class="calibre2">sum of squared errors</strong> (<strong class="calibre2">SSE</strong>), this time influencing coefficients and the intercept—in a word, the entire regression solution (that's why they are so important to catch).</p><p class="calibre8">After standardizing, we start having a look at the distributions using boxplots:</p><div><pre class="programlisting">
<strong class="calibre2">In: standardization = StandardScaler(with_mean=True, with_std=True)</strong>
<strong class="calibre2">  Xs = standardization.fit_transform(X)</strong>
<strong class="calibre2">  boxplot = plt.boxplot(Xs[:,0:7],labels=labels[0:7])</strong>
</pre></div><div><img src="img/00103.jpeg" alt="Outliers among the predictors" class="calibre10"/></div><p class="calibre11"> </p><div><pre class="programlisting">
<strong class="calibre2">In: boxplot = plt.boxplot(Xs[:,7:13],labels=labels[7:13])</strong>
</pre></div><div><img src="img/00104.jpeg" alt="Outliers among the predictors" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">After <a id="id457" class="calibre1"/>observing all the boxplots, we can conclude that there are variables with restricted variance, such as <strong class="calibre2">B</strong>, <strong class="calibre2">ZN</strong>, and <strong class="calibre2">CRIM</strong>, which are characterized by a long tail of values. There are also some suspect cases from <strong class="calibre2">DIS</strong> and <strong class="calibre2">LSTAT</strong>. We can delimit all these cases by looking for the values above the represented thresholds, variable after variable, but it would be helpful to catch all of them at once.</p><p class="calibre8"><strong class="calibre2">Principal Component Analysis</strong> (<strong class="calibre2">PCA</strong>) is a technique that can reduce complex datasets into fewer <a id="id458" class="calibre1"/>dimensions, the summation of the original variables of the dataset. Without delving too much into the technicalities of the algorithm, you just need to know that the new dimensions produced by the algorithm have decreasing explicatory power; consequently, plotting the top ones against each other is just like plotting all the dataset's information. By glancing at such synthetic representations, you can spot groups and isolated points that, if very far from the center of the graph, are also quite influential on the<a id="id459" class="calibre1"/> regression model.</p><div><pre class="programlisting">
<strong class="calibre2">In: from sklearn.decomposition import PCA</strong>
<strong class="calibre2">  pca = PCA()</strong>
<strong class="calibre2">  pca.fit(Xs)</strong>
<strong class="calibre2">  C = pca.transform(Xs)</strong>
<strong class="calibre2">  print (pca.explained_variance_ratio_)</strong>

<strong class="calibre2">Out: [ 0.47097  0.11016  0.09547  0.06598  0.0642   0.05074 \0.04146  0.0305	0.02134  0.01694  0.01432  0.01301  0.00489]</strong>

<strong class="calibre2">In: import numpy as np</strong>
<strong class="calibre2">  import matplotlib.pyplot as plt</strong>
<strong class="calibre2">  explained_variance = pca.explained_variance_ratio_</strong>
<strong class="calibre2">  plt.title('Portion of explained variance by component')</strong>
<strong class="calibre2">  range_ = [r+1 for r in range(len(explained_variance))]</strong>
<strong class="calibre2">  plt.bar(range_,explained_variance, color="b", alpha=0.4, \align="center")</strong>
<strong class="calibre2">  plt.plot(range_,explained_variance,'ro-')</strong>
<strong class="calibre2">  for pos, pct in enumerate(explained_variance):</strong>
<strong class="calibre2">    plt.annotate(str(round(pct,2)), (pos+1,pct+0.007))</strong>
<strong class="calibre2">  plt.xticks(range_)</strong>
<strong class="calibre2">  plt.show()</strong>

<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00105.jpeg" alt="Outliers among the predictors" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The first dimension created by PCA can explain 47% of the dataset's information, the second and the third 11% and 9.5%, respectively (the <code class="email">explained_variance_ratio_</code> method can provide you with such information). Now all we have to do is to plot the first dimension against the second and the third and look for lonely points away from the center<a id="id460" class="calibre1"/> because those are our high leverage cases to be investigated:</p><div><pre class="programlisting">
<strong class="calibre2">In: scatter = plt.scatter(C[:,0],C[:,1], facecolors='none', \edgecolors='black')</strong>
<strong class="calibre2">  plt.xlabel('Dimension 1')</strong>
<strong class="calibre2">  plt.ylabel('Dimension 2')</strong>
</pre></div><div><img src="img/00106.jpeg" alt="Outliers among the predictors" class="calibre10"/></div><p class="calibre11"> </p><div><pre class="programlisting">
<strong class="calibre2">In: scatter = plt.scatter(C[:,0],C[:,2], facecolors='none', \edgecolors='black')</strong>
<strong class="calibre2">  plt.xlabel('Dimension 1')</strong>
<strong class="calibre2">  plt.ylabel('Dimension 3')</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00107.jpeg" alt="Outliers among the predictors" class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec65" class="calibre1"/>Removing or replacing outliers</h2></div></div></div><p class="calibre8">After being able to<a id="id461" class="calibre1"/> detect outliers and influential observations, we just need to <a id="id462" class="calibre1"/>discuss what we can do with them. You might believe it's OK just to delete them but, on the contrary, removing or replacing an outlier is something to consider carefully.</p><p class="calibre8">In fact, outlying observations may be justified by three reasons (their remedies change accordingly):</p><div><ul class="itemizedlist"><li class="listitem">They are outliers because they are rare occurrences, so they appear unusual with regard to other observations. If this is the case, removing the data points could not be the correct solution because the points are part of the distribution you want to model and they stand out just because of chance. The best solution would be to increase the sample number. If augmenting your sample size is not possible, then remove them or try to resample in order to avoid having them drawn.</li><li class="listitem">Some errors have happened in the data processing and the outlying observations are from another distribution (some data has been mixed, maybe from different times or another geographical context). In this case, prompt removal is called for.</li><li class="listitem">The value is a mistake due to faulty input or processing. In such an occurrence, the value has to be considered as missing and you should perform an imputation of the now missing value to get a reasonable value.</li></ul></div><div><h3 class="title2"><a id="tip30" class="calibre1"/>Tip</h3><p class="calibre8">As a rule, just keep in mind that removing data points is necessary only when the points are different from the data you want to use for prediction and when, by removing them, you get direct confirmation that they had a lot of influence on the coefficients or on the intercept of the regression model. In all other cases, avoid any<a id="id463" class="calibre1"/> kind of selection in order to improve the model since<a id="id464" class="calibre1"/> it is a form of data snooping (more on the topic of how data snooping can negatively affect your models in the next chapter).</p></div></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec34" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we have dealt with many different problems that you may encounter when preparing your data to be analyzed by a linear model.</p><p class="calibre8">We started by discussing rescaling variables and understanding how new variables' scales not only permit a better insight into the data, but also help us deal with unexpectedly missing data.</p><p class="calibre8">Then, we learned how to encode qualitative variables and deal with the extreme variety of possible levels with unpredictable variables and textual information just by using the hashing trick. We then returned to quantitative variables and learned how to transform in a linear shape and obtain better regression models.</p><p class="calibre8">Finally, we dealt with some possible data pathologies, missing and outlying values, showing a few quick fixes that, in spite of their simplicity, are extremely effective and performant.</p><p class="calibre8">At this point, before proceeding to more sophisticated linear models, we just need to illustrate the data science principles that can help you obtain really good working predictive engines and not just mere mathematical curve fitting exercises. And that's precisely the topic of the next chapter.</p></div></body></html>