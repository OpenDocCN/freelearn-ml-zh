- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Techniques for Data Cleaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover six key dimensions of data quality and their
    corresponding techniques to improve data quality, commonly known as techniques
    for cleaning data in machine learning. Simply put, data cleaning is the process
    of implementing techniques to improve data quality by fixing errors in data or
    removing erroneous data. As covered in *Chapters 1* and *2*, reducing errors in
    data is a highly efficient and effective way to improve model quality over using
    model-centric techniques such as adding more data and/or implementing complex
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, data cleaning techniques involve fixing or removing incorrect,
    incomplete, invalid, biased, inconsistent, stale, or corrupted data. As data is
    captured at multiple sources, due to different annotators following their judgment
    or due to poor system designs, combining these sources can often result in data
    being mislabeled, inconsistent, duplicated, or incomplete. As discovered in earlier
    chapters, incorrect data can make algorithms and outcomes unreliable. So, to achieve
    reliability in machine learning systems, it is important to help data scientists
    and data engineers question the data and systematically improve data quality using
    data cleaning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that will be covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The six key dimensions of data quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring data quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data cleaning techniques required to improve data quality across the six dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The six key dimensions of data quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are six key dimensions we can use to check the overall health of data.
    Ensuring good health across the data can ensure we can build reliable systems
    and make better decisions. For example, if 20% of survey data is duplicated, and
    the majority of the duplicates are filled by male candidates, we can imagine that
    the actions taken by decision-makers will favor the male candidates if data duplication
    is undetected. Hence, it’s important to understand the overall health of the data
    to make reliable and unbiased decisions. To measure data quality or look at the
    overall health of the data, we can break down data quality into the following
    dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency**: This refers to whether the same data is maintained across
    the rows for a given column or feature. An example of this could be whether the
    gender label for males is consistent or not. The label can take values of “1,”
    “Male,” “M”, or “male,” but if the data has multiple values to represent males,
    then an algorithm will treat each label individually, which can cause randomness
    and errors. Hence, the goal of ensuring consistency is to ensure labels are defined
    consistently across the whole dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uniqueness**: This refers to whether each record can be uniquely identified.
    If duplicate values enter the system, the model will become biased due to those
    records. For example, if one region has a high loan approval rate and due to system
    failure, records from this region are duplicated, the algorithm will be biased
    toward approving more loans from that region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Completeness**: This refers to whether data is complete across the rows for
    a given column or feature, and whether data is missing due to system errors or
    not captured, especially when that information will be used in the machine learning
    system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`semi_urban` might be invalid if one or a couple of annotators believed some
    suburbs are neither urban nor rural, and they violated the rules of data and entered
    `semi_urban`. This can introduce noise in the data, so it’s important to ensure
    data conforms to the business rules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy**: This refers to whether data was entered correctly in the first
    place and can be verified from an internal or external source. An example of this
    in a healthcare setting could be that if the date and time of admission are entered
    in a different time zone, then the analysis and insights would be inaccurate.
    If the time of admission is a predictor of quality of care, then misaligned time
    zones might lead to wrong conclusions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Freshness**: This refers to whether the data available is recent and up to
    date to meet data requirements. Some applications require data to be real-time
    – that is, updated every second – whereas other applications require data to be
    available once a month or every few months. What was true yesterday may change
    due to changes in factors such as regulation, weather conditions, trends, competition,
    business changes, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will install various Python packages, and then dive into fixing and
    measuring data quality. In each section, we dive deeper into different data cleaning
    techniques and how to improve the quality of data.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the required packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, we will need the following Python packages or libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` version 1.5.3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` version 1.22.4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn` version 1.2.1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jupyter` version 1.0.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alibi` version 0.9.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alibi-detect` version 0.10.4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seaborn` version 0.12.2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` version 3.6.3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`missingno` version 0.5.1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature-engine` version 1.5.2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will provide a brief introduction to the dataset and start exploring
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let’s introduce our problem statement. For loan providers, it is important
    to ensure that people who get a loan can make payment and don’t default. However,
    it is equally important that people are not denied a loan due to a model trained
    on poor-quality data. This is where the data-centric approach helps make the world
    a better place – it provides a framework for data scientists and data engineers
    to question the quality of data.
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, we will use the loan prediction dataset from Analytics Vidhya.
    You can download the dataset from [https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/#ProblemStatement](https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/#ProblemStatement).
    There are two files – one for training and one for testing. The test file doesn’t
    contain any labels. For this chapter, we will utilize the training file, which
    has been downloaded and saved as `train_loan_prediction.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will look at the dataset and check the first five rows. To do this,
    we must import the following necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will read the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will read the data using pandas’ `read_csv` method. Then, we will visualize
    the first five rows of the dataset using `.head()`. We can apply the `.T` method
    at the end of the `.head()` method if we have a large feature set. This will represent
    columns as rows and rows as columns, where column names will not exceed 5 as we
    want to visualize the first five rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – The first five rows of our df dataset](img/B19297_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – The first five rows of our df dataset
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, there are some inconsistencies across column names. All the columns
    follow the camel case convention, except `Loan_ID`, and long column names are
    separated by `_` except for `LoanAmount`, `CoapplicantIncome`, and `ApplicantIncome`.
    This indicates that the column names have inconsistent naming conventions. Within
    the data, we can also see that some columns have data in camel case but `Loan_ID`
    has all its values in uppercase. Within the `Education` column, the `Not Graduate`
    value is separated by a space. In machine learning, it’s important to ensure data
    is consistent; otherwise, models may produce inconsistent results. For instance,
    what happens if the `Gender` column has two distinct values for male customers
    – `Male` and `male`? If we don’t treat this, then our machine learning model will
    consider `male` data points separate from `Male`, and the model will not get an
    accurate signal.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will extract the list of column names from the data, make them all
    lowercase, and ensure words will be separated by a unique character, `_`. We will
    also go through the data values of categorical columns and make them all lowercase
    before replacing all the special characters with `_` and making our data consistent.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring the data is consistent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To ensure the data is consistent, we must check the names of the columns in
    the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we must get all the column names that don’t contain underscores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Since some columns have two uppercase letters in their names, we must add the
    underscore before the start of the second uppercase letter. Next, we create a
    Boolean mapping against the index of each letter of the column, where the location
    of capital letters will be mapped as `True` so that we can locate the index of
    the second capital letter and prefix it with an underscore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we iterate over the mappings and print the column names that will require
    an underscore, and also print the location of the second capital letter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this information, we build some logic for the `ApplicantIncome` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we combine the previous steps and iterate over columns that require an
    underscore and build a mapping. Then, we print the names of the columns that require
    underscores. Finally, we create a mapping of old column names and new column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we apply the column mappings to update the column names and print
    the new column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Although the preceding code could have simply been updated by manually choosing
    the columns, by doing this programmatically, we can ensure that data is following
    programmatic rules. To improve consistency, we make all column names lowercase.
    First, we create some simple one-line logic to convert column names into lowercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we update the column names by passing the preceding logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we have ensured that the column names are consistent. But more importantly,
    we must ensure that categorical values are consistent so that we can future-proof
    machine learning systems from inconsistent data. First, we extract the `id` and
    `target` columns and then identify the categorical columns. These columns contain
    non-numerical data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We iterate over each column and check the unique values to ensure the values
    are distinct and not misspelled. We also check that the same value is not represented
    differently, such as it being in a different case or being abbreviated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the data, it seems that values are distinct and not abbreviated
    or misspelled. But machine learning systems can be made future-proof. For instance,
    if we make all values lowercase, then in the future, if the same value comes with
    a different case, before entering the system, it will be made lowercase. We can
    also see that some strings, such as `Not Graduate`, take up space. Just like how
    we ensured consistency for column names, we must replace all white spaces with
    underscores. First, we create a new DataFrame named `df_consistent`; then, we
    make all categorical values lowercase and replace all spaces with underscores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With that, we have ensured that data is consistent and all values are converted
    into lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the `dependents` column contains numerical information. However,
    due to the presence of the `3+` value, the column values are encoded as strings.
    We must remove the special character and then encode this back to a numerical
    value since this column is ordinal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we look at the `married` and `self_employed` columns since these are
    binary and must be encoded to `1` and `0`. The `gender` column has two values
    and can be binary encoded as well – for example, we can encode `male` as `1` and
    `female` as `0`. The `education` column also has two values, and we can encode
    `graduate` as `1` and `not_graduate` as `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the data is consistent and has been encoded correctly, we must create
    a function for preprocessing data so that we can consistently process any future
    variations to categorical labels. Then, we apply the function to the DataFrame
    and print the values to ensure the function was applied correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We have now ensured that the data is consistent so that if categorical values
    have spaces instead of `_` or are entered with a different case in the future,
    we can use the functionality we created here to clean the data and make it consistent
    before it enters our model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore data uniqueness to ensure no duplicate records have been
    provided to create bias in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Checking that the data is unique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have ensured the data is consistent, we must also ensure it's unique,
    before it enters the machine learning system.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will investigate the data and check whether the values in
    the `loan_id` column are unique, as well as whether a combination of certain columns
    can ensure data is unique.
  prefs: []
  type: TYPE_NORMAL
- en: 'In pandas, we can utilize the `.nunique()` method to check the number of unique
    records for the column and compare it with the number of rows. First, we will
    check that `loan_id` is unique and that no duplicate applications have been entered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'With this, we have ensured that loan IDs are unique. However, we can go one
    step further to ensure that incorrect data is not added to another loan application.
    We believe it’s quite unlikely that a loan application will require more than
    one combination of income and loan amount. We must check that we can use a combination
    of column values to ensure uniqueness across those columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, in the first row, there are two applications with the same income
    variables and loan amount. Let’s filter the dataset to find these believed-to-be
    duplicate records by using values from the first row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at this subset, it is quite obvious that the data contains duplicates
    or two different applications were made – one by the husband and another one by
    the wife. This data is not providing any more information, other than that one
    application has been made by a male candidate and another has been made by a female
    candidate. We could drop one of the data points, but there is a big imbalance
    in the ratio of male to female applications. Also, if the second one was a genuine
    application, then we should keep the data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Based on this, we have understood what makes a data point unique – that is,
    a combination of `gender`, `applicant_income`, `coapplicant_income`, and `loan_amount`.
    It’s our goal, as data scientists and data engineers, to ensure that once uniqueness
    rules have been defined, data coming into the machine learning system conforms
    to those uniqueness checks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss data completeness or issues with incomplete
    data, and how to handle incomplete data.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring that the data is complete and not missing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have achieved data consistency and uniqueness, it’s time to identify
    and address other quality issues. One such issue is missing information in the
    data or incomplete data. Missing data is a common problem with real datasets.
    As a dataset’s size increases, the chance of data points going missing in the
    data increases. Missing records can occur in several ways, some of which include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Merging of source datasets**: For example, when we try to match records against
    date of birth or a postcode to enrich data, and either of these is missing in
    one dataset or is inaccurate, such occurrences will take NA values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random events**: This is quite common in surveys, where the person may not
    be aware of whether the information required is compulsory or they may not know
    the answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failures of measurement**: For example, some traits, such as blood pressure,
    are known to have a very substantial component of random error when measured in
    the conventional way (that is, with a blood pressure cuff). If two people measure
    a subject’s blood pressure at almost the same time, or if one person measures
    a subject’s blood pressure twice in rapid succession, the measured values can
    easily differ by 10 mm/Hg (https://dept.stat.lsa.umich.edu/~kshedden/introds/topics/measurement/).
    If a person is aware of these errors, they may decide to omit this information;
    for some patients, this data will take NA values. In finance, an important measurement
    ratio to determine the credit worthiness of someone or a firm is the debt-to-income
    ratio. There are scenarios when income is not declared, and in those circumstances,
    dividing debt by 0 or missing data would result in missing information for the
    ratio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Poor process design around collecting data**: For example, in health surveys,
    people are often asked about their BMI, and not everyone knows their BMI or understands
    the measurement. It would be simpler and easier if we were to ask for someone’s
    height and weight as they are more likely to know this. Another problem arises
    when someone is asked about their weight measurement, where some people might
    be likely to omit or lie about this information. If BMI cannot be understood or
    measured at the time of collecting data, the data will take NA values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When training datasets contain missing values, machine learning models can
    produce inaccurate predictions or fail to train properly due to the lack of complete
    information. In this section, we will discuss the following techniques for handling
    missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: Deleting data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding missingness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imputation methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One way to get rid of missing data is by deleting the missing records. This
    is also known as the **complete case analysis** (**CCA**) method. This is fine
    if less than 5% of rows contain missing values, but deleting more records could
    reduce the power of the model since the sample size will become smaller. There
    might also be a systematic bias in the data since this technique assumes that
    the data is missing completely and random, but it violates other assumptions such
    as when data is **missing at random** (**MAR**) or **missing not at random** (**MNAR**).
    Hence, blindly removing the data could make the model more biased. For instance,
    if a minority population has not declared their income in the past or has not
    held credit in the past, they may not have a credit score. If we remove this data
    blindly without understanding the reason for missingness, the algorithm could
    be more biased toward giving loans to majority groups that have credit information,
    and minority groups will be denied the opportunity, despite some members having
    solid income and creditworthiness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore the dataset using the CCA technique, remove all rows where information
    is missing, and figure out what volume of data is lost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Since 21% is almost one-fourth of the dataset, this is not a feasible method.
    Hence, in this section, we will explore how to identify missing data, uncover
    patterns or reasons for data being missing, and discover techniques for handling
    missing data so that the dataset can be used for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will extract categorical features, binary features, and numerical
    features. To do this, we must separate the identifier and the target label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To check if data is missing in the dataset, pandas provides a convenience method
    called `.info()`. This method shows us how many rows are complete among the total
    records. The method also displays the data type of each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The pandas library has another convenience method called `.isnull()` to check
    which row is missing data for a column and which row is complete. By combining
    `.sum()` with `.isnull()`, we can get the total number of missing records for
    each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the `credit_history` , `self_employed`, and `loan_amount` columns
    have the most missing data. Raw values can sometimes be difficult to comprehend
    and it’s more useful to know the percentage of data that’s missing from each column.
    In the next step, we will create a function that will take the DataFrame and print
    out the missing percentage of data against each column. Then, we sort the data
    in descending order of missingness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can extract the magnitude of missing data. However, before we dive into
    handling missing data, it is important to understand the patterns for missing
    data. By understanding these relationships, we will be able to take appropriate
    steps. This is because imputing missing data can alter the distribution of the
    data, which may further affect variable interaction.
  prefs: []
  type: TYPE_NORMAL
- en: We will utilize the `missingno` library and other visualizations to understand
    where data is missing, and in the absence of subject matter experts, we will make
    some assumptions on reasons for missing data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see where values are missing and where there are gaps in the data, we will
    utilize a `matrix` plot. A `matrix` plot can be quite useful when the dataset
    has depth or when the data contains time-related information. The presence of
    data is represented in gray, while absent data is displayed in white:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Matrix plot](img/B19297_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Matrix plot
  prefs: []
  type: TYPE_NORMAL
- en: Looking closer, we can see that the `credit_history` column has a lot of missing
    points, and the occurrence of missingness is spread throughout the data and not
    at a given point in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we touched upon earlier, understanding the reasons behind the missingness
    of data can help us choose the right technique to handle missing data. At a high
    level, we can call these mechanisms of missing data and classify them into three
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing completely at** **random** (**MCAR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing not at** **random** (**MNAR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing at** **random** (**MAR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is MCAR when the likelihood of missing data is the same for all the observations,
    and there is no relationship between the data being missing and any other features
    in the dataset. For example, a mail questionnaire might get lost in the post or
    a person may have forgotten to answer a question if they were in a hurry. In such
    cases, data being missing has nothing to do with the type of question, age group,
    or gender (relationship with other variables), and we can classify such features
    or data points as MCAR. Removing these data points or changing the value to 0
    for such instances will not bias the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, data is MAR when the likelihood of a data point being missing
    depends on other existing data points. For instance, if men don’t disclose their
    weight 5% of the time on average, whereas women don’t disclose their weight 15%
    of the time, we can assume that missingness in data is caused by the presence
    of gender bias. This will lead to a higher percentage of data being missing for
    women than for men. For this mechanism, we can impute data using statistical techniques
    or machine learning to predict the missing value by utilizing other features in
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The third mechanism, MNAR, can often be confused with MCAR but is slightly different.
    In this scenario, a clear assumption can be made as to why data is not missing
    at random. For instance, if we are trying to understand what factors lead to depression
    (outcome), depressed people could be more likely to not answer questions or less
    likely to be contacted. Since missingness is related to the outcome, these missing
    records can be flagged as “missing” and for numerical features, we can use a combination
    of machine learning to impute missing data from other features and flag data points,
    where data is missing, by creating another variable.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the different types of missing data, we will utilize
    the `heatmap` function from `missingno`, which will create a correlation heatmap.
    The visualization shows a nullity correlation between columns of the dataset.
    It shows how strongly the presence or absence of one feature affects the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nullity correlation ranges from -1 to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: -1 means if one column (attribute) is present, the other is almost certainly
    absent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 means there is no dependence between the columns (attributes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 means that if one column (attribute) is present, the other is also certainly
    present
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike a standard correlation heatmap, the following visualization is about
    the relationship between missing data features since few of them have missing
    data. Those columns that are always full or always empty have no meaningful correlation
    and are removed from the visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'This heatmap helps identify data completeness correlations between attribute
    pairs, but it has limited explanatory ability for broader relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following heatmap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Heatmap plot](img/B19297_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Heatmap plot
  prefs: []
  type: TYPE_NORMAL
- en: From this plot, we can interpret relationships of missingness across a few variables.
    There is a correlation of 0.4 between `dependents` and `married`, which makes
    sense as the majority of the time, someone gets married first before having dependents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will extract columns that contain missing data and use these for the
    next visualization. The `dendrogram` method uses hierarchical clustering and groups
    attributes together where missingness is associated with the missingness of another
    variable or completeness is associated with the completeness of another variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Dendrogram plot](img/B19297_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Dendrogram plot
  prefs: []
  type: TYPE_NORMAL
- en: We interpret the dendrogram based on a top-down approach – that is, we focus
    on the height at which any two columns are connected with matters of nullity.
    The bigger the height, the smaller the relation, and vice versa. For example,
    the missingness or presence of data in `credit_history` has no relationship with
    the missingness or completeness of any other variable.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have understood patterns of missing data and if there are relationships
    between missing data columns. Next, we will explore the relationship between missing
    data and the outcome. Before we decide to remove missing data or impute it, we
    should also look at whether the missingness of a variable is associated with the
    outcome – that is, is there a chance that data may be MNAR?
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will visualize this relationship in missing categorical data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create some plots and show how categorical features are associated
    with the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19297_05_5.jpg)![](img/B19297_05_6.jpg)![](img/B19297_05_7.jpg)![](img/B19297_05_8.jpg)![](img/B19297_05_9.jpg)![](img/B19297_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – The output plots displaying the association of categorical features
    with the target variable
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, we can assume that for variables such as `married`, `dependents`,
    `loan_amount_term`, `gender`, and `credit_history`, the missingness of data is
    associated with the loan-approved status. Hence, we can say that the data for
    these variables is MNAR. For these three variables, we can encode missing data
    with the word “missing” as this signal will help predict the outcome. The missingness
    or completeness of `credit_history` is slightly associated with the `self_employed`
    status, as indicated in the heatmap plot, which shows that the data might be missing
    at random. Similarly, the missingness of the `married` status is associated with
    the missingness of `dependents` and `loan_amount`.
  prefs: []
  type: TYPE_NORMAL
- en: For all binary variables where data is missing, we can assume that data is not
    MCAR and rather assume that data is MNAR as there was some relationship of missingness
    of information with the outcome, or MAR, since missingness is associated with
    the presence or absence of other variables, as shown in the dendrogram.
  prefs: []
  type: TYPE_NORMAL
- en: One way to encode missing values is to encode these with the most frequent values
    or get rid of missing values or/and create an additional column that indicates
    missingness with 1 or 0\. However, for MAR scenarios, this is not the best technique.
    As mentioned earlier, the goal of a data-centric approach is to improve data quality
    and reduce bias. Hence, instead of using frequency imputation methods or just
    deleting records, we should consider asking annotators to provide information
    where data is missing or make system fixes to recover from missing information.
    If that is not possible, we should consider using machine learning techniques
    or probabilistic techniques to determine possible values over simple imputation
    methods of mode, mean, and median. However, when missingness exceeds certain thresholds,
    even advanced techniques are not reliable and it’s better to drop the feature.
    For the remaining variables, we will use a machine learning technique to determine
    the missing values since we cannot get annotators to help us provide complete
    information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have identified how the missingness of categorical values is associated
    with the outcome, next, we will study the relationship between missing numerical
    data and the outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Loan amount missing association with target](img/B19297_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Loan amount missing association with target
  prefs: []
  type: TYPE_NORMAL
- en: For `loan_amount`, it can be assumed that data is MNAR as well as MAR since
    the missingness or completion of data in the `married` and `dependents` variables
    is slightly associated with the missingness and completeness of `loan_amount`,
    as observed in the heatmap. Hence, we choose to impute missing values using machine
    learning and create an additional column to indicate missingness, which will provide
    a better signal to our model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will dive into various methods of imputing data and compare these,
    as well as talking about the shortcomings of each approach. We will also discuss
    the implications of machine learning on imputing missing data in data-centric
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Following a model-centric approach, the standard rule of thumb for imputing
    numerical variables is that when 5% of the data is missing, impute it with the
    mean, median, or mode. This approach assumes that data is missing completely at
    random. If this assumption is not true, these simple imputation methods may obscure
    distributions and relationships within the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will explore the distribution of `loan_amount` without imputation
    and when imputed with the median. The distribution changes when we impute 6% of
    the values with the median:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot is displayed as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Simple density plot imputation with the median](img/B19297_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Simple density plot imputation with the median
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compare the standard deviation of the loan amount before and after
    imputation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code shows how a simple imputation method can obscure the distribution
    of data. To counter these effects and preserve the distribution, we will use the
    random sample imputation method.
  prefs: []
  type: TYPE_NORMAL
- en: First, we extract all the rows where `loan_amount` is missing. Then, we compute
    the variables that are correlated with `loan_amount` and use those values to set
    a seed. This is because, if we use the same seed for all values, then the same
    random number will be generated and the method will behave similarly to arbitrary
    value imputation, which will be as ineffective as the simple imputation methods
    of mean and median.
  prefs: []
  type: TYPE_NORMAL
- en: The downside to random sample distribution is that covariance will be affected
    and we need a method that preserves the covariance as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we check which feature is highly correlated with `loan_amount`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that `loan_amount` is highly correlated with `applicant_income`,
    so for this example, we use this variable to set the seed. First, we extract the
    indexes where `loan_amount` is missing. Then, we use the `applicant_income` value
    at the missing location and use this value to set the seed. Next, we use this
    seed to generate a random value from `loan_amount` to impute the missing row.
    We use this approach to impute all the missing data for `loan_amount`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compare the distribution of `loan_amount` with the random sample imputed
    `loan_amount` and median imputed `loan_amount`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Density plot showing random and median imputations](img/B19297_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Density plot showing random and median imputations
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we compare the standard deviation of the pre-imputed loan with the random
    sample imputed method and median imputed method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The random sample imputation method is much closer in distribution and standard
    deviation to the pre-imputed `loan_amount` method than the median imputed `loan_amount`
    method. Next, we check whether the random sample imputation method preserves the
    correlation with other variables compared to other methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting DataFrame is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Correlation DataFrame](img/B19297_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Correlation DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'From this, it’s evident that the random imputation method can preserve the
    distribution but may obscure the inter-relationships with other variables. We
    need a method that can preserve the distribution and also maintain the inter-relationships
    with other variables. We will use machine learning to help us achieve this. Before
    we move on to machine learning, we will first discuss the impact of simple imputation
    on categorical/binary variables. We impute the `credit_history` binary column
    with the most frequent value and compare the distribution before and after imputation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: By imputing `credit_history` with the most frequent values, we have biased the
    data toward the `credit_history` status. As we discovered previously, the missingness
    of `credit_history` is not associated with any other variables, but it might be
    associated with the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding examples show that if we utilize simple imputation methods then
    we may bias the data, and the distribution will be altered as well, whereas if
    we utilize random methods, the distribution will be preserved but the data relationships
    may change and data variance may increase. Hence, when data is MAR or MNAR, to
    achieve a balance between data bias and data variance, we can use a machine learning
    model.
  prefs: []
  type: TYPE_NORMAL
- en: To utilize machine learning for numerical imputation, we will leverage the nearest
    neighbors imputation method available in the `scikit-learn` library – `KNNImputer`.
    One issue with the imputer is that we can only pass a DataFrame to it, and not
    pass a list of columns. Hence, we will use the `SklearnTransformerWrapper` module,
    which is available as part of the `feature-engine` library, to pass a list of
    columns. Since KNN is a distance-based algorithm, to ensure that the model converges
    and one variable doesn’t overpower the other variable, we must scale the data
    before using this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Another technique to impute data is referred to as **Multiple Imputation by
    Chained Equations** (**MICE**). MICE works by imputing all the data with the mean,
    median, or mode. Then, regarding the variable that will be imputed, the initial
    imputed values are converted back into missing values. Then, using other variables
    as predictors, a machine learning model is used to predict missing values. After
    this, the next variable is imputed in a similar manner, where initially imputed
    values are converted back into missing values, and other variables, including
    the recently imputed variable, are used as predictors to impute the missing values.
    Once all the variables with missing values have been modeled and values have been
    imputed with predictions, the first round of imputation is completed. This procedure
    is repeated *n* number of times (ideally 10), and from round two, round one predictions
    are used to predict records that were initially missing.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for using several rounds is that, initially, we are modeling the
    missing data using other variables that also have NA values, and the initial strategy
    of imputation uses suboptimal methods such as the mean, median, or mode, which
    may bias the predictions. As we continue to regress over multiple rounds, predictions
    will stabilize and become less biased.
  prefs: []
  type: TYPE_NORMAL
- en: One issue with MICE is that we have to choose which machine learning model to
    use for the task. We will implement MICE with the random forest algorithm, which
    in R language is referred to as `[missForest]`.
  prefs: []
  type: TYPE_NORMAL
- en: In our implementation of MICE, we will refer to it as `missForest` since it
    will be a replica of how it is implemented in R (https://rpubs.com/lmorgan95/MissForest#:~:text=MissForest%20is%20a%20random%20forest,then%20predicts%20the%20missing%20part).
    To counter the effects of choosing an algorithm, we encourage practitioners to
    leverage automated machine learning, where for each imputation and iteration,
    a new algorithm will be chosen. The one disadvantage of this approach is that
    it’s computationally intensive and time-intensive when utilized for big datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we extract the numerical columns by filtering any columns that may have
    more than 15 categories while filtering the `id` column and outcome, as well as
    filtering newly created variables using imputation methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the DataFrame with numerical variables and visualize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we build a function that takes the scaler (standard scaler or any other
    scaler) and DataFrame and returns scaled data and the processed scaler. We must
    scale the dataset before applying the KNN imputer since a distance-based method
    requires data to be on the same scale. Once we have scaled the data, we apply
    the KNN imputer to impute the data, and then unscale the data using the processed
    scaler returned by the function. Once we’ve done this, we can compare the machine
    learning imputed data with the median and random imputation methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the scaler and call the `scale_data` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Then, we apply the KNN imputer with a parameter of 10 neighbors to impute the
    data. We utilize the `weights='distance'` parameter so that more weightage will
    be given to the votes of the nearer neighbors compared to the ones that are further
    away when predicting the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we initialize the imputer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we apply the imputation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we unscale the data by calling the `inverse_transform` method from the
    scaler object and overwrite the `df_imputed` DataFrame with unscaled values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compare the distribution of the pre-imputed `loan_amount` and compare
    it with the machine learning imputed method. Then, we check the correlation of
    the machine learning imputed method to the applicant’s income and compare it with
    other imputed methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Loan amount KNN imputation](img/B19297_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Loan amount KNN imputation
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compare the standard deviation of the pre-imputed loan amount with
    all the imputation methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will check if the correlation is maintained when `loan_amount` is
    imputed using machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.11 – Correlation after loan_amount is imputed](img/B19297_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Correlation after loan_amount is imputed
  prefs: []
  type: TYPE_NORMAL
- en: The machine learning-imputed method has almost the same distribution as the
    original. However, the correlation is a bit higher with `applicant_income` compared
    to the pre-imputed `loan_amount`. We have now seen how to use out-of-the-box techniques
    to impute missing data. One advantage of this method is that it’s simple to implement.
    However, the disadvantage is that we cannot choose another algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, in the next step, we go one step further and build a MICE implementation
    with random forest. First, we convert categorical data into numerical data using
    one-hot encoding. Then, we impute missing categorical data with the MICE implementation
    with `RandomForestClassifier`.
  prefs: []
  type: TYPE_NORMAL
- en: Once the categorical data has been imputed, we use categorical and numerical
    data to impute numerical missing values by utilizing MICE with `RandomForestRegressor`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build the MICE implementation, we use `IterativeImputer`, which is available
    in scikit-learn, to help with 10 rounds of MICE. To leverage `IterativeImputer`,
    we must import `enable_iterative_imputer` from scikit-learn’s experimental packages,
    as per the docs: [https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we extract the categorical columns that are string-encoded so that we
    can one-hot encode these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we one-hot encode the categorical columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we visualize the first five results of the one-hot encoded data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we extract the categorical variables that are binary encoded, including
    the data that has been one-hot encoded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we build the MICE implementation with random forest to impute categorical
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we extract the features from the imputation by converting the NumPy array
    into a DataFrame called `df_cat_imputed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s ensure we don’t have any new unexpected values being created by the classifier.
    To check this, we iterate over all the columns and print the unique values for
    each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we combine the categorical imputed data with numerical data. Then, we
    use all the data to impute numerical data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we implement MICE imputation with random forest to impute numerical data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we extract the features from the imputation by converting the NumPy array
    into a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we check whether all the columns have been imputed and there are no missing
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compare the distribution of the pre-imputed `loan_amount` and compare
    it with the MICE imputed method. We then check the correlation of the MICE imputed
    method with the applicant’s income and compare it with other imputed methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – loan_amount_miss_forest_imputed](img/B19297_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – loan_amount_miss_forest_imputed
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compare the standard deviation of the pre-imputed loan amount with
    all the imputation methods, including the MICE imputation method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we check if the correlation is maintained when `loan_amount` is imputed
    using the MICE imputation method compared to other methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The output DataFrame is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.13 – Correlation after imputing loan_amou\uFEFFnt using the MICE\
    \ imputation method](img/B19297_05_18.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Correlation after imputing loan_amount using the MICE imputation
    method
  prefs: []
  type: TYPE_NORMAL
- en: The standard deviation is slightly below the random imputation but higher than
    the median imputed method. As we can see, the correlation with `applicant_income`
    hasn’t improved compared to the random imputation method or median imputation
    method. Hence, to test whether MICE with random forest is a better implementation
    for this use case, we can compare the evaluation metric of the machine learning
    model when MICE is utilized versus when median imputation is utilized.
  prefs: []
  type: TYPE_NORMAL
- en: But before we do that, we want machine learning practitioners to explore **automated
    machine learning** (**AutoML**) with the MICE imputation framework. Machine learning
    can be a tedious process that consists of trial and error, hence why AutoML frameworks
    are getting quite popular when it comes to reducing human time. These frameworks
    automate feature engineering, cross-validation, model selection, and model tuning.
    One issue with the current implementation of MICE is that we have to choose which
    machine learning model to use for the task. What if we wanted to trial multiple
    algorithms to see which provided the best prediction for the imputation task,
    and while doing that wanted to ensure the prediction was generalizable and the
    model was not overfitted or underfitted? We can imagine the complexity. To counter
    this, we’ll combine AutoML with MICE.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of this approach is that at each iteration, a new model will be
    picked by AutoML, thus freeing the machine learning practitioner from tedious
    tasks. However, the disadvantage of this approach is that when the data increases
    in size, a lot more resources will be needed, which may not be viable. Another
    disadvantage with some open source AutoML frameworks is that on some operating
    systems, full functionality is error-prone. For instance, on Mac computers, both
    TPOT and AutoSklearn frameworks give errors when parallel processing is used.
    Hence, we will let you explore your own flavor of AutoML with MICE.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will implement a scikit-learn pipeline that will include the MICE implementation
    with random forest. Then, we train the decision tree model with cross-validation
    and evaluate the model using accuracy and ROC. Once we’ve done this, we create
    another pipeline, which will use simple imputation methods, and compare the evaluation
    results. Finally, we explore techniques for further improving the data to enhance
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll be converting these steps into a scikit-learn pipeline since by using
    a pipeline, we can define the sequence of steps and also save these steps as a
    pickle object. By utilizing this practice, we maintain machine learning system
    best practices and can ensure reliability and reproducibility without replicating
    the code in the inference environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s drop all the newly created columns in the `df_consistent` DataFrame
    that end with `_imputed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we import all the necessary packages and modules to help split the data
    into train and test sets, evaluate the performance of the model, and create a
    machine learning pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we extract the features for the model and split the data into train and
    test sets, where 10% of the data is reserved for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we extract the categorical data and numerical data into separate lists
    so that we can use these to set the pipeline for each type of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create a function that will return the pipeline for categorical data.
    First, the pipeline one-hot encodes the list of columns in the `ohe_cols` variable,
    which includes `property_area`. The pipeline then imputes the columns with missing
    data using the MICE implementation with random forest. The function will return
    the transformer so that when we pass the categorical data, while the transformer
    one-hot encodes the data and then imputes the missing data. The transformer will
    be run against the training data first so that it learns about the data and saves
    all the metadata for running the same steps with new data. The transformer can
    then be used to transform the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a function that returns the pipeline transformer to impute
    numerical missing data with the MICE implementation. Similarly to the categorical
    transformer, the numerical transformer will be trained against the training data
    and then applied to the test data to impute missing values in the train and test
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize the categorical and numerical transformers, and then transform
    training and test data. The transformed categorical data is combined with numerical
    data before the numerical data is transformed. The output of this is imputed train
    and test DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Before passing the complete datasets to the machine learning model, we check
    if both the train and test labels have similar loan approval rates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Because the classes are slightly imbalanced, we can use the `class_weight='balanced'`
    option since this option uses the values of `y` to automatically adjust weights
    inversely proportional to class frequencies in the input data when training the
    algorithm. The objective of the problem is to identify better than a human being
    who is likely to get a loan. Since the majority class is trained on people who
    received a loan, the model will be biased toward giving someone a loan. By using
    `class_weight='balanced'`, the algorithm will put more emphasis on class label
    0 since it’s a minority class.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define grid search for the decision tree classifier to perform cross-validation,
    to ensure the model is generalizable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a custom function that will take in training data, testing
    data, the classifier, and grid search parameters. The function performs 10K cross-validation
    to find the best hyperparameters and trains the model on the best parameters.
    The function then returns the model, predictions, training and test accuracies,
    and ROC-AUC score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we run the custom classifier and calculate model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The test accuracy is just under 80%. Let’s see where the model is performing
    poorly by observing the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Confusion matrix](img/B19297_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: We have now applied machine learning pipelines with MICE imputation to create
    a machine learning model. To demonstrate that MICE imputation is a better technique
    than the simple imputation technique, we will recreate the machine learning pipelines
    with simple imputation methods and evaluate model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have created the pipeline steps, we transform the train and test data
    before passing it to the decision tree classifier and custom classifier function
    to measure model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we run the custom classifier and extract model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The test accuracy has dropped under 67%, which is a 12% reduction, and the
    ROC-AUC has dropped by 6%. Next, we review the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Confusion matrix](img/B19297_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – Confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of the true positive class has dropped from 88% to 67%, whereas
    the accuracy of the negative class has increased from 58% to 63%. By using basic
    imputation techniques, we can conclude that the model is more likely to be biased
    and the model performance may be less accurate.
  prefs: []
  type: TYPE_NORMAL
- en: In data-centric machine learning, the goal is to improve on the data and tune
    it, rather than improving on the algorithm and tuning the model. But how can we
    identify whether a dataset contains poorly labeled data, missing features, or
    another data-related issue? We will cover how to identify if data is poorly labeled
    and apply techniques to improve on mislabeled data in [*Chapter 6*](B19297_06.xhtml#_idTextAnchor089),
    *Techniques for Programmatic Labeling in* *Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: To find out if more features are needed or more data is needed, we utilize a
    technique called error analysis. In machine learning, error analysis is utilized
    to identify and diagnose erroneous predictions by focusing on the pockets of data
    where the model performed well and poorly. Although the overall performance of
    the model might be 79%, this performance may not be uniform across all pockets
    of the data, and these highs and lows could be due to inputs present in some pockets
    and absent in other pockets of data.
  prefs: []
  type: TYPE_NORMAL
- en: To identify data issues, we will start training the model with 10% of the data,
    and with each iteration add 10%. Then, we plot the training ROC and test the ROC
    concerning the increase in the size of the data. If the plot seems to converge
    and indicate an increase in the size of the data, this will lead to an improvement
    in the test ROC, at which point we will generate synthetic data to increase the
    data’s size. This technique will be covered in [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111)*,
    Using Synthetic Data in Data-Centric* *Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: If the plot doesn’t seem to converge and indicates an increase in data, it will
    have a minimal impact on improving test ROC. In this case, we can observe which
    data points the model performed poorly on, and may utilize feature engineering
    to generate new columns. Although feature engineering can be an iterative approach,
    for the scope of this chapter, we cover adding a feature or two.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run error analysis, first, we create data cutoff points from 0.1 to 1.0,
    where 0.1 means 10% of the training data and 1.0 means 100% of the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create an empty list called `scores` and run data preprocessing, model
    training, and evaluation with each cutoff of data. If the cutoff is < 1.0, we
    subset the training data; otherwise, we pass all the data for training. At the
    end of each iteration, we save the cutoff, train, and test evaluation metrics
    in `scores` by appending the metrics to the `scores` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a DataFrame from the `scores` list and pass the relevant column
    names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we plot the train and test ROC against each cutoff:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Error analysis train and test ROC](img/B19297_05_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – Error analysis train and test ROC
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, plot the train and test accuracy against each cutoff:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Error analysis train and test accuracy](img/B19297_05_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – Error analysis train and test accuracy
  prefs: []
  type: TYPE_NORMAL
- en: Both the test ROC and test accuracy seem to show signs of convergence with the
    train ROC and train accuracy, which indicates that model performance may be boosted
    if more data points were made available. This is why we will generate synthetic
    data (data that mimics the real data) in the next chapter and retrain the model
    with added data to get better model performance.
  prefs: []
  type: TYPE_NORMAL
- en: As we learned in the previous chapters, one of the principles of data-centric
    machine learning is keeping humans in the loop. Let’s imagine we spoke to the
    domain experts and they mentioned that one of the key determinants for someone
    getting a loan is the income-to-debt ratio – that is, the total income divided
    by the loan amount. This determines if someone will be able to pay back the loan
    or not. An application with a lower income-to-loan ratio is more likely to be
    rejected. In the dataset, there are two income variables – applicant income and
    co-applicant income. Also, the loan amount is represented in thousand figures
    – that is, a loan amount in the data loan amount of 66 represents 66,000\. To
    create this ratio, we will multiply the loan amount by 1,000 and then combine
    the income of the applicant and co-applicant. Once we’ve done this, we will divide
    the combined income by the loan amount to get the income-to-loan ratio. The domain
    experts also mentioned that **equated monthly installments** (**EMIs**) can also
    determine a candidate’s capability to pay the loan. The lower the EMI, the more
    likely a loan will be accepted, whereas the higher the EMI, the more likely a
    loan will be rejected. To calculate this without the interest rate, we can use
    the loan term and loan amount to get an approximate EMI amount for each month.
  prefs: []
  type: TYPE_NORMAL
- en: For the income-to-loan ratio, we will create a custom transformer for multiplying
    the loan amount by 1,000 so that we can use it in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: This transformer is a Python class that we can use to overload the fit and transform
    functions required by the pipeline. This class will inherit from the `BaseEstimator`
    and `TransformerMixin` classes, both of which can be found in the `sklearn.base`
    module. The class will be used to implement the fit and transform methods. These
    methods should contain `X` and `y` parameters, and the transform method should
    return a pandas DataFrame to ensure compatibility with the scikit-learn pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: To create a full income column, we leverage the `feature_engine` library since
    it is already compatible with the scikit-learn pipeline and has methods to apply
    mathematical operations relative to other variables. First, we sum the income
    variables. The output of that transformation will be divided by the `loan_amount`
    variable to create the income-to-loan ratio.
  prefs: []
  type: TYPE_NORMAL
- en: To create the EMI, we leverage the `feature_engine` library and divide `loan_amount`
    with `loan_amount_term`. Once we have created these features, we remove the two
    income variables since we already created a combination of the two. For this step,
    we use the `DropFeatures` class from the `feature_engine` library. All these feature
    engineering steps will be combined in a new pipeline called `feature-transformer`
    and will be applied post-data imputation.
  prefs: []
  type: TYPE_NORMAL
- en: We believe that by adding these extra features, the model performance of the
    decision tree algorithm will improve. Let’s run the algorithm post-feature engineering
    and evaluate the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create custom variables that will take in a list of variables for
    the feature engineering steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we import relevant packages from `feature_engine` to perform the feature
    engineering steps and import the `BaseEstimator` and `TransformerMixin` classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create a custom transformer that will take in variable names and a
    value that will be multiplied by each variable. By default, each variable will
    be multiplied by 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we call the `missForest` categorical and numerical transformers we created
    previously. Once we’ve done this, we create a feature transformer pipeline that
    multiplies `loan_amount` by 1,000 by leveraging the custom transformer we created
    previously. The new pipeline then adds income variables to create one income variable,
    the income-to-loan ratio, and the EMI features. Finally, the pipeline drops the
    two income variables since the new income variable will be created. By using the
    transformer pipelines, the train and test data will be transformed, and new features
    will be created. The output of this step will be fully transformed into train
    and test data so that it can be passed to the custom classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the categorical transformers for imputation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we add the numerical imputation and complete the imputation steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we transform the imputed data using the feature imputation pipeline we
    created previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we call the custom classifier function to evaluate model performance
    with added feature engineering steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we call the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting confusion matrix is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Confusion matrix when using custom feature engineering](img/B19297_05_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 – Confusion matrix when using custom feature engineering
  prefs: []
  type: TYPE_NORMAL
- en: Our test accuracy has increased from 79% to 82% and the ROC has been boosted
    from 78.5% to 81.8%. The preceding confusion matrix shows that the accuracy of
    the positive class has been boosted from 88% to 91%, while the accuracy of the
    negative class has been boosted from 58% to 63%.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have demonstrated that by using a data-centric approach, we can
    iterate over the data rather than iterate over multiple algorithms, and manage
    to improve model performance. We will explore how to add some synthetic data and
    boost model performance even further in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring that the data is valid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have ensured that our data is consistent, unique, and complete. But
    do we know if the data we have is valid? Do the data labels conform to the rules?
    For example, what if the property area in the dataset didn’t conform to the rules
    and `semi_urban` is invalid? What if one or a couple of annotators believed some
    suburbs are neither urban nor rural, and they violated the rules and entered `semi_urban`?
    To measure validity, we may need to look at business rules and check the percentage
    of data that conforms to these business rules. Let’s assume that `semi_urban`
    is an invalid value. In Python, we could check the percentage of invalid labels
    and then reach out to annotators to correct the data. We could also achieve this
    by using the data that was used to generate the label. If we had the `suburb_name`
    to `property_area` data mapping, and `suburb_name` was available in the dataset,
    then we could leverage the mapping and catch invalid values, as well as encoding
    labels programmatically. Building business rules in the system so that upcoming
    data is automatically encoded is referred to as programmatic labeling. We will
    dive into programmatic labeling in the upcoming chapters, where we explore techniques
    to make data consistent and valid at the time of data capture, so that when the
    data comes in, it’s already pristine and some of the data cleaning process will
    be redundant.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we make a fake dataset that consists of 10 rows and write a business
    rule. It will contain an `id` column with values from 1 to 10, a `population`
    column with 10 random values between 1,000 and 100,000, and a `property_area`
    column with four values set to `urban`, five values set to `semi_urban`, and one
    value set to `rural`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we print the first five rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Imagine that the business rule says a suburb or `property_area` is classified
    as urban when the population is more than 20,000; otherwise, it’s classified as
    rural. In this case, the validation rule should check that `property_area` only
    contains `urban` or `rural` values.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple way to check this in Python is to use the `value_counts()` method
    alongside the `normalize=True` parameter. The output of this will show that 50%
    of the data is invalid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can run the check against each row and flag when the value is in the
    expected list of values, as well as when the value is not in the expected set
    of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we sum the rows where the data validation rule was breached and divide
    the number of invalid rows by the total rows to provide a metric – that is, the
    percentage of invalid labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: Invalid data must be communicated to the source data providers and must be cleaned;
    otherwise, this data can creep in and the machine learning model will learn about
    these invalid labels. The model's learning capability significantly improves when
    trained with valid data, which provides stronger label signals, as opposed to
    invalid data that weakens these signals due to reduced exposure to valid label
    points.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring that the data is accurate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though the data is valid, it may not be accurate. Data accuracy measures
    the percentage of data that matches real-world data or verifiable sources. Considering
    the preceding example of the property area, to measure data accuracy, we may have
    to look up a reliable published dataset and check the population of the area and
    the type of the area. Let’s assume that the population matches the verifiable
    data source, but the area type source is unavailable. Using the rule of what defines
    a rural area and what defines an urban area, we can measure data accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this business rule, we will create a new label called `true_property_area`
    that takes `rural` as a value when the population is 20,000 or below; otherwise,
    takes `urban` as a value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we print the rows of the dataset to see if there are any mismatches between
    `property_area` and `true_property_area`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we sum the rows where `property_area` values match with the true values
    and divide this by the total number of rows to calculate data accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of creating a function to calculate the accuracy, we can leverage `accuracy_score`
    from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, both methods return the same score. If inaccurate data enters
    the system, the model may learn inaccurately about semi-urban and rural areas,
    and at the time of inference, produce undesirable outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring that the data is fresh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data freshness is another important aspect of measuring data quality that has
    an impact on the quality and robustness of machine learning applications. Let’s
    imagine that we have a machine learning application that’s been trained on 2019
    and 2020 customer behavior and utilized to predict hotel room bookings up to April
    2021\. Maybe January and February numbers were quite accurate, but when March
    and April hit, accuracy dropped. This might have been due to COVID-19, something
    that was unseen by the data, and its effects were not captured. In machine learning,
    this is called data drift. This is happening here; the data distribution in March
    and April was quite different from the data distribution in 2019 and 2020\. By
    ensuring that the data is fresh and up to date, we can train the model more regularly
    or as soon as data drift is detected.
  prefs: []
  type: TYPE_NORMAL
- en: To measure data drift, we will use the `alibi` Python package. However, there
    are more extensive Python packages that can help with this job. We recommend Evidently
    AI ([https://www.evidentlyai.com/](https://www.evidentlyai.com/)), a data and
    machine learning model monitoring toolkit, or WhyLogs ([https://whylabs.ai/whylogs](https://whylabs.ai/whylogs)),
    an open source initiative by WhyLabs, to monitor model degradation and data drift.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine that, on average, model accuracy starts degrading when the model
    is trained on data that is more than 5 days old and the model does poorly and
    starts costing the business when data is more than 10 days old. We want to be
    able to alert and capture when this happens. To demonstrate this scenario, we
    will create a sample dataset with a date column and define error and warning thresholds
    – that is, we print a warning if the data is 5 days old and block the application
    if the data is more than 10 days old. In practice, it is recommended to train
    the model on the most recent data available. Following a data-centric approach,
    we must encourage practitioners to have thresholds and **service-level agreements**
    (**SLAs**) defined with the providers of data so that they have mechanisms in
    place to request up-to-date data, penalize when SLAs are breached, and maybe reward
    when SLAs are met (encourage the importance of maintaining good-quality data).
  prefs: []
  type: TYPE_NORMAL
- en: Now we will generate 100 sample data points and demonstrate how to identify
    if data is stale using a date variable.
  prefs: []
  type: TYPE_NORMAL
- en: We utilize the `alibi` package to detect drift in the `loan_prediction` dataset.
    We will demonstrate the pitfalls of not detecting and acting on data drift by
    comparing accuracy before and after drift.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the `datetime` and `warning` packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we generate a base date – let’s say the date when we run the code – and
    from the base date, generate 100 more dates in the past by subtracting one day
    at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we print the first 10 dates in the order these dates were generated,
    with the most recent being the first date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a DataFrame with 100 rows by creating four columns. It will
    contain an `id` column with values from 1 to 100, a `date_loaded` column that
    contains the 100 dates we created previously, a `population` column with 100 random
    values between 1,000 and 100,000, and a `property_area` column with 40 values
    set to `urban`, 50 values set to `semi_urban,` and 10 values set to `rural`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we visualize the first five data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a one-line piece of code to demonstrate a way to subtract any
    date from today’s date and extract the number of days between the dates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create a function that will accept a DataFrame and the date column
    of the DataFrame, and by default will issue a warning if data is more than 5 days
    old and block the application when data is more than 10 days old:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we run the function with the sample DataFrame we created previously.
    The function will state that the data is fresh and just 0 days old:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: To demonstrate the function’s ability to issue a warning or error out when data
    is stale, we subset the data by removing 6 recent days and 12 recent days. We
    create two DataFrames – one that has 6 recent days removed and another that has
    12 recent days removed. Then, we run the `check_data_recency_day` function over
    these DataFrames. We see that when we run the function with 6-day-old data, the
    function will issue the warning, but when we run the function with 12-day-old
    data, the function will issue a `Value` error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create the two DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we run the function against the data that is 6 days old:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: You can also run the function against the data that is 12 days old; it will
    generate similar output.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have demonstrated how to measure data staleness, catch warnings,
    and block the application when data is extremely stale. Next, we will demonstrate
    the impact of data staleness on a real-life dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In real life, we wouldn’t expect a company not to change its products or for
    consumer behavior not to change as newer products become available on the market.
    Companies have to constantly study changes in consumer behavior; otherwise, their
    performance degrades. Machine learning systems face the same issues as market
    forces change, data changes, and data distributions change. This has an impact
    on a model’s performance if new data is quite different from the data it was trained
    on.
  prefs: []
  type: TYPE_NORMAL
- en: This is referred to as drift in machine learning and if it goes undetected and
    untreated, it can cause models to degrade.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore how to detect drift.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import `TabularDrift` from the `alibi-detect` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we show the `TabularDrift` reference data, which is the data the machine
    learning system was trained on – in our case, the loan prediction transformed
    data before we passed it to the decision tree classifier. We also pass a value
    of `0.05` for the p-value test. If this value is breached by the test data distribution,
    the package will inform us that the test data has drifted from the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we run the `predict` method to check if the test data has drifted. The
    `alibi` package utilizes the `Kolmogorov-Smirnov` test to determine if the two
    distributions differ. If the p-value exceeds 0.05, then the null hypothesis is
    rejected and it can be inferred that the `test` data distribution differs from
    the `train` data distribution. The output for this step will be `No`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s imagine that house prices started booming while incomes did not
    increase at the same rate. To simulate this scenario, we will increase the loan
    amount requested by 1.5 times the original test set, but increase the total income
    by 1.2 times the test set. Then, we update the new feature values that relied
    on the `loan_amount` and `income` variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we rerun TabularDrift’s `predict` method to check whether drift was detected.
    The output of this step is `Yes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we rerun the prediction on this drift-induced test data and check whether
    accuracy and ROC are affected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the distribution that was seen by the model when it was trained
    is different from the real data, and the impact is that model performance has
    degraded significantly. The ROC has dropped from 0.82 to 0.74 and accuracy has
    dropped from 82% to 70%. Hence, it’s important to ensure that data is fresh and
    that as soon as drift is detected, the model is retrained with new data to ensure
    model performance does not deteriorate.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we gained a good understanding of the six key dimensions of
    data quality and why it’s important to improve data quality for superior model
    performance. We further dived into the data-centric approach of improving model
    performance by iterating over the data, rather than iterating over various algorithms
    (model-centric approach), by improving the overall health of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned how to ensure data is consistent, unique, accurate, valid,
    fresh, and complete. We dived into various techniques of imputing missing values
    and when to apply which approach. We concluded that imputing missing values with
    machine learning can be better than using simple imputation methods, especially
    when data is MAR or MNAR. We also showed how to conduct error analysis and how
    to use the results to further improve model performance by either performing feature
    engineering, which involves building new features, or increasing the data size
    by creating synthetic data, which we will cover in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed why is it important to ensure data is fresh and not drifted
    from the original training set, and concluded that drifted data can hamper model
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood the importance of ensuring good-quality data across
    the six key dimensions of data quality, in the next chapter, we will dive into
    using synthetic data to further improve model performance, especially over edge
    cases. We will also dive into data augmentation, a technique that’s used to create
    synthetic data for images so that algorithms can learn from more and better, especially
    when these new examples can come in various forms.
  prefs: []
  type: TYPE_NORMAL
