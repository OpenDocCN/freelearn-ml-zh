["```py\n> N <- seq(1e3,1e4,1e3)\n> N\n [1]  1000  2000  3000  4000  5000  6000  7000  8000  9000 10000\n> B <- 1e3\n> Common_Prob <- NULL\n>index<- 1\n>for(i in N){\n+   temp_prob <- NULL\n+   for(j in 1:B){\n+     s1 <- sample(i,size=i,replace=TRUE)\n+     s2 <- sample(i,size=i,replace=TRUE)\n+     temp_prob <- c(temp_prob,length(intersect(s1,s2))/i)\n+   }\n+   Common_Prob[index] <- mean(temp_prob)\n+   index<- index + 1\n+ }\n> Common_Prob\n [1] 0.4011 0.4002 0.3996 0.3982 0.3998 0.3996 0.3994 0.3997 0.3996 0.3995\n```", "```py\n>load(\"../Data/GC2.RData\")\n>set.seed(12345)\n> Train_Test <- sample(c(\"Train\",\"Test\"),nrow(GC2),\n+ replace = TRUE,prob = c(0.7,0.3))\n> GC2_Train <- GC2[Train_Test==\"Train\",]\n> GC2_TestX <- within(GC2[Train_Test==\"Test\",],rm(good_bad))\n> GC2_TestY <- GC2[Train_Test==\"Test\",\"good_bad\"]\n> GC2_Formula <- as.formula(\"good_bad~.\")\n> GC2_RF <- randomForest(GC2_Formula,data=GC2_Train,ntree=500)\n> GC2_RF_Margin <- predict(GC2_RF,newdata = GC2_TestX,type=\"class\")\n>sum(GC2_RF_Margin==GC2_TestY)/313\n[1] 0.7795527\n```", "```py\n> GC2_RF2 <- randomForest(GC2_Formula,data=GC2_Train,mtry=8, \n+ ntree=500)\n> GC2_RF_Margin <- predict(GC2_RF,newdata = GC2_TestX, type=\"class\")\n> GC2_RF2_Margin <- predict(GC2_RF2,newdata = GC2_TestX,type=\"class\")\n> sum(GC2_RF2_Margin==GC2_TestY)/313\n[1] 0.7859425\n```", "```py\n>plot(GC2_RF2)\n> GC2_RF2.legend <- colnames(GC2_RF2$err.rate)\n> legend(x=300,y=0.5,legend = GC2_RF2.legend,lty=c(1,2,3), col=c(1,2,3))\n>head(GC2_RF2$err.rate,10)\n            OOB       bad      good\n [1,] 0.3206751 0.4743590 0.2452830\n [2,] 0.3218673 0.4769231 0.2490975\n [3,] 0.3222656 0.5437500 0.2215909\n [4,] 0.3006993 0.5224719 0.2005076\n [5,] 0.3262643 0.5445026 0.2274882\n [6,] 0.3125000 0.5522388 0.2027335\n [7,] 0.3068702 0.5631068 0.1893096\n [8,] 0.2951807 0.5741627 0.1670330\n [9,] 0.2976190 0.5619048 0.1774892\n[10,] 0.2955882 0.5801887 0.1666667\n```", "```py\n> data(\"PimaIndiansDiabetes\")\n> set.seed(12345)\n> Train_Test <- sample(c(\"Train\",\"Test\"),nrow(PimaIndiansDiabetes),\n+ replace = TRUE, prob = c(0.7,0.3))\n> head(Train_Test)\n[1] \"Test\"  \"Test\"  \"Test\"  \"Test\"  \"Train\" \"Train\"\n> PimaIndiansDiabetes_Train <- PimaIndiansDiabetes[Train_Test==\"Train\",]\n> PimaIndiansDiabetes_TestX <- within(PimaIndiansDiabetes[Train_Test==\"Test\",],rm(diabetes))\n> PimaIndiansDiabetes_TestY <- PimaIndiansDiabetes[ \n+ Train_Test==\"Test\",\"diabetes\"]\n> PID_Formula <- as.formula(\"diabetes~.\")\n> PID_RF <- randomForest(PID_Formula,data=PimaIndiansDiabetes_Train,coob=TRUE,\n+                        ntree=500,keepX=TRUE,mtry=5,\n+                        parms=list(prior=c(0.65,0.35)))\n> PID_RF_Margin <- predict(PID_RF,newdata = PimaIndiansDiabetes_TestX, type=\"class\")\n> sum(PID_RF_Margin==PimaIndiansDiabetes_TestY)/257\n[1] 0.7704\n```", "```py\n> data(kyphosis)\n> kc<- rpart(Kyphosis~.,data=kyphosis,maxsurrogate=0)\n> plot(kc);text(kc)\n```", "```py\n>kc$variable.importance\nStart   Age \n7.783 2.961 \n```", "```py\n>summary(kc)\nCall:\nrpart(formula = Kyphosis ~ ., data = kyphosis, maxsurrogate = 0)\n  n= 81 \n       CP nsplit rel error xerror   xstd\n1 0.17647      0    1.0000      1 0.2156\n2 0.01961      1    0.8235      1 0.2156\n3 0.01000      4    0.7647      1 0.2156\n\nVariable importance\nStart   Age \n   72    28 \n\nNode number 1: 81 observations,    complexity param=0.1765\npredicted class=absent   expected loss=0.2099  P(node) =1\nclass counts:    64    17\nprobabilities: 0.790 0.210 \nleft son=2 (62 obs) right son=3 (19 obs)\n  Primary splits:\nStart  < 8.5to the right, improve=6.762, (0 missing)\n      Number <5.5  to the left,  improve=2.867, (0 missing)\n      Age    < 39.5 to the left,  improve=2.250, (0 missing)\n\nNode number 2: 62 observations,    complexity param=0.01961\npredicted class=absent   expected loss=0.09677  P(node) =0.7654\nclass counts:    56     6\nprobabilities: 0.903 0.097 \nleft son=4 (29 obs) right son=5 (33 obs)\n  Primary splits:\nStart  < 14.5to the right, improve=1.0210, (0 missing)\n      Age    < 55   to the left,  improve=0.6849, (0 missing)\n      Number <4.5  to the left,  improve=0.2975, (0 missing)\n\nNode number 3: 19 observations\npredicted class=present  expected loss=0.4211  P(node) =0.2346\nclass counts:     8    11\nprobabilities: 0.421 0.579 \n\nNode number 4: 29 observations\npredicted class=absent   expected loss=0  P(node) =0.358\nclass counts:    29     0\nprobabilities: 1.000 0.000 \n\nNode number 5: 33 observations,    complexity param=0.01961\npredicted class=absent   expected loss=0.1818  P(node) =0.4074\nclass counts:    27     6\nprobabilities: 0.818 0.182 \nleft son=10 (12 obs) right son=11 (21 obs)\n  Primary splits:\nAge    < 55to the left,  improve=1.2470, (0 missing)\nStart  < 12.5 to the right, improve=0.2888, (0 missing)\n      Number <3.5  to the right, improve=0.1753, (0 missing)\n\nNode number 10: 12 observations\npredicted class=absent   expected loss=0  P(node) =0.1481\nclass counts:    12     0\nprobabilities: 1.000 0.000 \n\nNode number 11: 21 observations,    complexity param=0.01961\npredicted class=absent   expected loss=0.2857  P(node) =0.2593\nclass counts:    15     6\nprobabilities: 0.714 0.286 \nleft son=22 (14 obs) right son=23 (7 obs)\n  Primary splits:\nAge    <111  to the right, improve=1.71400, (0 missing)\nStart  < 12.5 to the right, improve=0.79370, (0 missing)\n      Number <3.5  to the right, improve=0.07143, (0 missing)\n\nNode number 22: 14 observations\npredicted class=absent   expected loss=0.1429  P(node) =0.1728\nclass counts:    12     2\nprobabilities: 0.857 0.143 \n\nNode number 23: 7 observations\npredicted class=present  expected loss=0.4286  P(node) =0.08642\nclass counts:     3     4\nprobabilities: 0.429 0.571 \n```", "```py\n>windows(height=100,width=200)\n>par(mfrow=c(1,2))\n>varImpPlot(GC2_RF,main=\"Variable Importance plot for \\n Random \n+ Forest of German Data\")\n>varImpPlot(PID_RF,main=\"Variable Importance plot for \\n Random Forest of Pima Indian Diabetes\")\n```", "```py\n> GC2_RF3 <- randomForest(GC2_Formula,data=GC2_Train,\n+                         ntree=500,proximity=TRUE,cob.prox=TRUE)\n> GC2_RF3$proximity[1:10,1:10]\n        5      6      7      8     11     12     14     15     16     17\n5  1.0000 0.0000 0.0000 0.0133 0.0139 0.0159 0.0508 0.0645 0.0000 0.0000\n6  0.0000 1.0000 0.0435 0.0308 0.0000 0.0000 0.0000 0.0000 0.0000 0.0417\n7  0.0000 0.0435 1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.2000\n8  0.0133 0.0308 0.0000 1.0000 0.0000 0.0000 0.0000 0.0000 0.0137 0.0000\n11 0.0139 0.0000 0.0000 0.0000 1.0000 0.0395 0.0000 0.2034 0.0147 0.0000\n12 0.0159 0.0000 0.0000 0.0000 0.0395 1.0000 0.0000 0.0323 0.0000 0.0000\n14 0.0508 0.0000 0.0000 0.0000 0.0000 0.0000 1.0000 0.0167 0.0435 0.0182\n15 0.0645 0.0000 0.0000 0.0000 0.2034 0.0323 0.0167 1.0000 0.0345 0.0000\n16 0.0000 0.0000 0.0000 0.0137 0.0147 0.0000 0.0435 0.0345 1.0000 0.0159\n17 0.0000 0.0417 0.2000 0.0000 0.0000 0.0000 0.0182 0.0000 0.0159 1.0000\n>MDSplot(GC2_RF3,fac = GC2_Train$good_bad,\n+         main=\"MDS Plot for Proximity Matrix of a RF\")\n```", "```py\n>which.max(GC2_RF3$proximity[1,-1])\n962 \n657 \n>which.max(GC2_RF3$proximity[2,-2])\n686 \n458 \n```", "```py\n> GC2_RF_MDD <- min_depth_distribution(GC2_RF)\n>head(GC2_RF_MDD)\ntree variable minimal_depth\n1    1      age             4\n2    1   amount             3\n3    1 checking             0\n4    1    coapp             2\n5    1  depends             8\n6    1 duration             2\n>windows(height=100,width=100)\n> plot_min_depth_distribution(GC2_RF_MDD,k=nrow(GC2_TestX))\n```", "```py\n> GC2_RF_VIM <- measure_importance(GC2_RF)\n[1] \"Warning: your forest does not contain information on local importance so 'accuracy_decrease' measure cannot be extracted. To add it regrow the forest with the option localImp = TRUE and run this function again.\"\n```", "```py\n> GC2_RF4<- randomForest(GC2_Formula,data=GC2_Train,\n+                        ntree=500,localImp=TRUE)\n> GC2_RF4_VIM <- measure_importance(GC2_RF4)\n```", "```py\n> P1 <- plot_multi_way_importance(GC2_RF4_VIM, size_measure = \"no_of_nodes\",\n+                           x_measure=\"mean_min_depth\",\n+                           y_measure = \"times_a_root\")\n> P2 <- plot_multi_way_importance(GC2_RF4_VIM, size_measure = \"no_of_nodes\",\n+                           x_measure=\"mean_min_depth\",\n+                           y_measure = \"gini_decrease\")\n> P3 <- plot_multi_way_importance(GC2_RF4_VIM, size_measure = \"no_of_nodes\",\n+                           x_measure=\"mean_min_depth\",\n+                           y_measure = \"no_of_trees\")\n> P4 <- plot_multi_way_importance(GC2_RF4_VIM, size_measure = \"no_of_nodes\",\n+                           x_measure=\"mean_min_depth\",\n+                           y_measure = \"p_value\")\n> grid.arrange(P1,P2,P3,P4, ncol=2)\n```", "```py\n> plot_importance_ggpairs(GC2_RF4_VIM)\n```", "```py\n> GC2_RF4_VIN <- important_variables(GC2_RF4, k = 5, \n+                                    measures = c(\"mean_min_depth\", \"no_of_trees\"))\n> GC2_RF4_VIN_Frame <- min_depth_interactions(GC2_RF4,GC2_RF4_VIN)\n>head(GC2_RF4_VIN_Frame[order(GC2_RF4_VIN_Frame$occurrences, decreasing = TRUE), ])\nvariable root_variable mean_min_depth occurrences       interaction\n7    amount      checking            1.6         442   checking:amount\n2       age      checking            2.0         433      checking:age\n27 duration      checking            2.1         426 checking:duration\n77  purpose      checking            2.0         420  checking:purpose\n32 employed      checking            2.6         417 checking:employed\n8    amount      duration            2.4         408   duration:amount\n   uncond_mean_min_depth\n7                    2.4\n2                    2.5\n27                   2.3\n77                   2.3\n32                   3.0\n8                    2.4\n> plot_min_depth_interactions(GC2_RF2_VIN_Frame)\n```", "```py\n> data(\"spam\")\n> set.seed(12345)\n> Train_Test <- sample(c(\"Train\",\"Test\"),nrow(spam),replace = TRUE,\n+ prob = c(0.7,0.3))\n> head(Train_Test)\n[1] \"Test\"  \"Test\"  \"Test\"  \"Test\"  \"Train\" \"Train\"\n> spam_Train <- spam[Train_Test==\"Train\",]\n> spam_TestX <- within(spam[Train_Test==\"Test\",],rm(type))\n> spam_TestY <- spam[Train_Test==\"Test\",\"type\"]\n> spam_Formula <- as.formula(\"type~.\")\n```", "```py\n> spam_ct <- rpart(spam_Formula,data=spam_Train)\n> spam_ct_predict <- predict(spam_ct,newdata=spam_TestX, \n+ type=\"class\")\n> ct_accuracy <- sum(spam_ct_predict==spam_TestY)/nrow(spam_TestX)\n> ct_accuracy\n[1] 0.8994\n```", "```py\n> spam_rf <- randomForest(spam_Formula,data=spam_Train,coob=TRUE,\n+                         ntree=500,keepX=TRUE,mtry=5)\n> spam_rf_predict <- predict(spam_rf,newdata=spam_TestX, \n+ type=\"class\")\n> rf_accuracy <- sum(spam_rf_predict==spam_TestY)/nrow(spam_TestX)\n> rf_accuracy\n[1] 0.9436\n```", "```py\n> spam_bag <- randomForest(spam_Formula,data=spam_Train,coob=TRUE,\n+ ntree=500,keepX=TRUE,mtry=ncol(spam_TestX))\n> spam_bag_predict <- predict(spam_bag,newdata=spam_TestX,\n+ type=\"class\")\n> bag_accuracy <- sum(spam_bag_predict==spam_TestY)/\n+ nrow(spam_TestX)\n> bag_accuracy\n[1] 0.935\n> windows(height=100,width=200)\n> par(mfrow=c(1,2))\n> plot(spam_rf,main=\"Random Forest for Spam Classification\")\n> plot(spam_bag,main=\"Bagging for Spam Classification\")\n```", "```py\n> TT <- read.csv(\"../Data/Travel_Times.csv\")\n>dim(TT)\n[1] 205  13\n>sum(is.na(TT))\n[1] 19\n>sapply(TT,function(x) sum(is.na(x)))\n          Date      StartTime      DayOfWeek        GoingTo       Distance \n             0              0              0              0              0 \n      MaxSpeed       AvgSpeed AvgMovingSpeed    FuelEconomy      TotalTime \n             0              0              0             19              0 \n    MovingTime     Take407All       Comments \n             0              0              0 \n> TT$FuelEconomy\n  [1]    NA    NA    NA    NA    NA    NA    NA    NA  8.89  8.89  8.89  8.89\n [13]  8.89  8.89  8.89  8.89  9.08  9.08  9.08  9.08  9.08  9.08  9.08  9.08\n [25]  9.76  9.76  9.76  9.76  9.76  9.76  9.76  9.16  9.16  9.16    NA    NA\n [37]    NA    NA    NA    NA    NA    NA  9.30  9.30  9.30  9.30  9.30  9.30\n [49] 10.05 10.05 10.05 10.05  9.53  9.53  9.53  9.53  9.53  9.53  9.53  9.53\n [61]  9.35  9.35  9.35  9.35  9.35  9.35  9.35  9.35  8.32  8.32  8.32  8.32\n\n[181]  8.48  8.48  8.48  8.45  8.45  8.45  8.45  8.45  8.45  8.45  8.45  8.45\n[193]  8.45  8.28  8.28  8.28  7.89  7.89  7.89  7.89  7.89  7.89    NA    NA\n[205]    NA\n```", "```py\n> TT_Missing <- missForest(TT[,-c(1,2,12)],\n+                          maxiter = 10,ntree=500,mtry=6)\nmissForest iteration 1 in progress...done!\nmissForest iteration 2 in progress...done!\nmissForest iteration 3 in progress...done!\nmissForest iteration 4 in progress...done!\n> TT_FuelEconomy <- cbind(TT_Missing$ximp[,7],TT$FuelEconomy)\n> TT_FuelEconomy[is.na(TT$FuelEconomy),]\n      [,1] [,2]\n [1,] 8.59   NA\n [2,] 8.91   NA\n [3,] 8.82   NA\n [4,] 8.63   NA\n [5,] 8.44   NA\n [6,] 8.63   NA\n [7,] 8.60   NA\n [8,] 8.50   NA\n [9,] 9.07   NA\n[10,] 9.10   NA\n[11,] 8.52   NA\n[12,] 9.12   NA\n[13,] 8.53   NA\n[14,] 8.85   NA\n[15,] 8.70   NA\n[16,] 9.42   NA\n[17,] 8.40   NA\n[18,] 8.49   NA\n[19,] 8.64   NA\n```", "```py\n>data(multishapes)\n>par(mfrow=c(1,2))\n>plot(multishapes[1:2],col=multishapes[,3], \n+      main=\"Six Multishapes Data Display\")\n> MS_RF <- randomForest(x=multishapes[1:2],y=NULL,ntree=1000,\n+ proximity=TRUE, oob.prox=TRUE,mtry = 1)\n```", "```py\n> MS_hclust <- hclust(as.dist(1-MS_RF$proximity),method=\"ward.D2\")\n> MS_RF_clust <- cutree(MS_hclust,k=6)\n>table(MS_RF_clust,multishapes$shape)\n\nMS_RF_clust   1   2   3   4   5   6\n          1 113   0   0   0  10   0\n          2 143   0   0   0  20  50\n3  57 170   0   0   3   0\n4  63  55   0   0   3   0\n5  24 175   0   0   2   0\n          6   0   0 100 100  12   0\n>plot(multishapes[1:2],col=MS_RF_clust,\n+      main=\"Clustering with Random Forest\n```"]