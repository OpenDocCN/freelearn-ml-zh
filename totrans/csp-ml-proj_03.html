<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Twitter Sentiment Analysis</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we are going to expand our knowledge of building classification models in C#. Along with the two packages, <span class="calibre5">Accord.NET and Deedle,</span> which we used in the previous chapter, we are going to start using the Stanford CoreNLP package to apply more advanced <strong class="calibre4">natural language processing</strong> (<strong class="calibre4">NLP</strong>) techniques, such as tokenization, <strong class="calibre4">part of speech</strong> (<strong class="calibre4">POS</strong>) tagging, and lemmatization. Using these packages, our goal for this chapter is to build a multi-class classification model that predicts the sentiments of tweets. We will be working with a raw Twitter dataset that contains not only words, but also emoticons, and will use it to train a <strong class="calibre4">machine learning</strong> (<strong class="calibre4">ML</strong>) model for sentiment prediction. We will be following the same steps that we follow when building ML models. We are going to start with the problem definition and then data preparation and analysis, feature engineering, and model development and validation. During our feature engineering step, we will expand our knowledge of NLP techniques and explore how we can apply tokenization, POS tagging, and lemmatization to build more advanced text features. In the model building step, we are going to explore a new classification algorithm, a random forest classifier, and compare its performance to the Naive Bayes classifier. Lastly, in our model validation step, we are going to expand our knowledge of confusion matrixes, precision, and recall, which we covered in the previous chapter, and discuss what the <strong class="calibre4">Receiver Operating Characteristic</strong> (<strong class="calibre4">ROC</strong>) curve and <strong class="calibre4">area under the curve</strong> (<strong class="calibre4">AUC</strong>) are and how these concepts can be used to evaluate our ML models.</p>
<p class="calibre2">In this chapter, we will cover the following:</p>
<ul class="calibre10">
<li class="calibre11">Setting up the environment with the Stanford CoreNLP package</li>
<li class="calibre11">Problem definition for the Twitter sentiment analysis project</li>
<li class="calibre11">Data preparation using Stanford CoreNLP</li>
<li class="calibre11">Data analysis using lemmas as tokens</li>
<li class="calibre11">Feature engineering using lemmatization and emoticons</li>
<li class="calibre11">Naive Bayes versus random forest</li>
<li class="calibre11">Model validations using the ROC curve and AUC metrics</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up the environment</h1>
                
            
            <article>
                
<p class="calibre2">Before we dive into our Twitter sentiment analysis project, let's set up our development environment with the Stanford CoreNLP package that we are going to use throughout this chapter. Multiple steps are required to get your environment ready with the Standford CoreNLP package, so it is a good idea to work through this:</p>
<ol class="calibre16">
<li value="1" class="calibre11">The first step is to create a new <span>Console App (.NET Framework)</span> project in Visual Studio. Make sure you use a .NET Framework version higher than or equal to 4.6.1. If you have an older version installed, go to <a href="https://docs.microsoft.com/en-us/dotnet/framework/install/guide-for-developers" target="_blank" class="calibre9">https://docs.microsoft.com/en-us/dotnet/framework/install/guide-for-developers</a> and follow the installation guide. Following is a screenshot of the project setup page (note that you can select your .NET Framework version in the top bar):</li>
</ol>
<div class="mce-root"><img class="alignnone4" src="../images/00040.jpeg"/></div>
<ol start="2" class="calibre16">
<li value="2" class="calibre11">Now, let's install the Stanford CoreNLP package. You can type in the following command in your <span>Package Manager Console</span>:</li>
</ol>
<pre class="calibre56"><strong class="calibre1">Install-Package Stanford.NLP.CoreNLP</strong></pre>
<div class="packt_infobox"><span class="calibre57">The version we are going to use in this chapter is <kbd class="calibre58">Stanford.NLP.CoreNLP</kbd> 3.9.1. Over time, the versions might change and you might have to update your installations.</span></div>
<ol start="3" class="calibre16">
<li class="calibre11" value="3">We just have to do one more thing and our environment will be ready to start using the package. We need to install <span><span>the CoreNLP models JAR, which contains various models for parsing, POS tagging, <strong class="calibre1">Named Entity Recognition</strong> (<strong class="calibre1">NER</strong>), and some other tools. Follow this link to download and unzip Stanford CoreNLP: <a href="https://stanfordnlp.github.io/CoreNLP/" target="_blank" class="calibre9">https://stanfordnlp.github.io/CoreNLP/</a>. Once you have downloaded and unzipped it, you will see multiple files in there. The particular file of interest is </span></span><kbd class="calibre12">stanford-corenlp-&lt;version-number&gt;-models.jar</kbd><span><span>. We need to extract the contents from that jar file into a directory so that we can load all the model files within our C# project. You can use the following command to extract the contents from </span></span><kbd class="calibre12">stanford-corenlp-&lt;version-number&gt;-models.jar</kbd><span><span>:<br class="title-page-name"/></span></span></li>
</ol>
<pre class="calibre56"><span><strong class="calibre1">jar xf stanford-corenlp-&lt;version-number&gt;-models.jar</strong> </span></pre>
<p class="calibre2">When you are done extracting all the model files from the models jar file, you are now ready to start using the Stanford CoreNLP package in your C# project.</p>
<p class="calibre2">Now, let's check whether our installation was successful. The following code is a slight modification for this example (<a href="https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html" target="_blank" class="calibre9">https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html</a>) :</p>
<pre class="calibre19">using System;<br class="title-page-name"/>using System.IO;<br class="title-page-name"/>using java.util;<br class="title-page-name"/>using java.io;<br class="title-page-name"/>using edu.stanford.nlp.pipeline;<br class="title-page-name"/>using Console = System.Console;<br class="title-page-name"/><br class="title-page-name"/>namespace Tokenizer<br class="title-page-name"/>{<br class="title-page-name"/>    class Program<br class="title-page-name"/>    {<br class="title-page-name"/>        static void Main()<br class="title-page-name"/>        {<br class="title-page-name"/>            // Path to the folder with models extracted from Step #3<br class="title-page-name"/>            var jarRoot = @"&lt;path-to-your-model-files-dir&gt;";<br class="title-page-name"/><br class="title-page-name"/>            // Text for processing<br class="title-page-name"/>            var text = "We're going to test our CoreNLP installation!!";<br class="title-page-name"/><br class="title-page-name"/>            // Annotation pipeline configuration<br class="title-page-name"/>            var props = new Properties();<br class="title-page-name"/>            props.setProperty("annotators", "tokenize, ssplit, pos, lemma");<br class="title-page-name"/>            props.setProperty("ner.useSUTime", "0");<br class="title-page-name"/><br class="title-page-name"/>            // We should change current directory, so StanfordCoreNLP could find all the model files automatically<br class="title-page-name"/>            var curDir = Environment.CurrentDirectory;<br class="title-page-name"/>            Directory.SetCurrentDirectory(jarRoot);<br class="title-page-name"/>            var pipeline = new StanfordCoreNLP(props);<br class="title-page-name"/>            Directory.SetCurrentDirectory(curDir);<br class="title-page-name"/><br class="title-page-name"/>            // Annotation<br class="title-page-name"/>            var annotation = new Annotation(text);<br class="title-page-name"/>            pipeline.annotate(annotation);<br class="title-page-name"/><br class="title-page-name"/>            // Result - Pretty Print<br class="title-page-name"/>            using (var stream = new ByteArrayOutputStream())<br class="title-page-name"/>            {<br class="title-page-name"/>                pipeline.prettyPrint(annotation, new PrintWriter(stream));<br class="title-page-name"/>                Console.WriteLine(stream.toString());<br class="title-page-name"/>                stream.close();<br class="title-page-name"/>            }<br class="title-page-name"/><br class="title-page-name"/>            Console.ReadKey();<br class="title-page-name"/>        }<br class="title-page-name"/>    }<br class="title-page-name"/>}</pre>
<p class="calibre2">If your installation was successful, you should see output similar to the following:</p>
<div class="mce-root"><img class="alignnone5" src="../images/00041.gif"/></div>
<p class="calibre2">Let's take a closer look at this output. Tokens are character sequences that are grouped as individual semantic units. Often, tokens are <em class="calibre13">words</em> or <em class="calibre13">terms</em>. In each token output, we can see the original text, such as <kbd class="calibre12">We</kbd>, <kbd class="calibre12">'re</kbd>, and <kbd class="calibre12">going</kbd>. The <kbd class="calibre12">PartOfSpeech</kbd> tag refers to the category of each word, such as noun, verb, and adjective. For example, the <kbd class="calibre12">PartOfSpeech</kbd> tag of the first token <span class="calibre5">in our example</span>, <kbd class="calibre12">We</kbd>, is <kbd class="calibre12">PRP</kbd> and it stands for <em class="calibre13">personal pronoun</em>. The <kbd class="calibre12">PartOfSpeech</kbd> tag of the second token in our example, <kbd class="calibre12">'re</kbd>, is <kbd class="calibre12">VBP</kbd> and it stands for <em class="calibre13">v</em><span class="calibre5"><em class="calibre13">erb, non-third-person singular present</em>. The complete list of POS tags can be found here (<a href="http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html" target="_blank" class="calibre9">http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a>) or in the following screenshot:</span></p>
<div class="mce-root"><img src="../images/00042.jpeg" class="calibre59"/></div>
<div class="packt_figref">A list of POS tags</div>
<p class="calibre2">Lastly, the <kbd class="calibre12">Lemma</kbd> tag in our tokenization example refers to the standard form of the given word. For example, the lemma of <kbd class="calibre12">am</kbd> and <kbd class="calibre12">are</kbd> is <kbd class="calibre12">be</kbd>. In our example, the word <kbd class="calibre12">going</kbd> in our third token has <kbd class="calibre12">go</kbd> as its lemma. We will discuss how we can use word lemmatization for feature engineering<span class="calibre5"> in the following sections</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Problem definition for Twitter sentiment analysis</h1>
                
            
            <article>
                
<p class="calibre2">Let's start our Twitter sentiment analysis project by clearly defining what models we will be building and what they are going to predict. You might have heard the term <strong class="calibre4">sentiment analysis</strong> in the past already. Sentiment analysis is essentially a process of computationally determining whether a given text expresses a positive, neutral, or negative emotion. Sentiment analysis for social media content can be used in various ways. For example, it can be used by marketers to identify how effective a marketing campaign was and how it affected consumers' opinions and attitudes towards a certain product or company. Sentiment analysis can also be used to predict stock market changes. Positive news and aggregate positive emotions towards a certain company often move its stock price in a positive direction, and sentiment analysis in the news and social media for a given company can be used to predict how stock prices will move in the near future. To experiment with how we can build a sentiment analysis model, we are going to use a precompiled and labeled airline sentiment Twitter dataset that originally came from CrowdFlower's Data for Everyone library (<a href="https://www.figure-eight.com/data-for-everyone/" target="_blank" class="calibre9">https://www.figure-eight.com/data-for-everyone/</a>). Then, we are going to apply some NLP techniques, especially word tokenization, POS tagging, and lemmatization, to build meaningful text and emoticon features from raw tweet data. Since <span class="calibre5">we want to predict</span> three different emotions (<span class="calibre5">positive, neutral, and negative)</span> <span class="calibre5">for each tweet</span>, we are going to build a multi-class classification model and experiment with different learning algorithms—Naive Bayes and random forest. Once we build the sentiment analysis models, we are going to evaluate the performance mainly via these three metrics: precision, recall, and AUC.</p>
<p class="calibre2">Let's summarize our problem definition for the Twitter sentiment analysis project:</p>
<ul class="calibre10">
<li class="calibre11">What is the problem? We need a Twitter sentiment analysis model to computationally identify the emotions in tweets.</li>
<li class="calibre11">Why is it a problem? Identifying and measuring the emotions of users or consumers about a certain topic, such as a product, company, advertisement, and so forth, are often an essential tool to measure the impact and success of certain tasks.</li>
<li class="calibre11">What are some of the approaches to solving this problem? We are going to use the Stanford CoreNLP package to apply various NLP techniques, such as tokenization, POS tagging, and lemmatization, to build meaningful features from a raw Twitter dataset. With these features, we are going to experiment with different learning algorithms to build a sentiment analysis model. We will use precision, recall, and AUC measures to evaluate the performance of the models.</li>
<li class="calibre11">What are the success criteria? We want high precision rates, without sacrificing too much for recall rates, as correctly classifying a tweet into one of three emotion buckets (positive, neutral, and negative) is more important than a higher retrieval rate. Also, we want a high AUC number, which we will discuss in more detail in later sections of this chapter.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data preparation using Stanford CoreNLP</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Now that we know what our goals are in this chapter, it is time to dive into the data. Similar to the last chapter, we are going to use precompiled and pre-labeled Twitter sentiment data. We are going to use a dataset from </span>CrowdFlower's Data for Everyone library (<a href="https://www.figure-eight.com/data-for-everyone/" target="_blank" class="calibre9">https://www.figure-eight.com/data-for-everyone/</a>) a<span class="calibre5">nd you can download the data from this link: <a href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment" target="_blank" class="calibre9">https://www.kaggle.com/crowdflower/twitter-airline-sentiment</a>. The data we have here is about 15,000 tweets about US airlines. This Twitter data was scraped from February of 2015 and was then labeled into three buckets—positive, negative, and neutral. The link provides you with two types of data: a CSV file and an SQLite database. We are going to work with the CSV file for this project.</span></p>
<p class="calibre2">Once you have downloaded this data, we need to get it prepared for our future analysis and model building. The two columns of interest in the dataset are <kbd class="calibre12">airline_sentiment</kbd> and <kbd class="calibre12">text</kbd>. The <kbd class="calibre12">airline_sentiment</kbd> column contains information about the sentiment—whether a tweet has positive, negative, or neutral sentiments—and the <kbd class="calibre12">text</kbd> column contains the raw Twitter text. To make this raw data readily available for our future data analysis and model building steps, we need to do the following tasks:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Clean up unnecessary text</strong>: It's hard to justify some parts of the text as providing many insights and much information for our models to learn from, such as URLs, user IDs, and raw numbers. So, the first step to prepare our raw data is to clean up unnecessary text that does not contain much information. In this example, we removed the URLs, Twitter user IDs, numbers, and hash signs in hashtags. We used <kbd class="calibre12">Regex</kbd> to replace such texts with empty strings. The following code illustrates the <kbd class="calibre12">Regex</kbd> expressions we used to filter out those texts:</li>
</ul>
<pre class="calibre56">// 1. Remove URL's<br class="title-page-name"/>string urlPattern = @"https?:\/\/\S+\b|www\.(\w+\.)+\S*";<br class="title-page-name"/>Regex rgx = new Regex(urlPattern);<br class="title-page-name"/>tweet = rgx.Replace(tweet, "");<br class="title-page-name"/><br class="title-page-name"/>// 2. Remove Twitter ID's<br class="title-page-name"/>string userIDPattern = @"@\w+";<br class="title-page-name"/>rgx = new Regex(userIDPattern);<br class="title-page-name"/>tweet = rgx.Replace(tweet, "");<br class="title-page-name"/><br class="title-page-name"/>// 3. Remove Numbers<br class="title-page-name"/>string numberPattern = @"[-+]?[.\d]*[\d]+[:,.\d]*";<br class="title-page-name"/>tweet = Regex.Replace(tweet, numberPattern, "");<br class="title-page-name"/><br class="title-page-name"/>// 4. Replace Hashtag<br class="title-page-name"/>string hashtagPattern = @"#";<br class="title-page-name"/>tweet = Regex.Replace(tweet, hashtagPattern, "");</pre>
<p class="calibre2">As you can see from this code, there are two ways to replace a string that matches a <kbd class="calibre12">Regex</kbd> pattern. You can instantiate a <kbd class="calibre12">Regex</kbd> object and then replace matching strings with the other string, as shown in the first two cases. You can also directly call the static <kbd class="calibre12">Regex.Replace</kbd> method for the same purpose, as shown in the last two cases. The static method is going to create a <kbd class="calibre12">Regex</kbd> object each time you call the <kbd class="calibre12">Regex.Replace</kbd> method, so if you are using the same pattern in multiple places, it will be better to go with the first approach:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Group and encode similar emoticons together</strong>: Emoticons, such as smiley faces and sad faces, are frequently used in tweets and provide useful insights about the emotion of each tweet. Intuitively, one user will use smiley face emoticons to tweet about positive events, while another will use sad face emoticons to tweet about negative events. However, different smiley faces show similar positive emotions and can be grouped together. For example, a smiley face with a parenthesis, <kbd class="calibre12">:)</kbd>, will have the same meaning as another smiley face with a capital letter <kbd class="calibre12">D</kbd>, <kbd class="calibre12">:D</kbd>. So, we want to group these similar emoticons together and encode them as one group rather than having them in separate groups. We will use the R code that Romain Paulus and Jeffrey Pennington shared (<a href="https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb" target="_blank" class="calibre9">https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb</a>), translate it into C#, and then apply it to our raw Twitter dataset. The following is how we translated the emoticon <kbd class="calibre12">Regex</kbd> codes, written in R, into C#, so that we can group and encode similar emoticons together:</li>
</ul>
<pre class="calibre56">// 1. Replace Smiley Faces<br class="title-page-name"/>string smileyFacePattern = String.Format(@"{0}{1}[)dD]+|[)dD]+{1}{0}", eyesPattern, nosePattern);<br class="title-page-name"/>tweet = Regex.Replace(tweet, smileyFacePattern, " emo_smiley ");<br class="title-page-name"/><br class="title-page-name"/>// 2. Replace LOL Faces<br class="title-page-name"/>string lolFacePattern = String.Format(@"{0}{1}[pP]+", eyesPattern, nosePattern);<br class="title-page-name"/>tweet = Regex.Replace(tweet, lolFacePattern, " emo_lol ");<br class="title-page-name"/><br class="title-page-name"/>// 3. Replace Sad Faces<br class="title-page-name"/>string sadFacePattern = String.Format(@"{0}{1}\(+|\)+{1}{0}", eyesPattern, nosePattern);<br class="title-page-name"/>tweet = Regex.Replace(tweet, sadFacePattern, " emo_sad ");<br class="title-page-name"/><br class="title-page-name"/>// 4. Replace Neutral Faces<br class="title-page-name"/>string neutralFacePattern = String.Format(@"{0}{1}[\/|l*]", eyesPattern, nosePattern);<br class="title-page-name"/>tweet = Regex.Replace(tweet, neutralFacePattern, " emo_neutral ");<br class="title-page-name"/><br class="title-page-name"/>// 5. Replace Heart<br class="title-page-name"/>string heartPattern = "&lt;3";<br class="title-page-name"/>tweet = Regex.Replace(tweet, heartPattern, " emo_heart ");</pre>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Group and encode additional helpful expressions together</strong>: Lastly, there are some more expressions that can help our models detect the emotions of tweets. Repeated punctuation, such as <kbd class="calibre12">!!!</kbd> and <kbd class="calibre12">???</kbd>, and elongated words, such as <kbd class="calibre12">wayyyy</kbd> and <kbd class="calibre12">soooo</kbd>, can provide some extra information about the sentiments of tweets. We will group and encode them separately so that our models can learn from such expressions. The following code shows how we encoded such expressions:</li>
</ul>
<pre class="calibre56">// 1. Replace Punctuation Repeat<br class="title-page-name"/>string repeatedPunctuationPattern = @"([!?.]){2,}";<br class="title-page-name"/>tweet = Regex.Replace(tweet, repeatedPunctuationPattern, " $1_repeat ");<br class="title-page-name"/><br class="title-page-name"/>// 2. Replace Elongated Words (i.e. wayyyy -&gt; way_emphasized)<br class="title-page-name"/>string elongatedWordsPattern = @"\b(\S*?)(.)\2{2,}\b";<br class="title-page-name"/>tweet = Regex.Replace(tweet, elongatedWordsPattern, " $1$2_emphasized ");</pre>
<p class="calibre2">As shown in the code, for repeated punctuation we appended a string with a suffix, <kbd class="calibre12">_repeat</kbd>. For example, <kbd class="calibre12">!!!</kbd> will become <kbd class="calibre12">!_repeat</kbd> and <kbd class="calibre12">???</kbd> will become <kbd class="calibre12">?_repeat</kbd>. For elongated words, we appended a string with a suffix, <kbd class="calibre12">_emphasized</kbd>. For example, <kbd class="calibre12">wayyyy</kbd> will become <kbd class="calibre12">way_emphasized</kbd> and <kbd class="calibre12">soooo</kbd> will become <kbd class="calibre12">so_emphasized</kbd>.</p>
<p class="calibre2"><span class="calibre5">The full code that takes the raw dataset, processes individual Twitter text as discussed previously, and exports the processed Twitter text into another data file can be found in this repository: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataProcessor.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataProcessor.cs</a>. Let's briefly walk through the code. It first reads the raw <kbd class="calibre12">Tweets.csv</kbd> dataset into a Deedle data frame (lines 76–82). Then, it calls a method named</span> <kbd class="calibre12">FormatTweets</kbd><span class="calibre5"> with a column series that contains all the raw Twitter text. The </span><kbd class="calibre12">FormatTweets</kbd><span class="calibre5"> method code in lines 56–65 looks like the following:</span></p>
<pre class="calibre19">private static string[] FormatTweets(Series&lt;int, string&gt; rows)<br class="title-page-name"/>{<br class="title-page-name"/>    var cleanTweets = rows.GetAllValues().Select((x, i) =&gt;<br class="title-page-name"/>    {<br class="title-page-name"/>        string tweet = x.Value;<br class="title-page-name"/>        return CleanTweet(tweet);<br class="title-page-name"/>    });<br class="title-page-name"/><br class="title-page-name"/>    return cleanTweets.ToArray();<br class="title-page-name"/>}</pre>
<p class="calibre2">This <kbd class="calibre12">FormatTweets</kbd><em class="calibre13"> </em>method iterates through each element in the series, which is the raw tweets, and calls the <kbd class="calibre12">CleanTweet</kbd> method. Within the <kbd class="calibre12">CleanTweet</kbd> method, each raw tweet is run against all the <kbd class="calibre12">Regex</kbd> patterns that we defined previously and is then processed as discussed earlier. The <kbd class="calibre12">CleanTweet</kbd><span class="calibre5"> method in lines 11–54 looks as follows:</span></p>
<pre class="calibre19">private static string CleanTweet(string rawTweet)<br class="title-page-name"/>{<br class="title-page-name"/>      string eyesPattern = @"[8:=;]";<br class="title-page-name"/>      string nosePattern = @"['`\-]?";<br class="title-page-name"/><br class="title-page-name"/>      string tweet = rawTweet;<br class="title-page-name"/>      // 1. Remove URL's<br class="title-page-name"/>      string urlPattern = @"https?:\/\/\S+\b|www\.(\w+\.)+\S*";<br class="title-page-name"/>      Regex rgx = new Regex(urlPattern);<br class="title-page-name"/>      tweet = rgx.Replace(tweet, "");<br class="title-page-name"/>      // 2. Remove Twitter ID's<br class="title-page-name"/>      string userIDPattern = @"@\w+";<br class="title-page-name"/>      rgx = new Regex(userIDPattern);<br class="title-page-name"/>      tweet = rgx.Replace(tweet, "");<br class="title-page-name"/>      // 3. Replace Smiley Faces<br class="title-page-name"/>      string smileyFacePattern = String.Format(@"{0}{1}[)dD]+|[)dD]+{1}{0}", eyesPattern, nosePattern);<br class="title-page-name"/>      tweet = Regex.Replace(tweet, smileyFacePattern, " emo_smiley ");<br class="title-page-name"/>      // 4. Replace LOL Faces<br class="title-page-name"/>      string lolFacePattern = String.Format(@"{0}{1}[pP]+", eyesPattern, nosePattern);<br class="title-page-name"/>      tweet = Regex.Replace(tweet, lolFacePattern, " emo_lol ");<br class="title-page-name"/>      // 5. Replace Sad Faces<br class="title-page-name"/>      string sadFacePattern = String.Format(@"{0}{1}\(+|\)+{1}{0}", eyesPattern, nosePattern);<br class="title-page-name"/>      tweet = Regex.Replace(tweet, sadFacePattern, " emo_sad ");<br class="title-page-name"/>      // 6. Replace Neutral Faces<br class="title-page-name"/>      string neutralFacePattern = String.Format(@"{0}{1}[\/|l*]", eyesPattern, nosePattern);<br class="title-page-name"/>      tweet = Regex.Replace(tweet, neutralFacePattern, " emo_neutral ");<br class="title-page-name"/>      // 7. Replace Heart<br class="title-page-name"/>      string heartPattern = "&lt;3";<br class="title-page-name"/>      tweet = Regex.Replace(tweet, heartPattern, " emo_heart ");<br class="title-page-name"/>      // 8. Replace Punctuation Repeat<br class="title-page-name"/>      string repeatedPunctuationPattern = @"([!?.]){2,}";<br class="title-page-name"/>      tweet = Regex.Replace(tweet, repeatedPunctuationPattern, " $1_repeat ");<br class="title-page-name"/>      // 9. Replace Elongated Words (i.e. wayyyy -&gt; way_emphasized)<br class="title-page-name"/>      string elongatedWordsPattern = @"\b(\S*?)(.)\2{2,}\b";<br class="title-page-name"/>      tweet = Regex.Replace(tweet, elongatedWordsPattern, " $1$2_emphasized ");<br class="title-page-name"/>      // 10. Replace Numbers<br class="title-page-name"/>      string numberPattern = @"[-+]?[.\d]*[\d]+[:,.\d]*";<br class="title-page-name"/>      tweet = Regex.Replace(tweet, numberPattern, "");<br class="title-page-name"/>      // 11. Replace Hashtag<br class="title-page-name"/>      string hashtagPattern = @"#";<br class="title-page-name"/>      tweet = Regex.Replace(tweet, hashtagPattern, "");<br class="title-page-name"/><br class="title-page-name"/>      return tweet;<br class="title-page-name"/>}</pre>
<p class="calibre2">Once all the raw Twitter tweets are cleaned and processed, the result gets added to the original Deedle data frame as a separate column with <kbd class="calibre12">tweet</kbd> as its column name. The following code (line 89) shows how you can add an array of strings to a data frame:</p>
<pre class="calibre19">rawDF.AddColumn("tweet", processedTweets);</pre>
<p class="calibre2">Once you have come this far, the only additional step we need to do is export the processed data. Using Deedle data frame's <kbd class="calibre12">SaveCsv</kbd> method, you can easily export a data frame into a CSV file. The following code shows how we exported the processed data into a CSV file:</p>
<pre class="calibre19">rawDF.SaveCsv(Path.Combine(dataDirPath, "processed-training.csv"));</pre>
<p class="calibre2">Now that we have clean Twitter text, let's tokenize and create a matrix representation of tweets. Similar to what we did in <a target="_blank" href="part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 2</a>, <em class="calibre13">Spam Email Filtering</em>, we are going to break a string into words. However, we are going to use the Stanford CoreNLP package that we installed in the previous section of this chapter and utilize the sample code that we wrote in the previous section. The code to tokenize tweets and build a matrix representation of them is as follows:</p>
<pre class="calibre19">private static Frame&lt;int, string&gt; CreateWordVec(Series&lt;int, string&gt; rows, ISet&lt;string&gt; stopWords, bool useLemma=false)<br class="title-page-name"/>        {<br class="title-page-name"/>            // Path to the folder with models extracted from `stanford-corenlp-&lt;version&gt;-models.jar`<br class="title-page-name"/>            var jarRoot = @"&lt;path-to-model-files-dir&gt;";<br class="title-page-name"/><br class="title-page-name"/>            // Annotation pipeline configuration<br class="title-page-name"/>            var props = new Properties();<br class="title-page-name"/>            props.setProperty("annotators", "tokenize, ssplit, pos, lemma");<br class="title-page-name"/>            props.setProperty("ner.useSUTime", "0");<br class="title-page-name"/><br class="title-page-name"/>            // We should change current directory, so StanfordCoreNLP could find all the model files automatically<br class="title-page-name"/>            var curDir = Environment.CurrentDirectory;<br class="title-page-name"/>            Directory.SetCurrentDirectory(jarRoot);<br class="title-page-name"/>            var pipeline = new StanfordCoreNLP(props);<br class="title-page-name"/>            Directory.SetCurrentDirectory(curDir);<br class="title-page-name"/><br class="title-page-name"/>            var wordsByRows = rows.GetAllValues().Select((x, i) =&gt;<br class="title-page-name"/>            {<br class="title-page-name"/>                var sb = new SeriesBuilder&lt;string, int&gt;();<br class="title-page-name"/><br class="title-page-name"/>                // Annotation<br class="title-page-name"/>                var annotation = new Annotation(x.Value);<br class="title-page-name"/>                pipeline.annotate(annotation);<br class="title-page-name"/><br class="title-page-name"/>                var tokens = annotation.get(typeof(CoreAnnotations.TokensAnnotation));<br class="title-page-name"/>                ISet&lt;string&gt; terms = new HashSet&lt;string&gt;();<br class="title-page-name"/><br class="title-page-name"/>                foreach (CoreLabel token in tokens as ArrayList)<br class="title-page-name"/>                {<br class="title-page-name"/>                    string lemma = token.lemma().ToLower();<br class="title-page-name"/>                    string word = token.word().ToLower();<br class="title-page-name"/>                    string tag = token.tag();<br class="title-page-name"/>                    //Console.WriteLine("lemma: {0}, word: {1}, tag: {2}", lemma, word, tag);<br class="title-page-name"/><br class="title-page-name"/>                    // Filter out stop words and single-character words<br class="title-page-name"/>                    if (!stopWords.Contains(lemma) &amp;&amp; word.Length &gt; 1)<br class="title-page-name"/>                    {<br class="title-page-name"/>                        if (!useLemma)<br class="title-page-name"/>                        {<br class="title-page-name"/>                            terms.Add(word);<br class="title-page-name"/>                        }<br class="title-page-name"/>                        else<br class="title-page-name"/>                        {<br class="title-page-name"/>                            terms.Add(lemma);<br class="title-page-name"/>                        }<br class="title-page-name"/>                    }<br class="title-page-name"/>                }<br class="title-page-name"/><br class="title-page-name"/>                foreach (string term in terms)<br class="title-page-name"/>                {<br class="title-page-name"/>                    sb.Add(term, 1);<br class="title-page-name"/>                }<br class="title-page-name"/><br class="title-page-name"/>                return KeyValue.Create(i, sb.Series);<br class="title-page-name"/>            });<br class="title-page-name"/><br class="title-page-name"/>            // Create a data frame from the rows we just created<br class="title-page-name"/>            // And encode missing values with 0<br class="title-page-name"/>            var wordVecDF = Frame.FromRows(wordsByRows).FillMissing(0);<br class="title-page-name"/><br class="title-page-name"/>            return wordVecDF;<br class="title-page-name"/>        }</pre>
<p class="calibre2">As you can see from the code, the main difference between this code and the sample code in the previous section is that this code iterates over each tweet and stores the tokens into a Deedle's data frame. As in <a target="_blank" href="part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 2</a>, <em class="calibre13">Spam Email Filtering,</em> we are using one-hot encoding to assign each term's value (0 versus 1) within the matrix. One thing to note here is how we have an option to create the matrix with lemmas or words. Words are the original untouched terms that are broken down from each tweet. For example, a string, <kbd class="calibre12">I am a data scientist</kbd>, will be broken down into <kbd class="calibre12">I</kbd>, <kbd class="calibre12">am</kbd>, <kbd class="calibre12">a</kbd>, <kbd class="calibre12">data</kbd>, and <kbd class="calibre12">scientist</kbd>, if you use words as tokens. Lemmas are standard forms of words in each token. For example, the same string, <kbd class="calibre12">I am a data scientist</kbd>, will be broken down into <kbd class="calibre12">I</kbd>, <kbd class="calibre12">be</kbd>, <kbd class="calibre12">a</kbd>, <kbd class="calibre12">data</kbd>, and <kbd class="calibre12">scientist</kbd>, if you use lemmas as tokens. Note that <kbd class="calibre12">be</kbd> is a lemma for <kbd class="calibre12">am</kbd>. We will discuss what lemmas are and what lemmatization is in the <em class="calibre13">Feature engineering using lemmatization and emoticons</em> section.</p>
<p class="calibre2">The full code to tokenize and create a matrix representation of tweets can be found here: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterTokenizer.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterTokenizer.cs</a>. There are a few things to note in this code. First, let's look at how it counts how many samples we have for each sentiment. The following code snippet (lines 122–127) shows how we computed the number of samples per sentiment:</p>
<pre class="calibre19">// Look at the sentiment distributions in our sample set<br class="title-page-name"/>var sampleSetDistribution = rawDF.GetColumn&lt;string&gt;(<br class="title-page-name"/>    "airline_sentiment"<br class="title-page-name"/>).GroupBy&lt;string&gt;(x =&gt; x.Value).Select(x =&gt; x.Value.KeyCount);<br class="title-page-name"/>sampleSetDistribution.Print();</pre>
<p class="calibre2">As you can see from this code, we first get the sentiment column, <kbd class="calibre12">airline_sentiment</kbd>, and group it by the values, where the values can be <kbd class="calibre12">neutral</kbd>, <kbd class="calibre12">negative</kbd>, or <kbd class="calibre12">positive</kbd>. Then, it counts the number of occurrences and returns the count.</p>
<p class="calibre2">The second thing to note in the TwitterTokenizer code is how we encoded sentiments with integer values. The following is what you see in lines 149–154 of the full code:</p>
<pre class="calibre19">tweetLemmaVecDF.AddColumn(<br class="title-page-name"/>    "tweet_polarity", <br class="title-page-name"/>    rawDF.GetColumn&lt;string&gt;("airline_sentiment").Select(<br class="title-page-name"/>        x =&gt; x.Value == "neutral" ? 0 : x.Value == "positive" ? 1 : 2<br class="title-page-name"/>    )<br class="title-page-name"/>);</pre>
<p class="calibre2">As you can see from this code snippet, we are creating and adding a new column, <kbd class="calibre12">tweet_polarity</kbd>, to the term matrix data frame. We are taking the values of the <kbd class="calibre12">airline_sentiment</kbd> column and encoding <kbd class="calibre12">0</kbd> for neutral, <kbd class="calibre12">1</kbd> for positive, and <kbd class="calibre12">2</kbd> for negative. We are going to use this newly added column in our future model building steps.</p>
<p class="calibre2">Lastly, note how we are calling the <kbd class="calibre12">CreateWordVec</kbd> method twice—once without lemmatization (lines 135-144) and once with lemmatization (lines 147-156). If we create a term matrix with one-hot encodings without lemmatization, we are essentially taking all the words as individual tokens in our term matrix. As you can imagine, this will create a much larger and more sparse matrix than one with lemmatization. We left both codes there for you to explore both options. You can try building ML models with a matrix with words as columns and compare them against those with lemmas as columns. In this chapter, we are going to use the matrix with lemmas instead of words.</p>
<p class="calibre2">When you run this code, it will output a bar chart that shows the sentiment distribution in the sample set. As you can see in the following chart, we have about 3,000 neutral tweets, 2,000 positive tweets, and 9,000 negative tweets in our sample set. The chart looks as follows:</p>
<div class="mce-root"><img src="../images/00043.jpeg" class="calibre60"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data analysis using lemmas as tokens</h1>
                
            
            <article>
                
<p class="calibre2">It is now time to look at the actual data and seek any patterns or differences in the distributions of term frequencies along with the different sentiments of tweets. We are going to take the output from the previous step and get the distributions of the top seven most frequently occurring tokens for each sentiment. In this example, we use a term matrix with lemmas. Feel free to run the same analysis for a term matrix with words. The code to analyze the top N most frequently used tokens in each sentiment of tweets can be found here: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataAnalyzer.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/DataAnalyzer.cs</a>.</p>
<p class="calibre2">There is one thing to note in this code. Unlike in the previous chapter, we need to compute term frequencies for three sentiment classes—neutral, negative, and positive. The following is the code snippet from the full code (lines 54-73):</p>
<pre class="calibre19">var neutralTermFrequencies = ColumnWiseSum(<br class="title-page-name"/>    tweetLemmaDF.Where(<br class="title-page-name"/>        x =&gt; x.Value.GetAs&lt;int&gt;("tweet_polarity") == 0<br class="title-page-name"/>    ),<br class="title-page-name"/>    "tweet_polarity"<br class="title-page-name"/>).Sort().Reversed;<br class="title-page-name"/><br class="title-page-name"/>var positiveTermFrequencies = ColumnWiseSum(<br class="title-page-name"/>    tweetLemmaDF.Where(<br class="title-page-name"/>        x =&gt; x.Value.GetAs&lt;int&gt;("tweet_polarity") == 1<br class="title-page-name"/>    ),<br class="title-page-name"/>    "tweet_polarity"<br class="title-page-name"/>).Sort().Reversed;<br class="title-page-name"/><br class="title-page-name"/>var negativeTermFrequencies = ColumnWiseSum(<br class="title-page-name"/>    tweetLemmaDF.Where(<br class="title-page-name"/>        x =&gt; x.Value.GetAs&lt;int&gt;("tweet_polarity") == 2<br class="title-page-name"/>    ),<br class="title-page-name"/>    "tweet_polarity"<br class="title-page-name"/>).Sort().Reversed;</pre>
<p class="calibre2">As you can see from the code, we call the <kbd class="calibre12">ColumnWiseSum</kbd> method for each sentiment class, and the code for this method is as follows:</p>
<pre class="calibre19">private static Series&lt;string, double&gt; ColumnWiseSum(Frame&lt;int, string&gt; frame, string exclude)<br class="title-page-name"/>{<br class="title-page-name"/>    var sb = new SeriesBuilder&lt;string, double&gt;();<br class="title-page-name"/>    foreach(string colname in frame.ColumnKeys)<br class="title-page-name"/>    {<br class="title-page-name"/>        double frequency = frame[colname].Sum();<br class="title-page-name"/>        if (!colname.Equals(exclude))<br class="title-page-name"/>        {<br class="title-page-name"/>            sb.Add(colname, frequency);<br class="title-page-name"/>        }<br class="title-page-name"/>    }<br class="title-page-name"/><br class="title-page-name"/>    return sb.ToSeries();<br class="title-page-name"/>}</pre>
<p class="calibre2">As you can see from this code, it iterates through each column or term and sums all the values within that column. Since we used one-hot encodings, a simple column-wise sum will give us the number of occurrences for each term in our Twitter dataset. Once we have computed all the column-wise summations, we return them as a Deedle series object. With these results, we rank-order the terms by their frequencies and store this information into three separate files, <kbd class="calibre12">neutral-frequencies.csv</kbd>, <kbd class="calibre12">negative-frequencies.csv</kbd>, and <kbd class="calibre12">positive-frequencies.csv</kbd>. We are going to use the term frequency output in later sections for feature engineering and model building.</p>
<p class="calibre2">When you run the code, it will generate the following charts:</p>
<div class="mce-root"><img src="../images/00044.jpeg" class="calibre61"/></div>
<p class="calibre2">As you can see from the charts, there are some obvious differences in distributions among different sentiments. Words such as <strong class="calibre4">thanks</strong> and <strong class="calibre4">great</strong> were two of the top seven frequently occurring terms in positive tweets, while words like <strong class="calibre4">delay</strong> and <strong class="calibre4">cancelled</strong> were two of the top seven frequently occurring terms in negative tweets. Intuitively, these make sense. You typically use words <strong class="calibre4">thanks</strong> and <strong class="calibre4">great</strong> when you express positive feelings towards someone or something. On the other hand, <strong class="calibre4">delay</strong> and <strong class="calibre4">cancelled</strong> are related to negative events in the context of flights or airlines. Maybe some of the users' flights were delayed or cancelled and they tweeted about their frustrations. Another interesting thing to note is how the term <kbd class="calibre12">emo_smiley</kbd> was ranked seventh of the most frequently occurring terms in positive tweets. If you remember, in the previous step we grouped and encoded all smiley face emoticons (such as <kbd class="calibre12">:)</kbd>, <kbd class="calibre12">:D</kbd>, and so on) as <kbd class="calibre12">emo_smiley</kbd>. This tells us that emoticons may play an important role for our models to learn how to classify the sentiment of each tweet. Now that we have a rough idea of what our data looks like and what kinds of terminology appear for each sentiment, let's talk about the feature engineering techniques we will employ in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Feature engineering using lemmatization and emoticons</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">We briefly talked about lemmas in the previous section. Let's take a deeper look at what lemmas are and what lemmatization is. Depending on how and where a word is being used in a sentence, the word is going to be in different forms. For example, the word <kbd class="calibre12">like</kbd> can take the form of <kbd class="calibre12">likes</kbd> or <kbd class="calibre12">liked</kbd> depending on what came before. If we simply tokenize sentences into words, then our program is going to see the words <kbd class="calibre12">like</kbd>, <kbd class="calibre12">likes</kbd>, and <kbd class="calibre12">liked</kbd> as three different tokens. However, that might not be something we want. Those three words share the same meaning and when we are building models, it would be useful to group those words as one token in our feature set. This is what lemmatization does. A lemma is the base form of a word and lemmatization is transforming each word into a lemma based on the part of the sentence each word was used in. In the preceding example, <kbd class="calibre12">like</kbd> is the lemma for <kbd class="calibre12">likes</kbd> and <kbd class="calibre12">liked</kbd>, and systematically transforming <kbd class="calibre12">likes</kbd> and <kbd class="calibre12">liked</kbd> into <kbd class="calibre12">like</kbd> is a lemmatization.</span></p>
<p class="calibre2"><span class="calibre5">Following is an example of a lemmatization using Stanford CoreNLP:</span></p>
<div class="mce-root"><img src="../images/00045.jpeg" class="calibre62"/></div>
<p class="calibre2"><span class="calibre5">Here, you can see that both <kbd class="calibre12">likes</kbd> and <kbd class="calibre12">like</kbd> were lemmatized into</span> <kbd class="calibre12">like</kbd>. This is because both of those words were used as verbs in a sentence and the lemma for the verbal form is <kbd class="calibre12">like</kbd>. Let's look at another example:</p>
<div class="mce-root"><img src="../images/00046.jpeg" class="calibre63"/></div>
<p class="calibre2">Here, the first <kbd class="calibre12">likes</kbd> and the second <kbd class="calibre12">likes</kbd> have different lemmas. The first one has <kbd class="calibre12">like</kbd> as its lemma, while the second one has <kbd class="calibre12">likes</kbd> as its lemma. This is because the first one is used as a verb, while the second one as a noun. As you can see from these examples, depending on the parts of the sentence, the lemmas for the same words can be different. Using lemmatization for your text dataset can greatly reduce the sparsity and dimensions of your feature space and can help your models learn better without being exposed to too much noise.</p>
<p class="calibre2">Similar to lemmatization, we also grouped similar emoticons into the same group. This is based on the assumption that similar emoticons have similar meanings. For example, <kbd class="calibre12">:)</kbd> and <kbd class="calibre12">:D</kbd> have almost the same meanings, if not exactly the same. In another case, depending on the users, the positions of the colon and parenthesis can differ. Some users might type <kbd class="calibre12">:)</kbd>, but some others might type <kbd class="calibre12">(:</kbd>. However, the only different between these two is the positioning of the colon and parenthesis and the meanings are the same. In all of these cases, we want our models to learn the same emotion and not create any noise. Grouping similar emoticons into the same group, as we did in the previous step, helps reduce unnecessary noise for our models and help them learn the most from these emoticons.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Naive Bayes versus random forest</h1>
                
            
            <article>
                
<p class="calibre2">It is finally time to train our ML models to predict the sentiments of tweets. In this section, we are going to experiment with Naive Bayes and random forest classifiers. There are two things that we are going to do differently from the previous chapter. First, we are going to split our sample set into a train set and a validation set, instead of running k-fold cross-validation. This is also a frequently used technique, where the models learn only from a subset of the sample set and then they are tested and validated with the rest, which they did not observe at training time. This way, we can test how the models will perform in the unforeseen dataset and simulate how they are going to behave in a real-world case. We are going to use the <kbd class="calibre12">SplitSetValidation</kbd> class in the Accord.NET package to split our sample set into train and validation sets with pre-defined proportions for each set and fit a learning algorithm to the train set.</p>
<p class="calibre2">Secondly, our target variable is no longer binary (0 or 1), unlike in the previous <a target="_blank" href="part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 2</a><em class="calibre13">, Spam Email Filtering</em>. Instead, it can take any values from 0, 1, or 2, where 0 stands for neutral sentiment tweets, 1 for positive <span class="calibre5">sentiment</span> tweets, and 2 for negative <span class="calibre5">sentiment</span> tweets. So, we are now dealing with a multi-class classification problem, rather than a binary classification problem. We will have to approach things differently when evaluating our models. We will have to modify our accuracy, precision, and recall calculation codes from the previous chapter to compute those numbers for each of the three target sentiment classes in this project. Also, we will have to use a one-versus-rest approach when we look at certain metrics, such as a ROC curve and AUC, which we will be discussing in the following section.</p>
<p class="calibre2">Let's first look at how to instantiate our learning algorithms with the <kbd class="calibre12">SplitSetValidation</kbd> class in the Accord.NET Framework. The following is how you can instantiate a <kbd class="calibre12">SplitSetValidation</kbd><em class="calibre13"> </em>object with the Naive Bayes classifier algorithm:</p>
<pre class="calibre19">var nbSplitSet = new SplitSetValidation&lt;NaiveBayes&lt;BernoulliDistribution&gt;, double[]&gt;()<br class="title-page-name"/>{<br class="title-page-name"/>    Learner = (s) =&gt; new NaiveBayesLearning&lt;BernoulliDistribution&gt;(),<br class="title-page-name"/><br class="title-page-name"/>    Loss = (expected, actual, p) =&gt; new ZeroOneLoss(expected).Loss(actual),<br class="title-page-name"/><br class="title-page-name"/>    Stratify = false,<br class="title-page-name"/><br class="title-page-name"/>    TrainingSetProportion = 0.8,<br class="title-page-name"/><br class="title-page-name"/>    ValidationSetProportion = 0.2<br class="title-page-name"/>};<br class="title-page-name"/>var nbResult = nbSplitSet.Learn(input, output);</pre>
<p class="calibre2">As you can see from the preceding code snippet, the major difference between the code you used in the previous chapter and the code shown here is the two parameters that we pass onto a <kbd class="calibre12">SplitSetValidation</kbd><span class="calibre5"> </span>object—<kbd class="calibre12">TrainingSetProportion</kbd><em class="calibre13"> </em>and<span class="calibre5"> </span><kbd class="calibre12">ValidationSetProportion</kbd>. As the name suggests, you can define what percentage of your sample set is should be used for training with the <kbd class="calibre12">TrainingSetProportion</kbd><em class="calibre13"> </em>parameter and <span class="calibre5">what percentage of your sample set to be used for validation with the</span> <kbd class="calibre12">ValidationSetProportion</kbd><span class="calibre5"> </span>parameter. Here in our code snippet, we are telling our program to use 80% of our sample for training and 20% for validation. In the last line of the code snippet, we fit a Naive Bayes classification model to the train set that was split from the sample set. Also, note here that we used <kbd class="calibre12">BernoulliDistribution</kbd> for our Naive Bayes classifier, as we used one-hot encoding to encode our features and all of our features have binary values, similar to what we did in the previous chapter.</p>
<p class="calibre2">Similar to how we instantiated a <kbd class="calibre12">SplitSetValidation</kbd> object with the Naive Bayes classifier, you can instantiate another one with the random forest classifier as in the following:</p>
<pre class="calibre19">var rfSplitSet = new SplitSetValidation&lt;RandomForest, double[]&gt;()<br class="title-page-name"/>{<br class="title-page-name"/>    Learner = (s) =&gt; new RandomForestLearning()<br class="title-page-name"/>    {<br class="title-page-name"/>        NumberOfTrees = 100, // Change this hyperparameter for further tuning<br class="title-page-name"/><br class="title-page-name"/>        CoverageRatio = 0.5, // the proportion of variables that can be used at maximum by each tree<br class="title-page-name"/><br class="title-page-name"/>        SampleRatio = 0.7 // the proportion of samples used to train each of the trees<br class="title-page-name"/><br class="title-page-name"/>    },<br class="title-page-name"/><br class="title-page-name"/>    Loss = (expected, actual, p) =&gt; new ZeroOneLoss(expected).Loss(actual),<br class="title-page-name"/><br class="title-page-name"/>    Stratify = false,<br class="title-page-name"/><br class="title-page-name"/>    TrainingSetProportion = 0.7,<br class="title-page-name"/><br class="title-page-name"/>    ValidationSetProportion = 0.3<br class="title-page-name"/>};<br class="title-page-name"/>var rfResult = rfSplitSet.Learn(input, output);</pre>
<p class="calibre2">We replaced the previous code with random forest as a model and <kbd class="calibre12">RandomForestLearning</kbd> as a learning algorithm. If you look closely, there are some hyperparameters that we can tune for <kbd class="calibre12">RandomForestLearning</kbd>. The first one is <kbd class="calibre12">NumberOfTrees</kbd>. This hyperparameter lets you choose the number of decision trees that go into your random forest. In general, having more trees in a random forest results in better performance, as you are essentially building more decision trees in the forest. However, the performance lift comes at the cost of training and prediction time. It will take more time to train and make predictions as you increase the number of trees in your random forest. The other two parameters to note here are <kbd class="calibre12">CoverageRatio</kbd><em class="calibre13"> </em>and <kbd class="calibre12">SampleRatio</kbd>. <kbd class="calibre12">CoverageRatio</kbd><em class="calibre13"> </em>sets the proportion of the feature set to be used in each tree, while <kbd class="calibre12">SampleRatio</kbd><em class="calibre13"> </em>sets the proportion of the train set to be used in each tree. Having a higher <kbd class="calibre12">CoverageRatio</kbd> and<em class="calibre13"> </em><kbd class="calibre12">SampleRatio</kbd> increases the performance of individual trees in the forest, but it also increases the correlation among the trees. Lower correlation among the trees helps reduce the generalization error; thus, finding a good balance between the prediction powers of individual trees and correlation among the trees will be essential in building a good random forest model. Tuning and experimenting with various combinations of these hyperparameters can help you avoid overfitting issues, as well as improving your model performance when training a random forest model. We recommend you build a number of random forest classifiers with various combinations of those hyperparameters and experiment with their effects on your model performances.</p>
<p class="calibre2"><span class="calibre5">The full code that we used to train Naive Bayes and random forest classification models and output validation results can be found here: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterSentimentModeling.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.3/TwitterSentimentModeling.cs</a>. Let's take a closer look at this code. In lines 36-41, it first reads in the token matrix file, <kbd class="calibre12">tweet-lemma.csv</kbd>, which we built in the data preparation step. Then in lines 43-51, we read in the term frequency files, <kbd class="calibre12">positive-frequencies.csv</kbd> and <kbd class="calibre12">negative-frequencies.csv</kbd>, which we built in the data analysis step. Similar to what we did in the previous chapter, we do feature selection based on the number of term occurrences in line 64. In this example, we experimented with 5, 10, 50, 100, and 150 as the thresholds for the minimum number of term occurrences in our sample tweets. From line 65, we iterate through those thresholds and start training and evaluating Naive Bayes and random forest classifiers. Each time a model is trained on a train set, it is then run against the validation set that was not observed during the training time. </span></p>
<p class="calibre2"><span class="calibre5">Following is part of the full code (lines 113-135) that runs the trained Naive Bayes model on the train and validation sets to measure in-sample and out-of-sample performance:</span></p>
<pre class="calibre19">// Get in-sample &amp; out-sample prediction results for NaiveBayes Classifier<br class="title-page-name"/>var nbTrainedModel = nbResult.Model;<br class="title-page-name"/><br class="title-page-name"/>int[] nbTrainSetIDX = nbSplitSet.IndicesTrainingSet;<br class="title-page-name"/>int[] nbTestSetIDX = nbSplitSet.IndicesValidationSet;<br class="title-page-name"/><br class="title-page-name"/>Console.WriteLine("* Train Set Size: {0}, Test Set Size: {1}", nbTrainSetIDX.Length, nbTestSetIDX.Length);<br class="title-page-name"/><br class="title-page-name"/>int[] nbTrainPreds = new int[nbTrainSetIDX.Length];<br class="title-page-name"/>int[] nbTrainActual = new int[nbTrainSetIDX.Length];<br class="title-page-name"/>for (int i = 0; i &lt; nbTrainPreds.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>   nbTrainActual[i] = output[nbTrainSetIDX[i]];<br class="title-page-name"/>   nbTrainPreds[i] = nbTrainedModel.Decide(input[nbTrainSetIDX[i]]);<br class="title-page-name"/>}<br class="title-page-name"/><br class="title-page-name"/>int[] nbTestPreds = new int[nbTestSetIDX.Length];<br class="title-page-name"/>int[] nbTestActual = new int[nbTestSetIDX.Length];<br class="title-page-name"/>for (int i = 0; i &lt; nbTestPreds.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>   nbTestActual[i] = output[nbTestSetIDX[i]];<br class="title-page-name"/>   nbTestPreds[i] = nbTrainedModel.Decide(input[nbTestSetIDX[i]]);<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">Following is the part of the full code (lines 167-189) that runs the trained random forest model on the train and validation sets to measure in-sample and out-of-sample performance:</span></p>
<pre class="calibre19">// Get in-sample &amp; out-sample prediction results for RandomForest Classifier<br class="title-page-name"/>var rfTrainedModel = rfResult.Model;<br class="title-page-name"/><br class="title-page-name"/>int[] rfTrainSetIDX = rfSplitSet.IndicesTrainingSet;<br class="title-page-name"/>int[] rfTestSetIDX = rfSplitSet.IndicesValidationSet;<br class="title-page-name"/><br class="title-page-name"/>Console.WriteLine("* Train Set Size: {0}, Test Set Size: {1}", rfTrainSetIDX.Length, rfTestSetIDX.Length);<br class="title-page-name"/><br class="title-page-name"/>int[] rfTrainPreds = new int[rfTrainSetIDX.Length];<br class="title-page-name"/>int[] rfTrainActual = new int[rfTrainSetIDX.Length];<br class="title-page-name"/>for (int i = 0; i &lt; rfTrainPreds.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    rfTrainActual[i] = output[rfTrainSetIDX[i]];<br class="title-page-name"/>    rfTrainPreds[i] = rfTrainedModel.Decide(input[rfTrainSetIDX[i]]);<br class="title-page-name"/>}<br class="title-page-name"/><br class="title-page-name"/>int[] rfTestPreds = new int[rfTestSetIDX.Length];<br class="title-page-name"/>int[] rfTestActual = new int[rfTestSetIDX.Length];<br class="title-page-name"/>for (int i = 0; i &lt; rfTestPreds.Length; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    rfTestActual[i] = output[rfTestSetIDX[i]];<br class="title-page-name"/>    rfTestPreds[i] = rfTrainedModel.Decide(input[rfTestSetIDX[i]]);<br class="title-page-name"/>}</pre>
<p class="calibre2">Let's take a closer look at these. For brevity, we will only take a look at the random forest model case, as it will be the same for the Naive Bayes classifier. In line 168, we first get the trained model from the learned results. Then, we get the indexes of in-sample (train set) and out-of-sample (test/validation set) sets from the <kbd class="calibre12">SplitSetValidation</kbd> object in lines 170-171, so that we can iterate through each row or record and make predictions. We iterate this process twice—once for the in-sample training set in lines 175-181 and again for the out-of-sample validation set in lines 183-189.</p>
<p class="calibre2">Once we have the prediction results on the train and test sets, we run those results through some validation methods (lines 138-141 for the Naive Bayes classifier and lines 192-196 for the random forest classifier). There are two methods that we wrote specifically for the model validation for this project—<kbd class="calibre12">PrintConfusionMatrix</kbd> and <kbd class="calibre12">DrawROCCurve</kbd>. <kbd class="calibre12">PrintConfusionMatrix</kbd><em class="calibre13"> </em>is an updated version of what we had in <a target="_blank" href="part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 2</a>, <em class="calibre13">Spam Email Filtering</em>, where it now prints a 3 x 3 confusion matrix, instead of a 2 x 2 confusion matrix. On the other hand, the <kbd class="calibre12">DrawROCCurve</kbd><em class="calibre13"> </em>method brings in some new concepts and new model validation methods for this project. Let's discuss those new evaluation metrics, which we are using for this project, in greater detail in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Model validations – ROC curve and AUC</h1>
                
            
            <article>
                
<p class="calibre2">As mentioned before, we are using different model validation metrics in this chapter: the ROC curve and AUC. The ROC curve is a plot of a true positive rate against a false positive rate at various thresholds. Each point in the curve represents the true positive and false positive rate pair corresponding at a certain probability threshold. It is commonly used to select the best and the most optimal models among different model candidates. </p>
<p class="calibre2">The area under the ROC curve (AUC) measures how well the model can distinguish the two classes. In the case of a binary classification, AUC measures how well a model distinguishes the positive outcomes from the negative outcomes. Since we are dealing with a multi-class classification problem in this project, we are using a one-versus-rest approach to build the ROC curve and compute the AUC. For example, one ROC curve can take positive tweets as positive outcomes and neutral and negative tweets as negative outcomes, while another ROC curve can take neutral tweets as positive outcomes and positive and negative tweets as negative outcomes. As shown in the following charts, we drew three ROC charts for each model we built—one for Neutral verses Rest (Positive and Negative), one for Positive versus Rest (Neutral and Negative), and one for Negative versus Rest (Neutral and Positive). The higher the AUC number is, the better the model is as it suggests that the model can distinguish positive classes from negative classes with much better chance. </p>
<p class="calibre2">The following charts show ROC curves for Naive Bayes classifiers with the minimum number of term occurrences at <strong class="calibre4">10</strong>:</p>
<div class="mce-root"><img class="alignnone6" src="../images/00047.jpeg"/></div>
<p class="calibre2"><span class="calibre5">The following charts show ROC curves for Naive Bayes classifiers with the minimum number of term occurrences at <strong class="calibre4">50</strong>:</span></p>
<div class="mce-root"><img class="alignnone7" src="../images/00048.jpeg"/></div>
<p class="calibre2"><span class="calibre5">The following charts show ROC curves for Naive Bayes classifiers with the minimum number of term occurrences at <strong class="calibre4">150</strong>:</span></p>
<div class="mce-root"><img src="../images/00049.jpeg" class="calibre26"/></div>
<p class="calibre2">As you can see from the charts, we can also detect overfitting issues from ROC charts by looking at the gaps between the curves from training and testing results. The larger the gap is, the more the model is overfitting. If you look at the first case, where we only filter out those terms that appear in tweets fewer than ten times, the gap between the two curves is large. As we increase the threshold, we can see that the gap decreases. When we are choosing the final model, we want the train ROC curve and the test/validation ROC curve to be as small as possible. As this resolution comes at the expense of the model performance, we need to find the right cutoff line for this trade-off.</p>
<p class="calibre2"><span class="calibre5">Let's now look at a sample of how one of our random forest classifiers did. The following is a sample result from fitting a random forest classifier:</span></p>
<div class="mce-root"><img src="../images/00050.jpeg" class="calibre26"/></div>
<p class="calibre2">Ensemble methods, such as random forest, generally work well for classification problems and accuracy can be improved by ensembling with more trees. However, they come with some limitations, one of which is shown in the previous sample results for the random forest classifier. As is true for all decision tree-based models, the random forest model tends to overfit, especially when it tries to learn from many categorical variables. As you can see from the ROC curves for the random forest classifier, the gap between the train and test ROC curves is large, especially when compared to those for the Naive Bayes classifier. The Naive Bayes classifier with a minimum number of term occurrences threshold at 150 has almost no gap between the train and test ROC curves, whereas a random forest classifier at the same threshold shows a large gap between the two ROC curves. When dealing with such a dataset, where there are lots of categorical variables, we need to be careful about which model to choose and pay special attention to tuning the hyperparameters, <span class="calibre5">such as </span><kbd class="calibre12">NumberOfTrees</kbd><span class="calibre5">, </span><kbd class="calibre12">CoverageRatio</kbd><span class="calibre5">, and </span><kbd class="calibre12">SampleRatio</kbd><em class="calibre13">,</em> for a random forest model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">In this chapter, we built and trained more advanced classification models for Twitter sentiment analysis. We applied what we have learned in the previous chapter to a multi-class classification problem with more complex text data. We first started off by setting up our environment with the Stanford CoreNLP package that we used for tokenization, POS tagging, and lemmatization in the data preparation and analysis steps. Then, we transformed the raw Twitter dataset into a one-hot encoded matrix by tokenizing and lemmatizing the tweets. During this data preparation step, we also discussed how we could use Regex to group similar emoticons together and remove unnecessary text, such as URLs, Twitter IDs, and raw numbers, from tweets. We further analyzed the distribution of frequently used terms and emoticons in our data analysis step and we saw how lemmatization and grouping similar emoticons together help in reducing unnecessary noise in the dataset. With data and insights from previous steps, we experimented with building multi-class classification models using Naive Bayes and random forest classifiers. As we built these models, we covered a frequently used model validation technique, where we split a sample set into two subsets, the train set and validation set, and used the train set to fit a model and the validation set to evaluate the model performance. We also covered new model validation metrics, the ROC curve and AUC, which we can use to select the best and most optimal model among model candidates.</span></p>
<p class="calibre2">In the next chapter, we are going to switch gears and start building regression models where the target variables are continuous variables. We will use a foreign exchange rate dataset to build <span class="calibre5"><span class="calibre5">time series </span></span>features and explore some other ML models for regression problems. We will also discuss how evaluating the performance of regression models is different from that of classification models.</p>


            </article>

            
        </section>
    </body></html>