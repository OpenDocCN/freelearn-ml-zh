- en: Beating CAPTCHAs with Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络击败CAPTCHA
- en: Images pose interesting and difficult challenges for data miners. Until recently,
    only small amounts of progress were made with analyzing images for extracting
    information. However recently, such as with the progress made on self-driving
    cars, significant advances have been made in a very short time-frame. The latest
    research is providing algorithms that can understand images for commercial surveillance,
    self-driving vehicles, and person identification.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 图像对数据挖掘者提出了有趣且具有挑战性的问题。直到最近，在分析图像以提取信息方面只有少量进展。然而，最近，例如在自动驾驶汽车方面的进展，在很短的时间内取得了重大进展。最新的研究正在提供可以理解图像的算法，用于商业监控、自动驾驶车辆和人员识别。
- en: There is lots of raw data in an image, and the standard method for encoding
    images - pixels - isn't that informative by itself. Images and photos can be blurry,
    too close to the targets, too dark, too light, scaled, cropped, skewed, or any
    other of a variety of problems that cause havoc for a computer system trying to
    extract useful information. Neural networks can combine these lower level features
    into higher level patterns that are more able to generalize and deal with these
    issues.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中包含大量原始数据，而编码图像的标准方法——像素——本身并不具有太多信息。图像和照片也可能模糊、过于接近目标、太暗、太亮、缩放、裁剪、倾斜，或任何其他可能导致计算机系统在提取有用信息时出现混乱的问题。神经网络可以将这些低级特征组合成更高级别的模式，这些模式更有能力进行泛化和处理这些问题。
- en: 'In this chapter, we look at extracting text data from images by using neural
    networks for predicting each letter in the CAPTCHA. CAPTCHAs are images designed
    to be easy for humans to solve and hard for a computer to solve, as per the acronym:
    **Completely Automated Public Turing test to tell Computers and Humans Apart**.
    Many websites use them for registration and commenting systems to stop automated
    programs flooding their site with fake accounts and spam comments.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过使用神经网络预测CAPTCHA中的每个字母来探讨从图像中提取文本数据。CAPTCHA是设计得对人类来说容易解决而对计算机来说难以解决的图像，正如其首字母缩略词：**完全自动化的公开图灵测试，用于区分计算机和人类**。许多网站使用它们作为注册和评论系统，以阻止自动化程序向其网站发送虚假账户和垃圾邮件评论。
- en: These tests help stop programs (bots) using websites, such as a bot intent on
    automatically signing up new people to a website. We play the part of such a spammer,
    trying to get around a CAPTCHA-protected system for posting messages to an online
    forum. The website is protected by a CAPTCHA, meaning we can't post unless we
    pass the test.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这些测试有助于阻止程序（机器人）使用网站，例如一个旨在自动将新用户注册到网站的机器人。我们扮演这样的垃圾邮件发送者的角色，试图绕过用于在线论坛发布消息的CAPTCHA保护系统。该网站受到CAPTCHA的保护，这意味着除非我们通过测试，否则我们无法发布。
- en: 'The topics covered in this chapter include:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题包括：
- en: Neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Creating our own dataset of CAPTCHAs and letters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建我们自己的CAPTCHA和字母数据集
- en: The scikit-image library for working with image data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于处理图像数据的scikit-image库
- en: Extracting basic features from images
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图像中提取基本特征
- en: Using neural networks for larger-scale classification tasks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络进行更大规模的分类任务
- en: Improving performance using postprocessing
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用后处理提高性能
- en: Artificial neural networks
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: Artificial neural networks
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: 'Neural networks are a class of algorithm that was originally designed based
    on the way that human brains work. However, modern advances are generally based
    on mathematics rather than biological insights. A neural network is a collection
    of neurons that are connected together. Each neuron is a simple function of its
    inputs, which are combined using some function to generate an output:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一类最初基于人类大脑工作方式设计的算法。然而，现代的进步通常基于数学而不是生物学洞察。神经网络是一组相互连接的神经元。每个神经元是其输入的简单函数，这些输入通过某种函数组合起来生成输出：
- en: '![](img/B06162_08_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_08_01.jpg)'
- en: 'The functions that define a neuron''s processing can be any standard function,
    such as a linear combination of the inputs, and is called the **activation function**.
    For the commonly used learning algorithms to work, we need the activation function
    to be *derivable* and  *smooth*. A frequently used activation function is the
    **logistic function**, which is defined by the following equation (*k* is often
    simply 1, *x* is the inputs into the neuron, and L is normally 1, that is, the
    maximum value of the function):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 定义神经元处理的函数可以是任何标准函数，例如输入的线性组合，这被称为**激活函数**。为了使常用的学习算法能够工作，我们需要激活函数是**可导的**和**平滑的**。一个常用的激活函数是**逻辑函数**，其定义如下（*k*通常简单地是1，*x*是神经元的输入，L通常是1，即函数的最大值）：
- en: '![](img/B06162_08_02.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_08_02.png)'
- en: The value of this graph, from -6 to +6, is shown below. The red lines indicate
    that the value is 0.5 when *x* is zero, but the function quickly climbs to 1.0
    as x increases, and quickly drops to -1.0 when x decreases.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该图的值从-6到+6，如下所示。红色线条表示当*x*为零时值为0.5，但随着*x*的增加，函数迅速上升到1.0，当*x*减少时，迅速下降到-1.0。
- en: '![](img/B06162_08_03.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_08_03.png)'
- en: Each individual neuron receives its inputs and then computes the output based
    on these values. Neural networks can be considered as a collection of these neurons
    connected together, and they can be very powerful for data mining applications.
    The combinations of these neurons, how they fit together, and how they combine
    to learn a model are one of the most powerful concepts in machine learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单独的神经元都会接收其输入，然后根据这些值计算输出。神经网络可以被视为这些神经元连接在一起的一个集合，它们对于数据挖掘应用来说可以非常强大。这些神经元的组合、它们如何组合以及它们如何组合来学习模型是机器学习中最强大的概念之一。
- en: An introduction to neural networks
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络简介
- en: For data mining applications, the arrangement of neurons is usually in **layers**.
    The first layer is called the **input layer** and takes its input from samples
    in the data. The outputs of each of these neurons are computed and then passed
    along to the neurons in the next layer. This is called a **feed-forward neural
    network**. We will refer to these simply as **neural networks** for this chapter,
    as they are the most common type used and the only type used in this chapter.
    There are other types of neural networks too that are used for different applications.
    We will see another type of network in [Chapter 11](lrn-dtmn-py-2e_ch11.html)*,
    Object Detection in Images Using Deep Neural Networks*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据挖掘应用，神经元的排列通常是**层**的形式。第一层被称为**输入层**，它从数据样本中获取输入。这些神经元的输出被计算后，传递给下一层的神经元。这被称为**前馈神经网络**。在本章中，我们将简单地称之为**神经网络**，因为它们是最常用的类型，也是本章唯一使用的类型。还有其他类型的神经网络，用于不同的应用。我们将在[第11章](lrn-dtmn-py-2e_ch11.html)*，使用深度神经网络的图像目标检测*中看到另一种类型的网络。
- en: 'The outputs of one layer become the inputs of the next layer, continuing until
    we reach the final layer: the **output layer**. These outputs represent the predictions
    of the neural network as the classification. Any layer of neurons between the
    input layer and the output layer is referred to as a **hidden layer**, as they
    learn a representation of the data not intuitively interpretable by humans. Most
    neural networks have at least three layers, although most modern applications
    use networks with many more layers than that.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一层的输出成为下一层的输入，一直持续到我们达到最后一层：**输出层**。这些输出代表了神经网络的预测作为分类。输入层和输出层之间的任何神经元层都被称为**隐藏层**，因为它们学习的数据表示对于人类来说不是直观可解释的。大多数神经网络至少有三层，尽管大多数现代应用使用的网络比这多得多。
- en: '![](img/B06162_08_04.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_08_04.jpg)'
- en: Typically, we consider fully connected layers. The outputs of each neuron in
    a layer go to all neurons in the next layer. While we do define a fully connected
    network, many of the weights will be set to zero during the training process,
    effectively removing these links. Additionally, many of these weights might retain
    very small values, even after training.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们考虑全连接层。一个层中每个神经元的输出都连接到下一层的所有神经元。虽然我们确实定义了一个全连接网络，但在训练过程中，许多权重将被设置为0，从而有效地移除了这些连接。此外，这些权重中许多可能在训练后仍然保留非常小的值。
- en: In addition to being one of the conceptually simpler forms for neural networks,
    fully connected neural networks are also simpler and more efficient to program
    than other connection patterns.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了是神经网络概念上更简单的一种形式之外，全连接神经网络在编程上比其他连接模式更简单、更高效。
- en: See [Chapter 11](lrn-dtmn-py-2e_ch11.html), *Object Detection in images using
    Deep Neural Networks*,  for an investigation into different types of neural networks,
    including layers built specifically for image processing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[第11章](lrn-dtmn-py-2e_ch11.html)，*使用深度神经网络进行图像目标检测*，了解对不同类型神经网络的调查，包括专门为图像处理构建的层。
- en: As the function of the neurons is normally the logistic function, and the neurons
    are fully connected to the next layer, the parameters for building and training
    a neural network must be other factors.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经元的函数通常是逻辑函数，并且神经元与下一层完全连接，因此构建和训练神经网络的参数必须是其他因素。
- en: 'The first factor for neural networks is in the building phase: the size and
    shape of the neural network. This includes how many layers the neural network
    has and how many neurons it has in each hidden layer (the size of the input and
    output layers is usually dictated by the dataset).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的第一个因素是在构建阶段：神经网络的大小和形状。这包括神经网络有多少层以及每个隐藏层中有多少个神经元（输入和输出层的大小通常由数据集决定）。
- en: 'The second parameter for neural networks is determined in the training phase:
    the weight of the connections between neurons. When one neuron connects to another,
    this connection has an associated weight that is multiplied by the signal (the
    output of the first neuron). If the connection has a weight of 0.8, the neuron
    is activated, and it outputs a value of 1, the resulting input to the next neuron
    is 0.8\. If the first neuron is not activated and has a value of 0, this stays
    at 0.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的第二个参数是在训练阶段确定的：神经元之间的连接权重。当一个神经元连接到另一个神经元时，这个连接有一个相关的权重，该权重会乘以信号（第一个神经元的输出）。如果连接的权重为0.8，则神经元被激活，并输出值为1，因此传递给下一个神经元的输入为0.8。如果第一个神经元未激活且值为0，则该值保持为0。
- en: The combination of an appropriately sized network and well-trained weights determines
    how accurate the neural network can be when making classifications. The word *appropriately*
    in the previous sentence also doesn't necessarily mean bigger, as neural networks
    that are too large can take a long time to train and can more easily over-fit
    the training data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 网络大小适当和权重训练良好这两者的结合决定了神经网络在分类时可以达到的准确性。上一句中的“适当”一词也不一定意味着更大，因为过大的神经网络可能需要很长时间来训练，并且更容易对训练数据进行过拟合。
- en: Weights can be set randomly to start with but are then updated during the training
    phase. Setting weights to zero is normally not a good idea, as all neurons in
    the network act similarly to begin with! Having randomly set weights gives each
    neuron a different *role* in the learning process that can be improved with training.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 权重可以随机设置以开始，但在训练阶段会更新。将权重设置为零通常不是一个好主意，因为网络中的所有神经元最初都表现得非常相似！随机设置权重给每个神经元在学习过程中的不同*角色*，这些角色可以通过训练得到改善。
- en: A neural network in this configuration is a classifier that can then be used
    to predict the target of a data sample based on the inputs, much like the classification
    algorithms we have used in previous chapters. But first, we need a dataset to
    train and test with.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种配置下的神经网络是一个分类器，可以用来根据输入预测数据样本的目标，就像我们在前面的章节中使用过的分类算法一样。但首先，我们需要一个用于训练和测试的数据集。
- en: Neural networks are, by a margin, the biggest area of advancement in data mining
    in recent years. This might make you think: *Why bother learning any other type
    of classification algorithm?* While neural networks are state of the art in pretty
    much every domain (at least, right now), the reason to learn other classifiers
    is that neural networks often require larger amounts of data to work well, and
    they take a long time to learn. If you don't have **big data**, you will probably
    get better results from another algorithm.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在数据挖掘领域是近年来最大的进步领域。这可能会让你想：“为什么还要学习其他类型的分类算法？”虽然神经网络在几乎所有领域都是最先进的（至少，现在是这样），但学习其他分类器的理由是神经网络通常需要更多的数据才能有效工作，并且学习时间较长。如果你没有**大数据**，你可能会从另一个算法中获得更好的结果。
- en: Creating the dataset
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据集
- en: In this chapter, to spice up things a little, let us take on the role of the
    bad guy. We want to create a program that can beat CAPTCHAs, allowing our comment
    spam program to advertise on someone's website. It should be noted that our CAPTCHAs
    will be a little easier than those used on the web today and that spamming isn't
    a very nice thing to do.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，为了使内容更加生动，让我们扮演坏人的角色。我们想要创建一个能够击败 CAPTCHA 的程序，允许我们的评论垃圾邮件程序在某个人的网站上做广告。需要注意的是，我们的
    CAPTCHA 将会比今天网络上使用的那些稍微简单一些，而且垃圾邮件并不是一件很体面的事情。
- en: We play the bad guy today, but please *don't* use this against real world sites.
    One reason to "play the bad guy" is to help improve the security of our website,
    by looking for issues with it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天扮演坏人，但请**不要**将此用于现实世界的网站。扮演坏人的一个原因是为了帮助提高我们网站的安全性，通过寻找它的问题。
- en: 'Our experiment will simplify a CAPTCHA to be individual English words of four
    letters only, as shown in the following image:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验将简化 CAPTCHA，使其仅包含四个字母的单独英语单词，如下面的图像所示：
- en: '![](img/B06162_08_05.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_08_05.png)'
- en: 'Our goal will be to create a program that can recover the word from images
    like this. To do this, we will use four steps:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是创建一个可以从这种图像中恢复单词的程序。为此，我们将使用四个步骤：
- en: Break the image into individual letters.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像分解成单个字母。
- en: Classify each individual letter.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个单独的字母进行分类。
- en: Recombine the letters to form a word.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将字母重新组合成一个单词。
- en: Rank words with a dictionary to try to fix errors.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用词典对单词进行排序，以尝试修复错误。
- en: Our CAPTCHA-busting algorithm will make the following assumptions. First, the
    word will be a whole and valid four-character English word (in fact, we use the
    same dictionary for creating and busting CAPTCHAs). Second, the word will only
    contain uppercase letters. No symbols, numbers, or spaces will be used.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 CAPTCHA 破解算法将做出以下假设。首先，单词将是一个完整的、有效的四个字符的英语单词（实际上，我们使用相同的词典来创建和破解 CAPTCHA）。其次，单词将只包含大写字母。不会使用符号、数字或空格。
- en: We are going to make the problem slightly harder than simply identifying letters,
    by performing a shear transform to the text, along with varying rates of shearing and
    scaling.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使问题比简单地识别字母稍微困难一些，通过对文本执行剪切变换，以及不同的剪切和缩放率。
- en: Drawing basic CAPTCHAs
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制基本的 CAPTCHA
- en: Before we can start classifying CAPTCHAs, we first need a dataset to learn from.
    In this section, we will be generating our own data to perform the data mining
    on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始对 CAPTCHA 进行分类之前，我们首先需要一个用于学习的数据集。在本节中，我们将生成自己的数据以进行数据挖掘。
- en: In more real-world applications, you'll be wanting to use an existing CAPTCHA
    service to generate the data, but for our purposes in this chapter, our own data
    will be sufficient. One of the issues that can arise is that we code in our assumptions
    around how the data works when we create the dataset ourselves, and then carry
    those same assumptions over to our data mining training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在更现实的应用中，你可能想要使用现有的 CAPTCHA 服务来生成数据，但就本章的目的而言，我们自己的数据就足够了。可能出现的一个问题是，当我们自己创建数据集时，我们会在数据工作方式上编码我们的假设，然后将这些相同的假设带到我们的数据挖掘训练中。
- en: Our goal here is to draw an image with a word on it, along with a shear transform.
    We are going to use the PIL library to draw our CAPTCHAs and the `scikit-image`
    library to perform the shear transform. The `scikit-image` library can read images
    in a NumPy array format that PIL can export to, allowing us to use both libraries.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是绘制一个带有文字的图像，并应用剪切变换。我们将使用 PIL 库来绘制我们的 CAPTCHA，并使用 `scikit-image` 库来进行剪切变换。`scikit-image`
    库可以读取 PIL 可以导出的 NumPy 数组格式的图像，这使得我们可以使用这两个库。
- en: 'Both PIL and scikit-image can be installed via Anaconda. However, I recommend
    getting PIL through its replacement called **pillow**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PIL 和 scikit-image 都可以通过 Anaconda 安装。然而，我推荐通过其替代品 **pillow** 来获取 PIL：
- en: conda install pillow scikit-image
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: conda install pillow scikit-image
- en: 'First, we import the necessary libraries and modules. We import NumPy and the
    Image drawing functions as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的库和模块。我们如下导入 NumPy 和图像绘制函数：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we create our base function for generating CAPTCHAs. This function takes
    a word and a shear value (which is normally between 0 and 0.5) to return an image
    in a NumPy array format. We allow the user to set the size of the resulting image,
    as we will use this function for single-letter training samples as well:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建生成 CAPTCHA 的基本函数。这个函数接受一个单词和一个剪切值（通常在 0 和 0.5 之间），以返回一个 NumPy 数组格式的图像。我们允许用户设置结果的图像大小，因为我们还将使用此函数进行单个字母的训练样本：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this function, we create a new image using L for the format, which means
    black-and-white pixels only, and create an instance of the `ImageDraw` class.
    This allows us to draw on this image using PIL. We then load the font, draw the
    text, and perform a `scikit-image` shear transform on it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们使用L格式创建一个新的图像，这意味着只有黑白像素，并创建一个`ImageDraw`类的实例。这允许我们使用PIL在这个图像上绘制。然后我们加载字体，绘制文本，并对它执行`scikit-image`的剪切变换。
- en: 'You can get the Coval font I used from the Open Font Library at:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从Open Font Library获取我使用的Coval字体：
- en: '[http://openfontlibrary.org/en/font/bretan](http://openfontlibrary.org/en/font/bretan)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://openfontlibrary.org/en/font/bretan](http://openfontlibrary.org/en/font/bretan)'
- en: Download the `.zip` file and extract the `Coval-Black.otf` file into the same
    directory as your Notebook.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下载`.zip`文件，并将`Coval-Black.otf`文件提取到与您的Notebook相同的目录中。
- en: 'From here, we can now generate images quite easily and use `pyplot` to display
    them. First, we use our inline display for the matplotlib graphs and import `pyplot`.
    The code is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们现在可以很容易地生成图像，并使用`pyplot`来显示它们。首先，我们使用matplotlib图形的内联显示并导入`pyplot`。代码如下：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result is the image shown at the start of this section: our CAPTCHA. Here
    are some other examples with different shear and scale values:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是本节开头显示的图像：我们的CAPTCHA。这里有一些具有不同剪切和缩放值的其他示例：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/B06162_08_12-2-e1493021528419.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_08_12-2-e1493021528419.png)'
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](img/B06162_08_13-e1493021566785.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_08_13-e1493021566785.png)'
- en: 'Here is a variant scaled to `1.5` sized. While it looks similar to the BONE
    image above, note the *x*-axis and *y*-axis values are larger:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个放大到`1.5`倍大小的变体。虽然它看起来与上面的BONE图像相似，但请注意*x*轴和*y*轴的值更大：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/B06162_08_14-e1493021653291.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_08_14-e1493021653291.png)'
- en: Splitting the image into individual letters
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将图像分割成单个字母
- en: 'Our CAPTCHAs are words. Instead of building a classifier that can identify
    the thousands and thousands of possible words, we will break the problem down
    into a smaller problem: predicting letters.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的CAPTCHA是单词。我们不会构建一个可以识别成千上万可能单词的分类器，而是将问题分解成更小的问题：预测字母。
- en: Our experiment is in English, and all uppercase, meaning we have 26 classes
    to predict from for each letter. If you try these experiments in other languages,
    keep in mind the number of output classes will have to change.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验是英文的，并且全部大写，这意味着对于每个字母，我们都有26个类别来预测。如果您在其他语言中尝试这些实验，请记住输出类别的数量将需要改变。
- en: The first step in our algorithm for beating these CAPTCHAs involves segmenting
    the word to discover each of the letters within it. To do this, we are going to
    create a function that finds contiguous sections of black pixels in the image
    and extract them as subimages. These are (or at least should be) our letters.
    The `scikit-image` function has tools for performing these operations.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 打败这些CAPTCHA的算法的第一步是分割单词以发现其中的每个字母。为此，我们将创建一个函数来找到图像中连续的黑色像素区域，并将它们作为子图像提取出来。这些（或者至少应该是）我们的字母。`scikit-image`函数有执行这些操作的工具。
- en: Our function will take an image, and return a list of sub-images, where each
    sub-image is a letter from the original word in the image. The first thing we
    need to do is to detect where each letter is. To do this, we will use the label
    function in `scikit-image`, which finds connected sets of pixels that have the
    same value. This has analogies to our connected component discovery in [Chapter
    7](lrn-dtmn-py-2e_ch07.html)*, Follow Recommendations Using Graph Mining*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能将接受一个图像，并返回一个子图像列表，其中每个子图像是图像中原始单词的字母。首先，我们需要做的是检测每个字母的位置。为此，我们将使用`scikit-image`中的标签函数，该函数可以找到具有相同值的像素的连通集合。这与我们在[第7章](lrn-dtmn-py-2e_ch07.html)*使用图挖掘进行推荐*中的连通组件发现有相似之处。
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can then get the subimages from the example CAPTCHA using this function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用此函数从示例CAPTCHA中获取子图像：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can also view each of these subimages:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看这些子图像中的每一个：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The result will look something like this:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来可能像这样：
- en: '![](img/B06162_08_11.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_08_11.jpg)'
- en: As you can see, our image segmentation does a reasonable job, but the results
    are still quite messy, with bits of previous letters showing. This is fine, and
    almost preferable. While training on data with regular noise makes our training
    worse, training on data with random noise can actually make it better. One reason
    is that the underlying data mining model learns the important aspects, namely
    the non-noise parts instead of specific noise inherent in the training data set.
    It is a fine line between too much and too little noise, and this can be hard
    to properly model. Testing on validation sets is a good way to ensure your training
    is improving.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们的图像分割做得相当合理，但结果仍然相当杂乱，之前的字母碎片仍然可见。这是可以接受的，甚至可以说是更可取的。虽然在对具有常规噪声的数据进行训练时会使我们的训练变得更差，但使用具有随机噪声的数据进行训练实际上可以使它变得更好。一个原因是底层的数据挖掘模型学会了重要的方面，即非噪声部分而不是训练数据集中固有的特定噪声。过多和过少的噪声之间的界限很微妙，这可能会很难正确建模。在验证集上进行测试是确保您的训练正在改进的好方法。
- en: 'One important note is that this code is not consistent in finding letters.
    Lower shear values typically result in accurately segmented images. For example,
    here is the code to segment the WOOF example from above:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的注意事项是，此代码在查找字母方面并不一致。较低的剪切值通常会导致准确分割的图像。例如，以下是分割上面WOOF示例的代码：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/B06162_08_15-e1493021829708.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_08_15-e1493021829708.png)'
- en: 'In contrast, higher shear values are not segmented properly. For example, here
    is the BARK example from before:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，较高的剪切值没有被正确分割。例如，这里是从前一个例子中的BARK示例：
- en: '![](img/B06162_08_16.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_08_16.png)'
- en: Notice the large overlap caused by the square segmentation. One suggestion for
    an improvement on this chapter's code is to improve our segmentation by finding
    non-square segments.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意由于正方形分割引起的大面积重叠。对于本章代码的一个改进建议是，通过找到非正方形分割来改进我们的分割。
- en: Creating a training dataset
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建训练数据集
- en: Using the functions we have already defined, we can now create a dataset of
    letters, each with different shear values. From this, we will train a neural network
    to recognize each letter from the image.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们已定义的函数，我们现在可以创建一个具有不同剪切值的字母数据集。从这些数据中，我们将训练一个神经网络来识别图像中的每个字母。
- en: 'We first set up our random state and an array that holds the options for letters,
    shear values and scale values that we will randomly select from. There isn''t
    much surprise here, but if you haven''t used NumPy''s `arange` function before,
    it is similar to Python''s `range` function—except this one works with NumPy arrays
    and allows the step to be a float. The code is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先设置随机状态和一个数组，该数组包含我们将随机选择的字母、剪切值和缩放值的选项。这里没有太多惊喜，但如果你之前没有使用过NumPy的`arange`函数，它类似于Python的`range`函数——只不过这个函数与NumPy数组一起工作，允许步长为浮点数。代码如下：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We then create a function (for generating a single sample in our training dataset)
    that randomly selects a letter, a shear value, and a scale value selected from
    the available options.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后创建一个函数（用于生成训练数据集中单个样本），该函数随机选择一个字母、一个剪切值和一个从可用选项中选择的缩放值。
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We return the image of the letter, along with the target value representing
    the letter in the image. Our classes will be 0 for A, 1 for B, 2 for C, and so
    on.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们返回字母的图像，以及表示图像中字母的目标值。我们的类别将是0代表A，1代表B，2代表C，依此类推。
- en: 'Outside the function block, we can now call this code to generate a new sample
    and then show it using `pyplot`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数块外部，我们现在可以调用此代码来生成一个新的样本，然后使用`pyplot`显示它：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The resulting image has just a single letter, with a random shear and random
    scale value.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图像只有一个字母，带有随机的剪切和随机缩放值。
- en: '![](img/B06162_08_17-e1493023909718.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_08_17-e1493023909718.png)'
- en: 'We can now generate all of our data by calling this several thousand times.
    We then put the data into NumPy arrays, as they are easier to work with than lists.
    The code is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过调用这个函数几千次来生成所有数据。然后我们将数据放入NumPy数组中，因为它们比列表更容易处理。代码如下：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Our targets are integer values between 0 and 26, with each representing a letter
    of the alphabet. Neural networks don''t usually support multiple values from a
    single neuron, instead preferring to have multiple outputs, each with values 0
    or 1\. We perform one-hot-encoding of the targets, giving us a target array that
    has 26 outputs per sample, using values near 1 if that letter is likely and near
    0 otherwise. The code is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是介于0和26之间的整数值，每个值代表字母表中的一个字母。神经网络通常不支持单个神经元来自多个值，而是更倾向于有多个输出，每个输出值为0或1。我们对目标进行one-hot-encoding，为每个样本生成一个有26个输出的目标数组，如果该字母可能存在，则使用接近1的值，否则使用接近0的值。代码如下：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: From this output, we know that our neural network's output layer will have 26
    neurons. The goal of the neural network is to determine which of these neurons
    to fire, based on a given input--the pixels that compose the image.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个输出中，我们知道我们的神经网络输出层将有26个神经元。神经网络的目标是根据给定的输入——构成图像的像素，确定要激活哪个神经元。
- en: 'The library we are going to use doesn''t support sparse arrays, so we need
    to turn our sparse matrix into a dense NumPy array. The code is as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的库不支持稀疏数组，因此我们需要将我们的稀疏矩阵转换为密集的NumPy数组。以下是相应的代码：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we perform a train/test split to later evaluate our data:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们进行训练/测试分割，以便稍后评估我们的数据：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Training and classifying
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和分类
- en: We are now going to build a neural network that will take an image as input
    and try to predict which (single) letter is in the image.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将构建一个神经网络，它将图像作为输入并尝试预测图像中包含的是哪个（单个）字母。
- en: We will use the training set of single letters we created earlier. The dataset
    itself is quite simple. We have a 20-by-20-pixel image, each pixel 1 (black) or
    0 (white). These represent the 400 features that we will use as inputs into the
    neural network. The outputs will be 26 values between 0 and 1, where higher values
    indicate a higher likelihood that the associated letter (the first neuron is A,
    the second is B, and so on) is the letter represented by the input image.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们之前创建的单个字母的训练集。数据集本身相当简单。我们有一个20x20像素的图像，每个像素为1（黑色）或0（白色）。这些代表我们将用作神经网络输入的400个特征。输出将是介于0和1之间的26个值，其中较高的值表示关联字母（第一个神经元是A，第二个是B，依此类推）是输入图像所代表的字母的可能性更高。
- en: We are going to use the scikit-learn's `MLPClassifier` for our neural network
    in this chapter.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用scikit-learn的`MLPClassifier`作为我们的神经网络。
- en: 'You will need a recent version of `scikit-learn` to use MLPClassifier. If the
    below import statement fails, try again after updating scikit-learn. You can do
    this using the following Anaconda command:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装`scikit-learn`的最新版本才能使用MLPClassifier。如果以下导入语句失败，请在更新scikit-learn后重试。您可以使用以下Anaconda命令来完成此操作：
- en: '`conda update scikit-learn`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`conda update scikit-learn`'
- en: 'As for other `scikit-learn` classifiers, we import the model type and create
    a new one. The constructor below specifies that we create one hidden layer with
    100 nodes in it. The size of the input and output layers is determined at training
    time:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他`scikit-learn`分类器，我们导入模型类型并创建一个新的实例。下面的构造函数指定我们创建一个包含100个节点的隐藏层。输入层和输出层的大小在训练时确定：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To see the internal parameters of the neural network, we can use the `get_params()`
    function. This function exists on all `scikit-learn` models. Here is the output
    from the above model. Many of these parameters can improve training or the speed
    of training. For example, increasing the learning rate will train the model faster,
    at the risk of missing optimal values:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看神经网络的内部参数，我们可以使用`get_params()`函数。此函数存在于所有`scikit-learn`模型上。以下是上述模型的输出。许多这些参数可以提高训练或训练速度。例如，增加学习率可以使模型训练得更快，但可能会错过最佳值：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we fit our model using the standard scikit-learn interface:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用标准的scikit-learn接口来拟合我们的模型：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Our model has now learned weights between each of the layers. We can view those
    weights by examining `clf.coefs_`, which is a list of NumPy arrays that join each
    of the layers. For example, the weights between the input layer with 400 neurons
    (from each of our pixels) to the hidden layer with 100 neurons (a parameter we
    set), can be obtained using `clf.coefs_[0]`. In addition, the weights between
    the hidden layer and the output layer (with 26 neurons) can be obtained using
    `clf.coefs_[1]`. These weights, together with the parameters above, wholly define
    our trained network.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式现在已经学会了每一层之间的权重。我们可以通过检查`clf.coefs_`来查看这些权重，它是一个NumPy数组列表，连接了每一层。例如，从包含400个神经元（来自我们每个像素）的输入层到包含100个神经元（我们设置的参数）的隐藏层之间的权重可以通过`clf.coefs_[0]`获得。此外，隐藏层和输出层（包含26个神经元）之间的权重可以通过`clf.coefs_[1]`获得。这些权重，连同上面的参数，完全定义了我们的训练网络。
- en: 'We can now use that trained network to predict our test dataset:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用那个训练好的网络来预测我们的测试数据集：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we evaluate the results:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估结果：
- en: '[PRE21]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The result is 0.96, which is pretty impressive. This version of the F1 score
    is based on the macro-average, which computes the individual F1 score for each
    class, and then averages them without considering the size of each class.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是0.96，相当令人印象深刻。这个版本的F1分数是基于宏观平均值的，它计算每个类的个别F1分数，然后不考虑每个类的大小进行平均。
- en: 'To examine these individual class results, we can view the classification report:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查这些个别类别的结果，我们可以查看分类报告：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The results from my experiment are shown here:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我实验的结果如下所示：
- en: '[PRE23]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The final `f1-score` for this report is shown on the bottom right, the second
    last number - 0.99\. This is the micro-average, where the `f1-score` is computed
    for each sample and then the mean is computed. This form makes more sense for
    relatively similar class sizes, while the macro-average makes more sense for imbalanced
    classes.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 该报告的最终`f1-score`显示在右下角，倒数第二个数字 - 0.99。这是微观平均值，其中对每个样本计算`f1-score`，然后计算平均值。这种形式对于相对相似的类别大小更有意义，而宏观平均值对于不平衡的类别更有意义。
- en: Pretty simple from an API perspective, as `scikit-learn` hides all of the complexity.
    However what actually happened in the backend? How do we train a neural network?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 从API的角度来看，这很简单，因为`scikit-learn`隐藏了所有的复杂性。然而，在后台实际上发生了什么？我们是如何训练一个神经网络的？
- en: Back-propagation
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: Training a neural network is specifically focused on the following things.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个神经网络特别关注以下事项。
- en: The first is the size and shape of the network - how many layers, what sized
    layers and what error functions they use. While types of neural networks exists
    that can alter their size and shape, the most common type, a feed-forward neural
    network, rarely has this capability. Instead, its size is fixed at initialization
    time, which in this chapter is 400 neurons in the first layer, 100 in the hidden
    layer and 26 in the final layer. Training for the shape is usually the job of
    a meta-algorithm that trains a set of neural networks and determines which is
    the most effective, outside of training the networks themselves.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一是网络的大小和形状——有多少层，层的大小以及它们使用的错误函数。虽然存在可以改变其大小和形状的神经网络类型，但最常见的类型，前馈神经网络，很少具有这种能力。相反，其大小在初始化时固定，在本章中是第一层400个神经元，隐藏层100个，最终层26个。通常，形状的训练是元算法的工作，它训练一组神经网络并确定哪一个是效果最好的，这超出了训练网络本身的工作范围。
- en: The second part of training a neural network is to alter the weights between
    neurons. In a standard neural network, nodes from one layer are attached to nodes
    of the next layer by edges with a specific weight. These can be initialized randomly
    (although several smarter methods do exist such as autoencoders), but need to
    be adjusted to allow the network to *learn* the relationship between training
    samples and training classes.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络的第二部分是改变神经元之间的权重。在一个标准的神经网络中，一个层的节点通过具有特定权重的边连接到下一层的节点。这些可以随机初始化（尽管存在一些更智能的方法，如自编码器），但需要调整以允许网络*学习*训练样本和训练类别之间的关系。
- en: This adjusting of weights was one of the key issues holding back very-early
    neural networks, before an algorithm called **back propagation** was developed
    to solve the issue.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种调整权重是早期神经网络面临的一个关键问题，在开发出称为**反向传播**的算法来解决该问题之前。
- en: The **back propagation** (**backprop**) algorithm is a way of assigning blame
    to each neuron for incorrect predictions. First, we consider the usage of a neural
    network, where we feed a sample into the input layer and see which of the output
    layer's neurons fire, as *forward propagation*. Back propagation goes backwards
    from the output layer to the input layer, assigning blame to each weight in the
    network, in proportion to the effect that weight has on any errors that the network
    makes.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播**（**backprop**）算法是一种将责任分配给每个神经元的错误预测的方法。首先，我们考虑神经网络的用法，我们将样本输入到输入层，并查看输出层的哪个神经元被激活，这称为**正向传播**。反向传播从输出层反向到输入层，按比例分配给网络中的每个权重，以反映该权重对网络犯的任何错误的影响。'
- en: 'The amount of change is based on two aspects:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 变化的量基于两个方面：
- en: Neuron activation
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元的激活
- en: The gradient of the activation function
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数的梯度
- en: The first is the degree to which the neuron was *activated*. Neurons that fire
    with high (absolute) values are considered to have a great impact on the result,
    while those that fired with small (absolute) values have a low impact on the result.
    Due to this, weights around neurons that fire with high values are changed more
    than those around small values.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方面是神经元激活的程度。那些以高（绝对）值激活的神经元被认为对结果有重大影响，而那些以小（绝对）值激活的神经元对结果的影响较小。因此，以高值激活的神经元周围的权重变化比以小值激活的神经元周围的权重变化更多。
- en: The second aspect to the amount that weights change is proportional to the *gradient
    of the activation function*. Many neural networks you use will have the same activation
    function for all neurons, but there are lots of situations where it makes sense
    to have different activation functions for different layers of neurons (or more
    rarely, different activation functions in the same layer). The gradient of the
    activation function, combined with the activation of the neuron, and the error
    assigned to that neuron, together form the amount that the weights are changed.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 权重变化的第二个方面是权重变化的量与激活函数的梯度成正比。你使用的许多神经网络将使用相同的激活函数来激活所有神经元，但有许多情况下，为不同层级的神经元（或更少的情况下，同一层级的不同激活函数）使用不同的激活函数是有意义的。激活函数的梯度，结合神经元的激活和分配给该神经元的错误，共同形成了权重变化的量。
- en: I've skipped over the maths involved in back propagation, as the focus of this
    book is on practical usage. As you increase your usage of neural networks, it
    pays to know more about what goes on inside the algorithm. I recommend looking
    into the details of the back-prop algorithm, which can be understood with some
    basic knowledge of gradients and derivatives.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我跳过了反向传播中涉及的数学，因为这本书的重点是实际应用。随着你对神经网络的更多使用，了解算法内部发生的事情是有益的。我建议研究反向传播算法的细节，这可以通过一些基本的梯度知识来理解。
- en: Predicting words
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测单词
- en: Now that we have a classifier for predicting individual letters, we now move
    onto the next step in our plan - predicting words. To do this, we want to predict
    each letter from each of these segments, and put those predictions together to
    form the predicted word from a given CAPTCHA.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了用于预测单个字母的分类器，我们现在转向我们计划中的下一步——预测单词。为此，我们想要预测每个段落的每个字母，并将这些预测组合起来，形成一个给定CAPTCHA的预测单词。
- en: 'Our function will accept a CAPTCHA and the trained neural network, and it will
    return the predicted word:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能将接受一个CAPTCHA和训练好的神经网络，并将返回预测的单词：
- en: '[PRE24]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can now test on a word using the following code. Try different words and
    see what sorts of errors you get, but keep in mind that our neural network only
    knows about capital letters:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下代码对一个单词进行测试。尝试不同的单词并查看你得到什么样的错误，但请注意，我们的神经网络只知道大写字母：
- en: '[PRE25]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can codify this into a function, allowing us to perform predictions more
    easily:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这编码成一个函数，使我们更容易进行预测：
- en: '[PRE26]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The returned results specify whether the prediction is correct, the original
    word, and the predicted word. This code correctly predicts the word GENE, but
    makes mistakes with other words. How accurate is it? To test, we will create a
    dataset with a whole bunch of four-letter English words from NLTK. The code is
    as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的结果指定了预测是否正确，原始单词和预测单词。此代码正确预测了单词GENE，但其他单词预测有误。它的准确度如何？为了测试，我们将创建一个包含大量来自NLTK的四字母英语单词的数据集。代码如下：
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Install NLTK using Anaconda: conda install nltk'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Anaconda安装NLTK：conda install nltk
- en: 'After installation, and before using it in code, you will need to download
    the corpus using:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，在使用代码之前，您需要使用以下命令下载语料库：
- en: python -c "import nltk; nltk.download('words')"
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: python -c "import nltk; nltk.download('words')"
- en: 'The words instance here is actually a corpus object, so we need to call `words()`
    on it to extract the individual words from this corpus. We also filter to get
    only four-letter words from this list:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的单词实例实际上是一个语料库对象，因此我们需要在它上面调用`words()`来提取这个语料库中的单个单词。我们还过滤以只从列表中获取四字母单词：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can then iterate over all of the words to see how many we get correct by
    simply counting the correct and incorrect predictions:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以遍历所有的单词，通过简单地计算正确和错误的预测来查看我们得到多少正确的单词：
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The results I get are 3,342 correct and 2,170 incorrect for an accuracy of just
    over 62 percent. From our original 99 percent per-letter accuracy, this is a big
    decline. What happened?
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我得到的结果是3,342个正确和2,170个错误，准确率仅为62%多一点。与我们的原始99%的字母准确率相比，这是一个很大的下降。发生了什么？
- en: 'The reasons for this decline are listed here:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下降的原因如下：
- en: The first factor to impact is our accuracy. All other things being equal, if
    we have four letters, and 99 percent accuracy per-letter, then we can expect about
    a 96 percent success rate (all other things being equal) getting four letters
    in a row (0.99⁴&ap;0.96). A single error in a single letter's prediction results
    in the wrong word being predicted.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 影响准确率的第一因素是我们的准确率。在其他条件相同的情况下，如果我们有四个字母，每个字母的准确率为99%，那么我们可以期望大约96%的成功率（在其他条件相同的情况下）连续得到四个字母（0.99⁴&ap;0.96）。单个字母预测中的单个错误会导致预测错误单词。
- en: The second impact is the shear value. Our dataset chose randomly between shear
    values of 0 to 0.5\. The previous test used a shear of 0.2\. For a value of 0,
    I get 75 percent accuracy; for a shear of 0.5, the result is much worse at 2.5
    percent. The higher the shear, the lower the performance.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个影响是剪切值。我们的数据集在0到0.5的剪切值之间随机选择。之前的测试使用了0.2的剪切值。对于0的值，我得到了75%的准确率；对于0.5的剪切值，结果要差得多，只有2.5%。剪切值越高，性能越低。
- en: The third impact is that often words are incorrectly segmented. Another issue
    is that some vowels are commonly mistaken, causing more errors than can be expected
    by the above error rates.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个影响是单词经常被错误分割。另一个问题是，一些元音字母经常被误判，导致错误率高于上述错误率。
- en: 'Let''s examine the second of these issues, and map the relationship between
    shear and performance. First, we turn our evaluation code into a function that
    is dependent on a given shear value:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查第二个问题，并映射剪切与性能之间的关系。首先，我们将评估代码转换为一个依赖于给定剪切值的函数：
- en: '[PRE30]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Then, we take a list of shear values and then use this function to evaluate
    the accuracy for each value. Note that this code will take a while to run, approximately
    30 minutes depending on your hardware.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们取一个剪切值的列表，并使用这个函数来评估每个值的准确性。请注意，这段代码将需要一段时间才能运行，大约30分钟，具体取决于你的硬件。
- en: '[PRE31]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, plot the result using matplotlib:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用matplotlib绘制结果：
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](img/B06162_08_18-1.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_08_18-1.png)'
- en: You can see that there is a severe drop in performance as the shear value increases
    past 0.4\. Normalizing the input would help, with tasks such as image rotation
    and unshearing the input.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，当剪切值超过0.4时，性能会急剧下降。对输入进行归一化将有所帮助，例如图像旋转和去除剪切。
- en: Another surprising option to address issues with shear is to increase the amount
    of training data with high shear values, which can lead to the model learning
    a more generalized output.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个解决剪切问题令人惊讶的选项是增加具有高剪切值的训练数据量，这可以使模型学习到更通用的输出。
- en: We look into improving the accuracy using post-processing in the next section.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨使用后处理来提高准确率。
- en: Improving accuracy using a dictionary
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用字典提高准确率
- en: Rather than just returning the given prediction, we can check whether the word
    actually exists in our dictionary. If it does, then that is our prediction. If
    it isn't in the dictionary, we can try and find a word that is similar to it and
    predict that instead. Note that this strategy relies on our assumption that all
    CAPTCHA words will be valid English words, and therefore this strategy wouldn't
    work for a random sequence of characters. This is one reason why some CAPTCHAs
    don't use words.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是仅仅返回给定的预测，我们可以检查这个单词实际上是否存在于我们的字典中。如果它在字典中，那么这就是我们的预测。如果它不在字典中，我们可以尝试找到一个与它相似的单词，并用它来预测。请注意，这种策略依赖于我们的假设，即所有CAPTCHA单词都将是有效的英语单词，因此这种策略对随机字符序列不起作用。这也是为什么一些CAPTCHA不使用单词的原因。
- en: There is one issue here—how do we determine the closest word? There are many
    ways to do this. For instance, we can compare the lengths of words. Two words
    that have a similar length could be considered more similar. However, we commonly
    consider words to be similar if they have the same letters in the same positions.
    This is where the **edit distance** comes in.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个问题——我们如何确定最接近的单词？有许多方法可以做到这一点。例如，我们可以比较单词的长度。长度相似的单词可以被认为是更相似的。然而，我们通常认为如果单词在相同的位置上有相同的字母，那么它们就是相似的。这就是**编辑距离**发挥作用的地方。
- en: Ranking mechanisms for word similarity
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字词相似度的排名机制
- en: 'The **Levenshtein edit distance** is a commonly used method for comparing two
    short strings to see how similar they are. It isn''t very scalable, so it isn''t
    commonly used for very long strings. The edit distance computes the number of
    steps it takes to go from one word to another. The steps can be one of the following
    three actions:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**Levenshtein 编辑距离**是用于比较两个短字符串以查看它们相似程度的一个常用方法。它不太可扩展，所以它通常不用于非常长的字符串。编辑距离计算从一个单词到另一个单词所需的步骤数。这些步骤可以是以下三种操作之一：'
- en: Insert a new letter into the word at any position
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单词的任何位置插入一个新的字母
- en: Delete any letter from the word
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从单词中删除任何字母
- en: Substitute a letter for another one
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用一个字母替换另一个字母
- en: The minimum number of actions needed to transform the first word into the second
    is given as the distance. Higher values indicate that the words are less similar.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 将第一个单词转换为第二个单词所需的最小操作数被给出作为距离。更高的值表示单词之间的相似度较低。
- en: 'This distance is available in NLTK as `nltk.metrics.edit_distance`. We can
    call it using only two strings and it returns the edit distance:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这个距离在 NLTK 中作为 `nltk.metrics.edit_distance` 提供。我们可以通过仅使用两个字符串来调用它，它返回编辑距离：
- en: '[PRE33]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: When used with different words, the edit distance is quite a good approximation
    to what many people would intuitively feel are similar words. The edit distance
    is great for testing spelling mistakes, dictation errors, and name matching (where
    you can mix up your Marc and Mark spelling quite easily).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当与不同的单词一起使用时，编辑距离相当好地近似于许多人直观上认为相似的字词。编辑距离非常适合测试拼写错误、听写错误和姓名匹配（在这种情况下，你很容易混淆
    Marc 和 Mark 的拼写）。
- en: 'However, it isn''t very good for our case. We don''t really expect letters
    to be moved around, just individual letter comparisons to be wrong. For this reason,
    we will create a different distance metric, which is simply the number of letters
    in the same positions that are incorrect. The code is as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这对我们的情况并不好。我们并不期望字母会被移动，只是单个字母的比较可能会出错。因此，我们将创建一个不同的距离度量，它只是相同位置上错误字母的数量。代码如下：
- en: '[PRE34]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We subtract the value from the length of the prediction word (which is four)
    to make it a distance metric where lower values indicate more similarity between
    the words.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从预测单词的长度（四个字母）中减去该值，使其成为一个距离度量，其中较低的值表示单词之间的相似度更高。
- en: Putting it all together
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'We can now test our improved prediction function using similar code to before.
    First, we define a prediction function, which also takes our list of valid words:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用与之前类似的代码来测试我们改进的预测函数。首先，我们定义一个预测函数，它也接受我们的有效单词列表：
- en: '[PRE35]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We compute the distance between our predicted word and each other word in the dictionary,
    and sort it by distance (lowest first). The changes in our testing code are in
    the following code:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算预测单词与字典中每个单词之间的距离，并按距离排序（距离最低的优先）。测试代码中的更改如下：
- en: '[PRE36]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The preceding code will take a while to run (computing all of the distances
    will take some time) but the net result is 3,037 samples correct and 2,476 samples
    incorrect. This is an accuracy of 71.5 percent for a boost of nearly 10 percentage
    points!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码将需要一段时间才能运行（计算所有距离将花费一些时间），但最终结果是 3,037 个样本正确，2,476 个样本错误。这是一个提高了近 10 个百分点的
    71.5% 的准确率！
- en: Looking for a challenge? Update the `predict_captcha` function to return the
    probabilities assigned to each letter. By default, the letter with the highest
    probability is chosen for each letter in a word. If that doesn't work, choose
    the next most probable word, by multiplying the per-letter probabilities together.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 想要挑战一下？更新 `predict_captcha` 函数以返回分配给每个字母的概率。默认情况下，每个单词中的字母都选择概率最高的字母。如果这不起作用，通过将每个字母的概率相乘来选择下一个最可能的单词。
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we worked with images in order to use simple pixel values to
    predict the letter being portrayed in a CAPTCHA. Our CAPTCHAs were a bit simplified;
    we only used complete four-letter English words. In practice, the problem is much
    harder--as it should be! With some improvements, it would be possible to solve
    much harder CAPTCHAs with neural networks and a methodology similar to what we
    discussed. The `scikit-image` library contains lots of useful functions for extracting
    shapes from images, functions for improving contrast, and other image tools that
    will help.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们处理图像以使用简单的像素值来预测 CAPTCHA 中描绘的字母。我们的 CAPTCHA 稍微简化了；我们只使用了完整的四个字母的英文单词。在实践中，这个问题要难得多--正如它应该的那样！通过一些改进，使用神经网络和与我们讨论的类似的方法，就有可能解决更难的
    CAPTCHA。`scikit-image` 库包含许多用于从图像中提取形状、改进对比度以及其他有助于图像的工具的有用函数。
- en: We took our larger problem of predicting words, and created a smaller and simple
    problem of predicting letters. From here, we were able to create a feed-forward
    neural network to accurately predict which letter was in the image. At this stage,
    our results were very good with 97 percent accuracy.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将预测单词的较大问题简化为一个较小且简单的问题，即预测字母。从这里，我们能够创建一个前馈神经网络来准确预测图像中的哪个字母。在这个阶段，我们的结果非常好，准确率达到
    97%。
- en: Neural networks are simply connected sets of neurons, which are basic computation
    devices consisting of a single function. However, when you connect these together,
    they can solve incredibly complex problems. Neural networks are the basis for
    deep learning, which is one of the most effective areas of data mining at the
    moment.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是简单连接的神经元集合，这些神经元是基本计算设备，由一个单一函数组成。然而，当你将它们连接在一起时，它们可以解决极其复杂的问题。神经网络是深度学习的基础，而深度学习是目前数据挖掘中最有效的领域之一。
- en: Despite our great per-letter accuracy, the performance when predicting a word
    drops to just over 60 percent when trying to predict a whole word. We improved
    our accuracy using a dictionary, searching for the best matching word. To do this,
    we considered the commonly used edit distance; however, we simplified it because
    we were only concerned with individual mistakes on letters, not insertions or
    deletions. This improvement netted some benefit, but there are still many improvements
    you could try to further boost the accuracy.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在逐字母预测上具有很高的准确性，但在尝试预测整个单词时，性能下降到仅超过 60%。我们通过使用字典并搜索最佳匹配单词来提高准确性。为此，我们考虑了常用的编辑距离；然而，我们简化了它，因为我们只关注单个字母错误，而不是插入或删除。这种改进带来了一些好处，但仍然有许多改进可以尝试以进一步提高准确性。
- en: To take the concepts in this chapter further, investigate changing the neural
    network structure, by adding more hidden layers, or changing the shape of those
    layers. Investigate the impact this has on the result. Further, try creating a
    more difficult CAPTCHA--does this drop the accuracy? Can you build a more complicated
    network to learn it?
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探讨本章的概念，研究改变神经网络结构，例如添加更多隐藏层或改变这些层的形状。研究这将对结果产生什么影响。进一步地，尝试创建一个更难的 CAPTCHA--这会降低准确性吗？你能构建一个更复杂的网络来学习它吗？
- en: Data mining problems such as the CAPTCHA example show that an initial problem
    statement, such as *guess this word*, can be broken into individual subtasks that
    can be performed using data mining. Further, those subtasks can be combined in
    a few different ways, such as with the use of external information. In this chapter,
    we combined our letter prediction with a dictionary of valid words to provide
    a final response, giving better accuracy than letter prediction alone.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 数据挖掘问题，例如 CAPTCHA 示例，表明一个初始问题陈述，例如 *猜这个词*，可以被分解成可以单独使用数据挖掘执行的任务。进一步来说，这些子任务可以通过几种不同的方式组合，例如使用外部信息。在本章中，我们将我们的字母预测与一个有效单词字典相结合，以提供最终响应，这比单独的字母预测提供了更高的准确性。
- en: In the next chapter, we will continue with string comparisons. We will attempt
    to determine which author (out of a set of authors) wrote a particular document--using
    only the content and no other information!
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续进行字符串比较。我们将尝试确定（在作者集合中）哪位作者撰写了特定的文档--仅使用内容，不使用其他信息！
