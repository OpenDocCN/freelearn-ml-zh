- en: Linear and Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the insights we gained by grouping similar information using common features,
    it's time to get a bit more mathematical and start to search for a way to describe
    the data by using a distinct function that will condense a large amount of information,
    and will allow us to predict future outcomes, assuming that the data samples maintain
    their previous properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with a step-by-step implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression and its implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softmax regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will begin with an explanation of the general principles. So,
    let''s ask the fundamental question: w*hat''s regression?*'
  prefs: []
  type: TYPE_NORMAL
- en: Before all considerations, regression is basically a **statistical process*****.***
    As we saw in the introductory section, regression will involve a set of data that
    has some particular probability distribution. In summary, we have a population
    of data that we need to characterize.
  prefs: []
  type: TYPE_NORMAL
- en: And what elements are we looking for in particular, in the case of regression? We
    want to determine the relationship between an independent variable and a dependent
    variable that optimally adjusts to the provided data. When we find such a functionbetween
    the described variables, it will be called the **regression function**.
  prefs: []
  type: TYPE_NORMAL
- en: There are a large number of function types available to help us model our current
    data, the most common example being the linear, polynomial, and exponential.
  prefs: []
  type: TYPE_NORMAL
- en: These techniques will aim to determine an objective function, which in our case
    will output a finite number of unknown optimum parameters of the function, called
    **parametric regression** techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression is normally applied in order to predict future variable values, and
    it's a very commonly employed technique for initial data modeling in data analysis
    projects, but it also can be used to optimize processes, finding common ground
    between related but dispersed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we will list a number of the possible application of regression analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: In social sciences, to predict the future values of all kinds of metrics, such
    as unemployment and population
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In economics, to predict future inflation rates, interest rates, and similar
    indicators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In earth sciences, to predict future phenomena, such as the thickness of the
    ozone layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To help with all elements of the normal corporate dashboard, adding probabilistic
    estimations for production throughput, earnings, spending, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proving the dependencies and relevancies between two phenomena
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the optimum mix of components in reaction experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimizing the risk portfolio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how sensitive a corporation’s sales are to changes in advertising
    expenditure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing how a stock's price is affected by changes to the interest rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantitative versus qualitative variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In day-to-day work with data, not all the elements we encounter are the same,
    and thus they require special treatment, depending on their properties. A very
    important distinction we can make to recognize how appropriate the problem variables
    are is by dividing the data types into quantitative and qualitative data variables
    using the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantitative variables**: In the realm of physical variables or measurements,
    we normally work with real numbers, or qualitative variables, because what matters
    the most is the quantity we are measuring. In this group, we have ordinal variables,
    that is, when we work with orders and rankings in an activity. Both of these variable
    types fall within the quantitative variables category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Qualitative variables:** On the other hand, we have measurements that show
    which class a sample belongs to. This can''t be expressed by a number, in the
    sense of a quantity; it is usually assigned a label, tag, or categorical value
    representing the group to which the sample belongs. We call these variables qualitative
    variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0a815a32-cec0-4afd-adbd-043f0be978dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Reference table addressing the differences between quantitative and qualitative
    analysis
  prefs: []
  type: TYPE_NORMAL
- en: Now let's address the question of which types of variable are suitable to apply
    to regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: The clear answer is quantitative variables, because the modeling of the data
    distribution can only be done through functions we use to detect a regular correspondence
    between those variables, and not on classes or types of elements. Regression requires
    one continuous output variable, which is only the case with quantitative metrics.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of qualitative variables, we assign the data to classification problems
    because the very definition of it is to search for non-numeric labels or tags
    to assign to a sample. This is the mission of classification, which we will see
    in the following chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, it's time to start with the simplest yet still very useful abstraction for
    our data–a linear regression function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In linear regression, we try to find a linear equation that minimizes the distance
    between the data points and the modeled line. The model function takes the following
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: y[i] = ßx[i] +α+ε[i]
  prefs: []
  type: TYPE_NORMAL
- en: Here, *α* is the intercept and *ß* is the slope of the modeled line. The variable
    *x* is normally called the independent variable, and *y* the dependent one, but
    it can also be called the regressor and the response variables.
  prefs: []
  type: TYPE_NORMAL
- en: The ε[i] variable is a very interesting element, and it's the error or distance
    from the sample *i* to the regressed line.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b75fbf6-d475-493d-91d9-c7426807bc65.png)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of the components of a regression line, including the original elements,
    the estimated ones (in red), and the error (ε)
  prefs: []
  type: TYPE_NORMAL
- en: The set of all those distances, calculated in the form of a function called
    the *cost* function, will give us, as a result of the solution process, the values
    of the unknown parameters that minimize the cost. Let's get to work on it.
  prefs: []
  type: TYPE_NORMAL
- en: Determination of the cost function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with all machine learning techniques, the process of learning depends on
    a minimized loss function, which shows us how right or wrong we are when predicting
    an outcome, depending on the stage of learning we are in.
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used cost function for linear regression is called *least
    squares*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define this cost function taking, for simplification a 2D regression,
    where we have a list of number tuples *(x[0], y[0])*, *(x[1], y[1])* ... *(x[n],
    y[n])* and the values to find, which are *β[0]*  and *β[1]*. The least squares
    cost function in this case can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68886061-ae2a-4eb5-954c-92219745fc37.png)'
  prefs: []
  type: TYPE_IMG
- en: Least squares function for a linear equation, using the standard variables β[0]
    and β[1] that will be used in the next sections
  prefs: []
  type: TYPE_NORMAL
- en: The summation for each of the elements gives a unique global number, which gives
    a global idea of the total differences between all the values (y[i]), and the
    corresponding point in our ideal regressing line (β[0] + β1*x*[i]).
  prefs: []
  type: TYPE_NORMAL
- en: 'The rationale for this operation is pretty clear:'
  prefs: []
  type: TYPE_NORMAL
- en: The summation gives us a unique global number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference model-real point gives us the distance or L1 error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Squaring this gives us a positive number, which also penalizes distances in
    an non-linear way, passing the one-error limit, so the more errors we commit,
    the more willing we will be to increase our penalization rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another way of phrasing this is that this process minimizes the sum of squared
    residuals, the residual being the difference between the value we get from the
    dataset and the expected value computed by the model, for the same input value.
  prefs: []
  type: TYPE_NORMAL
- en: The many ways of minimizing errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The least squares error function has several ways of getting a solution:'
  prefs: []
  type: TYPE_NORMAL
- en: The analytical way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the covariance and correlation values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most familiar way to the machine learning family of methods — the gradient
    descent way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analytical approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The analytical approach employs several linear algebra techniques in order to
    get an exact solution.
  prefs: []
  type: TYPE_NORMAL
- en: We are presenting this technique in a very succinct way because it's not directly
    related to the machine learning techniques we are reviewing in this book. We are
    presenting it for completeness.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we represent the error in a function in a matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a982ccbd-7d43-4efa-9f55-564990c6a257.png)'
  prefs: []
  type: TYPE_IMG
- en: Canonical form of the linear regression equation in the matrix form
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *J* is the cost function and has an analytical solution of:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26ba2a70-39a9-468f-88f9-ab93374edb34.png)'
  prefs: []
  type: TYPE_IMG
- en: Analytical solution of the matrix form of linear regression
  prefs: []
  type: TYPE_NORMAL
- en: Pros and cons of the analytical approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The approach of using linear algebra techniques to calculate the minimum error
    solution is an easier one, given the fact that we can give a really simple representation
    that is deterministic, so there is no additional guessing involved after applying
    the operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there are some possible problems with this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: First, matrix inversion and multiplication are very computationally intensive
    operations. They typically have a lower bound of approximately *O(n²**)* to *O(n³)*,
    so when the number of samples increases, the problem can become intractable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, and depending on the implementation, this direct approach could
    also have limited accuracy, because we can normally use the limits of the floating
    point capacity of the current hardware.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Covariance/correlation method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now it's time to introduce a new way of estimating the coefficient of our regressing
    line, and in the process, we will learn additional statistical measures, such
    as covariance and correlation, which will also help us when analyzing a dataset
    for the first time and drawing our first conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Covariance** is a statistical term, and can be canonically defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A measure of the systematic relationship between a pair of random variables
    wherein a change in one variable is reciprocated by an equivalent change in another
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance can take any value between -∞ to +∞, wherein a negative value is
    an indicator of a negative relationship, whereas a positive value represents a
    positive relationship. It also ascertains a linear relationship between the variables.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, when the value is zero, it indicates no direct linear relationship,
    and the values tend to form a blob-like distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Covariance is not affected by the unit of measure, that is, there is no change
    in the strength of the relationship between two variables when changing units.
    Nevertheless, the value of covariance will change. It has the following formula,
    which needs the mean of each axis as a prerequisite:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa14cc01-27e4-4d77-b042-68aaa589d1cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Remember when we described the process of normalization of a variable? We centered
    the variable by subtracting the mean and scaling it with the standard deviation
    of the dataset with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cf792e8-9f8e-456e-ae89-880e7c5e2ddb.png)'
  prefs: []
  type: TYPE_IMG
- en: Analytical form of the data normalization operation
  prefs: []
  type: TYPE_NORMAL
- en: This will be the starting point of our analysis, and we will be extending it
    towards each axis with the correlation value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The correlation value determines the degree to which two or more random variables
    move in tandem. During the study of two variables, if it has been observed that
    the movement of one variable is concordant with an equivalent movement in another
    variable, then the variables are said to be correlated, with the exact correlation
    value given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d9fb1b3-720a-4fd9-97fa-ef1a5326c698.png)'
  prefs: []
  type: TYPE_IMG
- en: Canonical definition of correlation
  prefs: []
  type: TYPE_NORMAL
- en: As a real value-based metric, it can be of two types, positive or negative.
    The variables are positively or directly correlated when the two variables move
    in the same direction. When the two variables move in opposite direction, the
    correlation is negative or inverse.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of correlation lies between *-1* to *+1*, wherein values close to
    *+1* represent a strong positive correlation and values close to *-1* are an indicator
    of a strong negative correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c75050d-fd72-437f-b0e2-21759ec14479.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphic depiction of how the distribution of the samples affects the correlation
    values
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways of measuring correlation. In this book, we will be talking
    mainly about linear correlation. There are other methods of studying non-linear
    correlation, which won't be covered in this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec163393-6f45-416d-bba8-1238a9438b08.png)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of the difference between linear correlation and non-linear correlation
    measures
  prefs: []
  type: TYPE_NORMAL
- en: Within the practical exercises of this chapter, you will find an implementation
    of both linear covariance and correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Searching for the slope and intercept with covariance and correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have known from the beginning, what we need is to find the equation of
    a line, representing the underlying data, in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d10261e6-ae1e-43dd-9245-aed8cb9f4cde.png)'
  prefs: []
  type: TYPE_IMG
- en: Approximate definition of a linear equation
  prefs: []
  type: TYPE_NORMAL
- en: 'As we know that this line passes through the average of all the points, we
    can estimate the intercept, with the only unknown being the estimated slope:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63f6700e-7d98-4c47-b89e-f2d66fd10257.png)'
  prefs: []
  type: TYPE_IMG
- en: Derived definition of intercept
  prefs: []
  type: TYPE_NORMAL
- en: The slope represents the change in the dependent variable divided by the change
    in the independent variable. In this case, we are dealing with variation in data
    rather than absolute differences between coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the data is of non-uniform nature, we are defining the slope as the proportion
    of the variance in the independent variable that covaries with the dependent variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c12db851-a7b6-4b75-aca2-47ea376f313f.png)'
  prefs: []
  type: TYPE_IMG
- en: Estimated slope coefficient
  prefs: []
  type: TYPE_NORMAL
- en: 'As it happens, if our data actually looks like a circular cloud when we plot
    it, our slope will become *zero*, suggesting no causal relationship between the
    variation in *x and *y*, *expressed in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c519abdc-078f-4cd4-aa61-4077e22947a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Expanded form of the estimated slope coefficient
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the formulas we have shown previously, we can finally simplify the
    expression of the slope of the estimated regression line to the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fde6a3d-5fab-4bb1-a91c-c7db6eda7d31.png)'
  prefs: []
  type: TYPE_IMG
- en: Final form of the slope coefficient
  prefs: []
  type: TYPE_NORMAL
- en: Here, *S[y]* is the standard deviation in *y*, and *S[x]* is the standard deviation
    in *x.*
  prefs: []
  type: TYPE_NORMAL
- en: 'With the help of the remaining elements in the equation, we can simply derive
    the intercept based on the knowledge that the line will reach the mean dataset
    point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0d9968f-f485-406a-bd3b-5f974da7435c.png)'
  prefs: []
  type: TYPE_IMG
- en: Final form of the approximated intercept coefficient
  prefs: []
  type: TYPE_NORMAL
- en: So, we are done with a very summarized expression of two preliminary forms of
    regression, which have also left many analysis elements for us to use. Now it's
    time to introduce the star of the current machine learning techniques, one that
    you will surely use in many projects as a practitioner, called **gradient descent**.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's time to talk about the method that will take us to the core of modern machine
    learning. The method explained here will be used with many of the more sophisticated
    models in a similar fashion, with increased difficulty but with the same principles.
  prefs: []
  type: TYPE_NORMAL
- en: Some intuitive background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To introduce gradient descent, we will first take a look at our objective—the
    fitting of a line function to a set of provided data. And what do we have as elements?
  prefs: []
  type: TYPE_NORMAL
- en: A model function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An error function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another element that we could get is a representation of all the possible errors
    for any of the combinations of the parameters. That would be cool, right? But
    look at what such a function looks like just for a problem with a simple line
    as the solution. This curve represents *z= x² + y²*, which follows the form of
    the least squares error function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1b2619c-d444-44e4-bb91-5876156fa5ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Least squares error surface, in the case of two variables. In the case of linear
    regression, they are the slope and the intersect
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, calculating all the possible outcomes per line parameter would
    consume too much CPU time. But we have an advantage: we know that the surface
    of such a curve is **c****onvex** (a discussion beyond the scope of this book),
    so it roughly looks like a bowl, and it has a unique minimum value (as seen in
    the previous folder). This will spare us the problem of locating local points
    that look like a minimum, but in fact are just bumps on the surface.'
  prefs: []
  type: TYPE_NORMAL
- en: The gradient descent loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So it''s time to look for a method to converge to the minimum of a function,
    knowing only where I am on the surface, and possibly the gradient at the point
    on the surface I am standing on:'
  prefs: []
  type: TYPE_NORMAL
- en: Start at a random position (remember, we don't know anything about the surface
    yet)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look for the direction of maximum change (as the function is convex, we know
    it will guide us to the minimum)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advance over the error surface in that direction, proportionally to the error
    amount
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjust the starting point of the next step to the new point on the surface where
    we landed and repeat the process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method allows us to discover the path for the minimization of our values
    in an iterative way and in a limited time, when compared to a brute force method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process for two parameters and the least squares function goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b1ff442-6ed4-42e6-9ce5-eb2d53028ba7.png)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of the gradient descent algorithm, Beginning from a starting high
    error point, and descending in the direction of maximum change
  prefs: []
  type: TYPE_NORMAL
- en: This gives us an idea of how the process functions work in a normal setting,
    when we use and choose good and appropriate initial parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapters, we will cover the process of gradient descent in more
    detail, including how choosing different elements (that we will call hyper-parameters)
    changes the behavior of the process.
  prefs: []
  type: TYPE_NORMAL
- en: Formalizing our concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's go over the mathematical side of our process, so we have a reference
    of all the parts involved, before using them in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The elements or our equations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The linear function variables, *β*[*0*] and *β[1]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of samples in the sampleset, *m*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The different elements in the sampleset, *x^((i))* and *y^((i))*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start with our error function, *J*. It was defined in previous sections
    under the name of the least squares function. We will add, for practicality, the
    term 1/2m at the start of the equation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/487739d9-a92a-4574-b71b-7a8f7976693c.png)'
  prefs: []
  type: TYPE_IMG
- en: Least squares error function
  prefs: []
  type: TYPE_NORMAL
- en: Let's introduce a new operator, which is the basis of all the following work,
    gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build the concept of it, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A function of one or more independent variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The partial derivative of the function for all independent variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we already know how partial derivatives work at the moment, it''s enough
    to say that the gradient is a vector containing all of the already mentioned partial
    derivatives; in our case, it will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aaf9f35f-f5c2-4095-88a8-921b55522b1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient of the error function
  prefs: []
  type: TYPE_NORMAL
- en: What's the purpose of such an operator? If we are able to calculate it, it will
    give us the direction of change of the whole function at a single point.
  prefs: []
  type: TYPE_NORMAL
- en: First, we calculate the partial derivative. You can try to derive it; basically,
    it uses the chain rule of deriving a squared expression and then multiplies it
    by the original expression.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second part of the equation, we simplify the linear function by the
    name of the model function, *h[a]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c81125f1-7fcb-4559-91a4-40788b068f26.png)'
  prefs: []
  type: TYPE_IMG
- en: Partial derivative of the error function for the β[1] variable
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of *β[1]* we get the additional factor of the *x^((i))* element
    because the derivative of *β[1]x*^(*(i)* )is *x^((i))*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c6376e9-d5cd-4a6e-8629-82cd539945c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Partial derivative of the error function for the β[1] variable
  prefs: []
  type: TYPE_NORMAL
- en: Now we introduce the recursion expression, which will provide (when iterating
    and if conditions are met) a combination of parameters that decrease the total
    error in a convergent way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we introduce a really important element: the step size, with the name α.
    What''s the purpose of it? It will allow us to scale how much we will advance
    in one step. We will discover that not choosing the right amount of power can
    lead to disastrous consequences, including the divergence of the error to the
    infinite.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the second formula only has the tiny difference of being multiplied
    by the current *x* value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e90f1ce1-08a6-4af4-9111-125caf313f1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Recursion equation for the  model functions
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we are ready to go! Now we will just add a bit of mathematical spice in
    order to produce a more compact representation of the algorithm. Let''s now express
    the unknowns in vector form, so all the expressions will be expressed as a whole:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f828048c-302b-4b06-a2c2-f67d83f54625.png)'
  prefs: []
  type: TYPE_IMG
- en: Expression of β in vector form
  prefs: []
  type: TYPE_NORMAL
- en: 'With this new expression, our recursion steps can be expressed in this simple
    and easy-to-remember expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ad85c8a-f5a5-432c-a237-d03e03297180.png)'
  prefs: []
  type: TYPE_IMG
- en: Expression of the gradient descent recursion in vector form
  prefs: []
  type: TYPE_NORMAL
- en: Expressing recursion as a process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The whole method of finding the minimum error can alternatively be expressed
    in a flowchart, so that we can have all the elements in the same place and understand
    how easy it looks if we, for a moment don''t take into account the somewhat complex
    analytic mechanisms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b239fa1-5f80-47f7-bcf6-70b9f2ff0d7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Flow diagram of the gradient descent method. Note the simple building blocks,
    without taking into account the subtle mathematics it involves
  prefs: []
  type: TYPE_NORMAL
- en: 'So, with this last procedural vision of the gradient descent process, we are
    ready to go on to the more practical parts of this chapter. We hope you enjoyed
    this journey towards finding an answer to the question: What''s the best way to
    represent our data in a simple way? And rest assured we will use much more powerful
    tools in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Going practical – new tools for new methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce a new library that will help us with covariance
    and correlation, especially in the data visualization realm.
  prefs: []
  type: TYPE_NORMAL
- en: What is **Seaborn**?
  prefs: []
  type: TYPE_NORMAL
- en: Seaborn is a library for making attractive and informative statistical graphics
    in Python. Moreover, it also provides very useful multivariate analysis primitives,
    which will help you decide whether or not and how to apply determinate regression
    analysis to your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the features that Seaborn offers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Several built-in themes of very high quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools for choosing color palettes to make beautiful plots that reveal patterns
    in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very important functions for visualizing univariate and bivariate distributions
    or for comparing them between subsets of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools that fit and visualize linear regression models for different kinds of
    independent and dependent variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting functions which try to do something useful when called with a minimal
    set of arguments; they expose a number of customizable options through additional
    parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An important additional feature is, given that Seaborn uses matplotlib, the
    graphics can be further tweaked using those tools and rendered with any of the
    matplotlib backends.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's explore the most useful utilities that Seaborn will bring.
  prefs: []
  type: TYPE_NORMAL
- en: Useful diagrams for variable explorations – pairplot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the stage of data exploration, one of the most useful measures we can have
    is a graphical depiction of how all the features in the dataset interact, and
    discover the joint variations in an intuitive manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09c1ba22-b1bf-49f0-a9f6-c2ac41f11b70.png)'
  prefs: []
  type: TYPE_IMG
- en: Pairplot for the variables in the Iris dataset
  prefs: []
  type: TYPE_NORMAL
- en: Correlation plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The correlation plot allows us to summarize the variable dependency in a much
    more succinct way, because it shows the direct correlation between variable pairs,
    using a color pallet. The diagonal values are of course 1, because all variables
    have a maximum correlation with themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21467333-0c02-4339-9102-565e8c79026e.png)'
  prefs: []
  type: TYPE_IMG
- en: Correlation plot of the San Francisco housing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration and linear regression in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will start using one of the most well-known *toy* datasets,
    explore it, and select one of the dimensions to learn how to build a linear regression
    model for its values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing all the libraries (`scikit-learn`, `seaborn`, and
    `matplotlib`); one of the excellent features of Seaborn is its ability to define
    very professional-looking style settings. In this case, we will use the `whitegrid`
    style:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The Iris dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s time to load the *Iris* dataset. This is one of the most well-known historical
    datasets. You will find it in many books and publications. Given the good properties
    of the data, it is useful for classification and regression examples. The Iris
    dataset ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris))
    contains 50 records for each of the three types of iris, 150 lines in a total
    over five fields. Each line is a measurement of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final field is the type of flower (*setosa*, *versicolor*, or *virginica*).
    Let’s use the `load_dataset` method to create a matrix of values from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to understand the dependencies between variables, we will implement
    the covariance operation. It will receive two arrays as parameters and will return
    the `covariance(x,y)` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try the implemented function and compare it with the NumPy function.
    Note that we calculated `cov(a,b)`, and NumPy generated a matrix of all the combinations
    `cov(a,a)`, `cov(a,b)`, so our result should be equal to the values `(1,0)` and
    `(0,1)` of that matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Having done a minimal amount of testing of the correlation function as defined
    earlier, receive two arrays, such as `covariance`, and use them to get the final
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s test this function with two sample arrays, and compare this with the
    `(0,1)` and `(1,0)` values of the correlation matrix from NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Getting an intuitive idea with Seaborn pairplot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very good idea when starting worked on a problem is to get a graphical representation
    of all the possible variable combinations.
  prefs: []
  type: TYPE_NORMAL
- en: Seaborn’s `pairplot` function provides a complete graphical summary of all the
    variable pairs, represented as scatterplots, and a representation of the univariate
    distribution for the matrix diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how this plot type shows all the variables dependencies, and
    try to look for a linear relationship as a base to test our regression methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00ae62c1-69b8-41b1-8105-3989b5102bdf.png)'
  prefs: []
  type: TYPE_IMG
- en: Pairplot of all the variables in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lets'' select two variables that, from our initial analysis, have the property
    of being linearly dependent. They are `petal_width` and `petal_length`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now take a look at this variable combination, which shows a clear linear
    tendency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the representation of the chosen variables, in a scatter type graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/492acb6c-06e9-40f5-9a6c-9261ab328337.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the current distribution of data that we will try to model with our
    linear prediction function.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the prediction function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s define the function that will abstractedly represent the modeled
    data, in the form of a linear function, with the form *y=beta*x+alpha*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Defining the error function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s now time to define the function that will show us the difference between
    predictions and the expected output during training. As we will explain in depth
    in the next chapter, we have two main alternatives: measuring the absolute difference
    between the values (or L1), or measuring a variant of the square of the difference
    (or L2). Let’s define both versions, including the first formulation inside the
    second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Correlation fit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will define a function implementing the correlation method to find
    the parameters for our regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s then run the fitting function and print the guessed parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now graph the regressed line with the data in order to intuitively show
    the appropriateness of the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the final plot we will get with our recently calculated slope and intercept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0873604a-4bd0-4782-ac23-c86db29e98fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Final regressed line.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial regression and an introduction to underfitting and overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When looking for a model, one of the main characteristics we look for is the
    power of generalizing with a simple functional expression. When we increase the
    complexity of the model, it's possible that we are building a model that is good
    for the training data, but will be too optimized for that particular subset of
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting, on the other hand, applies to situations where the model is too
    simple, such as this case, which can be represented fairly well with a simple
    linear model.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will work on the same problem as before, using
    the scikit-learn library to search higher-order polynomials to fit the incoming
    data with increasingly complex degrees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going beyond the normal threshold of a quadratic function, we will see how
    the function looks to fit every wrinkle in the data, but when we extrapolate,
    the values outside the normal range are clearly out of range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The combined graph shows how the different polynomials' coefficients describe
    the data population in different ways. The 20 degree polynomial shows clearly
    how it adjusts perfectly for the trained dataset, and after the known values,
    it diverges almost spectacularly, going against the goal of generalizing for future
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf696898-ba33-4679-a61a-b6eb42ed6443.png)'
  prefs: []
  type: TYPE_IMG
- en: Curve fitting of the initial dataset, with polynomials of increasing values.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with gradient descent in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So now we are working with gradient descent techniques in practice for the
    first time! The concepts we are now practicing will serve us well during the rest
    of the book. Let''s start by importing the prerequisite library, as always. We
    will use NumPy for numeric processing, and Seaborn and matplotlib for representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The loss function will be the guide for us to know how well we are doing. As
    we saw in the theoretical section, the least squares method will be used.
  prefs: []
  type: TYPE_NORMAL
- en: You can review the *J* or loss function definition and properties in the previous
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, this `least_squares` function will receive the current regression line
    parameters, *b[0]* and b[0], and the data elements to measure how good our representation
    of reality is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will define each step of the recurrence. As parameters, we will receive
    the current *b*[0] and *b*[1], the points used to train the model, and the learning
    rate. On line five of the `step_gradient` function, we see the calculation of
    both gradients, and then we create the `new_b0` and `new_b1` variables, updating
    their values in the error direction, scaled by the learning rate. On the last
    line, we return the updated values and the current error level after all points
    have been used for the gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define a function that will run a complete training outside the model
    so we can check all the combinations of parameters in one place. This function
    will initialize the parameters and will repeat the gradient step a fixed number
    of times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This process could prove inefficient when the convergence rate is high, wasting
    precious CPU iterations. A more clever stop condition would consist of adding
    an acceptable error value, which would stop the iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, time to try our model! Let''s start loading the Iris dataset again, for
    reference, and as a means of checking the correctness of our results. We will
    use the `petal_width` and `petal_length` parameters, which we have already seen
    and decided they are good candidates for linear regression. The `dstack` command
    from NumPy allows us to merge the two columns, which we converted to a list to
    discard the column headers. The only caveat is that the resulting list has an
    unused extra dimension, which we discard using the `[0]` index selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s try our model with what seem to be good initial parameters, a `0.0001`
    learning rate, initial parameters at `0`, and `1000` iterations; lets see how
    it behaves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/1bddad51-5230-49f5-b989-aad474339736.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Well, that''s bad; clearly, we are not yet there. Let''s see what happened
    with the error during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c854daf0-1fef-458e-ab5a-507a2bd34a3d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The process seems to be working, but it''s a bit slow. Maybe we can try to
    increase the step by a factor of 10 to see if it converges quickly? Let''s check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/41808f98-370f-4132-824a-ac418872dc86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That was better! The process converges much more quickly. Let''s check how
    the regressed line looks now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/dace0203-d296-4d39-8683-b5beae83a51b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Yes! It looks much better. We could think we are done, but a developer always
    wants to go faster. Let''s see what would occur if we wanted to go faster, with
    an enormous step of `2`, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0ba9dc83-0d77-4b5c-9a5c-093df03947ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is a bad move; as you can see, the error finally went to infinity! What
    happens here? Simply, the steps we are taking are so radical that instead of slicing
    the imaginary bowl we described before, we are just jumping around the surface,
    and as the iterations advance, we began to escalate the accumulated errors without
    control. Another measure that could be taken is to improve our seed values, which,
    as you have seen started, with a value of `0`. This is a very bad idea in general
    for this technique, especially when you are working with data that is not normalized.
    There are more reasons for this, which you can find in more advanced literature.
    So, let''s try to initialize the parameter on a pseudo-random location in order
    to allow the graphics to be the same across the code examples, and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/6cd0d7cb-29fd-4ecd-8c3d-d72f9df03d12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, even if you have the same sloppy error rate, the initial error
    value decreases tenfold (from 2e5 to 2e4). Now let''s try a final technique to
    improve the convergence of the parameters based on the normalization of the input
    values. As you have already studied in [Chapter 2](fa27740b-e9e0-4ad1-ab13-dfe57b30a956.xhtml), *The
    Learning Process,* it consists of centering and scaling the data. What''s the
    effect of that operation on the data? Using a graphical image, when data is not
    normalized, the error surface tends to be shallow and the values oscillate a lot.
    The normalization transforms that data into a more deep surface, with more definite
    gradients towards the center:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/35b4c6f1-dc4f-4b94-820d-f884d3b3a0e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have this set of clean and tidy data, let''s try again with the
    last slow convergence parameters, and see what happens to the error minimization
    speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/fd5f8a31-a6a3-40d4-bfa7-a795cc34616d.png)'
  prefs: []
  type: TYPE_IMG
- en: A very good starting point indeed! Just by normalizing the data, we have half
    the initial error values, and the error went down 20% after 1,000 iterations.
    The only thing we have to remember is to denormalize after we have the results,
    in order to have the initial scale and data center. So, that's all for now on
    gradient descent. We will be revisiting it in the next chapters for new challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The way of this book is one of generalizations. In the first chapter, we began
    with simpler representations of the reality, and so simpler criteria for grouping
    or predicting information structures.
  prefs: []
  type: TYPE_NORMAL
- en: After having reviewed linear regression, which is used mainly to predict a real
    value following a modeled linear function, we will advance to a generalization
    of it, which will allow us to separate binary outcomes (indicating that a sample
    belongs to a class), starting from a previously fitted linear function. So let's
    get started with this technique, which will be of fundamental use in almost all
    the following chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Problem domain of linear regression and logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To intuitively understand the problem domain of the logistic regression, we
    will employ a graphical representation.
  prefs: []
  type: TYPE_NORMAL
- en: In the first we show the linear fitting function, which is the main objective
    of the whole model building process, and at the bottom, the target data distribution.
    As you clearly see, data is now of a binary nature, and a sample belongs to one
    or another options, nothing in the middle. Additionally, we see that the modelling
    function is of a new type; we will later name it and study its properties. You
    may wonder what this has it to do with a linear function? Well, as we will see
    later, it will be inside of that s-like function, adapting its shape.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08c9647f-ab32-48d8-8a56-c50c4190de98.png)'
  prefs: []
  type: TYPE_IMG
- en: Simplified depiction of the common data distributions where Linear or Logistic
    regression are applied.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing, Linear regression can be imagined as a continuum of increasingly
    growing values. The other is a domain where the output can have just two different
    values based on the *x* value. In the particular case shown in the image, we can
    see a clear trend towards one of the possible outcomes, as the independent variable
    increases, and the sigmoid function allows us to transition from two outcomes,
    which don't have a clear separation in time, which gives us an estimated probability
    in the overlap zone of the non-occurrence/occurrence zone.
  prefs: []
  type: TYPE_NORMAL
- en: In some ways the terminology is a bit confusing, given that we are doing a regression
    that is obtaining a continuous value, but in reality, the final objective is building
    a prediction for a classification problem with discrete variables.
  prefs: []
  type: TYPE_NORMAL
- en: The key here is to understand that we will obtain probabilities of an item pertaining
    to a class and not a totally discrete value.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic function predecessor – the logit functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we study the logistic function, we will review the original function
    on which it is based, the logit function, which gives it some of its more general
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, when we talk about the logit function, we are working with the
    function of a random variable *p*; more specifically, one corresponding with a
    Bernoulli distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Link function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we are trying to build a **generalized linear model**, we want to start from
    a linear function and obtain a mapping to a probability distribution from the
    dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: Since the output type of our model is of a binary nature, the normally chosen
    distribution is the Bernoulli distribution, and the link function, leaning toward
    the logistic function, is the **logit function**.
  prefs: []
  type: TYPE_NORMAL
- en: Logit function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the possible variables that we could utilize is the natural logarithm
    of the odds, that *p* equals one. This function is called the logit function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ee45e25-bf32-4991-aad0-1ca557eab066.png)'
  prefs: []
  type: TYPE_IMG
- en: We can also call the logit function a log-odd function, because we are calculating
    the log of the odds *(p/1-p)* for a given probability *p*.
  prefs: []
  type: TYPE_NORMAL
- en: Logit function properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, as we can visually infer, we are replacing *x* with a combination of the
    independent variables, no matter their values, and replacing *x* with any occurrence
    from minus infinity to infinity. We are scaling the response to be between *0*
    and *1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/580123ba-396c-452e-8ff2-5ad2f6b8979b.png)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of the main range characteristics of the logit function
  prefs: []
  type: TYPE_NORMAL
- en: The importance of the logit inverse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose that we calculate the inverse of the logit function. The simple inverse
    transformation of the logit will give us the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fadf7d1d-7327-4ac1-af90-14a2c064d5d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Analytical definition of the logit function
  prefs: []
  type: TYPE_NORMAL
- en: This function is nothing less than a **sigmoid function**.
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid or logistic function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The logistic function will represent the binary options we are representing
    in our new regression tasks. The logistic function is defined as follows (changing the
    independent variable from α to *t* for clarity):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55f11a91-4e1a-4fc9-815c-d1c3a170bb38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will find this new figure common in the following sections, because it
    will be used very frequently as an activation function for neural networks and
    other applications. In the following figure, you will find the graphical representation
    of the sigmoid function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6177f95-f5bc-4103-8581-894a78bd5007.png)Standard sigmoid'
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we interpret and give this function a meaning for our modeling tasks?
    The normal interpretation of this equations is that *t* represents a simple independent
    variable, but we will improve this model, assuming that *t* is a linear function
    of a single explanatory variable *x* (the case where *t* is a linear combination
    of multiple explanatory variables is treated similarly), expressing it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc0dc978-01a0-44d4-a46e-efed47460598.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we can start again from the original logit equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d527bcb-6f1d-4939-a9d8-43d56d3e170c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will reach to the regression equation, which will give us the regressed
    probability with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bd65ee9-f49d-45f8-8033-7480823ef98d.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that *p* (hat) denotes an estimated probability. What will give us a measure
    of how approximate we are to the solution? Of course, a carefully chosen loss
    function!
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows how the mapping from an infinite domain of possible
    outcomes which will be finally reduced to the *[0,1]* range, with *p* being the
    probability of the occurrence of the event being represented. This is shown in
    a simple schema, which is the  structure and domain transformation of the logit
    function (from a linear one to a probability modeled by a Sigmoid):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0643bfd-984e-4cb5-8beb-7adfc19ae8c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Function mapping for the logit of a linear equation, resulting in a sigmoid
    curve
  prefs: []
  type: TYPE_NORMAL
- en: What changes will affect the parameters of the linear function? They are the
    values that will change the central slope and the displacement from zero of the
    sigmoid function, allowing it to more exactly reduce the error between the regressed
    values and the real data points.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of the logistic function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Every curve in the function space can be described by the possible objectives
    it could be applied to. In the case of the logistic function, they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Model the probability of an event *p*, depending on one or more independent
    variables. For example, the probability of being awarded a prize, given previous
    qualifications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimate (this is the regression part) *p* for a determined observation, related
    to the possibility of the event not occurring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict the effect of the change of independent variables using a binary response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classify observations by calculating the probability of an item being of a determined
    class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiclass application – softmax regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have been classifying in the case of only having two classes,
    or in probabilistic language, event occurrence probabilities *p*. But this logistic
    regression can also be conveniently generalized to account for many classes.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw before, in logistic regression we assumed that the labels were binary
    (*y(i)∈{0,1})*, but softmax regression allows us to handle *y(i)∈{1,…,K}*, where
    *K* is the number of classes and the label *y* can take on *K* different values,
    rather than only two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a test input *x*, we want to estimate the probability that *P(y=k|x)* for
    each value of *k=1,…,K*. The softmax regression will make this output a *K*-dimensional
    vector (whose elements sum to 1), giving us our *K* estimated probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea26194c-e41f-48e6-8a94-de9c5a9e6376.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison between the univariate logisic regression outcome and N classes softmax
    regression
  prefs: []
  type: TYPE_NORMAL
- en: Practical example – cardiac disease modeling with logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's time to finally solve a practical example with the help of the very useful
    logistic regression. In this first exercise, we will work on predicting the probability
    of having coronary heart disease, based on the age of the population. It's a classic
    problem, which will be a good start for understanding this kind of regression
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The CHDAGE dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the first simple example, we will use a very simple and often studied dataset,
    which was published in *Applied Logistic Regression*, from *David W. Hosmer*,
    *Jr. Stanley Lemeshow and **Rodney X. Sturdivant*. We list the age in years (`AGE`)
    and the presence or absence of evidence of significant **coronary heart disease**
    (**CHD**) for 100 subjects in a hypothetical study of risk factors for heart disease.
    The table also contains an identifier variable (`ID`) and an age group variable
    (`AGEGRP`).
  prefs: []
  type: TYPE_NORMAL
- en: The outcome variable is `CHD`, which is coded with a value of `0` to indicate
    that `CHD` is absent, or `1` to indicate that it is present in the individual.
    In general, any two values could be used, but we have found it most convenient
    to use zero and one. We refer to this dataset as the `CHDAGE` data.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `CHDAGE` dataset is a two-column CSV file that we will download from an
    external repository. In the first chapter, we used native TensorFlow methods to
    read the dataset. In this chapter, we will alternatively use a complementary and
    popular library to get the data. The cause for this new addition is that, given
    that the dataset only has 100 tuples, it is practical to just read it in one line,
    and also, we can get simple but powerful analysis methods for free from the pandas
    library.
  prefs: []
  type: TYPE_NORMAL
- en: In the first stage of this project, we will start loading an instance of the
    `CHDAGE` dataset. Then we will print vital statistics about the data, and then
    proceed to preprocessing. After doing some plots of the data, we will build a
    model composed of the activation function, which will be a softmax function, for
    the special case where it becomes a standard logistic regression, that is, when
    there are only two classes (existence or not of the illness).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s read the dataset from the CSV original file using `read_csv` from pandas,
    and draw the data distribution using the scatter function of matplotlib. As we
    can see, there is a definite pattern through the years that correlates to the
    presence of cardiac disease with increasing age:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the current plot of the original data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f4526d5-8e60-43e6-830f-fecc8d7db99c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we will create a logistic regression model using the logistic regression
    object from scikit-learn, and then we will call the `fit` function, which will
    create a sigmoid optimized to minimize the prediction error for our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now it's time to represent the results. Here, we will generate a linear space
    from 10 to 90 years with 100 subdivisions.
  prefs: []
  type: TYPE_NORMAL
- en: For each sample of the domain, we will show the probability of occurrence (1)
    and not occurrence (0, or the inverse of the previous one).
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we will show the predictions along with the original data points,
    so we can match all the elements in a single graphic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/73dd7c53-d8cc-4c9a-afe2-a8b01d1e1c8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Simultaneous plot of the original data distribution, the modeling logistic curve,
    and its inverse
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've reviewed the main ways to approach the problem of modeling
    data using simple and definite functions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be using more sophisticated models that can reach
    greater complexity and tackle higher-level abstractions, and can be very useful
    for the amazingly varied datasets that have emerged recently, starting with simple
    **feedforward networks**.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Galton, Francis, "*Regression towards mediocrity in hereditary stature.*" The
    Journal of the Anthropological Institute of Great Britain and Ireland 15 (1886):
    246-263.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Walker, Strother H., and David B. Duncan, "*Estimation of the probability of
    an event as a function of several independent variables.*" Biometrika 54.1-2 (1967):
    167-179.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cox, David R, "*The regression analysis of binary sequences.*" Journal of the
    Royal Statistical Society. Series B (Methodological)(1958): 215-242.'
  prefs: []
  type: TYPE_NORMAL
