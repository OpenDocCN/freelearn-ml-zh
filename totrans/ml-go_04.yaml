- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first group of machine learning techniques that we will explore is generally
    referred to as **regression**. Regression is a process through which we can understand
    how one variable (for example, sales) changes with respect to another variable
    (for example, number of users). These techniques are useful on their own. However,
    they are also a good starting point to discuss machine learning techniques because
    they form the basis of other, more complicated, techniques that we will discuss
    later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, regression techniques in machine learning are concerned with predicting
    continuous values (for example, stock price, temperature, or disease progression).
    **Classification**, which we will cover in the next chapter, is concerned with
    predicting discrete variables, or one of a discrete set of categories (for example,
    fraud/not fraud, sitting/standing/running, or hot dog/not hot dog). As mentioned,
    regression techniques are used throughout machine learning as part of classification
    algorithms, but in this chapter we will focus on their basic application to predict
    continuous values.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding regression model jargon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As already mentioned, regression itself is a process to analyze a relationship
    between one variable and another variable, but there are some terms used in machine
    learning to describe these variables along with various types of regression and
    processes associated with regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Response** or **dependent variable**: These terms will be used interchangeably
    for the variable that we are trying to predict based on one or more other variables.
    This variable is often labeled *y*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explanatory variables**, **independent variables**, **features**, **attributes**,
    or **regressors**: These terms will be used interchangeably for the variables
    that we are using to predict the response. These variables are often labeled *x*
    or *x[1], x[2],* and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear regression**: This type of regression assumes that the dependent variable
    depends on the independent variable linearly (that is, following the equation
    for a line).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nonlinear regression**: This type of regression assumes that the dependent
    variable depends on the independent variable in a relationship that is not linear
    (for example, polynomial or exponential).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple regression**: A regression with more than one independent variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fitting** or **training**: The process of parameterizing a model, such as
    a regression model, so that it can predict a certain dependent variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction**: The process of using a parameterized model, such as a regression
    model, to predict a certain dependent variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these terms will be used both in the context of regression and in other
    contexts throughout the rest of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression is one of the most simple machine learning models. However,
    you should not dismiss this model by any means. As mentioned previously, it is
    an essential building block that is utilized in other models, and it has some
    very important advantages.
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed throughout this book, integrity in machine learning applications
    is crucial, and the simpler and more interpretable a model is, the easier it is
    to maintain integrity. In addition, because the model is simple and interpretable,
    it allows you to understand inferred relationships between variables and check
    your work mentally as you develop. In the words of Mike Lee Williams from Fast
    Forward Labs (in [http://blog.fastforwardlabs.com/2017/08/02/interpretability.html](http://blog.fastforwardlabs.com/2017/08/02/interpretability.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: The future is algorithmic. Interpretable models offer a safer, more productive,
    and ultimately more collaborative relationship between humans and intelligent
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression models are interpretable, and thus, they can provide a safe
    and productive option for data scientists. When you are searching for a model
    to predict a continuous variable, you should consider and try linear regression
    (or even multiple linear regression) if your data and problem allows you to use
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In linear regression, we attempt to model our dependent variable, *y*, by an
    independent variable, *x*, using the equation for a line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3e2dab9-8cd9-4682-add5-28573549128b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *m* is the slope of the line and *b* is the intercept. For example, let''s
    say that we want to model daily *sales* by the *number of users* on our website
    each day. To do this with linear regression, we would want to determine an *m*
    and *b* that would allow us to predict sales via the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6852719e-3947-4a09-80c9-48471fd9739c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our trained model is really just this parameterized function. We put
    in a **Number of Users** and we get the predicted **Sales**, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f368c62d-8816-4c7e-ae7a-4b3045c1bb70.png)'
  prefs: []
  type: TYPE_IMG
- en: The training or fitting of a linear regression model involves determining the
    values of *m* and *b,* such that the resulting formula has predictive power for
    our response. There are a variety of methods to determine *m* and *b*, but the
    most common and simple method is called **ordinary least squares** (**OLS**).
  prefs: []
  type: TYPE_NORMAL
- en: 'To find *m* and *b* with OLS, we first pick a value for *m* and *b* to create
    a first example line. We then measure the vertical distance between each of our
    known points (for example, from our training set) and the example line. These
    distances are called **errors** or **residuals**, similar to the errors that we
    discussed in [Chapter 3](64db3465-cd9a-42ca-b0fb-d54c59b87037.xhtml), *Evaluation
    and Validation,* and are illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1828093f-2bf5-4848-a6c0-056b7bb2de0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we add up the sum of the squares of these errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f616d895-8121-488b-baef-ca27d1da6751.png)'
  prefs: []
  type: TYPE_IMG
- en: We adjust *m* and *b* until we minimize this sum of the squares of the errors.
    In other words, our trained linear regression line is the line that minimizes
    this sum of the squares.
  prefs: []
  type: TYPE_NORMAL
- en: There are a variety of methods to find the line that minimizes the sum of the
    squared errors and, for OLS, the line can be found analytically. However, a very
    popular and general optimization technique that is used to minimize the sum of
    the squared error is called **gradient descent**. This method can be more efficient
    in terms of implementation, advantageous computationally (in terms of memory,
    for example), and more flexible than analytic solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent is discussed in more detail in the [Appendix](718ca26d-465a-47c9-91b9-14e749be0c30.xhtml),
    *Algorithms/Techniques Related to Machine Learning*, so we will avoid a lengthy
    discussion here. Suffice it to say that many implementations of linear and other
    regressions utilize gradient descent for the fitting or training of the linear
    regression line. In fact, gradient descent is ubiquitous in machine learning and
    also powers much more complicated modeling techniques such as deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression assumptions and pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like all machine learning models, linear regression does not work in all situations
    and it does make certain assumptions about your data and the relationships in
    your data. The assumptions of linear regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear relationship**: This might seem obvious, but linear regression assumes
    that your dependent variable depends on your independent variable linearly (by
    means of the equation for a line). If this relationship is not linear, linear
    regression will likely perform poorly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normality**: This assumption means that your variables should be distributed
    according to a normal distribution (which looks like a bell shape). We will come
    back to this property later in the chapter and discuss some trade-offs and options
    when encountering non-normally distributed variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No multicollinearity**: Multicollinearity is a fancy term that means that
    independent variables are not really independent. They depend on each other in
    some fashion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No auto-correlation**: Auto-correlation is another fancy term that means
    that a variable depends on itself or some shifted version of itself (like in some
    predictable time series).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Homoscedasticity**: This may be the fanciest word of this bunch of terms,
    but it means something relatively simple and is not really something you have
    to worry about very often. Linear regression assumes that the variance of your
    data is about the same around the regression line for all values of your independent
    variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technically, all of these assumptions need to be fulfilled for us to use linear
    regression. It's very important that we know how our data is distributed and how
    it behaves. We will look into these assumptions when we profile data in an example
    use of linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a data scientist or analyst, you want to keep the following pitfalls in
    mind as you apply linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: You are training your linear regression model for a certain range of your independent
    variable. You should be careful making predictions for values outside of this
    range because your regression line might not be applicable (for example, your
    dependent variable may start behaving non-linearly at extreme values).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can misspecify a linear regression model by finding some spurious relationship
    between two variables that really have nothing to do with one another. You should
    check to make sure that there is some logical reason why variables might be functionally
    related.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers or extreme values in your data may throw off a regression line for
    certain types of fitting, such as OLS. There are ways to fit a regression line
    that is more immune to outliers, or behaves differently with respect to outliers,
    such as orthogonal least squares or ridge regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To illustrate linear regression, let''s take an example problem and create
    our first machine learning model! The example data that we are going to use is
    example advertising data. It is in the `.csv` format and looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The dataset includes a set of attributes representing spend on advertising outlets
    (`TV`, `Radio`, and `Newspaper`) along with corresponding sales (`Sales`). Our
    goal in this example will be to model the sales (our dependent variable) by one
    of the attributes of advertising spend (our independent variable).
  prefs: []
  type: TYPE_NORMAL
- en: Profiling the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make sure that we create a model, or at least process, that we understand,
    and to make sure that we can mentally check our results, we need to start every
    machine learning model building process with data profiling. We need to gain an
    understanding of how each of our variables are distributed and their range and
    variability.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we will calculate the summary statistics that we discussed earlier
    in [Chapter 2](5e7af3cb-ce60-4699-b2d3-7eeff8978a9e.xhtml), *Matrices, Probability,
    and Statistics*. Here, we will utilize a method built into the `github.com/kniren/gota/dataframe`
    package to calculate our summary statistics for all of the columns of our dataset
    in one operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this gives the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this prints out all of our summary statistics in a nice tabular
    form and includes mean, standard deviation, minimum value, maximum value, *25%/75%*
    percentiles, and median (or 50% percentile).
  prefs: []
  type: TYPE_NORMAL
- en: 'These values give us a good numerical reference for the numbers that we will
    be seeing as we train our linear regression model. However, this does not give
    us a very good visual understanding of our data. For this, we will create a histogram
    for the values in each of the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This program will create a `.png` image for each histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f0052c8-b6ad-4933-8f2a-1959a33c2a0d.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, looking at these histograms and the summary statistics that we calculated,
    we need to consider if we are working within the assumptions of linear regression.
    In particular, we can see that not all of our variables are normally distributed
    (that is, they are in a bell shape). The sales might be somewhat bell-shaped,
    but the others do not look to be normal.
  prefs: []
  type: TYPE_NORMAL
- en: We could use a statistical tool, such as a **quantile-quantile** (**q-q**) plot,
    to determine how close the distributions are to normal distributions, and we could
    even perform a statistical test to determine the probability of the variables
    following a normal distribution. However, most of the time, we can get a general
    idea from the histograms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have to make a decision. At least some of our data does not technically
    fit within the assumptions of our linear regression model. We could now do one
    of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Try to transform our variables (with, for example, a power transformation) that
    follow a normal distribution, and then use these transformed variables in our
    linear regression model. The advantage of this option is that we would be operating
    within the assumptions of the model. The disadvantage is that we would be making
    our model harder to understand, and less interpretable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get different data to solve our problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ignore our issue with the linear regression assumptions and try to create the
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There may be other views on this, but my recommendation is that you try the
    third option first. There is not much harm in this option because you can train
    the linear regression model quickly. If you end up with a model that performs
    nicely, you have avoided further complications and have a nice simple model. If
    you end up with a model that performs poorly, you might need to resort to one
    of the other options.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing our independent variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, now we have some intuition about our data and have come to terms with how
    our data fits within the assumptions of the linear regression model. Now, how
    do we choose which variable to use as our independent variable in trying to predict
    our dependent variable, and average points per game?
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to make this decision is by visually exploring the correlation
    between the dependent variable and all of the choices that you have for independent
    variables. In particular, you can make scatter plots (using `gonum.org/v1/plot`)
    of your dependent variable versus each of the other variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create the following scatter plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/926090dd-21c3-40a7-a44b-404163072193.png)'
  prefs: []
  type: TYPE_IMG
- en: As we look at these scatter plots, we want to deduce which of the attributes
    (**TV**, **Radio**, and/or **Newspaper**) have a linear relationship with our
    dependent variable, **Sales**. That is, could we draw a line on any of these scatter
    plots that would fit the trend of **Sales** versus the respective attribute? This
    is not always possible, and it likely will not be possible for all of the attributes
    that you have to work with for a given problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, both **Radio** and **TV** appear to be somewhat linearly correlated
    with **Sales**. **Newspaper** may be slightly correlated with **Sales**, but the
    correlation is far from obvious. The linear relationship with **TV** seems most
    obvious, so let''s start out with **TV** as our independent variable in our linear
    regression model. This would make our linear regression formula as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aed083c3-8921-4c5b-9dbb-77ff4a4af8cf.png)'
  prefs: []
  type: TYPE_IMG
- en: One other thing to note here is that the variable **TV** might not be strictly
    homoscedastic, which was discussed earlier as an assumption of linear regression.
    This is worth noting (and likely worth documenting in the project), but we will
    continue on to see if we can create the linear regression model with some predictive
    power. We can always revisit this assumption if our model is behaving poorly,
    as a possible explanation.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our training and test sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To avoid overfitting and make sure that our model can generalize, we are going
    to split our dataset into a training set and a test set, as was discussed in [Chapter
    3](64db3465-cd9a-42ca-b0fb-d54c59b87037.xhtml), *Evaluation and Validation*. We
    will not bother with a holdout set here, because we are only going to make one
    pass through our model training without an iterative back and forth between training
    and testing. However, if you are experimenting with various dependent variables
    and/or iteratively adjusting any parameters of your model, you would want to create
    a holdout set that you save until the end of your model development process for
    validation.
  prefs: []
  type: TYPE_NORMAL
- en: We will use `github.com/kniren/gota/dataframe` to create our training and test
    datasets and then save them to respective `.csv` files. In this case, we
  prefs: []
  type: TYPE_NORMAL
- en: 'will use an 80/20 split for our training and test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will output the following training and test sets that we will use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The data that we were using here was not sorted or ordered by data in any fashion.
    However, if you are dealing with data that is sorted by response, by date, or
    in any other way, it is important that you randomly split your data into training
    and test sets. If you do not do this, your training and test sets may include
    only certain ranges of the response, may be influenced artificially by time/date,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Training our model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we are going to actually train, or fit, our linear regression model.
    If you remember, this just means that we are finding the slope (*m*) and intercept
    (*b*) for the line that minimizes the sum of the squared errors. To perform this
    training, we will use a really great package from Sajari: `github.com/sajari/regression`.
    Sajari is a web search company that relies heavily on Go and machine learning,
    and they use [github.com/sajari/regression](http://github.com/sajari/regression)
    in production.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To train a regression model using [github.com/sajari/regression](http://github.com/sajari/regression),
    we need to initialize a `regression.Regression` value, set a couple of labels,
    and fill the `regression.Regression` value with labeled training data points.
    After this, training our linear regression model is as easy as calling the `Run()`
    method on the `regression.Regression` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this will result in the trained linear regression formula
    being printed to `stdout`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the package determined our linear regression line with
    an intercept of `7.07` and a slope of `0.5`. We can perform a little mental check
    here, because we saw in the scatter plots how the correlation between **TV** and
    **Sales** was up and to the right (that is, a positive correlation). This means
    that the slope should be positive in the formula, which it is.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the trained model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now need to measure the performance of our model to see if we really have
    any power to predict **Sales** using **TV** as in independent variable. To do
    this, we can load in our test set, make predictions using our trained model for
    each test example, and then calculate one of the evaluation metrics discussed
    in [Chapter 3](64db3465-cd9a-42ca-b0fb-d54c59b87037.xhtml), *Evaluation and Validation*.
  prefs: []
  type: TYPE_NORMAL
- en: For this problem, let's use the Mean Absolute Error (MAE) as our evaluation
    metric. This seems reasonable, because it results in something directly comparable
    with our `Sales` values and we do not have to be too worried about outliers or
    extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the predicted **Sales** values using our trained `regression.Regression`
    value, we just need to parse the values in our test set and call the `Predict()`
    method on the `regression.Regression` value. We will then take the difference
    of these predicted values from the observed values, get the absolute value of
    the difference, and then add up all of the absolute values to get the MAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this evaluation gives the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How do we know if `MAE = 3.01` is good or bad? This is, again, why having a
    good mental model of your data is important. If you remember, we already computed
    the mean, range, and standard deviation of sales. The mean sales value was `14.02`
    and the standard deviation was `5.21`. Thus, our MAE is less than the standard
    deviations of our sales values and is about 20% of the mean value, and our model
    has some predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: So, congratulations! We have built our first machine learning model that has
    predictive power!
  prefs: []
  type: TYPE_NORMAL
- en: 'To get better intuition about how our model is performing, we can also create
    a plot to help us visualize the linear regression line. This can be done with
    `gonum.org/v1/plot`. First, however, let''s create a predict function that allows
    us to make our predictions without importing `github.com/sajari/regression`. This
    gives us a lightweight, in-memory version of the trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can create the visualization of our regression line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'It will produce this plot when compiled and run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6a05d14-8179-452e-bb8f-358348a02708.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, our trained linear regression line follows the linear trend
    of the real data points. This is another visual confirmation that we are on the
    right track!
  prefs: []
  type: TYPE_NORMAL
- en: Multiple linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression is not limited to simple formulas of lines that depend on
    only one independent variable. Multiple linear regression is similar to what we
    discussed previously, but here we have multiple independent variables (*x[1]*,
    *x[2]*, and so on). In this case, our simple equation of a line is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f9f0a8c-4d78-47ad-9338-5b3775471f11.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the *x*'s are the various independent variables and the *m*'s are the
    various slopes associated with those independent variables. We also still have
    an intercept, *b*.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple linear regression is a little harder to visualize and think about because
    this is no longer a line that can be visualized in two dimensions. It is a linear
    surface in two, three, or more dimensions. However, many of the same techniques
    that we used for our single linear regression will carry through.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple linear regression has the same assumptions as regular linear regression.
    However, there are a few more pitfalls that we should keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfitting**: By adding more and more independent variables to our model,
    we are increasing our model complexity, which puts us at risk of overfitting.
    One technique to deal with this problem, which I would recommend looking into,
    is called **regularization**. Regularization creates a penalty term in your model
    that is a function of the complexity of your model, which helps keep this effect
    in check.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relative Scale**: In some cases, one of your independent variables will be
    orders of magnitude different in scale than another independent variable. The
    larger of these could wash out any effect of the smaller, and you may need to
    consider normalizing your variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this in mind, let''s try to expand our **Sales** model from a linear regression
    model to a multiple regression model. Looking back at our scatter plots from the
    previous section, we can see that **Radio** also appears to be linearly correlated
    with **Sales**, so let''s try to create a multiple linear regression model that
    looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f6caa9b-e5e9-4918-820b-91db3369e2f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To do this with [github.com/sajari/regression](http://github.com/sajari/regression),
    we just need to label another variable in the `regression.Regression` value and
    make sure that these values get paired in the training data points. We will then
    run the regression and see how the formula comes out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this gives us the following regression formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the regression formula now includes an additional term for the
    `Radio` independent variable. The intercept value has also changed from our previous
    single regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test this model similarly to the single regression model using the `Predict`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this reveals the following `MAE` for our new multiple regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Our new multiple regression model has improved our MAE! Now we are definitely
    in pretty good shape to predict `Sales` based on our advertising spends. You could
    also try adding `Newspaper` to the model as a follow-up exercise to see how the
    model performance is influenced.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that as you add more complication to the model, you are sacrificing
    simplicity and you are potentially in danger of overfitting, so you should only
    add more complication if the gains in model performance actually create more value
    for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear and other types of regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although we have focused on linear regression in this chapter, you certainly
    are not limited to performing regression with linear formulas. You can model your
    dependent variable by one or more nonlinear terms such as powers, exponentials,
    or other transformations on your independent variables. For example, we could
    model *Sales* by a polynomial series of *TV* terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7003a0ed-7126-4d23-b0a9-a68a28dd3a35.png)'
  prefs: []
  type: TYPE_IMG
- en: Keep in mind, however, that as you add this complexity, you are again putting
    yourself in danger of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of implementing non-linear regressions, you cannot use `github.com/sajari/regression`,
    which is limited to linear regression. However, `go-hep.org/x/hep/fit` allows
    you to fit or train certain nonlinear models, and there are other various people
    in the Go community that have, or are, developing other tools for nonlinear modeling.
  prefs: []
  type: TYPE_NORMAL
- en: There are also other linear regression techniques, outside of OLS, that help
    overcome some of the assumptions and weaknesses associated with least squared
    linear regression. These include **ridge regression** and **lasso regression**.
    Both of these techniques penalize regression coefficients so as to mitigate the
    effects of multicollinearity and non-normality of independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of Go implementations, ridge regression is implemented in `github.com/berkmancenter/ridge`.
    As opposed to `github.com/sajari/regression`, our independent variable and dependent
    variable data is input into `github.com/berkmancenter/ridge` via gonum matrices.
    Thus, to illustrate this method, let's first form a matrix containing our advertising
    spend features (`TV`, `Radio`, and `Newspaper`) and a matrix containing our `Sales`
    data. Note that in `github.com/berkmancenter/ridge`, we need to explicitly add
    a column to our input independent variable matrix for an intercept if we want
    to have an intercept in our model. Each value in this column is just `1.0`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a new `ridge.RidgeRegression` value with our independent and
    dependent variable matrices and call the `Regress()` method to train our model.
    We can then print out our trained regression formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling this program and running it gives the following regression formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here, you can that see the coefficients for `TV` and `Radio` are similar to
    what we got with least squares regression, but they are slightly different. Also,
    note that we went ahead and added a term for the `Newspaper` feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test this ridge regression formula by creating our own `predict` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use this `predict` function to test our ridge regression formula on
    our test examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this gives us the following new `MAE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Notice that adding `Newspaper` to the model did not actually improve our `MAE`.
    Thus, this would not be a good idea in this case, because it is adding further
    complications and not providing any significant changes in our model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Any complication or sophistication that you are adding to a model should be
    accompanied by a measurable justification for this added complication. Using a
    sophisticated model because it is intellectually interesting is a recipe for headaches.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ordinary least squares regression explained visually: [http://setosa.io/ev/ordinary-least-squares-regression/](http://setosa.io/ev/ordinary-least-squares-regression/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/sajari/regression` docs: [http://godoc.org/github.com/sajari/regression](http://godoc.org/github.com/sajari/regression)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple regression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple regression visualization: [http://shiny.stat.calpoly.edu/3d_regression/](http://shiny.stat.calpoly.edu/3d_regression/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nonlinear and other regressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`go-hep.org/x/hep/fit` docs: [https://godoc.org/go-hep.org/x/hep/fit](https://godoc.org/go-hep.org/x/hep/fit)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/berkmancenter/ridge` docs: [https://godoc.org/github.com/berkmancenter/ridge](https://godoc.org/github.com/berkmancenter/ridge)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You have officially done machine learning using Go. In particular,
    you have learned about regression models, including linear regression, multiple
    regression, nonlinear regression, and ridge regression. You should be able to
    implement basic linear regressions and multiple regressions in Go.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our feet wet with machine learning, we are going to move on
    to classification problems in the next chapter.
  prefs: []
  type: TYPE_NORMAL
