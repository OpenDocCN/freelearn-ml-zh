- en: '*Chapter 3*: Automated Machine Learning with Open Source Tools and Libraries'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Empowerment of individuals is a key part of what makes open source work since,
    in the end, innovations tend to come from small groups, not from large, structured
    efforts."'
  prefs: []
  type: TYPE_NORMAL
- en: – Tim O'Reilly
  prefs: []
  type: TYPE_NORMAL
- en: '"In open source, we feel strongly that to really do something well, you have
    to get a lot of people involved."'
  prefs: []
  type: TYPE_NORMAL
- en: – Linus Torvalds
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, you looked under the hood of automated **Machine Learning**
    (**ML**) technologies, techniques, and tools. You learned how AutoML actually
    works – that is, the algorithms and techniques of automated feature engineering,
    automated model and hyperparameter turning, and automated deep learning. You also
    explored Bayesian optimization, reinforcement learning, the evolutionary algorithm,
    and various gradient-based approaches by looking at their use in automated ML.
  prefs: []
  type: TYPE_NORMAL
- en: However, as a hands-on engineer, you probably don't get the satisfaction of
    understanding something fully until you get your hands dirty by trying it out.
    This chapter will give you the very opportunity to do this. AutoML **open source
    software** (**OSS**) tools and libraries automate the entire life cycle of ideating,
    conceptualizing, developing, and deploying predictive models. From data preparation
    through model training to validation as well as deployment, these tools do everything
    with almost zero human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will review the major OSS tools, including **TPOT**, **AutoKeras**,
    **auto-sklearn**, **Featuretools**, and **Microsoft NNI**, to help you understand
    the differential value propositions and approaches that are used in each of these
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The open source ecosystem for AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing TPOT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Featuretools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Microsoft NNI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing auto-sklearn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing AutoKeras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The technical requirements for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TPOT installation: [github.com/EpistasisLab/tpot](http://github.com/EpistasisLab/tpot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Featuretools installation: [https://pypi.org/project/featuretools/](https://pypi.org/project/featuretools/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microsoft NNI installation: [https://github.com/microsoft/nni](https://github.com/microsoft/nni)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'auto-sklearn installation: [https://automl.github.io/auto-sklearn/master/installation.html](https://automl.github.io/auto-sklearn/master/installation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AutoKeras installation: [https://autokeras.com/install/](https://autokeras.com/install/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MNIST download: [https://www.kaggle.com/c/digit-recognizer](https://www.kaggle.com/c/digit-recognizer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The open source ecosystem for AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By reviewing the history of automated ML, it is evident that, in the early
    days, the focus had always been on **hyperparameter** optimization. The earlier
    tools, such as **AutoWeka** and **HyperoptSkLearn**, and later **TPOT**, had an
    original focus on using Bayesian optimization techniques to find the most suitable
    **hyperparameters** for the model. However, this trend shifted left to include
    model selection, which eventually engulfed the entire pipeline by including feature
    selection, preprocessing, construction, and data cleaning. The following table
    shows some of the prominent automated ML tools that are available, including **TPOT**,
    **AutoKeras**, **auto-sklearn**, and **Featuretools**, along with their optimization
    techniques, ML tasks, and training frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Features of automated ML frameworks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Features of automated ML frameworks
  prefs: []
  type: TYPE_NORMAL
- en: 'For several of the examples in this chapter, we will be using the `datasets`
    package since it has already taken care of data loading and preprocessing **MNIST**
    60,000 training examples and 10,000 test examples. Most data scientists are ML
    enthusiasts and are very familiar with the **MNIST** database, which makes it
    a great candidate for teaching you how to use this library:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – MNIST database of handwritten digits – visualization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – MNIST database of handwritten digits – visualization
  prefs: []
  type: TYPE_NORMAL
- en: The preceding image shows what the MNIST dataset looks like. The dataset is
    available as part of all major ML and deep learning libraries, and can be downloaded
    from [https://www.kaggle.com/c/digit-recognizer](https://www.kaggle.com/c/digit-recognizer).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TPOT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Tree-based Pipeline Optimization Tool**, or **TPOT** for short, is a product
    of the University of Pennsylvania's, Computational Genetics Lab. TPOT is an automated
    ML tool written in Python. It helps build and optimize ML pipelines with genetic
    programming. Built on top of scikit-learn, TPOT helps automate the feature selection,
    preprocessing, construction, model selection, and parameter optimization processes
    by "*exploring thousands of possible pipelines to find the best one*". It is one
    of the only toolkits with a short learning curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'The toolkit is available on GitHub to be downloaded: [github.com/EpistasisLab/tpot](http://github.com/EpistasisLab/tpot).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain the framework, let''s start with a minimal working example. For
    this example, we will be using the **MNIST** database of handwritten digits:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new `pip install TPOT`. TPOT can be directly used from the command
    line or via Python code:![Figure 3.3 – Installing TPOT on a Colab notebook
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.3 – Installing TPOT on a Colab notebook
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Import `TPOTClassifier`, the scikit-learn `datasets` package, and the model
    selection libraries. We will use these libraries to load the data that we will
    be using for classification within TPOT:![Figure 3.4 – AutoML TPOT example – import
    statement
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.4 – AutoML TPOT example – import statement
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, proceed by loading the `train_test_split` method returns a list containing
    a train-test split of given inputs. In this case, the inputs are digits data and
    digits target arrays. Here, you can see that the training size is **0.75** and
    that the test size is **0.25**, which signifies a standard 75-25 split in training
    and testing data:![Figure 3.5 – AutoML TPOT example – loading the digits dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.5 – AutoML TPOT example – loading the digits dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In a typical scenario, this is where we will choose a model, assign `TPOTClassifier`.
    This class is parametrically quite extensive, as shown in the following screenshot,
    but we will only be using three key parameters; that is, `verbosity`, `max_time_mins`,
    and `population_size`:![Figure 3.6 – AutoML TPOT example – instantiating the TPOTClassifier
    object
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.6 – AutoML TPOT example – instantiating the TPOTClassifier object
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A quick note about the arguments being passed to `Classifier` – setting `Verbosity`
    to `2` will make TPOT print information alongside a progress bar. The `max_time_mins`
    parameter sets the time allocation in minutes for TPOT to optimize the pipeline,
    while the `population_size` parameter is the number of individuals in the genetic
    programming population for every generation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Upon starting the experiment, we will set the maximum time to only 1 minute:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.7 – AutoML TPOT example – optimization run of TPOTClassifier'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16890_03_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.7 – AutoML TPOT example – optimization run of TPOTClassifier
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will see that the optimization progress isn't quite as good; it's at 22%
    since only 9 out of the 40 individuals have been processed in this generation.
    In this case, the best recommended pipeline is based on `RandomForestClassifier`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let's increase this to 5 minutes and check the resulting pipeline. At this
    point, it seems like the recommended classifier is the Gradient Boosting classifier.
    This is quite interesting:![Figure 3.8 – AutoML TPOT example – executing TPOTClassifier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.8 – AutoML TPOT example – executing TPOTClassifier
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This time, we will gradually increase the time to `15` minutes, in which case
    the best pipeline will turn out to be from the **k-nearest neighbours** (**KNN**)
    classifier:![Figure 3.9 – AutoML TPOT classifier – TPOTClassifier fit to get the
    predictions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.9 – AutoML TPOT classifier – TPOTClassifier fit to get the predictions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Increasing the time to `25` minutes does not change the algorithm, but other
    **hyperparameters** (number of neighbors) and their accuracy are increased:![Figure
    3.10 – AutoML TPOT example – running multiple generations and scores
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.10 – AutoML TPOT example – running multiple generations and scores
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, let's run the experiment for an entire hour:![Figure 3.11 – AutoML
    TPOT example – TPOT generations and cross-validation scores
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.11 – AutoML TPOT example – TPOT generations and cross-validation scores
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The resulting best pipeline is `KNeighborsClassifier` using feature ranking
    with recursive feature elimination. Other hyperparameters include `max_features`
    and `n_estimators`, and the pipeline has an accuracy of `0.98666`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12 – AutoML TPOT example – the best pipeline'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16890_03_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.12 – AutoML TPOT example – the best pipeline
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This reminds me – if 666 is considered an evil number, then 25.8069758011 is
    the root of all evil.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Also, as you have probably observed, the amount of time TPOT had to run its
    **cross-validation** (**CV**) for multiple generations, the pipeline changes,
    and not only the algorithm but the **hyperparameters** have evolved. There are
    also diminishing returns. The improvements in CV scores become smaller and smaller
    to where, at a certain point, these refinements doesn't make much difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, you can export the actual model from TPOT by calling the `export` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – AutoML TPOT example – exploring the digits pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.13 – AutoML TPOT example – exploring the digits pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been exported, you will be able to see the file in the left-hand
    pane of **Google Colab**, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – AutoML TPOT example – visualizing the TPOT digits pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.14 – AutoML TPOT example – visualizing the TPOT digits pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know that this pipeline works the best, let''s try this out. Notice
    how we don''t have any need for TPOT anymore since we already have the tea (or
    pipeline, in this case):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – AutoML TPOT example – exporting the pipeline with ExtraTreesClassifier'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.15 – AutoML TPOT example – exporting the pipeline with ExtraTreesClassifier
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve created the exported pipeline, let''s load up the dataset.
    Instead of reading it from the CSV file, I can just use the `sklearn` datasets
    to expedite things. Also, I chose digit `target` `[10]`) in the array, and voila
    – the prediction was right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – AutoML TPOT example – results from the exported pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.16 – AutoML TPOT example – results from the exported pipeline
  prefs: []
  type: TYPE_NORMAL
- en: How does TPOT do this?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This looks great, but you didn''t buy this book just to learn how to use an
    API – you want to understand a bit more about what is going on under the hood.
    Well, here is the scoop: TPOT has automated the key components of the pipeline
    using genetic programming; it tried different approaches, as you saw, and then
    eventually settled on using KNN as the best classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Overview of the TPOT pipeline search'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.17 – Overview of the TPOT pipeline search
  prefs: []
  type: TYPE_NORMAL
- en: 'Behind the scenes, TPOT uses genetic programming constructs (selection, crossover,
    and mutation) to optimize transformation, which helps maximize classification
    accuracy. The following is a list of operators provided by TPOT:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – TPOT – a tree-based pipeline optimization tool for automating
    ML'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.18 – TPOT – a tree-based pipeline optimization tool for automating
    ML
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Featuretools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Featuretools is an excellent Python framework that helps with automated feature
    engineering by using DFS. Feature engineering is a tough problem due to its very
    nuanced nature. However, this open source toolkit, with its robust timestamp handling
    and reusable feature primitives, provides a proper framework for us to build and
    extract combinations of features and their impact.
  prefs: []
  type: TYPE_NORMAL
- en: 'The toolkit is available on GitHub to be downloaded: [https://github.com/FeatureLabs/featuretools/](https://github.com/FeatureLabs/featuretools/).
    The following steps will guide you through how to install Featuretools, as well
    as how to run an automated ML experiment using the library. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: To start Featuretools in Colab, you will need to use pip to install the package.
    In this example, we will try to create features for the Boston Housing Prices
    dataset:![Figure 3.19 – AutoML with Featuretools – installing Featuretools
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.19 – AutoML with Featuretools – installing Featuretools
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this experiment, we will be using the Boston Housing Prices dataset, which
    is a well-known and widely used dataset in ML. The following is a brief description
    and the metadata of the dataset:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.20 – AutoML with Featuretools – Boston Housing Prices dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16890_03_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.20 – AutoML with Featuretools – Boston Housing Prices dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Boston Housing Prices dataset is part of the `scikit-learn` dataset, which
    makes it very easy to import, as shown here:![Figure 3.21 – AutoML with Featuretools
    – installing Featuretools
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.21 – AutoML with Featuretools – installing Featuretools
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we will use `boston`) using the `featuretools` **Deep Feature Synthesis**
    (**DFS**) API:![Figure 3.22 – AutoML with Featuretools – loading the dataset as
    a pandas DataFrame
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.22 – AutoML with Featuretools – loading the dataset as a pandas DataFrame
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's create a feature tools entity set for the Boston table, and then define
    the target entries. In this case, we will just create some new features; that
    is, the products and the sum of existing features. Once **Featuretools** has run
    the DFS, you will have all the summation and product features:![Figure 3.23 –
    AutoML with Featuretools – results of DFS
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.23 – AutoML with Featuretools – results of DFS
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of features continues:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – AutoML with Featuretools – results of DFS – continued'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.24 – AutoML with Featuretools – results of DFS – continued
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you might be wondering, what is the point of doing DFS if it
    just contains the sums and products of existing features? I''m glad you asked.
    Think of these derived features as highlighting the latent relationships between
    multiple data points – and it''s not related to sum and product. For example,
    you can link multiple tables with average order summation, and the algorithms
    will have additional pre-defined features to work with to find correlations. This
    is a very strong and significant quantitative value proposition that''s provided
    by DFS, and it is typically used in machine learning algorithmic competitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25 – DFS – analyzing features from Entity'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.25 – DFS – analyzing features from Entity
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Featuretools** website contains an excellent set of demos for predicting
    next purchases, remaining useful life, appointment no-shows, loan repayment likelihood,
    customer churn, household poverty, and malicious internet traffic, among many
    other use cases: [https://www.featuretools.com/demos/](https://www.featuretools.com/demos/).'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Microsoft NNI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Microsoft Neural Network Intelligence** (**NNI**) is an open source platform
    that addresses the three key areas of any automated ML life cycle – automated
    feature engineering, architectural search (also referred to as **neural architectural
    search** or **NAS**), and **hyperparameter tuning** (**HPI**). The toolkit also
    offers model compression features and operationalization. NNI comes with many
    hyperparameter tuning algorithms already built in.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A high-level architecture diagram of NNI is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16890_03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.26 – Microsoft NNI high-level architecture
  prefs: []
  type: TYPE_NORMAL
- en: NNI has several state-of-the-art **hyperparameter** optimization algorithms
    built in, and they are called **tuners**. The list includes **TPE**, **Random
    Search**, **Anneal**, **Naive Evolution**, **SMAC**, **Metis Tuner**, **Batch
    Tuner**, **Grid Search**, **GP Tuner**, **Network Morphism**, **Hyperband**, **BOHB**,
    **PPO Tuner**, and **PBT Tuner**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The toolkit is available on GitHub to be downloaded: [https://github.com/microsoft/nni](https://github.com/microsoft/nni).
    More information about its built-in tuners can be found here: [https://nni.readthedocs.io/en/latest/builtin_tuner.html](https://nni.readthedocs.io/en/latest/builtin_tuner.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's learn how to install Microsoft NNI and how to run an automated ML
    experiment using this library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and install the NNI on our machine using `pip` :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27 – AutoML with Microsoft NNI – installation via Anaconda'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.27 – AutoML with Microsoft NNI – installation via Anaconda
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the best features offered by NNI is that it has both a **command-line
    interface** (**CLI**) and a **web UI** so that we can view the trials and experiments.
    NNICtl is the command line that''s used to manage the NNI application. You can
    see the options for experiments in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28 – AutoML with Microsoft NNI – the nnictl command'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.28 – AutoML with Microsoft NNI – the nnictl command
  prefs: []
  type: TYPE_NORMAL
- en: 'NNI can have a learning curve if you do not understand how it works. You need
    to become familiar with the three primary NNI elements for it to work. First,
    you must define the search space, which you can find in the `search_space.json`
    file. You also need to update the model code (`main.py`) so that it incorporates
    `config.yml`) so that you can define the tuner and trial (execution model code)
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.29 – AutoML with Microsoft NNI – the configuration and execution
    files'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.29 – AutoML with Microsoft NNI – the configuration and execution files
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, the search space describes the value range of each hyperparameter
    and for each trial, various hyperparameter values from this space are picked.
    While creating a configuration for a hyperparamter tuning experiment, we can limit
    the maximum number of trials. Also, while creating a hyperparameter search space,
    we can list the values that we want to try out in the tuning experiment when using
    the **choice** type hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we have taken a simple `nnictl` `create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.30 – AutoML with Microsoft NNI – running the experiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.30 – AutoML with Microsoft NNI – running the experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following commands to find out more about the experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.31 – AutoML with Microsoft NNI – the nnictrl parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_31.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.31 – AutoML with Microsoft NNI – the nnictrl parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at NNI''s secret weapon – its UI. The NNI UI can be accessed
    via the web UI URL shown in the output console shown in *Figure 3.29*. Here, you
    can see the experiment running, its parameters, and its details. For instance,
    in this case, we only ran 19 trials, so it ran through these quickly. However,
    there were no meaningful results, such as us finding out what the best metric
    is (`N/A`), as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.32 – AutoML with the Microsoft NNI UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_32.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.32 – AutoML with the Microsoft NNI UI
  prefs: []
  type: TYPE_NORMAL
- en: 'Increasing the number of trials to 30 takes longer, but it also gives you better
    accuracy in the results. Microsoft NNI helps you report intermediate results (results
    during a trial or training process before the training is complete). For instance,
    if the value of the metric being reported is stored in a variable, "x", you can
    do intermediate reporting using NNI like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following will be displayed on your screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.33 – AutoML with Microsoft NNI – the UI after completing an experiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.33 – AutoML with Microsoft NNI – the UI after completing an experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'The NNI UI also provides you with views of the **default metrics**, **hyperparameters**,
    **duration**, and **intermediate results** of each trial. The **hyperparameter**
    view is especially amazing because you can visualize how each **hyperparameter**
    was selected. For example, in this case, it seems like RELU with a batch size
    of 1,024 provided significantly good results. This gives you an idea of what underlying
    algorithm can be used for model selection, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.34 – AutoML with Microsoft NNI – the hyperparameters in the experiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_34.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.34 – AutoML with Microsoft NNI – the hyperparameters in the experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'As we learned earlier regarding diminishing returns, increasing the number
    of trials doesn''t increase the accuracy of the model significantly. In this case,
    the experiment spent 40 minutes completing 100 trials and provided a best metric
    of **0.981** compared to **0.980** from earlier, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.35 – AutoML with Microsoft NNI – the configuration parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_35.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.35 – AutoML with Microsoft NNI – the configuration parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also select a different top percentage of results for the **hyperparameters**
    to see what **hyperparameters** we used to get the best performing results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.36 – AutoML with Microsoft NNI – the hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_36.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.36 – AutoML with Microsoft NNI – the hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can just look at the top 5% of the results by selecting
    `Top 5%` from the dropdown on the top right of the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.37 – AutoML with Microsoft NNI – the hyperparameters for the top
    5%'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_37.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.37 – AutoML with Microsoft NNI – the hyperparameters for the top 5%
  prefs: []
  type: TYPE_NORMAL
- en: 'NNI also allows you to drill down into each trial visually. You can see all
    the trial jobs in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure3.38 – AutoML with Microsoft NNI – the experiments list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_38.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure3.38 – AutoML with Microsoft NNI – the experiments list
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can drill down into individual jobs and view various hyperparameters,
    including `dropout_rate`, `num_units`, learning rate, `batch_size`, and `activation`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.39 – AutoML with Microsoft NNI – the path for the top 20% of hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_39.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.39 – AutoML with Microsoft NNI – the path for the top 20% of hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: Being able to see this level of detail about experiments and hyperparameters
    is phenomenal, and makes NNI one of our top open source tools for automated ML.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on, it is important to note that, like AutoGluon is part of AWS's
    automated ML offering, NNI is part of Microsoft Azure's automated ML toolset,
    which makes it much more powerful and verstaile when it comes to reusing it.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing auto-sklearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**scikit-learn** (also known as **sklearn**) is a very popular ML library for
    Python development – so popular that it has its own memes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.40 – An ML meme'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_40.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.40 – An ML meme
  prefs: []
  type: TYPE_NORMAL
- en: As part of this ecosystem and based on *Efficient and Robust Automated Machine
    Learning* by *Feurer et al.*, **auto-sklearn** is an automated ML toolkit that
    performs algorithm selection and **hyperparameter tuning** using **Bayesian optimization**, **meta-learning**,
    and **ensemble construction**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The toolkit is available on GitHub to be downloaded: [github.com/automl/auto-sklearn](http://github.com/automl/auto-sklearn).'
  prefs: []
  type: TYPE_NORMAL
- en: '`auto-sklearn` touts its ease of use for performing automated ML since it''s
    a four-line automated ML solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.41 – AutoML with auto-sklearn – getting started'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_41.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.41 – AutoML with auto-sklearn – getting started
  prefs: []
  type: TYPE_NORMAL
- en: If the preceding syntax looks familiar, then it's because this is how `scikit-learn`
    does predictions and therefore makes `auto-sklearn` one of the easiest libraries
    to use. `auto-sklearn` uses `scikit-learn` as its backend framework and supports
    **Bayesian optimization** with automated **ensembled construction**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on `auto-sklearn` addresses the problem of finding the best model and
    its hyperparameters at the same time. The following diagram shows how `auto-sklearn`
    describes its internal pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.42 – An auto-sklearn automated ML pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_42.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.42 – An auto-sklearn automated ML pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'The underlying automated ML "engine" uses **Information Retrieval** (**IR**)
    and statistical meta feature approaches to select a variety of configurations,
    all of which are used as part of Bayesian optimization input. This process is
    iterative, and auto-sklearn keeps the models to create an ensemble, thus iteratively
    building a model to maximize performance. Setting up auto-sklearn on Colab can
    be tricky as you will need to install the following packages to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.43 – AutoML with auto-sklearn – installing the necessary libraries'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_43.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.43 – AutoML with auto-sklearn – installing the necessary libraries
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have to restart the runtime in Colab upon installation. You can also
    set up auto-sklearn on your local machine by following the instructions specified
    here: [https://automl.github.io/auto-sklearn/master/installation.html](https://automl.github.io/auto-sklearn/master/installation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have completed the installation, you can run the **auto-sklearn classifier**
    and get great accuracy and **hyperparameters** via the magic of automated ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.44 – AutoML with AutoSkLearn – running a simple experiment for the
    auto-sklearn classifier'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_44.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.44 – AutoML with AutoSkLearn – running a simple experiment for the
    auto-sklearn classifier
  prefs: []
  type: TYPE_NORMAL
- en: 'We should point out that **auto-sklearn 2**, an experimental version of auto-sklearn,
    is also out and includes the latest work that''s been done on automatic configuration
    and performance improvements. You can import **auto-sklearn 2** like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Examples of basic classification, regression, and multi-label classification
    datasets, as well as advanced examples of customizing auto-sklearn, are available
    here: [https://automl.github.io/auto-sklearn/master/examples/](https://automl.github.io/auto-sklearn/master/examples/).'
  prefs: []
  type: TYPE_NORMAL
- en: If you wish, you can try out the advanced use cases of changing the optimization
    metrics, the train-validation split, providing different feature types, using
    pandas DataFrames, and inspecting search procedures. These advanced examples also
    demonstrate how auto-sklearn can be used to **extend regression**, **classification**,
    and **preprocessor components**, as well as how a number of **hyperparameters**
    can be restricted.
  prefs: []
  type: TYPE_NORMAL
- en: AutoKeras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Keras** is one of the most widely used deep learning frameworks and is an
    integral part of the TensorFlow 2.0 ecosystem. **Auto-Keras** is based on the
    paper by Jin et al., ([https://arxiv.org/abs/1806.10282](https://arxiv.org/abs/1806.10282))
    which proposed "*a novel method for efficient neural architecture search with
    network morphism, enabling Bayesian optimization*". **AutoKeras** is built on
    the concept that since existing neural architecture search algorithms such as
    **NASNet** and **PNAS** are computationally quite expensive, using **Bayesian
    optimization** to guide the network''s morphism is an efficient approach to explore
    the search space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The toolkit is available on GitHub to be downloaded: [github.com/jhfjhfj1/autokeras](http://github.com/jhfjhfj1/autokeras).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will guide you through how to install AutoKeras and how
    to run an automated ML experiment using the library. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: To get started with Auto-Keras, run the following `install` commands in Colab
    or in a Jupyter Notebook. Doing this will install `git uri`:![Figure 3.45 – AutoML
    with AutoKeras – installation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_45.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.45 – AutoML with AutoKeras – installation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once you have met the dependencies, you can load the **MNIST dataset**:![Figure
    3.46 – AutoML with AutoKeras – loading the training data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_46.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.46 – AutoML with AutoKeras – loading the training data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, you can get **AutoKeras** and go through the code for a classifier – in
    this case, an image classifier. **AutoKeras** shows the accuracy of data as it
    calculates the classification metrics:![Figure 3.47 – AutoML with AutoKeras –
    running the epochs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_47.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.47 – AutoML with AutoKeras – running the epochs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fast forwarding through the fit procedure, now that you have discovered the
    `model.save` method and use it for `eval` at a later date. You can see the model
    stored in the `model_autokeras` folder on the left-hand pane of your Colab notebook,
    as shown in the following screenshot:![Figure 3.51 – AutoML with AutoKeras – exporting
    as a Keras model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16890_03_51.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.51 – AutoML with AutoKeras – exporting as a Keras model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the model has been saved, it can be used to retrieve data using `load_model`
    and make predictions against it, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.52 – AutoML with AutoKeras – predicting the values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16890_03_52.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.52 – AutoML with AutoKeras – predicting the values
  prefs: []
  type: TYPE_NORMAL
- en: '**AutoKeras** uses **Efficient Neural Architecture Search** (**ENAS**), an
    approach similar to transfer learning. Like ensembles, the **hyperparameters**
    that are learned during the search are reused for other models, which helps us
    avoid having to retrain and provide improved performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we conclude our overview of open source libraries, some honorable mentions
    go to two excellent and easy-to-use AutoML frameworks: **Ludwig** and **AutoGluon**.'
  prefs: []
  type: TYPE_NORMAL
- en: Ludwig – a code-free AutoML toolbox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Uber's automated ML tool, Ludwig, is an open source deep learning toolbox used
    for experimenting with, testing, and training ML models. Built on top of TensorFlow,
    Ludwig enables users to create model baselines and perform automated ML-style
    experimentation with different network architectures and models. In its latest
    release, Ludwig now integrates with CometML and supports BERT text encoders.
  prefs: []
  type: TYPE_NORMAL
- en: 'The toolkit is available on GitHub to be downloaded: [https://github.com/uber/ludwig](https://github.com/uber/ludwig).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are tons of great examples with regard to this topic over here: [https://ludwig-ai.github.io/ludwig-docs/examples/#image-classification-mnist](https://ludwig-ai.github.io/ludwig-docs/examples/#image-classification-mnist).'
  prefs: []
  type: TYPE_NORMAL
- en: AutoGluon – the AutoML toolkit for deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From AWS Labs, with the goal of democratization of ML in mind, AutoGluon is
    described as being developed to enable "*easy-to-use and easy-to-extend AutoML
    with a focus on deep learning and real-world applications spanning image, text,
    or tabular data*". AutoGluon, an integral part of AWS's automated ML strategy,
    enables both junior and seasoned data scientists to build deep learning models
    and end-to-end solutions with ease. Like other automated ML toolkits, AutoGluon
    offers network architecture search, model selection, and the ability for you to
    improve custom models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The toolkit is available on GitHub to be downloaded: [https://github.com/awslabs/autogluon](https://github.com/awslabs/autogluon).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you reviewed some major open source tools that are used for
    AutoML, including **TPOT**, **AutoKeras**, **auto-sklearn**, **Featuretools**,
    and **Microsoft NNI**. These tools have been provided to help you understand the
    concepts we discussed in [*Chapter 2*](B16890_02_Final_VK_ePub.xhtml#_idTextAnchor049),
    *Automated Machine Learning, Algorithms, and Techniques*, and the underlying approaches
    that are used in each of these libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will do an in-depth review of commercial automated ML
    offerings, starting with the Microsoft Azure platform.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics that were covered in this chapter, please
    refer to the resources and links:'
  prefs: []
  type: TYPE_NORMAL
- en: TPOT for Automated ML in Python:[https://machinelearningmastery.com/tpot-for-automated-machine-learning-in-python/](https://machinelearningmastery.com/tpot-for-automated-machine-learning-in-python/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Featuretools Demos:[https://www.featuretools.com/demos/](https://www.featuretools.com/demos/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boston Dataset:[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to Automate ML: [https://www.knime.com/blog/how-to-automate-machine-learning](https://www.knime.com/blog/how-to-automate-machine-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data-driven advice for applying ML to bioinformatics problems,* by Randal
    S. Olson:[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TPOT Automated ML in Python: [https://towardsdatascience.com/tpot-automated-machine-learning-in-python-4c063b3e5de9](https://towardsdatascience.com/tpot-automated-machine-learning-in-python-4c063b3e5de9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microsoft NNI:     [https://github.com/microsoft/nni](https://github.com/microsoft/nni)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: auto-sklearn:https://automl.github.io/auto-sklearn/master/examples/20_basic/example_regression.html#sphx-glr-examples-20-basic-example-regression-py
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TPOT Demos:https://github.com/EpistasisLab/tpot/blob/master/tutorials/Digits.ipynb
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
