<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Tracking Objects</h1></div></div></div><p>In this chapter, we will explore the vast topic of object tracking, which is the process of locating a moving object in a movie or video feed from a camera. Real-time object tracking is a critical task in many computer vision applications such as surveillance, perceptual user interfaces, augmented reality, object-based video compression, and driver assistance.</p><p>Tracking objects can be accomplished in several ways, with the optimal technique being largely dependent on the task at hand. We will learn how to identify moving objects and track them across frames.</p><div><div><div><div><h1 class="title"><a id="ch08lvl1sec46"/>Detecting moving objects</h1></div></div></div><p>The first <a id="id446" class="indexterm"/>task that needs to be accomplished for us to be able to track anything in a video is to identify those regions of a video frame that correspond to moving objects.</p><p>There are many ways to track objects in a video, all of them fulfilling a slightly different purpose. For example, you may want to track anything that moves, in which case differences between frames are going to be of help; you may want to track a hand moving in a video, in which case Meanshift based on the color of the skin is the most appropriate solution; you may want to track a particular object of which you know the aspect, in which case techniques such as template matching will be of help.</p><p>Object tracking techniques can get quite complex, let's explore them in the ascending order of difficulty, starting from the simplest technique.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec45"/>Basic motion detection</h2></div></div></div><p>The first <a id="id447" class="indexterm"/>and most intuitive solution is to calculate the differences between frames, or between a frame considered "background" and all the other frames.</p><p>Let's look at an example of this approach:</p><div><pre class="programlisting">import cv2
import numpy as np

camera = cv2.VideoCapture(0)

es = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,4))
kernel = np.ones((5,5),np.uint8)
background = None

while (True):
  ret, frame = camera.read()
  if background is None:
    background = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    background = cv2.GaussianBlur(background, (21, 21), 0)
    continue
  
  gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
  gray_frame = cv2.GaussianBlur(gray_frame, (21, 21), 0)
  
  diff = cv2.absdiff(background, gray_frame)
  diff = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)[1]
  diff = cv2.dilate(diff, es, iterations = 2)
  image, cnts, hierarchy = cv2.findContours(diff.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
  
  for c in cnts:
    if cv2.contourArea(c) &lt; 1500:
      continue
    (x, y, w, h) = cv2.boundingRect(c)
    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
  
  cv2.imshow("contours", frame)
  cv2.imshow("dif", diff)
  if cv2.waitKey(1000 / 12) &amp; 0xff == ord("q"):
      break

cv2.destroyAllWindows()
camera.release()</pre></div><p>After the necessary imports, we open the video feed obtained from the default system camera, and we set <a id="id448" class="indexterm"/>the first frame as the background of the entire feed. Each frame read from that point onward is processed to calculate the difference between the background and the frame itself. This is a trivial operation:</p><div><pre class="programlisting">diff = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)[1]</pre></div><p>Before we get to do that, though, we need to prepare our frame for processing. The first thing we do is convert the frame to grayscale and blur it a bit:</p><div><pre class="programlisting">gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
gray_frame = cv2.GaussianBlur(gray_frame, (21, 21), 0)</pre></div><div><h3 class="title"><a id="note31"/>Note</h3><p>You may wonder about the blurring: the reason why we blur the image is that, in each video feed, there's a natural noise coming from natural vibrations, changes in lighting, and the noise generated by the camera itself. We want to smooth this noise out so that it doesn't get detected as motion and consequently get tracked.</p></div><p>Now that our frame is grayscaled and smoothed, we can calculate the difference compared to the background (which has also been grayscaled and smoothed), and obtain a map of differences. This is <a id="id449" class="indexterm"/>not the only processing step, though. We're also going to apply a threshold, so as to obtain a black and white image, and dilate the image so holes and imperfections get normalized, like so:</p><div><pre class="programlisting">diff = cv2.absdiff(background, gray_frame)  
diff = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)[1]
diff = cv2.dilate(diff, es, iterations = 2)</pre></div><p>Note that eroding and dilating can also act as a noise filter, much like the blurring we applied, and that it can also be obtained in one function call using <code class="literal">cv2.morphologyEx</code>, we show both steps explicitly for transparency purposes. All that is left to do at this point is to find the contours of all the white blobs in the calculated difference map, and display them. Optionally, we only display contours for rectangles greater than an arbitrary threshold, so tiny movements are not displayed. Naturally, this is up to you and your application needs. With a constant lighting and a very noiseless camera, you may wish to have no threshold on the minimum size of the contours. This is how we display the rectangles:</p><div><pre class="programlisting">image, cnts, hierarchy = cv2.findContours(diff.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
for c in cnts:
    if cv2.contourArea(c) &lt; 1500:
      continue
    (x, y, w, h) = cv2.boundingRect(c)
    cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 255, 0), 2)
  
cv2.imshow("contours", frame)
cv2.imshow("dif", diff)</pre></div><p>OpenCV offers two very handy functions:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">cv2.findContours</code>: This function computes the contours of subjects in an image</li><li class="listitem"><code class="literal">cv2.boundinRect</code>: This function calculates their bounding box</li></ul></div><p>So there you have it, a basic motion detector with rectangles around subjects. The final result is <a id="id450" class="indexterm"/>something like this:</p><div><img src="img/image00242.jpeg" alt="Basic motion detection"/></div><p style="clear:both; height: 1em;"> </p><p>For such a simple technique, this is quite accurate. However, there are a few drawbacks that make this approach unsuitable for all business needs, most notably the fact that you need a first "default" frame to set as a background. In situations such as—for example—outdoor cameras, with lights changing quite constantly, this process results in a quite inflexible approach, so we need a bit more intelligence into our system. That's where background subtractors come into play.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec47"/>Background subtractors – KNN, MOG2, and GMG</h1></div></div></div><p>OpenCV <a id="id451" class="indexterm"/>provides a class called <code class="literal">BackgroundSubtractor</code>, which is a handy way to operate foreground and background segmentation.</p><p>This works similarly to the GrabCut algorithm we analyzed in <a class="link" title="Chapter 3. Processing Images with OpenCV 3" href="part0023.xhtml#aid-LTSU1">Chapter 3</a>, <em>Processing Images with OpenCV 3</em>, however, <code class="literal">BackgroundSubtractor</code> is a fully fledged class with a plethora of methods that not only perform background subtraction, but also improve background detection in time through machine learning and lets you save the classifier to a file.</p><p>To familiarize ourselves with <code class="literal">BackgroundSubtractor</code>, let's look at a basic example:</p><div><pre class="programlisting">import numpy as np
import cv2

cap = cv2.VideoCapture')

mog = cv2.createBackgroundSubtractorMOG2()

while(1):
    ret, frame = cap.read()
    fgmask = mog.apply(frame)
    cv2.imshow('frame',fgmask)
    if cv2.waitKey(30) &amp; 0xff:
        break

cap.release()
cv2.destroyAllWindows()</pre></div><p>Let's go through this in order. First of all, let's talk about the background subtractor object. There are three background subtractors available in OpenCV 3: <strong>K-Nearest Neighbors</strong> (<strong>KNN</strong>), <strong>Mixture of Gaussians</strong> (<strong>MOG2</strong>), and <strong>Geometric Multigrid</strong> (<strong>GMG</strong>), corresponding <a id="id452" class="indexterm"/>to the algorithm used to compute the background subtraction.</p><p>You may <a id="id453" class="indexterm"/>remember that we already elaborated on the topic of <a id="id454" class="indexterm"/>foreground and background detection in <a class="link" title="Chapter 5. Detecting and Recognizing Faces" href="part0043.xhtml#aid-190861">Chapter 5</a>, <em>Depth Estimation and Segmentation</em>, in particular when we talked about GrabCut and Watershed.</p><p>So why do we need the <code class="literal">BackgroundSubtractor</code> classes? The main reason behind this is that <code class="literal">BackgroundSubtractor</code> classes are specifically built with video analysis in mind, which means that the OpenCV <code class="literal">BackgroundSubtractor</code> classes "learn" something about the environment with every frame. For example, with GMG, you can specify the number of frames used to initialize the video analysis, with the default being 120 (roughly 5 seconds with average cameras). The constant aspect about the <code class="literal">BackgroundSubtractor</code> classes is that they operate a comparison between frames and they store a history, which allows them to improve motion analysis results as time passes.</p><p>Another fundamental (and frankly, quite amazing) feature of the <code class="literal">BackgroundSubtractor</code> classes is the ability to compute shadows. This is absolutely vital for an accurate reading of video frames; by detecting shadows, you can exclude shadow areas (by thresholding them) from the objects you detected, and concentrate on the real features. It also greatly reduces the unwanted "merging" of objects. An image comparison will give you a good idea of the concept I'm trying to illustrate. Here's a sample of <a id="id455" class="indexterm"/>background subtraction without shadow detection:</p><div><img src="img/image00243.jpeg" alt="Background subtractors – KNN, MOG2, and GMG"/></div><p style="clear:both; height: 1em;"> </p><p>Here's an example of shadow detection (with shadows thresholded):</p><div><img src="img/image00244.jpeg" alt="Background subtractors – KNN, MOG2, and GMG"/></div><p style="clear:both; height: 1em;"> </p><p>Note that shadow detection isn't absolutely perfect, but it helps bring the object contours back to the object's original shape. Let's take a look at a reimplemented example of motion detection utilizing <code class="literal">BackgroundSubtractorKNN</code>:</p><div><pre class="programlisting">import cv2
import numpy as np

bs = cv2.createBackgroundSubtractorKNN(detectShadows = True)
camera = cv2.VideoCapture("/path/to/movie.flv")

while True:
  ret, frame = camera.read()
  fgmask = bs.apply(frame)
  th = cv2.threshold(fgmask.copy(), 244, 255, cv2.THRESH_BINARY)[1]
  dilated = cv2.dilate(th, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3)), iterations = 2)
  image, contours, hier = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
  for c in contours:
    if cv2.contourArea(c) &gt; 1600:
      (x,y,w,h) = cv2.boundingRect(c)
      cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 255, 0), 2)

  cv2.imshow("mog", fgmask)
  cv2.imshow("thresh", th)
  cv2.imshow("detection", frame)
  if cv2.waitKey(30) &amp; 0xff == 27:
      break

camera.release()
cv2.destroyAllWindows()</pre></div><p>As a result of the <a id="id456" class="indexterm"/>accuracy of the subtractor, and its ability to detect shadows, we obtain a really precise motion detection, in which even objects that are next to each other don't get merged into one detection, as shown in the following screenshot:</p><div><img src="img/image00245.jpeg" alt="Background subtractors – KNN, MOG2, and GMG"/></div><p style="clear:both; height: 1em;"> </p><p>That's a remarkable result for fewer than 30 lines of code!</p><p>The core of the entire <a id="id457" class="indexterm"/>program is the <code class="literal">apply()</code> method of the background subtractor; it computes a foreground mask, which can be used as a basis for the rest of the processing:</p><div><pre class="programlisting">fgmask = bs.apply(frame)
th = cv2.threshold(fgmask.copy(), 244, 255, cv2.THRESH_BINARY)[1]
dilated = cv2.dilate(th, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3)), iterations = 2)
image, contours, hier = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
for c in contours:
    if cv2.contourArea(c) &gt; 1600:
        (x,y,w,h) = cv2.boundingRect(c)
        cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 255, 0), 2)</pre></div><p>Once a foreground mask is obtained, we can apply a threshold: the foreground mask has white values for the foreground and gray for shadows; thus, in the thresholded image, all pixels that are not almost pure white (244-255) are binarized to 0 instead of 1.</p><p>From there, we proceed with the same approach we adopted for the basic motion detection example: identifying objects, detecting contours, and drawing them on the original frame.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec46"/>Meanshift and CAMShift</h2></div></div></div><p>Background <a id="id458" class="indexterm"/>subtraction is a really effective technique, but not the only one available to track objects in a video. Meanshift is an algorithm that tracks objects by finding the maximum density of a discrete sample of a probability function (in our case, a region of interest in an image) and recalculating it at the next frame, which gives the algorithm an indication of the direction in which the object has moved.</p><p>This calculation gets repeated until the centroid matches the original one, or remains unaltered even after consecutive iterations of the calculation. This final matching is called <strong>convergence</strong>. For <a id="id459" class="indexterm"/>reference, the algorithm was first described in the paper, <em>The estimation of the gradient of a density function, with applications in pattern recognition</em>, <em>Fukunaga K. and Hoestetler L.</em>, <em>IEEE</em>, <em>1975</em>, which is available at <a class="ulink" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=1055330&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1055330">http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=1055330&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1055330</a> (note that this paper is not free for download).</p><p>Here's a visual representation of this process:</p><div><img src="img/image00246.jpeg" alt="Meanshift and CAMShift"/></div><p style="clear:both; height: 1em;"> </p><p>Aside from the theory, Meanshift is very useful when tracking a particular region of interest in a video, and this has a series of implications; for example, if you don't know a priori what the region you want to track is, you're going to have to manage this cleverly and develop programs that dynamically start tracking (and cease tracking) certain areas of the video, depending on arbitrary criteria. One example could be that you operate object detection with a trained SVM, and then start using Meanshift to track a detected object.</p><p>Let's not make our life complicated from the very beginning, though; let's first get familiar with Meanshift, and then use it in more complex scenarios.</p><p>We will start by simply marking a region of interest and keeping track of it, like so:</p><div><pre class="programlisting">import numpy as np
import cv2

cap = cv2.VideoCapture(0)
ret,frame = cap.read()
r,h,c,w = 10, 200, 10, 200
track_window = (c,r,w,h)

roi = frame[r:r+h, c:c+w]
hsv_roi =  cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
mask = cv2.inRange(hsv_roi, np.array((100., 30.,32.)), np.array((180.,120.,255.)))

roi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])
cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)

term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )

while True:
    ret ,frame = cap.read()

    if ret == True:
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)

        # apply meanshift to get the new location
        ret, track_window = cv2.meanShift(dst, track_window, term_crit)

        # Draw it on image
        x,y,w,h = track_window
        img2 = cv2.rectangle(frame, (x,y), (x+w,y+h), 255,2)
        cv2.imshow('img2',img2)

        k = cv2.waitKey(60) &amp; 0xff
        if k == 27:
            break

    else:
        break

cv2.destroyAllWindows()
cap.release()</pre></div><p>In the preceding code, I <a id="id460" class="indexterm"/>supplied the HSV values for tracking some shades of lilac, and here's the result:</p><div><img src="img/image00247.jpeg" alt="Meanshift and CAMShift"/></div><p style="clear:both; height: 1em;"> </p><p>If you ran the code on your machine, you'd notice how the Meanshift window actually looks for the specified color range; if it doesn't find it, you'll just see the window wobbling (it actually looks a bit impatient). If an object with the specified color range enters the window, the <a id="id461" class="indexterm"/>window will then start tracking it.</p><p>Let's examine the code so that we can fully understand how Meanshift performs this tracking operation.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec47"/>Color histograms</h2></div></div></div><p>Before showing <a id="id462" class="indexterm"/>the code for the preceding example, though, here is a not-so-brief digression on color histograms and the two very important built-in functions of OpenCV: <code class="literal">calcHist</code> and <code class="literal">calcBackProject</code>.</p><p>The function, <code class="literal">calcHist</code>, calculates color histograms of an image, so the next logical step is to explain the concept of color histograms. A color histogram is a representation of the color distribution of an image. On the <em>x</em> axis of the representation, we have color values, and on the <em>y</em> axis, we have the number of pixels corresponding to the color values.</p><p>Let's look at a visual representation of this concept, hoping the adage, "a picture speaks a thousand words", will apply in this instance too:</p><div><img src="img/image00248.jpeg" alt="Color histograms"/></div><p style="clear:both; height: 1em;"> </p><p>The picture shows a representation of a color histogram with one column per value from 0 to 180 (note that OpenCV uses H values 0-180. Other systems may use 0-360 or 0-255).</p><p>Aside from Meanshift, color <a id="id463" class="indexterm"/>histograms are used for a number of different and useful image and video processing operations.</p><div><div><div><div><h3 class="title"><a id="ch08lvl3sec32"/>The calcHist function</h3></div></div></div><p>The <code class="literal">calcHist()</code> function <a id="id464" class="indexterm"/>in OpenCV has the following Python signature:</p><div><pre class="programlisting">calcHist(...)
    calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]]) -&gt; hist</pre></div><p>The description of the <a id="id465" class="indexterm"/>parameters (as taken from the official OpenCV documentation) are as follows:</p><div><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Parameter</p>
</th><th valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>
<code class="literal">images</code>
</p>
</td><td valign="top">
<p>This parameter is the source arrays. They all should have the same depth, <code class="literal">CV_8U</code> or <code class="literal">CV_32F</code> , and the same size. Each of them can have an arbitrary number of channels.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">channels</code>
</p>
</td><td valign="top">
<p>This parameter is the list of the <code class="literal">dims</code> channels used to compute the histogram.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">mask</code>
</p>
</td><td valign="top">
<p>This parameter is the optional mask. If the matrix is not empty, it must be an 8-bit array of the same size as <code class="literal">images[i]</code>. The nonzero mask elements mark the array elements counted in the histogram.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">histSize</code>
</p>
</td><td valign="top">
<p>This parameter is the array of histogram sizes in each dimension.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">ranges</code>
</p>
</td><td valign="top">
<p>This parameter is the array of the <code class="literal">dims</code> arrays of the histogram <code class="literal">bin</code> boundaries in each dimension.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">hist</code>
</p>
</td><td valign="top">
<p>This parameter is the output histogram, which is a dense or sparse <code class="literal">dims</code> (dimensional) array.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">accumulate</code>
</p>
</td><td valign="top">
<p>This parameter is the accumulation flag. If it is set, the histogram is not cleared in the beginning when it is allocated. This feature enables you to compute a single histogram from several sets of arrays, or to update the histogram in time.</p>
</td></tr></tbody></table></div><p>In our example, we <a id="id466" class="indexterm"/>calculate the histograms of the region of interest like so:</p><div><pre class="programlisting">roi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])</pre></div><p>This can be interpreted as the calculation of color histograms for an array of images containing only the region of interest in the HSV space. In this region, we compute only the image values <a id="id467" class="indexterm"/>corresponding to the mask values not equal to 0, with <code class="literal">18</code> histogram columns, and with each histogram having <code class="literal">0</code> as the lower boundary and <code class="literal">180</code> as the upper boundary.</p><p>This is rather convoluted to describe but, once you have familiarized yourself with the concept of a histogram, the pieces of the puzzle should click into place.</p></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec33"/>The calcBackProject function</h3></div></div></div><p>The other function that <a id="id468" class="indexterm"/>covers a vital role in the Meanshift algorithm (but not only this) is <code class="literal">calcBackProject</code>, which is short for <strong>histogram back </strong>
<a id="id469" class="indexterm"/>
<strong>projection</strong> (calculation). A histogram back projection is so called because it takes a histogram and projects it back onto an image, with the <a id="id470" class="indexterm"/>result being the probability that each pixel will belong to the image that generated the histogram in the first place. Therefore, <code class="literal">calcBackProject</code> gives a probability estimation that a certain image is equal or similar to a model image (from which the original histogram was generated).</p><p>Again, if you thought <code class="literal">calcHist</code> was a bit convoluted, <code class="literal">calcBackProject</code> is probably even more complex!</p></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec34"/>In summary</h3></div></div></div><p>The <code class="literal">calcHist</code> <a id="id471" class="indexterm"/>function extracts a color histogram from an image, giving <a id="id472" class="indexterm"/>a statistical representation of the colors in an image, and <code class="literal">calcBackProject</code> helps in calculating the probability of each pixel of an image <a id="id473" class="indexterm"/>belonging to <a id="id474" class="indexterm"/>the original image.</p></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec48"/>Back to the code</h2></div></div></div><p>Let's get back <a id="id475" class="indexterm"/>to our example. First our usual imports, and then we mark the initial region of interest:</p><div><pre class="programlisting">cap = cv2.VideoCapture(0)
ret,frame = cap.read()
r,h,c,w = 10, 200, 10, 200
track_window = (c,r,w,h)</pre></div><p>Then, we extract and convert the ROI to HSV color space:</p><div><pre class="programlisting">roi = frame[r:r+h, c:c+w]
hsv_roi =  cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)</pre></div><p>Now, we create a mask to include all pixels of the ROI with HSV values between the lower and upper bounds:</p><div><pre class="programlisting">mask = cv2.inRange(hsv_roi, np.array((100., 30.,32.)), np.array((180.,120.,255.)))</pre></div><p>Next, we calculate the histograms of the ROI:</p><div><pre class="programlisting">roi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])
cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)</pre></div><p>After the histograms are calculated, the values are normalized to be included within the range 0-255.</p><p>Meanshift performs a number of iterations before reaching convergence; however, this convergence is not assured. So, OpenCV allows us to pass so-called termination criteria, which is a way to specify the behavior of Meanshift with regard to terminating the series of calculations:</p><div><pre class="programlisting">term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )</pre></div><p>In this particular case, we're specifying a behavior that instructs Meanshift to stop calculating the centroid shift after ten iterations or if the centroid has moved at least 1 pixel. That first flag (<code class="literal">EPS</code> or <code class="literal">CRITERIA_COUNT</code>) indicates we're going to use either of the two criteria (count or "epsilon", meaning the minimum movement).</p><p>Now that we have a histogram calculated, and termination criteria for Meanshift, we can start our usual infinite loop, grab the current frame from the camera, and start processing it. The first thing we do is switch to HSV color space:</p><div><pre class="programlisting">if ret == True:
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)</pre></div><p>Now that we have an HSV array, we can operate the long awaited histogram back projection:</p><div><pre class="programlisting">        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)</pre></div><p>The result of <code class="literal">calcBackProject</code> is a matrix. If you printed it to console, it looks more or less like this:</p><div><pre class="programlisting">[[  0   0   0 ...,   0   0   0]
 [  0   0   0 ...,   0   0   0]
 [  0   0   0 ...,   0   0   0]
 ..., 
 [  0   0  20 ...,   0   0   0]
 [ 78  20   0 ...,   0   0   0]
 [255 137  20 ...,   0   0   0]]</pre></div><p>Each pixel is represented with its probability.</p><p>This matrix can the <a id="id476" class="indexterm"/>finally be passed into Meanshift, together with the track window and the termination criteria as outlined by the Python signature of <code class="literal">cv2.meanShift</code>:</p><div><pre class="programlisting">meanShift(...)
    meanShift(probImage, window, criteria) -&gt; retval, window</pre></div><p>So here it is:</p><div><pre class="programlisting">ret, track_window = cv2.meanShift(dst, track_window, term_crit)</pre></div><p>Finally, we calculate the new coordinates of the window, draw a rectangle to display it in the frame, and then show it:</p><div><pre class="programlisting">x,y,w,h = track_window
img2 = cv2.rectangle(frame, (x,y), (x+w,y+h), 255,2)
cv2.imshow('img2',img2)</pre></div><p>That's it. You should by now have a good idea of color histograms, back projections, and Meanshift. However, there remains one issue to be resolved with the preceding program: the size of the window does not change with the size of the object in the frames being tracked.</p><p>One of the authorities in computer vision and author of the seminal book, <em>Learning OpenCV</em>, <em>Gary Bradski</em>, <em>O'Reilly</em>, published a paper in 1988 to improve the accuracy of Meanshift, and described a new algorithm called <strong>Continuously Adaptive Meanshift</strong> (<strong>CAMShift</strong>), which <a id="id477" class="indexterm"/>is very similar to Meanshift but also adapts the size of the track window when Meanshift reaches convergence.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec48"/>CAMShift</h1></div></div></div><p>While CAMShift <a id="id478" class="indexterm"/>adds complexity to Meanshift, the implementation of the the preceding program using CAMShift is surprisingly (or not?) similar to the Meanshift example, with the main difference being that, after the call to <code class="literal">CamShift</code>, the rectangle is drawn with a particular rotation that follows the rotation of the object being tracked.</p><p>Here's the code reimplemented with CAMShift:</p><div><pre class="programlisting">import numpy as np
import cv2

cap = cv2.VideoCapture(0)

# take first frame of the video
ret,frame = cap.read()

# setup initial location of window
r,h,c,w = 300,200,400,300  # simply hardcoded the values
track_window = (c,r,w,h)


roi = frame[r:r+h, c:c+w]
hsv_roi =  cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
mask = cv2.inRange(hsv_roi, np.array((100., 30.,32.)), np.array((180.,120.,255.)))
roi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])
cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)
term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )

while(1):
    ret ,frame = cap.read()

    if ret == True:
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)

        ret, track_window = cv2.CamShift(dst, track_window, term_crit)
        pts = cv2.boxPoints(ret)
        pts = np.int0(pts)
        img2 = cv2.polylines(frame,[pts],True, 255,2)

        cv2.imshow('img2',img2)
        k = cv2.waitKey(60) &amp; 0xff
        if k == 27:
            break

    else:
        break

cv2.destroyAllWindows()
cap.release()</pre></div><p>The difference <a id="id479" class="indexterm"/>between the CAMShift code and the Meanshift one lies in these four lines:</p><div><pre class="programlisting">ret, track_window = cv2.CamShift(dst, track_window, term_crit)
pts = cv2.boxPoints(ret)
pts = np.int0(pts)
img2 = cv2.polylines(frame,[pts],True, 255,2)</pre></div><p>The method signature of <code class="literal">CamShift</code> is identical to Meanshift.</p><p>The <code class="literal">boxPoints</code> function finds the vertices of a rotated rectangle, while the polylines function draws the lines of the rectangle on the frame.</p><p>By now, you should be familiar with the three approaches we adopted for tracking objects: basic motion detection, Meanshift, and CAMShift.</p><p>Let's now explore another technique: the Kalman filter.</p></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec49"/>The Kalman filter</h1></div></div></div><p>The Kalman <a id="id480" class="indexterm"/>filter is an algorithm mainly (but not only) developed by Rudolf Kalman in the late 1950s, and has found practical application in many fields, particularly navigation systems for all sorts of vehicles from nuclear submarines to aircrafts.</p><p>The Kalman filter operates recursively on streams of noisy input data (which in computer vision is normally a video feed) to produce a statistically optimal estimate of the underlying system state (the position inside the video).</p><p>Let's take a quick example to conceptualize the Kalman filter and translate the preceding (purposely broad and generic) definition into plainer English. Think of a small red ball on a table, and imagine you have a camera pointing at the scene. You identify the ball as the subject to be tracked, and flick it with your fingers. The ball will start rolling on the table, following the laws of motion we're familiar with.</p><p>If the ball is rolling at a speed of 1 meter per second (1 m/s) in a particular direction, you don't need the Kalman filter to estimate where the ball will be in 1 second's time: it will be 1 meter away. The Kalman filter applies these laws to predict an object's position in the current video frame based on observations gathered in the previous frames. Naturally, the Kalman filter cannot know about a pencil on the table deflecting the course of the ball, but it can adjust for this kind of unforeseeable event.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec49"/>Predict and update</h2></div></div></div><p>From the preceding description, we gather that the Kalman filter algorithm is divided into two phases:</p><div><ul class="itemizedlist"><li class="listitem"><strong>Predict</strong>: In the <a id="id481" class="indexterm"/>first phase, the Kalman filter uses the covariance calculated up to the current point in time to estimate the object's new position</li><li class="listitem"><strong>Update</strong>: In the <a id="id482" class="indexterm"/>second phase, it records the object's position and adjusts the covariance for the next cycle of calculations</li></ul></div><p>This adjustment is—in OpenCV terms—a correction, hence the API of the <code class="literal">KalmanFilter</code> class in the Python bindings of OpenCV is as follows:</p><div><pre class="programlisting"> class KalmanFilter(__builtin__.object)
 |  Methods defined here:
 |  
 |  __repr__(...)
 |      x.__repr__() &lt;==&gt; repr(x)
 |  
 |  correct(...)
 |      correct(measurement) -&gt; retval
 |  
 |  predict(...)
 |      predict([, control]) -&gt; retval</pre></div><p>We can deduce that, in our programs, we will call <code class="literal">predict()</code> to estimate the position of an object, and <code class="literal">correct()</code> to instruct the Kalman filter to adjust its calculations.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec50"/>An example</h2></div></div></div><p>Ultimately, we will <a id="id483" class="indexterm"/>aim to use the Kalman filter in combination with CAMShift to obtain the highest degree of accuracy and performance. However, before we go into such levels of complexity, let's analyze a simple example, specifically one that seems to be very common on the Web when it comes to the Kalman filter and OpenCV: mouse tracking.</p><p>In the following example, we will draw an empty frame and two lines: one corresponding to the actual movement of the mouse, and the other corresponding to the Kalman filter prediction. Here's the code:</p><div><pre class="programlisting">import cv2
import numpy as np

frame = np.zeros((800, 800, 3), np.uint8)
last_measurement = current_measurement = np.array((2,1), np.float32) 
last_prediction = current_prediction = np.zeros((2,1), np.float32)

def mousemove(event, x, y, s, p):
    global frame, current_measurement, measurements, last_measurement, current_prediction, last_prediction
    last_prediction = current_prediction
    last_measurement = current_measurement
    current_measurement = np.array([[np.float32(x)],[np.float32(y)]])
    kalman.correct(current_measurement)
    current_prediction = kalman.predict()
    lmx, lmy = last_measurement[0], last_measurement[1]
    cmx, cmy = current_measurement[0], current_measurement[1]
    lpx, lpy = last_prediction[0], last_prediction[1]
    cpx, cpy = current_prediction[0], current_prediction[1]
    cv2.line(frame, (lmx, lmy), (cmx, cmy), (0,100,0))
    cv2.line(frame, (lpx, lpy), (cpx, cpy), (0,0,200))


cv2.namedWindow("kalman_tracker")
cv2.setMouseCallback("kalman_tracker", mousemove)

kalman = cv2.KalmanFilter(4,2)
kalman.measurementMatrix = np.array([[1,0,0,0],[0,1,0,0]],np.float32)
kalman.transitionMatrix = np.array([[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]],np.float32)
kalman.processNoiseCov = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],np.float32) * 0.03

while True:
    cv2.imshow("kalman_tracker", frame)
    if (cv2.waitKey(30) &amp; 0xFF) == 27:
        break

cv2.destroyAllWindows()</pre></div><p>As usual, let's analyze it step by step. After the packages import, we create an empty frame, of size 800 x 800, and then initialize the arrays that will take the coordinates of the measurements and predictions of the mouse movements:</p><div><pre class="programlisting">frame = np.zeros((800, 800, 3), np.uint8)
last_measurement = current_measurement = np.array((2,1), np.float32) 
last_prediction = current_prediction = np.zeros((2,1), np.float32)</pre></div><p>Then, we declare the <a id="id484" class="indexterm"/>mouse move <code class="literal">Callback</code> function, which is going to handle the drawing of the tracking. The mechanism is quite simple; we store the last measurements and last prediction, correct the Kalman with the current measurement, calculate the Kalman prediction, and finally draw two lines, from the last measurement to the current and from the last prediction to the current:</p><div><pre class="programlisting">def mousemove(event, x, y, s, p):
    global frame, current_measurement, measurements, last_measurement, current_prediction, last_prediction
    last_prediction = current_prediction
    last_measurement = current_measurement
    current_measurement = np.array([[np.float32(x)],[np.float32(y)]])
    kalman.correct(current_measurement)
    current_prediction = kalman.predict()
    lmx, lmy = last_measurement[0], last_measurement[1]
    cmx, cmy = current_measurement[0], current_measurement[1]
    lpx, lpy = last_prediction[0], last_prediction[1]
    cpx, cpy = current_prediction[0], current_prediction[1]
    cv2.line(frame, (lmx, lmy), (cmx, cmy), (0,100,0))
    cv2.line(frame, (lpx, lpy), (cpx, cpy), (0,0,200))</pre></div><p>The next step is to initialize the window and set the <code class="literal">Callback</code> function. OpenCV handles mouse events with the <code class="literal">setMouseCallback</code> function; specific events must be handled using the first parameter of the <code class="literal">Callback</code> (event) function that determines what kind of event has been triggered (click, move, and so on):</p><div><pre class="programlisting">cv2.namedWindow("kalman_tracker")
cv2.setMouseCallback("kalman_tracker", mousemove)</pre></div><p>Now we're ready to create the Kalman filter:</p><div><pre class="programlisting">kalman = cv2.KalmanFilter(4,2)
kalman.measurementMatrix = np.array([[1,0,0,0],[0,1,0,0]],np.float32)
kalman.transitionMatrix = np.array([[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]],np.float32)
kalman.processNoiseCov = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],np.float32) * 0.03</pre></div><p>The Kalman filter <a id="id485" class="indexterm"/>class takes optional parameters in its constructor (from the OpenCV documentation):</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">dynamParams</code>: This parameter states the dimensionality of the state</li><li class="listitem"><code class="literal">MeasureParams</code>: This parameter states the dimensionality of the measurement</li><li class="listitem"><code class="literal">ControlParams</code>: This parameter states the dimensionality of the control</li><li class="listitem"><code class="literal">vector.type</code>: This parameter states the type of the created matrices that should be <code class="literal">CV_32F</code> or <code class="literal">CV_64F</code></li></ul></div><p>I found the preceding parameters (both for the constructor and the Kalman properties) to work very well.</p><p>From this point on, the program is straightforward; every mouse movement triggers a Kalman prediction, both the actual position of the mouse and the Kalman prediction are drawn in the frame, which is continuously displayed. If you move your mouse around, you'll notice that, if you make a sudden turn at high speed, the prediction line will have a wider trajectory, which is consistent with the momentum of the mouse movement at the time. Here's a sample result:</p><div><img src="img/image00249.jpeg" alt="An example"/></div><p style="clear:both; height: 1em;"> </p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec51"/>A real-life example – tracking pedestrians</h2></div></div></div><p>Up to this point, we <a id="id486" class="indexterm"/>have familiarized ourselves with the concepts <a id="id487" class="indexterm"/>of motion detection, object detection, and object tracking, so I imagine you are anxious to put this newfound knowledge to good use in a real-life scenario. Let's do just that by examining the video feed of a surveillance camera and tracking pedestrians in it.</p><p>First of all, we need a sample video; if you download the OpenCV source, you will find the perfect video file for this purpose in <code class="literal">&lt;opencv_dir&gt;/samples/data/768x576.avi</code>.</p><p>Now that we have the <a id="id488" class="indexterm"/>perfect asset to analyze, let's start building the application.</p><div><div><div><div><h3 class="title"><a id="ch08lvl3sec35"/>The application workflow</h3></div></div></div><p>The <a id="id489" class="indexterm"/>application will adhere to the following logic:</p><div><ol class="orderedlist arabic"><li class="listitem">Examine the first frame.</li><li class="listitem">Examine the following frames and perform background subtraction to identify pedestrians in the scene at the start of the scene.</li><li class="listitem">Establish an ROI per pedestrian, and use Kalman/CAMShift to track giving an ID to each pedestrian.</li><li class="listitem">Examine the next frames for new pedestrians entering the scene.</li></ol><div></div><p>If this were a real-world application, you would probably store pedestrian information to obtain information such as the average permanence of a pedestrian in the scene and most likely routes. However, this is all beyond the remit of this example application.</p><p>In a real-world application, you would make sure to identify new pedestrians entering the scene, but for now, we'll focus on tracking those objects that are in the scene at the start of the video, utilizing the CAMShift and Kalman filter algorithms.</p><p>You will find the code for this application in <code class="literal">chapter8/surveillance_demo/</code> of the code repository.</p></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec36"/>A brief digression – functional versus object-oriented programming</h3></div></div></div><p>Although <a id="id490" class="indexterm"/>most programmers are either familiar (or work on a constant basis) with <strong>Object-oriented Programming</strong> (<strong>OOP</strong>), I have found that, the more the years pass, the more I prefer <a id="id491" class="indexterm"/>
<strong>Functional Programming</strong> (<strong>FP</strong>) solutions.</p><p>For those not familiar with the terminology, FP is a programming paradigm adopted by many languages that treats programs as the evaluation of mathematical functions, allows functions to return functions, and permits functions as arguments in a function. The strength of FP does not only reside in what it can do, but also in what it can avoid, or aims at avoiding side-effects and changing states. If the topic of functional programming has sparked an interest, make sure to check out languages such as Haskell, Clojure, or ML.</p><div><h3 class="title"><a id="note32"/>Note</h3><p>What is a side-effect in programming terms? You can define a side effect as any function that changes any value that does not depend on the function's input. Python, along with many other languages, is susceptible to causing side-effects because—much like, for example, JavaScript—it allows access to global variables (and sometimes this access to global variables can be accidental!).</p></div><p>Another major issue encountered with languages that are not purely functional is the fact that a function's result will change over time, depending on the state of the variables involved. If a function takes an object as an argument—for example—and the computation relies on the internal state of that object, the function will return different results according to the changes in the object's state. This is something that very typically happens in languages, such as C and C++, in functions where one or more of the arguments are references to objects.</p><p>Why this digression? Because so far I have illustrated concepts using mostly functions; I did not shy away from accessing global variables where this was the simplest and most robust approach. However, the next program we will examine will contain OOP. So why do I choose to adopt OOP while advocating FP? Because OpenCV has quite an opinionated approach, which makes it hard to implement a program with a purely functional or object-oriented approach.</p><p>For <a id="id492" class="indexterm"/>example, any drawing function, such as <code class="literal">cv2.rectangle</code> and <code class="literal">cv2.circle</code>, modifies the argument passed into it. This approach contravenes one of the cardinal rules of functional programming, which is to avoid side-effects and changing states.</p><p>Out of curiosity, you could—in Python—redeclare the API of these drawing functions in a way that is more FP-friendly. For example, you could rewrite <code class="literal">cv2.rectangle</code> like this:</p><div><pre class="programlisting">def drawRect(frame, topLeft, bottomRight, color, thickness, fill = cv2.LINE_AA):
    newframe = frame.copy()
    cv2.rectangle(newframe, topLeft, bottomRight, color, thickness, fill)
    return newframe</pre></div><p>This approach—while computationally more expensive due to the <code class="literal">copy()</code> operation—allows the explicit reassignment of a frame, like so:</p><div><pre class="programlisting">frame = camera.read()
frame = drawRect(frame, (0,0), (10,10), (0, 255,0), 1)</pre></div><p>To conclude this digression, I will reiterate a belief very often mentioned in all programming forums and resources: there is no such thing as the best language or paradigm, only the best tool for the job in hand.</p><p>So let's get back to our program and explore the implementation of a surveillance application, tracking moving objects in a video.</p></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec52"/>The Pedestrian class</h2></div></div></div><p>The main <a id="id493" class="indexterm"/>rationale behind the creation of a <code class="literal">Pedestrian</code> class <a id="id494" class="indexterm"/>is the nature of the Kalman filter. The Kalman filter can predict the position of an object based on historical observations and correct the prediction based on the actual data, but it can only do that for one object.</p><p>As a consequence, we need one Kalman filter per object tracked.</p><p>So the <code class="literal">Pedestrian</code> class will act as a holder for a Kalman filter, a color histogram (calculated on the first detection of the object and used as a reference for the subsequent frames), and information about the region of interest, which will be used by the CAMShift algorithm (the <code class="literal">track_window</code> parameter).</p><p>Furthermore, we store the ID of each pedestrian for some fancy real-time info.</p><p>Let's take a look at the <code class="literal">Pedestrian</code> class:</p><div><pre class="programlisting">class Pedestrian():
  """Pedestrian class

  each pedestrian is composed of a ROI, an ID and a Kalman filter
  so we create a Pedestrian class to hold the object state
  """
  def __init__(self, id, frame, track_window):
    """init the pedestrian object with track window coordinates"""
    # set up the roi
    self.id = int(id)
    x,y,w,h = track_window
    self.track_window = track_window
    self.roi = cv2.cvtColor(frame[y:y+h, x:x+w], cv2.COLOR_BGR2HSV)
    roi_hist = cv2.calcHist([self.roi], [0], None, [16], [0, 180])
    self.roi_hist = cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)

    # set up the kalman
    self.kalman = cv2.KalmanFilter(4,2)
    self.kalman.measurementMatrix = np.array([[1,0,0,0],[0,1,0,0]],np.float32)
    self.kalman.transitionMatrix = np.array([[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]],np.float32)
    self.kalman.processNoiseCov = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],np.float32) * 0.03
    self.measurement = np.array((2,1), np.float32) 
    self.prediction = np.zeros((2,1), np.float32)
    self.term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )
    self.center = None
    self.update(frame)
    
  def __del__(self):
    print "Pedestrian %d destroyed" % self.id

  def update(self, frame):
    # print "updating %d " % self.id
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    back_project = cv2.calcBackProject([hsv],[0], self.roi_hist,[0,180],1)
    
    if args.get("algorithm") == "c":
      ret, self.track_window = cv2.CamShift(back_project, self.track_window, self.term_crit)
      pts = cv2.boxPoints(ret)
      pts = np.int0(pts)
      self.center = center(pts)
      cv2.polylines(frame,[pts],True, 255,1)
      
    if not args.get("algorithm") or args.get("algorithm") == "m":
      ret, self.track_window = cv2.meanShift(back_project, self.track_window, self.term_crit)
      x,y,w,h = self.track_window
      self.center = center([[x,y],[x+w, y],[x,y+h],[x+w, y+h]])  
      cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 255, 0), 1)

    self.kalman.correct(self.center)
    prediction = self.kalman.predict()
    cv2.circle(frame, (int(prediction[0]), int(prediction[1])), 4, (0, 255, 0), -1)
    # fake shadow
    cv2.putText(frame, "ID: %d -&gt; %s" % (self.id, self.center), (11, (self.id + 1) * 25 + 1),
        font, 0.6,
        (0, 0, 0),
        1,
        cv2.LINE_AA)
    # actual info
    cv2.putText(frame, "ID: %d -&gt; %s" % (self.id, self.center), (10, (self.id + 1) * 25),
        font, 0.6,
        (0, 255, 0),
        1,
        cv2.LINE_AA)</pre></div><p>At the core of the <a id="id495" class="indexterm"/>program lies the background subtractor object, which lets us identify regions of interest corresponding to moving objects.</p><p>When the program starts, we take each of these regions and instantiate a <code class="literal">Pedestrian</code> class, passing the ID (a simple counter), and the frame and track window coordinates (so we can extract the <a id="id496" class="indexterm"/>
<strong>Region of Interest</strong> (<strong>ROI</strong>), and, from this, the HSV histogram of the ROI).</p><p>The constructor function (<code class="literal">__init__</code> in Python) is more or less an aggregation of all the previous concepts: given <a id="id497" class="indexterm"/>an ROI, we calculate its histogram, set up a Kalman filter, and associate it to a property (<code class="literal">self.kalman</code>) of the object.</p><p>In the <code class="literal">update</code> method, we pass the current frame and convert it to HSV so that we can calculate the back projection of the pedestrian's HSV histogram.</p><p>We then use either CAMShift or Meanshift (depending on the argument passed; Meanshift is the default if no arguments are passed) to track the movement of the pedestrian, and correct the Kalman filter for that pedestrian with the actual position.</p><p>We also draw both CAMShift/Meanshift (with a surrounding rectangle) and Kalman (with a dot), so you can observe Kalman and CAMShift/Meanshift go nearly hand in hand, except for sudden movements that cause Kalman to have to readjust.</p><p>Lastly, we print some <a id="id498" class="indexterm"/>pedestrian information on the top-left corner of the image.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec53"/>The main program</h2></div></div></div><p>Now that we <a id="id499" class="indexterm"/>have a <code class="literal">Pedestrian</code> class holding all specific information for each object, let's take a look at the main function in the program.</p><p>First, we load a video (it could be a webcam), and then we initialize a background subtractor, setting 20 frames as the frames affecting the background model:</p><div><pre class="programlisting">history = 20
bs = cv2.createBackgroundSubtractorKNN(detectShadows = True)
bs.setHistory(history)</pre></div><p>We also create the main display window, and then set up a pedestrians dictionary and a <code class="literal">firstFrame</code> flag, which we're going to use to allow a few frames for the background subtractor to build history, so it can better identify moving objects. To help with this, we also set up a frame counter:</p><div><pre class="programlisting">cv2.namedWindow("surveillance")
  pedestrians = {}
  firstFrame = True
  frames = 0</pre></div><p>Now we start the loop. We read camera frames (or video frames) one by one:</p><div><pre class="programlisting">while True:
    print " -------------------- FRAME %d --------------------" % frames
    grabbed, frane = camera.read()
    if (grabbed is False):
      print "failed to grab frame."
      break

    ret, frame = camera.read()</pre></div><p>We let <code class="literal">BackgroundSubtractorKNN</code> build the history for the background model, so we don't actually process the first 20 frames; we only pass them into the subtractor:</p><div><pre class="programlisting">    fgmask = bs.apply(frame)
    # this is just to let the background subtractor build a bit of history
    if frames &lt; history:
      frames += 1
      continue</pre></div><p>Then we process the frame with the approach explained earlier in the chapter, by applying a process of <a id="id500" class="indexterm"/>dilation and erosion on the foreground mask so as to obtain easily identifiable blobs and their bounding boxes. These are obviously moving objects in the frame:</p><div><pre class="programlisting">    th = cv2.threshold(fgmask.copy(), 127, 255, cv2.THRESH_BINARY)[1]
    th = cv2.erode(th, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3)), iterations = 2)
    dilated = cv2.dilate(th, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (8,3)), iterations = 2)
    image, contours, hier = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)</pre></div><p>Once the contours are identified, we instantiate one pedestrian per contour for the first frame only (note that I set a minimum area for the contour to further denoise our detection):</p><div><pre class="programlisting">    counter = 0
    for c in contours:
      if cv2.contourArea(c) &gt; 500:
        (x,y,w,h) = cv2.boundingRect(c)
        cv2.rectangle(frame, (x,y), (x+w, y+h), (0, 255, 0), 1)
        # only create pedestrians in the first frame, then just follow the ones you have
        if firstFrame is True:
          pedestrians[counter] = Pedestrian(counter, frame, (x,y,w,h))
        counter += 1</pre></div><p>Then, for each pedestrian detected, we perform an <code class="literal">update</code> method passing the current frame, which is needed in its original color space, because the pedestrian objects are responsible for drawing their own information (text and Meanshift/CAMShift rectangles, and Kalman filter tracking):</p><div><pre class="programlisting">    for i, p in pedestrians.iteritems():
        p.update(frame)</pre></div><p>We set the <code class="literal">firstFrame</code> flag to <code class="literal">False</code>, so we don't instantiate any more pedestrians; we just keep track of the ones we have:</p><div><pre class="programlisting">    firstFrame = False
    frames += 1</pre></div><p>Finally, we show the result in the display window. The program can be exited by pressing the <em>Esc</em> key:</p><div><pre class="programlisting">    cv2.imshow("surveillance", frame)
    if cv2.waitKey(110) &amp; 0xff == 27:
        break

if __name__ == "__main__":
  main()</pre></div><p>There you <a id="id501" class="indexterm"/>have it: CAMShift/Meanshift working in tandem with the Kalman filter to track moving objects. All being well, you should obtain a result similar to this:</p><div><img src="img/image00250.jpeg" alt="The main program"/></div><p style="clear:both; height: 1em;"> </p><p>In this screenshot, the <a id="id502" class="indexterm"/>blue rectangle is the CAMShift detection and the green rectangle is the Kalman filter prediction with its center at the blue circle.</p><div><div><div><div><h3 class="title"><a id="ch08lvl3sec37"/>Where do we go from here?</h3></div></div></div><p>This program constitutes <a id="id503" class="indexterm"/>a basis for your application domain's needs. There are many improvements that can be made building on the program above to suit an application's additional requirements. Consider the following examples:</p><div><ul class="itemizedlist"><li class="listitem">You could destroy a pedestrian object if Kalman predicts its position to be outside the frame</li><li class="listitem">You could check whether each detected moving object is corresponding to existing pedestrian instances, and if not, create an instance for it</li><li class="listitem">You could train an SVM and operate classification on each moving object to establish whether or not the moving object is of the nature you intend to track (for instance, a dog might enter the scene but your application requires to only track humans)</li></ul></div><p>Whatever your needs, hopefully this chapter will have provided you with the necessary knowledge to build applications that satisfy your requirements.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec50"/>Summary</h1></div></div></div><p>This chapter explored the vast and complex topic of video analysis and tracking objects.</p><p>We learned about video background subtraction with a basic motion detection technique that calculates frame differences, and then moved to more complex and efficient tools such as <code class="literal">BackgroundSubtractor</code>.</p><p>We then explored two very important video analysis algorithms: Meanshift and CAMShift. In the course of this, we talked in detail about color histograms and back projections. We also familiarized ourselves with the Kalman filter, and its usefulness in a computer vision context. Finally, we put all our knowledge together in a sample surveillance application, which tracks moving objects in a video.</p><p>Now that our foundation in OpenCV and machine learning is solidifying, we are ready to tackle artificial neural networks and dive deeper into artificial intelligence with OpenCV and Python in the next chapter.</p></div></body></html>