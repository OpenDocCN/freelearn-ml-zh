<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic Algorithms - Classification, Regression, and Clustering</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we reviewed the key Java libraries that are used for machine learning and what they bring to the table. In this chapter, we will finally get our hands dirty. We will take a closer look at the basic machine learning tasks, such as classification, regression, and clustering. Each of the topics will introduce basic algorithms for classification, regression, and clustering. The example datasets will be small, simple, and easy to understand.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Loading data</li>
<li>Filtering attributes</li>
<li>Building classification, regression, and clustering models</li>
<li>Evaluating models</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Before you start</h1>
                </header>
            
            <article>
                
<p>Before you start, download the latest stable version of Weka (Weka 3.8 at the time of writing) from <span class="URLPACKT"><a href="http://www.cs.waikato.ac.nz/ml/weka/downloading.html">http://www.cs.waikato.ac.nz/ml/weka/downloading.html</a></span>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>There are multiple download options available. You'll want to use Weka as a library in your source code, so make sure that you skip the self-extracting executables and download the ZIP archive, as shown in the following screenshot. Unzip the archive and locate <kbd>weka.jar</kbd> within the extracted archive:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-535 image-border" src="Images/7db1f2c9-1619-4251-9a3b-ef6a59eb9538.png" style="width:153.58em;height:41.75em;" width="1843" height="501"/></p>
<p>We'll use the Eclipse IDE to show examples; follow these steps:</p>
<ol>
<li>Start a new Java project.</li>
<li>Right-click on the project properties, select <span class="packt_screen">Java Build Path</span>, click on the <span class="packt_screen">Libraries</span> tab, and select <span class="packt_screen">Add External JARs</span>.</li>
<li>Navigate to extract the Weka archive and select the <kbd>weka.jar</kbd> file.</li>
</ol>
<p>That's it; we are ready to implement the basic machine learning techniques!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classification</h1>
                </header>
            
            <article>
                
<p>We will start with the most commonly used machine learning technique: classification. As we reviewed in the first chapter, the main idea is to automatically build a mapping between the input variables and the outcome. In the following sections, we will look at how to load the data, select features, implement a basic classifier in Weka, and evaluate its performance.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data</h1>
                </header>
            
            <article>
                
<p>For this task, we will take a look at the <kbd>ZOO</kbd> database. The database contains 101 data entries of animals described with 18 attributes, as shown in the following table:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>animal</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>aquatic</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>fins</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>hair</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>predator</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>legs</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>feathers</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>toothed</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>tail</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>eggs</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>backbone</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>domestic</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>milk</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>breathes</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>cat size</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>airborne</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>venomous</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>type</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>An example entry in the dataset is a lion, with the following attributes:</p>
<ul>
<li><kbd>animal</kbd>: lion</li>
<li><kbd>hair</kbd>: true</li>
<li><kbd>feathers</kbd>: false</li>
<li><kbd>eggs</kbd>: false</li>
<li><kbd>milk</kbd>: true</li>
<li><kbd>airborne</kbd>: false</li>
<li><kbd>aquatic</kbd>: false</li>
<li><kbd>predator</kbd>: true</li>
<li><kbd>toothed</kbd>: true</li>
<li><kbd>backbone</kbd>: true</li>
<li><kbd>breathes</kbd>: true</li>
<li><kbd>venomous</kbd>: false</li>
<li><kbd>fins</kbd>: false</li>
<li><kbd>legs</kbd>: 4</li>
<li><kbd>tail</kbd>: true</li>
<li><kbd>domestic</kbd>: false</li>
<li><kbd>catsize</kbd>: true</li>
<li><kbd>type</kbd>: mammal</li>
</ul>
<p>Our task will be to build a model to predict the outcome variable, <kbd>animal</kbd>, given all of the other attributes as input.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading data</h1>
                </header>
            
            <article>
                
<p>Before we start the analysis, we will load the data in Weka's <strong>Attribute-Relation File Format</strong> (<strong>ARFF</strong>) and print the total number of loaded instances. Each data sample is held within a <kbd>DataSource</kbd> object, while the complete dataset, accompanied by meta-information, is handled by the <kbd>Instances</kbd> object.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To load the input data, we will use the <kbd>DataSource</kbd> object that accepts a variety of file formats and converts them into <kbd>Instances</kbd>:</p>
<pre>DataSource source = new DataSource("data/zoo.arff"); 
Instances data = source.getDataSet(); <br/>System.out.println(data.numInstances() + " instances loaded."); 
System.out.println(data.toString()); </pre>
<p>This will provide the number of loaded instances as output, as follows:</p>
<pre>101 instances loaded.</pre>
<p>We can also print the complete dataset by calling the <kbd>data.toString()</kbd> method.</p>
<p>Our task is to learn a model that is able to predict the <kbd>animal</kbd> attribute in the future examples for which we know the other attributes, but do not know the <kbd>animal</kbd> label. Hence, we will remove the <kbd>animal</kbd> attribute from the training set. We will accomplish this by filtering out the animal attribute, using the <kbd>Remove()</kbd> filter.</p>
<p>First, we set a string table of parameters, specifying that the first attribute must be removed. The remaining attributes are used as our dataset for training a classifier:</p>
<pre>Remove remove = new Remove(); 
String[] opts = new String[]{ "-R", "1"}; </pre>
<p>Finally, we call the <kbd>Filter.useFilter(Instances, Filter)</kbd> static method to apply the filter on the selected dataset:</p>
<pre>remove.setOptions(opts); 
remove.setInputFormat(data); 
data = Filter.useFilter(data, remove); <br/>System.out.println(data.toString()); </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feature selection</h1>
                </header>
            
            <article>
                
<p>As introduced in <a href="11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Applied Machine Learning Quick Start</em>, one of the preprocessing steps is focused on feature selection, also known as <strong>attribute selection</strong>. The goal is to select a subset of relevant attributes that will be used in a learned model. Why is feature selection important? A smaller set of attributes simplifies the models and makes them easier for users to interpret. This usually requires shorter training and reduces overfitting.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Attribute selection can take the class value into account or it cannot. In the first case, an attribute selection algorithm evaluates the different subsets of features and calculates a score that indicates the quality of selected attributes. We can use different searching algorithms, such as exhaustive search and best-first search, and different quality scores, such as information gain, the Gini index, and so on.</p>
<p>Weka supports this process with an <kbd>AttributeSelection</kbd> object, which requires two additional parameters: an evaluator, which computes how informative an attribute is, and a ranker, which sorts the attributes according to the score assigned by the evaluator.</p>
<p>We will use the following steps to perform selection:</p>
<ol>
<li>In this example, we will use information gain as an evaluator, and we will rank the features by their information gain score:</li>
</ol>
<pre style="padding-left: 60px">InfoGainAttributeEval eval = new InfoGainAttributeEval(); 
Ranker search = new Ranker(); </pre>
<ol start="2">
<li>We will initialize an <kbd>AttributeSelection</kbd> object and set the evaluator, ranker, and data:</li>
</ol>
<pre style="padding-left: 60px">AttributeSelection attSelect = new AttributeSelection(); 
attSelect.setEvaluator(eval); 
attSelect.setSearch(search); 
attSelect.SelectAttributes(data); </pre>
<ol start="3">
<li>We will print an order list of attribute <kbd>indices</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px">int[] indices = attSelect.selectedAttributes(); 
System.out.println(Utils.arrayToString(indices)); </pre>
<p>This process will provide the following result as output:</p>
<pre>12,3,7,2,0,1,8,9,13,4,11,5,15,10,6,14,16 </pre>
<p>The most informative attributes are <kbd>12</kbd> (fins), <kbd>3</kbd> (eggs), <kbd>7</kbd> (aquatic), <kbd>2</kbd> (hair), and so on. Based on this list, we can remove additional, non-informative features in order to help the learning algorithms achieve more accurate and faster learning models.</p>
<p>What would make the final decision about the number of attributes to keep? There's no rule of thumb related to an exact number; the number of attributes depends on the data and the problem. The purpose of attribute selection is to choose attributes that serve your model better, so it is best to focus on whether the attributes are improving the model.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Learning algorithms</h1>
                </header>
            
            <article>
                
<p>We have loaded our data and selected the best features, and we are ready to learn some classification models. Let's begin with basic decision trees.</p>
<p>In Weka, a decision tree is implemented within the <kbd>J48</kbd> class, which is a reimplementation of Quinlan's famous C4.5 decision tree learner (Quinlan, 1993).</p>
<p>We will make a decision tree by using the following steps:</p>
<ol>
<li>We initialize a new <kbd>J48</kbd> decision tree learner. We can pass additional parameters with a string table—for instance, the tree pruning that controls the model complexity (refer to <a href="11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Applied Machine Learning Quick Start</em>). In our case, we will build an un-pruned tree; hence, we will pass a single <kbd>-U</kbd> parameter, as follows:</li>
</ol>
<pre style="padding-left: 60px">J48 tree = new J48(); 
String[] options = new String[1]; 
options[0] = "-U"; 
 
tree.setOptions(options); </pre>
<ol start="2">
<li>We will call the <kbd>buildClassifier(Instances)</kbd> method to initialize the learning process:</li>
</ol>
<pre style="padding-left: 60px">tree.buildClassifier(data); </pre>
<ol start="3">
<li>The built model is now stored in a <kbd>tree</kbd> object. We can provide the entire <kbd>J48</kbd> unpruned tree by calling the <kbd>toString()</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">System.out.println(tree); </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The output will be as follows:</p>
<pre>    J48 unpruned tree
    ------------------
    
    feathers = false
    |   milk = false
    |   |   backbone = false
    |   |   |   airborne = false
    |   |   |   |   predator = false
    |   |   |   |   |   legs &lt;= 2: invertebrate (2.0)
    |   |   |   |   |   legs &gt; 2: insect (2.0)
    |   |   |   |   predator = true: invertebrate (8.0)
    |   |   |   airborne = true: insect (6.0)
    |   |   backbone = true
    |   |   |   fins = false
    |   |   |   |   tail = false: amphibian (3.0)
    |   |   |   |   tail = true: reptile (6.0/1.0)
    |   |   |   fins = true: fish (13.0)
    |   milk = true: mammal (41.0)
    feathers = true: bird (20.0)
    
    Number of Leaves  : 9
    
    Size of the tree : 17
  </pre>
<p>The tree in the output has <kbd>17</kbd> nodes in total and <kbd>9</kbd> of them are terminal (<kbd>Leaves</kbd>).</p>
<p>Another way to present the tree is to leverage the built-in <kbd>TreeVisualizer</kbd> tree viewer, as follows:</p>
<pre>TreeVisualizer tv = new TreeVisualizer(null, tree.graph(), new PlaceNode2()); 
JFrame frame = new javax.swing.JFrame("Tree Visualizer"); 
frame.setSize(800, 500); 
frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); 
frame.getContentPane().add(tv); 
frame.setVisible(true); 
tv.fitToScreen(); </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The preceding code results in the following output frame:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-536 image-border" src="Images/10ddbded-4f22-4f20-a2d5-eefd3158464e.png" style="width:62.83em;height:39.17em;" width="1950" height="1214"/></p>
<p>The decision process starts at the top node, also known as the root node. The node label specifies the attribute value that will be checked. In our example, first, we check the value of the <kbd>feathers</kbd> attribute. If the feather is present, we follow the right-hand branch, which leads us to the leaf labeled <kbd>bird</kbd>, indicating that there are <kbd>20</kbd> examples supporting this outcome. If the feather is not present, we follow the left-hand branch, which leads us to the <kbd>milk</kbd> attribute. We check the value of the attribute again, and we follow the branch that matches the attribute value. We repeat the process until we reach a leaf node.</p>
<p>We can build other classifiers by following the same steps: initialize a classifier, pass the parameters controlling the model complexity, and call the <kbd>buildClassifier(Instances)</kbd> method.</p>
<p>In the next section, you will learn how to use a trained model to assign a class label to a new example whose label is unknown.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classifying new data</h1>
                </header>
            
            <article>
                
<p>Suppose that we record attributes for an animal whose label we do not know; we can predict its label from the learned classification model. We will use the following animal for this process:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-537 image-border" src="Images/7f4f4365-22d9-44d1-9cfc-942e707c67cd.png" style="width:17.17em;height:17.17em;" width="420" height="420"/></p>
<p>First, we construct a feature vector describing the new specimen, as follows:</p>
<pre>double[] vals = new double[data.numAttributes()]; 
vals[0] = 1.0; //hair {false, true} 
vals[1] = 0.0;  //feathers {false, true} 
vals[2] = 0.0;  //eggs {false, true} 
vals[3] = 1.0;  //milk {false, true} 
vals[4] = 0.0;  //airborne {false, true} 
vals[5] = 0.0;  //aquatic {false, true} 
vals[6] = 0.0;  //predator {false, true} 
vals[7] = 1.0;  //toothed {false, true} 
vals[8] = 1.0;  //backbone {false, true} 
vals[9] = 1.0;  //breathes {false, true} 
vals[10] = 1.0;  //venomous {false, true} 
vals[11] = 0.0;  //fins {false, true} 
vals[12] = 4.0;  //legs INTEGER [0,9] 
vals[13] = 1.0;  //tail {false, true} 
vals[14] = 1.0;  //domestic {false, true} 
vals[15] = 0.0;  //catsize {false, true} 
DenseInstance myUnicorn = new DenseInstance(1.0, vals);<br/>myUnicorn.setDataset(data); </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Then, we call the <kbd>classify(Instance)</kbd> method on the model, in order to obtain the class value. The method returns the label index, as follows:</p>
<pre>double result = tree.classifyInstance(myUnicorn); 
System.out.println(data.classAttribute().value((int) result)); </pre>
<p>This will provide the <kbd>mammal</kbd> class label as output.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Evaluation and prediction error metrics</h1>
                </header>
            
            <article>
                
<p>We built a model, but we do not know if it can be trusted. To estimate its performance, we can apply a cross-validation technique that was explained in <a href="11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Applied Machine Learning Quick Start</em>.</p>
<p>Weka offers an <kbd>Evaluation</kbd> class for implementing cross-validation. We pass the model, data, number of folds, and an initial random seed, as follows:</p>
<pre>Classifier cl = new J48(); 
Evaluation eval_roc = new Evaluation(data); 
eval_roc.crossValidateModel(cl, data, 10, new Random(1), new Object[] {}); 
System.out.println(eval_roc.toSummaryString()); </pre>
<p>The evaluation results are stored in the <kbd>Evaluation</kbd> object.</p>
<p>A mix of the most common metrics can be invoked by calling the <kbd>toString()</kbd> method. Note that the output does not differentiate between regression and classification, so make sure to pay attention to the metrics that make sense, as follows:</p>
<pre>    Correctly Classified Instances          93               92.0792 %
    Incorrectly Classified Instances         8                7.9208 %
    Kappa statistic                          0.8955
    Mean absolute error                      0.0225
    Root mean squared error                  0.14  
    Relative absolute error                 10.2478 %
    Root relative squared error             42.4398 %
    Coverage of cases (0.95 level)          96.0396 %
    Mean rel. region size (0.95 level)      15.4173 %
    Total Number of Instances              101  
  </pre>
<p>In the classification, we are interested in the number of correctly/incorrectly classified instances.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The confusion matrix</h1>
                </header>
            
            <article>
                
<p>Furthermore, we can inspect where a particular misclassification has been made by examining the confusion matrix. The confusion matrix shows how a specific class value was predicted:</p>
<pre>double[][] confusionMatrix = eval_roc.confusionMatrix(); 
System.out.println(eval_roc.toMatrixString()); </pre>
<p>The resulting confusion matrix is as follows:</p>
<pre>    === Confusion Matrix ===
    
      a  b  c  d  e  f  g   &lt;-- classified as
     41  0  0  0  0  0  0 |  a = mammal
      0 20  0  0  0  0  0 |  b = bird
      0  0  3  1  0  1  0 |  c = reptile
      0  0  0 13  0  0  0 |  d = fish
      0  0  1  0  3  0  0 |  e = amphibian
      0  0  0  0  0  5  3 |  f = insect
      0  0  0  0  0  2  8 |  g = invertebrate
  </pre>
<p>The column names in the first row correspond to the labels assigned by the classification node. Each additional row then corresponds to an actual true class value. For instance, the second row corresponds to instances with the <kbd>mammal</kbd> true class label. In the column line, we read that all mammals were correctly classified as mammals. In the fourth row, <kbd>reptiles</kbd>, we notice that three were correctly classified as <kbd>reptiles</kbd>, while one was classified as <kbd>fish</kbd> and one as <kbd>insect</kbd>. The confusion matrix gives us insight into the kinds of errors that our classification model can make.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Choosing a classification algorithm</h1>
                </header>
            
            <article>
                
<p>Naive Bayes is one of the most simple, efficient, and effective inductive algorithms in machine learning. When features are independent, which is rarely true in the real world, it is theoretically optimal and, even with dependent features, its performance is amazingly competitive (Zhang, 2004). The main disadvantage is that it cannot learn how features interact with each other; for example, despite the fact that you like your tea with lemon or milk, you hate a tea that has both of them at the same time.</p>
<p><span>The main advantage of the d</span>ecision tree is that it is a model that is easy to interpret and explain, as we studied in our example. It can handle both nominal and numeric features, and you don't have to worry about whether the data is linearly separable.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Some other examples of classification algorithms are as follows:</p>
<ul>
<li><kbd>weka.classifiers.rules.ZeroR</kbd>: This predicts the majority class and is considered a baseline; that is, if your classifier's performance is worse than the average value predictor, it is not worth considering it.</li>
<li><kbd>weka.classifiers.trees.RandomTree</kbd>: This constructs a tree that considers <em>K</em> randomly chosen attributes at each node.</li>
<li><kbd>weka.classifiers.trees.RandomForest</kbd>: This constructs a set (forest) of random trees and uses majority voting to classify a new instance.</li>
<li><kbd>weka.classifiers.lazy.IBk</kbd>: This is the k-nearest neighbors classifier that is able to select an appropriate value of neighbors, based on cross-validation.</li>
<li><kbd>weka.classifiers.functions.MultilayerPerceptron</kbd>: This is a classifier based on neural networks that uses backpropagation to classify instances. The network can be built by hand, or created by an algorithm, or both.</li>
<li><kbd>weka.classifiers.bayes.NaiveBayes</kbd>: This is a Naive Bayes classifier that uses estimator classes, where numeric estimator precision values are chosen based on the analysis of the training data.</li>
<li><kbd>weka.classifiers.meta.AdaBoostM1</kbd>: This is the class for boosting a nominal class classifier by using the <kbd>AdaBoost M1</kbd> method. Only nominal class problems can be tackled. This often dramatically improves the performance, but sometimes, it overfits.</li>
<li><kbd>weka.classifiers.meta.Bagging</kbd>: This is the class for bagging a classifier to reduce the variance. This can perform classification and regression, depending on the base learner.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classification using Encog</h1>
                </header>
            
            <article>
                
<p>In the previous section, you saw how to use a Weka library for classification. In this section, we will quickly look at how the same can be achieved by using the Encog library. Encog requires us to build a model to do the classification. Download the Encog library from <a href="https://github.com/encog/encog-java-core/releases">https://github.com/encog/encog-java-core/releases</a>. Once downloaded, add the <kbd>.jar</kbd> file in the Eclipse project, as explained at the beginning of the chapter.</p>
<p>For this example, we will use the <kbd>iris</kbd> dataset, which is available in <kbd>.csv</kbd> format; it can be downloaded from <a href="https://archive.ics.uci.edu/ml/datasets/Iris">https://archive.ics.uci.edu/ml/datasets/Iris</a>. From the download path, copy the <kbd>iris.data.csv</kbd> file into your data directory. This file contains the data of 150 different flowers. It contains four different measurements about the flowers, and the last column is a label.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We will now perform the classification, using the following steps:</p>
<ol start="1">
<li>We will use the <kbd>VersatileMLDataSet</kbd> method to load the file and define all four columns. The next step is to call the <kbd>analyze</kbd> method that will read the entire file and find the statistical parameters, such as the mean, the standard deviation, and many more:</li>
</ol>
<pre style="padding-left: 60px">File irisFile = new File("data/iris.data.csv");<br/>VersatileDataSource source = new CSVDataSource(irisFile, false, CSVFormat.DECIMAL_POINT);<br/> <br/>VersatileMLDataSet data = new VersatileMLDataSet(source); <br/>data.defineSourceColumn("sepal-length", 0, ColumnType.continuous); <br/>data.defineSourceColumn("sepal-width", 1, ColumnType.continuous); <br/>data.defineSourceColumn("petal-length", 2, ColumnType.continuous); <br/>data.defineSourceColumn("petal-width", 3, ColumnType.continuous); <br/>        <br/>ColumnDefinition outputColumn = data.defineSourceColumn("species", 4, ColumnType.nominal);<br/>data.analyze(); </pre>
<ol start="2">
<li>The next step is to define the output column. Then, it's time to normalize the data; but before that, we need to decide on the model type according to which the data will be normalized, as follows:</li>
</ol>
<pre style="padding-left: 60px">data.defineSingleOutputOthersInput(outputColumn); <br/><br/>EncogModel model = new EncogModel(data); <br/>model.selectMethod(data, MLMethodFactory.TYPE_FEEDFORWARD);<br/> <br/>model.setReport(new ConsoleStatusReportable()); <br/>data.normalize(); </pre>
<ol start="3">
<li>The next step is to fit the model on a training set, leaving a test set aside. We will hold 30% of the data, as specified by the first argument, <kbd>0.3</kbd>; the next argument specifies that we want to shuffle the data in randomly. <kbd>1001</kbd> says that there is a seed value of 1001, so we use a <kbd>holdBackValidation</kbd> model:</li>
</ol>
<pre style="padding-left: 60px">model.holdBackValidation(0.3, true, 1001);</pre>
<p class="mce-root"/>
<ol start="4">
<li>Now, it's time to train the model and classify the data, according to the measurements and labels. The cross-validation breaks the training dataset into five different combinations:</li>
</ol>
<pre style="padding-left: 60px">model.selectTrainingType(data); <br/>MLRegression bestMethod = (MLRegression)model.crossvalidate(5, true); </pre>
<ol start="5">
<li>The next step is to display the results of each fold and the errors:</li>
</ol>
<pre style="padding-left: 60px">System.out.println( "Training error: " + EncogUtility.calculateRegressionError(bestMethod, model.getTrainingDataset())); <br/>System.out.println( "Validation error: " + EncogUtility.calculateRegressionError(bestMethod, model.getValidationDataset())); </pre>
<ol start="6">
<li>Now, we will start to use the model to predict the values, using the following code block:</li>
</ol>
<pre style="padding-left: 60px">while(csv.next()) { <br/>            StringBuilder result = new StringBuilder(); <br/>            line[0] = csv.get(0); <br/>            line[1] = csv.get(1); <br/>            line[2] = csv.get(2); <br/>            line[3] = csv.get(3); <br/>            String correct = csv.get(4); <br/>            helper.normalizeInputVector(line,input.getData(),false); <br/>            MLData output = bestMethod.compute(input); <br/>            String irisChosen = helper.denormalizeOutputVectorToString(output)[0]; <br/>             <br/>            result.append(Arrays.toString(line)); <br/>            result.append(" -&gt; predicted: "); <br/>            result.append(irisChosen); <br/>            result.append("(correct: "); <br/>            result.append(correct); <br/>            result.append(")"); <br/>             <br/>            System.out.println(result.toString()); <br/>        } </pre>
<p class="mce-root"/>
<p>This will yield an output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-538 image-border" src="Images/7c6cad63-e1ef-499b-8b39-74d333a78d85.png" style="width:57.00em;height:64.58em;" width="684" height="775"/></p>
<p>Encog supports many other options in <kbd>MLMethodFactory</kbd>, such as SVM, PNN, and so on.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classification using massive online analysis</h1>
                </header>
            
            <article>
                
<p><strong>Massive Online Analysis</strong> (<strong>MOA</strong>), as discussed in <a href="6fd557d7-2807-4a6d-8f93-d7c4ca094b7e.xhtml">Chapter 2</a>, <em>Java Libraries and Platforms for Machine Learning,</em> is another library that can be used to achieve classification. It is mainly designed to work with the stream. If it is working with the stream, a lot of data will be there; so, how do we evaluate the model? In the traditional batch learning mode, we usually divide the data into training and test sets and cross-validation is preferred if the data is limited. In stream processing, where the data seems to be unlimited, cross-validation proves to be expensive. Two approaches that we can use are as follows:</p>
<ul>
<li><strong>Holdout</strong>: This is useful when the data is already divided into two parts, which are predefined. It gives the estimation of the current classifier, if it is similar to the current data. This similarity is hard to guarantee between the holdout set and the current data.</li>
<li><strong>Interleaved test-then-train, or prequential</strong>: In this method, the model is tested on the example before it is used for training. So, the model is always tested for the data that it has never seen. In this, no holdout scheme is needed. It uses the available data. Over time, this approach will improve the accuracy of classification.</li>
</ul>
<p>MOA provides various ways to generate the stream of data. First, download the MOA library from <a href="https://moa.cms.waikato.ac.nz/downloads/">https://moa.cms.waikato.ac.nz/downloads/</a>. Add the downloaded <kbd>.jar</kbd> files to Eclipse, like we did for Weka at the beginning of this chapter. We will be using the GUI tool provided by MOA to see how to use MOA for streams. To launch the GUI, <span>make sure <kbd>moa.jar</kbd> and <kbd>sizeofag.jar</kbd> are in the current path; then, </span>run the following command in Command Prompt:</p>
<pre><strong>$ java -cp moa.jar -javaagent:sizeofag.jar moa.gui.GUI</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>It will display the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-539 image-border" src="Images/3ba39b4e-10ac-424d-b2a1-187bdee96629.png" style="width:72.08em;height:76.92em;" width="865" height="923"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can see that it has options for classification, regression, clustering, outliers, and more. Clicking on the <span class="packt_screen">Configure</span> button will display the screen used to make your classifier. It provides various learners and streams to work with, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-540 image-border" src="Images/16dcc58d-a17e-42bf-a59e-fa4a28af752c.png" style="width:27.50em;height:35.00em;" width="432" height="550"/></p>
<p>The following is an example of running <kbd>RandomTreeGenerator</kbd> with <kbd>NaiveBayes</kbd> and <kbd>HoeffdingTree</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-541 image-border" src="Images/f0662400-373d-4b18-9d2d-ac392275f3e6.png" style="width:99.33em;height:80.50em;" width="1192" height="966"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Evaluation</h1>
                </header>
            
            <article>
                
<p>Evaluation is the next important task, after the model has been developed. It lets you decide whether the model is performing on the given dataset well and ensures that it will be able to handle data that it has never seen. The evaluation framework mostly uses the following features:</p>
<ul>
<li><strong>Error estimation</strong>: This uses holdout or interleaved test-and-train methods to estimate the errors. K-fold cross-validation is also used.</li>
<li><strong>Performance measures</strong>: The <span class="packt_screen">Kappa</span> statistics are used, which are more sensitive towards streaming classifiers.</li>
<li><strong>Statistical validation</strong>: When comparing evaluating classifiers, we must look at the differences in random and non-random experiments. The McNemar's test is the most popular test in streaming, used to access the statistical significance of differences in two classifiers. If we are working with one classifier, the confidence intervals of parameter estimates indicate the reliability.</li>
<li><strong>The cost measure of the process</strong>: As we are dealing with streaming data, which may require access to third-party or cloud-based solutions to get and process the data, the cost per hour of usage and memory is considered for evaluation purposes.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Baseline classifiers</h1>
                </header>
            
            <article>
                
<p>Batch learning has led to the development of many classifiers in different paradigms, such as divide and conquer, lazy learners, kernel methods, graphics models, and so on. Now, if we move to a stream for the same, we need to understand how to make them incremental and fast for the large datasets in the streams. We have to think in terms of the complexity of the model versus the speed of the model update, and this is the main trade-off that needs to be taken care of.</p>
<p>The <strong>majority class algorithm</strong> is one of the simplest classifiers, and it is used as a baseline. It is also used as a default classifier for decision tree leaves. Another is the <strong>no</strong>-<strong>change classifier</strong>, which predicts the labels for new instances. The Naive Bayes algorithm is known for its low cost in terms of computational power and simplicity. It's an incremental algorithm and is best suited for streams.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Decision tree</h1>
                </header>
            
            <article>
                
<p>The decision tree is a very popular classifier technique, making it easy to interpret and visualize the models. It is based on trees. It divides or splits the nodes on the basis of the attribute value and the leaves of the tree usually fall to the majority class classifier. In streaming data, the Hoeffding tree is a very fast algorithm for decision trees; it waits for new instances, instead of reusing instances. It builds a tree for large data. The <strong>Concept-adapting Very Fast Decision Tree</strong> (<span><strong>CVFDT</strong>)</span> deals with the concept of drift, which maintains a model consistency with the instances in a sliding window. The other trees are the <strong>Ultra Fast Forest of Trees</strong> (<strong>UFFT</strong>), the Hoeffding adaptive tree, the exhaustive binary tree, and so on.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Lazy learning</h1>
                </header>
            
            <article>
                
<p>In the streaming context, <strong>k-nearest neighbor</strong> (<strong>KNN</strong>) is the most convenient batch method. A sliding window is used to determine the KNN for a new instance that is not yet classified. It normally uses the 1,000 most recent instances for the sliding window. As the sliding window slides, it handles the concept drift, too.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Active learning</h1>
                </header>
            
            <article>
                
<p>We all know that classifiers work well with labeled data, but that is not always the case with stream data. For example, the data from a stream may come unlabeled. Labeling data is costly, because it requires human intervention to label the unlabeled data. We understand that the streams generate large amounts of data. Active learning algorithms only do the labeling for selective data. The data to be labeled is decided on from historical data suited for pool-based settings. Regular retraining is required to decide whether a label is required for incoming instances. A simple strategy for labeling data is to use a random strategy. It is also called a baseline strategy, and it asks for a label for each incoming instance with probability of a budget for labelling. Another strategy is to ask for a label for the instance for which the current classifier is least confident. This may work fine, but soon, the classifier will exhaust its budget or reach its threshold.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Regression</h1>
                </header>
            
            <article>
                
<p>We will explore basic regression algorithms through an analysis of an energy efficiency dataset (Tsanas and Xifara, 2012). We will investigate the heating and cooling load requirements of the buildings based on their construction characteristics, such as surface, wall, and roof area; height; glazing area; and compactness. The researchers have used a simulator to design 12 different house configurations, while varying 18 building characteristics. In total, 768 different buildings were simulated.</p>
<p>Our first goal is to systematically analyze the impact that each building characteristic has on the target variable, that is, the heating or cooling load. The second goal is to compare the performance of a classical linear regression model against other methods, such as SVM regression, random forests, and neural networks. For this task, we will use the Weka library.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading the data</h1>
                </header>
            
            <article>
                
<p>Download the energy efficiency dataset from <span class="URLPACKT"><a href="https://archive.ics.uci.edu/ml/datasets/Energy+efficiency">https://archive.ics.uci.edu/ml/datasets/Energy+efficiency</a></span>.</p>
<p>The dataset is in Excel's XLSX format, which cannot be read by Weka. We can convert it into a <strong>comma-separated value</strong> (<strong>CSV</strong>) format by clicking on <span class="packt_screen">File</span> | <span class="packt_screen">Save As</span> and picking <kbd>.csv</kbd> in the saving dialog, as shown in the following screenshot. Confirm to save only the active sheet (since all of the others are empty), and confirm to continue, to lose some formatting features. Now, the file is ready to be loaded by Weka:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-542 image-border" src="Images/c14075de-336e-4f70-a82a-bc3405293095.png" style="width:45.00em;height:25.58em;" width="1231" height="699"/></div>
<p>Open the file in a text editor and inspect whether the file was correctly transformed. There might be some minor issues that could cause problems. For instance, in my export, each line ended with a double semicolon, as follows:</p>
<pre>X1;X2;X3;X4;X5;X6;X7;X8;Y1;Y2;; 
0,98;514,50;294,00;110,25;7,00;2;0,00;0;15,55;21,33;; 
0,98;514,50;294,00;110,25;7,00;3;0,00;0;15,55;21,33;; </pre>
<p>To remove the doubled semicolon, you can use the <span class="packt_screen">Find and Replace</span> function: find <kbd>;;</kbd> and replace it with <kbd>;</kbd>.</p>
<p>The second problem was that my file had a long list of empty lines at the end of the document, which can be deleted, as follows:</p>
<pre>0,62;808,50;367,50;220,50;3,50;5;0,40;5;16,64;16,03;; 
;;;;;;;;;;; 
;;;;;;;;;;; </pre>
<p>Now, we are ready to load the data. Let's open a new file and write a simple data import function by using Weka's converter for reading files in a CSV format, as follows:</p>
<pre>import weka.core.Instances; 
import weka.core.converters.CSVLoader; 
import java.io.File; 
import java.io.IOException; 
 
public class EnergyLoad { 
 
  public static void main(String[] args) throws IOException { 
 
    // load CSV 
    CSVLoader loader = new CSVLoader();<br/>    loader.setFieldSeparator(","); 
    loader.setSource(new File("data/ENB2012_data.csv")); 
    Instances data = loader.getDataSet(); 
 
    System.out.println(data); 
  } 
} </pre>
<p>The data is loaded! Let's move on.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Analyzing attributes</h1>
                </header>
            
            <article>
                
<p>Before we analyze the attributes, let's try to understand what we are dealing with. In total, there are eight attributes describing building characteristics, and there are also two target variables, the heating and cooling load, as shown in the following table:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Attribute</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Attribute name</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>X1</kbd></td>
<td class="CDPAlignCenter CDPAlign">Relative compactness</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>X2</kbd></td>
<td class="CDPAlignCenter CDPAlign">Surface area</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>X3</kbd></td>
<td class="CDPAlignCenter CDPAlign">Wall area</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>X4</kbd></td>
<td class="CDPAlignCenter CDPAlign">Roof area</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>X5</kbd></td>
<td class="CDPAlignCenter CDPAlign">Overall height</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>X6</kbd></td>
<td class="CDPAlignCenter CDPAlign">Orientation</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>X7</kbd></td>
<td class="CDPAlignCenter CDPAlign">Glazing area</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>X8</kbd></td>
<td class="CDPAlignCenter CDPAlign">Glazing area distribution</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>Y1</kbd></td>
<td class="CDPAlignCenter CDPAlign">Heating load</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>Y2</kbd></td>
<td class="CDPAlignCenter CDPAlign">Cooling load</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building and evaluating the regression model</h1>
                </header>
            
            <article>
                
<p>We will start by learning a model for the heating load by setting the class attribute at the feature position:</p>
<pre>data.setClassIndex(data.numAttributes() - 2); </pre>
<p>The second target variable, the cooling load, can now be removed:</p>
<pre>//remove last attribute Y2 
Remove remove = new Remove(); 
remove.setOptions(new String[]{"-R", data.numAttributes()+""}); 
remove.setInputFormat(data);
data = Filter.useFilter(data, remove); </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear regression</h1>
                </header>
            
            <article>
                
<p>We will start with a basic linear regression model, implemented with the <kbd>LinearRegression</kbd> class. Similar to the classification example, we will initialize a new model instance, pass the parameters and data, and invoke the <kbd>buildClassifier(Instances)</kbd> method, as follows:</p>
<pre>import weka.classifiers.functions.LinearRegression; 
... 
data.setClassIndex(data.numAttributes() - 2);<br/>LinearRegression model = new LinearRegression(); <br/>model.buildClassifier(data); <br/>System.out.println(model);</pre>
<p>The learned model, which is stored in the object, can be provided by calling the <kbd>toString()</kbd> method, as follows:</p>
<pre>    Y1 =
    
        -64.774  * X1 +
         -0.0428 * X2 +
          0.0163 * X3 +
         -0.089  * X4 +
          4.1699 * X5 +
         19.9327 * X7 +
          0.2038 * X8 +
         83.9329
  </pre>
<p>The linear regression model constructs a function that linearly combines the input variables to estimate the heating load. The number in front of the feature explains the feature's impact on the target variable: the sign corresponds to the positive/negative impact, while the magnitude corresponds to its significance. For instance, the <span>relative compactness</span> of the feature <kbd>X1</kbd> is negatively correlated with heating load, while the glazing area is positively correlated. These two features also significantly impact the final heating load estimate. The model's performance can similarly be evaluated with the cross-validation technique.</p>
<p>The ten-fold cross-validation is as follows:</p>
<pre>Evaluation eval = new Evaluation(data); 
eval.crossValidateModel(model, data, 10, new Random(1), new String[]{}); 
System.out.println(eval.toSummaryString()); </pre>
<p>We can provide the common evaluation metrics, including the correlation, the mean absolute error, the relative absolute error, and so on, as output, as follows:</p>
<pre>Correlation coefficient                  0.956  
Mean absolute error                      2.0923 
Root mean squared error                  2.9569 
Relative absolute error                 22.8555 % 
Root relative squared error             29.282  % 
Total Number of Instances              768      </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear regression using Encog</h1>
                </header>
            
            <article>
                
<p>Now, we will quickly look at how Encog can be used to make a regression model. We will be using the dataset that we used in a previous section, <em>Loading the data</em>. The following steps show how to make the model: </p>
<ol start="1">
<li>To load the data, we will use the <kbd>VersatileMLDataSet</kbd> function, as follows:</li>
</ol>
<pre style="padding-left: 60px">File datafile = new File("data/ENB2012_data.csv");<br/>VersatileDataSource source = new CSVDataSource(datafile, true, CSVFormat.DECIMAL_POINT);<br/>VersatileMLDataSet data = new VersatileMLDataSet(source); <br/>data.defineSourceColumn("X1", 0, ColumnType.continuous); <br/>data.defineSourceColumn("X2", 1, ColumnType.continuous); <br/>data.defineSourceColumn("X3", 2, ColumnType.continuous); <br/>data.defineSourceColumn("X4", 3, ColumnType.continuous);<br/>data.defineSourceColumn("X5", 4, ColumnType.continuous);<br/>data.defineSourceColumn("X6", 5, ColumnType.continuous);<br/>data.defineSourceColumn("X7", 6, ColumnType.continuous);<br/>data.defineSourceColumn("X8", 7, ColumnType.continuous);</pre>
<ol start="2">
<li>As we have two pieces of output, <kbd>Y1</kbd> and <kbd>Y2</kbd>, they can be added by using the <kbd>defineMultipleOutputsOthersInput</kbd> function, as follows:</li>
</ol>
<pre style="padding-left: 60px">ColumnDefinition outputColumn1 = data.defineSourceColumn("Y1", 8,    ColumnType.continuous);<br/>ColumnDefinition outputColumn2 = data.defineSourceColumn("Y2", 9,  ColumnType.continuous);<br/>ColumnDefinition outputscol [] = {outputColumn1, outputColumn2};<br/>data.analyze();<br/><br/>data.defineMultipleOutputsOthersInput(outputscol);</pre>
<ol start="3">
<li>The next step is to develop a simple regression model by using the <kbd>FEEDFORWARD</kbd> instance:</li>
</ol>
<pre style="padding-left: 60px">EncogModel model = new EncogModel(data); <br/>model.selectMethod(data, MLMethodFactory.TYPE_FEEDFORWARD);<br/>model.setReport(new ConsoleStatusReportable());<br/>            <br/>data.normalize();<br/>model.holdBackValidation(0.3, true, 1001);<br/>model.selectTrainingType(data);<br/>MLRegression bestMethod = (MLRegression)model.crossvalidate(5, true);            <br/>NormalizationHelper helper = data.getNormHelper(); <br/>            <br/>System.out.println(helper.toString()); <br/>System.out.println("Final model: " + bestMethod); </pre>
<p>Now, our regression model is ready. The last few lines of the output are given in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-543 image-border" src="Images/7a260d2b-211b-40d0-884b-21b04ec6ac34.png" style="width:70.50em;height:28.92em;" width="846" height="347"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Regression using MOA</h1>
                </header>
            
            <article>
                
<p>Using MOA for regression requires us to use the GUI. You can download the dataset from <a href="http://www.cs.waikato.ac.nz/~bernhard/halifax17/census.arff.gz">http://www.cs.waikato.ac.nz/~bernhard/halifax17/census.arff.gz</a>.</p>
<p>The following steps show how to perform regression:</p>
<ol start="1">
<li>Launch the MOA GUI by using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ java -cp moa.jar -javaagent:sizeofag-1.0.4.jar moa.gui.GUI</strong></pre>
<ol start="2">
<li>Select the <span class="packt_screen">Regression</span> tab and click on <span class="packt_screen">Configure,</span> as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-544 image-border" src="Images/afc20bde-fc01-44f6-864f-07b531de5c92.png" style="width:72.25em;height:76.83em;" width="867" height="922"/></p>
<ol start="3">
<li>We will use the downloaded <kbd>.arff</kbd> file for regression. When we click on <span class="packt_screen">Configure</span> in the preceding step, it will display the <span class="packt_screen">Configure task</span> window, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-545 image-border" src="Images/62d0e7ee-fbcd-41bc-aa15-5632716435ed.png" style="width:19.17em;height:25.50em;" width="411" height="546"/></p>
<ol start="4">
<li>In the <span class="packt_screen">stream</span> option, click on <span class="packt_screen">Edit </span>and select the <span class="packt_screen">ArffFileStream</span>; select the <kbd>.arff</kbd> file that we downloaded, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-546 image-border" src="Images/c679a3fd-fadd-410d-9635-c3bf2fd7c871.png" style="width:26.67em;height:16.92em;" width="560" height="356"/></p>
<ol start="5">
<li>In <span class="packt_screen">classIndex</span>, specify <kbd>-1</kbd>, which sets the first attribute as the target. Click on <span class="packt_screen">OK </span>in all pop-up windows and click on <span class="packt_screen">Run</span>. It will take some time, as the census file has a large amount of data to process, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-547 image-border" src="Images/82fc5070-7875-4ad8-a5e7-3860ed59c498.png" style="width:100.08em;height:76.17em;" width="1201" height="914"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Regression trees</h1>
                </header>
            
            <article>
                
<p>Another approach is to construct a set of regression models, each on its own part of the data. The following diagram shows the main difference between a regression model and a regression tree. A regression model constructs a single model that best fits all of the data. A regression tree, on the other hand, constructs a set of regression models, each modeling a part of the data, as shown on the right-hand side. Compared to the regression model, the regression tree can better fit the data, but the function is a piece-wise linear plot, with jumps between modeled regions, as seen in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-548 image-border" src="Images/05ac04c7-3b94-4b84-ad85-107afa117db8.png" style="width:35.83em;height:12.42em;" width="430" height="149"/></p>
<p>A regression tree in Weka is implemented within the <kbd>M5</kbd> class. The model construction follows the same paradigm: initialize the model, pass the parameters and data, and invoke the <kbd>buildClassifier(Instances)</kbd> method, as follows:</p>
<pre>import weka.classifiers.trees.M5P; 
... 
M5P md5 = new M5P(); 
md5.setOptions(new String[]{""}); 
md5.buildClassifier(data);  
System.out.println(md5); </pre>
<p>The induced model is a tree with equations in the leaf nodes, as follows:</p>
<pre>    M5 pruned model tree:
    (using smoothed linear models)
    
    X1 &lt;= 0.75 : 
    |   X7 &lt;= 0.175 : 
    |   |   X1 &lt;= 0.65 : LM1 (48/12.841%)
    |   |   X1 &gt;  0.65 : LM2 (96/3.201%)
    |   X7 &gt;  0.175 : 
    |   |   X1 &lt;= 0.65 : LM3 (80/3.652%)
    |   |   X1 &gt;  0.65 : LM4 (160/3.502%)
    X1 &gt;  0.75 : 
    |   X1 &lt;= 0.805 : LM5 (128/13.302%)
    |   X1 &gt;  0.805 : 
    |   |   X7 &lt;= 0.175 : 
    |   |   |   X8 &lt;= 1.5 : LM6 (32/20.992%)
    |   |   |   X8 &gt;  1.5 : 
    |   |   |   |   X1 &lt;= 0.94 : LM7 (48/5.693%)
    |   |   |   |   X1 &gt;  0.94 : LM8 (16/1.119%)
    |   |   X7 &gt;  0.175 : 
    |   |   |   X1 &lt;= 0.84 : 
    |   |   |   |   X7 &lt;= 0.325 : LM9 (20/5.451%)
    |   |   |   |   X7 &gt;  0.325 : LM10 (20/5.632%)
    |   |   |   X1 &gt;  0.84 : 
    |   |   |   |   X7 &lt;= 0.325 : LM11 (60/4.548%)
    |   |   |   |   X7 &gt;  0.325 : 
    |   |   |   |   |   X3 &lt;= 306.25 : LM12 (40/4.504%)
    |   |   |   |   |   X3 &gt;  306.25 : LM13 (20/6.934%)
    
    LM num: 1
    Y1 = 
      72.2602 * X1 
      + 0.0053 * X3 
      + 11.1924 * X7 
      + 0.429 * X8 
      - 36.2224
    
    ...
    
    LM num: 13
    Y1 = 
      5.8829 * X1 
      + 0.0761 * X3 
      + 9.5464 * X7 
      - 0.0805 * X8 
      + 2.1492
    
    Number of Rules : 13
  </pre>
<p>The tree has <kbd>13</kbd> leaves, each corresponding to a linear equation. The preceding output is visualized in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-549 image-border" src="Images/afb3b685-9ce9-4bcc-b3eb-9ba448748986.png" style="width:40.08em;height:28.17em;" width="716" height="502"/></p>
<p>The tree can be read similarly to a classification tree. The most important features are at the top of the tree. The terminal node, the leaf, contains a linear regression model explaining the data that reaches this part of the tree.</p>
<p>An evaluation will provide the following results as output:</p>
<pre>    Correlation coefficient                  0.9943
    Mean absolute error                      0.7446
    Root mean squared error                  1.0804
    Relative absolute error                  8.1342 %
    Root relative squared error             10.6995 %
    Total Number of Instances              768     </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tips to avoid common regression problems</h1>
                </header>
            
            <article>
                
<p>First, we have to use prior studies and domain knowledge to figure out which features to include in regression. Check literature, reports, and previous studies on what kinds of features work and some reasonable variables for modeling your problem. Suppose that you have a large set of features with random data; it is highly likely that several features will be correlated to the target variable (even though the data is random).</p>
<p>We have to keep the model simple, in order to avoid overfitting. The Occam's razor principle states that you should select a model that best explains your data, with the least assumptions. In practice, the model can be as simple as having two to four predictor features.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Clustering</h1>
                </header>
            
            <article>
                
<p>Compared to a supervised classifier, the goal of clustering is to identify intrinsic groups in a set of unlabeled data. It can be applied to identifying representative examples of homogeneous groups, finding useful and suitable groupings, or finding unusual examples, such as outliers.</p>
<p>We'll demonstrate how to implement clustering by analyzing a bank dataset. The dataset consists of 11 attributes, describing 600 instances, with age, sex, region, income, marital status, children, car ownership status, saving activity, current activity, mortgage status, and PEP. In our analysis, we will try to identify the common groups of clients by applying the <strong>expectation maximization</strong> (<strong>EM</strong>) clustering.</p>
<p>EM works as follows: given a set of clusters, EM first assigns each instance with a probability distribution of belonging to a particular cluster. For example, if we start with three clusters—namely, A, B, and C—an instance might get the probability distribution of 0.70, 0.10, and 0.20, belonging to the A, B, and C clusters, respectively. In the second step, EM re-estimates the parameter vector of the probability distribution of each class. The algorithm iterates these two steps until the parameters converge or the maximum number of iterations is reached.</p>
<p>The number of clusters to be used in EM can be set either manually or automatically by cross-validation. Another approach to determining the number of clusters in a dataset includes the elbow method. This method looks at the percentage of variance that is explained with a specific number of clusters. The method suggests increasing the number of clusters until the additional cluster does not add much information, that is, it explains little additional variance.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Clustering algorithms</h1>
                </header>
            
            <article>
                
<p>The process of building a cluster model is quite similar to the process of building a classification model, that is, loading the data and building a model. Clustering algorithms are implemented in the <kbd>weka.clusterers</kbd> package, as follows:</p>
<pre>import java.io.BufferedReader; 
import java.io.FileReader; 
 
import weka.core.Instances; 
import weka.clusterers.EM; 
 
public class Clustering { 
 
  public static void main(String args[]) throws Exception{ 
     
    //load data 
    Instances data = new Instances(new BufferedReader<br/>       (new FileReader("data/bank-data.arff"))); 
     
    // new instance of clusterer 
    EM model = new EM(); 
    // build the clusterer 
    model.buildClusterer(data); 
    System.out.println(model); 
 
  } 
} </pre>
<p>The model identified the following six clusters:</p>
<pre>    EM
    ==
    
    Number of clusters selected by cross validation: 6
    
                     Cluster
    Attribute              0        1        2        3        4        5
                       (0.1)   (0.13)   (0.26)   (0.25)   (0.12)   (0.14)
    ======================================================================
    age
      0_34            10.0535  51.8472 122.2815  12.6207   3.1023   1.0948
      35_51           38.6282  24.4056  29.6252  89.4447  34.5208   3.3755
      52_max          13.4293    6.693   6.3459  50.8984   37.861  81.7724
      [total]         62.1111  82.9457 158.2526 152.9638  75.4841  86.2428
    sex
      FEMALE          27.1812  32.2338  77.9304  83.5129  40.3199  44.8218
      MALE            33.9299  49.7119  79.3222  68.4509  34.1642   40.421
      [total]         61.1111  81.9457 157.2526 151.9638  74.4841  85.2428
    region
      INNER_CITY      26.1651  46.7431   73.874  60.1973  33.3759  34.6445
      TOWN            24.6991  13.0716  48.4446  53.1731   21.617  17.9946
    ...
  </pre>
<p>The table can be read as follows: the first line indicates six clusters, while the first column shows the attributes and their ranges. For example, the attribute <kbd>age</kbd> is split into three ranges: <kbd>0-34</kbd>, <kbd>35-51</kbd>, and <kbd>52-max</kbd>. The columns on the left indicate how many instances fall into the specific range in each cluster; for example, clients in the <kbd>0-34</kbd> years age group are mostly in cluster 2 (122 instances).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Evaluation</h1>
                </header>
            
            <article>
                
<p>A clustering algorithm's quality can be estimated by using the <kbd>logLikelihood</kbd> measure, which measures how consistent the identified clusters are. The dataset is split into multiple folds, and clustering is run with each fold. The motivation is that, if the clustering algorithm assigns a high probability to similar data that wasn't used to fit parameters, then it has probably done a good job of capturing the data structure. Weka offers the <kbd>CluterEvaluation</kbd> class to estimate it, as follows:</p>
<pre>double logLikelihood = ClusterEvaluation.crossValidateModel(model, data, 10, new Random(1));<br/>System.out.println(logLikelihood);  </pre>
<p>It provides the following output:</p>
<pre>-8.773410259774291 </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Clustering using Encog</h1>
                </header>
            
            <article>
                
<p>Encog supports k-means clustering. Let's consider a very simple example, with the data shown in the following code block:</p>
<pre>DATA = { { 28, 15, 22 }, { 16, 15, 32 }, { 32, 20, 44 }, { 1, 2, 3 }, { 3, 2, 1 } };</pre>
<p>To make <kbd>BasicMLDataSet</kbd> from this data, a simple <kbd>for</kbd> loop is used, which will add data to the dataset:</p>
<pre>BasicMLDataSet set = new BasicMLDataSet();<br/><br/>for (final double[] element : DATA) {<br/>    set.add(new BasicMLData(element));<br/>}</pre>
<p>Using the <kbd>KMeansClustering</kbd> function, let's clusters the dataset into two clusters, as follows:</p>
<pre>KMeansClustering kmeans = new KMeansClustering(2, set);<br/><br/>kmeans.iteration(100);<br/><br/>// Display the cluster<br/>int i = 1;<br/>for (MLCluster cluster : kmeans.getClusters()) {<br/>    System.out.println("*** Cluster " + (i++) + " ***");<br/>    final MLDataSet ds = cluster.createDataSet();<br/>    final MLDataPair pair = BasicMLDataPair.createPair(ds.getInputSize(), ds.getIdealSize());<br/>    for (int j = 0; j &lt; ds.getRecordCount(); j++) {<br/>        ds.getRecord(j, pair);<br/>        System.out.println(Arrays.toString(pair.getInputArray()));<br/>        }<br/>    }</pre>
<p>This will generate the following output:</p>
<pre>*** Cluster 1 ***<br/>[16.0, 15.0, 32.0]<br/>[1.0, 2.0, 3.0]<br/>[3.0, 2.0, 1.0]<br/>*** Cluster 2 ***<br/>[28.0, 15.0, 22.0]<br/>*** Cluster 3 ***<br/>[32.0, 20.0, 44.0]</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Clustering using ELKI</h1>
                </header>
            
            <article>
                
<p>ELKI supports many clustering algorithms. A few are listed as follows:</p>
<ul>
<li><strong>Affinity propagation clustering algorithm</strong>: This is a cluster analysis that uses affinity propagation.</li>
<li><strong>DBSCAN</strong>: This is density based clustering especially for the applications with noise; it finds the sets in the database on the basis of density.</li>
<li><strong>EM</strong>: This algorithm creates clusters based on the expectation maximization algorithm.</li>
<li><strong>AGNES</strong>: <strong>Hierarchical agglomerative clustering (HAC),</strong> or <strong>agglomerative nesting (AGNES),</strong> is a classic hierarchical clustering algorithm. </li>
<li><strong>SLINK</strong>: This is the single link algorithm.</li>
<li><strong>CLINK</strong>: This is used for complete linkage.</li>
<li><strong>HDBSCAN</strong>: This is an extracting cluster hierarchy.</li>
</ul>
<p>Also, KMeansSort, KMeansCompare, KMedianLloyd, KMediodsEM, KMeansBisecting, and so on, are some examples from the family of KMean.</p>
<p>A detailed list of clustering algorithms, with all of the algorithms supported by ELKI, can be found at <a href="https://elki-project.github.io/algorithms/">https://elki-project.github.io/algorithms/</a>.</p>
<p>We need to get the required <kbd>.jar</kbd> file from <a href="https://elki-project.github.io/releases/">https://elki-project.github.io/releases/</a>. Download the executable archive, and download the mouse dataset from <a href="https://elki-project.github.io/datasets/">https://elki-project.github.io/datasets/</a>.</p>
<p>From the Terminal or Command Prompt, run the following command:</p>
<pre><strong>$ java -jar elki-bundle-0.7.1.jar</strong> </pre>
<p>The preceding command generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-550 image-border" src="Images/ad7aa6a0-9316-4d68-ac6c-3fd4ad9f7bba.png" style="width:66.67em;height:46.67em;" width="800" height="560"/></p>
<p>We can see two options, in an orange color: <kbd>dbc.in</kbd> and <kbd>algorithm</kbd>. We need to specify the value. In <kbd>dbc.in</kbd>, click on the dots (<kbd>...</kbd>) to select the <kbd>mouse.csv</kbd> file that we downloaded. In <kbd>algorithm</kbd>, select <kbd>k-Mean Clustering algorithm</kbd> by clicking on the plus sign (<kbd>+</kbd>), find <kbd>kmean.k</kbd>, and fill it with the value <kbd>3</kbd>. Click on the <span class="packt_screen">Run Task</span> button, which is now enabled. It will generate the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-781 image-border" src="Images/ff63ada1-1a48-4ca8-bbfd-e32a2e6e34f6.png" style="width:162.50em;height:82.17em;" width="1950" height="986"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned how to implement basic machine learning tasks with Weka: classification, regression, and clustering. We briefly discussed the attribute selection process and trained models and evaluated their performance.</p>
<p>The next chapter will focus on how to apply these techniques to solving real-life problems, such as customer retention.</p>


            </article>

            
        </section>
    </div>



  </body></html>