<html><head></head><body>
<div class="book" title="Chapter&#xA0;6.&#xA0;Boosting Refinements" id="1AT9A1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06" class="calibre1"/>Chapter 6. Boosting Refinements</h1></div></div></div><p class="calibre7">In the previous chapter, we learned about the boosting algorithm. We looked at the algorithm in its structural form, illustrated with a numerical example, and then applied the algorithm to regression and classification problems. In this brief chapter, we will cover some theoretical aspects of the boosting algorithm and its underpinnings. The boosting theory is also important here.</p><p class="calibre7">In this chapter, we will also look at why the boosting algorithm works from a few different perspectives. Different classes of problems require different types of loss functions in order for the boosting techniques to be effective. In the next section, we will explore the different kinds of loss functions that we can choose from. The extreme gradient boosting method is outlined in the section dedicated to working with the <code class="literal">xgboost</code> package. Furthermore, the <code class="literal">h2o</code> package will ultimately be discussed in the final section, and this might be useful for other ensemble methods too. The chapter will cover the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Why does boosting work?</li><li class="listitem">The <code class="literal">gbm</code> package</li><li class="listitem">The <code class="literal">xgboost</code> package</li><li class="listitem">The <code class="literal">h2o</code> package</li></ul></div></div>

<div class="book" title="Chapter&#xA0;6.&#xA0;Boosting Refinements" id="1AT9A1-2006c10fab20488594398dc4871637ee">
<div class="book" title="Technical requirements"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch06lvl1sec48" class="calibre1"/>Technical requirements</h1></div></div></div><p class="calibre7">We will be using the following R libraries in this chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="literal">adabag</code></li><li class="listitem"><code class="literal">gbm</code></li><li class="listitem"><code class="literal">h2o</code></li><li class="listitem"><code class="literal">kernlab</code></li><li class="listitem"><code class="literal">rpart</code></li><li class="listitem"><code class="literal">xgboost</code></li></ul></div></div></div>
<div class="book" title="Why does boosting work?"><div class="book" id="1BRPS2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec49" class="calibre1"/>Why does boosting work?</h1></div></div></div><p class="calibre7">The <span class="strong"><em class="calibre9">Adaptive boosting algorithm</em></span> section in the previous chapter contained <span class="strong"><em class="calibre9">m </em></span>models, classifiers <span class="strong"><img src="../images/00282.jpeg" alt="Why does boosting work?" class="calibre15"/></span>, <span class="strong"><em class="calibre9">n </em></span>observations and weights, and a voting power that is determined sequentially. The<a id="id246" class="calibre1"/> adaptation of the adaptive boosting method was illustrated using a toy example, and then applied using specialized functions. When compared with the bagging and random forest methods, we found that boosting provides the highest accuracy, which you may remember from the results in the aforementioned section in<a id="id247" class="calibre1"/> the previous chapter. However, the implementation of the algorithm does not tell us why it was expected to perform better.</p><p class="calibre7">We don't have a universally accepted answer on why boosting works, but according to subsection 6.2.2 of Berk (2016), there are three possible explanations:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Boosting is a margin maximizer</li><li class="listitem">Boosting is a statistical optimizer</li><li class="listitem">Boosting is an interpolator</li></ul></div><p class="calibre7">But what do these actually mean? We will now cover each of these points one by one. The margin for an observation in a boosting algorithm is calculated as follows:</p><div class="mediaobject"><img src="../images/00283.jpeg" alt="Why does boosting work?" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">We can see that the margin is the difference between the sum of the passes over the correct classifications and incorrect classifications. In the preceding formula, the quantity <span class="strong"><img src="../images/00284.jpeg" alt="Why does boosting work?" class="calibre15"/></span> denotes the voting power. The reason why the boosting algorithm works so well, especially for the classification problem, is because it is a <span class="strong"><em class="calibre9">margin maximizer</em></span>. In their ground-breaking paper, Schapire et al., the inventors of the boosting algorithm, claim that boosting is particularly good at finding classifiers with large margins in that it concentrates on those examples whose margins are small (or negative) and forces the base learning algorithm to generate good classifications for those examples. The bold section of this quote is what will be illustrated in the next R code block.</p><p class="calibre7">The spam dataset from the <code class="literal">kernlab</code> package will be used for illustrating this key idea. The <code class="literal">boosting</code> function from the <code class="literal">gbm</code> package will fit on the data to distinguish between the spam emails and the good ones. We will begin an initial model with a single iteration only, accessing the accuracy, obtaining the margins, and then producing the summary in the following code block:</p><div class="informalexample"><pre class="programlisting">&gt; data("spam")
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(spam),replace = TRUE,prob = c(0.7,0.3))
&gt; spam_Train &lt;- spam[Train_Test=="Train",]
&gt; spam_Formula &lt;- as.formula("type~.")
&gt; spam_b0 &lt;- boosting(spam_Formula,data=spam_Train,mfinal=1)
&gt; sum(predict(spam_b0,newdata=spam_Train)$class==
+       spam_Train$type)/nrow(spam_Train)
[1] 0.905
&gt; mb0 &lt;- margins(spam_b0,newdata=spam_Train)$margins
&gt; mb0[1:20]
 [1]  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
&gt; summary(mb0)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 -1.000   1.000   1.000   0.809   1.000   1.000 </pre></div><p class="calibre7">Next, we will increase the <a id="id248" class="calibre1"/>number of iterations to 5, 10, 20, 50, and 200. The reader should track the accuracy and the summary of the margins as they hover over the following results:</p><div class="informalexample"><pre class="programlisting">&gt; spam_b1 &lt;- boosting(spam_Formula,data=spam_Train,mfinal=5)
&gt; sum(predict(spam_b1,newdata=spam_Train)$class==
+       spam_Train$type)/nrow(spam_Train)
[1] 0.948
&gt; mb1 &lt;- margins(spam_b1,newdata=spam_Train)$margins
&gt; mb1[1:20]
 [1]  1.0000 -0.2375  0.7479 -0.2375  0.1771  0.5702  0.6069  0.7479  1.0000
[10]  0.7479  1.0000  1.0000 -0.7479  1.0000  0.7479  1.0000  0.7479  1.0000
[19] -0.0146  1.0000
&gt; summary(mb1)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 -1.000   0.631   1.000   0.783   1.000   1.000 
&gt; spam_b2 &lt;- boosting(spam_Formula,data=spam_Train,mfinal=10)
&gt; sum(predict(spam_b2,newdata=spam_Train)$class==
+       spam_Train$type)/nrow(spam_Train)
[1] 0.969		
&gt; mb2 &lt;- margins(spam_b2,newdata=spam_Train)$margins
&gt; mb2[1:20]
 [1]  0.852  0.304  0.245  0.304  0.288  0.629  0.478  0.678  0.827  0.678
[11]  1.000  1.000 -0.272  0.517  0.700  0.517  0.700  0.478  0.529  0.852
&gt; summary(mb2)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 -0.517   0.529   0.807   0.708   1.000   1.000 
&gt; spam_b3 &lt;- boosting(spam_Formula,data=spam_Train,mfinal=20)
&gt; sum(predict(spam_b3,newdata=spam_Train)$class==
+       spam_Train$type)/nrow(spam_Train)
[1] 0.996
&gt; mb3 &lt;- margins(spam_b3,newdata=spam_Train)$margins
&gt; mb3[1:20]
 [1] 0.5702 0.3419 0.3212 0.3419 0.3612 0.6665 0.4549 0.7926 0.7687 0.6814
[11] 0.8958 0.5916 0.0729 0.6694 0.6828 0.6694 0.6828 0.6130 0.6813 0.7467
&gt; summary(mb3)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 -0.178   0.537   0.719   0.676   0.869   1.000 
&gt; spam_b4&lt;- boosting(spam_Formula,data=spam_Train,mfinal=50)
&gt; sum(predict(spam_b4,newdata=spam_Train)$class==
+       spam_Train$type)/nrow(spam_Train)
[1] 1
&gt; mb4 &lt;- margins(spam_b4,newdata=spam_Train)$margins
&gt; mb4[1:20]
 [1] 0.407 0.333 0.386 0.333 0.379 0.518 0.486 0.536 0.579 0.647 0.695 0.544
[13] 0.261 0.586 0.426 0.586 0.426 0.488 0.572 0.677
&gt; summary(mb4)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.098   0.444   0.590   0.586   0.729   1.000 
&gt; spam_b5&lt;- boosting(spam_Formula,data=spam_Train,mfinal=200)
&gt; sum(predict(spam_b5,newdata=spam_Train)$class==
+       spam_Train$type)/nrow(spam_Train)
[1] 1
&gt; mb5 &lt;- margins(spam_b5,newdata=spam_Train)$margins
&gt; mb5[1:20]
 [1] 0.386 0.400 0.362 0.368 0.355 0.396 0.368 0.462 0.489 0.491 0.700 0.486
[13] 0.317 0.426 0.393 0.426 0.393 0.385 0.624 0.581
&gt; summary(mb5)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.272   0.387   0.482   0.510   0.607   0.916 </pre></div><p class="calibre7">The first important difference is that the <a id="id249" class="calibre1"/>margins have moved completely away from negative numbers, and each of them is non-negative after the number of iterations reaches 50 or more. To get a clearer picture of this, we will column-bind the margins and then look at all of the observations that had a negative margin upon initialization:</p><div class="informalexample"><pre class="programlisting">&gt; View(cbind(mb1,mb2,mb3,mb4,mb5)[mb1&lt;0,])</pre></div><p class="calibre7">The following snapshot shows the result of the preceding code:</p><div class="mediaobject"><img src="../images/00285.jpeg" alt="Why does boosting work?" class="calibre10"/><div class="caption"><p class="calibre14">Figure 1: Margins of misclassified observations over the iterations</p></div></div><p class="calibre11"> </p><p class="calibre7">Thus, we can clearly see that the margins increase as the number of iterations increases.</p><p class="calibre7">The second point to bear in<a id="id250" class="calibre1"/> mind about boosting, especially adaptive boosting, is that it is a statistical optimizer. Pages 264–5 of Berk (2016) and 25–6 of Zhou (2012) show that boosting ensembles achieves the Bayes error rate. This means that since the exponential loss is minimized, the classification error rate is also minimized.</p><p class="calibre7">The third point about boosting being an interpolator is straightforward. It is evident that the iterations of boosting can be seen as the weighted averaging of a random forest.</p><p class="calibre7">Up to this point, the boosting methods have<a id="id251" class="calibre1"/> only addressed classification and regression problems. The loss function is central for a machine learning algorithm, and the next section will discuss a variety of loss functions that will help in setting the boosting algorithm for different formats of data.</p></div>

<div class="book" title="The gbm package"><div class="book" id="1CQAE2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec50" class="calibre1"/>The gbm package</h1></div></div></div><p class="calibre7">The R <code class="literal">gbm</code> package, created by <a id="id252" class="calibre1"/>Greg Ridgeway, is a very versatile package. The details of this package can be found at <a class="calibre1" href="http://www.saedsayad.com/docs/gbm2.pdf">http://www.saedsayad.com/docs/gbm2.pdf</a>. The document details the theoretical aspects of the gradient boosting and illustrates <a id="id253" class="calibre1"/>various other parameters of the <code class="literal">gbm</code> function. First, we will consider the shrinkage factor available in the <code class="literal">gbm</code> function.</p><p class="calibre7">Shrinkage parameters are very important, and also help with the problem of overfitting. Penalization is achieved through this option. For the spam dataset, we will set the shrinkage option to 0.1 (very large) and 0.0001 (very small) and also look at how the performance is affected:</p><div class="informalexample"><pre class="programlisting">&gt; spam_Train2 &lt;- spam_Train
&gt; spam_Train2$type &lt;- as.numeric(spam_Train2$type)-1
&gt; spam_gbm &lt;- gbm(spam_Formula,distribution="bernoulli",
+       data=spam_Train2, n.trees=500,bag.fraction = 0.8,
+       shrinkage = 0.1)
&gt; plot(spam_gbm) # output suppressed
&gt; summary(spam_gbm)
                                var      rel.inf
charExclamation     charExclamation 21.740302311
charDollar               charDollar 18.505561199
remove                       remove 11.722965305
your                           your  8.282553567
free                           free  8.142952834
hp                               hp  7.399617456


num415                       num415  0.000000000
direct                       direct  0.000000000
cs                               cs  0.000000000
original                   original  0.000000000
table                         table  0.000000000
charSquarebracket charSquarebracket  0.000000000</pre></div><p class="calibre7">The summary function also plots the relative variable importance plot. This is shown in the following screenshot:</p><div class="mediaobject"><img src="../images/00286.jpeg" alt="The gbm package" class="calibre10"/><div class="caption"><p class="calibre14">Figure 2: Relative variable influence plot</p></div></div><p class="calibre11"> </p><p class="calibre7">The details of the fitted object are <a id="id254" class="calibre1"/>obtained next:</p><div class="informalexample"><pre class="programlisting">&gt; spam_gbm
gbm(formula = spam_Formula, distribution = "bernoulli", data = spam_Train2, 
    n.trees = 500, shrinkage = 0.1, bag.fraction = 0.8)
A gradient boosted model with bernoulli loss function.
500 iterations were performed.
There were 57 predictors, of which 43 had nonzero influence.

Here, the choice of shrinkage = 0.1 leads to 43 nonzero influential variables. We can now reduce the shrinkage factor drastically and observe the impact: 

&gt; spam_gbm2 &lt;- gbm(spam_Formula,distribution="bernoulli",
+       data=spam_Train2,n.trees=500,bag.fraction = 0.8,
+       shrinkage = 0.0001)
&gt; spam_gbm2
gbm(formula = spam_Formula, distribution = "bernoulli", data = spam_Train2, 
    n.trees = 500, shrinkage = 1e-04, bag.fraction = 0.8)
A gradient boosted model with Bernoulli loss function.
500 iterations were performed.
There were 57 predictors of which 2 had nonzero influence.</pre></div><p class="calibre7">The shrinkage parameter is too low, and so almost none of the variables are influential. Next, we will generate a plot, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; windows(height=100,width=200)
&gt; par(mfrow=c(1,2))
&gt; gbm.perf(spam_gbm,plot.it=TRUE)
Using OOB method...
[1] 151
Warning message:
In gbm.perf(spam_gbm, plot.it = TRUE) :
  OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive. Using cv.folds&gt;0 when calling gbm usually results in improved predictive performance.
&gt; gbm.perf(spam_gbm2,plot.it=TRUE)
Using OOB method...
[1] 500
Warning message:
In gbm.perf(spam_gbm2, plot.it = TRUE) :
  OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive. Using cv.folds&gt;0 when calling gbm usually results in improved predictive performance.</pre></div><div class="mediaobject"><img src="../images/00287.jpeg" alt="The gbm package" class="calibre10"/><div class="caption"><p class="calibre14">Figure 3: Convergence as a factor of the shrinkage factor</p></div></div><p class="calibre11"> </p><p class="calibre7">We don't have a clear convergence for the extremely small shrinkage factor.</p><p class="calibre7">Further details of the <code class="literal">gbm</code> function can be obtained from the package documentation or the source that was provided earlier. Boosting is a very versatile technique, and Ridgeway has implemented this for the varieties of <a id="id255" class="calibre1"/>data structures. The next table lists four of the most commonly occurring data structures, showing the statistical model, the deviance (related to the loss function), the initial values, the gradient, and the estimate of the terminal node output for each one:</p><div class="informalexample"><table border="1" class="calibre16"><colgroup class="calibre17"><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/></colgroup><thead class="calibre19"><tr class="calibre20"><th valign="bottom" class="calibre21">
<p class="calibre22">Output type</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Stat model</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Deviance</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Initial value</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Gradient </p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Terminal node estimate</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre20"><td class="calibre25">
<p class="calibre22">Numeric</p>
</td><td class="calibre25">
<p class="calibre22">Gaussian</p>
</td><td class="calibre25">
<span><img src="../images/00288.jpeg" alt="The gbm package" class="calibre23"/></span>
</td><td class="calibre25">
<span><img src="../images/00289.jpeg" alt="The gbm package" class="calibre23"/></span>
</td><td class="calibre25">
<span><img src="../images/00290.jpeg" alt="The gbm package" class="calibre23"/></span>
</td><td class="calibre25">
<span><img src="../images/00291.jpeg" alt="The gbm package" class="calibre23"/></span>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">Binary</p>
</td><td class="calibre25">
<p class="calibre22">Bernoulli</p>
</td><td class="calibre25">
<span><img src="../images/00292.jpeg" alt="The gbm package" class="calibre23"/></span>
</td><td class="calibre25">
<span><img src="../images/00293.jpeg" alt="The gbm package" class="calibre23"/></span>
</td><td class="calibre25">
<span><img src="../images/00294.jpeg" alt="The gbm package" class="calibre23"/></span>
</td><td class="calibre25">
<span><img src="../images/00295.jpeg" alt="The gbm package" class="calibre23"/></span>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">Count</p>
</td><td class="calibre25">
<p class="calibre22">Poisson</p>
</td><td class="calibre25">
<span><img src="../images/00296.jpeg" alt="The gbm package" class="calibre23"/></span>
</td><td class="calibre25">
<span><img src="../images/00297.jpeg" alt="The gbm package" class="calibre23"/></span>
</td><td class="calibre25">
<span><img src="../images/00298.jpeg" alt="The gbm package" class="calibre23"/></span>
</td><td class="calibre25">
<span><img src="../images/00299.jpeg" alt="The gbm package" class="calibre23"/></span>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">Survival data</p>
</td><td class="calibre25">
<p class="calibre22">Cox proportional hazards model</p>
</td><td class="calibre25">
<span><img src="../images/00300.jpeg" alt="The gbm package" class="calibre23"/></span>
</td><td class="calibre25">
<span><img src="../images/00301.jpeg" alt="The gbm package" class="calibre23"/></span>
</td><td class="calibre25">
<p class="calibre22">0</p>
</td><td class="calibre25">
<p class="calibre22">Newton–Raphson algorithm</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote1"><p class="calibre27">Table 1: GBM boosting options</p></blockquote></div><p class="calibre7">We will apply the <code class="literal">gbm</code> function for the count data and the survival data.</p></div>

<div class="book" title="The gbm package">
<div class="book" title="Boosting for count data"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec31" class="calibre1"/>Boosting for count data</h2></div></div></div><p class="calibre7">The number of accidents, mistakes/typographical errors, births, and so on are popular examples of count data. Here, we <a id="id256" class="calibre1"/>count the number of incidents over a particular time, place, and/or space. Poisson distribution is very popular for modeling count data. When we have additional information in the form of covariates and independent variables, the related regression problem is often of interest. The generalized linear model is a popular technique for modeling the count data.</p><p class="calibre7">Let's look at a simulated dataset available at <a class="calibre1" href="https://stats.idre.ucla.edu/r/dae/poisson-regression/">https://stats.idre.ucla.edu/r/dae/poisson-regression/</a>. The necessary changes are made as indicated<a id="id257" class="calibre1"/> in this source. First, we fit the Poisson regression model using the <code class="literal">glm</code> function. Next, we fit the boosting model, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; # Poisson regression and boosting
&gt; # https://stats.idre.ucla.edu/r/dae/poisson-regression/
&gt; pregnancy &lt;- read.csv("https://stats.idre.ucla.edu/stat/data/poisson_sim.csv")
&gt; pregnancy &lt;- within(pregnancy, {
+   prog &lt;- factor(prog, levels=1:3, 
+   labels=c("General", "Academic","Vocational"))
+   id &lt;- factor(id)
+ })
&gt; summary(pregnancy)
       id        num_awards           prog          math     
 1      :  1   Min.   :0.00   General   : 45   Min.   :33.0  
 2      :  1   1st Qu.:0.00   Academic  :105   1st Qu.:45.0  
 3      :  1   Median :0.00   Vocational: 50   Median :52.0  
 4      :  1   Mean   :0.63                    Mean   :52.6  
 5      :  1   3rd Qu.:1.00                    3rd Qu.:59.0  
 6      :  1   Max.   :6.00                    Max.   :75.0  
 (Other):194                                                 
&gt; pregnancy_Poisson &lt;- glm(num_awards ~ prog + math, 
+                     family="poisson", data=pregnancy)
&gt; summary(pregnancy_Poisson)

Call:
glm(formula = num_awards ~ prog + math, family = "poisson", data = pregnancy)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-2.204  -0.844  -0.511   0.256   2.680  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -5.2471     0.6585   -7.97  1.6e-15 ***
progAcademic     1.0839     0.3583    3.03   0.0025 ** 
progVocational   0.3698     0.4411    0.84   0.4018    
math             0.0702     0.0106    6.62  3.6e-11 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 287.67  on 199  degrees of freedom
Residual deviance: 189.45  on 196  degrees of freedom
AIC: 373.5

Number of Fisher Scoring iterations: 6

&gt; pregnancy_boost &lt;- gbm(num_awards ~ prog+math,dist="poisson",
+ n.trees=100,interaction.depth = 2,shrinkage=0.1,data=pregnancy)
&gt; cbind(pregnancy$num_awards,predict(m1,type="response"),
+       predict(pboost,n.trees=100,type="response"))
    [,1]   [,2]   [,3]
1      0 0.1352 0.1240
2      0 0.0934 0.1072
3      0 0.1669 0.3375
4      0 0.1450 0.0850
5      0 0.1260 0.0257
6      0 0.1002 0.0735

195    1 1.0469 1.4832
196    2 2.2650 2.0241
197    2 1.4683 0.4047
198    1 2.2650 2.0241
199    0 2.4296 2.0241
200    3 2.6061 2.0241
&gt; sum((pregnancy$num_awards-predict(m1,type="response"))^2)
[1] 151
&gt; sum((pregnancy$num_awards-predict(pboost,n.trees=100,
+    type="response"))^2)
[1] 141
&gt; summary(pregnancy_boost)
      var rel.inf
math math    89.7
prog prog    10.3</pre></div><p class="calibre7">This has been a very simple example with only two covariates. However, in practice, we have seen that the count data is<a id="id258" class="calibre1"/> treated as the regression problem many times. This is unfortunate, and the general regression technique is not any sort of alchemy. The data structure must be respected and the count data analysis needs to be carried out. Note that the model fitted here is nonlinear. Though the benefit is not apparent here, it is natural that as the number of variables increases, the count data framework becomes more appropriate. We will close this discussion of count data analysis with a plot of the variable importance of two trees, as well as a tabular display:</p><div class="mediaobject"><img src="../images/00302.jpeg" alt="Boosting for count data" class="calibre10"/><div class="caption"><p class="calibre14">Figure 4: Boosting count data – variable importance</p></div></div><p class="calibre11"> </p><div class="informalexample"><pre class="programlisting">&gt; pretty.gbm.tree(pregnancy_boost,i.tree=18)
  SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight
0        1      64.50000        1         5           6          10.41    100
1        1      57.50000        2         3           4           1.14     90
2       -1      -0.01146       -1        -1          -1           0.00     71
3       -1       0.02450       -1        -1          -1           0.00     19
4       -1      -0.00387       -1        -1          -1           0.00     90
5       -1       0.05485       -1        -1          -1           0.00     10
6       -1       0.00200       -1        -1          -1           0.00    100
  Prediction
0    0.00200
1   -0.00387
2   -0.01146
3    0.02450
4   -0.00387
5    0.05485
6    0.00200
&gt; pretty.gbm.tree(pregnancy_boost,i.tree=63)
  SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight
0        1      60.50000        1         5           6          3.837    100
1        0      20.00000        2         3           4          0.407     79
2       -1      -0.00803       -1        -1          -1          0.000     40
3       -1       0.05499       -1        -1          -1          0.000     39
4       -1       0.02308       -1        -1          -1          0.000     79
5       -1       0.02999       -1        -1          -1          0.000     21
6       -1       0.02453       -1        -1          -1          0.000    100
  Prediction
0    0.02453
1    0.02308
2   -0.00803
3    0.05499
4    0.02308
5    0.02999
6    0.02453</pre></div><p class="calibre7">The <code class="literal">pretty.gbm.tree</code> function helps in <a id="id259" class="calibre1"/>extracting the hidden trees of the <code class="literal">gbm</code> objects. In the next section, we will deal with a gradient boosting technique for survival data.</p></div></div>

<div class="book" title="The gbm package">
<div class="book" title="Boosting for survival data"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec32" class="calibre1"/>Boosting for survival data</h2></div></div></div><p class="calibre7">The <code class="literal">pbc</code> dataset has already been introduced in <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapters 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>, and <a class="calibre1" title="Chapter 2. Bootstrapping" href="part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee">Chapter 2</a>, <span class="strong"><em class="calibre9">Bootstrapping</em></span>. As seen earlier, the survival data has incomplete observations, and we need specialized techniques for this. In Table 1, we saw that the deviance<a id="id260" class="calibre1"/> function is quite complex. Thanks to Ridgeway, we don't have to worry much about such computations. Instead, we simply use the <code class="literal">gbm</code> function with the option of <code class="literal">dist="coxph"</code> and carry out the analyses as follows:</p><div class="informalexample"><pre class="programlisting">&gt; # Survival data
&gt; pbc_boost &lt;- gbm(Surv(time,status==2)~trt + age + sex+ascites +
+                    hepato + spiders + edema + bili + chol + 
+                    albumin + copper + alk.phos + ast + trig + 
+                    platelet + protime + stage,
+                  n.trees=100,interaction.depth = 2,
+                  shrinkage=0.01,dist="coxph",data=pbc)
&gt; summary(pbc_boost)
              var rel.inf
bili         bili  54.220
age           age  10.318
protime   protime   9.780
stage       stage   7.364
albumin   albumin   6.648
copper     copper   5.899
ascites   ascites   2.361
edema       edema   2.111
ast           ast   0.674
platelet platelet   0.246
alk.phos alk.phos   0.203
trig         trig   0.178
trt           trt   0.000
sex           sex   0.000
hepato     hepato   0.000
spiders   spiders   0.000
chol         chol   0.000
&gt; pretty.gbm.tree(pbc_boost,i.tree=2)  # output suppressed
&gt; pretty.gbm.tree(pbc_boost,i.tree=72) # output suppressed</pre></div><p class="calibre7">Hence, using the versatile <code class="literal">gbm</code> function, we<a id="id261" class="calibre1"/> can easily carry out the gradient boosting technique for a variety of data structures.</p></div></div>
<div class="book" title="The xgboost package"><div class="book" id="1DOR02-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec51" class="calibre1"/>The xgboost package</h1></div></div></div><p class="calibre7">The <code class="literal">xgboost</code> R package is an optimized, distributed implementation<a id="id262" class="calibre1"/> of the gradient boosting <a id="id263" class="calibre1"/>method. This is an engineering optimization that is known to be efficient, flexible, and portable—see <a class="calibre1" href="https://github.com/dmlc/xgboost">https://github.com/dmlc/xgboost</a> for more details and regular updates. This provides parallel tree boosting, and therefore has been found to be immensely useful in the data science community. This is especially<a id="id264" class="calibre1"/> the case given that a great fraction of the competition winners at <a class="calibre1" href="http://www.kaggle.org">www.kaggle.org</a> use the <code class="literal">xgboost</code> technique. A partial list of Kaggle winners is available at <a class="calibre1" href="https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions">https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions</a>.</p><p class="calibre7">The main advantages of the extreme gradient boosting implementation are shown in the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre8">Parallel computing</strong></span>: This<a id="id265" class="calibre1"/> package is enabled with parallel processing using OpenMP, which then uses all the cores of the computing machine</li><li class="listitem"><span class="strong"><strong class="calibre8">Regularization</strong></span>: This helps in <a id="id266" class="calibre1"/>circumventing the problem of overfitting by incorporating the regularization ideas</li><li class="listitem"><span class="strong"><strong class="calibre8">Cross-validation</strong></span>: No extra coding is required for<a id="id267" class="calibre1"/> carrying out cross-validation</li><li class="listitem"><span class="strong"><strong class="calibre8">Pruning</strong></span>: This grows the tree <a id="id268" class="calibre1"/>up to the maximum depth and then prunes backward</li><li class="listitem"><span class="strong"><strong class="calibre8">Missing values</strong></span>: Missing values<a id="id269" class="calibre1"/> are internally handled</li><li class="listitem"><span class="strong"><strong class="calibre8">Saving and reloading</strong></span>: This has<a id="id270" class="calibre1"/> features that not only help in saving an existing model, but can also continue the iterations from the step where it was last stopped</li><li class="listitem"><span class="strong"><strong class="calibre8">Cross platform</strong></span>: This is available<a id="id271" class="calibre1"/> for Python, Scala, and so on</li></ul></div><p class="calibre7">We will illustrate these ideas with the spam <a id="id272" class="calibre1"/>dataset that we saw earlier in the book. The functions of the <code class="literal">xgboost</code> package require all the variables to be numeric, and the output should also be labelled as <code class="literal">0</code> and <code class="literal">1</code>. Furthermore, the covariate matrix and the output need to be given to the <code class="literal">xgboost</code> R package separately. As a result, we will first load the spam dataset and then create the partitions and the formula, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; ## The xgboost Package
&gt; data("spam")
&gt; spam2 &lt;- spam
&gt; spam2$type &lt;- as.numeric(spam2$type)-1
&gt; head(data.frame(spam2$type,spam$type))
  spam2.type spam.type
1          1      spam
2          1      spam
3          1      spam
4          1      spam
5          1      spam
6          1      spam
&gt; # 1 denotes spam, and 0 - nonspam
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(spam2),replace = TRUE,
+                      prob = c(0.7,0.3))
&gt; spam2_Train &lt;- spam2[Train_Test=="Train",]
&gt; spam_Formula &lt;- as.formula("type~.")
&gt; spam_train &lt;- list()</pre></div><p class="calibre7">The <code class="literal">xgboost</code> package also requires the training regression data to be specified in a special <code class="literal">dgCMatrix</code> matrix. Thus, we can convert it using the <code class="literal">as</code> function:</p><div class="informalexample"><pre class="programlisting">&gt; spam_train$data &lt;- as(spam_train$data,"dgCMatrix")
&gt; spam_train$label &lt;- spam2_Train$type
&gt; class(spam_train$data)
[1] "dgCMatrix"
attr(,"package")
[1] "Matrix"</pre></div><p class="calibre7">We now have the infrastructure<a id="id273" class="calibre1"/> ready for applying the <code class="literal">xgboost</code> function. The option of <code class="literal">nrounds=100</code> and the logistic function are chosen, and the results are obtained as follows:</p><div class="informalexample"><pre class="programlisting">&gt; # Simple XGBoosting
&gt; spam_xgb&lt;- xgboost(data=spam_train$data,label=spam_train$label,+                     nrounds = 100,objective="binary:logistic")
[1]	train-error:0.064062 
[2]	train-error:0.063437 
[3]	train-error:0.053438 
[4]	train-error:0.050313 
[5]	train-error:0.047812 
[6]	train-error:0.045313 

[95]	train-error:0.002188 
[96]	train-error:0.001875 
[97]	train-error:0.001875 
[98]	train-error:0.001875 
[99]	train-error:0.000937 
[100]	train-error:0.000937 </pre></div><p class="calibre7">Using the fitted boosting model, we now apply the <code class="literal">predict</code> function and assess the accuracy:</p><div class="informalexample"><pre class="programlisting">&gt; xgb_predict &lt;- predict(spam_xgb,spam_train$data)
&gt; sum(xgb_predict&gt;0.5)
[1] 1226
&gt; sum(spam_train$label)
[1] 1229
&gt; table(spam_train$label,c(xgb_predict&gt;0.5))
   
    FALSE TRUE
  0  1971    0
  1     3 1226</pre></div><p class="calibre7">If the probability (and the response, in this case) of the observation being marked 1 is more than 0.5, then we can label the<a id="id274" class="calibre1"/> observation as 1, and 0 otherwise. The predicted label and actual label contingency table is obtained by using the table R function. Clearly, we have very good accuracy, and there are only three misclassifications.</p><p class="calibre7">We claimed that the <code class="literal">xgboost</code> package does not require extra coding for the cross-validation analysis. The <code class="literal">xgb.cv</code> function is useful here, and it works with the same arguments as the <code class="literal">xgboost</code> function with the cross-validation folds specified by the <code class="literal">nfold</code> option. Here, we choose <code class="literal">nfold=10</code>. Now, using the <code class="literal">xgb.cv</code> function, we carry out the analysis and assess the prediction accuracy:</p><div class="informalexample"><pre class="programlisting">&gt; # XGBoosting with cross-validation
&gt; spam_xgb_cv &lt;- xgb.cv(data=spam_train$data,
+           label=spam_train$label,nfold=10,nrounds = 100,
+           objective="binary:logistic",prediction = TRUE)
[1]	train-error:0.064410+0.001426	test-error:0.091246+0.01697
[2]	train-error:0.058715+0.001862	test-error:0.082805+0.01421
[3]	train-error:0.052986+0.003389	test-error:0.077186+0.01472
[4]	train-error:0.049826+0.002210	test-error:0.073123+0.01544
[5]	train-error:0.046910+0.001412	test-error:0.070937+0.01340
[6]	train-error:0.043958+0.001841	test-error:0.066249+0.01346

[95]	train-error:0.001667+0.000340	test-error:0.048119+0.00926
[96]	train-error:0.001528+0.000318	test-error:0.047181+0.01008
[97]	train-error:0.001458+0.000260	test-error:0.046868+0.00974
[98]	train-error:0.001389+0.000269	test-error:0.047181+0.00979
[99]	train-error:0.001215+0.000233	test-error:0.047182+0.00969
[100]	train-error:0.001111+0.000260	test-error:0.045932+0.01115
&gt; xgb_cv_predict &lt;- spam_xgb_cv$pred
&gt; sum(xgb_cv_predict&gt;0.5)
[1] 1206
&gt; table(spam_train$label,c(xgb_cv_predict&gt;0.5))
   
    FALSE TRUE
  0  1909   62
  1    85 1144</pre></div><p class="calibre7">The cross-validation analysis shows that the accuracy has decreased. This is an indication that we had an overfitting problem with the <code class="literal">xgboost</code> function. We will now look at the other features of the <code class="literal">xgboost</code> package. At the beginning of this section, we claimed that the technique allowed flexibility through early stoppings and also the resumption of earlier fitted model objects.</p><p class="calibre7">However, an important question is <span class="strong"><em class="calibre9">when do you need to stop the iterations early?</em></span> We don't have any underlying theory <a id="id275" class="calibre1"/>with regards to the number of iterations that are required as a function of the number of variables and the number of observations. Consequently, we will kick off the proceedings with a specified number of iterations. If the convergence of error reduction does not fall below the threshold level, then we will continue with more iterations, and this task will be taken up next. However, if the specified number of iterations is way too high and the performance of the boosting method is getting worse, then we will have to stop the iterations. This is achieved by specifying the <code class="literal">early_stopping_rounds</code> option, which we will put into action in the following code:</p><div class="informalexample"><pre class="programlisting">&gt; # Stop early
&gt; spam_xgb_cv2 &lt;- xgb.cv(data=spam_train$data,label=
+             spam_train$label, early_stopping_rounds = 5,
+             nfold=10,nrounds = 100,objective="binary:logistic",
+             prediction = TRUE)
[1]	train-error:0.064271+0.002371	test-error:0.090294+0.02304
Multiple eval metrics are present. Will use test_error for early stopping.
Will train until test_error hasn't improved in 5 rounds.

[2]	train-error:0.059028+0.003370	test-error:0.085614+0.01808
[3]	train-error:0.052048+0.002049	test-error:0.075930+0.01388
[4]	train-error:0.049236+0.002544	test-error:0.072811+0.01333
[5]	train-error:0.046007+0.002775	test-error:0.070622+0.01419
[6]	train-error:0.042882+0.003065	test-error:0.066559+0.01670

[38]	train-error:0.010382+0.001237	test-error:0.048121+0.01153[39]	train-error:0.010069+0.001432	test-error:0.048434+0.01162
[40]	train-error:0.009653+0.001387	test-error:0.048435+0.01154
[41]	train-error:0.009236+0.001283	test-error:0.048435+0.01179
[42]	train-error:0.008924+0.001173	test-error:0.048748+0.01154
Stopping. Best iteration:
[37]	train-error:0.010625+0.001391	test-error:0.048121+0.01162</pre></div><p class="calibre7">Here, the best iteration has already occurred at number <code class="literal">37</code>, and the confirmation of this is obtained five iterations down the line, thanks to the <code class="literal">early_stopping_rounds = 5</code> option. Now that we've found the best iteration, we stop the process.</p><p class="calibre7">We will now look at how to <a id="id276" class="calibre1"/>add more iterations. This coding is for illustration purposes only. Using the option of <code class="literal">nrounds = 10</code>, and the earlier fitted <code class="literal">spam_xgb</code>, along with the options of data and label, we will ask the <code class="literal">xgboost</code> function to perform ten more iterations:</p><div class="informalexample"><pre class="programlisting">&gt; # Continue training
&gt; xgboost(xgb_model=spam_xgb,
+         data=spam_train$data,label=spam_train$label,
+         nrounds = 10)
[101]	train-error:0.000937 
[102]	train-error:0.000937 
[103]	train-error:0.000937 
[104]	train-error:0.000937 
[105]	train-error:0.000937 
[106]	train-error:0.000937 
[107]	train-error:0.000937 
[108]	train-error:0.000937 
[109]	train-error:0.000937 
[110]	train-error:0.000937 
##### xgb.Booster
raw: 136 Kb 
call:
  xgb.train(params = params, data = dtrain, nrounds = nrounds, 
    watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, maximize = maximize, save_period = save_period, save_name = save_name, xgb_model = xgb_model, callbacks = callbacks)
params (as set within xgb.train):
  silent = "1"
xgb.attributes:
  niter
callbacks:
  cb.print.evaluation(period = print_every_n)
  cb.evaluation.log()
  cb.save.model(save_period = save_period, save_name = save_name)
niter: 110
evaluation_log:
    iter train_error
       1    0.064062
       2    0.063437
---                 
     109    0.000937
     110    0.000937</pre></div><p class="calibre7">The bold and large font of the iteration number is not the format thrown out by the R software. This change has been made to emphasize the fact that the number of iterations from the earlier fitted <code class="literal">spam_xgb</code> object will now continue from <code class="literal">101</code> and will go up to <code class="literal">110</code>. Adding additional iterations is easily <a id="id277" class="calibre1"/>achieved with the <code class="literal">xgboost</code> function.</p><p class="calibre7">The <code class="literal">xgb.plot.importance</code> function, working with the <code class="literal">xgb.importance</code> function, can be used to extract and display the most important variables as identified by the boosting method:</p><div class="informalexample"><pre class="programlisting">&gt; # Variable Importance
&gt; xgb.plot.importance(xgb.importance(names(spam_train$data),
+                                    spam_xgb)[1:10,])</pre></div><p class="calibre7">The result is the following plot:</p><div class="mediaobject"><img src="../images/00303.jpeg" alt="The xgboost package" class="calibre10"/><div class="caption"><p class="calibre14">Figure 5: Variable importance plot of the xgboost function</p></div></div><p class="calibre11"> </p><p class="calibre7">We have now seen the<a id="id278" class="calibre1"/> power of the <code class="literal">xgboost</code> package. Next, we will outline the capabilities of the <code class="literal">h2o</code> package.</p></div>
<div class="book" title="The h2o package" id="1ENBI1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec52" class="calibre1"/>The h2o package</h1></div></div></div><p class="calibre7">The R software <code class="literal">.exe</code> file size is 75 MB (version 3.4.1). The size of the <code class="literal">h2o</code> R package is 125 MB. This will probably indicate to you<a id="id279" class="calibre1"/> the importance of the <code class="literal">h2o</code> package. All the datasets used in this book are very limited in size, with the number of observations not exceeding 10,000. In most cases, the file size has been of a maximum of a few MB. However, the data science world works hard, and throws around files in GB, and in even higher formats. Thus, we need more capabilities, and the <code class="literal">h2o</code> package provides just that. We simply load the <code class="literal">h2o</code> package and have a peek:</p><div class="informalexample"><pre class="programlisting">&gt; library(h2o)

----------------------------------------------------------------------

Your next step is to start H2O:
    &gt; h2o.init()

For H2O package documentation, ask for help:
    &gt; ??h2o

After starting H2O, you can use the Web UI at http://localhost:54321
For more information visit http://docs.h2o.ai

----------------------------------------------------------------------


Attaching package: 'h2o'

The following objects are masked from 'package:stats':

    cor, sd, var

The following objects are masked from 'package:base':

    %*%, %in%, &amp;&amp;, ||, apply, as.factor, as.numeric, colnames,
    colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log,
    log10, log1p, log2, round, signif, trunc

Warning message:
package 'h2o' was built under R version 3.4.4 
&gt; h2o.init()

H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    C:\Users\tprabhan\AppData\Local\Temp\Rtmpu6f0fO/h2o_TPRABHAN_started_from_r.out
    C:\Users\tprabhan\AppData\Local\Temp\Rtmpu6f0fO/h2o_TPRABHAN_started_from_r.err

java version "1.7.0_67"
Java(TM) SE Runtime Environment (build 1.7.0_67-b01)
Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)

Starting H2O JVM and connecting: .. Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         4 seconds 449 milliseconds 
    H2O cluster version:        3.16.0.2 
    H2O cluster version age:    7 months and 7 days !!! 
    H2O cluster name:           H2O_started_from_R_TPRABHAN_saz680 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   1.76 GB 
    H2O cluster total cores:    4 
    H2O cluster allowed cores:  4 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         AutoML, Algos, Core V3, Core V4 
    R Version:                  R version 3.4.0 (2017-04-21) 

Warning message:
In h2o.clusterInfo() : 
Your H2O cluster version is too old (7 months and 7 days)!
Please download and install the latest version from http://h2o.ai/download/</pre></div><p class="calibre7">Clusters and threads help in<a id="id280" class="calibre1"/> scaling up computations. For the more enthusiastic reader, the following <a id="id281" class="calibre1"/>sources will help in using the <code class="literal">h2o</code> package:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><a class="calibre1" href="http://blog.revolutionanalytics.com/2014/04/a-dive-into-h2o.html">http://blog.revolutionanalytics.com/2014/04/a-dive-into-h2o.html</a></li><li class="listitem"><a class="calibre1" href="http://docs.h2o.ai/">http://docs.h2o.ai/</a></li><li class="listitem"><a class="calibre1" href="https://www.analyticsvidhya.com/blog/2016/05/h2o-data-table-build-models-large-data-sets/">https://www.analyticsvidhya.com/blog/2016/05/h2o-data-table-build-models-large-data-sets/</a></li></ul></div><p class="calibre7">Using the <code class="literal">gbm</code>, <code class="literal">xgboost</code>, and <code class="literal">h2o</code> packages, the reader can analyze complex and large datasets.</p></div>
<div class="book" title="Summary" id="1FLS41-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec53" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">We began this chapter by briefly thinking about why boosting works. There are three perspectives that possibly explain the success of boosting, and these were covered before we looked deeper into this topic. The <code class="literal">gbm</code> package is very powerful, and it offers different options for tuning the gradient boosting algorithm, which deals with numerous data structures. We illustrated its capabilities with the shrinkage option and applied it to the count and survival data structures. The <code class="literal">xgboost</code> package is an even more efficient implementation of the gradient boosting method. It is faster and offers other flexibilities, too. We illustrated using the <code class="literal">xgboost</code> function with cross-validation, early stopping, and continuing further iterations as required. The <code class="literal">h2o</code> package/platform helps to implement the ensemble machine learning techniques on a bigger scale. </p><p class="calibre7">In the next chapter, we will look into the details of why ensembling works. In particular, we will see why putting multiple models together is often a useful practice, and we will also explore the scenarios that we can do this in.</p></div></body></html>