<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Text Mining</h1>
            </header>

            <article>
                
<div class="packt_quote">"I think it's much more interesting to live not knowing than to have answers which might be wrong."<br/>
                                                                                                                 - Richard Feynman</div>
<p>The world is awash with textual data. If you Google, Bing, or Yahoo how much of that data is unstructured, that is, in a textual format, estimates would range from 80 to 90 percent. The real number doesn't matter. What does matter is that a large proportion of the data is in a text format. The implication is that anyone seeking to find insights in that data must develop the capability to process and analyze text.</p>
<p>When I first started out as a market researcher, I used to manually pore through page after page of moderator-led focus groups and interviews with the hope of capturing some qualitative insight an Aha! moment if you will-and then haggle with fellow team members over whether they had the same insight or not. Then, you would always have that one individual in a project who would swoop in and listen to two interviews-out of the 30 or 40 on the schedule and, alas, they had their mind made up on what was really happening in the world. Contrast that with the techniques being used now, where an analyst can quickly distill data into meaningful quantitative results, support qualitative understanding, and maybe even sway the swooper.</p>
<p>Over the last few years, I've applied the techniques discussed here to mine physician-patient interactions, understand FDA fears on prescription drug advertising, and capture patient concerns about a rare cancer, to name just a few. Using R and the methods in this chapter, you too can extract the powerful information in textual data.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Text mining framework and methods</h1>
            </header>

            <article>
                
<p>There are many different methods to use in text mining. The goal here is to provide a basic framework to apply to such an endeavor. This framework is not all-inclusive of the possible methods but will cover those that are probably the most important for the vast majority of projects that you will work on. Additionally, I will discuss the modeling methods in as succinct and clear a manner as possible, because they can get quite complicated. Gathering and compiling text data is a topic that could take up several chapters. Therefore, let's begin with the assumption that our data is available from Twitter, a customer call center, scraped off the web, or whatever, and is contained in some sort of text file or files.</p>
<p>The first task is to put the text files in one structured file referred to as a <strong>corpus</strong>. The number of documents could be just one, dozens, hundreds, or even thousands. R can handle a number of raw text files, including RSS feeds, PDF files, and MS Word documents. With the corpus created, the data preparation can begin with the text transformation.</p>
<p>The following list is comprised of probably some of the most common and useful transformations for text files:</p>
<ul>
<li>Change capital letters to lowercase</li>
<li>Remove numbers</li>
<li>Remove punctuation</li>
<li>Remove stop words</li>
<li>Remove excess whitespace</li>
<li>Word stemming</li>
<li>Word replacement</li>
</ul>
<p>In transforming the corpus, you are creating not only a more compact dataset, but also simplifying the structure in order to facilitate relationships among the words, thereby leading to an increased understanding. However, keep in mind that not all of these transformations are necessary all the time and judgment must be applied, or you can iterate to find the transformations that make the most sense.</p>
<p>By changing words to lowercase, you can prevent the improper counting of words. Say that you have a count for hockey three times and Hockey once, where it is the first word in a sentence. R will not give you a count of hockey=4, but hockey=3 and Hockey=1.</p>
<p>Removing punctuation also achieves the same purpose, but as we will see in the business case, punctuation is important if you want to split your documents by sentences.</p>
<p>In removing stop words, you are getting rid of the common words that have no value; in fact, they are detrimental to the analysis, as their frequency masks important words. Examples of stop words <em>are, and, is, the, not,</em> and <em>to</em>. Removing whitespace makes a more compact corpus by getting rid of things such as tabs, paragraph breaks, double-spacing, and so on.</p>
<p>The stemming of words can get a bit tricky and might add to your confusion because it deletes word suffixes, creating the base word, or what is known as the <strong>radical</strong>. I personally am not a big fan of stemming and the analysts I've worked with agree with that sentiment. However, you can use the stemming algorithm included in the R package, <kbd>tm</kbd>, where the function calls the <strong>porter stemming algorithm</strong> from the <kbd>SnowballC</kbd> package. An example of stemming would be where your corpus has family and families. Recall that R would count this as two separate words. By running the stemming algorithm, the stemmed word for the two instances would become <em>famili</em>. This would prevent the incorrect count, but in some cases, it can be odd to interpret and is not very visually appealing in a wordcloud for presentation purposes. In some cases, it may make sense to run your analysis with both stemmed and unstemmed words in order to see which one makes sense.</p>
<p>Probably the most optional of the transformations is to replace the words. The goal of replacement is to combine the words with a similar meaning, for example, management and leadership. You can also use it in lieu of stemming. I once examined the outcome of stemmed and unstemmed words and concluded that I could achieve a more meaningful result by replacing about a dozen words instead of stemming. </p>
<p>With the transformation of the corpus completed, the next step is to create either a <strong>Document-Term Matrix</strong> (<strong>DTM</strong>) or <strong>Term-Document Matrix</strong> (<strong>TDM</strong>). What either of these matrices does is create a matrix of word counts for each individual document in the matrix. A DTM would have the documents as rows and the words as columns, while in a TDM, the reverse is true. Text mining can be performed on either matrix.</p>
<p>With a matrix, you can begin to analyze the text by examining word counts and producing visualizations such as <kbd>wordclouds</kbd>. One can also find word associations by producing correlation lists for specific words. It also serves as a necessary data structure in order to build topic models.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Topic models</h1>
            </header>

            <article>
                
<p>Topic models are a powerful method to group documents by their main topics. Topic models allow probabilistic modeling of term frequency occurrences in documents. The fitted model can be used to estimate the similarity between documents, as well as between a set of specified keywords using an additional layer of latent variables, which are referred to as topics (Grun and Hornik, 2011). In essence, a document is assigned to a topic based on the distribution of the words in that document, and the other documents in that topic will have roughly the same frequency of words.</p>
<p>The algorithm that we will focus on is <strong>Latent Dirichlet Allocation</strong> (<strong>LDA</strong>) with Gibbs sampling, which is probably the most commonly used sampling algorithm. In building topic models, the number of topics must be determined before running the algorithm (k-dimensions). If no apriori reason for the number of topics exists, then you can build several and apply judgment and knowledge to the final selection. LDA with Gibbs sampling is quite complicated mathematically, but my intent is to provide an introduction so that you are at least able to describe how the algorithm learns to assign a document to a topic in layman terms. If you are interested in mastering the math, block out a couple of hours on your calendar and have a go at it. Excellent background material is available at <a href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf">http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf</a>.</p>
<p>LDA is a generative process, and so, the following will iterate to a steady state:</p>
<ol>
<li>For each document (<em>j</em>), there are <em>1</em> to <em>j</em> documents. We will randomly assign it a multinomial distribution (<strong>dirichlet</strong> <strong>distribution</strong>) to the topics (<em>k</em>) with <em>1</em> to <em>k</em> topics, for example, document <em>A</em> is 25 percent topic one, 25 percent topic two, and 50 percent topic three.</li>
<li>Probabilistically, for each word (<em>i</em>), there are <em>1</em> to <em>i</em> words to a topic (<em>k</em>); for example, the word <em>mean</em> has a probability of 0.25 for the topic statistics.</li>
<li>For each word(<em>i</em>) in document(<em>j</em>) and topic(<em>k</em>), calculate the proportion of words in that document assigned to that topic; note it as the probability of topic(<em>k</em>) with document(<em>j</em>), <em>p(k|j)</em>, and the proportion of word(<em>i</em>) in topic(<em>k</em>) from all the documents containing the word. Note it as the probability of word(<em>i</em>) with topic(<em>k</em>), <em>p(i|k)</em>.</li>
<li>Resample, that is, assign <em>w</em> a new <em>t</em> based on the probability that <em>t</em> contains <em>w</em>, which is based on <em>p(k|j)</em> times <em>p(i|k)</em>.</li>
<li>Rinse and repeat; over numerous iterations, the algorithm finally converges and a document is assigned a topic based on the proportion of words assigned to a topic in that document.</li>
</ol>
<p>The LDA that we will be doing assumes that the order of words and documents does not matter. There has been work done to relax these assumptions in order to build models of language generation and sequence models over time (known as <strong>dynamic topic modelling</strong>).</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Other quantitative analyses</h1>
            </header>

            <article>
                
<p>We will now shift gears to analyze text semantically based on sentences and the tagging of words based on the parts of speech, such as noun, verb, pronoun, adjective, adverb, preposition, singular, plural, and so on. Often, just examining the frequency and latent topics in the text will suffice for your analysis. However, you may find occasions when a deeper understanding of the style is required in order to compare the speakers or writers.</p>
<p>There are many methods to accomplish this task, but we will focus on the following five:</p>
<ul>
<li>Polarity (sentiment analysis)</li>
<li>Automated readability index (complexity)</li>
<li>Formality</li>
<li>Diversity</li>
<li>Dispersion</li>
</ul>
<p><strong>Polarity</strong> is often referred to as sentiment analysis, which tells you how positive or negative the text is. By analyzing polarity in R with the <kbd>qdap</kbd> package, a score will be assigned to each sentence and you can analyze the average and standard deviation of polarity by groups such as different authors, text, or topics. Different polarity dictionaries are available and <kbd>qdap</kbd> defaults to one created by Hu and Liu, 2004. You can alter or change this dictionary according to your requirements.</p>
<p>The algorithm works by first tagging the words with a positive, negative, or neutral sentiment based on the dictionary. The tagged words are then clustered based on the four words prior and two words after a tagged word, and these clusters are tagged with what are known as <strong>valence shifters</strong> (neutral, negator, amplifier, and de-amplifier). A series of weights based on their number and position are applied to both the words and clusters. This is then summed and divided by the square root of the number of words in that sentence.</p>
<p>Automated readability index is a measure of the text complexity and a reader's ability to understand. A specific formula is used to calculate this index: <em>4.71(# of characters / #of words) + 0.5(# of words / # of sentences) - 21.43</em>.</p>
<p>The index produces a number, which is a rough estimate of a student's grade level to fully comprehend. If the number is 9, then a high school freshman, aged 13 to 15, should be able to grasp the meaning of the text.</p>
<p>The formality measure provides an understanding of how a text relates to the reader or speech relates to a listener. I like to think of it as a way to understand how comfortable the person producing the text is with the audience, or an understanding of the setting where this communication takes place. If you want to experience formal text, attend a medical conference or read a legal document. Informal text is said to be contextual in nature.</p>
<p>The formality measure is called <strong>F-Measure</strong>. This measure is calculated as follows:</p>
<ul>
<li>Formal words (<em>f</em>) are nouns, adjectives, prepositions, and articles</li>
<li>Contextual words (<em>c</em>) are pronouns, verbs, adverbs, and interjections</li>
<li><em>N = sum of (f + c + conjunctions)</em></li>
<li><em>Formality Index = 50((sum of f - sum of c / N) + 1)</em></li>
</ul>
<p>This is totally irrelevant, but when I was in Iraq, one of the army generals-who shall remain nameless. I had to brief and write situation reports for was absolutely adamant that adverbs were not to be used, ever, or there would be wrath. The idea was that you can't quantify words such as highly or mostly because they mean different things to different people. Five years later, I still scour my business e-mails and PowerPoint presentations for unnecessary adverbs. Formality writ large!</p>
<p>Diversity, as it relates to text mining, refers to the number of different words used in relation to the total number of words used. This can also mean the expanse of the text producer's vocabulary or lexicon richness. The <kbd>qdap</kbd> package provides five--that's right, five--different measures of diversity: Simpson, Shannon, Collision, Bergen Parker, and Brillouin. I won't cover these five in detail but will only say that the algorithms are used not only for communication and information science retrieval, but also for biodiversity in nature.</p>
<p>Finally, dispersion, or lexical dispersion, is a useful tool in order to understand how words are spread throughout a document and serves as an excellent way to explore a text and identify patterns. The analysis is conducted by calling the specific word or words of interest, which are then produced in a plot showing when the word or words occurred in the text over time. As we will see, the <kbd>qdap</kbd> package has a built-in plotting function to analyze the text dispersion.</p>
<p>We covered a framework on text mining about how to prepare the text, count words, and create topic models and, finally, dived deep into other lexical measures. Now, let's apply all this and do some real-world text mining.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business understanding</h1>
            </header>

            <article>
                
<p>For this case study, we will take a look at president Obama's State of the Union speeches. I have no agenda here; just curious as to what can be uncovered in particular and if and how his message changed over time. Perhaps this will serve as a blueprint to analyze any politician's speech in order to prepare an opposing candidate in a debate or speech of their own. If not, so be it.</p>
<p>The two main analytical goals are to build topic models on the six State of the Union speeches and then compare the first speech in 2010 and the last in January, 2016 for sentence-based textual measures, such as sentiment and dispersion.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data understanding and preparation</h1>
            </header>

            <article>
                
<p>The primary package that we will use is <kbd>tm</kbd>, the text mining package. We will also need <kbd>SnowballC</kbd> for the stemming of the words, <kbd>RColorBrewer</kbd> for the color palettes in <kbd>wordclouds</kbd>, and the <kbd>wordcloud</kbd> package. Please ensure that you have these packages installed before attempting to load them:</p>
<pre>
    <strong>&gt; library(tm)</strong><br/>    <br/>    <strong>&gt; library(wordcloud)</strong><br/>    <br/>    <strong>&gt; library(RColorBrewer)</strong>
</pre>
<p>The data files are available for download in <a href="https://github.com/datameister66/data" target="_blank">https://github.com/datameister66/data</a>. Please ensure you put the text files into a separate directory because it will all go into our corpus for analysis.</p>
<p>Download the seven <kbd>.txt</kbd> files, for example <kbd>sou2012.txt</kbd>, into your working R directory. You can identify your current working directory and set it with these functions:</p>
<pre>
<strong>    &gt; getwd()</strong><br/>    <br/>    <strong>&gt; setwd(".../data")<br/></strong>
</pre>
<p>We can now begin to create the corpus by first creating an object with the path to the speeches and then seeing how many files are in this directory and what they are named:</p>
<pre>
    <strong>&gt; name &lt;- file.path(".../text")</strong><br/>    <br/>    <strong>&gt; length(dir(name))</strong><br/>    <strong>[1] 7</strong><br/>    <br/>    <strong>&gt; dir(name)</strong><br/>    <strong>[1] "sou2010.txt" "sou2011.txt" "sou2012.txt" "sou2013.txt"</strong><br/>    <strong>[5] "sou2014.txt" "sou2015.txt" "sou2016.txt"</strong>
</pre>
<p>We will name our corpus <kbd>docs</kbd> and create it with the <kbd>Corpus()</kbd> function, wrapped around the directory source function, <kbd>DirSource()</kbd>, which is also part of the <kbd>tm</kbd> package:</p>
<pre>
    <strong>&gt; docs &lt;- Corpus(DirSource(name))</strong><br/>    <br/><strong>    &gt; docs</strong><br/>    <strong>&lt;&lt;VCorpus&gt;&gt;</strong><br/>    <strong>Metadata:  corpus specific: 0, document level (indexed): 0</strong><br/>    <strong>Content:  documents: 7</strong>
</pre>
<div class="packt_infobox">Note that there is no <kbd>corpus</kbd> or <kbd>document level</kbd> metadata. There are functions in the <kbd>tm</kbd> package to apply things such as author's names and timestamp information, among others, at both <kbd>document level</kbd> and <kbd>corpus</kbd>. We will not utilize this for our purposes.</div>
<p>We can now begin the text transformations using the <kbd>tm_map()</kbd> function from the <kbd>tm</kbd> package. These will be the transformations that we discussed previously--lowercase letters, remove numbers, remove punctuation, remove stop words, strip out the whitespace, and stem the words:</p>
<pre>
    <br/>    <strong>&gt; docs &lt;- tm_map(docs, tolower)</strong><br/>    <br/>    <strong>&gt; docs &lt;- tm_map(docs, removeNumbers)</strong><br/>    <br/>    <strong>&gt; docs &lt;- tm_map(docs, removePunctuation)</strong><br/>    <br/>    <strong>&gt; docs &lt;- tm_map(docs, removeWords, stopwords("english"))</strong><br/>    <br/>    <strong>&gt; docs &lt;- tm_map(docs, stripWhitespace)</strong>
</pre>
<p>At this point, it is a good idea to eliminate unnecessary words. For example, during the speeches, when <kbd>Congress</kbd> applauds a statement, you will find <kbd>(Applause)</kbd> in the text. This must be removed:</p>
<pre>
    <strong>&gt; docs &lt;- tm_map(docs, removeWords, c("applause", "can", "cant", <br/>      "will",<br/>    "that", "weve", "dont", "wont", "youll", "youre"))</strong>
</pre>
<p>After completing the transformations and removal of other words, make sure that your documents are plain text, put it in a document-term matrix, and check the dimensions:</p>
<pre>
    <strong>&gt; docs = tm_map(docs, PlainTextDocument)</strong><br/>    <br/>    <strong>&gt; dtm = DocumentTermMatrix(docs)</strong><br/>    <br/>    <strong>&gt; dim(dtm)</strong><br/>    <strong>[1]    7 4738</strong>
</pre>
<p>The six speeches contain <kbd><span><span>4738</span></span></kbd> words. It is optional, but one can remove the sparse terms with the <kbd>removeSparseTerms()</kbd> function. You will need to specify a number between zero and one where the higher the number, the higher the percentage of <kbd>sparsity</kbd> in the matrix. Sparsity is the relative frequency of a term in the documents. So, if your sparsity threshold is 0.75, only terms with sparsity greater than 0.75 are removed. For us that would be <em>(1 - 0.75) * 7</em>, which is equal to 1.75.  Therefore, any term in fewer than two documents would be removed:</p>
<pre>
    <strong>&gt; dtm &lt;- removeSparseTerms(dtm, 0.75)</strong><br/>    <br/>    <strong>&gt; dim(dtm)</strong><br/>    <strong>[1]    7 2254</strong>
</pre>
<p>As we don't have the metadata on the documents, it is important to name the rows of the matrix so that we know which document is which:</p>
<pre>
    <strong>&gt; rownames(dtm) &lt;- c("2010", "2011", "2012", "2013", "2014", <br/>       "2015", "2016")</strong>
</pre>
<p>Using the <kbd>inspect()</kbd> function, you can examine the matrix. Here, we will look at the seven rows and the first five columns:</p>
<pre>
<strong>    &gt; inspect(dtm[1:7, 1:5])</strong><br/><strong>          Terms</strong><br/><strong>      Docs abandon ability able abroad absolutely</strong><br/><strong>      2010       0       1    1      2          2</strong><br/><strong>      2011       1       0    4      3          0</strong><br/><strong>      2012       0       0    3      1          1</strong><br/><strong>      2013       0       3    3      2          1</strong><br/><strong>      2014       0       0    1      4          0</strong><br/><strong>      2015       1       0    1      1          0</strong><br/><strong>      2016       0       0    1      0          0<br/>  </strong>
</pre>
<p>It appears that our data is ready for analysis, starting with looking at the word frequency counts. Let me point out that the output demonstrates why I've been trained to not favor wholesale stemming. You may be thinking that 'ability' and 'able' could be combined. If you stemmed the document you would end up with 'abl'. How does that help the analysis? I think you lose context, at least in the initial analysis. Again, I recommend applying stemming thoughtfully and judiciously.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling and evaluation</h1>
            </header>

            <article>
                
<p>Modeling will be broken into two distinct parts. The first will focus on word frequency and correlation and culminate in the building of a topic model. In the next portion, we will examine many different quantitative techniques by utilizing the power of the <kbd>qdap</kbd> package in order to compare two different speeches.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Word frequency and topic models</h1>
            </header>

            <article>
                
<p>As we have everything set up in the document-term matrix, we can move on to exploring word frequencies by creating an object with the column sums, sorted in descending order. It is necessary to use <kbd>as.matrix()</kbd> in the code to sum the columns. The default order is ascending, so putting <kbd>-</kbd> in front of <kbd>freq</kbd> will change it to descending:</p>
<pre>
    <strong>&gt; freq &lt;- colSums(as.matrix(dtm))</strong><br/>    <br/>    <strong>&gt; ord &lt;- order(-freq)</strong>
</pre>
<p>We will examine the <kbd>head</kbd> and <kbd>tail</kbd> of the object with the following code:</p>
<pre>
    <strong>&gt; freq[head(ord)]</strong><br/>        <strong>new  america  people   jobs    now  years </strong><br/><strong>        193      174     168    163    157    148</strong> <br/>    <br/>    <strong>&gt; freq[tail(ord)]</strong><br/>        <strong>wright written yearold youngest youngstown zero </strong><br/><strong>             2       2       2        2          2    2</strong>
</pre>
<p>The most frequent word is <kbd>new</kbd> and, as you might expect, the president mentions <kbd>america</kbd> frequently. Also notice how important employment is with the frequency of <kbd>jobs</kbd>. I find it interesting that he mentions Youngstown, for Youngstown, OH, a couple of times.</p>
<p>To look at the frequency of the word frequency, you can create tables, as follows:</p>
<pre>
    <strong>&gt; head(table(freq))</strong><br/>    <strong>freq</strong><br/><strong>      2   3   4   5   6  7 </strong><br/><strong>    596 354 230 141 137 89</strong><br/>    <br/>    <strong>&gt; tail(table(freq))</strong><br/>    <strong>freq</strong><br/><strong>    148 157 163 168 174 193 </strong><br/><strong>      1   1   1   1   1   1</strong>
</pre>
<p>What these tables show is the number of words with that specific frequency. So 354 words occurred three times; and one word, <kbd>new</kbd> in our case, occurred 193 times.</p>
<p>Using <kbd>findFreqTerms()</kbd>, we can see which words occurred at least <kbd>125</kbd> times:</p>
<pre>
    <strong>&gt; findFreqTerms(dtm, 125)</strong><br/>    <strong> [1] "america" "american" "americans" "jobs" "make" "new" <br/>     [7] "now"     "people"   "work"      "year" "years" </strong>
</pre>
<p>You can find associations with words by correlation with the <kbd>findAssocs()</kbd> function. Let's look at <kbd>jobs</kbd> as two examples using <kbd>0.85</kbd> as the correlation cutoff:</p>
<pre>
<strong>    <br/>    &gt; findAssocs(dtm, "jobs", corlimit = 0.85)</strong><br/><strong>    $jobs</strong><br/><strong>    colleges serve market shouldnt defense  put  tax came </strong><br/><strong>        0.97  0.91   0.89     0.88    0.87 0.87 0.87 0.86</strong>
</pre>
<p>For visual portrayal, we can produce <kbd>wordclouds</kbd> and a bar chart. We will do two <kbd>wordclouds</kbd> to show the different ways to produce them: one with a minimum frequency and the other by specifying the maximum number of words to include. The first one with minimum frequency, also includes code to specify the color. The scale syntax determines the minimum and maximum word size by frequency; in this case, the minimum frequency is <kbd>70</kbd>:</p>
<pre>
    <strong>&gt; wordcloud(names(freq), freq, min.freq = 70, scale = c(3, .5),  <br/>      colors = brewer.pal(6, "Dark2"))</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="278" width="300" class="image-border" src="assets/image_12_01.png"/></div>
<p>One can forgo all the fancy graphics, as we will in the following image, capturing the <kbd><span><span>25</span></span></kbd> most frequent words:</p>
<pre>
    <strong>&gt; wordcloud(names(freq), freq, max.words = 25)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="188" width="225" class="image-border" src="assets/image_12_02.png"/></div>
<p>To produce a bar chart, the code can get a bit complicated, whether you use base R, <kbd>ggplot2</kbd>, or <kbd>lattice</kbd>. The following code will show you how to produce a bar chart for the <kbd>10</kbd> most frequent words in base R:</p>
<pre>
<strong>    &gt; freq &lt;- sort(colSums(as.matrix(dtm)), decreasing = TRUE)<br/></strong><br/><strong>    &gt; wf &lt;- data.frame(word = names(freq), freq = freq)<br/></strong><br/><strong>    &gt; wf &lt;- wf[1:10, ]<br/></strong><br/><strong>    &gt; barplot(wf$freq, names = wf$word, main = "Word Frequency",</strong><br/><strong>     xlab = "Words", ylab = "Counts", ylim = c(0, 250))</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="198" width="430" class="image-border" src="assets/image_12_03-1.png"/></div>
<p>We will now move on to building topic models using the <kbd>topicmodels</kbd> package, which offers the <kbd>LDA()</kbd> function. The question now is how many topics to create. It seems logical to solve for three  <kbd>topics</kbd> (<kbd>k=3</kbd>). Certainly, I encourage you to try other numbers of topics:</p>
<pre>
    <strong>&gt; library(topicmodels)</strong><br/>    <br/>    <strong>&gt; set.seed(123)</strong><br/>    <br/>    <strong>&gt; lda3 &lt;- LDA(dtm, k = 3, method = "Gibbs")</strong><br/>    <br/>    <strong>&gt; topics(lda3)</strong><br/>    <strong>2010 2011 2012 2013 2014 2015 2016 </strong><br/>    <strong>   2    1    1    1    3    3    2</strong>
</pre>
<p>We can see an interesting transition over time. The first and last addresses have the same topic grouping, almost as if he opened and closed his tenure with the same themes.</p>
<p>Using the <kbd>terms()</kbd> function produces a list of an ordered word frequency for each topic. The list of words is specified in the function, so let's look at the top <kbd>20</kbd> per topic:</p>
<pre>
<strong>    &gt; terms(lda3, 25)</strong><br/><strong>          Topic 1      Topic 2       Topic 3 </strong><br/><strong>     [1,] "jobs"       "people"      "america" </strong><br/><strong>     [2,] "now"        "one"         "new" </strong><br/><strong>     [3,] "get"        "work"        "every" </strong><br/><strong>     [4,] "tonight"    "just"        "years" </strong><br/><strong>     [5,] "last"       "year"        "like" </strong><br/><strong>     [6,] "energy"     "know"        "make" </strong><br/><strong>     [7,] "tax"        "economy"     "time" </strong><br/><strong>     [8,] "right"      "americans"   "need" </strong><br/><strong>     [9,] "also"       "businesses"  "american" </strong><br/><strong>    [10,] "government" "even"        "world" </strong><br/><strong>    [11,] "home"       "give"        "help" </strong><br/><strong>    [12,] "well"       "many"        "lets" </strong><br/><strong>    [13,] "american"   "security"    "want" </strong><br/><strong>    [14,] "two"        "better"      "states" </strong><br/><strong>    [15,] "congress"   "come"        "first" </strong><br/><strong>    [16,] "country"    "still"       "country" </strong><br/><strong>    [17,] "reform"     "workers"     "together" </strong><br/><strong>    [18,] "must"       "change"      "keep" </strong><br/><strong>    [19,] "deficit"    "take"        "back" </strong><br/><strong>    [20,] "support"    "health"      "americans"</strong><br/><strong>    [21,] "business"   "care"        "way" </strong><br/><strong>    [22,] "education"  "families"    "hard" </strong><br/><strong>    [23,] "companies"  "made"        "today" </strong><br/><strong>    [24,] "million"    "future"      "working" </strong><br/><strong>    [25,] "nation"     "small"       "good" <br/> </strong>
</pre>
<p><kbd>Topic 2</kbd> covers the first and last speeches. Nothing really stands out as compelling in that topic like the others. It will be interesting to see how the next analysis can yield insights into those speeches.</p>
<p><kbd>Topic 1</kbd> covers the next three speeches. Here, the message transitions to <kbd>"jobs"</kbd>, <kbd>"energy"</kbd>, <kbd>"reform"</kbd>, and the <kbd>"deficit"</kbd>, not to mention the comments about <kbd>"education"</kbd> and as we saw above, the correlation of <kbd>"jobs"</kbd> and <kbd>"colleges"</kbd>.</p>
<p><kbd>Topic 3</kbd> brings us to the next two speeches. The focus seems to really shift on to the economy and business with mentions to <kbd>"security"</kbd> and healthcare.</p>
<p>In the next section, we can dig into the exact speech content further, along with comparing and contrasting the first and last State of the Union addresses.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Additional quantitative analysis</h1>
            </header>

            <article>
                
<p>This portion of the analysis will focus on the power of the <kbd>qdap</kbd> package. It allows you to compare multiple documents over a wide array of measures. Our effort will be on comparing the 2010 and 2016 speeches. For starters, we will need into turn the text into data frames, perform sentence splitting, and then combine them to one data frame with a variable created that specifies the year of the speech. We will use this as our grouping variable in the analyses. Dealing with text data, even in R, can be tricky. The code that follows seemed to work the best in this case to get the data loaded and ready for analysis. We first load the <kbd><span>qdap</span></kbd> package. Then, to bring in the data from a text file, we will use the <kbd>readLines()</kbd> function from base R, collapsing the results to eliminate unnecessary whitespace. I also recommend putting your text encoding to ASCII, otherwise you may run into some bizarre text that will mess up your analysis. That is done with the <kbd>iconv()</kbd> function:</p>
<pre>
    <strong>&gt; library(qdap)</strong><br/>    <strong><br/>    &gt; speech16 &lt;- paste(readLines("sou2016.txt"), collapse=" ")<br/>    Warning message:<br/>    In readLines("sou2016.txt") : incomplete final line found on <br/>      'sou2016.txt'<br/><br/>    &gt; speech16 &lt;- iconv(speech16, "latin1", "ASCII", "")<br/></strong>
</pre>
<p>The warning message is not an issue as it is just telling us that the final line of text is not the same length as the other lines in the <kbd>.txt</kbd> file. We now apply the <kbd>qprep()</kbd> function from <kbd>qdap</kbd>.</p>
<p>This function is a wrapper for a number of other replacement functions and using it will speed pre-processing, but it should be used with caution if more detailed analysis is required. The functions it passes through are as follows:</p>
<ul>
<li><kbd>bracketX()</kbd>: apply bracket removal</li>
<li><kbd>replace_abbreviation()</kbd>: replaces abbreviations</li>
<li><kbd>replace_number()</kbd>: numbers to words, for example '100' becomes 'one hundred'</li>
<li><kbd>replace_symbol()</kbd>: symbols become words, for example @ becomes 'at'</li>
</ul>
<pre>
<strong>    &gt; prep16 &lt;- qprep(speech16)<br/></strong>
</pre>
<p>The other pre-processing we should do is to replace contractions (can't to cannot), remove stopwords, in our case the top 100, and remove unwanted characters, with the exception of periods and question marks. They will come in handy shortly:</p>
<pre>
<strong>    &gt; prep16 &lt;- replace_contraction(prep16)</strong><br/><br/><strong>    &gt; prep16 &lt;- rm_stopwords(prep16, Top100Words, separate = F)</strong><br/><br/><strong>    &gt; prep16 &lt;- strip(prep16, char.keep = c("?", "."))<br/></strong>
</pre>
<p>Critical to this analysis is to now split it into sentences and add what will be the grouping variable, the year of the speech. This also creates the <kbd>tot</kbd> variable, which stands for Turn of Talk, serving as an indicator of sentence order. This is especially helpful in a situation where you are analyzing dialogue, say in a debate or question and answer session:</p>
<pre>
<strong>    &gt; sent16 &lt;- data.frame(speech = prep16)</strong><br/><br/><strong>    &gt; sent16 &lt;- sentSplit(sent16, "speech")</strong><br/><br/><strong>    &gt; sent16$year &lt;- "2016"</strong>
</pre>
<p>Repeat the steps for the 2010 speech:</p>
<pre>
<strong>    &gt; speech10 &lt;- paste(readLines("sou2010.txt"), collapse=" ")</strong><br/><strong><br/>    &gt; speech10 &lt;- iconv(speech10, "latin1", "ASCII", "")</strong><br/><strong><br/>    &gt; speech10 &lt;- gsub("(Applause.)", "", speech10)</strong><br/><strong><br/>    &gt; prep10 &lt;- qprep(speech10)</strong><br/><strong><br/>    &gt; prep10 &lt;- replace_contraction(prep10)</strong><br/><strong><br/>    &gt; prep10 &lt;- rm_stopwords(prep10, Top100Words, separate = F)</strong><br/><strong><br/>    &gt; prep10 &lt;- strip(prep10, char.keep = c("?", "."))</strong><br/><strong><br/>    &gt; sent10 &lt;- data.frame(speech = prep10)</strong><br/><strong>    <br/>    &gt; sent10 &lt;- sentSplit(sent10, "speech")</strong><br/><br/><strong>    &gt; sent10$year &lt;- "2010"<br/></strong>
</pre>
<p>Concatenate the separate years into one dataframe:</p>
<pre>
<strong>    &gt; sentences &lt;- data.frame(rbind(sent10, sent16))<br/></strong>
</pre>
<p>One of the great things about the <kbd>qdap</kbd> package is that it facilitates basic text exploration, as we did before. Let's see a plot of frequent terms:</p>
<pre>
<strong>    &gt; plot(freq_terms(sentences$speech))</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="398" width="655" class="image-border" src="assets/image_12_04.png"/></div>
<p>You can create a word frequency matrix that provides the counts for each word by speech:</p>
<pre>
<strong>    &gt; wordMat &lt;- wfm(sentences$speech, sentences$year)</strong><br/><br/><strong>    &gt; head(wordMat[order(wordMat[, 1], wordMat[, 2],decreasing = <br/>      TRUE),])</strong><br/><strong>              2010 2016</strong><br/><strong>    our        120   85</strong><br/><strong>    us          33   33</strong><br/><strong>    year        29   17</strong><br/><strong>    americans   28   15</strong><br/><strong>    why         27   10</strong><br/><strong>    jobs        23    8</strong>
</pre>
<p>This can also be converted into a document-term matrix with the function <kbd>as.dtm()</kbd> should you so desire. Let's next build <kbd>wordclouds</kbd>, by year with <kbd>qdap</kbd> functionality:</p>
<pre>
<strong>    &gt; trans_cloud(sentences$speech, sentences$year, min.freq = 10)</strong>
</pre>
<p class="CDPAlignLeft CDPAlign">The preceding command produces the following two images:</p>
<div class="CDPAlignCenter CDPAlign"><img height="277" width="306" class="image-border" src="assets/image_12_05.png"/></div>
<div class="CDPAlignCenter CDPAlign"><img height="225" width="235" class="image-border" src="assets/image_12_06.png"/></div>
<p>Comprehensive word statistics are available. Here is a plot of the stats available in the package. The plot loses some of its visual appeal with just two speeches, but is revealing nonetheless. A complete explanation of the stats is available under <kbd>?word_stats</kbd>:</p>
<pre>
<strong>    &gt; ws &lt;- word_stats(sentences$speech, sentences$year, rm.incomplete = T)</strong><br/><br/><strong>    &gt; plot(ws, label = T, lab.digits = 2)</strong>
</pre>
<p><span>The output of the preceding command is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="291" width="454" class="image-border" src="assets/image_12_07.png"/></div>
<p>Notice that the 2016 speech was much shorter, with over a hundred fewer sentences and almost a thousand fewer words. Also, there seems to be the use of asking questions as a rhetorical device in 2016 versus 2010 (n.quest 10 versus n.quest 4).</p>
<p>To compare the polarity (sentiment scores), use the <kbd>polarity()</kbd> function, specifying the text and grouping variables:</p>
<pre>
    <strong>&gt; pol = polarity(sentences$speech, sentences$year)</strong><br/>    <br/>    <strong>&gt; pol<br/>     year total.sentences total.words ave.polarity sd.polarity <br/>       stan.mean.polarity<br/>   1 2010             435        3900        0.052       0.432              <br/>      0.121<br/>   2 2016             299        2982        0.105       0.395              <br/>      0.267<br/></strong>
</pre>
<p>The <kbd>stan.mean.polarity</kbd> value represents the standardized mean polarity, which is the average polarity divided by the standard deviation. We see that <kbd>2015</kbd> was slightly higher (<kbd>0.267</kbd>) than <kbd>2010</kbd> (<kbd>0.121</kbd>). This is in line with what we would expect, wanting to end on a more positive note. You can also plot the data. The plot produces two charts. The first shows the polarity by sentences over time and the second shows the distribution of the polarity:</p>
<pre>
    <strong>&gt; plot(pol)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="240" width="375" class="image-border" src="assets/image_12_08.png"/></div>
<p>This plot may be a challenge to read in this text, but let me do my best to interpret it. The <kbd>2010</kbd> speech starts out with a strong negative sentiment and is slightly more negative than <kbd>2016</kbd>. We can identify the most negative sentiment sentence by creating a dataframe of the <kbd>pol</kbd> object, find the sentence number, and produce it:</p>
<pre>
    <strong>&gt; pol.df &lt;- pol$all</strong><br/>    <br/>    <strong>&gt; which.min(pol.df$polarity)</strong><br/>    <strong>[1] 12</strong><br/>    <br/>    <strong>&gt; pol.df$text.var[12]</strong><br/>    <br/>    <strong>[1] "One year ago, I took office amid two wars, an economy rocked <br/>       by a severe recession, a financial system on the verge of <br/>          collapse, and a government deeply in debt.</strong>
</pre>
<p>Now that is negative sentiment! Ironically, the government is even more in debt today. We will look at the readability index next:</p>
<pre>
    <strong>&gt; ari &lt;- automated_readability_index(sentences$speech, <br/>      sentences$year) </strong><br/>    <br/>    <strong>&gt; ari$Readability</strong><br/>    <strong>  year word.count sentence.count character.count</strong><br/>    <strong>1 2010       3900            435           23859</strong><br/>    <strong>2 2016       2982            299           17957</strong><br/>    <strong>  Automated_Readability_Index</strong><br/>    <strong>1                    11.86709</strong><br/>    <strong>2                    11.91929</strong>
</pre>
<p>I think it is no surprise that they are basically the same. Formality analysis is next. This takes a couple of minutes to run in R:</p>
<pre>
    <strong>&gt; form &lt;- formality(sentences$speech, sentences$year)</strong><br/>    <br/>    <strong>&gt; form</strong><br/>    <strong>  year word.count formality</strong><br/>    <strong>1 2016       2983     65.61</strong><br/>    <strong>2 2010       3900     63.88</strong>
</pre>
<p>This looks to be very similar. We can examine the proportion of the parts of the speech. A plot is available, but adds nothing to the analysis, in this instance:</p>
<pre>
    <strong>&gt; form$form.prop.by</strong><br/>    <strong>  year word.count  noun   adj  prep articles pronoun</strong><br/>    <strong>1 2010       3900 44.18 15.95  3.67     0        4.51</strong><br/>    <strong>2 2016       2982 43.46 17.37  4.49     0        4.96</strong><br/>    <strong>   verb adverb interj other</strong><br/>    <strong>1 23.49   7.77   0.05  0.38</strong><br/>    <strong>2 21.73   7.41   0.00  0.57</strong>
</pre>
<p>Now, the diversity measures are produced. Again, they are nearly identical. A plot is also available, (<kbd>plot(div)</kbd>), but being so similar, it once again adds no value. It is important to note that Obama's speech writer for 2010 was Jon Favreau, and in 2016, it was Cody Keenan:</p>
<pre>
    <strong>&gt; div &lt;- diversity(sentences$speech, sentences$year)</strong><br/>    <br/>    <strong>&gt; div</strong><br/>    <strong>  year   wc simpson shannon collision berger_parker brillouin</strong><br/>    <strong>1 2010 3900   0.998   6.825     5.970         0.031     6.326</strong><br/>    <strong>2 2015 2982   0.998   6.824     6.008         0.029     6.248</strong>
</pre>
<p>One of my favorite plots is the dispersion plot. This shows the dispersion of a word throughout the text. Let's examine the dispersion of <kbd>"jobs"</kbd>, <kbd>"families",</kbd> and <kbd>"economy"</kbd>:</p>
<pre>
<strong>   &gt; dispersion_plot(sentences$speech,<br/>     rm.vars = sentences$year,<br/>     c("security", "jobs", "economy"),<br/>     color = "black", bg.color = "white")</strong><strong><br/></strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="253" width="395" class="image-border" src="assets/image_12_09.png"/></div>
<p>This is quite interesting as you can visualize how much longer the 2010 speech is. In 2010, the first half of his speech was focused heavily on jobs while in 2016 it appears it was more about the state of the overall economy; no doubt how much of a hand he played in saving it from the brink of disaster. In 2010, security was not brought in until later in the speech versus placed throughout the final address. You can see and understand how text analysis can provide insight into what someone is thinking, what their priorities are, and how they go about communicating them.</p>
<p>This completes our analysis of the two speeches. I must confess that I did not listen to any of these speeches. In fact, I haven't watched a State of the Union address since Reagan was president, probably with the exception of the 2002 address. This provided some insight for me on how the topics and speech formats have changed over time to accommodate political necessity, while the overall style of formality and sentence structure has remained consistent. Keep in mind that this code can be adapted to text for dozens, if not hundreds, of documents and with multiple speakers, for example screenplays, legal proceedings, interviews, social media, and on and on. Indeed, text mining can bring quantitative order to what has been qualitative chaos.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we looked at how to address the massive volume of textual data that exists through text mining methods. We looked at a useful framework for text mining, including preparation, word frequency counts and visualization, and topic models using LDA with the <kbd>tm</kbd> package. Included in this framework were other quantitative techniques, such as polarity and formality, in order to provide a deeper lexical understanding, or what one could call style, with the <kbd>qdap</kbd> package. The framework was then applied to president Obama's seven State of the Union addresses, which showed that, although the speeches had a similar style, the core messages changed over time as the political landscape changed. Despite it not being practical to cover every possible text mining technique, those discussed in this chapter should be adequate for most problems that one might face. In the next chapter, we are going to shift gears away from building models and focus on a technique to get R on the cloud, allowing you to scale your machine learning to whatever problem you may be trying to solve.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>