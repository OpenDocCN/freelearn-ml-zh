<html><head></head><body>
		<div id="_idContainer095">
			<h1 class="chapter-number" id="_idParaDest-58"><a id="_idTextAnchor123"/>3</h1>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor124"/>Building Out Our Bronze Layer</h1>
			<p class="author-quote">“Data is a precious thing and will last longer than the systems themselves.”</p>
			<p class="author-quote">– Tim Berners-Lee, generally credited as the inventor of the World Wide Web</p>
			<p>In this chapter, you’ll embark on the beginning of your data journey in the Databricks platform, exploring the fundamentals of the Bronze layer. We recommend employing the Medallion design pattern within the lake house architecture (as described in <a href="B16865_02.xhtml#_idTextAnchor073"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>) to organize<a id="_idIndexMarker116"/> your data. We’ll start with <strong class="bold">Auto Loader</strong>, which you can<a id="_idIndexMarker117"/> implement with or without <strong class="bold">Delta Live Tables</strong> (<strong class="bold">DLT</strong>) to insert and transform data in your architecture. The benefits of using Auto Loader include quickly transforming new data into the Delta format and enforcing or evolving schemas, which are essential for maintaining consistent data delivery to the business and customers. As a data scientist, strive for efficiency in building your data pipelines and ensuring your data is ready for the steps in the machine learning development cycle. You will best learn these topics through the example projects, so the <em class="italic">Applying our learning</em> section is the main focus of <span class="No-Break">this chapter.</span></p>
			<p>Let’s see what topics you will cover in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Revisiting the Medallion <span class="No-Break">architecture pattern</span></li>
				<li>Transforming data to Delta with <span class="No-Break">Auto Loader</span></li>
				<li>DLT, starting <span class="No-Break">with Bronze</span></li>
				<li>Maintaining and optimizing <span class="No-Break">Delta Tables</span></li>
				<li>Applying <span class="No-Break">our learnin<a id="_idTextAnchor125"/>g</span></li>
			</ul>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor126"/>Revisiting the Medallion architecture pattern</h1>
			<p>We introduced the Medallion <a id="_idIndexMarker118"/>architecture in <a href="B16865_02.xhtml#_idTextAnchor073"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. As a reminder, this refers to the data design pattern used to organize data logically. It has three layers – Bronze, Silver, and Gold. There are also cases where additional levels of refinement are required, so your Medallion architecture could be extended to Diamond and Platinum levels if needed. The Bronze layer contains raw data, the Silver layer contains cleaned and transformed data, and the Gold layer contains aggregated and curated data. Curated data refers to the datasets<a id="_idIndexMarker119"/> selected, cleaned, and organized for a specific business or modeling purpose. This architecture is a good fit for data science projects. Maintaining the original data as a source of truth is important, while curated data is valuable for research, analytics, and machine learning applications. By selecting, cleaning, and organizing data for a specific purpose, curated data can help improve its accuracy, relevance, <span class="No-Break">and usability.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">This book will introduce quite a few project tasks, such as <em class="italic">building curated datasets</em>, that often fall under the domain of a data engineer. There will also be tasks that machine learning engineers, data analysts, and so on commonly perform. We include all of this work in our examples because roles are blurred in today’s fast-moving world and will vary by company. Titles and expectations quickly evolve. Therefore, it’s imperative to have a handle on the entire <span class="No-Break">end-to-end workflow.</span></p>
			<p>Maintaining a Bronze layer allows us to go back to the original data when we want to create a different feature, solve a new problem that requires us to look at historical data from another point of view, or simply maintain the raw level of our data for governance purposes. As the world of technology evolves, it’s crucial to stay current and follow trends, but the core principles<a id="_idIndexMarker120"/> will remain for years. For the remainder of this chapter, we will cover DI platform features that facilitate building the <span class="No-Break">Bronze la<a id="_idTextAnchor127"/>yer.</span></p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor128"/>Transforming data to Delta with Auto Loader</h1>
			<p>Harness the power<a id="_idIndexMarker121"/> of Auto Loader<a id="_idIndexMarker122"/> to automate your data ingestion process, significantly enhancing your data product’s workflow efficiency. It can ingest data from cloud storage and streaming data sources. You can configure Auto Loader to run on a schedule or be <span class="No-Break">triggered manually.</span></p>
			<p>Here are some benefits<a id="_idIndexMarker123"/> of using Databricks’ <span class="No-Break">Auto Loader:</span></p>
			<ul>
				<li><strong class="bold">It keeps data up to date</strong>: Auto Loader maintains checkpoints, removing the need to know which data is new. Auto Loader handles all that on <span class="No-Break">its own.</span></li>
				<li><strong class="bold">It improves data quality</strong>: Auto Loader can automatically detect schema changes and rescue any new data columns, so you can be confident that your data <span class="No-Break">is accurate.</span></li>
				<li><strong class="bold">It increases data agility</strong>: Auto Loader can help you quickly and easily ingest new data sources so that you can be more agile in responding to changes in <span class="No-Break">your business.</span></li>
				<li><strong class="bold">Flexible ingestion</strong>: Auto Loader can stream files in batches or continuously. This means it can consume batch data as a stream to reduce the overhead of a more manual <span class="No-Break">batch</span><span class="No-Break"><a id="_idIndexMarker124"/></span><span class="No-Break"> pipeline.</span></li>
			</ul>
			<p>Auto Loader is a powerful tool. It can be used standalone, as the underlying<a id="_idIndexMarker125"/> technology for DLT, or with <strong class="bold">Spark Structured Streaming</strong>. Spark Structured Streaming is a near-real-time processing engine; we will cover how to create a real-time feature in <a href="B16865_05.xhtml#_idTextAnchor244"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>. This chapter also covers<a id="_idIndexMarker126"/> an example of streaming ingestion<a id="_idIndexMarker127"/> in the streaming transactions project. Let’s discuss Auto Loader’s ability to evolve a schema<a id="_idTextAnchor129"/> <span class="No-Break">over time.</span></p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor130"/>Schema evolution</h2>
			<p>Auto Loader’s schema evolution<a id="_idIndexMarker128"/> allows you to add or modify fields in streaming data seamlessly. Databricks automatically adjusts relevant data to fit the new schema while preserving existing data integrity. This automated schema handling makes it easy to evolve your data schema over time as your business needs and data sources change, without having to worry about data loss or downtime. Furthermore, schema handling reduces the amount of work and headache associated with managing data pipelines that could <span class="No-Break">change unexpectedly.</span></p>
			<p>The new schema includes an additional column in the <em class="italic">Applying our learning</em> section. We will show how no data is lost despite a schema changing without notice. In our case, the default schema evolution mode for Auto Loader, <strong class="source-inline">.option("cloudFiles.schemaEvolutionMode", "addNewColumns")</strong>, in combination with <strong class="source-inline">.option("mergeSchema", "true")</strong>, perform schema evolution. Auto Loader will handle changes in the schema for us. Auto Loader tracking the changes is beneficial when new data fields become available with or without prior notice; code changes <span class="No-Break">are unnecessary.</span></p>
			<p>There’s documentation on all schema options. The method we use for our project is the only option for automatic schema evolution. Every other option will require manual intervention. For example, you can use “rescue” mode to rescue data from being lost. Alternatively, you can use “failOnNewColumns” mode to cause the pipeline to fail and keep the schema unchanged until the production code is updated. There are numerous options and patterns<a id="_idIndexMarker129"/> for Auto Loader. Check out the <em class="italic">Common loading patterns with Auto Loader</em> link in the <em class="italic">Further reading</em> section for <span class="No-Break">more<a id="_idTextAnchor131"/> information.</span></p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor132"/>DLT, starting with Bronze</h1>
			<p>As we touched upon<a id="_idIndexMarker130"/> in a previous chapter, remember that DLT actively simplifies your pipeline operations, empowering you to focus on setting clear objectives for your pipeline rather than getting bogged down in operational details. Building on this foundation, we will now delve into DLT’s capabilities. Auto Loader’s schema evolution is integrated with DLT, underscoring its utility in handling dynamic data schemas with minimal <span class="No-Break">manual<a id="_idTextAnchor133"/> intervention.</span></p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor134"/>DLT benefits and features</h2>
			<p>DLT is a sophisticated framework designed to construct reliable data pipelines. DLT automates and streamlines complex operations such as orchestration and cluster management, significantly boosting the efficiency of data workflows. All you need to do is specify your transformation logic to be up and running. We will focus on just a few of the benefits of DLT as they pertain to creating your Bronze data layer, but we’ve included a link to DLT documentation in the <em class="italic">Further reading</em> section at the end of this chapter as well. DLT is another great tool in your toolbox for ingesting data. It provides several benefits<a id="_idIndexMarker131"/> over traditional ETL pipelines, including <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Declarative pipeline development</strong>: DLT allows you to define your data pipelines using SQL or Python, which makes them easier to understand <span class="No-Break">and maintain</span></li>
				<li><strong class="bold">Automatic data quality testing</strong>: DLT can automatically apply your tests to your data, preventing quality issues, which helps to ensure that your pipelines are producing <span class="No-Break">accurate results</span></li>
				<li><strong class="bold">Deep visibility for monitoring and recovery</strong>: DLT provides detailed tracking and logging information, making troubleshooting problems and recovering from <span class="No-Break">failures easier</span></li>
				<li><strong class="bold">Cost-effective streaming through efficient compute autoscaling</strong>: DLT can automatically scale your compute resources up or down based on demand, which helps<a id="_idIndexMarker132"/> to <span class="No-Break">reduce costs</span></li>
			</ul>
			<p>DLT is a powerful tool for building reliable and scalable data pipelines in batch or streaming. It can help you improve the quality, efficiency, and visibility of your data <span class="No-Break">processing workflows.</span></p>
			<p>Here are some of the specific features<a id="_idIndexMarker133"/> of DLT that provide <span class="No-Break">these benefits:</span></p>
			<ul>
				<li><strong class="bold">Streaming tables</strong>: DLT uses streaming and live tables to process data in real time and to keep your pipelines up to date with the <span class="No-Break">latest data.</span></li>
				<li><strong class="bold">Materialized views</strong>: DLT uses materialized views to create snapshots of your data. You can query your data in real time and use it for <span class="No-Break">downstream processing.</span></li>
				<li><strong class="bold">Expectations</strong>: DLT uses expectations to test your data for quality issues automatically and to take action if the data does not meet <span class="No-Break">your expectations.</span></li>
				<li><strong class="bold">Autoscaling</strong>: DLT can automatically scale your compute resources up or down based on demand, reducing costs and improving the performance of <span class="No-Break">your pipelines.</span></li>
			</ul>
			<p>DLT is a good way to build reliable, scalable, and testable data pipelines. Next, we will focus on DLT in the context of<a id="_idTextAnchor135"/> <a id="_idTextAnchor136"/>the <span class="No-Break">Bronze layer.</span></p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor137"/>Bronze data with DLT</h2>
			<p>DLT uses Auto Loader<a id="_idIndexMarker134"/> to support <a id="_idIndexMarker135"/>schema evolution, offering significant benefits in terms of data quality, a key aspect of the Bronze layer in a Medallion architecture. In this architecture, the Bronze layer serves as the foundational stage where raw data is initially ingested and stored. DLT contributes to this layer by ensuring that each transformation applied to the raw data is precisely captured and managed. As a data scientist, understanding these transformations is crucial for maintaining the integrity of the data processing workflow. A powerful feature of DLT is its ability to automatically generate<a id="_idIndexMarker136"/> an accurate workflow Directed Acyclic Graph (<strong class="bold">DAG</strong>). This DAG not only visualizes the sequence and relationships of these data transformations but also enhances the reliability of the entire <span class="No-Break">data pipeline.</span></p>
			<p>The following screenshot shows the DLT <span class="No-Break">pipeline workflow:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer061">
					<img alt="Figure 3.1 – The DLT pipeline workflow for the transactional dataset in this chapter" src="image/B16865_03_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – The DLT pipeline workflow for the transactional dataset in this chapter</p>
			<p>In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em>, there is one task in the DLT workflow, but it can accommodate a complex workflow with <span class="No-Break">multiple dependencies.</span></p>
			<p>Once we get data landed in Delta, there are actions we can take to ensure we take full advantage o<a id="_idTextAnchor138"/>f <span class="No-Break">Delta’s benefits.</span></p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor139"/>Maintaining and optimizing Delta tables</h1>
			<p>While the primary<a id="_idIndexMarker137"/> focus of this book<a id="_idIndexMarker138"/> is not on the intricate details of Delta table optimization, understanding these techniques is crucial for developing a data-centric machine learning solution. Efficient management of Delta tables directly impacts the performance and reliability of ML models, as these models heavily rely on the quality and accessibility of the underlying data. Employ techniques such as <strong class="source-inline">VACUUM</strong>, liquid clustering, <strong class="source-inline">OPTIMIZE</strong>, and bucketing to store, access, and manage your data with unparalleled efficiency. Optimized tables ensure that the data feeding into ML algorithms is processed efficiently. We’ll cover these briefly here, but we also suggest that you refer to the Delta Lake documentation<a id="_idIndexMarker139"/> for a comprehensive<a id="_idIndexMarker140"/> understan<a id="_idTextAnchor140"/>ding of <span class="No-Break">each technique.</span></p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor141"/>VACUUM</h2>
			<p>The <strong class="source-inline">VACUUM</strong> command is crucial <a id="_idIndexMarker141"/>in managing resources<a id="_idIndexMarker142"/> within Delta tables. It works by cleaning up invalidated files and optimizing the metadata layout. If your Delta<a id="_idIndexMarker143"/> table undergoes frequent <strong class="bold">Data Manipulation Language</strong> (<strong class="bold">DML</strong>) operations (<strong class="source-inline">update</strong>, <strong class="source-inline">insert</strong>, <strong class="source-inline">delete</strong>, and <strong class="source-inline">merge</strong>), we recommend running the <strong class="source-inline">VACUUM</strong> operation periodically. DML operations can generate numerous small files over time. Failure to run <strong class="source-inline">VACUUM</strong> may result in several small files with minimal data remaining online, leading to <span class="No-Break">poor performance.</span></p>
			<p>Note that <strong class="source-inline">VACUUM</strong> doesn’t run automatically; you must explicitly schedule it. Consider scheduling <strong class="source-inline">VACUUM</strong> at regular intervals, such as weekly or monthly, depending on your data ingestion frequency and how often you update the data. Additionally, you have the option to configure the retention period to optimize storage and query performance. The <strong class="source-inline">delta.logRetentionDuration</strong> Delta configuration command allows you to control the retention period by specifying the number of days you want to keep the data files. This means you will delete all transaction log data for a table that goes back beyond the retention setting (e.g., only the last seven days of metadata remain). This way, you can control the duration the Delta table retains data files before the <strong class="source-inline">VACUUM</strong> operation removes them. The retention duration affects your ability to look back in time to previous versions of a table. Keep that in mind when deciding how long you want to retain the metadata and transaction log. Additionally, the transa<a id="_idTextAnchor142"/>ction log is not <span class="No-Break">very big.</span></p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor143"/>Liquid clustering</h2>
			<p>Liquid clustering<a id="_idIndexMarker144"/> is a great alternative<a id="_idIndexMarker145"/> to partitioning; see how to implement it in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2</em>. Frequently, data gets over-partitioned, leaving too few files in a partition or unbalanced partitions. You do not need partitioning unless a table is a terabyte or larger. In addition to replacing partitioning, liquid clustering also replaces <em class="italic">Z</em>-ordering on Delta tables. <em class="italic">Z</em>-ordering is not compatible with clustered tables. When choosing columns to cluster on, include high cardinality columns that you use to query filters. Another commonly given example is a timestamp column. Instead of creating a derived column date to minimize the cardinality, simply cluster on the timestamp. Liquid clustering benefits Delta tables with skewed data distribution or changing access patterns. It allows a table to adapt to analytical needs by redefining clustering keys without rewriting data, resulting in optimized query performance and a flexible, maintenance-efficient structure. With liquid clustering enabled, you must use DBR 13.3+ to create, write, or optimize <span class="No-Break">Delta tables.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer062">
					<img alt="Figure 3.2 – Example code to create a table optim﻿ized with liquid clustering" src="image/B16865_03_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – Example code to create a table optim<a id="_idTextAnchor144"/>ized with liquid clustering</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor145"/>OPTIMIZE</h2>
			<p>The <strong class="source-inline">OPTIMIZE</strong> command will trigger<a id="_idIndexMarker146"/> clustering. This is extra<a id="_idIndexMarker147"/> important when streaming data, as tables are not clustered on write. <strong class="source-inline">OPTIMIZE</strong> also compacts data files, which is vital for Delta tables with numerous small files. It merges these files into larger ones, enhancing read query speed and storage efficiency, which is especially b<a id="_idTextAnchor146"/>eneficial for <span class="No-Break">large datasets.</span></p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor147"/>Predictive optimization</h2>
			<p>Predictive optimization<a id="_idIndexMarker148"/> is a feature<a id="_idIndexMarker149"/> that runs <strong class="source-inline">VACUUM</strong> and <strong class="source-inline">OPTIMIZE</strong> for you. Your admin can enable it in settings if you have the premium version, serverless enabled, and Unity Catalog <span class="No-Break">set up.</span></p>
			<p>Now that we’ve covered the basics of the Medallion architecture, Auto Loader, DLT, and some techniques to optimize your Delta tables, get ready to follow along in your own Databricks workspace as we work through<a id="_idTextAnchor148"/> the <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> code <span class="No-Break">by project.</span></p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor149"/>Applying our learning</h1>
			<p>You will ingest the data before diving into each project’s core “data science” aspects. We’ve discussed how Auto Loader, schema evolution, and DLT can format your data in storage. You’ll notice that the following projects use different patterns to load data into the Bronze layer. The streaming transactions project uses Auto Loader to ingest incoming JSON files. You will transform the data with DLT and Structure Streaming independently, allowing you to ge<a id="_idTextAnchor150"/>t experience with <span class="No-Break">both methods.</span></p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor151"/>Technical requirements</h2>
			<p>Before we begin, review the technical requirements needed to complete the hands-on work in <span class="No-Break">this chapter.</span></p>
			<p>Databricks ML Runtime includes several pre-installed libraries useful for ML and data science projects. For this reason, we will us<a id="_idTextAnchor152"/>e clusters with an <span class="No-Break">ML runtime.</span></p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor153"/>Project – streaming transactions</h2>
			<p>The next step in our streaming transactions project<a id="_idIndexMarker150"/> is to build out the Bronze layer, or the raw data we’ll eventually use in our classification model. We specifically created this streaming project to practice using Auto Loader, schema evolution, Spark Structured Streaming, and DLT features, so we’ll use those throughout this part of the project. To follow along in your workspace, open the <span class="No-Break">following notebooks:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">CH3-01-Auto_Loader_and_Schema_Evolution</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">CH3-02-Generating_Records_and_Schema_Change</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">delta_live_tables/CH3-03-Formatting_to_Delta_with_DLT</strong></span></li>
			</ul>
			<p>Here’s where we are in the <span class="No-Break">project flow:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer063">
					<img alt="Figure 3.3 – The project plan for the synthetic streaming transactions project" src="image/B16865_03_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – The project plan for the synthetic streaming transactions project</p>
			<p>In <a href="B16865_02.xhtml#_idTextAnchor073"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, we generated<a id="_idIndexMarker151"/> and stored our synthetic transaction data in JSON files. Now, we will read that data into a Delta table. We’ll also update the data generation notebook to add a product string column, which will demonstrate schema evolution, as mentioned previously in the <em class="italic">Schema evolution</em> section. Let’s walk through two options for reading and writing the data stream to a Delta table. Both options use Auto Loader to ingest. The files are then handled and written out by either Spark Structured Streaming or DLT. There are two notebooks for this section. We will start <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">CH3-01-A<a id="_idTextAnchor154"/>uto_Loader_and_Schema_Evolution</strong></span><span class="No-Break">.</span></p>
			<h3>Continuous data ingestion using Auto Loader with Structured Streaming</h3>
			<p>The code in this section<a id="_idIndexMarker152"/> uses Auto Loader<a id="_idIndexMarker153"/> to format the incoming JSON files as <strong class="source-inline">Delta</strong>. The table name we are using is <strong class="source-inline">synthetic_transactions</strong>. Before processing data, we create a widget (see <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.4</em>) to determine whether we want to reset the schema and checkpoint history. Resetting the history can be helpful when altering or debugging pipelines. If you reset (remove) your checkpoint history, you will reprocess all <span class="No-Break">historical data.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer064">
					<img alt="Figure 3.4 – Creating a widget to reset the checkpoint and schema" src="image/B16865_03_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – Creating a widget to reset the checkpoint and schema</p>
			<p>Next, we set our variables for the script, which are <span class="No-Break">mainly paths:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer065">
					<img alt="Figure 3.5 – Setting the path variables" src="image/B16865_03_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Setting the path variables</p>
			<p>The setup file provides us with <strong class="source-inline">volume_file_path</strong>, where we store the synthetic data, schema, and <span class="No-Break">checkpoint folders.</span></p>
			<p>As shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.6</em>, we add spark configurations to optimize and reduce the sample size <span class="No-Break">for inference:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer066">
					<img alt="Figure 3.6 – Setting configurations to optimize and reduce the sample size" src="image/B16865_03_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Setting configurations to optimize and reduce the sample size</p>
			<p>You can set these Spark<a id="_idIndexMarker154"/> configurations in the<a id="_idIndexMarker155"/> cluster’s advanced options. These configurations will automatically compact sets of small files into larger fi<a id="_idTextAnchor155"/>les for optimal <span class="No-Break">read performance.</span></p>
			<h3>Configuring stream for data ingestion</h3>
			<p>The stream command<a id="_idIndexMarker156"/> is relatively long, so let’s walk through each chunk of <span class="No-Break">the code:</span></p>
			<ol>
				<li>We are creating a stream to read from the synthetic dataset. The format option references the stream format. The <strong class="source-inline">cloudFiles</strong> refers to the files located in cloud storage. In this case, we generate data and write it to cloud storage. However, creating a stream with <strong class="source-inline">.format("kafka")</strong> is possible, which ingests directly from the stream without writing to cloud storage first. We also designate the file format <span class="No-Break">as JSON.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer067">
					<img alt="Figure 3.7 – Creating the stream using CloudFiles and the JSON file format" src="image/B16865_03_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – Creating the stream using CloudFiles and the JSON file format</p>
			<ol>
				<li value="2">The default is to set<a id="_idIndexMarker157"/> column types to <strong class="source-inline">string</strong>. However, we can provide schema hints so that the columns we are sure about get typed appropriately. While inferring the schema, we also want to infer the column types. The new column is a <strong class="source-inline">string</strong>, so we do not see this option in action, as new columns default to <span class="No-Break">a </span><span class="No-Break"><strong class="source-inline">string</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer068">
					<img alt="Figure 3.8 – Setting schema hints to reduce possible type mismatches. We want to infer the data type for columns not in the schema hint" src="image/B16865_03_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Setting schema hints to reduce possible type mismatches. We want to infer the data type for columns not in the schema hint</p>
			<ol>
				<li value="3">Auto Loader uses the <strong class="source-inline">rescue</strong> column to catch the change and quickly puts the new column into play without data loss! Note that the stream will fail and need to be restarted. The schema location is needed if we want Auto Loader to keep track of the schema and evolve it <span class="No-Break">over time.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer069">
					<img alt="Figure 3.9 – Setting schema evolution to add new columns and designating the schema location so that Auto Loader keeps track of schema changes over time" src="image/B16865_03_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – Setting schema evolution to add new columns and designating the schema location so that Auto Loader keeps track of schema changes over time</p>
			<ol>
				<li value="4">Next, we load the location of the raw data files. We need to select the fields we want. We will select all data fields, but you could be selective on the fields you <span class="No-Break">pull in.</span></li>
				<li>The following lines of code begin the “write” portion of the stream. Here, we start <strong class="source-inline">writeStream</strong>. Delta is the default data format, but we prefer to explicitly set it. We also designate that this stream is append-only, as we are not performing any updates <span class="No-Break">or inserts.</span></li>
				<li>A checkpoint is a mechanism in Spark Structured Streaming that allows you to save the state of your stream. If your stream fails, when it restarts, it uses the checkpoint to resume processing where it left off. The mergeSchema option is essential. Merging the schema adds the new columns as they arrive <span class="No-Break">without intervention.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer070">
					<img alt="Figure 3.10 – Creating our checkpoint location and setting merge schema" src="image/B16865_03_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Creating our checkpoint location and setting merge schema</p>
			<ol>
				<li value="7">The next step is to set the trigger. The trigger setting refers to the processing<a id="_idIndexMarker158"/> pace and has two main modes. Per the following code, you can specify the time-based trigger interval in seconds. Our data from the generation notebook is continuous. Here, we efficiently handle it with micro-batches. We provided a select statement in <em class="italic">step 4</em>. We can now write the result of that statement to Delta files in the <span class="No-Break"><strong class="source-inline">destination_location</strong></span><span class="No-Break"> path.</span><p class="list-inset">In <em class="italic">step 7</em>, we use a trigger<a id="_idIndexMarker159"/> with a processing time of 10 seconds. The processing time means micro-batches of data will be processed every 10 seconds. Suppose you do not require micro-batches. If you need your data processed once an hour or once per day, then the <strong class="source-inline">trigger.availableNow</strong> option is best. If you want to process whatever new data has arrived in the last hour, use <strong class="source-inline">trigger.AvailableNow</strong> in your pipeline, and schedule the pipeline to kick off in workflows using a job cluster every hour. At that time, Auto Loader will process all data available and then <span class="No-Break">shut down.</span></p></li>
				<li>Next, to show schema evolution, we update our data generation notebook from<em class="italic"> </em><a href="B16865_02.xhtml#_idTextAnchor073"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> to include an additional <strong class="source-inline">data</strong> column. You’ll find the new version, <strong class="source-inline">CH3-02-Generating_Records_and_Schema_Change</strong>, in the <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> folder. Note that we provide a list of possible products to <strong class="source-inline">writeJsonFile</strong>. The result is an additional field, <strong class="source-inline">Product</strong>, with a product string for the record (<span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.11</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer071">
					<img alt="Figure 3.11 – The updated method for generating data with or without a product string" src="image/B16865_03_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.11 – The updated method for generating data with or without a product string</p>
			<p>As a result of our changes, you now have a stream of data that adds an additional column midstream. You can start the Auto Loader notebook to see the schema evolution in action. The stream stops when it detects the extra column, providing an exception – <strong class="source-inline">[UNKNOWN_FIELD_EXCEPTION.NEW_FIELDS_IN_RECORD_WITH_FILE_PATH]</strong>. Don’t worry; upon restart, the schema evolution takes over. Run the cell with the stream in it again <span class="No-Break">to restart.</span></p>
			<p>You are finished reading and writing<a id="_idIndexMarker160"/> with Auto Loader with Spark Structured Streaming. Next, we’ll show you how to accomplish the same <a id="_idTextAnchor156"/>task with DLT (using <span class="No-Break">less code!).</span></p>
			<h3>Continuous data ingestion using Auto Loader with DLT</h3>
			<p>This section uses the<a id="_idIndexMarker161"/> Auto Loader code<a id="_idIndexMarker162"/> to read the stream of JSON files and then DLT to write the files to a table. This notebook is in the <strong class="source-inline">delta_live_tables</strong> folder and <span class="No-Break">titled </span><span class="No-Break"><strong class="source-inline">CH3-03-Formatting_to_Delta_with_DLT</strong></span><span class="No-Break">.</span></p>
			<p>The only import you need is DLT – <strong class="source-inline">import dlt</strong>. The DLT code is relatively short, partly because the pipeline configuration occurs in the pipeline object rather than the code. Before we look through our source code, let’s navigate to the <strong class="bold">Databricks Workflows</strong> pane in the left-hand navigation bar, select <strong class="bold">Delta Live Tables</strong>, and click <strong class="bold">Create Pipeline</strong>, which opens the pipeline<a id="_idIndexMarker163"/> settings page, as<a id="_idIndexMarker164"/> shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer072">
					<img alt="Figure 3.12 – The DL﻿T pipeline setup in the workflow UI" src="image/B16865_03_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.12 – The DL<a id="_idTextAnchor157"/>T pipeline setup in the workflow UI</p>
			<h3>Pipeline configuration for DLT</h3>
			<p>The pipeline settings<a id="_idIndexMarker165"/> are minimal, given that DLT<a id="_idIndexMarker166"/> does the optimization, so the setup instructions are minimal. The parameters entered into the pipeline configuration are accessible in the pipeline code using <strong class="source-inline">spark.conf.get</strong>, as shown in <em class="italic">Figures 3.13</em> <span class="No-Break">and </span><span class="No-Break"><em class="italic">3.14</em></span><span class="No-Break">:</span></p>
			<ol>
				<li>Enter a pipeline<a id="_idIndexMarker167"/> name. We will<a id="_idIndexMarker168"/> <span class="No-Break">use </span><span class="No-Break"><strong class="source-inline">MLIA_Streaming_Transactions</strong></span><span class="No-Break">.</span></li>
				<li>For <strong class="bold">Product edition</strong>, select <strong class="bold">Advanced</strong>. This DLT edition comes with the most features, including DLT’s <span class="No-Break">expectation rules.</span></li>
				<li>For <strong class="bold">Pipeline mode</strong>, select <strong class="bold">Triggered</strong>. This will ensure that the pipeline stops processing after a <span class="No-Break">successful run.</span></li>
				<li>For <strong class="bold">Paths</strong>, select this notebook from the repository for the <span class="No-Break">source code:</span><pre class="source-code">
Databricks-ML-in-Action/
Chapter 3: Building Out Our Bronze Layer/
Project: Streaming Transactions/
delta_live_tables/
CH3-03-Formatting_to_Delta_with_DLT</pre></li>				<li>Select <strong class="bold">Unity Catalog</strong> as your <span class="No-Break">storage option.</span></li>
				<li>Select <strong class="source-inline">ml_in_action</strong> for the catalog and <strong class="source-inline">synthetic_transactions_dlt</strong> as the <span class="No-Break">target schema.</span></li>
				<li>Enter <strong class="source-inline">table_name</strong> and <strong class="source-inline">synthetic_transactions_dlt</strong> as configurations in the <strong class="bold">Advanced</strong> section. Additionally, specify <a id="_idIndexMarker169"/>your<strong class="source-inline"> raw_data_location</strong> as<a id="_idIndexMarker170"/> follows (as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">):</span><pre class="source-code">
/Volumes/ml_in_action/
synthetic_transactions/files/
synthetic_transactions,</pre></li>			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer073">
					<img alt="Figure 3.13 – Advanced pipel﻿ine configuration settings for variables" src="image/B16865_03_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.13 – Advanced pipel<a id="_idTextAnchor158"/>ine configuration settings for variables</p>
			<h3>Methods in the DLT pipeline</h3>
			<p>Now that we<a id="_idIndexMarker171"/> have filled<a id="_idIndexMarker172"/> in the pipeline UI, let’s focus on the methods used in the source code of the pipeline. In this function, we mostly reuse our previous code (<span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.14</em>). Note that we do not use the setup file in this notebook. Instead, we use the variables we set in the pipeline settings’ advanced <span class="No-Break">configuration section.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer074">
					<img alt="Figure 3﻿.﻿14 – The autoloader stream method details" src="image/B16865_03_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3<a id="_idTextAnchor159"/>.<a id="_idTextAnchor160"/>14 – The autoloader stream method details</p>
			<h3>Generating the bronze table in a DLT pipeline</h3>
			<p>The <strong class="source-inline">generate_table()</strong> function feeds<a id="_idIndexMarker173"/> the read stream<a id="_idIndexMarker174"/> created using<a id="_idIndexMarker175"/> Auto Loader into DLT. We use <strong class="source-inline">spark.conf.get('variable_name')</strong> to access the variable values we defined in the pipeline’s advanced settings (<span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.13</em>). In the notebook, you will see the final step, a one-line cell <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">generate_table()</strong></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer075">
					<img alt="Figure 3.15 – Generating the Bronze table in a DLT pipeline" src="image/B16865_03_15.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.15 – Generating the Bronze table in a DLT pipeline</p>
			<p>DLT is unique. It is not code you run line by line, so you don’t execute the pipeline in the notebook. Back in the DLT UI, we save and click the <strong class="bold">Start</strong> button. After the setup process, you will see a graph that includes our table. <strong class="bold">Start</strong> not only starts the pipeline but creates it as well. Once<a id="_idIndexMarker176"/> it finishes, you will have<a id="_idTextAnchor161"/><a id="_idIndexMarker177"/> a screen like the<a id="_idIndexMarker178"/> one shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
			<h3>Querying streaming tables created with DLT</h3>
			<p>You can query a streaming<a id="_idIndexMarker179"/> table created with DLT just like you query other tables. This is handy for populating dashboards, which we will cover in <a href="B16865_08.xhtml#_idTextAnchor384"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Monitoring, Evaluating, </em><span class="No-Break"><em class="italic">and More</em></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">If in a notebook you try to query your streaming table, you may get <span class="No-Break">this error:</span></p>
			<p class="callout"><strong class="source-inline">ExecutionException: org.apache.spark.sql.AnalysisException: 403: Your token is missing the required scopes for </strong><span class="No-Break"><strong class="source-inline">this endpoint.</strong></span></p>
			<p class="callout">This is because to query streaming tables created by a DLT pipeline, you must use a shared cluster using Databricks Runtime 13.1 and above, or a SQL warehouse. Streaming tables created in a Unity Catalog-enabled pipeline cannot be queried from assigned or no-isolation clusters. You can change your cluster or use the DBSQL <span class="No-Break">query editor.</span></p>
			<p>The pipeline creates our Bronze table, wrapping<a id="_idTextAnchor162"/> up this project on streaming <span class="No-Break">transaction data.</span></p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor163"/>Project – Favorita store sales – time series forecasting</h2>
			<p>You downloaded the <em class="italic">Favorita Sales Forecasting</em> dataset<a id="_idIndexMarker180"/> from Kaggle using<strong class="source-inline"> opendatasets</strong> in the last chapter. We will use that data now to create Delta tables. To follow along in your own workspace, open the <span class="No-Break"><strong class="source-inline">CH3-01-Loading_Sales_CSV_Data_as_Delta</strong></span><span class="No-Break"> notebook.</span></p>
			<p>The downloaded data is in single CSV files. We use pandas to read the datasets and Spark to write to a Delta table. We demonstrate this for only the first file in the following code block (<span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.16</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer076">
					<img alt="Figure 3.16 – Reading in the sales holiday events datasets with Pandas" src="image/B16865_03_16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.16 – Reading in the sales holiday events datasets with Pandas</p>
			<p>We utilized the data profile capability (<span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.17</em>) to check the data types before writing to a table. Profiling shows the inferred data type for the date field is a string rather than a date or timestamp. Therefore, we alter the data type before writing to a <span class="No-Break">Delta table.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer077">
					<img alt="Figure 3.17 – Utilizing display(df), we can click + to see the data profile. This lets us look at the data types and distributions quickly" src="image/B16865_03_17.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.17 – Utilizing display(df), we can click + to see the data profile. This lets us look at the data types and distributions quickly</p>
			<p>Each table for the Favorita project is transformed into Delta tables similarly. As a result, we only include the first table in the book’s pages. However, the code that transforms each<a id="_idIndexMarker181"/> <a id="_idTextAnchor164"/><a id="_idTextAnchor165"/>of the tables is, of course, in <span class="No-Break">the repository.</span></p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor166"/>Project – a retrieval augmented generation chatbot</h2>
			<p><strong class="bold">RAG</strong> stands for <strong class="bold">Retrieval Augmented Generation</strong>. A RAG system often consists<a id="_idIndexMarker182"/> of your data, a vector database, a search algorithm, and a <strong class="bold">generative AI</strong> model to generate an answer<a id="_idIndexMarker183"/> to a user’s query. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.18</em> shows the pipeline we will build throughout <span class="No-Break">this book.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer078">
					<img alt="Figure 3.18 – A pipeline example that we will try to replicate through the book" src="image/B16865_03_18.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.18 – A pipeline example that we will try to replicate through the book</p>
			<p>We can see from <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.18</em> that we need to retrieve relevant data according to the user query. This is where our search method<a id="_idIndexMarker184"/> would access a vector database and conduct a semantic or hybrid search. At this stage in the RAG chatbot project, we have our PDFs in their raw, unstructured form. We want our users to access this knowledge via our chatbot, so we have to bring relevant content with information from all the PDFs into our chatbot, running in real time. To do so, we’ll convert all our PDFs into a machine-readable format. This chapter shows how to extract unstructured data from PDF files, chunk it, and transform your text into embeddings. Then, we store the embeddings in a <span class="No-Break">Delta table:</span></p>
			<ol>
				<li>Our first step for data preparation is to extract unstructured information from PDF files. To follow along in your workspace, open the <span class="No-Break"><strong class="source-inline">CH3-01-Creating_EmbeddedChunks</strong></span><span class="No-Break"> notebook.</span></li>
				<li>To start building the Bronze data layer, create an empty Delta table that includes the table’s schema. Use the <strong class="source-inline">GENERATED BY DEFAULT AS IDENTITY</strong> feature to leverage Delta table capabilities to index the newly arriving data automatically. Also, add a table property to set the <strong class="source-inline">Change Data Feed</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">true</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer079">
					<img alt="Figure 3.19 – Create an empty Delta table with the name pdf_documentation_text" src="image/B16865_03_19.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.19 – Create an empty Delta table with the name pdf_documentation_text</p>
			<ol>
				<li value="3">For the next step, read the raw <a id="_idIndexMarker185"/>PDFs from the <strong class="source-inline">volume</strong> folder and save them to a table named <strong class="source-inline">pdf_raw</strong> (<span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.20</em>). We will come back to the <span class="No-Break"><strong class="source-inline">pdf_documentation_text</strong></span><span class="No-Break"> table.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer080">
					<img alt="Figure 3.20 – Read the PDF file in the binary format and write it into a Bronze layer table" src="image/B16865_03_20.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.20 – Read the PDF file in the binary format and write it into a Bronze layer table</p>
			<ol>
				<li value="4">Store the PDFs in the binary format in the <strong class="source-inline">content</strong> column to extract the text later. Let’s see how it looks in a Delta table view. The binary format of each PDF is in the <span class="No-Break"><strong class="source-inline">content</strong></span><span class="No-Break"> column:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer081">
					<img alt="Figure 3.21 – Displaying ingested content in the Delta table" src="image/B16865_03_21.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.21 – Displaying ingested content in the Delta table</p>
			<ol>
				<li value="5">Next, we write a helper function using the <strong class="source-inline">unstructured</strong> library to extract the text from the PDF bytes. The function is in the <span class="No-Break"><strong class="source-inline">mlia_utils.rag_funcs</strong></span><span class="No-Break"> script.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer082">
					<img alt="Figure 3.22 – Creating a helper function to extract document text" src="image/B16865_03_22.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.22 – Creating a helper function to extract document text</p>
			<ol>
				<li value="6">Let’s apply this function<a id="_idIndexMarker186"/> to one of the PDF files we have and check the content of <span class="No-Break">our document:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer083">
					<img alt="Figure 3.23 – Applying a helper function to extract information from the PDF" src="image/B16865_03_23.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.23 – Applying a helper function to extract information from the PDF</p>
			<p class="list-inset">Great! <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.23</em> gives us a glimpse into one of the ingested documents. You may have many PDFs, which can be very long. Long documents can potentially be a problem for our future chatbot because they can easily exceed most LLMs maximum context lengths (check out <em class="italic">Further reading</em> for more information on context lengths). Additionally, we likely don’t need an entire document’s worth of text to answer a specific question. Instead, we need a section or “chunk” of this text. For this project, we create chunks of no more than 500 tokens with a chunk overlap of 50, using the <strong class="source-inline">SentenceSplitter</strong> module of the <strong class="source-inline">LlamaIndex</strong> library. You could also use the <strong class="source-inline">LangChain</strong> library or any library you choose to split the content into chunks. We will use the open source <strong class="source-inline">Llama-tokenizer</strong>, as this will be our main family of models across our project. Note that tokenizers may play a crucial role in your RAG quality. We have leveraged a Pandas <strong class="bold">User Defined Function</strong> (<strong class="bold">UDF</strong>) to scale across all pages<a id="_idIndexMarker187"/> of <span class="No-Break">all documents.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer084">
					<img alt="Figure 3.24 – Creating a pandas UDF for our extractor function" src="image/B16865_03_24.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.24 – Creating a pandas UDF for our extractor function</p>
			<ol>
				<li value="7">Now, let’s apply this function to our <span class="No-Break">Delta table:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer085">
					<img alt="Figure 3.25 – Applying a helper function to extract information from the PDF using PySpark" src="image/B16865_03_25.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.25 – Applying a helper function to extract information from the PDF using PySpark</p>
			<p>Once our chunks are ready, we need to convert them into embeddings. Embeddings are the format required to perform a semantic search when ingesting into our Silver <span class="No-Break">layer later.</span></p>
			<p>Databricks Model<a id="_idIndexMarker188"/> Serving now supports <strong class="bold">Foundation Model APIs</strong> (<strong class="bold">FMAPIs</strong>), which allow you to access<a id="_idIndexMarker189"/> and query state-of-the-art open models from a serving endpoint. With FMAPIs, you can quickly and easily build applications that leverage a high-quality GenAI model without maintaining your own model deployment (for more information, see <em class="italic">Deploy provisioned throughput Foundation Model APIs</em> in the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section).</span></p>
			<p>FMAPIs are provided in two <span class="No-Break">access modes:</span></p>
			<ul>
				<li><strong class="bold">Pay-per-token</strong>: This is the easiest way<a id="_idIndexMarker190"/> to start accessing foundation models on Databricks and is recommended for beginning your journey <span class="No-Break">with them.</span></li>
				<li><strong class="bold">Provisioned throughput</strong>: This model is recommended<a id="_idIndexMarker191"/> for workloads that require performance guarantees, fine-tuned models, or have additional <span class="No-Break">security requirements:</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer086">
					<img alt="Figure 3.26 – Available models for access via the FMAPIs of Databricks" src="image/B16865_03_26.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.26 – Available models for access via the FMAPIs of Databricks</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">At the time of writing, the FMAPI is available only to US regions. If your workspace is not yet in a supported region, you can use any model of your choice (OpenAI, BERT, a LlaMA tokenizer, etc.) to convert your content <span class="No-Break">into embeddings.</span></p>
			<p class="callout">You may also need to fine-tune your model embedding to learn from your own content for better <span class="No-Break">retrieval results.</span></p>
			<p>Next, we leverage the DI Platform’s pay-per-token<a id="_idIndexMarker192"/> capability from the FMAPI that provides you with access to the <strong class="source-inline">BGE_large_En</strong> endpoint, through the new functionality recently added<a id="_idIndexMarker193"/> to the <strong class="source-inline">mlflow &gt;=2.9</strong> - <strong class="source-inline">mlflow</strong> deployments (previously known as AI Gateway). This functionality unifies the model serving endpoint management <span class="No-Break">on Databricks.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer087">
					<img alt="Figure 3.27 – Applying the FMAPI with the BGE endpoint to convert “What is ChatGPT?” into an embedding" src="image/B16865_03_27.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.27 – Applying the FMAPI with the BGE endpoint to convert “What is ChatGPT?” into an embedding</p>
			<p>Now, we apply this embedding<a id="_idIndexMarker194"/> conversion across all our chunks, and we again make <strong class="source-inline">pandasUDF</strong> for <span class="No-Break">scalability purposes.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer088">
					<img alt="Figure 3.28 – pandasUDF to apply embedding conversion across all chunks" src="image/B16865_03_28.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.28 – pandasUDF to apply embedding conversion across all chunks</p>
			<p>Applying our UDF will append our <strong class="source-inline">raw_table</strong> chunks with the <span class="No-Break">corresponding embeddings:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer089">
					<img alt="Figure 3.29 – pandasUDF to apply embedding conversion across all chunks" src="image/B16865_03_29.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.29 – pandasUDF to apply embedding conversion across all chunks</p>
			<p>Once the final step of our data preparation process is completed, we save our table in the initially pre-created<a id="_idIndexMarker195"/> Delta table <strong class="source-inline">pdf_documentation_text</strong> using <span class="No-Break">append mode.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">We’ve ingested our PDFs for this project in one big batch, which works perfectly fine as an example. However, it also means that anytime you want to add a new PDF to your chatbot’s knowledge base, you must manually rerun all of the preceding steps. We recommend a workflow for production-grade solutions to automate the preceding steps and incrementally ingest PDFs as they arrive <span class="No-Break">in storage.</span></p>
			<p>We now have a dataset ready for a vect<a id="_idTextAnchor167"/>or search index, which we’ll cover in <a href="B16865_04.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 4</em></span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor168"/>Project – multilabel image classification</h2>
			<p>In <a href="B16865_02.xhtml#_idTextAnchor073"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, we extracted<a id="_idIndexMarker196"/> and stored our raw image data in our volume. In this chapter, we prepare our image dataset and save training and validation sets into Delta tables. To follow along in your workspace, open the <span class="No-Break"><strong class="source-inline">Ch3-01-Loading_Images_2_DeltaTables</strong></span><span class="No-Break"> notebook:</span></p>
			<ol>
				<li>We start by creating variables and removing existing data if the <strong class="source-inline">Reset</strong> widget value <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">True</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer090">
					<img alt="Figure 3.30 – Cleaning up existing data if Reset = True" src="image/B16865_03_30.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.30 – Cleaning up existing data if Reset = True</p>
			<ol>
				<li value="2">Next, we create a function to ingest all our images in one table. Initially, each label is in its own <strong class="source-inline">folder_label_name</strong>. We extract <strong class="source-inline">image_name</strong>, <strong class="source-inline">image_id</strong>, and <strong class="source-inline">label_id</strong>, as well as create <strong class="source-inline">label_name</strong> using <span class="No-Break">append mode.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer091">
					<img alt="Figure 3.31 – Creating the prep_data2delta function" src="image/B16865_03_31.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.31 – Creating the prep_data2delta function</p>
			<p>We use the <strong class="source-inline">prep_data2delta</strong> function to load and prepare our training and validation datasets (<span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.32</em>). Note that the function will save a Delta table if the <strong class="source-inline">write2delta</strong> flag is <strong class="source-inline">True</strong> and will return <a id="_idIndexMarker197"/>a DataFrame if the value for the <strong class="source-inline">returnDF</strong> flag is <strong class="source-inline">True</strong>. Next, in the notebook, we call <strong class="source-inline">prep<a id="_idTextAnchor169"/>_data2delta</strong> for the training and <span class="No-Break">validation sets.</span></p>
			<h3>Let’s talk about data loaders</h3>
			<p>We load data in<a id="_idIndexMarker198"/> a fixed-size batch<a id="_idIndexMarker199"/> when fine-tuning or training our deep learning models. Each framework natively supports specific data types; some expand their native formats to other open source formats. Data scientists sometimes prefer to keep their data (images, in our case) in blob storage and read it directly from storage rather than using Delta tables, as they think this avoids additional work. However, we recommend storing images in a Delta table unless you are working with large images greater than one GB per image. Storing your images in a Delta table allows you to take advantage of Delta and Unity Catalog’s additional benefits, such as lineage of data and models, data version control, duplicate data checks, and <span class="No-Break">quality assurance.</span></p>
			<p>At the time of writing, you have a few options to read your data while working with a PyTorch or PyTorch <span class="No-Break">Lightning framework:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">DeltaTorchLoader</strong></span><span class="No-Break"> (recommended)</span></li>
				<li><span class="No-Break">Petastorm</span></li>
				<li>Reading images directly <span class="No-Break">from blob/disk/volumes</span></li>
			</ul>
			<p>Our recommendation<a id="_idIndexMarker200"/> is to use DeltaTorchLoader. It handles<a id="_idIndexMarker201"/> data batching, sampling, and multiprocessing while training PyTorch pipelines without requiring a temporary copy of files, like with Petastorm. See <em class="italic">Further reading</em> for <span class="No-Break">more information.</span></p>
			<p>At the time of writing, DeltaTorchLoader, which we are going to use to load and transform our data from Delta into the PyTorch/Lightning dataloader framework to train our model in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, requires you to have tables in the unmanaged Delta format in volumes when using UC. Don’t worry; the lineage is associated with the same path to the volume as your main dataset. We’ll talk more about lineage in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>. This requirement is due to UC’s read/write security permissions with blob storage. The blob storage maintainers do not support these security settings yet. If you are not using UC, you should be able to read Delta tables directly from the <span class="No-Break">managed tables.</span></p>
			<p>There is also an option to read your data using the Petastorm library. We don’t recommend Petastorm because it requires a deeper understanding of certain pitfalls. The most common are memory usage issues due to data caching and the fact that it uses Apache <strong class="source-inline">Parquet</strong> files rather than Delta files, so it consumes all versions of your <span class="No-Break">parquet files.</span></p>
			<p>The creators of the DeltaTorchLoader performed a few<a id="_idIndexMarker202"/> benchmarks with Petastorm. The benchmark was shared at the <strong class="bold">Data and AI Summit</strong> (<strong class="bold">DAIS</strong>) and is featured in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.33</em>. In this project, we will compare Petastorm to the classic Torch loader in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> to show the performance gain. The comparison demonstrates an incredible speed increase when reading the batch of data. We’ve also included a great video on <em class="italic">TorchDeltaLoader</em> in <em class="italic">Further reading</em> if you want to <span class="No-Break">learn more.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer092">
					<img alt="Figure 3.32 – The DeltaTorchLoader benchmark – a performance comparison between the DeltaTorch and Petastorm loaders" src="image/B16865_03_32.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.32 – The DeltaTorchLoader benchmark – a performance comparison between the DeltaTorch and Petastorm loaders</p>
			<p>You can keep filenames in your Delta table instead of the images and collect them while passing them to the main PyTorch Loader with the <strong class="source-inline">trainer</strong> function. Keeping files in Delta is essential to avoid duplicates and control the list used during training and validation, as you can pass the Delta version<a id="_idIndexMarker203"/> to<a id="_idTextAnchor170"/> MLflow during tracking<a id="_idIndexMarker204"/> for full <span class="No-Break">replication purposes.</span></p>
			<h3>Optimizing our data</h3>
			<p>Once our tables are created<a id="_idIndexMarker205"/> and written to the storage, we use<a id="_idIndexMarker206"/> a few functions to improve read performance on the Delta tables. First, we use <strong class="source-inline">OPTIMIZE</strong> to keep an ideal number of files (<span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.34</em>). Second, we disable deletion vectors because the <strong class="source-inline">DeltaTorchReader</strong> does not support them yet <em class="italic">(</em><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.35</em>). We use the SQL magic command, <strong class="source-inline">%sql</strong>, to perform these operations, using SQL in the <span class="No-Break">Python notebook.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer093">
					<img alt="Figure 3.33 – Optimizing the file size and count of the training table" src="image/B16865_03_33.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.33 – Optimizing the file size and count of the training table</p>
			<p>Note that the variables we saved in Python are inaccessible in SQL, so we hardcode them in this example. You could include SQL variables in the <strong class="source-inline">global-setup</strong> notebook to <span class="No-Break">avoid this.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer094">
					<img alt="Figure 3.34 – Optimizing the training table" src="image/B16865_03_34.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.34 – Optimizing the training table</p>
			<p>Now, we have the training and validation<a id="_idIndexMarker207"/> tables loaded and optimized<a id="_idIndexMarker208"/> to efficiently work with this image <a id="_idTextAnchor171"/>data to fine-tune our multi-class computer <span class="No-Break">vision models.</span></p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor172"/>Summary</h1>
			<p>In this chapter, we focused on the essentials of building out the Bronze data layer within the Databricks Data Intelligence Platform. We emphasized the importance of schema evolution, DLT, and the conversion of data into the Delta format and applied these principles in our example projects. This chapter highlighted the significance of tools such as Auto Loader and DLT in this process. Auto Loader, with its proficiency in handling file tracking and automating schema management, alongside DLT’s robust capabilities in pipeline development and data quality assurance, are pivotal in our data management strategy. These tools facilitate an efficient and streamlined approach to data pipeline management, enabling us as data scientists to focus more on valuable tasks, such as feature engineering <span class="No-Break">and experimentation.</span></p>
			<p>With our Bronze layer created, we now move on from this foundational work to a more advanced layer of data – the Silver layer. <a href="B16865_04.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Transformations toward Our Silver Layer</em>, will take us deeper into our data and demonstrate various Databricks tools that will ai<a id="_idTextAnchor173"/>d us in the exploration and transformations of <span class="No-Break">our data.</span></p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor174"/>Questions</h1>
			<p>The following questions solidify key points to remember and tie the content back to <span class="No-Break">your experience:</span></p>
			<ol>
				<li>What are the names of the layers in the Medallion <span class="No-Break">architecture design?</span></li>
				<li>If you wanted to build a managed pipeline with streaming data, which product would you use – Structured Streaming <span class="No-Break">or DLT?</span></li>
				<li>What feature did we use to add the <strong class="source-inline">product</strong> column to our streaming transaction data without <span class="No-Break">manual intervention?</span></li>
				<li>Do you have projects from your current position, experience, or on your roadmap that would benefit from one or more of the topics covered in <span class="No-Break">this chapter?</span></li>
				<li>What is a possible way to lessen the number of p<a id="_idTextAnchor175"/>artitions when partitioning on a high <span class="No-Break">cardinality column?</span></li>
			</ol>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor176"/>Answers</h1>
			<p>After putting thought into the questions, compare your answers <span class="No-Break">to ours:</span></p>
			<ol>
				<li>The layers of the Medallion architecture are Bronze, Silver, <span class="No-Break">and Gold.</span></li>
				<li>We recommend DLT build <span class="No-Break">managed pipelines.</span></li>
				<li>In the streaming transactions project example, we used Auto Loader’s schema evolution feature to add a column without <span class="No-Break">manual intervention.</span></li>
				<li>We hope so! One example is a managed streaming data pipeline that could benefit from the built-in data quality monitoring that comes <span class="No-Break">with DLT.</span></li>
				<li>Bucketing is an optimal method specifically designed to provide an additional layer of organization in your data. It can reduce the number of output files and organize the data better for subsequent reading, and it can be especially <a id="_idTextAnchor177"/>useful when the partitioning column has <span class="No-Break">high cardinality.</span></li>
			</ol>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor178"/>Further reading</h1>
			<p>This chapter covered different methods of ingesting data into your Bronze layer. Take a look at these resources to read more about the areas that interest <span class="No-Break">you most:</span></p>
			<ul>
				<li><em class="italic">Use liquid clustering for Delta </em><span class="No-Break"><em class="italic">tables</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://docs.databricks.com/en/delta/clustering.html"><span class="No-Break">https://docs.databricks.com/en/delta/clustering.html</span></a></li>
				<li><em class="italic">Spark Structured </em><span class="No-Break"><em class="italic">Streaming</em></span><span class="No-Break">: </span><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"><span class="No-Break">https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</span></a></li>
				<li><em class="italic">Delta Live </em><span class="No-Break"><em class="italic">Tables</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/delta-live-tables/index.html"><span class="No-Break">https://docs.databricks.com/en/delta-live-tables/index.html</span></a></li>
				<li><em class="italic">DLT Databricks </em><span class="No-Break"><em class="italic">Demo</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://www.databricks.com/resources/demos/tutorials/lakehouse-platform/full-delta-live-table-pipeline"><span class="No-Break">https://www.databricks.com/resources/demos/tutorials/lakehouse-platform/full-delta-live-table-pipeline</span></a></li>
				<li><em class="italic">Auto Loader </em><span class="No-Break"><em class="italic">options</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/ingestion/auto-loader/options.html"><span class="No-Break">https://docs.databricks.com/ingestion/auto-loader/options.html</span></a></li>
				<li><em class="italic">Schema evolution with Auto </em><span class="No-Break"><em class="italic">Loader</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/ingestion/auto-loader/schema.html#configure-schema-inference-and-evolution-in-auto-loader"><span class="No-Break">https://docs.databricks.com/ingestion/auto-loader/schema.html#configure-schema-inference-and-evolution-in-auto-loader</span></a></li>
				<li><em class="italic">Common loading patterns with Auto </em><span class="No-Break"><em class="italic">Loader</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/ingestion/auto-loader/patterns.html"><span class="No-Break">https://docs.databricks.com/ingestion/auto-loader/patterns.html</span></a></li>
				<li><em class="italic">Stream processing with Apache Kafka and </em><span class="No-Break"><em class="italic">Databricks</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://docs.databricks.com/structured-streaming/kafka.html"><span class="No-Break">https://docs.databricks.com/structured-streaming/kafka.html</span></a></li>
				<li><em class="italic">How We Performed ETL on One Billion Records For Under $1 With Delta Live </em><span class="No-Break"><em class="italic">Tables</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html"><span class="No-Break">https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html</span></a></li>
				<li><em class="italic">Create tables – Managed vs </em><span class="No-Break"><em class="italic">External</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/data-governance/unity-catalog/create-tables.html#create-tables"><span class="No-Break">https://docs.databricks.com/en/data-governance/unity-catalog/create-tables.html#create-tables</span></a></li>
				<li><em class="italic">Take full advantage of the auto-tuning </em><span class="No-Break"><em class="italic">available</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://docs.databricks.com/delta/tune-file-size.html#configure-delta-lake-to-control-data-file-size"><span class="No-Break">https://docs.databricks.com/delta/tune-file-size.html#configure-delta-lake-to-control-data-file-size</span></a></li>
				<li>Import Python modules from Databricks repos: <a href="https://docs.databricks.com/en/delta-live-tables/import-workspace-files.html">https://docs.databricks.com/en/delta-live-tables/import-workspace-files.html</a></li>
				<li>Deletion <span class="No-Break">Vectors: </span><a href="https://docs.databricks.com/en/delta/deletion-vectors.html"><span class="No-Break">https://docs.databricks.com/en/delta/deletion-vectors.html</span></a></li>
				<li><em class="italic">Databricks ML </em><span class="No-Break"><em class="italic">Runtime</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://docs.databricks.com/runtime/mlruntime.html#introduction-to-databricks-runtime-for-machine-learning"><span class="No-Break">https://docs.databricks.com/runtime/mlruntime.html#introduction-to-databricks-runtime-for-machine-learning</span></a></li>
				<li><em class="italic">Cluster advanced </em><span class="No-Break"><em class="italic">options</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://docs.databricks.com/en/clusters/configure.html#spark-configuration"><span class="No-Break">https://docs.databricks.com/en/clusters/configure.html#spark-configuration</span></a></li>
				<li><em class="italic">Deploy provisioned throughput Foundation Model </em><span class="No-Break"><em class="italic">APIs</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html"><span class="No-Break">https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html</span></a></li>
				<li><em class="italic">Scaling Deep Learning Using Delta Lake Storage Format on </em><span class="No-Break"><em class="italic">Databricks</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/dataaisummit/session/scaling-deep-learning-using-delta-lake-storage-format-databricks/"><span class="No-Break">https://www.databricks.com/dataaisummit/session/scaling-deep-learning-using-delta-lake-storage-format-databricks/</span></a></li>
				<li><span class="No-Break"><em class="italic">DeltaTorchLoader</em></span><span class="No-Break">: </span><a href="https://github.com/delta-incubator/deltatorch"><span class="No-Break">https://github.com/delta-incubator/deltatorch</span></a></li>
			</ul>
		</div>
	

		<div class="Content" id="_idContainer096">
			<h1 id="_idParaDest-81" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor179"/>Part 2: Heavily Use Case-Focused</h1>
		</div>
		<div id="_idContainer097">
			<p>This part introduces you to taking a set of data sources and working with them throughout the platform, from one end to another. The goal of this part is simply to demonstrate how to thoughtfully use all the bells and whistles of the platform. This part provides stories, code, lakehouse features, and <span class="No-Break">best practices.</span></p>
			<p>This part has the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B16865_04.xhtml#_idTextAnchor180"><em class="italic">Chapter 4</em></a>, <em class="italic">Getting to Know Your Data</em></li>
				<li><a href="B16865_05.xhtml#_idTextAnchor244"><em class="italic">Chapter 5</em></a>, <em class="italic">Feature Engineering on Databricks</em></li>
				<li><a href="B16865_06.xhtml#_idTextAnchor297"><em class="italic">Chapter 6</em></a>, <em class="italic">Searching for a Signal</em></li>
				<li><a href="B16865_07.xhtml#_idTextAnchor325"><em class="italic">Chapter 7</em></a>, <em class="italic">Productionizing ML on Databricks</em></li>
				<li><a href="B16865_08.xhtml#_idTextAnchor384"><em class="italic">Chapter 8</em></a>, <em class="italic">Monitoring, Evaluating, and More</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer098">
			</div>
		</div>
		<div>
			<div class="Basic-Graphics-Frame" id="_idContainer099">
			</div>
		</div>
	</body></html>