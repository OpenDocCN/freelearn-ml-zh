- en: Text Mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"I think it''s much more interesting to live not knowing than to have answers
    which might be wrong."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Richard Feynman'
  prefs: []
  type: TYPE_NORMAL
- en: The world is awash with textual data. If you Google, Bing, or Yahoo how much
    of that data is unstructured, that is, in a textual format, estimates would range
    from 80 to 90 percent. The real number doesn't matter. What does matter is that
    a large proportion of the data is in a text format. The implication is that anyone
    seeking to find insights in that data must develop the capability to process and
    analyze text.
  prefs: []
  type: TYPE_NORMAL
- en: When I first started out as a market researcher, I used to manually pore through
    page after page of moderator-led focus groups and interviews with the hope of
    capturing some qualitative insight an Aha! moment if you will-and then haggle
    with fellow team members over whether they had the same insight or not. Then,
    you would always have that one individual in a project who would swoop in and
    listen to two interviews-out of the 30 or 40 on the schedule and, alas, they had
    their mind made up on what was really happening in the world. Contrast that with
    the techniques being used now, where an analyst can quickly distill data into
    meaningful quantitative results, support qualitative understanding, and maybe
    even sway the swooper.
  prefs: []
  type: TYPE_NORMAL
- en: Over the last few years, I've applied the techniques discussed here to mine
    physician-patient interactions, understand FDA fears on prescription drug advertising,
    and capture patient concerns about a rare cancer, to name just a few. Using R
    and the methods in this chapter, you too can extract the powerful information
    in textual data.
  prefs: []
  type: TYPE_NORMAL
- en: Text mining framework and methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many different methods to use in text mining. The goal here is to
    provide a basic framework to apply to such an endeavor. This framework is not
    all-inclusive of the possible methods but will cover those that are probably the
    most important for the vast majority of projects that you will work on. Additionally,
    I will discuss the modeling methods in as succinct and clear a manner as possible,
    because they can get quite complicated. Gathering and compiling text data is a
    topic that could take up several chapters. Therefore, let's begin with the assumption
    that our data is available from Twitter, a customer call center, scraped off the
    web, or whatever, and is contained in some sort of text file or files.
  prefs: []
  type: TYPE_NORMAL
- en: The first task is to put the text files in one structured file referred to as
    a **corpus**. The number of documents could be just one, dozens, hundreds, or
    even thousands. R can handle a number of raw text files, including RSS feeds,
    PDF files, and MS Word documents. With the corpus created, the data preparation
    can begin with the text transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list is comprised of probably some of the most common and useful
    transformations for text files:'
  prefs: []
  type: TYPE_NORMAL
- en: Change capital letters to lowercase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove punctuation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove stop words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove excess whitespace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word stemming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word replacement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In transforming the corpus, you are creating not only a more compact dataset,
    but also simplifying the structure in order to facilitate relationships among
    the words, thereby leading to an increased understanding. However, keep in mind
    that not all of these transformations are necessary all the time and judgment
    must be applied, or you can iterate to find the transformations that make the
    most sense.
  prefs: []
  type: TYPE_NORMAL
- en: By changing words to lowercase, you can prevent the improper counting of words.
    Say that you have a count for hockey three times and Hockey once, where it is
    the first word in a sentence. R will not give you a count of hockey=4, but hockey=3
    and Hockey=1.
  prefs: []
  type: TYPE_NORMAL
- en: Removing punctuation also achieves the same purpose, but as we will see in the
    business case, punctuation is important if you want to split your documents by
    sentences.
  prefs: []
  type: TYPE_NORMAL
- en: In removing stop words, you are getting rid of the common words that have no
    value; in fact, they are detrimental to the analysis, as their frequency masks
    important words. Examples of stop words *are, and, is, the, not,* and *to*. Removing
    whitespace makes a more compact corpus by getting rid of things such as tabs,
    paragraph breaks, double-spacing, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The stemming of words can get a bit tricky and might add to your confusion because
    it deletes word suffixes, creating the base word, or what is known as the **radical**.
    I personally am not a big fan of stemming and the analysts I've worked with agree
    with that sentiment. However, you can use the stemming algorithm included in the
    R package, `tm`, where the function calls the **porter stemming algorithm** from
    the `SnowballC` package. An example of stemming would be where your corpus has
    family and families. Recall that R would count this as two separate words. By
    running the stemming algorithm, the stemmed word for the two instances would become
    *famili*. This would prevent the incorrect count, but in some cases, it can be
    odd to interpret and is not very visually appealing in a wordcloud for presentation
    purposes. In some cases, it may make sense to run your analysis with both stemmed
    and unstemmed words in order to see which one makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: Probably the most optional of the transformations is to replace the words. The
    goal of replacement is to combine the words with a similar meaning, for example,
    management and leadership. You can also use it in lieu of stemming. I once examined
    the outcome of stemmed and unstemmed words and concluded that I could achieve
    a more meaningful result by replacing about a dozen words instead of stemming.
  prefs: []
  type: TYPE_NORMAL
- en: With the transformation of the corpus completed, the next step is to create
    either a **Document-Term Matrix** (**DTM**) or **Term-Document Matrix** (**TDM**).
    What either of these matrices does is create a matrix of word counts for each
    individual document in the matrix. A DTM would have the documents as rows and
    the words as columns, while in a TDM, the reverse is true. Text mining can be
    performed on either matrix.
  prefs: []
  type: TYPE_NORMAL
- en: With a matrix, you can begin to analyze the text by examining word counts and
    producing visualizations such as `wordclouds`. One can also find word associations
    by producing correlation lists for specific words. It also serves as a necessary
    data structure in order to build topic models.
  prefs: []
  type: TYPE_NORMAL
- en: Topic models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic models are a powerful method to group documents by their main topics.
    Topic models allow probabilistic modeling of term frequency occurrences in documents.
    The fitted model can be used to estimate the similarity between documents, as
    well as between a set of specified keywords using an additional layer of latent
    variables, which are referred to as topics (Grun and Hornik, 2011). In essence,
    a document is assigned to a topic based on the distribution of the words in that
    document, and the other documents in that topic will have roughly the same frequency
    of words.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm that we will focus on is **Latent Dirichlet Allocation** (**LDA**)
    with Gibbs sampling, which is probably the most commonly used sampling algorithm.
    In building topic models, the number of topics must be determined before running
    the algorithm (k-dimensions). If no apriori reason for the number of topics exists,
    then you can build several and apply judgment and knowledge to the final selection.
    LDA with Gibbs sampling is quite complicated mathematically, but my intent is
    to provide an introduction so that you are at least able to describe how the algorithm
    learns to assign a document to a topic in layman terms. If you are interested
    in mastering the math, block out a couple of hours on your calendar and have a
    go at it. Excellent background material is available at [http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'LDA is a generative process, and so, the following will iterate to a steady
    state:'
  prefs: []
  type: TYPE_NORMAL
- en: For each document (*j*), there are *1* to *j* documents. We will randomly assign
    it a multinomial distribution (**dirichlet** **distribution**) to the topics (*k*)
    with *1* to *k* topics, for example, document *A* is 25 percent topic one, 25
    percent topic two, and 50 percent topic three.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Probabilistically, for each word (*i*), there are *1* to *i* words to a topic
    (*k*); for example, the word *mean* has a probability of 0.25 for the topic statistics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each word(*i*) in document(*j*) and topic(*k*), calculate the proportion
    of words in that document assigned to that topic; note it as the probability of
    topic(*k*) with document(*j*), *p(k|j)*, and the proportion of word(*i*) in topic(*k*)
    from all the documents containing the word. Note it as the probability of word(*i*)
    with topic(*k*), *p(i|k)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resample, that is, assign *w* a new *t* based on the probability that *t* contains
    *w*, which is based on *p(k|j)* times *p(i|k)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rinse and repeat; over numerous iterations, the algorithm finally converges
    and a document is assigned a topic based on the proportion of words assigned to
    a topic in that document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LDA that we will be doing assumes that the order of words and documents
    does not matter. There has been work done to relax these assumptions in order
    to build models of language generation and sequence models over time (known as
    **dynamic topic modelling**).
  prefs: []
  type: TYPE_NORMAL
- en: Other quantitative analyses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now shift gears to analyze text semantically based on sentences and
    the tagging of words based on the parts of speech, such as noun, verb, pronoun,
    adjective, adverb, preposition, singular, plural, and so on. Often, just examining
    the frequency and latent topics in the text will suffice for your analysis. However,
    you may find occasions when a deeper understanding of the style is required in
    order to compare the speakers or writers.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many methods to accomplish this task, but we will focus on the following
    five:'
  prefs: []
  type: TYPE_NORMAL
- en: Polarity (sentiment analysis)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated readability index (complexity)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diversity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dispersion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polarity** is often referred to as sentiment analysis, which tells you how
    positive or negative the text is. By analyzing polarity in R with the `qdap` package,
    a score will be assigned to each sentence and you can analyze the average and
    standard deviation of polarity by groups such as different authors, text, or topics.
    Different polarity dictionaries are available and `qdap` defaults to one created
    by Hu and Liu, 2004\. You can alter or change this dictionary according to your
    requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm works by first tagging the words with a positive, negative, or
    neutral sentiment based on the dictionary. The tagged words are then clustered
    based on the four words prior and two words after a tagged word, and these clusters
    are tagged with what are known as **valence shifters** (neutral, negator, amplifier,
    and de-amplifier). A series of weights based on their number and position are
    applied to both the words and clusters. This is then summed and divided by the
    square root of the number of words in that sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Automated readability index is a measure of the text complexity and a reader''s
    ability to understand. A specific formula is used to calculate this index: *4.71(#
    of characters / #of words) + 0.5(# of words / # of sentences) - 21.43*.'
  prefs: []
  type: TYPE_NORMAL
- en: The index produces a number, which is a rough estimate of a student's grade
    level to fully comprehend. If the number is 9, then a high school freshman, aged
    13 to 15, should be able to grasp the meaning of the text.
  prefs: []
  type: TYPE_NORMAL
- en: The formality measure provides an understanding of how a text relates to the
    reader or speech relates to a listener. I like to think of it as a way to understand
    how comfortable the person producing the text is with the audience, or an understanding
    of the setting where this communication takes place. If you want to experience
    formal text, attend a medical conference or read a legal document. Informal text
    is said to be contextual in nature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formality measure is called **F-Measure**. This measure is calculated as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Formal words (*f*) are nouns, adjectives, prepositions, and articles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contextual words (*c*) are pronouns, verbs, adverbs, and interjections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N = sum of (f + c + conjunctions)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Formality Index = 50((sum of f - sum of c / N) + 1)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is totally irrelevant, but when I was in Iraq, one of the army generals-who
    shall remain nameless. I had to brief and write situation reports for was absolutely
    adamant that adverbs were not to be used, ever, or there would be wrath. The idea
    was that you can't quantify words such as highly or mostly because they mean different
    things to different people. Five years later, I still scour my business e-mails
    and PowerPoint presentations for unnecessary adverbs. Formality writ large!
  prefs: []
  type: TYPE_NORMAL
- en: 'Diversity, as it relates to text mining, refers to the number of different
    words used in relation to the total number of words used. This can also mean the
    expanse of the text producer''s vocabulary or lexicon richness. The `qdap` package
    provides five--that''s right, five--different measures of diversity: Simpson,
    Shannon, Collision, Bergen Parker, and Brillouin. I won''t cover these five in
    detail but will only say that the algorithms are used not only for communication
    and information science retrieval, but also for biodiversity in nature.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, dispersion, or lexical dispersion, is a useful tool in order to understand
    how words are spread throughout a document and serves as an excellent way to explore
    a text and identify patterns. The analysis is conducted by calling the specific
    word or words of interest, which are then produced in a plot showing when the
    word or words occurred in the text over time. As we will see, the `qdap` package
    has a built-in plotting function to analyze the text dispersion.
  prefs: []
  type: TYPE_NORMAL
- en: We covered a framework on text mining about how to prepare the text, count words,
    and create topic models and, finally, dived deep into other lexical measures.
    Now, let's apply all this and do some real-world text mining.
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this case study, we will take a look at president Obama's State of the Union
    speeches. I have no agenda here; just curious as to what can be uncovered in particular
    and if and how his message changed over time. Perhaps this will serve as a blueprint
    to analyze any politician's speech in order to prepare an opposing candidate in
    a debate or speech of their own. If not, so be it.
  prefs: []
  type: TYPE_NORMAL
- en: The two main analytical goals are to build topic models on the six State of
    the Union speeches and then compare the first speech in 2010 and the last in January,
    2016 for sentence-based textual measures, such as sentiment and dispersion.
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding and preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The primary package that we will use is `tm`, the text mining package. We will
    also need `SnowballC` for the stemming of the words, `RColorBrewer` for the color
    palettes in `wordclouds`, and the `wordcloud` package. Please ensure that you
    have these packages installed before attempting to load them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The data files are available for download in [https://github.com/datameister66/data](https://github.com/datameister66/data).
    Please ensure you put the text files into a separate directory because it will
    all go into our corpus for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the seven `.txt` files, for example `sou2012.txt`, into your working
    R directory. You can identify your current working directory and set it with these
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now begin to create the corpus by first creating an object with the
    path to the speeches and then seeing how many files are in this directory and
    what they are named:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will name our corpus `docs` and create it with the `Corpus()` function,
    wrapped around the directory source function, `DirSource()`, which is also part
    of the `tm` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that there is no `corpus` or `document level` metadata. There are functions
    in the `tm` package to apply things such as author's names and timestamp information,
    among others, at both `document level` and `corpus`. We will not utilize this
    for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now begin the text transformations using the `tm_map()` function from
    the `tm` package. These will be the transformations that we discussed previously--lowercase
    letters, remove numbers, remove punctuation, remove stop words, strip out the
    whitespace, and stem the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, it is a good idea to eliminate unnecessary words. For example,
    during the speeches, when `Congress` applauds a statement, you will find `(Applause)`
    in the text. This must be removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After completing the transformations and removal of other words, make sure
    that your documents are plain text, put it in a document-term matrix, and check
    the dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The six speeches contain `4738` words. It is optional, but one can remove the
    sparse terms with the `removeSparseTerms()` function. You will need to specify
    a number between zero and one where the higher the number, the higher the percentage
    of `sparsity` in the matrix. Sparsity is the relative frequency of a term in the
    documents. So, if your sparsity threshold is 0.75, only terms with sparsity greater
    than 0.75 are removed. For us that would be *(1 - 0.75) * 7*, which is equal to
    1.75\.  Therefore, any term in fewer than two documents would be removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As we don''t have the metadata on the documents, it is important to name the
    rows of the matrix so that we know which document is which:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `inspect()` function, you can examine the matrix. Here, we will look
    at the seven rows and the first five columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It appears that our data is ready for analysis, starting with looking at the
    word frequency counts. Let me point out that the output demonstrates why I've
    been trained to not favor wholesale stemming. You may be thinking that 'ability'
    and 'able' could be combined. If you stemmed the document you would end up with
    'abl'. How does that help the analysis? I think you lose context, at least in
    the initial analysis. Again, I recommend applying stemming thoughtfully and judiciously.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modeling will be broken into two distinct parts. The first will focus on word
    frequency and correlation and culminate in the building of a topic model. In the
    next portion, we will examine many different quantitative techniques by utilizing
    the power of the `qdap` package in order to compare two different speeches.
  prefs: []
  type: TYPE_NORMAL
- en: Word frequency and topic models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have everything set up in the document-term matrix, we can move on to
    exploring word frequencies by creating an object with the column sums, sorted
    in descending order. It is necessary to use `as.matrix()` in the code to sum the
    columns. The default order is ascending, so putting `-` in front of `freq` will
    change it to descending:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will examine the `head` and `tail` of the object with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The most frequent word is `new` and, as you might expect, the president mentions `america` frequently.
    Also notice how important employment is with the frequency of `jobs`. I find it
    interesting that he mentions Youngstown, for Youngstown, OH, a couple of times.
  prefs: []
  type: TYPE_NORMAL
- en: 'To look at the frequency of the word frequency, you can create tables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: What these tables show is the number of words with that specific frequency.
    So 354 words occurred three times; and one word, `new` in our case, occurred 193 times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `findFreqTerms()`, we can see which words occurred at least `125` times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find associations with words by correlation with the `findAssocs()`
    function. Let''s look at `jobs` as two examples using `0.85` as the correlation
    cutoff:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For visual portrayal, we can produce `wordclouds` and a bar chart. We will
    do two `wordclouds` to show the different ways to produce them: one with a minimum
    frequency and the other by specifying the maximum number of words to include.
    The first one with minimum frequency, also includes code to specify the color.
    The scale syntax determines the minimum and maximum word size by frequency; in
    this case, the minimum frequency is `70`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One can forgo all the fancy graphics, as we will in the following image, capturing
    the `25` most frequent words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To produce a bar chart, the code can get a bit complicated, whether you use
    base R, `ggplot2`, or `lattice`. The following code will show you how to produce
    a bar chart for the `10` most frequent words in base R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_03-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now move on to building topic models using the `topicmodels` package,
    which offers the `LDA()` function. The question now is how many topics to create.
    It seems logical to solve for three  `topics` (`k=3`). Certainly, I encourage
    you to try other numbers of topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can see an interesting transition over time. The first and last addresses
    have the same topic grouping, almost as if he opened and closed his tenure with
    the same themes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `terms()` function produces a list of an ordered word frequency for
    each topic. The list of words is specified in the function, so let''s look at
    the top `20` per topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`Topic 2` covers the first and last speeches. Nothing really stands out as
    compelling in that topic like the others. It will be interesting to see how the
    next analysis can yield insights into those speeches.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Topic 1` covers the next three speeches. Here, the message transitions to
    `"jobs"`, `"energy"`, `"reform"`, and the `"deficit"`, not to mention the comments
    about `"education"` and as we saw above, the correlation of `"jobs"` and `"colleges"`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Topic 3` brings us to the next two speeches. The focus seems to really shift
    on to the economy and business with mentions to `"security"` and healthcare.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we can dig into the exact speech content further, along
    with comparing and contrasting the first and last State of the Union addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Additional quantitative analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This portion of the analysis will focus on the power of the `qdap` package.
    It allows you to compare multiple documents over a wide array of measures. Our
    effort will be on comparing the 2010 and 2016 speeches. For starters, we will
    need into turn the text into data frames, perform sentence splitting, and then
    combine them to one data frame with a variable created that specifies the year
    of the speech. We will use this as our grouping variable in the analyses. Dealing
    with text data, even in R, can be tricky. The code that follows seemed to work
    the best in this case to get the data loaded and ready for analysis. We first
    load the `qdap` package. Then, to bring in the data from a text file, we will
    use the `readLines()` function from base R, collapsing the results to eliminate
    unnecessary whitespace. I also recommend putting your text encoding to ASCII,
    otherwise you may run into some bizarre text that will mess up your analysis.
    That is done with the `iconv()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The warning message is not an issue as it is just telling us that the final
    line of text is not the same length as the other lines in the `.txt` file. We
    now apply the `qprep()` function from `qdap`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is a wrapper for a number of other replacement functions and
    using it will speed pre-processing, but it should be used with caution if more
    detailed analysis is required. The functions it passes through are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bracketX()`: apply bracket removal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replace_abbreviation()`: replaces abbreviations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replace_number()`: numbers to words, for example ''100'' becomes ''one hundred'''
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replace_symbol()`: symbols become words, for example @ becomes ''at'''
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The other pre-processing we should do is to replace contractions (can''t to
    cannot), remove stopwords, in our case the top 100, and remove unwanted characters,
    with the exception of periods and question marks. They will come in handy shortly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Critical to this analysis is to now split it into sentences and add what will
    be the grouping variable, the year of the speech. This also creates the `tot`
    variable, which stands for Turn of Talk, serving as an indicator of sentence order.
    This is especially helpful in a situation where you are analyzing dialogue, say
    in a debate or question and answer session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Repeat the steps for the 2010 speech:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Concatenate the separate years into one dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the great things about the `qdap` package is that it facilitates basic
    text exploration, as we did before. Let''s see a plot of frequent terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can create a word frequency matrix that provides the counts for each word
    by speech:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This can also be converted into a document-term matrix with the function `as.dtm()`
    should you so desire. Let''s next build `wordclouds`, by year with `qdap` functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command produces the following two images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_05.png)![](img/image_12_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Comprehensive word statistics are available. Here is a plot of the stats available
    in the package. The plot loses some of its visual appeal with just two speeches,
    but is revealing nonetheless. A complete explanation of the stats is available
    under `?word_stats`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that the 2016 speech was much shorter, with over a hundred fewer sentences
    and almost a thousand fewer words. Also, there seems to be the use of asking questions
    as a rhetorical device in 2016 versus 2010 (n.quest 10 versus n.quest 4).
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare the polarity (sentiment scores), use the `polarity()` function,
    specifying the text and grouping variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `stan.mean.polarity` value represents the standardized mean polarity, which
    is the average polarity divided by the standard deviation. We see that `2015`
    was slightly higher (`0.267`) than `2010` (`0.121`). This is in line with what
    we would expect, wanting to end on a more positive note. You can also plot the
    data. The plot produces two charts. The first shows the polarity by sentences
    over time and the second shows the distribution of the polarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This plot may be a challenge to read in this text, but let me do my best to
    interpret it. The `2010` speech starts out with a strong negative sentiment and
    is slightly more negative than `2016`. We can identify the most negative sentiment
    sentence by creating a dataframe of the `pol` object, find the sentence number,
    and produce it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that is negative sentiment! Ironically, the government is even more in
    debt today. We will look at the readability index next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'I think it is no surprise that they are basically the same. Formality analysis
    is next. This takes a couple of minutes to run in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This looks to be very similar. We can examine the proportion of the parts of
    the speech. A plot is available, but adds nothing to the analysis, in this instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the diversity measures are produced. Again, they are nearly identical.
    A plot is also available, (`plot(div)`), but being so similar, it once again adds
    no value. It is important to note that Obama''s speech writer for 2010 was Jon
    Favreau, and in 2016, it was Cody Keenan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'One of my favorite plots is the dispersion plot. This shows the dispersion
    of a word throughout the text. Let''s examine the dispersion of `"jobs"`, `"families",`
    and `"economy"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_09.png)'
  prefs: []
  type: TYPE_IMG
- en: This is quite interesting as you can visualize how much longer the 2010 speech
    is. In 2010, the first half of his speech was focused heavily on jobs while in
    2016 it appears it was more about the state of the overall economy; no doubt how
    much of a hand he played in saving it from the brink of disaster. In 2010, security
    was not brought in until later in the speech versus placed throughout the final
    address. You can see and understand how text analysis can provide insight into
    what someone is thinking, what their priorities are, and how they go about communicating them.
  prefs: []
  type: TYPE_NORMAL
- en: This completes our analysis of the two speeches. I must confess that I did not
    listen to any of these speeches. In fact, I haven't watched a State of the Union
    address since Reagan was president, probably with the exception of the 2002 address.
    This provided some insight for me on how the topics and speech formats have changed
    over time to accommodate political necessity, while the overall style of formality
    and sentence structure has remained consistent. Keep in mind that this code can
    be adapted to text for dozens, if not hundreds, of documents and with multiple
    speakers, for example screenplays, legal proceedings, interviews, social media,
    and on and on. Indeed, text mining can bring quantitative order to what has been
    qualitative chaos.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to address the massive volume of textual data
    that exists through text mining methods. We looked at a useful framework for text
    mining, including preparation, word frequency counts and visualization, and topic
    models using LDA with the `tm` package. Included in this framework were other
    quantitative techniques, such as polarity and formality, in order to provide a
    deeper lexical understanding, or what one could call style, with the `qdap` package.
    The framework was then applied to president Obama's seven State of the Union addresses,
    which showed that, although the speeches had a similar style, the core messages
    changed over time as the political landscape changed. Despite it not being practical
    to cover every possible text mining technique, those discussed in this chapter
    should be adequate for most problems that one might face. In the next chapter,
    we are going to shift gears away from building models and focus on a technique
    to get R on the cloud, allowing you to scale your machine learning to whatever
    problem you may be trying to solve.
  prefs: []
  type: TYPE_NORMAL
