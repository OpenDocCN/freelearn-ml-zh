["```py\nC:\\> pip3 install --upgrade tensorflow\n```", "```py\nSuccessfully installed absl-py-0.1.10 markdown-2.6.11 numpy-1.14.0 protobuf-3.5.1 setuptools-38.5.1 tensorflow-1.5.0 tensorflow-tensorboard-1.5.1 werkzeug-0.14.1\n```", "```py\npython\n```", "```py\n>>> import tensorflow as tf\n>>> hello = tf.constant('Hello, TensorFlow!')\n>>> sess = tf.Session()\n>>> print(sess.run(hello))\n```", "```py\nHello, TensorFlow!\n```", "```py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n```", "```py\nimport numpy as np\nimport tensorflow as tf\n```", "```py\ntf.logging.set_verbosity(tf.logging.INFO)\n```", "```py\ndef cnn_model_fn(features, labels, mode):\n```", "```py\ninput_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n```", "```py\nconv1 = tf.layers.conv2d(\n      inputs=input_layer,\n      filters=32,\n      kernel_size=[5, 5],\n      padding=\"same\",\n      activation=tf.nn.relu)\n```", "```py\npool1 = tf.layers.max_pooling2d(inputs=conv1,\n                       pool_size=[2, 2], strides=2)\n```", "```py\nconv2 = tf.layers.conv2d(\n    inputs=pool1,\n    filters=64,\n    kernel_size=[5, 5],\n    padding=\"same\",\n    activation=tf.nn.relu)\n```", "```py\npool2 = tf.layers.max_pooling2d(inputs=conv2,\n                   pool_size=[2, 2], strides=2)\npool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n```", "```py\ndense = tf.layers.dense(inputs=pool2_flat,\n                units=1024, activation=tf.nn.relu)\n```", "```py\ndropout = tf.layers.dropout(inputs=dense,\n            rate=0.4, training=mode ==\n                      tf.estimator.ModeKeys.TRAIN)\n```", "```py\nlogits = tf.layers.dense(inputs=dropout, units=10)\n```", "```py\npredictions = {\n      \"classes\": tf.argmax(input=logits, axis=1),\n       \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n  }\n  if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode=mode,\n                           predictions=predictions)\n```", "```py\nloss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n```", "```py\nif mode == tf.estimator.ModeKeys.TRAIN:\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize(\n        loss=loss,\n        global_step=tf.train.get_global_step())\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n```", "```py\n  eval_metric_ops = {\n      \"accuracy\": tf.metrics.accuracy(\n          labels=labels, predictions=predictions[\"classes\"])}\n  return tf.estimator.EstimatorSpec(\n      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n```", "```py\ndef main(unused_argv):\n```", "```py\n  mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n  train_data = mnist.train.images \n  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n  eval_data = mnist.test.images \n  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n```", "```py\n  mnist_classifier = tf.estimator.Estimator(\n      model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\n```", "```py\n  tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n  logging_hook = tf.train.LoggingTensorHook(\n      tensors=tensors_to_log, every_n_iter=50)\n```", "```py\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n      x={\"x\": train_data},\n      y=train_labels,\n      batch_size=100,\n      num_epochs=None,\n      shuffle=True)\n  mnist_classifier.train(\n      input_fn=train_input_fn,\n      steps=15000,\n      hooks=[logging_hook])\n```", "```py\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n      x={\"x\": eval_data},\n      y=eval_labels,\n      num_epochs=1,\n      shuffle=False)\n  eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n  print(eval_results)\n```", "```py\nif __name__ == \"__main__\":\n  tf.app.run()\n```", "```py\nmkdir CNN-HWR\n```", "```py\ngsutil cp gs://cnn-hwr-mlengine/cnn_hwr.py CNN-HWR\n```", "```py\ngiuseppe_ciaburro@progetto-1-191608:~$ gsutil cp gs://cnn-hwr/cnn_hwr.py CNN-HWR\nCopying gs://cnn-hwr/cnn_hwr.py...\n/ [1 files][ 5.7 KiB/ 5.7 KiB]\nOperation completed over 1 objects/5.7 KiB.\n```", "```py\n$cd CNN-HWR\n$ls\ncnn_hwr.py\n```", "```py\n$ python cnn_hwr.py\n```", "```py\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting MNIST-data/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting MNIST-data/train-labels-idx1-ubyte.gz\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting MNIST-data/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting MNIST-data/t10k-labels-idx1-ubyte.gz\nINFO:tensorflow:Using default config.\n```", "```py\nINFO:tensorflow:Saving checkpoints for 15000 into /tmp/mnist_convnet_model/model.ckpt.\nINFO:tensorflow:Loss for final step: 2.2751274.INFO:tensorflow:Starting evaluation at 2018-02-19-08:47:04\nINFO:tensorflow:Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-15000\nINFO:tensorflow:Finished evaluation at 2018-02-19-08:47:56\nINFO:tensorflow:Saving dict for global step 15000: accuracy = 0.9723, global_step = 15000, loss = 0.098432\n{'loss': 0.098432, 'global_step': 15000, 'accuracy': 0.9723}\n```", "```py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\n```", "```py\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n```", "```py\nlearning_rate = 0.001\ntraining_steps = 20000\nbatch_size = 128\ndisplay_step = 1000\n```", "```py\nnum_input = 28\ntimesteps = 28\nnum_hidden = 128\nnum_classes = 10\n```", "```py\nX = tf.placeholder(\"float\", [None, timesteps, num_input])\nY = tf.placeholder(\"float\", [None, num_classes])\n```", "```py\nweights = {\n    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n}\nbiases = {\n    'out': tf.Variable(tf.random_normal([num_classes]))\n}\n```", "```py\ndef RNN(x, weights, biases):\n    x = tf.unstack(x, timesteps, 1)\n    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n```", "```py\nlogits = RNN(X, weights, biases)\nprediction = tf.nn.softmax(logits)\n```", "```py\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n    logits=logits, labels=Y))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss_op)\n```", "```py\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n```", "```py\ninit = tf.global_variables_initializer()\n```", "```py\nwith tf.Session() as sess:\n    sess.run(init)\n    for step in range(1, training_steps+1):\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n        if step % display_step == 0 or step == 1:\n            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n                                                                 Y: batch_y})\n            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n                  \"{:.3f}\".format(acc))\n    print(\"End of the optimization process \")\n```", "```py\n    test_len = 128\n    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n    test_label = mnist.test.labels[:test_len]\n    print(\"Testing Accuracy:\", \\\n        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n```", "```py\nmkdir RNN-HWR\n```", "```py\ngsutil cp gs://rnn-hwr-mlengine/rnn_hwr.py RNN-HWR\n```", "```py\ngiuseppe_ciaburro@progetto-1-191608:~$ gsutil cp gs://rnn-hwr/rnn_hwr.py RNN-HWR\nCopying gs://rnn-hwr/rnn_hwr.py...\n/ [1 files][ 4.0 KiB/ 4.0 KiB]\nOperation completed over 1 objects/4.0 KiB.\n```", "```py\n$cd RNN-HWR\n$ls\nrnn_hwr.py\n```", "```py\n$ python rnn_hwr.py\n```", "```py\nExtracting /tmp/data/train-images-idx3-ubyte.gz\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\n```", "```py\nStep 1, Minibatch Loss= 2.9727, Training Accuracy= 0.117\nStep 1000, Minibatch Loss= 1.8381, Training Accuracy= 0.430\nStep 2000, Minibatch Loss= 1.4021, Training Accuracy= 0.602\nStep 3000, Minibatch Loss= 1.1560, Training Accuracy= 0.672\nStep 4000, Minibatch Loss= 0.9748, Training Accuracy= 0.727\nStep 5000, Minibatch Loss= 0.8156, Training Accuracy= 0.750\nStep 6000, Minibatch Loss= 0.7572, Training Accuracy= 0.758\nStep 7000, Minibatch Loss= 0.5930, Training Accuracy= 0.812\nStep 8000, Minibatch Loss= 0.5583, Training Accuracy= 0.805\nStep 9000, Minibatch Loss= 0.4324, Training Accuracy= 0.914\nStep 10000, Minibatch Loss= 0.4227, Training Accuracy= 0.844\nStep 11000, Minibatch Loss= 0.2818, Training Accuracy= 0.906\nStep 12000, Minibatch Loss= 0.3205, Training Accuracy= 0.922\nStep 13000, Minibatch Loss= 0.4042, Training Accuracy= 0.891\nStep 14000, Minibatch Loss= 0.2918, Training Accuracy= 0.914\nStep 15000, Minibatch Loss= 0.1991, Training Accuracy= 0.938\nStep 16000, Minibatch Loss= 0.2815, Training Accuracy= 0.930\nStep 17000, Minibatch Loss= 0.1790, Training Accuracy= 0.953\nStep 18000, Minibatch Loss= 0.2627, Training Accuracy= 0.906\nStep 19000, Minibatch Loss= 0.1616, Training Accuracy= 0.945\nStep 20000, Minibatch Loss= 0.1017, Training Accuracy= 0.992\nOptimization Finished!\nTesting Accuracy: 0.9765625\n```"]