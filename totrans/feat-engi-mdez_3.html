<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Feature Improvement - Cleaning Datasets</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the last two chapters, we have gone from talking about a basic understanding of feature engineering and how it can be used to enhance our machine learning pipelines to getting our hands dirty with datasets and evaluating and understanding the different types of data that we can encounter in the wild.</p>
<p>In this chapter, we will be using what we learned and taking things a step further and begin to change the datasets that we work with. Specifically, we will be starting to <em>clean</em> and <em>augment</em> our datasets. By cleaning, we will generally be referring to the process of altering columns and rows already given to us. By augmenting, we will generally refer to the processes of removing columns and adding columns to datasets. As always, our goal in all of these processes is to enhance our machine learning pipelines.</p>
<p>In the following chapters, we will be:</p>
<ul>
<li>Identifying missing values in data</li>
<li>Removing harmful data</li>
<li>Imputing (filling in) these missing values</li>
<li>Normalizing/standardizing data</li>
<li>Constructing brand new features</li>
<li>Selecting (removing) features manually and automatically</li>
<li>Using mathematical matrix computations to transform datasets to different dimensions</li>
</ul>
<p><span>These methods will help us develop a better sense of which features are important within our data. </span>In this chapter, we will be diving deeper into the first four methods, and leave the other three for future chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying missing values in data</h1>
                </header>
            
            <article>
                
<p><span>Our first method of identifying missing values is to give us a better understanding of how to work with real-world data. Often, data can have missing values due to a variety of reasons, for example with survey data, some observations may not have been recorded. It is important for us to analyze our data, and get a sense of what the missing values are so we can decide how we want to handle missing values for our machine learning. To start, let's dive into a dataset that we will be interested in for the duration of this chapter, the <kbd>Pima Indian Diabetes Prediction</kbd> dataset.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Pima Indian Diabetes Prediction dataset</h1>
                </header>
            
            <article>
                
<p>This dataset is available on the UCI Machine Learning Repository at:</p>
<p> <a href="https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes" target="_blank">https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes</a>.</p>
<p>From the main website, we can learn a few things about this publicly available dataset. We have nine columns and <span>768 instances (rows). The dataset is primarily used for predicting the onset of diabetes within five years in females of Pima Indian heritage over the age of 21 given medical details about their bodies.</span></p>
<p>The dataset is meant to correspond with a binary (2-class) classification machine learning problem. Namely, the answer to the question, <em>will this person develop diabetes within five years?</em> The column names are provided as follows (in order):</p>
<ol>
<li>Number of times pregnant</li>
<li>Plasma glucose concentration a 2 hours in an oral glucose tolerance test</li>
<li>Diastolic blood pressure (mm Hg)</li>
<li>Triceps skinfold thickness (mm)</li>
<li>2-Hour serum insulin measurement (mu U/ml)</li>
<li>Body mass index (weight in kg/(height in m)<sup>2</sup>)</li>
</ol>
<p> </p>
<ol start="7">
<li>Diabetes pedigree function</li>
<li>Age (years)</li>
<li>Class variable (zero or one)</li>
</ol>
<p>The goal of the dataset is to be able to predict the final column of <kbd>class</kbd> variable, which predicts if the patient has developed diabetes, using the other eight features as inputs to a machine learning function.</p>
<p>There are two very important reasons we will be working with this dataset:</p>
<ul>
<li>We will have to work with missing values</li>
<li>All of the features we will be working with will be quantitative</li>
</ul>
<p>The first point makes more sense for now as a reason, because the point of this chapter is to deal with missing values. As far as only choosing to work with quantitative data, this will only be the case for this chapter. We do not have enough tools to deal with missing values in categorical columns. In the next chapter, when we talk about feature construction, we will deal with this procedure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The exploratory data analysis (EDA)</h1>
                </header>
            
            <article>
                
<p>To identify our missing values we will begin with an EDA of our dataset. We will be using some useful python packages, pandas and numpy, to store our data and make some simple calculations as well as some popular visualization tools to see what the distribution of our data looks like. Let's begin and dive into some code. First, we will do some imports:</p>
<pre># import packages we need for exploratory data analysis (EDA)<br/>import pandas as pd # to store tabular data<br/>import numpy as np # to do some math<br/>import matplotlib.pyplot as plt # a popular data visualization tool<br/>import seaborn as sns  # another popular data visualization tool<br/>%matplotlib inline <br/>plt.style.use('fivethirtyeight') # a popular data visualization theme</pre>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<p><span class="n">We will import our tabular data through a CSV, as follows:<br/></span></p>
<pre><span class="n"># load in our dataset using pandas<br/>pima</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'../data/pima.data'</span><span class="p">)<br/><br/>pima.head()<br/></span></pre>
<p>The<span> </span><kbd>head</kbd><span> method allows us to see the first few rows in our dataset. </span>The output is as follows:</p>
<table>
<tbody>
<tr>
<td/>
<td><strong>6</strong></td>
<td><strong>148</strong></td>
<td><strong>72</strong></td>
<td><strong>35</strong></td>
<td><strong>0</strong></td>
<td><strong>33.6</strong></td>
<td><strong>0.627</strong></td>
<td><strong>50</strong></td>
<td><strong>1</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>1</td>
<td>85</td>
<td>66</td>
<td>29</td>
<td>0</td>
<td>26.6</td>
<td>0.351</td>
<td>31</td>
<td>0</td>
</tr>
<tr>
<td><strong>1</strong></td>
<td>8</td>
<td>183</td>
<td>64</td>
<td>0</td>
<td>0</td>
<td>23.3</td>
<td>0.627</td>
<td>32</td>
<td>1</td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>1</td>
<td>89</td>
<td>66</td>
<td>23</td>
<td>94</td>
<td>28.1</td>
<td>0.167</td>
<td>21</td>
<td>0</td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>0</td>
<td>137</td>
<td>40</td>
<td>35</td>
<td>168</td>
<td>43.1</td>
<td>2.288</td>
<td>33</td>
<td>1</td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>5</td>
<td>116</td>
<td>74</td>
<td>0</td>
<td>0</td>
<td>25.6</td>
<td>0.201</td>
<td>30</td>
<td>0</td>
</tr>
</tbody>
</table>
<p> </p>
<p class=" highlight hl-ipython2">Something's not right here, there's no column names. The CSV must not have the names for the columns built into the file. No matter, we can use the data source's website to fill this in, as shown in the following code:</p>
</div>
</div>
<p class="cell border-box-sizing code_cell rendered"/>
<pre><span class="n">pima_column_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'times_pregnant'</span><span class="p">,</span> <span class="s1">'plasma_glucose_concentration'</span><span class="p">,</span> <span class="s1">'diastolic_blood_pressure'</span><span class="p">,</span> <span class="s1">'triceps_thickness'</span><span class="p">,</span> <span class="s1">'serum_insulin'</span><span class="p">,</span> <span class="s1">'bmi'</span><span class="p">,</span> <span class="s1">'pedigree_function'</span><span class="p">,</span> <span class="s1">'age'</span><span class="p">,</span> <span class="s1">'onset_diabetes'</span><span class="p">]<br/><br/>pima = pd.read_csv('../data/pima.data', names=pima_column_names)<br/><br/>pima.head()<br/></span></pre>
<p>Now, using the<span> </span><kbd>head</kbd><span> method again, we can see our columns with the appropriate headers. </span>The output of the preceding code is as follows:</p>
<table>
<tbody>
<tr>
<td/>
<td><strong>times_pregnant</strong></td>
<td><strong>plasma_glucose_concentration</strong></td>
<td><strong>diastolic_blood_pressure</strong></td>
<td><strong>triceps_thickness</strong></td>
<td><strong>serum_insulin</strong></td>
<td><strong>bmi</strong></td>
<td><strong>pedigree_function</strong></td>
<td><strong>age</strong></td>
<td><strong>onset_diabetes</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>6</td>
<td>148</td>
<td>72</td>
<td>35</td>
<td>0</td>
<td>33.6</td>
<td>0.627</td>
<td>50</td>
<td>1</td>
</tr>
<tr>
<td><strong>1</strong></td>
<td>1</td>
<td>85</td>
<td>66</td>
<td>29</td>
<td>0</td>
<td>26.6</td>
<td>0.351</td>
<td>31</td>
<td>0</td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>8</td>
<td>183</td>
<td>64</td>
<td>0</td>
<td>0</td>
<td>23.3</td>
<td>0.672</td>
<td>32</td>
<td>1</td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>1</td>
<td>89</td>
<td>66</td>
<td>23</td>
<td>94</td>
<td>28.1</td>
<td>0.167</td>
<td>21</td>
<td>0</td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>0</td>
<td>137</td>
<td>40</td>
<td>35</td>
<td>168</td>
<td>43.1</td>
<td>2.288</td>
<td>33</td>
<td>1</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Much better, now we can use the column names to do some basic stats, selecting, and visualizations. Let's first get our null accuracy as follows:</p>
<div class="cell border-box-sizing code_cell rendered">
<pre>pima['onset_diabetes'].value_counts(normalize=True)<span> </span><br/># get null accuracy, 65% did not develop diabetes<br/><br/>0    0.651042
1    0.348958
Name: onset_diabetes, dtype: float64</pre></div>
<p>If our eventual goal is to exploit patterns in our data in order to predict the onset of diabetes, let us try to visualize some of the differences between those that developed diabetes and those that did not. Our hope is that the histogram will reveal some sort of pattern, or obvious difference in values between the classes of prediction:</p>
<pre># get a histogram of the plasma_glucose_concentration column for<br/># both classes<br/><br/>col = 'plasma_glucose_concentration'<br/>plt.hist(pima[pima['onset_diabetes']==0][col], 10, alpha=0.5, label='non-diabetes')<br/>plt.hist(pima[pima['onset_diabetes']==1][col], 10, alpha=0.5, label='diabetes')<br/>plt.legend(loc='upper right')<br/>plt.xlabel(col)<br/>plt.ylabel('Frequency')<br/>plt.title('Histogram of {}'.format(col))<br/>plt.show()</pre>
<p>The output of the preceding code is as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="177" src="assets/408febaa-777c-4619-a4ca-76cb171ced80.png" style="color: #333333;font-size: 1em" width="275"/></div>
<p class="mce-root">It seems<span> </span><span>that this histogram is showing us a pretty big difference between <kbd>plasma_glucose_concentration</kbd> between the two prediction classes. </span><span>Let's show the same histogram style for multiple columns as follows:</span></p>
<pre>for col in ['bmi', 'diastolic_blood_pressure', 'plasma_glucose_concentration']:<br/>    plt.hist(pima[pima['onset_diabetes']==0][col], 10, alpha=0.5, label='non-diabetes')<br/>    plt.hist(pima[pima['onset_diabetes']==1][col], 10, alpha=0.5, label='diabetes')<br/>    plt.legend(loc='upper right')<br/>    plt.xlabel(col)<br/>    plt.ylabel('Frequency')<br/>    plt.title('Histogram of {}'.format(col))<br/>    plt.show()</pre>
<p>The output of the preceding code will give us the following three histograms. The first one is show us the distributions of <strong>bmi</strong> for the two class variables (non-diabetes and diabetes):</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="176" src="assets/b05cd7b9-0005-471d-84da-5e8d3d8320e3.png" width="256"/></div>
<p class="mce-root">The next histogram to appear will shows us again contrastingly different distributions between a feature across our two class variables. This time we are looking at <strong>diastolic_blood_pressure</strong>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="173" src="assets/1eb28f27-66a3-4265-af6d-13dbbfef4a20.png" width="251"/></div>
<p>The final graph will show <strong>plasma_glucose_concentration</strong> differences between our two class variables:</p>
<p class="mce-root"/>
<div class="mce-root CDPAlignCenter CDPAlign"><br/>
<img height="158" src="assets/a5887f0e-8069-41af-9e70-884e0be4a426.png" style="color: #333333;font-family: Merriweather, serif;font-size: 1em" width="245"/></div>
<p>We can definitely see some major differences simply by looking at just a few histograms. For example, there seems to be a large jump in <kbd>plasma_glucose_concentration</kbd> for those who will eventually develop diabetes. To solidify this, perhaps we can visualize a linear correlation matrix in an attempt to quantify the relationship between these variables. We will use the visualization tool, seaborn, which we imported at the beginning of this chapter for our correlation matrix as follows:</p>
<pre># look at the heatmap of the correlation matrix of our dataset<br/>sns.heatmap(pima.corr())<br/># plasma_glucose_concentration definitely seems to be an interesting feature here</pre>
<p>Following is the correlation matrix of our dataset. This is showing us the correlation amongst the different columns in our <kbd>Pima</kbd> dataset. The output is as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="unconfined" height="238" src="assets/d4f2b0c8-3429-4b62-9ccc-039296923e95.png" width="298"/></div>
<p>This correlation matrix is showing a strong correlation between<span> </span><kbd>plasma_glucose_concentration</kbd><span> and <kbd>onset_diabetes</kbd>. Let's take a further look at the numerical correlations for the <kbd>onset_diabetes</kbd> column, with the following code:</span></p>
<pre>pima.corr()['onset_diabetes'] # numerical correlation matrix<br/># plasma_glucose_concentration definitely seems to be an interesting feature here<br/><br/>times_pregnant                  0.221898
<strong>plasma_glucose_concentration    0.466581
</strong>diastolic_blood_pressure        0.065068
triceps_thickness               0.074752
serum_insulin                   0.130548
bmi                             0.292695
pedigree_function               0.173844
age                             0.238356
onset_diabetes                  1.000000
Name: onset_diabetes, dtype: float64</pre>
<p>We will explore the powers of correlation in a later <a href="430d621e-7ce6-48c0-9990-869e82a0d0c6.xhtml" target="_blank">Chapter 4</a>, <em>Feature Construction</em>, but for now we are using <strong>exploratory data analysis</strong> (<strong>EDA</strong>) to hint at the fact that the <kbd>plasma_glucose_concentration</kbd> column will be an important factor in our prediction of the onset of diabetes.</p>
<p class="cell border-box-sizing code_cell rendered">Moving on to more important matters at hand, let's see if we are missing any values in our dataset by invoking the built-in <kbd>isnull()</kbd> method of the pandas DataFrame:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="output_wrapper">
<div class="output">
<div class="output_text output_subarea output_execute_result">
<pre>pima.isnull().sum()<br/>&gt;&gt;&gt;&gt;<br/>times_pregnant                  0
plasma_glucose_concentration    0
diastolic_blood_pressure        0
triceps_thickness               0
serum_insulin                   0
bmi                             0
pedigree_function               0
age                             0
onset_diabetes                  0
dtype: int64</pre></div>
<p class="output_area">Great! We don't have any missing values. Let's go on to do some more EDA, first using the <kbd>shape</kbd> method to see the number of rows and columns we are working with:</p>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_text output_result">
<pre>pima.shape . # (# rows, # cols)<br/>(768, 9)</pre></div>
</div>
</div>
</div>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<p class="CodeMirror cm-s-ipython">Confirming we have <kbd>9</kbd> columns (including our response variable) and <kbd>768</kbd> data observations (rows). Now, let's take a peak at the percentage of patients who developed diabetes, using the following code: </p>
</div>
</div>
<pre>pima['onset_diabetes'].value_counts(normalize=True) <br/># get null accuracy, 65% did not develop diabetes<br/><br/>0    0.651042
1    0.348958
Name: onset_diabetes, dtype: float64</pre></div>
</div>
</div>
</div>
<p class="mce-root">This shows us that <kbd>65%</kbd> of the patients did not develop diabetes, while about 35% did. We can use a nifty built-in method of a pandas DataFrame called <kbd>describe</kbd> to look at some basic descriptive statistics:</p>
<pre><span>pima.describe()  # get some basic descriptive statistics<br/></span></pre>
<p>We get the output as follows:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>times_pregnant</strong></p>
</td>
<td>
<p><strong>plasma_glucose<br/>
_concentration</strong></p>
</td>
<td>
<p><strong>diastolic_<br/>
blood_pressure</strong></p>
</td>
<td>
<p><strong>triceps<br/>
_thickness</strong></p>
</td>
<td>
<p><strong>serum<br/>
_insulin</strong></p>
</td>
<td>
<p><strong>bmi</strong></p>
</td>
<td>
<p><strong>pedigree<br/>
_function</strong></p>
</td>
<td>
<p><strong>age</strong></p>
</td>
<td>
<p><strong>onset<br/>
_diabetes</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>count</span></strong></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>mean</span></strong></p>
</td>
<td>
<p><span>3.845052</span></p>
</td>
<td>
<p><span>120.894531</span></p>
</td>
<td>
<p><span>69.105469</span></p>
</td>
<td>
<p><span>20.536458</span></p>
</td>
<td>
<p><span>79.799479</span></p>
</td>
<td>
<p><span>31.992578</span></p>
</td>
<td>
<p><span>0.471876</span></p>
</td>
<td>
<p><span>33.240885</span></p>
</td>
<td>
<p><span>0.348958</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>std</span></strong></p>
</td>
<td>
<p><span>3.369578</span></p>
</td>
<td>
<p><span>31.972618</span></p>
</td>
<td>
<p><span>19.355807</span></p>
</td>
<td>
<p><span>15.952218</span></p>
</td>
<td>
<p><span>115.244002</span></p>
</td>
<td>
<p><span>7.884160</span></p>
</td>
<td>
<p><span>0.331329</span></p>
</td>
<td>
<p><span>11.760232</span></p>
</td>
<td>
<p><span>0.476951</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>min</span></strong></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.078000</span></p>
</td>
<td>
<p><span>21.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>25%</span></strong></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>99.000000</span></p>
</td>
<td>
<p><span>62.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>27.300000</span></p>
</td>
<td>
<p><span>0.243750</span></p>
</td>
<td>
<p><span>24.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>50%</span></strong></p>
</td>
<td>
<p>3.000000</p>
</td>
<td>
<p><span>117.000000</span></p>
</td>
<td>
<p><span>72.000000</span></p>
</td>
<td>
<p><span>23.000000</span></p>
</td>
<td>
<p><span>30.500000</span></p>
</td>
<td>
<p><span>32.000000</span></p>
</td>
<td>
<p><span>0.372500</span></p>
</td>
<td>
<p><span>29.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>75%</strong></p>
</td>
<td>
<p><span>6.000000</span></p>
</td>
<td>
<p><span>140.250000</span></p>
</td>
<td>
<p><span>80.000000</span></p>
</td>
<td>
<p><span>32.000000</span></p>
</td>
<td>
<p><span>127.250000</span></p>
</td>
<td>
<p><span>36.600000</span></p>
</td>
<td>
<p><span>0.626250</span></p>
</td>
<td>
<p><span>41.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>max</strong></p>
</td>
<td>
<p><span>17.000000</span></p>
</td>
<td>
<p><span>199.000000</span></p>
</td>
<td>
<p><span>122.000000</span></p>
</td>
<td>
<p><span>99.000000</span></p>
</td>
<td>
<p><span>846.000000</span></p>
</td>
<td>
<p><span>67.100000</span></p>
</td>
<td>
<p><span>2.420000</span></p>
</td>
<td>
<p><span>81.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>This shows us quite quickly some basic stats such as mean, standard deviation, and some different percentile measurements of our data. But, notice that the minimum value of the <kbd>BMI</kbd> column is <kbd>0</kbd>. That is medically impossible; there must be a reason for this to happen. Perhaps the number zero has been encoded as a missing value instead of the None value or a missing cell. Upon closer inspection, we see that the value 0 appears as a minimum value for the following columns:</p>
<ul>
<li><kbd>times_pregnant</kbd></li>
<li><kbd>plasma_glucose_concentration</kbd></li>
<li><kbd>diastolic_blood_pressure</kbd></li>
<li><kbd>triceps_thickness</kbd></li>
<li><kbd>serum_insulin</kbd></li>
<li><kbd>bmi</kbd></li>
<li><kbd>onset_diabetes</kbd></li>
</ul>
<p>Because zero is a class for <kbd>onset_diabetes</kbd> and 0 is actually a viable number for <kbd>times_pregnant</kbd>, we may conclude that the number 0 is encoding missing values for:</p>
<ul>
<li class="mce-root"><kbd>plasma_glucose_concentration</kbd></li>
<li class="mce-root"><kbd>diastolic_blood_pressure</kbd></li>
<li class="mce-root"><kbd>triceps_thickness</kbd></li>
<li class="mce-root"><kbd>serum_insulin</kbd></li>
<li class="mce-root"><kbd>bmi</kbd></li>
</ul>
<p>So, we actually do having missing values! It was obviously not luck that we happened upon the zeros as missing values, we knew it beforehand. As a data scientist, you must be ever vigilant and make sure that you know as much about the dataset as possible in order to find missing values encoded as other symbols. Be sure to read any and all documentation that comes with open datasets in case they mention any missing values.</p>
<p>If no documentation is available, some common values used instead of missing values are:</p>
<ul>
<li><strong>0</strong> (for numerical values)</li>
<li><strong>unknown</strong> or <strong>Unknown</strong> (for categorical variables)</li>
<li><strong>?</strong> (for categorical variables)</li>
</ul>
<p>So, we have five columns in which missing values exist, so now we get to talk about how to deal with them, in depth.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dealing with missing values in a dataset</h1>
                </header>
            
            <article>
                
<p>When working with data, one of the most common issues a data scientist will run into is the problem of missing data. Most commonly, this refers to empty cells (row/column intersections) where the data just was not acquired for whatever reason. This can become a problem for many reasons; notably, when applying learning algorithms to data with missing values, most (not all) algorithms are not able to cope with missing values. </p>
<p>For this reason, data scientists and machine learning engineers have many tricks and tips on how to deal with this problem. Although there are many variations of methodologies, the two major ways in which we can deal with missing data are:</p>
<ul>
<li>Remove rows with missing values in them</li>
<li>Impute (fill in) missing values</li>
</ul>
<p>Each method will <strong>clean</strong> our dataset to a point where a learning algorithm can handle it, but each method will have its pros and cons.</p>
<p>First off, before we go too far, let's get rid of the zeros and replace them all with the value <kbd>None</kbd> in Python. This way, our <kbd>fillna</kbd> and <kbd>dropna</kbd> methods will work correctly. We could manually replace all zeros with None, each column at a time, like so:</p>
<pre><span># Our number of missing values is (incorrectly) 0</span><br/><span>pima['serum_insulin'].isnull().sum()</span><br/><br/>0<br/><br/>pima['serum_insulin'] = pima['serum_insulin'].map(lambda x:x if x != 0 else None)<br/># manually replace all 0's with a None value<br/><br/>pima['serum_insulin'].isnull().sum()<br/># check the number of missing values again<br/><br/>374</pre>
<p>We could repeat this procedure for every column with incorrectly labeled missing values, or we could use a <kbd>for</kbd> loop and a built-in <kbd>replace</kbd> method to speed things up, as shown in the following code:</p>
<pre># A little faster now for all columns<br/><br/>columns = ['serum_insulin', 'bmi', 'plasma_glucose_concentration', 'diastolic_blood_pressure', 'triceps_thickness']<br/><br/>for col in columns:<br/>    pima[col].replace([0], [None], inplace=True)</pre>
<p>So, now if we try to count the number of missing values using the <kbd>isnull</kbd> method, we should start to see missing values being counted as follows:</p>
<pre><span>pima.isnull().sum()  # this makes more sense now!</span><br/><br/>times_pregnant                    0
plasma_glucose_concentration      5
diastolic_blood_pressure         35
triceps_thickness               227
serum_insulin                   374
bmi                              11
pedigree_function                 0
age                               0
onset_diabetes                    0
dtype: int64<br/><br/><br/>pima.head()</pre>
<p>Now, looking at the first few rows of our dataset, we get the output as follows:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>times_pregnant</strong></p>
</td>
<td>
<p><strong>plasma_glucose_concentration</strong></p>
</td>
<td>
<p><strong>diastolic_blood_pressure</strong></p>
</td>
<td>
<p><strong>triceps_thickness</strong></p>
</td>
<td>
<p><strong>serum_insulin</strong></p>
</td>
<td>
<p><strong>bmi</strong></p>
</td>
<td>
<p><strong>pedigree_function</strong></p>
</td>
<td>
<p><strong>age</strong></p>
</td>
<td>
<p><strong>onset_diabetes</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>148</p>
</td>
<td>
<p>72</p>
</td>
<td>
<p>35</p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p>33.6</p>
</td>
<td>
<p><span>0.627</span></p>
</td>
<td>
<p>50</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>85</p>
</td>
<td>
<p>66</p>
</td>
<td>
<p>29</p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p>26.6</p>
</td>
<td>
<p><span>0.351</span></p>
</td>
<td>
<p>31</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>183</p>
</td>
<td>
<p>64</p>
</td>
<td>
<p>None</p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p>23.3</p>
</td>
<td>
<p><span>0.672</span></p>
</td>
<td>
<p>32</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>89</p>
</td>
<td>
<p>66</p>
</td>
<td>
<p>23</p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p>28.1</p>
</td>
<td>
<p><span>0.167</span></p>
</td>
<td>
<p>21</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>137</p>
</td>
<td>
<p>40</p>
</td>
<td>
<p>35</p>
</td>
<td>
<p><span>NaN</span></p>
</td>
<td>
<p>43.1</p>
</td>
<td>
<p><span>2.288</span></p>
</td>
<td>
<p>33</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>OK, this is starting to make much more sense. We can now see that five columns have missing values, and the degree to which data is missing is staggering. Some columns, such as <kbd>plasma_glucose_concentration</kbd>, are only missing five values, but look at <kbd>serum_insulin</kbd>; that column is missing almost half of its values.</p>
<p>Now that we have missing values properly injected into our dataset instead of the <kbd>0</kbd> placeholders that the dataset originally came with, our exploratory data analysis will be more accurate:</p>
<pre>pima.describe()  # grab some descriptive statistics</pre>
<p>The preceding code produces the following output:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>times_pregnant</strong></p>
</td>
<td>
<p><strong>serum_insulin</strong></p>
</td>
<td>
<p><strong>pedigree_function</strong></p>
</td>
<td>
<p><strong>age</strong></p>
</td>
<td>
<p><strong>onset_diabetes</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>count</strong></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>394.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>mean</strong></p>
</td>
<td>
<p><span>3.845052</span></p>
</td>
<td>
<p><span>155.548223</span></p>
</td>
<td>
<p><span>0.471876</span></p>
</td>
<td>
<p><span>33.240885</span></p>
</td>
<td>
<p><span>0.348958</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>std</strong></p>
</td>
<td>
<p><span>3.369578</span></p>
</td>
<td>
<p><span>118.775855</span></p>
</td>
<td>
<p><span>0.331329</span></p>
</td>
<td>
<p><span>11.760232</span></p>
</td>
<td>
<p><span>0.476951</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>min</strong></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>14.000000</span></p>
</td>
<td>
<p><span>0.078000</span></p>
</td>
<td>
<p><span>21.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>25%</strong></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>76.250000</span></p>
</td>
<td>
<p><span>0.243750</span></p>
</td>
<td>
<p><span>24.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>50%</strong></p>
</td>
<td>
<p><span>3.000000</span></p>
</td>
<td>
<p><span>125.000000</span></p>
</td>
<td>
<p><span>0.372500</span></p>
</td>
<td>
<p><span>29.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>75%</strong></p>
</td>
<td>
<p><span>6.000000</span></p>
</td>
<td>
<p><span>190.000000</span></p>
</td>
<td>
<p><span>0.626250</span></p>
</td>
<td>
<p><span>41.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>max</strong></p>
</td>
<td>
<p><span>17.000000</span></p>
</td>
<td>
<p><span>846.000000</span></p>
</td>
<td>
<p><span>2.420000</span></p>
</td>
<td>
<p><span>81.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Notice that the <kbd>describe</kbd> method doesn't include columns with missing values, which while not ideal, doesn't mean that we cannot obtain them by computing the mean and standard deviation of the specific columns, like so:</p>
<pre>pima['plasma_glucose_concentration'].mean(), pima['plasma_glucose_concentration'].std()<br/><br/>(121.68676277850589, 30.53564107280403)</pre>
<p>Let us move on to our two ways of dealing with missing data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing harmful rows of data</h1>
                </header>
            
            <article>
                
<p>Probably the most common and easiest of our two options for dealing with missing data is to simply remove the observations that have any missing values. By doing so, we will be left with only the <strong>complete</strong> data points with all data filled in. We can obtain a new DataFrame by invoking the <kbd>dropna</kbd> method in pandas, as shown in the following code:</p>
<pre># drop the rows with missing values<br/>pima_dropped = pima.dropna()</pre>
<p>Now, of course, the obvious problem here is that we lost a few rows. To check how many exactly, use the following code:</p>
<pre>num_rows_lost = round(100*(pima.shape[0] - pima_dropped.shape[0])/float(pima.shape[0]))<br/><br/>print "retained {}% of rows".format(num_rows_lost)<br/># lost over half of the rows!<br/><br/>retained 49.0% of rows</pre>
<div class="cell code_cell rendered unselected">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<p class="CodeMirror cm-s-ipython">Wow! We lost about 51% of the rows from the original dataset, and if we think about this from a machine learning perspective, even though now we have clean data with everything filled in, we aren't really learning as much as we possibly could be by ignoring over half of the data's observations. That's like a doctor trying to understand how heart attacks happen, ignoring over half of their patients coming in for check-ups.</p>
</div>
<p>Let's perform some more EDA on the dataset and compare the statistics about the data from before and after dropping the missing-values rows:</p>
<pre># some EDA of the dataset before it was dropped and after<br/><br/># split of trues and falses before rows dropped<br/>pima['onset_diabetes'].value_counts(normalize=True)<br/><br/>0    0.651042<br/>1    0.348958<br/>Name: onset_diabetes, dtype: float64</pre></div>
</div>
</div>
<p>Now, let's look at the same split after we dropped the rows, using the following code:</p>
<pre>pima_dropped['onset_diabetes'].value_counts(normalize=True)  <br/><br/>0    0.668367
1    0.331633
Name: onset_diabetes, dtype: float64<br/><br/># the split of trues and falses stay relatively the same</pre>
<p>It seems that the binary response stayed relatively the same during the drastic transformation of our dataset. Let's take a look at the <em>shape</em> of our data by comparing the average values of columns before and after the transformation, using the <kbd>pima.mean</kbd> function, as follows:</p>
<pre><span># the mean values of each column (excluding missing values)</span><br/><span>pima.mean()</span><br/><br/>times_pregnant                    3.845052
plasma_glucose_concentration    121.686763
diastolic_blood_pressure         72.405184
triceps_thickness                29.153420
serum_insulin                   155.548223
bmi                              32.457464
pedigree_function                 0.471876
age                              33.240885
onset_diabetes                    0.348958
dtype: float64</pre>
<p>And now for the same averages after dropping the rows, using the <kbd>pima_dropped.mean()</kbd> function, as follows:</p>
<pre><span># the mean values of each column (with missing values rows dropped)</span><br/><span>pima_dropped.mean()</span><br/><br/>times_pregnant                    3.301020
plasma_glucose_concentration    122.627551
diastolic_blood_pressure         70.663265
triceps_thickness                29.145408
serum_insulin                   156.056122
bmi                              33.086224
pedigree_function                 0.523046
age                              30.864796
onset_diabetes                    0.331633
dtype: float64</pre>
<p>To get a better look at how these numbers changed, let's create a new chart that visualizes the percentages changed on average for each column. First, let's create a table of the percent changes in the average values of each column, as shown in the following code:</p>
<pre># % change in means<br/>(pima_dropped.mean() - pima.mean()) / pima.mean()<br/><br/>times_pregnant                 -0.141489
plasma_glucose_concentration    0.007731
diastolic_blood_pressure       -0.024058
triceps_thickness              -0.000275
serum_insulin                   0.003265
bmi                             0.019372
pedigree_function               0.108439
age                            -0.071481
onset_diabetes                 -0.049650
dtype: float64</pre>
<p>And now let's visualize these changes as a bar chart, using the following code:</p>
<pre># % change in means as a bar chart<br/>ax = ((pima_dropped.mean() - pima.mean()) / pima.mean()).plot(kind='bar', title='% change in average column values')<br/>ax.set_ylabel('% change')</pre>
<p>The preceding code produces the following output:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="360" src="assets/512ce682-ec85-47c9-af15-0620dc6cec7d.png" width="338"/></div>
<p>We can see that the number of <kbd>times_pregnant</kbd> variable average fell 14% after dropping missing values, which is a big change! The <kbd>pedigree_function</kbd> also rose 11%, another big leap. We can see how dropping rows (observations) severely affects the shape of the data and we should try to retain as much data as possible. Before moving on to the next method of dealing with missing values, let's introduce some actual machine learning into the mix.</p>
<p>The following code block (which we will go over line by line in a moment) will become a very familiar code block in this book. It describes and achieves a single fitting of a machine learning model over a variety of parameters in the hope of obtaining the best possible model, given the features at hand:</p>
<pre># now lets do some machine learning<br/><br/># note we are using the dataset with the dropped rows<br/><br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import GridSearchCV<br/><br/>X_dropped = pima_dropped.drop('onset_diabetes', axis=1)<br/># create our feature matrix by removing the response variable<br/>print "learning from {} rows".format(X_dropped.shape[0])<br/>y_dropped = pima_dropped['onset_diabetes']<br/><br/><br/># our grid search variables and instances<br/><br/># KNN parameters to try<br/>knn_params = {'n_neighbors':[1, 2, 3, 4, 5, 6, 7]}<br/><br/>knn = KNeighborsClassifier() . # instantiate a KNN model<br/><br/>grid = GridSearchCV(knn, knn_params)<br/>grid.fit(X_dropped, y_dropped)<br/><br/>print grid.best_score_, grid.best_params_<br/># but we are learning from way fewer rows..</pre>
<p>OK, let's go through this line by line. First, we have two new import statements:</p>
<pre>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import GridSearchCV</pre>
<p>We will be utilizing scikit-learn's <strong>K-Nearest Neighbors</strong> (<strong>KNN</strong>) classification model, as well as a grid search module that will automatically find the best combo of parameters (using brute force) for the KNN model that best fits our data with respect to cross-validated accuracy. Next, let's take our dropped dataset (with the missing-valued rows removed) and create an <kbd>X</kbd> and a <kbd>y</kbd> variable for our predictive model. Let's start with our <kbd>X</kbd> (our feature matrix):</p>
<pre>X_dropped = pima_dropped.drop('onset_diabetes', axis=1) <br/># create our feature matrix by removing the response variable<br/>print "learning from {} rows".format(X_dropped.shape[0])<br/><br/>learning from 392 rows</pre>
<p>Ouch, it's already obvious that there's a major problem with this approach. Our machine learning algorithm is going to be fitting and learning from far fewer data observations than which we started with. Let's now create our <kbd>y</kbd> (response series):</p>
<pre>y_dropped = pima_dropped['onset_diabetes']</pre>
<p>Now that we have our <kbd>X</kbd> and our <kbd>y</kbd> variable, we can introduce the variables and instances we need to successfully run a <strong>grid search</strong>. We will set the number of <kbd>params</kbd> to try at seven to keep things simple in this chapter. For every data cleaning and feature engineering method we try (dropping rows, filling in data), we will try to fit the best KNN as having somewhere between one and seven neighbors complexity. We can set this model up as follows:</p>
<pre># our grid search variables and instances<br/><br/># KNN parameters to try<br/><br/>knn_params = {'n_neighbors':[1, 2, 3, 4, 5, 6, 7]}</pre>
<p>Next, we will instantiate a grid search module, as shown in the following code, and fit it to our feature matrix and response variable. Once we do so, we will print out the best accuracy as well as the best parameter used to learn:</p>
<pre>grid = GridSearchCV(knn, knn_params)<br/>grid.fit(X_dropped, y_dropped)<br/><br/>print grid.best_score_, grid.best_params_<br/># but we are learning from way fewer rows..<br/><br/>0.744897959184 {'n_neighbors': 7}</pre>
<div class="cell code_cell rendered unselected">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<p class="CodeMirror cm-s-ipython">So, it seems that using seven neighbors as its parameter, our KNN model was able to achieve a <kbd>74.4%</kbd> accuracy (better than our null accuracy of around 65%), but keep in mind that it is only learning from 49% of the original data, so who knows how it could have done on the rest of the data.</p>
</div>
</div>
</div>
</div>
<div class="packt_infobox">This is our first real look into using machine learning in this book. We will be assuming that the reader does have basic familiarity with machine learning as well as statistical procedures such as cross-validation.</div>
<p>It's probably pretty clear that while dropping the <em>dirty</em> rows may not exactly be feature engineering, it is still a data cleaning technique we can utilize to help sanitize our machine learning pipeline inputs. Let's try for a slightly harder method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Imputing the missing values in data</h1>
                </header>
            
            <article>
                
<p>Imputing is the more involved method of dealing with missing values. By <em>imputing, </em>we refer to the act of filling in missing data values with numerical quantities that are somehow ascertained from existing knowledge/data. We have a few options on how we can fill in these missing values, the most common of them being filling in missing values with the average value for the rest of the column, as shown in the following code:</p>
<pre><span>pima.isnull().sum()  # let's fill in the plasma column</span><br/><br/>times_pregnant                    0
<strong>plasma_glucose_concentration      5
</strong>diastolic_blood_pressure         35
triceps_thickness               227
serum_insulin                   374
bmi                              11
pedigree_function                 0
age                               0
onset_diabetes                    0
dtype: int64</pre>
<p>Let's look at the five rows where <kbd>plasma_glucose_concentration</kbd> is missing:</p>
<pre>empty_plasma_index = pima[pima['plasma_glucose_concentration'].isnull()].index<br/>pima.loc[empty_plasma_index]['plasma_glucose_concentration']<br/><br/><br/>75     None
182    None
342    None
349    None
502    None
Name: plasma_glucose_concentration, dtype: object</pre>
<p>Now, let's use the built-in <kbd>fillna</kbd> method to replace all of the <kbd>None</kbd> values with the mean value of the rest of the <kbd>plasma_glucose_concentration</kbd> column:</p>
<pre>pima['plasma_glucose_concentration'].fillna(pima['plasma_glucose_concentration'].mean(), inplace=True)<br/># fill the column's missing values with the mean of the rest of the column<br/><br/>pima.isnull().sum()  # the column should now have 0 missing values<br/><br/><br/>times_pregnant                    0
<strong>plasma_glucose_concentration      0
</strong>diastolic_blood_pressure         35
triceps_thickness               227
serum_insulin                   374
bmi                              11
pedigree_function                 0
age                               0
onset_diabetes                    0
dtype: int64</pre>
<p>And if we check out the column, we should see that the <kbd>None</kbd> values have been replaced by <kbd>121.68</kbd>, the mean value we obtained earlier for this column:</p>
<pre>pima.loc[empty_plasma_index]['plasma_glucose_concentration']<br/><br/>75     121.686763
182    121.686763
342    121.686763
349    121.686763
502    121.686763
Name: plasma_glucose_concentration, dtype: float64</pre>
<p>Great! But this can be cumbersome. Let's use a module in the scikit-learn preprocessing class (the documentation can be found at <a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing">http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing</a>) called the <kbd>Imputer</kbd> (aptly named). We can import it as follows:</p>
<pre>from sklearn.preprocessing import Imputer</pre>
<p>As with most scikit-learn modules, we have a few new parameters to play with, but I will focus on the major one, called the <kbd>strategy</kbd>. We can define how we want to impute values into our dataset by setting this parameter. For quantitative values, we can use the built-in mean and median strategies to fill in values with either quantity. To use the <kbd>Imputer</kbd>, we must first instantiate the object, as shown in the following code:</p>
<pre>imputer = Imputer(strategy='mean')</pre>
<p>Then, we can call the <kbd>fit_transform</kbd> method to create a new object, as shown in the following code:</p>
<pre>pima_imputed = imputer.fit_transform(pima)</pre>
<p>We do have a small issue to deal with. The output of the Imputer is not a pandas DataFrame, but rather the output is of type <strong>NumPy</strong> array:</p>
<pre><span>type(pima_imputed)  # comes out as an array</span><br/><br/>numpy.ndarray</pre>
<p>This can be easily dealt with, as we could just cast the array as a DataFrame, as shown in the following code:</p>
<pre>pima_imputed = pd.DataFrame(pima_imputed, columns=pima_column_names)<br/># turn our numpy array back into a pandas DataFrame object</pre>
<p>Let's take a look at our new DataFrame:</p>
<pre>pima_imputed.head()  # notice for example the triceps_thickness missing values were replaced with 29.15342</pre>
<p>The preceding code produces the following output:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>times_pregnant</strong></p>
</td>
<td>
<p><strong>plasma_glucose_concentration</strong></p>
</td>
<td>
<p><strong>diastolic_blood_pressure</strong></p>
</td>
<td>
<p><strong>triceps_thickness</strong></p>
</td>
<td>
<p><strong>serum_insulin</strong></p>
</td>
<td>
<p><strong>bmi</strong></p>
</td>
<td>
<p><strong>pedigree_function</strong></p>
</td>
<td>
<p><strong>age</strong></p>
</td>
<td>
<p><strong>onset_diabetes</strong></p>
</td>
</tr>
<tr>
<td>
<p>0</p>
</td>
<td>
<p><span>6.0</span></p>
</td>
<td>
<p><span>148.0</span></p>
</td>
<td>
<p><span>72.0</span></p>
</td>
<td>
<p><span>35.00000</span></p>
</td>
<td>
<p><span>155.548223</span></p>
</td>
<td>
<p><span>33.6</span></p>
</td>
<td>
<p><span>0.627</span></p>
</td>
<td>
<p><span>50.0</span></p>
</td>
<td>
<p><span>1.0</span></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>1.0</p>
</td>
<td>
<p><span>85.0</span></p>
</td>
<td>
<p><span>66.0</span></p>
</td>
<td>
<p><span>29.00000</span></p>
</td>
<td>
<p><span>155.548223</span></p>
</td>
<td>
<p><span>26.6</span></p>
</td>
<td>
<p><span>0.351</span></p>
</td>
<td>
<p><span>31.0</span></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>8.0</p>
</td>
<td>
<p><span>183.0</span></p>
</td>
<td>
<p><span>64.0</span></p>
</td>
<td>
<p><span>29.15342</span></p>
</td>
<td>
<p><span>155.548223</span></p>
</td>
<td>
<p><span>23.3</span></p>
</td>
<td>
<p><span>0.672</span></p>
</td>
<td>
<p><span>32.0</span></p>
</td>
<td>
<p>1.0</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>1.0</p>
</td>
<td>
<p><span>89.0</span></p>
</td>
<td>
<p><span>66.0</span></p>
</td>
<td>
<p><span>23.00000</span></p>
</td>
<td>
<p><span>94.000000</span></p>
</td>
<td>
<p><span>28.1</span></p>
</td>
<td>
<p><span>0.167</span></p>
</td>
<td>
<p><span>21.0</span></p>
</td>
<td>
<p>0.0</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>0.0</p>
</td>
<td>
<p><span>137.0</span></p>
</td>
<td>
<p><span>40.0</span></p>
</td>
<td>
<p><span>35.00000</span></p>
</td>
<td>
<p><span>168.000000</span></p>
</td>
<td>
<p><span>43.1</span></p>
</td>
<td>
<p><span>2.288</span></p>
</td>
<td>
<p><span>33.0</span></p>
</td>
<td>
<p>1.0</p>
</td>
</tr>
</tbody>
</table>
<div class="cell code_cell rendered unselected">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_html rendered_html output_result">
<p><span>Let's check in on our</span> <kbd>plasma_glucose_concentration</kbd><span> column to make sure that the values are still filled in with the same mean we calculated manually earlier:</span></p>
</div>
</div>
</div>
</div>
</div>
<div class="cell code_cell rendered selected">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class="CodeMirror cm-s-ipython">
<div class="CodeMirror-scroll">
<div class="CodeMirror-sizer">
<div class="CodeMirror-lines">
<div class="CodeMirror-measure">
<pre>pima_imputed.loc[empty_plasma_index]['plasma_glucose_concentration'] <br/># same values as we obtained with fillna<br/><br/>75     121.686763
182    121.686763
342    121.686763
349    121.686763
502    121.686763
Name: plasma_glucose_concentration, dtype: float64</pre></div>
<p class="CodeMirror-gutter-wrapper">As a final check, our imputed DataFrame should have no missing values, as shown in the following code:</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<pre><span>pima_imputed.isnull().sum()  # no missing values</span><br/><br/>times_pregnant                  0
plasma_glucose_concentration    0
diastolic_blood_pressure        0
triceps_thickness               0
serum_insulin                   0
bmi                             0
pedigree_function               0
age                             0
onset_diabetes                  0
dtype: int64</pre>
<p class="CodeMirror-gutter-wrapper">Great! The <kbd>Imputer</kbd> helps a great deal with the menial task of imputing data values into missing slots. Let's try imputing a few types of values and seeings its effect on our KNN model for classification. Let's first try an even simpler imputing method. Let's re-fill in the empty values with zeros:</p>
<pre>pima_zero = pima.fillna(0) # impute values with 0<br/><br/>X_zero = pima_zero.drop('onset_diabetes', axis=1)<br/>print "learning from {} rows".format(X_zero.shape[0])<br/>y_zero = pima_zero['onset_diabetes']<br/><br/>knn_params = {'n_neighbors':[1, 2, 3, 4, 5, 6, 7]}<br/>grid = GridSearchCV(knn, knn_params)<br/>grid.fit(X_zero, y_zero)<br/><br/>print grid.best_score_, grid.best_params_ <br/># if the values stayed at 0, our accuracy goes down<br/><br/><br/>learning from 768 rows
0.73046875 {'n_neighbors': 6}</pre>
<p>If we had left the values as <kbd>0</kbd>, our accuracy would have been lower than dropping the rows with missing values. Our goal now is to obtain a machine learning pipeline that can learn from all <kbd>768</kbd> rows, but can perform better than the model that learned from only 392 rows. This means that the accuracy to beat is <span>0.745, or 74.5%.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Imputing values in a machine learning pipeline</h1>
                </header>
            
            <article>
                
<p>If we wish to transfer the <kbd>Imputer</kbd> over to a production-ready machine learning pipeline, we will need to talk briefly about the topic of pipelines.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pipelines in machine learning</h1>
                </header>
            
            <article>
                
<p>When we talk about <em>pipelines</em> in machine learning, we are usually talking about the fact that data is only passed through a learning algorithm raw, but also through a variety of preprocessing steps and even multiple learning algorithms before the final output is interpreted. Because it is so common to have several steps and transformation and prediction within a single machine learning pipeline, scikit-learn has a built-in module for building these pipelines.</p>
<p>Pipelines are especially important because it is actually <em>improper</em> to not use a pipeline when imputing values using the <kbd>Imputer</kbd> class. This is because the goal of the learning algorithm is to generalize the patterns in the training set in order to apply those patterns to the testing set. If we impute values for the entire dataset before splitting the set and applying learning algorithms, then we are cheating and our models are not actually learning any patterns. To visualize this concept, let's take a single train test split, a potential one of many during a cross-validation training phase.</p>
<p>Let's take a copy of a single column of the <kbd>Pima</kbd> dataset in order to emphasize our point a bit more drastically, and also import a single train test split module from scikit-learn:</p>
<pre>from sklearn.model_selection import train_test_split<br/><br/>X = pima[['serum_insulin']].copy()<br/>y = pima['onset_diabetes'].copy()<br/><br/>X.isnull().sum()<br/><br/>serum_insulin    374<br/>dtype: int64</pre>
<div class="cell code_cell rendered unselected">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<p class="CodeMirror cm-s-ipython">Now, let's take a single split. But before doing so, we will impute the average value of <kbd>X</kbd> in the entire dataset, using the following code:</p>
</div>
</div>
</div>
</div>
<pre># the improper way.. imputing values BEFORE splitting<br/><br/>entire_data_set_mean = X.mean()    # take the entire datasets mean<br/>X = X.fillna(entire_data_set_mean) # and use it to fill in the missing spots<br/>print entire_data_set_mean<br/><br/>serum_insulin    155.548223
dtype: float64<br/><br/># Take the split using a random state so that we can examine the same split.<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)</pre>
<p>Now, let's fit a KNN model to the training and testing sets:</p>
<pre>knn = KNeighborsClassifier()<br/><br/>knn.fit(X_train, y_train)<br/><br/>knn.score(X_test, y_test)<br/><br/>0.65625  # the accuracy of the improper split</pre>
<p>Note that we aren't implementing any grid searching here, just a plain fit. We see that our model boasts a 66% accuracy rate (not great, but that's not the point). The important thing to note here is that both the training and the testing set of <kbd>X</kbd> were imputed using the mean of the entire <kbd>X</kbd> matrix. This is in direct violation of a core tenet of the machine learning procedure. We cannot assume that we know the mean of the entire dataset when predicting the test set's response values. Simply put, our KNN model is using information gained from the testing set to fit to the training set. This is a big red flag.</p>
<div class="packt_tip">For more information on pipelines and why we need to use them, check out <em>The Principles of Data Science</em> (available from Packt Publishing) at <a href="https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science">https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science</a></div>
<p>Now, let's do it properly by first taking the mean of the training set and then using the mean of the training set to fill in values of the testing set. Again, this procedure tests the model's ability to use the average value of training data to predict unseen test cases:</p>
<pre># the proper way.. imputing values AFTER splitting<br/>from sklearn.model_selection import train_test_split<br/><br/>X = pima[['serum_insulin']].copy()<br/>y = pima['onset_diabetes'].copy()<br/><br/># using the same random state to obtain the same split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)<br/><br/>X.isnull().sum()<br/><br/>serum_insulin    374
dtype: int64</pre>
<p>Now, instead of taking the mean of the entire <kbd>X</kbd> matrix, we will properly only do so for the training set and use that value to fill in missing cells in <strong>both</strong> the training and test set:</p>
<pre>training_mean = X_train.mean()<br/>X_train = X_train.fillna(training_mean)<br/>X_test = X_test.fillna(training_mean)<br/><br/>print training_mean <br/><br/>serum_insulin    158.546053
dtype: float64<br/><br/># not the entire dataset's mean, it's much higher!!</pre>
<p>Finally, let's score a KNN model on the <em>same</em> dataset, but imputed correctly, as shown in the following code:</p>
<pre>knn = KNeighborsClassifier()<br/><br/>knn.fit(X_train, y_train)<br/><br/>print knn.score(X_test, y_test)<br/><br/>0.4895<br/><br/># lower accuracy, but much more honest in the mode's ability to generalize a pattern to outside data</pre>
<p>This is of course a much lower accuracy, but at least it is a more honest representation of the model's ability to learn from the training set's features and apply what it learned to unseen and withheld testing data. Scikit-learn's pipelines make this entire process much easier by giving structure and order to the steps of our machine learning pipelines. Let's take a look at a code block of how to use the scikit-learn <kbd>Pipeline</kbd> with the <kbd>Imputer</kbd>:</p>
<pre>from sklearn.pipeline import Pipeline<br/><br/>knn_params = {'classify__n_neighbors':[1, 2, 3, 4, 5, 6, 7]}<br/># must redefine params to fit the pipeline<br/><br/>knn = KNeighborsClassifier() . # instantiate a KNN model<br/><br/>mean_impute = Pipeline([('imputer', Imputer(strategy='mean')), ('classify', knn)])<br/><br/>X = pima.drop('onset_diabetes', axis=1)<br/>y = pima['onset_diabetes']<br/><br/>grid = GridSearchCV(mean_impute, knn_params)<br/>grid.fit(X, y)<br/><br/>print grid.best_score_, grid.best_params_<br/><br/>0.731770833333 {'classify__n_neighbors': 6}<br/>mean_impute = Pipeline([('imputer', Imputer(strategy='mean')), ('classify', knn)])</pre>
<p>A few new things to note. First off, our <kbd>Pipeline</kbd> has two steps:</p>
<ul>
<li>An <kbd>Imputer</kbd> with <kbd>strategy= mean</kbd></li>
<li>A classifier of type KNN</li>
</ul>
<p>Secondly, we had to redefine our <kbd>param</kbd> dict for the grid search as we have to specify exactly to which step of the pipeline the <kbd>n_neighbors</kbd> parameter belongs:</p>
<pre>knn_params = {'classify__n_neighbors':[1, 2, 3, 4, 5, 6, 7]}</pre>
<p>Other than that, everything is normal and proper. The <kbd>Pipeline</kbd> class will handle most of the procedure for us. It will handle properly imputing values from several training sets and using them to fill in missing values in the test set, properly testing the KNN's ability to generalize patterns in the data and finally outputting the best performing model, having an accuracy of 73%, just beneath our goal to beat of .745. Now that we have this syntax down, let's try the entire procedure again, but with a slight modification, as shown in the following code:</p>
<pre>knn_params = {'classify__n_neighbors':[1, 2, 3, 4, 5, 6, 7]}<br/><br/>knn = KNeighborsClassifier() . # instantiate a KNN model<br/><br/>median_impute = Pipeline([('imputer', Imputer(strategy='median')), ('classify', knn)])<br/>X = pima.drop('onset_diabetes', axis=1)<br/>y = pima['onset_diabetes']<br/><br/>grid = GridSearchCV(median_impute, knn_params)<br/>grid.fit(X, y)<br/><br/>print grid.best_score_, grid.best_params_<br/><br/><span>0.735677083333 {'classify__n_neighbors': 6}</span></pre>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<p class="CodeMirror cm-s-ipython">Here, the only difference is that our pipeline will try a different strategy of imputing <strong>median</strong>, wherein the missing values will be filled in the median of the remaining values. It is important to reiterate that our accuracies may be lower than the model's fit on the dropped rows, but they were made on more than twice the size of the dataset with missing values! And they were still better than leaving them all at 0, as the data was originally presented to us.</p>
</div>
</div>
<p>Let's take a minute to recap the scores we have gotten so far using our proper pipelines:</p>
</div>
<table>
<tbody>
<tr>
<td>
<p><strong>Pipeline description</strong></p>
</td>
<td>
<p><strong># rows model learned from</strong></p>
</td>
<td>
<p><strong>Cross-validated accuracy</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>drop missing-valued rows</strong></p>
</td>
<td>
<p><strong>392</strong></p>
</td>
<td>
<p><strong>.74489</strong></p>
</td>
</tr>
<tr>
<td>
<p>Impute values with 0</p>
</td>
<td>
<p>768</p>
</td>
<td>
<p>.7304</p>
</td>
</tr>
<tr>
<td>
<p>Impute values with mean of column</p>
</td>
<td>
<p>768</p>
</td>
<td>
<p>.7318</p>
</td>
</tr>
<tr>
<td>
<p>Impute values with median of column</p>
</td>
<td>
<p>768</p>
</td>
<td>
<p>.7357</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p><span>If we go by accuracy alone, it appears the best procedure is to drop the missing-values rows. Perhaps using the</span> <kbd>Pipeline</kbd> <span>and</span> <kbd>Imputer</kbd> <span>features alone in scikit-learn is not enough. We still would like to see comparable (if not better) performance coming from all 768 rows if possible. In order to achieve this, let's try introducing a brand new feature engineering trick, standardization and normalization.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Standardization and normalization</h1>
                </header>
            
            <article>
                
<p>Up until now, we have dealt with identifying the types of data as well as the ways data can be missing and finally, the ways we can fill in missing data. Now, let's talk about how we can manipulate our data (and our features) in order to enhance our machine pipelines further. So far, we have tried four different ways of manipulating our dataset, and the best cross-validated accuracy we have achieved with a KNN model is .745. If we look back at some of the EDA we have previously done, we will notice something about our features:</p>
<pre>impute = Imputer(strategy='mean')<br/># we will want to fill in missing values to see all 9 columns<br/><br/>pima_imputed_mean = pd.DataFrame(impute.fit_transform(pima), columns=pima_column_names)</pre>
<p>Now, let's use a standard histogram to see the distribution across all nine columns, as follows, specifying a figure size:</p>
<pre>pima_imputed_mean.hist(figsize=(15, 15))</pre>
<p>The preceding code produces the following output:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="399" src="assets/a54e3566-6330-47f0-be03-ef5f2629f983.png" width="435"/></div>
<p>Nice, but notice anything off? Every single column has a vastly different mean, min, max, and standard deviation. This is also obvious through the describe method, using the following code:</p>
<pre>pima_imputed_mean.describe()</pre>
<p>The output is as follows:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>times<br/>
_pregnant</strong></p>
</td>
<td>
<p><strong>plasma<br/>
_glucose<br/>
_concentration</strong></p>
</td>
<td>
<p><strong>diastolic_<br/>
blood_pressure</strong></p>
</td>
<td>
<p><strong>triceps<br/>
_thickness</strong></p>
</td>
<td>
<p><strong>serum<br/>
_insulin</strong></p>
</td>
<td>
<p><strong>bmi</strong></p>
</td>
<td>
<p><strong>pedigree<br/>
_function</strong></p>
</td>
<td>
<p><strong>age</strong></p>
</td>
<td>
<p><strong>onset<br/>
_diabetes</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>count</span></strong></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>mean</span></strong></p>
</td>
<td>
<p><span>3.845052</span></p>
</td>
<td>
<p><span>121.686763</span></p>
</td>
<td>
<p><span>72.405184</span></p>
</td>
<td>
<p><span>29.153420</span></p>
</td>
<td>
<p><span>155.548223</span></p>
</td>
<td>
<p><span>32.457464</span></p>
</td>
<td>
<p><span>0.471876</span></p>
</td>
<td>
<p><span>33.240885</span></p>
</td>
<td>
<p><span>0.348958</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>std</span></strong></p>
</td>
<td>
<p><span>3.369578</span></p>
</td>
<td>
<p><span>30.435949</span></p>
</td>
<td>
<p><span>12.096346</span></p>
</td>
<td>
<p><span>8.790942</span></p>
</td>
<td>
<p><span>85.021108</span></p>
</td>
<td>
<p><span>6.875151</span></p>
</td>
<td>
<p><span>0.331329</span></p>
</td>
<td>
<p><span>11.760232</span></p>
</td>
<td>
<p><span>0.476951</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>min</span></strong></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>44.000000</span></p>
</td>
<td>
<p><span>24.000000</span></p>
</td>
<td>
<p><span>7.000000</span></p>
</td>
<td>
<p><span>14.000000</span></p>
</td>
<td>
<p><span>18.200000</span></p>
</td>
<td>
<p><span>0.078000</span></p>
</td>
<td>
<p><span>21.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>25%</span></strong></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>99.750000</span></p>
</td>
<td>
<p><span>64.000000</span></p>
</td>
<td>
<p><span>25.000000</span></p>
</td>
<td>
<p><span>121.500000</span></p>
</td>
<td>
<p><span>27.500000</span></p>
</td>
<td>
<p><span>0.243750</span></p>
</td>
<td>
<p><span>24.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>50%</span></strong></p>
</td>
<td>
<p>3.000000</p>
</td>
<td>
<p><span>117.000000</span></p>
</td>
<td>
<p><span>72.202592</span></p>
</td>
<td>
<p><span>29.153420</span></p>
</td>
<td>
<p><span>155.548223</span></p>
</td>
<td>
<p><span>32.400000</span></p>
</td>
<td>
<p><span>0.372500</span></p>
</td>
<td>
<p><span>29.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>75%</strong></p>
</td>
<td>
<p><span>6.000000</span></p>
</td>
<td>
<p><span>140.250000</span></p>
</td>
<td>
<p><span>80.000000</span></p>
</td>
<td>
<p><span>32.000000</span></p>
</td>
<td>
<p><span>155.548223</span></p>
</td>
<td>
<p><span>36.600000</span></p>
</td>
<td>
<p><span>0.626250</span></p>
</td>
<td>
<p><span>41.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>max</strong></p>
</td>
<td>
<p><span>17.000000</span></p>
</td>
<td>
<p><span>199.000000</span></p>
</td>
<td>
<p><span>122.000000</span></p>
</td>
<td>
<p><span>99.000000</span></p>
</td>
<td>
<p><span>846.000000</span></p>
</td>
<td>
<p><span>67.100000</span></p>
</td>
<td>
<p><span>2.420000</span></p>
</td>
<td>
<p><span>81.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p><span>But why does this matter? Well, some machine learning models rely on learning methods that are affected greatly by the </span><em>scale</em> <span>of the data, meaning that if we have a column such as </span><kbd>diastolic_blood_pressure</kbd> <span>that lives between 24 and 122, and an age column between 21 and 81, then our learning algorithms will not learn optimally. To really see the differences in scales, let's invoke two optional parameters in the histogram method,</span> <kbd>sharex</kbd> <span>and</span> <kbd>sharey</kbd><span>, so that we can see each graph on the same scale as every other graph, using the following code:</span></p>
<pre class="mce-root">pima_imputed_mean.hist(figsize=(15, 15), sharex=True)<br/># with the same x axis (the y axis is not as important here)</pre>
<p>The preceding code produces the following output:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="403" src="assets/19ac589d-4953-4cba-9484-ba41aec2a6ab.png" style="color: #333333;font-family: Merriweather, serif;font-size: 1em" width="439"/></div>
<p>It is quite clear that our data all lives on vastly different scales. Data engineers have options on how to deal with this problem in our machine learning pipelines that are under a family of operations called <strong>normalization</strong>. Normalization operations are meant to align and transform both columns and rows to a consistent set of rules. For example, a common form of normalization is to transform all quantitative columns to be between a consistent and static range of values (for example all values must be between 0 and 1). We may also impose mathematical rules such as, <em>all columns must have the same mean and standard deviation</em> so that they appear nicely on the same histogram (unlike the pima histograms we computed recently). Normalization techniques are meant to <em>level the playing field</em> of data by ensuring that all rows and columns are treated equally under the eyes of machine learning.</p>
<p>We will focus on three methods of data normalization:</p>
<ul>
<li>Z-score standardization</li>
<li>Min-max scaling</li>
<li>Row normalization</li>
</ul>
<p>The first two deal specifically with altering features in place, while the third option actually manipulates the rows of the data, but is still just as pertinent as the first two.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Z-score standardization</h1>
                </header>
            
            <article>
                
<p>The most common of the normalization techniques, <strong>z-score standardization</strong>, utilizes a very simple statistical idea of a z-score. The output of a z-score normalization are features that are re-scaled to have a mean of zero and a standard deviation of one. By doing this, by re-scaling our features to have a uniform mean and variance (square of standard deviation), then we allow models such as KNN to learn optimally and not skew towards larger scaled features. The formula is simple: for every column, we replace the cells with the following value:</p>
<p style="padding-left: 210px"><em>z = (x - μ) / σ</em></p>
<p>Where:</p>
<ul>
<li><em>z</em> is our new value (z-score)</li>
<li><em>x</em> is the previous value of the cell</li>
<li><span><em>μ</em> is the mean of the column</span></li>
<li><span><em>σ</em> is the standard deviation of the columns</span></li>
</ul>
<p>Let's see an example by scaling the <kbd>plasma_glucose_concentration</kbd> column in our dataset:</p>
<pre>print pima['plasma_glucose_concentration'].head()<br/><br/><br/>0    148.0
1     85.0
2    183.0
3     89.0
4    137.0
Name: plasma_glucose_concentration, dtype: float64</pre>
<p>And now let's manually compute z-scores for every value in our column, using the following code:</p>
<pre class="mce-root"># get the mean of the column<br/>mu = pima['plasma_glucose_concentration'].mean()<br/><br/># get the standard deviation of the column<br/>sigma = pima['plasma_glucose_concentration'].std()<br/><br/># calculate z scores for every value in the column.<br/>print ((pima['plasma_glucose_concentration'] - mu) / sigma).head()<br/><br/>0    0.864545
1   -1.205376
2    2.014501
3   -1.073952
4    0.503130
Name: plasma_glucose_concentration, dtype: float64</pre>
<p>We see that every single value in the column will be replaced, and also notice how now some of them are negative. This is because the resulting values represent a <em>distance</em> from the mean. So, if a value originally was below the mean of the column, the resulting z-score will be negative. Of course, in scikit-learn, we have built-in objects to help us out, as shown in the following code:</p>
<pre># built in z-score normalizer<br/>from sklearn.preprocessing import StandardScaler</pre>
<p>Let's try it out, shown as follows:</p>
<pre># mean and std before z score standardizing<br/>pima['plasma_glucose_concentration'].mean(), pima['plasma_glucose_concentration'].std()<br/><br/>(121.68676277850591, 30.435948867207657)<br/><br/><br/>ax = pima['plasma_glucose_concentration'].hist()<br/>ax.set_title('Distribution of plasma_glucose_concentration')</pre>
<p>The preceding code produces the following output:</p>
<div class="CodeMirror cm-s-ipython CDPAlignCenter CDPAlign"><img height="181" src="assets/0d15f3d1-4137-4f8e-b6db-b0a921abdca1.png" width="300"/></div>
<p>Here, we can see the distribution of the column before doing anything. Now, let's apply a z-score scaling, as shown in the following code:</p>
<pre>scaler = StandardScaler()<br/><br/>glucose_z_score_standardized = scaler.fit_transform(pima[['plasma_glucose_concentration']])<br/># note we use the double bracket notation [[ ]] because the transformer requires a dataframe<br/><br/># mean of 0 (floating point error) and standard deviation of 1<br/>glucose_z_score_standardized.mean(), glucose_z_score_standardized.std()<br/><br/>(-3.5619655373390441e-16, 1.0)</pre>
<p class="CodeMirror cm-s-ipython">We can see that after we apply our scaler to the column, or mean drops to zero and our standard deviation is one. Furthermore, if we take a look at the distribution of values across our recently scaled data:</p>
<pre>ax = pd.Series(glucose_z_score_standardized.reshape(-1,)).hist()<br/>ax.set_title('Distribution of plasma_glucose_concentration after Z Score Scaling')</pre>
<p>The output is as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="172" src="assets/9ba0f92b-7fc8-46f6-92e2-848bc109359b.png" width="418"/></div>
<p>We will notice that our <em>x</em> axis is now much more constrained, while our <em>y </em>axis is unchanged. Also note that the shape of the data is unchanged entirely. Let's take a look at the histograms of our DataFrame after we apply a z-score transformation on every single column. When we do this, the <kbd>StandardScaler</kbd> will compute a mean and standard deviation for every column separately:</p>
<pre>scale = StandardScaler() # instantiate a z-scaler object<br/><br/>pima_imputed_mean_scaled = pd.DataFrame(scale.fit_transform(pima_imputed_mean), columns=pima_column_names)<br/>pima_imputed_mean_scaled.hist(figsize=(15, 15), sharex=True)<br/># now all share the same "space"</pre>
<p>The preceding code produces the following output:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="542" src="assets/2e944967-c803-4a98-b357-ad8766bcdc18.png" width="589"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">Notice that our <em>x</em> axes are all now much more constrained across the entire dataset. Let's now plug a <kbd>StandardScaler</kbd> into our machine learning pipeline from before:</p>
<pre>knn_params = {'imputer__strategy':['mean', 'median'], 'classify__n_neighbors':[1, 2, 3, 4, 5, 6, 7]}<br/><br/>mean_impute_standardize = Pipeline([('imputer', Imputer()), ('standardize', StandardScaler()), ('classify', knn)])<br/>X = pima.drop('onset_diabetes', axis=1)<br/>y = pima['onset_diabetes']<br/><br/>grid = GridSearchCV(mean_impute_standardize, knn_params)<br/>grid.fit(X, y)<br/><br/>print grid.best_score_, grid.best_params_<br/><br/><br/>0.7421875 {'imputer__strategy': 'median', 'classify__n_neighbors': 7}</pre>
<div class="cell code_cell rendered unselected">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<p class="CodeMirror cm-s-ipython">Note a few things here. We included a new set of parameters to grid search, namely the strategy of imputing missing values. Now, I am looking for the best combination of strategy and number of neighbors in our KNN attached to a z-score scaling and our result is .742, which so far is the closest score we have gotten to our goal of beating .745, and this pipeline is learning from all 768 rows. Let's now look at another method of column normalization.</p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The min-max scaling method</h1>
                </header>
            
            <article>
                
<p><strong>Min-max scaling </strong>is similar to z-score normalization in that it will replace every value in a column with a new value using a formula. In this case, that formula is:</p>
<p style="padding-left: 180px"><em>m = (x -x<sub>min</sub>) / (x<sub>max</sub> -x<sub>min</sub>)</em></p>
<p>Where:</p>
<ul>
<li><em>m</em> is our new value</li>
<li><em>x</em> is the original cell value</li>
<li><em>x<sub>min</sub></em> is the minimum value of the column</li>
<li><em>x<sub>max</sub></em> is the maximum value of the column</li>
</ul>
<p>Using this formula, we will see that the values of each column will now be between zero and one. Let's take a look at an example using a built-in scikit-learn module:</p>
<pre># import the sklearn module<br/>from sklearn.preprocessing import MinMaxScaler<br/><br/>#instantiate the class<br/>min_max = MinMaxScaler()<br/><br/># apply the Min Max Scaling<br/>pima_min_maxed = pd.DataFrame(min_max.fit_transform(pima_imputed), columns=pima_column_names)<br/><br/># spit out some descriptive statistics<br/>pima_min_maxed.describe()</pre>
<p>Here is the output of our <kbd>describe</kbd> method:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>times<br/>
_pregnant</strong></p>
</td>
<td>
<p><strong>plasma<br/>
_glucose<br/>
_concentration</strong></p>
</td>
<td>
<p><strong>diastolic<br/>
_blood<br/>
_pressure</strong></p>
</td>
<td>
<p><strong>triceps<br/>
_thickness</strong></p>
</td>
<td>
<p><strong>serum<br/>
_insulin</strong></p>
</td>
<td>
<p><strong>bmi</strong></p>
</td>
<td>
<p><strong>pedigree<br/>
_function</strong></p>
</td>
<td>
<p><strong>age</strong></p>
</td>
<td>
<p><strong>onset<br/>
_diabetes</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>count</span></strong></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
<td>
<p><span>768.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>mean</span></strong></p>
</td>
<td>
<p><span>0.226180</span></p>
</td>
<td>
<p><span>0.501205</span></p>
</td>
<td>
<p><span>0.493930</span></p>
</td>
<td>
<p><span>0.240798</span></p>
</td>
<td>
<p><span>0.170130</span></p>
</td>
<td>
<p><span>0.291564</span></p>
</td>
<td>
<p><span>0.168179</span></p>
</td>
<td>
<p><span>0.204015</span></p>
</td>
<td>
<p><span>0.348958</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>std</span></strong></p>
</td>
<td>
<p><span>0.198210</span></p>
</td>
<td>
<p><span>0.196361</span></p>
</td>
<td>
<p><span>0.123432</span></p>
</td>
<td>
<p><span>0.095554</span></p>
</td>
<td>
<p><span>0.102189</span></p>
</td>
<td>
<p><span>0.140596</span></p>
</td>
<td>
<p><span>0.141473</span></p>
</td>
<td>
<p><span>0.196004</span></p>
</td>
<td>
<p><span>0.476951</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>min</span></strong></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>25%</span></strong></p>
</td>
<td>
<p><span>0.058824</span></p>
</td>
<td>
<p><span>0.359677</span></p>
</td>
<td>
<p><span>0.408163</span></p>
</td>
<td>
<p><span>0.195652</span></p>
</td>
<td>
<p><span>0.129207</span></p>
</td>
<td>
<p><span>0.190184</span></p>
</td>
<td>
<p><span>0.070773</span></p>
</td>
<td>
<p><span>0.050000</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong><span>50%</span></strong></p>
</td>
<td>
<p><span>0.176471</span></p>
</td>
<td>
<p><span>0.470968</span></p>
</td>
<td>
<p><span>0.491863</span></p>
</td>
<td>
<p><span>0.240798</span></p>
</td>
<td>
<p><span>0.170130</span></p>
</td>
<td>
<p><span>0.290389</span></p>
</td>
<td>
<p><span>0.125747</span></p>
</td>
<td>
<p><span>0.133333</span></p>
</td>
<td>
<p><span>0.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>75%</strong></p>
</td>
<td>
<p><span>0.352941</span></p>
</td>
<td>
<p><span>0.620968</span></p>
</td>
<td>
<p><span>0.571429</span></p>
</td>
<td>
<p><span>0.271739</span></p>
</td>
<td>
<p><span>0.170130</span></p>
</td>
<td>
<p><span>0.376278</span></p>
</td>
<td>
<p><span>0.234095</span></p>
</td>
<td>
<p><span>0.333333</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>max</strong></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
<td>
<p><span>1.000000</span></p>
</td>
</tr>
</tbody>
</table>
<p><br/>
Notice how the <kbd>min</kbd> are all zeros and the <kbd>max</kbd> values are all ones. Note further that the standard deviations are now all very very small, a side effect of this type of scaling. This can hurt some models as it takes away weight from outliers. Let's plug our new normalization technique into our pipeline:</p>
<pre>knn_params = {'imputer__strategy': ['mean', 'median'], 'classify__n_neighbors':[1, 2, 3, 4, 5, 6, 7]}<br/><br/>mean_impute_standardize = Pipeline([('imputer', Imputer()), ('standardize', MinMaxScaler()), ('classify', knn)])<br/>X = pima.drop('onset_diabetes', axis=1)<br/>y = pima['onset_diabetes']<br/><br/>grid = GridSearchCV(mean_impute_standardize, knn_params)<br/>grid.fit(X, y)<br/><br/>print grid.best_score_, grid.best_params_<br/><br/>0.74609375 {'imputer__strategy': 'mean', 'classify__n_neighbors': 4}</pre>
<p>Woah, this is the best accuracy we've gotten so far working with the missing data and using all of the 768 original rows in the dataset. It seems as though the min-max scaling is helping our KNN a great deal! Wonderful; let's try a third type of normalization, and this time let's move away from normalizing columns and onto normalizing rows instead.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The row normalization method</h1>
                </header>
            
            <article>
                
<p>Our final normalization method works row-wise instead of column-wise. Instead of calculating statistics on each column, mean, min, max, and so on, the row normalization technique will ensure that each row of data has a <em>unit norm</em>, meaning that each row will be the same vector length. Imagine if each row of data belonged to an n-dimensional space; each one would have a vector norm, or length. Another way to put it is if we consider every row to be a vector in space:</p>
<p style="padding-left: 180px"><em>x = (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>)</em></p>
<p>Where 1, 2, ..., n in the case of Pima would be 8, 1 for each feature (not including the response), the norm would be calculated as: </p>
<p style="padding-left: 180px"><em>||x|| = √(x<sub>1</sub><sup>2 + </sup>x<sub>2</sub><sup>2 +</sup> ... + x<sub>n</sub><sup>2</sup>)</em></p>
<p>This is called the <strong>L-2 Norm</strong>. Other types of norms exist, but we will not get into that in this text. Instead, we are concerned with making sure that every single row has the same norm. This comes in handy, especially when working with text data or clustering algorithms.</p>
<p>Before doing anything, let's see the average norm of our mean-imputed matrix, using the following code:</p>
<div class="cell code_cell rendered selected">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_text output_result">
<pre><span>np.sqrt((pima_imputed**2).sum(axis=1)).mean() </span><br/><span># average vector length of imputed matrix</span><br/><br/>223.36222025823744</pre></div>
</div>
</div>
</div>
</div>
<p>Now, let's bring in our row-normalizer, as shown in the following code:</p>
<pre>from sklearn.preprocessing import Normalizer # our row normalizer<br/><br/>normalize = Normalizer()<br/><br/>pima_normalized = pd.DataFrame(normalize.fit_transform(pima_imputed), columns=pima_column_names)<br/><br/>np.sqrt((pima_normalized**2).sum(axis=1)).mean()<br/># average vector length of row normalized imputed matrix<br/><br/>1.0</pre>
<p>After normalizing, we see that every single row has a norm of one now. Let's see how this method fares in our pipeline:</p>
<pre>knn_params = {'imputer__strategy': ['mean', 'median'], 'classify__n_neighbors':[1, 2, 3, 4, 5, 6, 7]}<br/><br/>mean_impute_normalize = Pipeline([('imputer', Imputer()), ('normalize', Normalizer()), ('classify', knn)])<br/>X = pima.drop('onset_diabetes', axis=1)<br/>y = pima['onset_diabetes']<br/><br/>grid = GridSearchCV(mean_impute_normalize, knn_params)<br/>grid.fit(X, y)<br/><br/>print grid.best_score_, grid.best_params_<br/><br/>0.682291666667 {'imputer__strategy': 'mean', 'classify__n_neighbors': 6}</pre>
<div class="cell code_cell rendered unselected">
<div class="input">
<p class="prompt input_prompt">Ouch, not great, but worth a try. Now that we have seen three different methods of data normalization, let's put it all together and see how we did on this dataset.</p>
</div>
</div>
<p>There are many learning algorithms that are affected by the scale of data. Here is a list of some popular learning algorithms that are affected by the scale of data:</p>
<ul>
<li>KNN-due to its reliance on the Euclidean Distance</li>
<li>K-Means Clustering - same reasoning as KNN</li>
<li>Logistic regression, SVM, neural networks—if you are using gradient descent to learn weights</li>
<li>Principal component analysis—eigen vectors will be skewed towards larger columns</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting it all together</h1>
                </header>
            
            <article>
                
<p>After dealing with a variety of problems with our dataset, from identifying missing values hidden as zeros, imputing missing values, and normalizing data at different scales, it's time to put all of our scores together into a single table and see what combination of feature engineering did the best:</p>
<table>
<tbody>
<tr>
<td>
<p><strong>Pipeline description</strong></p>
</td>
<td>
<p><strong># rows model learned from</strong></p>
</td>
<td>
<p><strong>Cross-validated accuracy</strong></p>
</td>
</tr>
<tr>
<td>
<p>Drop missing-valued rows</p>
</td>
<td>
<p>392</p>
</td>
<td>
<p>.7449</p>
</td>
</tr>
<tr>
<td>
<p>Impute values with 0</p>
</td>
<td>
<p>768</p>
</td>
<td>
<p>.7304</p>
</td>
</tr>
<tr>
<td>
<p>Impute values with mean of column</p>
</td>
<td>
<p>768</p>
</td>
<td>
<p>.7318</p>
</td>
</tr>
<tr>
<td>
<p>Impute values with median of column</p>
</td>
<td>
<p>768</p>
</td>
<td>
<p>.7357</p>
</td>
</tr>
<tr>
<td>
<p>Z-score normalization with median imputing</p>
</td>
<td>
<p>768</p>
</td>
<td>
<p>.7422</p>
</td>
</tr>
<tr>
<td>
<p><strong>Min-max normalization with mean imputing</strong></p>
</td>
<td>
<p><strong>768</strong></p>
</td>
<td>
<p><strong>.7461</strong></p>
</td>
</tr>
<tr>
<td>
<p>Row-normalization with mean imputing</p>
</td>
<td>
<p>768</p>
</td>
<td>
<p>.6823</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root">It seems as though we were finally able to get a better accuracy by applying mean imputing and min-max normalization to our dataset and still use all <kbd>768</kbd> available rows. Great!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">Feature improvement is about recognizing areas of issue and improvement in our data and figuring out which cleaning methods will be the most effective. Our main takeaway should be to look at data with the eyes of a data scientist. Instead of immediately dropping rows/columns with problems, we should think about the best ways of fixing these problems. More often than not, our machine learning performance will thank us in the end.</p>
<p class="mce-root">This chapter contains several ways of dealing with issues with our quantitative columns. The next chapter will deal with the imputing of categorical columns, as well as how to introduce brand new features into the mix from existing features. We will be working with scikit-learn pipelines with a mix of numerical and categorical columns to really expand the types of data we can work with.</p>


            </article>

            
        </section>
    </body></html>