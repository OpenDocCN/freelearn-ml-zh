<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Sentiment Analysis of Amazon Reviews with NLP</h1>
                </header>
            
            <article>
                
<p class="mce-root">Every day, we generate data from emails, online posts such as blogs, social media comments, and more. It is not surprising to say that unstructured text data is much larger in size than the tabular data that exists in the databases of any organization. It is important for organizations to acquire useful insights from the text data pertaining to the organization. Due to the different nature of the text data when compared to data in databases, the methods that need to be employed to understand the text data are different. In this chapter, we will learn a number of key techniques in <strong class="calibre3">natural language processing</strong> (<strong class="calibre3">NLP</strong>) that help us to work on text data. </p>
<p class="mce-root">The common definition of NLP is as follows: an area of computer science and artificial intelligence that deals with the interactions between computers and human (natural) languages; in particular, how to program computers to fruitfully process large amounts of natural language data.</p>
<p class="mce-root">In general terms, NLP deals with understanding human speech as it is spoken. It helps machines read and understand "text".</p>
<p class="mce-root">Human languages are highly complex and several ambiguities need to be resolved in order to correctly comprehend the spoken language or written text. In the area of NLP, several techniques are applied in order to deal with these ambiguities, including the <strong class="calibre3">Part-of-Speech</strong> (<strong class="calibre3">POS</strong>) tagger, term disambiguation, entity extraction, relations' extraction, key term recognition, and more.</p>
<p class="mce-root">For natural language systems to work successfully, a consistent knowledge base, such as a detailed thesaurus, a lexicon of words, a dataset for linguistic and grammatical rules, an ontology, and up-to-date entities, are prerequisites.</p>
<p class="mce-root">It may be noted that NLP is concerned with understanding the text from not just the syntactic perspective, but also from a semantic perspective. Similar to humans, the idea is for the machines to be able to perceive underlying messages behind the spoken words and not just the structure of words in sentences. There are numerous application areas of NLP, and the following are just a few of these:</p>
<ul class="calibre9">
<li class="calibre10">Speech recognition systems</li>
<li class="calibre10">Question answering systems</li>
<li class="calibre10">Machine translation</li>
<li class="calibre10">Text summarization</li>
<li class="calibre10">Virtual agents or chatbots</li>
<li class="calibre10">Text classification</li>
<li class="calibre10">Topic segmentation</li>
</ul>
<p class="mce-root">As the NLP subject area in itself is very vast, it is not practical to cover all the areas in just one chapter. Therefore, we will be focusing on "text classification" in this chapter. We do this by implementing a project that performs sentiment analysis in the reviews expressed by Amazon.com customers. Sentiment analysis is a type of text classification task where we classify each of the documents (reviews) into one of the possible categories. The possible categories could be positive, negative, or neutral, or it could be positive, negative, or a rating on a scale of 1 to 10.</p>
<p class="mce-root">Text documents that need to be classified cannot be input directly to a machine learning algorithm. Each of the documents needs to be represented in a certain format that is acceptable for the ML algorithm as input to work on. In this chapter, we explore, implement, and understand the <strong class="calibre3">Bag of Words</strong> (<strong class="calibre3">BoW</strong>) word embedding approaches. These are approaches in which text can be represented.</p>
<p class="mce-root">As we progress with the chapter, we will cover the following topics:</p>
<ul class="calibre9">
<li class="calibre10">The sentiment analysis problem</li>
<li class="calibre10">Understanding the Amazon reviews dataset</li>
<li class="calibre10">Building a text sentiment classifier with the BoW approach</li>
<li class="calibre10">Understanding word embedding approaches</li>
<li class="calibre10">Building a text sentiment classifier with pretrained Word2Vec word embedding based on Reuters news corpus</li>
<li class="calibre10">Building a text sentiment classifier with GloVe word embedding</li>
<li class="calibre10">Building a text sentiment classifier with fastText</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The sentiment analysis problem</h1>
                </header>
            
            <article>
                
<p class="mce-root">Sentiment analysis is one of the most general text classification applications. The purpose of it is to analyze messages such as user reviews, and feedback from employees, in order to identify whether the underlying sentiment is positive, negative, or neutral.</p>
<p class="mce-root">Analyzing and reporting sentiment in texts allows businesses to quickly get a consolidated high-level insight without having to read each one of the comments received. </p>
<p class="mce-root">While it is possible to generate holistic sentiment based on the overall comments received, there is also an extended area called <strong class="calibre3">aspect-based sentiment analysis</strong>. It is focused on deriving sentiment based on each area of the service. For example, a customer that visited a restaurant when writing a review would generally cover areas such as ambience, food quality, service quality, and price. Though the feedback about each of the areas may not be quoted under a specific heading, the sentences in the review comments would naturally cover the customer's opinion of one or more of these areas. Aspect-based sentiment analysis attempts to identify the sentences in the reviews in each of the areas and then identify whether the sentiment is positive, negative, or neutral. Providing sentiment by each area helps businesses quickly identify their weak areas.</p>
<p class="mce-root">In this chapter, we will discuss and implement methods that are aimed at identifying the overall sentiment from the review texts. The task can be achieved in several ways, ranging from a simple lexicon method to a complex word embedding method.</p>
<p class="mce-root">A <strong class="calibre3">lexicon</strong> method is not really a machine learning method. It is more a rule based method that is based on a predefined positive and negative words dictionary. The method involves looking up the number of positive words and negative words in each review. If the count of positive words in the review is more than the count of negative words, then the review is marked as positive, otherwise it is marked as negative. If there are an equal number of positive and negative words, then the review is marked as neutral. As implementing this method is straightforward, and as it comes with a requirement for a predefined dictionary, we will not cover the implementation of the lexicon method in this chapter.</p>
<p class="mce-root">While it is possible to consider the sentiment analysis problem as an unsupervised clustering problem, in this chapter we consider it as a supervised classification problem. This is because, we have the Amazon reviews labeled dataset available. We can make use of these labels to build classification models, and therefore, the supervised algorithm. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="calibre4">The dataset is available for download and use at the following URL:</span></p>
<p class="mce-root"><span class="calibre4"> </span><a href="https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M" class="calibre8">https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M</a><span class="calibre4"> .</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the Amazon reviews dataset</h1>
                </header>
            
            <article>
                
<p class="mce-root">We use the Amazon product reviews polarity dataset for the various projects in this chapter. It is an open dataset constructed and made available by Xiang Zhang. It is used as a text classification benchmark in the paper: <em class="calibre15">Character-level Convolutional Networks for Text Classification</em> and <em class="calibre15">Advances in Neural Information Processing Systems</em> 28,<em class="calibre15"> </em><em class="calibre15">Xiang Zhang, Junbo Zhao, Yann LeCun,</em><em class="calibre15"> (NIPS 2015)</em>.</p>
<p class="mce-root">The Amazon reviews polarity dataset is constructed by taking review score 1 and 2 as negative, 4 and 5 as positive. Samples of score 3 are ignored. In the dataset, class 1 is the negative and class 2 is the positive. The dataset has 1,800,000 training samples and 200,000 testing samples.</p>
<p class="mce-root">The <kbd class="calibre11">train.csv</kbd> and <kbd class="calibre11">test.csv</kbd> files contains all the samples as comma-separated values. There are three columns in them, corresponding to class index (1 or 2), review title, and review text. The review title and text are escaped using double quotes ("), and any internal double quote is escaped by 2 double quotes (""). New lines are escaped by a backslash followed with an "n" character that is "\n".</p>
<p class="mce-root">To ensure that we are able to run our projects, even with minimal infrastructure, let's restrict the number of records to be considered in our dataset to 1,000 records only. Of course, the code that we use in the projects can be extended to any number of records, as long as the hardware infrastructure support is available. Let's first read the data and visualize the records with the following code:</p>
<pre class="calibre16"># reading first 1000 reviews<br class="title-page-name"/>reviews_text&lt;-readLines('/home/sunil/Desktop/sentiment_analysis/amazon _reviews_polarity.csv', n = 1000)<br class="title-page-name"/># converting the reviews_text character vector to a dataframe<br class="title-page-name"/>reviews_text&lt;-data.frame(reviews_text)<br class="title-page-name"/># visualizing the dataframe<br class="title-page-name"/>View(reviews_text)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter30" src="assets/b6bce54f-dc46-455b-ad50-0c6c06ae41ca.png"/></p>
<p class="mce-root">Post reading the file, we can see that there is only one column in the dataset and this column had both the review text and the sentiment components in it. We will slightly modify the format of the dataset for the purpose of using it with sentiment analysis projects in this chapter involving the BoW, Word2vec, and GloVe approaches. Let's modify the format of the dataset with the following code:</p>
<pre class="calibre16"># separating the sentiment and the review text<br class="title-page-name"/># post separation the first column will have the first 4 characters<br class="title-page-name"/># second column will have the rest of the characters<br class="title-page-name"/># first column should be named "Sentiment"<br class="title-page-name"/># second column to be named "SentimentText"<br class="title-page-name"/>library(tidyr)<br class="title-page-name"/>reviews_text&lt;-separate(data = reviews_text, col = reviews_text, into = c("Sentiment", "SentimentText"), sep = 4)<br class="title-page-name"/># viewing the dataset post the column split<br class="title-page-name"/>View(reviews_text)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter31" src="assets/79b22882-b09c-4e3e-9e20-92f7bbbed8f0.png"/></p>
<p class="mce-root">Now we have two columns in our dataset. However, there is unnecessary punctuation that exists in both the columns that may cause problems with processing the dataset further. Let's attempt to remove the punctuation with the following code:</p>
<pre class="calibre16"># Retaining only alphanumeric values in the sentiment column<br class="title-page-name"/>reviews_text$Sentiment&lt;-gsub("[^[:alnum:] ]","",reviews_text$Sentiment)<br class="title-page-name"/># Retaining only alphanumeric values in the sentiment text<br class="title-page-name"/>reviews_text$SentimentText&lt;-gsub("[^[:alnum:] ]"," ",reviews_text$SentimentText)<br class="title-page-name"/># Replacing multiple spaces in the text with single space<br class="title-page-name"/>reviews_text$SentimentText&lt;-gsub("(?&lt;=[\\s])\\s*|^\\s+|\\s+$", "", reviews_text$SentimentText, perl=TRUE)<br class="title-page-name"/># Viewing the dataset<br class="title-page-name"/>View(reviews_text)<br class="title-page-name"/># Writing the output to a file that can be consumed in other projects<br class="title-page-name"/>write.table(reviews_text,file = "/home/sunil/Desktop/sentiment_analysis/Sentiment Analysis Dataset.csv",row.names = F,col.names = T,sep=',')</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter32" src="assets/0b22278e-b011-4631-bb32-b1e1742aba4f.png"/></p>
<p class="mce-root">From the preceding output, we see that we have a clean dataset that is ready for use. Also, we have written the output to a file. When we build the sentiment analyzer, we can start directly reading the dataset from the <kbd class="calibre11">Sentiment Analysis Dataset.csv</kbd> file.</p>
<p class="mce-root">The fastText algorithm expects the dataset to be in a different format. The data input to fastText should comply the following format:</p>
<pre class="calibre16">__label__&lt;X&gt;  &lt;Text&gt;</pre>
<p class="mce-root">In this example<span class="calibre4">, </span><kbd class="calibre11">X</kbd> is the class name. Text is the actual review text that led to the rating specified under the class. Both the rating and text should be placed on one line with no quotes. The classes are <kbd class="calibre11">__label__1</kbd> and <kbd class="calibre11">__label__2</kbd>, and there should be only one class per row. Let's accomplish the <kbd class="calibre11">fastText</kbd> library required format with the following code block:</p>
<pre class="calibre16"># reading the first 1000 reviews from the dataset<br class="title-page-name"/>reviews_text&lt;-readLines('/home/sunil/Desktop/sentiment_analysis/amazon _reviews_polarity.csv', n = 1000)<br class="title-page-name"/># basic EDA to confirm that the data is read correctly<br class="title-page-name"/>print(class(reviews_text))<br class="title-page-name"/>print(length(reviews_text))<br class="title-page-name"/>print(head(reviews_text,2))<br class="title-page-name"/># replacing the positive sentiment value 2 with __label__2<br class="title-page-name"/>reviews_text&lt;-gsub("\\\"2\\\",","__label__2 ",reviews_text)<br class="title-page-name"/># replacing the negative sentiment value 1 with __label__1<br class="title-page-name"/>reviews_text&lt;-gsub("\\\"1\\\",","__label__1 ",reviews_text)<br class="title-page-name"/># removing the unnecessary \" characters<br class="title-page-name"/>reviews_text&lt;-gsub("\\\""," ",reviews_text)<br class="title-page-name"/># replacing multiple spaces in the text with single space<br class="title-page-name"/>reviews_text&lt;-gsub("(?&lt;=[\\s])\\s*|^\\s+|\\s+$", "", reviews_text, perl=TRUE)<br class="title-page-name"/># Basic EDA post the required processing to confirm input is as desired<br class="title-page-name"/>print("EDA POST PROCESSING")<br class="title-page-name"/>print(class(reviews_text))<br class="title-page-name"/>print(length(reviews_text))<br class="title-page-name"/>print(head(reviews_text,2))<br class="title-page-name"/># writing the revamped file to the directory so we could use it with<br class="title-page-name"/># fastText sentiment analyzer project<br class="title-page-name"/>fileConn&lt;-file("/home/sunil/Desktop/sentiment_analysis/Sentiment Analysis Dataset_ft.txt")<br class="title-page-name"/>writeLines(reviews_text, fileConn)<br class="title-page-name"/>close(fileConn)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">[1] "EDA PRIOR TO PROCESSING"<br class="title-page-name"/>[1] "character"<br class="title-page-name"/>[1] 1000<br class="title-page-name"/>[1] "\"2\",\"Stuning even for the non-gamer\",\"This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\""                                                                                  <br class="title-page-name"/>[2] "\"2\",\"The best soundtrack ever to anything.\",\"I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\""<br class="title-page-name"/>[1] "EDA POST PROCESSING"<br class="title-page-name"/>[1] "character"<br class="title-page-name"/>[1] 1000\<br class="title-page-name"/>[1] "__label__2 Stuning even for the non-gamer , This sound track was beautiful! It paints the senery in your mind so well I would recommend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^"                                                                                   <br class="title-page-name"/>[2] "__label__2 The best soundtrack ever to anything. , I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade. The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny."</pre>
<p class="mce-root">From the output of basic EDA code, we can see that the dataset is in the required format, therefore we can proceed to our next section of implementing the sentiment analysis engine using the BoW approach. Along side the implementation, we will delve into learning the concept behind the approach, and explore the sub-techniques that can be leveraged in the approach to obtain better results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a text sentiment classifier with the BoW approach</h1>
                </header>
            
            <article>
                
<p class="mce-root">The intent of the BoW approach is to convert the review text provided into a matrix form. It represents documents as a set of distinct words by ignoring the order and meaning of the words. Each row of the matrix represents each review (otherwise called a document in NLP), and the columns represent the universal set of words present in all the reviews. For each document, and across each word, the existence of the word, or the frequency of the word occurrence, in that specific document is recorded. Finally, the matrix created from word frequency vectors represents the documents set. This methodology is used to create input datasets that are required to train the models, and also to prepare the test dataset that need to be used by the trained models to perform text classification. Now that we understand the BoW motivation, let's jump into implementing the steps to build a sentiment analysis classifier based on this approach, as shown in the following code block:</p>
<pre class="calibre16"># including the required libraries<br class="title-page-name"/>library(SnowballC)<br class="title-page-name"/>library(tm)<br class="title-page-name"/># setting the working directory where the text reviews dataset is located<br class="title-page-name"/># recollect that we pre-processed and transformed the raw dataset format<br class="title-page-name"/>setwd('/home/sunil/Desktop/sentiment_analysis/')<br class="title-page-name"/># reading the transformed file as a dataframe<br class="title-page-name"/>text &lt;- read.table(file='Sentiment Analysis Dataset.csv', sep=',',header = TRUE)<br class="title-page-name"/># checking the dataframe to confirm everything is in tact<br class="title-page-name"/>print(dim(text))<br class="title-page-name"/>View(text)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">&gt; print(dim(text))<br class="title-page-name"/>[1] 1000 2<br class="title-page-name"/>&gt; View(text)</pre>
<p class="CDPAlignCenter1"><img class="aligncenter33" src="assets/068d0389-1c4c-418b-b6c3-ecb897f30581.png"/></p>
<p class="mce-root">The first step in processing text data involves creating a <em class="calibre15">corpus</em>, which is a collection of text documents. The <kbd class="calibre11">VCorpus</kbd> function in the <kbd class="calibre11">tm</kbd> package enables conversion of the reviews comments column in the data frame into a volatile corpus. This can be achieved through the following code:</p>
<pre class="calibre16"># transforming the text into volatile corpus<br class="title-page-name"/>train_corp = VCorpus(VectorSource(text$SentimentText))<br class="title-page-name"/>print(train_corp)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">&gt; print(train_corp)<br class="title-page-name"/>&lt;&lt;VCorpus&gt;&gt;<br class="title-page-name"/>Metadata:  corpus specific: 0, document level (indexed): 0<br class="title-page-name"/>Content:  documents: 1000</pre>
<p class="mce-root">From the volatile corpus, we create a <strong class="calibre3">Document Term Matrix</strong> (<strong class="calibre3">DTM</strong>). A DTM is a sparse matrix that is created using <span class="calibre4">the </span><kbd class="calibre11">tm</kbd><span class="calibre4"> </span>library's <kbd class="calibre11">DocumentTermMatrix</kbd> function. The rows of the matrix indicate documents and the columns indicate features, that is, words. The matrix is sparse because all unique unigram sets of the dataset become columns in DTM and, as each review comment does not have all elements of the unigram set, most cells will have a 0, indicating the absence of the unigram. </p>
<p class="mce-root">While it is possible to extract n-grams (unigrams, bigrams, trigrams, and so on) as part of the BoW approach, the tokenize parameter can be set and passed as part of the control list in the <kbd class="calibre11">DocumentTermMatrix</kbd> function to accomplish n-grams in DTM. It must be noted that using n-grams as part of the DTM creates a very high number of columns in the DTM. This is one of the demerits of the BoW approach, and, in some cases, it could stall the execution of the project due to limited memory. As our specific case is also limited by hardware infrastructure, we restrict ourselves by including only the unigrams in DTM in this project. Apart from just generating unigrams, we also perform some additional processing on the reviews text document by passing parameters to the control list in the <kbd class="calibre11">tm</kbd> library's <kbd class="calibre11">DocumentTermMatrix</kbd> function. The processing we do on the review text documents during the creation of the DTM is given here:</p>
<ol class="calibre12">
<li class="calibre10">Change the case of the text to lowercase.</li>
<li class="calibre10">Remove any numbers.</li>
<li class="calibre10">Remove stop words using the English language stop word list from the Snowball stemmer project.  Stop words are common words, such as a, an, in, and the, that do not add value in deciding sentiment based on review comments.</li>
<li class="calibre10">Remove punctuation.</li>
<li class="calibre10">Perform stemming, which aims at resolving a word into the base form of the word, that is, strip the plural <em class="calibre22">s</em> from nouns, the <em class="calibre22">ing</em> from verbs, or other affixes. A stem is a natural group of words with equal or very similar meaning. After the stemming process, every word is represented by its stem. The <kbd class="calibre11">SnowballC</kbd> library provides the capability to obtain the root for each of the words in the review comments.</li>
</ol>
<p class="mce-root">Let's now create a DTM from the volatile corpus and do the text preprocessing with the following code block:</p>
<pre class="calibre16"># creating document term matrix<br class="title-page-name"/>dtm_train &lt;- DocumentTermMatrix(train_corp, control = list(<br class="title-page-name"/>  tolower = TRUE,removeNumbers = TRUE,<br class="title-page-name"/>  stopwords = TRUE,<br class="title-page-name"/>  removePunctuation = TRUE,<br class="title-page-name"/>  stemming = TRUE<br class="title-page-name"/>))<br class="title-page-name"/># Basic EDA on dtm<br class="title-page-name"/>inspect(dtm_train)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">&gt; inspect(dtm_train)<br class="title-page-name"/>&lt;&lt;DocumentTermMatrix (documents: 1000, terms: 5794)&gt;&gt;<br class="title-page-name"/>Non-/sparse entries: 34494/5759506<br class="title-page-name"/>Sparsity           : 99%<br class="title-page-name"/>Maximal term length: 21<br class="title-page-name"/>Weighting          : term frequency (tf)<br class="title-page-name"/>Sample             :<br class="title-page-name"/>     Terms<br class="title-page-name"/>Docs  book can get great just like love one read time<br class="title-page-name"/>  111    0   3   2     0    0    0    2   1    0    2<br class="title-page-name"/>  162    4   1   0     0    0    1    0   0    1    0<br class="title-page-name"/>  190    0   0   0     0    0    0    0   0    0    0<br class="title-page-name"/>  230    0   1   1     0    0    0    1   0    0    0<br class="title-page-name"/>  304    0   0   0     0    0    3    0   2    0    0<br class="title-page-name"/>  399    0   0   0     0    0    0    0   0    0    0<br class="title-page-name"/>  431    9   1   0     0    0    1    2   0    0    1<br class="title-page-name"/>  456    1   0   0     0    0    0    0   1    2    0<br class="title-page-name"/>  618    0   2   3     1    4    1    3   1    0    1<br class="title-page-name"/>  72     0   0   1     0    2    0    0   1    0    1</pre>
<p class="mce-root">We see from the output that there are 1,000 documents that were processed and form rows in the matrix. There are 5,794 columns representing unique unigrams from the reviews following the additional text processing. We also see that the DTM is 99% sparse and consists of non-zero entries only in 34,494 cells. The non-zero cells represent the frequency of occurrence of the word on the column in the document represent on the row of the DTM. The weighting is done through the default 'term frequency' weighting, as we did not specify any weighting parameter in the control list supplied to the <kbd class="calibre11">DocumentTermMatrix</kbd> function. Other forms of weighting, such as <strong class="calibre3">term frequency-inverse document frequency</strong> (<strong class="calibre3">TFIDF</strong>), are also possible just by passing the appropriate weight parameter in the control list to the <kbd class="calibre11">DocumentTermMatrix</kbd> function. For now, we will stick to weighting based on term frequency, which is the default. We also see from the <kbd class="calibre11">inspect</kbd> function that several sample documents were <span class="calibre4">output </span>along with the term frequencies in these documents.</p>
<p class="mce-root">The DTM tends to get very big, even for normal sized datasets. Removing sparse terms, that is, terms occurring only in very few documents, is the technique that can be tried to reduce the size of the matrix without losing significant relations inherent to the matrix. Let's remove sparse columns from the matrix. We will attempt to remove those terms that have at least a 99% of sparse elements <span class="calibre4">with the following line of code</span>:</p>
<pre class="calibre16"># Removing sparse terms<br class="title-page-name"/>dtm_train= removeSparseTerms(dtm_train, 0.99)<br class="title-page-name"/>inspect(dtm_train)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">&gt; inspect(dtm_train)<br class="title-page-name"/>&lt;&lt;DocumentTermMatrix (documents: 1000, terms: 686)&gt;&gt;<br class="title-page-name"/>Non-/sparse entries: 23204/662796<br class="title-page-name"/>Sparsity           : 97%<br class="title-page-name"/>Maximal term length: 10<br class="title-page-name"/>Weighting          : term frequency (tf)<br class="title-page-name"/>Sample             :<br class="title-page-name"/>     Terms<br class="title-page-name"/>Docs  book can get great just like love one read time<br class="title-page-name"/>  174    0   0   1     1    1    2    0   2    0    1<br class="title-page-name"/>  304    0   0   0     0    0    3    0   2    0    0<br class="title-page-name"/>  355    3   0   0     0    1    1    2   3    1    0<br class="title-page-name"/>  380    4   1   0     0    1    0    0   1    0    2<br class="title-page-name"/>  465    5   0   1     1    0    0    0   2    6    0<br class="title-page-name"/>  618    0   2   3     1    4    1    3   1    0    1<br class="title-page-name"/>  72     0   0   1     0    2    0    0   1    0    1<br class="title-page-name"/>  836    1   0   0     0    0    3    0   0    5    1<br class="title-page-name"/>  866    8   0   1     0    0    1    0   0    4    0<br class="title-page-name"/>  959    0   0   2     1    1    0    0   2    0    1</pre>
<p class="mce-root">We now see from the output of the <kbd class="calibre11">inspect</kbd> function that the sparsity of the matrix is reduced to 97%, and the number of unigrams (columns of the matrix) is reduced to <kbd class="calibre11">686</kbd>. We are now ready with the DTM that can be used for training with any machine learning classification algorithm. In the next few lines of code, let's attempt to divide our DTM into training and test dataset:</p>
<pre class="calibre16"># splitting the train and test DTM<br class="title-page-name"/>dtm_train_train &lt;- dtm_train[1:800, ]<br class="title-page-name"/>dtm_train_test &lt;- dtm_train[801:1000, ]<br class="title-page-name"/>dtm_train_train_labels &lt;- as.factor(as.character(text[1:800, ]$Sentiment))<br class="title-page-name"/>dtm_train_test_labels &lt;- as.factor(as.character(text[801:1000, ]$Sentiment))</pre>
<p class="mce-root">We will be using a machine learning algorithm called <strong class="calibre3">Naive Bayes</strong> to create a model. Naive Bayes is generally trained on data with nominal features. We can observe that the cells in our DTM are numeric and therefore need to be converted to nominal prior to feeding the dataset as input for creating the model with Naive Bayes. As each cell indicates the word frequency in the review, and as the number of times a word used in the review does not impact sentiment, let's write a function to convert the cell values with a non-zero value to <kbd class="calibre11">Y</kbd>, and in case of a zero, let's convert it to <kbd class="calibre11">N</kbd>, with the following code:</p>
<pre class="calibre16">cellconvert&lt;- function(x) {<br class="title-page-name"/>x &lt;- ifelse(x &gt; 0, "Y", "N")<br class="title-page-name"/>}</pre>
<p class="mce-root">Now, let's apply the function on all rows of the training dataset, and test dataset we have previously created in this project with the following code:</p>
<pre class="calibre16"># applying the function to rows in training and test datasets<br class="title-page-name"/>dtm_train_train &lt;- apply(dtm_train_train, MARGIN = 2,cellconvert)<br class="title-page-name"/>dtm_train_test &lt;- apply(dtm_train_test, MARGIN = 2,cellconvert)<br class="title-page-name"/># inspecting the train dtm to confirm all is in tact<br class="title-page-name"/>View(dtm_train_train)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter34" src="assets/c894b708-f3b8-4a46-9951-4f55652ce045.png"/></p>
<p class="mce-root">We can see from the output that all the cells in the training and test DTMs are now converted to nominal values. Thus, let's proceed to build a text sentiment analysis classifier using the Naive Bayes algorithm from the <kbd class="calibre11">e1071</kbd> library, as follows:</p>
<pre class="calibre16"># training the naive bayes classifier on the training dtm<br class="title-page-name"/>library(e1071)<br class="title-page-name"/>nb_senti_classifier=naiveBayes(dtm_train_train,dtm_train_train_labels)<br class="title-page-name"/># printing the summary of the model created<br class="title-page-name"/>summary(nb_senti_classifier)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">&gt; summary(nb_senti_classifier)<br class="title-page-name"/>        Length Class  Mode    <br class="title-page-name"/>apriori   2    table  numeric <br class="title-page-name"/>tables  686    -none- list    <br class="title-page-name"/>levels    2    -none- character<br class="title-page-name"/>call      3    -none- call  </pre>
<p class="mce-root">The preceding summary output shows that the <kbd class="calibre11">nb_senti_classifier</kbd> object is successfully created from the training DTM. Let's now use the model object to predict sentiment on the test data DTM. In the following code block, we are instructing that the predictions should be classes and not prediction probabilities:</p>
<pre class="calibre16"># making predictions on the test data dtm<br class="title-page-name"/>nb_predicts&lt;-predict(nb_senti_classifier, dtm_train_test,type="class")<br class="title-page-name"/># printing the predictions from the model<br class="title-page-name"/>print(nb_predicts)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">[1] 1 1 2 1 1 1 1 1 1 2 2 1 2 2 2 2 1 2 1 1 2 1 2 1 1 1 2 2 1 2 2 2 2 1 2 1 1 1 1 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 2 1 1 2 2 1 2 2 2 2 1 2 2 1 1 1 1 1 2 1 1 2 1 1 1 1 1 2 2 2 2 2 2 1 2 2 1 2 1 1 1 1 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 1 2 2 1 2 2 1 1 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 1 1 2 1 2 1 2 2 1 1 1 1 2<br class="title-page-name"/>Levels: 1 2</pre>
<p class="mce-root">With the following code, let us now compute the accuracy of the model using the <kbd class="calibre11">mmetric</kbd> function in the <kbd class="calibre11">rminer</kbd> library:</p>
<pre class="calibre16"># computing accuracy of the model<br class="title-page-name"/>library(rminer)<br class="title-page-name"/>print(mmetric(nb_predicts, dtm_train_test_labels, c("ACC")))</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">[1] 79</pre>
<p class="mce-root">We achieved a 79% accuracy just with a very quick and basic BoW model. The model can be further improved by means of techniques such as parameter tuning, lemmatization, new features creation, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pros and cons of the BoW approach</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now that we have an understanding of both the theory and implementation of the BoW approach, let's examine the pros and cons of the approach. When it comes to pros, the BoW approach is very simple to understand and implement and therefore offers a lot of flexibility for customization on any text dataset. It may be observed that the BoW approach does not retain the order of words specifically when only unigrams are considered. This problem is generally overcome by retaining n-grams in the DTM. However, it comes at the cost as larger infrastructure is needed to process the text and build a classifier. Another severe drawback of the approach is that it does not respect the semantics of the word. For example, the words "car" and "automobile" are often used in the same context. A model built based on BoW treats the sentences "buy used cars" and "purchase old automobiles" as very different sentences. While these sentences are the same, BoW models do not classify these sentences as the same, as the words in these sentences are not matching. It is possible to consider the semantics of words in sentences using an approach called word embedding. This is something we will explore in our next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding word embedding</h1>
                </header>
            
            <article>
                
<p class="mce-root">The BoW models that we discussed in our earlier section suffer from a problem that they do not capture information about a word’s meaning or context. This means that potential relationships, such as contextual closeness, are not captured across collections of words. For example, the approach cannot capture simple relationships, such as determining that the words "cars" and "buses" both refer to vehicles that are often discussed in the context of transportation. This problem that we experience with the BoW approach will be overcome by word embedding, which is an improved approach to mapping semantically similar words.</p>
<p class="mce-root">Word vectors represent words as multidimensional continuous floating point numbers, where semantically similar words are mapped to proximate points in geometric space. For example, the words <em class="calibre15">fruit</em> and <em class="calibre15">leaves</em> would have a similar word vector, <em class="calibre15">tree</em>. This is due to the similarity of their meanings, whereas the word <em class="calibre15">television</em> would be quite distant in the geometrical space. In other words, words that are used in a similar context will be mapped to a proximate vector space.</p>
<p class="mce-root">The word vectors can be of <em class="calibre15">n</em> dimensions, and <em class="calibre15">n</em> can take any number as input from the user creating it (for example 10, 70, 500). The dimensions are latent in the sense that it may not be apparent to humans what each of these dimensions means in reality. There are methods such as <strong class="calibre3">Continuous Bag of Words</strong> (<strong class="calibre3">CBOW</strong>) and <strong class="calibre3">Skip-Gram</strong> that enable conceiving the word vectors from the text provided as training input to word embedding algorithms. Also, the individual numbers in the word vector represent the word's distributed weight across dimensions. In a general sense, each dimension represents a latent meaning, and the word's numerical weight on that dimension captures the closeness of its association with and to that meaning. Thus, the semantics of the word are embedded across the dimensions of the vector.</p>
<p class="mce-root">Though the word vectors are multidimensional and cannot be visualized directly, it is possible to visualize the vectors<span class="calibre4"> learned,</span> by projecting them down to two dimensions using techniques such as the t-SNE dimensionality reduction technique. The following diagram displays learned word vectors in two dimensional spaces for country capitals, verb tenses, and gender relationships:</p>
<p class="CDPAlignCenter1"><img class="aligncenter35" src="assets/09d8ca48-57d7-4e72-a2f1-bf3c03b1a68a.png"/></p>
<div class="packtfigref">Visualization of word embeddings in a two dimensional space</div>
<p class="mce-root">When we observe the word embedding visualization, we can perceive that the vectors captured some general, and in fact quite useful, semantic information about words and their relationships to one another. With this, each word in the text now can be represented as a row in the matrix similar to that of the BoW approach, but, unlike the BoW approach, it captures the relationships between the words.</p>
<p class="mce-root">The advantage of representing words as vectors is that they lend themselves to mathematical operators. For example, we can add and subtract vectors. The canonical example here is showing that by using word vectors we can determine the following:</p>
<p class="CDPAlignCenter1"><em class="calibre15">king - man + woman = queen</em></p>
<p class="mce-root">In the given example, we subtracted the gender (man) from the word vector for king and added another gender (woman), and we obtained a new word vector from the operation (<em class="calibre15">king - man + woman</em>) that maps most closely to the word vector for queen.</p>
<p class="mce-root">A few more amazing examples of mathematical operations that can be achieved on word vectors are shown as follows:</p>
<ul class="calibre9">
<li class="calibre10">Given two words, we can establish the degree of similarity between them:</li>
</ul>
<pre class="calibre26">model.similarity('woman','man')</pre>
<p class="calibre29">And the output is as follows:</p>
<pre class="calibre26">0.73723527</pre>
<ul class="calibre9">
<li class="calibre10">Finding the odd one out from the set of words given as input:</li>
</ul>
<pre class="calibre26">model.doesnt_match('breakfast cereal dinner lunch';.split())</pre>
<p class="calibre23">The odd one is given as the following output:</p>
<pre class="calibre26">'cereal'</pre>
<ul class="calibre9">
<li class="calibre10">Derive analogies, for example:</li>
</ul>
<pre class="calibre26">model.most_similar(positive=['woman','king'],negative=['man'],topn=1)</pre>
<p class="calibre23">The output is as follows:</p>
<pre class="calibre26">queen: 0.508</pre>
<p class="mce-root">Now, what it all means for us is that machines are able to identify semantically similar words given in a sentence. The following diagram is a gag related to word embedding that made me laugh, but the gag does convey the power of word embedding application, which otherwise would not be possible with the BoW kind of text representations:</p>
<p class="CDPAlignCenter1"><img class="aligncenter36" src="assets/e972fc21-e370-464a-9b08-22fef9b041bb.png"/></p>
<div class="packtfigref">A gag demonstrating the power of word embeddings application</div>
<p class="mce-root">There are several techniques that can be used to learn word embedding from text data. Word2vec, GloVe, and fastText are some of the popular techniques. Each of these techniques allows us to either train our own word embedding from the text data we have, or use the readily available pretrained vectors.</p>
<p class="mce-root">This approach of learning our own word embedding requires a lot of training data and can be slow, but this option will learn an embedding both targeted to the specific text data and the NLP task at hand.</p>
<p class="mce-root">Pretrained word embedding vectors are vectors that are trained on large amounts of text data (usually billions of words) available on sources such as Wikipedia. These are generally high-quality word embedding vectors made available by companies such as Google or Facebook. We can download these pretrained vector files and consume them to obtain word vectors for the words in the text that we would like to classify or cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a text sentiment classifier with pretrained word2vec word embedding based on Reuters news corpus</h1>
                </header>
            
            <article>
                
<p class="mce-root">Word2vec was developed by Tomas Mikolov, et al. at Google in 2013 as a response to making the neural-network-based training of the embedding more efficient, and since then it has become the de facto standard for developing pretrained word embedding.</p>
<p class="mce-root">Word2vec introduced the following two different learning models to learn the word embedding:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">CBOW</strong>: Learns the embedding by predicting the current word based on its context.</li>
<li class="calibre10"><strong class="calibre1">Continuous Skip-Gram</strong>: The continuous Skip-Gram model learns by predicting the surrounding words given a current word.</li>
</ul>
<p class="mce-root">Both CBOW and Skip-Gram methods of learning are focused on learning the words given their local usage context, where the context of the word itself is defined by a window of neighboring words. This window is a configurable parameter of the model.</p>
<p class="mce-root">The <kbd class="calibre11">softmaxreg</kbd> library in R offers pretrained <kbd class="calibre11">word2vec</kbd> word embedding that can be used for building our sentiment analysis engine for the Amazon reviews data. The pretrained vector is built using the <kbd class="calibre11">word2vec</kbd> model, and it is based on the <kbd class="calibre11">Reuter_50_50</kbd> dataset, UCI Machine Learning Repository (<a href="https://archive.ics.uci.edu/ml/datasets/Reuter_50_50" class="calibre8">https://archive.ics.uci.edu/ml/datasets/Reuter_50_50</a>).</p>
<p class="mce-root">Without any delay, let's get into the code and also review the approach followed in this code:</p>
<pre class="calibre16"># including the required library<br class="title-page-name"/>library(softmaxreg)<br class="title-page-name"/># importing the word2vec pretrained vector into memory<br class="title-page-name"/>data(word2vec)</pre>
<p class="mce-root">Let's examine the <kbd class="calibre11">word2vec</kbd> pretrained emdeddings. It is just another data frame, and therefore can be reviewed through the regular <kbd class="calibre11">dim</kbd> and <kbd class="calibre11">View</kbd> commands as follows:</p>
<pre class="calibre16">View(word2vec)</pre>
<p class="calibre23"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter37" src="assets/7a78e718-5842-47c2-ab0d-09a046f7ebe5.png"/></p>
<p class="mce-root">Here, let's use the following <kbd class="calibre11">dim</kbd> command:</p>
<pre class="calibre16">dim(word2vec)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">[1] 12853 21</pre>
<p class="mce-root">From the preceding output, we can observe that there are <kbd class="calibre11">12853</kbd> words that have got word vectors in the pretrained vector. Each of the words is defined using 20 dimensions, and these dimensions define the context of the words. In the next step, we can look up the word vector for each of the words in the review comments. As there are only 12,853 words in the pretrained word embedding, there is a possibility that we encounter a word that does not exist in the pretrained embedding. In such a case, the unidentified word is represented with a 20 dimension vector that is filled with zeros.</p>
<p class="mce-root">We also need to understand that the word vectors are available only at a word level, and therefore in order to decode the entire review, we take the mean of all the word vectors of the words that made up the review. Let's review the concept of getting the word vector for a sentence from individual word vectors with an example.</p>
<p class="mce-root">Assume the sentence we want to get the word vector for is, <em class="calibre15">it is very bright and sunny this morning</em>. Individual words that comprise the sentence are <em class="calibre15">it</em>, <em class="calibre15">is</em>, <em class="calibre15">very</em>, <em class="calibre15">bright</em>, <em class="calibre15">and</em>, <em class="calibre15">sunny</em>, <em class="calibre15">this</em>, and <em class="calibre15">morning</em>.</p>
<p class="mce-root">Now, we can look up each of these words in the pretrained vector and get the corresponding word vectors as shown in the following table:</p>
<table border="1" class="calibre17">
<tbody class="calibre18">
<tr class="calibre19">
<td class="calibre30">
<p class="CDPAlignCenter1"><strong class="calibre3">Word</strong></p>
</td>
<td class="calibre31">
<p class="CDPAlignCenter1"><strong class="calibre3">dim1</strong></p>
</td>
<td class="calibre32">
<p class="CDPAlignCenter1"><strong class="calibre3">dim2</strong></p>
</td>
<td class="calibre32">
<p class="CDPAlignCenter1"><strong class="calibre3">dim3</strong></p>
</td>
<td class="calibre31">
<p class="mce-root"><strong class="calibre3">.....</strong></p>
</td>
<td class="calibre31">
<p class="mce-root"><strong class="calibre3">....</strong></p>
</td>
<td class="calibre33">
<p class="CDPAlignCenter1"><strong class="calibre3">dim19</strong></p>
</td>
<td class="calibre34">
<p class="CDPAlignCenter1"><strong class="calibre3">dim20</strong></p>
</td>
<td class="calibre35"/>
</tr>
<tr class="calibre19">
<td class="calibre30">
<p class="mce-root"><kbd class="calibre11">it</kbd></p>
</td>
<td class="calibre31">
<p class="mce-root">-2.25</p>
</td>
<td class="calibre32">
<p class="mce-root">0.75</p>
</td>
<td class="calibre32">
<p class="mce-root">1.75</p>
</td>
<td class="calibre31">
<p class="mce-root">-1.25</p>
</td>
<td class="calibre31">
<p class="mce-root">-0.25</p>
</td>
<td class="calibre33">
<p class="mce-root">-3.25</p>
</td>
<td class="calibre34">
<p class="mce-root">-2.25</p>
</td>
<td class="calibre35"/>
</tr>
<tr class="calibre19">
<td class="calibre30">
<p class="mce-root"><kbd class="calibre11">is</kbd></p>
</td>
<td class="calibre31">
<p class="mce-root">0.75</p>
</td>
<td class="calibre32">
<p class="mce-root">1.75</p>
</td>
<td class="calibre32">
<p class="mce-root">1.75</p>
</td>
<td class="calibre31">
<p class="mce-root">-2.25</p>
</td>
<td class="calibre31">
<p class="mce-root">-2.25</p>
</td>
<td class="calibre33">
<p class="mce-root">0.75</p>
</td>
<td class="calibre34">
<p class="mce-root">-0.25</p>
</td>
<td class="calibre35"/>
</tr>
<tr class="calibre19">
<td class="calibre30">
<p class="mce-root"><kbd class="calibre11">very</kbd></p>
</td>
<td class="calibre31">
<p class="mce-root">-2.25</p>
</td>
<td class="calibre32">
<p class="mce-root">2.75</p>
</td>
<td class="calibre32">
<p class="mce-root">1.75</p>
</td>
<td class="calibre31">
<p class="mce-root">-0.25</p>
</td>
<td class="calibre31">
<p class="mce-root">0.75</p>
</td>
<td class="calibre33">
<p class="mce-root">0.75</p>
</td>
<td class="calibre34">
<p class="mce-root">-2.25</p>
</td>
<td class="calibre35"/>
</tr>
<tr class="calibre19">
<td class="calibre30">
<p class="mce-root"><kbd class="calibre11">bright</kbd></p>
</td>
<td class="calibre31">
<p class="mce-root">-3.25</p>
</td>
<td class="calibre32">
<p class="mce-root">-3.25</p>
</td>
<td class="calibre32">
<p class="mce-root">-2.25</p>
</td>
<td class="calibre31">
<p class="mce-root">-1.25</p>
</td>
<td class="calibre31">
<p class="mce-root">0.75</p>
</td>
<td class="calibre33">
<p class="mce-root">1.75</p>
</td>
<td class="calibre34">
<p class="mce-root">-0.25</p>
</td>
<td class="calibre35"/>
</tr>
<tr class="calibre19">
<td class="calibre30">
<p class="mce-root"><kbd class="calibre11">and</kbd></p>
</td>
<td class="calibre31">
<p class="mce-root">-0.25</p>
</td>
<td class="calibre32">
<p class="mce-root">-1.25</p>
</td>
<td class="calibre32">
<p class="mce-root">-2.25</p>
</td>
<td class="calibre31">
<p class="mce-root">2.75</p>
</td>
<td class="calibre31">
<p class="mce-root">-3.25</p>
</td>
<td class="calibre33">
<p class="mce-root">-0.25</p>
</td>
<td class="calibre34">
<p class="mce-root">1.75</p>
</td>
<td class="calibre35"/>
</tr>
<tr class="calibre19">
<td class="calibre30">
<p class="mce-root"><kbd class="calibre11">sunny</kbd></p>
</td>
<td class="calibre31">
<p class="mce-root">0</p>
</td>
<td class="calibre32">
<p class="mce-root">0</p>
</td>
<td class="calibre32">
<p class="mce-root">0</p>
</td>
<td class="calibre31">
<p class="mce-root">0</p>
</td>
<td class="calibre31">
<p class="mce-root">0</p>
</td>
<td class="calibre33">
<p class="mce-root">0</p>
</td>
<td class="calibre34">
<p class="mce-root">0</p>
</td>
<td class="calibre35"/>
</tr>
<tr class="calibre19">
<td class="calibre30">
<p class="mce-root"><kbd class="calibre11">this</kbd></p>
</td>
<td class="calibre31">
<p class="mce-root">-2.25</p>
</td>
<td class="calibre32">
<p class="mce-root">-3.25</p>
</td>
<td class="calibre32">
<p class="mce-root">2.75</p>
</td>
<td class="calibre31">
<p class="mce-root">0.75</p>
</td>
<td class="calibre31">
<p class="mce-root">-0.25</p>
</td>
<td class="calibre33">
<p class="mce-root">-0.25</p>
</td>
<td class="calibre34">
<p class="mce-root">-0.25</p>
</td>
<td class="calibre35"/>
</tr>
<tr class="calibre19">
<td class="calibre30">
<p class="mce-root"><kbd class="calibre11">morning</kbd></p>
</td>
<td class="calibre31">
<p class="mce-root">-0.25</p>
</td>
<td class="calibre32">
<p class="mce-root">-3.25</p>
</td>
<td class="calibre32">
<p class="mce-root">-2.25</p>
</td>
<td class="calibre31">
<p class="mce-root">1.75</p>
</td>
<td class="calibre31">
<p class="mce-root">0.75</p>
</td>
<td class="calibre33">
<p class="mce-root">2.75</p>
</td>
<td class="calibre34">
<p class="mce-root">2.75</p>
</td>
<td class="calibre35"/>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">Now, we have word vectors that comprise the sentence. Please note that these are not actual word vector values but just are made up to demonstrate the approach. Also, observe that the word <kbd class="calibre11">sunny</kbd> is represented with zeros across the dimensions to symbolize that the word is not found in the pretrained word embedding. In order to get the word vector for the sentence, we just compute the mean of each dimension. The resulting vector is a 1 x 20 vector representing the sentence, as follows:</p>
<table border="1" class="calibre17">
<tbody class="calibre18">
<tr class="calibre19">
<td class="calibre36">
<p class="mce-root">Sentence</p>
</td>
<td class="calibre36">
<p class="mce-root">-1.21875</p>
</td>
<td class="calibre36">
<p class="mce-root">-0.71875</p>
</td>
<td class="calibre36">
<p class="mce-root">0.15625</p>
</td>
<td class="calibre36">
<p class="mce-root">0.03125</p>
</td>
<td class="calibre36">
<p class="mce-root">-0.46875</p>
</td>
<td class="calibre36">
<p class="mce-root">0.28125</p>
</td>
<td class="calibre36">
<p class="mce-root">-0.09375</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root">The <kbd class="calibre11">softmaxreg</kbd> library offers the <kbd class="calibre11">wordEmbed</kbd> function where we could pass a sentence and ask it to compute the <kbd class="calibre11">mean</kbd> word vector for the sentence. The following code is a custom function that was created to apply the <kbd class="calibre11">wordEmbed</kbd> function on each of the Amazon reviews we have in hand. At the end of applying this function to the reviews dataset, we expect to have a <em class="calibre15">n</em> x 20 matrix that is the word vector representation of our reviews. The <em class="calibre15">n</em> in the <em class="calibre15">n</em> x 20 represents the number of rows and 20 is the number of dimensions through which each review is represented, as seen in the following code:</p>
<pre class="calibre16"># function to get word vector for each review<br class="title-page-name"/>docVectors = function(x)<br class="title-page-name"/>{<br class="title-page-name"/>  wordEmbed(x, word2vec, meanVec = TRUE)<br class="title-page-name"/>}<br class="title-page-name"/># setting the working directory and reading the reviews dataset<br class="title-page-name"/>setwd('/home/sunil/Desktop/sentiment_analysis/')<br class="title-page-name"/>text = read.csv(file='Sentiment Analysis Dataset.csv', header = TRUE)<br class="title-page-name"/># applying the docVector function on each of the reviews<br class="title-page-name"/># storing the matrix of word vectors as temp<br class="title-page-name"/>temp=t(sapply(text$SentimentText, docVectors))<br class="title-page-name"/># visualizing the word vectors output<br class="title-page-name"/>View(temp)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter38" src="assets/af33d08a-c790-4859-ae29-5e9f48ab5d29.png"/></p>
<p class="mce-root">Then we review <kbd class="calibre11">temp</kbd> using the <kbd class="calibre11">dim</kbd> command, as follows:</p>
<pre class="calibre16">dim(temp)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">1000 20</pre>
<p class="mce-root">We can see from the output that we have word vectors created for each of the reviews in our corpus. This data frame can now be used to build classification models using an ML algorithm. The following code to achieve classification is no different from the one we did for the BoW approach:</p>
<pre class="calibre16"># splitting the dataset into train and test<br class="title-page-name"/>temp_train=temp[1:800,]<br class="title-page-name"/>temp_test=temp[801:1000,]<br class="title-page-name"/>labels_train=as.factor(as.character(text[1:800,]$Sentiment))<br class="title-page-name"/>labels_test=as.factor(as.character(text[801:1000,]$Sentiment))<br class="title-page-name"/># including the random forest library<br class="title-page-name"/>library(randomForest)<br class="title-page-name"/># training a model using random forest classifier with training dataset<br class="title-page-name"/># observe that we are using 20 trees to create the model<br class="title-page-name"/>rf_senti_classifier=randomForest(temp_train, labels_train,ntree=20)<br class="title-page-name"/>print(rf_senti_classifier)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">randomForest(x = temp_train, y = labels_train, ntree = 20)<br class="title-page-name"/>               Type of random forest: classification<br class="title-page-name"/>                     Number of trees: 20<br class="title-page-name"/>No. of variables tried at each split: 4<br class="title-page-name"/>        OOB estimate of  error rate: 44.25%<br class="title-page-name"/>Confusion matrix:<br class="title-page-name"/>    1   2 class.error<br class="title-page-name"/>1 238 172   0.4195122<br class="title-page-name"/>2 182 208   0.4666667</pre>
<p class="mce-root">The preceding output shows that the Random Forest model object is successfully created. Of course, the model can be improved further; however we are not going to be doing that here as the focus is to demonstrate making use of word embeddings, rather than getting the best performing classifier.</p>
<p class="mce-root">Next, with the following code we make use of the Random Forest model to make predictions on the test data and then report out the performance:</p>
<pre class="calibre16"># making predictions on the dataset<br class="title-page-name"/>rf_predicts&lt;-predict(rf_senti_classifier, temp_test)<br class="title-page-name"/>library(rminer)<br class="title-page-name"/>print(mmetric(rf_predicts, labels_test, c("ACC")))</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">[1] 62.5</pre>
<p class="mce-root">We see that we get a 62% accuracy from using the pretrained <kbd class="calibre11">word2vec</kbd> embeddings made out of the Reuters news group's dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a text sentiment classifier with GloVe word embedding</h1>
                </header>
            
            <article>
                
<p class="mce-root">Stanford University's Pennington, et al. developed an extension of the <kbd class="calibre11">word2vec</kbd> method that is called <strong class="calibre3"><span class="calibre4">Global Vectors for Word Representation</span></strong> (<strong class="calibre3">GloVe</strong>) for efficiently learning word vectors. </p>
<p class="mce-root">GloVe combines the global statistics of matrix factorization techniques, such as LSA, with the local context-based learning in <kbd class="calibre11">word2vec</kbd>. Also, unlike <kbd class="calibre11">word2vec</kbd>, rather than using a window to define local context, GloVe constructs an explicit word context or word co-occurrence matrix using statistics across the whole text corpus. As an effect, the learning model yields generally better word embeddings.</p>
<p class="mce-root">The <kbd class="calibre11">text2vec</kbd> library in R has a GloVe implementation that we could use to train to obtain word embeddings from our own training corpus. Alternatively, pretrained GloVe word embeddings can be downloaded and reused, similar to the way we did in the earlier <kbd class="calibre11">word2vec</kbd> pretrained embedding project covered in the previous section.</p>
<p class="mce-root">The following code block demonstrates the way in which GloVe word embeddings can be created and used for sentiment analysis, or, for that matter, any text classification task. We are not going to discuss explicitly the steps involved, since the code is already heavily commented with detailed explanations of each of the steps:</p>
<pre class="calibre16"># including the required library<br class="title-page-name"/>library(text2vec)<br class="title-page-name"/># setting the working directory<br class="title-page-name"/>setwd('/home/sunil/Desktop/sentiment_analysis/')<br class="title-page-name"/># reading the dataset<br class="title-page-name"/>text = read.csv(file='Sentiment Analysis Dataset.csv', header = TRUE)<br class="title-page-name"/># subsetting only the review text so as to create Glove word embedding<br class="title-page-name"/>wiki = as.character(text$SentimentText)<br class="title-page-name"/># Create iterator over tokens<br class="title-page-name"/>tokens = space_tokenizer(wiki)<br class="title-page-name"/># Create vocabulary. Terms will be unigrams (simple words).<br class="title-page-name"/>it = itoken(tokens, progressbar = FALSE)<br class="title-page-name"/>vocab = create_vocabulary(it)<br class="title-page-name"/># consider a term in the vocabulary if and only if the term has appeared aleast three times in the dataset<br class="title-page-name"/>vocab = prune_vocabulary(vocab, term_count_min = 3L)<br class="title-page-name"/># Use the filtered vocabulary<br class="title-page-name"/>vectorizer = vocab_vectorizer(vocab)<br class="title-page-name"/># use window of 5 for context words and create a term co-occurance matrix<br class="title-page-name"/>tcm = create_tcm(it, vectorizer, skip_grams_window = 5L)<br class="title-page-name"/># create the glove embedding for each each in the vocab and<br class="title-page-name"/># the dimension of the word embedding should set to 50<br class="title-page-name"/># x_max is the maximum number of co-occurrences to use in the weighting<br class="title-page-name"/># function<br class="title-page-name"/># note that training the word embedding is time consuming - be patient<br class="title-page-name"/>glove = GlobalVectors$new(word_vectors_size = 50, vocabulary = vocab, x_max = 100)<br class="title-page-name"/>wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">INFO [2018-10-30 06:58:14] 2018-10-30 06:58:14 - epoch 1, expected cost 0.0231<br class="title-page-name"/>INFO [2018-10-30 06:58:15] 2018-10-30 06:58:15 - epoch 2, expected cost 0.0139<br class="title-page-name"/>INFO [2018-10-30 06:58:15] 2018-10-30 06:58:15 - epoch 3, expected cost 0.0114<br class="title-page-name"/>INFO [2018-10-30 06:58:15] 2018-10-30 06:58:15 - epoch 4, expected cost 0.0100<br class="title-page-name"/>INFO [2018-10-30 06:58:15] 2018-10-30 06:58:15 - epoch 5, expected cost 0.0091<br class="title-page-name"/>INFO [2018-10-30 06:58:15] 2018-10-30 06:58:15 - epoch 6, expected cost 0.0084<br class="title-page-name"/>INFO [2018-10-30 06:58:16] 2018-10-30 06:58:16 - epoch 7, expected cost 0.0079<br class="title-page-name"/>INFO [2018-10-30 06:58:16] 2018-10-30 06:58:16 - epoch 8, expected cost 0.0074<br class="title-page-name"/>INFO [2018-10-30 06:58:16] 2018-10-30 06:58:16 - epoch 9, expected cost 0.0071<br class="title-page-name"/>INFO [2018-10-30 06:58:16] 2018-10-30 06:58:16 - epoch 10, expected cost 0.0068</pre>
<p class="mce-root">The following uses the <kbd class="calibre11">glove</kbd> model to obtain the combined word vector:</p>
<pre class="calibre16"># Glove model learns two sets of word vectors - main and context.<br class="title-page-name"/># both matrices may be added to get the combined word vector<br class="title-page-name"/>wv_context = glove$components<br class="title-page-name"/>word_vectors = wv_main + t(wv_context)<br class="title-page-name"/># converting the word_vector to a dataframe for visualization<br class="title-page-name"/>word_vectors=data.frame(word_vectors)<br class="title-page-name"/># the word for each embedding is set as row name by default<br class="title-page-name"/># using the tibble library rownames_to_column function, the rownames is copied as first column of the dataframe<br class="title-page-name"/># we also name the first column of the dataframe as words<br class="title-page-name"/>library(tibble)<br class="title-page-name"/>word_vectors=rownames_to_column(word_vectors, var = "words")<br class="title-page-name"/>View(word_vectors)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter39" src="assets/644b36df-c202-4d9a-9433-2579473dd24a.png"/></p>
<p class="mce-root">We make use of the <kbd class="calibre11">softmaxreg</kbd> library to obtain the mean word vector for each review. This is similar to what we did in <kbd class="calibre11">word2vec</kbd> pretrained embedding in the previous section. Observe that we are passing our own trained word embedding <kbd class="calibre11">word_vectors</kbd> to the <kbd class="calibre11">wordEmbed()</kbd> function, as follows:</p>
<pre class="calibre16">library(softmaxreg)<br class="title-page-name"/>docVectors = function(x)<br class="title-page-name"/>{<br class="title-page-name"/>  wordEmbed(x, word_vectors, meanVec = TRUE)<br class="title-page-name"/>}<br class="title-page-name"/># applying the function docVectors function on the entire reviews dataset<br class="title-page-name"/># this will result in word embedding representation of the entire reviews # dataset<br class="title-page-name"/>temp=t(sapply(text$SentimentText, docVectors))<br class="title-page-name"/>View(temp)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter40" src="assets/c48d11a0-df92-4dfa-8ee4-910afd3931bf.png"/></p>
<p class="mce-root">We will now split the dataset into train and test portions, and use the <kbd class="calibre11">randomforest</kbd> library to build a model to train, as shown in the following lines of code:</p>
<pre class="calibre16"># splitting the dataset into train and test portions<br class="title-page-name"/>temp_train=temp[1:800,]<br class="title-page-name"/>temp_test=temp[801:1000,]<br class="title-page-name"/>labels_train=as.factor(as.character(text[1:800,]$Sentiment))<br class="title-page-name"/>labels_test=as.factor(as.character(text[801:1000,]$Sentiment))<br class="title-page-name"/># using randomforest to build a model on train data<br class="title-page-name"/>library(randomForest)<br class="title-page-name"/>rf_senti_classifier=randomForest(temp_train, labels_train,ntree=20)<br class="title-page-name"/>print(rf_senti_classifier)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">Call:<br class="title-page-name"/> randomForest(x = temp_train, y = labels_train, ntree = 20)<br class="title-page-name"/>               Type of random forest: classification<br class="title-page-name"/>                     Number of trees: 20<br class="title-page-name"/>No. of variables tried at each split: 7<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>        OOB estimate of  error rate: 42.12%<br class="title-page-name"/>Confusion matrix:<br class="title-page-name"/>    1   2 class.error<br class="title-page-name"/>1 250 160   0.3902439<br class="title-page-name"/>2 177 213   0.4538462</pre>
<p class="mce-root">Then, we use the Random Forest model created to predict labels, as follows:</p>
<pre class="calibre16"># predicting labels using the randomforest model created<br class="title-page-name"/>rf_predicts&lt;-predict(rf_senti_classifier, temp_test)<br class="title-page-name"/># estimating the accuracy from the predictions<br class="title-page-name"/>library(rminer)<br class="title-page-name"/>print(mmetric(rf_predicts, labels_test, c("ACC")))</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">[1] 66.5</pre>
<p class="mce-root">With this method, we obtain an accuracy of 66%. This is despite the fact that the word embeddings are obtained from words in just 1,000 text samples. The model may be further improved by using a pretrained embedding. The overall framework to use the pretrained embedding remains the same as what we did in <kbd class="calibre11">word2vec</kbd> project in the previous section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a text sentiment classifier with fastText</h1>
                </header>
            
            <article>
                
<p class="mce-root"><kbd class="calibre11">fastText</kbd> is a library and is an extension of <kbd class="calibre11">word2vec</kbd> for word representation. It was created by the Facebook Research Team in 2016. While Word2vec and GloVe approaches treat words as the smallest unit to train on, fastText breaks words into several n-grams, that is, subwords. For example, the trigrams for the word apple are app, ppl, and ple. The word embedding for the word apple is sum of all the word n-grams. Due to the nature of the algorithm's embedding generation, fastText is more resource-intensive and takes additional time to train. Some of the advantages of <kbd class="calibre11">fastText</kbd> are as follows:</p>
<ul class="calibre9">
<li class="calibre10">It generates better word embeddings for rare words (including misspelled words).</li>
<li class="calibre10">For out of vocabulary words, fastText can construct the vector for a word from its character n-grams, even if a word doesn't appear in training corpus. This is not a possibility for both Word2vec and GloVe.</li>
</ul>
<p class="mce-root">The <kbd class="calibre11">fastTextR</kbd> library provides an interface to the fastText. Let's make use of the <kbd class="calibre11">fastTextR</kbd> library for our project to build a sentiment analysis engine on Amazon reviews. While it is possible to download pretrained fastText word embedding and make use of it for our project, let's make an attempt to train a word embedding based on the reviews dataset we have in hand. It should be noted that the approach in terms of making use of fastText pretrained word embedding is similar to the approach we followed in the <kbd class="calibre11">word2vec</kbd> based project that we dealt with earlier. </p>
<p class="mce-root">Similar to the project covered in the previous section, comments are included inline in the code. The comments explain each of the lines indicating the approach taken to build the Amazon reviews sentiment analyzer in this project. Let's look into the following code now:</p>
<pre class="calibre16"># loading the required libary<br class="title-page-name"/>library(fastTextR)<br class="title-page-name"/># setting the working directory<br class="title-page-name"/>setwd('/home/sunil/Desktop/sentiment_analysis/')<br class="title-page-name"/># reading the input reviews file<br class="title-page-name"/># recollect that fastText needs the file in a specific format and we created one compatiable file in<br class="title-page-name"/># "Understanding the Amazon Reviews Dataset" section of this chaptertext = readLines("Sentiment Analysis Dataset_ft.txt")<br class="title-page-name"/># Viewing the text vector for conformation<br class="title-page-name"/>View(text)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter41" src="assets/26daddc5-2f6b-4dbf-b95a-0b13ec3705a7.png"/></p>
<p class="mce-root">Now let's divide the reviews into training and test datasets, and view them using the following lines of code:</p>
<pre class="calibre16"># dividing the reviews into training and test<br class="title-page-name"/>temp_train=text[1:800]temp_test=text[801:1000]<br class="title-page-name"/># Viewing the train datasets for confirmation<br class="title-page-name"/>View(temp_train)</pre>
<p class="mce-root">This will give the following output:</p>
<p class="CDPAlignCenter1"><img src="assets/e27d1e8e-4ef3-48da-853a-807bf9c58c63.png" class="calibre37"/></p>
<p class="CDPAlignLeft1">Use the following code to view the test dataset:</p>
<pre class="CDPAlignLeft2">View(temp_test)</pre>
<p class="mce-root">This will give the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter42" src="assets/eed25069-abad-4348-bac7-0b4b5b15f278.png"/></p>
<p class="mce-root">We will now create a <kbd class="calibre11">.txt</kbd> file for the train and test dataset using the following code:</p>
<pre class="calibre16"># creating txt file for train and test dataset<br class="title-page-name"/># the fasttext function expects files to be passed for training and testing<br class="title-page-name"/>fileConn&lt;-file("/home/sunil/Desktop/sentiment_analysis/train.ft.txt")<br class="title-page-name"/>writeLines(temp_train, fileConn)<br class="title-page-name"/>close(fileConn)<br class="title-page-name"/>fileConn&lt;-file("/home/sunil/Desktop/sentiment_analysis/test.ft.txt")<br class="title-page-name"/>writeLines(temp_test, fileConn)<br class="title-page-name"/>close(fileConn)<br class="title-page-name"/># creating a test file with no labels<br class="title-page-name"/># recollect the original test dataset has labels in it<br class="title-page-name"/># as the dataset is just a subset obtained from full dataset<br class="title-page-name"/>temp_test_nolabel&lt;- gsub("__label__1", "", temp_test, perl=TRUE)<br class="title-page-name"/>temp_test_nolabel&lt;- gsub("__label__2", "", temp_test_nolabel, perl=TRUE)</pre>
<p class="mce-root">Now we will view the no labels test dataset for confirmation using the following command:</p>
<pre class="calibre16">View(temp_test_nolabel)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter43" src="assets/b70533eb-7e36-4f83-a262-00279dcf2f66.png"/></p>
<p class="mce-root">Let's now write the no labels test dataset to a file so we can use it for testing, as follows:</p>
<pre class="calibre16">fileConn&lt;-file("/home/sunil/Desktop/sentiment_analysis/test_nolabel.ft.txt")<br class="title-page-name"/>writeLines(temp_test_nolabel, fileConn)<br class="title-page-name"/>close(fileConn)<br class="title-page-name"/># training a supervised classification model with training dataset file<br class="title-page-name"/>model&lt;-fasttext("/home/sunil/Desktop/sentiment_analysis/train.ft.txt",<br class="title-page-name"/>method = "supervised", control = ft.control(nthreads = 3L))<br class="title-page-name"/># Obtain all the words from a previously trained model=<br class="title-page-name"/>words&lt;-get_words(model)<br class="title-page-name"/># viewing the words for confirmation. These are the set of words present  # in our training data<br class="title-page-name"/>View(words)</pre>
<p class="mce-root">This will result in the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter44" src="assets/fa10b4e0-e95f-4b37-b32f-191a4f03d45f.png"/></p>
<p class="mce-root">Now we will obtain the word vectors from a previously trained model and view the word vectors for each word in our training dataset, as follows:</p>
<pre class="calibre16"># Obtain word vectors from a previously trained model.<br class="title-page-name"/>word_vec&lt;-get_word_vectors(model, words)<br class="title-page-name"/># Viewing the word vectors for each word in our training dataset<br class="title-page-name"/># observe that the word embedding dimension is 5<br class="title-page-name"/>View(word_vec)</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter45" src="assets/7a2104e3-a082-4b1b-b99b-d0b9c33536ed.png"/></p>
<p class="mce-root">We will predict the labels for the reviews in the no labels test dataset and write it to a file for future reference. Then we will get the predictions into a data frame to compute the performance and see the estimate of the accuracy using the following lines of code:</p>
<pre class="calibre16"># predicting the labels for the reviews in the no labels test dataset<br class="title-page-name"/># and writing it to a file for future reference<br class="title-page-name"/>predict(model, newdata_file= "/home/sunil/Desktop/sentiment_analysis/test_nolabel.ft.txt",result_file="/home/sunil/Desktop/sentiment_analysis/fasttext_result.txt")<br class="title-page-name"/># getting the predictions into a dataframe so as to compute performance   # measurementft_preds&lt;-predict(model, newdata_file= "/home/sunil/Desktop/sentiment_analysis/test_nolabel.ft.txt")<br class="title-page-name"/># reading the test file to extract the actual labels<br class="title-page-name"/>reviewstestfile&lt;<br class="title-page-name"/>readLines("/home/sunil/Desktop/sentiment_analysis/test.ft.txt")<br class="title-page-name"/># extracting just the labels frm each line<br class="title-page-name"/>library(stringi)<br class="title-page-name"/>actlabels&lt;-stri_extract_first(reviewstestfile, regex="\\w+")<br class="title-page-name"/># converting the actual labels and predicted labels into factors<br class="title-page-name"/>actlabels&lt;-as.factor(as.character(actlabels))<br class="title-page-name"/>ft_preds&lt;-as.factor(as.character(ft_preds))<br class="title-page-name"/># getting the estimate of the accuracy<br class="title-page-name"/>library(rminer)<br class="title-page-name"/>print(mmetric(actlabels, ft_preds, c("ACC")))</pre>
<p class="mce-root"><span class="calibre4">This will result in the following output:</span></p>
<pre class="calibre16">[1] 58</pre>
<p class="mce-root">We have a 58% accuracy with the <kbd class="calibre11">fastText</kbd> method on our reviews data. As a next step, we could check whether the accuracy may be further improved by making use of fastText pretrained word embedding. As we already know, implementing a project by making use of pretrained embedding is not very different from the implementation that we followed in the <kbd class="calibre11">word2vec</kbd> project described in the earlier section of this chapter. The difference is just that the training step to obtain word embedding needs to be discarded and the model variable in the code covered in this project code should be initiated with the pretrained word embeddings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we learned various NLP techniques, namely BoW, Word2vec, GloVe, and fastText. We built projects involving these techniques to perform sentiment analysis on an Amazon reviews dataset. The projects that were built involved two approaches, making use of pretrained word embeddings and building the word embeddings from our own dataset. We tried both these approaches to represent text in a format that can be consumed by ML algorithms that resulted in models with the ability to perform sentiment analysis.</p>
<p class="mce-root">In the next chapter, we will learn about customer segmentation by making use of a wholesale dataset. We will look at customer segmentation as an unsupervised problem and build projects with various techniques that can identify inherent groups within the e-commerce company's customer base. Come, let's explore the world of building an e-commerce customer segmentation engine with ML!</p>


            </article>

            
        </section>
    </body></html>