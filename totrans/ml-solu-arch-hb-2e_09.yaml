- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing an Enterprise ML Architecture with AWS ML Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many organizations opt to build enterprise ML platforms to support numerous
    fast-moving initiatives. These platforms are designed to facilitate the entire
    ML lifecycle and accommodate various usage patterns, all while emphasizing automation
    and scalability. As a practitioner, I often get asked to provide architectural
    guidance for creating such enterprise ML platforms. In this chapter, we will explore
    the fundamental requirements for designing enterprise ML platforms. We will cover
    a range of topics, such as workflow automation, infrastructure scalability, and
    system monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the discussion, you will gain insights into architecture patterns
    that enable the development of technology solutions to automate the end-to-end
    ML workflow and ensure seamless deployment at a large scale. Additionally, we
    will delve deep into essential components of enterprise ML architecture, such
    as model training, model hosting, the feature store, and the model registry, all
    tailored to meet the demands of enterprise-level operations.
  prefs: []
  type: TYPE_NORMAL
- en: AI risk, governance, and security are other important considerations for enterprise
    ML platforms, and we will cover them in greater detail in *Chapters 12* and *13*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Key considerations for ML platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key requirements for an enterprise ML platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enterprise ML architecture pattern overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adopting MLOps for an ML workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices in building and operating ML platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will continue to use the AWS environment for the hands-on portion of this
    chapter. All the source code mentioned in this chapter can be found at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter09](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: Key considerations for ML platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing, building, and operating ML platforms are complex endeavors as there
    are many different considerations, including the personas, key ML process workflows,
    and various technical capability requirements for the different personas and workflows.
    In this section, we will delve into each of these key considerations in depth.
    Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: The personas of ML platforms and their requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous chapter, we talked about building a data science environment
    for the data scientists and ML engineers who mainly focus on experimentation and
    model development. In an enterprise setting where an ML platform is needed, there
    are other personas involved, each with their own specific requirements. At a high
    level, there are two types of personas associated with the ML platform: ML platform
    builders and ML platform users.'
  prefs: []
  type: TYPE_NORMAL
- en: ML platform builders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ML platform builders have the crucial responsibility of constructing the infrastructure
    for data and ML platforms. Here are some essential builder types required to build
    a cloud-based ML platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud infrastructure architect/engineer**: These experts design the overall
    cloud infrastructure, selecting appropriate cloud services and setting up the
    foundation for the ML platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security engineer**: Security engineers ensure that the ML platform adheres
    to industry-standard security practices, safeguarding sensitive data and protecting
    against potential threats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML platform product manager**: ML platform product managers are responsible
    for understanding functional user requirements and non-functional requirements,
    defining ML platform capabilities and the implementation roadmap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML platform engineers**: ML platform engineers are responsible for designing,
    building, and maintaining the infrastructure and systems that support the end-to-end
    ML lifecycle within an organization. ML platform engineers play a crucial role
    in ensuring that data scientists and ML practitioners can efficiently develop,
    deploy, and manage ML models on the organization’s ML platform. They are responsible
    for the platform design, covering key functional areas such as training and hosting,
    taking into account scalability, performance, security, and integration with existing
    systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data engineers**: Data engineers are responsible for building data pipelines,
    data storage solutions, and data processing frameworks to ensure seamless data
    access and processing for ML tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML platform tester**: ML platform testers are responsible for testing the
    core capabilities of platforms to meet the desired functional and non-functional
    requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform users and operators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Platform users and operators are the actual users of an ML platform. They use
    the ML platform to perform full lifecycle ML tasks from data exploration to model
    monitoring. The following are some of the key platform user and operator types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data scientists/ML engineers**: Data scientists and ML engineers are the
    primary users of an ML platform. They use the platform to explore data, build
    and train ML models, perform feature engineering, and evaluate model performance.
    They partner with ML platform engineers and ops engineers to integrate trained
    models into production systems, optimize model inference performance, and ensure
    the models are scalable and reliable in real-world environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model tester and validator**: The primary responsibilities for a model tester
    involve assessing the performance and reliability of ML models developed by data
    scientists using the ML platform. Specifically, model testers are responsible
    for model testing using different datasets, model performance metrics calculation
    and evaluation, overfitting/underfitting detection, and testing edge cases. Model
    validators are responsible for validating models against business objectives,
    risk assessment, and other issues such as ethics considerations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model approver**: This individual or team is responsible for reviewing and
    approving the deployment of ML models into production or other critical environments.
    The model approver’s primary role is to ensure that the developed ML models meet
    the organization’s standards, business requirements, and compliance policies before
    they are deployed. They also help ensure all the required testing, post-deployment
    operations, and policies are in place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operations and support engineers**: This role ensures the smooth operation,
    maintenance, and ongoing support of the ML platform within an organization. Their
    responsibilities encompass various technical and operational aspects to keep the
    ML platform running efficiently and to provide assistance to users. Some of the
    key functions include platform maintenance and upgrades, performance monitoring
    and optimization, incident management, infrastructure management, security and
    access control, and platform documentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI risk/governance manager**: The primary responsibility of an AI risk/governance
    manager is to manage and mitigate the potential risks associated with the use
    of AI/ML systems. Their role is essential in ensuring that AI technologies are
    developed, deployed, and used responsibly, ethically, and in compliance with relevant
    regulations. They help ensure appropriate processes, policies, and technology
    standards are created and adhered to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might wonder where the ML solutions architect fits into this overall picture.
    The ML solutions architect plays a pivotal role that spans builders, users, and
    operators. They serve as a bridge between these groups, offering valuable insights
    and guidance. Firstly, ML solutions architect collaborate with builders, understanding
    user requirements, and assisting in the end-to-end architecture design. They ensure
    that the ML platform aligns with the specific needs of users and operators. Secondly,
    ML solutions architects advise users and operators on effectively utilizing the
    ML platform. They educate them on best practices for configuring and leveraging
    the platform to meet diverse needs and use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Common workflow of an ML initiative
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Different organizations have diverse workflows and governance processes when
    running ML initiatives. However, most of these workflows typically consist of
    the following key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting and processing data from different sources and making it available
    for data scientists.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing data exploratory analysis, forming hypotheses, creating ML features,
    performing experimentation, and building different ML models using different techniques
    and ML algorithms using a subset of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data labeling workflow is sometimes needed to label training data for supervised
    ML tasks such as document classification or object detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducting full model training and tuning using a full dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Candidate models trained using the full dataset are promoted into a testing
    environment for formal quality assurance. Testers document testing details for
    all the testers and verify if the models meet desired performance metrics and
    other evaluation criteria, such as latency and scalability. Model validators evaluate
    the ML techniques, perform an analysis of the model, and check the alignment with
    the business outcome.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model risk assessment is performed to ensure risk items are assessed, mitigated,
    or accepted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the model passes the testing and validation step, the model is sent to
    the model approver for final review and approval for production deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is deployed into production with approval. The model is registered
    in the model registry, the dataset is versioned and retained, any code artifacts
    are also versioned and stored, and detailed training configuration details are
    also documented.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is monitored in production for model performance, data drift, system
    issues, and security exposure and attacks. The incident management process is
    followed to address issues identified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At required schedules, auditors perform end-to-end audits to ensure all processes
    and policies are followed, artifacts are stored, access to systems and models
    is properly logged, documentation meets the required standards, and any violations
    are flagged and escalated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is worth noting that these steps are not exhaustive. Depending on the organizational,
    risk, and regulatory requirements, organizations can carry out more steps to address
    these requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Platform requirements for the different personas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML platforms involve a diverse array of potential players and users. The following
    table outlines the essential needs for ML platforms for both users and operators
    of the platform. Note that the table does not include the builder of the platform.
  prefs: []
  type: TYPE_NORMAL
- en: '| **User/operator** | **Tools/capability requirements** |'
  prefs: []
  type: TYPE_TB
- en: '| Data scientist | Access to various ML libraries, tools, and frameworks for
    model development and experimentationAccess to different datasets to perform different
    ML tasksCapabilities to perform data exploration and model training using different
    hardwareWorkflow automation including data retrieval and processing, feature engineering,
    experimentation, model building, and model versioning for reproducibility |'
  prefs: []
  type: TYPE_TB
- en: '| Model tester and validator | Access to different test datasets for model
    testing and validationAccess to various libraries and tools for data visualization,
    model evaluation, an ML testing framework, bias detection tools, model interpretability
    tools, and statistical testing tools |'
  prefs: []
  type: TYPE_TB
- en: '| Model approvers | Access to model documentation, model evaluation metrics,
    a compliance checklist, and a model explainability reportAccess to an approval
    workflow management tool |'
  prefs: []
  type: TYPE_TB
- en: '| Ops and support engineers | Access to all infrastructure components within
    the ML platform, including code and container repositories, library packages,
    training, hosting, pipelines, logging, monitoring and alerting, security and access
    control, backup and discovery, performance testing, and incident management toolsAccess
    to tools for platform automation and management |'
  prefs: []
  type: TYPE_TB
- en: '| AI risk officer | Access to an AI risk assessment tool, a governance platform,
    model explainability and interpretability tools, bias detection and fairness assessment
    tools, AI risk reporting and dashboards, and AI regulation and policy monitoring
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9.1: ML platform requirements by personas'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the success of an ML platform relies heavily on meeting the distinct
    tools and capability requirements of its users/operators. By addressing these
    distinct needs, the ML platform can effectively support its users and operators
    in building, deploying, and managing AI solutions with confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Key requirements for an enterprise ML platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To deliver business benefits through ML at scale, organizations must have the
    capability to rapidly experiment with diverse scientific approaches, ML technologies,
    and extensive datasets. Once ML models are trained and validated, they need to
    seamlessly transition to production deployment. While some similarities exist
    between a traditional enterprise software system and an ML platform, such as scalability
    and security concerns, an enterprise ML platform presents distinctive challenges.
    These include the need to integrate with the data platform and high-performance
    computing infrastructure to facilitate large-scale model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s delve into some specific core requirements of an enterprise ML platform
    to meet the needs of different users and operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Support for the end-to-end ML lifecycle**: An enterprise ML platform must
    cater to both data science experimentation and production-grade operations and
    deployments. In *Chapter 8*, *Building a Data Science Environment Using AWS ML
    Services,* we explored the essential architecture components required to construct
    a data science experimentation environment using AWS ML services. However, to
    facilitate seamless production-grade operations and deployment, the enterprise
    ML platform should also include specific architecture components dedicated to
    large-scale model training, model management, feature management, and highly available
    and scalable model hosting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support for continuous integration (CI), continuous training (CT), and continuous
    deployment (CD)**: In addition to testing and validating code and components,
    an enterprise ML platform extends its CI capabilities to include data and models.
    The CD capability for ML goes beyond merely deploying a single software piece;
    it involves managing both ML models and inference engines in conjunction. CT is
    a unique aspect of ML, wherein a model is continuously monitored, and automated
    model retraining can be triggered upon detecting data drift, model drift, or changes
    in the training data. Data drift refers to a change in the data wherein the statistical
    characteristics of the production data differ from the data used for model training.
    On the other hand, model drift signifies a decline in model performance compared
    to the performance achieved during the model training phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operations support**: An enterprise ML platform should provide capabilities
    to monitor the statuses, errors, and metrics of different pipeline workflows,
    processing/training jobs, model behavior changes, data drift, and model-serving
    engines. Additionally, infrastructure-level statistics and resource usage are
    continuously monitored to ensure efficient operations. An automated alert mechanism
    is a crucial component of operations, promptly notifying relevant stakeholders
    of any issues or anomalies. Moreover, implementing automated failure recovery
    mechanisms wherever possible further enhances the platform’s robustness and minimizes
    downtime, ensuring smooth and reliable ML operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support for different languages and ML frameworks**: An enterprise ML platform
    empowers data scientists and ML engineers to use their preferred programming languages
    and ML libraries. It should accommodate popular languages like Python and R, along
    with well-known ML frameworks such as TensorFlow, PyTorch, and scikit-learn. This
    flexibility ensures that teams can leverage their expertise and utilize the most
    suitable tools for efficient and effective model development within the platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computing hardware resource management**: An enterprise ML platform should
    cater to diverse model training and inference requirements, taking into account
    cost considerations. This entails providing support for various types of computing
    hardware, such as CPUs and GPUs, to optimize performance and cost-effectiveness.
    Furthermore, the platform should be equipped to handle specialized ML hardware,
    like AWS’s Inferentia and Tranium chips, wherever relevant, to leverage the benefits
    of specialized hardware accelerators for specific ML workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with other third-party systems and software**: An enterprise
    ML platform rarely operates in isolation. It must offer robust integration capabilities
    with various third-party software and platforms, including workflow orchestration
    tools, container registries, and code repositories. This seamless integration
    enables smooth collaboration and interoperability, allowing teams to leverage
    existing tools and workflows while benefiting from the advanced features and capabilities
    of the ML platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authentication and authorization**: For an enterprise ML platform, ensuring
    secure access to data, artifacts, and ML platform resources is essential. This
    requires offering various levels of authentication and authorization control.
    The platform may include built-in authentication and authorization capabilities,
    or it can integrate with an external authentication and authorization service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data encryption**: In regulated industries like financial services and healthcare,
    data encryption is a critical requirement. An enterprise ML platform must offer
    robust capabilities for encrypting data both at rest and in transit, often allowing
    customers to manage their encryption keys. This level of data protection ensures
    that sensitive information remains secure and compliant with industry regulations,
    providing the necessary reassurance for handling confidential data within these
    sectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifact management**: In the ML lifecycle, an enterprise ML platform handles
    datasets and generates various artifacts at different stages. These artifacts
    can be features, code, models, and containers. To ensure reproducibility and adhere
    to governance and compliance standards, the platform must possess the capability
    to track, manage, and version-control these artifacts. By effectively managing
    and recording the changes made throughout the ML process, the platform maintains
    a clear and organized record, facilitating the reproducibility of results and
    providing a reliable audit trail for compliance purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML library package management**: Standardizing and approving ML library packages
    used by data scientists is crucial for many organizations. By establishing a central
    library that contains pre-approved packages, it becomes possible to enforce consistent
    standards and policies across the usage of library packages. This approach ensures
    that data scientists work with vetted and authorized libraries, promoting reliability,
    security, and adherence to organizational guidelines when developing ML solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access to different data stores**: An essential feature of an enterprise
    ML platform is to offer seamless access to various data stores, simplifying model
    development and training processes. This accessibility to diverse data sources
    streamlines the workflow for data scientists and ML engineers, enabling them to
    efficiently access and utilize the necessary data for their tasks within the platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-service capability**: To enhance operational efficiency and reduce reliance
    on central teams, an enterprise ML platform should incorporate self-service capabilities
    for tasks like user onboarding, environment setup, and pipeline provisioning.
    By enabling users to perform these tasks independently, the platform streamlines
    operations, empowering data scientists and ML engineers to work more autonomously
    and efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model testing and validation**: An enterprise ML platform should provide
    comprehensive model testing and validation features to support thorough assessments
    of ML models. This can include features such as A/B testing infrastructure, model
    robustness testing packages, automated testing pipelines, performance metrics
    tracking and error analysis tools, and visualization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having covered the essential requirements of an enterprise ML platform, let’s
    now explore how AWS ML and DevOps services, such as SageMaker, CodePipeline, and
    Step Functions, can be effectively utilized to construct a robust, enterprise-grade
    ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise ML architecture pattern overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building an enterprise ML platform on AWS starts with creating different environments
    to enable different data science and operation functions. The following diagram
    shows the core environments that normally make up an enterprise ML platform. From
    an isolation perspective, in the context of the AWS cloud, each environment in
    the following diagram is a separate AWS account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Enterprise ML architecture environments ](img/B20836_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Enterprise ML architecture environments'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed in *Chapter 8*, *Building a Data Science Environment Using
    AWS ML Services*, data scientists utilize the data science environment for experimentation,
    model building, and tuning. Once these experiments are completed, the data scientists
    commit their work to the proper code and data repositories. The next step is to
    train and tune the ML models in a controlled and automated environment using the
    algorithms, data, and training scripts that were created by the data scientists.
    This controlled and automated model training process will help ensure consistency,
    reproducibility, and traceability for scalable model building. The following are
    the core functionalities and technology options provided by the training, hosting,
    and shared services environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model training environment**: This environment manages the full lifecycle
    of model training, from computing and storage infrastructure resource provisioning
    to training job monitoring and model persistence. For this purpose, the SageMaker
    training service offers a suitable technology option to construct the training
    infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model hosting environment**: This environment is used to serve the trained
    models behind web service endpoints or in batch inference mode. For this purpose,
    you can use the SageMaker hosting service for this environment. Other supporting
    services such as the online feature store and API management service can also
    run in the model hosting environment. There can be multiple model hosting environments
    for different stages. For example, you can have a testing hosting environment
    designated for model testing, and a production hosting environment for production
    model deployment serving real-world traffic. Model testers can perform different
    tests, such as model performance, robustness, bias, explainability analysis, and
    model hosting testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shared services environment**: The shared services environment hosts common
    services, tooling such as workflow orchestration tools, CI/CD tools, code repositories,
    Docker image repositories, and private library package tools. A central model
    registry can also run in the shared services environment for model registration
    and model lifecycle management. Service provisioning capabilities, such as creating
    resources in different environments through **Infrastructure as Code** (**IaC**)
    or APIs, also run out of this environment. Any service ticketing tools, such as
    ServiceNow, and service provisioning tools, such as Service Catalog, can also
    be hosted in this environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to the core ML environments, there are other supporting environments,
    such as security, governance, monitoring, and logging, that are required for designing
    and building enterprise ML platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security and governance environment**: The security and governance environment
    centrally manages authentication services, user credentials, and data encryption
    keys. Security audit and reporting processes also run in this environment. Native
    AWS services, such as Amazon IAM, AWS KMS, and AWS Config, can be used for various
    security and governance functions. Any custom-built risk and governance tools
    can also be hosted in this environment to service AI risk/governance manager.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and logging environment**: The monitoring and logging environment
    centrally aggregates monitoring and logging data from other environments for further
    processing and reporting. Custom dashboarding and alerting mechanisms are normally
    developed to provide easy access to key metrics and alerts from the underlying
    monitoring and logging data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you have a comprehensive overview of the fundamental elements that
    constitute an enterprise ML platform, let’s delve deeper into specific core areas.
    It is important to recognize that there are various patterns and services available
    for constructing an ML platform on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, while *Figure 9.1* showcases distinct AWS environments (i.e., AWS
    accounts) to host different ML platform environments, organizations can also choose
    to combine some of the environments in a single AWS account as long as there are
    proper boundaries between different environments to ensure the isolation of infrastructure,
    process flow, and security control. Furthermore, organizations can create separate
    AWS accounts for hosting a specific environment for different users or groups.
    For instance, a large enterprise can choose to create one data science environment
    for each of the LoBs, or a separate production hosting environment based on either
    organizational structure or workload separation. In this chapter, we will focus
    mainly on exploring one of the enterprise patterns to build an efficient and scalable
    ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Model training environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Within an enterprise, a model training environment is a controlled environment
    with well-defined processes and policies on how it is used and who can use it.
    Normally, it should be an automated environment that’s managed by an ML operations
    team, though self-service can be enabled for direct usage by data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: Automated model training and tuning are the core capabilities of the model training
    environment. To support a broad range of use cases, a model training environment
    needs to support different ML and deep learning frameworks, training patterns
    (single-node and distributed training), and hardware (different CPUs, GPUs, and
    custom silicon chips).
  prefs: []
  type: TYPE_NORMAL
- en: The model training environment manages the lifecycle of the model training process.
    This can include authentication and authorization, infrastructure provisioning,
    data movement, data preprocessing, ML library deployment, training loop management
    and monitoring, model persistence and registry, training job management, and lineage
    tracking. From a security perspective, the training environment needs to provide
    security capabilities for different isolation requirements, such as network isolation,
    job isolation, and artifact isolation. To assist with operational support, a model
    training environment also needs to be able to support training status logging,
    metrics reporting, and training job monitoring and alerting. In the following
    sections, we will discuss how Amazon SageMaker can be used as a managed model
    training engine for enterprises.
  prefs: []
  type: TYPE_NORMAL
- en: Model training engine using SageMaker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The SageMaker training service provides built-in model training capabilities
    for a range of ML/DL libraries. In addition, you can bring your own Docker containers
    for customized model training needs. The following are a subset of supported options
    for the SageMaker Python SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training TensorFlow models**: SageMaker provides a built-in training container
    for TensorFlow models. The following code sample shows how to train a TensorFlow
    model using the built-in container through the TensorFlow estimator API:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Training PyTorch models**: SageMaker provides a built-in training container
    for PyTorch models. The following code sample shows how to train a PyTorch model
    using the PyTorch estimator:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Training XGBoost models**: XGBoost training is also supported via a built-in
    container. The following code shows the syntax for training an XGBoost model using
    the XGBoost estimator:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Training scikit-learn models**: The following code sample shows how to train
    a scikit-learn model using the built-in container:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Training models using custom containers**: You can also build a custom training
    container and use the SageMaker training service for model training. See the following
    code for an example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In addition to using the SageMaker Python SDK to kick off training, you can
    also use the `boto3` library and SageMaker CLI commands to start training jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Automation support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SageMaker training service is exposed through a set of APIs and can be automated
    by integrating with external applications or workflow tools, such as SageMaker
    Pipelines, Airflow, and AWS Step Functions. For example, it can be one of the
    steps in an Airflow-based pipeline for an end-to-end ML workflow. Some workflow
    tools, such as Airflow and AWS Step Functions, also provide SageMaker-specific
    connectors to interact with the SageMaker training service more seamlessly. The
    SageMaker training service also provides Kubernetes operators, so it can be integrated
    and automated as part of the Kubernetes application flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following sample code shows how to kick off a training job using the low-level
    API via the AWS `boto3` SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Regarding using Airflow as the workflow tool, the following sample shows how
    to use the Airflow SageMaker operator as part of the workflow definition. Here,
    `train_config` contains training configuration details, such as the training estimator,
    training instance type and number, and training data location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: SageMaker also has a built-in workflow automation tool called **SageMaker Pipelines**.
    A training step can be created using the SageMaker **Training Step** API and integrated
    into the larger SageMaker Pipelines workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Model training lifecycle management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SageMaker training manages the lifecycle of the model training process. It uses
    Amazon IAM as the mechanism to authenticate and authorize access to its functions.
    Once authorized, it provides the desired infrastructure, deploys the software
    stacks for the different model training requirements, moves the data from sources
    to training nodes, and kicks off the training job. Once the training job has been
    completed, the model artifacts are saved into an S3 output bucket and the infrastructure
    is torn down. For lineage tracing, model training metadata such as source datasets,
    model training containers, hyperparameters, and model output locations are captured.
    Any logging from the training job runs is saved in CloudWatch Logs, and system
    metrics such as CPU and GPU utilization are captured in the CloudWatch metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the overall end-to-end ML platform architecture, a model training
    environment can also host services for data preprocessing, model validation, and
    model training postprocessing, as those are important steps in an end-to-end ML
    flow. There are multiple technology options available for this, such as the SageMaker
    Processing service, AWS Glue, and AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: Model hosting environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An enterprise-grade model hosting environment needs to support a broad range
    of ML frameworks in a secure, performant, and scalable way. It should come with
    a list of pre-built inference engines that can serve common models out of the
    box behind a **RESTful API** or via the **gRPC protocol**. It also needs to provide
    flexibility to host custom-built inference engines for unique requirements. Users
    should also have access to different hardware devices, such as CPU, GPU, and purpose-built
    chips, for different inference needs.
  prefs: []
  type: TYPE_NORMAL
- en: Some model inference patterns demand more complex inference graphs, such as
    traffic split, request transformations, or model ensemble support. A model hosting
    environment can provide this capability as an out-of-the-box feature or provide
    technology options for building custom inference graphs. Other common model hosting
    capabilities include **concept drift detection** and **model performance drift
    detection**. Concept drift occurs when the statistical characteristics of the
    production data deviate from the data that’s used for model training. An example
    of concept drift is the mean and standard deviation of a feature changing significantly
    in production from that of the training dataset. Model performance drift happens
    when the accuracy of the model degrades in production.
  prefs: []
  type: TYPE_NORMAL
- en: Components in a model hosting environment can participate in an automation workflow
    through its API, scripting, or IaC deployment (such as AWS CloudFormation). For
    example, a RESTful endpoint can be deployed using a CloudFormation template or
    by invoking its API as part of an automated workflow.
  prefs: []
  type: TYPE_NORMAL
- en: From a security perspective, the model hosting environment needs to provide
    authentication and authorization control to manage access to both the **control
    plane** (management functions) and the **data plane** (model endpoints). The accesses
    and operations that are performed against the hosting environments should be logged
    for auditing purposes. For operations support, a hosting environment needs to
    enable status logging and system monitoring to support system observability and
    problem troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: The SageMaker hosting service is a fully managed model hosting service. Similar
    to KFServing and Seldon Core, which we reviewed earlier in this book, the SageMaker
    hosting service is also a multi-framework model-serving service. Next, let’s take
    a closer look at its various capabilities for enterprise-grade model hosting.
  prefs: []
  type: TYPE_NORMAL
- en: Inference engines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SageMaker provides built-in inference engines for multiple ML frameworks, including
    scikit-learn, XGBoost, TensorFlow, PyTorch, and Spark ML. SageMaker supplies these
    built-in inference engines as Docker containers. To stand up an API endpoint to
    serve a model, you just need to provide the model artifacts and infrastructure
    configuration. The following is a list of model-serving options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Serving TensorFlow models**: SageMaker uses TensorFlow Serving as the inference
    engine for TensorFlow models. The following code sample shows how to deploy a
    TensorFlow Serving model using the SageMaker hosting service where a TensorFlow
    model is loaded using the `Model` class from the S3 location and deployed as a
    SageMaker endpoint using the `deploy()` function with the specified compute instance
    type and number:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Serving PyTorch models**: SageMaker hosting uses TorchServe under the hood
    to serve PyTorch models. The following code sample shows how to deploy a PyTorch
    model, which is very similar to the code for deploying TensorFlow models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Serving Spark ML models**: For Spark ML-based models, SageMaker uses MLeap
    as the backend to serve Spark ML models. These Spark ML models need to be serialized
    into MLeap format so they can be used by the MLeap engine. The following code
    sample shows how to deploy a Spark ML model using the SageMaker hosting service
    where the `SparkMLModel` class is used to specify the model configuration and
    the `deploy()` function is used for the actual deployment into a SageMaker endpoint:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Serving XGBoost models**: SageMaker provides an XGBoost model server for
    serving trained XGBoost models. Under the hood, it uses Nginx, Gunicorn, and Flask
    as part of the model-serving architecture. The entry Python script loads the trained
    XGBoost model and can optionally perform pre- and post-data processing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Serving scikit-learn models**: SageMaker provides a built-in serving container
    for serving scikit-learn-based models. The technology stack is similar to the
    one for the XGBoost model server, which is also based on Nginx, Gunicorn, and
    Flask:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Serving models with custom containers**: For custom-created inference containers,
    you can follow a similar syntax to deploy the model. The main difference is that
    a custom inference container image’s uri must be provided to specify the custom
    container location. You can find detailed documentation on building a custom inference
    container at [https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: SageMaker hosting provides an inference pipeline feature that allows you to
    create a linear sequence of containers to perform custom data processing before
    and after invoking a model for predictions. SageMaker hosting can support traffic
    splits between multiple versions of a model for A/B testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker hosting can be provisioned using an AWS CloudFormation template.
    There is also support for the AWS CLI for scripting automation, and it can be
    integrated into custom applications via its API. The following are some code samples
    for different endpoint deployment automation methods:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a CloudFormation code sample for SageMaker endpoint deployment.
    In this code sample, you specify the compute instance, the number of instances,
    the model name, and the model-serving container used to host the model. You can
    find the complete code at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter09/sagemaker_hosting.yaml](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter09/sagemaker_hosting.yaml):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is an AWS CLI sample for SageMaker endpoint deployment, which
    consists of three main steps: creating a model, specifying a SageMaker endpoint
    configuration, and the actual deployment of the model into a SageMaker endpoint:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If the built-in inference engines do not meet your requirements, you should
    consider bringing your own Docker container to serve your ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication and security control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SageMaker hosting service uses AWS IAM as the mechanism to control access
    to its control plane APIs (for example, an API for creating an endpoint) and data
    plane APIs (for example, an API for invoking a hosted model endpoint). If you
    need to support other authentication methods for the data plane API, such as **OpenID
    Connect** (**OIDC**), you can implement a proxy service as the frontend to manage
    user authentication. A common pattern is to use AWS API Gateway to frontend the
    SageMaker API for custom authentication management, as well as other API management
    features such as metering and throttling management.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and logging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SageMaker provides out-of-the-box monitoring and logging capabilities to assist
    with support operations. It monitors both system resource metrics (for example,
    CPU/GPU utilization) and model invocation metrics (for example, the number of
    invocations, model latencies, and failures). These monitoring metrics and any
    model processing logs are captured by AWS CloudWatch metrics and CloudWatch Logs.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the training, inference, security, and monitoring aspects
    of an ML platform, we will dive into implementing MLOps to automate ML workflows
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Adopting MLOps for ML workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to the DevOps practice, which has been widely adopted for the traditional
    software development and deployment process, the MLOps practice is intended to
    streamline the building and deployment processes of ML pipelines while enhancing
    the collaborations between data scientists/ML engineers, data engineering, and
    the operations team. Specifically, the primary objective of MLOps practice is
    to yield the following main benefits throughout the entire ML lifecycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Process consistency**: The MLOps practice aims to create consistency in the
    ML model-building and deployment process. A consistent process improves the efficiency
    of the ML workflow and ensures a high degree of certainty in the input and output
    of the ML workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tooling and process reusability**: One of the core objectives of the MLOps
    practice is to create reusable technology tooling and templates for faster adoption
    and deployment of new ML use cases. These can include common tools such as code
    and library repositories, package and image building tools, pipeline orchestration
    tools, the model registry, as well as common infrastructure for model training
    and model deployment. From a reusable template perspective, these can include
    common reusable scripts for Docker image builds, workflow orchestration definitions,
    and CloudFormation scripts for model building and model deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model-building reproducibility**: ML is highly iterative and can involve
    a large number of experimentations and model training runs using different datasets,
    algorithms, and hyperparameters. An MLOps process needs to capture all the data
    inputs, source code, and artifacts that are used to build an ML model and establish
    model lineage from this input data, code, and artifacts for the final models.
    This is important for both experiment tracking as well as governance and control
    purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Delivery scalability**: An MLOps process and the associated tooling enable
    a large number of ML pipelines to run in parallel for high delivery throughputs.
    Different ML project teams can use the standard MLOps processes and common tools
    independently without creating conflicts from a resource contention, environment
    isolation, and governance perspective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Process and operation auditability**: MLOps enables greater audibility in
    the process and the auditability of ML pipelines. This includes capturing the
    details of machine pipeline executions, dependencies, and lineage across different
    steps, job execution statuses, model training and deployment details, approval
    tracking, and actions that are performed by human operators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we are familiar with the intended goals and benefits of the MLOps practice,
    let’s delve into the specific operational process and concrete technology architecture
    of MLOps on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Components of the MLOps architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most important MLOps concepts is the automation pipeline, which executes
    a sequence of tasks, such as data processing, model training, and model deployment.
    This pipeline can be a linear sequence of steps or a more complex **directed acyclic
    graph** (**DAG**) with parallel execution for multiple tasks. The following diagram
    illustrates a sample DAG for an ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of steps to a model  Description automatically generated](img/B20836_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Sample ML pipeline flow'
  prefs: []
  type: TYPE_NORMAL
- en: 'An MLOps architecture also has several repositories for storing different assets
    and metadata as part of pipeline executions. The following diagram lists the core
    components and tasks involved in an MLOps operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – MLOps components ](img/B20836_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: MLOps components'
  prefs: []
  type: TYPE_NORMAL
- en: A **code repository** is an MLOps architecture component that not only serves
    as a source code control mechanism for data scientists and engineers – it can
    also be the triggering mechanism to kick off different pipeline executions. For
    example, when a data scientist checks an updated training script into the code
    repository, a model training pipeline execution can be triggered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **feature repository** stores reusable ML features and can be the target of
    a data processing/feature engineering job. The features from the feature repository
    can be a part of the training datasets where applicable. The feature repository
    is also used as a part of the model inference request.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **container repository** stores the container images that are used for data
    processing tasks, model training jobs, and model inference engines. It is usually
    the target of the container-building pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **model registry** keeps an inventory of trained models, along with all the
    metadata associated with the model, such as its algorithm, hyperparameters, model
    metrics, and training dataset location. It also maintains the status of the model
    lifecycle, such as its deployment approval status.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **pipeline repository** maintains the definition of automation pipelines and
    the statuses of different pipeline job executions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In an enterprise setting, a task ticket also needs to be created when different
    tasks, such as model deployment, are performed, so that these actions can be tracked
    in a common enterprise ticketing management system. To support audit requirements,
    the lineage of different pipeline tasks and their associated artifacts need to
    be tracked.
  prefs: []
  type: TYPE_NORMAL
- en: Another critical component of the MLOps architecture is **monitoring**. In general,
    you want to monitor items such as the pipeline’s execution status, model training
    status, and model endpoint status. Model endpoint monitoring can also include
    system/resource performance monitoring, model statistical metrics monitoring,
    drift and outlier monitoring, and model explainability monitoring. Alerts can
    be triggered on certain execution statuses to invoke human or automation actions
    that are needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS provides multiple technology options for implementing an MLOps architecture.
    The following diagram shows where these technology services fit in an enterprise
    MLOps architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – MLOps architecture using AWS services  ](img/B20836_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: MLOps architecture using AWS services'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, the shared service environment hosts common tools for
    pipeline management and execution, as well as common repositories such as code
    repositories and model registries.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use AWS CodePipeline to orchestrate the overall CI/CD pipeline. AWS
    CodePipeline is a continuous delivery service. We use this service here as it
    integrates natively with different code repositories such as AWS CodeCommit, GitHub
    repos, and Bitbucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can source files from the code repository and make them available to downstream
    tasks such as building containers using the AWS CodeBuild service, or training
    models in the model training environment. You can create different pipelines to
    meet different needs. A pipeline can be triggered on-demand via an API or the
    CodePipeline management console, or it can be triggered by code changes in a code
    repository. Depending on your requirements, you can create different pipelines.
    In the preceding diagram, we can see four example pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: A container build pipeline for building different container images for training,
    processing, and inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model training pipeline for training a model for release
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model deployment pipeline for deploying trained models to production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A development, training, and testing pipeline for model training and deployment
    testing in a data science environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that while *Figure 9.4* only showcases four distinct pipelines, in reality,
    organizations can have many more pipelines based on specific requirements. Moreover,
    they can run multiple instances of the same pipeline in parallel to accommodate
    various ML projects. For instance, different instances of training job pipelines
    may run independently and concurrently, each dedicated to training distinct ML
    models using different datasets and configurations.
  prefs: []
  type: TYPE_NORMAL
- en: A code repository is one of the most essential components in an MLOps environment.
    It is not only used by data scientists/ML engineers and other engineers to persist
    code artifacts, but it also serves as a triggering mechanism for a CI/CD pipeline.
    This means that when a data scientist/ML engineer commits a code change, it can
    automatically kick off a CI/CD pipeline. For example, if the data scientist makes
    a change to the model training script and wants to test the automated training
    pipeline in the development environment, they can commit the code to a development
    branch to kick off a model training pipeline in the dev environment. When it is
    ready for production release deployment, the data scientist can commit/merge the
    code to a release branch to kick off the production release pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, in the MLOps architecture in *Figure 9.4*, we use:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon **Elastic Container Registry** (**ECR**) as the central container registry
    service. ECR is used to store containers for data processing, model training,
    and model inference. You can tag the container images to indicate different lifecycle
    statuses, such as development or production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SageMaker Model Registry** as the central model repository. The central model
    repository can reside in the shared service environment, so it can be accessed
    by different projects. All the models that go through the formal training and
    deployment cycles should be managed and tracked in the central model repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SageMaker Feature Store** provides a common feature repository for reusable
    features to be used by different projects. It can reside in the shared services
    environment or be part of the data platform. Features are normally pre-calculated
    in a data management environment and sent to SageMaker Feature Store for offline
    model training in the model training environment, as well as online inferences
    by the different model hosting environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ML platform presents some unique challenges in terms of monitoring. In addition
    to monitoring common software system-related metrics and statuses, such as infrastructure
    utilization and processing status, an ML platform also needs to monitor model,
    and data-specific metrics and performances. Also, unlike traditional system-level
    monitoring, which is fairly straightforward to understand, the opaqueness of ML
    models makes it inherently difficult to understand the system. Now, let’s take
    a closer look at the three main areas of monitoring for an ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Model training monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Model training monitoring provides visibility into the training progress and
    helps identify training bottlenecks and error conditions during the training process.
    It enables operational processes such as training job progress reporting and response,
    model training performance progress evaluation and response, training problem
    troubleshooting, and data and model bias detection and model interpretability
    and response. Specifically, we want to monitor the following key metrics and conditions
    during model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '**General system and resource utilization and error metrics**: These provide
    visibility into how the infrastructure resources (such as CPU, GPU, disk I/O,
    and memory) are utilized for model training. These can help with making decisions
    on provisioning infrastructure for the different model training needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training job events and status**: This provides visibility into the progress
    of a training job, such as a job starting and running, its completion, and failure
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training metrics**: These are model training metrics such as the loss
    curve and accuracy reports to help you understand the model’s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias detection metrics and model explainability reporting**: These metrics
    help you understand if there is any bias in the training datasets or ML models.
    Model explainability can also be monitored and reported to help you understand
    high-importance features versus low-importance features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training bottlenecks and training issues**: These provide visibility
    into training issues such as vanishing gradients, poor weight initialization,
    and overfitting to help determine the required data and algorithmic and training
    configuration changes. Metrics such as CPU and I/O bottlenecks, uneven load balancing,
    and low GPU utilization can help determine infrastructure configuration changes
    for more efficient model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are multiple native AWS services for building out a model monitoring
    architecture on AWS. The following diagram shows an example architecture for building
    a monitoring solution for a SageMaker-based model training environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Model training monitoring architecture  ](img/B20836_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: Model training monitoring architecture'
  prefs: []
  type: TYPE_NORMAL
- en: This architecture lets you monitor training and system metrics and perform log
    capture and processing, training event capture and processing, and model training
    bias and explainability reporting. It helps enable operation processes, such as
    training progress and status reporting, model metric evaluation, system resource
    utilization reporting and response, training problem troubleshooting, bias detection,
    and model decision explainability.
  prefs: []
  type: TYPE_NORMAL
- en: During model training, SageMaker can emit model training metrics, such as training
    loss and accuracy, to AWS CloudWatch to help with model training evaluation. AWS
    CloudWatch is the AWS monitoring and observability service. It collects metrics
    and logs from other AWS services and provides dashboards for visualizing and analyzing
    these metrics and logs. System utilization metrics (such as CPU/GPU/memory utilization)
    are also reported to CloudWatch for analysis to help you understand any infrastructure
    constraints or under-utilization. CloudWatch alarms can be created for a single
    metric or composite metrics to automate notifications or responses. For example,
    you can create alarms on low CPU/GPU utilization to help proactively identify
    sub-optimal hardware configuration for the training job. Also, when an alarm is
    triggered, it can send automated notifications (such as SMS and emails) to support
    for review via AWS **Simple Notification Service** (**SNS**).
  prefs: []
  type: TYPE_NORMAL
- en: You can use CloudWatch Logs to collect, monitor, and analyze the logs that are
    emitted by your training jobs. You can use these captured logs to understand the
    progress of your training jobs and identify errors and patterns to help troubleshoot
    any model training problems. For example, CloudWatch Logs might contain errors
    such as insufficient GPU memory to run model training or permission issues when
    accessing specific resources to help you troubleshoot model training problems.
    By default, CloudWatch Logs provides a UI tool called CloudWatch Logs Insights
    for interactively analyzing logs using a purpose-built query language. Alternatively,
    these logs can also be forwarded to an Elasticsearch cluster for analysis and
    querying. These logs can be aggregated in a designated logging and monitoring
    account to centrally manage log access and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker training jobs can also send events, such as a training job status
    changing from running to complete. You can create automated notification and response
    mechanisms based on these different events. For example, you can send out notifications
    to data scientists when a training job has either completed successfully or failed,
    along with a failure reason. You can also automate responses to these failures
    to the different statuses, such as model retraining on a particular failure condition.
  prefs: []
  type: TYPE_NORMAL
- en: The SageMaker Clarify component can detect data and model bias and provide model
    explainability reporting on the trained model. You can access bias and model explainability
    reports inside the SageMaker Studio UI or SageMaker APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Model endpoint monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Model endpoint monitoring provides visibility into the performance of the model
    serving infrastructure, as well as model-specific metrics such as data drift,
    model drift, and inference explainability. The following are some of the key metrics
    for model endpoint monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '**General system and resource utilization and error metrics**: These provide
    visibility into how the infrastructure resources (such as CPU, GPU, and memory)
    are utilized for model servicing. They can help with making decisions on provisioning
    infrastructure for the different model-serving needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data statistics monitoring metrics**: The statistical nature of data could
    change over time, which can result in degraded ML model performance from the original
    benchmarks. These metrics can include basic statistics deviations such as mean
    and standard changes, as well as data distribution changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model quality monitoring metrics**: These model quality metrics provide visibility
    into model performance deviation from the original benchmark. These metrics can
    include regression metrics (such as MAE and RMSE) and classification metrics (such
    as confusion matrix, F1, precision, recall, and accuracy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model inference explainability**: This provides model explainability on a
    per-prediction basis to help you understand what features had the most influence
    on the decision that was made by the prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model bias monitoring metrics**: Similar to bias detection for training,
    the bias metrics help us understand model bias at inference time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The model monitoring architecture relies on many of the same AWS services,
    including CloudWatch, EventBridge, and SNS. The following diagram shows an architecture
    pattern for a SageMaker-based model monitoring solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Model endpoint monitoring architecture ](img/B20836_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Model endpoint monitoring architecture'
  prefs: []
  type: TYPE_NORMAL
- en: This architecture works similarly to the model training architecture. **CloudWatch
    metrics** capture endpoint metrics such as CPU/GPU utilization, model invocation
    metrics (number of invocations and errors), and model latencies. These metrics
    help with operations such as hardware optimization and endpoint scaling.
  prefs: []
  type: TYPE_NORMAL
- en: '**CloudWatch Logs** captures logs that are emitted by the model-serving endpoint
    to help us understand the status and troubleshoot technical problems. Similarly,
    endpoint events, such as the status changing from **Creating** to **InService**,
    can help you build automated notification pipelines to kick off corrective actions
    or provide status updates.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to system- and status-related monitoring, this architecture also
    supports data and model-specific monitoring through a combination of SageMaker
    Model Monitor and SageMaker Clarify. Specifically, SageMaker Model Monitor can
    help you monitor data drift and model quality.
  prefs: []
  type: TYPE_NORMAL
- en: For data drift, SageMaker Model Monitor can use the training dataset to create
    baseline statistics metrics such as standard deviation, mean, max, min, and data
    distribution for the dataset features. It uses these metrics and other data characteristics,
    such as data types and completeness, to establish constraints. Then, it captures
    the input data in the production environment, calculates the metrics, compares
    them with the baseline metrics/constraints, and reports baseline drifts. Model
    Monitor can also report data quality issues such as incorrect data types and missing
    values. Data drift metrics can be sent to CloudWatch metrics for visualization
    and analysis, and CloudWatch alarms can be configured to trigger a notification
    or automated response when a metric crosses a predefined threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'For model quality monitoring, it creates baseline metrics (such as MAE for
    regression and accuracy for classification) using the baseline dataset, which
    contains both predictions and true labels. Then, it captures the predictions in
    production, ingests ground-truth labels, and merges the ground truth with the
    predictions to calculate various regression and classification metrics before
    comparing those with the baseline metrics. Similar to data drift metrics, model
    quality metrics can be sent to CloudWatch Metrics for analysis and visualization
    and CloudWatch alarms can be configured for automated notifications and/or responses.
    The following diagram shows how SageMaker Model Monitor works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – SageMaker Model Monitor process flow ](img/B20836_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: SageMaker Model Monitor process flow'
  prefs: []
  type: TYPE_NORMAL
- en: For bias detection, SageMaker Clarify can monitor bias metrics for deployed
    models continuously and raises alerts through CloudWatch when a metric crosses
    a threshold. We will cover bias detection in detail in *Chapter 13, Bias, Explainability,
    Privacy, and, Adversarial Attacks*.
  prefs: []
  type: TYPE_NORMAL
- en: ML pipeline monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The ML pipeline’s execution needs to be monitored for statuses and errors,
    so corrective actions can be taken as needed. During a pipeline execution, there
    are pipeline-level statuses/events as well as stage-level and action-level statuses/events.
    You can use these events and statuses to understand the progress of each pipeline
    and stage and be alerted when something is wrong. The following diagram shows
    how AWS CodePipeline, CodeBuild, and CodeCommit can work with CloudWatch, CloudWatch
    Logs, and EventBridge for general status monitoring and reporting, as well as
    problem troubleshooting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – ML CI/CD pipeline monitoring architecture ](img/B20836_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: ML CI/CD pipeline monitoring architecture'
  prefs: []
  type: TYPE_NORMAL
- en: CodeBuild can send metrics, such as `SucceededBuilds`, `FailedBuilds`, and `Duration`
    metrics. These CodeBuild metrics can be accessed through both the CodeBuild console
    and the CloudWatch dashboard. CodeBuild, CodeCommit, and CodePipeline can all
    emit events to EventBridge to report detailed status changes and trigger custom
    event processing, such as notifications, or log the events to another data repository
    for event archiving. All three services can send detailed logs to CloudWatch Logs
    to support operations such as troubleshooting or detailed error reporting.
  prefs: []
  type: TYPE_NORMAL
- en: Step Functions also provides a list of monitoring metrics to CloudWatch, such
    as execution metrics (such as execution failure, success, abort, and timeout)
    and activity metrics (such as activity started, scheduled, and succeeded). You
    can view these metrics in the management console and set a threshold to set up
    alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Service provisioning management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another key component of enterprise-scale ML platform management is **service**
    **provisioning management**. For large-scale service provisioning and deployment,
    an automated and controlled process should be adopted. Here, we will focus on
    provisioning the ML platform itself, not provisioning AWS accounts and networking,
    which should be established as the base environment for ML platform provisioning
    in advance. For ML platform provisioning, there are the following two main provisioning
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data science environment provisioning**: Provisioning the data science environment
    for data scientists mainly includes provisioning data science and data management
    tools, storage for experimentation, as well as access entitlement for data sources
    and pre-built ML automation pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML automation pipeline provisioning**: ML automation pipelines need to be
    provisioned in advance for data scientists and MLOps engineers to use them to
    automate different tasks such as container build, model training, and model deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are multiple technical approaches to automating service provisioning
    on AWS, such as using provisioning shell scripts, CloudFormation scripts, and
    AWS Service Catalog. With shell scripts, you can sequentially call the different
    AWS CLI commands in a script to provision different components, such as creating
    a SageMaker Studio notebook. CloudFormation is the IaC service for infrastructure
    deployment on AWS. With CloudFormation, you create templates that describe the
    desired resources and dependencies that can be launched as a single stack. When
    the template is executed, all the resources and dependencies specified in the
    stack will be deployed automatically. The following code shows the template for
    deploying a SageMaker Studio domain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: AWS Service Catalog allows you to create different IT products to be deployed
    on AWS. These IT products can include SageMaker notebooks, a CodeCommit repository,
    and CodePipeline workflow definitions. AWS Service Catalog uses CloudFormation
    templates to describe IT products. With Service Catalog, administrators create
    IT products with CloudFormation templates, organize these products by product
    portfolio, and entitle end users to access. The end users then access the products
    from the Service Catalog product portfolio.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the flow of creating a Service Catalog product
    and launching the product from the Service Catalog service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Service Catalog workflow ](img/B20836_09_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: Service Catalog workflow'
  prefs: []
  type: TYPE_NORMAL
- en: For large-scale and governed IT product management, Service Catalog is the recommended
    approach. Service Catalog supports multiple deployment options, including single
    AWS account deployments and hub-and-spoke cross-account deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'A hub-and-spoke deployment allows you to centrally manage all the products
    and make them available in different accounts. For our enterprise ML reference
    architecture in *Figure 9.4*, we can use the hub-and-spoke architecture to support
    the provisioning of data science environments and ML pipelines, as shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – The hub-and-spoke Service Catalog architecture for enterprise
    ML product management ](img/B20836_09_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: The hub-and-spoke Service Catalog architecture for enterprise
    ML product management'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding architecture, we set up the central portfolio in the shared
    services account. All the products, such as creating new Studio domains, new Studio
    user profiles, CodePipeline definitions, and training pipeline definitions, are
    centrally managed in the central hub account. Some products are shared with the
    different data science accounts to create data science environments for data scientists
    and teams. Some other products are shared with model training accounts for standing
    up ML training pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in building and operating an ML platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Constructing an enterprise ML platform is a multifaceted undertaking. It often
    requires significant time, with organizations taking six months or more to implement
    the initial phase of their ML platform. Continuous efforts are needed to incorporate
    new functionalities and enhancements for many years to come. Onboarding users
    and ML projects onto the new platform is another demanding aspect, involving extensive
    education for the user base and providing direct technical support.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, platform adjustments might be necessary to ensure smooth onboarding
    and successful utilization. Having collaborated with many customers in building
    their enterprise ML platform, I have identified some best practices for the construction
    and adoption of an ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: ML platform project execution best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Assemble cross-functional teams**: Bring together data engineers, ML researchers,
    DevOps engineers, application developers, and business domain experts into integrated
    teams. This diversity of skills and perspectives will enrich the platform design
    and implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Develop governance requirements and processes**: Define processes and requirements
    early on for model verification, explainability, ethics reviews, and approvals
    prior to production deployment. This will embed responsible AI practices into
    the platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define key performance indicators (KPIs) to measure success**: Identify relevant
    business KPIs and implement processes to actively monitor and report on model
    and platform impact on these KPIs. Share reports with stakeholders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Select pilot ML workloads**: Choose a few pilot ML projects or workloads
    to implement first on the new platform. Learn from these real-world use cases
    to validate and improve the platform design and capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define the target state and execute in phases**: Articulate the long-term
    vision and target state for the enterprise ML platform. However, strategically
    execute adoption in incremental phases for faster learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML platform design and implementation best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Adopt fully managed built-in capabilities**: Leverage SageMaker’s managed
    algorithms, containers, and features as defaults to reduce overhead and simplify
    integration. Only use custom built features if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implement infrastructure as code**: Use CloudFormation or Terraform to provision,
    configure, and manage ML infrastructure through code. This enables consistency
    and automation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build CI/CD pipelines**: Implement continuous integration and deployment
    pipelines leveraging CodePipeline, CodeBuild, CodeDeploy, and SageMaker for automated
    workflows. Consider GitHub Actions/Jenkins if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automate experiment tracking**: Configure SageMaker or third-party tools
    to automatically log model training metadata like parameters, metrics, and artifacts.
    This enables debugging, comparisons, and reproducibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Establish an approved library repository**: Create a centralized, governed
    repository of approved libraries and packages for training, deployment, and inference
    code. This ensures consistency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Design for scalability and spikes**: Architect the platform to handle varied
    usage patterns and traffic spikes via auto-scaling capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prioritize security from the start**: Implement security best practices including
    scanning, patching, encryption, and access controls. Have an incident response
    plan.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build self-service capability**: Develop self-service functionality early
    and evolve it to empower users while maintaining governance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centralize the model repository**: Use a single, central repository for models
    to improve collaboration, discovery, compliance, and efficient deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Establish a central feature store**: Implement a centralized feature store
    for sharing, monitoring, and governing feature engineering work and usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform use and operations best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Limit production access**: Restrict access to production systems to only
    essential support and operations staff. This reduces the risk of mistakes or unauthorized
    changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize costs**: Leverage auto-scaling, spot instances, availability-based
    pricing, and other capabilities to optimize and reduce cloud costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and observability**: Actively monitor model accuracy, data drift,
    system performance, and so on using CloudWatch, SageMaker Debugger, Model Monitor,
    and other tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Establish change management**: Define a structured process for managing,
    reviewing, approving, and communicating platform/model changes prior to deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incident management process**: Institute an incident response plan with procedures
    to detect, escalate, and resolve production issues and anomalies in a timely manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-AZ and region deployments**: Deploy models and platform infrastructure
    across multiple availability zones and regions to improve resilience and minimize
    latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Release management**: Implement structured release processes for coordinating,
    reviewing, and planning changes and new model/platform versions before deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Capacity planning**: Proactively assess and project infrastructure capacity
    needs based on roadmaps and workloads. Scale appropriately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource tagging**: A properly designed tagging strategy provides organization,
    discovery, security, automation, compliance, and improved visibility in an ML
    platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a robust enterprise ML platform requires thoughtful strategy and
    orchestration across people, processes, and technology. By bringing together cross-functional
    teams, instituting responsible AI governance, monitoring business impact, and
    designing for scalability, security, and collaboration, organizations can accelerate
    their AI journey. Adopting modern infrastructure as code, CI/CD pipelines, and
    cloud services lays a solid technology foundation. However, to realize value,
    platforms must be tightly integrated with line-of-business priorities and continuously
    provide trustworthy AI. With deliberate planning and phased execution centered
    on business goals and users, companies can transform into AI-driven enterprises.
  prefs: []
  type: TYPE_NORMAL
- en: The key is to balance innovation with governance, move fast through automation
    while maintaining control, and evolve a platform that responsibly democratizes
    AI capabilities for both experts and business users. This enables embedding reliable
    and accountable AI throughout operations for a competitive advantage.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the key requirements and best practices for building
    an enterprise ML platform. We discussed how to design a platform that supports
    the end-to-end ML lifecycle, process automation, and separation of environments.
    Architectural patterns were reviewed, including how to leverage AWS services to
    build a robust ML platform on the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: The core capabilities of different ML environments were covered, such as training,
    hosting, and shared services. Best practices around platform design, operations,
    governance, and integration were also discussed. You should now have a solid understanding
    of what an enterprise-grade ML platform entails and key considerations for building
    one on AWS leveraging proven patterns.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deeper into advanced ML engineering topics.
    This includes distributed training techniques to scale model development and low-latency
    serving methods for optimizing inference.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code70205728346636561.png)'
  prefs: []
  type: TYPE_IMG
