- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Designing an Enterprise ML Architecture with AWS ML Services
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AWS 机器学习服务设计企业级机器学习架构
- en: Many organizations opt to build enterprise ML platforms to support numerous
    fast-moving initiatives. These platforms are designed to facilitate the entire
    ML lifecycle and accommodate various usage patterns, all while emphasizing automation
    and scalability. As a practitioner, I often get asked to provide architectural
    guidance for creating such enterprise ML platforms. In this chapter, we will explore
    the fundamental requirements for designing enterprise ML platforms. We will cover
    a range of topics, such as workflow automation, infrastructure scalability, and
    system monitoring.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织选择构建企业级机器学习平台以支持众多快速发展的项目。这些平台旨在促进整个机器学习生命周期，并适应各种使用模式，同时强调自动化和可扩展性。作为一名从业者，我经常被要求提供创建此类企业级机器学习平台的架构指导。在本章中，我们将探讨设计企业级机器学习平台的基本要求。我们将涵盖一系列主题，例如工作流程自动化、基础设施可扩展性和系统监控。
- en: Throughout the discussion, you will gain insights into architecture patterns
    that enable the development of technology solutions to automate the end-to-end
    ML workflow and ensure seamless deployment at a large scale. Additionally, we
    will delve deep into essential components of enterprise ML architecture, such
    as model training, model hosting, the feature store, and the model registry, all
    tailored to meet the demands of enterprise-level operations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个讨论过程中，你将深入了解那些能够开发自动化端到端机器学习工作流程并确保大规模无缝部署的技术解决方案的架构模式。此外，我们还将深入探讨企业级机器学习架构的必要组件，例如模型训练、模型托管、特征存储和模型注册，所有这些均针对满足企业级运营的需求而定制。
- en: AI risk, governance, and security are other important considerations for enterprise
    ML platforms, and we will cover them in greater detail in *Chapters 12* and *13*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能风险、治理和安全是企业级机器学习平台的另一些重要考虑因素，我们将在第 12 章和第 13 章中更详细地介绍它们。
- en: 'In a nutshell, the following topics will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章将涵盖以下主题：
- en: Key considerations for ML platforms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习平台的关键考虑因素
- en: Key requirements for an enterprise ML platform
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 企业级机器学习平台的关键要求
- en: Enterprise ML architecture pattern overview
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 企业级机器学习架构模式概述
- en: Adopting MLOps for an ML workflow
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用 MLOps 进行机器学习工作流程
- en: Best practices in building and operating ML platforms
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和运营机器学习平台的最佳实践
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We will continue to use the AWS environment for the hands-on portion of this
    chapter. All the source code mentioned in this chapter can be found at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter09](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter09).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用 AWS 环境为本章的动手部分。本章中提到的所有源代码都可以在[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter09](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter09)找到。
- en: Key considerations for ML platforms
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习平台的关键考虑因素
- en: Designing, building, and operating ML platforms are complex endeavors as there
    are many different considerations, including the personas, key ML process workflows,
    and various technical capability requirements for the different personas and workflows.
    In this section, we will delve into each of these key considerations in depth.
    Let’s dive in!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 设计、构建和运营机器学习平台是复杂的任务，因为有许多不同的考虑因素，包括角色、关键机器学习流程工作流以及针对不同角色和工作流的各种技术能力要求。在本节中，我们将深入探讨这些关键考虑因素。让我们深入探讨吧！
- en: The personas of ML platforms and their requirements
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习平台的角色及其需求
- en: 'In the previous chapter, we talked about building a data science environment
    for the data scientists and ML engineers who mainly focus on experimentation and
    model development. In an enterprise setting where an ML platform is needed, there
    are other personas involved, each with their own specific requirements. At a high
    level, there are two types of personas associated with the ML platform: ML platform
    builders and ML platform users.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了为专注于实验和模型开发的数据科学家和机器学习工程师构建数据科学环境。在企业环境中，当需要机器学习平台时，涉及到的角色和各自的具体需求就更多了。从高层次来看，与机器学习平台相关的角色有两种：机器学习平台构建者和机器学习平台用户。
- en: ML platform builders
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习平台构建者
- en: 'ML platform builders have the crucial responsibility of constructing the infrastructure
    for data and ML platforms. Here are some essential builder types required to build
    a cloud-based ML platform:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ML平台构建者承担着构建数据和ML平台基础设施的关键责任。以下是构建基于云的ML平台所需的某些基本构建者类型：
- en: '**Cloud infrastructure architect/engineer**: These experts design the overall
    cloud infrastructure, selecting appropriate cloud services and setting up the
    foundation for the ML platform.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云基础设施架构师/工程师**：这些专家设计整体云基础设施，选择合适的云服务并为ML平台搭建基础。'
- en: '**Security engineer**: Security engineers ensure that the ML platform adheres
    to industry-standard security practices, safeguarding sensitive data and protecting
    against potential threats.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全工程师**：安全工程师确保ML平台遵循行业标准的安全实践，保护敏感数据并防范潜在威胁。'
- en: '**ML platform product manager**: ML platform product managers are responsible
    for understanding functional user requirements and non-functional requirements,
    defining ML platform capabilities and the implementation roadmap.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ML平台产品经理**：ML平台产品经理负责理解功能和非功能用户需求，定义ML平台的能力和实施路线图。'
- en: '**ML platform engineers**: ML platform engineers are responsible for designing,
    building, and maintaining the infrastructure and systems that support the end-to-end
    ML lifecycle within an organization. ML platform engineers play a crucial role
    in ensuring that data scientists and ML practitioners can efficiently develop,
    deploy, and manage ML models on the organization’s ML platform. They are responsible
    for the platform design, covering key functional areas such as training and hosting,
    taking into account scalability, performance, security, and integration with existing
    systems.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ML平台工程师**：ML平台工程师负责设计、构建和维护支持组织内端到端ML生命周期的基础设施和系统。ML平台工程师在确保数据科学家和ML实践者能够高效地在组织的ML平台上开发、部署和管理ML模型方面发挥着关键作用。他们负责平台设计，涵盖关键功能领域，如训练和托管，考虑到可扩展性、性能、安全性和与现有系统的集成。'
- en: '**Data engineers**: Data engineers are responsible for building data pipelines,
    data storage solutions, and data processing frameworks to ensure seamless data
    access and processing for ML tasks.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据工程师**：数据工程师负责构建数据管道、数据存储解决方案和数据处理框架，以确保ML任务的无缝数据访问和处理。'
- en: '**ML platform tester**: ML platform testers are responsible for testing the
    core capabilities of platforms to meet the desired functional and non-functional
    requirements.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ML平台测试员**：ML平台测试员负责测试平台的核心功能，以满足所需的预期功能和非功能需求。'
- en: Platform users and operators
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平台用户和运维人员
- en: 'Platform users and operators are the actual users of an ML platform. They use
    the ML platform to perform full lifecycle ML tasks from data exploration to model
    monitoring. The following are some of the key platform user and operator types:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 平台用户和运维人员是ML平台的实际用户。他们使用ML平台执行从数据探索到模型监控的全生命周期ML任务。以下是一些关键的平台用户和运维人员类型：
- en: '**Data scientists/ML engineers**: Data scientists and ML engineers are the
    primary users of an ML platform. They use the platform to explore data, build
    and train ML models, perform feature engineering, and evaluate model performance.
    They partner with ML platform engineers and ops engineers to integrate trained
    models into production systems, optimize model inference performance, and ensure
    the models are scalable and reliable in real-world environments.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据科学家/ML工程师**：数据科学家和ML工程师是ML平台的主要用户。他们使用平台来探索数据、构建和训练ML模型、执行特征工程以及评估模型性能。他们与ML平台工程师和运维工程师合作，将训练好的模型集成到生产系统中，优化模型推理性能，并确保模型在实际环境中可扩展且可靠。'
- en: '**Model tester and validator**: The primary responsibilities for a model tester
    involve assessing the performance and reliability of ML models developed by data
    scientists using the ML platform. Specifically, model testers are responsible
    for model testing using different datasets, model performance metrics calculation
    and evaluation, overfitting/underfitting detection, and testing edge cases. Model
    validators are responsible for validating models against business objectives,
    risk assessment, and other issues such as ethics considerations.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型测试和验证人员**：模型测试人员的主要职责是评估数据科学家使用机器学习平台开发的机器学习模型的性能和可靠性。具体来说，模型测试人员负责使用不同的数据集进行模型测试、计算和评估模型性能指标、检测过拟合/欠拟合以及测试边缘情况。模型验证人员负责验证模型是否符合业务目标、风险评估以及其他问题，如伦理考量。'
- en: '**Model approver**: This individual or team is responsible for reviewing and
    approving the deployment of ML models into production or other critical environments.
    The model approver’s primary role is to ensure that the developed ML models meet
    the organization’s standards, business requirements, and compliance policies before
    they are deployed. They also help ensure all the required testing, post-deployment
    operations, and policies are in place.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型审批人**：此个人或团队负责审查和批准将机器学习模型部署到生产或其他关键环境。模型审批人的主要职责是在模型部署之前确保开发的机器学习模型符合组织标准、业务需求和合规政策。他们还帮助确保所有必要的测试、部署后的运营和政策都到位。'
- en: '**Operations and support engineers**: This role ensures the smooth operation,
    maintenance, and ongoing support of the ML platform within an organization. Their
    responsibilities encompass various technical and operational aspects to keep the
    ML platform running efficiently and to provide assistance to users. Some of the
    key functions include platform maintenance and upgrades, performance monitoring
    and optimization, incident management, infrastructure management, security and
    access control, and platform documentation.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运维和支持工程师**：此角色确保机器学习平台在组织内的平稳运行、维护和持续支持。他们的职责包括各种技术和运营方面，以保持机器学习平台的效率运行，并向用户提供帮助。一些关键功能包括平台维护和升级、性能监控和优化、事件管理、基础设施管理、安全和访问控制以及平台文档。'
- en: '**AI risk/governance manager**: The primary responsibility of an AI risk/governance
    manager is to manage and mitigate the potential risks associated with the use
    of AI/ML systems. Their role is essential in ensuring that AI technologies are
    developed, deployed, and used responsibly, ethically, and in compliance with relevant
    regulations. They help ensure appropriate processes, policies, and technology
    standards are created and adhered to.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工智能风险/治理经理**：人工智能风险/治理经理的主要职责是管理和减轻使用人工智能/机器学习系统可能带来的潜在风险。他们的角色对于确保人工智能技术得到负责任、道德地开发、部署和使用，并符合相关法规至关重要。他们帮助确保创建并遵守适当的流程、政策和技术标准。'
- en: You might wonder where the ML solutions architect fits into this overall picture.
    The ML solutions architect plays a pivotal role that spans builders, users, and
    operators. They serve as a bridge between these groups, offering valuable insights
    and guidance. Firstly, ML solutions architect collaborate with builders, understanding
    user requirements, and assisting in the end-to-end architecture design. They ensure
    that the ML platform aligns with the specific needs of users and operators. Secondly,
    ML solutions architects advise users and operators on effectively utilizing the
    ML platform. They educate them on best practices for configuring and leveraging
    the platform to meet diverse needs and use cases.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道机器学习解决方案架构师在这个整体图景中处于什么位置。机器学习解决方案架构师在构建者、用户和操作者之间扮演着关键角色。他们作为这些群体之间的桥梁，提供有价值的见解和指导。首先，机器学习解决方案架构师与构建者合作，了解用户需求，并协助端到端架构设计。他们确保机器学习平台与用户和操作者的特定需求保持一致。其次，机器学习解决方案架构师向用户和操作者提供有关有效利用机器学习平台的建议。他们教育他们如何配置和利用平台以满足不同的需求和用例。
- en: Common workflow of an ML initiative
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习项目的常见工作流程
- en: 'Different organizations have diverse workflows and governance processes when
    running ML initiatives. However, most of these workflows typically consist of
    the following key steps:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行机器学习项目时，不同的组织具有不同的工作流程和治理流程。然而，这些工作流程通常包括以下关键步骤：
- en: Collecting and processing data from different sources and making it available
    for data scientists.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从不同来源收集和处理数据，使其可供数据科学家使用。
- en: Performing data exploratory analysis, forming hypotheses, creating ML features,
    performing experimentation, and building different ML models using different techniques
    and ML algorithms using a subset of data.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据子集进行数据探索性分析、形成假设、创建机器学习特征、进行实验以及使用不同技术和机器学习算法构建不同的机器学习模型。
- en: A data labeling workflow is sometimes needed to label training data for supervised
    ML tasks such as document classification or object detection.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时需要数据标注工作流程来标注用于监督机器学习任务（如文档分类或目标检测）的训练数据。
- en: Conducting full model training and tuning using a full dataset.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用完整数据集进行完整模型训练和调整。
- en: Candidate models trained using the full dataset are promoted into a testing
    environment for formal quality assurance. Testers document testing details for
    all the testers and verify if the models meet desired performance metrics and
    other evaluation criteria, such as latency and scalability. Model validators evaluate
    the ML techniques, perform an analysis of the model, and check the alignment with
    the business outcome.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用完整数据集训练的候选模型被提升到测试环境进行正式的质量保证。测试人员记录所有测试人员的测试细节，并验证模型是否满足预期的性能指标和其他评估标准，如延迟和可扩展性。模型验证者评估机器学习技术，对模型进行分析，并检查与业务成果的一致性。
- en: Model risk assessment is performed to ensure risk items are assessed, mitigated,
    or accepted.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行模型风险评估以确保风险项得到评估、缓解或接受。
- en: After the model passes the testing and validation step, the model is sent to
    the model approver for final review and approval for production deployment.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型通过测试和验证步骤后，模型被发送给模型审批人员进行最终审查和批准以进行生产部署。
- en: The model is deployed into production with approval. The model is registered
    in the model registry, the dataset is versioned and retained, any code artifacts
    are also versioned and stored, and detailed training configuration details are
    also documented.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在获得批准后部署到生产环境中。模型在模型注册表中注册，数据集进行版本控制和保留，任何代码工件也进行版本控制和存储，详细的训练配置细节也进行了文档记录。
- en: The model is monitored in production for model performance, data drift, system
    issues, and security exposure and attacks. The incident management process is
    followed to address issues identified.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在生产中监控模型性能、数据漂移、系统问题和安全暴露及攻击。遵循事件管理流程来处理识别出的问题。
- en: At required schedules, auditors perform end-to-end audits to ensure all processes
    and policies are followed, artifacts are stored, access to systems and models
    is properly logged, documentation meets the required standards, and any violations
    are flagged and escalated.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在需要的时间表上，审计员执行端到端审计以确保所有流程和政策得到遵守，工件得到存储，系统和对模型的访问得到适当记录，文档符合所需标准，任何违规行为都会被标记并升级。
- en: It is worth noting that these steps are not exhaustive. Depending on the organizational,
    risk, and regulatory requirements, organizations can carry out more steps to address
    these requirements.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这些步骤并不详尽。根据组织、风险和监管要求，组织可以执行更多步骤来满足这些要求。
- en: Platform requirements for the different personas
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同角色的平台要求
- en: ML platforms involve a diverse array of potential players and users. The following
    table outlines the essential needs for ML platforms for both users and operators
    of the platform. Note that the table does not include the builder of the platform.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习平台涉及各种潜在的参与者和用户。以下表格概述了机器学习平台对于用户和操作者的基本需求。请注意，该表格不包括平台的建设者。
- en: '| **User/operator** | **Tools/capability requirements** |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| **用户/操作员** | **工具/能力要求** |'
- en: '| Data scientist | Access to various ML libraries, tools, and frameworks for
    model development and experimentationAccess to different datasets to perform different
    ML tasksCapabilities to perform data exploration and model training using different
    hardwareWorkflow automation including data retrieval and processing, feature engineering,
    experimentation, model building, and model versioning for reproducibility |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 数据科学家 | 访问各种机器学习库、工具和框架以进行模型开发和实验访问不同的数据集以执行不同的机器学习任务使用不同硬件的能力进行数据探索和模型训练工作流程自动化，包括数据检索和处理、特征工程、实验、模型构建和模型版本化以实现可重复性
    |'
- en: '| Model tester and validator | Access to different test datasets for model
    testing and validationAccess to various libraries and tools for data visualization,
    model evaluation, an ML testing framework, bias detection tools, model interpretability
    tools, and statistical testing tools |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 模型测试和验证人员 | 访问不同的测试数据集以进行模型测试和验证，以及访问用于数据可视化、模型评估、机器学习测试框架、偏差检测工具、模型可解释性工具和统计测试工具的各种库和工具
    |'
- en: '| Model approvers | Access to model documentation, model evaluation metrics,
    a compliance checklist, and a model explainability reportAccess to an approval
    workflow management tool |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 模型审批者 | 访问模型文档、模型评估指标、合规清单和模型可解释性报告，以及访问审批工作流管理工具 |'
- en: '| Ops and support engineers | Access to all infrastructure components within
    the ML platform, including code and container repositories, library packages,
    training, hosting, pipelines, logging, monitoring and alerting, security and access
    control, backup and discovery, performance testing, and incident management toolsAccess
    to tools for platform automation and management |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 运维和支持工程师 | 访问机器学习平台内的所有基础设施组件，包括代码和容器存储库、库包、训练、托管、管道、日志记录、监控和警报、安全性和访问控制、备份和发现、性能测试和事件管理工具，以及访问平台自动化和管理工具
    |'
- en: '| AI risk officer | Access to an AI risk assessment tool, a governance platform,
    model explainability and interpretability tools, bias detection and fairness assessment
    tools, AI risk reporting and dashboards, and AI regulation and policy monitoring
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 人工智能风险官 | 访问人工智能风险评估工具、治理平台、模型可解释性和可解释性工具、偏差检测和公平性评估工具、人工智能风险报告和仪表板，以及人工智能法规和政策监控
    |'
- en: 'Table 9.1: ML platform requirements by personas'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1：面向不同用户和操作者的机器学习平台需求
- en: In summary, the success of an ML platform relies heavily on meeting the distinct
    tools and capability requirements of its users/operators. By addressing these
    distinct needs, the ML platform can effectively support its users and operators
    in building, deploying, and managing AI solutions with confidence.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，机器学习平台的成功在很大程度上取决于满足其用户/操作者的独特工具和能力需求。通过解决这些独特需求，机器学习平台可以有效地支持其用户和操作者自信地构建、部署和管理人工智能解决方案。
- en: Key requirements for an enterprise ML platform
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 企业机器学习平台的关键需求
- en: To deliver business benefits through ML at scale, organizations must have the
    capability to rapidly experiment with diverse scientific approaches, ML technologies,
    and extensive datasets. Once ML models are trained and validated, they need to
    seamlessly transition to production deployment. While some similarities exist
    between a traditional enterprise software system and an ML platform, such as scalability
    and security concerns, an enterprise ML platform presents distinctive challenges.
    These include the need to integrate with the data platform and high-performance
    computing infrastructure to facilitate large-scale model training.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过大规模机器学习实现业务效益，组织必须具备快速实验多种科学方法、机器学习技术和大量数据集的能力。一旦机器学习模型经过训练和验证，它们需要无缝过渡到生产部署。虽然传统企业软件系统和机器学习平台之间存在一些相似之处，例如可扩展性和安全担忧，但企业机器学习平台提出了独特的挑战。这些挑战包括需要与数据平台和高性能计算基础设施集成，以促进大规模模型训练。
- en: 'Let’s delve into some specific core requirements of an enterprise ML platform
    to meet the needs of different users and operators:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨企业机器学习平台的一些具体核心需求，以满足不同用户和操作者的需求：
- en: '**Support for the end-to-end ML lifecycle**: An enterprise ML platform must
    cater to both data science experimentation and production-grade operations and
    deployments. In *Chapter 8*, *Building a Data Science Environment Using AWS ML
    Services,* we explored the essential architecture components required to construct
    a data science experimentation environment using AWS ML services. However, to
    facilitate seamless production-grade operations and deployment, the enterprise
    ML platform should also include specific architecture components dedicated to
    large-scale model training, model management, feature management, and highly available
    and scalable model hosting.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持端到端机器学习生命周期**：企业机器学习平台必须满足数据科学实验和生产级操作和部署的需求。在*第8章*，*使用AWS机器学习服务构建数据科学环境*中，我们探讨了构建使用AWS机器学习服务的数据科学实验环境所需的基本架构组件。然而，为了促进无缝的生产级操作和部署，企业机器学习平台还应包括专门针对大规模模型训练、模型管理、特征管理和高度可用且可扩展的模型托管的特定架构组件。'
- en: '**Support for continuous integration (CI), continuous training (CT), and continuous
    deployment (CD)**: In addition to testing and validating code and components,
    an enterprise ML platform extends its CI capabilities to include data and models.
    The CD capability for ML goes beyond merely deploying a single software piece;
    it involves managing both ML models and inference engines in conjunction. CT is
    a unique aspect of ML, wherein a model is continuously monitored, and automated
    model retraining can be triggered upon detecting data drift, model drift, or changes
    in the training data. Data drift refers to a change in the data wherein the statistical
    characteristics of the production data differ from the data used for model training.
    On the other hand, model drift signifies a decline in model performance compared
    to the performance achieved during the model training phase.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持持续集成（CI）、持续训练（CT）和持续部署（CD）**：除了测试和验证代码和组件外，企业级机器学习平台的CI能力还扩展到包括数据和模型。机器学习的CD能力不仅限于部署单个软件组件；它还涉及管理与推理引擎相结合的机器学习模型。CT是机器学习的一个独特方面，其中模型会持续监控，并在检测到数据漂移、模型漂移或训练数据变化时自动触发模型重新训练。数据漂移指的是数据发生变化，其中生产数据的统计特征与用于模型训练的数据不同。另一方面，模型漂移表示模型性能相对于模型训练阶段达到的性能有所下降。'
- en: '**Operations support**: An enterprise ML platform should provide capabilities
    to monitor the statuses, errors, and metrics of different pipeline workflows,
    processing/training jobs, model behavior changes, data drift, and model-serving
    engines. Additionally, infrastructure-level statistics and resource usage are
    continuously monitored to ensure efficient operations. An automated alert mechanism
    is a crucial component of operations, promptly notifying relevant stakeholders
    of any issues or anomalies. Moreover, implementing automated failure recovery
    mechanisms wherever possible further enhances the platform’s robustness and minimizes
    downtime, ensuring smooth and reliable ML operations.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作支持**：企业级机器学习平台应提供监控不同管道工作流程状态、错误和指标、处理/训练作业、模型行为变化、数据漂移和模型服务引擎的能力。此外，基础设施级别的统计信息和资源使用情况持续监控，以确保高效的操作。自动警报机制是操作的关键组成部分，能够及时通知相关利益相关者任何问题或异常。此外，尽可能实施自动故障恢复机制，进一步增强了平台的安全性并最小化了停机时间，确保了平稳和可靠的机器学习操作。'
- en: '**Support for different languages and ML frameworks**: An enterprise ML platform
    empowers data scientists and ML engineers to use their preferred programming languages
    and ML libraries. It should accommodate popular languages like Python and R, along
    with well-known ML frameworks such as TensorFlow, PyTorch, and scikit-learn. This
    flexibility ensures that teams can leverage their expertise and utilize the most
    suitable tools for efficient and effective model development within the platform.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持不同语言和机器学习框架**：企业级机器学习平台使数据科学家和机器学习工程师能够使用他们偏好的编程语言和机器学习库。它应容纳流行的语言，如Python和R，以及知名的机器学习框架，如TensorFlow、PyTorch和scikit-learn。这种灵活性确保团队可以利用他们的专业知识，并在平台内利用最合适的工具进行高效和有效的模型开发。'
- en: '**Computing hardware resource management**: An enterprise ML platform should
    cater to diverse model training and inference requirements, taking into account
    cost considerations. This entails providing support for various types of computing
    hardware, such as CPUs and GPUs, to optimize performance and cost-effectiveness.
    Furthermore, the platform should be equipped to handle specialized ML hardware,
    like AWS’s Inferentia and Tranium chips, wherever relevant, to leverage the benefits
    of specialized hardware accelerators for specific ML workloads.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算硬件资源管理**：企业级机器学习平台应满足多样化的模型训练和推理需求，同时考虑成本因素。这包括提供对各种计算硬件的支持，例如CPU和GPU，以优化性能和成本效益。此外，平台还应具备处理专用机器学习硬件的能力，如AWS的Inferentia和Tranium芯片，在相关情况下，以利用专用硬件加速器为特定机器学习工作负载带来的好处。'
- en: '**Integration with other third-party systems and software**: An enterprise
    ML platform rarely operates in isolation. It must offer robust integration capabilities
    with various third-party software and platforms, including workflow orchestration
    tools, container registries, and code repositories. This seamless integration
    enables smooth collaboration and interoperability, allowing teams to leverage
    existing tools and workflows while benefiting from the advanced features and capabilities
    of the ML platform.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与其他第三方系统和软件的集成**：企业机器学习平台很少独立运行。它必须提供与各种第三方软件和平台（包括工作流程编排工具、容器注册库和代码仓库）的强大集成能力。这种无缝集成使得团队合作和互操作性变得顺畅，允许团队利用现有工具和工作流程，同时受益于机器学习平台的先进功能和能力。'
- en: '**Authentication and authorization**: For an enterprise ML platform, ensuring
    secure access to data, artifacts, and ML platform resources is essential. This
    requires offering various levels of authentication and authorization control.
    The platform may include built-in authentication and authorization capabilities,
    or it can integrate with an external authentication and authorization service.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**身份验证和授权**：对于企业机器学习平台来说，确保对数据、文物和机器学习平台资源的安全访问是至关重要的。这需要提供各种级别的身份验证和授权控制。平台可能包括内置的身份验证和授权功能，或者它可以与外部身份验证和授权服务集成。'
- en: '**Data encryption**: In regulated industries like financial services and healthcare,
    data encryption is a critical requirement. An enterprise ML platform must offer
    robust capabilities for encrypting data both at rest and in transit, often allowing
    customers to manage their encryption keys. This level of data protection ensures
    that sensitive information remains secure and compliant with industry regulations,
    providing the necessary reassurance for handling confidential data within these
    sectors.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据加密**：在金融服务和医疗保健等受监管的行业中，数据加密是一个关键要求。企业机器学习平台必须提供强大的能力来加密静态和传输中的数据，通常允许客户管理他们的加密密钥。这种数据保护水平确保敏感信息保持安全并符合行业法规，为在这些行业中处理机密数据提供了必要的保证。'
- en: '**Artifact management**: In the ML lifecycle, an enterprise ML platform handles
    datasets and generates various artifacts at different stages. These artifacts
    can be features, code, models, and containers. To ensure reproducibility and adhere
    to governance and compliance standards, the platform must possess the capability
    to track, manage, and version-control these artifacts. By effectively managing
    and recording the changes made throughout the ML process, the platform maintains
    a clear and organized record, facilitating the reproducibility of results and
    providing a reliable audit trail for compliance purposes.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文物管理**：在机器学习生命周期中，企业机器学习平台在各个阶段处理数据集并生成各种文物。这些文物可以是特征、代码、模型和容器。为确保可重复性和遵守治理和合规标准，平台必须具备跟踪、管理和版本控制这些文物的能力。通过有效管理和记录整个机器学习过程中所做的更改，平台保持了一个清晰且有序的记录，促进了结果的重复性，并为合规目的提供了可靠的审计轨迹。'
- en: '**ML library package management**: Standardizing and approving ML library packages
    used by data scientists is crucial for many organizations. By establishing a central
    library that contains pre-approved packages, it becomes possible to enforce consistent
    standards and policies across the usage of library packages. This approach ensures
    that data scientists work with vetted and authorized libraries, promoting reliability,
    security, and adherence to organizational guidelines when developing ML solutions.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习库包管理**：对于许多组织来说，标准化和批准数据科学家使用的机器学习库包至关重要。通过建立一个包含预先批准包的中心库，可以强制执行跨库包使用的统一标准和政策。这种方法确保数据科学家在开发机器学习解决方案时使用经过审查和授权的库，促进可靠性、安全性和遵守组织指南。'
- en: '**Access to different data stores**: An essential feature of an enterprise
    ML platform is to offer seamless access to various data stores, simplifying model
    development and training processes. This accessibility to diverse data sources
    streamlines the workflow for data scientists and ML engineers, enabling them to
    efficiently access and utilize the necessary data for their tasks within the platform.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问不同的数据存储**：企业机器学习平台的一个基本功能是提供对各种数据存储的无缝访问，简化模型开发和训练过程。这种对多样化数据源的访问简化了数据科学家和机器学习工程师的工作流程，使他们能够高效地访问和利用平台内执行任务所需的必要数据。'
- en: '**Self-service capability**: To enhance operational efficiency and reduce reliance
    on central teams, an enterprise ML platform should incorporate self-service capabilities
    for tasks like user onboarding, environment setup, and pipeline provisioning.
    By enabling users to perform these tasks independently, the platform streamlines
    operations, empowering data scientists and ML engineers to work more autonomously
    and efficiently.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自助服务能力**：为了提高运营效率并减少对中央团队的依赖，企业ML平台应包含自助服务能力，以支持用户入职、环境设置和管道提供等任务。通过使用户能够独立执行这些任务，平台简化了操作，使数据科学家和ML工程师能够更加自主和高效地工作。'
- en: '**Model testing and validation**: An enterprise ML platform should provide
    comprehensive model testing and validation features to support thorough assessments
    of ML models. This can include features such as A/B testing infrastructure, model
    robustness testing packages, automated testing pipelines, performance metrics
    tracking and error analysis tools, and visualization.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型测试和验证**：企业ML平台应提供全面的模型测试和验证功能，以支持对ML模型的彻底评估。这可能包括A/B测试基础设施、模型鲁棒性测试包、自动化测试管道、性能指标跟踪和错误分析工具以及可视化等功能。'
- en: Having covered the essential requirements of an enterprise ML platform, let’s
    now explore how AWS ML and DevOps services, such as SageMaker, CodePipeline, and
    Step Functions, can be effectively utilized to construct a robust, enterprise-grade
    ML platform.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了企业ML平台的基本要求后，现在让我们探讨如何有效地利用AWS ML和DevOps服务，如SageMaker、CodePipeline和Step
    Functions，来构建一个强大、企业级ML平台。
- en: Enterprise ML architecture pattern overview
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 企业ML架构模式概述
- en: 'Building an enterprise ML platform on AWS starts with creating different environments
    to enable different data science and operation functions. The following diagram
    shows the core environments that normally make up an enterprise ML platform. From
    an isolation perspective, in the context of the AWS cloud, each environment in
    the following diagram is a separate AWS account:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS上构建企业ML平台的第一步是创建不同的环境，以启用不同的数据科学和运营功能。以下图表显示了通常构成企业ML平台的核心环境。从隔离的角度来看，在AWS云的背景下，以下图表中的每个环境都是一个独立的AWS账户：
- en: '![Figure 9.1 – Enterprise ML architecture environments ](img/B20836_09_01.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – 企业ML架构环境](img/B20836_09_01.png)'
- en: 'Figure 9.1: Enterprise ML architecture environments'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：企业ML架构环境
- en: 'As we discussed in *Chapter 8*, *Building a Data Science Environment Using
    AWS ML Services*, data scientists utilize the data science environment for experimentation,
    model building, and tuning. Once these experiments are completed, the data scientists
    commit their work to the proper code and data repositories. The next step is to
    train and tune the ML models in a controlled and automated environment using the
    algorithms, data, and training scripts that were created by the data scientists.
    This controlled and automated model training process will help ensure consistency,
    reproducibility, and traceability for scalable model building. The following are
    the core functionalities and technology options provided by the training, hosting,
    and shared services environments:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*第8章*中讨论的，*使用AWS ML服务构建数据科学环境*，数据科学家利用数据科学环境进行实验、模型构建和调整。一旦这些实验完成，数据科学家会将他们的工作提交到适当的代码和数据存储库。下一步是在受控和自动化的环境中使用数据科学家创建的算法、数据和训练脚本来训练和调整ML模型。这个受控和自动化的模型训练过程将有助于确保可扩展模型构建的一致性、可重复性和可追溯性。以下是由训练、托管和共享服务环境提供的核心功能和技术选项：
- en: '**Model training environment**: This environment manages the full lifecycle
    of model training, from computing and storage infrastructure resource provisioning
    to training job monitoring and model persistence. For this purpose, the SageMaker
    training service offers a suitable technology option to construct the training
    infrastructure.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练环境**：此环境管理模型训练的整个生命周期，从计算和存储基础设施资源分配到训练作业监控和模型持久化。为此，SageMaker训练服务提供了一个合适的技术选项来构建训练基础设施。'
- en: '**Model hosting environment**: This environment is used to serve the trained
    models behind web service endpoints or in batch inference mode. For this purpose,
    you can use the SageMaker hosting service for this environment. Other supporting
    services such as the online feature store and API management service can also
    run in the model hosting environment. There can be multiple model hosting environments
    for different stages. For example, you can have a testing hosting environment
    designated for model testing, and a production hosting environment for production
    model deployment serving real-world traffic. Model testers can perform different
    tests, such as model performance, robustness, bias, explainability analysis, and
    model hosting testing.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型托管环境**：此环境用于在Web服务端点或批量推理模式下提供训练好的模型。为此，你可以使用SageMaker托管服务。其他支持服务，如在线特征存储和API管理服务，也可以在模型托管环境中运行。可以存在多个模型托管环境，用于不同的阶段。例如，你可以有一个专门用于模型测试的测试托管环境，以及一个用于生产模型部署并服务于现实世界流量的生产托管环境。模型测试人员可以执行不同的测试，例如模型性能、鲁棒性、偏差、可解释性分析和模型托管测试。'
- en: '**Shared services environment**: The shared services environment hosts common
    services, tooling such as workflow orchestration tools, CI/CD tools, code repositories,
    Docker image repositories, and private library package tools. A central model
    registry can also run in the shared services environment for model registration
    and model lifecycle management. Service provisioning capabilities, such as creating
    resources in different environments through **Infrastructure as Code** (**IaC**)
    or APIs, also run out of this environment. Any service ticketing tools, such as
    ServiceNow, and service provisioning tools, such as Service Catalog, can also
    be hosted in this environment.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享服务环境**：共享服务环境托管常见服务，如工作流编排工具、CI/CD工具、代码仓库、Docker镜像仓库和私有库包工具。中央模型注册库也可以在共享服务环境中运行，用于模型注册和模型生命周期管理。服务提供能力，如通过**基础设施即代码**（**IaC**）或API在不同环境中创建资源，也运行在这个环境中。任何服务票证工具，如ServiceNow，以及服务提供工具，如服务目录，也可以托管在这个环境中。'
- en: 'In addition to the core ML environments, there are other supporting environments,
    such as security, governance, monitoring, and logging, that are required for designing
    and building enterprise ML platforms:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了核心的机器学习环境之外，还有其他支持环境，例如安全、治理、监控和日志记录，这些环境对于设计和构建企业级机器学习平台是必需的：
- en: '**Security and governance environment**: The security and governance environment
    centrally manages authentication services, user credentials, and data encryption
    keys. Security audit and reporting processes also run in this environment. Native
    AWS services, such as Amazon IAM, AWS KMS, and AWS Config, can be used for various
    security and governance functions. Any custom-built risk and governance tools
    can also be hosted in this environment to service AI risk/governance manager.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全和治理环境**：安全和治理环境集中管理身份验证服务、用户凭证和数据加密密钥。安全审计和报告流程也运行在这个环境中。可以用于各种安全和治理功能的原生AWS服务，如Amazon
    IAM、AWS KMS和AWS Config。任何定制的风险和治理工具也可以托管在这个环境中，以服务AI风险/治理经理。'
- en: '**Monitoring and logging environment**: The monitoring and logging environment
    centrally aggregates monitoring and logging data from other environments for further
    processing and reporting. Custom dashboarding and alerting mechanisms are normally
    developed to provide easy access to key metrics and alerts from the underlying
    monitoring and logging data.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控和日志环境**：监控和日志环境集中聚合来自其他环境的监控和日志数据，以进行进一步的处理和报告。通常开发定制的仪表板和警报机制，以便轻松访问底层监控和日志数据中的关键指标和警报。'
- en: Now that you have a comprehensive overview of the fundamental elements that
    constitute an enterprise ML platform, let’s delve deeper into specific core areas.
    It is important to recognize that there are various patterns and services available
    for constructing an ML platform on AWS.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对企业级机器学习平台构成的基本元素有了全面的了解，让我们更深入地探讨具体的核心领域。重要的是要认识到，在AWS上构建机器学习平台有各种模式和可用服务。
- en: Moreover, while *Figure 9.1* showcases distinct AWS environments (i.e., AWS
    accounts) to host different ML platform environments, organizations can also choose
    to combine some of the environments in a single AWS account as long as there are
    proper boundaries between different environments to ensure the isolation of infrastructure,
    process flow, and security control. Furthermore, organizations can create separate
    AWS accounts for hosting a specific environment for different users or groups.
    For instance, a large enterprise can choose to create one data science environment
    for each of the LoBs, or a separate production hosting environment based on either
    organizational structure or workload separation. In this chapter, we will focus
    mainly on exploring one of the enterprise patterns to build an efficient and scalable
    ML platform.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然*图9.1*展示了不同的AWS环境（即，AWS账户）以托管不同的机器学习平台环境，但组织也可以选择将某些环境组合在单个AWS账户中，只要不同环境之间存在适当的边界，以确保基础设施、流程和安全管理上的隔离。此外，组织可以为托管不同用户或组的特定环境创建单独的AWS账户。例如，大型企业可以选择为每个业务部门创建一个数据科学环境，或者基于组织结构或工作负载分离创建单独的生产托管环境。在本章中，我们将主要关注探索一种企业模式来构建高效且可扩展的机器学习平台。
- en: Model training environment
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练环境
- en: Within an enterprise, a model training environment is a controlled environment
    with well-defined processes and policies on how it is used and who can use it.
    Normally, it should be an automated environment that’s managed by an ML operations
    team, though self-service can be enabled for direct usage by data scientists.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业内部，模型训练环境是一个受控环境，具有关于其使用方式和谁可以使用它的明确流程和政策。通常，它应该是一个由机器学习运营团队管理的自动化环境，尽管可以启用自助服务以供数据科学家直接使用。
- en: Automated model training and tuning are the core capabilities of the model training
    environment. To support a broad range of use cases, a model training environment
    needs to support different ML and deep learning frameworks, training patterns
    (single-node and distributed training), and hardware (different CPUs, GPUs, and
    custom silicon chips).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化模型训练和调优是模型训练环境的核心功能。为了支持广泛的使用案例，模型训练环境需要支持不同的机器学习和深度学习框架、训练模式（单节点和分布式训练）以及硬件（不同的CPU、GPU和定制硅芯片）。
- en: The model training environment manages the lifecycle of the model training process.
    This can include authentication and authorization, infrastructure provisioning,
    data movement, data preprocessing, ML library deployment, training loop management
    and monitoring, model persistence and registry, training job management, and lineage
    tracking. From a security perspective, the training environment needs to provide
    security capabilities for different isolation requirements, such as network isolation,
    job isolation, and artifact isolation. To assist with operational support, a model
    training environment also needs to be able to support training status logging,
    metrics reporting, and training job monitoring and alerting. In the following
    sections, we will discuss how Amazon SageMaker can be used as a managed model
    training engine for enterprises.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练环境管理着模型训练过程的生命周期。这可以包括身份验证和授权、基础设施配置、数据移动、数据预处理、机器学习库部署、训练循环管理和监控、模型持久化和注册、训练作业管理和谱系跟踪。从安全角度来看，训练环境需要提供针对不同隔离需求的安全功能，例如网络隔离、作业隔离和工件隔离。为了协助运营支持，模型训练环境还需要能够支持训练状态日志记录、指标报告以及训练作业监控和警报。在接下来的章节中，我们将讨论如何使用Amazon
    SageMaker作为企业管理的模型训练引擎。
- en: Model training engine using SageMaker
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用SageMaker的模型训练引擎
- en: 'The SageMaker training service provides built-in model training capabilities
    for a range of ML/DL libraries. In addition, you can bring your own Docker containers
    for customized model training needs. The following are a subset of supported options
    for the SageMaker Python SDK:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker训练服务为各种机器学习/深度学习库提供了内置的模型训练功能。此外，您还可以为定制模型训练需求带来自己的Docker容器。以下是SageMaker
    Python SDK支持的选项子集：
- en: '**Training TensorFlow models**: SageMaker provides a built-in training container
    for TensorFlow models. The following code sample shows how to train a TensorFlow
    model using the built-in container through the TensorFlow estimator API:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练TensorFlow模型**：SageMaker为TensorFlow模型提供了内置的训练容器。以下代码示例展示了如何通过TensorFlow估计器API使用内置容器训练TensorFlow模型：'
- en: '[PRE0]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Training PyTorch models**: SageMaker provides a built-in training container
    for PyTorch models. The following code sample shows how to train a PyTorch model
    using the PyTorch estimator:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练PyTorch模型**：SageMaker为PyTorch模型提供内置训练容器。以下代码示例展示了如何使用PyTorch估计器训练PyTorch模型：'
- en: '[PRE1]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Training XGBoost models**: XGBoost training is also supported via a built-in
    container. The following code shows the syntax for training an XGBoost model using
    the XGBoost estimator:'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练XGBoost模型**：XGBoost训练也通过内置容器支持。以下代码展示了使用XGBoost估计器训练XGBoost模型的语法：'
- en: '[PRE2]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Training scikit-learn models**: The following code sample shows how to train
    a scikit-learn model using the built-in container:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用scikit-learn训练模型**：以下代码示例展示了如何使用内置容器训练scikit-learn模型：'
- en: '[PRE3]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Training models using custom containers**: You can also build a custom training
    container and use the SageMaker training service for model training. See the following
    code for an example:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用自定义容器训练模型**：您还可以构建自定义训练容器，并使用SageMaker训练服务进行模型训练。以下代码示例展示了如何进行：'
- en: '[PRE4]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In addition to using the SageMaker Python SDK to kick off training, you can
    also use the `boto3` library and SageMaker CLI commands to start training jobs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用SageMaker Python SDK启动训练外，您还可以使用`boto3`库和SageMaker CLI命令来启动训练作业。
- en: Automation support
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动化支持
- en: The SageMaker training service is exposed through a set of APIs and can be automated
    by integrating with external applications or workflow tools, such as SageMaker
    Pipelines, Airflow, and AWS Step Functions. For example, it can be one of the
    steps in an Airflow-based pipeline for an end-to-end ML workflow. Some workflow
    tools, such as Airflow and AWS Step Functions, also provide SageMaker-specific
    connectors to interact with the SageMaker training service more seamlessly. The
    SageMaker training service also provides Kubernetes operators, so it can be integrated
    and automated as part of the Kubernetes application flow.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker训练服务通过一组API进行暴露，可以通过与外部应用程序或工作流程工具集成来自动化，例如SageMaker Pipelines、Airflow和AWS
    Step Functions。例如，它可以是端到端机器学习工作流程中基于Airflow的管道的步骤之一。一些工作流程工具，如Airflow和AWS Step
    Functions，还提供了SageMaker特定的连接器，以便更无缝地与SageMaker训练服务交互。SageMaker训练服务还提供Kubernetes操作符，因此它可以作为Kubernetes应用程序流程的一部分进行集成和自动化。
- en: 'The following sample code shows how to kick off a training job using the low-level
    API via the AWS `boto3` SDK:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例代码展示了如何通过AWS `boto3` SDK的低级API启动训练作业：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Regarding using Airflow as the workflow tool, the following sample shows how
    to use the Airflow SageMaker operator as part of the workflow definition. Here,
    `train_config` contains training configuration details, such as the training estimator,
    training instance type and number, and training data location:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用Airflow作为工作流程工具，以下示例展示了如何在工作流程定义中将Airflow SageMaker操作符作为一部分使用。在这里，`train_config`包含训练配置细节，例如训练估计器、训练实例类型和数量以及训练数据位置：
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: SageMaker also has a built-in workflow automation tool called **SageMaker Pipelines**.
    A training step can be created using the SageMaker **Training Step** API and integrated
    into the larger SageMaker Pipelines workflow.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker还内置了一个名为**SageMaker Pipelines**的工作流程自动化工具。可以使用SageMaker **Training
    Step** API创建训练步骤并将其集成到更大的SageMaker Pipelines工作流程中。
- en: Model training lifecycle management
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型训练生命周期管理
- en: SageMaker training manages the lifecycle of the model training process. It uses
    Amazon IAM as the mechanism to authenticate and authorize access to its functions.
    Once authorized, it provides the desired infrastructure, deploys the software
    stacks for the different model training requirements, moves the data from sources
    to training nodes, and kicks off the training job. Once the training job has been
    completed, the model artifacts are saved into an S3 output bucket and the infrastructure
    is torn down. For lineage tracing, model training metadata such as source datasets,
    model training containers, hyperparameters, and model output locations are captured.
    Any logging from the training job runs is saved in CloudWatch Logs, and system
    metrics such as CPU and GPU utilization are captured in the CloudWatch metrics.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker训练管理模型训练过程的生命周期。它使用Amazon IAM作为验证和授权访问其功能的机制。一旦授权，它提供所需的架构，部署满足不同模型训练需求的软件堆栈，将数据从源移动到训练节点，并启动训练作业。一旦训练作业完成，模型工件将被保存到S3输出桶中，并且架构将被拆除。对于谱系追踪，捕获模型训练元数据，如源数据集、模型训练容器、超参数和模型输出位置。任何来自训练作业运行的日志都将保存到CloudWatch日志中，系统指标，如CPU和GPU利用率，将捕获到CloudWatch指标中。
- en: Depending on the overall end-to-end ML platform architecture, a model training
    environment can also host services for data preprocessing, model validation, and
    model training postprocessing, as those are important steps in an end-to-end ML
    flow. There are multiple technology options available for this, such as the SageMaker
    Processing service, AWS Glue, and AWS Lambda.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 根据整体端到端机器学习平台架构，模型训练环境也可以托管数据预处理、模型验证和模型训练后处理等服务，因为这些是端到端机器学习流程中的重要步骤。为此，有多种技术选项可用，例如SageMaker处理服务、AWS
    Glue和AWS Lambda。
- en: Model hosting environment
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型托管环境
- en: An enterprise-grade model hosting environment needs to support a broad range
    of ML frameworks in a secure, performant, and scalable way. It should come with
    a list of pre-built inference engines that can serve common models out of the
    box behind a **RESTful API** or via the **gRPC protocol**. It also needs to provide
    flexibility to host custom-built inference engines for unique requirements. Users
    should also have access to different hardware devices, such as CPU, GPU, and purpose-built
    chips, for different inference needs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一个企业级的模型托管环境需要以安全、高效和可扩展的方式支持广泛的机器学习框架。它应该附带一系列预构建的推理引擎，这些引擎可以支持常见的模型，通过**RESTful
    API**或**gRPC协议**直接提供。它还需要提供灵活性，以托管针对特定需求的定制构建的推理引擎。用户还应能够访问不同的硬件设备，如CPU、GPU和专用芯片，以满足不同的推理需求。
- en: Some model inference patterns demand more complex inference graphs, such as
    traffic split, request transformations, or model ensemble support. A model hosting
    environment can provide this capability as an out-of-the-box feature or provide
    technology options for building custom inference graphs. Other common model hosting
    capabilities include **concept drift detection** and **model performance drift
    detection**. Concept drift occurs when the statistical characteristics of the
    production data deviate from the data that’s used for model training. An example
    of concept drift is the mean and standard deviation of a feature changing significantly
    in production from that of the training dataset. Model performance drift happens
    when the accuracy of the model degrades in production.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型推理模式需要更复杂的推理图，例如流量分割、请求转换或模型集成支持。模型托管环境可以提供这种功能作为即插即用的特性，或者提供技术选项来构建自定义推理图。其他常见的模型托管功能包括**概念漂移检测**和**模型性能漂移检测**。当生产数据的统计特性与用于模型训练的数据不符时，就会发生概念漂移。概念漂移的一个例子是，在生产环境中，一个特征的平均值和标准差与训练数据集相比发生了显著变化。当模型在生产中的准确性下降时，就会发生模型性能漂移。
- en: Components in a model hosting environment can participate in an automation workflow
    through its API, scripting, or IaC deployment (such as AWS CloudFormation). For
    example, a RESTful endpoint can be deployed using a CloudFormation template or
    by invoking its API as part of an automated workflow.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 模型托管环境中的组件可以通过其API、脚本或IaC部署（如AWS CloudFormation）参与自动化工作流程。例如，可以使用CloudFormation模板部署RESTful端点，或者作为自动化工作流程的一部分调用其API。
- en: From a security perspective, the model hosting environment needs to provide
    authentication and authorization control to manage access to both the **control
    plane** (management functions) and the **data plane** (model endpoints). The accesses
    and operations that are performed against the hosting environments should be logged
    for auditing purposes. For operations support, a hosting environment needs to
    enable status logging and system monitoring to support system observability and
    problem troubleshooting.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从安全角度来看，模型托管环境需要提供身份验证和授权控制，以管理对**控制平面**（管理功能）和**数据平面**（模型端点）的访问。针对托管环境的访问和操作应记录下来，以供审计。为了支持操作支持，托管环境需要启用状态日志记录和系统监控，以支持系统可观察性和问题调试。
- en: The SageMaker hosting service is a fully managed model hosting service. Similar
    to KFServing and Seldon Core, which we reviewed earlier in this book, the SageMaker
    hosting service is also a multi-framework model-serving service. Next, let’s take
    a closer look at its various capabilities for enterprise-grade model hosting.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker托管服务是一个完全托管的模型托管服务。类似于我们在本书中之前审查的KFServing和Seldon Core，SageMaker托管服务也是一个多框架模型托管服务。接下来，我们将更详细地探讨其为企业级模型托管提供的各种功能。
- en: Inference engines
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理引擎
- en: 'SageMaker provides built-in inference engines for multiple ML frameworks, including
    scikit-learn, XGBoost, TensorFlow, PyTorch, and Spark ML. SageMaker supplies these
    built-in inference engines as Docker containers. To stand up an API endpoint to
    serve a model, you just need to provide the model artifacts and infrastructure
    configuration. The following is a list of model-serving options:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 为多个 ML 框架提供内置推理引擎，包括 scikit-learn、XGBoost、TensorFlow、PyTorch 和 Spark
    ML。SageMaker 以 Docker 容器形式提供这些内置推理引擎。为了搭建一个 API 端点来服务模型，您只需提供模型工件和基础设施配置。以下是一个模型服务选项列表：
- en: '**Serving TensorFlow models**: SageMaker uses TensorFlow Serving as the inference
    engine for TensorFlow models. The following code sample shows how to deploy a
    TensorFlow Serving model using the SageMaker hosting service where a TensorFlow
    model is loaded using the `Model` class from the S3 location and deployed as a
    SageMaker endpoint using the `deploy()` function with the specified compute instance
    type and number:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务 TensorFlow 模型**：SageMaker 使用 TensorFlow Serving 作为 TensorFlow 模型的推理引擎。以下代码示例展示了如何使用
    SageMaker 托管服务部署 TensorFlow Serving 模型，其中使用 `Model` 类从 S3 位置加载 TensorFlow 模型，并使用指定计算实例类型和数量的
    `deploy()` 函数将其部署为 SageMaker 端点：'
- en: '[PRE7]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Serving PyTorch models**: SageMaker hosting uses TorchServe under the hood
    to serve PyTorch models. The following code sample shows how to deploy a PyTorch
    model, which is very similar to the code for deploying TensorFlow models:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务 PyTorch 模型**：SageMaker 托管在底层使用 TorchServe 来服务 PyTorch 模型。以下代码示例展示了如何部署
    PyTorch 模型，其代码与部署 TensorFlow 模型的代码非常相似：'
- en: '[PRE8]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Serving Spark ML models**: For Spark ML-based models, SageMaker uses MLeap
    as the backend to serve Spark ML models. These Spark ML models need to be serialized
    into MLeap format so they can be used by the MLeap engine. The following code
    sample shows how to deploy a Spark ML model using the SageMaker hosting service
    where the `SparkMLModel` class is used to specify the model configuration and
    the `deploy()` function is used for the actual deployment into a SageMaker endpoint:'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务 Spark ML 模型**：对于基于 Spark ML 的模型，SageMaker 使用 MLeap 作为后端来服务 Spark ML 模型。这些
    Spark ML 模型需要序列化为 MLeap 格式，以便它们可以被 MLeap 引擎使用。以下代码示例展示了如何使用 SageMaker 托管服务部署 Spark
    ML 模型，其中使用 `SparkMLModel` 类指定模型配置，并使用 `deploy()` 函数将模型实际部署到 SageMaker 端点：'
- en: '[PRE9]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Serving XGBoost models**: SageMaker provides an XGBoost model server for
    serving trained XGBoost models. Under the hood, it uses Nginx, Gunicorn, and Flask
    as part of the model-serving architecture. The entry Python script loads the trained
    XGBoost model and can optionally perform pre- and post-data processing:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务 XGBoost 模型**：SageMaker 为训练好的 XGBoost 模型提供 XGBoost 模型服务器。在底层，它使用 Nginx、Gunicorn
    和 Flask 作为模型服务架构的一部分。入口 Python 脚本加载训练好的 XGBoost 模型，并可选择执行预处理和后处理数据：'
- en: '[PRE10]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Serving scikit-learn models**: SageMaker provides a built-in serving container
    for serving scikit-learn-based models. The technology stack is similar to the
    one for the XGBoost model server, which is also based on Nginx, Gunicorn, and
    Flask:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务 scikit-learn 模型**：SageMaker 为基于 scikit-learn 的模型提供内置服务容器。技术堆栈与 XGBoost
    模型服务器类似，也是基于 Nginx、Gunicorn 和 Flask：'
- en: '[PRE11]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Serving models with custom containers**: For custom-created inference containers,
    you can follow a similar syntax to deploy the model. The main difference is that
    a custom inference container image’s uri must be provided to specify the custom
    container location. You can find detailed documentation on building a custom inference
    container at [https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html):'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用自定义容器服务模型**：对于自定义创建的推理容器，您可以遵循类似的语法来部署模型。主要区别是必须提供自定义推理容器镜像的 uri，以指定自定义容器的位置。您可以在
    [https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html)
    找到有关构建自定义推理容器的详细文档：'
- en: '[PRE12]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: SageMaker hosting provides an inference pipeline feature that allows you to
    create a linear sequence of containers to perform custom data processing before
    and after invoking a model for predictions. SageMaker hosting can support traffic
    splits between multiple versions of a model for A/B testing.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 托管提供推理管道功能，允许您创建一个线性序列的容器，在调用模型进行预测前后执行自定义数据处理。SageMaker 托管可以支持在模型多个版本之间进行流量拆分，以进行
    A/B 测试。
- en: 'SageMaker hosting can be provisioned using an AWS CloudFormation template.
    There is also support for the AWS CLI for scripting automation, and it can be
    integrated into custom applications via its API. The following are some code samples
    for different endpoint deployment automation methods:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 AWS CloudFormation 模板来配置 SageMaker 托管。同时，也支持 AWS CLI 用于脚本自动化，并且可以通过其 API
    集成到自定义应用程序中。以下是一些不同端点部署自动化方法的代码示例：
- en: 'The following is a CloudFormation code sample for SageMaker endpoint deployment.
    In this code sample, you specify the compute instance, the number of instances,
    the model name, and the model-serving container used to host the model. You can
    find the complete code at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter09/sagemaker_hosting.yaml](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter09/sagemaker_hosting.yaml):'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是一个用于 SageMaker 端点部署的 CloudFormation 代码示例。在这个代码示例中，你指定了计算实例、实例数量、模型名称以及用于托管模型的模型服务容器。你可以在这里找到完整的代码：[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter09/sagemaker_hosting.yaml](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter09/sagemaker_hosting.yaml)
- en: '[PRE13]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following is an AWS CLI sample for SageMaker endpoint deployment, which
    consists of three main steps: creating a model, specifying a SageMaker endpoint
    configuration, and the actual deployment of the model into a SageMaker endpoint:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是一个用于 SageMaker 端点部署的 AWS CLI 示例，它包括三个主要步骤：创建模型、指定 SageMaker 端点配置以及将模型实际部署到
    SageMaker 端点：
- en: '[PRE14]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If the built-in inference engines do not meet your requirements, you should
    consider bringing your own Docker container to serve your ML models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果内置的推理引擎不符合你的要求，你应该考虑引入自己的 Docker 容器来托管你的机器学习模型。
- en: Authentication and security control
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 认证和安全控制
- en: The SageMaker hosting service uses AWS IAM as the mechanism to control access
    to its control plane APIs (for example, an API for creating an endpoint) and data
    plane APIs (for example, an API for invoking a hosted model endpoint). If you
    need to support other authentication methods for the data plane API, such as **OpenID
    Connect** (**OIDC**), you can implement a proxy service as the frontend to manage
    user authentication. A common pattern is to use AWS API Gateway to frontend the
    SageMaker API for custom authentication management, as well as other API management
    features such as metering and throttling management.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 托管服务使用 AWS IAM 作为控制平面 API（例如，创建端点的 API）和数据平面 API（例如，调用托管模型端点的 API）的访问控制机制。如果你需要支持数据平面
    API 的其他认证方法，例如 **OpenID Connect**（**OIDC**），你可以实现一个代理服务作为前端来管理用户认证。一种常见的模式是使用
    AWS API Gateway 作为 SageMaker API 的前端，以实现自定义认证管理以及其他 API 管理功能，如计费和节流管理。
- en: Monitoring and logging
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控和日志记录
- en: SageMaker provides out-of-the-box monitoring and logging capabilities to assist
    with support operations. It monitors both system resource metrics (for example,
    CPU/GPU utilization) and model invocation metrics (for example, the number of
    invocations, model latencies, and failures). These monitoring metrics and any
    model processing logs are captured by AWS CloudWatch metrics and CloudWatch Logs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 提供了开箱即用的监控和日志记录功能，以协助支持操作。它监控系统资源指标（例如，CPU/GPU 利用率）和模型调用指标（例如，调用次数、模型延迟和失败）。这些监控指标以及任何模型处理日志都由
    AWS CloudWatch 指标和 CloudWatch 日志捕获。
- en: Now that we have covered the training, inference, security, and monitoring aspects
    of an ML platform, we will dive into implementing MLOps to automate ML workflows
    next.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了机器学习平台训练、推理、安全和监控方面的内容，接下来我们将深入探讨如何实现 MLOps 以自动化机器学习工作流程。
- en: Adopting MLOps for ML workflows
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采用 MLOps 进行机器学习工作流程
- en: 'Similar to the DevOps practice, which has been widely adopted for the traditional
    software development and deployment process, the MLOps practice is intended to
    streamline the building and deployment processes of ML pipelines while enhancing
    the collaborations between data scientists/ML engineers, data engineering, and
    the operations team. Specifically, the primary objective of MLOps practice is
    to yield the following main benefits throughout the entire ML lifecycle:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与广泛采用于传统软件开发和部署过程的DevOps实践类似，MLOps实践旨在简化机器学习管道的构建和部署流程，同时增强数据科学家/机器学习工程师、数据工程和运维团队之间的协作。具体来说，MLOps实践的主要目标是在整个机器学习生命周期中产生以下主要好处：
- en: '**Process consistency**: The MLOps practice aims to create consistency in the
    ML model-building and deployment process. A consistent process improves the efficiency
    of the ML workflow and ensures a high degree of certainty in the input and output
    of the ML workflow.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流程一致性**：MLOps实践旨在在机器学习模型构建和部署过程中创建一致性。一致的流程可以提高机器学习工作流的效率，并确保机器学习工作流的输入和输出具有高度的确定性。'
- en: '**Tooling and process reusability**: One of the core objectives of the MLOps
    practice is to create reusable technology tooling and templates for faster adoption
    and deployment of new ML use cases. These can include common tools such as code
    and library repositories, package and image building tools, pipeline orchestration
    tools, the model registry, as well as common infrastructure for model training
    and model deployment. From a reusable template perspective, these can include
    common reusable scripts for Docker image builds, workflow orchestration definitions,
    and CloudFormation scripts for model building and model deployment.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具和流程可重用性**：MLOps实践的核心目标之一是创建可重用的技术工具和模板，以加快新机器学习用例的采用和部署。这可以包括常见的工具，如代码和库存储库、打包和镜像构建工具、管道编排工具、模型注册表以及模型训练和模型部署的通用基础设施。从可重用模板的角度来看，这可以包括用于Docker镜像构建的通用可重用脚本、工作流编排定义以及用于模型构建和模型部署的CloudFormation脚本。'
- en: '**Model-building reproducibility**: ML is highly iterative and can involve
    a large number of experimentations and model training runs using different datasets,
    algorithms, and hyperparameters. An MLOps process needs to capture all the data
    inputs, source code, and artifacts that are used to build an ML model and establish
    model lineage from this input data, code, and artifacts for the final models.
    This is important for both experiment tracking as well as governance and control
    purposes.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型构建可重复性**：机器学习高度迭代，可能涉及大量使用不同数据集、算法和超参数的实验和模型训练运行。MLOps流程需要捕获用于构建机器学习模型的所有数据输入、源代码和工件，并从这些输入数据、代码和工件中建立最终模型的模型血缘。这对于实验跟踪以及治理和控制目的都至关重要。'
- en: '**Delivery scalability**: An MLOps process and the associated tooling enable
    a large number of ML pipelines to run in parallel for high delivery throughputs.
    Different ML project teams can use the standard MLOps processes and common tools
    independently without creating conflicts from a resource contention, environment
    isolation, and governance perspective.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交付可扩展性**：MLOps流程和相关工具能够并行运行大量机器学习管道，以实现高吞吐量的交付。不同的机器学习项目团队可以独立使用标准的MLOps流程和通用工具，而不会因资源竞争、环境隔离和治理方面的冲突而产生冲突。'
- en: '**Process and operation auditability**: MLOps enables greater audibility in
    the process and the auditability of ML pipelines. This includes capturing the
    details of machine pipeline executions, dependencies, and lineage across different
    steps, job execution statuses, model training and deployment details, approval
    tracking, and actions that are performed by human operators.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流程和操作可审计性**：MLOps使流程和机器学习管道的可审计性更高。这包括捕获机器管道执行的详细信息、不同步骤之间的依赖关系和血缘、作业执行状态、模型训练和部署细节、审批跟踪以及由人工操作员执行的操作。'
- en: Now that we are familiar with the intended goals and benefits of the MLOps practice,
    let’s delve into the specific operational process and concrete technology architecture
    of MLOps on AWS.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了MLOps实践的预期目标和好处，让我们深入了解AWS上MLOps的具体操作流程和具体技术架构。
- en: Components of the MLOps architecture
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLOps架构的组件
- en: One of the most important MLOps concepts is the automation pipeline, which executes
    a sequence of tasks, such as data processing, model training, and model deployment.
    This pipeline can be a linear sequence of steps or a more complex **directed acyclic
    graph** (**DAG**) with parallel execution for multiple tasks. The following diagram
    illustrates a sample DAG for an ML pipeline.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps 最重要的概念之一是自动化管道，它执行一系列任务，如数据处理、模型训练和模型部署。这个管道可以是步骤的线性序列，也可以是一个更复杂的**有向无环图**（**DAG**），支持多个任务的并行执行。以下图表展示了
    ML 管道的示例 DAG。
- en: '![A diagram of steps to a model  Description automatically generated](img/B20836_09_02.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![模型步骤描述自动生成图](img/B20836_09_02.png)'
- en: 'Figure 9.2: Sample ML pipeline flow'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：样本 ML 管道流程
- en: 'An MLOps architecture also has several repositories for storing different assets
    and metadata as part of pipeline executions. The following diagram lists the core
    components and tasks involved in an MLOps operation:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps 架构还包括几个存储库，用于存储在管道执行过程中不同资产和元数据。以下图表列出了 MLOps 操作中的核心组件和任务：
- en: '![Figure 9.2 – MLOps components ](img/B20836_09_03.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – MLOps 组件](img/B20836_09_03.png)'
- en: 'Figure 9.3: MLOps components'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：MLOps 组件
- en: A **code repository** is an MLOps architecture component that not only serves
    as a source code control mechanism for data scientists and engineers – it can
    also be the triggering mechanism to kick off different pipeline executions. For
    example, when a data scientist checks an updated training script into the code
    repository, a model training pipeline execution can be triggered.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码存储库**是 MLOps 架构组件，它不仅作为数据科学家和工程师的源代码控制机制，还可以作为触发不同管道执行的机制。例如，当数据科学家将更新的训练脚本检查到代码存储库中时，可以触发模型训练管道执行。'
- en: A **feature repository** stores reusable ML features and can be the target of
    a data processing/feature engineering job. The features from the feature repository
    can be a part of the training datasets where applicable. The feature repository
    is also used as a part of the model inference request.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征存储库**存储可重用的 ML 特征，可以是数据处理/特征工程作业的目标。特征存储库中的特征可以是适用情况下的训练数据集的一部分。特征存储库还用作模型推理请求的一部分。'
- en: A **container repository** stores the container images that are used for data
    processing tasks, model training jobs, and model inference engines. It is usually
    the target of the container-building pipeline.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器存储库**存储用于数据处理任务、模型训练作业和模型推理引擎的容器镜像。它通常是容器构建管道的目标。'
- en: A **model registry** keeps an inventory of trained models, along with all the
    metadata associated with the model, such as its algorithm, hyperparameters, model
    metrics, and training dataset location. It also maintains the status of the model
    lifecycle, such as its deployment approval status.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型注册库**保存训练模型的清单，以及与模型相关的所有元数据，例如其算法、超参数、模型指标和训练数据集位置。它还维护模型生命周期的状态，例如其部署批准状态。'
- en: A **pipeline repository** maintains the definition of automation pipelines and
    the statuses of different pipeline job executions.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道存储库**维护自动化管道的定义和不同管道作业执行的状态。'
- en: In an enterprise setting, a task ticket also needs to be created when different
    tasks, such as model deployment, are performed, so that these actions can be tracked
    in a common enterprise ticketing management system. To support audit requirements,
    the lineage of different pipeline tasks and their associated artifacts need to
    be tracked.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业环境中，当执行不同任务，如模型部署时，也需要创建任务票据，以便这些操作可以在一个共同的票据管理系统中被追踪。为了支持审计要求，需要追踪不同管道任务及其相关工件的历史。
- en: Another critical component of the MLOps architecture is **monitoring**. In general,
    you want to monitor items such as the pipeline’s execution status, model training
    status, and model endpoint status. Model endpoint monitoring can also include
    system/resource performance monitoring, model statistical metrics monitoring,
    drift and outlier monitoring, and model explainability monitoring. Alerts can
    be triggered on certain execution statuses to invoke human or automation actions
    that are needed.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps 架构的另一个关键组件是**监控**。通常，您希望监控管道的执行状态、模型训练状态和模型端点状态。模型端点监控还可以包括系统/资源性能监控、模型统计指标监控、漂移和异常监控以及模型可解释性监控。可以在某些执行状态上触发警报，以调用所需的人类或自动化操作。
- en: 'AWS provides multiple technology options for implementing an MLOps architecture.
    The following diagram shows where these technology services fit in an enterprise
    MLOps architecture:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: AWS为实施MLOps架构提供了多种技术选项。以下图表显示了这些技术服务在企业MLOps架构中的位置：
- en: '![Figure 9.3 – MLOps architecture using AWS services  ](img/B20836_09_04.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 使用AWS服务的MLOps架构](img/B20836_09_04.png)'
- en: 'Figure 9.4: MLOps architecture using AWS services'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.4**：使用AWS服务的MLOps架构'
- en: As we mentioned earlier, the shared service environment hosts common tools for
    pipeline management and execution, as well as common repositories such as code
    repositories and model registries.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，共享服务环境托管了用于流水线管理和执行的常用工具，以及如代码仓库和模型注册表等常用仓库。
- en: Here, we use AWS CodePipeline to orchestrate the overall CI/CD pipeline. AWS
    CodePipeline is a continuous delivery service. We use this service here as it
    integrates natively with different code repositories such as AWS CodeCommit, GitHub
    repos, and Bitbucket.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用AWS CodePipeline来编排整体的CI/CD流水线。AWS CodePipeline是一个持续交付服务。我们在这里使用此服务，因为它可以与不同的代码仓库如AWS
    CodeCommit、GitHub仓库和Bitbucket等原生集成。
- en: 'It can source files from the code repository and make them available to downstream
    tasks such as building containers using the AWS CodeBuild service, or training
    models in the model training environment. You can create different pipelines to
    meet different needs. A pipeline can be triggered on-demand via an API or the
    CodePipeline management console, or it can be triggered by code changes in a code
    repository. Depending on your requirements, you can create different pipelines.
    In the preceding diagram, we can see four example pipelines:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以从代码仓库中提取文件，并将它们提供给下游任务，例如使用AWS CodeBuild服务构建容器或训练模型训练环境中的模型。您可以根据不同的需求创建不同的流水线。流水线可以通过API或CodePipeline管理控制台按需触发，或者可以通过代码仓库中的代码更改触发。根据您的需求，您可以创建不同的流水线。在先前的图表中，我们可以看到四个示例流水线：
- en: A container build pipeline for building different container images for training,
    processing, and inference
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于构建用于训练、处理和推理的不同容器镜像的容器构建流水线
- en: A model training pipeline for training a model for release
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于为发布训练模型的模型训练流水线
- en: A model deployment pipeline for deploying trained models to production
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于将训练好的模型部署到生产的模型部署流水线
- en: A development, training, and testing pipeline for model training and deployment
    testing in a data science environment
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于在数据科学环境中进行模型训练和部署测试的开发、训练和测试流水线
- en: Note that while *Figure 9.4* only showcases four distinct pipelines, in reality,
    organizations can have many more pipelines based on specific requirements. Moreover,
    they can run multiple instances of the same pipeline in parallel to accommodate
    various ML projects. For instance, different instances of training job pipelines
    may run independently and concurrently, each dedicated to training distinct ML
    models using different datasets and configurations.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然**图9.4**只展示了四个不同的流水线，但实际上，根据具体需求，组织可以有更多流水线。此外，它们可以并行运行相同流水线的多个实例，以适应各种机器学习项目。例如，不同的训练作业流水线实例可以独立且并发运行，每个实例都专门用于使用不同的数据集和配置训练不同的机器学习模型。
- en: A code repository is one of the most essential components in an MLOps environment.
    It is not only used by data scientists/ML engineers and other engineers to persist
    code artifacts, but it also serves as a triggering mechanism for a CI/CD pipeline.
    This means that when a data scientist/ML engineer commits a code change, it can
    automatically kick off a CI/CD pipeline. For example, if the data scientist makes
    a change to the model training script and wants to test the automated training
    pipeline in the development environment, they can commit the code to a development
    branch to kick off a model training pipeline in the dev environment. When it is
    ready for production release deployment, the data scientist can commit/merge the
    code to a release branch to kick off the production release pipelines.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 代码仓库是MLOps环境中最重要的组件之一。它不仅被数据科学家/机器学习工程师和其他工程师用于持久化代码工件，而且还作为CI/CD流水线的触发机制。这意味着当数据科学家/机器学习工程师提交代码更改时，它可以自动启动CI/CD流水线。例如，如果数据科学家修改了模型训练脚本并希望在开发环境中测试自动化的训练流水线，他们可以将代码提交到开发分支以在开发环境中启动模型训练流水线。当它准备进行生产发布部署时，数据科学家可以将/合并代码到发布分支以启动生产发布流水线。
- en: 'In a nutshell, in the MLOps architecture in *Figure 9.4*, we use:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在**图9.4**的MLOps架构中，我们使用了：
- en: Amazon **Elastic Container Registry** (**ECR**) as the central container registry
    service. ECR is used to store containers for data processing, model training,
    and model inference. You can tag the container images to indicate different lifecycle
    statuses, such as development or production.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊**弹性容器注册库**（**ECR**）作为中央容器注册服务。ECR用于存储数据处理、模型训练和模型推理的容器。您可以为容器镜像添加标签，以指示不同的生命周期状态，如开发或生产。
- en: '**SageMaker Model Registry** as the central model repository. The central model
    repository can reside in the shared service environment, so it can be accessed
    by different projects. All the models that go through the formal training and
    deployment cycles should be managed and tracked in the central model repository.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker模型注册库**作为中央模型仓库。中央模型仓库可以位于共享服务环境中，因此可以被不同的项目访问。所有经过正式训练和部署周期的模型都应该在中央模型仓库中进行管理和跟踪。'
- en: '**SageMaker Feature Store** provides a common feature repository for reusable
    features to be used by different projects. It can reside in the shared services
    environment or be part of the data platform. Features are normally pre-calculated
    in a data management environment and sent to SageMaker Feature Store for offline
    model training in the model training environment, as well as online inferences
    by the different model hosting environments.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker特征存储**提供了一个通用的特征仓库，供不同项目重用。它可以位于共享服务环境中，或成为数据平台的一部分。特征通常在数据管理环境中预先计算，并发送到SageMaker特征存储，以便在模型训练环境中进行离线模型训练，以及不同模型托管环境中的在线推理。'
- en: Monitoring and logging
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控和日志记录
- en: The ML platform presents some unique challenges in terms of monitoring. In addition
    to monitoring common software system-related metrics and statuses, such as infrastructure
    utilization and processing status, an ML platform also needs to monitor model,
    and data-specific metrics and performances. Also, unlike traditional system-level
    monitoring, which is fairly straightforward to understand, the opaqueness of ML
    models makes it inherently difficult to understand the system. Now, let’s take
    a closer look at the three main areas of monitoring for an ML platform.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在监控方面，机器学习平台带来了一些独特的挑战。除了监控常见的软件系统相关指标和状态，如基础设施利用率和处理状态外，机器学习平台还需要监控模型和数据特定的指标和性能。此外，与传统的系统级监控相比，其直观性较强，而机器学习模型的透明度使得理解系统本身变得固有困难。现在，让我们更深入地了解机器学习平台监控的三个主要领域。
- en: Model training monitoring
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型训练监控
- en: 'Model training monitoring provides visibility into the training progress and
    helps identify training bottlenecks and error conditions during the training process.
    It enables operational processes such as training job progress reporting and response,
    model training performance progress evaluation and response, training problem
    troubleshooting, and data and model bias detection and model interpretability
    and response. Specifically, we want to monitor the following key metrics and conditions
    during model training:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练监控提供了对训练进度的可见性，并有助于在训练过程中识别训练瓶颈和错误条件。它使操作流程，如训练作业进度报告和响应、模型训练性能进度评估和响应、训练问题故障排除以及数据和模型偏差检测以及模型可解释性和响应成为可能。具体来说，我们希望在模型训练期间监控以下关键指标和条件：
- en: '**General system and resource utilization and error metrics**: These provide
    visibility into how the infrastructure resources (such as CPU, GPU, disk I/O,
    and memory) are utilized for model training. These can help with making decisions
    on provisioning infrastructure for the different model training needs.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用系统和资源利用及错误指标**：这些指标提供了对基础设施资源（如CPU、GPU、磁盘I/O和内存）在模型训练中如何被利用的可见性。这些指标可以帮助您在满足不同模型训练需求时做出基础设施配置的决策。'
- en: '**Training job events and status**: This provides visibility into the progress
    of a training job, such as a job starting and running, its completion, and failure
    details.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练作业事件和状态**：这提供了对训练作业进度的可见性，例如作业的开始和运行、完成以及失败详情。'
- en: '**Model training metrics**: These are model training metrics such as the loss
    curve and accuracy reports to help you understand the model’s performance.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练指标**：这些是模型训练指标，例如损失曲线和准确率报告，有助于您了解模型的表现。'
- en: '**Bias detection metrics and model explainability reporting**: These metrics
    help you understand if there is any bias in the training datasets or ML models.
    Model explainability can also be monitored and reported to help you understand
    high-importance features versus low-importance features.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差检测指标和模型可解释性报告**：这些指标有助于您了解训练数据集或机器学习模型中是否存在偏差。模型可解释性也可以被监控和报告，以帮助您理解重要特征与不重要特征之间的区别。'
- en: '**Model training bottlenecks and training issues**: These provide visibility
    into training issues such as vanishing gradients, poor weight initialization,
    and overfitting to help determine the required data and algorithmic and training
    configuration changes. Metrics such as CPU and I/O bottlenecks, uneven load balancing,
    and low GPU utilization can help determine infrastructure configuration changes
    for more efficient model training.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练瓶颈和训练问题**：这些提供了对训练问题（如梯度消失、权重初始化不良和过拟合）的可见性，以帮助确定所需的数据和算法以及训练配置更改。例如，CPU和I/O瓶颈、负载不均和低GPU利用率等指标可以帮助确定更高效模型训练的基础设施配置更改。'
- en: 'There are multiple native AWS services for building out a model monitoring
    architecture on AWS. The following diagram shows an example architecture for building
    a monitoring solution for a SageMaker-based model training environment:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供了多个本地服务，用于在AWS上构建模型监控架构。以下图显示了为基于SageMaker的模型训练环境构建监控解决方案的示例架构：
- en: '![Figure 9.4 – Model training monitoring architecture  ](img/B20836_09_05.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – 模型训练监控架构](img/B20836_09_05.png)'
- en: 'Figure 9.5: Model training monitoring architecture'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：模型训练监控架构
- en: This architecture lets you monitor training and system metrics and perform log
    capture and processing, training event capture and processing, and model training
    bias and explainability reporting. It helps enable operation processes, such as
    training progress and status reporting, model metric evaluation, system resource
    utilization reporting and response, training problem troubleshooting, bias detection,
    and model decision explainability.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此架构允许您监控训练和系统指标，并执行日志捕获和处理、训练事件捕获和处理以及模型训练偏差和可解释性报告。它有助于启用操作流程，如训练进度和状态报告、模型指标评估、系统资源利用率报告和响应、训练问题故障排除、偏差检测和模型决策可解释性。
- en: During model training, SageMaker can emit model training metrics, such as training
    loss and accuracy, to AWS CloudWatch to help with model training evaluation. AWS
    CloudWatch is the AWS monitoring and observability service. It collects metrics
    and logs from other AWS services and provides dashboards for visualizing and analyzing
    these metrics and logs. System utilization metrics (such as CPU/GPU/memory utilization)
    are also reported to CloudWatch for analysis to help you understand any infrastructure
    constraints or under-utilization. CloudWatch alarms can be created for a single
    metric or composite metrics to automate notifications or responses. For example,
    you can create alarms on low CPU/GPU utilization to help proactively identify
    sub-optimal hardware configuration for the training job. Also, when an alarm is
    triggered, it can send automated notifications (such as SMS and emails) to support
    for review via AWS **Simple Notification Service** (**SNS**).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练期间，SageMaker可以向AWS CloudWatch发出模型训练指标，如训练损失和准确率，以帮助进行模型训练评估。AWS CloudWatch是AWS监控和可观察性服务。它收集来自其他AWS服务的指标和日志，并提供仪表板以可视化和分析这些指标和日志。系统利用率指标（如CPU/GPU/内存利用率）也报告给CloudWatch进行分析，以帮助您了解任何基础设施限制或低利用率。可以为单个指标或复合指标创建CloudWatch警报，以自动通知或响应。例如，您可以为低CPU/GPU利用率创建警报，以帮助主动识别训练作业的次优硬件配置。此外，当警报被触发时，它可以通过AWS
    **简单通知服务**（**SNS**）发送自动通知（如短信和电子邮件），以支持通过AWS进行审查。
- en: You can use CloudWatch Logs to collect, monitor, and analyze the logs that are
    emitted by your training jobs. You can use these captured logs to understand the
    progress of your training jobs and identify errors and patterns to help troubleshoot
    any model training problems. For example, CloudWatch Logs might contain errors
    such as insufficient GPU memory to run model training or permission issues when
    accessing specific resources to help you troubleshoot model training problems.
    By default, CloudWatch Logs provides a UI tool called CloudWatch Logs Insights
    for interactively analyzing logs using a purpose-built query language. Alternatively,
    these logs can also be forwarded to an Elasticsearch cluster for analysis and
    querying. These logs can be aggregated in a designated logging and monitoring
    account to centrally manage log access and analysis.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用CloudWatch日志收集、监控和分析训练作业发出的日志。您可以使用这些捕获的日志来了解训练作业的进度，并识别错误和模式，以帮助解决任何模型训练问题。例如，CloudWatch日志可能包含错误，如模型训练时GPU内存不足或访问特定资源时的权限问题，以帮助您解决模型训练问题。默认情况下，CloudWatch日志提供了一个名为CloudWatch日志洞察的UI工具，用于使用专门构建的查询语言交互式分析日志。或者，这些日志也可以转发到Elasticsearch集群进行分析和查询。这些日志可以在指定的日志和监控账户中聚合，以集中管理日志访问和分析。
- en: SageMaker training jobs can also send events, such as a training job status
    changing from running to complete. You can create automated notification and response
    mechanisms based on these different events. For example, you can send out notifications
    to data scientists when a training job has either completed successfully or failed,
    along with a failure reason. You can also automate responses to these failures
    to the different statuses, such as model retraining on a particular failure condition.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker训练作业还可以发送事件，例如训练作业状态从运行变为完成。您可以根据这些不同的事件创建自动通知和响应机制。例如，当训练作业成功完成或失败时，您可以向数据科学家发送通知，包括失败原因。您还可以自动对不同状态（如特定失败条件下的模型重新训练）的失败做出响应。
- en: The SageMaker Clarify component can detect data and model bias and provide model
    explainability reporting on the trained model. You can access bias and model explainability
    reports inside the SageMaker Studio UI or SageMaker APIs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Clarify组件可以检测数据和模型偏差，并提供训练模型的模型可解释性报告。您可以在SageMaker Studio UI或SageMaker
    API中访问偏差和模型可解释性报告。
- en: Model endpoint monitoring
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型端点监控
- en: 'Model endpoint monitoring provides visibility into the performance of the model
    serving infrastructure, as well as model-specific metrics such as data drift,
    model drift, and inference explainability. The following are some of the key metrics
    for model endpoint monitoring:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 模型端点监控提供了对模型服务基础设施性能的可见性，以及模型特定的指标，如数据漂移、模型漂移和推理可解释性。以下是一些模型端点监控的关键指标：
- en: '**General system and resource utilization and error metrics**: These provide
    visibility into how the infrastructure resources (such as CPU, GPU, and memory)
    are utilized for model servicing. They can help with making decisions on provisioning
    infrastructure for the different model-serving needs.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用系统和资源利用及错误指标**：这些指标提供了对基础设施资源（如CPU、GPU和内存）用于模型服务利用情况的可见性。它们可以帮助您在为不同的模型服务需求提供基础设施时做出决策。'
- en: '**Data statistics monitoring metrics**: The statistical nature of data could
    change over time, which can result in degraded ML model performance from the original
    benchmarks. These metrics can include basic statistics deviations such as mean
    and standard changes, as well as data distribution changes.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据统计监控指标**：数据的统计性质可能会随时间变化，这可能导致从原始基准测试中降级ML模型性能。这些指标可以包括基本统计偏差，如均值和标准差的变化，以及数据分布的变化。'
- en: '**Model quality monitoring metrics**: These model quality metrics provide visibility
    into model performance deviation from the original benchmark. These metrics can
    include regression metrics (such as MAE and RMSE) and classification metrics (such
    as confusion matrix, F1, precision, recall, and accuracy).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型质量监控指标**：这些模型质量指标提供了对模型性能偏离原始基准的可见性。这些指标可以包括回归指标（如MAE和RMSE）和分类指标（如混淆矩阵、F1、精确率、召回率和准确率）。'
- en: '**Model inference explainability**: This provides model explainability on a
    per-prediction basis to help you understand what features had the most influence
    on the decision that was made by the prediction.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型推理可解释性**：这为每个预测提供了模型可解释性，以帮助您了解哪些特征对预测所做的决策影响最大。'
- en: '**Model bias monitoring metrics**: Similar to bias detection for training,
    the bias metrics help us understand model bias at inference time.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型偏差监控指标**：类似于训练过程中的偏差检测，偏差指标帮助我们理解推理时的模型偏差。'
- en: 'The model monitoring architecture relies on many of the same AWS services,
    including CloudWatch, EventBridge, and SNS. The following diagram shows an architecture
    pattern for a SageMaker-based model monitoring solution:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控架构依赖于许多相同的 AWS 服务，包括 CloudWatch、EventBridge 和 SNS。以下图表显示了基于 SageMaker 的模型监控解决方案的架构模式：
- en: '![Figure 9.5 – Model endpoint monitoring architecture ](img/B20836_09_06.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – 模型端点监控架构](img/B20836_09_06.png)'
- en: 'Figure 9.6: Model endpoint monitoring architecture'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：模型端点监控架构
- en: This architecture works similarly to the model training architecture. **CloudWatch
    metrics** capture endpoint metrics such as CPU/GPU utilization, model invocation
    metrics (number of invocations and errors), and model latencies. These metrics
    help with operations such as hardware optimization and endpoint scaling.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构的工作原理与模型训练架构类似。**CloudWatch 指标**捕获端点指标，例如 CPU/GPU 利用率、模型调用指标（调用次数和错误数）以及模型延迟。这些指标有助于硬件优化和端点扩展等操作。
- en: '**CloudWatch Logs** captures logs that are emitted by the model-serving endpoint
    to help us understand the status and troubleshoot technical problems. Similarly,
    endpoint events, such as the status changing from **Creating** to **InService**,
    can help you build automated notification pipelines to kick off corrective actions
    or provide status updates.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**CloudWatch 日志**捕获由模型服务端点发出的日志，帮助我们了解状态和解决技术问题。同样，端点事件，例如状态从 **创建中** 变为 **服务中**，可以帮助您构建自动通知管道以启动纠正措施或提供状态更新。'
- en: In addition to system- and status-related monitoring, this architecture also
    supports data and model-specific monitoring through a combination of SageMaker
    Model Monitor and SageMaker Clarify. Specifically, SageMaker Model Monitor can
    help you monitor data drift and model quality.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 除了系统和相关状态监控之外，此架构还通过结合 SageMaker 模型监控器和 SageMaker Clarify 支持数据和模型特定的监控。具体来说，SageMaker
    模型监控器可以帮助您监控数据漂移和模型质量。
- en: For data drift, SageMaker Model Monitor can use the training dataset to create
    baseline statistics metrics such as standard deviation, mean, max, min, and data
    distribution for the dataset features. It uses these metrics and other data characteristics,
    such as data types and completeness, to establish constraints. Then, it captures
    the input data in the production environment, calculates the metrics, compares
    them with the baseline metrics/constraints, and reports baseline drifts. Model
    Monitor can also report data quality issues such as incorrect data types and missing
    values. Data drift metrics can be sent to CloudWatch metrics for visualization
    and analysis, and CloudWatch alarms can be configured to trigger a notification
    or automated response when a metric crosses a predefined threshold.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据漂移，SageMaker 模型监控器可以使用训练数据集创建基线统计指标，例如标准差、平均值、最大值、最小值和数据集特征的数据分布。它使用这些指标和其他数据特征，如数据类型和完整性，来建立约束。然后，它捕获生产环境中的输入数据，计算指标，将它们与基线指标/约束进行比较，并报告基线漂移。模型监控器还可以报告数据质量问题，例如数据类型错误和缺失值。数据漂移指标可以发送到
    CloudWatch 指标进行可视化和分析，并且可以配置 CloudWatch 警报，在指标超过预定义阈值时触发通知或自动响应。
- en: 'For model quality monitoring, it creates baseline metrics (such as MAE for
    regression and accuracy for classification) using the baseline dataset, which
    contains both predictions and true labels. Then, it captures the predictions in
    production, ingests ground-truth labels, and merges the ground truth with the
    predictions to calculate various regression and classification metrics before
    comparing those with the baseline metrics. Similar to data drift metrics, model
    quality metrics can be sent to CloudWatch Metrics for analysis and visualization
    and CloudWatch alarms can be configured for automated notifications and/or responses.
    The following diagram shows how SageMaker Model Monitor works:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型质量监控，它使用包含预测和真实标签的基线数据集创建基线指标（例如回归的 MAE 和分类的准确率）。然后，它捕获生产中的预测，摄取真实标签，并将真实标签与预测合并以计算各种回归和分类指标，然后再将它们与基线指标进行比较。类似于数据漂移指标，模型质量指标可以发送到
    CloudWatch 指标进行分析和可视化，并且可以配置 CloudWatch 警报以进行自动通知和/或响应。以下图表显示了 SageMaker 模型监控器的工作原理：
- en: '![Figure 9.6 – SageMaker Model Monitor process flow ](img/B20836_09_07.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – SageMaker模型监控流程](img/B20836_09_07.png)'
- en: 'Figure 9.7: SageMaker Model Monitor process flow'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7：SageMaker模型监控流程
- en: For bias detection, SageMaker Clarify can monitor bias metrics for deployed
    models continuously and raises alerts through CloudWatch when a metric crosses
    a threshold. We will cover bias detection in detail in *Chapter 13, Bias, Explainability,
    Privacy, and, Adversarial Attacks*.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于偏差检测，SageMaker Clarify可以持续监控部署模型的偏差指标，并在指标超过阈值时通过CloudWatch发出警报。我们将在第13章“偏差、可解释性、隐私和对抗攻击”中详细介绍偏差检测。
- en: ML pipeline monitoring
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习管道监控
- en: 'The ML pipeline’s execution needs to be monitored for statuses and errors,
    so corrective actions can be taken as needed. During a pipeline execution, there
    are pipeline-level statuses/events as well as stage-level and action-level statuses/events.
    You can use these events and statuses to understand the progress of each pipeline
    and stage and be alerted when something is wrong. The following diagram shows
    how AWS CodePipeline, CodeBuild, and CodeCommit can work with CloudWatch, CloudWatch
    Logs, and EventBridge for general status monitoring and reporting, as well as
    problem troubleshooting:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 需要监控机器学习管道的执行状态和错误，以便在需要时采取纠正措施。在管道执行期间，有管道级别的状态/事件以及阶段级别和动作级别的状态/事件。您可以使用这些事件和状态来了解每个管道和阶段的进度，并在出现问题时收到警报。以下图表显示了AWS
    CodePipeline、CodeBuild和CodeCommit如何与CloudWatch、CloudWatch Logs和EventBridge协同工作，以进行一般状态监控和报告，以及问题故障排除：
- en: '![Figure 9.7 – ML CI/CD pipeline monitoring architecture ](img/B20836_09_08.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 机器学习CI/CD管道监控架构](img/B20836_09_08.png)'
- en: 'Figure 9.8: ML CI/CD pipeline monitoring architecture'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8：机器学习CI/CD管道监控架构
- en: CodeBuild can send metrics, such as `SucceededBuilds`, `FailedBuilds`, and `Duration`
    metrics. These CodeBuild metrics can be accessed through both the CodeBuild console
    and the CloudWatch dashboard. CodeBuild, CodeCommit, and CodePipeline can all
    emit events to EventBridge to report detailed status changes and trigger custom
    event processing, such as notifications, or log the events to another data repository
    for event archiving. All three services can send detailed logs to CloudWatch Logs
    to support operations such as troubleshooting or detailed error reporting.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: CodeBuild可以发送指标，例如`SucceededBuilds`、`FailedBuilds`和`Duration`指标。这些CodeBuild指标可以通过CodeBuild控制台和CloudWatch仪表板访问。CodeBuild、CodeCommit和CodePipeline都可以向EventBridge发出事件，以报告详细的状态变化并触发自定义事件处理，例如通知，或将事件记录到其他数据存储库以进行事件归档。所有三个服务都可以将详细日志发送到CloudWatch
    Logs，以支持诸如故障排除或详细错误报告等操作。
- en: Step Functions also provides a list of monitoring metrics to CloudWatch, such
    as execution metrics (such as execution failure, success, abort, and timeout)
    and activity metrics (such as activity started, scheduled, and succeeded). You
    can view these metrics in the management console and set a threshold to set up
    alerts.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Step Functions还向CloudWatch提供一系列监控指标，例如执行指标（例如执行失败、成功、中止和超时）和活动指标（例如活动开始、计划中和成功）。您可以在管理控制台中查看这些指标，并设置阈值以设置警报。
- en: Service provisioning management
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务提供管理
- en: 'Another key component of enterprise-scale ML platform management is **service**
    **provisioning management**. For large-scale service provisioning and deployment,
    an automated and controlled process should be adopted. Here, we will focus on
    provisioning the ML platform itself, not provisioning AWS accounts and networking,
    which should be established as the base environment for ML platform provisioning
    in advance. For ML platform provisioning, there are the following two main provisioning
    tasks:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 企业级机器学习平台管理的关键组成部分是**服务提供管理**。对于大规模的服务提供和部署，应采用自动化和受控的过程。在这里，我们将专注于提供机器学习平台本身，而不是提供AWS账户和网络，这些应在提前建立作为机器学习平台提供的基础环境。对于机器学习平台提供，有以下两个主要的提供任务：
- en: '**Data science environment provisioning**: Provisioning the data science environment
    for data scientists mainly includes provisioning data science and data management
    tools, storage for experimentation, as well as access entitlement for data sources
    and pre-built ML automation pipelines.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据科学环境提供**：为数据科学家提供数据科学环境主要包括提供数据科学和数据管理工具、实验存储以及数据源和预构建ML自动化管道的访问权限。'
- en: '**ML automation pipeline provisioning**: ML automation pipelines need to be
    provisioned in advance for data scientists and MLOps engineers to use them to
    automate different tasks such as container build, model training, and model deployment.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习自动化管道配置**：机器学习自动化管道需要提前配置，以便数据科学家和 MLOps 工程师可以使用它们来自动化不同的任务，例如容器构建、模型训练和模型部署。'
- en: 'There are multiple technical approaches to automating service provisioning
    on AWS, such as using provisioning shell scripts, CloudFormation scripts, and
    AWS Service Catalog. With shell scripts, you can sequentially call the different
    AWS CLI commands in a script to provision different components, such as creating
    a SageMaker Studio notebook. CloudFormation is the IaC service for infrastructure
    deployment on AWS. With CloudFormation, you create templates that describe the
    desired resources and dependencies that can be launched as a single stack. When
    the template is executed, all the resources and dependencies specified in the
    stack will be deployed automatically. The following code shows the template for
    deploying a SageMaker Studio domain:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 上自动提供服务配置有多种技术方法，例如使用配置脚本、CloudFormation 脚本和 AWS 服务目录。使用脚本，你可以在脚本中依次调用不同的
    AWS CLI 命令来配置不同的组件，例如创建 SageMaker Studio 笔记本。CloudFormation 是 AWS 上基础设施部署的 IaC
    服务。使用 CloudFormation，你可以创建模板来描述所需的资源和依赖关系，这些资源和依赖关系可以作为一个单独的堆栈启动。当模板执行时，堆栈中指定的所有资源和依赖关系将自动部署。以下代码显示了部署
    SageMaker Studio 域的模板：
- en: '[PRE15]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: AWS Service Catalog allows you to create different IT products to be deployed
    on AWS. These IT products can include SageMaker notebooks, a CodeCommit repository,
    and CodePipeline workflow definitions. AWS Service Catalog uses CloudFormation
    templates to describe IT products. With Service Catalog, administrators create
    IT products with CloudFormation templates, organize these products by product
    portfolio, and entitle end users to access. The end users then access the products
    from the Service Catalog product portfolio.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 服务目录允许你创建不同的 IT 产品，以便在 AWS 上部署。这些 IT 产品可以包括 SageMaker 笔记本、CodeCommit 仓库和
    CodePipeline 工作流定义。AWS 服务目录使用 CloudFormation 模板来描述 IT 产品。使用服务目录，管理员可以使用 CloudFormation
    模板创建 IT 产品，按产品组合组织这些产品，并授权最终用户访问。然后，最终用户从服务目录产品组合中访问产品。
- en: 'The following diagram shows the flow of creating a Service Catalog product
    and launching the product from the Service Catalog service:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了创建服务目录产品和从服务目录服务启动产品的流程：
- en: '![Figure 9.8 – Service Catalog workflow ](img/B20836_09_09.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – 服务目录工作流程](img/B20836_09_09.png)'
- en: 'Figure 9.9: Service Catalog workflow'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：服务目录工作流程
- en: For large-scale and governed IT product management, Service Catalog is the recommended
    approach. Service Catalog supports multiple deployment options, including single
    AWS account deployments and hub-and-spoke cross-account deployments.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大规模和受管理的 IT 产品管理，服务目录是推荐的方法。服务目录支持多种部署选项，包括单个 AWS 账户部署和中心辐射式跨账户部署。
- en: 'A hub-and-spoke deployment allows you to centrally manage all the products
    and make them available in different accounts. For our enterprise ML reference
    architecture in *Figure 9.4*, we can use the hub-and-spoke architecture to support
    the provisioning of data science environments and ML pipelines, as shown in the
    following diagram:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 中心辐射式部署允许你集中管理所有产品，并在不同的账户中使它们可用。对于我们的企业机器学习参考架构（图 9.4），我们可以使用中心辐射式架构来支持数据科学环境和机器学习管道的配置，如下所示：
- en: '![Figure 9.9 – The hub-and-spoke Service Catalog architecture for enterprise
    ML product management ](img/B20836_09_10.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – 企业机器学习产品管理的中心辐射式服务目录架构](img/B20836_09_10.png)'
- en: 'Figure 9.10: The hub-and-spoke Service Catalog architecture for enterprise
    ML product management'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：企业机器学习产品管理的中心辐射式服务目录架构
- en: In the preceding architecture, we set up the central portfolio in the shared
    services account. All the products, such as creating new Studio domains, new Studio
    user profiles, CodePipeline definitions, and training pipeline definitions, are
    centrally managed in the central hub account. Some products are shared with the
    different data science accounts to create data science environments for data scientists
    and teams. Some other products are shared with model training accounts for standing
    up ML training pipelines.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in building and operating an ML platform
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Constructing an enterprise ML platform is a multifaceted undertaking. It often
    requires significant time, with organizations taking six months or more to implement
    the initial phase of their ML platform. Continuous efforts are needed to incorporate
    new functionalities and enhancements for many years to come. Onboarding users
    and ML projects onto the new platform is another demanding aspect, involving extensive
    education for the user base and providing direct technical support.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, platform adjustments might be necessary to ensure smooth onboarding
    and successful utilization. Having collaborated with many customers in building
    their enterprise ML platform, I have identified some best practices for the construction
    and adoption of an ML platform.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: ML platform project execution best practices
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Assemble cross-functional teams**: Bring together data engineers, ML researchers,
    DevOps engineers, application developers, and business domain experts into integrated
    teams. This diversity of skills and perspectives will enrich the platform design
    and implementation.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Develop governance requirements and processes**: Define processes and requirements
    early on for model verification, explainability, ethics reviews, and approvals
    prior to production deployment. This will embed responsible AI practices into
    the platform.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define key performance indicators (KPIs) to measure success**: Identify relevant
    business KPIs and implement processes to actively monitor and report on model
    and platform impact on these KPIs. Share reports with stakeholders.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Select pilot ML workloads**: Choose a few pilot ML projects or workloads
    to implement first on the new platform. Learn from these real-world use cases
    to validate and improve the platform design and capabilities.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define the target state and execute in phases**: Articulate the long-term
    vision and target state for the enterprise ML platform. However, strategically
    execute adoption in incremental phases for faster learning.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML platform design and implementation best practices
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Adopt fully managed built-in capabilities**: Leverage SageMaker’s managed
    algorithms, containers, and features as defaults to reduce overhead and simplify
    integration. Only use custom built features if needed.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implement infrastructure as code**: Use CloudFormation or Terraform to provision,
    configure, and manage ML infrastructure through code. This enables consistency
    and automation.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build CI/CD pipelines**: Implement continuous integration and deployment
    pipelines leveraging CodePipeline, CodeBuild, CodeDeploy, and SageMaker for automated
    workflows. Consider GitHub Actions/Jenkins if needed.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automate experiment tracking**: Configure SageMaker or third-party tools
    to automatically log model training metadata like parameters, metrics, and artifacts.
    This enables debugging, comparisons, and reproducibility.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Establish an approved library repository**: Create a centralized, governed
    repository of approved libraries and packages for training, deployment, and inference
    code. This ensures consistency.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Design for scalability and spikes**: Architect the platform to handle varied
    usage patterns and traffic spikes via auto-scaling capabilities.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prioritize security from the start**: Implement security best practices including
    scanning, patching, encryption, and access controls. Have an incident response
    plan.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build self-service capability**: Develop self-service functionality early
    and evolve it to empower users while maintaining governance.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centralize the model repository**: Use a single, central repository for models
    to improve collaboration, discovery, compliance, and efficient deployment.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Establish a central feature store**: Implement a centralized feature store
    for sharing, monitoring, and governing feature engineering work and usage.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform use and operations best practices
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Limit production access**: Restrict access to production systems to only
    essential support and operations staff. This reduces the risk of mistakes or unauthorized
    changes.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize costs**: Leverage auto-scaling, spot instances, availability-based
    pricing, and other capabilities to optimize and reduce cloud costs.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and observability**: Actively monitor model accuracy, data drift,
    system performance, and so on using CloudWatch, SageMaker Debugger, Model Monitor,
    and other tools.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Establish change management**: Define a structured process for managing,
    reviewing, approving, and communicating platform/model changes prior to deployment.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incident management process**: Institute an incident response plan with procedures
    to detect, escalate, and resolve production issues and anomalies in a timely manner.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-AZ and region deployments**: Deploy models and platform infrastructure
    across multiple availability zones and regions to improve resilience and minimize
    latency.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Release management**: Implement structured release processes for coordinating,
    reviewing, and planning changes and new model/platform versions before deployment.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Capacity planning**: Proactively assess and project infrastructure capacity
    needs based on roadmaps and workloads. Scale appropriately.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource tagging**: A properly designed tagging strategy provides organization,
    discovery, security, automation, compliance, and improved visibility in an ML
    platform.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a robust enterprise ML platform requires thoughtful strategy and
    orchestration across people, processes, and technology. By bringing together cross-functional
    teams, instituting responsible AI governance, monitoring business impact, and
    designing for scalability, security, and collaboration, organizations can accelerate
    their AI journey. Adopting modern infrastructure as code, CI/CD pipelines, and
    cloud services lays a solid technology foundation. However, to realize value,
    platforms must be tightly integrated with line-of-business priorities and continuously
    provide trustworthy AI. With deliberate planning and phased execution centered
    on business goals and users, companies can transform into AI-driven enterprises.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: The key is to balance innovation with governance, move fast through automation
    while maintaining control, and evolve a platform that responsibly democratizes
    AI capabilities for both experts and business users. This enables embedding reliable
    and accountable AI throughout operations for a competitive advantage.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the key requirements and best practices for building
    an enterprise ML platform. We discussed how to design a platform that supports
    the end-to-end ML lifecycle, process automation, and separation of environments.
    Architectural patterns were reviewed, including how to leverage AWS services to
    build a robust ML platform on the cloud.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: The core capabilities of different ML environments were covered, such as training,
    hosting, and shared services. Best practices around platform design, operations,
    governance, and integration were also discussed. You should now have a solid understanding
    of what an enterprise-grade ML platform entails and key considerations for building
    one on AWS leveraging proven patterns.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deeper into advanced ML engineering topics.
    This includes distributed training techniques to scale model development and low-latency
    serving methods for optimizing inference.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code70205728346636561.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
