["```py\nimport os \nimport sys \ndata_folder = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"books\")\n\n```", "```py\ndef clean_book(document):\n    lines = document.split(\"n\")\n    start= 0\n    end = len(lines)\n    for i in range(len(lines)):\n        line = lines[i]\n        if line.startswith(\"*** START OF THIS PROJECT GUTENBERG\"):\n            start = i + 1\n        elif line.startswith(\"*** END OF THIS PROJECT GUTENBERG\"):\n            end = i - 1\n    return \"n\".join(lines[start:end])\n\n```", "```py\nimport numpy as np\n\ndef load_books_data(folder=data_folder):\n    documents = []\n    authors = []\n    subfolders = [subfolder for subfolder in os.listdir(folder)\n                  if os.path.isdir(os.path.join(folder, subfolder))]\n    for author_number, subfolder in enumerate(subfolders):\n        full_subfolder_path = os.path.join(folder, subfolder)\n        for document_name in os.listdir(full_subfolder_path):\n            with open(os.path.join(full_subfolder_path, document_name), errors='ignore') as inf:\n                documents.append(clean_book(inf.read()))\n                authors.append(author_number)\n    return documents, np.array(authors, dtype='int')\n\n```", "```py\ndocuments, classes = load_books_data(data_folder)\n\n```", "```py\ndocument_lengths = [len(document) for document in documents]\n\n```", "```py\nimport seaborn as sns\nsns.distplot(document_lengths)\n\n```", "```py\nfunction_words = [\"a\", \"able\", \"aboard\", \"about\", \"above\", \"absent\", \"according\" , \"accordingly\", \"across\", \"after\", \"against\",\"ahead\", \"albeit\", \"all\", \"along\", \"alongside\", \"although\", \"am\", \"amid\", \"amidst\", \"among\", \"amongst\", \"amount\", \"an\", \"and\", \"another\", \"anti\", \"any\", \"anybody\", \"anyone\", \"anything\", \"are\", \"around\", \"as\", \"aside\", \"astraddle\", \"astride\", \"at\", \"away\", \"bar\", \"barring\", \"be\", \"because\", \"been\", \"before\", \"behind\", \"being\", \"below\", \"beneath\", \"beside\", \"besides\", \"better\", \"between\", \"beyond\", \"bit\", \"both\", \"but\", \"by\", \"can\", \"certain\", \"circa\", \"close\", \"concerning\", \"consequently\", \"considering\", \"could\", \"couple\", \"dare\", \"deal\", \"despite\", \"down\", \"due\", \"during\", \"each\", \"eight\", \"eighth\", \"either\", \"enough\", \"every\", \"everybody\", \"everyone\", \"everything\", \"except\", \"excepting\", \"excluding\", \"failing\", \"few\", \"fewer\", \"fifth\", \"first\", \"five\", \"following\", \"for\", \"four\", \"fourth\", \"from\", \"front\", \"given\", \"good\", \"great\", \"had\", \"half\", \"have\", \"he\", \"heaps\", \"hence\", \"her\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"however\", \"i\", \"if\", \"in\", \"including\", \"inside\", \"instead\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keeping\", \"lack\", \"less\", \"like\", \"little\", \"loads\", \"lots\", \"majority\", \"many\", \"masses\", \"may\", \"me\", \"might\", \"mine\", \"minority\", \n\"minus\", \"more\", \"most\", \"much\", \"must\", \"my\", \"myself\", \"near\", \"need\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"ninth\", \"no\", \"nobody\", \"none\", \"nor\", \"nothing\", \"notwithstanding\", \"number\", \"numbers\", \"of\", \"off\", \"on\", \"once\", \"one\", \"onto\", \"opposite\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"part\", \"past\", \"pending\", \"per\", \"pertaining\", \"place\", \"plenty\", \"plethora\", \"plus\", \"quantities\", \"quantity\", \"quarter\", \"regarding\", \"remainder\", \"respecting\", \"rest\", \"round\", \"save\", \"saving\", \"second\", \"seven\", \"seventh\", \"several\",\"shall\", \"she\", \"should\", \"similar\", \"since\", \"six\", \"sixth\", \"so\", \"some\", \"somebody\", \"someone\", \"something\", \"spite\",\"such\", \"ten\", \"tenth\", \"than\", \"thanks\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"therefore\", \"these\", \"they\", \"third\", \"this\", \"those\",\"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"till\", \"time\", \"to\", \"tons\", \"top\", \"toward\", \"towards\", \"two\", \"under\", \"underneath\", \"unless\", \"unlike\", \"until\", \"unto\", \"up\", \"upon\", \"us\", \"used\", \"various\", \"versus\",\"via\", \"view\", \"wanting\", \"was\", \"we\", \"were\", \"what\", \"whatever\", \"when\", \"whenever\", \"where\", \"whereas\", \"wherever\", \"whether\", \"which\", \"whichever\", \"while\",\"whilst\", \"who\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whose\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n\n```", "```py\nfrom sklearn.feature_extraction.text \nimport CountVectorizer \nextractor = CountVectorizer(vocabulary=function_words)\n\n```", "```py\nextractor.fit(documents)\ncounts = extractor.transform(documents)\n\n```", "```py\nnormalized_counts = counts.T / np.array(document_lengths)\n\n```", "```py\naveraged_counts = normalized_counts.mean(axis=1)\n\n```", "```py\nfrom matplotlib import pyplot as plt\nplt.plot(averaged_counts)\n\n```", "```py\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline from sklearn import grid_search\n\n```", "```py\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\nsvr = SVC()\ngrid = grid_search.GridSearchCV(svr, parameters)\n\n```", "```py\npipeline1 = Pipeline([('feature_extraction', extractor), ('clf', grid) ])\n\n```", "```py\npipeline = Pipeline([('feature_extraction', CountVectorizer(analyzer='char', ngram_range=(3,3))),\n                     ('classifier', grid) ]\nscores = cross_val_score(pipeline, documents, classes, scoring='f1') \nprint(\"Score: {:.3f}\".format(np.mean(scores)))\n\n```", "```py\nenron_data_folder = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"enron_mail_20150507\", \"maildir\")\n\n```", "```py\nfrom email.parser \nimport Parser p = Parser()\n\n```", "```py\nfrom sklearn.utils import check_random_state\n\ndef get_enron_corpus(num_authors=10, data_folder=enron_data_folder, min_docs_author=10,\n                     max_docs_author=100, random_state=None):\n    random_state = check_random_state(random_state)\n    email_addresses = sorted(os.listdir(data_folder))\n    # Randomly shuffle the authors. We use random_state here to get a repeatable shuffle\n    random_state.shuffle(email_addresses)\n    # Setup structures for storing information, including author information\n    documents = []\n    classes = []\n    author_num = 0\n    authors = {}  # Maps author numbers to author names\n    for user in email_addresses:\n        users_email_folder = os.path.join(data_folder, user)\n        mail_folders = [os.path.join(users_email_folder, subfolder)\n                        for subfolder in os.listdir(users_email_folder)\n                        if \"sent\" in subfolder]\n        try:\n            authored_emails = [open(os.path.join(mail_folder, email_filename),\n                                    encoding='cp1252').read()\n                               for mail_folder in mail_folders\n                               for email_filename in os.listdir(mail_folder)]\n        except IsADirectoryError:\n            continue\n        if len(authored_emails) < min_docs_author:\n            continue\n        if len(authored_emails) > max_docs_author:\n            authored_emails = authored_emails[:max_docs_author]\n        # Parse emails, store the content in documents and add to the classes list\n        contents = [p.parsestr(email)._payload for email in authored_emails]\n        documents.extend(contents)\n        classes.extend([author_num] * len(authored_emails))\n        authors[user] = author_num\n        author_num += 1\n        if author_num >= num_authors or author_num >= len(email_addresses):\n            break\n     return documents, np.array(classes), authors\n\n```", "```py\ndocuments, classes, authors = get_enron_corpus(data_folder=enron_data_folder, random_state=14)\n\n```", "```py\ndocument_lengths = [len(document) for document in documents]\nsns.distplot(document_lengths)\n\n```", "```py\nscores = cross_val_score(pipeline, documents, classes, scoring='f1') \n\nprint(\"Score: {:.3f}\".format(np.mean(scores)))\n\n```", "```py\nfrom sklearn.cross_validation import train_test_split training_documents, \n\ntesting_documents, y_train, y_test = train_test_split(documents, classes, random_state=14)\n\n```", "```py\npipeline.fit(training_documents, y_train) \ny_pred = pipeline.predict(testing_documents)\n\n```", "```py\nprint(pipeline.named_steps['classifier'].best_params_)\n\n```", "```py\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_pred, y_test)\ncm = cm / cm.astype(np.float).sum(axis=1)\n\n```", "```py\nsorted_authors = sorted(authors.keys(), key=lambda x:authors[x])\n\n```", "```py\n%matplotlib inline \nfrom matplotlib import pyplot as plt \nplt.figure(figsize=(10,10))\nplt.imshow(cm, cmap='Blues', interpolation='nearest')\ntick_marks = np.arange(len(sorted_authors))\nplt.xticks(tick_marks, sorted_authors) \nplt.yticks(tick_marks, sorted_authors) \nplt.ylabel('Actual') \nplt.xlabel('Predicted') \nplt.show()\n\n```"]