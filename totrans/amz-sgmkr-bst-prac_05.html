<html><head></head><body>
		<div id="_idContainer055">
			<p><a id="_idTextAnchor071"/></p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor072"/>Chapter 4: Data Preparation at Scale Using Amazon SageMaker Data Wrangler and Processing</h1>
			<p><a id="_idTextAnchor073"/>So far, we've identified our dataset and explored both manual and automated labeling. Now it's time to turn our attention to preparing the data for training. Data scientists are familiar with the steps of feature engineering, such as <strong class="bold">scaling numeric features</strong>, <strong class="bold">encoding categorical features</strong>, and <strong class="bold">dimensionality reduction</strong>. </p>
			<p>As motivation, let's consider our weather dataset. What if our input dataset is imbalanced or not really representative of the data we'll encounter in production? Our model will not be as accurate as we'd like, and the consequences can be profound. Some facial recognition systems have been trained on datasets weighted toward white faces, with distressing consequences (<a href="https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/?web=1&amp;wdLOR=cB09A9880-DF39-442C-A728-B00E70AF1CA9">https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/?web=1&amp;wdLOR=cB09A9880-DF39-442C-A728-B00E70AF1CA9</a>). </p>
			<p>We need to understand what input features are affecting the model. That's important from a business standpoint as well as a legal or regulatory standpoint. Consider a model that predicts operational outages for an application. Understanding why outages happen is perhaps more valuable than predicting when an outage will occur – is the problem in our application or due to some external factor such as a network hiccup? Then, in some industries such as financial services, we cannot use a model without being able to demonstrate that it doesn't violate regulations against discriminatory lending, say. </p>
			<p>The smaller version of our dataset (covering 1 month) is about 5 GB of data. We can analyze that dataset on a modern workstation without too much difficulty. But what about the full dataset, which is closer to 500 GB? If we want to prepare the full dataset, we need to work with horizontally scalable cluster computing frameworks. Furthermore, activities such as encoding categorical variables can take quite some time if we use inefficient processing frameworks. </p>
			<p>In this chapter, we'll look at the challenges involved in data preparation when processing a large dataset and examining the <strong class="bold">SageMaker</strong> features that help us with large-scale feature engineering.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Visual data preparation with Data Wrangler</li>
				<li>Bias detection and explainability with Data Wrangler</li>
				<li>Data preparation at scale with SageMaker Processing</li>
			</ul>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor074"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you have not set up the data science environment yet, please refer to <a href="B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a><em class="italic">, Data Science Environments</em>, which walks you through the setup process.</p>
			<p>The code examples included in the book are available on GitHub at <a href="https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter04">https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter04</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>).</p>
			<p>The code for this chapter is in the <strong class="source-inline">CH04</strong> folder of the GitHub repository.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor075"/>Visual data preparation with Data Wrangler</h1>
			<p>Let's start small <a id="_idIndexMarker136"/>with our 1-month dataset. Working with a <a id="_idIndexMarker137"/>small dataset is a good way to get familiar with the data before diving into more scalable techniques. SageMaker Data Wrangler gives us an easy way to construct a data flow, a series of data preparation steps powered by a visual interface. </p>
			<p>In the rest of this section, we'll use Data Wrangler to inspect and transform data, and then export the Data Wrangler steps into a reusable flow<a id="_idTextAnchor076"/>.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor077"/>Data inspection</h2>
			<p>Let's get started with <a id="_idIndexMarker138"/>Data Wrangler for data inspection, where we look at the properties of our data and determine how to prepare it for model training. Begin by adding a new flow in SageMaker Studio; go to the <strong class="bold">File</strong> menu, then <strong class="bold">New</strong>, then <strong class="bold">Flow</strong>. After the flow starts up and connects to Data Wrangler, we need to import our data. The following screenshot shows the data import step in Data Wrangler:</p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B17249_04_01.jpg" alt="Figure 4.1 – Import data source in Data Wrangler&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – Import data source in Data Wrangler</p>
			<p>Because our<a id="_idIndexMarker139"/> dataset consists of multiple small JSON files scattered in date-partitioned folders, we'll<a id="_idIndexMarker140"/> use <strong class="bold">Athena</strong> (a managed version of <strong class="bold">Presto</strong>) for the import. The <strong class="source-inline">PrepareData.ipynb</strong> notebook walks you through creating a <strong class="source-inline">Glue</strong> database and table and registering the partitions in the section called <strong class="source-inline">Glue Catalog</strong>. Once that's done, click on <strong class="bold">Athena</strong> to start importing the small dataset. </p>
			<p>On the next screen, specify the database you created in the notebook. Enter the following query to import 1 month's worth of data:</p>
			<p class="source-code">select * from openaq where aggdate like '2019-01%'</p>
			<p>The following screenshot <a id="_idIndexMarker141"/>shows the import step in Data Wrangler:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B17249_04_02.jpg" alt="Figure 4.2 – Athena import into Data Wrangler&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – Athena import into Data Wrangler</p>
			<p>Run the query and click on <strong class="bold">Import dataset</strong>. </p>
			<p>Now we're ready to perform some analysis and transformation. Click the <strong class="bold">+</strong> symbol next to the last box in the data flow and select <strong class="bold">Add analysis</strong>. You'll now have a screen where you can <a id="_idIndexMarker142"/>choose one of the available analyses, as you can see in the following screenshot:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B17249_04_03.jpg" alt="Figure 4.3 – Data analysis configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – Data analysis configuration</p>
			<p>Start with a <strong class="bold">Table summary</strong> step, which shows some statistical properties of numeric features, as you<a id="_idIndexMarker143"/> can see in the following screenshot:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B17249_04_04.jpg" alt="Figure 4.4 – Table summary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – Table summary</p>
			<p>Next, let's try a scatter plot to help us visualize the distribution of the measurement values. Set the <strong class="source-inline">y</strong> axis to <strong class="source-inline">value</strong>, the <strong class="source-inline">x</strong> axis to <strong class="source-inline">aggdate</strong>, color by <strong class="source-inline">country</strong>, and facet by <strong class="source-inline">parameter</strong>. We can see in the following preview chart that the value for nitrogen dioxide is relatively steady over time, while the value for carbon monoxide shows more variability for some countries:</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B17249_04_05.jpg" alt="Figure 4.5 – Scatter plot showing measurement values by date, color-coded by country, and faceted &#13;&#10;by parameter"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – Scatter plot showing measurement values by date, color-coded by country, and faceted by parameter</p>
			<p>Feel free to add more<a id="_idIndexMarker144"/> scatter plots or try a histogram. We'll explore the bias report and quick mode in the <em class="italic">Bias detection and explainability with Data Wrangler and Clarify</em> section.</p>
			<p>Now that we've done some basic data inspection, we move on to data transformati<a id="_idTextAnchor078"/>on.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor079"/>Data transformation</h2>
			<p>In this section, we <a id="_idIndexMarker145"/>will convert the data from the raw format into a format usable for model training. Recall the basic format of our raw data:</p>
			<p class="source-code">{“date”:{“utc”:”2021-03-20T19:00:00.000Z”,”local”:”2021-03-20T23:00:00+04:00”},”parameter”:”pm25”,”value”:32,”unit”:”µg/m³”,”averagingPeriod”:{“val</p>
			<p class="source-code">ue”:1,”unit”:”hours”},”location”:”US Diplomatic Post:Dubai”,”city”:”Dubai”,”country”:”AE”,”coordinates”:{“latitude”:25.25848,”longitude”:55.309166</p>
			<p class="source-code">},”attribution”:[{“name”:”EPA AirNow DOS”,”url”:”http://airnow.gov/index.cfm?action=airnow.global_summary”}],”sourceName”:”StateAir_Dubai”,”sourceT</p>
			<p class="source-code">ype”:”government”,”mobile”:false}</p>
			<p>We'll perform the following steps using Data Wrangler:</p>
			<ul>
				<li>Scale numeric values.</li>
				<li>Encode categorical values.</li>
				<li>Add features related to the date (for example, day of the week, day in a month).</li>
				<li>Drop unwanted columns (<strong class="source-inline">source name</strong>, <strong class="source-inline">coordinates</strong>, <strong class="source-inline">averaging period</strong>, <strong class="source-inline">attribution</strong>, <strong class="source-inline">units</strong>, and <strong class="source-inline">location</strong>). These columns are either redundant (for example, the important part of the location is in the city and country columns) or not usable as features.</li>
			</ul>
			<p>Back <a id="_idIndexMarker146"/>to the <em class="italic">Preparation</em> part of the flow, click the <strong class="bold">+</strong> symbol next to the last box in the data flow panel and select <strong class="bold">Add Transform</strong>. You'll see a preview of the dataset and a list of the available transforms as follows:</p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B17249_04_06.jpg" alt="Figure 4.6 – Data transformations in Data Wrangler&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – Data transformations in Data Wrangler</p>
			<p>For our first <a id="_idIndexMarker147"/>transformation, select <strong class="bold">Encode categorical</strong>. In the transformation options panel, pick <strong class="bold">One-hot encode</strong> as the transformation, specify <strong class="source-inline">sourcetype</strong> as the column, set <strong class="bold">Output Style</strong> to <strong class="bold">Columns</strong>, and add a prefix for the new column names: </p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B17249_04_07.jpg" alt="Figure 4.7 – One-hot encoding in Data Wrangler&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – One-hot encoding in Data Wrangler</p>
			<p>When you're done <a id="_idIndexMarker148"/>setting up the transformation, click <strong class="bold">Preview</strong> and then <strong class="bold">Add</strong> to add the transform. You can now add additional transformations to drop the unwanted columns, scale the numeric columns, and featurize the date. You can also provide your own custom code if you <a id="_idTextAnchor080"/>like.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor081"/>Exporting the flow</h2>
			<p>Data Wrangler is very <a id="_idIndexMarker149"/>handy when we want to quickly explore a dataset. But we can also export the results of a flow into Amazon SageMaker <strong class="bold">Feature Store</strong>, generate a <strong class="bold">SageMaker pipeline</strong>, create a Data Wrangler job, or generate <strong class="bold">Python</strong> code. We will not use these capabilities now, but feel free to experiment with<a id="_idTextAnchor082"/> them.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor083"/>Bias detection and explainability with Data Wrangler and Clarify</h1>
			<p>Now that we've done some initial work in exploring and preparing our data, let's do a sanity check on our input data. While bias can mean many things, one particular symptom is a dataset that has many more samples of one type of data than another, which will affect our model's performance. We'll use Data Wrangler to see if our input data is imbalanced and understand which features are most important to our model.</p>
			<p>To begin, add an<a id="_idIndexMarker150"/> analysis to the flow. Choose <strong class="bold">Bias Report</strong> from the list of available transformations and use the <strong class="source-inline">mobile</strong> column as the label, with <strong class="source-inline">1</strong> as the predicted <a id="_idIndexMarker151"/>value. Choose <strong class="source-inline">city</strong> as the column to use for bias analysis, then click <strong class="bold">Check for bias</strong>. In this scenario, we want to determine whether our dataset is somehow imbalanced with respect to the city and <a id="_idIndexMarker152"/>whether the data was collected at a mobile station. If the quality of data from mobile sources is inferior to non-mobile sources, it'd be good to know if the mobile sources are unevenly distributed among cities.</p>
			<p>Next, we'll examine <strong class="bold">feature importance</strong>. Feature importance<a id="_idIndexMarker153"/> is one aspect of model explainability. We want to understand which parts of the dataset are most important to model behavior. Another aspect, which we'll visit in <a href="B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210"><em class="italic">Chapter 11</em></a><em class="italic">, Monitoring Production Models with Amazon SageMaker Model Monitor and Clarify</em>, in the <em class="italic">Monitor bias drift and feature importance drift using Amazon SageMaker Clarify </em>section, is understanding which features contributed to a specific inference.</p>
			<p>Add another analysis in the last step of the flow. Select <strong class="bold">Quick Model</strong> for the <strong class="source-inline">value</strong> column (Data Wrangler will infer that this is a regression problem). Preview and create the analysis. You should see a screen that looks similar to the following screenshot:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B17249_04_08.jpg" alt="Figure 4.8 – Feature importance generated by Data Wrangler&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 – Feature importance generated by Data Wrangler</p>
			<p>This analysis generates a random forest model, evaluates performance using a test set with 30% of the data, and calculates a <strong class="bold">Gini importance score</strong> for <a id="_idIndexMarker154"/>each feature. As you can see in <em class="italic">Figure 4.8</em>, the city and day of the month are the most important features. </p>
			<p>So far we've used Data Wrangler for visual inspection and transformation. Now, we'll look at how to handle larger datasets using SageMaker P<a id="_idTextAnchor084"/>rocessing.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor085"/>Data preparation at scale with SageMaker Processing</h1>
			<p>Now let's turn <a id="_idIndexMarker155"/>our attention to preparing the <a id="_idIndexMarker156"/>entire dataset. At 500 GB, it's too large to process using <strong class="source-inline">sklearn</strong> on a single EC2 instance. We will write a SageMaker processing job that <a id="_idIndexMarker157"/>uses <strong class="bold">Spark ML</strong> for data preparation. (Alternatively, you can use <strong class="bold">Dask</strong>, but at the time of writing, SageMaker Processing does not provide a Dask container out of the box.)</p>
			<p>The <strong class="source-inline">Processing Job</strong> part of this chapter's notebook walks you through launching the processing job. Note that we'll use a cluster of 15 EC2 instances to run the job (if you need limits raised, you can contact AWS support). </p>
			<p>Also note that up until now, we've been working with the uncompressed JSON version of the data. This format containing thousands of small JSON files is not ideal for Spark processing as <a id="_idIndexMarker158"/>the <strong class="bold">Spark executors</strong> will spend a lot of time doing I/O. Luckily, the <strong class="source-inline">OpenAQ</strong> dataset also includes a <strong class="bold">gzipped Parquet version</strong> of the<a id="_idIndexMarker159"/> data. Compression will save on storage space and is a good idea unless our processing job is CPU-bound rather than I/O-bound. Note, however, that <strong class="source-inline">gzip</strong> is not a preferred compression format as it is not splittable; if you have a choice, use the Snappy compression format.</p>
			<p>We will use the gzipped Parquet version of our data for the larger data preparation job:</p>
			<ol>
				<li>First, we will define the <strong class="source-inline">processor</strong> class, using <strong class="bold">Spark 3.0</strong>. We will set the max runtime to <strong class="source-inline">7200</strong> seconds (2 hours). Two <a id="_idIndexMarker160"/>hours is more than sufficient to process at least one of the 8 tables in the Parquet dataset. If you want to process all eight of them, change the timeout to 3 hours and make an adjustment in the <strong class="source-inline">preprocess.py</strong> script:<p class="source-code">spark_processor = PySparkProcessor(</p><p class="source-code">    base_job_name=”spark-preprocessor”,</p><p class="source-code">    framework_version=”3.0”,</p><p class="source-code">    role=role,</p><p class="source-code">    instance_count=15,</p><p class="source-code">    instance_type=”ml.m5.4xlarge”,</p><p class="source-code">    max_runtime_in_seconds=7200,</p><p class="source-code">)</p></li>
				<li>Next, we'll set<a id="_idIndexMarker161"/> the Spark configuration, following the formulas defined in an EMR blog (<a href="https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/">https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/</a>):<p class="source-code">configuration = [</p><p class="source-code">    {</p><p class="source-code">    “Classification”: “spark-defaults”,</p><p class="source-code">    “Properties”: {“spark.executor.memory”: “18g”, </p><p class="source-code">        “spark.yarn.executor.memoryOverhead”: “3g”,</p><p class="source-code">                   “spark.driver.memory”: “18g”,</p><p class="source-code">          “spark.yarn.driver.memoryOverhead”: “3g”,</p><p class="source-code">                   “spark.executor.cores”: “5”, </p><p class="source-code">                   “spark.driver.cores”: “5”,</p><p class="source-code">                   “spark.executor.instances”: “44”,</p><p class="source-code">                   “spark.default.parallelism”: “440”,</p><p class="source-code">            “spark.dynamicAllocation.enabled”: “false”</p><p class="source-code">                  },</p><p class="source-code">    },</p><p class="source-code">    {</p><p class="source-code">    “Classification”: “yarn-site”,</p><p class="source-code">    “Properties”: {“yarn.nodemanager.vmem-check-enabled”: “false”, </p><p class="source-code">      “yarn.nodemanager.mmem-check-enabled”: “false”},</p><p class="source-code">    }</p><p class="source-code">]</p></li>
				<li>Finally, we'll<a id="_idIndexMarker162"/> launch the job. We need to include a JSON <strong class="source-inline">serde</strong> class:<p class="source-code">spark_processor.run(</p><p class="source-code">    submit_app=”scripts/preprocess.py”,</p><p class="source-code">    submit_jars=[“s3://crawler-public/json/serde/json-serde.jar”],</p><p class="source-code">    arguments=['--s3_input_bucket', s3_bucket,</p><p class="source-code">              '--s3_input_key_prefix', s3_prefix_parquet,</p><p class="source-code">               '--s3_output_bucket', s3_bucket,</p><p class="source-code">             '--s3_output_key_prefix', s3_output_prefix],</p><p class="source-code">    spark_event_logs_s3_uri=”s3://{}/{}/spark_event_logs”.format(s3_bucket, 'sparklogs'),</p><p class="source-code">    logs=True,</p><p class="source-code">    configuration=configuration</p><p class="source-code">)</p></li>
			</ol>
			<p>The processing script, <strong class="source-inline">CH04/scripts/preprocess.py</strong>, walks through several steps, which we'll explain in the su<a id="_idTextAnchor086"/>bsequent sections.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor087"/>Loading the dataset</h2>
			<p>We will load one or <a id="_idIndexMarker163"/>more of the Parquet table sets from S3. If you want to process more than one, modify the <strong class="source-inline">get_tables</strong> function to return more table names in the list as follows:</p>
			<p class="source-code"># the helper function `get_tables` lists the tables we want to include</p>
			<p class="source-code">tables = get_tables()</p>
			<p class="source-code">df = spark.read.parquet( </p>
			<p class="source-code">    f”s3://{args.s3_input_bucket}/” +</p>
			<p class="source-code">    f”{args.s3_input_key_prefix}/{tables[0]}/”)</p>
			<p class="source-code">for t in tables[1:]:</p>
			<p class="source-code">    df_new = spark.read.parquet( </p>
			<p class="source-code">        f”s3://{args.s3_input_bucket}/” +</p>
			<p class="source-code">        f”{args.s3_input_key_prefix}/{t}/”)</p>
			<p class="source-code">    df = df.union(df_new)</p>
			<p>The next step in the processing script is dropping unnecessary column<a id="_idTextAnchor088"/>s from the dataset.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor089"/>Drop columns</h2>
			<p>We'll <a id="_idIndexMarker164"/>repeat most of the steps we did in Data Wrangler using <strong class="bold">PySpark</strong>. We need to drop some columns that we don't want, as follows:</p>
			<p class="source-code">df = df.drop('date_local') \     </p>
			<p class="source-code">.drop('unit') \</p>
			<p class="source-code">.drop('attribution') \</p>
			<p class="source-code">.drop('averagingperiod') \</p>
			<p class="source-code"><a id="_idTextAnchor090"/>.drop('coordinates')</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor091"/>Converting data types</h2>
			<p>We'll <a id="_idIndexMarker165"/>convert the <strong class="source-inline">mobile</strong> field to an integer:</p>
			<p class="source-code">df = df.withColumn(“ismobile”,col(“mobile”).cast(IntegerType(<a id="_idTextAnchor092"/>))) \</p>
			<p class="source-code">.drop('mobile')</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor093"/>Scaling numeric fields</h2>
			<p>We'll use the <a id="_idIndexMarker166"/>Spark ML standard scaler to transform the <strong class="source-inline">value</strong> field:</p>
			<p class="source-code">value_assembler = VectorAssembler(inputCols=[“value”], outputCol=”value_vec”)</p>
			<p class="source-code">value_scaler = StandardScaler(inputCol=”value_vec”, outputCol=”value_scaled”)</p>
			<p class="source-code">value_pipeline = Pipeline(stages=[value_assembler, value_scaler])</p>
			<p class="source-code">value_model = value_pipeline.fit(df)</p>
			<p class="source-code">xform_df = val<a id="_idTextAnchor094"/>ue_model.transform(df)</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor095"/>Featurizing the date</h2>
			<p>The date by itself isn't <a id="_idIndexMarker167"/>that useful, so we'll extract several new features from it indicating the day, month, quarter, and year:</p>
			<p class="source-code">xform_df = xform_df.withColumn('aggdt', </p>
			<p class="source-code">               to_date(unix_timestamp(col('date_utc'), </p>
			<p class="source-code">“yyyy-MM-dd'T'HH:mm:ss.SSSX”).cast(“timestamp”)))</p>
			<p class="source-code">xform_df = xform_df.withColumn('year',year(xform_df.aggdt)) \</p>
			<p class="source-code">        .withColumn('month',month(xform_df.aggdt)) \</p>
			<p class="source-code">        .withColumn('quarter',quarter(xform_df.aggdt))</p>
			<p class="source-code">xform_df = xform_df.withColumn(“day”, date_for<a id="_idTextAnchor096"/>mat(col(“aggdt”), “d”))</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor097"/>Simulating labels for air quality</h2>
			<p>Although we <a id="_idIndexMarker168"/>used ground truth in <a href="B17249_03_Final_JM_ePub.xhtml#_idTextAnchor052"><em class="italic">Chapter 3</em></a><em class="italic">, Data Labeling with Amazon SageMaker Ground Truth</em>, for labeling, for the sake of this demonstration we'll use a simple heuristic to assign these labels instead:</p>
			<p class="source-code">isBadAirUdf = udf(isBadAir, IntegerType())</p>
			<p class="source-code">xform_df = xform_df.withColumn('isBadAir', isBadAirUd<a id="_idTextAnchor098"/>f('value', 'parameter'))</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor099"/>Encoding categorical variables</h2>
			<p>Now we'll <a id="_idIndexMarker169"/>encode the categorical features. Most of these features have fairly high cardinality, so we'll perform ordinal encoding here and learn embeddings later in our training process. We will only use one-hot encoding for the parameter, which only has seven possible choices:</p>
			<p class="source-code">parameter_indexer = StringIndexer(inputCol=”parameter”, \</p>
			<p class="source-code">outputCol=”indexed_parameter”, handleInvalid='keep')</p>
			<p class="source-code">location_indexer = StringIndexer(inputCol=”location”, \</p>
			<p class="source-code">outputCol=”indexed_location”, handleInvalid='keep')</p>
			<p class="source-code">city_indexer = StringIndexer(inputCol=”city”, \ </p>
			<p class="source-code">outputCol=”indexed_city”, handleInvalid='keep')</p>
			<p class="source-code">country_indexer = StringIndexer(inputCol=”country”, \</p>
			<p class="source-code">outputCol=”indexed_country”, handleInvalid='keep')</p>
			<p class="source-code">sourcename_indexer = StringIndexer(inputCol=”sourcename”, \</p>
			<p class="source-code">outputCol=”indexed_sourcename”, handleInvalid='keep')</p>
			<p class="source-code">sourcetype_indexer = StringIndexer(inputCol=”sourcetype”, \</p>
			<p class="source-code">outputCol=”indexed_sourcetype”, handleInvalid='keep')</p>
			<p class="source-code">enc_est = OneHotEncoder(inputCols=[“indexed_parameter”], \</p>
			<p class="source-code">outputCols=[“vec_parameter”])</p>
			<p class="source-code">enc_pipeline = Pipeline(stages=[parameter_indexer, location_indexer, </p>
			<p class="source-code">        city_indexer, country_indexer, sourcename_indexer, </p>
			<p class="source-code">        sourcetype_indexer, enc_est])</p>
			<p class="source-code">enc_model = enc_pipeline.fit(xform_df)</p>
			<p class="source-code">enc_df = enc_model.transform(xform_df)</p>
			<p class="source-code">param_cols = enc_df.schema.fields[17].me<a id="_idTextAnchor100"/>tadata['ml_attr']['vals']</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor101"/>Splitting and saving the dataset</h2>
			<p>After some final <a id="_idIndexMarker170"/>cleanup of the dataset, we can split the dataset into train, validation, and test sets, and save<a id="_idIndexMarker171"/> them to S3:</p>
			<p class="source-code">(train_df, validation_df, test_df) = final_df.randomSplit([0.7, 0.2, 0.1])</p>
			<p class="source-code">train_df.write.option(“header”,True).csv('s3://' + \</p>
			<p class="source-code">os.path.join(args.s3_output_bucket, </p>
			<p class="source-code">      args.s3_output_key_prefix, 'train/'))</p>
			<p class="source-code">validation_df.write.option(“header”,True).csv('s3://' + \</p>
			<p class="source-code">os.path.join(args.s3_output_bucket, </p>
			<p class="source-code">      args.s3_output_key_prefix, 'validation/'))</p>
			<p class="source-code">test_df.write.option(“header”,True).csv('s3://' + \</p>
			<p class="source-code">os.path.join(args.s3_output_bucket, </p>
			<p class="source-code">      args.s3_output_key_prefix, 'test/'))</p>
			<p>In this section, we saw how to use a SageMaker Processing job to perform data preparation on a larger dataset using Apache Spark. In the field, many datasets are large enough to require a distributed processing framework, and now you understand how to integrate a Spark job in<a id="_idTextAnchor102"/>to your SageMaker workflow.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor103"/>Summary</h1>
			<p>In this chapter, we tackled feature engineering for a large (~ 500 GB) dataset. We looked at challenges including scalability, bias, and explainability. We saw how to use SageMaker Data Wrangler, Clarify, and Processing jobs to explore and prepare data. </p>
			<p>While there are many ways to use these tools, we recommend using Data Wrangler for interactive exploration of small to mid-sized datasets. For processing large datasets in their entirety, switch to programmatic use of processing jobs using the Spark framework to take advantage of parallel processing. (At the time of writing, Data Wrangler does not support running on multiple instances, but you can run a processing job on multiple instances.) You can always export a Data Wrangler flow as a starting point.</p>
			<p>If your dataset is many terabytes, consider running a Spark job directly in <strong class="bold">EMR</strong> or Glue and invoking SageMaker using the SageMaker Spark SDK. EMR and Glue have optimized Spark runtimes and more efficient integration with S3 storage.</p>
			<p>At this point, we have our data ready for model training. In the next chapter, we'll explore using Amazon SageMaker Feature Store to help us manage prepared feature data.</p>
		</div>
		<div>
			<div id="_idContainer056">
			</div>
		</div>
	</body></html>