<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deploying and Distributing Analyses and Models</h1>
                </header>
            
            <article>
                
<p class="mce-root">We have implemented all sorts of models in Go, including regressions, classifications, clustering, and more. You have also learned about some of the process around developing a machine learning model. Our models have successfully predicted disease progression, flower species, and objects within images. Yet, we are still missing an important piece of the machine learning puzzle: deployment, maintenance, and scaling.</p>
<p>If our models just stay on our laptops, they are not doing any good or creating value within a company. We need to know how to take our machine learning workflows and integrate them into the systems that are already deployed in our organization, and we need to know how to scale, update, and maintain these workflows over time.</p>
<p>The fact that our machine learning workflows are, by their very nature, multi-stage workflows might make this deployment and maintenance a little bit of a challenge. We need to train, test, and utilize our models and, in some cases, we need to preprocess and/or postprocess our data. We may also need to chain certain models together. How can we deploy and connect all of these stages while maintaining the simplicity and integrity of our applications? For example, how can we update a training dataset over time while still knowing which training dataset produced which models, and which of those models produced which results? How can we easily scale our predictions as the demand for those predictions scales up and down? Finally, how can we integrate our machine learning workflows with other applications in our infrastructure or with pieces of the infrastructure itself (databases, queues, and so on)?</p>
<p>We will tackle all of these questions in this chapter. As it turns out, Go and infrastructure tooling written in Go provide an excellent platform to deploy and manage machine learning workflows. We will use a completely Go-based approach from bottom to top and illustrate how each of those pieces helps us do data science at scale!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running models reliably on remote machines</h1>
                </header>
            
            <article>
                
<p>Whether your company uses on--premise infrastructure or cloud infrastructure, you will need to run your machine learning models somewhere other than your laptop at some point. These models might need to be ready to serve fraud predictions, or they may need to process user-uploaded images in real time. You cannot have the model sitting on your laptop and successfully serve this information.</p>
<p>However, as we get our data processing and machine learning applications off of our laptops, we should ensure the following:</p>
<ol>
<li>We should not complicate our applications just for the sake of deployment and scaling. We should keep our applications simple, which will help us maintain them and ensure integrity over time.</li>
<li>We should ensure that our applications behave like they did on our local machine, where we developed them.</li>
</ol>
<p>If we deploy our machine learning workflows and they do no perform or behave like they did during development, they will not be able to produce their anticipated value. We should be able to understand how our models will perform locally and assume that they will perform the same way in production. This becomes increasingly hard to accomplish as you add unnecessary complexity to your applications during deployment.</p>
<p>One way to keep your deployments simple, portable, and reproducible is with Docker (<a href="https://www.docker.com/">https://www.docker.com/</a>), and we will utilize it here to deploy our machine learning applications.</p>
<div class="packt_infobox">Docker is itself written in Go, making it the first Go-based infrastructure tool that we will make use of in our machine learning deployment stack.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief introduction to Docker and Docker jargon</h1>
                </header>
            
            <article>
                
<p>Docker and the whole container ecosystem has its own set of jargon, which can be confusing, especially for those with experience in things like virtual machines. Before we continue, let's solidify this jargon as follows:</p>
<ul>
<li>A <strong>Docker image</strong> is a collection of data layers that together define a filesystem, libraries, environmental variables, and so on that will be seen by your application running inside a software container. Think of the image as a package that includes your application, other related libraries or packages, and other portions of an environment that your applications, need to run. A Docker image does not include a full operating system.</li>
<li>A <strong>Dockerfile</strong> is a file in which you define the various layers of your Docker image.</li>
<li>The <strong>Docker engine</strong> helps you build, manage, and run Docker images.</li>
<li>The process of building a Docker image for your application is commonly referred to as <strong>Docker-izing</strong> your application.</li>
<li>A <strong>container</strong> or <strong>software container</strong> is a running instance of a Docker image. Essentially, this running container includes all of the layers of your Docker image plus a read/write layer allowing your application to run, input/output data, and so on.</li>
<li>A <strong>Docker registry</strong> is a place where you keep Docker images. This registry could be local or it could be running on a remote machine. It also could be a hosted registry service such as <strong>Docker Hub</strong> or Quay, <strong>Amazon Web Service</strong> (<strong>AWS</strong>), <strong>Amazon EC2 Container Registry</strong> (<strong>ECR</strong>), and so on.</li>
</ul>
<div class="packt_infobox"><strong>Note</strong>: A Docker image is not the same as a virtual machine. A Docker image includes your application, a filesystem, and various libraries and packages, but it does not actually include a guest operation system. Moreover, it does not take up a set amount of memory, disk, and CPU on the host machine when running. Docker containers share the resources of the underlying kernel on which the Docker engine is running.</div>
<p>We hope that all of this jargon is solidified in the examples that follow this section, but as with other subjects in this book, you can dive into Docker and software containers deeper. We will include some links in this chapter's references to more Docker resources.</p>
<p>In the following examples, we will assume that you are able to build and run Docker images locally. To install Docker, you can follow the detailed instructions at <a href="https://www.docker.com/community-edition#/download">https://www.docker.com/community-edition#/download</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker-izing a machine learning application</h1>
                </header>
            
            <article>
                
<p>The machine learning workflow that we will be deploying and scaling in this chapter will be the linear regression workflow to predict diabetes disease progression that we developed in <a href="c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml" target="_blank">Chapter 4</a>, <em>Regression</em>. In our deployment, we will consider three different pieces of the workflow:</p>
<ul>
<li>The training and exporting of a single regression model (modeling disease progression with body mass index)</li>
<li>The training and exporting of a multiple regression model (modeling disease progression with body mass index and the blood measurement LTG)</li>
<li>An inference of disease progression based on one of the trained models and input attributes</li>
</ul>
<p>Later in the chapter, it will become clear why we might want to split the workflow into these pieces. For now, let's focus on getting these pieces of the workflow Docker-ized (building Docker images that can run these portions of our workflow, that is).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker-izing the model training and export</h1>
                </header>
            
            <article>
                
<p>We are going to utilize essentially the same code for model training as that from <a href="c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml" target="_blank">Chapter 4</a>, <em>Regression</em>. However, we are going to make a few tweaks to the code to make it more user-friendly and able to interface with other portions of our workflow. We would not consider these complications to the actual modeling code. Rather, these are things that you would probably do to any application that you are getting ready to utilize more generally.</p>
<p>First, we are going to add some command line flags to our application that will allow us to specify the input directory where our training dataset will be located, and an output directory where we are going to export a persisted representation of our model. You can implement these command line arguments as follows:</p>
<pre>// Declare the input and output directory flags.<br/>inDirPtr := flag.String("inDir", "", "The directory containing the training data")<br/>outDirPtr := flag.String("outDir", "", "The output directory")<br/><br/>// Parse the command line flags.<br/>flag.Parse()</pre>
<p>Then we will create a couple of struct types that will allow us to export the coefficient and intercept of our model to a JSON file. This exported JSON file is essentially a persisted version of our trained model, because the coefficients and intercept fully parameterize our model. These structs are defined here:</p>
<pre>// ModelInfo includes the information about the<br/>// model that is output from the training.<br/>type ModelInfo struct {<br/>    Intercept float64 `json:"intercept"`<br/>    Coefficients []CoefficientInfo `json:"coefficients"`<br/>}<br/><br/>// CoefficientInfo include information about a<br/>// particular model coefficient.<br/>type CoefficientInfo struct {<br/>    Name string `json:"name"`<br/>    Coefficient float64 `json:"coefficient"`<br/>}</pre>
<p>Other than that, our training code is the same as it was from <a href="c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml" target="_blank">Chapter <span>4</span></a>, <em>Regression</em>. We will still use <kbd>github.com/sajari/regression</kbd> to train our model. We will just export the model to the JSON file. The training and exporting of the single regression model is included in the following snippet:</p>
<pre>// Train/fit the single regression model<br/>// with github.com/sajari/regression<br/>// like in Chapter 5, Regression.<br/><br/>// Fill in the model information.<br/>modelInfo := ModelInfo{<br/>    Intercept: r.Coeff(0),<br/>    Coefficients: []CoefficientInfo{<br/>        CoefficientInfo{<br/>            Name: "bmi",<br/>            Coefficient: r.Coeff(1),<br/>        },<br/>    },<br/>}<br/><br/>// Marshal the model information.<br/>outputData, err := json.MarshalIndent(modelInfo, "", " ")<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/><br/>// Save the marshalled output to a file, with<br/>// certain permissions (http://permissions-calculator.org/decode/0644/).<br/>if err := ioutil.WriteFile(filepath.Join(*outDirPtr, "model.json"), outputData, 0644); err != nil {e model to the JSON fil<br/>    log.Fatal(err)<br/>}</pre>
<p>Then, for the multiple regression model, the process looks like the following:</p>
<pre>// Train/fit the multiple regression model<br/>// with github.com/sajari/regression<br/>// like in Chapter 5, Regression.<br/><br/>// Fill in the model information.
modelInfo := ModelInfo{
    Intercept: r.Coeff(0),
    Coefficients: []CoefficientInfo{
        CoefficientInfo{
            Name:        "bmi",
            Coefficient: r.Coeff(1),
        },
        CoefficientInfo{
            Name:        "ltg",
            Coefficient: r.Coeff(2),
        },
    },
}

// Marshal the model information.
outputData, err := json.MarshalIndent(modelInfo, "", "    ")
if err != nil {
    log.Fatal(err)
}

// Save the marshalled output to a file.
if err := ioutil.WriteFile(filepath.Join(*outDirPtr, "model.json"), outputData, 0644); err != nil {
    log.Fatal(err)
}</pre>
<p>To Docker-ize these training processes, we need to create a Dockerfile for each of the single regression training program and the multiple regression training program. It turns out, however, that we can use essentially the same Dockerfile for each of these. This Dockerfile, which should be placed in the same directory as our program, looks like this:</p>
<pre>FROM alpine<br/>ADD goregtrain / </pre>
<p>Pretty simple, right? Let's explore the purpose of these two lines. Remember how a Docker image is a series of layers that specify the environment in which your application will run? Well, this Dockerfile builds up two of those layers with two Dockerfile commands, <kbd>FROM</kbd> and <kbd>ADD</kbd>.</p>
<p><kbd>FROM alpine</kbd> specifies that we want our Docker image filesystem, applications, and libraries to be based on the official Alpine Linux Docker image available on Docker Hub. The reason that we are using <kbd>alpine</kbd> as a base image is that it is a very small Docker image (making it very portable) and it includes a few Linux shell niceties.</p>
<p><kbd>ADD goregtrain /</kbd> specifies that we want to add a <kbd>goregtrain</kbd> <span>file</span> to the <kbd>/</kbd> directory in the Docker image. This <kbd>goregtrain</kbd> file is actually the Go binary that we are going to build from our Go code. Thus, all the Dockerfile is saying is that we want to run our Go binary in Alpine Linux.</p>
<div class="packt_tip">Unless you are using <kbd>cgo</kbd> and depend on a bunch of external C libraries, always build your Go binary before building your Docker image and copy this statically linked Go binary into the Docker image. This will speed up your Docker builds and make your Docker images extremely small, which means that you will be able to port them easily to any system and start them super quick.</div>
<p>Now, we need to build our Go binary before we build our Docker image, because we are copying that Go binary into the image. To do this, we will use a <kbd>Makefile</kbd> that looks like the following:</p>
<pre>all: compile docker push clean<br/><br/>compile:<br/>        GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -o goregtrain<br/><br/>docker:<br/>        sudo docker build --force-rm=true -t dwhitena/goregtrain:single .<br/><br/>push:<br/>        sudo docker push dwhitena/goregtrain:single<br/><br/>clean:<br/>        rm goregtrain</pre>
<p>As you can see:</p>
<ul>
<li><kbd>make compile</kbd> will compile our Go binary for the target architecture and name it <kbd>goregtrain</kbd>.</li>
<li><kbd>make docker</kbd> will use the Docker engine (via the <kbd>docker</kbd> CLI) to build an image based on our Dockerfile, and it will <strong>tag</strong> our image, <kbd>dwhitena/goregtrain:single</kbd>.</li>
<li>The <kbd>dwhitena</kbd> portion of the tag specifies the Docker Hub username under which we will store our image (in this case, <kbd>dwhitena</kbd>), <kbd>goregtrain</kbd> specifies the name of the image, and <kbd>:single</kbd> specifies a tagged version of this image.</li>
<li>Once the image is built, <kbd>make push</kbd> will push the newly built Docker image to a registry. In this case, it will push it to Docker Hub under the username of <kbd>dwhitena</kbd> (of course, you could push to any other private or public registry).</li>
<li>Finally, <kbd>make clean</kbd> will clean up our binary.</li>
</ul>
<p>As mentioned, this <kbd>Dockerfile</kbd> and <kbd>Makefile</kbd> are the same for both the single and multiple regression models. However, we will utilize different Docker image tags to differentiate the two models. We will utilize <span><kbd>dwhitena/goregtrain:single</kbd> for the single regression model and <kbd>dwhitena/goregtrain:multi</kbd> for the multiple regression model.</span></p>
<div class="packt_tip packt_infobox">In these examples and in the rest of the chapter, you can follow along locally using either the public Docker images under <kbd>dwhitena</kbd> on Docker Hub, which would not require you to modify the examples printed here. Just note that you will not be able to build and push your own image to <kbd>dwhitena</kbd> on Docker Hub, as you are not <kbd>dwhitena</kbd>. Alternatively, you could replace <kbd>dwhitena</kbd> everywhere in the examples with your own Docker Hub username. This would allow you to build, push, and utilize your own images.</div>
<p>To build, push, and clean up after building the Docker image for either the single or multiple regression models, we can just run <kbd>make</kbd>, as shown in the following code, which is itself written in Go:</p>
<pre><strong>$ make</strong><br/><strong>GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -o goregtrain</strong><br/><strong>sudo docker build --force-rm=true -t dwhitena/goregtrain:multi .</strong><br/><strong>[sudo] password for dwhitena: </strong><br/><strong>Sending build context to Docker daemon 2.449MB</strong><br/><strong>Step 1/2 : FROM alpine</strong><br/><strong> ---&gt; 7328f6f8b418</strong><br/><strong>Step 2/2 : ADD goregtrain /</strong><br/><strong> ---&gt; 9c13594ad7de</strong><br/><strong>Removing intermediate container 6e717232c6d1</strong><br/><strong>Successfully built 9c13594ad7de</strong><br/><strong>Successfully tagged dwhitena/goregtrain:multi</strong><br/><strong>sudo docker push dwhitena/goregtrain:multi</strong><br/><strong>The push refers to a repository [docker.io/dwhitena/goregtrain]</strong><br/><strong>375642156c06: Pushed </strong><br/><strong>5bef08742407: Layer already exists </strong><br/><strong>multi: digest: sha256:f35277a46ed840922a0d7e94c3c57632fc947f6ab004deed66d3eb80a331e40f size: 738</strong><br/><strong>rm goregtrain</strong></pre>
<p>You can see in the preceding output that the Docker engine built the two layers of our image, tagged the image, and pushed the image up to Docker Hub. We can now see the Docker image in our local registry via the following:</p>
<pre><strong>$ sudo docker images | grep "goregtrain"</strong><br/><strong>dwhitena/goregtrain multi 9c13594ad7de About a minute ago 6.41MB</strong></pre>
<p>We can also see it on Docker Hub (<a href="https://hub.docker.com/r/dwhitena/goregtrain/">https://hub.docker.com/r/dwhitena/goregtrain/</a>) as illustrated here:</p>
<div class="CDPAlignCenter CDPAlign"><img height="211" width="412" class="image-border" src="assets/1b62cc44-5fd4-4a69-aa54-7a451709f74f.png"/></div>
<p>We will see how to run and utilize this Docker image shortly, but let's first build another Docker image that will generate predictions based on our JSON-persisted models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker-izing model predictions</h1>
                </header>
            
            <article>
                
<p>As with the training of our model, we are going to utilize command line arguments to specify the input directories and output directories utilized by our prediction program. This time we will have two input directories; one for the persisted model and one for a directory that will contain attributes from which we are to make predictions. Our program will, thus, do the following:</p>
<ol>
<li>Read in the model from the model input directory.</li>
</ol>
<p> </p>
<p> </p>
<ol start="2">
<li>Walk over files in the attributes input directory.</li>
<li>For each file in the attributes input directory (containing attributes with no corresponding prediction of disease progression), utilize our loaded model to make a prediction of disease progression.</li>
<li>Output the disease progression to an output file in the directory specified as a command line argument.</li>
</ol>
<p>Think of this process as follows. We have trained our model on historical data to predict disease progression and we want doctors or clinics to utilize this prediction in some way for new patients. They send us those patients attributes (<strong>body mass index</strong> (<strong>BMI</strong>) or body mass index and <strong>long term growth</strong> (<strong>LTG</strong>)) and we make predictions based on these input attributes.</p>
<p>We will assume that the input attributes come to our program in the form of JSON files (which could also be thought of as a JSON message off a queue or a JSON API response) that look like the following:</p>
<pre>{<br/>  "independent_variables": [<br/>    {<br/>      "name": "bmi",<br/>      "value": 0.0616962065187<br/>    },<br/>    {<br/>      "name": "ltg",<br/>      "value": 0.0199084208763<br/>    }<br/>  ]<br/>}</pre>
<p>As such, let's create the following struct types in our prediction program to decode the input attributes and marshal the output prediction:</p>
<pre>// PredictionData includes the data necessary to make<br/>// a prediction and encodes the output prediction.<br/>type PredictionData struct {<br/>    Prediction float64 `json:"predicted_diabetes_progression"`<br/>    IndependentVars []IndependentVar `json:"independent_variables"`<br/>}<br/><br/>// IndependentVar include information about and a<br/>// value for an independent variable.<br/>type IndependentVar struct {<br/>    Name string `json:"name"`<br/>    Value float64 `json:"value"`<br/>}</pre>
<p>Let's also create a function that will allow us to make a prediction based on a <kbd>ModelInfo</kbd> value that we read in from the model input directory (our persisted model, that is). This <kbd>prediction</kbd> function is shown in the following code:</p>
<pre>// Predict makes a prediction based on input JSON.<br/>func Predict(modelInfo *ModelInfo, predictionData *PredictionData) error {<br/><br/>    // Initialize the prediction value<br/>    // to the intercept.<br/>    prediction := modelInfo.Intercept<br/><br/>    // Create a map of independent variable coefficients.<br/>    coeffs := make(map[string]float64)<br/>    varNames := make([]string, len(modelInfo.Coefficients))<br/>    for idx, coeff := range modelInfo.Coefficients {<br/>        coeffs[coeff.Name] = coeff.Coefficient<br/>        varNames[idx] = coeff.Name<br/>    }<br/><br/>    // Create a map of the independent variable values.<br/>    varVals := make(map[string]float64)<br/>    for _, indVar := range predictionData.IndependentVars {<br/>        varVals[indVar.Name] = indVar.Value<br/>    }<br/><br/>    // Loop over the independent variables.<br/>    for _, varName := range varNames {<br/><br/>        // Get the coefficient.<br/>        coeff, ok := coeffs[varName]<br/>        if !ok {<br/>            return fmt.Errorf("Could not find model coefficient %s", varName)<br/>        }<br/><br/>        // Get the variable value.<br/>        val, ok := varVals[varName]<br/>        if !ok {<br/>            return fmt.Errorf("Expected a value for variable %s", varName)<br/>        }<br/><br/>        // Add to the prediction.<br/>        prediction = prediction + coeff*val<br/>    }<br/><br/>    // Add the prediction to the prediction data.<br/>    predictionData.Prediction = prediction<br/><br/>    return nil<br/>}</pre>
<p>This prediction function, along with the types, will then allow us to walk over any attribute JSON files in a specified input directory and output disease predictions for each of those files. This process is implemented in the following code:</p>
<pre>// Declare the input and output directory flags.<br/>inModelDirPtr := flag.String("inModelDir", "", "The directory containing the model.")        <br/>inVarDirPtr := flag.String("inVarDir", "", "The directory containing the input attributes.")
outDirPtr := flag.String("outDir", "", "The output directory")

// Parse the command line flags.
flag.Parse()

// Load the model file.
f, err := ioutil.ReadFile(filepath.Join(*inModelDirPtr, "model.json"))
if err != nil {
    log.Fatal(err)
}

// Unmarshal the model information.
var modelInfo ModelInfo
if err := json.Unmarshal(f, &amp;modelInfo); err != nil {
    log.Fatal(err)
}

// Walk over files in the input.
if err := filepath.Walk(*inVarDirPtr, func(path string, info os.FileInfo, err error) error {

    // Skip any directories.
    if info.IsDir() {
        return nil
    }

    // Open any files.
    f, err := ioutil.ReadFile(filepath.Join(*inVarDirPtr, info.Name()))
    if err != nil {
        return err
    }

    // Unmarshal the independent variables.
    var predictionData PredictionData
    if err := json.Unmarshal(f, &amp;predictionData); err != nil {
        return err
    }

    // Make the prediction.
    if err := Predict(&amp;modelInfo, &amp;predictionData); err != nil {
        return err
    }

    // Marshal the prediction data.
    outputData, err := json.MarshalIndent(predictionData, "", "    ")
    if err != nil {
        log.Fatal(err)
    }

    // Save the marshalled output to a file.
    if err := ioutil.WriteFile(filepath.Join(*outDirPtr, info.Name()), outputData, 0644); err != nil {
        log.Fatal(err)
    }

    return nil
}); err != nil {
    log.Fatal(err)
}</pre>
<p>We will use the same <kbd>Dockerfile</kbd> and <kbd>Makefile</kbd> that we used in the preceding section to Docker-ize this prediction program. The only difference is that we will name the Go binary <kbd>goregpredict</kbd> and we will tag the Docker image <kbd>dwhitena/goregpredict</kbd>. Building the Docker image with <kbd>make</kbd> should return a similar output to that in the preceding section:</p>
<pre><strong>$ cat Makefile </strong><br/><strong>all: compile docker push clean</strong><br/><br/><strong>compile:</strong><br/><strong>        GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -o goregpredict</strong><br/><br/><strong>docker:</strong><br/><strong>        sudo docker build --force-rm=true -t dwhitena/goregpredict .</strong><br/><br/><strong>push:</strong><br/><strong>        sudo docker push dwhitena/goregpredict</strong><br/><br/><strong>clean:</strong><br/><strong>        rm goregpredict</strong><br/><strong>$ cat Dockerfile </strong><br/><strong>FROM alpine</strong><br/><strong>ADD goregpredict /</strong><br/><strong>$ make</strong><br/><strong>GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -o goregpredict</strong><br/><strong>sudo docker build --force-rm=true -t dwhitena/goregpredict .</strong><br/><strong>[sudo] password for dwhitena: </strong><br/><strong>Sending build context to Docker daemon 2.38MB</strong><br/><strong>Step 1/2 : FROM alpine</strong><br/><strong> ---&gt; 7328f6f8b418</strong><br/><strong>Step 2/2 : ADD goregpredict /</strong><br/><strong> ---&gt; a2d9a63f4926</strong><br/><strong>Removing intermediate container c1610b425835</strong><br/><strong>Successfully built a2d9a63f4926</strong><br/><strong>Successfully tagged dwhitena/goregpredict:latest</strong><br/><strong>sudo docker push dwhitena/goregpredict</strong><br/><strong>The push refers to a repository [docker.io/dwhitena/goregpredict]</strong><br/><strong>77f12cb6c6d4: Pushed </strong><br/><strong>5bef08742407: Layer already exists </strong><br/><strong>latest: digest: sha256:9a8a754c434bf2250b2f6102bb72d56fdf723f305aebcbf5bff7e5de707dd384 size: 738</strong><br/><strong>3a05f65b1d1d: Layer already exists </strong><br/><strong>5bef08742407: Layer already exists </strong><br/><strong>ult: digest: sha256:153adaa9b4b9a1f2bf02466201163c60230ae85164d9d22261f455979a94aed4 size: 738</strong><br/><strong>rm goregpredict</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the Docker images locally</h1>
                </header>
            
            <article>
                
<p>Before pushing our Docker-ized modeling processes to any servers, it's wise to test them locally to ensure that we are seeing the behavior that we expect. Then, once we are satisfied with that behavior, we can rest assured that these Docker images will run exactly the same on any other host that is running Docker. This assurance makes the use of Docker images a significant contributor to maintaining reproducibility with our deploys.</p>
<p>Let's suppose that we have our training data and some input attribute files in the following directories:</p>
<pre><strong>$ ls</strong><br/><strong>attributes model training</strong><br/><strong>$ ls model</strong><br/><strong>$ ls attributes </strong><br/><strong>1.json 2.json 3.json</strong><br/><strong>$ ls training </strong><br/><strong>diabetes.csv</strong></pre>
<p>We can run our Docker images as software containers locally using the <kbd>docker run</kbd> command. We will also utilize the <kbd>-v</kbd> flag that will let us mount local directories inside of a running container, allowing us to read and write files to and from our local filesystem.</p>
<p>First, let's run our single regression model training inside of the Docker container as follows:</p>
<pre><strong>$ sudo docker run -v $PWD/training:/tmp/training -v $PWD/model:/tmp/model dwhitena/goregtrain:single /goregtrain -inDir=/tmp/training -outDir=/tmp/model </strong><br/><br/><strong>Regression Formula:</strong><br/><strong>Predicted = 152.13 + bmi*949.44</strong></pre>
<p>Now, if we look at what is in our <kbd>model</kbd> directory, we will see the newly trained model coefficients and intercept in <kbd>model.json</kbd>, which was output from our program running in the Docker image. This is illustrated in the following code:</p>
<pre><strong>$ ls model </strong><br/><strong>model.json</strong><br/><strong>$ cat model/model.json </strong><br/>{<br/>    "intercept": 152.13348416289818,<br/>    "coefficients": [<br/>        {<br/>            "name": "bmi",<br/>            "coefficient": 949.4352603839862<br/>        }<br/>    ]<br/>}</pre>
<p class="mce-root">Excellent! We trained our model using the Docker container. Now let's utilize this model to make predictions. Specifically, let's run our <kbd>goregpredict</kbd> Docker image using the trained model to make predictions for the three attribute files in <kbd>attributes</kbd>. In the following code, you will see that the attribute files do not have corresponding predictions before running the Docker image, but they do after running the Docker image:</p>
<pre><strong>$ cat attributes/1.json</strong><br/>{<br/>    "independent_variables": [<br/>        {<br/>            "name": "bmi",<br/>            "value": 0.0616962065187<br/>        },<br/>        {<br/>            "name": "ltg",<br/>            "value": 0.0199084208763<br/>        }<br/>    ]<br/>}<br/><strong>$ sudo docker run -v $PWD/attributes:/tmp/attributes -v $PWD/model:/tmp/model dwhitena/goregpredict /goregpredict -inModelDir=/tmp/model -inVarDir=/tmp/attributes -outDir=/tmp/attributes</strong><br/><strong>$ cat attributes/1.json </strong><br/>{<br/>    "predicted_diabetes_progression": 210.7100380636843,<br/>    "independent_variables": [<br/>        {<br/>            "name": "bmi",<br/>            "value": 0.0616962065187<br/>        },<br/>        {<br/>            "name": "ltg",<br/>            "value": 0.0199084208763<br/>        }<br/>    ]<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the Docker images on remote machines</h1>
                </header>
            
            <article>
                
<p>You might be thinking, <em>what's the big deal, you get the same functionality with Docker that we could get by just building and running your Go program</em>. Well, that's true locally, but the magic of Docker happens when we want to run our same functionality in an environment other than our laptop.</p>
<p>We can take those Docker images and run them in the exact same way on any host that is running Docker, and they will produce the same exact behavior. This is true for any Docker image. You don't have to install any of the dependencies that might be layered inside of the Docker image. All you need is Docker.</p>
<p>Guess what? Our Docker images are just about 3 MB in size! This means that they will download to any host super fast and start up and run super fast. You don't have to worry about lugging around multiple gigabyte-sized virtual machines and manually specifying resources.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a scalable and reproducible machine learning pipeline</h1>
                </header>
            
            <article>
                
<p>Docker sets up quite a bit of the way towards having our machine learning workflows deployed in our company's infrastructure. However, there are still a few missing pieces, as outlined here:</p>
<ul>
<li>How do we string the various stages of our workflow together? In this simple example, we have a training stage and a prediction stage. In other pipelines, you might also have data preprocessing, data splitting, data combining, visualization, evaluation, and so on.</li>
<li>How to get the right data to the right stages of our workflow, especially as we receive new data and/or our data changes? It's not sustainable to manually copy new attributes over to a folder that is co-located with our prediction image every time we need to make new predictions, and we cannot log in to a server every time we need to update our training set.</li>
<li>How will we be able to track and reproduce the various runs of our workflow for maintenance, continued development, or debugging? If we are making predictions over time and updating our model and/or training set over time, we need to understand which models produced which results to maintain reproducibility (and to meet compliance, in some cases).</li>
<li>How can we scale our processing over multiple machines and, in some cases, over multiple shared resources? We likely need to run our processing on some shared set of resources in our company. It would be nice for us to be able to scale our processing over these resources as we need more computation power and/or as other applications are scheduled on these resources.</li>
</ul>
<p>Thankfully, there are a couple more open source tools (both written in Go) that solve these issues for us. Not only that, they let us use the Docker images that we already built as the main units of data processing. These tools are <strong>Kubernetes</strong> (<strong>k8s</strong>) and <strong>Pachyderm</strong>.</p>
<p>You have already been exposed to Pachyderm in <a href="4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml" target="_blank">Chapter 1</a>, <em>Gathering and Organizing Data</em>. In that chapter, we utilized Pachyderm to illustrate the idea of data versioning and, as you might be guessing, Pachyderm will help us solve some of the issues around managing data, tracking, and reproducibility. Pachyderm will also solve all of the remaining issues that we need to address around scalability and pipelining because Pachyderm provides both data management/versioning capabilities and data pipelining capabilities.</p>
<p>Kubernetes is a container orchestration engine, which is amazing at scheduling containerized workloads (like our Docker images) across a cluster of shared resources. It is wildly popular right now, and Pachyderm utilizes Kubernetes under the hood to manage containerized data processing pipelines.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up a Pachyderm and Kubernetes cluster</h1>
                </header>
            
            <article>
                
<p>Pachyderm, which runs on Kubernetes, can be deployed almost anywhere because Kubernetes can be deployed anywhere. You can deploy Pachyderm on any of the popular cloud providers, on premise, or even on your own laptop. All you need to do to deploy a Pachyderm cluster is the following:</p>
<ol>
<li>Have a running Kubernetes cluster.</li>
<li>Have access to an object store of your choice (for example, S3, <strong>Glasglow Coma Scale</strong> (<strong>GCS</strong>), or Minio). This object store will serve as the storage backing for the Pachyderm cluster where all the versioning and processed data is stored.</li>
<li>Deploy Pachyderm on the Kubernetes cluster using the Pachyderm CLI <kbd>pachctl</kbd>.</li>
</ol>
<div class="packt_infobox">Detailed instructions to deploy Pachyderm on any cloud provider or on-premise can be found at <a href="http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html">http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html</a>. Alternatively, you can easily experiment with and develop against a local Pachyderm cluster using <kbd>minikube</kbd>, as further detailed at <a href="http://pachyderm.readthedocs.io/en/latest/getting_started/local_installation.html">http://pachyderm.readthedocs.io/en/latest/getting_started/local_installation.html</a>.</div>
<p>Following one set of those instructions should get you to a state where you have a Kubernetes cluster running in the following state (which can be checked using Kubernetes CLI tool called <kbd>kubectl</kbd>):</p>
<pre><strong>$ kubectl get all</strong><br/><strong>NAME READY STATUS RESTARTS AGE</strong><br/><strong>po/etcd-2142892294-38ptw 1/1 Running 0 2m</strong><br/><strong>po/pachd-776177201-04l6w 1/1 Running 0 2m</strong><br/><br/><strong>NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong><br/><strong>svc/etcd 10.0.0.141 &lt;nodes&gt; 2379:32379/TCP 2m</strong><br/><strong>svc/kubernetes 10.0.0.1 &lt;none&gt; 443/TCP 3m</strong><br/><strong>svc/pachd 10.0.0.178 &lt;nodes&gt; 650:30650/TCP,651:30651/TCP,652:30652/TCP 2m</strong><br/><br/><strong>NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong><br/><strong>deploy/etcd 1 1 1 1 2m</strong><br/><strong>deploy/pachd 1 1 1 1 2m</strong><br/><br/><strong>NAME DESIRED CURRENT READY AGE</strong><br/><strong>rs/etcd-2142892294 1 1 1 2m</strong><br/><strong>rs/pachd-776177201 1 1 1 2m</strong></pre>
<p>Here, you can see that the Pachyderm daemon (or server), <kbd>pachd</kbd>, is running in the Kubernetes cluster as a <strong>pod</strong>, which is simply a group of one or more containers. Our pipeline stages will also run as Kubernetes pods, but you won't have to worry too much about that as Pachyderm will take care of the details.</p>
<div class="packt_tip packt_infobox">The ports and IPs listed in the above output may vary depending on your deployment and various configurations. However, a healthy Pachyderm cluster should look very similar.</div>
<p>If your Pachyderm cluster is running and you followed one of the Pachyderm deploy guides, you should also have the <kbd>pachctl</kbd> CLI tool installed. When <kbd>pachctl</kbd> is successfully connected to the Pachyderm cluster, you can run the following as a further sanity check (where the version number may change based on the Pachyderm version you are running):</p>
<pre><strong>$ pachctl version
COMPONENT           VERSION             
pachctl             1.6.0           
pachd               1.6.0</strong></pre>
<div class="packt_tip">If you have any trouble getting Pachyderm deployed or have any trouble building or running pipelines, the Pachyderm community has a great public Pachyderm Slack channel. You can join by visiting <a href="http://slack.pachyderm.io/">http://slack.pachyderm.io/</a>. The community is active there every day, and any questions you have will be welcome.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a Pachyderm machine learning pipeline</h1>
                </header>
            
            <article>
                
<p>As you have already seen, our example machine learning pipeline has two stages. The model stage of our pipeline will train a model and export a persisted version of this model to a file. The prediction stage of our pipeline will utilize the trained model to make predictions for input attribute files. Overall, this pipeline will look like the following in Pachyderm:</p>
<div class="CDPAlignCenter CDPAlign"><img height="277" width="489" class="image-border" src="assets/cfb8d094-54f6-4338-b91a-4d3008681c50.png"/></div>
<p>Each of the cylinders in the preceding figure represents a Pachyderm data repository of versioned data. You were already exposed to these data repositories in <a href="4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml">Chapter 1</a>, <em>Gathering and Organizing Data</em>. Each of the boxes represents a containerized data pipeline stage. The basic units of data processing in Pachyderm are Docker images. Thus, we can utilize the Docker images that we created in preceding sections in this data pipeline.</p>
<p>By stringing together versioned collections of data (again, think about these as a sort of <em>Git for data</em>), processing these collections of data with containers, and saving results to other versioned collections of data, Pachyderm pipelines have some pretty interesting and useful implications.</p>
<p>First, we can go back at any point in time to see the state of any portion of our data at that point in time. This might help us as we further develop our model if we are wanting to develop a certain state of our data. It might also help us to collaboratively develop as a team by tracking the team's data over time. Not all changes to data are good, and we need the ability to revert after bad or corrupt data is committed into the system.</p>
<p>Next, all of our pipeline results are linked to the exact Docker images and exact states of other data that produced these results. The Pachyderm team calls this <strong>provenance</strong>. It is the ability to understand what pieces of processing and which pieces of data contributed to a particular result. For example, with this pipeline, we are able to determine the exact version of our persisted model that produced a particular result, along with the exact Docker image that produced that model and the exact state of the training data that was input to that exact Docker image. We are, thus, able to exactly reproduce an entire run of any pipeline, debug strange model behavior, and attribute results to the corresponding input data.</p>
<p>Finally, because Pachyderm knows which portions of your data repositories are new or different, our data pipeline can process data incrementally and can be triggered automatically. Let's say that we have already committed one million attribute files into the <kbd>attributes</kbd> data repository, and then we commit ten more attribute files. We don't have to reprocess the first million to update our results. Pachyderm knows that we only need to process the ten new files and will send these to the <kbd>prediction</kbd> stage to be processed. Moreover, Pachyderm will (by default) automatically trigger this update because it knows that the update is needed to keep the results in sync with the input.</p>
<div class="packt_infobox">Incremental processing and automatic triggers are some of the default behaviors of Pachyderm pipelines, but they are not the only things you can do with Pachyderm. In fact, we will only scratch the surface here of what is possible with Pachyderm. You can do distributed map/reduce style operations, distributed image processing, periodic processing of database tables, and much, much more. Thus, the techniques here should be transferable to a variety of domains.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating and filling the input repositories</h1>
                </header>
            
            <article>
                
<p>To create the Pachyderm pipeline on our deployed Pachyderm cluster, we first need to create the input repositories for the pipeline. Remember, our pipeline has the <kbd>training</kbd> and <kbd>attributes</kbd> data repositories that drive the rest of the pipeline. As we put data into these, Pachyderm will trigger the downstream portions of the pipeline and calculate the results.</p>
<p>We saw in <a href="4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml" target="_blank">Chapter 1</a>, <em>Gathering and Organizing Data</em>, how we can create data repositories in Pachyderm and commit data into these repositories using <kbd>pachctl</kbd>, but let's try to do this directly from a Go program here. In this particular example, there is not any real advantage to doing this, due to the fact that we already have our training and test data in files and we can easily commit those files into Pachyderm by name.</p>
<p>However, imagine a more realistic scenario. We might want our data pipeline to be integrated with some other Go services that we have already written at our company. One of these services might process incoming patient medical attributes from doctors or clinics, and we want to feed these attributes to our data pipeline, such that the data pipeline can make the corresponding predictions of disease progression.</p>
<p>In such a scenario, we would ideally like to pass the attributes directly into Pachyderm from our service instead of manually copying all that data. This is where the Pachyderm Go client, <kbd>github.com/pachyderm/pachyderm/src/client</kbd>, can come in very handy. Using the Pachyderm Go client, we can create our input repositories as follows (after connecting to the Pachyderm cluster):</p>
<pre>// Connect to Pachyderm using the IP of our<br/>// Kubernetes cluster. Here we will use localhost<br/>// to mimic the scenario when you have k8s running<br/>// locally and/or you are forwarding the Pachyderm<br/>// port to your localhost.. By default<br/>// Pachyderm will be exposed on port 30650.<br/>c, err := client.NewFromAddress("0.0.0.0:30650")<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>defer c.Close()<br/><br/>// Create a data repository called "training."<br/>if err := c.CreateRepo("training"); err != nil {<br/>    log.Fatal(err)<br/>}<br/><br/>// Create a data repository called "attributes."<br/>if err := c.CreateRepo("attributes"); err != nil {<br/>    log.Fatal(err)<br/>}<br/><br/>// Now, we will list all the current data repositories<br/>// on the Pachyderm cluster as a sanity check. We<br/>// should now have two data repositories.<br/>repos, err := c.ListRepo(nil)<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/><br/>// Check that the number of repos is what we expect.<br/>if len(repos) != 2 {<br/>    log.Fatal("Unexpected number of data repositories")<br/>}<br/><br/>// Check that the name of the repo is what we expect.<br/>if repos[0].Repo.Name != "attributes" || repos[1].Repo.Name != "training" {<br/>    log.Fatal("Unexpected data repository name")<br/>}</pre>
<p>Compiling this and running it creates the <kbd>repos</kbd> as expected, which can be confirmed as follows:</p>
<pre><strong>$ go build</strong><br/><strong>$ ./myprogram</strong><br/><strong>$ pachctl list-repo</strong><br/><strong>NAME CREATED SIZE </strong><br/><strong>attributes 3 seconds ago 0B </strong><br/><strong>training 3 seconds ago 0B</strong> </pre>
<div class="packt_tip packt_infobox">For simplicity, the above code just creates and checks the repo once. If you ran the code again, you would get an error because the repo was already created. To improve on this, you could check if the repo exists and then create it if it does not exist. You can also check that the repo exists by name.</div>
<p>Now, let's go ahead and put an attributes file in the <kbd>attributes</kbd> repository and the <kbd>diabetes.csv</kbd> training data set into the <kbd>training</kbd> repository. This is also very easily done directly in Go, via the Pachyderm client:</p>
<pre>// Connect to Pachyderm.
c, err := client.NewFromAddress("0.0.0.0:30650")
if err != nil {
    log.Fatal(err)
}
defer c.Close()

// Start a commit in our "attributes" data repo on the "master" branch.
commit, err := c.StartCommit("attributes", "master")
if err != nil {
    log.Fatal(err)
}

// Open one of the attributes JSON files.
f, err := os.Open("1.json")
if err != nil {
    log.Fatal(err)
}

// Put a file containing the attributes into the data repository.
if _, err := c.PutFile("attributes", commit.ID, "1.json", f); err != nil {
    log.Fatal(err)
}

// Finish the commit.
if err := c.FinishCommit("attributes", commit.ID); err != nil {
    log.Fatal(err)
}

// Start a commit in our "training" data repo on the "master" branch.
commit, err = c.StartCommit("training", "master")
if err != nil {
    log.Fatal(err)
}

// Open up the training data set.
f, err = os.Open("diabetes.csv")
if err != nil {
    log.Fatal(err)
}

// Put a file containing the training data set into the data repository.
if _, err := c.PutFile("training", commit.ID, "diabetes.csv", f); err != nil {
    log.Fatal(err)
}

// Finish the commit.
if err := c.FinishCommit("training", commit.ID); err != nil {
    log.Fatal(err)
}</pre>
<p>Running this does indeed commit the data into Pachyderm in the proper data repositories, as can be seen here:</p>
<pre><strong>$ go build</strong><br/><strong>$ ./myprogram </strong><br/><strong>$ pachctl list-repo</strong><br/><strong>NAME CREATED SIZE </strong><br/><strong>training 13 minutes ago 73.74KiB </strong><br/><strong>attributes 13 minutes ago 210B </strong><br/><strong>$ pachctl list-file training master</strong><br/><strong>NAME TYPE SIZE </strong><br/><strong>diabetes.csv file 73.74KiB </strong><br/><strong>$ pachctl list-file attributes master</strong><br/><strong>NAME TYPE SIZE </strong><br/><strong>1.json file 210B</strong></pre>
<div class="packt_tip">It is important to finish the commits that we started above. If you leave an orphaned commit hanging you might block other commits of data. Thus, you might consider deferring the finish of the commit, just to be safe.</div>
<p>Good! Now we have data in the proper input data repositories on our remote Pachyderm cluster. Next, we can actually process this data and produce results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating and running the processing stages</h1>
                </header>
            
            <article>
                
<p>Processing stages are created declaratively via a <strong>pipeline specification</strong> in Pachyderm. If you have worked with Kubernetes before, this type of interaction should be familiar. Basically, we declare to Pachyderm what processing we want to take place, and Pachyderm handles all the details to make sure that this processing happens as declared.</p>
<p>Let's first create the <kbd>model</kbd> stage of our pipeline using a JSON pipeline specification that is stored in <kbd>train.json</kbd>. This JSON specification is as follows:</p>
<pre>{
  "pipeline": {
    "name": "model"
  },
  "transform": {
    "image": "dwhitena/goregtrain:single",
    "cmd": [ 
      "/goregtrain",
      "-inDir=/pfs/training",
      "-outDir=/pfs/out"
    ] 
  },
  "parallelism_spec": {
    "constant": "1"
  },
  "input": {
    "atom": {
      "repo": "training",
      "glob": "/"
    }
  }
}</pre>
<p>This might look somewhat complicated, but there are really only a few things happening here. Let's dissect the specification to learn about the different pieces:</p>
<ol>
<li>First, we are telling Pachyderm to name our pipeline <kbd>model</kbd>.</li>
</ol>
<p> </p>
<p> </p>
<ol start="2">
<li>Next, we are telling Pachyderm that we want this <kbd>model</kbd> pipeline to process data using our <kbd>dwhitena/goregtrain:single</kbd>, and we want the pipeline to run our <kbd>goregtrain</kbd> binary in the container when processing the data.</li>
<li>Finally, the specification tells Pachyderm that we intend to process the <kbd>training</kbd> data repository as input.</li>
</ol>
<p>To understand some of the other details of the specification, we need to first discuss what will happen when we create the pipeline on our Pachyderm cluster using the specification. When this happens, Pachyderm will use Kubernetes to schedule one or more worker pods on the Kubernetes cluster. These worker pods will then wait, ready to process any data supplied to them by the Pachyderm daemon. When there is data that needs to be processed in the input repository, <kbd>training</kbd>, the Pachyderm daemon will trigger a job in which these workers will process the data. Pachyderm will automatically mount the input data in the container at <kbd>/pfs/&lt;input repo name&gt;</kbd> (<kbd>/pfs/training</kbd> in this case). Data the container writes out to <kbd>/pfs/out</kbd> will be automatically versioned in an output data repository with the same name as the pipeline (<kbd>model</kbd> in this case).</p>
<p>The <kbd>parallelism_spec</kbd> portion of the specification lets us tell Pachyderm how many workers it should spin up to process the input data. Here we will just spin up a single worker and process the data serially. Later in the chapter, we will discuss parallelizing our pipeline and the associated data sharding, which is controlled by the <kbd>glob</kbd> pattern in the <kbd>input</kbd> section of the specification.</p>
<p>Enough talk though! Let's actually create this <kbd>model</kbd> pipeline stage on our Pachyderm cluster and get to processing some data. The pipeline can easily be created as follows:</p>
<pre><strong>$ pachctl create-pipeline -f model.json</strong></pre>
<p>When we do this, we can see the worker(s) that Pachyderm creates on the Kubernetes cluster using <kbd>kubectl</kbd> as shown in the following code:</p>
<pre><strong>$ kubectl get pods</strong><br/><strong>NAME READY STATUS RESTARTS AGE</strong><br/><strong>etcd-2142892294-38ptw 1/1 Running 0 2h</strong><br/><strong>pachd-776177201-04l6w 1/1 Running 0 2h</strong><br/><strong>pipeline-model-v1-p0lnf 2/2 Running 0 1m</strong></pre>
<p>We will also see that Pachyderm has automatically triggered a job to perform our model training. Remember, this happens because Pachyderm knows that there is data in the input repository, <kbd>training</kbd>, that has not been processed. You can see the triggered job along with its results as follows:</p>
<pre><strong>$ pachctl list-job</strong><br/><strong>ID OUTPUT COMMIT STARTED DURATION RESTART PROGRESS DL UL STATE </strong><br/><strong>14f052ae-878d-44c9-a1f9-ab0cf6d45227 model/a2c7b7dfb44a40e79318c2de30c7a0c8 3 minutes ago Less than a second 0 1 + 0 / 1 73.74KiB 160B success </strong><br/><strong>$ pachctl list-repo</strong><br/><strong>NAME CREATED SIZE </strong><br/><strong>model 3 minutes ago 160B </strong><br/><strong>training About an hour ago 73.74KiB </strong><br/><strong>attributes About an hour ago 210B </strong><br/><strong>$ pachctl list-file model master</strong><br/><strong>NAME TYPE SIZE k8s</strong><br/><strong>model.json file 160B </strong><br/><strong>$ pachctl get-file model master model.json</strong><br/>{<br/>    "intercept": 152.13348416289818,<br/>    "coefficients": [<br/>        {<br/>            "name": "bmi",<br/>            "coefficient": 949.4352603839862<br/>        }<br/>    ]<br/>}</pre>
<p>Great! We can see that our model training produced the results that we expected. However, Pachyderm was smart enough to trigger the pipeline, and it will update this model whenever we modify the training dataset.</p>
<p>The <kbd>prediction</kbd> pipeline can be created in a similar manner. Its pipeline specification, <kbd>pipeline.json</kbd>, is shown here:</p>
<pre>{<br/>  "pipeline": {<br/>    "name": "prediction"<br/>  },<br/>  "transform": {<br/>    "image": "dwhitena/goregpredict",<br/>    "cmd": [ <br/>      "/goregpredict", <br/>      "-inModelDir=/pfs/model", <br/>      "-inVarDir=/pfs/attributes", <br/>      "-outDir=/pfs/out" <br/>    ]<br/>  },<br/>  "parallelism_spec": {<br/>    "constant": "1"<br/>  },<br/>  "input": {<br/>    "cross": [<br/>      {<br/>        "atom": {<br/>          "repo": "attributes",<br/>          "glob": "/*"<br/>        }<br/>      },<br/>      {<br/>        "atom": {<br/>          "repo": "model",<br/>          "glob": "/"<br/>        }<br/>      }<br/>    ]<br/>  }<br/>}</pre>
<p>The major difference in this pipeline specification is that we have two input repositories rather than one. These two input repositories are the <kbd>attributes</kbd> repository that we previously created and the <kbd>model</kbd> repository that contains the output of the <kbd>model</kbd> pipeline. These are combined using a <kbd>cross</kbd> primitive. <kbd>cross</kbd> ensures that all relevant pairs of our model and attribute files are processed.</p>
<div class="packt_tip">If you are interested, you can find out more about the other ways to combine data in Pachyderm in the Pachyderm docs; go to <a href="http://pachyderm.readthedocs.io/en/latest/">http://pachyderm.readthedocs.io/en/latest/</a>.</div>
<p>Creating this pipeline and examining the results (corresponding to <kbd>1.json</kbd>) can be accomplished as follows:</p>
<pre><strong>$ pachctl create-pipeline -f prediction.json </strong><br/><strong>$ pachctl list-job</strong><br/><strong>ID OUTPUT COMMIT STARTED DURATION RESTART PROGRESS DL UL STATE </strong><br/><strong>03f36398-89db-4de4-ad3d-7346d56883c0 prediction/5ce47c9e788d4893ae00c7ee6b1e8431 About a minute ago Less than a second 0 1 + 0 / 1 370B 266B success </strong><br/><strong>14f052ae-878d-44c9-a1f9-ab0cf6d45227 model/a2c7b7dfb44a40e79318c2de30c7a0c8 19 minutes ago Less than a second 0 1 + 0 / 1 73.74KiB 160B success </strong><br/><strong>$ pachctl list-repo</strong><br/><strong>NAME CREATED SIZE </strong><br/><strong>prediction About a minute ago 266B </strong><br/><strong>model 19 minutes ago 160B </strong><br/><strong>training About an hour ago 73.74KiB </strong><br/><strong>attributes About an hour ago 210B </strong><br/><strong>$ pachctl list-file prediction master</strong><br/><strong>NAME TYPE SIZE </strong><br/><strong>1.json file 266B </strong><br/><strong>$ pachctl get-file prediction master 1.json</strong><br/>{<br/>    "predicted_diabetes_progression": 210.7100380636843,<br/>    "independent_variables": [<br/>        {<br/>            "name": "bmi",<br/>            "value": 0.0616962065187<br/>        },<br/>        {<br/>            "name": "ltg",<br/>            "value": 0.0199084208763<br/>        }<br/>    ]<br/>}</pre>
<p>You can see that another data repository, <kbd>prediction</kbd>, was created to version the output of the <kbd>prediction</kbd> pipeline. In this manner, Pachyderm gradually builds up links between input data repositories, data processing stages, and output data repositories, such that it always knows what data and processing components are linked.</p>
<p>The full machine learning pipeline is now deployed and we have run each stage of the pipeline once. However, the pipeline is ready and waiting to process more data. All we have to do is put data at the top of the pipeline and Pachyderm will automatically trigger any downstream processing that needs to take place to update our results. Let's illustrate this by committing two more files into the <kbd>attributes</kbd> repository, as follows:</p>
<pre><strong>$ ls</strong><br/><strong>2.json 3.json</strong><br/><strong>$ pachctl put-file attributes master -c -r -f .</strong><br/><strong>$ pachctl list-file attributes master</strong><br/><strong>NAME TYPE SIZE </strong><br/><strong>1.json file 210B </strong><br/><strong>2.json file 211B </strong><br/><strong>3.json file 211B</strong></pre>
<p>This will automatically trigger another prediction job to update our results. The new job can be seen in the following code:</p>
<pre><strong>$ pachctl list-job</strong><br/><strong>ID OUTPUT COMMIT STARTED DURATION RESTART PROGRESS DL UL STATE </strong><br/><strong>be71b034-9333-4f75-b443-98d7bc5b48ab prediction/bb2fd223df664e70ad14b14491109c0f About a minute ago Less than a second 0 2 + 1 / 3 742B 536B success </strong><br/><strong>03f36398-89db-4de4-ad3d-7346d56883c0 prediction/5ce47c9e788d4893ae00c7ee6b1e8431 8 minutes ago Less than a second 0 1 + 0 / 1 370B 266B success </strong><br/><strong>14f052ae-878d-44c9-a1f9-ab0cf6d45227 model/a2c7b7dfb44a40e79318c2de30c7a0c8 26 minutes ago Less than a second 0 1 + 0 / 1 73.74KiB 160B success</strong></pre>
<p>Also, we can see that the <kbd>prediction</kbd> repository has two new results from running our prediction code again on the two new input files:</p>
<pre><strong>$ pachctl list-file prediction master</strong><br/><strong>NAME TYPE SIZE </strong><br/><strong>1.json file 266B </strong><br/><strong>2.json file 268B </strong><br/><strong>3.json file 268B</strong></pre>
<div class="packt_infobox">Note that Pachyderm did not reprocess <kbd>1.json</kbd> here even though it is shown with the latest results. Under the hood, Pachyderm knows that there was already a result corresponding to <kbd>1.json</kbd> in <kbd>attributes</kbd>, so there is no need to reprocess it.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating pipelines and examining provenance</h1>
                </header>
            
            <article>
                
<p>Over time, we are likely going to want to update our machine learning models. The data that they are processing may have changed over time or maybe we have just developed a different or better model. In any event, it is likely that we are going to need to update our pipeline.</p>
<p>Let's assume that our pipeline is in the state described in the previous section, where we have created the full pipeline, the training process has run once, and we have processed two commits into the <kbd>attributes</kbd> repository. Now, let's update the model pipeline to train our multiple regression</p>
<p>Now, let's update the model pipeline to train our multiple regression models instead of our single regression model. This is actually really easy. We just need to change <kbd>"image": "dwhitena/goregtrain:single"</kbd> in our <kbd>model.json</kbd> pipeline specification to <kbd>"image": "dwhitena/goregtrain:multi"</kbd> and then update the pipeline as follows:</p>
<pre><strong>$ pachctl update-pipeline -f model.json --reprocess</strong></pre>
<p>When we give Pachyderm this new specification, Pachyderm will automatically update the worker pods such that they are running the containers with the multiple regression model. Also, because we supplied the <kbd>--reprocess</kbd> flag, Pachyderm will trigger one or more new jobs to reprocess any input commits with the new image that was previously processed with the old image. We can see these reprocessing jobs as follows:</p>
<pre><strong>$ pachctl list-job</strong><br/><strong>ID OUTPUT COMMIT STARTED DURATION RESTART PROGRESS DL UL STATE </strong><br/><strong>03667301-980b-484b-afbe-7ea30da695f5 prediction/ef32a74b04ee4edda7bc2a2b469f3e03 2 minutes ago Less than a second 0 3 + 0 / 3 1.355KiB 803B success </strong><br/><strong>5587e13c-4854-4f3a-bc4c-ae88c65c007f model/f54ca1a0142542579c1543e41f5e9403 2 minutes ago Less than a second 0 1 + 0 / 1 73.74KiB 252B success </strong><br/><strong>be71b034-9333-4f75-b443-98d7bc5b48ab prediction/bb2fd223df664e70ad14b14491109c0f 16 minutes ago Less than a second 0 2 + 1 / 3 742B 536B success </strong><br/><strong>03f36398-89db-4de4-ad3d-7346d56883c0 prediction/5ce47c9e788d4893ae00c7ee6b1e8431 23 minutes ago Less than a second 0 1 + 0 / 1 370B 266B success </strong><br/><strong>14f052ae-878d-44c9-a1f9-ab0cf6d45227 model/a2c7b7dfb44a40e79318c2de30c7a0c8 41 minutes ago Less than a second 0 1 + 0 / 1 73.74KiB 160B success</strong></pre>
<p>Looking at this output, we can notice something interesting and very useful. When our <kbd>model</kbd> stage trained our new multiple regression model and output this model as <kbd>model.json</kbd>, Pachyderm saw that the results of our <kbd>prediction</kbd> pipeline were now out of sync with the latest model. As such, Pachyderm automatically triggered another job to update our previous results using the new multiple regression models.</p>
<p>This workflow is super useful to manage <span>both</span> deployed models and during the development of models, because you do not need to manually shuffle around a bunch of model versions and related data. Everything happens automatically. However, a natural question arises from this update. How are we to know which models produced which results?</p>
<p>Well, this is where Pachyderm's data versioning combined with the pipelining really shines. We can look at any prediction result and inspect that commit to determine the commit's provenance. This is illustrated here:</p>
<pre><strong>$ pachctl inspect-commit prediction ef32a74b04ee4edda7bc2a2b469f3e03</strong><br/><strong>Commit: prediction/ef32a74b04ee4edda7bc2a2b469f3e03</strong><br/><strong>Parent: bb2fd223df664e70ad14b14491109c0f </strong><br/><strong>Started: 8 minutes ago</strong><br/><strong>Finished: 8 minutes ago </strong><br/><strong>Size: 803B</strong><br/><strong>Provenance: training/e0f9357455234d8bb68540af1e8dde81 attributes/2e5fef211f14498ab34c0520727296bb model/f54ca1a0142542579c1543e41f5e9403</strong></pre>
<p>In this case, we can see that prediction <kbd>ef32a74b04ee4edda7bc2a2b469f3e03</kbd> resulted from model <kbd>f54ca1a0142542579c1543e41f5e9403</kbd>, which can be retrieved here:</p>
<pre><strong>$ pachctl get-file model f54ca1a0142542579c1543e41f5e9403 model.json</strong><br/>{<br/>    "intercept": 152.13348416289583,<br/>    "coefficients": [<br/>        {<br/>            "name": "bmi",<br/>            "coefficient": 675.069774431606<br/>        },<br/>        {<br/>            "name": "ltg",<br/>            "coefficient": 614.9505047824742<br/>        }<br/>    ]<br/>}</pre>
<p>However, if we look at a previous prediction, we will see that it resulted from a different model:</p>
<pre><strong>$ pachctl inspect-commit prediction bb2fd223df664e70ad14b14491109c0f</strong><br/><strong>Commit: prediction/bb2fd223df664e70ad14b14491109c0f</strong><br/><strong>Parent: 5ce47c9e788d4893ae00c7ee6b1e8431 </strong><br/><strong>Started: 25 minutes ago</strong><br/><strong>Finished: 25 minutes ago </strong><br/><strong>Size: 802B</strong><br/><strong>Provenance: model/a2c7b7dfb44a40e79318c2de30c7a0c8 training/e0f9357455234d8bb68540af1e8dde81 attributes/2e5fef211f14498ab34c0520727296bb </strong><br/><strong>$ pachctl get-file model a2c7b7dfb44a40e79318c2de30c7a0c8 model.json</strong><br/>{<br/>    "intercept": 152.13348416289818,<br/>    "coefficients": [<br/>        {<br/>            "name": "bmi",<br/>            "coefficient": 949.4352603839862<br/>        }<br/>    ]<br/>}</pre>
<p>We can always trace back the provenance of any piece of data in our pipeline, no matter how many jobs are run and how many times we update our pipeline. This is very important in creating sustainable machine learning workflows that can be maintained over time, improved upon, and updated without major headaches.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling pipeline stages</h1>
                </header>
            
            <article>
                
<p>Each pipeline specification in Pachyderm can have a corresponding <kbd>parallelism_spec</kbd> field. This field, along with the <kbd>glob</kbd> patterns in your inputs, lets you parallelize your pipeline stages over their input data. Each pipeline stage is individually scalable independent of all the other pipeline stages.</p>
<p>The <kbd>parallelism_spec</kbd> field in the pipeline specification lets you control how many workers Pachyderm will spin up to process data in that pipeline stage. For example, the following <kbd>parallelism_spec</kbd> would tell Pachyderm to spin up 10 workers for a pipeline:</p>
<pre>  "parallelism_spec": {<br/>    "constant": "10"<br/>  },</pre>
<p>The <kbd>glob</kbd> patterns in your inputs tell Pachyderm how it can share your input data over the workers declared by the <kbd>parallelism_spec</kbd>. For example, a <kbd>glob</kbd> pattern of <kbd>/*</kbd> would tell Pachyderm that it can split up your input data into datums consisting of any files of directories at the root of your repository. A glob pattern of <kbd>/*/*</kbd> would tell Pachyderm that it can split up your data into datums consisting of files or directories at a level deep into your repository directory structure. If you are new to glob patterns, check out this description with some examples: <a href="https://en.wikipedia.org/wiki/Glob_(programming)">https://en.wikipedia.org/wiki/Glob_(programming)</a>.</p>
<p>In the case of our example pipeline, we could imagine that we need to scale the <kbd>prediction</kbd> stage of our pipeline because we are seeing a huge influx of attributes that need corresponding predictions. If this was the case, we could change <kbd>parallelism_spec</kbd> in our <kbd>prediction.json</kbd> pipeline specification to <kbd>"constant": "10"</kbd>. As soon as we update the pipeline with the new specification, Pachyderm will spin up 10 new workers for the <kbd>prediction</kbd> pipeline, as follows:</p>
<pre><strong>$ pachctl update-pipeline -f prediction.json</strong><br/><strong> $ kubectl get pods</strong><br/><strong> NAME READY STATUS RESTARTS AGE</strong><br/><strong> etcd-2142892294-38ptw 1/1 Running 0 3h</strong><br/><strong> pachd-776177201-04l6w 1/1 Running 0 3h</strong><br/><strong> pipeline-model-v2-168k5 2/2 Running 0 29m</strong><br/><strong> pipeline-prediction-v2-0p6zs 0/2 Init:0/1 0 6s</strong><br/><strong> pipeline-prediction-v2-3fbsc 0/2 Init:0/1 0 6s</strong><br/><strong> pipeline-prediction-v2-c3m4f 0/2 Init:0/1 0 6s</strong><br/><strong> pipeline-prediction-v2-d11b9 0/2 Init:0/1 0 6s</strong><br/><strong> pipeline-prediction-v2-hjdll 0/2 Init:0/1 0 6s</strong><br/><strong> pipeline-prediction-v2-hnk64 0/2 Init:0/1 0 6s</strong><br/><strong> pipeline-prediction-v2-q29f1 0/2 Init:0/1 0 6s</strong><br/><strong> pipeline-prediction-v2-qlhm4 0/2 Init:0/1 0 6s</strong><br/><strong> pipeline-prediction-v2-qrnf9 0/2 Init:0/1 0 6s</strong><br/><strong> pipeline-prediction-v2-rdt81 0/2 Init:0/1 0 6s</strong><br/><strong> $ kubectl get pods</strong><br/><strong> NAME READY STATUS RESTARTS AGE</strong><br/><strong> etcd-2142892294-38ptw 1/1 Running 0 3h</strong><br/><strong> pachd-776177201-04l6w 1/1 Running 0 3h</strong><br/><strong> pipeline-model-v2-168k5 2/2 Running 0 30m</strong><br/><strong> pipeline-prediction-v2-0p6zs 2/2 Running 0 26s</strong><br/><strong> pipeline-prediction-v2-3fbsc 2/2 Running 0 26s</strong><br/><strong> pipeline-prediction-v2-c3m4f 2/2 Running 0 26s</strong><br/><strong> pipeline-prediction-v2-d11b9 2/2 Running 0 26s</strong><br/><strong> pipeline-prediction-v2-hjdll 2/2 Running 0 26s</strong><br/><strong> pipeline-prediction-v2-hnk64 2/2 Running 0 26s</strong><br/><strong> pipeline-prediction-v2-q29f1 2/2 Running 0 26s</strong><br/><strong> pipeline-prediction-v2-qlhm4 2/2 Running 0 26s</strong><br/><strong> pipeline-prediction-v2-qrnf9 2/2 Running 0 26s</strong><br/><strong> pipeline-prediction-v2-rdt81 2/2 Running 0 26s</strong></pre>
<p>Now, any new commits of attributes into the <kbd>attributes</kbd> repo will be processed in parallel by the 10 workers. For example, if we committed 100 more attribute files into <kbd>attributes</kbd>, Pachyderm would send 1/10 of those files to each of the 10 workers to process in parallel. All of the results would still be seen the same way in the <kbd>prediction</kbd> repo.</p>
<p>Of course, setting a constant number of workers is not the only way you can do scaling with Pachyderm and Kubernetes. Pachyderm will let you scale down workers as well by setting a scale down threshold for idle workers. In addition, Pachyderm auto-scaling of workers can be combined with auto-scaling of Kubernetes cluster resources to deal with bursts of data and batch processing. Still, further, Pachyderm will allow you to specify the resources needed for pipelines and, using this resource specification, you can offload machine learning workloads to accelerators (GPUs, for example).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p>Docker:</p>
<ul>
<li>Introduction to Docker: <a href="https://training.docker.com/introduction-to-docker">https://training.docker.com/introduction-to-docker</a></li>
<li>Building minimal Docker images for Go apps: <a href="https://blog.codeship.com/building-minimal-docker-containers-for-go-applications/">https://blog.codeship.com/building-minimal-docker-containers-for-go-applications/</a></li>
</ul>
<p>Pachyderm:</p>
<ul>
<li>Public Slack channel: <a href="http://slack.pachyderm.io/">http://slack.pachyderm.io/</a></li>
<li>Getting started guide: <a href="http://pachyderm.readthedocs.io/en/latest/getting_started/getting_started.html">http://pachyderm.readthedocs.io/en/latest/getting_started/getting_started.html</a></li>
<li>Go client docs: <a href="https://godoc.org/github.com/pachyderm/pachyderm/src/client">https://godoc.org/github.com/pachyderm/pachyderm/src/client</a> <a href="https://godoc.org/github.com/pachyderm/pachyderm/src/client"/></li>
<li>Machine learning with Pachyderm: <a href="http://pachyderm.readthedocs.io/en/latest/cookbook/ml.html">http://pachyderm.readthedocs.io/en/latest/cookbook/ml.html</a></li>
<li>Machine learning examples: <a href="http://pachyderm.readthedocs.io/en/latest/examples/readme.html#machine-learning">http://pachyderm.readthedocs.io/en/latest/examples/readme.html#machine-learning</a></li>
<li>Distributed processing with Pachyderm: <a href="http://pachyderm.readthedocs.io/en/latest/fundamentals/distributed_computing.html">http://pachyderm.readthedocs.io/en/latest/fundamentals/distributed_computing.html</a></li>
<li>Common Pachyderm deployments: <a href="http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html">http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html</a></li>
<li>Auto-scaling Pachyderm clusters: <a href="http://pachyderm.readthedocs.io/en/latest/managing_pachyderm/autoscaling.html">http://pachyderm.readthedocs.io/en/latest/managing_pachyderm/autoscaling.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>From merely gathering CSV data to a fully deployed, distributed machine learning pipeline, we did it! We can now build machine learning models with Go and deploy these models with confidence on a cluster of machines. These patterns should allow you to build and deploy a whole variety of intelligent applications, and I can't wait to hear about what you create! Please stay in touch.</p>


            </article>

            
        </section>
    </body></html>