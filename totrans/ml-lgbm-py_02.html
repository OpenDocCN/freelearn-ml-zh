<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer024">
<h1 class="chapter-number" id="_idParaDest-36"><a id="_idTextAnchor036"/>2</h1>
<h1 id="_idParaDest-37"><a id="_idTextAnchor037"/>Ensemble Learning – Bagging and Boosting</h1>
<p>In the previous chapter, we covered the <a id="_idIndexMarker081"/>fundamentals of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), working with data and models, and concepts such as overfitting <a id="_idIndexMarker082"/>and <strong class="bold">supervised learning</strong> (<strong class="bold">SL</strong>). We also introduced decision trees and saw how to apply them practically <span class="No-Break">in scikit-learn.</span></p>
<p>In this chapter, we will learn about ensemble learning and the two most significant types of ensemble learning: bagging and boosting. We will cover the theory and practice of applying ensemble learning to decision trees and conclude the chapter by focusing on more advanced <span class="No-Break">boosting methods.</span></p>
<p>By the end of this chapter, you will have a good understanding of ensemble learning and how to practically build decision tree ensembles through bagging or boosting. We will also be ready to dive deep into LightGBM, including its more advanced <span class="No-Break">theoretical aspects.</span></p>
<p>The main topics we will cover are set <span class="No-Break">out here:</span></p>
<ul>
<li><span class="No-Break">Ensemble learning</span></li>
<li>Bagging and <span class="No-Break">random forests</span></li>
<li><strong class="bold">Gradient-boosted decision </strong><span class="No-Break"><strong class="bold">trees</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GBDTs</strong></span><span class="No-Break">)</span></li>
<li>Advanced boosting algorithm—<strong class="bold">Dropouts meet Multiple Additive Regression </strong><span class="No-Break"><strong class="bold">Trees</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DART</strong></span><span class="No-Break">)</span></li>
</ul>
<h1 id="_idParaDest-38"><a id="_idTextAnchor038"/>Technical requirements</h1>
<p>The chapter includes examples of simple ML algorithms and introduces working with scikit-learn. You must install a Python environment with scikit-learn, NumPy, pandas, and Jupyter. The code for this chapter is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-2"><span class="No-Break">https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-2</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor039"/>Ensemble learning</h1>
<p><strong class="bold">Ensemble learning</strong> is the <a id="_idIndexMarker083"/>practice of combining multiple predictors, or models, to create a more robust model. Models can either be of the same type (homogenous ensembles) or different types (heterogenous ensembles). Further, ensemble learning is not specific to decision trees and can be applied to any ML technique, including linear<a id="_idIndexMarker084"/> models, <strong class="bold">neural networks</strong> (<strong class="bold">NNs</strong>), <span class="No-Break">and more.</span></p>
<p>The central idea behind ensemble learning is that by aggregating the results of many models, we compensate for the weaknesses of a <span class="No-Break">single model.</span></p>
<p>Of course, training the same models on the same data is not helpful in an ensemble (as the models will have similar predictions). Therefore, we<a id="_idIndexMarker085"/> aim for <strong class="bold">diversity</strong> in the models. Diversity refers to the degree to which each model in the ensemble differs. A high-diversity ensemble has widely <span class="No-Break">different models.</span></p>
<p>There are several ways we can ensure diversity in our ensemble. One method is to train models on different subsets of the training data. Each model is exposed to different patterns and noise in the training data, increasing the diversity of the <span class="No-Break">trained models.</span></p>
<p>Similarly, we can train each model on a different subset of features in the training data. Some features are more valuable than others, and some might be irrelevant, leading to diversity in the <span class="No-Break">model predictions.</span></p>
<p>We can also train each model with different hyperparameters, leading to different models of varying complexity and ability. The impact of hyperparameters is especially pronounced in the case of decision trees, where the hyperparameters significantly impact the structure of the trees, leading to very <span class="No-Break">different models.</span></p>
<p>Finally, we could diversify the ensemble by using different types of models. Each model has unique strengths and weaknesses, leading to diversity in <span class="No-Break">the ensemble.</span></p>
<p>The <strong class="bold">ensemble learning method</strong> refers<a id="_idIndexMarker086"/> to how we introduce diversity in ensemble models by specifying how we train the member models and how we combine the results of the models. The most common ensemble methods are set <span class="No-Break">out here:</span></p>
<ul>
<li><strong class="bold">Bootstrap aggregation (bagging)</strong>: These are methods where models are trained on <a id="_idIndexMarker087"/>subsets of the training data (either samples or features), and the predictions <span class="No-Break">are aggregated.</span></li>
<li><strong class="bold">Boosting</strong>: This involves iteratively training models on the errors of previous models. The final prediction is made by combining the prediction of all models in <span class="No-Break">the chain.</span></li>
<li><strong class="bold">Stacking</strong>: This involves methods where multiple base models are trained, then a higher-order model (known as a meta-model) is trained to learn from the base model predictions and make the <span class="No-Break">final prediction.</span></li>
<li><strong class="bold">Blending</strong>: This is very similar to stacking. However, the meta-model is trained on predictions made by the base models on a <em class="italic">hold-out set</em> (a part of the training data the base learners were not trained on) instead of the whole <span class="No-Break">training set.</span></li>
</ul>
<p>The purpose of ensemble learning methods is to improve our prediction performance, and there are several ways ensembles improve the performance over individual models, as <span class="No-Break">outlined here:</span></p>
<ul>
<li><strong class="bold">Improved accuracy</strong>: By<a id="_idIndexMarker088"/> combining predictions, we increase the likelihood that the final prediction is accurate as, in aggregate, models make <span class="No-Break">fewer mistakes.</span></li>
<li><strong class="bold">Improved generalization and overfitting</strong>: By aggregating predictions, we reduce the variance in the final prediction, improving generalization. Further, in some ensemble methods, the models cannot access all data (bagging ensembles), reducing noise <span class="No-Break">and overfitting.</span></li>
<li><strong class="bold">Improved prediction stability</strong>: The aggregation of predictions reduces the random fluctuations of individual predictions. Ensembles are less sensitive to outliers, and outlying predictions of member models have a limited impact on the <span class="No-Break">final prediction.</span></li>
</ul>
<p>Decision trees are<a id="_idIndexMarker089"/> well suited for ensemble learning, and decision tree-specific algorithms exist for ensemble learning. The following section discusses bagging ensembles in decision trees focusing on <span class="No-Break"><strong class="bold">random forests</strong></span><span class="No-Break">.</span></p>
<h1 id="_idParaDest-40"><a id="_idTextAnchor040"/>Bagging and random forests</h1>
<p><strong class="bold">Bagging</strong> is an<a id="_idIndexMarker090"/> ensemble <a id="_idIndexMarker091"/>method where multiple models are trained on subsets of the training data. The models’ predictions are combined to make a final prediction, usually by taking the average for numerical prediction (for regression) or the majority vote for a class (for classification). When training each model, we select a subset of data from the original training dataset with replacement—that is, a specific training pattern can be a member of multiple subsets. Since each model is only presented with a sample of the training data, no single model can “memorize” the training data, which reduces overfitting. The following diagram illustrates the <span class="No-Break">bagging process:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<img alt="Figure 2.1 – Illustration of the bagging process; each independent classifier is trained on a random subsample from the training data and a final prediction is made by aggregating the predictions of all classifiers" height="853" src="image/B16690_02_1.jpg" width="1122"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Illustration of the bagging process; each independent classifier is trained on a random subsample from the training data and a final prediction is made by aggregating the predictions of all classifiers</p>
<p>Each model in a bagging <a id="_idIndexMarker092"/>ensemble is still a complete model, capable of standing on its own. As such, bagging works best with strong models—that is, in the case of decision trees, deep or wide <span class="No-Break">decision trees.</span></p>
<p>While the previous example illustrates sampling patterns from the training set, it is also possible to subsample random features from the dataset for each model. Selecting features at random when creating a training set is known as a random subspace method or feature bagging. Feature bagging prevents occurrences where a specific attribute might dominate the prediction or mislead the model and further <span class="No-Break">reduces overfitting.</span></p>
<p>In decision trees, a popular algorithm that applies both sample bagging and feature bagging is the random forest. Let’s have a look at this <span class="No-Break">algorithm now.</span></p>
<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/>Random forest</h2>
<p><strong class="bold">Random forest</strong> is a decision<a id="_idIndexMarker093"/> tree-specific bagging ensemble learning method [1]. Instead of building a single decision tree, as the name implies, many decision trees are trained using bagging: each tree is trained on either random samples, random features from the training data, or both. Random forests support both classification <span class="No-Break">and regression.</span></p>
<p>The training methodology for individual trees in random forests is the same as for a single decision tree, and as explained previously, each tree is a complete tree. The final prediction for the forest is made by taking the arithmetic mean of all trees in the case of prediction or the majority vote for a class (in the case <span class="No-Break">of classification).</span></p>
<p>Regarding performance, random forest learning produces a more robust model with higher accuracy, which tends to avoid overfitting (since no single model can overfit on all <span class="No-Break">training data).</span></p>
<h3>Random forest hyperparameters</h3>
<p>In scikit-learn, as may<a id="_idIndexMarker094"/> be expected, the hyperparameters available for random forests are the same as those available to train decision trees. We can specify <strong class="source-inline">max_depth</strong>, <strong class="source-inline">min_samples_split</strong>, <strong class="source-inline">max_leaf_nodes</strong>, and so on, which are then used to train the individual trees. However, there are three notable additional parameters, <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="source-inline">n_estimators</strong>: Controls the number of trees in the forest. Generally, more trees are better. However, a point of diminishing returns is <span class="No-Break">often reached.</span></li>
<li><strong class="source-inline">max_features</strong>: Determines the maximum number of features to be used as a subset when splitting a node. Setting <strong class="source-inline">max_features=1.0</strong> allows all features to be used in the <span class="No-Break">random selection.</span></li>
<li><strong class="source-inline">bootstrap</strong> determines whether bagging is used. All trees use the entire training set if <strong class="source-inline">bootstrap</strong> is set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">False</strong></span><span class="No-Break">.</span></li>
</ul>
<p>A list of all parameters available in scikit-learn is available <span class="No-Break">here: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml<span id="_idTextAnchor042"/></span></a><span class="No-Break">.</span></p>
<h3>ExtraTrees</h3>
<p><strong class="bold">Extremely Randomized Trees</strong> (<strong class="bold">ExtraTrees</strong>) is a<a id="_idIndexMarker095"/> related method for building randomized decision<a id="_idIndexMarker096"/> trees. With ExtraTrees, when building a decision node, several candidate splits are created randomly instead of using the <em class="italic">Gini index</em> or <em class="italic">information gain</em> metric to calculate the optimal split [2]. The best split from all random splits is then chosen for the node. The methodology for ExtraTrees can be applied to a single decision tree or used in conjunction with Random Forests. scikit-learn implements ExtraTrees as an extension to random <span class="No-Break">forests (</span><span class="No-Break">https://scikit-learn.org/stable/modules/ensemble.xhtml#extremely-randomized-trees</span><span class="No-Break">).</span></p>
<p>In scikit-learn, the ExtraTrees implementation has the same hyperparameters as <span class="No-Break">random forests.</span></p>
<h3>Training random forests using scikit-learn</h3>
<p>We’ll now have <a id="_idIndexMarker097"/>a look at using random forests <span class="No-Break">with scikit-learn.</span></p>
<p>In this example, we’ll use the <em class="italic">Forest CoverType</em> dataset (<a href="https://archive.ics.uci.edu/ml/datasets/Covertype">https://archive.ics.uci.edu/ml/datasets/Covertype</a>), which is available wit<a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.xhtml%23sklearn.datasets.fetch_covtype">hin scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.xhtml#sklearn.data</a>sets.fetch_covtype). The dataset is a significant step up from the toy datasets we were using previously. The dataset consists of 581,012 samples and has a dimensionality (number of features) of 54. The features describe a 30x30m patch of forest in the US (for example, elevation, aspect, slope, and distances to hydrology). We must build a classifier to classify each patch into one of seven classes describing the forest <span class="No-Break">cover type.</span></p>
<p>In addition to training a <strong class="source-inline">RandomForestClassifier</strong>, we’ll train a standalone <strong class="source-inline">DecisionTreeClassifier</strong> and an <strong class="source-inline">ExtraTreesClassifier</strong> and compare the performance of <span class="No-Break">the algorithms.</span></p>
<p><strong class="source-inline">RandomForestClassifier</strong> and <strong class="source-inline">ExtraTreesClassifier</strong> live in the <strong class="source-inline">sklearn.ensemble</strong> package. In addition to our regular imports, we import the classifiers from there, <span class="No-Break">like so:</span></p>
<pre class="source-code">
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score</pre>
<p>The scikit-learn datasets package provides the Forest Cover dataset. We can use scikit-learn to fetch the dataset and split it into our training and test sets, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
dataset = datasets.fetch_covtype()
X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, random_state=179)</pre>
<p>Finally, we can<a id="_idIndexMarker098"/> train our classifiers and evaluate each <a id="_idIndexMarker099"/>of them against the <span class="No-Break">test set:</span></p>
<pre class="source-code">
tree = DecisionTreeClassifier(random_state=179, min_samples_leaf=3, min_samples_split=6)
tree = tree.fit(X_train, y_train)
print(f1_score(y_test, tree.predict(X_test), average="macro"))
forest = RandomForestClassifier(random_state=179, min_samples_leaf=1, min_samples_split=2, n_estimators=140)
forest = forest.fit(X_train, y_train)
print(f1_score(y_test, forest.predict(X_test), average="macro"))
extra_tree = ExtraTreesClassifier(random_state=179, min_samples_leaf=1, min_samples_split=2, n_estimators=180)
extra_tree = extra_tree.fit(X_train, y_train)
print(f1_score(y_test, extra_tree.predict(X_test), average="macro"))</pre>
<p>We have set hyperparameters for the models that are appropriate to the problem. An additional step would be to optimize the algorithm hyperparameters to discover the best parameter values. Parameter optimization is discussed in detail in a <span class="No-Break">later chapter.</span></p>
<p>Running the preceding code, we get the following F1 scores for each of <span class="No-Break">the algorithms:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Model</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">F1 score</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Decision Tree</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.8917</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Random Forest</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.9209</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">ExtraTrees</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.9231</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.1 – F1 scores for each of the algorithms on the Forest CoverType dataset</p>
<p>The ExtraTrees model slightly outperforms the random forest model, and both perform better than the decision <span class="No-Break">tree classifier.</span></p>
<p>In this section, we gave an overview of bagging and random forests, a bagging-based decision tree ensemble <a id="_idIndexMarker100"/>learning method that<a id="_idIndexMarker101"/> provides some benefits over standard decision trees. The following section examines an alternative ensemble learning method: <span class="No-Break">gradient boosting.</span></p>
<h1 id="_idParaDest-42"><a id="_idTextAnchor043"/>Gradient-boosted decision trees</h1>
<p><strong class="bold">Gradient boosting</strong> is an ensemble <a id="_idIndexMarker102"/>learning methodology that combines multiple models <em class="italic">sequentially</em> to produce a more robust ensemble model. Unlike bagging, where multiple <em class="italic">strong</em> models are used (in parallel), with boosting, multiple weak learners are trained, each learning from the mistakes of those before it to build a more accurate and robust ensemble model. Another distinct difference from bagging is that each model uses the entire dataset <span class="No-Break">for training.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">As discussed next, gradient boosting always builds a series of regression trees to form part of the ensemble, regardless of whether a regression or classification problem is solved. Gradient boosting is <a id="_idIndexMarker103"/>also called <strong class="bold">Multiple Additive Regression </strong><span class="No-Break"><strong class="bold">Trees</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MART</strong></span><span class="No-Break">).</span></p>
<p>Abstractly, the boosting process starts with a weak base learner. In the case of decision trees, the base learner might have only a single split (also known as a decision stump). The error residuals (the difference between the predicted and actual targets) are then calculated. A new learner is then trained on the error residuals of the previous learner, looking to minimize the errors. The final prediction is a <em class="italic">summation</em> of the predictions from all the learners. The<a id="_idIndexMarker104"/> following diagram illustrates the iterative <span class="No-Break">gradient-boosting process:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<img alt="Figure 2.2 – Illustration of the gradient-boosting process; in each iteration, a new regression tree is added to compensate for the error residuals of the previous iteration" height="570" src="image/B16690_02_2.jpg" width="571"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Illustration of the gradient-boosting process; in each iteration, a new regression tree is added to compensate for the error residuals of the previous iteration</p>
<p>One of the critical questions relates to how we can determine the changes that reduce the error residuals. Gradient boosting solves the error minimization problem by applying a widely used <a id="_idIndexMarker105"/>optimization problem: <span class="No-Break">gradient descent.</span></p>
<h2 id="_idParaDest-43"><a id="_idTextAnchor044"/>Gradient descent</h2>
<p><strong class="bold">Gradient descent</strong> is an <a id="_idIndexMarker106"/>optimization algorithm that attempts to find the optimal parameters to <a id="_idIndexMarker107"/>minimize a <strong class="bold">loss function</strong>. The parameters are updated iteratively by taking small steps in the direction of the negative gradient of the loss function (thereby decreasing the function value). A loss function is similar in concept to an error function but has two important properties, as <span class="No-Break">outlined here:</span></p>
<ul>
<li>A loss function produces a <a id="_idIndexMarker108"/>numeric value that quantifies the model’s performance or precisely how poorly a model is doing. A good loss function produces significantly different output for models with different performances. Some error functions can also be used as loss functions—for <a id="_idIndexMarker109"/>example, <strong class="bold">mean squared </strong><span class="No-Break"><strong class="bold">error</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MSE</strong></span><span class="No-Break">).</span></li>
<li>The second property is that a loss function must be differentiable, specifically in the context of gradient descent. An example of an error function that’s not differentiable is the F1 score. The F1 score may produce a numeric value of the model’s performance, but it is not differentiable and cannot be used as a <span class="No-Break">loss function.</span></li>
</ul>
<p>The process for gradient descent can, then, be defined as follows. Suppose we have a loss function <span class="_-----MathTools-_Math_Variable">L</span> defined for parameters <span class="_-----MathTools-_Math_Variable">x</span>. For an initial set of parameters, the loss is calculated as <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base">)</span>. Gradient descent proceeds iteratively to minimize the <span class="No-Break">loss function:</span></p>
<p><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&lt;</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
<p>To update the parameters, <em class="italic">we take a step in the direction of the negative gradient of </em><span class="_-----MathTools-_Math_Variable">L</span>. We can specify the gradient descent update rule <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∇</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
<p>Here, <span class="_-----MathTools-_Math_Variable_v-normal">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span> is the <strong class="bold">learning rate</strong>, which<a id="_idIndexMarker110"/> defines the step size, and <span class="_-----MathTools-_Math_Operator_Extended">∇</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">)</span> is the gradient of <span class="_-----MathTools-_Math_Variable">L</span> at <span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span><span class="No-Break">.</span></p>
<p>The graph in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.3</em> illustrates the gradient <span class="No-Break">descent process:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<img alt="Figure 2.3 – Graph showing the gradient descent process to find the minimum of a function" height="675" src="image/B16690_02_3.jpg" width="927"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Graph showing the gradient descent process to find the minimum of a function</p>
<p>Choosing an appropriate learning rate is critical to gradient descent’s success. If the learning rate is too low, the optimization will be very slow, potentially not reaching a minimum in the allowed number of iterations. A small learning rate could also lead to the process getting stuck in a local minimum: the step size being too small to escape. Conversely, suppose<a id="_idIndexMarker111"/> the learning rate is too large. In that case, we could step over the minimum and miss it entirely or get stuck oscillating around a minimum (constantly jumping back and forth but never descending to the <span class="No-Break">optimal value).</span></p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor045"/>Gradient boosting</h2>
<p>Now that we<a id="_idIndexMarker112"/> understand how gradient descent works, we can see how it is applied in gradient boosting. We will work through the entire gradient-boosting algorithm in detail at the hand of a small example. In our example, we’ll use a regression tree, as it is a bit easier to understand than the case <span class="No-Break">for classification.</span></p>
<h3>Gradient-boosting algorithm</h3>
<p>The gradient-boosting algorithm<a id="_idIndexMarker113"/> is defined as follows, where <span class="_-----MathTools-_Math_Variable">M</span> is the number of boosted <span class="No-Break">trees [3]:</span></p>
<ol>
<li>Given training data <span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">}</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span>, containing <span class="_-----MathTools-_Math_Variable">n</span> training samples (defined by features <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> and target <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span>) and a differentiable loss function <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">)</span>, where <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span> are the predictions from <span class="No-Break">model </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">F</span></span><span class="No-Break">.</span></li>
<li>Initialize the model with a constant prediction value <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Function_v-normal">argmin</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">γ</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></li>
<li>For <span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">to</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">M</span></span><span class="No-Break">:</span><p class="list-inset">Compute pseudo residuals <span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">im</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">]</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">for</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">n</span></p><p class="list-inset">Fit a regression tree to the <span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">im</span> values and create terminal regions <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">jm</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">for</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">1</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">…</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">J</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">m</span></span></p><p class="list-inset">For <span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">…</span><span class="_-----MathTools-_Math_Variable">J</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span> compute <span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">jm</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Function_v-normal">argmin</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">ij</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">γ</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p><p class="list-inset">Update <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">ν</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">J</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">jm</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">I</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">R</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">jm</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p></li>
<li>Result: <span class="No-Break"><span class="_-----MathTools-_Math_Variable">F</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">M</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></li>
</ol>
<p>Although the algorithm and especially the mathematics might look intimidating, it is practically much more straightforward than it appears. We’ll go through the algorithm step by step. Consider the following <span class="No-Break">toy dataset:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Gender</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Fasting </strong><span class="No-Break"><strong class="bold">Blood Sugar</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Waist Circumference</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">BMI</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">LDL Cholesterol</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Male</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">105</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">110</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">29.3</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">170</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Female</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">85</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">80</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">21</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">90</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Male</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">95</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">93</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">26</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">113</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.2 – Example dataset consisting of a patient’s physical measurements and measured low-density lipoprotein (LDL) cholesterol</p>
<p>Given the physical measurements, we aim to predict a patient’s <span class="No-Break">LDL cholesterol.</span></p>
<p>The preceding table defines our training data <span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">}</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span>, where <span class="_-----MathTools-_Math_Variable">x</span> is the features (blood sugar, waist circumference, BMI) and <span class="_-----MathTools-_Math_Variable">y</span> is the target: <span class="No-Break">LDL cholesterol.</span></p>
<p>We need a differentiable loss function, and to simplify some of the mathematical derivations in this example, we choose the following loss function, which is similar to the <span class="No-Break">MSE function:</span></p>
<p><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
<p>We now work through each algorithm step in detail to see how the gradient-boosted tree <span class="No-Break">is produced.</span></p>
<p>The first step is to find <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Function_v-normal">argmin</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base">)</span>, where <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> is our target value and <span class="_-----MathTools-_Math_Variable">γ</span> is our initial <a id="_idIndexMarker114"/>predicted value. <em class="italic">Our initial prediction is constant and is simply the average of the target values</em>. But let’s <span class="No-Break">see why.</span></p>
<p>The equation for <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span> is stating that we need to find a value for <span class="_-----MathTools-_Math_Variable">γ</span> that minimizes our loss function. To find the minimum, we take the derivative of the loss function with respect <span class="No-Break">to gamma:</span></p>
<p class="list-inset"><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span></p>
<p>Then, set it to 0 and solve the <span class="No-Break">following equation:</span></p>
<p class="list-inset"><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span></p>
<p class="list-inset"><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span></p>
<p class="list-inset"><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span></p>
<p>The equation simplifies to calculating the average of the <span class="No-Break">target values.</span></p>
<p>Updating our table with the predictions, we have <span class="No-Break">the following:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table003-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Gender</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">F. </strong><span class="No-Break"><strong class="bold">Blood Sugar</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">W. Circum.</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">BMI</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">LDL Cholesterol</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">PredictionF 0(x)</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Male</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">105</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">110</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">29.3</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">170</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">125</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Female</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">85</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">80</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">21</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">90</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">125</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Male</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">95</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">93</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">26</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">113</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">125</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.3 – Our initial prediction of LDL cholesterol predictions <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">x</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">)</span><span class="_-----MathTools-_Math_Base">)</span> for each patient is constant</p>
<p>We repeat the<a id="_idIndexMarker115"/> following <span class="_-----MathTools-_Math_Variable">M</span> times, where <span class="_-----MathTools-_Math_Variable">M</span> is the number of trees we choose <span class="No-Break">to build.</span></p>
<p>We now need to calculate the pseudo residuals <span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">im</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">]</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Variable">)</span>. This equation for <span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">im</span> is stating that we use the negative of the partial derivative of the loss function, with respect to the predictions, to calculate the pseudo residuals. <em class="italic">This portion of the gradient-boosting algorithm relates to gradient descent: we are taking the negative of the gradient to minimize the residuals</em>. Fortunately, we have already calculated <span class="No-Break">this derivative:</span></p>
<p><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">]</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Operator"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">F</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
<p>Here, <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span> is the predicted value. In other words, <em class="italic">the equation simplifies the difference between the target and predicted values</em>. We can add the residuals to the table, <span class="No-Break">as follows:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table004">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Gender</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">F. Blood</strong></span><strong class="bold"> </strong><span class="No-Break"><strong class="bold">Sugar</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">W. Circum.</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">BMI</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">LDL Cholesterol</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Prediction </strong><span class="No-Break"><strong class="bold">F 0(x)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Residuals</strong></span><strong class="bold"> for </strong><span class="No-Break"><strong class="bold">F 0(x)</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Male</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">105</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">110</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">29.3</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">170</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">125</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">45</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Female</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">85</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">80</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">21</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">90</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">125</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">35</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Male</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">95</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">93</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">26</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">113</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">125</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">12</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.4 – Based on our initial prediction, we can calculate the residuals for each patient, as shown in the Residuals for F 0(x) column</p>
<p>The next step is straightforward: we build a regression tree to predict the residuals. We do not use the predictions of the regression tree directly. Instead, we use the <em class="italic">terminal regions</em> to calculate our updated prediction. The terminal regions refer to the tree’s <span class="No-Break">leaf nodes.</span></p>
<p>For this example, we <a id="_idIndexMarker116"/>assume the following simple regression tree <span class="No-Break">is built:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer023">
<img alt="Figure 2.4 – Regression tree predicting the residuals" height="366" src="image/B16690_02_4.jpg" width="565"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Regression tree predicting the residuals</p>
<p>With our regression tree built and our leaf nodes defined, we can proceed with the next step. We need to compute <span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">jm</span> that minimizes our loss function, taking into account the previous predictions as per <span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">jm</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Function_v-normal">argmin</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">ij</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base">)</span>. This is almost precisely what we did in <em class="italic">step 1</em>, where we showed that the equation simplifies taking the average of the predicted values due to our choice of the loss function. <em class="italic">Here, that means taking the average of the residuals in each leaf node</em>. Therefore, we have <span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1,1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">35</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">12</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">23.5</span> and <span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2,1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">45</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">45</span></span><span class="No-Break">.</span></p>
<p>Finally, we can now calculate our next prediction, <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span>, defined by: <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">ν</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">J</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">jm</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">I</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">R</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">jm</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break">,</span> meaning our next prediction consists of the previous prediction plus the <span class="_-----MathTools-_Math_Variable">γ</span> values calculated in <em class="italic">step 2.3</em>, weighed by a learning rate <span class="_-----MathTools-_Math_Variable">ν</span>. The summation here means that should a sample be part of multiple leaf nodes, we take the sum of the gamma values. Let’s calculate <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span> for the first sample in our dataset, using a learning rate of 0.1. According to the regression tree from <em class="italic">step 2.2</em>, our sample (which has a BMI &gt; 26) maps to <span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2,1</span>. Since it’s only mapping to a single leaf, we don’t need the summation part of the equation. Therefore, the equation looks <span class="No-Break">like this:</span></p>
<p><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">ν</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2,1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">125</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">45</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">129.5</span></span></p>
<p>As expected, our <a id="_idIndexMarker117"/>prediction has improved in the direction of our target values. Doing the same for the other samples, we have <span class="No-Break">the following:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table005">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Gender</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">F. Blood</strong></span><strong class="bold"> </strong><span class="No-Break"><strong class="bold">Sugar</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">W. Circum.</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">BMI</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">LDL Cholesterol</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Prediction </strong><span class="No-Break"><strong class="bold">F 0(x)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Residuals</strong></span><strong class="bold"> for </strong><span class="No-Break"><strong class="bold">F 0(x)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Prediction </strong><span class="No-Break"><strong class="bold">F 1(x)</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Male</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">105</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">110</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">29.3</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">170</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">125</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">45</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">129.5</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Female</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">85</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">80</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">21</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">90</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">125</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">35</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">122,65</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Male</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">95</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">93</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">26</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">113</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">125</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">12</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">122,65</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.5 – Following steps 2.1 to 2.4, we calculate a new prediction, <span class="_-----MathTools-_Math_Variable_v-bold-italic">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">x</span><span class="_-----MathTools-_Math_Base">)</span>, based on the initial prediction and the residuals</p>
<p>The purpose of the learning rate is to limit the impact each tree might have on the overall prediction: by improving our prediction with small steps, we end up with an overall more <span class="No-Break">accurate model.</span></p>
<p><em class="italic">Step 2</em> is then repeated until we have a final prediction <span class="No-Break"><span class="_-----MathTools-_Math_Variable">F</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">M</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break">.</span></p>
<p>In summary, our gradient-boosting ensemble consists of a weighted (by the learning rate) summation of predictions made by a series of regression trees, each predicting the pseudo residuals (the error gradient, with respect to previous prediction) of previous predictions, thereby <a id="_idIndexMarker118"/>minimizing the error of previous predictions to produce an accurate <span class="No-Break">final prediction.</span></p>
<h3>Gradient boosting for classification</h3>
<p>Our explanation of <a id="_idIndexMarker119"/>gradient boosting given previously used a regression problem as an example. We will not be going through a detailed example for classification as the algorithm is the same. However, instead of working with continuous predicted values, we use the same techniques as logistic regression<a href="https://en.wikipedia.org/wiki/Logistic_regression"> (https://en.wikipedia.org/wiki/Logistic_regressi</a>on). The individual trees, therefore, predict the probabilities of a sample belonging to the class. The probabilities are calculated by taking the log odds of a sample and converting them to a probability using the logistic function, <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Operator">/</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Base"> </span></p>
<p>The pseudo residuals are calculated as the difference between the observed value (1 or 0 for the class) and the predicted value (the probability from the logistic function). A final difference is the loss function. Instead of a function such as MSE, we can use cross-entropy for loss, <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable">H</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">q</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Variable_v-normal">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
<h2 id="_idParaDest-45"><a id="_idTextAnchor046"/>Gradient-boosted decision tree hyperparameters</h2>
<p>In addition to the <a id="_idIndexMarker120"/>parameters for standard decision tree training, the following new hyperparameters are available in scikit-learn specific to <span class="No-Break">gradient-boosted trees:</span></p>
<ul>
<li><strong class="source-inline">n_estimators</strong>: Controls the number of trees in the ensemble. Generally, more trees are better. However, a point of diminishing returns is often reached, and overfitting occurs when there are too <span class="No-Break">many trees.</span></li>
<li><strong class="source-inline">learning_rate</strong>: Controls the contribution of each tree to the ensemble. Lower learning rates lead to longer training times and may require more trees to be built (larger values for <strong class="source-inline">n_estimators</strong>). Setting <strong class="source-inline">learning_rate</strong> to a very large value may cause the optimization to miss optimum points and must be combined with <span class="No-Break">fewer trees.</span></li>
</ul>
<p>A complete list o<a id="_idIndexMarker121"/>f scikit-learn gradient-boosting hyperparameters can be found <span class="No-Break">at </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.xhtml</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-46"><a id="_idTextAnchor047"/>Gradient boosting in scikit-learn</h2>
<p>The details of<a id="_idIndexMarker122"/> gradient boosting are mathematical <a id="_idIndexMarker123"/>and complicated; fortunately, the algorithm is as accessible as any other via scikit-learn. The following is an example of a <strong class="source-inline">GradientBoostingClassifier</strong> class in scikit-learn, again using the <em class="italic">Forest CoverType</em> dataset we used earlier in the chapter to train a random <span class="No-Break">forest classifier.</span></p>
<p>The classifier is also imported from the <strong class="source-inline">ensemble</strong> package, <span class="No-Break">like so:</span></p>
<pre class="source-code">
from sklearn.ensemble import GradientBoostingClassifier</pre>
<p>We fetch and split the data as before and then fit the model, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
dataset = datasets.fetch_covtype()
X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, random_state=179)
booster = GradientBoostingClassifier(random_state=179, min_samples_leaf=3, min_samples_split=3, learning_rate=0.13, n_estimators=180)
booster = booster.fit(X_train, y_train)
print(f1_score(y_test, booster.predict(X_test), average="macro"))</pre>
<p>Running the preceding code should produce an F1 score of 0.7119, a score that’s significantly worse than even a standard decision tree. We can spend time optimizing our hyperparameters to improve performance. However, there is a more significant issue. The previous code takes very long to execute—in the order of 45 minutes on our hardware—compared to ExtraTrees, which takes approximately <span class="No-Break">3 minutes.</span></p>
<p>LightGBM addresses both issues we have with the gradient-boosted tree and builds a gradient-boosted<a id="_idIndexMarker124"/> tree with significantly better performance in a<a id="_idIndexMarker125"/> fraction of <span class="No-Break">the time.</span></p>
<p>In the following section, we’ll briefly cover an advanced algorithm related to gradient <span class="No-Break">boosting: DART.</span></p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor048"/>Advanced boosting algorithm – DART</h1>
<p><strong class="bold">DART</strong> is an extension<a id="_idIndexMarker126"/> of the standard GBDT algorithm discussed in the previous section [4]. DART <a id="_idIndexMarker127"/>employs <strong class="bold">dropouts</strong>, a technique from <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>), to <a id="_idIndexMarker128"/>avoid overfitting by the decision tree ensemble. The extension is straightforward and consists of two parts. First, when fitting the next prediction tree, <span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span>, which consists of the scaled sum of all previous trees <span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Operator">…</span><span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>, a random subset of the previous trees is instead used, with other trees dropped from the sum. The <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">drop</span> parameter controls the probability of a previous tree being included. The second part of the DART algorithm is to apply additional scaling of the contribution of the new tree. Let <span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Space"> </span>be the number of trees dropped when the new tree, <span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span>was calculated. Since <span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Number">1</span> was calculated without the contribution of those <span class="_-----MathTools-_Math_Variable">k</span> trees when updating our prediction, <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Number">1</span>, which includes all trees, the prediction overshoots. Therefore, the new tree is scaled by a factor of <span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Variable"> </span> <span class="No-Break">to compensate.</span></p>
<p>DART has been shown to outperform standard GBDTs while also significantly <span class="No-Break">improving overfitting.</span></p>
<p>Scikit-learn does not implement DART for GBDTs, but DART is incorporated <span class="No-Break">in LightGBM.</span></p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor049"/>Summary</h1>
<p>In conclusion, this chapter looked at the two most common methods of ensemble learning for decision trees: bagging and boosting. We looked at the Random Forests and ExtraTrees algorithms, which build decision tree ensembles <span class="No-Break">using bagging.</span></p>
<p>This chapter also gave a detailed overview of boosting in decision trees by going through the GBDT algorithm step by step, illustrating how gradient boosting is applied. We covered practical examples of random forests, ExtraTrees, and GBDTs <span class="No-Break">for scikit-learn.</span></p>
<p>Finally, we looked at how dropouts can be applied to GBDTs with the DART algorithm. We now thoroughly understand decision tree ensemble techniques and are ready to dive deep <span class="No-Break">into LightGBM.</span></p>
<p>The next chapter introduces the LightGBM library in detail, both the theoretical advancements made by the library and the practical application thereof. We will also look at using LightGBM with Python to solve <span class="No-Break">ML problems.</span></p>
<h1 id="_idParaDest-49"><a id="_idTextAnchor050"/>References</h1>
<table class="No-Table-Style" id="table006">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">1]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">L. Breiman, “Random forests,” Machine learning, vol. 45, p. </em><span class="No-Break"><em class="italic">5-32, 2001.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">2]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">P. Geurts, D. Ernst and L. Wehenkel, “Extremely randomized trees,” Machine learning, vol. 63, p. </em><span class="No-Break"><em class="italic">3-42, 2006.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">3]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">J. H. Friedman, “Greedy function approximation: a gradient boosting machine,” The Annals of Statistics, p. </em><span class="No-Break"><em class="italic">1189-1232, 2001.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">4]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">R. K. Vinayak and R. Gilad-Bachrach, “Dart: Dropouts meet multiple additive regression trees,” in Artificial Intelligence and </em><span class="No-Break"><em class="italic">Statistics, 2015.</em></span></p>
</td>
</tr>
</tbody>
</table>
</div>
</div></body></html>