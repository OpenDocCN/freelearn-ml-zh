<html><head></head><body>
		<div class="Content" id="_idContainer087">
			<p class="hidden">3</p>
		</div>
		<div class="Content" id="_idContainer088">
			<h1 id="_idParaDest-60"><a id="_idTextAnchor062"/>Perform Topic Modeling and Theme Extraction</h1>
		</div>
		<div class="Content" id="_idContainer089">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Extract and analyze common themes through topic modeling with Amazon Comprehend</li>
				<li class="bullets">Describe the basics of topic modeling analysis</li>
				<li class="bullets">Perform topic modeling on a set of documents and analyze the results</li>
			</ul>
			<p>This chapter describes Topic Modeling on common themes using Amazon Comprehend analyzing the result for document set.</p>
		</div>
		<div class="Content" id="_idContainer133">
			<h2 id="_idParaDest-61"><a id="_idTextAnchor063"/>Introduction</h2>
			<p>In the first part of this chapter, you will learn how to analyze Topic modeling output from Amazon Comprehend. Specifically, you will learn the fundamentals of the algorithm used for Topic modeling, Latent Dirichlet Allocation (LDA). Learning LDA will allow you to apply Topic modeling to a multitude of unique business use cases. </p>
			<p>You will then perform Topic modeling on two documents with a known Topic structure. The first is the story <strong class="bold">Romeo and Juliet</strong> and the second is <strong class="bold">War of the Worlds</strong>. Lastly, you will analyze topics from 1,000 text documents containing negative movie reviews with Amazon Comprehend.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor064"/>Extracting and Analyzing Common Themes</h2>
			<p>You can also utilize Amazon Comprehend to analyze a corpus of archives to locate the normal topics contained inside the corpus. Amazon Comprehend inspects reports in the corpus and, afterward, restores the most noticeable themes and the reports that are related to every subject. Subject displaying is an offbeat procedure: you present an arrangement of records for preparation and later get the outcomes when handling is finished. Amazon Comprehend performs point displaying on huge report sets. For the best results, you ought to incorporate around 1,000 records when you present a subject demonstrating work.</p>
			<h3 id="_idParaDest-63"><a id="_idTextAnchor065"/>Topic Modeling with Latent Dirichlet Allocation (LDA)</h3>
			<p>The subjects or <strong class="keyword">common themes</strong> of a set of documents can be determined with Amazon Comprehend. For example, you have a movie review website with two message boards, and you want to determine which message board is discussing two newly released movies (one about sport and the other about a political Topic). You can provide the message board text data to Amazon Comprehend to discover the most prominent topics discussed on each message board.</p>
			<p>The machine learning algorithm that Amazon Comprehend uses to perform Topic Modeling is called latent Dirichlet allocation (LDA). LDA is a learning-based model that's used to determine the most important topics in a collection of documents. </p>
			<p>The way that LDA works is it considers every document to be a combination of topics, and each word in the document is associated to one of these topics.</p>
			<p>For example, if the first paragraph of a document consists of words like <strong class="bold">eat</strong>, <strong class="bold">chicken</strong>, <strong class="bold">restaurant</strong>, and <strong class="bold">cook</strong> then you conclude that the Topic can be generalized to <strong class="bold">Food</strong>. And if the second paragraph of a document contains words like <strong class="bold">ticket</strong>, <strong class="bold">train</strong>, <strong class="bold">kilometer</strong>, and <strong class="bold">vacation</strong> then you can conclude that the Topic is <strong class="bold">Travel</strong>. </p>
			<h3 id="_idParaDest-64"><a id="_idTextAnchor066"/>Basic LDA example</h3>
			<p>Topic modeling can seem complex, and understanding the fundamental steps of how LDA determines Topics is essential to performing Topic modeling on more complex business use cases. Thus, let's deconstruct LDA with the following simple example.</p>
			<p>You have one document with five sentences. Your goal is to determine the two most common topics present in the document: </p>
			<ul>
				<li>I like to eat bread and bananas.</li>
				<li>I ate a bread and banana smoothie for breakfast.</li>
				<li>Puppies and kittens are cute.</li>
				<li>My brother adopted a puppy yesterday.</li>
				<li>Look at this cute opossum munching a piece of broccoli.</li>
			</ul>
			<p>LDA discovers the topics that these sentences contain.  For example, given the above sentences and asked for two topics, LDA might produce the following:</p>
			<p>Sentences 1 and 2: 100% Topic A</p>
			<p>Sentences 3 and 4: 100% Topic B</p>
			<p>Sentence 5: 60% Topic A, 40% Topic B</p>
			<p><strong class="bold">Topic A</strong>: 30% bread, 15% banana, 10% breakfast, 10% munching, (Thus, you can assume Topic A is about food)</p>
			<p><strong class="bold">Topic B</strong>: 20% Puppies, 20% kittens, 20% cute, 15% opossum, (This, you can assume Topic B is about cute animals).</p>
			<h3 id="_idParaDest-65"><a id="_idTextAnchor067"/>Why Use LDA?</h3>
			<p>LDA is useful when you have an arrangement of records that you need to find designs inside, without thinking about the reports themselves. LDA can be utilized to create subjects to comprehend an archives, general Topic. This is, usually utilized in suggestion frameworks, report arrangement, and record synopsis. In conclusion, LDA is helpful in preparing prescient models with subjects and events.</p>
			<p>LDA has many use cases. For example, you have 30,000 user emails and want to determine the most common topics to provide group-specific recommended content based on the most prevalent topics. Manually reading, or even outsourcing the manual reading of, 30,000 emails, would take an excessive investment in terms of time and money, and the accuracy would be difficult to confirm. However, Amazon Comprehend can seamlessly provide the most common topics present in 30,000 emails in a few steps with incredible accuracy. First, convert the emails to text files, upload them to an S3 bucket, then imitate a Topic modeling job with Amazon Comprehend. The output is two <strong class="inline">CSV</strong> with the corresponding Topics and terms.</p>
			<h3 id="_idParaDest-66"><a id="_idTextAnchor068"/>Amazon Comprehend–Topic Modeling Guidelines</h3>
			<p>The most accurate results are given if you provide Comprehend with the largest possible corpus. More specifically: </p>
			<ul>
				<li>You should use no less than 1,000 records in every subject demonstrating work</li>
				<li>Each report ought to be something like three sentences in length </li>
				<li>If a record comprises of for the most part numeric information, you should expel it from the corpus</li>
			</ul>
			<p>Currently, Topic Modeling is limited to two document languages: <strong class="bold">English</strong> and <strong class="bold">Spanish</strong>.</p>
			<p>A Topic modeling job allows two format types for input data (see the following table 1). This allows users to process both collections of large documents (for example, newspaper articles or scientific journals), and short documents (for example, tweets or social media posts). </p>
			<p><strong class="bold">Input Format Options:</strong></p>
			<div>
				<div class="IMG---Figure" id="_idContainer090">
					<img alt="Figure3.1:  AWS Comprehend - Topic modeling input format options.&#13;&#10;" src="image/Image_Lesson3_001.jpg"/>
				</div>
			</div>
			<h6>Figure3.1: AWS Comprehend– Topic modeling input format options</h6>
			<p><strong class="bold">Output Format Options:</strong></p>
			<div>
				<div class="IMG---Figure" id="_idContainer091">
					<img alt="Figure 3.2: AWS Comprehend - topic modeling output files description.&#13;&#10;" src="image/Image_Lesson3_002.jpg"/>
				</div>
			</div>
			<h6>Figure 3.2: AWS Comprehend– Topic modeling output files description</h6>
			<p>After Amazon Comprehend processes your document collection, the modeling outputs two CSV files: Topic-terms.csv (see Figure 1) and <strong class="inline">doc-topics.csv</strong>.</p>
			<p>The <strong class="inline">topic-terms.csv</strong> file provides a list of topics in the document collection with the terms, respective Topic and weight. For example, if you gave Amazon Comprehend two hypothetical documents, <strong class="bold">learning to garden</strong> and <strong class="bold">investment strategies</strong>, it might return the following to describe the two topics in the collection:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer092">
					<img alt="Figure 3.3: Sample topic modeling output (topic-terms.csv) for two documents input.&#13;&#10;" src="image/Image_Lesson3_003.jpg"/>
				</div>
			</div>
			<h6>Figure 3.3: Sample Topic modeling output (<strong class="inline">topic-terms.csv</strong>) for two document's input</h6>
			<p>The <strong class="inline">doc-topics.csv</strong> file provides a list of the documents provided for the Topic modeling job with the document names, and the respective topics and their proportions in each document. Given two hypothetical documents, <strong class="inline">learning_to_garden.txt</strong> and <strong class="inline">investment_strategies.txt,</strong> you can expect the following output:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer093">
					<img alt="Figure 3.4: Sample topic modeling output (doc-topics.csv) for two documents input&#13;&#10;" src="image/Image_Lesson3_004.jpg"/>
				</div>
			</div>
			<h6>Figure 3.4: Sample Topic modeling output (<strong class="inline">doc-topics.csv</strong>) for two document's input</h6>
			<h3 id="_idParaDest-67"><a id="_idTextAnchor069"/>Exercise 13: Topic Modeling of a Known Topic Structure </h3>
			<p>In this exercise, we will use Amazon Comprehend to perform Topic modeling on two documents with known topics (<strong class="bold">Romeo and Juliet</strong> and <strong class="bold">War of the Worlds</strong>). We are using two known topics to better understand LDA. Before proceeding to the exercise, just look at an overview of the data pipeline architecture: </p>
			<div>
				<div class="IMG---Figure" id="_idContainer094">
					<img alt="Figure 3.5: Data pipeline architecture overview&#13;&#10;" src="image/Image_Lesson3_005.jpg"/>
				</div>
			</div>
			<h6>Figure 3.5: Data pipeline architecture overview</h6>
			<p>The following are the steps to complete the Topic modelling of a known Topic structure:</p>
			<ol>
				<li>We need an input and output S3 bucket. Let's create both. Navigate to <a href="https://s3.console.aws.amazon.com/s3/">https://s3.console.aws.amazon.com/s3/</a>.</li>
				<li>Now, click on the <strong class="bold">Create</strong> <strong class="bold">bucket</strong> button to create a bucket:<div class="IMG---Figure" id="_idContainer095"><img alt="Figure 3.6: Create bucket&#13;&#10;" src="image/Image_Lesson3_006.jpg"/></div><h6>Figure 3.6: Creating a bucket</h6></li>
				<li>For Bucket name, enter a unique name that describes the function. Here, the name <strong class="inline">aws-ml-input-for-topic-modeling</strong> is used. Click on the <strong class="bold">Create</strong> button:<h4>Note</h4><p class="callout">Clicking Create versus Next uses all default settings for: properties and permissions.</p><div class="IMG---Figure" id="_idContainer096"><img alt="Figure 3.7: Create bucket name input&#13;&#10;" src="image/Image_Lesson3_007.jpg"/></div><h6>Figure 3.7: Creating bucket name input</h6></li>
				<li> Now, click on the <strong class="bold">Create </strong>button to create a folder:<div class="IMG---Figure" id="_idContainer097"><img alt="Figure 3.8: Create a folder in S3 for topic modeling input&#13;&#10;" src="image/Image_Lesson3_008.jpg"/></div><h6>Figure 3.8: Creating a folder in S3 for Topic modeling input</h6></li>
				<li>Now, type in <strong class="inline">known_structure</strong>, as the folder name, and then click on the <strong class="bold">Save</strong> button:<div class="IMG---Figure" id="_idContainer098"><img alt="Figure 3.9: Save the ‘known_structure’ folder name&#13;&#10;" src="image/Image_Lesson3_009.jpg"/></div><h6>Figure 3.9: Saving the <strong class="inline">'known_structure</strong>' folder name</h6></li>
				<li>After clicking on the <strong class="bold">Save</strong> button, your folder will be generated. Now, click on the <strong class="inline">known_structure</strong> folder:<div class="IMG---Figure" id="_idContainer099"><img alt="Figure 3.9: Input bucket screen&#13;&#10;" src="image/Image_Lesson3_010.jpg"/></div><h6>Figure 3.9: Input bucket screen</h6></li>
				<li>Now, click on the <strong class="bold">Upload</strong> button:<div class="IMG---Figure" id="_idContainer100"><img alt="Figure 3.10: Upload screen&#13;&#10;" src="image/Image_Lesson3_011.jpg"/></div><h6>Figure 3.10: Upload screen</h6></li>
				<li>Now, you will prompted by the following for adding files. Click on <strong class="bold">Add files</strong> or drag the files onto the screen:<div class="IMG---Figure" id="_idContainer101"><img alt="Figure 3.11 Add files screen&#13;&#10;" src="image/Image_Lesson3_012.jpg"/></div><h6>Figure 3.11 Add files screen</h6></li>
				<li>Navigate to download and upload the following two text files from the machine:<h4>Note</h4><p class="callout">You can download the Romeo and Juliet text file from /lesson3/topic_a/romeo_and_juliet.txt <a href="https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/romeo_and_juliet.txt">https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/romeo_and_juliet.txt</a> /lesson3/topic_a/the_war_of_the_worlds.txt <a href="https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/the_war_of_the_worlds.txt">https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/the_war_of_the_worlds.txt</a></p></li>
				<li>Once the files have been uploaded, click on the <strong class="bold">Upload</strong> button to upload the files:<div class="IMG---Figure" id="_idContainer102"><img alt="Figure 3.12: Click Upload for the two known_structure text files&#13;&#10;" src="image/Image_Lesson3_013.jpg"/></div><h6>Figure 3.12: Uploading for the two <strong class="inline">known_structure</strong> text files</h6></li>
				<li>Navigate to the Amazon S3 homescreen:<div class="IMG---Figure" id="_idContainer103"><img alt="Figure 3.13: Click Amazon S3.&#13;&#10;" src="image/Image_Lesson3_014.jpg"/></div><h6>Figure 3.13: Amazon S3</h6></li>
				<li>Next, create an output S3 bucket. Use the same S3 bucket creation process. Click on the <strong class="bold">Create</strong> <strong class="bold">bucket</strong> button:<div class="IMG---Figure" id="_idContainer104"><img alt="Figure 3.14: Click Create Bucket.&#13;&#10;" src="image/Image_Lesson3_015.jpg"/></div><h6>Figure 3.14: Creating a bucket</h6></li>
				<li> Now, name the bucket, and then click on the <strong class="bold">Create</strong> button:<div class="IMG---Figure" id="_idContainer105"><img alt="Figure 3.15: Create bucket output for topic modeling&#13;&#10;" src="image/Image_Lesson3_016.jpg"/></div><h6>Figure 3.15: Create bucket output for Topic modeling</h6></li>
				<li>Navigate to Amazon Comprehend: <a href="https://console.aws.amazon.com/comprehend/">https://console.aws.amazon.com/comprehend/</a>. If you are presented with the following screen, click <strong class="bold">Try Amazon Comprehend</strong>:<div class="IMG---Figure" id="_idContainer106"><img alt="Figure 3.16: Amazon Comprehend home screen.&#13;&#10;" src="image/Image_Lesson3_017.jpg"/></div><h6>Figure 3.16: Amazon Comprehend home screen</h6></li>
				<li>Now, Click on the <strong class="bold">Organization</strong> in the left-hand side toolbar:<div class="IMG---Figure" id="_idContainer107"><img alt="Figure 3.17: Amazon Comprehend Organization screen &#13;&#10;" src="image/Image_Lesson3_018.jpg"/></div><h6>Figure 3.17: Amazon Comprehend Organization screen</h6></li>
				<li>Now, click on the <strong class="bold">Create</strong> button and then enter <strong class="inline">known_structure_topic_modeling_job</strong> in the <strong class="bold">Name </strong>field:<div class="IMG---Figure" id="_idContainer108"><img alt="Figure 3.18: Name of the topic modeling job input&#13;&#10;" src="image/Image_Lesson3_019.jpg"/></div><h6>Figure 3.18: Name of the Topic modeling job</h6></li>
				<li>Now, scroll down to <strong class="bold">Choose input data</strong> and then click on <strong class="bold">Search</strong>: <div class="IMG---Figure" id="_idContainer109"><img alt="Figure 3.19: Click Search to locate the topic modeling input data source&#13;&#10;" src="image/Image_Lesson3_020.jpg"/></div><h6>Figure 3.19: Clicking Search to locate the Topic modeling input data source</h6></li>
				<li>Navigate to the <strong class="inline">known_structure</strong> folder and then click on <strong class="bold">Select</strong>:<div class="IMG---Figure" id="_idContainer110"><img alt="Figure 3.20: Click Select for the S3 folder.&#13;&#10;" src="image/Image_Lesson3_021.jpg"/></div><h6>Figure 3.20: Clicking on Select for the S3 folder</h6></li>
				<li>Now, from the drop-down menu, select <strong class="bold">One document per file</strong>:<div class="IMG---Figure" id="_idContainer111"><img alt="Figure 3.21: Select One document per file&#13;&#10;" src="image/Image_Lesson3_022.jpg"/></div><h6>Figure 3.21: Selecting One document per file</h6></li>
				<li>Now, enter <strong class="bold">two</strong> for the <strong class="bold">Number of Topics </strong>you need to have:<div class="IMG---Figure" id="_idContainer112"><img alt="Figure 3.22: Enter 2 for the number of topics to perform topic modeling&#13;&#10;" src="image/Image_Lesson3_023.jpg"/></div><h6>Figure 3.22: Entering 2 for the number of topics to perform Topic modeling</h6></li>
				<li>Next, click on <strong class="bold">Search</strong> to search the bucket that was created previously:<div class="IMG---Figure" id="_idContainer113"><img alt="Figure 3.23: Click search for the topic modeling S3 output location&#13;&#10;" src="image/Image_Lesson3_024.jpg"/></div><h6>Figure 3.23: Clicking on search for the Topic modeling S3 output location</h6></li>
				<li>Once you find the bucket you created, click on the bucket you created to output Topic modeling:<div class="IMG---Figure" id="_idContainer114"><img alt="Figure 3.24: Select the output S3 bucket&#13;&#10;" src="image/Image_Lesson3_025.jpg"/></div><h6>Figure 3.24: Selecting the output S3 bucket</h6></li>
				<li>Now, select the appropriate bucket and then click on <strong class="bold">Select</strong>:<div class="IMG---Figure" id="_idContainer115"><img alt="Figure 3.25: Confirm by clicking Select&#13;&#10;" src="image/Image_Lesson3_026.jpg"/></div><h6>Figure 3.25: Confirming by clicking Select</h6></li>
				<li>Scroll down to choose an <strong class="bold">IAM</strong> role, and click the circle next to create an <strong class="bold">IAM</strong> role:<div class="IMG---Figure" id="_idContainer116"><img alt="Figure 3.26: Select ‘Create an IAM role’&#13;&#10;" src="image/Image_Lesson3_027.jpg"/></div><h6>Figure 3.26: Selecting Create an IAM role</h6></li>
				<li>Now, select the <strong class="bold">Input </strong>and<strong class="bold"> Output S3 buckets</strong> from the <strong class="bold">Permissions to access</strong>:<div class="IMG---Figure" id="_idContainer117"><img alt="Figure 3.27: Provide permission to Input and Output S3 buckets&#13;&#10;" src="image/Image_Lesson3_028.jpg"/></div><h6>Figure 3.27: Providing permission to Input and Output S3 buckets</h6></li>
				<li>Enter <strong class="inline">myTopicModelingRole</strong> in the Name suffix field and then click on the <strong class="bold">Create job</strong> button:<div class="IMG---Figure" id="_idContainer118"><img alt="Figure 3.28: Click the Create job button&#13;&#10;" src="image/Image_Lesson3_029.jpg"/></div><h6>Figure 3.28: Clicking the Create job button</h6></li>
				<li>Creating the job may take a few minutes, but when completed, you will be redirected to the Comprehend home screen:<div class="IMG---Figure" id="_idContainer119"><img alt="Figure 3.29: Comprehend home screen&#13;&#10;" src="image/Image_Lesson3_030.jpg"/></div><h6>Figure 3.29: Comprehend home screen</h6></li>
				<li>While the job is being processed, the status displayed will be <strong class="bold">In Progress</strong>:<div class="IMG---Figure" id="_idContainer120"><img alt="Figure 3.30: ‘In progress’ status displayed&#13;&#10;" src="image/Image_Lesson3_031.jpg"/></div><h6>Figure 3.30: In progress status displayed</h6></li>
				<li>When the status updates to <strong class="bold">Completed</strong>, click on the Topic Modeling job name:<div class="IMG---Figure" id="_idContainer121"><img alt="Figure3.31: Completed status displayed.&#13;&#10;" src="image/Image_Lesson3_032.jpg"/></div><h6>Figure3.31: Completed status displayed</h6></li>
				<li>Now, scroll down to the <strong class="bold">Output</strong> section:<div class="IMG---Figure" id="_idContainer122"><img alt="Figure 3.32: Topic modeling output display home screen&#13;&#10;" src="image/Image_Lesson3_033.jpg"/></div><h6>Figure 3.32: Topic modeling output display home screen</h6></li>
				<li>Click on the hyperlink under <strong class="bold">Data location</strong>:<div class="IMG---Figure" id="_idContainer123"><img alt="Figure 3.33: Topic modeling data output hyperlinked location&#13;&#10;" src="image/Image_Lesson3_034.jpg"/></div><h6>Figure 3.33: Topic modeling data output hyperlinked location</h6></li>
				<li>Click on the link of the output folder:<div class="IMG---Figure" id="_idContainer124"><img alt="Figure 34: Topic modeling output folder&#13;&#10;" src="image/Image_Lesson3_035.jpg"/></div><h6>Figure 34: Topic modeling output folder</h6></li>
				<li>Click on the output folder. Then, click on <strong class="inline">output.tar.gz</strong> and download the file:<div class="IMG---Figure" id="_idContainer125"><img alt="Figure 3.35: Clicking on Download&#13;&#10;" src="image/Image_Lesson3_036.jpg"/></div><h6>Figure 3.35: Clicking on Download</h6></li>
				<li>Click on <strong class="inline">output.tar.gz</strong> and select <strong class="bold">Show in folder</strong>. Click on <strong class="bold">OK</strong> to extract the files on your desktop:<div class="IMG---Figure" id="_idContainer126"><img alt="Figure 3.36: Click OK&#13;&#10;" src="image/Image_Lesson3_037.jpg"/></div><h6>Figure 3.36: Clicking on OK</h6></li>
				<li>Navigate to your desktop. Two files will be extracted: <strong class="inline">doc-topics.csv</strong> and <strong class="inline">topics-terms.csv</strong>. There will be two files to examine: <strong class="inline">topic-terms.xlsx</strong> and <strong class="inline">doc-topics.xlsx</strong>:</li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer127">
					<img alt="Figure3.37: Topic modeling output CSV files&#13;&#10;" src="image/Image_Lesson3_038.jpg"/>
				</div>
			</div>
			<h6>Figure3.37: Topic modeling output CSV files</h6>
			<h4>Note</h4>
			<p class="callout">Your Topic-terms.csv and doc-topics.csv results should be the same as the following results. If your results are NOT the same, use the output files for the remainder of the chapter, which are located at Lesson3\topic_a\doc-topics.csv <a href="https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/doc-topics.csv">https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/doc-topics.csv</a> and lesson3\topic_a\topic-terms.csv <a href="https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/topic-terms.csv">https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/topic-terms.csv</a>.</p>
			<p>The following is the output generated:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer128">
					<img alt="Figure 3.38: topic-terms.csv result&#13;&#10;" src="image/Image_Lesson3_039.jpg"/>
				</div>
			</div>
			<h6>Figure 3.38: <strong class="inline">topic-terms.csv</strong> result</h6>
			<div>
				<div class="IMG---Figure" id="_idContainer129">
					<img alt="Figure 3.39: doc-topics.csv results&#13;&#10;" src="image/Image_Lesson3_040.jpg"/>
				</div>
			</div>
			<h6>Figure 3.39: doc-topics.csv results</h6>
			<h3 id="_idParaDest-68"><a id="_idTextAnchor070"/>Exercise 14: Performing Known Structure Analysis</h3>
			<p>In this exercise, we will programmatically upload the CSV (<strong class="inline">doc-topics.csv</strong> and <strong class="inline">Topic-terms.csv</strong>) to S3, merge the CSV on the Topic column, and print the output to the console. The following are the steps for performing Known Structure Analysis: </p>
			<h4>Note</h4>
			<p class="callout">For this step, you may either follow along with the exercise and type in the code or obtain it from the source code folder, <strong class="inline">local_csv_to_s3_for_analysis.py</strong>, and paste it into the editor. The source code is available on GitHub in the following repository: <a href="https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/local_csv_to_s3_for_analysis.py">https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/topic_a/local_csv_to_s3_for_analysis.py</a>.</p>
			<ol>
				<li value="1">First, we will import <strong class="inline">boto3</strong> using the following command:<p class="snippet">import boto3</p></li>
				<li>Next, we will import <strong class="inline">pandas</strong> using the following command:<p class="snippet">import pandas as <strong class="inline">pd</strong> </p></li>
				<li>Now, we will create the S3 client object using the following command:<p class="snippet">s3 = boto3.client('s3')</p></li>
				<li>Next, we will create a variable with a unique bucket name. Here, the selected bucket name is <strong class="inline">known-tm-analysis</strong>, but you will need to create a unique name:<p class="snippet">bucket_name = '&lt;insert a unique bucket name&gt;' #  </p></li>
				<li>Next, create a new bucket:<p class="snippet">s3.create_bucket(Bucket=bucket_name)</p></li>
				<li>Create a list of the CSV filenames to import:<p class="snippet">filenames_list = ['doc-topics.csv', 'topic-terms.csv']</p></li>
				<li>Now, iterate on each file to upload to S3 using the following line of code:<p class="snippet">for filename in filenames_list:</p><p class="snippet">    s3.upload_file(filename, bucket_name, filename)</p></li>
				<li>Next, check if the filename is <strong class="inline">doc-topics.csv</strong> :<p class="snippet">    if filename == 'doc-topics.csv':</p></li>
				<li>Now, get the <strong class="inline">doc-topics.csv</strong> file object and assign it to the <strong class="inline">obj</strong> variable using the following command:<p class="snippet"> obj = s3.get_object(Bucket=bucket_name, Key=filename)</p></li>
				<li>	Next, read the <strong class="inline">csv</strong> obj and assign it to the <strong class="inline">doc_topics</strong> variable:<p class="snippet">        doc_topics = pd.read_csv(obj['Body'])</p><p class="snippet">    else:</p><p class="snippet">        obj = s3.get_object(Bucket=bucket_name, Key=filename)</p><p class="snippet">        topic_terms = pd.read_csv(obj['Body'])</p></li>
				<li>Now, merge the files on the Topic column to obtain the most common terms per document using the following command:<p class="snippet">merged_df = pd.merge(doc_topics, topic_terms, on='topic')</p><p class="snippet">Print the merged_df to the console</p><p class="snippet">print(merged_df)</p></li>
				<li>Next, navigate to the location of the <strong class="inline">CSV</strong> files in the Command Prompt, and execute the code with the following command:<p class="snippet">python local_csv_to_s3.py</p></li>
				<li>The console output is a merged <strong class="inline">dataframe</strong> that provides the <strong class="inline">docnames</strong> with their respective terms and the term's weights (see the following):<div class="IMG---Figure" id="_idContainer130"><img alt="Figure 3.40: known_strucutre topic modeling merged results&#13;&#10;" src="image/Image_Lesson3_041.jpg"/></div><h6>Figure 3.40: known_strucutre Topic modeling merged results</h6></li>
				<li>To verify the CSV's, navigate to S3, (reload the page if the new bucket does not appear), and the new bucket has been created in S3. Click on the bucket to verify a successful import:<div class="IMG---Figure" id="_idContainer131"><img alt="Figure 3.41: known-tm-analysis S3 bucket&#13;&#10;" src="image/Image_Lesson3_042.jpg"/></div><h6>Figure 3.41: known-tm-analysis S3 bucket</h6></li>
				<li>There will be two CSV files in the bucket: <strong class="inline">doc-topics.csv</strong> and <strong class="inline">topic-terms.csv</strong>:</li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer132">
					<img alt="Figure 3.42: Topic modeling results uploaded to S3&#13;&#10;" src="image/Image_Lesson3_043.jpg"/>
				</div>
			</div>
			<h6>Figure 3.42: Topic modeling results uploaded to S3</h6>
			<h3 id="_idParaDest-69">Activity 4: Perform Topic Modeling on a Set<a id="_idTextAnchor071"/> of Documents with Unknown Topics</h3>
			<p>In this activity, we will perform Topic modeling on a set of documents with unknown topics. Topic modeling, we will consider an example. Suppose your employer wants you to build a data pipeline to analyze negative movie reviews that are in individual text files with a unique ID filename. Thus, you need to perform Topic modeling to determine which files represent the respective topics. Overall, negative reviews provide more monetary benefit or loss to the company, thus, they are prioritizing negative reviews versus positive reviews. The company's end goal is to incorporate the data into a feedback chatbot application. To ensure that this happens correctly, you need have a file that contains negative comments. The expected outcome for this activity will be the Topic modeling results from the negative movie review files.</p>
			<p><strong class="bold">Performing Topic Modeling</strong></p>
			<ol>
				<li value="1">Navigate to the following link to obtain the text data file that contains negative review comments: <a href="https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/activity/localfoldernegative_movie_review_files/cv000_29416.txt">https://github.com/TrainingByPackt/Machine-Learning-with-AWS/blob/master/lesson3/activity/localfoldernegative_movie_review_files/cv000_29416.txt</a>.</li>
				<li>Create a bucket for the Topic modelling with a unique name.</li>
				<li>Create a folder for the Topic modelling.</li>
				<li>Import the OS and Boto3. Mention your unique bucket name.</li>
				<li>Gather all of the working directories of the local path and make them into text files.</li>
				<li>Create a list for<a id="_idTextAnchor072"/> all of the text files.</li>
				<li>Iterate the files and upload them to S3.</li>
				<li>Create a job on Organization using Amazon Comprehend.</li>
				<li>As per the requirements, choose the input data. It may be <strong class="bold">My document</strong> or <strong class="bold">Example document</strong>.</li>
				<li>Choose the file from the data source.</li>
				<li>Apply the input format.</li>
				<li>Provide the number of topics to perform the modeling.</li>
				<li>Choose an IAM role and create a job.</li>
				<li>Download the output file and extract the file.</li>
				<li>The generated output will include the two <strong class="inline">.CSV</strong> files.</li>
			</ol>
			<p><strong class="bold">Analysis of Unknown Topics</strong></p>
			<ol>
				<li value="1">Import <strong class="inline">Boto3</strong> and <strong class="inline">pandas</strong>.</li>
				<li>Create the S3 client.</li>
				<li>Create a new bucket with a unique name.</li>
				<li>Create a list of CSV filenames to import.</li>
				<li>Check the filename and assign it to the <strong class="bold">obj</strong> variable.</li>
				<li>Read the <strong class="bold">obj</strong> variable.</li>
				<li>Merge the files on the Topic column.</li>
				<li>Print the merged files to the console.<h4>Note</h4><p class="callout">To refer to the detailed steps, go to the <em class="italics">Appendix A</em> at the end of this book on Page no.203</p></li>
			</ol>
			<h3 id="_idParaDest-70"><a id="_idTextAnchor073"/>Summary</h3>
			<p>In this chapter, we learned about analyzing Topic modeling results from AWS Comprehend. You are now able to incorporate S3 to store data and can use it to perform analysis. In addition, we learned how to analyze documents that we know the topics of before performing Topic modeling, and those documents where the Topic is known. The latter requires additional analysis to determine the relevant topics. </p>
			<p>In the next chapter, we will dive into the concept of chatbots and their understanding by using Natural Processing Language.</p>
		</div>
		<div>
			<div class="Content" id="_idContainer134">
			</div>
		</div>
		<div>
			<div class="Content" id="_idContainer135">
			</div>
		</div>
	</body></html>