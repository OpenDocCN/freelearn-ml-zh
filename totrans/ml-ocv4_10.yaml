- en: Discovering Hidden Structures with Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have focused our attention exclusively on supervised learning problems,
    where every data point in the dataset had a known label or target value. However,
    what do we do when there is no known output or no teacher to supervise the learning
    algorithm?
  prefs: []
  type: TYPE_NORMAL
- en: This is what **unsupervised learning** is all about. In unsupervised learning,
    the learning process is shown only in the input data and is asked to extract knowledge
    from this data without further instruction. We have already talked about one of
    the many forms that unsupervised learning comes in—**dimensionality reduction**.
    Another popular domain is **cluster analysis**, which aims to partition data into
    distinct groups of similar items.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the problems where clustering techniques can be useful are document
    analysis, image retrieval, finding spam emails, identifying fake news, identifying
    criminal activities, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we want to understand how different clustering algorithms can
    be used to extract hidden structures in simple, unlabeled datasets. These hidden
    structures have many benefits, whether they are used in feature extraction, image
    processing, or even as a preprocessing step for supervised learning tasks. As
    a concrete example, we will learn how to apply clustering to images to reduce
    their color spaces to 16 bits.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**k-means clustering** and **expectation-maximization** and implementing these
    in OpenCV'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arranging clustering algorithms in hierarchical trees and what are the benefits
    that come from that
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using unsupervised learning for preprocessing, image processing, and classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Using TF-IDF to improve the result
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It was called the **Term Frequency-Inverse Document Frequency** (**TF****-IDF**),
    and we encountered it in [Chapter 4](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml),
    *Representing Data and Engineering Features*. If you recall, what TF-IDF does
    is basically weigh the word count by a measure of how often the words appear in
    the entire dataset. A useful side effect of this method is the IDF part—the inverse
    frequency of words. This makes sure that frequent words, such as *and*, *the*,
    and *but*, carry only a small weight in the classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply TF-IDF to the feature matrix by calling `fit_transform` on our existing
    feature matrix, `X`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Don't forget to split the data; also, ...
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took our first look at probability theory, learning about
    random variables and conditional probabilities, which allowed us to get a glimpse
    of Bayes' theorem—the underpinning of a Naive Bayes classifier. We talked about
    the difference between discrete and continuous random variables, likelihoods and
    probabilities, priors and evidence, and normal and Naive Bayes classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, our theoretical knowledge would be of no use if we didn't apply it
    to practical examples. We obtained a dataset of raw email messages, parsed it,
    and trained Bayesian classifiers on it to classify emails as either spam or ham
    (not spam) using a variety of feature extraction approaches.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will switch gears and, for once, discuss what to do
    if we have to deal with unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can refer to the code for this chapter from the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter08](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the software and hardware requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Python version 3.6 (any Python version 3.x will be fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Anaconda Python 3 for installing Python and the required modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use any operating system—macOS, Windows, and Linux-based OSes—along
    with this book. We recommend you have at least 4 GB RAM in your system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't need to have a GPU to run the code provided along with this book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning might come in many shapes and forms, but the goal is always
    to convert original data into a richer, more meaningful representation, whether
    that means making it easier for humans to understand or easier for machine learning
    algorithms to parse.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common applications of unsupervised learning include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: This takes a high-dimensional representation
    of data consisting of many features and tries to compress the data so that its
    main characteristics can be explained with a small number of highly informative
    features. For example, when applied to housing prices in the neighborhoods of
    Boston, dimensionality reduction might be able to tell us that the indicators
    we should pay most attention to are the property tax and the neighborhood''s crime
    rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Factor analysis**: This tries to find the hidden causes or unobserved components
    that gave rise to the observed data. For example, when applied to all of the episodes
    of the 1970s TV show, *Scooby-Doo, Where Are You!*, factor analysis might be able
    to tell us that (spoiler alert!) every ghost or monster on the show is essentially
    some disgruntled count playing an elaborate hoax on the town.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster analysis**: This tries to partition the data into distinct groups
    of similar items. This is the type of unsupervised learning we will focus on in
    this chapter. For example, when applied to all of the movies on Netflix, cluster
    analysis might be able to automatically group them into genres.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make things more complicated, these analyses have to be performed on unlabeled
    data, where we do not know beforehand what the right answer should be. Consequently,
    a major challenge in unsupervised learning is to determine whether an algorithm
    did well or learned anything useful. Often, the only way to evaluate the result
    of an unsupervised learning algorithm is to inspect it manually and determine
    by hand whether the result makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, unsupervised learning can be immensely helpful, for example,
    as a preprocessing or feature extraction step. You can think of unsupervised learning
    as a **data transformation**—a way to transform data from its original representation
    into a more informative form. Learning a new representation might give us deeper
    insights into our data, and sometimes, it might even improve the accuracy of supervised
    learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The essential clustering algorithm that OpenCV provides is *k*-means clustering, which
    searches for a predestined number of *k-*clusters (or groups) from an unlabeled
    multi-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: 'It achieves this by using two simple hypotheses about what optimal clustering
    should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: The center of each cluster is basically the mean of all of the points belonging
    to that cluster, also known as the centroid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each data point in that cluster is closer to its center than to all other cluster
    centers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's easiest to understand the algorithm by looking at a concrete example.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing our first k-means example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s generate a 2D dataset containing four distinct blobs. To emphasize
    that this is an unsupervised approach, we will leave the labels out of the visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will continue using `matplotlib` for all of our visualization purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the same recipe from earlier chapters, we will create a total of
    300 blobs (`n_samples=300`) belonging to four distinct clusters (`centers=4`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1479c696-e19b-4992-bbfe-1d4e38a58124.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows an example dataset of 300 unlabeled points organized
    into four distinct clusters. Even without assigning target labels to the data,
    it is straightforward to pick out the four clusters by eye. The *k*-means algorithm
    can do this, too, without having any information about target labels or underlying
    data distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although *k*-means is, of course, a statistical model, in OpenCV, it does not
    come via the `ml` module and the common `train` and `predict` API calls. Instead,
    it is directly available as `cv2.kmeans`. To use the model, we have to specify
    some arguments, such as the termination criteria and some initialization flags.
    Here, we will tell the algorithm to terminate whenever the error is smaller than
    1.0 (`cv2.TERM_CRITERIA_EPS`) or when ten iterations have been executed (`cv2.TERM_CRITERIA_MAX_ITER`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can pass the preceding data matrix (`X`) to `cv2.means`. We also specify
    the number of clusters (`4`) and the number of attempts the algorithm should make
    with different random initial guesses (`10`), as shown in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Three different variables are returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one, `compactness`, returns the sum of squared distances from each
    point to their corresponding cluster centers. A high compactness score indicates
    that all points are close to their cluster centers, whereas a low compactness
    score indicates that the different clusters might not be well separated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, this number strongly depends on the actual values in `X`. If the
    distances between points were large, to begin with, we could not expect an arbitrarily
    small compactness score. Hence, it is more informative to plot the data points,
    colored to their assigned cluster labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces a scatter plot of all of the data points colored according to
    whichever cluster they belong to, with the corresponding cluster centers indicated
    with a blob of a darker shade in the center of each cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9305c4e4-b92e-45a9-865b-ffd047e3ca5e.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows the result of *k*-means clustering for *k=4*. The
    good news here is that the *k*-means algorithm (at least, in this simple case)
    assigns the points to clusters very similarly to how we might have, had we done
    the job by eye. But how did the algorithm find these different clusters so quickly?
    After all, the number of possible combinations of cluster assignments is exponential
    to the number of data points! By hand, trying all possible combinations would
    have certainly taken forever.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, an exhaustive search is not necessary. Instead, the typical approach
    that *k*-means takes is to use an iterative algorithm, also known as **expectation-maximization**.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding expectation-maximization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*k*-means clustering is but one concrete application of a more general algorithm
    known as expectation-maximization. In short, the algorithm works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with some random cluster centers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat until convergence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expectation step**: Assign all data points to their nearest cluster center.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maximization step**: Update the cluster centers by taking the mean of all
    of the points in the cluster.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, the expectation step is so named because it involves updating our expectation
    of which cluster each point in the dataset belongs to. The maximization step is
    so named because it involves maximizing a fitness function that defines the location
    of the cluster centers. In the case of *k*-means, maximization ...
  prefs: []
  type: TYPE_NORMAL
- en: Implementing our expectation-maximization solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The expectation-maximization algorithm is simple enough for us to code it ourselves.
    To do so, we will define a function, `find_clusters(X, n_clusters, rseed=5)`,
    that takes as input a data matrix (`X`), the number of clusters we want to discover
    (`n_clusters`), and a random seed (optional, `rseed`). As will become clear in
    a second, scikit-learn''s `pairwise_distances_argmin` function will come in handy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can implement expectation-maximization for *k*-means in five essential steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialization**: Randomly choose a number of cluster centers, `n_clusters`.
    We don''t just pick any random number but instead pick actual data points to be
    the cluster centers. We do this by permuting `X` along its first axis and picking
    the first `n_clusters` points in this random permutation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**`while` looping forever**: Assign labels based on the closest cluster centers.
    Here, scikit-learn''s `pairwise_distance_argmin` function does exactly what we
    want. It computes, for each data point in `X`, the index of the closest cluster
    center in `centers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Find the new cluster centers**: In this step, we have to take the arithmetic
    mean of all data points in `X` that belong to a specific cluster (`X[labels ==
    i]`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Check for convergence and break the** `while` **loop if necessary**: This
    is the last step to make sure that we stop the execution of the algorithm once
    the job is done. We determine whether the job is done by checking whether all
    of the new cluster centers are equal to the old cluster centers. If this is true,
    we exit the loop; otherwise, we keep looping:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Exit the function and return the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can apply our function to the preceding data matrix, `X`, we created. Since
    we know what the data looks like, we know that we are looking for four clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following plot. The vital point to observe from the
    following diagram is that, before applying *k*-means clustering, all data points
    were categorized to the same one color; however, after using *k*-means clustering,
    each color is a different cluster (similar data points are clustered or grouped
    in one color) :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b09ee68-3da7-4eaf-b90b-324b62fa1266.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows the outcome of our home-made *k*-means using expectation-maximization.
    As we can see, our home-made algorithm got the job done! Granted, this particular
    clustering example was fairly easy, and most real-life implementations of *k*-means
    clustering will do a bit more under the hood. But for now, we are happy.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing the limitations of expectation-maximization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For all its simplicity, expectation-maximization performs incredibly well in
    a range of scenarios. That being said, there are some potential limitations that
    we need to be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: Expectation-maximization does not guarantee that we will find the globally best
    solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must know the number of desired clusters beforehand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decision boundaries of the algorithms are linear.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is slow for large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's quickly discuss these potential caveats in a little more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The first caveat – no guarantee of finding the global optimum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although mathematicians have proved that the expectation-maximization step
    improves the result in each step, there is still no guarantee that, in the end,
    we will find the global best solution. For example, if we use a different random
    seed in our simple example (such as using seed `10` instead of `5`), we suddenly
    get very poor results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec702b4a-9401-4088-b676-9f5b5149ee5d.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows an example of *k*-means missing the global optimum.
    What happened?
  prefs: []
  type: TYPE_NORMAL
- en: The short answer is that the random initialization of cluster centers was unfortunate.
    It led to the center of the yellow cluster migrating in-between the two top blobs,
    essentially combining them into one. As a result, the other clusters got confused
    because they suddenly had to split two visually distinct blobs into three clusters.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, it is common for the algorithm to be run for multiple initial
    states. Indeed, OpenCV does this by default (set by the optional `attempts` parameter).
  prefs: []
  type: TYPE_NORMAL
- en: The second caveat – we must select the number of clusters beforehand
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another potential limitation is that *k*-means cannot learn the number of clusters
    from the data. Instead, we must tell it how many clusters we expect beforehand.
    You can see how this could be problematic for complicated real-world data that
    you don't fully understand yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the viewpoint of *k*-means, there is no wrong or nonsensical number of
    clusters. For example, if we ask the algorithm to identify six clusters in the
    dataset generated in the preceding section, it will happily proceed and find the
    best six clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The third caveat – cluster boundaries are linear
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *k*-means algorithm is based on a simple assumption, which is that points
    will be closer to their own cluster center than to others. Consequently, *k*-means
    always assumes linear boundaries between clusters, meaning that it will fail whenever
    the geometry of the clusters is more complicated than that.
  prefs: []
  type: TYPE_NORMAL
- en: 'We see this limitation for ourselves by generating a slightly more complicated
    dataset. Instead of generating data points from Gaussian blobs, we want to organize
    the data into two overlapping half circles. We can do this using scikit-learn''s
    `make_moons`. Here, we choose 200 data points belonging to two half circles, in
    combination with some Gaussian noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, we tell *k*-means to look for two clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting scatter plot looks like this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/305edf9a-f7c3-491e-8a1e-10c6427d1917.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows an example of *k*-means finding linear boundaries
    in nonlinear data. As is evident from the plot, *k*-means failed to identify the
    two half circles and instead split the data with what looks like a diagonal straight
    line (from bottom-left to top-right).
  prefs: []
  type: TYPE_NORMAL
- en: This scenario should ring a bell. We had the same problem when we talked about
    linear SVMs in [Chapter 6](419719a8-3340-483a-86be-1d9b94f4a682.xhtml), *Detecting
    Pedestrians with Support Vector Machines.* The idea there was to use the kernel
    trick to transform the data into a higher-dimensional feature space. Can we do
    the same here?
  prefs: []
  type: TYPE_NORMAL
- en: 'We most certainly can. There is a form of kernelized *k*-means that works akin
    to the kernel trick for SVMs, called **spectral clustering**. Unfortunately, OpenCV
    does not provide an implementation of spectral clustering. Fortunately, scikit-learn
    does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm uses the same API as all other statistical models: we set optional
    arguments in the constructor and then call `fit_predict` on the data. Here, we
    want to use the graph of the nearest neighbors to compute a higher-dimensional
    representation of the data and then assign labels using *k*-means:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of spectral clustering looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c51c11b-66f9-408d-b404-ae809e59bb5b.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that spectral clustering gets the job done. Alternatively, we could have
    transformed the data into a more suitable representation ourselves and then applied
    OpenCV's linear *k*-means to it. The lesson of all of this is that, perhaps, again,
    feature engineering saved the day.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth caveat – k-means is slow for a large number of samples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The final limitation of *k*-means is that it is relatively slow for large datasets.
    You can imagine that quite a lot of algorithms might suffer from this problem.
    However, *k*-means is affected especially badly: each iteration of *k*-means must
    access every single data point in the dataset and compare it to all of the cluster
    centers.'
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder whether the requirement to access all data points during each
    iteration is really necessary. For example, you might just use a subset of the
    data to update the cluster centers at each step. Indeed, this is the exact idea
    that underlies a variation of the algorithm called **batch-based *k*-means**.
    Unfortunately, this algorithm is not implemented ...
  prefs: []
  type: TYPE_NORMAL
- en: Compressing color spaces using k-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One interesting use case of *k*-means is the compression of image color spaces.
    For example, a standard **color image** comes with 24-bit color depth, providing
    for a total of 16,777,216 color varieties. However, in most images, a large number
    of colors will be unused, and many of the pixels in the image will have similar
    values. The compressed image can then be sent over the internet at a faster speed,
    and at the receiver end, it can be decompressed to get back the original image.
    Hence, reducing the storage and transmission cost. However, the image color space
    compression will be **lossy** and you might not notice fine details in the image
    after compression.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can use *k*-means to reduce the color palette too. The idea
    here is to think of the cluster centers as the decreased color palette. Then, *k*-means
    will organize the millions of colors in the original image into the appropriate
    number of colors.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the true-color palette
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By performing the following steps, you will be able to visualize the true-color
    palette of a color image:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at a particular image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'By now, we know how to start Matplotlib in our sleep:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this time, we want to disable the grid lines that the `ggplot` option
    typically displays over images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can visualize Lena with the following command (don''t forget to switch
    the BGR ordering of the color channels to RGB):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Reducing the color palette using k-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By referring to the following steps, you will be able to project a color image
    into a reduced color palette using *k*-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s reduce the 16 million colors to a mere 16 by indicating *k*-means
    to cluster all of the 16 million color variations into 16 distinct clusters. We
    will use the previously mentioned procedure, but now define 16 as the number of
    clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The 16 different colors of your reduced color palette correspond to the resultant
    clusters. The output from the `centers` array reveals that all colors have three
    entries—`B`, `G`, and `R`—with values between 0 and 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `labels` vector contains the 16 colors corresponding to the 16 cluster `labels`.
    So, all of the data points with the label 0 will be colored according to row 0
    in the `centers` array; similarly, all data points with the label 1 will be colored
    according to row 1 in the `centers` array and so on. Hence, we want to use `labels`
    as an index in the `centers` array—these are our new colors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot the data again, but this time, we will use `new_colors` to color
    the data points accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the recoloring of the original pixels, where each pixel is assigned
    the color of its closest cluster center:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3b6e0d8-5b3b-4c4f-98fc-c75abe2aefe1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To observe the effect of recoloring, we have to plot `new_colors` as an image.
    We flattened the earlier image to get from the image to the data matrix. To get
    back to the image now, we need to do the inverse, which is reshape `new_colors`
    according to the shape of the Lena image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can visualize the recolored Lena image like any other image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The result looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db7c60af-4276-454d-9f22-a32cda3bdb5e.png)'
  prefs: []
  type: TYPE_IMG
- en: It's pretty awesome, right?
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the preceding screenshot is quite clearly recognizable although some
    details are arguably lost. Given that you compressed the image by a factor of
    around 1 million, this is pretty remarkable.
  prefs: []
  type: TYPE_NORMAL
- en: You can repeat this procedure for any desired number of colors.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to reduce the color palette of images involves the use of **bilateral
    filters**. The resulting images often look like cartoon versions of the original
    image. You can find an example of this in the book, *OpenCV with Python Blueprints*,
    by M. Beyeler, Packt Publishing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another potential application of *k*-means is something you might not expect:
    using it for image classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying handwritten digits using k-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the last application was a pretty creative use of *k*-means, we can
    do better still. We have previously discussed *k*-means in the context of unsupervised
    learning, where we tried to discover some hidden structure in the data.
  prefs: []
  type: TYPE_NORMAL
- en: However, doesn't the same concept apply to most classification tasks? Let's
    say our task was to classify handwritten digits. Don't most zeros look similar,
    if not the same? And don't all zeros look categorically different from all possible
    ones? Isn't this exactly the kind of *hidden structure* we set out to discover
    with unsupervised learning? Doesn't this mean we could use clustering for classification
    as well?
  prefs: []
  type: TYPE_NORMAL
- en: Let's find out together. In this section, we will attempt ...
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the earlier chapters, you might recall that scikit-learn provides a whole
    range of handwritten digits via its `load_digits` utility function. The dataset
    consists of 1,797 samples with 64 features each, where each of the features has
    the brightness of one pixel in an *8 x 8* image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Running k-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Setting up *k*-means works exactly the same as in the previous examples. We
    tell the algorithm to perform at most 10 iterations and stop the process if our
    prediction of the cluster centers does not improve within a distance of `1.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we apply *k*-means to the data as we did before. Since there are 10 different
    digits (0-9), we tell the algorithm to look for 10 distinct clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: And we're done!
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the *N x 3* matrices ...
  prefs: []
  type: TYPE_NORMAL
- en: Organizing clusters as a hierarchical tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative to *k*-means is **hierarchical clustering**. One advantage of
    hierarchical clustering is that it allows us to organize the different clusters
    in a hierarchy (also known as a **dendrogram**), which can make it easier to interpret
    the results. Another useful advantage is that we do not need to specify the number
    of clusters upfront.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two approaches to hierarchical clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: In **agglomerative hierarchical clustering**, we start with each data point
    potentially being its own cluster, and we subsequently merge the closest pair
    of clusters until only one cluster remains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In **divisive hierarchical clustering**, it's the other way around; we start
    by assigning all of the data points to the same cluster, and we subsequently split
    the cluster into smaller clusters until each cluster only contains one sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Of course, we can specify the number of desired clusters if we wish to. In
    the following screenshot, we asked the algorithm to find a total of three clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows a step-by-step example of agglomerative ...
  prefs: []
  type: TYPE_NORMAL
- en: Implementing agglomerative hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although OpenCV does not provide an implementation of agglomerative hierarchical
    clustering, it is a popular algorithm that should, by all means, belong to our
    machine learning skill set:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by generating 10 random data points, just like in the previous screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the familiar statistical modeling API, we import the `AgglomerativeClustering`
    algorithm and specify the desired number of clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Fitting the model to the data works, as usual, via the `fit_predict` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can generate a scatter plot where every data point is colored according
    to the predicted label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting clustering is equivalent to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c9d3311-d0f0-4f90-89aa-f93c45af9b60.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, before we end this chapter, let's look at how to compare clustering
    algorithms and choose the correct clustering algorithm for the data you have!
  prefs: []
  type: TYPE_NORMAL
- en: Comparing clustering algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are around thirteen different clustering algorithms in the `sklearn`
    library. Having thirteen different sets of choices, the question is: what clustering
    algorithms should you use? The answer is your data. What type of data you have
    and which clustering you would like to apply on it is how you will select the
    algorithm. Having said that, there can be many possible algorithms that could
    be useful for the kind of problem and data you have. Each of the thirteen classes
    in `sklearn` is specialized for specific tasks (such as co-clustering and bi-clustering
    or clustering features instead of data points). An algorithm specializing in text
    clustering would be the right choice for clustering text data. Hence, if ...'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about some unsupervised learning algorithms, including
    *k*-means, spherical clustering, and agglomerative hierarchical clustering. We
    saw that *k*-means is just a specific application of the more general expectation-maximization
    algorithm, and we discussed its potential limitations. Furthermore, we applied
    *k*-means to two specific applications, which were reducing the color palette
    of images and classifying handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will move back into the world of supervised learning
    and talk about some of the most powerful current machine learning algorithms:
    neural networks and deep learning.'
  prefs: []
  type: TYPE_NORMAL
