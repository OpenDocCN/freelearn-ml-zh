- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unveiling the Dynamics of Marketing Success
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Key performance indicators** (**KPIs**) are great at capturing the current
    status or results of marketing initiatives. However, they lack the ability to
    show what drives such results and how different factors may have affected the
    end results of marketing campaigns. We briefly looked at how you can further analyze
    relationships between various variables and KPIs with correlation analysis in
    the last chapter. In this chapter, we are going to delve deeper into how we can
    utilize **data science** (**DS**) and **machine learning** (**ML**) techniques
    and tools to unveil the dynamics of marketing success.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter touches on three critical phases of the customer journey: engagement,
    conversion, and churn. These three types of customer behaviors are the key components
    and marketing goals to optimize for, and marketers often question what factors
    actually affect certain outcomes, such as why people engage more or less, why
    people end up converting more or less, and why people churn or don’t churn. The
    three most common approaches that are taken to unveil the factors that affect
    marketing outcomes, on top of the correlation analysis we have done in the last
    chapter, are (1) regression analysis, (2) decision tree interpretation, and (3)
    causal inference. In this chapter, we will discuss how we can perform those ML
    techniques in Python to have a better and more in-depth interpretation of the
    drivers behind marketing successes and failures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why people engage with regression analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why people convert with decision tree interpretation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why people churn with causal inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why people engage in regression analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Usually, increasing the engagement rate is the very first step in marketing.
    Potential customers need to engage with the company or products first before they
    can convert into paying customers and then, eventually, repeat or loyal customers.
    There will be numerous factors that affect how potential customers engage with
    or click on your marketing campaigns. Customers with certain socio-economic backgrounds
    may be more attracted to your services and/or products than others. People from
    certain regions may engage more frequently in your marketing campaigns than others.
    Some who have been exposed to your previous sales may have a more favorable appetite
    for your services or products that results in higher engagement rates than others.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how potential customers interact with different components is
    the key to achieving higher engagement rates and the start of targeted or personalized
    marketing. **Regression analysis**, a well-known and widely used technique across
    various domains and often considered a fundamental concept in ML, is an intuitive
    and effective approach that we can use to understand the interactions among various
    components that may affect higher or lower engagement among your target customers.
    The two most commonly used types of regression analysis are **linear regression**
    and **logistic regression**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A linear regression assumes a linear relationship between a target variable
    and other factors. The linear relationship is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: where *Y* is the target variable, which is the outcome that you observe, each
    ![](img/B30999_03_002.png) is the factor that may affect the outcome, each ![](img/B30999_03_003.png)
    is the coefficient and degree of impact each factor has on the target variable,
    and *a* is the intercept. Linear regression is used when the target variable is
    a continuous variable, such as customer lifetime value, sales volume, and customer
    tenure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression, on the other hand, is used when the target variable is
    binary, such as `Pass` vs. `Fail`, `True` vs. `False`, and 1 vs. 0\. When linear
    regression estimates the value of an outcome, logistic regression estimates the
    odds of success, and the relationship is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Where log denotes the natural logarithm and *P*(*y* = 1) is the probability
    of the outcome being 1
  prefs: []
  type: TYPE_NORMAL
- en: As logistic regression estimates the probability of an outcome, it is a great
    tool to use when understanding the impacts of various factors on a binary outcome,
    such as whether a customer will respond to email marketing or whether a customer
    will click on an email message. In this section, we will use an auto insurance
    marketing dataset to discuss how logistic regression can be used to unveil the
    drivers behind customer engagement.
  prefs: []
  type: TYPE_NORMAL
- en: '**Source code and data**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.3/Why%20People%20Engage.ipynb](https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.3/Why%20People%20Engage.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data source: [https://www.kaggle.com/datasets/pankajjsh06/ibm-watson-marketing-customer-value-data](https://www.kaggle.com/datasets/pankajjsh06/ibm-watson-marketing-customer-value-data)'
  prefs: []
  type: TYPE_NORMAL
- en: Target variable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing we need to do is define the target or outcome we want to analyze.
    As we are interested in understanding how various factors affect customer engagement,
    we will have the `Response` variable as our target variable, which tells us whether
    a customer responded to a marketing call or not. The following code can be used
    for encoding the target variable with `0` for when a customer did not respond
    and `1` for when a customer did respond:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code first loads the data into a variable, `df`, by using the `pandas`
    library’s `read_csv` function, and we create a new variable, `Engaged`, by encoding
    `Yes` as `1` and `No` as `0` from the `Response` column.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are largely two types of variants of factors that may affect the outcome
    or target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous variables**: These are the variables that have real numbers as
    the values and can include fractions or decimals, such as monetary amounts, sales
    numbers, and pageview counts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical variables**: These are discrete variables that have a distinct
    set of options or categories as values, such as education levels, genders, and
    customer account types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will first look at the relationships between some of the continuous variables
    in our auto insurance marketing dataset and the target variable, `Engaged`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B30999_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Distributions of continuous variables in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'As this screenshot shows, when we run the `df.describe()` function, it shows
    the overall statistics of the continuous variables. In our auto insurance example
    dataset, there are eight continuous variables: `Customer_Lifetime_Value`, `Income`,
    `Monthly_Premium_Auto`, `Months_Since_Last_Claim`, `Months_Since_Policy_Inception`,
    `Number_of_Open_Complaints`, `Number_of_Policies`, and `Total_Claim_Amount`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for us to analyze the relationships between these variables and the
    outcome variable, `Engaged`, we are going to use `Logit` within the `statsmodels`
    package. The code to fit a logistic regression model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we first import the required module, `statsmodels.api`, and define the
    continuous variable, `continuous_vars`. Then, we can use the `Logit` object in
    the `statsmodels` package to train a logistic regression model. We can view the
    trained logistic regression model with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code looks as in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Summary of logistic regression results'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s analyze this logistic regression output. The two most important things
    to look at are, *coef* and *P*>|*z*|. The leftmost column, `coef`, in the bottom
    table is the coefficient for each variable. For example, the coefficient for the
    variable `Monthly_Premium_Auto` is `-0.0084` and the coefficient for the variable
    `Total_Claim_Amount` is `0.0001`. This means that customers paying higher monthly
    premiums are less likely to respond to marketing campaigns, which aligns with
    the heuristics that high-value customers often are the harder ones to get and
    encourage engagement. On the other hand, customers with higher claim amounts are
    more likely to engage, as they may be the ones utilizing insurance policies the
    most.
  prefs: []
  type: TYPE_NORMAL
- en: The *P*>|*z*| column suggests the statistical significance each variable has
    on the outcome. The values lower than `0.05` are typically considered to have
    a statistically significant impact on the outcome, as p-values lower than `0.05`
    suggest there is strong evidence against the null hypothesis that there is no
    impact of this variable on the target variable. However, values larger than `0.05`
    do not necessarily suggest it does not have any significance on the outcome. You
    may also notice the coefficients for `Customer_Lifetime_Value` and `Income` are
    very small.
  prefs: []
  type: TYPE_NORMAL
- en: This does not suggest they have low impacts on the outcome. As you may have
    guessed, these two variables have monetary values and the scales for these two
    variables are in thousands, which results in small coefficient values, but with
    significant impact on the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen how continuous variables interact with the outcome, `Engaged`,
    in a positive or negative way. You may wonder, then, what about categorical or
    discrete variables? We have a handful of categorical variables within our auto
    insurance example dataset; however, we will use the `Education` and `Gender` variables
    as examples to discuss how to handle categorical variables when performing regression
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Factorize
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first approach we can take is to utilize the `factorize` functionality
    within the `pandas` library. Using `Education` as an example, you can run the
    following code to encode a textual discrete variable into a numerical variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of this factorization look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Categorical
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second approach we can take to encode categorical variables is to use the
    `Categorical` function within the `pandas` library. The following code shows an
    example of how to use the `Categorical` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `categories` argument in the `Categorical` function lets you define the
    order of the categories. Here, we defined the `Education` category to order from
    `High School or Below` to `Doctor`. You can access the categories by calling the
    `categories` attribute and you can get the encodings for each record by calling
    the `codes` attribute. For example, in our case, `High School or Below` is encoded
    with `0`, `Bachelor` with `1`, and `Doctor` with `4`.
  prefs: []
  type: TYPE_NORMAL
- en: Dummy variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One other approach that can be taken to encode categorical variables is to create
    dummy variables. Dummy variables are one-hot encoded variables for each category.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in our example, the following dummy variables can be created
    for the `Education` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B30999_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Dummy variables created for the Education variable'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from this example, the `pandas` library’s `get_dummies` function
    was used to create dummy variables for the `Education` variable. This creates
    five new variables: `Bachelor`, `College`, `Doctor`, `High School or Below`, and
    `Master`. Each of these newly created variables is encoded `0/1` (or `False/True`),
    where if a given record belongs to a given category, it is encoded as `1` or `True`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**How to choose which method to use for categorical variable encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: '**One-hot encoding**/**dummy variables**: Use these when there is no natural
    ordering in the variable. Both methods create binary variables for each category.
    Note that creating one-hot encoded and dummy variables for all categories can
    result in large dimensional data and make data sparse. Thus, you may want to subselect
    important categories for one-hot encoding/dummy variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Factorize**: Use this as another quick and efficient method when you have
    nominal categorical variables that do not have any inherent order.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical**: When there is a natural ordering in the categorical variable,
    such as education levels, it is best to use `Categorical()` with a specified order.
    This ensures that the encoding respects the hierarchy of the categories, leading
    to accurate model predictions and evaluations such as AUC-ROC in analyses where
    order matters.'
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have explored different approaches that can be taken to encode
    categorical variables, we can run a regression analysis on how some of the categorical
    variables affect the outcome. Using the newly created categorical variables from
    the previous section, `GenderFactorized` and `EducationFactorized`, we can fit
    a logistic regression model with the following code to model the relationships
    and impacts of the `Gender` and `Education` levels on the level of marketing engagement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Running `logit.summary()` will result in the following summary of the fitted
    logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Summary of logistic regression results'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from this logistic regression model output, both variables, `GenderFactorized`
    and `EducationFactorized`, have negative coefficients or impact on the outcome,
    `Engaged`. As `Gender` is encoded as `0` for female and `1` for male, this suggests
    that males are less likely to respond than females.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, education levels are encoded from 0 to 4, and having a negative coefficient
    suggests that the higher the education level is, the less likely it is for a customer
    to respond.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the results we have seen from regression analysis with continuous
    variables in the previous section, these are great insights that show directional
    relationships among various factors against the engagement outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Interaction variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Regression models are great at identifying and estimating linear relationships
    between the variables and the target variable. However, variables are likely to
    have some relationships among themselves. For example, income may have a high
    correlation with education levels. One approach that can be taken to address such
    intrinsic relationships within the variables in regression analysis is to introduce
    multiplicative terms, as shown in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This way, the regression model considers the interactions between variables
    that have relationships within themselves. The same `Logit` within the `statsmodels`
    library can be used for running regression analysis with interaction terms in
    Python. The following code will fit a logistic regression model with interaction
    terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the code, we are fitting a logistic regression model with two variables,
    `Income` and `Education`, and one interaction term, `Income x Education`. The
    following is the summary of the fitted logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Summary of logistic regression results'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the first two components, `Income`and `EducationFactorized`, this
    output suggests that `Income` and `Education` have positive impacts on engagement
    likelihood. However, the interaction term, `Income:EducationFactorized`, negates
    the individual positive effects of `Income` and `Education`, as it has a negative
    coefficient. In our case of marketing, this means that when you compare customers
    within the same income level, the higher the education level is, the lower their
    engagement rate will be. Similarly, this also suggests that for customers with
    the same education level, the higher their income is, the lower their engagement
    rate will be. As you can see from this example, by introducing the interaction
    terms, we can analyze the relationships among the variables more deeply and gain
    insights that may not be unveiled when analyzed individually.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in this section, regression analysis is a powerful tool to unveil
    the dynamics of the drivers behind the successes and failures of marketing efforts.
    In our example, we have seen how various variables have negative and positive
    impacts on customer engagement rates. As shown, regression analysis is great at
    identifying linear relationships between various factors and the outcome and can
    be used to understand the interactions within the factors.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is generally considered that regression models lack the understanding
    of intervariable relationships. Part of this limitation is because we have to
    introduce interaction terms for all possible intervariable relationships. As we
    saw when we created `Income` and `Education` interaction terms previously, we
    will have to build similar interaction terms to address other possible interactions
    among the variables. Furthermore, if you want to introduce or consider interactions
    among more than two variables, it starts to get complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees, unlike linear regression analysis, have a strength in addressing
    complex interactions among variables. We will examine the use of decision trees
    in the following section, which captures the interactions among various factors
    and how these interactions ultimately affect the final outcome very well.
  prefs: []
  type: TYPE_NORMAL
- en: Why people convert with decision tree interpretation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once potential customers start engaging with your marketing campaigns, it is
    time to start converting them into paying customers. Similar to how various backgrounds
    may affect the tendency to engage, there are numerous factors affecting who converts
    more often than others. Socio-economic factors, of course, will always play a
    vital role in affecting the conversion rates. Age, region, and gender may also
    be important factors in different conversion results. Months or seasons of the
    year can also cause conversion rates to vary, depending on the services or products
    you provide.
  prefs: []
  type: TYPE_NORMAL
- en: Not only single factors but also combinations of various factors may have significant
    influences on who may convert. A person who is employed and owns a house is more
    likely to convert for refinance loans than a person who owns a house but is retired.
    A 25-year-old student is less likely to look for a credit card than a 25-year-old
    entrepreneur. We have briefly discussed how regression analysis can help identify
    how factors interacting with each other affect customer behaviors by introducing
    interaction or multiplicative factors. Another great approach to uncovering the
    hidden insights around why certain people convert more than others is to use **decision
    trees** and their power to understand the interactions and relationships among
    various factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees, as the name suggests, learn from data by growing trees. From
    a root node, a tree branches out by splitting the data into subcategories and
    eventually reaching the leaf nodes, from which you can understand what factors
    are affected in which ways to reach the corresponding leaf nodes. A sample of
    a decision tree may look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Sample of a decision tree'
  prefs: []
  type: TYPE_NORMAL
- en: We will have a detailed discussion on how to interpret the decision tree results
    with an example dataset, so we will save further explanation and discussion for
    later. Let’s talk first about how a decision tree branches out from each node.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are mainly two methods that are commonly used for splitting the data
    or branching out into child nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gini Impurity**: Gini impurity measures how impure a partition is. The formula
    for the Gini impurity measure is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B30999_03_006.png)'
  prefs: []
  type: TYPE_IMG
- en: In this formula, *c* stands for the class labels, and ![](img/B30999_03_007.png)
    stands for the probability of a record with the class label *i* being chosen.
    When all records in each node of a tree are pure with all records being in a single
    target class, the Gini impurity measure reaches `0`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Entropy Information Gain**: Entropy measures how much information it gains
    from splitting the data with the criteria being tested. The formula for *Entropy*
    is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B30999_03_008.png)'
  prefs: []
  type: TYPE_IMG
- en: As for the *Gini* measure, *c* stands for the class labels, and ![](img/B30999_03_009.png)
    stands for the probability of a record with the class label *i* being chosen.
    The split that gives the biggest change in the entropy measure will be chosen
    to branch out to child nodes, as it suggests that results in the highest information
    gain.
  prefs: []
  type: TYPE_NORMAL
- en: With this fundamental knowledge about decision trees, we will dive into applying
    one to understand what drives successful marketing campaigns for better conversion
    rates.
  prefs: []
  type: TYPE_NORMAL
- en: '**Source code and data**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.3/Why%20People%20Convert.ipynb](https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.3/Why%20People%20Convert.ipynb
    )'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data source:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/dataset/222/bank+marketing](https://archive.ics.uci.edu/dataset/222/bank+marketing)'
  prefs: []
  type: TYPE_NORMAL
- en: Target variable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be using a bank marketing dataset for this exercise, where the marketing
    campaign was to convert customers to subscribe to term deposits. First things
    first, let’s load the data into a DataFrame and encode our target variable. Take
    a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are loading the data into a variable, `df`, and encoding the `y` column
    into `1` for `yes` and `0` for `no`. When you run `df["conversion"].mean()`, you
    should see about 12% of the customers converted and subscribed to term deposits.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous versus categorical variable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For our analysis, we will be using the following variables, which have continuous
    values: `age`, `balance`, `duration`, `campaign`, and `previous`. You can use
    the following code to see the distribution for these variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The distribution looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Distribution of continuous variables'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the previous exercise, there are some categorical variables in this
    dataset as well. The first discrete text variable that we will convert into numerical
    values is `month`. If you look at the values of the variable `month`, you will
    notice that the values are 3-letter month abbreviations. As months have an inherent
    order, we will convert the values into corresponding numerical values using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now, the variable `month` will have values from `1` to `12` for months respectively
    from `January` to `December`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variable `job` is another categorical variable. Unlike the `month` variable,
    the `job` variable does not have an inherent order within itself. Thus, we will
    create dummy variables for each of the `job` categories, which will be one-hot
    encoded for the corresponding job categories. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B30999_03_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Encoded dummy variables'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the previous exercise, we have used the `get_dummies` function to
    create dummy variables for each of the job categories in the dataset. You can
    use the following code to add these newly created dummy variables back to the
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: With this code run, the DataFrame, `df`, will now have newly created dummy variables
    of the `job` column added to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we are going to encode `marital` and `housing` variables as well using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, we have created dummy variables for each of the
    marital categories, which are `divorced`, `married`, and `single`. Then, we concatenated
    these newly created dummy variables back to the DataFrame, `df`. We have also
    converted the textual values of the `housing` column into `1` for *yes* and `0`
    for *no*.
  prefs: []
  type: TYPE_NORMAL
- en: As categorical variable encoding is somewhat covered in depth in the previous
    exercise, we quickly went through this step for this exercise. Take your time
    going through this categorical variable encoding step and try different approaches
    that we discussed during the previous exercise as well!
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have compiled the variables of our interest, which are `age`, `balance`,
    `duration`, `campaign`, `previous`, `housing`, `month`, `job`, and `marital`status.
    We are going to start looking into how these factors have affected the conversion
    rate outcome of this marketing campaign. First, we are going to define our features
    and the target or response variable, as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As noted in this code, we are using the `conversion` column as our outcome
    or target variable and the continuous variables and encoded categorical variables
    that we have created previously. The resulting list of features should look as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B30999_03_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: List of features'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the feature set and target variable defined, we are ready to build a decision
    tree. Take a look at the following code first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We are using the `tree` module within the `sklearn` or `scikit-learn` library
    and using the `DecisionTreeClassifier` class to train a decision tree model. You
    will notice one argument that we have defined, `max_depth`. This argument controls
    how much the tree grows. The deeper the tree grows, the more accurately the tree
    learns the data; however, it will likely overfit to the dataset and not be generalizable,
    meaning it will perform worse for the data that has not been observed. It is best
    to limit how much a tree grows and balance between how well and accurately a tree
    learns the data and how well this knowledge can be generalized. In this exercise,
    we will set `max_depth` to `3`. Finally, you can use the `fit` function to let
    the tree learn from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Try finding a golden spot for how deep a decision tree should grow by splitting
    the dataset into training and testing sets and choosing the depth that minimizes
    the difference between the training and testing sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most intuitive method to use the knowledge that the decision tree gained
    from the data is to visualize how it branched out from each node and how the interactions
    between the variables lead to the leaf nodes. To do this, we will use the `graphviz`
    package, which you can install with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use the following code to draw the trained decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the code, we are giving the function the trained model,
    `dt_model`, and the features, `features`, that were used for training the model.
    Then, you can output the graph into an image file with the `render` function of
    the graph. This code will generate an image file named `conversion-dt-depth-3.png`
    and it will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Decision tree diagram'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from this decision tree graph, the depth of the tree is 3, which
    is what we defined it to be when we were training the decision tree. Let’s take
    a closer look at this graph.
  prefs: []
  type: TYPE_NORMAL
- en: The root node is being split based on the variable `duration`. As the arrows
    suggest, if the duration is less than or equal to `521.5`, then it moves to the
    left child node; otherwise, it moves to the right child node. If we traverse to
    the rightmost leaf node by following the path with a duration greater than `827.5`
    and `marital_married` greater than `0.5`, the majority of the customers in this
    leaf node have converted. In other words, those customers who have had a contact
    duration that is longer than `827.5` seconds and who are married have converted
    more.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if we traverse to the leftmost leaf node, it suggests that
    those customers who have had a contact duration that is shorter than or equal
    to `521.5` seconds (root node), 0 previous contacts (left child node at depth
    1), and their age is less than or equal to `60.5` (leftmost child node at depth
    2) are more likely not to convert than the others.
  prefs: []
  type: TYPE_NORMAL
- en: There are other parameters you can use to build decision trees. Try fine-tuning
    your trees and see how different parameters affect the formation of the decision
    trees and how you can deduce various other insights from the same dataset!
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from this example, by traversing through the tree and its branches
    and nodes, we can see the impacts of different factors of marketing on the customer
    behavior of conversion. Compared to the regression analysis that we did in the
    previous section, this decision tree has the advantage of better understanding
    more complex interactions and inter-relationships among different factors and
    how they affect the outcome variable. One thing to note is that both regression
    analysis and decision tree analysis results show correlations between the factors
    and the outcome rather than the causal relationships of what causes certain outcomes.
    Examining the causality of certain outcomes is not an easy task; however, in the
    following section, we will discuss how we can estimate the causal relationships
    between the features and the outcome from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Why people churn with causal inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A high churn rate is especially a problem in marketing as it negates all the
    good marketing income that was generated from previous marketing campaigns. It
    is critical to have an in-depth understanding of why people churn and what factors
    need to be optimized to reduce the customer churn rate. As we have seen in previous
    sections, regression analysis and decision tree analysis are great at identifying
    linear relationships between the potential factors and the outcome and the inter-relationships
    between various factors and the outcome variable. However, as noted before, these
    identified relationships or correlations do not necessarily mean causations.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the causes of certain outcomes (for example, causes of customer
    churn) is often a difficult and complex task to achieve. This is where **causal
    analysis** comes in. If regression analysis was used to identify the relationships
    among the variables and decision tree analysis was used to identify the *interactions*
    among the variables, causal analysis is used to identify the *causes* and *effects*
    of certain outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: This is typically done through various experiments with control groups and treatment
    or experimental groups. The control group, as the name suggests, is the group
    to which no treatment is given. In a medical experiment setting, this control
    group is the one that will get a placebo. The treatment group, on the other hand,
    is the group that actually gets the treatment. Similarly, in a medical experiment
    setting, this treatment group is the one that gets the actual medical treatments,
    such as new medicine pills being developed.
  prefs: []
  type: TYPE_NORMAL
- en: This paradigm of the necessity of setting up experiments prior to running a
    marketing campaign restricts the after-the-fact analysis. However, with the data,
    we can still run a causal analysis to estimate the effects of different factors
    on the outcome. We will be utilizing a Python package, `dowhy`, to run causal
    analysis based on the marketing campaign data. We will be using bank churn data
    as an example to discuss how a causal analysis can be done to unveil the causes
    and effects of certain variables on the marketing outcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**Source code and data**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.3/Why%20People%20Churn.ipynb](https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.3/Why%20People%20Churn.ipynb
    )'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data source**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/datasets/shrutimechlearn/churn-modelling](https://www.kaggle.com/datasets/shrutimechlearn/churn-modelling
    )'
  prefs: []
  type: TYPE_NORMAL
- en: 'Installation of the `dowhy` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '(Note: you may require additional installations for the `dowhy` package. Please
    refer to the official documentation at [https://pypi.org/project/dowhy/](https://pypi.org/project/dowhy/).)'
  prefs: []
  type: TYPE_NORMAL
- en: Causal effect estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step to estimating the causal effects of certain variables on the
    outcome is defining the problem with some assumptions. Let’s take the bank churn
    dataset as an example. First, when you load the data into a DataFrame, you will
    see variables as in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B30999_03_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: Snapshot of the DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the `Exited` column is the target outcome we are interested in analyzing
    the causes for. As you may notice, there are several other columns that may be
    interesting to see causal effects on customer churn. Some may be interested in
    seeing whether a high or low credit score affects customer churn. Some others
    may be interested in seeing whether a long or short tenure affects customer churn.
    You may also be curious whether having multiple products affects customer churn.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our exercise in this chapter, we will define our problem or a causal effect
    of our interest as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “Does having multiple products affect customer churn?”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'With this problem definition, we can build some assumptions that eventually
    lead to customer churn:'
  prefs: []
  type: TYPE_NORMAL
- en: Having multiple products may affect the churn rate, as it is likely that the
    people with multiple products will find it more difficult or cumbersome to switch
    to different banks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CreditScore` may affect whether a customer can have multiple products or not,
    as the credit score is used to decide whether a person is qualified to have certain
    bank products or not, such as credit cards or term loans.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Age` can also affect whether a customer has multiple products or not, as the
    older you are, the more likely you will have needed more bank products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, `Tenure`, `Balance`, and `Salary` can affect whether a customer may
    have multiple products or not. These factors may also affect customer churn.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these assumptions and the problem statement, let’s analyze the causal
    effect of a `MultipleProduct` variable on customer churn, which, using a do-calculus
    notation, can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This, in turn, means the expected change in the likelihood of customer churn
    due to a change in the variable `MultipleProduct`. As with our assumptions, there
    can be a set of other variables that affect the outcome and we may be interested
    in estimating the causal effect of these covariates on the outcome. In our example,
    these covariates are `Tenure`, `Balance`, and `Salary`, and the estimated causal
    effect of these covariates on customer churn can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_011.png)'
  prefs: []
  type: TYPE_IMG
- en: Causal model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the pre-defined problem statement and our assumptions, let’s start building
    a causal model.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will need to build a variable that we will use as the treatment.
    Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In our dataset, there is a column called `NumOfProducts`. We are simply encoding
    this into 0s and 1s, where customers with more than one product will be encoded
    as 1s and the rest as 0s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will use the `CausalModel` class within the `dowhy` package to define
    the causal model based on our assumptions, as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, we defined the newly created variable, `MultipleProduct`,
    as the treatment for which we are interested in learning the causal effect on
    the outcome, `Exited`. We defined `Balance`, `EstimatedSalary`, and `Tenure` as
    common causes that may have causal effects on the customer churn and on whether
    a customer may have multiple products or not. Lastly, we defined `Age` and `CreditScore`
    as instruments as they may affect a customer’s ability to have multiple products.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can visualize this causal model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting causal graph looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: Visualization of the causal graph'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the actual output may differ each time this code is run.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the instrument variables, `CreditScore` and `Age`, affect the treatment
    variable, `MultipleProduct`, which affects the outcome variable, `Exited`, and
    the covariates or common causes, `Balance`, `EstimatedSalary`, and `Tenure`, affect
    the treatment variable as well as the outcome variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can have the model identify the causal effects, as with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will show the following expressions for the causal effect under our
    assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: Code expressions for causal effect'
  prefs: []
  type: TYPE_NORMAL
- en: This should look familiar as this is the same as the causal effect formulas
    we discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the causal model defined, we can now estimate the effects of the treatment
    variable, `MultipleProduct`. The `estimate_effect` function makes the estimation
    of these effects straightforward, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: Causal estimate results'
  prefs: []
  type: TYPE_NORMAL
- en: Based on this result, we estimate the mean effect of the treatment variable,
    `MultipleProduct`, on the outcome, `Exited`, to be `-0.1390`. This means that
    having multiple products causes a decrease in the probability of customer churn
    of around 14%. This confirms our assumption that having multiple products will
    create stickiness toward the bank and reduce the chance of customer churn.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various methods you can use for estimation methods. To name a few:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance matching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Propensity score stratification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Propensity score matching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Propensity score weighting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used propensity score weighting for our example, but we recommend trying
    different estimation methods and analyzing how they affect the causal effect estimations.
  prefs: []
  type: TYPE_NORMAL
- en: Refutation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we have estimated the causal effect of the treatment on the outcome, we
    need to validate it. This assumption validation step is called **refutation**.
    We are going to run robustness checks of our assumptions with a few approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first approach is to introduce a random common cause or covariate variable.
    You can run this check as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We are using the `refute_estimate` function within our causal model, `model`,
    with the argument ″`random_common_cause`". This refutation method introduces a
    new, randomly generated variable to test if the causal estimate significantly
    changes, helping assess the model’s robustness against omitted variable bias.
    As you can see from the output, the original estimated effect and the new effect
    after introducing a random common cause are the same with about -0.1390 as the
    estimated causal effect value. This confirms the fact that our assumption was
    correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second approach to refute our assumption is to randomly subsample the data
    and test on this random subset. You can run this check as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the previous case, we used the same `refute_estimate` function but
    with the ″`data_subset_refuter`" argument this time. The output suggests that
    the original estimated effect and the new estimated effect with a random subset
    of the data are very close. This confirms our assumption was correct about the
    causal relationship between the `MultipleProduct` variable and the `Exited` outcome
    variable and is a generalizable result across the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the multiple estimation method options, there are multiple ways
    you can run refutations. We have experimented with random common cause addition
    and random subset replacement in this chapter. However, there are various other
    ways you can validate your assumptions. To name a few:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Replace treatment with placebo**: Replace the treatment variable with one
    known not to have a causal effect on the outcome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replace with dummy outcome**: Instead of replacing the treatment variable,
    replace the outcome variable with one unrelated to the treatment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Refute with bootstrapped random sample**: Generate multiple bootstrapped
    samples from the data and estimate the causal effect on each sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We recommend trying different refutation methods as well! The examples in the
    official documentation may be a set of useful resources and can be found here:
    [https://www.pywhy.org/dowhy/v0.11.1/example_notebooks/nb_index.html](https://www.pywhy.org/dowhy/v0.11.1/example_notebooks/nb_index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Graphical causal model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the previous causal effect estimation was to analyze and identify the
    effects of treatment variables, the **graphical causal model** enables the root
    cause analysis and the analysis of causal effects and linkages of various factors.
    This will be a great tool to have when you want to understand the causal effects
    of underlying variables and how they eventually lead to certain outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: With the same assumptions as previously, we will learn how to use the `dowhy`
    graphical causal model to better understand causal linkages among the variables.
  prefs: []
  type: TYPE_NORMAL
- en: Causal influence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the key differences using the graphical causal model is that we need
    to build a directed graph, where we define a start node to the next node. Let’s
    discuss this further with an example by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'To install the `networkx` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we are using the `networkx` package’s `DiGraph` class to create
    a directed graph. You can visualize this directed graph with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15: Directed graph'
  prefs: []
  type: TYPE_NORMAL
- en: As you may notice, this is a similar graph to one we saw in the previous section,
    where there are directed edges that go from `Age` and `CreditScore` to `MultipleProduct`.
    Similarly, there are also directed edges that go from `Balance`, `EstimatedSalary`,
    and `Tenure` to `MultipleProduct` and `Exited`. Finally, there is a directed edge
    that goes from `MultipleProduct` to `Exited`. In summary, this is a visualization
    of our assumptions about how different variables may have causal effects on one
    another.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this directed graph, we can now fit a graphical causal model as in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are using the `gcm` module in the `dowhy` package and defining the
    causal model with the `StructuralCausalModel` class within the `gcm` module. Then,
    we assign our dataset to each node in the graph with the `assign_causal_mechanisms`
    function and, finally, fit the graphical causal model with the `fit` function
    and the structural causal model and dataset as the input to the `fit` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our graphical causal model has learned the causal effects of various
    variables on the outcome, we can quantify these learned relationships using the
    `arrow_strength` function, as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'You can visualize this by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may notice from this code, we are normalizing the arrow strengths into
    percentages of the total in the first two lines of this code. The resulting graph
    should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.16: Graph with arrow strengths'
  prefs: []
  type: TYPE_NORMAL
- en: As this graph suggests, there is the strongest causal relationship between `MultipleProduct`
    and `Exited`, with about 45% contribution, and `Tenure` follows as the second
    most impactful variable for customer churn with about 25% contribution. This directed
    graph with estimated causal impacts on the outcome provides great insight for
    understanding the quantified causal effects of different variables on the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: While the arrow strengths show us the impacts of direct linkages to the outcome,
    they do not show us indirect relationships between the instrument variables, `Age`
    and `CreditScore`, and the outcome variable, `Exited`. One approach we can take
    to compute and visualize the impacts of all the causal factors on the outcome
    variable is to quantify the intrinsic causal contributions that attribute the
    variance of the outcome variable to all the upstream nodes in the directed graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `intrinsic_causal_influence` function to quantify the intrinsic
    causal contributions, as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are using 100 random samples for evaluations, as you see from the
    `num_samples_randomization` parameter input. Before we visualize the quantified
    intrinsic causal influences, we will normalize the scale so that they sum up to
    100% first and then visualize them with a bar plot, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code should produce a bar plot like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.17: Bar plot of variance attribution for each factor'
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the variable `MultipleProduct` has the highest contribution to
    the variance of the outcome variable among all the variables. `Balance` comes
    next as the second most contributing variable. The variables `Age` and `CreditScore`
    seem to have very little attribution to the variance of the outcome, `Exited`.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a Shapley value?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may notice that “Shapley values” appear while running causal inference
    code. Simply put, Shapley values are the estimations of the degrees of impact
    of individual variables on the target variable. If you would like to learn more,
    we suggest you take a look at the package documentation on how it computes the
    feature relevance here: [https://www.pywhy.org/dowhy/main/user_guide/causal_tasks/root_causing_and_explaining/feature_relevance.html](https://www.pywhy.org/dowhy/main/user_guide/causal_tasks/root_causing_and_explaining/feature_relevance.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly attribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, we have seen ways to analyze the overall causal effects of individual
    variables on the outcome variable. However, there will be cases where we want
    to dig deeper into understanding which variables may have contributed in what
    way. For example, within our dataset, one may be curious about how a certain customer’s
    `Balance` amount may have caused them to churn or how that person’s `Tenure` with
    the bank may have impacted their churn. In order to figure out the causal contributions
    of different variables for individual customer churn, we can use the `attribute_anomalies`
    function, as in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example, we chose the first customer who churned as an example, as you
    can see from the input for the `anomaly_samples` parameter. We can visualize the
    individual attributions on the outcome using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: As this code suggests, we are normalizing the attributions so that they sum
    up to 100% and then visualizing individual attributions with a bar plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting bar plot will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.18: Bar plot of anomaly attribution for each factor'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go deeper into the interpretations of this bar plot, let’s first
    look at the sample we chose for quantifying individual causal contributions. Take
    a look at the following code to take a closer look at an individual sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_19.png)Figure 3.19: Selected sample'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we combine the attribution results with this customer’s info, we can dissect
    which factors were the most influential that ended up making this customer churn.
    We see from this customer’s record that this person only had one product with
    the bank and this was the most contributing factor to why this customer churned.
    On the other hand, `Balance`, `EstimatedSalary`, and `Tenure` actually reduced
    the chance of customer churn as they have negative scores. Let’s look at another
    sample, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This has output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.20: Selected sample'
  prefs: []
  type: TYPE_NORMAL
- en: 'This customer’s causal contribution bar plot looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_03_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.21: Bar plot of customer’s causal contribution'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from this second example, this customer had a much higher balance,
    longer tenure, and more than one product with the bank, compared to the first
    example. For this customer, this higher balance, longer tenure, and higher estimated
    salary had a positive impact on why the customer churned, while the fact that
    this customer had more than one product reduced the chance of this customer churning.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen from these examples, causal analysis and causal effect estimations
    bring deep insights into why people churn from your products. When used along
    with other methods and AI/ML models, this causal analysis will bring actionable
    insights that you can employ in your future marketing campaigns.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we gained an in-depth understanding of the factors affecting
    certain customer behaviors. We explored how **regression analysis** can help us
    understand the directional relationships between various factors and the outcome
    of customer behavior. Using our auto insurance marketing dataset as an example,
    we saw how to implement the `statsmodels` package in Python to run regression
    analysis and unveil the successes behind engagement rate marketing campaigns.
    We also discussed how **decision trees** can help us identify complex interactions
    that result in certain outcomes. Using a bank marketing dataset as an example
    and the `scikit-learn` package in Python, we successfully built a decision tree
    that unveiled the hidden interactions among various factors that lead to customer
    conversions. Lastly, with the bank churn dataset and the `dowhy` package in Python,
    we saw how **causal analysis** can bring deep insights into the root causes and
    directional contributions to the outcome of an event.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to explore time-series data. In marketing,
    there are certain temporal patterns that repeat in certain ways. We will be discussing
    how to break down time-series data into overall trend, seasonality, and random
    noise components and how to utilize the trends and seasonalities that are identified
    through time-series decomposition for marketing initiatives, as well as how to
    do time-series forecasting, which can equip you with better planning and preparedness
    for certain events that may happen in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/genai](https://packt.link/genai)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code12856128601808671.png)'
  prefs: []
  type: TYPE_IMG
